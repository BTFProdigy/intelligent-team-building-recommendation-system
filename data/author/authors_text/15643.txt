Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 433?443,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Timeline Generation through Evolutionary Trans-Temporal Summarization
Rui Yan?, Liang Kong? , Congrui Huang?, Xiaojun Wan?, Xiaoming Li\, Yan Zhang??
?School of Electronics Engineering and Computer Science, Peking University, China
?Institute of Computer Science and Technology, Peking University, China
\State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China
{r.yan,kongliang,hcr,lxm}@pku.edu.cn,
wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn
Abstract
We investigate an important and challeng-
ing problem in summary generation, i.e.,
Evolutionary Trans-Temporal Summarization
(ETTS), which generates news timelines from
massive data on the Internet. ETTS greatly
facilitates fast news browsing and knowl-
edge comprehension, and hence is a neces-
sity. Given the collection of time-stamped web
documents related to the evolving news, ETTS
aims to return news evolution along the time-
line, consisting of individual but correlated
summaries on each date. Existing summariza-
tion algorithms fail to utilize trans-temporal
characteristics among these component sum-
maries. We propose to model trans-temporal
correlations among component summaries for
timelines, using inter-date and intra-date sen-
tence dependencies, and present a novel com-
bination. We develop experimental systems to
compare 5 rival algorithms on 6 instinctively
different datasets which amount to 10251 doc-
uments. Evaluation results in ROUGE metrics
indicate the effectiveness of the proposed ap-
proach based on trans-temporal information.
1 Introduction
Along with the rapid growth of the World Wide
Web, document floods spread throughout the Inter-
net. Given a large document collection related to
a news subject (for example, BP Oil Spill), readers
get lost in the sea of articles, feeling confused and
powerless. General search engines can rank these
?Corresponding author.
news webpages by relevance to a user specified as-
pect, i.e., a query such as ?first relief effort for BP
Oil Spill?, but search engines are not quite capable
of ranking documents given the whole news subject
without particular aspects. Faced with thousands of
news documents, people usually have a myriad of in-
terest aspects about the beginning, the development
or the latest situation. However, traditional infor-
mation retrieval techniques can only rank webpages
according to their understanding of relevance, which
is obviously insufficient (Jin et al, 2010).
Even if the ranked documents could be in a satis-
fying order to help users understand news evolution,
readers prefer to monitor the evolutionary trajecto-
ries by simply browsing rather than navigate every
document in the overwhelming collection. Summa-
rization is an ideal solution to provide an abbrevi-
ated, informative reorganization for faster and bet-
ter representation of news documents. Particularly,
a timeline (see Table 1) can summarize evolutionary
news as a series of individual but correlated com-
ponent summaries (items in Table 1) and offer an
option to understand the big picture of evolution.
With unique characteristics, summarizing time-
lines is significantly different from traditional sum-
marization methods which are awkward in such sce-
narios. We first study a manual timeline of BP Oil
Spill in Mexico Gulf in Table 1 from Reuters News1
to understand why timelines generation is observ-
ably different from traditional summarization. No
traditional method has considered to partition corpus
into subsets by timestamps for trans-temporal cor-
relations. However, we discover two unique trans-
1http://www.reuters.com
433
Table 1: Part of human generated timeline about BP Oil
Spill in 2010 from Reuters News website.
April 22, 2010
The Deepwater Horizon rig, valued at more than $560 million,
sinks and a five mile long (8 km) oil slick is seen.
April 25, 2010
The Coast Guard approves a plan to have remote underwater vehi-
cles activate a blowout preventer and stop leak. Efforts to activate
the blowout preventer fail.
April 28, 2010
The Coast Guard says the flow of oil is 5,000 barrels per day (bpd)
(210,000 gallons/795,000 litres) ? five times greater than first esti-
mated. A controlled burn is held on the giant oil slick.
April 29, 2010
U.S. President Barack Obama pledges ?every single available re-
source,? including the U.S. military, to contain the spreading spill.
Obama also says BP is responsible for the cleanup. Louisiana de-
clares state of emergency due to the threat to the state?s natural
resources.
April 30, 2010
An Obama aide says no drilling will be allowed in new areas, as the
president had recently proposed, until the cause of the Deepwater
Horizon accident is known.
temporal characteristics of component summaries
from the handcrafted timeline. Individuality. The
component summaries are summarized locally: the
component item on date t is constituted by sentences
with timestamp t. Correlativeness. The compo-
nent summaries are correlative across dates, based
on the global collection. To the best of our knowl-
edge, no traditional method has examined the rela-
tionships among these timeline items.
Although it is profitable, summarizing timeline
faces with new challenges:
? The first challenge for timeline generation is
to deliver important contents and avoid information
overlaps among component summaries under the
trans-temporal scenario based on global/local source
collection. Component items are individual but not
completely isolated due to the dynamic evolution.
? As we have individuality and correlativeness
to evaluate the qualities of component summaries,
both locally and globally, the second challenge is to
formulate the combination task into a balanced op-
timization problem to generate the timelines which
satisfy both standards with maximum utilities.
We introduce a novel approach for the web min-
ing problem Evolutionary Trans-Temporal Summa-
rization (ETTS). Taking a collection relevant to a
news subject as input, the system automatically out-
puts a timeline with items of component summaries
which represent evolutionary trajectories on specific
dates. We classify sentence relationships as inter-
date and intra-date dependencies. Particularly, the
inter-date dependency calculation includes temporal
decays to project sentences from all dates onto the
same time horizon (Figure 1 (a)). Based on intra-
/inter-date sentence dependencies, we then model
affinity and diversity to compute the saliency score
of each sentence and merge local and global rank-
ings into one unified ranking framework. Finally we
select top ranked sentences. We build an experimen-
tal system on 6 real datasets to verify the effective-
ness of our methods compared with other 4 rivals.
2 Related Work
Multi-document summarization (MDS) aims to pro-
duce a summary delivering the majority of informa-
tion content from a set of documents and has drawn
much attention in recent years. Conferences such as
ACL, SIGIR, EMNLP, etc., have advanced the tech-
nology and produced several experimental systems.
Generally speaking, MDS methods can be either
extractive or abstractive summarization. Abstractive
summarization (e.g. NewsBlaster2) usually needs
information fusion, sentence compression and refor-
mulation. We focus on extraction-based methods,
which usually involve assigning saliency scores to
some units (e.g. sentences, paragraphs) of the docu-
ments and extracting the units with highest scores.
To date, various extraction-based methods have
been proposed for generic multi-document summa-
rization. The centroid-based method MEAD (Radev
et al, 2004) is an implementation of the centroid-
based method that scores sentences based on fea-
tures such as cluster centroids, position, and TF.IDF,
etc. NeATS (Lin and Hovy, 2002) adds new features
such as topic signature and term clustering to select
important content, and use MMR (Goldstein et al,
1999) to remove redundancy.
Graph-based ranking methods have been pro-
posed to rank sentences/passages based on ?votes?
or ?recommendations? between each other. Tex-
tRank (Mihalcea and Tarau, 2005) and LexPageR-
ank (Erkan and Radev, 2004) use algorithms similar
to PageRank and HITS to compute sentence impor-
tance. Wan et al have improved the graph-ranking
2http://www1.cs.columbia.edu/nlp/newsblaster/
434
algorithm by differentiating intra-document and
inter-document links between sentences (2007b),
and have proposed a manifold-ranking method to
utilize sentence-to-sentence and sentence-to-topic
relationships (Wan et al, 2007a).
ETTS seems to be related to a very recent task of
?update summarization? started in DUC 2007 and
continuing with TAC. However, update summariza-
tion only dealt with a single update and we make a
novel contribution with multi-step evolutionary up-
dates. Further related work includes similar timeline
systems proposed by (Swan and Allan, 2000) us-
ing named entities, by (Allan et al, 2001) measured
in usefulness and novelty, and by (Chieu and Lee,
2004) measured in interest and burstiness. We have
proposed a timeline algorithm named ?Evolution-
ary Timeline Summarization (ETS)? in (Yan et al,
2011b) but the refining process based on generated
component summaries is time consuming. We aim
to seek for more efficient summarizing approach.
To the best of our knowledge, neither update sum-
marization nor traditional systems have considered
the relationship among ?component summaries?, or
have utilized trans-temporal properties. ETTS ap-
proach can also naturally and simultaneously take
into account global/local summarization with biased
information richness and information novelty, and
combine both summarization in optimization.
3 Trans-temporal Summarization
We conduct trans-temporal summarization based on
the global biased graph using inter-date dependency
and local biased graph using intra-date dependency.
Each graph is the complementary graph to the other.
3.1 Global Biased Summarization
The intuition for global biased summarization is that
the selected summary should be correlative with sen-
tences from neighboring dates, especially with those
informative ones. To generate the component sum-
mary on date t, we project all sentences in the collec-
tion onto the time horizon of t to construct a global
affinity graph, using temporal decaying kernels.
3.1.1 Temporal Proximity Based Projection
Clearly, a major technical challenge in ETTS is
how to define the temporal biased projection func-
tion ?(?t), where ?t is the distance between the
Figure 1: Construct global/local biased graphs. Solid cir-
cles denote intra-date sentences on the pending date t and
dash ones represent inter-date sentences from other dates.
Figure 2: Proximity-based kernel functions, where ?=10.
pending date t and neighboring date t?, i.e., ?t =
|t? ? t|. As in (Lv and Zhai, 2009), we present 5
representative kernel functions: Gaussian, Triangle,
Cosine, Circle, and Window, shown in Figure 2. Dif-
ferent kernels lead to different projections.
1. Gaussian kernel
?(?t) = exp[??t
2
2?2 ]
2. Triangle kernel
?(?t) =
{
1? ?t? if ?t ? ?
0 otherwise
3. Cosine (Hamming) kernel
?(?t) =
{
1
2 [1 + cos(?t?pi? )] if ?t ? ?
0 otherwise
4. Circle kernel
?(?t) =
{?
1? (?t? )2 if ?t ? ?
0 otherwise
435
5. Window kernel
?(?t) =
{
1 if ?t ? ?
0 otherwise
All kernels have one parameter ? to tune, which
controls the spread of kernel curves, i.e., it restricts
the projection scope of each sentence. In general,
the optimal setting of ? may vary according to the
news set because sentences presumably would have
wider semantic scope in certain news subjects, thus
requiring a higher value of ? and vice versa.
3.1.2 Modeling Global Affinity
Given the sentence collectionC partitioned by the
timestamp set T , C = {C1, C2, . . . , C |T |}, we ob-
tain Ct = {sti|1 ? i ? |Ct|} where si is a sentence
with the timestamp t = tsi . When we generate com-
ponent summary on t, we project all sentences onto
time horizon t. After projection, all sentences are
weighted by their influence on t. We use an affinity
matrix M t with the entry of the inter-date transition
probability on date t. The sum of each row equals to
1. Note that for the global biased matrix, we mea-
sure the affinity between local sentences from t and
global sentences from other dates. Therefore, intra-
date transition probability between sentences with
the timestamp t is set to 0 for local summarization.
M ti,j is the transition probability of si to sj based
on the perspective of date t, i.e., p(si ? sj |t):
p(si ? sj |t) =
{ f(si?sj |t)?
|C| f(si?sk|t)
if ? f 6= 0
0 if tsi = tsj = t
(1)
f(si ? sj |t) is defined as the temporal weighted
cosine similarity between two sentences:
f(si ? sj |t) =
?
w?si?sj
pi(w, si|t) ? pi(w, sj |t) (2)
where the weight pi associated with term w is calcu-
lated with the temporal weighted tf.isf formula:
pi(w, s|t) =
?|t? ts| ? tf(w, s)(1 + log( |C|Nw ))??
|s|(tf(w, s)(1 + log(
|C|
Nw )))2
.
(3)
where ts is the timestamp of sentence s, and
tf(w, s) is the term frequency of w in s. ts can be
any date from T . |C| is the sentences set size and
Nw is the number of sentences containing term w.
We let p(si ? si|t)=0 to avoid self transition.
Note that although f(.) is a symmetric function,
p(si ? sj |t) is usually not equal to p(sj ? si|t),
depending on the degrees of nodes si and sj .
Now we establish the affinity matrix M ti,j and by
using the general form of PageRank, we obtain:
~? = ?M?1~?+ 1? ?|C| ~e (4)
where ~? is the selective probability of all sentence
nodes and ~e is a column vector with all elements
equaling to 1. ? is the damping factor set as 0.85.
Usually the convergence of the iteration algorithm is
achieved when difference between the scores com-
puted at two successive iterations for any sentences
falls below a given threshold (0.0001 in this study).
3.1.3 Modeling Diversity
Diversity is to reflect both biased information
richness and sentence novelty, which aims to reduce
information redundancy. However, using standard
PageRank of Equation (4) will not result in diver-
sity. The aggregational effect of PageRank assigns
high salient scores to closely connected node com-
munities (Figure 3 (b)). A greedy vertex selection
algorithm may achieve diversity by iteratively se-
lecting the most prestigious vertex and then penal-
izing the vertices ?covered? by the already selected
ones, such as Maximum Marginal Relevance and its
applications in Wan et al (2007b; 2007a). Most re-
cently diversity rank DivRank is another solution
to diversity penalization in (Mei et al, 2010).
We incorporate DivRank in our general ranking
framework, which creates a dynamicM during each
iteration, rather than a static one. After z times of
iteration, the matrix M becomes:
M (z) = ?M (z?1) ? ~?(z?1) + 1? ?|C| ~e (5)
Equation (5) raises the probability for nodes with
higher centrality and nodes already having high
weights are likely to ?absorb? the weights of its
neighbors directly, and the weights of neighbors?
neighbors indirectly. The process is to iteratively ad-
just matrix M according to ~? and then to update ~?
according to the changed M . As iteration increases
436
there emerges a rich-gets-richer phenomenon (Fig-
ure 3 (c) and (d)). By incorporating DivRank, we
obtain rank r?i and the global biased ranking score
Gi for sentence si from date t to summarize Ct.
3.2 Local Biased Summarization
Naturally, the component summary for date t should
be informative within Ct. Given the sentence col-
lection Ct = {sti|1 ? i ? |Ct|}, we build an affin-
ity matrix for Figure 1 (b), with the entry of intra-
date transition probability calculated from standard
cosine similarity. We incorporate DivRank within
local summarization and we obtain the local biased
rank and ranking score for si, denoted as r?i and Li.
3.3 Optimization of Global/Local Combination
We do not directly add the global biased ranking
score and local biased ranking score, as many previ-
ous works did (Wan et al, 2007b; Wan et al, 2007a),
because even the same ranking score gap may indi-
cate different rank gaps in two ranking lists.
Given subset Ct, let R = {ri}(i = 1,. . . ,|Ct|), ri
is the final ranking of si to estimate, optimize the
following objective cost function O(R),
O(R) =?
|Ct|?
i=1
Gi?
ri
?i
? r
?
i
Gi
?2
+ ?
|Ct|?
i=1
Li?
ri
?i
? r
?
i
Li
?2
(6)
where Gi is the global biased ranking score while Li
is the local biased ranking score. ?i is expected to
be the merged ranking score, namely sentence im-
portance, which will be defined later. Among the
two components in the objective function, the first
component means that the refined rank should not
deviate too much from the global biased rank. We
use ? ri?i ?
r?i
Gi ?
2 instead of ?ri? r?i ?2 in order to dis-
tinguish the differences between sentences from the
same rank gap. The second component is similar by
refining rank from local biased summarization.
Our goal is to find R = R? to minimize the cost
function, i.e.,R? = argmin{O(R)}. R? is the final
rank merged by our algorithm. To minimize O(R),
we compute its first-order partial derivatives.
?O(R)
?ri
= 2??i
( Gi?i
ri ? r?i ) +
2?
?i
(Li?i
ri ? r?i ) (7)
Let ?O(R)?ri = 0, we get
r?i =
??ir?i + ??ir
?
i
?Gi + ?Li
(8)
Two special cases are that if (1) ? = 0, ? 6= 0:
we obtain ri = ?ir?i /Li, indicating we only use the
local ranking score. (2) ? 6= 0, ? = 0, indicating we
ignore local ranking score and only consider global
biased summarization using inter-date dependency.
There can be many ways to calculate the sen-
tence importance ?i. Here we define ?i as the
weighted combination of itself with ranking scores
from global biased and local biased summarization:
?(z)i =
?Gi + ?Li + ??(z?1)i
?+ ? + ? . (9)
To save one parameter we let ?+?+? = 1. In the z-
th iteration, r(z)i is dependent on ?(z?1)i and ?(z)i is
indirectly dependent on r(z)i via ?(z?1)i . ?(0)i = 0.
We iteratively approximate final ?i for the ultimate
rank listR?. The expectation of stable ?i is obtained
when ?(z)i = ?(z?1)i . Final ?i is expected to satisfy
?i = ?Gi + ?Li + ??i:
?i =
?Gi + ?Li
1? ? =
?Gi + ?Li
?+ ? (10)
Final ?i is dependent only on original global/local
biased ranking scores. Equation (8) becomes more
concise with no ? or ?: r? is a weighted combina-
tion of global and local ranks by ?? (? 6= 0, ? 6= 0):
r?i =
?
?+ ? r
?
i +
?
?+ ? r
?
i
= 11 + ?/?r
?
i +
1
1 + ?/? r
?
i
(11)
4 Experiments and Evaluation
4.1 Datasets
There is no existing standard test set for ETTS meth-
ods. We randomly choose 6 news subjects with
special coverage and handcrafted timelines by ed-
itors from 10 selected news websites: these 6 test
sets consist of news datasets and golden standards to
evaluate our proposed framework empirically, which
amount to 10251 news articles. As shown in Ta-
ble 2, three of the sources are in UK, one of them
437
(a) An illustrative network. (a) PageRank on t. (b) DivRank on t (c) DivRank on t?
Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from
DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect
temporal biased processes on t {1,9} in (c) and t? {2,12} in (d).
is in China and the rest are in the US. We choose
these sites because many of them provide timelines
edited by professional editors, which serve as refer-
ence summaries. The news belongs to different cate-
gories of Rule of Interpretation (ROI) (Kumaran and
Allan, 2004). More detailed statistics are in Table 3.
Table 2: News sources of 6 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 6 datasets.
News Subjects #size #docs #stamps #RT AL
1.Influenza A 115026 2557 331 5 83
2.Financial Crisis 176435 2894 427 2 118
3.BP Oil Spill 63021 1468 135 6 76
4.Haiti Earthquake 12073 247 83 2 32
5.Jackson Death 37819 925 168 3 64
6.Obama Presidency 79761 2160 349 5 92
size: the whole sentence counts; #stamps: the number of timestamps;
Note average size of subsets is calculated as: avg.size=#size/#stamps;
RT: reference timelines; AL: avg. length of RT measured in sentences.
4.2 Experimental System Setups
? Preprocessing. As ETTS faces with much larger
corpus compared with traditional MDS, we apply
further data preprocessing besides stemming and
stop-word removal. We extract text snippets repre-
senting atomic ?events? from all documents with a
toolkit provided by Yan et al (2010; 2011a), by
which we attempt to assign more fine-grained and
accurate timestamps for every sentence within the
text snippets. After the snippet extraction procedure,
we filter the corpora by discarding non-event texts.
? Compression Rate and Date Selection. After
preprocessing, we obtain numerous snippets with
fine-grained timestamps, and then decompose them
into temporally tagged sentences as the global col-
lection C. We partition C according to timestamps
of sentences, i.e., C = C1 ? C2 ? ? ? ? ? C |T |.
Each component summary is generated from its cor-
responding sub-collection. The sizes of component
summaries are not necessarily equal, and moreover,
not all dates may be represented, so date selection
is also important. We apply a simple mechanism
that users specify the overall compression rate ?, and
we extract more sentences for important dates while
fewer sentences for others. The importance of dates
is measured by the burstiness, which indicates prob-
able significant occurrences (Chieu and Lee, 2004).
The compression rate on ti is set as ?i = |Ci||C| .
4.3 Evaluation Metrics
The ROUGE measure is widely used for evaluation
(Lin and Hovy, 2003): the DUC contests usually of-
ficially employ ROUGE for automatic summariza-
tion evaluation. In ROUGE evaluation, the summa-
rization quality is measured by counting the num-
ber of overlapping units, such as N-gram, word se-
quences, and word pairs between the candidate time-
lines CT and the reference timelines RT . There are
several kinds of ROUGE metrics, of which the most
important one is ROUGE-N with 3 sub-metrics:
1 ROUGE-N-R is an N-gram recall metric:
ROUGE-N-R =
?
I?RT
?
N-gram?I
Countmatch(N-gram)
?
I?RT
?
N-gram?I
Count (N-gram)
438
2 ROUGE-N-P is an N-gram precision metric:
ROUGE-N-P =
?
I?CT
?
N-gram?I
Countmatch(N-gram)
?
I?CT
?
N-gram?I
Count (N-gram)
3 ROUGE-N-F is an N-gram F1 metric:
ROUGE-N-F = 2? ROUGE-N-P? ROUGE-N-RROUGE-N-P + ROUGE-N-R
I denotes a timeline. N in these metrics stands for
the length of N-gram and N-gram?RT denotes the
N-grams in reference timelines while N-gram?CT
denotes the N-grams in the candidate timeline.
Countmatch(N-gram) is the maximum number of N-
gram in the candidate timeline and in the set of ref-
erence timelines. Count(N-gram) is the number of N-
grams in reference timelines or candidate timelines.
According to (Lin and Hovy, 2003), among all
sub-metrics, unigram-based ROUGE (ROUGE-1)
has been shown to agree with human judgment most
and bigram-based ROUGE (ROUGE-2) fits summa-
rization well. We report three ROUGE F-measure
scores: ROUGE-1, ROUGE-2, and ROUGE-W,
where ROUGE-W is based on the weighted longest
common subsequence. The weight W is set to be
1.2 in our experiments by ROUGE package (version
1.55). Intuitively, the higher the ROUGE scores, the
similar the two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used sum-
marization algorithms as baseline systems. They
are designed for traditional summarization without
trans-temporal dimension. The first intuitive way to
generate timelines by these methods is via a global
summarization on collection C and then distribu-
tion of selected sentences to their source dates. The
other one is via an equal summarization on all local
sub-collections. For baselines, we average both in-
tuitions as their performance scores. For fairness we
conduct the same preprocessing for all baselines.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according
to the following three parameters: centroid value,
positional value, and first-sentence overlap.
GMDS: The graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
Chieu: (Chieu and Lee, 2004) present a simi-
lar timeline system with different goals and frame-
works, utilizing interest and burstiness ranking but
neglecting trans-temporal news evolution.
ETTS: ETTS is an algorithm with optimized
combination of global/local biased summarization.
RefTL: As we have used multiple human time-
lines as references, we not only provide ROUGE
evaluations of the competing systems but also of the
human timelines against each other, which provides
a good indicator as to the upper bound ROUGE
score that any system could achieve.
4.5 Overall Performance Comparison
We use a cross validation manner among 6 datasets,
i.e., train parameters on one subject set and exam-
ine the performance on the others. After 6 training-
testing processes, we take the average F-score per-
formance in terms of ROUGE-1, ROUGE-2, and
ROUGE-W on all sets. The overall results are shown
in Figure 4 and details are listed in Tables 4?6.
Figure 4: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected.
? The results of Centroid are better than those of
Random, mainly because the Centroid method takes
439
Table 4: Overall performance comparison on Influenza
A (ROI? category: Science) and Financial Crisis (ROI
category: Finance). ?=0.4, kernel=Gaussian, ?=60.
1. Influenza A 2. Financial Crisis
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.491 0.114 0.161 0.458 0.112 0.159
Random 0.257 0.039 0.081 0.230 0.030 0.071
Centroid 0.331 0.050 0.114 0.305 0.041 0.108
GMDS 0.364 0.062 0.130 0.327 0.054 0.110
Chieu 0.350 0.059 0.128 0.325 0.052 0.109
ETTS 0.375 0.071 0.132 0.339 0.058 0.112
Table 5: Overall performance comparison on BP Oil
(ROI category: Accidents) and Haiti Quake (ROI cate-
gory: Disasters). ?=0.4, kernel=Gaussian, ?=30.
3. BP Oil 4. Haiti Quake
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.517 0.135 0.183 0.528 0.139 0.187
Random 0.262 0.041 0.096 0.266 0.043 0.093
Centroid 0.369 0.062 0.128 0.362 0.060 0.129
GMDS 0.389 0.084 0.139 0.380 0.106 0.137
Chieu 0.384 0.083 0.139 0.383 0.110 0.138
ETTS 0.441 0.107 0.158 0.436 0.111 0.145
Table 6: Overall performance comparison on Jackson
Death (ROI category: Legal Cases) and Obama Presi-
dency (ROI category: Politics). ?=0.4, kernel=Gaussian,
?=30.
5. Jackson Death 6. Obama Presidency
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.482 0.113 0.161 0.495 0.115 0.163
Random 0.232 0.033 0.080 0.254 0.039 0.084
Centroid 0.320 0.051 0.109 0.325 0.053 0.111
GMDS 0.341 0.059 0.127 0.359 0.061 0.129
Chieu 0.344 0.059 0.128 0.346 0.060 0.125
ETTS 0.358 0.061 0.130 0.369 0.074 0.133
?ROI: news categorization defined by Linguistic Data Consortium.
into account positional value and first-sentence over-
lap, which facilitate main aspects summarization.
? The GMDS system outperforms centroid-based
summarization methods. This is due to the fact that
PageRank-based framework ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
Traditional MDS only consider sentence selection
from either the global or the local scope, and hence
bias occurs. Mis-selected sentences result in a low
recall. Generally the performance of global priority
intuition (i.e. only global summarization and then
distribution to temporal subsets) is better than local
priority methods (only local summarization). Proba-
ble bias is enlarged by searching for worthy sentence
in single dates. However, precision drops due to ex-
cessive choice of global timeline-worthy sentences.
Figure 5: ?/?: global/local combination.
Figure 6: ? on long topics (?1 year).
Figure 7: ? on short topics (<1 year).
? In general, the result of Chieu is better than
Centroid but unexpectedly, worse than GMDS. The
reason may be that Chieu does not capture suffi-
cient timeline attributes. The ?interest? modeled
440
in the algorithms actually performs flat clustering-
based summarization which is proved to be less use-
ful (Wang and Li, 2010). GMDS utilizes sentence
linkage, and partly captures ?correlativeness?.
? ETTS under our proposed framework outper-
forms baselines, indicating that the properties we
use for timeline generation are beneficial. We also
add a direct comparison between ETTS and ETS
(Yan et al, 2011b). We notice that both balanced
algorithms achieve comparable performance (0.386
v.s. 0.412: a gap of 0.026 in terms of ROUGE-
1), but ETTS is much faster than ETS. It is under-
standable that ETS refines timelines based on neigh-
boring component summaries iteratively while for
ETTS neighboring information is incorporated in
temporal projection and hence there is no such pro-
cedure. Furthermore, ETS has 8 free parameters to
tune while ETTS has only 2 parameters. In other
words, ETTS is more simple to control.
? The performance on intensive focused news
within short time range (|last timestamp?first times-
tamp |<1 year) is better than on long lasting news.
Having proved the effectiveness of our proposed
methods, we carry the next move to identity how
global?local combination ratio ?/? and projection
kernels take effects to enhance the quality of a sum-
mary in parameter tuning.
4.6 Parameter Tuning
Each time we tune one parameter while others are
fixed. To identify how global and local biased sum-
marization combine, we provide experiments on the
performance of varying ?/? in Figure 5. Results in-
dicate that a balance between global and local biased
summarization is essential for timeline generation
because the performance is best when ?? ? [10, 100]and outperforms global and local summarization in
isolation, i.e., when ?=0 or ? = 0 in Figure 5. Inter-
estingly, we conclude an opposite observation com-
pared with ETS. Different approaches might lead to
different optimum of global/local combination.
Another key parameter ? measures the temporal
projection influence from global collection to local
collection and hence the size of neighboring sen-
tence set. 6 datasets are classified into two groups.
Subject 1, 2, 6 are grouped as long news with a time
span of more than one year and the others are short
news. The effect of ? varies on long news sets and
short news sets. In Figure 6 ? is best around 60 and
in Figure 7 it is best at about 20?40, indicating long
news has relatively wider semantic scope.
We then examine the effect of different projection
kernels. Generally, Gaussian kernel outperforms
others and window kernel is the worst, probably be-
cause Gaussian kernel provides the best smoothing
effect with no arbitrary cutoffs. Window kernel fails
to distinguish different weights of neighboring sets
by temporal proximity, so its performance is as ex-
pected. Other 3 kernels are comparable.
4.7 Sample Output and Case Study
Sample output is presented in Table 7 and it shares
major information similarity with the human time-
line in Table 1. Besides, we notice that a dynamic
?i is reasonable. Important burstiness is worthy of
more attention. Fewer sentences are selected on the
dates when nothing new occurs.
Interesting Findings. We notice that humans have
biases to generate timelines for they have (1) pref-
erence on local occurrences and (2) different writ-
ing styles. For instance, news outlets from United
States tend to summarize reactions by US govern-
ment while UK websites tend to summarize British
affairs. Some editors favor statistical reports while
others prefer narrative style, and some timelines
have detailed explanations while others are ex-
tremely concise with no more than two sentences for
each entry. Our system-generated timelines have a
large variance among all golden standards. Proba-
bly a new evaluation metric should be introduced to
measure the quality of human generated timelines
to mitigate the corresponding biases. A third in-
teresting observation is that subjects have different
volume patterns, e.g., H1N1 has a slow start and a
bursty evolution and BP Oil has a bursty start and a
quick decay. Obama is different in nature because
the report volume is temporally stable and scattered.
5 Conclusion
We present a novel solution for the important
web mining problem, Evolutionary Trans-Temporal
Summarization (ETTS), which generates trajectory
timelines for news subjects from massive data. We
formally formulate ETTS as a combination of global
and local summarization, incorporating affinity and
441
Table 7: Selected part of timeline generated by ETTS for BP Oil.
April 20, 2010
s1: An explosion on the Deepwater Horizon offshore oil drilling rig in
the Gulf of Mexico, around 40 miles south east of Louisiana, causing
several kills and injuries.
s2: The rig was drilling in about 5,000ft (1,525m) of water, pushing
the boundaries of deepwater drilling technology.
s3: The rig is owned and operated by Transocean, a company hired by
BP to carry out the drilling work.
s4: Deepwater Horizon oil rig fire leaves 11 missing.
April 22, 2010
s1: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s2: The Deepwater Horizon sinks to the bottom of the Gulf after burn-
ing for 36 hours, raising concerns of a catastrophic oil spill.
s3: Deepwater Horizon rig sinks in 5,000ft of water.
April 23, 2010
s1: The US coast guard suspends the search for missing workers, who
are all presumed dead.
s2: The Coast Guard says it had no indication that oil was leaking from
the well 5,000ft below the surface of the Gulf.
s3: Underwater robots try to shut valves on the blowout preventer to
stop the leak, but BP abandons that failed effort two weeks later.
s4: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s5: Deepwater Horizon clean-up workers fight to prevent disaster.
April 24, 2010
s1: Oil is found to be leaking from the well.
April 26, 2010
s1: BP?s shares fall 2% amid fears that the cost of cleanup and legal
claims will hit the London-based company hard.
s2: Roughly 15,000 gallons of dispersants and 21,000ft of containment
boom are placed at the spill site.
April 27, 2010
s1: BP reports a rise in profits, due in large part to oil price increases,
as shares rise again.
s2: The US departments of interior and homeland security announce
plans for a joint investigation of the explosion and fire.
s3: Minerals Management Service (MMS) approves a plan for two re-
lief wells.
s4: BP chairman Tony Hayward says the company will take full re-
sponsibility for the spill, paying for legitimate claims and cleanup cost.
April 28, 2010
s1: The coast guard says the flow of oil is 5,000bpd, five times greater
than first estimated, after a third leak is discovered.
s2: BP?s attempts to repair a hydraulic leak on the blowout preventer
valve are unsuccessful.
s3: BP reports that its first-quarter profits more than double to ?3.65
billion following a rise in oil prices.
s4: Controlled burns begin on the giant oil slick.
diversity into a unified ranking framework. We im-
plement a system under such framework for ex-
periments on real web datasets to compare all ap-
proaches. Through our experiment we notice that
the combination plays an important role in timeline
generation, and global optimization weights slightly
higher (?/? ? [10, 100]), but auxiliary local infor-
mation does help to enhance performance in ETTS.
Acknowledgments
This work was partially supported by NSFC with
Grant No.61073082, 60933004, 70903008 and
61073081, and Xiaojun Wan was supported by
NSFC with Grant No.60873155 and Beijing Nova
Program (2008B03).
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 121?128.
Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010.
Topic initiator detection on the world wide web. In
Proceedings of the 19th international conference on
WWW?10, pages 481?490.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL?03, pages 71?78.
442
Yuanhua Lv and ChengXiang Zhai. 2009. Positional lan-
guage models for information retrieval. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09, pages 299?306.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD?10, pages 1009?1018.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Russell Swan and James Allan. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR?00, pages 49?56.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Dingding Wang and Tao Li. 2010. Document update
summarization using incremental hierarchical cluster-
ing. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ?10, pages 279?288.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In Information Retrieval Tech-
nology - 6th Asia Information Retrieval Societies Con-
ference, AIRS 2010, pages 490?501.
Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming
Li. 2011a. A fine-grained digestion of news webpages
through event snippet extraction. In Proceedings of
the 20th international conference companion on world
wide web, WWW ?11, pages 157?158.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings of
the 34th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?11.
443
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1342?1351,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Summarize What You Are Interested In:
An Optimization Framework for Interactive Personalized Summarization
Rui Yan
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Jian-Yun Nie
De?partement d?informatique
et de recherche ope?rationnelle,
Universite? de Montre?al,
Montre?al, H3C 3J7 Que?bec, Canada
nie@iro.umontreal.ca
Xiaoming Li
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
lxm@pku.edu.cn
Abstract
Most traditional summarization methods treat
their outputs as static and plain texts, which
fail to capture user interests during summa-
rization because the generated summaries are
the same for different users. However, users
have individual preferences on a particular
source document collection and obviously a
universal summary for all users might not al-
ways be satisfactory. Hence we investigate
an important and challenging problem in sum-
mary generation, i.e., Interactive Personalized
Summarization (IPS), which generates sum-
maries in an interactive and personalized man-
ner. Given the source documents, IPS captures
user interests by enabling interactive clicks
and incorporates personalization by model-
ing captured reader preference. We develop
experimental systems to compare 5 rival al-
gorithms on 4 instinctively different datasets
which amount to 5197 documents. Evalua-
tion results in ROUGE metrics indicate the
comparable performance between IPS and the
best competing system but IPS produces sum-
maries with much more user satisfaction ac-
cording to evaluator ratings. Besides, low
ROUGE consistency among these user pre-
ferred summaries indicates the existence of
personalization.
1 Introduction
In the era of information explosion, people need new
information to update their knowledge whilst infor-
mation on Web is updating extremely fast. Multi-
document summarization has been proposed to ad-
dress such dilemma by producing a summary de-
livering the majority of information content from a
document set, and hence is a necessity.
Traditional summarization methods play an im-
portant role with the exponential document growth
on the Web. However, for the readers, the impact of
human interests has seldom been considered. Tra-
ditional summarization utilizes the same methodol-
ogy to generate the same summary no matter who is
reading. However, users may have bias on what they
prefer to read due to their potential interests: they
need personalization. Therefore, traditional summa-
rization methods are to some extent insufficient.
Topic biased summarization tries for personaliza-
tion by pre-defining human interests as several gen-
eral categories, such as health or science. Readers
are required to select their possible interests before
summary generation so that the chosen topic has
priority during summarization. Unfortunately, such
topic biased summarization is not sufficient for two
reasons: (1) interests cannot usually be accurately
pre-defined by ambiguous topic categories and (2)
user interests cannot always be foreknown. Often
users do not really know what general ideas or detail
information they are interested in until they read the
summaries. Therefore, more flexible interactions
are required to establish personalization.
Due to all the insufficiencies of existed sum-
marization approaches, we introduce a new multi-
document summarization task of Interactive Person-
alized Summarization (IPS) and a novel solution for
the task. Taking a document collection as input, the
system outputs a summary aligned both with source
corpus and with user personalization, which is cap-
tured by flexible human?system interactions. We
1342
build an experimental system on 4 real datasets to
verify the effectiveness of our methods compared
with 4 rivals. The contribution of IPS is manifold
by addressing following challenges:
? The 1st challenge for IPS is to integrate user
interests into traditional summary components. We
measure the utilities of these components and com-
bine them. We formulate the task into a balanced
optimization framework via iterative substitution to
generate summaries with maximum overall utilities.
? The 2nd challenge is to capture user inter-
ests through interaction. We develop an interactive
mechanism of ?click? and ?examine? between read-
ers and summaries and address sparse data by ?click
smoothing? under the scenario of few user clicks.
We start by reviewing previous works. In Section
3 we provide IPS overview, describe user interac-
tion and optimize component combination with per-
sonalization. We conduct empirical evaluation and
demonstrate the experimental system in Section 4.
Finally we draw conclusions in Section 5.
2 Related Work
Multi-Document Summarization (MDS) has drawn
much attention in recent years and gained emphasis
in conferences such as ACL, EMNLP and SIGIR,
etc. General MDS can either be extractive or ab-
stractive. The former assigns salient scores to se-
mantic units (e.g. sentences, paragraphs) of the doc-
uments indicating their importance and then extracts
top ranked ones, while the latter demands informa-
tion fusion(e.g. sentence compression and reformu-
lation). Here we focus on extractive summarization.
Centroid-based method is one of the most popular
extractive summarization method. MEAD (Radev
et al, 2004) and NeATS (Lin and Hovy, 2002) are
such implementations, using position and term fre-
quency, etc. MMR (Goldstein et al, 1999) algorithm
is used to remove redundancy. Most recently, the
graph-based ranking methods have been proposed to
rank sentences or passages based on the ?votes? or
?recommendations? between each other. The graph-
based methods first construct a graph representing
the sentence relationships at different granularities
and then evaluate the saliency score of the sentences
based on the graph. TextRank (Mihalcea and Tarau,
2005) and LexPageRank (Erkan and Radev, 2004)
use algorithms similar to PageRank and HITS to
compute sentence importance. Wan et al improve
the graph-ranking algorithm by differentiating intra-
document and inter-document links between sen-
tences (2007b) and incorporate cluster information
in the graph model to evaluate sentences (2008).
To date, topics (or themes, clusters) in documents
have been discovered and used for sentence selec-
tion for topic biased summarization (Wan and Yang,
2008; Gong and Liu, 2001). Wan et al have
proposed a manifold-ranking method to make uni-
form use of sentence-to-sentence and sentence-to-
topic relationships to generate topic biased sum-
maries (2007a). Leuski et al in (2003) pre-define
several topic concepts, assuming users will foresee
their interested topics and then generate the topic
biased summary. However, such assumption is not
quite reasonable because user interests may not be
forecasted, or pre-defined accurately as we have ex-
plained in last section.
The above algorithms are usually traditional ex-
tensions of generic summarizers. They do not in-
volve interactive mechanisms to capture reader in-
terests, nor do they utilize user preference for per-
sonalization in summarization. Wan et al in (2008)
have proposed a summarization biased to neighbor-
ing reading context through anchor texts. How-
ever, such scenario does not apply to contexts with-
out human-edited anchor texts like Wikipedia they
have used. Our approach can naturally and simulta-
neously take into account traditional summary ele-
ments and user interests and combine both in opti-
mization under a wider practical scenario.
3 Interactive Personalized Summarization
Personalization based on user preference can be
captured via various alternative ways, such as eye-
tracking or mouse-tracking instruments used in (Guo
and Agichtein, 2010). In this study, we utilize inter-
active user clicks/examinations for personalization.
Unlike traditional summarization, IPS supports
human?system interaction by clicking into the sum-
mary sentences and examining source contexts. The
implicit feedback of user clicks indicates what they
are interested in and the system collects preference
information to update summaries if readers wish to.
We obtain an associated tuple <q, c> between a
1343
clicked sentence q and the examined contexts c.
As q has close semantic coherence with neigh-
boring contexts due to consistency in human natural
language, we consider a window of sentences cen-
tered at the clicked sentence q as c, which is a bag of
sentences. The window size k is a parameter to set.
However, click data is often sparse: users are not
likely to click more than 1/10 of total summary sen-
tences within a single generation. We amplify these
tiny hints of user interest by click smoothing.
We change the flat summary structure into a hi-
erarchical organization by extracting important se-
mantic units (denoted as u) and establishing link-
age between them. If the clicked sentence q con-
tains u, we diffuse the click impact to the correlated
units, which makes a single click perform as multi-
ple clicks and the sparse data is smoothed.
Problem Formulation
Input: Given the sentence collection D decom-
posed by documents, D = {s1, s2, . . . , s|D|} and
the clicked sentence record Q = {q1, q2, . . . }, we
generate summaries in sentences. A user click is
associated with a tuple <q, (u), c> where the exis-
tence of u depends on whether q contains u. The
collection of semantic units is denoted as M =
{u1, u2, . . . , u|M |}.
Output: A summary S as a set of sentences
{s1, s2, . . . , s|S|} and S ? D according to the pre-
specified compression rate ? (0 < ? < 1).
After the overview and formulation of IPS prob-
lem, we move on to the major components of User
Interaction and Personalized Summarization.
3.1 User Interaction
Hypertexify Summaries. We hypertexify the sum-
mary structure by establishing linkage between se-
mantic units. There are several possible formats for
semantic units, such as words or n-grams, etc. As
single words are proved to be not illustrative of se-
mantic meanings (Zhao et al, 2011) and n-grams are
rigid in length, we choose to extract semantic units
at a phrase granularity. Among all phrases from
source texts, some are of higher importance to at-
tract user interests, such as hot concepts or popu-
lar event names. We utilize the toolkit provided by
(Zhao et al, 2011) based on graph proximity LDA
(Blei et al, 2003) to extract key phrases and their
corresponding topic. A topic T is represented by
{(u1, pi(u1, T )), (u2, pi(u2, T )), . . . }where pi(u, T )
is the probability of u belonging to topic T . We in-
vert the topic-unit representation in Table 1, where
each u is represented as a topic vector. The corre-
lation corr(.) between ui, uj is measured by cosine
similarity sim(.) on topic distribution vector ~ui, ~uj .
corr(ui, uj) = simtopic(~ui, ~uj) (1)
Table 1: Inverted representation of topic-unit vector.
~u1 pi(u1, T1) pi(u1, T2) . . . pi(u1, Tn)
~u2 pi(u2, T1) pi(u2, T2) . . . pi(u2, Tn)
... ... ... ... ...
~u|M | pi(u|M |, T1) pi(u|M |, T2) . . . pi(u|M |, Tn)
When the summary is hypertexified by established
linkage, users click into the generated summary to
examine what they are interested in. A single click
on one sentence become multiple clicks via click
smoothing when the indicative function I(u|q) = 1.
I(u|q) =
{
1 q contains u;
0 otherwise. (2)
The click smoothing brings pseudo clicks q? asso-
ciated with u? and contexts c?. The entire user feed-
back texts A from q can be written as:
A(q) = I(u|q)
|M |?
j=1
corr(u?, u)(u?+? ?c?)+? ?c (3)
where ? is the weight tradeoff between u and asso-
ciated contexts c. If I(u|q) = 0, only the examined
context c is feedbacked for user preference; other-
wise, correlative contexts with u are taken into con-
sideration, which is a process of impact diffusion.
3.2 Personalized Summarization
Traditional summarization involves two essential re-
quirements: (1) coverage: the summary should
keep alignment with the source collection, which is
proved to be significant (Li et al, 2009). (2) di-
versity: according to MMR principle (Goldstein et
al., 1999) and its applications (Wan et al, 2007b;
Wan and Yang, 2008), a good summary should be
concise and contain as few redundant sentences as
possible, i.e., two sentences providing similar infor-
mation should not both present. According to our
1344
investigation, we observe that a well generated sum-
mary should properly consider a key component of
(3) user interests, which captures user preference to
summarize what they are interested in.
All above requirements involve a measurement
of similarity between two word distributions ?1
and ?2. Cosine, Kullback-Leibler divergence DKL
and Jensen Shannon divergence DJS are all able
to measure the similarity, but (Louis and Nenkova,
2009) indicate the superiority of DJS in summa-
rization task. We also introduce a pair of decreas-
ing/increasing logistic functions, L1(x) = 1/(1 +
ex) and L2(x) = ex/(1 + ex), to map the diver-
gence into interval [0,1]. V is the vocabulary set
and tf denotes the term frequency for word w.
DJS(?1||?2) =
1
2[DKL(?1||?2)+DKL(?2||?1)]
where
DKL(?1||?2) =
?
k?V
p(w|?1)log
p(w|?1)
p(w|?2)
where
p(w|?) = tf(w,?)?
w? tf(w?,?)
.
Modeling Interest for User Utility. Given a gener-
ated summary S, users tend to scrutinize texts rele-
vant to their interests. Texts related to user implicit
feedback are collected as A = ?|Q|i=1A(qi). Intu-
itively, the smaller distance between the word distri-
bution of final summary (?S) and the word distri-
bution of user preference (?A), the higher utility of
user interests Uuser(S) will be, i.e.,
Uuser(S) = L1(DJS(?S ||?A)). (4)
We model the utility of traditional summarization
Utrad(S) using a linear interpolation controlled by
parameter ? between utility from coverage Uc(S)
and utility Ud(S) from diversity:
Utrad(S) = Uc(S) + ? ? Ud(S). (5)
Coverage Utility. The summary should share a
closer word distribution with the source collection
(Allan et al, 2001; Li et al, 2009). A good summary
focuses on minimizing the loss of main information
from the whole collection D. Utility from coverage
Uc(S) is defined as follows and for coverage utility,
smaller divergence is desired.
Uc(S) = L1(DJS(?S ||?D)). (6)
Diversity Utility. Diversity measures the novelty
degree of any sentence s compared with all other
sentences within S, i.e., the distances between all
other sentences and itself. Diversity utility Ud(S) is
an average novelty score for all sentences in S. For
diversity utility, larger distance is desired, and hence
we use the increasing function L2 as follows:
Ud(S) =
1
|S|
?
s?S
L2(DJS(?s||?(S?s))). (7)
3.3 Balanced Optimization Framework
A well generated summary S should be sufficiently
aligned with the original source corpus, and also
be optimized given the user interests. The utility
of an individual summary U(S) is evaluated by the
weighted combination of these components, con-
trolled by parameter ? for balanced weights.
U(S) = Utrad(S) + ? ? Uuser(S) (8)
Given the sentence setD and the compression rate
?, there are ??|D| out of |D| possibilities to generate
S. The IPS task is to predict the optimized sentence
subset of S? from the space of all combinations. The
objective function is as follows:
S? = argmax
S
U(S). (9)
As U(S) is measured based on preferred interests
from user interaction within a generation in our sys-
tem, we extract S iteratively to approximate S?, i.e,
maximize U(S) based on the user feedbacks from
the interaction sessions. Each session is an iteration.
We use a similar framework as we have proposed in
(Yan et al, 2011).
During every session, the top ranked sentences are
strong candidates for the summary to generate and
the rank methodology is based on the metrics U(.).
The algorithm tends to highly rank sentences which
are with both coverage utility and interest utility, and
are diversified in balance: we rank each sentence s
according to U(s) under such metrics.
Consider S(n?1) generated in the (n-1)-th session
which consists of top ?|D| ranked sentences, as well
1345
as the top ?|D| ranked sentences in the n-th iteration
(denoted by O(n)), they have an intersection set of
Z(n) = Sn?1?On. There is a substitutable sentence
set X (n) = S(n?1) ?Z(n) and a new candidate sen-
tence set Y(n) = O(n) ? Z(n). We substitute x(n)
sentences with y(n), where x(n) ? X (n) and y(n)
? Y(n). During every iteration, our goal is to find a
substitutive pair <x,y> for S:
<x,y> : X ? Y ? R.
To measure the performance of such a substitu-
tion, a discriminant utility gain function ?Ux,y
?U (n)x(n),y(n) = U(S
(n))? U(S(n?1))
= U((S(n?1) ? x(n)) ? y(n))? U(S(n?1))
(10)
is employed to quantify the penalty. Therefore, we
predict the substitutive pair by maximizing the gain
function ?Ux,y over the state set R, with a size of?Y
k=0AkXCkY , where <x,y>? R. Finally the ob-
jective function of Equation (9) changes into maxi-
mization of utility gain by substitute x? with y? during
each iteration:
< x?, y? >= argmax
x?X ,y?Y
?Ux,y. (11)
Note that the objectives of interest utility opti-
mization and traditional utility optimization are not
always the same because the word distributions in
these texts are usually different. The substitutive
pair <x,y> may perform well based on the user
preference component while not on the traditional
summary part and vice versa. There is a tradeoff
between both user optimization and traditional opti-
mization and hence we need to balance them by ?.
The objective Equation (11) is actually to maxi-
mize ?U(S) from all possible substitutive pairs be-
tween two iteration sessions to generate S. The al-
gorithm is shown in Algorithm 1. The threshold  is
set at 0.001 in this study.
4 Experiments and Evaluation
4.1 Datasets
IPS can be tested on any document set but a tiny
corpus to summarize may not cover abundant effec-
tive interests to attract user clicks indicating their
Algorithm 1 Regenerative Optimization
1: Input: D, , ?
2: for all s ? D do
3: calculate Utrad(s)
4: end for
5: S ? top ?|D| ranked sentences
6: while new generation=TRUE do
7: collect clicks and update utility from U ? to U
8: if |U(S)? U ?(S)| >  then
9: for all s ? D do
10: calculate U(s)
11: end for
12: O ? top ?|D| ranked sentences by U(s)
13: Z ? S ? O
14: X ? S ?Z , Y ? O ?Z
15: for all <x,y> pair where x ? X ,y ? Y
do
16: ?Ux,y = U((S ? x) ? y)? U(S)
17: end for
18: < x?, y? >= argmax ?Ux,y
19: S ? (S ? x?) ? y?
20: end if
21: end while
preference. Besides, the scenario of small corpus is
not quite practical for the exponential growing web.
Therefore, we test IPS on large real world datasets.
We build 4 news story sets which consist of docu-
ments and reference summaries to evaluate our pro-
posed framework empirically. We downloaded 5197
news articles from 10 selected sources. As shown in
Table 2, three of the sources are in UK, one of them
is in China and the rest are in US. We choose them
because many of these websites provide handcrafted
summaries for their special reports, which serve as
reference summaries. These events belong to differ-
ent categories of Rule of Interpretation (ROI) (Ku-
maran and Allan, 2004). Statistics are in Table 3.
4.2 Experimental System Setups
? Preprocessing. Given a collection of documents,
we first decompose them into sentences. Stop-words
are removed and words stemming is performed.
Then the word distributions can be calculated.
? User Interface Design. Users are required to
specify the overall compression rate ? and the sys-
tem extracts ?|D| sentences according to user utility
1346
Figure 1: A demonstration system for Interactive Personalized Summarization when compression rate ? is specified
(e.g. 5%). For convenience of browsing, we number the selected sentences (see in part 3). Extracted semantic units,
such as ?drilling mud?, are in bold and underlined format (see in part 1). When the user clicks a sentence (part 4), the
clicked sentence ID is kept in the click record (part 2). Mis-clicked records revocation can be operated by clicking
the deletion icon ?X? (see in part 3). Once a sentence is clicked, user can track the sentence into the popup source
document to examine the contexts. The selected sentences are highlighted in the source documents (see in part 5).
Table 2: News sources of 4 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 4 datasets.
News Subjects #size #docs #RS Avg.L
1.Influenza A 115026 2557 5 83
2.BP Oil Spill 63021 1468 6 76
3.Haiti Earthquake 12073 247 2 32
4.Jackson Death 37819 925 3 64
#size: total sentence counts; #RS: the number of reference summaries;
Avg.L: average length of reference summary measured in sentences.
and traditional utility. User utility is obtained from
interaction. The system keeps the clicked sentence
records and calculates the user feedback by Equa-
tion (3) during every session. Consider sometimes
users click into the summary due to confusion or
mis-operations, but not their real interests. The sys-
tem supports click records revocation. More details
of the user interface is demonstrated in Figure 1.
4.3 Evaluation Metrics
We include both subjective evaluation from 3 evalu-
ators based on their personalized interests and pref-
erence, and the objective evaluation based on the
widely used ROUGE metrics (Lin and Hovy, 2003).
Evaluator Judgments
Evaluators are requested to express an opinion
over all summaries based on the sentences which
they deem to be important for the news. In general
a summary can be rated in a 5-point scale, where
?1? for ?terrible?, ?2? for ?bad?, ?3? for ?normal?,
?4? for ?good? and ?5? for ?excellent?. Evaluators
are allowed to judge at any scores between 1 and 5,
e.g. a score of ?3.3? is adopted when the evaluator
feels difficult to decide whether ?3? or ?4? is more
1347
appropriate but with preference towards ?3?.
ROUGE Evaluation
The DUC usually officially employs ROUGE
measures for summarization evaluation, which mea-
sures summarization quality by counting overlap-
ping units such as the N-gram, word sequences, and
word pairs between the candidate summary and the
reference summary. We use ROUGE-N as follows:
ROUGE-N =
?
S?{RefSum}
?
N-gram?S
Countmatch(N-gram)
?
S?{RefSum}
?
N-gram?S
Count (N-gram)
whereN stands for the length of the N-gram and N-
gram?RefSum denotes the N-grams in the reference
summaries while N-gram?CandSum denotes the N-
grams in the candidate summaries. Countmatch(N-
gram) is the maximum number of N-gram in the
candidate summary and in the set of reference sum-
maries. Count(N-gram) is the number of N-grams in
the reference summaries or candidate summary.
According to (Lin and Hovy, 2003), among all
sub-metrics in ROUGE, ROUGE-N (N=1, 2) is rela-
tively simple and works well. In this paper, we eval-
uate our experiments using all methods provided by
the ROUGE package (version 1.55) and only report
ROUGE-1, since the conclusions drawn from differ-
ent methods are quite similar. Intuitively, the higher
the ROUGE scores, the similar two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used multi-
document summarization algorithms as the baseline
systems, which are all designed for traditional sum-
marization without user interaction. For fairness we
conduct the same preprocessing for all algorithms.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according to
the following parameters: centroid value, positional
value, and first-sentence overlap.
GMDS: The Graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
IPSini: The initial generated summary from IPS
merely models coverage and diversity utility, which
is similar to the previous work described in (Allan et
al., 2001) with different goals and frameworks.
IPS: Our proposed algorithms with personaliza-
tion component to capture interest by user feed-
backs. IPS generates summaries via iterative sen-
tence substitutions within user interactive sessions.
RefSum: As we have used multiple reference
summaries from websites, we not only provide
ROUGE evaluations of the competing systems but
also of the reference summaries against each other,
which provides a good indicator of not only the
upper bound ROUGE score that any system could
achieve, but also human inconsistency among refer-
ence summaries, indicating personalization.
4.5 Overall Performance Comparison
We take the average ROUGE-1 performance and hu-
man ratings on all sets. The overall results are shown
in Figure 2 and details are listed in Tables 4?6.
Figure 2: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected,
both in ROUGE-1 scores and human judgements.
? The ROUGE-1 and human ratings of Centroid
and GMDS are better than those of Random. This is
mainly because the Centroid based algorithm takes
into account positional value and first-sentence over-
lap, which facilitates main aspects summarization
and PageRank-based GMDS ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
? In general, the GMDS system slightly outper-
forms Centroid system in ROUGE-1, but the human
judgements of GMDS and Centroid are of no signifi-
cant difference. This is probably due to the difficulty
1348
Table 4: Overall performance comparison on Influenza A.
ROI? category: Science.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.491 0.44958 3.5 3.0 3.9
Random 0.257 0.75694 1.2 1.0 1.0
Centroid 0.331 0.45073 2.5 3.0 3.5
GMDS 0.364 0.33269 3.0 2.7 3.5
IPSini 0.302 0.21213 2.0 2.5 2.5
IPS 0.337 0.46757 4.8 4.5 4.5
Table 5: Overall performance comparison on BP Oil
Leak. ROI category: Accidents.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.517 0.48618 4.0 3.3 3.9
Random 0.262 0.64406 1.5 1.0 1.5
Centroid 0.369 0.34743 3.2 3.0 3.5
GMDS 0.389 0.43877 3.5 3.0 3.9
IPSini 0.327 0.53722 3.0 2.5 3.0
IPS 0.372 0.35681 4.8 4.5 4.5
Table 6: Overall performance comparison on Haiti Earth-
quake. ROI category: Disasters.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.528 0.30450 3.8 4.0 4.0
Random 0.266 0.75694 1.5 1.5 1.8
Centroid 0.362 0.43045 3.6 3.0 4.0
GMDS 0.380 0.33694 3.9 3.5 4.0
IPSini 0.331 0.34120 2.8 2.5 3.0
IPS 0.391 0.40069 5.0 4.7 5.0
Table 7: Overall performance comparison on Michael
Jackson Death. ROI category: Legal Cases.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.482 0.47052 3.5 3.5 4.0
Random 0.232 0.52426 1.2 1.0 1.5
Centroid 0.320 0.21045 3.0 2.5 2.7
GMDS 0.341 0.30070 3.5 3.3 3.9
IPSini 0.287 0.48526 2.5 2.0 2.2
IPS 0.324 0.36897 5.0 4.5 4.8
?ROI: news categorization defined by Linguistic Data Consortium.
Available at http://www.ldc.upenn.edu/projects/tdt4/annotation
of human judgements on comparable summaries.
? The results of ROUGE-1 and ratings for IPSini
are better than Random but worse than Centroid and
GMDS. The reason in this case may be that IPSini
does not capture sufficient attributes: coverage and
diversity are merely fundamental requirements.
? Traditional summarization considers sentence
selection based on corpus only, and hence neglects
Table 8: Ratings consistency between evaluators: mean
? standard deviation over the 4 datasets.
RefSum Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.09 0.30?0.33
Evaluator 2 0.50?0.14
Random Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.23?0.04 0.20?0.02
Evaluator 2 0.33?0.06
Centroid Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.03 0.50?0.12
Evaluator 2 0.55?0.11
GMDS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.02 0.35?0.03
Evaluator 2 0.70?0.03
IPSini Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.01 0.25?0.04
Evaluator 2 0.30?0.06
IPS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.01 0.18?0.02
Evaluator 2 0.28?0.04
user interests. Many sentences are extracted due to
arbitrary assumption of reader preference, which re-
sults in a low user satisfaction. Human judgements
under our proposed IPS framework greatly outper-
form baselines, indicating that the appropriate use
of human interests for summarization are beneficial.
The ROUGE-1 performance for IPS is not as ideal
as that of GMDS. This situation may result from the
divergence between user interests and general infor-
mation provided by mass media propaganda, which
again motivates the need for personalization.
Although the high disparities between different
human evaluators have been observed in (Gong and
Liu, 2001), we still examine the consistency among
3 evaluators and their preferred summaries to prove
the motivation of personalization in our work.
4.6 Consistency Analysis for Personalization
The low ROUGE-1 scores of RefSum indicate the
inconsistency among reference summaries. We con-
duct personalization analysis from two perspectives:
(1) human rating consistency and (2) content consis-
tency among human supervised summaries.
We calculate the mean and variance of rating vari-
ations among evaluator judgements, listed in Table
1349
Table 9: Content consistency among evaluators super-
vised summaries.
Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.273 0.398
Evaluator 2 0.289 0.257
Evaluator 3 0.407 0.235
RefSum 0.365 0.302 0.394
8. We see that for Random the average rating vari-
ation is 0.25, for IPS is 0.27, for IPSini is 0.33, for
RefSum is 0.38, for GMDS is 0.47 and for Centroid
is the highest, 0.50. Such phenomenon indicates
for poor generated summaries, such as Random or
IPSini, humans have consensus, but for normal sum-
maries without personalized interests, they are likely
to have disparities, surprisingly, even for RefSum.
General summaries provided by mass media satisfy
part of audiences, but obviously not all of them.
The high rating consistency of IPS indicates peo-
ple tend to favor summaries generated according to
their interests. We next examine content consistency
of these summaries with high rating consistency.
As shown in Table 9, although highly scored,
these human supervised summaries still have low
content consistency (especially Evaluator 2). The
low content consistency between RefSum and su-
pervised summaries shows reader have individual
personalization. Note that the inconsistency among
evaluators is larger than that between RefSum and
supervised summaries, indicating interests take a
high proportion in evaluator supervised summaries.
4.7 Parameter Settings
? controls coverage/diversity tradeoff. We tune ? on
IPSini and apply the optimal ? directly in IPS. Ac-
cording to the statistics in (Yan et al, 2010), the se-
mantic coherent context is about 7 sentences. There-
fore, we empirically choose k=3 for the examined
context window. The number of topics is set at
n=50. We assign an equal weight (? = 1) to seman-
tic units and examined contexts according to analog-
ical research of summarization from implicit feed-
backs via clickthrough data (Sun et al, 2005).
? is the key parameter in IPS approach, control-
ling the weight of user utility during the process of
interactive personalized summarization.
Through Figure 3, we see that when ? is small
Figure 3: ? v.s. human ratings and ROUGE scores.
(? ? [0.01, 0.1]), both human judgements and
ROUGE evaluation scores have little difference.
When ? ? [0.1, 1], ROUGE scores increase signifi-
cantly but human satisfaction shows little response.
? ? [1, 10] brings large user utility enhancement be-
cause user may find what they are interested in but
ROUGE scores start to decay. When ? ? [10, 100],
ROUGE scores drop much because the emphasized
user interests may guide the generated summaries
divergent away from the original corpus.
In Figure 4 we examine how ? attracts user clicks
and regeneration counts until satisfaction. As the re-
sult indicates, both counts increase as ? increases.
When ? is small (from 0.01 to 0.1), readers find
no more interesting aspects through clicks and re-
generations and stop due to the bad user experience.
As ? increases, the system mines more relevant sen-
tences according to personalized interests and hence
attracts user clicks and intention to regenerate.
Figure 4: ? v.s. click counts and regeneration counts.
1350
5 Conclusion
We present an important and novel summariza-
tion problem, Interactive Personalized Summariza-
tion (IPS), which generates summaries based on
human?system interaction for ?interests? and per-
sonalization. We formally formulate IPS as a combi-
nation of user utility and traditional summary utility,
such as coverage and diversity. We implement a sys-
tem under such framework for experiments on real
web datasets to compare all approaches. Through
our experiments we notice that user personalization
of interests plays an important role in summary gen-
eration, which largely increase human ratings due to
user satisfaction. Besides, our experiments indicate
the inconsistency between user preferred summaries
and reference summaries measured by ROUGE, and
hence prove the effectiveness of personalization.
Acknowledgments
This work was partially supported by HGJ 2010
Grant 2011ZX01042-001-001 and NSFC with Grant
No.61073082, 60933004. Rui Yan was supported by
the MediaTek Fellowship.
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international SIGIR?01, pages 10?18.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP?04, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of SIGIR?99, pages 121?128.
Yihong Gong and Xin Liu. 2001. Generic text sum-
marization using relevance measure and latent seman-
tic analysis. In Proceedings of the 24th international
ACM SIGIR conference, SIGIR ?01, pages 19?25.
Q. Guo and E. Agichtein. 2010. Ready to buy or just
browsing?: detecting web searcher goals from inter-
action data. In Proceeding of the 33rd international
ACM SIGIR conference, SIGIR?10, pages 130?137.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: interactive multi-document summarization. In
Proceedings of ACL?03, pages 125?128.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through structure
learning. In Proceedings of WWW?09, pages 71?80.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of ACL?02, pages
457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of NAACL?03, pages 71?78.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization without
human models. In EMNLP?09, pages 306?314.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings of
SIGIR?05, pages 194?201.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: generating elaborative summaries biased
towards the reading context. In ACL-HLT?08, pages
129?132.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In AIRS?10, pages 490?501.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th annual international ACM SIGIR?11.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li. 2011.
Topical Keyphrase Extraction from Twitter. In Pro-
ceedings of ACL-HLT?11.
1351
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 516?525,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tweet Recommendation with Graph Co-Ranking
Rui Yan?
?Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Mirella Lapata?
?Institute for Language,
Cognition and Computation,
University of Edinburgh,
Edinburgh EH8 9AB, UK
mlap@inf.ed.ac.uk
Xiaoming Li?, \
\State Key Laboratory of Software
Development Environment,
Beihang University,
Beijing 100083, China
lxm@pku.edu.cn
Abstract
As one of the most popular micro-blogging
services, Twitter attracts millions of users,
producing millions of tweets daily. Shared in-
formation through this service spreads faster
than would have been possible with tradi-
tional sources, however the proliferation of
user-generation content poses challenges to
browsing and finding valuable information. In
this paper we propose a graph-theoretic model
for tweet recommendation that presents users
with items they may have an interest in. Our
model ranks tweets and their authors simulta-
neously using several networks: the social net-
work connecting the users, the network con-
necting the tweets, and a third network that
ties the two together. Tweet and author entities
are ranked following a co-ranking algorithm
based on the intuition that that there is a mu-
tually reinforcing relationship between tweets
and their authors that could be reflected in the
rankings. We show that this framework can be
parametrized to take into account user prefer-
ences, the popularity of tweets and their au-
thors, and diversity. Experimental evaluation
on a large dataset shows that our model out-
performs competitive approaches by a large
margin.
1 Introduction
Online micro-blogging services have revolutionized
the way people discover, share, and distribute infor-
mation. Twitter is perhaps the most popular such
service with over 140 million active users as of
2012.1 Twitter enables users to send and read text-
based posts of up to 140 characters, known as tweets.
Twitter users follow others or are followed. Being a
follower on Twitter means that the user receives all
the tweets from those she follows. Common prac-
tice of responding to a tweet has evolved into a well-
defined markup culture (e.g., RT stands for retweet,
?@? followed by an identifier indicates the user).
The strict limit of 140 characters allows for quick
and immediate communication in real time, whilst
enforcing brevity. Moreover, the retweet mecha-
nism empowers users to spread information of their
choice beyond the reach of their original followers.
Twitter has become a prominent broadcast-
ing medium, taking priority over traditional news
sources (Teevan et al, 2011). Shared information
through this channel spreads faster than would have
been possible with conventional news sites or RSS
feeds and can reach a far wider population base.
However, the proliferation of user-generated con-
tent comes at a price. Over 340 millions of tweets
are being generated daily amounting to thousands
of tweets per second!2 Twitter?s own search en-
gine handles more than 1.6 billion search queries per
day.3 This enormous amount of data renders it in-
feasible to browse the entire Twitter network; even
if this was possible, it would be extremely difficult
for users to find information they are interested in.
A hypothetical tweet recommendation system could
1For details see http://blog.twitter.com/2012/03/
twitter-turns-six.html
2In fact, the peak record is 6,939 tweets per second, reported
by http://blog.twitter.com/2011/03/numbers.html.
3See http://engineering.twitter.com/2011/05/
engineering-behind-twitters-new-search.html
516
alleviate this acute information overload, e.g., by
limiting the stream of tweets to those of interest to
the user, or by discovering intriguing content outside
the user?s following network.
The tweet recommendation task is challenging for
several reasons. Firstly, Twitter does not merely
consist of a set of tweets. Rather, it contains many
latent networks including the following relationships
among users and the retweeting linkage (which in-
dicates information diffusion). Secondly, the rec-
ommendations ought to be of interest to the user
and likely to to attract user response (e.g., to be
retweeted). Thirdly, recommendations should be
personalized (Cho and Schonfeld, 2007; Yan et al,
2011), avoid redundancy, and demonstrate diversity.
In this paper we present a graph-theoretic approach
to tweet recommendation that attempts to address
these challenges.
Our recommender operates over a heterogeneous
network that connects the users (or authors) and the
tweets they produce. The user network represents
links among authors based on their following be-
havior, whereas the tweet network connects tweets
based on content similarity. A third bipartite graph
ties the two together. Tweet and author entities in
this network are ranked simultaneously following a
co-ranking algorithm (Zhou et al, 2007). The main
intuition behind co-ranking is that there is a mu-
tually reinforcing relationship between authors and
tweets that could be reflected in the rankings. Tweets
are important if they are related to other important
tweets and authored by important users who in turn
are related to other important users. The model ex-
ploits this mutually reinforcing relationship between
tweets and their authors and couples two random
walks, one on the tweet graph and one on the author
graph, into a combined one. Rather than creating a
global ranking over all tweets in a collection, we ex-
tend this framework to individual users and produce
personalized recommendations. Moreover, we in-
corporate diversity by allowing the random walk on
the tweet graph to be time-variant (Mei et al, 2010).
Experimental results on a real-world dataset con-
sisting of 364,287,744 tweets from 9,449,542 users
show that the co-ranking approach substantially im-
proves performance over the state of the art. We ob-
tain a relative improvement of 18.3% (in nDCG) and
7.8% (in MAP) over the best comparison system.
2 Related Work
Tweet Search Given the large amount of tweets
being posted daily, ranking strategies have be-
come extremely important for retrieving information
quickly. Many websites currently offer a real-time
search service which returns ranked lists of Twit-
ter posts or shared links according to user queries.
Ranking methods used by these sites employ three
criteria, namely recency, popularity and content rel-
evance (Dong et al, 2010). State-of-art tweet re-
trieval methods include a linear regression model bi-
ased towards text quality with a regularization factor
inspired by the hypothesis that documents similar
in content may have similar quality (Huang et al,
2011). Duan et al (2010) learn a ranking model us-
ing SVMs and features based on tweet content, the
relations among users, and tweet specific character-
istics (e.g., urls, number of retweets).
Tweet Recommendation Previous work has also
focused on tweet recommendation systems, assum-
ing no explicit query is provided by the users.
Collaborative filtering is perhaps the most obvious
method for recommending tweets (Hannon et al,
2010). Chen et al (2010) investigate how to se-
lect interesting URLs linked from Twitter and rec-
ommend the top ranked ones to users. Their rec-
ommender takes three dimensions into account: the
source, the content topic, and social voting. Sim-
ilarly, Abel et al (2011a; 2011b; 2011c) recom-
mend external websites linked to Twitter. Their
method incorporates user profile modeling and tem-
poral recency, but they do not utilize the social
networks among users. R. et al (2009) propose
a diffusion-based recommendation framework es-
pecially for tweets representing critical events by
constructing a diffusion graph. Hong et al (2011)
recommend tweets based on popularity related fea-
tures. Ramage et al (2010) investigate which topics
users are interested in following a Labeled-LDA ap-
proach, by deciding whether a user is in the followee
list of a given user or not. Uysal and Croft (2011) es-
timate the likelihood of a tweet being reposted from
a user-centric perspective.
Our work also develops a tweet recommendation
system. Our model exploits the information pro-
vided by the tweets and the underlying social net-
works in a unified co-ranking framework. Although
517
these sources have been previously used to search
or recommend tweets, our model considers them
simultaneously and produces a ranking that is in-
formed by both. Furthermore, we argue that the
graph-theoretic framework upon which co-ranking
operates is beneficial as it allows to incorporate per-
sonalization (we provide user-specific rankings) and
diversity (the ranking is optimized so as to avoid re-
dundancy). The co-ranking framework has been ini-
tially developed for measuring scientific impact and
modeling the relationship between authors and their
publications (Zhou et al, 2007). However, the adap-
tation of this framework to the tweet recommenda-
tion task is novel to our knowledge.
3 Tweet Recommendation Framework
Our method operates over a heterogeneous network
that connects three graphs representing the tweets,
their authors and the relationships between them.
Let G denote the heterogeneous graph with nodes V
and edges E, and G = (V,E) = (VM ?VU ,EM ?EU ?
EMU). G is divided into three subgraphs, GM, GU
and GMU . GM = (VM,EM) is a weighted undirected
graph representing the tweets and their relationships.
Let VM = {mi|mi ?VM} denote a collection of |VM|
tweets and EM the set of links representing relation-
ships between them. The latter are established by
measuring how semantically similar any two tweets
are (see Section 3.4 for details). GU = (VU ,EU) is
an unweighted directed graph representing the so-
cial ties among Twitter users. VU = {ui|ui ? VU} is
the set of users with size |VU |. Links EU among
users are established by observing their following
behavior. GMU = (VMU ,EMU) is an unweighted bi-
partite graph that ties GM and GU together and repre-
sents tweet-author relationships. The graph consists
of nodes VMU = VM ?VU and edges EMU connect-
ing each tweet with all of its authors. Typically, a
tweet m is written by only one author u. However,
because of retweeting we treat all users involved in
reposting a tweet as ?co-authors?. The three subnet-
works are illustrated in Figure 1.
The framework includes three random walks, one
on GM, one on GU and one on GMU . A random walk
on a graph is a Markov chain, its states being the
vertices of the graph. It can be described by a square
n? n matrix M, where n is the number of vertices
in the graph. M is a stochastic matrix prescribing
Figure 1: Tweet recommendation based on a co-ranking
framework including three sub-networks. The undirected
links between tweets indicate semantic correlation. The
directed links between users denotes following. A bipar-
tite graph (whose edges are shown with dashed lines) ties
the tweet and author networks together.
the transition probabilities from one vertex to the
next. The framework couples the two random walks
on GM, and GU that rank tweets and theirs authors in
isolation. and allows to obtain a more global rank-
ing by taking into account their mutual dependence.
In the following sections we first describe how we
obtain the rankings on GM and GU , and then move
on to discuss how the two are coupled.
3.1 Ranking the Tweet Graph
Popularity We rank the tweet network follow-
ing the PageRank paradigm (Brin and Page, 1998).
Consider a random walk on GM and let M be the
transition matrix (defined in Section 3.4). Fix some
damping factor ? and say that at each time step with
probability (1-?) we stick to random walking and
with probability ? we do not make a usual random
walk step, but instead jump to any vertex, chosen
uniformly at random:
m = (1??)MTm+
?
|VM|
11T (1)
Here, vector m contains the ranking scores for the
vertices in GM. The fact that there exists a unique so-
518
lution to (1) follows from the random walk M being
ergodic (? >0 guarantees irreducibility, because we
can jump to any vertex). MT is the transpose of M.
1 is the vector of |VM| entries, each being equal to
one. Let m? RVM , ||m||1 = 1 be the only solution.
Personalization The standard PageRank algo-
rithm performs a random walk, starting from any
node, then randomly selects a link from that node to
follow considering the weighted matrix M, or jumps
to a random node with equal probability. It pro-
duces a global ranking over all tweets in the col-
lection without taking specific users into account.
As there are billions of tweets available on Twit-
ter covering many diverse topics, it is reasonable
to assume that an average user will only be inter-
ested in a small subset (Qiu and Cho, 2006). We
operationalize a user?s topic preference as a vec-
tor t = [t1, t2, . . . , tn]1?n, where n denotes the num-
ber of topics, and ti represents the degree of prefer-
ence for topic i. The vector t is normalized such
that ?ni=1 ti = 1. Intuitively, such vectors will be
different for different users. Note that user prefer-
ences can be also defined at the tweet (rather than
topic) level. Although tweets can illustrate user in-
terests more directly, in most cases a user will only
respond to a small fraction of tweets. This means
that most tweets will not provide any information
relating to a user?s interests. The topic preference
vector allows to propagate such information (based
on whether a tweet has been reposted or not) to other
tweets within the same topic cluster.
Given n topics, we obtain a topic distribution ma-
trix D using Latent Dirichlet Allocation (Blei et al,
2003). Let Di j denote the probability of tweet mi to
belong to topic t j. Consider a user with a topic pref-
erence vector t and topic distribution matrix D. We
calculate the response probability r for all tweets for
this user as:
r = tDT (2)
where r=[r1, r2, . . . , rVM ]1?|VM | represents the re-
sponse probability vector and ri the probability for a
user to respond to tweet mi. We normalize r so that
?ri?r ri = 1. Now, given the observed response prob-
ability vector r = [r1,r2, . . . ,rw]1?w, where w<|VM|
for a given user and the topic distribution ma-
trix D, our task is estimate the topic preference
vector t. We do this using maximum-likelihood
estimation. Assuming a user has responded to w
tweets, we approximate t so as to maximize the ob-
served response probability. Let r(t) = tDT. As-
suming all responses are independent, the probabil-
ity for w tweets r1, r2, . . . , rw is then ?wi=1 ri(t) under
a given t. The value of t is chosen when the proba-
bility is maximized:
t = argmax
t
( w
?
i=1
ri(t)
)
(3)
In a simple random walk, it is assumed that all
nodes in the matrix M are equi-probable before the
walk. In contrast, we use the topic preference vector
as a prior on M. Let Diag(r) denote a diagonal ma-
trix whose eigenvalue is vector r. Then m becomes:
m = (1??)[Diag(r)M]Tm+?r
= (1??)[Diag(tDT)M]Tm+?tDT
(4)
Diversity We would also like our output to be
diverse without redundant information. Unfortu-
nately, equation (4) will have the opposite effect,
as it assigns high scores to closely connected node
communities. A greedy algorithm such as Maxi-
mum Marginal Relevance (Carbonell and Goldstein,
1998; Wan et al, 2007; Wan et al, 2010) may
achieve diversity by iteratively selecting the most
prestigious or popular vertex and then penalizing the
vertices ?covered? by those that have been already
selected. Rather than adopting a greedy vertex selec-
tion method, we follow DivRank (Mei et al, 2010)
a recently proposed algorithm that balances popular-
ity and diversity in ranking, based on a time-variant
random walk. In contrast to PageRank, DivRank as-
sumes that the transition probabilities change over
time. Moreover, it is assumed that the transition
probability from one state to another is reinforced by
the number of previous visits to that state. At each
step, the algorithm creates a dynamic transition ma-
trix M(.). After z iterations, the matrix becomes:
M(z) = (1??)M(z?1) ?m(z?1)+?tDT (5)
and hence, m can be calculated as:
m(z) = (1??)[Diag(tDT)M(z)]Tm+?tDT (6)
Equation (5) increases the probability for nodes
with higher popularity. Nodes with high weights are
519
likely to ?absorb? the weights of their neighbors di-
rectly, and the weights of their neighbors? neighbors
indirectly. The process iteratively adjusts the ma-
trix M according to m and then updates m according
to the changed M. Essentially, the algorithm favors
nodes with high popularity and as time goes by there
emerges a rich-gets-richer effect (Mei et al, 2010).
3.2 Ranking the Author Graph
As mentioned earlier, we build a graph of au-
thors (and obtain the affinity U) using the follow-
ing linkage. We rank the author network using
PageRank analogously to equation (1). Besides
popularity, we also take personalization into ac-
count. Intuitively, users are likely to be interested
in their friends even if these are relatively unpopu-
lar. Therefore, for each author, we include a vec-
tor p = [p1, p2, . . . , p|VU |]1?|VU | denoting their prefer-
ence for other authors. The preference factor for au-
thor u toward other authors ui is defined as:
pui =
#tweets from ui
#tweets of u
(7)
which represents the proportion of tweets inherited
from user ui. A large pui means that u is more likely
to respond to ui?s tweets.
In theory, we could also apply DivRank on the au-
thor graph. However, as the authors are unique, we
assume that they are sufficiently distinct and there is
no need to promote diversity.
3.3 The Co-Ranking Algorithm
So far we have described how we rank the network
of tweets GM and their authors GU independently
following the PageRank paradigm. The co-ranking
framework includes a random walk on GM, GU ,
and GMU . The latter is a bipartite graph representing
which tweets are authored by which users. The ran-
dom walks on GM and GU are intra-class random
walks, because take place either within the tweets?
or the users? networks. The third (combined) ran-
dom walk on GMU is an inter-class random walk. It
is sufficient to describe it by a matrix MU|VM|?|VU|
and a matrix UM|VU|?|VM|, since GMU is bipartite.
One intra-class step changes the probability distribu-
tion from (m, 0) to (Mm, 0) or from (0, u) to (0, Uu),
while one inter-class step changes the probability
distribution from (m, u) to (UMT u, MUT m). The
design of M, U, MU and UM is detailed in Sec-
tion 3.4.
The two intra-class random walks are coupled
using the inter-class random walk on the bipartite
graph. The coupling is regulated by ?, a parameter
quantifying the importance of GMU versus GM and
GU . In the extreme case, if ? is set to 0, there is no
coupling. This amounts to separately ranking tweets
and authors by PageRank. In general, ? represents
the extent to which the ranking of tweets and their
authors depend on each other.
There are two intuitions behind the co-ranking al-
gorithm: (1) a tweet is important if it associates to
other important tweets, and is authored by impor-
tant users and (2) a user is important if they asso-
ciate to other important users, and they write impor-
tant tweets. We formulate these intuitions using the
following iterative procedure:
Step 1 Compute tweet saliency scores:
m(z+1) = (1??)([Diag(r)M(z)]T)m(z)+?UMTu(z)
m(z+1) = m(z+1)/||m(z+1)|| (8)
Step 2 Compute author saliency scores:
u(z+1) = (1??)([Diag(p)U]T)u(z)+?MUTm(z)
u(z+1) = u(z+1)/||u(z+1)|| (9)
Here, m(z) and u(z) are the ranking vectors for tweets
and authors for the z-th iteration. To guarantee con-
vergence, m and u are normalized after each itera-
tion. Note that the tweet transition matrix M is dy-
namic due to the computation of diversity while the
author transition matrix U is static. The algorithm
typically converges when the difference between the
scores computed at two successive iterations for any
tweet/author falls below a threshold ? (set to 0.001
in this study).
3.4 Affinity Matrices
The co-ranking framework is controlled by four
affinity matrices: M, U, MU and UM. In this section
we explain how these matrices are defined in more
detail.
The tweet graph is an undirected weighted graph,
where an edge between two tweets mi and m j repre-
sents their cosine similarity. An adjacency matrix M
520
describes the tweet graph where each entry corre-
sponds to the weight of a link in the graph:
Mij =
F (mi,m j)
?kF (mi,mk)
, F (mi,m j) =
~mi ?~m j
||~mi||||~m j||
(10)
where F (.) is the cosine similarity and ~m is a term
vector corresponding to tweet m. We treat a tweet
as a short document and weight each term with tf.idf
(Salton and Buckley, 1988), where tf is the term fre-
quency and idf is the inverse document frequency.
The author graph is a directed graph based on the
following linkage. When ui follows u j, we add a link
from ui to u j. Let the indicator function I (ui,u j) de-
note whether ui follows u j. The adjacency matrix U
is then defined as:
Uij =
I (ui,u j)
?k I (ui,uk)
, I (ui,u j)=
{
1if ei j ? EU
0if ei j /? EU
(11)
In the bipartite tweet-author graph GMU , the
entry EMU(i, j) is an indicator function denoting
whether tweet mi is authored by user u j:
A(mi,u j) =
{
1 if ei j ? EMU
0 if ei j /? EMU
(12)
Through EMU we define MU and UM, using the
weight matrices MU= [W?ij] and UM=[W?ji], con-
taining the conditional probabilities of transitioning
from mi to u j and vice versa:
W?ij =
A(mi,u j)
?kA(mi,uk)
, W?ji =
A(mi,u j)
?kA(mk,u j)
(13)
4 Experimental Setup
Data We crawled Twitter data from 23 seed users
(who were later invited to manually evaluate the
output of our system). In addition, we collected
the data of their followees and followers by travers-
ing the following edges, and exploring all newly
included users in the same way until no new
users were added. This procedure resulted in
a relatively large dataset consisting of 9,449,542
users, 364,287,744 tweets, 596,777,491 links, and
55,526,494 retweets. The crawler monitored the
data from 3/25/2011 to 5/30/2011. We used approx-
imately one month of this data for training and the
rest for testing.
Before building the graphs (i.e., the tweet graph,
the author graph, and the tweet-author graph), the
dataset was preprocessed as follows. We removed
tweets of low linguistic quality and subsequently
discarded users without any linkage to the remain-
ing tweets. We measured linguistic quality follow-
ing the evaluation framework put forward in Pitler
et al (2010). For instance, we measured the out-of-
vocabulary word ratio (as a way of gauging spelling
errors), entity coherence, fluency, and so on. We fur-
ther removed stopwords and performed stemming.
Parameter Settings We ran LDA with 500 itera-
tions of Gibbs sampling. The number of topics n
was set to 100 which upon inspection seemed gen-
erally coherent and meaningful. We set the damp-
ing factor ? to 0.15 following the standard PageRank
paradigm. We opted for more or less generic param-
eter values as we did not want to tune our frame-
work to the specific dataset at hand. We examined
the parameter ? which controls the balance of the
tweet-author graph in more detail. We experimented
with values ranging from 0 to 0.9, with a step size
of 0.1. Small ? values place little emphasis on the
tweet graph, whereas larger values rely more heav-
ily on the author graph. Mid-range values take both
graphs into account. Overall, we observed better
performance with values larger than 0.4. This sug-
gests that both sources of information ? the content
of the tweets and their authors ? are important for
the recommendation task. All our experiments used
the same ? value which was set to 0.6.
System Comparison We compared our approach
against three naive baselines and three state-of-the-
art systems recently proposed in the literature. All
comparison systems were subject to the same fil-
tering and preprocessing procedures as our own al-
gorithm. Our first baseline ranks tweets randomly
(Random). Our second baseline ranks tweets ac-
cording to token length: longer tweets are ranked
higher (Length). The third baseline ranks tweets
by the number of times they are reposted assum-
ing that more reposting is better (RTnum). We also
compared our method against Duan et al (2010).
Their model (RSVM) ranks tweets based on tweet
content features and tweet authority features using
the RankSVM algorithm (Joachims, 1999). Our
fifth comparison system (DTC) was Uysal and Croft
521
(2011) who use a decision tree classifier to judge
how likely it is for a tweet to be reposted by a spe-
cific user. This scenario is similar to ours when rank-
ing tweets by retweet likelihood. Finally, we com-
pared against Huang et al (2011) who use weighted
linear combination (WLC) to grade the relevance of
a tweet given a query. We implemented their model
without any query-related features as in our setting
we do not discriminate tweets depending on their
relevance to specific queries.
Evaluation We evaluated system output in two
ways, i.e., automatically and in a user study. Specif-
ically, we assume that if a tweet is retweeted it is rel-
evant and is thus ranked higher over tweets that have
not been reposted. We used our algorithm to predict
a ranking for the tweets in the test data which we
then compared against a goldstandard ranking based
on whether a tweet has been retweeted or not. We
measured ranking performance using the normalized
Discounted Cumulative Gain (nDCG; Ja?rvelin and
Keka?la?inen (2002)):
nDCG(k,VU) =
1
|VU|
?
u?VU
1
Zu
k
?
i=1
2r
u
i ?1
log(1+ i)
(14)
where VU denotes users, k indicates the top-k posi-
tions in a ranked list, and Zu is a normalization factor
obtained from a perfect ranking for a particular user.
rui is the relevance score (i.e., 1: retweeted, 0: not
retweeted) for the i-th tweet in the ranking list for
user u.
We also evaluated system output in terms of Mean
Average Precision (MAP), under the assumption
that retweeted tweets are relevant and the rest irrele-
vant:
MAP =
1
|VU|
?
u?VU
1
Nu
k
?
i=1
Pui ? r
u
i (15)
where Nu is the number of reposted tweets for user u,
and Pui is the precision at i-th position for user u
(Manning et al, 2008).
The automatic evaluation sketched above does not
assess the full potential of our recommendation sys-
tem. For instance, it is possible for the algorithm to
recommend tweets to users with no linkage to their
publishers. Such tweets may be of potential interest,
however our goldstandard data can only provide in-
formation for tweets and users with following links.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.068 0.111 0.153 0.180 0.167
Length 0.275 0.288 0.298 0.335 0.258
RTNum 0.233 0.219 0.225 0.249 0.239
RSVM 0.392 0.400 0.421 0.444 0.558
DTC 0.441 0.468 0.492 0.473 0.603
WLC 0.404 0.421 0.437 0.464 0.592
CoRank 0.519 0.546 0.550 0.585 0.617
Table 1: Evaluation of tweet ranking output produced by
our system and comparison baselines against goldstan-
dard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.081 0.103 0.116 0.107 0.175
Length 0.291 0.307 0.246 0.291 0.264
RTNum 0.258 0.318 0.343 0.346 0.257
RSVM 0.346 0.443 0.384 0.414 0.447
DTC 0.545 0.565 0.579 0.526 0.554
WLC 0.399 0.447 0.460 0.481 0.506
CoRank 0.567 0.644 0.715 0.643 0.628
Table 2: Evaluation of tweet ranking output produced by
our system and comparison baselines against judgments
elicited by users.
We therefore asked the 23 users whose Twitter data
formed the basis of our corpus to judge the tweets
ranked by our algorithm and comparison systems.
The users were asked to read the systems? recom-
mendations and decide for every tweet presented to
them whether they would retweet it or not, under the
assumption that retweeting takes place when users
find the tweet interesting.
In both automatic and human-based evaluations
we ranked all tweets in the test data. Then for each
date and user we selected the top 50 ones. Our
nDCG and MAP results are averages over users and
dates.
5 Results
Our results are summarized in Tables 1 and 2. Ta-
ble 1 reports results when model performance is
evaluated against the gold standard ranking obtained
from the Twitter network. In Table 2 model per-
formance is compared against rankings elicited by
users.
As can be seen, the Random method performs
worst. This is hardly surprising as it recommends
tweets without any notion of their importance or user
interest. Length performs considerably better than
522
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.493 0.481 0.509 0.536 0.604
PersRank 0.501 0.542 0.558 0.560 0.611
DivRank 0.487 0.505 0.518 0.523 0.585
CoRank 0.519 0.546 0.550 0.585 0.617
Table 3: Evaluation of individual system components
against goldstandard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.557 0.549 0.623 0.559 0.588
PersRank 0.571 0.595 0.655 0.613 0.601
DivRank 0.538 0.591 0.594 0.547 0.589
CoRank 0.637 0.644 0.715 0.643 0.628
Table 4: Evaluation of individual system components
against human judgments.
Random. This might be due to the fact that infor-
mativeness is related to tweet length. Using merely
the number of retweets does not seem to capture the
tweet importance as well as Length. This suggests
that highly retweeted posts are not necessarily in-
formative. For example, in our data, the most fre-
quently reposted tweet is a commercial advertise-
ment calling for reposting!
The supervised systems (RSVM, DTC, and
WLC) greatly improve performance over the naive
baselines. These methods employ standard machine
learning algorithms (such as SVMs, decision trees
and linear regression) on a large feature space. Aside
from the learning algorithm, their main difference
lies in the selection of the feature space, e.g., the way
content is represented and whether authority is taken
into account. DTC performs best on most evalua-
tion criteria. However, neither DTC nor RSVM, or
WLC take personalization into account. They gen-
erate the same recommendation lists for all users.
Our co-ranking algorithm models user interest with
respect to the content of the tweets and their pub-
lishers. Moreover, it attempts to create diverse out-
put and has an explicit mechanism for minimizing
redundancy. In all instances, using both DCG and
MAP, it outperforms the comparison systems. Inter-
estingly, the performance of CoRank is better when
measured against human judgments. This indicates
that users are interested in tweets that fall outside
the scope of their followers and that recommenda-
tion can improve user experience.
We further examined the contribution of the in-
dividual components of our system to the tweet
recommendation task. Tables 3 and 4 show how
the performance of our co-ranking algorithm varies
when considering only tweet popularity using the
standard PageRank algorithm, personalization (Per-
sRank), and diversity (DivRank). Note that DivRank
is only applied to the tweet graph. The PageR-
ank algorithm on its own makes good recommenda-
tions, while incorporating personalization improves
the performance substantially, which indicates that
individual users show preferences to specific topics
or other users. Diversity on its own does not seem
to make a difference, however it improves perfor-
mance when combined with personalization. Intu-
itively, users are more likely to repost tweets from
their followees, or tweets closely related to those
retweeted previously.
6 Conclusions
We presented a co-ranking framework for a tweet
recommendation system that takes popularity, per-
sonalization and diversity into account. Central to
our approach is the representation of tweets and
their users in a heterogeneous network and the abil-
ity to produce a global ranking that takes both in-
formation sources into account. Our model obtains
substantial performance gains over competitive ap-
proaches on a large real-world dataset (it improves
by 18.3% in DCG and 7.8% in MAP over the best
baseline). Our experiments suggest that improve-
ments are due to the synergy of the two information
sources (i.e., tweets and their authors). The adopted
graph-theoretic framework is advantageous in that
it allows to produce user-specific recommendations
and incorporate diversity in a unified model. Evalua-
tion with actual Twitter users shows that our recom-
mender can indeed identify interesting information
that lies outside the the user?s immediate following
network. In the future, we plan to extend the co-
ranking framework so as to incorporate information
credibility and temporal recency.
Acknowledgments This work was partially
funded by the Natural Science Foundation of China
under grant 60933004, and the Open Fund of the
State Key Laboratory of Software Development
Environment under grant SKLSDE-2010KF-03.
Rui Yan was supported by a MediaTek Fellowship.
523
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011a. Analyzing temporal dynamics in Twitter pro-
files for personalized recommendations in the social
web. In Proceedings of the ACM Web Science Confer-
ence 2011, pages 1?8, Koblenz, Germany.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011b. Analyzing user modeling on Twitter for per-
sonalized news recommendations. User Modeling,
Adaptation and Personalization, pages 1?12.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011c. Semantic enrichment of twitter posts for user
profile construction on the social web. The Semanic
Web: Research and Applications, pages 375?389.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alddress. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. Pro-
ceedings of the 7th International Conference on World
Wide Web, 30(1-7):107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 335?336, Melbourne, Australia.
Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein,
and Ed Chi. 2010. Short and tweet: experiments on
recommending content from information streams. In
Proceedings of the 28th International Conference on
Human Factors in Computing Systems, pages 1185?
1194, Atlanta, Georgia.
Junghoo Cho and Uri Schonfeld. 2007. Rankmass
crawler: a crawler with high personalized pagerank
coverage guarantee. In Proceedings of the 33rd Inter-
national Conference on Very Large Data Bases, pages
375?386, Vienna, Austria.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: improv-
ing recency ranking using Twitter data. In Proceed-
ings of the 19th International Conference on World
Wide Web, pages 331?340, Raleigh, North Carolina.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 295?303, Beijing, China.
John Hannon, Mike Bennett, and Barry Smyth. 2010.
Recommending twitter users to follow using content
and collaborative filtering approaches. In Proceedings
of the 4th ACM Conference on Recommender Systems,
pages 199?206, Barcelona, Spain.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison. 2011.
Predicting popular messages in Twitter. In Proceed-
ings of the 20th International Conference Companion
on World Wide Web, pages 57?58, Hyderabad, India.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblogging
services. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
373?382, Chiang Mai, Thailand.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:422?446.
Thorsten Joachims. 1999. Making large-scale svm learn-
ing practical. In Advances in Kernel Methods: Support
Vector Learning, pages 169?184. MIT press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schutze. 2008. Introduction to Information Re-
trieval, volume 1. Cambridge University Press.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010.
Divrank: the interplay of prestige and diversity in
information networks. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1009?1018,
Washington, DC.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554, Uppsala, Sweden.
Feng Qiu and Junghoo Cho. 2006. Automatic identi-
fication of user interest for personalized search. In
Proceedings of the 15th International Conference on
World Wide Web, pages 727?736, Edinburgh, Scot-
land.
Sun Aaron R., Cheng Jiesi, Zeng, and Daniel Dajun.
2009. A novel recommendation framework for micro-
blogging based on information diffusion. In Pro-
ceedings of the 19th Annual Workshop on Information
Technologies and Systems, pages 199?216, Phoenix,
Arizona.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, pages 130?137. The AAAI Press.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #Twittersearch: a comparison of microblog
search and web search. In Proceedings of the 4th ACM
524
International Conference on Web Search and Data
Mining, pages 35?44, Hong Kong, China.
Ibrahim Uysal and W. Bruce Croft. 2011. User oriented
tweet ranking: a filtering approach to microblogs.
In Proceedings of the 20th ACM International Con-
ference on Information and Knowledge Management,
pages 2261?2264, Glasgow, Scotland.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proceedings of the 22nd Conference
on Artificial Intelligence, pages 931?936, Vancouver,
British Columbia.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. 2010.
Cross-language document summarization based on
machine translation quality prediction. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 917?926, Uppsala,
Sweden.
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. Sum-
marize what you are interested in: An optimiza-
tion framework for interactive personalized summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1342?1351. Association for Computational Lin-
guistics.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing, pages 739?744. IEEE.
525
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1134?1144,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Are Two Heads Better than One? Crowdsourced Translation via a
Two-Step Collaboration of Non-Professional Translators and Editors
Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
Computer and Information Science Department,
University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
{ruiyan,gmingkun,epavlick}@seas.upenn.edu, ccb@cis.upenn.edu
Abstract
Crowdsourcing is a viable mechanism for
creating training data for machine trans-
lation. It provides a low cost, fast turn-
around way of processing large volumes
of data. However, when compared to pro-
fessional translation, naive collection of
translations from non-professionals yields
low-quality results. Careful quality con-
trol is necessary for crowdsourcing to
work well. In this paper, we examine
the challenges of a two-step collaboration
process with translation and post-editing
by non-professionals. We develop graph-
based ranking models that automatically
select the best output from multiple redun-
dant versions of translations and edits, and
improves translation quality closer to pro-
fessionals.
1 Introduction
Statistical machine translation (SMT) systems are
trained using bilingual sentence-aligned parallel
corpora. Theoretically, SMT can be applied to
any language pair, but in practice it produces the
state-of-art results only for language pairs with
ample training data, like English-Arabic, English-
Chinese, French-English, etc. SMT gets stuck
in a severe bottleneck for many minority or ?low
resource? languages with insufficient data. This
drastically limits which languages SMT can be
successfully applied to. Because of this, collect-
ing parallel corpora for minor languages has be-
come an interesting research challenge. There are
various options for creating training data for new
language pairs. Past approaches have examined
harvesting translated documents from the web
(Resnik and Smith, 2003; Uszkoreit et al, 2010;
Smith et al, 2013), or discovering parallel frag-
ments from comparable corpora (Munteanu and
Marcu, 2005; Abdul-Rauf and Schwenk, 2009;
Smith et al, 2010). Until relatively recently, lit-
tle consideration has been given to creating par-
allel data from scratch. This is because the cost
of hiring professional translators is prohibitively
high. For instance, Germann (2001) hoped to hire
professional translators to create a modest sized
100,000 word Tamil-English parallel corpus, but
were stymied by the costs and the difficulty of
finding good translators for a short-term commit-
ment.
Recently, crowdsourcing has opened the possi-
bility of translating large amounts of text at low
cost using non-professional translators. Facebook
localized its web site into different languages us-
ing volunteers (TechCrunch, 2008). DuoLingo
turns translation into an educational game, and
translates web content using its language learners
(von Ahn, 2013).
Rather than relying on volunteers or gamifica-
tion, NLP research into crowdsourcing transla-
tion has focused on hiring workers on the Ama-
zon Mechanical Turk (MTurk) platform (Callison-
Burch, 2009). This setup presents unique chal-
lenges, since it typically involves non-professional
translators whose language skills are varied, and
since it sometimes involves participants who try
to cheat to get the small financial reward (Zaidan
and Callison-Burch, 2011). A natural approach
for trying to shore up the skills of weak bilinguals
is to pair them with a native speaker of the tar-
get language to edit their translations. We review
relevant research from NLP and human-computer
interaction (HCI) on collaborative translation pro-
cesses in Section 2. To sort good translations from
bad, researchers often solicit multiple, redundant
translations and then build models to try to predict
which translations are the best, or which transla-
tors tend to produce the highest quality transla-
tions.
The contributions of this paper are:
1134
? An analysis of the difficulties posed by a two-
step collaboration between editors and trans-
lators in Mechanical Turk-style crowdsourc-
ing environments. Editors vary in quality,
and poor editing can be difficult to detect.
? A new graph-based algorithm for selecting
the best translation among multiple transla-
tions of the same input. This method takes
into account the collaborative relationship
between the translators and the editors.
2 Related work
In the HCI community, several researchers have
proposed protocols for collaborative translation
efforts (Morita and Ishida, 2009b; Morita and
Ishida, 2009a; Hu, 2009; Hu et al, 2010). These
have focused on an iterative collaboration between
monolingual speakers of the two languages, facil-
itated with a machine translation system. These
studies are similar to ours in that they rely on na-
tive speakers? understanding of the target language
to correct the disfluencies in poor translations. In
our setup the poor translations are produced by
bilingual individuals who are weak in the target
language, and in their experiments the translations
are the output of a machine translation system.
1
Another significant difference is that the HCI
studies assume cooperative participants. For in-
stance, Hu et al (2010) recruited volunteers from
the International Children?s Digital Library (Hour-
cade et al, 2003) who were all well intentioned
and participated out a sense of altruism and to
build a good reputation among the other volunteer
translators at childrenslibrary.org. Our
setup uses anonymous crowd workers hired on
Mechanical Turk, whose motivation to participate
is financial. Bernstein et al (2010) characterized
the problems with hiring editors via MTurk for a
word processing application. Workers were either
lazy (meaning they made only minimal edits) or
overly zealous (meaning they made many unnec-
essary edits). Bernstein et al (2010) addressed
this problem with a three step find-fix-verify pro-
cess. In the first step, workers click on one word
or phrase that needed to be corrected. In the next
step, a separate group of workers proposed correc-
1
A variety of HCI and NLP studies have confirmed the
efficacy of monolingual or bilingual individuals post-editing
of machine translation output (Callison-Burch, 2005; Koehn,
2010; Green et al, 2013). Past NLP work has also examined
automatic post-editing(Knight and Chander, 1994).
tions to problematic regions that had been identi-
fied by multiple workers in the first pass. In the
final step, other workers would validate whether
the proposed corrections were good.
Most NLP research into crowdsourcing has fo-
cused on Mechanical Turk, following pioneering
work by Snow et al (2008) who showed that the
platform was a viable way of collecting data for a
wide variety of NLP tasks at low cost and in large
volumes. They further showed that non-expert an-
notations are similar to expert annotations when
many non-expert labelings for the same input
are aggregated, through simple voting or through
weighting votes based on how closely non-experts
matched experts on a small amount of calibra-
tion data. MTurk has subsequently been widely
adopted by the NLP community and used for an
extensive range of speech and language applica-
tions (Callison-Burch and Dredze, 2010).
Although hiring professional translators to cre-
ate bilingual training data for machine translation
systems has been deemed infeasible, Mechanical
Turk has provided a low cost way of creating large
volumes of translations (Callison-Burch, 2009;
Ambati and Vogel, 2010). For instance, Zbib et
al. (2012; Zbib et al (2013) translated 1.5 mil-
lion words of Levine Arabic and Egyptian Arabic,
and showed that a statistical translation system
trained on the dialect data outperformed a system
trained on 100 times more MSA data. Post et al
(2012) used MTurk to create parallel corpora for
six Indian languages for less than $0.01 per word.
MTurk workers translated more than half a million
words worth of Malayalam in less than a week.
Several researchers have examined the use of ac-
tive learning to further reduce the cost of transla-
tion (Ambati et al, 2010; Ambati, 2012; Blood-
good and Callison-Burch, 2010). Crowdsourcing
allowed real studies to be conducted whereas most
past active learning were simulated. Pavlick et al
(2014) conducted a large-scale demographic study
of the languages spoken by workers on MTurk by
translating 10,000 words in each of 100 languages.
Chen and Dolan (2012) examined the steps neces-
sary to build a persistent multilingual workforce
on MTurk.
This paper is most closely related to previous
work by Zaidan and Callison-Burch (2011), who
showed that non-professional translators could ap-
proach the level of professional translators. They
solicited multiple redundant translations from dif-
1135
Urdu translator:
According to the territory?s people the pamphlets from
the Taaliban had been read in the announcements in all
the mosques of the Northern Wazeerastan.
English post-editor:
According to locals, the pamphlet released by the Taliban
was read out on the loudspeakers of all the mosques in
North Waziristan.
LDC professional:
According to the local people, the Taliban?s pamphlet
was read over the loudspeakers of all mosques in North
Waziristan.
Table 1: Different versions of translations.
ferent Turkers for a collection of Urdu sentences
that had been previously professionally translated
by the Linguistics Data Consortium. They built a
model to try to predict on a sentence-by-sentence
and Turker-by-Turker which was the best transla-
tion or translator. They also hired US-based Turk-
ers to edit the translations, since the translators
were largely based in Pakistan and exhibited er-
rors that are characteristic of speakers of English
as a language. Zaidan and Callison-Burch (2011)
observed only modest improvements when incor-
porating these edited translation into their model.
We attempt to analyze why this is, and we pro-
posed a new model to try to better leverage their
data.
3 Crowdsourcing Translation
Setup We conduct our experiments using the
data collected by Zaidan and Callison-Burch
(2011). This data set consists 1,792 Urdu sen-
tences from a variety of news and online sources,
each paired with English translations provided by
non-professional translators on Mechanical Turk.
Each Urdu sentence was translated redundantly
by 3 distinct translators, and each translation was
edited by 3 separate (native English-speaking) ed-
itors to correct for grammatical and stylistic er-
rors. In total, this gives us 12 non-professional
English candidate sentences (3 unedited, 9 edited)
per original Urdu sentence. 52 different Turkers
took part in the translation task, each translating
138 sentences on average. In the editing task, 320
Turkers participated, averaging 56 sentences each.
For comparison, the data also includes 4 differ-
ent reference translations for each source sentence,
produced by professional translators.
Table 1 gives an example of an unedited trans-
lation, an edited translation, and a professional
translation for the same sentence. The transla-
tions provided by translators on MTurk are gen-
erally done conscientiously, preserving the mean-
ing of the source sentence, but typically con-
tain simple mistakes like misspellings, typos, and
awkward word choice. English-speaking editors,
despite having no knowledge of the source lan-
guage, are able to fix these errors. In this work,
we show that the collaboration design of two
heads? non-professional Urdu translators and non-
professional English editors? yields better trans-
lated output than would either one working in iso-
lation, and can better approximate the quality of
professional translators.
Analysis We know from inspection that trans-
lations seem to improve with editing (Table 1).
Given the data from MTurk, we explore whether
this is the case in general: Do all translations im-
prove with editing? To what extent does the in-
dividual translator and the individual editor effect
the quality of the final sentence?
Figure 1: Relationship between editor aggressive-
ness and effectiveness. Each point represents an
editor/translation pair. Aggressiveness (x-axis) is
measured as the TER between the pre-edit and
post-edit version of the translation, and effective-
ness (y-axis) is measured as the average amount
by which the editing reduces the translation?s
TER
gold
. While many editors make only a few
changes, those who make many changes can bring
the translation substantially closer to professional
quality.
We use translation edit rate (TER) as a mea-
sure of translation similarity. TER represents the
amount of change necessary to transform one sen-
tence into another, so a low TER means the two
1136
0.020.05
0.07
? T
ER g
old
Editor ? TERgold 0.03  0.50
0.00
0.01
0.02
? T
ER g
old
Editor ? TERgold 0.01  0.03
-0.03
-0.01
0.01
? T
ER g
old
Editor ? TERgold -0.01  0.01
-0.08
-0.04
-0.00
? T
ER g
old
Editor ? TERgold -0.03  -0.01
0.3  0.9 0.2  0.3 0.2  0.2 0.1  0.2 0.0  0.1Translation TERgold
-0.30
-0.15
-0.01
? T
ER g
old
Editor ? TERgold -0.64  -0.03
Figure 2: Effect of editing on translations of vary-
ing quality. Rows reflect bins of editors, with the
worse editors (those whose changes result in in-
creased TER
gold
) on the top and the most effective
editors (those whose changes result in the largest
reduction in TER
gold
) on the bottom. Bars re-
flect bins of translations, with the highest TER
gold
translations on the left, and the lowest on the
right. We can see from the consistently negative
? TER
gold
in the bottom row that good editors are
able to improve both good and bad translations.
sentences are very similar. To capture the quality
(?professionalness?) of a translation, we take the
average TER of the translation against each of our
gold translations. That is, we define TER
gold
of
translation t as
TER
gold
=
1
4
4
?
i=1
TER(gold
i
, t) (1)
where a lower TER
gold
is indicative of a higher
quality (more professional-sounding) translation.
We first look at editors along two dimensions:
their aggressiveness and their effectiveness. Some
editors may be very aggressive (they make many
changes to the original translation) but still be in-
effective (they fail to bring the quality of the trans-
lation closer to that of a professional). We measure
aggressiveness by looking at the TER between
the pre- and post-edited versions of each editor?s
translations; higher TER implies more aggressive
editing. To measure effectiveness, we look at the
change in TER
gold
that results from the editing;
negative ?TER
gold
means the editor effectively
improved the quality of the translation, while pos-
itive ?TER
gold
means the editing actually brought
the translation further from our gold standard.
Figure 1 shows the relationship between these
two qualities for individual editor/translation
pairs. We see that while most translations re-
quire only a few edits, there are a large number
of translations which improve substantially after
heavy editing. This trend conforms to our intu-
ition that editing is most useful when the transla-
tion has much room for improvement, and opens
the question of whether good editors can offer im-
provements to translations of all qualities.
To address this question, we split our transla-
tions into 5 bins, based on their TER
gold
. We also
split our editors into 5 bins, based on their effec-
tiveness (i.e. the average amount by which their
editing reduces TER
gold
). Figure 2 shows the de-
gree to which editors at each level are able to im-
prove the translations from each bin. We see that
good editors are able to make improvements to
translations of all qualities, but that good editing
has the greatest impact on lower quality transla-
tions. This result suggests that finding good ed-
itor/translator pairs, rather than good editors and
good translators in isolation, should produce the
best translations overall. Figure 3 gives an exam-
ple of how an initially medium-quality translation,
when combined with good editing, produces a bet-
ter result than the higher-quality translation paired
with mediocre editing.
4 Problem Formulation
The problem definition of the crowdsourcing
translation task is straightforward: given a set of
candidate translations for a source sentence, we
want to choose the best output translation.
This output translation is the result of the com-
bined translation and editing stages. Therefore,
our method operates over a heterogeneous net-
work that includes translators and post-editors as
well as the translated sentences that they pro-
duce. We frame the problem as follows. We form
two graphs: the first graph (G
T
) represents Turk-
ers (translator/editor pairs) as nodes; the second
graph (G
C
) represents candidate translated and
1137
Figure 3: Three alternative translations (left) and the edited versions of each (right). Each edit on the
right was produced by a different editor. Order reflects the TER
gold
of each translation, with the lowest
TER
gold
on the top. Some translators receive low TER
gold
scores due to superficial errors, which can be
easily improved through editing. In the above example, the middle-ranked translation (green) becomes
the best translation after being revised by a good editor.
post-edited sentences (henceforth ?candidates?) as
nodes. These two graphs, G
T
and G
C
are com-
bined as subgraphs of a third graph (G
TC
). Edges
in G
TC
connect author pairs (nodes in G
T
) to the
candidate that they produced (nodes in G
C
). To-
gether, G
T
, G
C
, and G
TC
define a co-ranking
problem (Yan et al, 2012a; Yan et al, 2011b; Yan
et al, 2012b) with linkage establishment (Yan et
al., 2011a; Yan et al, 2012c), which we define for-
mally as follows.
Let G denote the heterogeneous graph with
nodes V and edges E. Let G = (V ,E) =
(V
T
, V
C
, E
T
, E
C
, E
TC
). G is divided into three
subgraphs, G
T
, G
C
, and G
TC
. G
C
= (V
C
, E
C
) is
a weighted undirected graph representing the can-
didates and their lexical relationships to one an-
other. Let V
C
denote a collection of translated
and edited candidates, and E
C
the lexical simi-
larity between the candidates (see Section 4.3 for
details). G
T
= (V
T
, E
T
) is a weighted undirected
graph representing collaborations between Turk-
ers. V
T
is the set of translator/editor pairs. Edges
E
T
connect translator/editor pairs in V
T
which
share a translator and/or editor. Each collabora-
tion (i.e. each node in V
T
) produces a candidate
(i.e. a node in V
C
). G
TC
= (V
TC
, E
TC
) is an
unweighted bipartite graph that ties G
T
and G
C
together and represents ?authorship?. The graph
G consists of nodes V
TC
= V
T
? V
C
and edges
E
TC
connecting each candidate with its authoring
translator/post-editor pair. The three sub-networks
(G
T
, G
C
, and G
TC
) are illustrated in Figure 4.
4.1 Inter-Graph Ranking
The framework includes three random walks, one
on G
T
, one on G
C
and one on G
TC
. A random
walk on a graph is a Markov chain, its states be-
ing the vertices of the graph. It can be described
by a stochastic square matrix, where the dimen-
sion is the number of vertices in the graph, and the
entries describe the transition probabilities from
one vertex to the next. The mutual reinforcement
framework couples the two random walks on G
T
and G
C
that rank candidates and Turkers in iso-
lation. The ranking method allows us to obtain
a global ranking by taking into account the intra-
/inter-component dependencies. In the following
sections, we describe how we obtain the rankings
on G
T
and G
C
, and then move on to discuss how
the two are coupled.
Our algorithm aims to capture the following in-
tuitions. A candidate is important if 1) it is similar
to many of the other proposed candidates and 2)
it is authored by better qualified translators and/or
post-editors. Analogously, a translator/editor pair
is believed to be better qualified if 1) the editor
is collaborating with a good translator and vice
versa and 2) the pair has authored important candi-
dates. This ranking schema is actually a reinforced
process across the heterogeneous graphs. We use
two vectors c = [pi(c)]
|c|?1
and t = [pi(t)]
|t|?1
to
denote the saliency scores pi(.) of candidates and
Turker pairs. The above-mentioned intuitions can
be formulated as follows:
? Homogeneity. We use adjacency matrix
1138
Figure 4: 2-step collaborative crowdsourcing translation model based on graph ranking framework in-
cluding three sub-networks. The undirected links between users denotes translation-editing collabora-
tion. The undirected links between candidate translations indicate lexical similarity between candidates.
A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer,
some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source
sentence to translate.
[M ]
|c|?|c|
to describe the homogeneous affinity
between candidates and [N ]
|t|?|t|
to describe the
affinity between Turkers.
c ?M
T
c, t ? N
T
t (2)
where c = |V
C
| is the number of vertices in the
candidate graph and t = |V
T
| is the number of ver-
tices in the Turker graph. The adjacency matrix
[M ] denotes the transition probabilities between
candidates, and analogously matrix [N ] denotes
the affinity between Turker collaboration pairs.
? Heterogeneity. We use an adjacency matrix
[
?
W ]
|c|?|t|
and [
?
W ]
|t|?|c|
to describe the authorship
between the output candidate and the producer
Turker pair from both of the candidate-to-Turker
and Turker-to-candidate perspectives.
c ?
?
W
T
t, t ?
?
W
T
c (3)
All affinity matrices will be defined in the next
section. By fusing the above equations, we can
have the following iterative calculation in matrix
forms. For numerical computation of the saliency
scores, the initial scores of all sentences and Turk-
ers are set to 1 and the following two steps are
alternated until convergence to select the best can-
didate.
Step 1: compute the saliency scores of candi-
dates, and then normalize using `-1 norm.
c
(n)
= (1? ?)M
T
c
(n?1)
+ ?
?
W t
(n?1)
c
(n)
= c
(n)
/||c
(n)
||
1
(4)
Step 2: compute the saliency scores of Turker
pairs, and then normalize using `-1 norm.
t
(n)
= (1? ?)N
T
t
(n?1)
+ ?
?
W c
(n?1)
t
(n)
= t
(n)
/||t
(n)
||
1
(5)
where ? specifies the relative contributions to the
saliency score trade-off between the homogeneous
affinity and the heterogeneous affinity. In order
to guarantee the convergence of the iterative form,
we must force the transition matrix to be stochastic
and irreducible. To this end, we must make the
c and t column stochastic (Langville and Meyer,
2004). c and t are therefore normalized after each
iteration of Equation (4) and (5).
4.2 Intra-Graph Ranking
The standard PageRank algorithm starts from an
arbitrary node and randomly selects to either fol-
low a random out-going edge (considering the
weighted transition matrix) or to jump to a random
node (treating all nodes with equal probability).
1139
In a simple random walk, it is assumed that all
nodes in the transitional matrix are equi-probable
before the walk starts. Then c and t are calculated
as:
c = ?M
T
c + (1? ?)
1
|V
C
|
(6)
and
t = ?N
T
t + (1? ?)
1
|V
T
|
(7)
where 1 is a vector with all elements equaling to 1
and the size is correspondent to the size of V
C
or
V
T
. ? is the damping factor usually set to 0.85, as
in the PageRank algorithm.
4.3 Affinity Matrix Establishment
We introduce the affinity matrix calculation, in-
cluding homogeneous affinity (i.e., M,N ) and
heterogeneous affinity (i.e.,
?
W,
?
W ).
As discussed, we model the collection of can-
didates as a weighted undirected graph, G
C
, in
which nodes in the graph represent candidate sen-
tences and edges represent lexical relatedness. We
define an edge?s weight to be the cosine similarity
between the candidates represented by the nodes
that it connects. The adjacency matrix M describes
such a graph, with each entry corresponding to the
weight of an edge.
F(c
i
, c
j
) =
c
i
? c
j
||c
i
||||c
j
||
M
ij
=
F(c
i
, c
j
)
?
k
F(c
i
, c
k
)
(8)
where F(.) is the cosine similarity and c is a term
vector corresponding to a candidate. We treat a
candidate as a short document and weight each
term with tf.idf (Manning et al, 2008), where tf
is the term frequency and idf is the inverse docu-
ment frequency.
The Turker graph, G
T
, is an undirected graph
whose edges represent ?collaboration.? Formally,
let t
i
and t
j
be two translator/editor pairs; we say
that pair t
i
?collaborates with? pair t
j
(and there-
fore, there is an edge between t
i
and t
j
) if t
i
and
t
j
share either a translator or an editor (or share
both a translator and an editor). Let the function
I(t
i
, t
j
) denote the number of ?collaborations?
(#col) between t
i
and t
j
.
I(t
i
, t
j
) =
{
#col (e
ij
? E
T
)
0 otherwise
, (9)
Then the adjacency matrix N is then defined as
N
ij
=
I(t
i
, t
j
)
?
k
I(t
i
, t
k
)
(10)
In the bipartite candidate-Turker graph G
TC
,
the entry E
TC
(i, j) is an indicator function denot-
ing whether the candidate c
i
is generated by t
j
:
A(c
i
, t
j
) =
{
1 (e
ij
? E
TC
)
0 otherwise
(11)
Through E
TC
we define the weight matrices
?
W
ij
and
?
W
ij
, containing the conditional probabil-
ities of transitions from c
i
to t
j
and vice versa:
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
i
, t
k
)
,
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
k
, t
j
)
(12)
5 Evaluation
We are interested in testing our random walk
method, which incorporates information from
both the candidate translations and from the Turk-
ers. We want to test two versions of our pro-
posed collaborative co-ranking method: 1) based
on the unedited translations only and 2) based on
the edited sentences after translator/editor collab-
orations.
Metric Since we have four professional transla-
tion sets, we can calculate the Bilingual Evalu-
ation Understudy (BLEU) score (Papineni et al,
2002) for one professional translator (P1) using
the other three (P2,3,4) as a reference set. We
repeat the process four times, scoring each pro-
fessional translator against the others, to calculate
the expected range of professional quality transla-
tion. In the following sections, we evaluate each of
our methods by calculating BLEU scores against
the same four sets of three reference translations.
Therefore, each number reported in our experi-
mental results is an average of four numbers, cor-
responding to the four possible ways of choosing 3
of the 4 reference sets. This allows us to compare
the BLEU score achieved by our methods against
the BLEU scores achievable by professional trans-
lators.
Baselines As a naive baseline, we choose one
candidate translation at random for each input
Urdu sentence. To establish an upper bound for
our methods, and to determine if there exist high-
quality Turker translations at all, we compute four
1140
Reference (Avg.) 42.51
Oracle (Seg-Trans) 44.93
Oracle (Seg-Trans+Edit) 48.44
Oracle (Turker-Trans) 38.66
Oracle (Turker-Trans+Edit) 39.16
Random 30.52
Lowest TER 35.78
Graph Ranking (Trans) 38.88
Graph Ranking (Trans+Edit) 41.43
Table 2: Overall BLEU performance for all
methods (with and without post-editing). The
highlighted result indicates the best performance,
which is based on both candidate sentences and
Turker information.
oracle scores. The first oracle operates at the seg-
ment level on the sentences produced by transla-
tors only: for each source segment, we choose
from the translations the one that scores highest
(in terms of BLEU) against the reference sen-
tences. The second oracle is applied similarly,
but chooses from the candidates produced by the
collaboration of translator/post-editor pairs. The
third oracle operates at the worker level: for each
source segment, we choose from the translations
the one provided by the worker whose transla-
tions (over all sentences) score the highest on
average. The fourth oracle also operates at the
worker level, but selects from sentences produced
by translator/post-editor collaborations. These or-
acle methods represent ideal solutions under our
scenario. We also examine two voting-inspired
methods. The first method selects the translation
with the minimum average TER (Snover et al,
2006) against the other translations; intuitively,
this would represent the ?consensus? translation.
The second method selects the translation gen-
erated by the Turker who, on average, provides
translations with the minimum average TER.
Results A summary of our results in given in Ta-
ble 2. As expected, random selection yields bad
performance, with a BLEU score of 30.52. The
oracles indicate that there is usually an acceptable
translation from the Turkers for any given sen-
tence. Since the oracles select from a small group
of only 4 translations per source segment, they are
not overly optimistic, and rather reflect the true po-
tential of the collected translations. On average,
the reference translations give a score of 42.38. To
put this in perspective, the output of a state-of-the-
Figure 5: Effect of candidate-Turker coupling (?)
on BLEU score.
art machine translation system (the syntax-based
variant of Joshua) achieves a score of 26.91, which
is reported in (Zaidan and Callison-Burch, 2011).
The approach which selects the translations with
the minimum average TER (Snover et al, 2006)
against the other three translations (the ?consen-
sus? translation) achieves BLEU scores of 35.78.
Using the raw translations without post-editing,
our graph-based ranking method achieves a BLEU
score of 38.89, compared to Zaidan and Callison-
Burch (2011)? s reported score of 28.13, which
they achieved using a linear feature-based classi-
fication. Their linear classifier achieved a reported
score of 39.06
2
when combining information from
both translators and editors. In contrast, our pro-
posed graph-based ranking framework achieves a
score of 41.43 when using the same information.
This boost in BLEU score confirms our intuition
that the hidden collaboration networks between
candidate translations and transltor/editor pairs are
indeed useful.
Parameter Tuning There are two parameters in
our experimental setups: ? controls the probability
of starting a new random walk and ? controls the
coupling between the candidate and Turker sub-
graphs. We set the damping factor ? to 0.85, fol-
lowing the standard PageRank paradigm. In order
to determine a value for ?, we used the average
BLEU, computed against the professional refer-
2
Note that the data we used in our experiments are slightly
different, by discarding nearly 100 NULL sentences in the
raw data. We do not re-implement this baseline but report the
results from the paper directly. According to our experiments,
most of the results generated by baselines and oracles are very
close to the previously reported values.
1141
Plain ranking 38.89
w/o collaboration 38.88
Shared translator 41.38
Shared post-editor 41.29
Shared Turker 41.43
Table 3: Variations of all component settings.
ence translations, as a tuning metric. We experi-
mented with values of ? ranging from 0 to 1, with
a step size of 0.05 (Figure 5). Small ? values place
little emphasis on the candidate/Turker coupling,
whereas larger values rely more heavily on the co-
ranking. Overall, we observed better performance
with values within the range of 0.05-0.15. This
suggests that both sources of information? the can-
didate itself and its authors? are important for the
crowdsourcing translation task. In all of our re-
ported results, we used the ? = 0.1.
Analysis We examine the relative contribution
of each component of our approach on the overall
performance. We first examine the centroid-based
ranking on the candidate sub-graph (G
C
) alone
to see the effect of voting among translated sen-
tences; we denote this strategy as plain ranking.
Then we incorporate the standard random walk on
the Turker graph (G
T
) to include the structural in-
formation but without yet including any collabo-
ration information; that is, we incorporate infor-
mation from G
T
and G
C
without including edges
linking the two together. The co-ranking paradigm
is exactly the same as the framework described in
Section 3.2, but with simplified structures.
Finally, we examine the two-step collaboration
based candidate-Turker graph using several varia-
tions on edge establishment. As before, the nodes
are the translator/post-editor working pairs. We
investigate three settings in which 1) edges con-
nect two nodes when they share only a transla-
tor, 2) edges connect two nodes when they share
only a post-editor, and 3) edges connect two nodes
when they share either a translator or a post-editor.
These results are summarized in Table 3.
Interestingly, we observe that when modeling
the linkage between the collaboration pairs, con-
necting Turker pairs which share either a transla-
tor or the post-editor achieves better performance
than connecting pairs that share only translators or
connecting pairs which share only editors. This
result supports the intuition that a denser collabo-
ration matrix will help propagate saliency to good
translators/post-editors and hence provides better
predictions for candidate quality.
6 Conclusion
We have proposed an algorithm for using a two-
step collaboration between non-professional trans-
lators and post-editors to obtain professional-
quality translations. Our method, based on a
co-ranking model, selects the best crowdsourced
translation from a set of candidates, and is capable
of selecting translations which near professional
quality.
Crowdsourcing can play a pivotal role in fu-
ture efforts to create parallel translation datasets.
In addition to its benefits of cost and scalabil-
ity, crowdsourcing provides access to languages
that currently fall outside the scope of statistical
machine translation research. In future work on
crowdsourced translation, further benefits in qual-
ity improvement and cost reduction could stem
from 1) building ground truth data sets based on
high-quality Turkers? translations and 2) identify-
ing when sufficient data has been collected for a
given input, to avoid soliciting unnecessary redun-
dant translations.
Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as represent-
ing official policies or endorsements by DARPA
or the U.S. Government. This research was sup-
ported by the Johns Hopkins University Human
Language Technology Center of Excellence and
through gifts from Microsoft, Google and Face-
book.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Workshop on Creating Speech and Lan-
guage Data with MTurk.
1142
Vamshi Ambati, Stephan Vogel, and Jaime G Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In LREC, volume 11, pages
2169?2174. Citeseer.
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mel-
lon University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of the ACM Symposium on
User Interface Software and Technology (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12,
June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exercise.
In Proceedings of Machine Translation Evaluation
Workshop.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon?s me-
chanical turk. In Proceedings of EMNLP.
David L. Chen and William B. Dolan. 2012. Build-
ing a persistent workforce on mechanical turk for
multilingual data collection. In Proceedings of the
Human Computer Interaction International Confer-
ence.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In Proceedings of
the Workshop on Data-driven Methods in Machine
Translation - Volume 14, DMMT ?01, pages 1?8.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448.
Juan Pablo Hourcade, Benjamin B Bederson, Allison
Druin, Anne Rose, Allison Farber, and Yoshifumi
Takayama. 2003. The international children?s digi-
tal library: viewing digital books online. Interacting
with Computers, 15(2):151?167.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration be-
tween monolingual users. In Proceedings of
ACM SIGKDD Workshop on Human Computation
(HCOMP).
Chang Hu, Benjamin B. Bederson, Philip Resnik, and
Yakov Kronrod. 2011. Monotrans2: A new human
computation system to support monolingual trans-
lation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ?11,
pages 1133?1136.
Chang Hu. 2009. Collaborative translation by mono-
lingual users. In CHI ?09 Extended Abstracts on Hu-
man Factors in Computing Systems, CHI EA ?09,
pages 3105?3108.
Martin Kay. 1998. The proper place of men and ma-
chines in language translation. Machine Transla-
tion, 12(1/2):3?23, January.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In In Proceedings of
AAAI.
Philipp Koehn. 2010. Enabling monolingual transla-
tors: Post-editing vs. options. In HLT-NAACL?10,
pages 537?545, June.
Amy N Langville and Carl D Meyer. 2004. Deeper
inside pagerank. Internet Mathematics, 1(3):335?
380.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 133?137, July.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? first experiments on article quality
prediction in the science journalism domain. Trans-
actions of Association for Computational Linguis-
tics.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Daisuke Morita and Toru Ishida. 2009a. Collaborative
translation by monolinguals with machine transla-
tors. In Proceedings of the 14th International Con-
ference on Intelligent User Interfaces, IUI ?09, pages
361?366.
Daisuke Morita and Toru Ishida. 2009b. Designing
protocols for collaborative translation. In Interna-
tional Conference on Principles of Practice in Multi-
Agent Systems (PRIMA-09), pages 17?32. Springer.
1143
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477?504, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 2(Feb):79?92.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six Indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380, September.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
HLT-NAACL?10, pages 403?411.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the Common Crawl. In Proceedings of
the 2013 Conference of the Association for Compu-
tational Linguistics (ACL 2013), July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263.
TechCrunch. 2008. Facebook taps users to create
translated versions of site, January.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109.
Luis von Ahn. 2013. Duolingo: Learn a language for
free while helping to translate the web. In Proceed-
ings of the 2013 International Conference on Intel-
ligent User Interfaces, IUI ?13, pages 1?2.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceed-
ings of the 34th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?11, pages 745?754.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012a.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 516?525.
Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012b.
Visualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
streams. In Proceedings of the 21st ACM Interna-
tional Conference on Information and Knowledge
Management, CIKM ?12, pages 275?284.
Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xi-
aoming Li. 2012c. Hierarchical graph summariza-
tion: leveraging hybrid information through visible
and invisible linkage. In PAKDD?12, pages 97?108.
Springer.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1220?1229.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In The 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013.
Systematic comparison of professional and crowd-
sourced reference translations for machine transla-
tion. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia.
Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He,
and Xiaoming Li. 2013. Timeline generation with
social attention. In Proceedings of the 36th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?13,
pages 1061?1064.
1144
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611?617,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Enriching Cold Start Personalized Language Model 
Using Social Network Information 
  Yu-Yang Huang
?
, Rui Yan*, Tsung-Ting Kuo
?
, Shou-De Lin
??
 
?
Graduate Institute of Computer Science and Information Engineering,  
National Taiwan University, Taipei, Taiwan 
?
Graduate Institute of Network and Multimedia,  
National Taiwan University, Taipei, Taiwan 
*Computer and Information Science Department,  
University of Pennsylvania, Philadelphia, PA 19104, U.S.A. 
{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.edu 
 
Abstract 
We introduce a generalized framework to enrich 
the personalized language models for cold start 
users. The cold start problem is solved with 
content written by friends on social network 
services. Our framework consists of a mixture 
language model, whose mixture weights are es-
timated with a factor graph. The factor graph is 
used to incorporate prior knowledge and heuris-
tics to identify the most appropriate weights. 
The intrinsic and extrinsic experiments show 
significant improvement on cold start users. 
1 Introduction 
Personalized language models (PLM) on social 
network services are useful in many aspects (Xue 
et al, 2009; Wen et al, 2012; Clements, 2007), 
For instance, if the authorship of a document is 
in doubt, a PLM may be used as a generative 
model to identify it. In this sense, a PLM serves 
as a proxy of one?s writing style. Furthermore, 
PLMs can improve the quality of information 
retrieval and content-based recommendation sys-
tems, where documents or topics can be recom-
mended based on the generative probabilities. 
However, it is challenging to build a PLM for 
users who just entered the system, and whose 
content is thus insufficient to characterize them. 
These are called ?cold start? users. Producing 
better recommendations is even more critical for 
cold start users to make them continue to use the 
system. Therefore, this paper focuses on how to 
overcome the cold start problem and obtain a 
better PLM for cold start users. 
The content written by friends on a social 
network service, such as Facebook or Twitter, is 
exploited. It can be either a reply to an original 
post or posts by friends. Here the hypothesis is 
that friends, who usually share common interests, 
tend to discuss similar topics and use similar 
words than non-friends. In other words, we be-
lieve that a cold start user?s language model can 
be enriched and better personalized by incorpo-
rating content written by friends. 
Intuitively, a linear combination of document-
level language models can be used to incorporate 
content written by friends. However, it should be 
noticed that some documents are more relevant 
than others, and should be weighted higher. To 
obtain better weights, some simple heuristics 
could be exploited. For example, we can measure 
the similarity or distance between a user lan-
guage model and a document language model. In 
addition, documents that are shared frequently in 
a social network are usually considered to be 
more influential, and could contribute more to 
the language model. More complex heuristics 
can also be derived. For instance, if two docu-
ments are posted by the same person, their 
weights should be more similar. The main chal-
lenge lies in how such heuristics can be utilized 
in a systematic manner to infer the weights of 
each document-level language model. 
In this paper, we exploit the information on 
social network services in two ways. First, we 
impose the social dependency assumption via a 
finite mixture model. We model the true, albeit 
unknown, personalized language model as a 
combination of a biased user language model and 
a set of relevant document language models. Due 
to the noise inevitably contained in social media 
content, instead of using all available documents, 
we argue that by properly specifying the set of 
relevant documents, a better personalized lan-
guage model can be learnt. In other words, each 
user language model is enriched by a personal-
ized collection of background documents. 
Second, we propose a factor graph model 
(FGM) to incorporate prior knowledge (e.g. the 
heuristics described above) into our model. Each 
611
mixture weight is represented by a random vari-
able in the factor graph, and an efficient algo-
rithm is proposed to optimize the model and infer 
the marginal distribution of these variables. Use-
ful information about these variables is encoded 
by a set of potential functions. 
The main contributions of this work are sum-
marized below: 
? To solve the cold start problem encountered 
when estimating PLMs, a generalized frame-
work based on FGM is proposed. We incorpo-
rate social network information into user lan-
guage models through the use of FGM. An it-
erative optimization procedure utilizing per-
plexity is presented to learn the parameters. 
To our knowledge, this is the first proposal to 
use FGM to enrich language models. 
? Perplexity is selected as an intrinsic evalua-
tion, and experiment on authorship attribution 
is used as an extrinsic evaluation. The results 
show that our model yields significant im-
provements for cold start users. 
2 Methodology 
2.1 Social-Driven Personalized Language 
Model 
The language model of a collection of documents 
can be estimated by normalizing the counts of 
words in the entire collection (Zhai, 2008). To 
build a user language model, one na?ve way is to 
first normalize word frequency ?(?, ?)  within 
each document, and then average over all the 
documents in a user?s document collection. The 
resulting unigram user language model is: 
??(?) =
1
|??|
?
?(?, ?)
|?|????
 
=
1
|??|
? ??(?)
????
 
(1) 
where ??(?) is the language model of a particu-
lar document, and ?? is the user?s document col-
lection. This formulation is basically an equal-
weighted finite mixture model. 
A simple yet effective way to smooth a lan-
guage model is to linearly interpolate with a 
background language model (Chen and Good-
man, 1996; Zhai and Lafferty, 2001). In the line-
ar interpolation method, all background docu-
ments are treated equally. The entire document 
collection is added to the user language model 
??(?) with the same interpolation coefficient. 
Our main idea is to specify a set of relevant 
documents for the target user using information 
embedded in a social network, and enrich the 
smoothing procedure with these documents. Let 
????  denote the content from relevant persons 
(e.g. social neighbors) of u1, our idea can be con-
cisely expressed as: 
??1
? (?) = ??1??1(?) + ? ??????(?)
???????
 (2) 
where ??? is the mixture weight of the language 
model of document di, and ??1 + ???? = 1 . 
Documents posted by irrelevant users are not 
included as we believe the user language model 
can be better personalized by exploiting the so-
cial relationship in a more structured way. In our 
experiment, we choose the first degree neighbor 
documents as ????. 
Also note that we have made no assumption 
about how the ?base? user language model 
??1(?) is built. In practice, it need not be models 
following maximum likelihood estimation, but 
any language model can be integrated into our 
framework to achieve a better refined model. 
Furthermore, any smoothing method can be ap-
plied to the language model without degrading 
the effectiveness. 
2.2 Factor Graph Model (FGM) 
Now we discuss how the mixture weights can be 
estimated. We introduce a factor graph model 
(FGM) to make use of the diverse information on 
a social network. Factor graph (Kschischang et 
al., 2006) is a bipartite graph consisting of a set 
of random variables and a set of factors which 
signifies the relationships among the variables. It 
is best suited in situations where the data is clear-
ly of a relational nature (Wang et al, 2012). The 
joint distribution of the variables is factored ac-
cording to the graph structure. Using FGM, one 
can incorporate the knowledge into the potential 
function for optimization and perform joint in-
ference over documents. As shown in Figure 1, 
the variables included in the model are described 
as follows: 
Candidate variables ?? = ??, ??? . The ran-
dom variables in the top layer stand for the de-
grees of belief that a document di should be in-
cluded in the PLM of the target user ?. 
Figure 1: A two-layered factor graph (FGM) 
proposed to estimate the mixture weights. 
612
Attribute variables xi. Local information is 
stored as the random variables in the bottom lay-
er. For example, x1 might represent the number 
of common friends between the author of a doc-
ument di and our target user. 
The potential functions in the FGM are: 
Attribute-to-candidate function. This poten-
tial function captures the local dependencies of a 
candidate variable to the relevant attributes. Let 
the candidate variable yi correspond to a docu-
ment di, the attribute-to-candidate function of yi 
is defined in a log-linear form: 
?(?? , ?) =
1
??
???{???(??, ?)} (3) 
where A is the set of attributes of either the doc-
ument di or target user u; f is a vector of feature 
functions which locally model the value of yi 
with attributes in A; ??  is the local partition 
function and ? is the weight vector to be learnt. 
In our experiment, we define the vector of 
functions as ? = ?????, ???? , ????, ????, ????
? as: 
? Similarity function ???? . The similarity be-
tween language models of the target user and 
a document should play an important role. We 
use cosine similarity between two unigram 
models in our experiments. 
? Document quality function ????. The out-of-
vocabulary (OOV) ratio is used to measure the 
quality of a document. It is defined as 
???? = 1 ?
|{?:? ? ?? ? ? ? ?}|
|??|
 (4) 
where ?  is the vocabulary set of the entire 
corpus, with stop words excluded. 
? Document popularity function ???? . This 
function is defined as the number of times di is 
shared to model the popularity of documents. 
? Common friend function ????. It is defined 
as the number of common friends between the 
target user u1 and the author of di. 
? Author friendship function ??? . Assuming 
that documents posted by a user with more 
friends are more influential, this function is 
defined as the number of friends of di?s author. 
Candidate-to-candidate function. This po-
tential function defines the correlation of a can-
didate variable yi with another candidate variable 
yj in the factor graph. The function is defined as 
?(?? , ??) =
1
???,?
???{???(??, ??)} (5) 
where g is a vector of feature functions indicat-
ing whether two variables are correlated. If we 
further denote the set of all related variables as 
?(??) , then for any candidate variable yi, we 
have the following brief expression: 
?(?? , ?(??)) = ? ?(?? , ??)
????(??)
 (6) 
For two candidate variables, let the corre-
sponding document be di and dj, respectively, we 
define the vector ? = ????? , ?????
? as: 
? User relationship function ????. We assume 
that two candidate variables have higher de-
pendency if they represent documents of the 
same author or the two authors are friends. 
The dependency should be even greater if two 
documents are similar. Let ?(?)  denote the 
author of a document d and ?[?] denote the 
closed neighborhood of a user u, we define 
???? = ?{?(??) ? ?[?(??)]} ? ???(?? , ??) (7) 
? Co-category function ????. For any two can-
didate variables, it is intuitive that the two var-
iables would have a higher correlation if di 
and dj are of the same category. Let ?(?) de-
note the category of document d, we define 
???? = ?{?(??) = ?(??)} ? ???(?? , ??) (8) 
2.3 Model Inference and Optimization 
Let Y and X be the set of all candidate variables 
and attribute variables, respectively. The joint 
distribution encoded by the FGM is given by 
multiplying all potential functions. 
?(?, ?) =??(??, ?)?(?? , ?(??))
?
 (9) 
The desired marginal distribution can be ob-
tained by marginalizing all other variables. Since 
under most circumstances, however, the factor 
graph is densely connected, the exact inference is 
intractable and approximate inference is required. 
After obtaining the marginal probabilities, the 
mixture weights ???  in Eq. 2 are estimated by 
normalizing the corresponding marginal proba-
bilities ?(??) over all candidate variables, which 
can be written as 
??? = (1 ? ??1)
?(??)
? ?(??)?:???????
 (10) 
where the constraint ??1 + ???? = 1 leads to a 
valid probability distribution for our mixture 
model. 
A factor graph is normally optimized by gra-
dient-based methods. Unfortunately, since the 
ground truth values of the mixture weights are 
not available, we are prohibited from using su-
pervised approaches. Here we propose a two-step 
iterative procedure to optimize our model. At 
613
first, all the model parameters (i.e. ?, ?, ??) are 
randomly initialized. Then, we infer the marginal 
probabilities of candidate variables. Given these 
marginal probabilities, we can evaluate the per-
plexity of the user language model on a held-out 
dataset, and search for better parameters. This 
procedure is repeated until convergence. Also, 
notice that by using FGM, we reduce the number 
of parameters from 1 + |????| to 1 + |?| + |?|, 
lowering the risk of overfitting. 
3 Experiments 
3.1 Dataset and Experiment Setup 
We perform experiments on the Twitter dataset 
collected by Galuba et al (2010). Twitter data 
have been used to verify models with different 
purposes (Lin et al, 2011; Tan et al, 2011). To 
emphasize on the cold start scenario, we random-
ly selected 15 users with about 35 tweets and 70 
friends as candidates for an authorship attribution 
task. Our experiment corpus consists of 4322 
tweets. All words with less than 5 occurrences 
are removed. Stop words and URLs are also re-
moved and all tweets are stemmed. We identify 
the 100 most frequent terms as categories. The 
size of the vocabulary set is 1377. 
We randomly partitioned the tweets of each 
user into training, validation and testing sets. The 
reported result is the average of 10 random splits. 
In all experiments, we vary the size of training 
data from 1% to 15%, and hold out the same 
number of tweets from each user as validation 
and testing data. The statistics of our dataset, 
given 15% training data, are shown in Table 1. 
 Loopy belief propagation (LBP) is used to ob-
tain the marginal probabilities of the variables 
(Murphy et al, 1999). Parameters are searched 
with the pattern search algorithm (Audet and 
Dennis, 2002). To not lose generality, we use the 
default configuration in all experiments. 
# of Max. Min. Avg. 
Tweets 70 19 35.4 
Friends 139 24 68.9 
Variables 467 97 252.7 
Edges 9216 231 3427.1 
Table 1: Dataset statistics 
3.2 Baseline Methods 
We compare our framework with two baseline 
methods. The first (?Cosine?) is a straightfor-
ward implementation that sets all mixture 
weights ??? to the cosine similarity between the 
probability mass vectors of the document and 
user unigram language models. The second 
(?PS?) uses the pattern search algorithm to per-
form constrained optimization over the mixture 
weights. As mentioned in section 2.3, the main 
difference between this method and ours 
(?FGM?) is that we reduce the search space of 
the parameters by FGM. Furthermore, social 
network information is exploited in our frame-
work, while the PS method performs a direct 
search over mixture weights, discarding valuable 
knowledge. 
Different from other smoothing methods that 
are usually mutually exclusive, any other 
smoothing methods can be easily merged into 
our framework. In Eq. 2, the base language 
model ??1(?) can be already smoothed by any 
techniques before being plugged into our frame-
work. Our framework then enriches the user lan-
guage model with social network information. 
We select four popular smoothing methods to 
demonstrate such effect, namely additive 
smoothing, absolute smoothing (Ney et al, 1995), 
Jelinek-Mercer smoothing (Jelinek and Mercer, 
1980) and Dirichlet smoothing (MacKay and 
Peto, 1994). The results of using only the base 
model (i.e. set ??? = 0 in Eq. 2) are denoted as 
?Base? in the following tables. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 
5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 
10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 
15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 
5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** 
10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0** 
15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4 
Table 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet-
ter than the next highest score, by t-test at a significance level of 0.05. 
614
3.3 Perplexity 
As an intrinsic evaluation, we first compute the 
perplexity of unseen sentences under each user 
language model. The result is shown in Table 2. 
Our method significantly outperforms all of 
the methods in almost all settings. We observe 
that the ?PS? method takes a long time to con-
verge and is prone to overfitting, likely because 
it has to search about a few hundred parameters 
on average. As expected, the advantage of our 
model is more apparent when the data is sparse. 
3.4 Authorship Attribution (AA) 
The authorship attribution (AA) task is chosen as 
the extrinsic evaluation metric. Here the goal is 
not about comparing with the state-of-the-art ap-
proaches in AA, but showing that LM-based ap-
proaches can benefit from our framework. 
To apply PLM on this task, a na?ve Bayes 
classifier is implemented (Peng et al, 2004). The 
most probable author of a document d is the one 
whose PLM yields the highest probability, and is 
determined by ?? = argmax?{? ??(?)??? }. 
The result is shown in Table 3. Our model im-
proves personalization and outperforms the base-
lines under cold start settings. When data is 
sparse, the ?PS? method tends to overfit the 
noise, while the ?Cosine? method contains too 
few information and is severely biased. Our 
method strikes a balance between model com-
plexity and the amount of information included, 
and hence performs better than the others. 
4 Related Work 
Personalization has long been studied in various 
textual related tasks. Personalized search is es-
tablished by modeling user behavior when using 
search engines (Shen et al, 2005; Xue et al, 
2009). Query language model could be also ex-
panded based on personalized user modeling 
(Chirita et al, 2007). Personalization has also 
been modeled in many NLP tasks such as sum-
marization (Yan et al, 2011) and recommenda-
tion (Yan et al, 2012). Different from our pur-
pose, these models do not aim at exploiting so-
cial media content to enrich a language model. 
Wen et al (2012) combines user-level language 
models from a social network, but instead of fo-
cusing on the cold start problem, they try to im-
prove the speech recognition performance using 
a mass amount of texts on social network. On the 
other hand, our work explicitly models the more 
sophisticated document-level relationships using 
a probabilistic graphical model. 
5 Conclusion 
The advantage of our model is threefold. First, 
prior knowledge and heuristics about the social 
network can be adapted in a structured way 
through the use of FGM. Second, by exploiting a 
well-studied graphical model, mature inference 
techniques, such as LBP, can be applied in the 
optimization procedure, making it much more 
effective and efficient. Finally, different from 
most smoothing methods that are mutually ex-
clusive, any other smoothing method can be in-
corporated into our framework to be further en-
hanced. Using only 1% of the training corpus, 
our model can improve the perplexity of base 
models by as much as 40% and the accuracy of 
authorship attribution by at most 15%. 
6 Acknowledgement 
This work was sponsored by AOARD grant 
number No. FA2386-13-1-4045 and National 
Science Council, National Taiwan University 
and Intel Corporation under Grants NSC102-
2911-I-002-001 and NTU103R7501 and grant 
102-2923-E-002-007-MY2, 102-2221-E-002-170, 
101-2628-E-002-028-MY2. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27** 
5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53** 
10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87** 
15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67** 
5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93 
10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53 
15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40 
Table 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is 
significantly better than the next highest score, by t-test at a significance level of 0.05. 
615
Reference 
Charles Audet and J. E. Dennis, Jr. 2002. Analysis of 
generalized pattern searches. SIAM J. on Optimiza-
tion, 13(3):889?903, August. 
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language 
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics, 
ACL ?96, pages 310?318, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-
gang Nejdl. 2007. Personalized query expansion 
for the web. In Proceedings of the 30th Annual In-
ternational ACM SIGIR Conference on Research 
and Development in Information Retrieval, 
SIGIR ?07, pages 7?14, New York, NY, USA. 
ACM. 
Maarten Clements. 2007. Personalization of social 
media. In Proceedings of the 1st BCS IRSG Con-
ference on Future Directions in Information Access, 
FDIA?07, pages 14?14, Swinton, UK, UK. British 
Computer Society. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic, and Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. In Proceedings of the 3rd 
Conference on Online Social Networks, WOSN?10, 
pages 3?3, Berkeley, CA, USA. USENIX Associa-
tion. 
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters 
from sparse data. In In Proceedings of the Work-
shop on Pattern Recognition in Practice, pages 
381?397, Amsterdam, The Netherlands: North-
Holland, May. 
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 
2006. Factor graphs and the sum-product algorithm. 
IEEE Trans. Inf. Theor., 47(2):498?519, Septem-
ber. 
Jimmy Lin, Rion Snow, and William Morgan. 2011. 
Smoothing techniques for adaptive online language 
models: Topic tracking in tweet streams. In Pro-
ceedings of the 17th ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining, KDD ?11, pages 422?429, New York, NY, 
USA. ACM. 
David J.C. MacKay and Linda C. Bauman Peto. 1994. 
A hierarchical dirichlet language model. Natural 
Language Engineering, 1:1?19. 
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the 
Fifteenth Conference on Uncertainty in Artificial 
Intelligence, UAI?99, pages 467?475, San Francis-
co, CA, USA. Morgan Kaufmann Publishers Inc. 
Hermann Ney, Ute Essen, and Reinhard Kneser. 1995. 
On the estimation of ?small? probabilities by leav-
ing-one-out. IEEE Trans. Pattern Anal. Mach. In-
tell., 17(12):1202?1212, December. 
Fuchun Peng, Dale Schuurmans, and Shaojun Wang. 
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Inf. Retr., 7(3-4):317?345, 
September. 
Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. 
Implicit user modeling for personalized search. In 
Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Manage-
ment, CIKM ?05, pages 824?831, New York, NY, 
USA. ACM. 
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, 
Ming Zhou, and Ping Li. 2011. User-level senti-
ment analysis incorporating social networks. In 
Proceedings of the 17th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and 
Data Mining, KDD ?11, pages 1397?1405, New 
York, NY, USA. ACM. 
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie 
Tang. 2012. Cross-lingual knowledge linking 
across wiki knowledge bases. In Proceedings of the 
21st International Conference on World Wide Web, 
WWW ?12, pages 459?468, New York, NY, USA. 
ACM. 
Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and 
Lin-Shan Lee. 2012. Personalized language model-
ing by crowd sourcing with social network data for 
voice access of cloud applications. In Spoken Lan-
guage Technology Workshop (SLT), 2012 IEEE, 
pages 188?193. 
Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 
2009. User language model for collaborative per-
sonalized search. ACM Trans. Inf. Syst., 
27(2):11:1?11:28, March. 
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. 
Summarize what you are interested in: An optimi-
zation framework for interactive personalized 
summarization. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1342?1351, Strouds-
burg, PA, USA. Association for Computational 
Linguistics. 
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. 
Tweet recommendation with graph co-ranking. In 
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long Pa-
pers - Volume 1, ACL ?12, pages 516?525, 
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics. 
ChengXiang Zhai. 2008. Statistical Language Models 
for Information Retrieval. Now Publishers Inc., 
Hanover, MA, USA. 
616
Chengxiang Zhai and John Lafferty. 2001. A study of 
smoothing methods for language models applied to 
ad hoc information retrieval. In Proceedings of the 
24th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval, SIGIR ?01, pages 334?342, New York, NY, 
USA. ACM. 
 
617
