Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 441?448,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semi-Supervised Learning of Partial Cognates using  
Bilingual Bootstrapping 
 
Oana Frunza and Diana Inkpen 
 
School of Information Technology and Engineering 
University of Ottawa 
Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
  
Abstract 
Partial cognates are pairs of words in two 
languages that have the same meaning in 
some, but not all contexts. Detecting the 
actual meaning of a partial cognate in 
context can be useful for Machine Trans-
lation tools and for Computer-Assisted 
Language Learning tools. In this paper 
we propose a supervised and a semi-
supervised method to disambiguate par-
tial cognates between two languages: 
French and English. The methods use 
only automatically-labeled data; therefore 
they can be applied for other pairs of lan-
guages as well. We also show that our 
methods perform well when using cor-
pora from different domains. 
1 Introduction 
When learning a second language, a student 
can benefit from knowledge in his / her first lan-
guage (Gass, 1987), (Ringbom, 1987), (LeBlanc 
et al 1989). Cognates ? words that have similar 
spelling and meaning ? can accelerate vocabu-
lary acquisition and facilitate the reading com-
prehension task. On the other  hand, a student has 
to pay attention to the pairs of words that look 
and sound similar but have different meanings ? 
false friends pairs, and especially to pairs of 
words that share meaning in some but not all 
contexts ? the partial cognates.  
Carroll (1992) claims that false friends can be 
a hindrance in second language learning. She 
suggests that a cognate pairing process between 
two words that look alike happens faster in the 
learner?s mind than a false-friend pairing. Ex-
periments with second language learners of dif-
ferent stages conducted by Van et al (1998) 
suggest that missing false-friend recognition can 
be corrected when cross-language activation is 
used ? sounds, pictures, additional explanation, 
feedback. 
   Machine Translation (MT) systems can benefit 
from extra information when translating a certain 
word in context. Knowing if a word in the source 
language is a cognate or a false friend with a 
word in the target language can improve the 
translation results. Cross-Language Information 
Retrieval systems can use the knowledge of the 
sense of certain words in a query in order to re-
trieve desired documents in the target language.  
Our task, disambiguating partial cognates, is in 
a way equivalent to coarse grain cross-language 
Word-Sense Discrimination. Our focus is disam-
biguating French partial cognates in context: de-
ciding if they are used as cognates with an 
English word, or if they are used as false friends. 
There is a lot of work done on monolingual 
Word Sense Disambiguation (WSD) systems that 
use supervised and unsupervised methods and 
report good results on Senseval data, but there is 
less work done to disambiguate cross-language 
words. The results of this process can be useful 
in many NLP tasks. 
   Although French and English belong to differ-
ent branches of the Indo-European family of lan-
guages, their vocabulary share a great number of 
similarities. Some are words of Latin and Greek 
origin: e.g., education and theory. A small num-
ber of very old, ?genetic" cognates go back all 
the way to Proto-Indo-European, e.g., m?re - 
mother and pied - foot. The majority of these 
pairs of words penetrated the French and English 
language due to the geographical, historical, and 
cultural contact between the two countries over 
441
many centuries (borrowings). Most of the bor-
rowings have changed their orthography, follow-
ing different orthographic rules (LeBlanc and 
Seguin, 1996) and most likely their meaning as 
well. Some of the adopted words replaced the 
original word in the language, while others were 
used together but with slightly or completely dif-
ferent meanings. 
   In this paper we describe a supervised and also 
a semi-supervised method to discriminate the 
senses of partial cognates between French and 
English. In the following sections we present 
some definitions, the way we collected the data, 
the methods that we used, and evaluation ex-
periments with results for both methods.   
2 Definitions  
We adopt the following definitions. The defini-
tions are language-independent, but the examples 
are pairs of French and English words, respec-
tively. 
Cognates, or True Friends (Vrais Amis), are 
pairs of words that are perceived as similar and 
are mutual translations. The spelling can be iden-
tical or not, e.g., nature - nature, reconnaissance 
- recognition. 
False Friends (Faux Amis) are pairs of words in 
two languages that are perceived as similar but 
have different meanings, e.g., main (= hand) - 
main (= principal or essential), blesser (= to in-
jure) - bless (= b?nir).  
Partial Cognates are pairs of words that have 
the same meaning in both languages in some but 
not all contexts. They behave as cognates or as 
false friends, depending on the sense that is used 
in each context. For example, in French, facteur 
means not only factor, but also mailman, while 
?tiquette can also mean label or sticker, in addi-
tion to the cognate sense. 
Genetic Cognates are word pairs in related lan-
guages that derive directly from the same word 
in the ancestor (proto-)language. Because of 
gradual phonetic and semantic changes over long 
periods of time, genetic cognates often differ in 
form and/or meaning, e.g., p?re - father, chef - 
head. This category excludes lexical borrowings, 
i.e., words transferred from one language to an-
other at some point of time, such as concierge. 
3 Related Work 
As far as we know there is no work done to dis-
ambiguate partial cognates between two lan-
guages.  
   Ide (2000) has shown on a small scale that 
cross-lingual lexicalization can be used to define 
and structure sense distinctions. Tufis et al 
(2004) used cross-lingual lexicalization, word-
nets alignment for several languages, and a clus-
tering algorithm to perform WSD on a set of 
polysemous English words. They report an accu-
racy of 74%. 
   One of the most active researchers in identify-
ing cognates between pairs of languages is 
Kondrak (2001; 2004).  His work is more related 
to the phonetic aspect of cognate identification. 
He used in his work algorithms that combine dif-
ferent orthographic and phonetic measures, re-
current sound correspondences, and some 
semantic similarity based on glosses overlap. 
Guy (1994) identified letter correspondence be-
tween words and estimates the likelihood of re-
latedness. No semantic component is present in 
the system, the words are assumed to be already 
matched by their meanings. Hewson (1993), 
Lowe and Mazadon (1994) used systematic 
sound correspondences to determine proto-
projections for identifying cognate sets.  
   WSD is a task that has attracted researchers 
since 1950 and it is still a topic of high interest. 
Determining the sense of an ambiguous word, 
using bootstrapping and texts from a different 
language was done by Yarowsky (1995),  Hearst 
(1991), Diab (2002), and Li and Li (2004).   
   Yarowsky (1995) has used a few seeds and 
untagged sentences in a bootstrapping algorithm 
based on decision lists. He added two constrains 
? words tend to have one sense per discourse and 
one sense per collocation. He reported high accu-
racy scores for a set of 10 words. The monolin-
gual bootstrapping approach was also used by 
Hearst (1991), who used a small set of hand-
labeled data to bootstrap from a larger corpus for 
training a noun disambiguation system for Eng-
lish. Unlike Yarowsky (1995), we use automatic 
collection of seeds. Besides our monolingual 
bootstrapping technique, we also use bilingual 
bootstrapping. 
   Diab (2002) has shown that unsupervised WSD 
systems that use parallel corpora can achieve 
results that are close to the results of a supervised 
approach. She used parallel corpora in French, 
English, and Spanish, automatically-produced 
with MT tools to determine cross-language lexi-
calization sets of target words. The major goal of 
her work was to perform monolingual English 
WSD. Evaluation was performed on the nouns 
from the English all words data in Senseval2. 
Additional knowledge was added to the system 
442
from WordNet in order to improve the results. In 
our experiments we use the parallel data in a dif-
ferent way: we use words from parallel sentences 
as features for Machine Learning (ML). Li and 
Li (2004) have shown that word translation and 
bilingual bootstrapping is a good combination for 
disambiguation. They were using a set of 7 pairs 
of Chinese and English words. The two senses of 
the words were highly distinctive: e.g. bass as 
fish or music; palm as tree or hand. 
Our work described in this paper shows that 
monolingual and bilingual bootstrapping can be 
successfully used to disambiguate partial cog-
nates between two languages. Our approach dif-
fers from the ones we mentioned before not only 
from the point of human effort needed to anno-
tate data ? we require almost none, and from the 
way we use the parallel data to automatically 
collect training examples for machine learning, 
but also by the fact that we use only off-the-shelf 
tools and resources: free MT and ML tools, and 
parallel corpora. We show that a combination of 
these resources can be used with success in a task 
that would otherwise require a lot of time and 
human effort.  
4 Data for Partial Cognates 
We performed experiments with ten pairs of par-
tial cognates. We list them in Table 1. For a 
French partial cognate we list its English cognate 
and several false friends in English. Often the 
French partial cognate has two senses (one for 
cognate, one for false friend), but sometimes it 
has more than two senses: one for cognate and 
several for false friends (nonetheless, we treat 
them together). For example, the false friend 
words for note have one sense for grades and one 
for bills. 
The partial cognate (PC), the cognate (COG) 
and false-friend (FF) words were collected from 
a web resource1. The resource contained a list of 
400 false-friends with 64 partial cognates. All 
partial cognates are words frequently used in the 
language. We selected ten partial cognates pre-
sented in Table 1 according to the number of ex-
tracted sentences (a balance between the two 
meanings), to evaluate and experiment our pro-
posed methods. 
The human effort that we required for our 
methods was to add more false-friend English 
words, than the ones we found in the web re-
source. We wanted to be able to distinguish the 
                                                          
1 http://french.about.com/library/fauxamis/blfauxam_a.htm 
senses of cognate and false-friends for a wider 
variety of senses. This task was done using a bi-
lingual dictionary2.  
 
Table 1. The ten pairs of partial cognates. 
French par-
tial cognate 
English  
cognate 
English false friends 
blanc blank white, livid 
circulation circulation traffic 
client client customer, patron, patient, 
spectator, user, shopper 
corps corps body, corpse 
d?tail detail retail 
mode mode fashion, trend, style, 
vogue 
note note mark, grade, bill, check,  
account 
police police policy, insurance, font, 
face 
responsable responsi-
ble 
in charge, responsible 
party, official, representa-
tive, person in charge, 
executive, officer  
route route road, roadside 
 
4.1 Seed Set Collection 
Both the supervised and the semi-supervised 
method that we will describe in Section 5 are 
using a set of seeds. The seeds are parallel sen-
tences, French and English, which contain the 
partial cognate. For each partial-cognate word, a 
part of the set contains the cognate sense and 
another part the false-friend sense.  
As we mentioned in Section 3, the seed sen-
tences that we use are not hand-tagged with the 
sense (the cognate sense or the false-friend 
sense); they are automatically annotated by the 
way we collect them. To collect the set of seed 
sentences we use parallel corpora from Hansard3, 
and EuroParl4, and the, manually aligned BAF 
corpus.5  
The cognate sense sentences were created by 
extracting parallel sentences that had on the 
French side the French cognate and on the Eng-
lish side the English cognate. See the upper part 
of Table 2 for an example. 
     The same approach was used to extract sen-
tences with the false-friend sense of the partial 
cognate, only this time we used the false-friend 
English words. See lower the part of Table 2. 
                                                          
2 http://www.wordreference.com 
3 http://www.isi.edu/natural-language/download/hansard/   
   and  http://www.tsrali.com/ 
4 http://people.csail.mit.edu/koehn/publications/europarl/ 
5 http://rali.iro.umontreal.ca/Ressources/BAF/  
443
Table 2. Example sentences from parallel corpus. 
Fr 
(PC:COG) 
Je note, par exemple, que l'accus? a fait 
une autre d?claration tr?s incriminante ? 
Hall environ deux mois plus tard. 
En 
(COG) 
I note, for instance, that he made another 
highly incriminating statement to Hall 
two months later. 
Fr 
(PC:FF) 
S'il g?le les gens ne sont pas capables de 
r?gler leur note de chauffage 
En 
(FF) 
If there is a hard frost, people are unable 
to pay their bills. 
 
   To keep the methods simple and language-
independent, no lemmatization was used. We 
took only sentences that had the exact form of 
the French and English word as described in Ta-
ble 1. Some improvement might be achieved 
when using lemmatization. We wanted to see 
how well we can do by using sentences as they 
are extracted from the parallel corpus, with no 
additional pre-processing and without removing 
any noise that might be introduced during the 
collection process. 
From the extracted sentences, we used 2/3 of 
the sentences for training (seeds) and 1/3 for test-
ing when applying both the supervised and semi-
supervised approach. In Table 3 we present the 
number of seeds used for training and testing.  
We will show in Section 6, that even though 
we started with a small amount of seeds from a 
certain domain ? the nature of the parallel corpus 
that we had, an improvement can be obtained in  
discriminating the senses of partial cognates us-
ing free text from other domains.  
 
Table 3. Number of parallel sentences used as seeds. 
Partial 
Cognates 
Train 
CG 
Train 
FF 
Test 
CG 
Test 
FF 
Blanc 54 78 28 39 
Circulation 213 75 107 38 
Client 105 88 53 45 
Corps 88 82 44 42 
D?tail 120 80 60 41 
Mode 76 104 126 53 
Note 250 138 126 68 
Police 154 94 78 48 
Responsable 200 162 100 81 
Route 69 90 35 46 
AVERAGE 132.9 99.1 66.9 50.1 
 
5 Methods 
In this section we describe the supervised and the 
semi-supervised methods that we use in our ex-
periments. We will also describe the data sets 
that we used for the monolingual and bilingual 
bootstrapping technique.  
   For both methods we have the same goal: to 
determine which of the two senses (the cognate 
or the false-friend sense) of a partial-cognate 
word is present in a test sentence. The classes in 
which we classify a sentence that contains a par-
tial cognate are: COG (cognate) and FF (false-
friend). 
5.1 Supervised Method 
For both the supervised and semi-supervised 
method we used the bag-of-words (BOW) ap-
proach of modeling context, with binary values 
for the features. The features were words from 
the training corpus that appeared at least 3 times 
in the training sentences. We removed the stop-
words from the features. A list of stopwords for 
English and one for French was used. We ran 
experiments when we kept the stopwords as fea-
tures but the results did not improve.  
Since we wanted to learn the contexts in which 
a partial cognate has a cognate sense and the con-
texts in which it has a false-friend sense, the cog-
nate and false friend words were not taken into 
account as features. Leaving them in would mean 
to indicate the classes, when applying the 
methods for the English sentences since all the 
sentences with the cognate sense contain the cog-
nate word and all the false-friend sentences do 
not contain it. For the French side all collected 
sentences contain the partial cognate word, the 
same for both senses.  
As a baseline for the experiments that we pre-
sent we used the ZeroR classifier from WEKA6, 
which predicts the class that is the most frequent 
in the training corpus. The classifiers for which 
we report results are: Na?ve Bayes with a kernel 
estimator, Decision Trees - J48, and a Support 
Vector Machine implementation - SMO. All the 
classifiers can be found in the WEKA package. 
We used these classifiers because we wanted to 
have a probabilistic, a decision-based and a func-
tional classifier. The decision tree classifier al-
lows us to see which features are most 
discriminative. 
Experiments were performed with other classi-
fiers and with different levels of tuning, on a 10-
fold cross validation approach as well; the classi-
fiers we mentioned above were consistently the 
ones that obtained the best accuracy results.   
The supervised method used in our experi-
ments consists in training the classifiers on the 
                                                          
6 http://www.cs.waikato.ac.nz/ml/weka/ 
444
automatically-collected training seed sentences, 
for each partial cognate, and then test their per-
formance on the testing set. Results for this 
method are presented later, in Table 5. 
5.2 Semi-Supervised Method 
For the semi-supervised method we add unla-
belled examples from monolingual corpora: the 
French newspaper LeMonde7 1994, 1995 (LM), 
and the BNC8 corpus, different domain corpora 
than the seeds. The procedure of adding and us-
ing this unlabeled data is described in the Mono-
lingual Bootstrapping (MB) and Bilingual 
Bootstrapping (BB) sections.  
5.2.1  Monolingual Bootstrapping 
The monolingual bootstrapping algorithm that 
we used for experiments on French sentences 
(MB-F) and on English sentences (MB-E) is:  
 
For each pair of partial cognates (PC)  
1. Train a classifier on the training seeds ? us-
ing the BOW approach and a NB-K classifier 
with attribute selection on the features. 
2. Apply the classifier on unlabeled data ? 
sentences that contain the PC word, extracted 
from LeMonde (MB-F) or from BNC (MB-E)  
3. Take the first k newly classified sentences, 
both from the COG and FF class and add 
them to the  training seeds  (the most confident 
ones ? the  prediction  accuracy greater or 
equal than a threshold =0.85) 
4. Rerun the experiments training on the new 
training set 
5. Repeat steps 2 and 3 for t times  
   endFor 
 
For the first step of the algorithm we used NB-K 
classifier because it was the classifier that consis-
tently performed better. We chose to perform 
attribute selection on the features after we tried 
the method without attribute selection. We ob-
tained better results when using attribute selec-
tion. This sub-step was performed with the 
WEKA tool, the Chi-Square attribute selection 
was chosen. 
In the second step of the MB algorithm the 
classifier that was trained on the training seeds 
was then used to classify the unlabeled data that 
was collected from the two additional resources. 
For the MB algorithm on the French side we 
trained the classifier on the French side of the 
                                                          
7 http://www.lemonde.fr/ 
8 http://www.natcorp.ox.ac.uk/ 
training seeds and then we applied the classifier 
to classify the sentences that were extracted from 
LeMonde and contained the partial cognate. The 
same approach was used for the MB on the Eng-
lish side only this time we were using the English 
side of the training seeds for training the classi-
fier and the BNC corpus to extract new exam-
ples. In fact, the MB-E step is needed only for 
the BB method. 
Only the sentences that were classified with a 
probability greater than 0.85 were selected for 
later use in the bootstrapping algorithm.  
   The number of sentences that were chosen 
from the new corpora and used in the first step of 
the MB and BB are presented in Table 4. 
 
Table 4. Number of sentences selected from the 
LeMonde and BNC corpus. 
PC LM 
COG 
LM 
FF 
BNC 
COG 
BNC 
FF 
Blanc 45 250 0 241 
Circulation 250 250 70 180 
Client 250 250 77 250 
Corps 250 250 131 188 
D?tail 250 163 158 136 
Mode 151 250 176 262 
Note 250 250 178 281 
Police 250 250 186 200 
Responsable 250 250 177 225 
Route 250 250 217 118 
 
For the partial-cognate Blanc with the cognate 
sense, the number of sentences that had a prob-
ability distribution greater or equal with the 
threshold was low. For the rest of partial cog-
nates the number of selected sentences was lim-
ited by the value of parameter k in the algorithm.  
5.2.2   Bilingual Bootstrapping 
The algorithm for bilingual bootstrapping that we 
propose and tried in our experiments is: 
 
1. Translate the English sentences that were col-
lected in the MB-E step into French using an 
online MT9 tool and add them to the French seed 
training data.  
2.  Repeat the MB-F and MB-E steps for T times. 
 
For the both monolingual and bilingual boot-
strapping techniques the value of the parameters 
t and T is 1 in our experiments. 
                                                          
9 http://www.freetranslation.com/free/web.asp 
445
6 Evaluation and Results 
In this section we present the results that we 
obtained with the supervised and semi-
supervised methods that we applied to disam-
biguate partial cognates. 
Due to space issue we show results only for 
testing on the testing sets and not for the 10-fold 
cross validation experiments on the training data. 
For the same reason, we present the results that 
we obtained only with the French side of the par-
allel corpus, even though we trained classifiers 
on the English sentences as well. The results for 
the 10-fold cross validation and for the English 
sentences are not much different than the ones 
from Table 5 that describe the supervised method 
results on French sentences. 
 
   Table 5. Results for the Supervised Method.    
PC ZeroR NB-K Trees SMO 
Blanc 58% 95.52% 98.5% 98.5% 
Circulation 74% 91.03% 80% 89.65% 
Client 54.08% 67.34% 66.32% 61.22% 
Corps 51.16% 62% 61.62% 69.76% 
D?tail 59.4% 85.14% 85.14% 87.12% 
Mode 58.24% 89.01% 89.01% 90% 
Note 64.94% 89.17% 77.83% 85.05% 
Police 61.41% 79.52% 93.7% 94.48% 
Responsable 55.24% 85.08% 70.71% 75.69% 
Route 56.79% 54.32% 56.79% 56.79% 
AVERAGE 59.33% 80.17% 77.96% 80.59% 
 
Table 6 and Table 7 present results for the MB 
and BB. More experiments that combined MB 
and BB techniques were also performed. The 
results are presented in Table 9. 
   Our goal is to disambiguate partial cognates 
in general, not only in the particular domain of 
Hansard and EuroParl. For this reason we used 
another set of automatically determined sen-
tences from a multi-domain parallel corpus. 
The set of new sentences (multi-domain) was 
extracted in the same manner as the seeds from 
Hansard and EuroParl. The new parallel corpus 
is a small one, approximately 1.5 million words, 
but contains texts from different domains: maga-
zine articles, modern fiction, texts from interna-
tional organizations and academic textbooks. We 
are using this set of sentences in our experiments 
to show that our methods perform well on multi-
domain corpora and also because our aim is to be 
able to disambiguate PC in different domains. 
From this parallel corpus we were able to extract 
the number of sentences shown in Table 8. 
With this new set of sentences we performed 
different experiments both for MB and BB. All 
results are described in Table 9. Due to space 
issue we report the results only on the average 
that we obtained for all the 10 pairs of partial 
cognates.  
The symbols that we use in Table 9 represent:  
S ? the seed training corpus, TS ? the seed test 
set,  BNC and LM ? sentences extracted from 
LeMonde and BNC (Table 4), and NC ? the sen-
tences that were extracted from the multi-domain 
new corpus. When we use the + symbol we put 
together all the sentences extracted from the re-
spective corpora. 
 
Table 6. Monolingual Bootstrapping on the French side. 
PC ZeroR NB-K Dec.Tree SMO 
Blanc 58.20% 97.01% 97.01% 98.5% 
Circulation 73.79% 90.34% 70.34% 84.13% 
Client 54.08% 71.42% 54.08% 64.28% 
Corps 51.16% 78% 56.97% 69.76% 
D?tail 59.4% 88.11% 85.14% 82.17% 
Mode 58.24% 89.01% 90.10% 85% 
Note 64.94% 85.05% 71.64% 80.41% 
Police 61.41% 71.65% 92.91% 71.65% 
Responsable 55.24% 87.29% 77.34% 81.76% 
Route 56.79% 51.85% 56.79% 56.79% 
AVERAGE 59.33% 80.96% 75.23% 77.41% 
 
Table 7. Bilingual Bootstrapping. 
PC ZeroR NB-K Dec.Tree SMO 
Blanc 58.2% 95.52% 97.01% 98.50% 
Circulation 73.79% 92.41% 63.44% 87.58% 
Client 45.91% 70.4% 45.91% 63.26% 
Corps 48.83% 83% 67.44% 82.55% 
D?tail 59% 91.08% 85.14% 86.13% 
Mode 58.24% 87.91% 90.1% 87% 
Note 64.94% 85.56% 77.31% 79.38% 
Police 61.41% 80.31% 96.06% 96.06% 
Responsable 44.75% 87.84% 74.03% 79.55% 
Route 43.2% 60.49% 45.67% 64.19% 
AVERAGE 55.87% 83.41% 74.21% 82.4% 
 
 
446
Table 8. New Corpus (NC) sentences. 
PC COG FF 
Blanc 18 222 
Circulation 26 10 
Client 70 44 
Corps 4 288 
D?tail 50 0 
Mode 166 12 
Note 214 20 
Police 216 6 
Responsable 104 66 
Route 6 100 
 
6.1  Discussion of the Results
The results of the experiments and the methods 
that we propose show that we can use with suc-
cess unlabeled data to learn from, and that the 
noise that is introduced due to the seed set collec-
tion is tolerable by the ML techniques that we 
use.  
Some results of the experiments we present in 
Table 9 are not as good as others. What is impor-
tant to notice is that every time we used MB or 
BB or both, there was an improvement. For some 
experiments MB did better, for others BB was 
the method that improved the performance; 
nonetheless for some combinations MB together 
with BB was the method that worked best.  
In Tables 5 and 7 we show that BB improved 
the results on the NB-K classifier with 3.24%, 
compared with the supervised method (no boot-
strapping), when we tested only on the test set 
(TS), the one that represents 1/3 of the initially-
collected parallel sentences. This improvement is 
not statistically significant, according to a t-test.  
In Table 9 we show that our proposed methods 
bring improvements for different combinations 
of training and testing sets. Table 9, lines 1 and 2 
show that BB with NB-K brought an improve-
ment of 1.95% from no bootstrapping, when we 
tested on the multi-domain corpus NC. For the 
same setting, there was an improvement of 
1.55% when we tested on TS (Table 9, lines 6 
and 8). When we tested on the combination 
TS+NC, again BB brought an improvement of 
2.63% from no bootstrapping (Table 9, lines 10 
and 12). The difference between MB and BB 
with this setting is 6.86% (Table 9, lines 11 and 
12). According to a t-test the 1.95% and 6.86% 
improvements are statistically significant. 
 Table 9. Results for different experiments with 
monolingual and bilingual bootstrapping (MB and 
BB).  
Train Test ZeroR NB-K Trees SMO 
S (no 
bootstrapping) 
NC 67% 71.97% 73.75% 76.75%
S+BNC 
(BB) 
NC 64% 73.92% 60.49% 74.80%
S+LM 
(MB) 
NC 67.85% 67.03% 64.65% 65.57%
S +LM+BNC 
(MB+BB) 
NC 64.19% 70.57% 57.03% 66.84%
S+LM+BNC 
(MB+BB) 
TS 55.87% 81.98% 74.37% 78.76%
S+NC 
(no bootstr.) 
TS 57.44% 82.03% 76.91% 80.71%
S+NC+LM 
(MB) 
TS 57.44% 82.02% 73.78% 77.03%
S+NC+BNC 
(BB) 
TS 56.63% 83.58% 68.36% 82.34%
S+NC+LM+ 
BNC(MB+BB)
TS 58% 83.10% 75.61% 79.05%
S (no bootstrap-
ping) 
TS+NC 62.70% 77.20% 77.23% 79.26%
S+LM 
(MB) 
TS+NC 62.70% 72.97% 70.33% 71.97%
S+BNC 
(BB) 
TS+NC 61.27% 79.83% 67.06% 78.80%
S+LM+BNC 
(MB+BB) 
TS+NC 61.27% 77.28% 65.75% 73.87%
 
    The number of features that were extracted 
from the seeds was more than double at each MB 
and BB experiment, showing that even though 
we started with seeds from a language restricted 
domain, the method is able to capture knowledge 
form different domains as well. Besides the 
change in the number of features, the domain of 
the features has also changed form the parlia-
mentary one to others, more general, showing 
that the method will be able to disambiguate sen-
tences where the partial cognates cover different 
types of context.  
Unlike previous work that has done with 
monolingual or bilingual bootstrapping, we tried 
to disambiguate not only words that have senses 
that are very different e.g. plant ? with a sense of 
biological plant or with the sense of factory. In 
our set of partial cognates the French word route 
is a difficult word to disambiguate even for hu-
mans: it has a cognate sense when it refers to a 
maritime or trade route and a false-friend sense 
when it is used as road. The same observation 
applies to client (the cognate sense is client, and 
the false friend sense is customer, patron, or pa-
tient) and to circulation (cognate in air or blood 
circulation, false friend in street traffic).  
447
7 Conclusion and Future Work 
We showed that with simple methods and using 
available tools we can achieve good results in the 
task of partial cognate disambiguation. 
   The accuracy might be increased by using de-
pendencies relations, lemmatization, part-of-
speech tagging ? extract sentences where the par-
tial cognate has the same POS, and other types of 
data representation combined with different se-
mantic tools (e.g. decision lists, rule based sys-
tems).  
In our experiments we use a machine language 
representation ? binary feature values, and we 
show that nonetheless machines are capable of 
learning from new information, using an iterative 
approach, similar to the learning process of hu-
mans. New information was collected and ex-
tracted by classifiers when additional corpora 
were used for training. 
   In addition to the applications that we men-
tioned in Section 1, partial cognates can also be 
useful in Computer-Assisted Language Learning 
(CALL) tools. Search engines for E-Learning can 
find useful a partial cognate annotator. A teacher 
that prepares a test to be integrated into a CALL 
tool can save time by using our methods to 
automatically disambiguate partial cognates, 
even though the automatic classifications need to 
be checked by the teacher.  
In future work we plan to try different repre-
sentations of the data, to use knowledge of the 
relations that exists between the partial cognate 
and the context words, and to run experiments 
when we iterate the MB and BB steps more than 
once. 
References  
Susane Carroll 1992. On Cognates. Second Language 
Research, 8(2):93-119 
Mona Diab and Philip Resnik. 2002. An unsupervised 
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Meeting of the As-
sociation for Computational Linguistics (ACL 
2002), Philadelphia, pp. 255-262. 
S. M. Gass. 1987. The use and acquisition of the sec-
ond language lexicon (Special issue). Studies in 
Second Language Acquisition, 9 (2).  
Jacques B. M. Guy. 1994. An algorithm for identify-
ing cognates in bilingual word lists and its applica-
bility to machine translation. Journal of 
Quantitative Linguistics, 1(1):35-42. 
Marti Hearst 1991. Noun homograph disambiguation 
using local context in large text corpora. 7th An-
nual Conference of the University of Waterloo 
Center for the new OED and Text Research, Ox-
ford. 
W.J.B Van Heuven, A. Dijkstra, and J. Grainger. 
1998.  Orthographic neighborhood effects in bilin-
gual word recognition. Journal of Memory and 
Language 39: 458-483. 
John Hewson 1993. A Computer-Generated Diction-
ary of Proto-Algonquian. Ottawa: Canadian Mu-
seum of Civilization. 
Nancy Ide. 2000 Cross-lingual sense determination: 
Can it work? Computers and the Humanities, 34:1-
2, Special Issue on the Proceedings of the SIGLEX 
SENSEVAL Workshop, pp.223-234. 
Grzegorz Kondrak. 2004. Combining Evidence in 
Cognate Identification. Proceedings of Canadian 
AI 2004: 17th Conference of the Canadian Society 
for Computational Studies of Intelligence, pp.44-
59.  
Grzegorz Kondrak. 2001. Identifying Cognates by 
Phonetic and Semantic Similarity. Proceedings of 
NAACL 2001: 2nd Meeting of the North American 
Chapter of the Association for Computational Lin-
guistics, pp.103-110. 
Raymond LeBlanc and Hubert S?guin. 1996. Les 
cong?n?res homographes et parographes anglais-
fran?ais. Twenty-Five Years of Second Language 
Teaching at the University of Ottawa, pp.69-91.  
Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrap. Computational 
Linguistics, 30(1):1-22. 
John B. Lowe and Martine Mauzaudon. 1994. The 
reconstruction engine: a computer implementation 
of the comparative method. Computational Lin-
guistics, 20:381-417. 
Hakan Ringbom. 1987. The Role of the First Lan-
guage in Foreign Language Learning. Multilingual 
Matters Ltd., Clevedon, England. 
Dan Tufis, Ion Radu, Nancy Ide 2004. Fine-Grained 
Word Sense Disambiguation Based on Parallel 
Corpora, Word Alignment, Word Clustering and 
Aligned WordNets. Proceedings of the 20th Inter-
national Conference on Computational Linguistics, 
COLING 2004, Geneva, pp. 1312-1318. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of the 33th Annual Meeting of the As-
sociation for Computational Linguistics, Cam-
bridge, MA, pp 189-196. 
448
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 110?111,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Textual Information for Predicting Functional Properties of the Genes 
 
Oana Frunza and Diana Inkpen 
School of Information Technology and Engineering 
University of Ottawa Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
1 Overview 
This paper is focused on determining which pro-
teins affect the activity of Aryl Hydrocarbon Re-
ceptor (AHR) system when learning a model that 
can accurately predict its activity when single 
genes are knocked out. Experiments with results 
are presented when models are trained on a single 
source of information: abstracts from Medline 
(http://medline.cos.com/) that talk about the genes in-
volved in the experiments. The results suggest that 
AdaBoost classifier with a binary bag-of-words 
representation obtains significantly better results. 
2 Task Description and Data Sets 
The task that we address is a biology-specific task 
considered a competition track for KDDCup2002 
(http://www.biostat.wisc.edu/~craven/kddcup/winners.html).  
   The organizers of the KDD Cup competition 
provided data obtained from experiments per-
formed on a set of yeast strains in which each 
strain contains a single gene that is knocked out (a 
gene sequence in which a single gene is inopera-
tive). Each experiment had associated a discretized 
value of the activity of the AHR system when a 
single gene was knocked out. 3 possible classes 
describe the systems? response. The "nc" label in-
dicates that the activity of the hidden system was 
not significantly different than the baseline (the 
wild-type yeast); the "control" label indicates that 
the activity was significantly different than the 
baseline for the given instance, and that the activity 
of another hidden system (the control) was also 
significantly changed compared to its baseline; the 
"change" label shows that the activity of the hid-
den system was significantly changed, but the ac-
tivity of the control system was not significantly 
changed. 
   The organizers of the KDD Cup evaluate the task 
as a two-class problem with focus on the positive 
class. The first definition is called the ?narrow? 
definition of the positive class and it is specific to 
the knocked-out genes that had an AHR-specific 
effect. In this case the positive class is defined by 
the experiments in which the label of the system is 
?change? and the negative examples are the ex-
periments that consist of those genes with either 
the "nc" or the "control" label. The second defini-
tion consists of those genes labeled with either the 
"change" or the "control" label. The negative class 
consists of those genes labeled with the "nc" label. 
The second partitioning corresponds to the 
?broad? characterization of the positive class 
genes that affect the hidden system.  
   The area under the Receiver Operating Charac-
teristic (ROC) - AUC curve is chosen as an evalua-
tion measure. The global score for the task will be 
the summed AUC values for both the ?narrow? and 
the ?broad? partition of the data. 
   The sources of information provided by the or-
ganizers of the task contain: hierarchical informa-
tion about the function and localization of the 
genes; relational information describing the pro-
tein-protein interactions; and textual information in 
abstracts from Medline that talk about the genes. 
Some characteristics of the data need to be taken 
into consideration in order to make suitable deci-
sions for choosing the trainable system/classifier, 
the representation of the data, etc. Missing infor-
mation is a characteristic of the data set. Not all 
genes had the location and function annotation, the 
protein-protein interaction information, or abstracts 
associated with the gene name. Besides the missing 
information, the high class imbalance is another 
fact that needs to be taken into account.  
   From the data that was released for the KDD 
competition we run experiments only with the 
genes that had associated abstracts. Table 1 pre-
sents a summary of the data sets used in our ex-
periments after considering only the genes that had 
abstracts associated with them. The majority of the 
genes had one abstract, while others had as many 
as 22 abstracts.   
110
Table 1. Summary of the data for our experiments with 
the two definitions of the positive class. In brackets are 
the original sizes of the data sets. 
Narrow Broad Data 
set Pos Neg Pos Neg 
Training 24 
(37) 
1,435 
(2,980) 
51 
(83) 
1,408 
(2,934) 
Test 11 
(19) 
715 
(1,469) 
30 
(43) 
696 
(1,445) 
3 Related Work  
Previous research on the task was done by the 
teams that participated in the KDD Cup 2002. The 
textual information available in the task was con-
sidered as an auxiliary source of information and 
not the primary one, as in this article.  
   The winners of the task, Kowalczyk and Raskutti 
(2002) used the textual information as additional 
features to the ones extracted from other available 
information for the genes. They used a ?bag-of-
words? representation, removed stop words and 
words with low frequency. They used Support 
Vector machine (SVM) as a classifier.  
   Krogel et. al. (2002) used the textual information  
with an information extraction system in order to 
extract missing information (function, localization, 
protein class) for the genes in the released data set.  
   Vogel and Axelrod (2002) used the Medline ab-
stracts to extract predictive keywords, and added 
them to their global system. 
   Our study investigates and suggests a textual rep-
resentation and a trainable model suitable for this 
task and similar tasks in the biomedical domain. 
4 Method  
The method that we propose to solve the biology 
task is using Machine Learning (ML) classifiers 
suitable for a text classification task and various 
feature representations that are known to work well 
for data sets with high class imbalance. The task 
becomes a two-class classification: ?Positive? ver-
sus ?Negative?, with a ?narrow? and ?broad? 
definition for the positive class. As classification 
algorithms we used: Complement Naive Bayes 
(CNB), AdaBoost, and SVM all from the Weka 
toolkit (http://www.cs.waikato.ac.nz/ml/weka/). Similar to 
the evaluation done for the KDD Cup, we consider 
the sum of the 2 AUC measures for the definitions 
of the positive class as an evaluation score. The 
random classifier with an AUC measure of 0.5 is 
considered as a baseline.  
As a representation technique we used binary 
and frequency values for features that are: words 
extracted from the abstracts (bag-of-words (BOW) 
representation), UMLS concepts and UMLS 
phrases identified using the MetaMap system 
(http://mmtx.nlm.nih.gov/), and UMLS relations ex-
tracted from the UMLS metathesaurus. We also 
ran experiments with feature selection techniques.  
   Table 2 presents our best results using AdaBoost 
classifier for BOW, UMLS concepts, and UMLS 
relations representation techniques. ?B? stands for 
binary and ?Freq? stands for frequency counts. 
 
Table 2. Sum of the AUC results for the two classes 
without feature selection.  
Represen- 
tation 
AdaBoost 
(AUC) 
Narrow 
AdaBoost 
(AUC) 
Broad 
Sumed 
AUC 
BOW_B 0.613 0.598 1.211 
BOW_Freq 0.592 0.557 1.149 
UMLS_B 0.571 0.607 1.178 
UMLS_Freq 0.5 0.606 1.106 
UMLS_Rel_B 0.505 0.547 1.052 
UMLS_Rel_Freq 0.5 0.5 1 
5 Discussion and Conclusion 
Looking at the obtained results, a general conclu-
sion can be made: textual information is useful for 
biology-specific tasks. Not only that it can improve 
the results but can also be considered a stand-alone 
source of knowledge in this domain. Without any 
additional knowledge, our result of 1.21 AUC sum 
is comparable with the sum of 1.23 AUC obtained 
by the winners of the KDD competition.  
References  
Adam Kowalczyk and Bhavani Raskutti, 2002. One 
Class SVM for Yeast Regulation Prediction, ACM 
SIGKDD Explorations Newsletter, Volume 4, Issue 
2, pp. 99-100. 
Mark A Krogel, Marcus Denecke, Marco Landwehr, 
and Tobias Scheffer. 2002. Combining data and text 
mining techniques for yeast gene regulation predic-
tion: a case study, ACM SIGKDD Explorations 
Newsletter, Volume 4, Issue 2, pp. 104-105. 
David S. Vogel and Randy C. Axelrod. 2002. Predicting 
the Effects of Gene Deletion, ACM SIGKDD Explo-
rations Newsletter, Volume 4, Issue 2, pp. 101-103.    
111
Coling 2010: Poster Volume, pages 303?311,
Beijing, August 2010
Building Systematic Reviews Using Automatic Text Classification 
Techniques  
 
Oana Frunza, Diana Inkpen, and Stan Matwin 
School of Information Technology and Engineering 
University of Ottawa  
{ofrunza,diana,stan}@site.uottawa.ca 
 
 
  Abstract 
The amount of information in medical 
publications continues to increase at a 
tremendous rate. Systematic reviews help 
to process this growing body of informa-
tion. They are fundamental tools for evi-
dence-based medicine. In this paper, we 
show that automatic text classification can 
be useful in building systematic reviews 
for medical topics to speed up the review-
ing process. We propose a per-question 
classification method that uses an ensem-
ble of classifiers that exploit the particular 
protocol of a systematic review. We also 
show that when integrating the classifier 
in the human workflow of building a re-
view the per-question method is superior 
to the global method. We test several 
evaluation measures on a real dataset. 
1 Introduction 
Systematic reviews are the result of a tedious 
process which involves human reviewers to ma-
nually screen references of papers to determine 
their relevance to the review. This process often 
entails reading thousands or even tens of thou-
sands of abstracts from prospective articles. As 
the body of available articles continues to grow, 
this process is becoming increasingly difficult.  
 Common systematic review practices stipu-
late that two reviewers are used at the screening 
phases of a systematic review to review each ab-
stract of the documents retrieved after a simple 
query-based search. After a final decision is 
made for each abstract (the two reviewers decide 
if the abstract is relevant or not to the topic of 
review), in the next phase further analysis (more 
strict screening steps) on the entire article is 
done. A systematic review has to be complete, 
articles that are published on a certain topic and 
are clinically relevant need to be part of the re-
view. This requires near-perfect recall since the 
accidental exclusion of a potentially relevant ab-
stract can have a significantly negative impact on 
the validity of the overall systematic review (Co-
hen et al, 2006). Our goal in this paper is to pro-
pose an automatic system that can help human 
judges in the process of triaging articles by look-
ing only at abstracts and not the entire docu-
ments. This decision step is known as the initial 
screening phase in the protocol of building sys-
tematic reviews, only the abstracts are used as 
source of information.  
One reviewer will still read the entire collec-
tion of abstracts while the other will benefit from 
the help of the system; this reviewer will have to 
label only the articles that will be used to train 
the classifier (ideally a small proportion for 
workload reduction), the rest of the articles will 
be labeled by the classifier.  
 In the systematic review preparation, if at 
least one reviewer agrees to include an abstract, 
the abstract will have the labeled included and it 
will pass to the next screening phase; otherwise, 
it will be discarded. Therefore, the benefit of 
doubt plays an important role in the decision 
process. When we replace one reviewer with the 
automatic classifier, because we keep one human 
judge in the process, the confidence and reliabil-
ity of the review is still higher while the overall 
workload is reduced. The reduction is from the 
time required for two passes through the collec-
tion (for the two humans) to only one pass and 
the smaller part labeled by the reviewer which is 
assisted by the classifier.  Figure 1 presents on 
overview of our proposed workflow.   
303
  
Figure 1. Embedding automatic text classification in 
the process of building a systematic review. 
 
The task that needs to be solved in order to help 
the systematic review process is a text classifica-
tion task intended to classify an abstract as rele-
vant or not relevant to the topic of review. 
 The hypothesis that guides our research is 
that it is possible to save time for the human re-
viewers and obtain good performance levels, 
similar to the ones obtained by humans. In this 
current study we show that we can achieve this 
by building a classification model that is based 
on the natural human workflow used for building 
systematic reviews. We show, on a real data set, 
that a human-machine system obtains the best 
results when an ensemble of classifiers is used as 
the classification model.  
2 Related Work  
The traditional way to collect and triage the ab-
stracts from a systematic review consists in using 
simple query search techniques based on MeSH1 
or keywords terms. The queries are usual Boo-
lean-based and are optimized either for precision 
or for recall. The studies done by Haynes et al 
(1994) show that it is difficult to obtain high per-
formance for both measures.  
 The research done by Aphinyanaphongs and 
Aliferis (2005) is probably the first application of 
automatic text classification to the task of creat-
                                                 
1
 http://www.nlm.nih.gov/mesh/ 
ing systematic reviews. In that paper the authors 
experimented with a variety of text classification 
techniques using the data derived from the ACP 
Journal Club as their corpus. They found that 
support vector machine (SVM) was the best clas-
sifier according to a variety of measures. Further 
work for systematic reviews was done by Cohen 
et al (2006). Their work is mostly focused on the 
elimination of non relevant documents. As their 
main goal is to save work for the reviewers in-
volved in systematic review preparation, they 
define a measure, called work saved over sam-
pling (WSS) that captures the amount of work 
that the reviewers will save with respect to a 
baseline of just sampling for a given value of 
recall. The idea is that a classifier returns, with 
high recall, a set of abstracts, and only those ab-
stracts need to be read to weed out the non-
relevant ones. The savings are measured with 
respect to the number of abstracts that would 
have to be read if a random baseline classifier 
was used. Such baseline corresponds to uni-
formly sampling a given percentage of abstracts 
(equal to the desired recall) from the entire set. In 
Cohen et al (2006), the WSS measure is applied 
to report the reduction in reviewer's work when 
retrieving 95% of the relevant documents; the 
precision was very low.  
 We focus on developing a classifier for sys-
tematic review preparation, relying on character-
istics of the data that were not included in the 
Cohen et al?s (2006), because the questions 
asked in the preparation of the reviews are not 
available, Therefore we cannot perform a direct 
comparison of results here. Also, the data sets 
that they used in their experiments are signifi-
cantly smaller than the one that we used. 
3 The Data Set 
A set of 47,274 abstracts with titles were col-
lected from MEDLINE2 as part of a systematic 
review done by the McMaster University?s Evi-
dence-Based Practice Center using TrialStat 
Corporation?s Systematic Review System 3 , a 
web-based software platform used to conduct 
systematic reviews.  
The initial set of abstracts was collected using 
a set of Boolean search queries that were run for 
                                                 
2
 http://medline.cos.com 
3
 http://www.trialstat.com/ 
304
the specific topic of the systematic review: ?the 
dissemination strategy of health care services 
for elderly people of age 65 and over?.  
In the protocol applied, two reviewers work in 
parallel. They read the entire collection of 47,274 
abstracts and answer a set of questions to deter-
mine if an abstract is relevant or not to the topic 
of review. Examples of questions present in the 
protocol: Is this article about a dissemination 
strategy or a behavioral intervention?; Is this a 
primary study?; Is this a review?; etc. An ab-
stract is not considered to pass to the next screen-
ing phase, when the entire article is available, if 
the two reviewers respond negative to the same 
question for a certain abstract. All other cases of 
possible responses suggest that the abstract will 
be part of the next screening phase. In this paper 
we focus on the initial screening phase, the only 
source of information is the abstract and the title 
of the article, with the main goal to achieve an 
acceptable level of recall not to mistakenly ex-
clude relevant abstracts.  
 From the entire collection of labeled ab-
stracts only 7,173 are relevant. Usually in the 
process of building systematic reviews the num-
ber of non-relevant documents is much higher 
than the number of relevant ones. The initial re-
trieval query is purposefully very broad, so as not 
to miss any relevant papers.  
4 Methods 
The machine learning techniques that could be 
used in the process of automating the creation of 
systematic reviews need to take into account 
some issues that can arise when dealing with 
such tasks. Imbalanced data sets are usually 
what we deal with when building reviews, the 
proportion of relevant articles that end up being 
present in the review is significantly lower com-
pared with the original data set. The benefit of 
doubt will affect the quality of the data used to 
train the classifier, since a certain amount of 
noise is introduced: abstracts that are in fact non-
relevant can be labeled as being relevant in the 
first screening process. The relatively high num-
ber of abstracts involved in the process will make 
the classification algorithms deal with a high 
number of features and the representation tech-
nique should try to capture aspects pertaining of 
the medical domain.    
 
4.1 Representation Techniques 
In our current research, we use three representa-
tion techniques: bag-of-words (BOW), concepts 
from the Unified Medical Language System 
(UMLS), and a combination of both.  
The bag-of-words representation is com-
monly used for text classification and we have 
chosen to use binary feature values. Binary fea-
ture values were shown to out-perform weighted 
values for text classification tasks in the medical 
domain as shown by Cohen et al (2006) and bi-
nary values tend to be more stable in results than 
frequency values for a task similar to ours, as 
shown by Ma (2007). 
We considered feature words delimitated by 
space and simple punctuation marks that ap-
peared at least three times in the training data, 
were not part of a stop words list4, and had a 
length greater than three characters. 30,000 word 
features were extracted. No stemming was used. 
UMLS concepts which are part of the U.S. 
National Library of Medicine 5  (NLM) knowl-
edge repository are identified and extracted form 
the collection of abstracts using the MetaMap6 
system. This conceptual representation helped us 
overcome some of the shortcomings of BOW 
representation, and allowed us to use multi-word 
features, medical knowledge, and higher-level 
meanings of words in context. As Cohen (2008) 
shows, multi-word and medical concept repre-
sentations are suitable to use.  
4.2 Classification Algorithms  
As a classification algorithm we have chosen to 
use the complement naive Bayes (CNB) (Frank 
and Bouckaert, 2006) classifier from the Weka7 
tool. The reason for this choice is that the CNB 
classifier implements state-of-the-art modifica-
tions of the standard multinomial na?ve Bayes 
(MNB) classifier for a classification task with 
highly skewed class distribution (Drummond and 
Holte, 2003). As the systematic reviews data 
usually contain a large majority of not relevant 
abstracts, resulting in a skewness reaching even 
below 1%, it is important to use appropriate clas-
sifiers.  Other classifiers, such as decision tress, 
                                                 
4
 http://www.site.uottawa.ca/~diana/csi5180/StopWords 
5
 http://www.nlm.nih.gov/pubs/factsheets/umls.html 
6
 http://mmtx.nlm.nih.gov/ 
7
 www.cs.waikato.ac.nz/machine learning/weka/ 
305
support vector machine, instance-based learning, 
and boosting, were used but the results obtained 
with CNB were always better. 
4.3 Global Text Classification Method 
The first method that we propose in order to 
solve the text classification task that is intended 
to help a systematic review process is a straight-
forward machine learning approach. We trained a 
classifier, CNB, on a collection of abstracts and 
then evaluated the classifier?s performance on a 
separate test data set. The power of this classifi-
cation technique stands in the ability to use a 
suitable classification algorithm and a good rep-
resentation for the text classification task; Cohen 
et al (2006) also used this approach. We ran-
domly split the data set described in Section 3, 
into a training set and a test set. The two possible 
classes are Included (relevant) or Excluded 
(non relevant). We decided to work with a train-
ing set smaller than the test set because ideally 
good results need to be obtained without using 
too much training data. We have to take into 
consideration that training a classifier for a par-
ticular topic, human effort is required for annota-
tion.  
  Table 1 presents a summary of the data 
along with the class distribution in the training 
and test data sets. We randomly sampled the data 
to build the training and test data sets, and the 
original distribution of 1:5.6 between the two 
classes holds in both sets.  
 
Data 
set 
No. of 
abstracts 
Class distribution 
Included : Excluded (ratio) 
Training 20,000 3,056 : 16,944 (1:5.6) 
Testing 27,274 4,117 : 23,157 (1:5.6) 
Table 1. Training and test data sets. 
 
4.3.1 Feature Selection 
 
Using the global method, we performed experi-
ments with several feature selection algorithms. 
We used only the BOW representation. 
Chi2 is a measure that evaluates the worth of an 
attribute by computing the value of the chi-
squared statistic with respect to the class. We 
selected the top k1 CHI2 features that are exclu-
sively included (appeared only in the training 
abstracts that are classified as Included) and the 
top k2 CHI2 features that are exclusively excluded 
(appeared only in the training abstracts that are 
classified as Excluded) and used them as a rep-
resentation for our data set. We varied the k1 pa-
rameter from 10 to 150 and k2 from 5 to 150 We 
used a minimum of 20 features and a maximum 
of 300. 
InfoGain evaluates the worth of an attribute 
by measuring the information gain with respect 
to the class. We run experiments when we varied 
the number of selected features from 50 to 500. 
We used a number of 50, 100, 150, 250, 300 and 
500 top features.  
Bi-Normal Separation (BNS) is a feature se-
lection technique that measures the separation 
between the threshold occurrences of a feature in 
one of the two classes. The latter measure is de-
scribed in detail in Forman (2002). We used a 
ratio of features that varies from 10 to 150 for the 
most representative features for the Included 
class and from 5 to 150 for the Excluded class. 
For some experiments the number of features for 
the Included class is higher than the number of 
features for the Excluded class. We have chosen 
to do so because we wanted to re-balance the 
imbalance of classes in the training data set. Af-
ter selecting the number of Included and Ex-
cluded features, we used the combination to rep-
resent our entire collection of abstracts.  
We used the implementation from the Weka 
package for the Chi2 and InfoGain and the BNS 
implementation done by Ma (2007).  
4.4 Per-Question Classification Method 
The second method that we propose for solving 
the task takes into account the specifics of the 
systematic review process. It takes advantage of 
the set of questions the reviewers use in the proc-
ess of deciding if an abstract is relevant or not. 
These questions are created in the design step of 
the systematic review and almost all systematic 
reviews have them. By using these questions we 
better emulate how the human judges work when 
building systematic reviews.  
 We have chosen to use only the questions 
that have inclusion/exclusion criteria, there were 
also some opened answer questions involved in 
the review, because they are the ones that are 
important for reviewers to make a decision. To 
collect training data for each question, we used 
the same training and test data set as in the pre-
vious method (but note that not all the abstracts 
306
have answers for all the questions; therefore the 
training set sizes differ for each question). Table 
2 presents the questions and data sets used. 
When we created a training data set for each 
question we removed the abstracts for which we 
had a disagreement between the human experts ? 
two different answers for a specific question, 
they represent noise in the training data.  For 
each of the questions from Table 2, we trained a 
CNB classifier on the corresponding data set.  
 
Question 
(Training : Included class : Excluded class) 
Q1 - Is this article about a dissemination strat-
egy or a behavioural intervention? (14,057:1,145: 
12,912) 
Q2 - Is the population in this article made of indi-
viduals 65-year old or older or does it comprise 
individuals who serve the elderly population needs 
(i.e. health care providers, policy makers, organi-
zations, community)? (15,005:7,360:7,645) 
Q3 - Is this a primary study? (8,825:6,895:1,930) 
Q4 - Is this a review? (6,429:5,640:789)  
Table 2. Data sets for the per-question classification 
method. 
 
We used the same representation for the per-
question classifiers as we did for the global clas-
sifier: BOW, UMLS (the concepts that appeared 
only in the new question-oriented training data 
sets), and the combination BOW+UMLS. We 
used each trained model to obtain a prediction 
for each instance from the test set; therefore each 
test instance was assigned four prediction values 
of 0 or 1. To assign a final class for each test in-
stance, from the prediction of all four classifiers, 
the class of a test instance is decided according to 
one of the following four schemes:  
 1. If any one vote is Excluded, the final class 
of a test instance is Excluded. This is a 1-vote 
scheme. 
       2. If any two votes are Excluded, the final 
class of a test instance is Excluded. This is a 2-
vote scheme. 
 3. If any three votes are Excluded, the final 
class of a test instance is Excluded. This is a 3-
vote scheme.  
 4. If all four votes are Excluded, the final 
class of a test instance is Excluded. This is a 4-
vote scheme.  
 When we combined of the classifiers, we 
gave each classifier an equal importance. 
5 Evaluation Measures and Results 
When performing the evaluation for the task of 
classifying an abstract into one of the two classes 
Included (relevant) or Excluded (non rele-
vant), two objectives are of great importance: 
Objective 1 - ensure the completeness of the sys-
tematic review (maximize the number of relevant 
documents included); Objective 2 - reduce the 
reviewers' workload (maximize the number of 
irrelevant documents excluded).  
 We observe that objective 1 is more impor-
tant than objective 2 and this is why we decided 
to report recall and precision for the Included 
class. We also report F-measure, since we are 
dealing with imbalanced data sets.  
   Besides the standard evaluation measures, 
we report WSS8 measure as well in order to give 
a clearer view of the results we obtain.  
 As baseline for our methods we consider: 
two extreme baselines and a random-baseline 
classifier that takes into account the distribution 
of the two classes in the training data set. The 
baselines results are: Include_All ? a baseline 
that classifies everything in the majority class: 
Recall = 100%, Precision = 15%, F-measure = 
26.2%; WSS = 0% Exclude_All ? a baseline that 
classifies everything as Excluded: Recall = 0%, 
Precision = 100%, F-measure = 64.2%; WSS = 
0% Random baseline: Recall = 8.9%, Precision = 
15.4%, F-measure = 67.8%; WSS = 0.23%. 
5.1 Results for the Global Method 
In this subsection, we present the results obtained 
using our global method with the three represen-
tation techniques and CNB as classification algo-
rithm. To get a clear image of the results we 
show the confusion matrix in Table 3 for the 
reader to better understand the workload reduc-
tion when using classifiers to help the process of 
building systematic reviews.  
BOW features were identified following the 
guidelines presented in Section 3.4 and a number 
of 23,906 features were selected. UMLS con-
cepts were identified using the MetaMap system. 
                                                 
8
 WSS = (TE + FE)/(TE + FE + TI + FI) ? 1+ TI/(TI + FE) 
where T stands for true; F ? false I ? Included class; E- Ex-
cluded class. 
307
  
BOW UMLS BOW+UMLS 
True Inc.  2,692 2,793 2,715 
False Inc. 5,022 8,922 5,086 
True Exc. 18,135 14,235 18,071 
False Exc. 
 1,425 1,324 1,402 
Recall 65.3% 67.8% 65.9% 
Precision 34.9% 23.8% 34.8% 
F-measure 45.5% 35.2% 45.5% 
WSS 37.1% 24.9% 37.3% 
Table 3. Results for the global method. 
 
From the whole training abstracts collection, 
a number of 459 UMLS features were identified. 
Analyzing the results from Table 5, in terms of 
recall, the UMLS representation obtained the 
best recall results, 67.8% for the global method 
but much lower precision, 23.8% than BOW rep-
resentation, 34.9%. The hybrid representation, 
BOW+ UMLS features had similar results with 
the BOW alone. Recall increased a bit for the 
hybrid representation compared to BOW alone, 
0.6% but its value is still not acceptable. We 
conclude that the levels of recall, our main objec-
tive for this task, were not acceptable for a classi-
fier to be used as replacement of a human judge 
in the workflow of building a systematic review. 
The levels of precision that we obtained with the 
global method are acceptable but they cannot 
substitute the low level of recall. Since our major 
focus is recall, we investigated more and we fur-
ther improved our precision scores with the per-
question classification method. 
 
5.1.1 Results for Feature Selection 
 
Table 4 presents the results obtained with our 
feature selection techniques. We decided to re-
port only representative results using CNB as a 
classifier and a specific representation setting. 
The number of features used in the experiment is 
presented in the round brackets. The first number 
represents the number of features extracted from 
the Included class data set while the second 
from the Excluded class data set.  
 Similar experiments were performed when 
using Na?ve Bayes as classifier. The results ob-
tained were opposite to ones obtained for CNB, 
all abstracts were classified as Excluded. We 
believe that this is the case because the CNB 
classifier tries to compensate for the class imbal-
ance and gives more credit to the minority class,  
 
 
Chi2 
(150:150) 
InfoGain 
(300) 
BNS 
(10:8) 
True Inc. 3,819 3,875 2,690 
False Inc. 19,233 19,638 13,905 
True Exc. 3,924 3,518 9,253 
False Exc. 298 242 1,427 
Recall 92.8% 94.1% 65.3% 
Precision 16.6% 16.5% 16.2% 
F-measure 28% 28% 25% 
WSS 8.2% 7.9% 4.5% 
Table 4. Representative results obtained for various 
feature selection techniques. 
  
while the Na?ve Bayes classifier will let the ma-
jority class overwhelm the classifier. 
 Besides the results presented in Table 4, we 
also tried to boost the representative features for 
the Included class hoping to re-balance the im-
balance present in the training data set. To per-
form these experiments we selected the top k 
CHI2 word features and then added to this set of 
features the top k1 CHI2 representative features 
only for the Included class. The parameter k var-
ied from 50 to 100 and the parameter k1 from 30 
to 70. We performed experiments when using the 
original imbalanced training data set and using a 
balanced data set as well, with both CNB and 
Na?ve Bayes classifier. The results obtained for 
these experiments were similar to the ones when 
we used the previous feature selection tech-
niques. There was no significant difference in the 
results compared to the ones in Table 5. 
5.2 Results for the Per-Question Method 
The results for our second method using the four 
voting schemes are presented in Table 5.  
Compared with the global method the results 
obtained by the per-question method, especially 
the ones for 2 votes are the best so far in terms of 
the balance between the two objectives. A large 
number of abstracts that should be excluded are 
classified as Excluded whereas wrongly exclud-
ing very few abstracts that should have been in-
cluded (a lot fewer than in the case of the global 
classification method).  
The 2-votes scheme performs better than the 
1-vote schemes because of potential classifica-
tion errors. When the classifiers for two different 
questions (that look at two different aspects of 
the systematic review topic) are confident that 
the abstract is not relevant, the chance of correct 
308
prediction is higher; a balance between excluding 
an article and keeping it as relevant is achieved. 
When using the classifiers for 3 or 4 questions 
the performance goes down in terms of precision; 
a higher number of abstracts get classified as In-
cluded - some abstracts do not address all target 
question of the review topic.  
 
1-Vote  BOW UMLS BOW+UMLS 
True Inc. 1,262 1,222 1,264 
False Inc. 745 2,266 741 
True Exc. 22,412 20,891 22,416 
False Exc. 2,855 2,895 2,853 
Recall 30.6% 29.6% 30.7% 
Precision 62.8% 35.0% 63.0% 
F-measure 41.2% 32.1% 41.2% 
WSS 23.2% 16.8% 23.3% 
2-Vote BOW UMLS BOW+UMLS 
True Inc. 3,181 2,603 3,283 
False Inc. 9,976 9,505 10,720 
True Exc. 13,181 13,652 12,437 
False Exc. 936 1,514 834 
Recall 77.2% 63.2% 79.7% 
Precision 24.1% 21.5% 23.4% 
F-measure 36.8% 32.0% 36.2% 
WSS 29.0% 18.8% 28.4% 
3-Vote  BOW UMLS BOW+UMLS 
True Inc. 3,898 3,480 3,890 
False Inc. 18,915 16,472 18,881 
True Exc. 4,242 6,685 4,276 
False Exc. 219 637 227 
Recall 94.6% 84.5% 94.4% 
Precision 17.0% 17.4% 17.0% 
F-measure 28.9% 28.9% 28.9% 
WSS 11.0% 11.3% 11.0% 
4-Vote  BOW UMLS BOW+UMLS 
True Inc. 4,085 3,947 4,086 
False Inc. 21,946 20,869 21,964 
True Exc. 1,211 2,288 1,193 
False Exc. 32 170 31 
Recall 99.2% 95.8% 99.2% 
Precision 15.6% 15.9% 15.6% 
F-measure 27.1% 27.2% 27.0% 
WSS 3.7% 4.8% 3.7% 
Table 5. Results for the per-question method for the 
Included class. 
 
For the per-question technique the recall value 
peaked at 99.2% with the 4-vote method BOW 
and BOW+UMLS representation technique. In 
the same time the lowest values of precision for 
the per-question technique, 15.6% is obtained 
with the same experimental setting. It is impor-
tant to aim for a high recall but not to dismiss the 
precision values. The difference of even less than 
2% in precision values can cause the reviewers to 
read additional thousands of documents, as ob-
served in the confusion matrices for 2-vote, 3-
vote and 4-vote methods in Table 5.  
From the confusion matrix in Table 5 for the 
2-vote method and the 3- and 4-vote method we 
observe the high difference in the number of 
documents a reviewer will have to read (the 
falsely included documents). The difference in 
precision from 24.1% for the 2-vote method to 
15.6% for the 4-vote method makes the reviewer 
go through 11,988 additional abstracts.  
The best value for the WSS measure for the 
per-question method is achieved by the 2-vote 
scheme. The result is lower than the one obtained 
by the global method but the recall level is higher 
therefore, we still keep as a potential winner the 
2-vote scheme.  
5.3 Results for Human-Machine Workflow 
In Figure 1, we envisioned the way we can use 
the automatic classifier in the workflow of build-
ing a systematic review. In order to determine the 
performance of the human-machine workflow 
that we propose we computed the recall values 
when the human reviewer?s labels are combined 
with the labels obtained from the classifier. The 
same labeling technique is applied as for the hu-
man-human workflow: if at least one decision for 
an abstract is to include it in the systematic re-
view, then the final label is Included.  
 We also calculated the evaluation measures 
for the two reviewers. The evaluation measures 
for the human judge that is kept in the human-
machine workflow, Reviewer 1 in Figure 1, are 
64.29% for recall and 15.20% for precision. The 
evaluation measures for the reviewer that is to be 
replaced in the human-machine classification, 
Reviewer 2 in Figure 1 are 59.66% for recall and 
15.09% for precision. The recall value for the 
two human judges combined is 85.26% and the 
precision value is 100%. As we can observe the 
recall value for the second reviewer, the one that 
is replaced in the human-classifier workflow is 
low. In Table 6 we present precision and recall 
results for the symbiotic model for both our me-
thods. In these results we can clearly see that the 
2-vote technique is superior to the other voting 
techniques and to the global method. For almost 
the same level of precision the level or recall it is 
much higher. These observations support the fact 
309
that the extra effort spent in identifying the most 
suitable methodology pays off.  
 The fact that we keep a human in the loop 
makes our method acceptable as a workflow for 
building a systematic review.  
 
Method BOW UMLS BOW+ 
UMLS 
Global    17.9/87.7% 17.0/88.6% 17.9/87.7% 
1-Vote 17.1/75.3% 16.5/74.8% 17.1/75.4% 
2-Vote 17.1/91.6% 16.4/86.6% 17.1/92.7% 
3-Vote 15.8/97.9% 15.8/94.2% 15.8/97.8% 
4-Vote 15.3/99.6% 15.4/98.3% 15.3/99.6% 
Table 6. Precision/recall results for the human-
classifier workflow for the Included class. 
6 Discussion 
The global method achieves good results in terms 
of precision while the best recall is obtained by 
the per-question method.   
 The best results for the task were obtained 
using the per-question method with the 2-vote 
scheme with or without UMLS features. The 3-
vote scheme with UMLS representation is close 
to the 2-vote scheme but looking at F-measure 
and WSS results the 2-vote scheme is better. The 
clear distinction between the methods comes 
when we combined the classifiers with the hu-
man judge in the workflow of building reviews. 
The per-question technique is more robust 
and it offers the possibility to choose the desired 
type of performance. If the reviewers are willing 
to read almost the entire collection of documents, 
knowing that the recall is high, then a 3 or 4-vote 
scheme can be the set-up (though the 3 or 4-vote 
method is not likely to achieve 100% recall be-
cause it is very rare that an abstract contain an-
swers to three or four of the questions associated 
with the systematic review). If the reviewers will 
like to read a small collection being confident 
that almost all the abstracts are relevant, then a 1-
vote scheme can be the set-up required. The per-
question method confirms the fact that an en-
semble of classifiers is better than one classifier; 
(Dietterich, 1997).  
When we combine the human and the system 
results we obtain a major improved in terms of 
recall. We base our discussion for the human-
machine results for the experiment that obtained 
the best results, the 2-vote scheme with a 
BOW+UMLS representation technique. When 
combining the human and classifier decisions, 
the precision level decreased a bit compared to 
the one that the machine obtained. We believe 
that this is the case because some of the abstracts 
that the classifier excluded were included by the 
first human reviewer and, with this decision 
process in place, the level of precision dropped. 
Our goal of improving the recall level from 
the first level of screening is achieved, since 
when both the classifier and the human judge are 
integrated in the workflow, the recall level jumps 
from 79.7% to 92.7%. 
We believe that the low level of precision 
that is obtained for the human reviewer, for the 
human-classifier workflow, and for the classifier, 
is due to the fact that we are running experiments 
for the first screening phase when we use only 
the abstracts as source of information and not the 
entire articles.  
We believe that further investigations are re-
quired to fully replace a human reviewer with an 
automatic classifier but the results obtained with 
the per-question method encourage us to believe 
that this is a suitable solution for reaching our 
final goal.  
7 Conclusions and Future Work  
In this paper, we looked at two methods by 
which we envision the way automatic text classi-
fication techniques could help the workflow of 
building systematic reviews.  
The first method is a straight-forward appli-
cation of the representations and learning algo-
rithms that capture the specifics of the data: med-
ical domain, huge number of features, misclassi-
fication, and imbalanced classes.  
We showed that the specifics of the human 
protocol in which systematic reviews are built 
have a positive effect when deployed in an auto-
matic way. We believe that the tedious process 
that is currently used for building systematic re-
views can be lightened by the use of a classifier 
in combination with only one human judge. By 
having a human judge in the loop, we ensure that 
the workflow is reliable and that the system can 
be easily integrated in the workflow.  
 In future work we would like to look into 
ways of improving the results by the way we 
chose the training data set and by integrating 
more domain specific knowledge. We would also 
like to investigate ways by witch we can update 
systematic reviews.  
310
References  
Aphinyanaphongs Y. and Aliferis C. Text Categoriza-
tion Models for Retrieval of High Quality Articles. 
Journal of the American Medical Informatics As-
sociation 2005; 12:207-216. 
Cohen A.M. Optimizing Feature Representation for 
Automated Systematic Review Work Prioritization. 
Proceedings of the AMIA Annual Symposium 
2008; 6:121-126. 
Cohen A.M., Hersh W.R., Peterson K., Yen P.Y. Re-
ducing Workload in Systematic Review Prepara-
tion Using Automated Citation Classification. 
Journal of the American Medical Informatics As-
sociation 2006; 13:206-219. 
Dietterich, T. Machine-Learning Research: Four 
Current Directions. Artificial Intelligence Maga-
zine. 18(4): 97-136 (1997) 
Drummond C. and Holte R.C. C4.5, Class Imbalance, 
and Cost Sensitivity: Why Under-Sampling beats 
Over-Sampling. Proceedings of the Twentieth In-
ternational Conference on Machine Learning: 
Workshop on Learning from Imbalanced Data Sets 
(II), 2003. 
Forman G. Choose Your Words Carefully: An Empiri-
cal Study of Feature Selection Metrics for Text 
Classification. In the Joint Proceedings of the 13th 
European Conference on Machine Learning and 
the 6th European Conference on Principles and 
Practice of Knowledge Discovery in Databases 
(ECML/PKDD), 2002. 
Frank E. and Bouckaert R.R. Naive Bayes for Text 
Classification with Unbalanced Classes. In the 
Proceedings of the 10th European Conference on 
Principles and Practice of Knowledge Discovery in 
Databases, Berlin, Germany, 2006, pp. 503-510. 
Haynes R.B., Wilczynski N., McKibbon K.A., Walker 
C.J., Sinclair J.C. Developing optimal search strat-
egies for detecting clinically sound studies in 
MEDLINE. Journal of the American Medical In-
formatics Association 1994; 1:447-58. 
Kohavi R. and Provost F. Glossary of Terms. Editorial 
for the Special Issue on Applications of Machine 
Learning and the Knowledge Discovery Process 
1998; 30:271-274. 
Ma Y. 2007. Text classification on imbalanced data: 
Application to Systematic Reviews Automation.  
M.Sc. Thesis. University of Ottawa. 
311
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 91?98,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Extraction of Disease-Treatment Semantic Relations from Biomedical 
Sentences 
 
Oana Frunza and Diana Inkpen 
School of Information Technology and Engineering 
University of Ottawa Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
  
 
 
Abstract 
This paper describes our study on identi-
fying semantic relations that exist between 
diseases and treatments in biomedical sen-
tences. We focus on three semantic rela-
tions: Cure, Prevent, and Side Effect. The 
contributions of this paper consists in the 
fact that better results are obtained com-
pared to previous studies and the fact that 
our research settings allow the integration 
of biomedical and medical knowledge. 
We obtain 98.55% F-measure for the Cure 
relation, 100% F-measure for the Prevent 
relation, and 88.89% F-measure for the 
Side Effect relation. 
1 Introduction 
Research in the fields of life-science and bio-
medical domain has been the focus of the Natural 
Language Processing (NLP) and Machine Learn-
ing (ML) community for some time now. This 
trend goes very much inline with the direction 
the medical healthcare system is moving to: the 
electronic world. The research focus of scientists 
that work in the filed of computational linguistics 
and life science domains also followed the trends 
of the medicine that is practiced today, an Evi-
dence Based Medicine (EBM). This new way of 
medical practice is not only based on the experi-
ence a healthcare provider acquires as time 
passes by, but on the latest discoveries as well. 
We live in an information explosion era where it 
is almost impossible to find that piece of relevant 
information that we need. With easy and cheep 
access to disk-space we sometimes even find 
challenging to find our stored local documents. It 
should come to no surprise that the global trend 
in domains like biomedicine and not only is to 
rely on technology to identify and upraise infor-
mation. The amount of publications and research 
that is indexed in the life-science domain grows 
almost exponentially (Hunter and Cohen (2006) 
making the task of finding relevant information, 
a hard and challenging task for NLP research.  
The search for information in the life-science 
domain is not only the focus of researchers that 
work in these fields, but the focus of laypeople as 
well. Studies reveal that people are searching the 
web for medical-related articles to be better in-
formed about their health. Ginsberg et al (2009) 
show how a new outbreak of the influenza virus 
can be detected from search engine query data.   
The aim of this paper is to show which NLP 
and ML techniques are suitable for the task of 
identifying semantic relations between diseases 
and treatments in short biomedical texts. The 
value of our work stands in the results we obtain 
and the new feature representation techniques.  
2 Related Work  
The most relevant work for our study is the work 
of Rosario and Hearst (2004). The authors of this 
paper are the ones that created and distributed the 
data set used in our research. The data set is an-
notated with disease and treatments entities and 
with 8 semantic relations between diseases and 
treatments. The main focus of their work is on 
entity recognition ? the task of identifying enti-
ties, diseases and treatments in biomedical text 
sentences. The authors use Hidden Markov 
Models and maximum entropy models to per-
form both the task of entity recognition and of 
relation discrimination. Their representation 
techniques are based on words in context, part-
of-speech information, phrases, and terms from 
MeSH1, a medical lexical knowledge-base. Com-
pared to previous work, our research is focused 
                                                 
1
 http://www.nlm.nih.gov/mesh/meshhome.html 
91
on different representation techniques, different 
classification models, and most importantly in 
obtaining improved results without using the an-
notations of the entities (new data will not have 
them). In previous research, the best results were 
obtained when the entities involved in the rela-
tions were identified and used as features.  
The biomedical literature contains a wealth of 
work on semantic relation extraction, mostly fo-
cused on more biology-specific tasks: subcellu-
lar-location (Craven 1999), gene-disorder asso-
ciation (Ray and Craven 2001), and diseases and 
drugs relations (Srinivasan and Rindflesch 2002, 
Ahlers et al, 2007). 
Text classification techniques combined with a 
Na?ve Bayes classifier and relational learning 
algorithms are methods used by Craven (1999). 
Hidden Markov Models are used in Craven 
(2001), but similarly to Rosario and Hearst 
(2004), the research focus was entity recognition.  
A context based approach using MeSH term 
co-occurrences are used by Srinivasan and Rind-
flesch (2002) for relationship discrimination be-
tween diseases and drugs.  
A lot of work is focused on building rules used 
to extract relation. Feldman et al (2002) use a 
rule-based system to extract relations that are 
focused on genes, proteins, drugs, and diseases. 
Friedman et al (2001) go deeper into building a 
rule-based system by hand-crafting a semantic 
grammar and a set of semantic constraints in or-
der to recognize a range of biological and mo-
lecular relations. 
3 Task and Data Sets 
Our task is focused on identifying disease-
treatment relations in sentences. Three relations: 
Cure, Prevent, and Side Effect, are the main ob-
jective of our work. We are tackling this task by 
using techniques based on NLP and supervised 
ML techniques. We decided to focus on these 
three relations because these are the ones that are 
better represented in the original data set and in 
the end will allow us to draw more reliable con-
clusions. Also, looking at the meaning of all rela-
tions in the original data set, the three that we 
focus on are the ones that could be useful for 
wider research goals and are the ones that really 
entail relations between two entities. In the su-
pervised ML settings the amount of training data 
is a factor that influences the performance; sup-
port for this stands not only in the related work 
performed on the same data set, but in the re-
search literature as well. The aim of this paper is 
to focus on few relations of interest and try to 
identify what predictive model and what repre-
sentation techniques bring the best results of 
identifying semantic relations in short biomedi-
cal texts. We mostly focused on the value that 
the research can bring, rather than on an incre-
mental research. 
As mentioned in the previous section, the data 
set that we use to run our experiments is the one 
of Rosario and Hearst (2004). The entire data set 
is collected from Medline2 2001 abstracts. Sen-
tences from titles and abstracts are annotated 
with entities and with 8 relations, based only on 
the information present in a certain sentence. The 
first 100 titles and 40 abstracts from each of the 
59 Medline 2001 files were used for annotation. 
Table 1, presents the original data set, as pub-
lished in previous research. The numbers in pa-
renthesis represent the training and test set sizes.  
 
Relationship Definition and Example 
Cure 
810 (648, 162) 
TREAT cures DIS 
Intravenous immune globulin for 
recurrent spontaneous abortion 
Only DIS 
616 (492, 124) 
TREAT not mentioned 
Social ties and susceptibility to 
the common cold 
Only TREAT 
166 (132, 34) 
DIS not mentioned 
Flucticasome propionate is safe in 
recommended doses 
Prevent 
63 (50, 13) 
TREAT prevents the DIS 
Statins for prevention of stroke 
Vague 
36 (28, 8) 
Very unclear relationship 
Phenylbutazone and leukemia 
Side Effect 
29 (24, 5) 
DIS is a result of a TREAT 
Malignant mesodermal mixed 
tumor of the uterus following 
irradiation 
NO Cure 
4 (3, 1) 
TREAT does not cure DIS 
Evidence for double resistance to 
permethrin and malathion in head 
lice 
     Total relevant: 1724 (1377, 347) 
Irrelevant 
1771 (1416, 355) 
Treat and DIS not present 
Patients were followed up for 6 
months 
Total: 3495 (2793, 702) 
 Table 1. Original data set.  
     
From this original data set, the sentences that are 
annotated with Cure, Prevent, Side Effect, Only 
DIS, Only TREAT, and Vague are the ones that 
used in our current work. While our main focus 
is on the Cure, Prevent, and Side Effect, we also 
run experiments for all relations such that a di-
rect comparison with the previous work is done.  
                                                 
2
 http://medline.cos.com/ 
92
Table 2 describes the data sets that we created 
from the original data and used in our experi-
ments. For each of the relations of interest we 
have 3 labels attached: Positive, Negative, and 
Neutral. The Positive label is given to sentences 
that are annotated with the relation in question in 
the original data; the Negative label is given to 
the sentences labeled with Only DIS and Only 
TREAT classes in the original data; Neutral label 
is given to the sentences annotated with Vague 
class in the original data set.  
 
Table 2. Our data sets3. 
4 Methodology 
The experimental settings that we follow are 
adapted to the domain of study (we integrate ad-
ditional medical knowledge), yielding for the 
methods to bring improved performance.  
The challenges that can be encountered while 
working with NLP and ML techniques are: find-
ing the suitable model for prediction ? since the 
ML field offers a suite of predictive models (al-
gorithms), the task of finding the suitable one 
relies heavily on empirical studies and knowl-
edge expertise; and finding the best data repre-
sentation ? identifying the right and sufficient 
features to represent the data is a crucial aspect. 
These challenges are addressed by trying various 
predictive algorithms based on different learning 
techniques, and by using various textual repre-
sentation techniques that we consider suitable.  
The task of identifying the three semantic rela-
tions is addressed in three ways: 
       Setting 1: build three models, each focused 
on one relation that can distinguish sentences 
that contain the relation ? Positive label, from 
other sentences that are neutral ? Neutral label, 
and from sentences that do not contain relevant 
information ? Negative label; 
                                                 
3
 The number of sentences available for download is 
not the same as the ones from the original data set, 
published in Rosario and Hearst (?04). 
Setting 2: build three models, each focused on 
one relation that can distinguish sentences that 
contain the relation from sentences that do not 
contain any relevant information. This setting is 
similar to a two-class classification task in which 
instances are labeled either with the relation in 
question ? Positive label, or with non-relevant 
information ? Negative label; 
  Setting 3: build one model that distinguishes the 
three relations ? a three-way classification task 
where each sentence is labeled with one of the 
semantic relations, using the data with all the 
Positive labels. 
The first set of experiments is influenced by 
previous research done by Koppel and Schler 
(2005). The authors claim that for polarity learn-
ing ?neutral? examples help the learning algo-
rithms to better identify the two polarities. Their 
research was done on a corpus of posts to chat 
groups devoted to popular U.S. television and 
posts to shopping.com?s product evaluation page. 
As classification algorithms, a set of 6 repre-
sentative models: decision-based models (Deci-
sion trees ? J48), probabilistic models (Na?ve 
Bayes and complement Na?ve Bayes (CNB), 
which is adapted for imbalanced class distribu-
tion), adaptive learning (AdaBoost), linear classi-
fier (support vector machine (SVM) with poly-
nomial kernel), and a classifier, ZeroR, that al-
ways predicts the majority class in the training 
data used as a baseline. All classifiers are part of 
a tool called Weka4. 
As representation technique, we rely on fea-
tures such as the words in the context, the noun 
and verb-phrases, and the detected biomedical 
and medical entities. In the following subsec-
tions, we describe all the representation tech-
niques that we use.  
4.1 Bag-of-words representation 
 
The bag-of-words (BOW) representation is 
commonly used for text classification tasks. It is 
a representation in which the features are chosen 
among the words that are present in the training 
data. Selection techniques are used in order to 
identify the most suitable words as features. Af-
ter the feature space is identified, each training 
and test instance is mapped into this feature rep-
resentation by giving values to each feature for a 
certain instance. Two feature value representa-
tions are the most commonly used for the BOW 
representation: binary feature values ? the value 
                                                 
4
 http://www.cs.waikato.ac.nz/ml/weka/ 
Train  
          Relation Positive Negative Neutral 
Cure 554 531 25 
Prevent 42 531 25 
SideEffect 20 531 25 
 Test   
Relation Positive Negative Neutral 
Cure 276 266 12 
Prevent 21 266 12 
SideEffect 10 266 12 
93
of a feature is 1 if the feature is present in the 
instance and 0 otherwise, or frequency feature 
values ? the feature value is the number of times 
it appears in an instance, or 0 if it did not appear.  
Taking into consideration the fact that an in-
stance is a sentence, the textual information is 
relatively small. Therefore a frequency value 
representation is chosen. The difference between 
a binary value representation and a frequency 
value representation is not always significant, 
because sentences tend to be short. Nonetheless, 
if a feature appears more than once in a sentence, 
this means that it is important and the frequency 
value representation captures this aspect. 
The selected features are words (not lemma-
tized) delimited by spaces and simple punctua-
tion marks: space, ( , ) , [ , ] , . , ' , _ that ap-
peared at least three times in the training collec-
tion and contain at least an alpha-numeric char-
acter, are not part of an English list of stop 
words5 and are longer than three characters. Stop 
words are function words that appear in every 
document (e.g., the, it, of, an) and therefore do 
not help in classification. The frequency thresh-
old of three is commonly used for text collec-
tions because it removes non-informative fea-
tures and also strings of characters that might be 
the result of a wrong tokenization when splitting 
the text into words. Words that have length of 
one or two characters are not considered as fea-
tures because of two reasons: possible incorrect 
tokenization and problems with very short acro-
nyms in the medical domain that could be highly 
ambiguous (could be a medical acronym or an 
abbreviation of a common word).  
4.2 NLP and biomedical concepts represen-
tation  
The second type of representation is based on 
NLP information ? noun-phrases, verb-phrases 
and biomedical concepts (Biomed). In order to 
extract this type of information from the data, we 
used the Genia6 tagger. The tagger analyzes Eng-
lish sentences and outputs the base forms, part-
of-speech tags, chunk tags, and named entity 
tags. The tagger is specifically tuned for bio-
medical text such as Medline abstracts.  
Figure 1 presents an output example by the 
Genia tagger for the sentence: ?Inhibition of NF-
kappaB activation reversed the anti-apoptotic 
effect of isochamaejasmin.?. The tag O stands 
for Outside, B for Beginning, and I for Inside. 
                                                 
5
 http://www.site.uottawa.ca/~diana/csi5180/StopWords 
6
 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 
Figure 1. Example of Genia tagger output 
Inhibition     Inhibition  NN  B-NP  O 
of       of   IN  B-PP  O  
NF-kappaB NF-kappaB  NN  B-NP B-protein  
activation    activation   NN  I-NP  O  
reversed       reverse  VBD  B-VP  O  
the       the   DT  B-NP  O  
anti-apoptotic anti-apoptotic JJ  I-NP  O  
effect        effect  NN  I-NP  O  
of        of   IN  B-PP  O  
isochamaejasmin isochamaejasmin NN B-NP  O  
.  .   .  O  O 
 
The noun-phrases and verb-phrases identified by 
the tagger are considered as features for our sec-
ond representation technique. The following pre-
processing steps are applied before defining the 
set of final features: remove features that contain 
only punctuation, remove stop-words, and con-
sider valid features only the lemma-based forms 
of the identified noun-phrases, verb-phrases and 
biomedical concepts. The reason to do this is 
because there are a lot of inflected forms (e.g., 
plural forms) for the same word and the lemma-
tized form (the base form of a word) will give us 
the same base form for all the inflected forms.  
4.3 Medical concepts (UMLS) representa-
tion 
In order to work with a representation that pro-
vides features that are more general than the 
words in the abstracts (used in the BOW repre-
sentation), we also used the unified medical lan-
guage system7 (here on UMLS) concept repre-
sentations. UMLS is a knowledge source devel-
oped at the U.S. National Library of Medicine 
(here on NLM) and it contains a meta-thesaurus, 
a semantic network, and the specialist lexicon for 
biomedical domain. The meta-thesaurus is organ-
ized around concepts and meanings; it links al-
ternative names and views of the same concept 
and identifies useful relationships between dif-
ferent concepts. UMLS contains over 1 million 
medical concepts, and over 5 million concept 
names which are hierarchical organized. Each 
unique concept that is present in the thesaurus 
has associated multiple text strings variants 
(slight morphological variations of the concept). 
All concepts are assigned at least one semantic 
type from the semantic network providing a gen-
eralization of the existing relations between con-
cepts. There are 135 semantic types in the 
knowledge base linked through 54 relationships.  
                                                 
7 http://www.nlm.nih.gov/pubs/factsheets/umls.html 
94
In addition to the UMLS knowledge base, 
NLM created a set of tools that allow easier ac-
cess to the useful information. MetaMap8  is a 
tool created by NLM that maps free text to medi-
cal concepts in the UMLS, or equivalently, it 
discovers meta-thesaurus concepts in text. With 
this software, text is processed through a series 
of modules that in the end will give a ranked list 
of all possible concept candidates for a particular 
noun-phrase. For each of the noun phrases that 
the system finds in the text, variant noun phrases 
are generated. For each of the variant noun 
phrases, candidate concepts (concepts that con-
tain the noun phrase variant) from the UMLS 
meta-thesaurus are retrieved and evaluated. The 
retrieved concepts are compared to the actual 
phrase using a fit function that measures the text 
overlap between the actual phrase and the candi-
date concept (it returns a numerical value). The 
best of the candidates are then organized accord-
ing to the decreasing value of the fit function. 
We used the top concept candidate for each iden-
tified phrase in an abstract as a feature.  Figure 2 
presents an example of the output of the Meta-
Map system for the phrase ?to an increased 
risk". The information presented in the brackets, 
the semantic type, ?Qualitative Concept, Quanti-
tative Concept? for the candidate with the fit 
function value 861 is the feature used for our 
UMLS representation. 
 
Figure 2. Example of MetaMap system output 
Meta Candidates (6) 
861 Risk [Qualitative Concept, Quantitative Concept] 
694 Increased (Increased (qualifier value)) [Func-
tional Concept] 
623 Increase (Increase (qualifier value)) [Functional 
Concept] 
601 Acquired (Acquired (qualifier value)) [Temporal 
Concept] 
601 Obtained (Obtained (attribute)) [Functional Con-
cept] 
588 Increasing (Increasing (qualifier value)) [Func-
tional Concept] 
 
Another reason to use a UMLS concept represen-
tation is the concept drift phenomenon that can 
appear in a BOW representation. Especially in 
the medical domain texts, this is a frequent prob-
lem as stated by Cohen et al (2004). New arti-
cles that publish new research on a certain topic 
bring with them new terms that might not match 
the ones that were seen in the training process in 
a certain moment of time.  
                                                 
8
 http://mmtx.nlm.nih.gov/ 
Experiments for the task tackled in our re-
search are performed with all the above-
mentioned representations, plus combinations of 
them. We combine the BOW, UMLS and NLP 
and biomedical concepts by putting all features 
together to represent an instance.   
5 Results 
This section presents the results obtained for the 
task of identifying semantic relations with the 
methods described above. As evaluation meas-
ures we report F-measure and accuracy values. 
The main evaluation metric that we consider is 
the F-measure9, since it is a suitable when the 
data set is imbalanced. We report the accuracy 
measure as well, because we want to compare 
our results with previous work. Table A1 from 
appendix A presents the results that we obtained 
with our methods. The table contains F-measure 
scores for all three semantic relations with the 
three experimental settings proposed for all com-
binations of representation and classification al-
gorithms. In this section, since we cannot report 
all the results for all the classification algorithms, 
we decided to report the classifiers that obtained 
the lower and upper margin of results for every 
representation setting. More detailed descriptions 
for the results are present in appendix A. We 
consider as baseline a classifier that always pre-
dicts the majority class. For the relation Cure the 
F-measure baseline is 66.51%, for Prevent and 
Side Effect 0%. 
The next three figures present the best results 
obtained for the three experimental settings. 
 
Figure 3. Best results for Setting 1. 
85.14%
62.50%
34.48%
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Cure - BOW +
NLP + Biomed+
UMLS - SMO
Prevent -
UMLS + NLP +
Biomed - SVM
SideEffect -
BOW- NB
Results - Setting1F-measure
 
                                                 
9
 F-measure represents the harmonic mean between 
precision and recall. Precision represents the percent-
age of correctly classified sentences while recall 
represents the percentage of sentences identified as 
relevant by the classifier.  
95
Figure 4. Best results for Setting 2. 
82.00%
84.00%
86.00%
88.00%
90.00%
92.00%
94.00%
96.00%
98.00%
100.00%
Cure -
BOW + 
NLP + 
Biomed+ 
UMLS - NB
Prevent -
BOW + 
NLP + 
Biomed+ 
UMLS - NB
SideEffect 
- BOW + 
NLP + 
Biomed+ 
UMLS -
CNB
98.55% 100%
88.89%
Results - Setting 2
F-measure
 
 
Figure 5. Best results for Setting 3. 
98.55% 100%
88.89%
80.00%
85.00%
90.00%
95.00%
100.00%
Cure -  BOW +
NLP +
Biomed+
UMLS - NB
Prevent - 
BOW + NLP +
Biomed+
UMLS - NB
SideEffect -
BOW + NLP +
Biomed+
UMLS - CNB
Results - Setting 3
F-measure
 
 
6 Discussion 
Our goal was to obtain high performance results 
for the three semantic relations. The first set of 
experiments was influenced by previous work on 
a different task. The results obtained show that 
this setting might not be suitable for the medical 
domain, due to one of the following possible ex-
planations: the number of examples that are con-
sidered as being neutral is not sufficient or not 
appropriate (the neutral examples are considered 
sentences that are annotated with a Vague rela-
tion in the original data); or the negative exam-
ples are not appropriate (the negative examples 
are considered sentences that talk about either 
treatment or about diseases). The results of these 
experiments are shown in Figure 3. As future 
work, we want to run similar setting experiments 
when considering negative examples sentences 
that are not informative, labeled Irrelevant, from 
the original data set, and the neutral examples the 
ones that are considered negative in this current 
experiments.  
In Setting 2, the results are better than in the 
previous setting, showing that the neutral exam-
ples used in the previous experiments confused 
the algorithms and were not appropriate. These 
results validate the fact that the previous setting 
was not the best one for the task. 
The best results for the task are obtained with 
the third setting, when a model is built and 
trained on a data set that contains all sentences 
annotated with the three relations. The represen-
tation and the classification algorithms were able 
to make the distinction between the relations and 
obtained the best results for this task. The results 
are: 98.55% F-measure for the Cure class, 100% 
F-measure for the Prevent class, and 88.89% for 
the Side Effect class.  
Some important observations can be drawn 
from the obtained results: probabilistic and linear 
models combined with informative feature repre-
sentations bring the best results. They are consis-
tent in outperforming the other classifiers in all 
the three settings. AdaBoost classifier was out-
performed by other classifiers, which is a little 
surprising, taking into consideration the fact that 
this classifier tends to work better on imbalanced 
data. BOW is a representation technique that 
even though it is simplistic, most of the times it 
is really hard to outperform. One of the major 
contributions of this work is the fact that the cur-
rent experiments show that additional informa-
tion used in the representation settings brings 
improvements for the task. The task itself is a 
knowledge-charged task and the experiments 
show that classifiers can perform better when 
richer information (e.g. concepts for medical  
ontologies) is provided.  
6.1 Comparison to previous work 
Even though our main focus is on the three rela-
tions mentioned earlier, in order to validate our 
methodology, we also performed the 8-class 
classification task, similar to the one done by 
Rosario and Hearst (2004). Figure 3 presents a 
graphical comparison of the results of our meth-
ods to the ones obtained in the previous work. 
We report accuracy values for these experiments, 
as it was done in the previous work. 
In Figure 3, the first set of bar-results repre-
sents the best individual results for each relation. 
The representation technique and classification 
model that obtains the best results are the ones 
described on the x-axis.  
 
 
 
 
 
96
Figure 3. Comparison of results. 
Results for all semantic relations
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
120.00%
Cu
re
 
-
 
BO
W+
NL
P+
Bio
m
ed
+U
ML
S-C
NB
No
_
Cu
re
Pr
ev
en
t-B
OW
+N
LP
+B
iom
ed
-
CN
B
Va
gu
e 
-
 
BO
W 
+ 
NL
P+
Bi
om
ed
 
-
 
NB
Sid
eE
ffe
ct 
-
BO
W+
NL
P+
Bio
m
ed
-
NB
Tre
ar
m
en
t_O
nly
 
-
BO
W+
NL
P+
Bio
m
ed
-
NB
Dis
ea
se
_
On
ly-
BO
W+
NL
P+
Bi
om
ed
-
J4
8
Irr
ele
va
nt
 
-
 
BO
W+
NL
P+
Bio
m
ed
+U
ML
S-A
da
B
Models
Ac
cu
ra
cy
Best Models
Best Model
Previous Work
 
 
The second series of results represents the 
overall best model that is reported for each rela-
tion. The model reported here is a combination 
of BOW, verb and noun-phrases, biomedical and 
UMLS concepts, with a CNB classifier. 
The third series of results represent the accu-
racy results obtained in previous work by Rosa-
rio and Hearst (2004). As we can see from the 
figure, the best individual models have a major 
improvement over previous results. When a sin-
gle model is used for all relations, our results 
improve the previous ones in four relations with 
the difference varying from: 3 percentage point 
difference (Cure) to 23 percentage point differ-
ence (Prevent). We obtain the same results for 
two semantic relations, No_Cure and Vague and 
we believe that this is the case due to the fact that 
these two classes are significantly under-
represented compared to the other ones involved 
in the task. For the Treatment_Only relation our 
results are outperformed with 1.5 percentage 
points and for the Irrelevant relation with 0.1 
percentage point, only when we use the same 
model for all relations.  
7 Conclusion and Future Work 
We can conclude that additional knowledge and 
deeper analysis of the task and data in question 
are required in order to obtain reliable results. 
Probabilistic models are stable and reliable for 
the classification of short texts in the medical 
domain. The representation techniques highly 
influence the results, common for the ML com-
munity, but more informative representations 
where the ones that consistently obtained the best 
results.  
As future work, we would like to extend the 
experimental methodology when the first setting 
is applied, and to use additional sources of in-
formation as representation techniques. 
 
References  
Ahlers C., Fiszman M., Fushman D., Lang F.-M., 
Rindflesch T. 2007. Extracting semantic predica-
tions from Medline citations for pharmacogenom-
ics. Pacific Symposium on Biocomputing, 12:209-
220. 
Craven M. 1999. Learning to extract relations from 
Medline. AAAI-99 Workshop on Machine Learn-
ing for Information Extraction. 
Feldman R. Regev Y., Finkelstein-Landau M., Hur-
vitz E., and Kogan B. 2002. Mining biomedical lit-
erature using information extraction. Current Drug 
Discovery.  
Friedman C., Kra P., Yu H., Krauthammer M., and 
Rzhetzky A. 2001. Genies: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 
17(1). 
Ginsberg J., Mohebbi Matthew H., Rajan S. Patel, 
Lynnette Brammer, Mark S. Smolinski & Larry 
Brilliant. 2009. Detecting influenza epidemics 
using search engine query data. Nature 457, 
1012-1014. 
Hunter Lawrence and K. Bretonnel Cohen. 2006. 
Biomedical Language Processing: What?s Beyond 
PubMed? Molecular Cell 21, 589?594. 
Ray S. and Craven M. 2001. Representing sentence 
structure in Hidden Markov Models for informa-
tion extraction. Proceedings of IJCAI-2001. 
Rosario B. and Marti A. Hearst. 2004. Classifying 
semantic relations in bioscience text. Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics, 430. 
 Koppel M. and J. Schler. 2005. Using Neutral Ex-
amples for Learning Polarity, Proceedings of 
IJCAI, Edinburgh, Scotland. 
Srinivasan P. and T. Rindflesch 2002. Exploring text 
mining from Medline. Proceedings of the AMIA 
Symposium.  
 
 
 
 
97
Appendix A. Detailed Results. 
 
Classification Algorithm - F-Measure (%) 
 
 
 
Relation 
 
 
Representation 
Setting1 Setting2 Setting3 
Cure NLP+Biomed AdaB 
ZeroR 
32.22 
66.51 
AdaB 
ZeroR 
35.69 
67.48 
CNB 
SVM 
87.88 
94.85 
 BOW AdaB 
CNB 
63.60 
79.22 
AdaB 
SVM 
67.23 
81.43 
CNB 
NB 
92.57 
96.80 
 UMLS AdaB 
NB 
61.08 
74.73 
AdaB 
NB 
64.78 
76.04 
CNB 
SVM 
88.20 
95.62 
 BOW+UMLS AdaB 
CNB 
56.07 
84.54 
AdaB 
NB 
74.68 
86.48 
J48 
NB 
96.13 
97.50 
 NLP+Biomed 
+UMLS 
AdaB 
NB 
61.08 
75.18 
AdaB 
NB 
64.78 
76.70 
CNB 
SVM 
90.87 
96.58 
 NLP+Biomed 
+BOW 
AdaB 
SVM 
53.04 
78.98 
AdaB 
CNB 
77.46 
81.86 
J48 
NB 
96.14 
97.86 
 NLP+Biomed+ 
BOW+UMLS 
AdaB 
SVM 
53.04 
85.14 
AdaB 
SVM 
72.32 
87.10 
J48 
NB 
96.32 
98.55 
Prevent NLP+Biomed AdaB 
NB 
0 
17.02 
AdaB,J48 
NB 
0 
22.86 
Ada,J48 
CNB 
0 
55.17 
 BOW CNB 
NB 
31.78 
50 
J48 
NB 
0 
61.9 
SVM 
CNB 
50 
89.47 
 UMLS AdaB 
NB 
0 
28.57 
J48 
SVM 
0 
48.28 
J48 
CNB 
0 
68.75 
 BOW+UMLS J48 
NB 
39.02 
57.14 
J48 
NB 
9.09 
75.68 
AdaB 
CNB 
60 
89.47 
 NLP+Biomed 
+UMLS 
AdaB 
SVM 
0 
62.50 
J48 
SVM 
16 
57.69 
J48 
CNB 
0 
97.56 
 NLP+Biomed 
+BOW 
SVM 
NB 
35 
54.90 
J48 
NB 
0 
66.67 
AdaB 
CNB 
64.52 
92.31 
 NLP+Biomed+ 
BOW+UMLS 
J48 
NB 
30.77 
62.30 
J48 
SVM 
0 
77.78 
AdaB,J48 
NB 
64.52 
100 
Side 
Effect 
NLP+Biomed AdaB 
NB,CNB 
0 
7.69 
J48,SVM 
AdaB 
0 
18.18 
AdaB,J48 
CNB 
0 
33.33 
 BOW AdaB 
NB 
0 
34.48 
AdaB,J48 
NB 
0 
50 
Ada,J48 
CNB 
0 
66.67 
 UMLS AdaB,J48,
SVM NB 
0 
22.22 
J48,SVM 
NB 
0 
33.33 
AdaB,J48 
NB,CNB 
0 
46.15 
 BOW+UMLS AdaB,J48 
NB 
0 
21.43 
J48 
NB 
0 
47 
AdaB 
CNB 
0 
75 
 NLP+Biomed+ 
UMLS 
AdaB,J48 
NB 
0 
19.35 
J48 
NB 
0 
31.58 
AdaB.J48 
NB,CNB 
0 
46.15 
 NLP+Biomed+ 
BOW 
AdaB,J48 
NB 
0 
33.33 
J48 
NB 
0 
55.56 
AdaB,J48 
CNB 
0 
88.89 
 NLP+Biomed+ 
BOW+UMLS 
AdaB,J48 
NB 
0 
24 
J48 
NB 
0 
46.15 
AdaB 
CNB 
0 
88.89 
Table A1. Results obtained with our methods. 
The Representation column describes all the feature representation techniques that we tried. The acro-
nym NLP stands from verb and noun-phrase features put together and Biomed for bio-medical con-
cepts (the ones extracted by Genia tagger). The first line of results for every representation technique 
presents the classier that obtained the lowest results, while the second line represents the classifier 
with the best F-measure score. In bold we mark the best scores for all semantic relations in each of the 
three settings. 
 
98
