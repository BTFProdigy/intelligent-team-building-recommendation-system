Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394?1404,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Experimental Support for a Categorical Compositional
Distributional Model of Meaning
Edward Grefenstette
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
edward.grefenstette@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
mehrs@cs.ox.ac.uk
Abstract
Modelling compositional meaning for sen-
tences using empirical distributional methods
has been a challenge for computational lin-
guists. We implement the abstract categorical
model of Coecke et al (2010) using data from
the BNC and evaluate it. The implementation
is based on unsupervised learning of matrices
for relational words and applying them to the
vectors of their arguments. The evaluation is
based on the word disambiguation task devel-
oped by Mitchell and Lapata (2008) for intran-
sitive sentences, and on a similar new experi-
ment designed for transitive sentences. Our
model matches the results of its competitors
in the first experiment, and betters them in the
second. The general improvement in results
with increase in syntactic complexity show-
cases the compositional power of our model.
1 Introduction
As competent language speakers, we humans can al-
most trivially make sense of sentences we?ve never
seen or heard before. We are naturally good at un-
derstanding ambiguous words given a context, and
forming the meaning of a sentence from the mean-
ing of its parts. But while human beings seem
comfortable doing this, machines fail to deliver.
Search engines such as Google either fall back on
bag of words models?ignoring syntax and lexical
relations?or exploit superficial models of lexical
semantics to retrieve pages with terms related to
those in the query (Manning et al, 2008).
However, such models fail to shine when it comes
to processing the semantics of phrases and sen-
tences. Discovering the process of meaning as-
signment in natural language is among the most
challenging and foundational questions of linguis-
tics and computer science. The findings thereof will
increase our understanding of cognition and intelli-
gence and shall assist in applications to automating
language-related tasks such as document search.
Compositional type-logical approaches (Mon-
tague, 1974; Lambek, 2008) and distributional mod-
els of lexical semantics (Schutze, 1998; Firth, 1957)
have provided two partial orthogonal solutions to the
question. Compositional formal semantic models
stem from classical ideas from mathematical logic,
mainly Frege?s principle that the meaning of a sen-
tence is a function of the meaning of its parts (Frege,
1892). Distributional models are more recent and
can be related to Wittgenstein?s later philosophy of
?meaning is use?, whereby meanings of words can be
determined from their context (Wittgenstein, 1953).
The logical models relate to well known and robust
logical formalisms, hence offering a scalable theory
of meaning which can be used to reason inferen-
tially. The distributional models have found their
way into real world applications such as thesaurus
extraction (Grefenstette, 1994; Curran, 2004) or au-
tomated essay marking (Landauer, 1997), and have
connections to semantically motivated information
retrieval (Manning et al, 2008). This two-sortedness
of defining properties of meaning: ?logical form?
versus ?contextual use?, has left the quest for ?what is
the foundational structure of meaning?? even more
of a challenge.
Recently, Coecke et al (2010) used high level
cross-disciplinary techniques from logic, category
1394
theory, and physics to bring the above two ap-
proaches together. They developed a unified mathe-
matical framework whereby a sentence vector is by
definition a function of the Kronecker product of its
word vectors. A concrete instantiation of this the-
ory was exemplified on a toy hand crafted corpus
by Grefenstette et al (2011). In this paper we imple-
ment it by training the model over the entire BNC.
The highlight of our implementation is that words
with relational types, such as verbs, adjectives, and
adverbs are matrices that act on their arguments. We
provide a general algorithm for building (or indeed
learning) these matrices from the corpus.
The implementation is evaluated against the task
provided by Mitchell and Lapata (2008) for disam-
biguating intransitive verbs, as well as a similar new
experiment for transitive verbs. Our model improves
on the best method evaluated in Mitchell and Lapata
(2008) and offers promising results for the transitive
case, demonstrating its scalability in comparison to
that of other models. But we still feel there is need
for a different class of experiments to showcase mer-
its of compositionality in a statistically significant
manner. Our work shows that the categorical com-
positional distributional model of meaning permits
a practical implementation and that this opens the
way to the production of large scale compositional
models.
2 Two Orthogonal Semantic Models
Formal Semantics To compute the meaning of a
sentence consisting of n words, meanings of these
words must interact with one another. In formal se-
mantics, this further interaction is represented as a
function derived from the grammatical structure of
the sentence, but meanings of words are amorphous
objects of the domain: no distinction is made be-
tween words that have the same type. Such models
consist of a pairing of syntactic interpretation rules
(in the form of a grammar) with semantic interpreta-
tion rules, as exemplified by the simple model pre-
sented in Figure 1.
The parse of a sentence such as ?cats like milk?
typically produces its semantic interpretation by
substituting semantic representation for their gram-
matical constituents and applying ?-reduction where
needed. Such a derivation is shown in Figure 2.
Syntactic Analysis Semantic Interpretation
S? NP VP |V P |(|NP |)
NP? cats, milk, etc. |cats|, |milk|, . . .
VP? Vt NP |V t|(|NP |)
Vt? like, hug, etc. ?yx.|like|(x, y), . . .
Figure 1: A simple model of formal semantics.
|like|(|cats|, |milk|)
|cats| ?x.|like|(x, |milk|)
|milk| ?yx.|like|(x, y)
Figure 2: A parse tree showing a semantic derivation.
This methodology is used to translate sentences
of natural language into logical formulae, then use
computer-aided automation tools to reason about
them (Alshawi, 1992). One major drawback is that
the result of such analysis can only deal with truth
or falsity as the meaning of a sentence, and says
nothing about the closeness in meaning or topic of
expressions beyond their truth-conditions and what
models satisfy them, hence do not perform well on
language tasks such as search. Furthermore, an un-
derlying domain of objects and a valuation function
must be provided, as with any logic, leaving open
the question of how we might learn the meaning of
language using such a model, rather than just use it.
Distributional Models Distributional models of
semantics, on the other hand, dismiss the interaction
between syntactically linked words and are solely
concerned with lexical semantics. Word meaning
is obtained empirically by examining the contexts1
in which a word appears, and equating the meaning
of a word with the distribution of contexts it shares.
The intuition is that context of use is what we ap-
peal to in learning the meaning of a word, and that
words that frequently have the same sort of context
in common are likely to be semantically related.
For instance, beer and sherry are both drinks, al-
coholic, and often cause a hangover. We expect
these facts to be reflected in a sufficiently large cor-
pus: the words ?beer? and ?sherry? occur within the
1E.g. words which appear in the same sentence or n-word
window, or words which hold particular grammatical or depen-
dency relations to the word being learned.
1395
context of identifying words such as ?drink?, ?alco-
holic? and ?hangover? more frequently than they oc-
cur with other content words.
Such context distributions can be encoded as vec-
tors in a high dimensional space with contexts as
basis vectors. For any word vector ???word, the scalar
weight cwordi associated with each context basis vec-
tor ??ni is a function of the number of times the
word has appeared in that context. Semantic vectors
(cword1 , cword2 , ? ? ? , cwordn ) are also denoted by sums
of such weight/basis vector pairs:
???word =
?
i
cwordi ??ni
Learning a semantic vector is just learning its ba-
sis weights from the corpus. This setting offers ge-
ometric means to reason about semantic similarity
(e.g. via cosine measure or k-means clustering), as
discussed in Widdows (2005).
The principal drawback of such models is their
non-compositional nature: they ignore grammatical
structure and logical words, and hence cannot com-
pute the meanings of phrases and sentences in the
same efficient way that they do for words. Com-
mon operations discussed in (Mitchell and Lapata,
2008) such as vector addition (+) and component-
wise multiplication (, cf. ?4 for details) are com-
mutative, hence if ??vw = ??v + ??w or ??v  ??w , then
??vw = ??wv, leading to unwelcome equalities such as
??????????????the dog bit the man = ??????????????the man bit the dog
Non-commutative operations, such as the Kronecker
product (cf. ?4 for definition) can take word-order
into account (Smolensky, 1990) or even some more
complex syntactic relations, as described in Clark
and Pulman (2007). However, the dimensionality of
sentence vectors produced in this manner differs for
sentences of different length, barring all sentences
from being compared in the same vector space, and
growing exponentially with sentence length hence
quickly becoming computationally intractable.
3 A Hybrid Logico-Distributional Model
Whereas semantic compositional mechanisms for
set-theoretic constructions are well understood,
there are no obvious corresponding methods for vec-
tor spaces. To solve this problem, Coecke et al
(2010) use the abstract setting of category theory to
turn the grammatical structure of a sentence into a
morphism compatible with the higher level logical
structure of vector spaces.
One pragmatic consequence of this abstract idea
is as follows. In distributional models, there is a
meaning vector for each word, e.g. ??cats, ??like, and???milk. The logical recipe tells us to apply the mean-
ing of the verb to the meanings of subject and object.
But how can a vector apply to other vectors? The so-
lution proposed above implies that one needs to have
different levels of meaning for words with different
types. This is similar to logical models where verbs
are relations and nouns are atomic sets. So verb vec-
tors should be built differently from noun vectors,
for instance as matrices.
The general information as to which words should
be matrices and which words atomic vectors is in
fact encoded in the type-logical representation of the
grammatical structure of the sentence. This is the
linear map with word vectors as input and sentence
vectors as output. Hence, at least theoretically, one
should be able to build sentence vectors and com-
pare their synonymity in exactly the same way as
one measures word synonymity.
Pregroup Grammars The aforementioned linear
maps turn out to be the grammatical reductions
of a type-logic called a Lambek pregroup gram-
mar (Lambek, 2008)2. Pregroups and vector spaces
share the same high level mathematical structure, re-
ferred to as a compact closed category, for a proof
and details of this claim see Coecke et al (2010); for
a friendly introduction to category theory, see Co-
ecke and Paquette (2011). One consequence of this
parity is that the grammatical reductions of a pre-
group grammar can be directly transformed into lin-
ear maps that act on vectors.
In a nutshell, pregroup types are either atomic
or compound. Atomic types can be simple (e.g. n
for noun phrases, s for statements) or left/right
superscripted?referred to as adjoint types (e.g. nr
and nl). An example of a compound type is that of
a verb nrsnl. The superscripted types express that
the verb is a relation with two arguments of type n,
2The usage of pregroup types is not essential, the types of
any other logic, for instance CCG can be used, but should be
translated into the language of pregroups.
1396
which have to occur to the right and to the left of
it, and that it outputs an argument of the type s. A
transitive sentence has types as shown in Figure 3.
Each type n cancels out with its right adjoint nr
from the right and its left adjoint nl from the left;
mathematically speaking these mean3
nln ? 1 and nnr ? 1
Here 1 is the unit of concatenation: 1n = n1 =
n. The corresponding grammatical reduction of a
transitive sentence is nnrsnl ? 1s1 = s. Each such
reduction can be depicted as a wire diagram. The
diagram of a transitive sentence is shown in Figure 3.
Cats
n
like
nr s nl
milk.
n
Figure 3: The pregroup types and reduction diagram for
a transitive sentence.
Syntax-guided Semantic Composition Accord-
ing to Coecke et al (2010) and based on a general
completeness theorem between compact categories,
wire diagrams, and vector spaces, the meaning of
sentences can be canonically reduced to linear alge-
braic formulae. The following is the meaning vector
of our transitive sentence:
?????????cats like milk = (f)
(??cats???like????milk
)
(I)
Here f is the linear map that encodes the grammati-
cal structure. The categorical morphism correspond-
ing to it is denoted by the tensor product of 3 compo-
nents: V ?1S?W , where V andW are subject and
object spaces, S is the sentence space, the ?s are the
cups, and 1S is the straight line in the diagram. The
cups stand for taking inner products, which when
done with the basis vectors imitate substitution. The
straight line stands for the identity map that does
nothing. By the rules of the category, equation (I) re-
duces to the following linear algebraic formula with
3The relation? is the partial order of the pregroup. It corre-
sponds to implication =? in a logical reading thereof. If these
inequalities are replaced by equalities, i.e. if nln = 1 = nnr ,
then the pregroup collapses into a group where nl = nr .
lower dimensions, hence the dimensional explosion
problem for Kronecker products is avoided:
?
itj
citj???cats|??vi ???st ???wj|???milk? ? S (II)
??vi ,??wj are basis vectors of V and W . The inner
product ???cats | ??vi ? substitutes the weights of ??cats
into the first argument place of the verb (similarly
for object and second argument place). ??st is a basis
vector of the sentence space S in which meanings of
sentences live, regardless of their grammatical struc-
ture.
The degree of synonymity of sentences is ob-
tained by taking the cosine measure of their vectors.
S is an abstract space: it needs to be instantiated
to provide concrete meanings and synonymity mea-
sures. For instance, a truth-theoretic model is ob-
tained by taking the sentence space S to be the 2-
dimensional space with basis vectors |1? (True) and
|0? (False).
4 Building Matrices for Relational Words
In this section we present a general scheme to build
matrices for relational words. Recall that given
a vector space A with basis {??ni}i, the Kronecker
product of two vectors ??v = ?i cai??ni and ??w =?
i cbi??ni is defined as follows:
??v ???w =
?
ij
cai cbj (??ni ???nj)
where (??ni ???nj) is just the pairing of the basis of A,
i.e. (??ni ,??nj). The Kronecker product vectors belong
in the tensor product of A with itself: A?A, hence
ifA has dimension r, these will be of dimensionality
r?r. The point-wise multiplication of these vectors
is defined as follows
??v ??w =
?
i
cai cbi ??ni
The intuition behind having a matrix for a rela-
tional word is that any relation R on sets X and Y ,
i.e. R ? X ? Y can be represented as a matrix,
namely one that has as row-bases x ? X and as
column-bases y ? Y , with weight cxy = 1 where
(x, y) ? R and 0 otherwise. In a distributional set-
ting, the weights, which are natural or real numbers,
1397
will represent more: ?the extent according to which
x and y are related?. This can be determined in dif-
ferent ways.
Suppose X is the set of animals, and ?chase? is a
relation on it: chase ? X ? X . Take x = ?dog?
and y = ?cat?: with our type-logical glasses on, the
obvious choice would be to take cxy to be the num-
ber of times ?dog? has chased ?cat?, i.e. the number
of times the sentence ?the dog chases the cat? has
appeared in the corpus. But in the distributional set-
ting, this method will be too syntactic and dismissive
of the actual meaning of ?cat? and ?dog?. If instead
the corpus contains the sentence ?the hound hunted
the wild cat?, cxy will be 0, restricting us to only
assign meaning to sentences that have directly ap-
peared in the corpus. We propose to, instead, use a
level of abstraction by taking words such as verbs to
be distributions over the semantic information in the
vectors of their context words, rather than over the
context words themselves.
Start with an r-dimensional vector space N with
basis {??n i}i, in which meaning vectors of atomic
words, such as nouns, live. The basis vectors of N
are in principle all the words from the corpus, how-
ever in practice and following Mitchell and Lapata
(2008) we had to restrict these to a subset of the
most occurring words. These basis vectors are not
restricted to nouns: they can as well be verbs, adjec-
tives, and adverbs, so that we can define the mean-
ing of a noun in all possible contexts?as is usual
in context-based models?and not only in the con-
text of other nouns. Note that basis words with re-
lational types are treated as pure lexical items rather
than as semantic objects represented as matrices. In
short, we count how many times a noun has occurred
close to words of other syntactic types such as ?elect?
and ?scientific?, rather than count how many times it
has occurred close to their corresponding matrices:
it is the lexical tokens that form the context, not their
meaning.
Each relational word P with grammatical type pi
and m adjoint types ?1, ?2, ? ? ? , ?m is encoded as
an (r ? . . .? r) matrix with m dimensions. Since
our vector space N has a fixed basis, each such ma-
trix is represented in vector form as follows:
??P =
?
ij ? ? ? ?? ?? ?
m
cij???? (??n i ???n j ? ? ? ? ? ??n ?)? ?? ?
m
This vector lives in the tensor space
N ?N ? ? ? ? ?N? ?? ?
m
. Each cij???? is computed
according to the procedure described in Figure 4.
1) Consider a sequence of words containing a re-
lational word ?P? and its arguments w1, w2, ? ? ? ,
wm, occurring in the same order as described in
P?s grammatical type pi. Refer to these sequences
as ?P?-relations. Suppose there are k of them.
2) Retrieve the vector ??w l of each argument wl.
3) Suppose w1 has weight c1i on basis vector ??n i,
w2 has weight c2j on basis vector ??n j , ? ? ? , and
wm has weight cm? on basis vector ??n ? . Multiply
these weights
c1i ? c2j ? ? ? ? ? cm?
4) Repeat the above steps for all the k ?P?-
relations, and suma the corresponding weights
cij???? =
?
k
(
c1i ? c2j ? ? ? ? ? cm?
)
k
aWe also experimented with multiplication, but the spar-
sity of noun vectors resulted in most verb matrices being
empty.
Figure 4: Procedure for learning weights for matrices of
words ?P? with relational types pi of m arguments.
Linear algebraically, this procedure corresponds to
computing the following
??P =
?
k
(??w 1 ???w 2 ? ? ? ? ? ??wm
)
k
Type-logical examples of relational words are
verbs, adjectives, and adverbs. A transitive verb is
represented as a 2 dimensional matrix since its type
is nrsnl with two adjoint types nr and nl. The cor-
responding vector of this matrix is
???verb =
?
ij
cij (??n i ???n j)
1398
The weight cij corresponding to basis vector??n i???n j , is the extent according to which words that have
co-occurred with ??n i have been the subject of the
?verb? and words that have co-occurred with ??n j
have been the object of the ?verb?. This example
computation is demonstrated in Figure 5.
1) Consider phrases containing ?verb?, its subject
w1 and object w2. Suppose there are k of them.
2) Retrieve vectors ??w 1 and ??w 2.
3) Suppose ??w 1 has weight c1i on ??n i and ??w 2 has
c2j on ??n j . Multiply these weights c1i ? c2j .
4) Repeat the above steps for all k ?verb?-
relations and sum the corresponding weights?
k(c1i ? c2j )k
Figure 5: Procedure for learning weights for matrices of
transitive verbs.
Linear algebraically, we are computing
???verb =
?
k
(??w 1 ???w 2
)
k
As an example, consider the verb ?show? and sup-
pose there are two ?show?-relations in the corpus:
s1 = table show result
s2 = map show location
The vector of ?show? is
???show = ???table?????result + ???map??????location
Consider an N space with four basis vectors ?far?,
?room?, ?scientific?, and ?elect?. The TF/IDF-
weighted values for vectors of the above four nouns
(built from the BNC) are as shown in Table 1.
i ??ni table map result location
1 far 6.6 5.6 7 5.9
2 room 27 7.4 0.99 7.3
3 scientific 0 5.4 13 6.1
4 elect 0 0 4.2 0
Table 1: Sample weights for selected noun vectors.
Part of the matrix of ?show? is presented in Table 2.
As a sample computation, the weight c11 for
vector (1, 1), i.e. (??far,??far) is computed by multiply-
ing weights of ?table? and ?result? on??far, i.e. 6.6?7,
far room scientific elect
far 79.24 47.41 119.96 27.72
room 232.66 80.75 396.14 113.2
scientific 32.94 31.86 32.94 0
elect 0 0 0 0
Table 2: Sample semantic matrix for ?show?.
multiplying weights of ?map? and ?location? on ??far,
i.e. 5.6 ? 5.9 then adding these 46.2 + 33.04 and
obtaining the total weight 79.24.
The same method is applied to build matrices for di-
transitive verbs, which will have 3 dimensions, and
adjectives and adverbs, which will be of 1 dimension
each.
5 Computing Sentence Vectors
Meaning of sentences are vectors computed by tak-
ing the variables of the categorical prescription of
meaning (the linear map f obtained from the gram-
matical reduction of the sentence) to be determined
by the matrices of the relational words. For instance
the meaning of the transitive sentence ?sub verb obj?
is:
?????????sub verb obj =
?
itj
???sub | ??v i????w j | ??obj? citj??s t
We take V := W := N and S = N ? N , then?
itj citj??s t is determined by the matrix of the verb,
i.e. substitute it by ?ij cij(??n i ? ??n j)4. Hence?????????sub verb obj becomes:
?
ij
???sub | ??n i????n j | ??obj?cij(??n i ???n j) =
?
ij
csubi cobjj cij(??n i ???n j)
This can be decomposed to point-wise multiplica-
tion of two vectors as follows:
(?
ij
csubi cobjj (??n i???n j)
)

(?
ij
cij(??n i???n j)
)
4Note that by doing so we are also reducing the verb space
from N ? (N ?N)?N to N ?N , since for our construction
we only need tuples of the form ??n i ???n i ???n j ???n j which
are isomorphic to pairs (??n i ???n j).
1399
The left argument is the Kronecker product of sub-
ject and object vectors and the right argument is the
vector of the verb, so we obtain
(??sub???obj
)
???verb
Since  is commutative, this provides us with a dis-
tributional version of the type-logical meaning of the
sentence: point-wise multiplication of the meaning
of the verb to the Kronecker product of its subject
and object:
?????????sub verb obj = ???verb
(??sub???obj
)
This mathematical operation can be informally de-
scribed as a structured ?mixing? of the information
of the subject and object, followed by it being ?fil-
tered? through the information of the verb applied
to them, in order to produce the information of the
sentence.
In the transitive case, S = N ? N , hence ??s t =??n i ? ??n j . More generally, the vector space cor-
responding to the abstract sentence space S is the
concrete tensor space (N ? . . .?N) for m the di-
mension of the matrix of the ?verb?. As we have
seen above, in practice we do not need to build this
tensor space, as the computations thereof reduce to
point-wise multiplications and summations.
Similar computations yield meanings of sentences
with adjectives and adverbs. For instance the mean-
ing of a transitive sentence with a modified subject
and a modified verb we have
??????????????adj sub verb obj adv =
(??adv???verb
)

((??adj??sub
)
???obj
)
After building vectors for sentences, we can com-
pare their meaning and measure their degree of syn-
onymy by taking their cosine measure.
6 Evaluation
Evaluating such a framework is no easy task. What
to evaluate depends heavily on what sort of applica-
tion a practical instantiation of the model is geared
towards. In (Grefenstette et al, 2011), it is sug-
gested that the simplified model we presented and
expanded here could be evaluated in the same way as
lexical semantic models, measuring compositionally
built sentence vectors against a benchmark dataset
such as that provided by Mitchell and Lapata (2008).
In this section, we briefly describe the evaluation of
our model against this dataset. Following this, we
present a new evaluation task extending the experi-
mental methodology of Mitchell and Lapata (2008)
to transitive verb-centric sentences, and compare our
model to those discussed by Mitchell and Lapata
(2008) within this new experiment.
First Dataset Description The first experiment,
described in detail by Mitchell and Lapata (2008),
evaluates how well compositional models disam-
biguate ambiguous words given the context of a po-
tentially disambiguating noun. Each entry of the
dataset provides a noun, a target verb and landmark
verb (both intransitive). The noun must be com-
posed with both verbs to produce short phrase vec-
tors the similarity of which is measured by the can-
didate. Also provided with each entry is a classifi-
cation (?High? or ?Low?) indicating whether or not
the verbs are indeed semantically close within the
context of the noun, as well as an evaluator-set simi-
larity score between 1 and 7 (along with an evaluator
identifier), where 1 is low similarity and 7 is high.
Evaluation Methodology Candidate models pro-
vide a similarity score for each entry. The scores
of high similarity entries and low similarity entries
are averaged to produce a mean High score and
mean Low score for the model. The correlation of
the model?s similarity judgements with the human
judgements is also calculated using Spearman?s ?, a
metric which is deemed to be more scrupulous, and
ultimately that by which models should be ranked,
by Mitchell and Lapata (2008). The mean for each
model is on a [0, 1] scale, except for UpperBound
which is on the same [1, 7] scale the annotators used.
The ? scores are on a [?1, 1] scale. It is assumed
that inter-annotator agreement provides the theoret-
ical maximum ? for any model for this experiment.
The cosine measure of the verb vectors, ignoring the
noun, is taken to be the baseline (no composition).
Other Models The other models we compare
ours to are those evaluated by Mitchell and Lap-
ata (2008). We provide a selection of the results
1400
from that paper for the worst (Add) and best5 (Mul-
tiply) performing models, as well as the previous
second-best performing model (Kintsch). The ad-
ditive and multiplicative models are simply applica-
tions of vector addition and component-wise multi-
plication. We invite the reader to consult (Mitchell
and Lapata, 2008) for the description of Kintsch?s
additive model and parametric choices.
Model Parameters To provide the most accurate
comparison with the existing multiplicative model,
and exploiting the aforementioned feature that the
categorical model can be built ?on top of? existing
lexical distributional models, we used the parame-
ters described by Mitchell and Lapata (2008) to re-
produce the vectors evaluated in the original exper-
iment as our noun vectors. All vectors were built
from a lemmatised version of the BNC. The noun
basis was the 2000 most common context words,
basis weights were the probability of context words
given the target word divided by the overall proba-
bility of the context word. Intransitive verb function-
vectors were trained using the procedure presented
in ?4. Since the dataset only contains intransitive
verbs and nouns, we used S = N . The cosine mea-
sure of vectors was used as a similarity metric.
First Experiment Results In Table 3 we present
the comparison of the selected models. Our categor-
ical model performs significantly better than the ex-
isting second-place (Kintsch) and obtains a ? quasi-
identical to the multiplicative model, indicating sig-
nificant correlation with the annotator scores.
There is not a large difference between the mean
High score and mean Low score, but the distri-
bution in Figure 6 shows that our model makes a
non-negligible distinction between high similarity
phrases and low similarity phrases, despite the ab-
solute scores not being different by more than a few
percentiles.
5The multiplicative model presented here is what is quali-
fied as best in (Mitchell and Lapata, 2008). However, they also
present a slightly better performing (? = 0.19) model which
is a combination of their multiplicative model and a weighted
additive model. The difference in ? is qualified as ?not sta-
tistically significant? in the original paper, and furthermore the
mixed model requires parametric optimisation hence was not
evaluated against the entire test set. For these reasons, we chose
not to include it in the comparison.
Model High Low ?
Baseline 0.27 0.26 0.08
Add 0.59 0.59 0.04
Kintsch 0.47 0.45 0.09
Multiply 0.42 0.28 0.17
Categorical 0.84 0.79 0.17
UpperBound 4.94 3.25 0.40
Table 3: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, first experiment (Mitchell and Lapata, 2008). p <
0.05 for each ?.
High Low0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 6: Distribution of predicted similarities for the cat-
egorical distributional model on High and Low similarity
items.
Second Dataset Description The second dataset6,
developed by the authors, follows the format of the
(Mitchell and Lapata, 2008) dataset used for the first
experiment, with the exception that the target and
landmark verbs are transitive, and an object noun
is provided in addition to the subject noun, hence
forming a small transitive sentence. The dataset
comprises 200 entries consisting of sentence pairs
(hence a total of 400 sentences) constructed by fol-
lowing the procedure outlined in ?4 of (Mitchell and
Lapata, 2008), using transitive verbs from CELEX7.
For examples of these sentences, see Table 4. The
dataset was split into four sections of 100 entries
each, with guaranteed 50% exclusive overlap with
6http://www.cs.ox.ac.uk/activities/CompD
istMeaning/GS2011data.txt
7http://celex.mpi.nl/
1401
exactly two other datasets. Each section was given
to a group of evaluators, with a total of 25, who were
asked to form simple transitive sentence pairs from
the verbs, subject and object provided in each entry;
for instance ?the table showed the result? from ?table
show result?. The evaluators were then asked to rate
the semantic similarity of each verb pair within the
context of those sentences, and offer a score between
1 and 7 for each entry. Each entry was given an arbi-
trary classification of HIGH or LOW by the authors,
for the purpose of calculating mean high/low scores
for each model. For example, the first two pairs in
table 4 were classified as HIGH, whereas the second
two pairs as LOW.
Sentence 1 Sentence 2
table show result table express result
map show location map picture location
table show result table picture result
map show location map express location
Table 4: Example entries from the transitive dataset with-
out annotator score, second experiment.
Evaluation Methodology The evaluation
methodology for the second experiment was
identical to that of the first, as are the scales for
means and scores. Here also, Spearman?s ? is
deemed a more rigorous way of determining how
well a model tracks difference in meaning. This is
both because of the imprecise nature of the classifi-
cation of verb pairs as HIGH or LOW; and since the
objective similarity scores produced by a model that
distinguishes sentences of different meaning from
those of similar meaning can be renormalised in
practice. Therefore the delta between HIGH means
and LOW mean cannot serve as a definite indication
of the practical applicability (or lack thereof) of
semantic models; the means are provided just to aid
comparison with the results of the first experiment.
Model Parameters As in the first experiment, the
lexical vectors from (Mitchell and Lapata, 2008)
were used for the other models evaluated (additive,
multiplicative and baseline)8 and for the noun vec-
8Kintsch was not evaluated as it required optimising model
parameters against a held-out segment of the test set, and we
could not replicate the methodology of Mitchell and Lapata
tors of our categorical model. Transitive verb vec-
tors were trained as described in ?4 with S = N?N .
Second Experiment Results The results for the
models evaluated against the second dataset are pre-
sented in Table 5.
Model High Low ?
Baseline 0.47 0.44 0.16
Add 0.90 0.90 0.05
Multiply 0.67 0.59 0.17
Categorical 0.73 0.72 0.21
UpperBound 4.80 2.49 0.62
Table 5: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, second experiment. p < 0.05 for each ?.
We observe a significant (according to p < 0.0.5)
improvement in the alignment of our categorical
model with the human judgements, from 0.17 to
0.21. The additive model continues to make lit-
tle distinction between senses of the verb during
composition, and the multiplicative model?s align-
ment does not change, but becomes statistically in-
distinguishable from the non-compositional baseline
model.
Once again we note that the high-low means are
not very indicative of model performance, as the dif-
ference between high mean and the low mean of the
categorical model is much smaller than that of the
both the baseline model and multiplicative model,
despite better alignment with annotator judgements.
7 Discussion
In this paper, we described an implementation of the
categorical model of meaning (Coecke et al, 2010),
which combines the formal logical and the empiri-
cal distributional frameworks into a unified seman-
tic model. The implementation is based on build-
ing matrices for words with relational types (ad-
jectives, verbs), and vectors for words with atomic
types (nouns), based on data from the BNC. We
then show how to apply verbs to their subject/object,
in order to compute the meaning of intransitive and
transitive sentences.
(2008) with full confidence.
1402
Other work uses matrices to model meaning (Ba-
roni and Zamparelli, 2010; Guevara, 2010), but only
for adjective-noun phrases. Our approach easily ap-
plies to such compositions, as well as to sentences
containing combinations of adjectives, nouns, verbs,
and adverbs. The other key difference is that they
learn their matrices in a top-down fashion, i.e. by re-
gression from the composite adjective-noun context
vectors, whereas our model is bottom-up: it learns
sentence/phrase meaning compositionally from the
vectors of the compartments of the composites. Fi-
nally, very similar functions, for example a verb with
argument alternations such as ?break? in ?Y breaks?
and ?X breaks Y?, are not treated as unrelated. The
matrix of the intransitive ?break? uses the corpus-
observed information about the subject of break, in-
cluding that of ?Y?, similarly the matrix of the tran-
sitive ?break? uses information about its subject and
object, including that of ?X? and ?Y?. We leave a
thorough study of these phenomena, which fall un-
der providing a modular representation of passive-
active similarities, to future work.
We evaluated our model in two ways: first against
the word disambiguation task of Mitchell and Lap-
ata (2008) for intransitive verbs, and then against a
similar new experiment for transitive verbs, which
we developed.
Our findings in the first experiment show that
the categorical method performs on par with the
leading existing approaches. This should not sur-
prise us given that the context is so small and our
method becomes similar to the multiplicative model
of Mitchell and Lapata (2008). However, our ap-
proach is sensitive to grammatical structure, lead-
ing us to develop a second experiment taking this
into account and differentiating it from models with
commutative composition operations.
The second experiment?s results deliver the ex-
pected qualitative difference between models, with
our categorical model outperforming the others and
showing an increase in alignment with human judge-
ments in correlation with the increase in sentence
complexity. We use this second evaluation princi-
pally to show that there is a strong case for the devel-
opment of more complex experiments measuring not
only the disambiguating qualities of compositional
models, but also their syntactic sensitivity, which is
not directly measured in the existing experiments.
These results show that the high level categori-
cal distributional model, uniting empirical data with
logical form, can be implemented just like any other
concrete model. Furthermore it shows better results
in experiments involving higher syntactic complex-
ity. This is just the tip of the iceberg: the mathe-
matics underlying the implementation ensures that
it uniformly scales to larger, more complicated sen-
tences and enables it to compare synonymity of sen-
tences that are of different grammatical structure.
8 Future Work
Treatment of function words such as ?that?, ?who?,
as well as logical words such as quantifiers and con-
junctives are left to future work. This will build
alongside the general guidelines of Coecke et al
(2010) and concrete insights from the work of Wid-
dows (2005). It is not yet entirely clear how ex-
isting set-theoretic approaches, for example that of
discourse representation and generalised quantifiers,
apply to our setting. Preliminary work on integration
of the two has been presented by Preller (2007) and
more recently also by Preller and Sadrzadeh ( 2009).
As mentioned by one of the reviewers, our pre-
group approach to grammar flattens the sentence
representation, in that the verb is applied to its sub-
ject and object at the same time; whereas in other
approaches such as CCG, it is first applied to the
object to produce a verb phrase, then applied to the
subject to produce the sentence. The advantages and
disadvantages of this method and comparisons with
other systems, in particular CCG, constitutes ongo-
ing work.
9 Acknowledgement
We wish to thank P. Blunsom, S. Clark, B. Coecke,
S. Pulman, and the anonymous EMNLP review-
ers for discussions and comments. Support from
EPSRC grant EP/F042728/1 is gratefully acknowl-
edged by M. Sadrzadeh.
References
H. Alshawi (ed). 1992. The Core Language Engine.
MIT Press.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices. Proceedings of Conference
1403
on Empirical Methods in Natural Language Processing
(EMNLP).
S. Clark and S. Pulman. 2007. Combining Symbolic
and Distributional Models of Meaning. Proceedings
of AAAI Spring Symposium on Quantum Interaction.
AAAI Press.
B. Coecke, and E. Paquette. 2011. Categories for the
Practicing Physicist. New Structures for Physics, 167-
271. B. Coecke (ed.). Lecture Notes in Physics 813.
Springer.
B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-
ical Foundations for Distributed Compositional Model
of Meaning. Lambek Festschrift. Linguistic Analysis
36, 345?384. J. van Benthem, M. Moortgat and W.
Buszkowski (eds.).
J. Curran. 2004. From Distributional to Semantic Simi-
larity. PhD Thesis, University of Edinburgh.
K. Erk and S. Pado?. 2004. A Structured Vector Space
Model for Word Meaning in Context. Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 897?906.
G. Frege 1892. U?ber Sinn und Bedeutung. Zeitschrift
fu?r Philosophie und philosophische Kritik 100.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis.
E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,
S. Pulman. 2011. Concrete Compositional Sentence
Spaces for a Compositional Distributional Model of
Meaning. International Conference on Computational
Semantics (IWCS?11). Oxford.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer.
E. Guevara. 2010. A Regression Model of Adjective-
Noun Compositionality in Distributional Semantics.
Proceedings of the ACL GEMS Workshop.
Z. S. Harris. 1966. A Cycling Cancellation-Automaton
for Sentence Well-Formedness. International Compu-
tation Centre Bulletin 5, 69?94.
R. Hudson. 1984. Word Grammar. Blackwell.
J. Lambek. 2008. From Word to Sentence. Polimetrica,
Milan.
T. Landauer, and S. Dumais. 2008. A solution to Platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to information retrieval. Cambridge Uni-
versity Press.
J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, 236?244.
R. Montague. 1974. English as a formal language. For-
mal Philosophy, 189?223.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
A. Preller. Towards Discourse Representation via Pre-
group Grammars. Journal of Logic Language Infor-
mation 16 173?194.
A. Preller and M. Sadrzadeh. Semantic Vector Mod-
els and Functional Models for Pregroup Grammars.
Journal of Logic Language Information. DOI:
10.1007/s10849-011-9132-2. to appear.
J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-
tion: The role of distributional cues. Journal of Mem-
ory and Language 35, 606?621.
H. Schuetze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics 24, 97?123.
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Computational Linguistics 46, 1?
2, 159?216.
M. Steedman. 2000. The Syntactic Process. MIT Press.
D. Widdows. 2005. Geometry and Meaning. University
of Chicago Press.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
1404
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590?1601,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Prior Disambiguation of Word Tensors
for Constructing Sentence Vectors
Dimitri Kartsaklis
University of Oxford
Department of
Computer Science
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrs@eecs.qmul.ac.uk
Abstract
Recent work has shown that compositional-
distributional models using element-wise op-
erations on contextual word vectors benefit
from the introduction of a prior disambigua-
tion step. The purpose of this paper is to
generalise these ideas to tensor-based models,
where relational words such as verbs and ad-
jectives are represented by linear maps (higher
order tensors) acting on a number of argu-
ments (vectors). We propose disambiguation
algorithms for a number of tensor-based mod-
els, which we then test on a variety of tasks.
The results show that disambiguation can pro-
vide better compositional representation even
for the case of tensor-based models. Further-
more, we confirm previous findings regarding
the positive effect of disambiguation on vec-
tor mixture models, and we compare the ef-
fectiveness of the two approaches.
1 Introduction
Distributional models of meaning have been proved
extremely useful for a number of natural language
processing tasks, ranging from thesaurus extraction
(Curran, 2004) to topic modelling (Landauer and
Dumais, 1997) and information retrieval (Manning
et al, 2008), to name just a few. These models
are based on the distributional hypothesis of Har-
ris (1968), which states that the meaning of a word
depends on its context. This idea allows the words
to be represented by vectors of statistics collected
from a sufficiently large corpus of text; each ele-
ment of the vector reflects how many times a word
co-occurs in the same context with another word
of the vocabulary. However, due to the generative
power of natural language, which is able to pro-
duce infinite new structures from a finite set of re-
sources (words), no text corpus, regardless of its
size, can provide reliable distributional representa-
tions for anything longer than single words or per-
haps very short phrases consisting of two words; in
other words, this technique cannot scale up to the
phrase or sentence level.
Much research activity has been recently dedi-
cated to provide a solution to this problem: although
the direct construction of a sentence vector is not
possible, we might still be able to synthetically cre-
ate such a vectorial representation by somehow com-
posing the vectors of the words that comprise the
sentence. Towards this goal, researchers have em-
ployed a variety of approaches that roughly fall into
two general categories. Following an influential
work (Mitchell and Lapata, 2008), the models in the
first category compute a sentence vector as a mix-
ture of the original word vectors, using simple oper-
ations such as element-wise multiplication and ad-
dition; we refer to these models as vector mixtures.
The main characteristic of these models is that they
do not distinguish between the type-logical identi-
ties of the different words: an intransitive verb, for
example, is of the same order as its subject (a noun),
and both will contribute equally to the composite
sentence vector.
However, this symmetric treatment of composi-
tion seems unjustified from a formal semantics point
of view. Words with special meanings, such as verbs
and adjectives, are usually seen as functions acting
on, hence modifying, a number of arguments rather
than lexical units of the same order as them; an
adjective, for example, is a function that returns a
modified version of its input noun. Inspired from
1590
this more-aligned-to-formal-semantics view, a sec-
ond research direction aims to represent relational
words as linear maps (tensors of various orders)
that can be applied to one or more arguments (vec-
tors). Baroni and Zamparelli (2010), for example,
model adjectives as matrices which, when matrix-
multiplied with a noun vector, will produce a vec-
torial representation of the specific adjective-noun
compound. The notion of a framework where re-
lational words are entities living in vector spaces of
higher order than nouns, which are simple vectors,
has been formalized by Coecke et al (2010) in the
context of the abstract mathematical framework of
compact closed categories. We refer to this class of
models as tensor-based.
Regardless of the way they approach the repre-
sentation of relational words and their composition
operation, however, most current compositional-
distributional models do share a common feature:
they all rely on ambiguous vector representations,
where all the senses of a polysemous word, such as
the verb ?file? (which can mean register or smooth),
are merged into the same vector or tensor. At least
for the vector mixture approach, this practice has
been proved suboptimal: Reddy et al (2011) and
Kartsaklis et al (2013) test a number of simple mul-
tiplicative and additive models using disambiguated
vector representations on various tasks, showing that
the introduction of a disambiguation step prior to ac-
tual composition can indeed increase the quality of
the composite vectors. However, the fact that disam-
biguation can be beneficial for models based on vec-
tor mixtures is not very surprising. Both additive and
multiplicative compositions are but a kind of average
of the vectors of the words in the sentence, hence
can directly benefit from the provision of more ac-
curate starting points. Perhaps a more interesting
question, and one that the current paper aims to ad-
dress, is to what extent disambiguation can also pro-
vide benefits for tensor-based approaches, which in
general constitute more powerful models for natural
language (see discussion in Section 2).
Specifically, this paper aims to: (a) propose dis-
ambiguation algorithms for a number of tensor-
based distributional models; (b) examine the effect
of disambiguation on tensors for relational words;
and (c) meaningfully compare the effectiveness of
tensor-based against vector mixture models in a
number of tasks. Based on the generic procedure of
Schu?tze (1998), we propose algorithms for a num-
ber of tensor-based models, where the composition
is modelled as the application of linear maps (ten-
sor contractions). Following Mitchell and Lapata
(2008) and many others, we test our models on
two disambiguation tasks similar to that of Kintsch
(2001), and on the phrase similarity task introduced
in (Mitchell and Lapata, 2010). In almost every
case, the results show that disambiguation can make
a great difference in the case of tensor-based models;
they also reconfirm previous findings regarding the
effectiveness of the method for simple vector mix-
ture models.
2 Vectors vs tensors
The simple models of Mitchell and Lapata (2008)
constitute the easiest and perhaps the most intuitive
way of composing two or more vectors: each ele-
ment of the resulting vector is computed as the sum
or the product of the corresponding elements in the
input vectors (left part in Figure 1). In the case of
addition, the components of the output vector are
simply the cumulative scores of the corresponding
input components. So in a sense the output element
embraces both input elements, resembling a union of
the input features. On the other hand, the element-
wise multiplication of two vectors can be seen as the
intersection of their features: a zero element in one
of the input vectors will eliminate the corresponding
feature in the output, no matter how high the other
input component was. In addition to failing to iden-
tify the special roles of words in a sentence, vector
mixture models disregard grammar in another way:
the commutativity of operators make them a bag-
of-words approach, where the meaning of sentence
?dog bites man? is equated to that of ?man bites dog?.
On the contrary to the above element-wise treat-
ment, a compositional approach based on linear
maps computes each element of the resulting vec-
= =
Vector mixture Tensor-based
Figure 1: Vector mixture and tensor-based models for
composition. In the latter approach, the ith element of
the output vector is the linear combination of the input
vector with the ith row of the matrix.
1591
tor via a linear combination of all the elements of
the input vector (right part of Figure 1); in other
words, possible interdependencies between differ-
ent features are also taken into account, offering (in
principle) more power. Furthermore, by design, the
bag-of-words problem is not present here. Over-
all, tensor-based models offer a more complete and
linguistically motivated solution to the problem of
composition. For example, one can consider build-
ing linear maps for prepositions and logical words,
rather than treating them as noise and discard them,
as commonly done in the vector mixture models.
3 Disambiguation in vector mixtures
For a compositional model based on vector mix-
tures, polysemy of words can be a critical factor.
Pulman (2013) and Kartsaklis et al (2013) point out
that the element-wise combination of ?ambiguous?
vectors produces results that are hard to interpret;
the composed vector is not a purely compositional
representation but a product of two tasks that take
place in parallel: composition and some amount of
disambiguation that emerges as a side-effect of the
compositional process, leaving the resulting vector
in an intermediate state.
This effect is demonstrated in Figure 2, which
shows the composition of the ambiguous verb ?run?
(with meanings moving fast and dissolving) with the
subject ?horse?. The first three components of our
toy vector space are related to the dissolving mean-
ing, while the last three of them to the moving fast
meaning. An ambiguous vector for ?run? will have
non-zero values for every component. On the other
hand, we would expect the vector for ?horse? to have
high values for the ?race?, ?gallop?, and ?move? com-
ponents, and very low values (but not necessarily
zero) for the dissolving-related ones?it is always
possible for the word ?horse? to appear in the same
c o l o u r
d i s s o l v e
p a i n t i n g
r a c e
g a l l o p
m o v e
5
9
4
1 1
8
1 5
1
0
2
6
1 3
7
A mbig uous 
runhorse
=
5
0
8
6 6
1 0 4
1 0 5
0
0
0
1 1
8
1 5
1
0
2
6
1 3
7
D isambig uated
runhorse
=
0
0
0
6 6
1 0 4
1 0 5
Figure 2: The effect of disambiguation on vector compo-
sition. The numbers are (artificial) co-occurrence counts
of each target word with the 6 basis words on the left.
context with the word ?painting?, for example. The
left part of Figure 2 shows what happens when the
ambiguous ?run? vector is used; the multiplication
with the ?horse? vector will produce an impure re-
sult, half affected by composition and half by disam-
biguation. However, what we really want is a vec-
tor where all the dissolving-related components will
be eliminated, since they are irrelevant to the way
the word ?run? is used in the sentence. In order to
achieve this, we have to introduce a disambiguation
step prior to composition (right part of Figure 2).
These ideas are experimentally verified in the
works of Reddy et al (2011) and Kartsaklis et al
(2013); Pulman (2013) also presents a comprehen-
sive analysis of the problem. What remains to be
seen is if disambiguation can also provide bene-
fits for the linguistically motivated setting of tensor-
based models, the principles of which are shortly
discussed in the next section.
4 Tensors as multilinear maps
A tensor is a geometric object that can be seen as the
generalization of the familiar notion of a vector in
higher dimensions. The order of a tensor is the num-
ber of its dimensions; in other words, the number of
indices we need to fully describe a random element
of the tensor. Hence, a vector is a tensor of order
1, a matrix is a tensor of order 2, and so on. Ten-
sors and multilinear maps stand in one-to-one cor-
respondence, as stated by the following well-known
?map-state? isomorphism (Bourbaki, 1989):
f : V1 ? . . .? Vj ? Vk ?= Vk?Vj?. . .?V1 (1)
This offers an elegant way to adopt a formal se-
mantics view of natural language in vector spaces.
Let nouns live in a basic vector space N ? RD; re-
turning to our previous example, an adjective then
can be seen as a map f : N ? N which is isomor-
phic to N ?N (that is, to a matrix). In general, the
order of the tensor is equal to the number of argu-
ments plus one dimension that carries the result; so
a unary function (e.g. adjectives, intransitive verbs)
is represented by a tensor of order 2 (a matrix), a bi-
nary function (e.g. a transitive verb) as an order 3
tensor, and so on. Due to the above isomorphism,
function application (and hence our compositional
operation) becomes a generalisation of matrix mul-
tiplication, formalised in terms of the inner product.
In the case of a unary relational word, such as an
adjective, this is nothing more than the usual notion
1592
of matrix multiplication between a matrix and a vec-
tor. The generalization of this process to tensors of
higher order is known as tensor contraction. Given
two tensors of orders n and m, the tensor contrac-
tion operation will always produce a tensor of order
n+m? 2.
Let us see an example of how this works for a
simple transitive sentence. Let V ? RI?J?K be the
tensor of order 3 for the verb and S ? RI , O ? RK
the tensors of order 1 (vectors) for the subject and
the object of the verb, respectively. Then V ?O will
return a new tensor living in RI?J (i.e. a matrix)1;
a further interaction of this result with the subject
will return a vector for the whole transitive sentence
living in RJ . We should note that the order in which
the verb is applied to its arguments is not important;
so in general the meaning of a transitive sentence is
given by:
(V ?O)T ? S = (VT ? S)?O (2)
where T denotes a transpose and makes indices
match, since subject precedes the verb.
5 Creating verb tensors
In this section we review a number of proposals re-
garding concrete methods of constructing tensors for
relational words in the context of the frameworks
of Coecke et al (2010) and Baroni and Zamparelli
(2010), which both comply to the setting of Section
4.2
Relational Following ideas from the set-theoretic
view of formal semantics, Grefenstette and
Sadrzadeh (2011a) suggest that the meaning of a
relational word should be represented as the sum
of its arguments. The meaning of adjective ?red?,
for example, becomes the sum of the vectors of
all the nouns that ?red? modifies in the corpus; so
??
red =
?
i
????nouni, where i iterates through all the
occurrences of ?red?. This can be generalised to
relational words of any arity, by summing the tensor
product of their arguments. So for a transitive verb
we have:
verb
2
=
?
i
(
???
subji ?
???
obji) (3)
1The symbol ? denotes tensor contraction.
2In what follows we use the case of a transitive verb as an
example; however the descriptions apply to any relational word
of any arity. A vector (an order-1 tensor) is denoted as ??x ; ten-
sors of order n > 1 are shown as xn for clarity.
where i again iterates over all occurrences of the spe-
cific verb in the corpus and the superscript denotes
the order of the tensor.
In order to achieve a more expressive represen-
tation for the sentences, the authors used the con-
vention that the arity of the head word in a sentence
will also determine the order of the sentence space;
that is, the space of intransitive sentences will be of
order 1, of transitive ones will be of order 2, and
so on. Recall from Section 4 that for the transitive
case this increases the order of the verb tensor to 4
(2 dimensions for the arguments plus another 2 for
the result). In spite of this, however, note that the
method of Equation 3 produces a matrix. The other
two dimensions of the tensor remain empty (filled
with zeros), a fact that simplifies the calculations but
also considerably weakens the expressive power of
the model. This simplification transforms Equation
2 to the following:
subj verb obj
2
= (
???
subj ?
??
obj) verb
2
(4)
where ? denotes the tensor product and  element-
wise multiplication.
Kronecker In a subsequent work (Grefenstette
and Sadrzadeh, 2011b), the same team proposes the
creation of a verb matrix as the Kronecker product
of the verb?s contextual vector with itself:
verb
2
=
???
verb?
???
verb (5)
Again in this model the sentence space is of order
2, and the meaning of a transitive sentence is calcu-
lated using Equation 4.
Frobenius The previous models bring the impor-
tant limitation that only sentences of the same struc-
ture can be meaningfully compared; it is not pos-
sible, for example, to compare an intransitive sen-
tence (e.g. ?kids play?) with a transitive one (?chil-
dren play football?), since the former is a vector and
the latter a matrix. Using Frobenius algebras, Kart-
saklis et al (2012) provide a unified sentence space
for every sentence regardless of its type. These mod-
els turn the matrix of Equation 3 to a tensor of or-
der 3 (as required by the type-logical identities) by
copying one of the existing dimensions. When the
dimension of rows (corresponding to subjects) is
copied, the calculation of a vector for a transitive
sentence becomes:
??????????
subj verb obj =
???
subj  (verb
2
?
??
obj) (6)
1593
Copying the column dimension (objects) gives:
??????????
subj verb obj =
??
obj 
(
(verb
2
)T ?
???
subj
)
(7)
Linear regression None of the above models cre-
ate tensors that are fully populated: one or more
dimensions will always remain empty. Following
an idea first introduced by Baroni and Zamparelli
(2010) for the creation of adjective matrices, Grefen-
stette et al (2013) use linear regression in order
to learn full tensors of order 3 for transitive verbs.
Linear regression is a supervised method of learn-
ing, so it needs a number of exemplar data points.
In the case of the adjective ?red?, for example, we
would need a set of the form ???car,
?????
red car?, ?
???
shirt,
??????
red shirt?, ?
???
shoe,
??????
red shoe? and so on, where the sec-
ond vector in each pair is the contextual vector of the
whole phrase created exactly as if it were a single
word. The goal of the learning process is to find the
parameters adj
2
and
??
b such that:
???????
adj noun ? adj
2
?????noun+
??
b (8)
for all nouns modified by the specific adjective. In
practice, the bias
??
b is embedded in adj
2
, hence
the above procedure provides us with a matrix for
the adjective. One can generalize this procedure to
tensors of higher order by proceeding step-wise, as
done by Grefenstette et al (2013). For the case
of a transitive verb, they first use exemplar pairs
of the form ?
???
subj,
?????????
subj verb obj? to learn a matrix
verb obj
2
for the verb phrase; then, they perform
a new training session with exemplars of the form
?
??
obj, verb obj
2
?, the result of which is an order 3
tensor for the verb.
6 Generic context-based disambiguation
In all of the models of Section 5, the training of a
relational word tensor is based on the set of contexts
where this word occurs. Hence, in these models the
problem of creating disambiguated versions of ten-
sors can be recasted to that of further breaking the
set of contexts in a way that each subset reflects a
different sense of the word in the corpus. If, for ex-
ample, S is the whole set of sentences for a word
w that occurs in the corpus under n different senses,
then the goal is to create n subsets S1, . . . Sn such
that S1 contains the sentences where w appears un-
der the first sense, S2 the sentences where w occurs
under the second sense, and so on. Each one of these
subsets can then be used to train a tensor for a spe-
cific sense of the target relational word.
Towards this purpose we use a variation of the ef-
fective procedure of Schu?tze (1998): first, each con-
text for a target word wt is represented by a context
vector of the form 1n(
??w1 + . . . +
??wn), where
??wi is
the lexical vector of some other word wi 6= wt in the
same context. Next, we apply a clustering method
on this set of vectors in order to discover the latent
senses of wt. The assumption is that the contexts
of wt will vary according to the specific sense this
word is used: ?bank? as a financial institution should
appear in quite different contexts than as land.
The above procedure will give us a number of
clusters, each consisting of context vectors; we use
the centroid of each cluster as a vectorial repre-
sentation of the corresponding sense. So in our
model each wordw is initially represented by a tuple
???w ,S?, where??w is the lexical vector of the word as
created by the usual distributional practice, and S
is a set of sense vectors (centroids of context vec-
tor clusters) produced by the above procedure. The
disambiguation of a new word w under a context C
can now be accomplished as follows: we create a
context vector ??c for C as above, and we compare it
with every sense vector of w; the word is assigned to
the sense corresponding to the closest sense vector.
Specifically, if Sw is the set of sense vectors for w,
??c the context vector for C, and d(??v ,??u ) our vector
distance measure, the preferred sense s? is given by:
s? = argmin
??vs?Sw
d(??vs ,
??c ) (9)
For the actual clustering step we follow the set-
ting of Kartsaklis et al (2013), which worked well in
tasks very similar to ours. Specifically, we perform
hierarchical agglomerative clustering (HAC) using
Ward?s method as the inter-cluster distance, while
the distance between vectors is measured with Pear-
son?s correlation.3 In the above work, this configura-
tion has been found to return the highest V-measure
(Rosenberg and Hirschberg, 2007) on the noun set
of SEMEVAL 2010 Word Sense Induction & Disam-
biguation Task (Manandhar et al, 2010). As con-
text for a word, we consider the sentence in which
this word occurs. The output of HAC is a dendro-
gram embedding all the possible partitionings of the
3Informal experimentation with more robust probabilistic
techniques, such as Dirichlet process gaussian mixture models,
revealed no significant benefits for our setting.
1594
data. In order to select the optimal partitioning, we
rely on the Calin?ski/Harabasz index (Calin?ski and
Harabasz, 1974), also known as variance ratio cri-
terion (VRC). VRC is calculated as the ratio of the
sum of the inter-cluster variances over the sum of
the intra-cluster variances, bearing the intuition that
the optimal partitioning should be the one that re-
sults in the most compact and maximally separated
clusters. We compute the VRC for a range of differ-
ent partitionings (from 2 to 10 clusters) and keep the
partitioning with the highest score.
7 Constructing unambiguous verb tensors
The procedure described in Section 6 provides us
with n clusters of context vectors for a target word.
Since in our case each context vector corresponds
to a distinct sentence, the output of the clustering
scheme can also be seen as n subsets of sentences,
where the word appears under different senses. It
is now quite straightforward to use this partitioning
of the training corpus in order to learn unambiguous
versions of verb tensors, as detailed below.
Relational/Frobenius Both the Relational and the
Frobenius models use the same way of creating an
initial verb matrix (Equation 3) which then they ex-
pand to a higher order tensor. Let S1 . . . Sn be the
sets of sentences returned by the clustering step for
a verb. Then, the verb tensor for the ith sense is:
verb
2
i =
?
s?Si
(
????
subjs ?
???
objs) (10)
where subjs and objs refer to the subject/object pair
that occurred with the verb in sentence s. This can
be generalized to any arity n as follows:
word
n
i =
?
s?Si
n?
k=1
????argk,s (11)
where argk,s denotes the kth argument of the target
word in sentence s.
Kronecker For a given verb v in a context C, let
??vi be the sense vector of v given C corresponding to
the sense i returned by Equation 9. Then we have:
verb
2
i =
??vi ?
??vi (12)
The generalized version to arity n is given by:
word
n
i =
n?
k=1
??vi (13)
Linear regression Creating unambiguous full ten-
sors using linear regression is also quite straightfor-
ward. Let us assume again that the clustering step
for a verb v returns n sets of sentences S1 . . . Sn,
where each sentence set corresponds to a different
sense. Then, we have n different regression prob-
lems, each one of which will be trained on exemplar
pairs derived exclusively from the sentences of the
corresponding set. This will result in n verb tensors,
which will correspond to the different senses of the
verb. Generalization to higher arities is a straightfor-
ward extension of the step-wise process in Section 5
for transitive verbs.
8 Experiments
In this section we will test the effect of disambigua-
tion on the models of Section 5 in a variety of tasks.
Due to the significant methodological differences
of the linear regression model from the other ap-
proaches and the variety of its set of parameters, we
decided that it would be better if this was left as the
subject of a distinct work.
Experimental setting We train our vectors using
ukWaC (Ferraresi et al, 2008), a corpus of English
text with 2 billion words (100m sentences). We use
2000 dimensions, with weights calculated as the ra-
tio of the probability of the context word given the
target word to the probability of the context word
overall. The context here is a 5-word window on
both sides of the target word. The vectors are disam-
biguated both syntactically and semantically: first,
separate vectors have been created for different syn-
tactic usages of the same word in the corpus; for ex-
ample, the word ?book? has two vectors, one for its
noun sense and one for its verb sense. Furthermore,
each word is semantically disambiguated according
to the method of Section 6.
Models We compare the tensor-based models of
Section 5 with the multiplicative and additive mod-
els of Mitchell and Lapata (2008), reporting results
for both ambiguous and disambiguated versions.
For all the disambiguated models, the best sense for
each word in the sentence or phrase is first selected
by applying the procedure of Section 6 and Equa-
tion 9. If the model is based on a vector mixture, the
sense vectors corresponding to these senses are mul-
tiplied or added to form the composite representa-
tion for the sentence or phrase. For the tensor-based
models, the composite meanings are calculated ac-
1595
cording to the equations of Section 5, using verb
tensors created by the procedures of Section 7. The
semantic similarity of two phrases or sentences is
measured as the cosine distance between their com-
posite vectors. For models that return a matrix (e.g.
Relational, Kronecker), the distance is based on the
Frobenius inner product.
Implementation details Our code is mainly writ-
ten in Python and C++, and for the actual cluster-
ing step we use the Python interface of the efficient
FASTCLUSTER library (Mu?llner, 2013). In a shared
24-core Xeon machine with 72 GB of memory, and
with a fair amount of parallelism applied, the aver-
age processing time per word was about 4 minutes;
this is roughly translated to 12-13 hours of training
on average per dataset.
8.1 Verb disambiguation task
Perhaps surprisingly, one of the most popular tasks
for testing compositionality in distributional models
is based on disambiguation. This task, originally in-
troduced by Kintsch (2001), has been adopted by
Mitchell and Lapata (2008) and others for evaluating
the quality of composition in vector spaces. Given
an ambiguous verb such as ?file?, the goal is to find
out to what extent the presence of an appropriate
context will disambiguate its intended meaning. The
context (e.g. a subject/object pair) is composed with
two landmark verbs corresponding to the different
senses (?smooth? and ?register?) to create simple sen-
tences. The assumption is that a good compositional
model should be able to reflect that ?woman files ap-
plication? is closer to ?woman registers application?
than to ?woman smooths application?.
In this paper we test our models on two different
datasets of transitive sentences, that of Grefenstette
and Sadrzadeh (2011a) and Kartsaklis et al (2013)4.
Specific details about the creation of the datasets can
be found in the above papers; for the purposes of
the current work it is sufficient to mention that their
main difference is that in the former the verbs and
their alternative meanings have been selected auto-
matically using the JCN metric of semantic similar-
ity (Jiang and Conrath, 1997), while in the latter the
selection was based on human judgements from the
work of Pickering and Frisson (2001). So, while
4This dataset has been created by Mehrnoosh Sadrzadeh in
collaboration with Edward Grefenstette, but remained unpub-
lished until (Kartsaklis et al, 2013).
in the first dataset many verbs cannot be consid-
ered as genuinely ambiguous (e.g. ?say? with mean-
ings state and allege or ?write? with meanings pub-
lish and spell), the landmarks in the second dataset
correspond to clearly separated senses (e.g. ?file?
with meanings register and smooth or ?charge? with
meanings accuse and bill). Furthermore, subjects
and objects of this latter case are modified by appro-
priate adjectives, overall creating a richer and more
linguistically balanced dataset.
In both cases the evaluation methodology is the
same: each entry of the dataset has the form
?subject, verb, object, high-sim landmark, low-sim
landmark?. The context is combined with the verb
and the two landmarks, creating three simple tran-
sitive sentences. The main-verb sentence is paired
with both the landmark sentences, and these pairs
are randomly presented to human evaluators, the
duty of which is to evaluate the similarity of the sen-
tences within a pair in a scale from 1 to 7. The scores
of the compositional models are the cosine distances
(or the Frobenius inner products, in the case of ma-
trices) between the composite representations of the
sentences of each pair. As an overall score for each
model, we report its Spearman?s ? correlation with
the human judgements. Both datasets consist of 200
pairs of sentences (10 main verbs ? 2 landmarks ?
10 contexts).
Results The results for the G&S dataset are shown
in Table 1.5 The verbs-only model (BL) refers to a
non-compositional evaluation, where the similarity
between two sentences is solely based on the dis-
tance between the two verbs, without applying any
compositional step with subject and object.
The tensor-based models present much better per-
formance than the vector mixture ones, with the dis-
ambiguated version of the copy-object model sig-
nificantly higher than the relational model. By de-
sign, the copy-object model retains more informa-
tion about the objects; so this result confirms pre-
vious findings, that in this certain dataset the role
of objects is more important than that of subjects
(Kartsaklis et al, 2012). In general, the disambigua-
tion step improves the results of all the tensor-based
models except Kronecker; the effect is reversed for
the vector mixture models, where the disambiguated
versions present much worse performance (these
5For all tables in this section,  and denote highly sta-
tistically significant differences with p < 0.001.
1596
Model Ambig. Disamb.
BL Verbs only 0.198  0.132
M1 Multiplicative 0.137  0.044
M2 Additive 0.127  0.047
T1 Relational 0.219 < 0.223
T2 Kronecker 0.207  0.061
T3 Copy-subject 0.070  0.122
T4 Copy-object 0.241  0.262
Human agreement 0.599
Difference between T4 and T1 is s.s. with p < 0.001
Table 1: Results for the G&S dataset.
Model Ambig. Disamb.
BL Verbs only 0.151  0.217
M1 Multiplicative 0.131 < 0.137
M2 Additive 0.085  0.193
T1 Relational 0.036  0.121
T2 Kronecker 0.159 < 0.166
T3 Copy-subject 0.035  0.117
T4 Copy-object 0.033  0.095
Human agreement 0.383
Difference between BL and M2 is s.s. with p < 0.001
Table 2: Results for the Kartsaklis et al dataset.
findings are further discussed in Section 9).
The result of disambiguation is clearer for the
dataset of Kartsaklis et al (Table 2). The longer
context in combination with genuinely ambiguous
verbs produces two effects: first, disambiguation is
now helpful for all models, either vector mixtures
or tensor-based; second, the disambiguation of just
the verb (verbs-only model), without any interac-
tion with the context, is sufficient to provide the best
score (0.22) with a difference statistically significant
from the second model (0.19 for disambiguated ad-
ditive). In fact, further composition of the verb with
the context decreases performance, confirming the
results reported by Kartsaklis et al (2013) for vec-
tors trained using BNC. Given the nature of the spe-
cific task, which is designed around the ambiguity of
the verb, this result is not surprising: a direct disam-
biguation of the verb based on the rest of the con-
text should naturally constitute the best method to
achieve top performance?no composition is neces-
sary for this task to be successful.
However, when one does use a task like this in
order to evaluate compositional models (as we do
here and as is commonly the case), they implic-
itly correlate the strength of the disambiguation ef-
fect that takes place during the composition with the
quality of composition, essentially assuming that the
stronger the disambiguation, the better the composi-
tional model that produced this side-effect. Unfor-
tunately, the extent to which this assumption is valid
or not is still not quite clear; the subject is addressed
in more detail in (Kartsaklis et al, 2013). Keeping a
note of this observation, we now proceed to examine
the performance of our models in a task that does not
use disambiguation as a criterion of composition.
8.2 Phrase/sentence similarity task
Our second set of experiments is based on the phrase
similarity task of Mitchell and Lapata (2010). On
the contrary with the task of Section 8.1, this one
does not involve any assumptions about disambigua-
tion, and thus it seems like a more genuine test of
models aiming to provide appropriate phrasal or sen-
tential semantic representations; the only criterion is
the degree to which these models correctly evaluate
the similarity between pairs of sentences or phrases.
We work on the verb-phrase part of the dataset, con-
sisting of 72 short verb phrases (verb-object struc-
tures). These 72 phrases have been paired in three
different ways to form groups exhibiting various
degrees of similarity: the first group contains 36
pairs of highly similar phrases (e.g. produce effect-
achieve result), the pairs of the second group are
of medium similarity (e.g. write book-hear word),
while a last group contains low-similarity pairs (use
knowledge-provide system). The task is again to
compare the similarity scores given by the various
models for each phrase pair with those of human an-
notators. Additionally to the verb phrases task, we
also perform a richer version of the experiment us-
ing transitive sentences.
Verb phrases It can be shown that for simple verb
phrases the relational model reduces itself to the
copy-subject model; for both of these methods, the
meaning of the verb phrase is calculated according
to Equation 6. Furthermore, according to the copy-
object model the meaning of a verb phrase computed
by a verb matrix
?
ij vij(
??ni?
??nj) and an object vec-
tor
?
j oj
??nj becomes:
verb object
2
=
?
ij
vijoj(
??ni ?
??nj) (14)
Finally, the Kronecker model has no meaning for
verb phrases, since the vector of a verb phrase
will become (??vs ?
??vs) ?
??
obj, which is equal to
???vs |
??
obj???vs , where ?
??vs |
??
obj? denotes the inner prod-
uct between vectors of verb and object. Hence, the
1597
Model Ambig. Disamb.
BL Verbs only 0.310  0.420
M1 Multiplicative 0.315  0.448
M2 Additive 0.291  0.436
T1 Rel./Copy-sbj 0.340  0.367
T2 Copy-object 0.290  0.393
Human agreement 0.550
Difference between M1 and M2 is not s.s.
Difference between M1 and BL is s.s. with p < 0.001
Table 3: Results for the original M&L task.
meaning of a verb phrase becomes a scalar multipli-
cation of the meaning of its verb. As a result, the
cosine distance (used for measuring similarity) be-
tween the meanings of two verb phrases is reduced
to the distance between the vectors of their verbs,
completely dismissing the role of their objects.
Hence our models are limited to those of Table
3. The effects of disambiguation for this task are
quite impressive: the differences between the scores
of all disambiguated models and those of the am-
biguous versions are highly statistically significant
(with p < 0.001), while 4 of the 5 models present
an improvement greater than 10 units of correla-
tion. The models that benefit the most from disam-
biguation are the vector mixtures; both of these ap-
proaches perform significantly better than the best
tensor-based model (copy-object). In fact, the score
of M1 (0.45) is quite high, given that the inter-
annotator agreement is 0.55 (best score reported by
Mitchell and Lapata was 0.41 for their LDA-dilation
model).
Transitive sentences The second part of this ex-
periment aims to examine the extent to which the
above picture can change for the case of text struc-
tures longer than verb phrases. In order to achieve
this, we extend each one of the 72 verb phrases to
a full transitive sentence by adding an appropriate
subject such that the similarity relationships of the
original dataset are retained as much as possible,
so the human judgements for the verb phrase pairs
could as well be used for the transitive cases. We
worked pair-wise: for each pair of verb phrases, we
first selected one of the 5 most frequent subjects for
the first phrase; then, the subject of the other phrase
was selected by a list of synonyms of the first sub-
ject in a way that the new pair of transitive sen-
tences constitutes the least more specific version of
the given verb-phrase pair. So, for example, the pair
produce effect/achieve result became drug produce
effect/medication achieve result, while the pair pose
problem/address question became study pose prob-
lem/paper address question.6
The restrictions of the verb-phrase version do not
hold here, so we evaluate on the full set of models
(Table 4). Once more disambiguation produces bet-
ter results in all cases, with highly statistically sig-
nificant differences for all but one model. Further-
more, now the best score is delivered by one of the
tensor-based models (Kronecker), with a difference
not statistically significant from disambiguated ad-
ditive. In any case, the result suggests that as the
length of the text segments increases, the perfor-
mance of vector mixtures and tensor-based models
converges. Indeed, note how the performance of the
vector mixture models are significantly decreased
compared to the verb phrase task.
9 Discussion
The purpose of this work was twofold: our main ob-
jective was to investigate how disambiguation can
affect the compositional models which are based on
higher order vector spaces; a second, but not less
important goal, was to compare this more linguisti-
cally motivated approach to the simpler vector mix-
ture methods. Based on the experimental work pre-
sented here, we can say with enough confidence that
disambiguation as an additional step prior to com-
position is indeed very beneficial for tensor-based
models. Furthermore, our experiments confirm and
strengthen previous work (Reddy et al, 2011; Kart-
saklis et al, 2013) that showed better performance of
disambiguated vector mixture models compared to
their ambiguous versions. The positive effect of dis-
ambiguation is more evident for the vector mixture
models (especially for the additive model) than for
6The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.
Model Ambig. Disamb.
BL Verbs only 0.310  0.341
M1 Multiplicative 0.325  0.404
M2 Additive 0.368  0.410
T1 Relational 0.368  0.397
T2 Kronecker 0.404 < 0.412
T3 Copy-subject 0.310  0.337
T4 Copy-object 0.321  0.368
Human agreement 0.550
Difference between T2 and M2 is not s.s.
Table 4: Transitive version of M&L task.
1598
the tensor-based ones. This is expected: composite
representations created by element-wise operations
are averages, and a prior step of disambiguation can
make a great difference.
From a task perspective, the effect of disambigua-
tion was much more definite in the phrase/sentence
similarity task. This observation is really interest-
ing, since the words of that dataset were not se-
lected in order to be ambiguous in any way. The
superior performance of the disambiguated models,
therefore, implies that the proposed methodology
can improve tasks based on phrase or sentence sim-
ilarity regardless of the level of ambiguity in the
vocabulary. For these cases, the proposed disam-
biguation algorithm acts as a fine-tuning process, the
outcome of which seems to be always positive; it
can only produce better composite representations,
not worse. In general, the positive effect of dis-
ambiguation in the phrase/sentence similarity task is
quite encouraging, especially given the fact that this
task constitutes a more appropriate test for evaluat-
ing compositional models, avoiding the pitfalls of
disambiguation-based experiments (as shortly dis-
cussed in Section 8.1).
For disambiguation-based tasks similar to those
of Section 8.1, the form of dataset is very important;
hence the inferior performance of disambiguated
models in the G&S dataset, compared to the dataset
of Kartsaklis et al. In fact, the G&S dataset was
the only one where disambiguation was not helpful
for some cases (specifically, for vector mixtures and
the Kronecker model). We believe the reason behind
this lies in the fact that the automatic selection of
landmark verbs using the JCN metric (as done with
the G&S dataset) was not very efficient for certain
cases. Note, for example, that the bare baseline of
comparing just ambiguous versions of verbs (with-
out any composition) in that dataset aleady achieves
a very high correlation of 0.198 with human judge-
ments (Table 1).7. This number is only 0.15 for the
Kartsaklis et al dataset, due to the more efficient
verb selection procedure. In general, we consider
the results gained by this latter experiment more re-
liable for the specific task, the successful evaluation
of which requires genuinely ambiguous verbs.
The results are less conclusive for the second
question we posed in the beginning of this section,
regarding the comparison of the two classes of mod-
7The reported number for this baseline by Grefenstette and
Sadrzadeh (2011a) was 0.16 using vectors trained from BNC.
els. Despite the obvious benefits of the tensor-based
approaches, this work suggests for one more time
that vector mixture models might constitute a hard-
to-beat baseline; similar observations have been
made, for example, in the comparative study of Bla-
coe and Lapata (2012). However, when trying to in-
terpret the mixing results regarding the effectiveness
of the tensor-based models compared to vector mix-
tures, we need to take into account that the tensor-
based models tested in this work were all ?hybrid?,
in the sense that they all involved some element
of point-wise operation; in other words, they con-
stituted a trade-off between transformational power
and complexity.
Even with this compromise, though, the study
presented in Section 8.2 implies that the effective-
ness of each method depends to some extent on the
length of the text segment: when more words are
involved, vector mixture models tend to be less ef-
fective; on the contrary, the performance of tensor-
based models seems to be proportional to the length
of the phrase or sentence?the more, the better.
These observations comply with the nature of the
approaches: ?averaging? larger numbers of points
results in more general (hence less accurate) repre-
sentations; on the other hand, a larger number of
arguments makes a function (such as a verb) more
accurate.
10 Conclusion and future work
In the present paper we showed how to improve
a number of tensor-based compositional distribu-
tional models of meaning by introducing a step
of disambiguation prior to composition. Our sim-
ple algorithm (based on the procedure of Schu?tze
(1998)) creates unambiguous versions of tensors be-
fore these are composed with vectors of nouns in
order to construct vectors for sentences and phrases.
This algorithm is quite generic, and can be applied
to any model that follows the tensor contraction pro-
cess described in Section 4. As for future work, we
aim to investigate the application of this procedure
to the regression model of Grefenstette et al (2013).
Acknowledgements
We would like to thank Edward Grefenstette for his
comments on the first draft of this paper, as well as
the three anonymous reviewers for their fruitful sug-
gestions. Support by EPSRC grant EP/F042728/1 is
gratefully acknowledged by the authors.
1599
References
Baroni, M. and Zamparelli, R. (2010). Nouns are
Vectors, Adjectives are Matrices. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Blacoe, W. and Lapata, M. (2012). A comparison of
vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Bourbaki, N. (1989). Commutative Algebra: Chap-
ters 1-7. Srpinger Verlag, Berlin/New York.
Calin?ski, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications in
Statistics-Theory and Methods, 3(1):1?27.
Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning. Lam-
bek Festschrift. Linguistic Analysis, 36:345?384.
Curran, J. (2004). From Distributional to Seman-
tic Similarity. PhD thesis, School of Informatics,
University of Edinburgh.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukWaC, a very large web-derived corpus of En-
glish. In Proceedings of the 4th Web as Corpus
Workshop (WAC-4) Can we beat Google, pages
47?54.
Grefenstette, E., Dinu, G., Zhang, Y.-Z., Sadrzadeh,
M., and Baroni, M. (2013). Multi-step regres-
sion learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS
2013).
Grefenstette, E. and Sadrzadeh, M. (2011a). Exper-
imental Support for a Categorical Compositional
Distributional Model of Meaning. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Grefenstette, E. and Sadrzadeh, M. (2011b). Exper-
imenting with Transitive Verbs in a DisCoCat. In
Proceedings of Workshop on Geometrical Models
of Natural Language Semantics (GEMS).
Harris, Z. (1968). Mathematical Structures of Lan-
guage. Wiley.
Jiang, J. and Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
19?33, Taiwan.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categori-
cal distributional-compositional semantics: The-
ory and experiments. In Proceedings of 24th
International Conference on Computational Lin-
guistics (COLING 2012): Posters, pages 549?
558, Mumbai, India. The COLING 2012 Orga-
nizing Committee.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2013). Separating Disambiguation from Com-
position in Distributional Semantics. In Proceed-
ings of 17th Conference on Computational Nat-
ural Language Learning (CoNLL-2013), Sofia,
Bulgaria.
Kintsch, W. (2001). Predication. Cognitive Science,
25(2):173?202.
Landauer, T. and Dumais, S. (1997). A Solution to
Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representa-
tion of Knowledge. Psychological Review.
Manandhar, S., Klapaftis, I., Dligach, D., and Prad-
han, S. (2010). Semeval-2010 task 14: Word
sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Manning, C., Raghavan, P., and Schu?tze, H. (2008).
Introduction to Information Retrieval. Cambridge
University Press.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 236?244.
Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388?1439.
Mu?llner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal of
Statistical Software, 9(53):1?18.
Pickering, M. and Frisson, S. (2001). Processing
ambiguous verbs: Evidence from eye movements.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 27(2):556.
1600
Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Univer-
sity Press.
Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural
Language Processing, pages 705?713.
Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 410?420.
Schu?tze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97?
123.
1601
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708?719,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Neural Word Representations in
Tensor-Based Compositional Settings
Dmitrijs Milajevs
1
Dimitri Kartsaklis
2
Mehrnoosh Sadrzadeh
1
Matthew Purver
1
1
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road, London, UK
{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk
2
University of Oxford
Department of Computer Science
Parks Road, Oxford, UK
dimitri.kartsaklis@cs.ox.ac.uk
Abstract
We provide a comparative study be-
tween neural word representations and
traditional vector spaces based on co-
occurrence counts, in a number of com-
positional tasks. We use three differ-
ent semantic spaces and implement seven
tensor-based compositional models, which
we then test (together with simpler ad-
ditive and multiplicative approaches) in
tasks involving verb disambiguation and
sentence similarity. To check their scala-
bility, we additionally evaluate the spaces
using simple compositional methods on
larger-scale tasks with less constrained
language: paraphrase detection and di-
alogue act tagging. In the more con-
strained tasks, co-occurrence vectors are
competitive, although choice of composi-
tional method is important; on the larger-
scale tasks, they are outperformed by neu-
ral word embeddings, which show robust,
stable performance across the tasks.
1 Introduction
Neural word embeddings (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al.,
2013a) have received much attention in the dis-
tributional semantics community, and have shown
state-of-the-art performance in many natural lan-
guage processing tasks. While they have been
compared with co-occurrence based models in
simple similarity tasks at the word level (Levy et
al., 2014; Baroni et al., 2014), we are aware of
only one work that attempts a comparison of the
two approaches in compositional settings (Blacoe
and Lapata, 2012), and this is limited to additive
and multiplicative composition, compared against
composition via a neural autoencoder.
The purpose of this paper is to provide a more
complete picture regarding the potential of neu-
ral word embeddings in compositional tasks, and
meaningfully compare them with the traditional
distributional approach based on co-occurrence
counts. We are especially interested in investi-
gating the performance of neural word vectors in
compositional models involving general mathe-
matical composition operators, rather than in the
more task- or domain-specific deep-learning com-
positional settings they have generally been used
with so far (for example, by Socher et al. (2012),
Kalchbrenner and Blunsom (2013) and many oth-
ers).
In particular, this is the first large-scale study
to date that applies neural word representations in
tensor-based compositional distributional models
of meaning similar to those formalized by Coecke
et al. (2010). We test a range of implementations
based on this framework, together with additive
and multiplicative approaches (Mitchell and Lap-
ata, 2008), in a variety of different tasks. Specif-
ically, we use the verb disambiguation task of
Grefenstette and Sadrzadeh (2011a) and the tran-
sitive sentence similarity task of Kartsaklis and
Sadrzadeh (2014) as small-scale focused experi-
ments on pre-defined sentence structures. Addi-
tionally, we evaluate our vector spaces on para-
phrase detection (using the Microsoft Research
Paraphrase Corpus of Dolan et al. (2005)) and di-
alogue act tagging using the Switchboard Corpus
(see e.g. (Stolcke et al., 2000)).
In all of the above tasks, we compare the neural
word embeddings of Mikolov et al. (2013a) with
two vector spaces both based on co-occurrence
counts and produced by standard distributional
techniques, as described in detail below. The gen-
eral picture we get from the results is that in almost
all cases the neural vectors are more effective than
the traditional approaches.
We proceed as follows: Section 2 provides a
concise introduction to distributional word repre-
sentations in natural language processing. Section
708
3 takes a closer look to the subject of composi-
tionality in vector space models of meaning and
describes the range of compositional operators ex-
amined here. In Section 4 we provide details about
the vector spaces used in the experiments. Our ex-
perimental work is described in detail in Section 5,
and the results are discussed in Section 6. Finally,
Section 7 provides conclusions.
2 Meaning representation
There are several approaches to the representation
of word, phrase and sentence meaning. As nat-
ural languages are highly creative and it is very
rare to see the same sentence twice, any practical
approach dealing with large text segments must
be compositional, constructing the meaning of
phrases and sentences from their constituent parts.
The ideal method would therefore express not
only the similarity in meaning between those con-
stituent parts, but also between the results of their
composition, and do this in ways which fit with
linguistic structure and generalisations thereof.
Formal semantics Formal approaches to the
semantics of natural language have long built
upon the classical idea of compositionality ?
that the meaning of a sentence is a function
of the meanings of its parts (Frege, 1892). In
compositional type-logical approaches, predicate-
argument structures representing phrases and sen-
tences are built from their constituent parts by ?-
reduction within the lambda calculus framework
(Montague, 1970): for example, given a represen-
tation of John as john
?
and sleeps as ?x.sleep
?
(x),
the meaning of the sentence ?John sleeps?
can be constructed as ?x.sleep
?
(x)(john
?
) =
sleep
?
(john
?
). Given a suitable pairing between
words and semantic representations of them, this
method can produce structured sentential repre-
sentations with broad coverage and good gener-
alisability (see e.g. (Bos, 2008)). The above logi-
cal approach is extremely powerful because it can
capture complex aspects of meaning such as quan-
tifiers and their interaction (see e.g. (Copestake et
al., 2005)), and enables inference using well stud-
ied and developed logical methods (see e.g. (Bos
and Gabsdil, 2000)).
Distributional hypothesis However, such for-
mal approaches are less able to express similar-
ity in meaning. We would like to capture the
intuition that while John and Mary are distinct,
they are rather similar to each other (both of them
are humans) and dissimilar to words such as dog,
pavement or idea. The same applies at the phrase
and sentence level: ?dogs chase cats? is similar in
meaning to ?hounds pursue kittens?, but less so to
?cats chase dogs? (despite the lexical overlap).
Distributional methods provide a way to address
this problem. By representing words and phrases
as vectors or tensors in a (usually highly dimen-
sional) vector space, one can express similarity
in meaning via a suitable distance metric within
that space (usually cosine distance); furthermore,
composition can be modelled via suitable linear-
algebraic operations.
Co-occurrence-based word representations
One way to produce such vectorial representa-
tions is to directly exploit Harris (1954)?s intuition
that semantically similar words tend to appear in
similar contexts. We can construct a vector space
in which the dimensions correspond to contexts,
usually taken to be words as well. The word
vector components can then be calculated from
the frequency with which a word has co-occurred
with the corresponding contexts in a window of
words, with a predefined length.
Table 1 shows 5 3-dimensional vectors for the
words Mary, John, girl, boy and idea. The words
philosophy, book and school signify vector space
dimensions. As the vector for John is closer to
Mary than it is to idea in the vector space?a di-
rect consequence of the fact that John?s contexts
are similar to Mary?s and dissimilar to idea?s?we
can infer that John is semantically more similar to
Mary than to idea.
Many variants of this approach exist: perfor-
mance on word similarity tasks has been shown
to be improved by replacing raw counts with
weighted values (e.g. mutual information)?see
(Turney et al., 2010) and below for discussion, and
(Kiela and Clark, 2014) for a detailed comparison.
philosophy book school
Mary 0 10 22
John 4 60 59
girl 0 19 93
boy 0 12 164
idea 10 47 39
Table 1: Word co-occurrence frequencies ex-
tracted from the BNC (Leech et al., 1994).
709
Neural word embeddings Deep learning tech-
niques exploit the distributional hypothesis dif-
ferently. Instead of relying on observed co-
occurrence frequencies, a neural language model
is trained to maximise some objective function re-
lated to e.g. the probability of observing the sur-
rounding words in some context (Mikolov et al.,
2013b):
1
T
T
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
) (1)
Optimizing the above function, for example, pro-
duces vectors which maximise the conditional
probability of observing words in a context around
the target word w
t
, where c is the size of the
training window, and w
1
w
2
, ? ? ?w
T
a sequence of
words forming a training instance. Therefore, the
resulting vectors will capture the distributional in-
tuition and can express degrees of lexical similar-
ity.
This method has an obvious advantage com-
pared to co-occurrence method: since now the
context is predicted, the model in principle can
be much more robust in data sparsity prob-
lems, which is always an important issue for co-
occurrence word spaces. Additionally, neural vec-
tors have also proven successful in other tasks
(Mikolov et al., 2013c), since they seem to en-
code not only attributional similarity (the degree to
which similar words are close to each other), but
also relational similarity (Turney, 2006). For ex-
ample, it is possible to extract the singular:plural
relation (apple:apples, car:cars) using vector sub-
traction:
????
apple ?
?????
apples ?
??
car ?
???
cars
Perhaps even more importantly, semantic relation-
ships are preserved in a very intuitive way:
???
king ?
???
man ?
????
queen ?
?????
woman
allowing the formation of analogy queries similar
to
???
king ?
???
man +
?????
woman = ?, obtaining
????
queen as
the result.
1
Both neural and co-occurrence-based ap-
proaches have advantages over classical formal
approaches in their ability to capture lexical se-
mantics and degrees of similarity; their success at
1
Levy et al. (2014) improved Mikolov et al. (2013c)?s
method of retrieving relational similarities by changing the
underlying objective function.
extending this to the sentence level and to more
complex semantic phenomena, though, depends
on their applicability within compositional mod-
els, which is the subject of the next section.
3 Compositional models
Compositional distributional models represent
meaning of a sequence of words by a vector, ob-
tained by combining meaning vectors of the words
within the sequence using some vector composi-
tion operation. In a general classification of these
models, one can distinguish between three broad
cases: simplistic models which combine word
vectors irrespective of their order or relation to one
another, models which exploit linear word order,
and models which use grammatical structure.
The first approach combines word vectors
by vector addition or point-wise multiplication
(Mitchell and Lapata, 2008)?as this is indepen-
dent of word order, it cannot capture the differ-
ence between the two sentences ?dogs chase cats?
and ?cats chase dogs?. The second approach has
generally been implemented using some form of
deep learning, and captures word order, but not by
necessarily caring about the grammatical structure
of the sentence. Here, one works by recursively
building and combining vectors for subsequences
of words within the sentence using e.g. autoen-
coders (Socher et al., 2012) or convolutional fil-
ters (Kalchbrenner et al., 2014). We do not con-
sider this approach in this paper. This is because,
as mentioned in the introduction, their vectors and
composition operators are task-specific. These are
trained directly to achieve specific objectives in
certain pre-determined tasks. We are interested
in vector and composition operators that work for
any compositional task, and which can be com-
bined with results in linguistics and formal se-
mantics to provide generalisable models that can
canonically extend to complex semantic phenom-
ena. The third (i.e. the grammatical) approach
promises a way to achieve this, and has been in-
stantiated in various ways in the work of Baroni
and Zamparelli (2010),Grefenstette and Sadrzadeh
(2011a), and Kartsaklis et al. (2012).
General framework Formally, we can spec-
ify the vector representation of a word sequence
w
1
w
2
? ? ?w
n
as the vector
??
s =
??
w
1
?
??
w
2
? ? ? ??
??
w
n
,
where ? is a vector operator, such as addition +,
point-wise multiplication , tensor product ?, or
matrix multiplication ?.
710
In the simplest compositional models (the first
approach described above), ? is + or , e.g. see
(Mitchell and Lapata, 2008). Grammar-based
compositional models (the third approach) are
based on a generalisation of the notion of vectors,
known as tensors. Whereas a vector
??
v is an ele-
ment of an atomic vector space V , a tensor z is an
element of a tensor space V ?W ? ? ? ? ? Z. The
number of tensored spaces is referred to by the or-
der of the space. Using a general duality theorem
from multi-linear algebra (Bourbaki, 1989), it fol-
lows that tensors are in one-one correspondence
with multi-linear maps, that is we have:
z ? V ?W?? ? ??Z
?
=
f
z
: V ?W ? ? ? ? ? Z
In such a tensor-based formalism, meanings of
nouns are vectors and meanings of predicates such
as adjectives and verbs are tensors. Meaning of a
string of words is obtained by applying the compo-
sitions of multi-linear map duals of the tensors to
the vectors. For the sake of demonstration, take
the case of an intransitive sentence ?Sbj Verb?;
the meaning of the subject is a vector
??
Sbj ? V
and the meaning of the intransitive verb is a ten-
sor Verb ? V ?W . Meaning of the sentence is
obtained by applying f
V erb
to
??
Sbj, as follows:
??????
Sbj Verb = f
V erb
(
??
Sbj)
By tensor-map duality, the above becomes
equivalent to the following, where composition
has now become the familiar notion of matrix mul-
tiplication, that is ? is ?:
Verb?
??
Sbj
In general and for words with tensors of order
higher than two, ? becomes a generalisation of ?,
referred to by tensor contraction, see e.g. Kartsak-
lis and Sadrzadeh (2013). Since the creation and
manipulation of tensors of order higher than 2 is
difficult, one can work with simplified versions of
tensors, faithful to their underlying mathematical
basis; these have found intuitive interpretations,
e.g. see Grefenstette and Sadrzadeh (2011a), Kart-
saklis and Sadrzadeh (2014). In such cases, ? be-
comes a combination of a range of operations such
as ?, ?, , and +.
Specific models In the current paper we will ex-
periment with a variety of models. In Table 2, we
present these models in terms of their composi-
tion operators and a reference to the main paper in
which each model was introduced. For the sim-
ple compositional models the sentence is a string
of any number of words; for the grammar-based
models, we consider simple transitive sentences
?Sbj Verb Obj? and introduce the following abbre-
viations for the concrete method used to build a
tensor for the verb:
1. Verb is a verb matrix computed using the for-
mula
?
i
???
Sbj
i
?
???
Obj
i
, where
???
Sbj
i
and
???
Obj
i
are
the subjects and objects of the verb across the
corpus. These models are referred to by rela-
tional (Grefenstette and Sadrzadeh, 2011a);
they are generalisations of predicate seman-
tics of transitive verbs, from pairs of individ-
uals to pairs of vectors. The models reduce
the order 3 tensor of a transitive verb to an
order 2 tensor (i.e. a matrix).
2.
?
Verb is a verb matrix computed using the for-
mula
???
Verb ?
???
Verb, where
???
Verb is the distri-
butional vector of the verb. These models are
referred to by Kronecker, which is the term
sometimes used to denote the outer prod-
uct of tensors (Grefenstette and Sadrzadeh,
2011b). This models also reduces the order
3 tensor of a transitive verb to an order 2 ten-
sor.
3. The models of the last five lines of the table
use the so-called Frobenius operators from
categorical compositional distributional se-
mantics (Kartsaklis et al., 2012) to expand
the relational matrices of verbs from order 2
to order 3. The expansion is obtained by ei-
ther copying the dimension of the subject into
the space provided by the third tensor, hence
referred to by Copy-Sbj, or copying the di-
mension of the object in that space, hence re-
ferred to by Copy-Obj; furthermore, we can
take addition, multiplication, or outer product
of these, which are referred to by Frobenius-
Add, Frobenius-Mult, and Frobenius-Outer
(Kartsaklis and Sadrzadeh, 2014).
4 Semantic word spaces
Co-occurrence-based vector space instantiations
have received a lot of attention from the scientific
community (refer to (Kiela and Clark, 2014; Pola-
jnar and Clark, 2014) for recent studies). We in-
stantiate two co-occurrence-based vectors spaces
with different underlying corpora and weighting
schemes.
711
Method Sentence Linear algebraic formula Reference
Addition w
1
w
2
? ? ?w
n
??
w
1
+
??
w
2
+ ? ? ?+
??
w
n
Mitchell and Lapata (2008)
Multiplication w
1
w
2
? ? ?w
n
??
w
1

??
w
2
 ? ? ? 
??
w
n
Mitchell and Lapata (2008)
Relational Sbj Verb Obj Verb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011a)
Kronecker Sbj Verb Obj V?erb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011b)
Copy object Sbj Verb Obj
??
Sbj (Verb?
??
Obj) Kartsaklis et al. (2012)
Copy subject Sbj Verb Obj
??
Obj (Verb
T
?
??
Sbj) Kartsaklis et al. (2012)
Frob. add. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) + (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. mult. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. outer Sbj Verb Obj (
??
Sbj (Verb?
??
Obj))? (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Table 2: Compositional methods.
GS11 Our first word space is based on a typ-
ical configuration that has been used in the past
extensively for compositional distributional mod-
els (see below for details), so it will serve as a
useful baseline for the current work. In this vec-
tor space, the co-occurrence counts are extracted
from the British National Corpus (BNC) (Leech et
al., 1994). As basis words, we use the most fre-
quent nouns, verbs, adjectives and adverbs (POS
tags SUBST, VERB, ADJ and ADV in the BNC
XML distribution
2
). The vector space is lemma-
tized, that is, it contains only ?canonical? forms of
words.
In order to weight the raw co-occurrence counts,
we use positive point-wise mutual information
(PPMI). The component value for a target word
t and a context word c is given by:
PPMI(t, c) = max
(
0, log
p(c|t)
p(c)
)
where p(c|t) is the probability of word c given t
in a symmetric window of length 5 and p(c) is the
probability of c overall.
Vector spaces based on point-wise mutual in-
formation (or variants thereof) have been success-
fully applied in various distributional and compo-
sitional tasks; see e.g. Grefenstette and Sadrzadeh
(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details. PPMI has been shown to
achieve state-of-the-art results (Levy et al., 2014)
and is suggested by the review of Kiela and Clark
(2014). Our use here of the BNC as a corpus
and the window length of 5 is based on previ-
ous use and better performance of these param-
eters in a number of compositional experiments
(Grefenstette and Sadrzadeh, 2011a; Grefenstette
2
http://www.natcorp.ox.ac.uk/
and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;
Kartsaklis et al., 2012).
KS14 In this variation, we train a vector space
from the ukWaC corpus
3
(Ferraresi et al., 2008),
originally using as a basis the 2,000 content words
with the highest frequency (but excluding a list of
stop words as well as the 50 most frequent content
words since they exhibit low information content).
The vector space is again lemmatized. As context
we consider a 5-word window from either side of
the target word, while as our weighting scheme we
use local mutual information (i.e. point-wise mu-
tual information multiplied by raw counts). In a
further step, the vector space was normalized and
projected onto a 300-dimensional space using sin-
gular value decomposition (SVD).
In general, dimensionality reduction produces
more compact word representations that are robust
against potential noise in the corpus (Landauer and
Dumais, 1997; Sch?utze, 1997). SVD has been
shown to perform well on a variety of tasks similar
to ours (Baroni and Zamparelli, 2010; Kartsaklis
and Sadrzadeh, 2014).
Neural word embeddings (NWE) For our neu-
ral setting, we used the skip-gram model of
Mikolov et al. (2013b) trained with negative sam-
pling. The specific implementation that was tested
in our experiments was a 300-dimensional vec-
tor space learned from the Google News corpus
and provided by the word2vec
4
toolkit. Fur-
thermore, the gensim library (
?
Reh?u?rek and So-
jka, 2010) was used for accessing the vectors.
On the contrary with the previously described co-
3
http://wacky.sslmit.unibo.it/
4
https://code.google.com/p/word2vec/
712
occurrence vector spaces, this version is not lem-
matized.
The negative sampling method improves the ob-
jective function of Equation 1 by introducing neg-
ative examples to the training algorithm. Assume
that the probability of a specific (c, t) pair of words
(where t is a target word and c another word in
the same context with t), coming from the training
data, is denoted as p(D = 1|c, t). The objective
function is then expressed as follows:
?
(c,t)?D
p(D = 1|c, t) (2)
That is, the goal is to set the model parameters in
a way that maximizes the probability of all obser-
vations coming from the training data. Assume
now that D
?
is a set of randomly selected incorrect
(c
?
, t
?
) pairs that do not occur in D, then Equation
2 above can be recasted in the following way:
?
(c,t)?D
p(D = 1|c, t)
?
(c
?
,t
?
)?D
?
p(D = 0|c
?
, t
?
)
(3)
In other words, the model tries to distinguish a tar-
get word t from random draws that come from a
noise distribution. In the implementation we used
for our experiments, c is always selected from
a 5-word window around t. More details about
the negative sampling approach can be found in
(Mikolov et al., 2013b); the note of Goldberg and
Levy (2014) also provides an intuitive explanation
of the underlying setting.
5 Experiments
Our experiments explore the use of the vector
spaces above, together with the compositional op-
erators described in Section 3, in a range of tasks
all of which require semantic composition: verb
sense disambiguation; sentence similarity; para-
phrasing; and dialogue act tagging.
5.1 Disambiguation
We use the transitive verb disambiguation dataset
described in Grefenstette and Sadrzadeh (2011a)
5
.
This dataset consists of ambiguous transitive verbs
together with their arguments, landmark verbs
that identify one of the verb senses, and human
judgements that specify how similar is the disam-
biguated sense of the verb in the given context to
5
This and the sentence similarity dataset are avail-
able at http://www.cs.ox.ac.uk/activities/
compdistmeaning/
one of the landmarks. This is similar to the in-
transitive dataset described in (Mitchell and Lap-
ata, 2008). Consider the sentence ?system meets
specification?; here, meets is the ambiguous tran-
sitive verb, and system and specification are its ar-
guments in this context. Possible landmarks for
meet are satisfy and visit; for this sentence, the
human judgements show that the disambiguated
meaning of the verb is more similar to the land-
mark satisfy and less similar to visit.
The task is to estimate the similarity of the sense
of a verb in a context with a given landmark. To
get our similarity measures, we compose the verb
with its arguments using one of our compositional
models; we do the same for the landmark and then
compute the cosine similarity of the two vectors.
We evaluate the performance by averaging the hu-
man judgements for the same verb, argument and
landmark entries, and calculating the Spearman?s
correlation between the average values and the co-
sine scores. As a baseline, we compare this with
the correlation produced by using only the verb
vector, without composing it with its arguments.
Table 3 shows the results of the experiment.
NWE copy-object composition yields the best cor-
relation with the human judgements, and top per-
formance across all vector spaces and models with
a Spearman ? of 0.456. For the KS14 space, the
best result comes from Frobenius outer (0.350),
Method GS11 KS14 NWE
Verb only 0.212 0.325 0.107
Addition 0.103 0.275 0.149
Multiplication 0.348 0.041 0.095
Kronecker 0.304 0.176 0.117
Relational 0.285 0.341 0.362
Copy subject 0.089 0.317 0.131
Copy object 0.334 0.331 0.456
Frobenius add. 0.261 0.344 0.359
Frobenius mult. 0.233 0.341 0.239
Frobenius outer 0.284 0.350 0.375
Table 3: Spearman ? correlations of models with
human judgements for the word sense disam-
biguation task. The best result (NWE Copy ob-
ject) outperforms the nearest co-occurrence-based
competitor (KS14 Frobenius outer) with a statisti-
cally significant difference (p < 0.05, t-test).
713
while the best operator for the GS11 space is
point-wise multiplication (0.348).
For simple point-wise composition, only mul-
tiplicative GS11 and additive NWE improve over
their corresponding verb-only baselines (but both
perform worse than the KS14 baseline). With
tensor-based composition in co-occurrence based
spaces, copy subject yields lower results than
the corresponding baselines. Other composition
methods, except Kronecker for KS14, improve
over the verb-only baselines. Finally we should
note that, despite the small training corpus, the
GS11 vector space performs comparatively well:
for instance, Kronecker model improves the pre-
viously reported score of 0.28 (Grefenstette and
Sadrzadeh, 2011b).
5.2 Sentence similarity
In this experiment we use the transitive sen-
tence similarity dataset described in Kartsaklis and
Sadrzadeh (2014). The dataset consists of transi-
tive sentence pairs and a human similarity judge-
ment
6
. The task is to estimate a similarity measure
between two sentences. As in the disambiguation
task, we first compose word vectors to obtain sen-
tence vectors, then compute cosine similarity of
them. We average the human judgements for iden-
tical sentence pairs to compute a correlation with
cosine scores.
Table 4 shows the results. Again, the best
performing vector space is KS14, but this time
with addition: the Spearman ? correlation score
with averaged human judgements is 0.732. Addi-
tion was the means for the other vector spaces to
achieve top performance as well: GS11 and NWE
got 0.682 and 0.689 respectively.
None of the models in tensor-based composi-
tion outperformed addition. KS14 performs worse
with tensor-based methods here than in the other
vector spaces. However, GS11 and NWE, except
copy subject for both of them and Frobenius multi-
plication for NWE, improved over their verb-only
baselines.
5.3 Paraphrasing
In this experiment we evaluate our vector spaces
on a mainstream paraphrase detection task.
6
The textual content of this dataset is the same as that of
(Kartsaklis and Sadrzadeh, 2013), the difference is that the
dataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-
man judgements whereas the previous dataset used the orig-
inal annotations of the intransitive dataset of (Mitchell and
Lapata, 2010).
Method GS11 KS14 NWE
Verb only 0.491 0.602 0.561
Addition 0.682 0.732 0.689
Multiplication 0.597 0.321 0.341
Kronecker 0.581 0.408 0.561
Relational 0.558 0.437 0.618
Copy subject 0.370 0.448 0.405
Copy object 0.571 0.306 0.655
Frobenius add. 0.566 0.460 0.585
Frobenius mult. 0.525 0.226 0.387
Frobenius outer 0.560 0.439 0.622
Table 4: Results for sentence similarity. There
is no statistically significant difference between
KS14 addition and NWE addition (the second best
result).
Specifically, we get classification results on the
Microsoft Research Paraphrase Corpus paraphrase
corpus (Dolan et al., 2005) working in the follow-
ing way: we construct vectors for the sentences
of each pair; if the cosine similarity between the
two sentence vectors exceeds a certain threshold,
the pair is classified as a paraphrase, otherwise as
not a paraphrase. For this experiment and that of
Section 5.4 below, we investigate only the addi-
tion and point-wise multiplication compositional
models, since at their current stage of development
tensor-based models can only efficiently handle
sentences of fixed structure. Nevertheless, the
simple point-wise compositional models still al-
low for a direct comparison of the vector spaces,
which is the main goal of this paper.
For each vector space and model, a number of
different thresholds were tested on the first 2000
pairs of the training set, which we used as a de-
velopment set; in each case, the best-performed
threshold was selected for a single run of our
?classifier? on the test set (1726 pairs). Addition-
ally, we evaluate the NWE model with a lemma-
tized version of the corpus, so that the experimen-
tal setup is maximally similar for all vector spaces.
The results are shown in the first part of Table 5.
Additive NWE gives the highest performance,
with both lemmatized and un-lemmatized versions
outperforming the GS11 and KS14 spaces. In
the un-lemmatized case, the accuracy of our sim-
ple ?classifier? (0.73) is close to state-of-the-art
range. The state-of-the art result (0.77 accuracy
714
Co-occurrence Neural word embeddings
Baseline GS11 KS14 Unlemmatized Lemmatized
Model Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score
MSR addition
0.65 0.75
0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81
MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36
SWDA addition
0.60 0.58
0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40
SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38
Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results
significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, ?
2
test.
and 0.84 F-score
7
) by the time of this writing has
been obtained using 8 machine translation metrics
and three constituent classifiers (Madnani et al.,
2012).
The multiplicative model gives lower results
than the additive model across all vector spaces.
The KS14 vector space shows the steadiest per-
formance, with a drop in accuracy of only 0.04
and no drop in F-score, while for the GS11 and
NWE spaces both accuracy and F-score experi-
enced drops by more than 0.20.
5.4 Dialogue act tagging
As our last experiment, we evaluate the word
spaces on a dialogue act tagging task (Stolcke et
al., 2000) over the Switchboard corpus (Godfrey
et al., 1992). Switchboard is a collection of ap-
proximately 2500 dialogs over a telephone line by
500 speakers from the U.S. on predefined topics.
8
The experiment pipeline follows (Milajevs and
Purver, 2014). The input utterances are prepro-
cessed so that the parts of interrupted utterances
are concatenated (Webb et al., 2005). Disfluency
markers and commas are removed from the utter-
ance raw texts. For GS11 and KS14 the utterance
tokens are POS-tagged and lemmatized; for NWE,
we test the vectors in both a lemmatized and an
un-lemmatized version of the corpus.
9
We split
the training and testing utterances as suggested by
Stolcke et al. (2000). Utterance vectors are then
obtained as in the previous experiments; they are
reduced to 50 dimensions using SVD and a k-
nearest-neighbour classifier is trained on these re-
duced utterance vectors (the 5 closest neighbours
by Euclidean distance are retrieved to make a clas-
7
F-scores use the standard definition F = 2(precision ?
recall)/(precision + recall).
8
The dataset and a Python interface to it are available
at http://compprag.christopherpotts.net/
swda.html
9
We use WordNetLemmatizer of the NLTK library
(Bird, 2006).
sification decision). The results are shown in the
second part of Table 5.
Un-lemmatized NWE addition gave the best ac-
curacy (0.63) and F-score (0.60) (averaged over
tag classes), i.e. similar results to (Milajevs and
Purver, 2014)?although note that the dimension-
ality of our NWE vectors is 10 times lower than
theirs. Multiplicative NWE outperformed the cor-
responding model in (Milajevs and Purver, 2014).
In general, addition consistently outperforms mul-
tiplication for all the models. Lemmatization
dramatically lowers tagging accuracy: the lem-
matized GS11, KS14 and NWE models perform
much worse than un-lemmatized NWE, suggest-
ing that morphological features are important for
this task.
6 Discussion
Previous comparisons of co-occurrence-based and
neural word vector representations vary widely
in their conclusions. While Baroni et al. (2014)
conclude that ?context-predicting models obtain
a thorough and resounding victory against their
count-based counterparts?, this seems to contra-
dict, at least at the first consideration, the more
conservative conclusion of Levy et al. (2014) that
?analogy recovery is not restricted to neural word
embeddings [. . . ] a similar amount of relational
similarities can be recovered from traditional dis-
tributional word representations? and the findings
of Blacoe and Lapata (2012) that ?shallow ap-
proaches are as good as more computationally in-
tensive alternatives? on phrase similarity and para-
phrase detection tasks.
It seems clear that neural word embeddings
have an advantage when used in tasks for which
they have been trained; our main questions here
are whether they outperform co-occurrence based
alternatives across the board; and which ap-
proach lends itself better to composition using
general mathematical operators. To partially an-
715
swer this question, we can compare model be-
haviour against the baselines in isolation.
For the disambiguation and sentence similarity
tasks the baseline is the similarity between verbs
only, ignoring the context?see above. For the
paraphrase task, we take the global vector-based
similarity reported in (Mihalcea et al., 2006): 0.65
accuracy and 0.75 F-score. For the dialogue act
tagging task the baseline is the accuracy of the
bag-of-unigrams model in (Milajevs and Purver,
2014): 0.60.
Sections 5.1 and 5.2 show that although the best
choice of vector representation might vary, for
small-scale tasks all methods give fairly compet-
itive results. The choice of compositional oper-
ator seems to be more important and more task-
specific: while a tensor-based operation (Frobe-
nius copy-object) performs best for verb disam-
biguation, the best result for sentence similarity
is achieved by a simple additive model, with all
other compositional methods behaving worse than
the verb-only baseline in the KS14 case. GS11 and
NWE, on the other hand, outperform their base-
lines with a number of compositional methods, al-
though both of them achieve lower performance
than KS14 overall.
Based on only small-scale experiment results,
one could conclude that there is little significant
difference between the two ways of obtaining vec-
tors. GS11 and NWE show similar behaviour in
comparison to their baselines, while it is possible
to tune a co-occurrence based vector space (KS14)
and obtain the best result. Large scale tasks reveal
another pattern: the GS11 vector space, which be-
haves stably on the small scale, drags behind the
KS14 and NWE spaces in the paraphrase detec-
tion task. In addition, NWE consistently yields
best results. Finally, only the NWE space was able
to provide adequate results on the dialogue act tag-
ging task. Table 6 summarizes model performance
with regard to baselines.
7 Conclusion
In this work we compared the performance of two
co-occurrence-based semantic spaces with vectors
learned by a neural network in compositional set-
tings. We carried out two small-scale tasks (word
sense disambiguation and sentence similarity) and
two large-scale tasks (paraphrase detection and di-
alogue act tagging).
Task GS11 KS14 NWE
Disambiguation + + +
Sentence similarity + ? +
Paraphrase ? + +
Dialog act tagging ? ? +
Table 6: Summary of vector space performance
against baselines. General improvement (cases
where more than a half of the models perform bet-
ter) and decrease with regard to a corresponding
baseline is respectively marked by + and ?. A
bold value means that the model gave the best re-
sult in the task.
On small-scale tasks, where the sentence struc-
tures are predefined and relatively constrained,
NWE gives better or similar results to count-based
vectors. Tensor-based composition does not al-
ways outperform simple compositional operators,
but for most of the cases gives results within the
same range.
On large-scale tasks, neural vectors are more
successful than the co-occurrence based alterna-
tives. However, this study does not reveal whether
this is because of their neural nature, or just be-
cause they are trained on a larger amount of data.
The question of whether neural vectors outper-
form co-occurrence vectors therefore requires fur-
ther detailed comparison to be entirely resolved;
our experiments suggest that this is indeed the case
in large-scale tasks, but the difference in size and
nature of the original corpora may be a confound-
ing factor. In any case, it is clear that the neural
vectors of word2vec package perform steadily
off-the-shelf across a large variety of tasks. The
size of the vector space (3 million words) and the
available code-base that simplifies the access to
the vectors, makes this set a good and safe choice
for experiments in the future. Of course, even bet-
ter performances can be achieved by training neu-
ral language models specifically for a given task
(see e.g. Kalchbrenner et al. (2014)).
The choice of compositional operator (tensor-
based or a simple point-wise operation) depends
strongly on the task and dataset: tensor-based
composition performed best with the verb dis-
ambiguation task, where the verb senses depend
strongly on the arguments of the verb. However, it
seems to depend less on the nature of the vectors
itself: in the disambiguation task, tensor-based
716
composition proved best for both co-occurrence-
based and neural vectors; in the sentence similar-
ity task, where point-wise operators proved best,
this was again true across vector spaces.
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Sup-
port by EPSRC grant EP/F042728/1 is grate-
fully acknowledged by Milajevs, Kartsaklis and
Sadrzadeh. Purver is partly supported by Con-
CreTe: the project ConCreTe acknowledges the fi-
nancial support of the Future and Emerging Tech-
nologies (FET) programme within the Seventh
Framework Programme for Research of the Eu-
ropean Commission, under FET grant number
611733.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69?72. Asso-
ciation for Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43?50.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
N. Bourbaki. 1989. Commutative Algebra: Chapters
1-7. Srpinger Verlag, Berlin/New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2-3):281?332.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved May,
29:2013.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563?584.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: deriving Mikolov et al.?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404.
Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language
Semantics, pages 62?66, Edinburgh, UK, July. As-
sociation for Computational Linguistics.
Z.S. Harris. 1954. Distributional structure. Word.
717
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNL), pages 1590?1601, Seat-
tle, USA, October. Association for Computational
Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL), Kyoto,
Japan, June.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549?558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21?30, Gothenburg, Sweden, April.
Association for Computational Linguistics.
T. Landauer and S. Dumais. 1997. A Solution
to Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representation
of Knowledge. Psychological Review.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182?190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 40?47, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373?398.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Hinrich Sch?utze. 1997. Ambiguity resolution in natu-
ral language learning. csli. Stanford, CA, 4:12?36.
718
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
719
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 212?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Resolving Lexical Ambiguity in
Tensor Regression Models of Meaning
Dimitri Kartsaklis
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Nal Kalchbrenner
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
nkalch@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary Univ. of London
School of Electronic Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrnoosh.sadrzadeh@qmul.ac.uk
Abstract
This paper provides a method for improv-
ing tensor-based compositional distribu-
tional models of meaning by the addition
of an explicit disambiguation step prior to
composition. In contrast with previous re-
search where this hypothesis has been suc-
cessfully tested against relatively simple
compositional models, in our work we use
a robust model trained with linear regres-
sion. The results we get in two experi-
ments show the superiority of the prior dis-
ambiguation method and suggest that the
effectiveness of this approach is model-
independent.
1 Introduction
The provision of compositionality in distributional
models of meaning, where a word is represented as
a vector of co-occurrence counts with every other
word in the vocabulary, offers a solution to the
fact that no text corpus, regardless of its size, is
capable of providing reliable co-occurrence statis-
tics for anything but very short text constituents.
By composing the vectors for the words within
a sentence, we are still able to create a vectorial
representation for that sentence that is very useful
in a variety of natural language processing tasks,
such as paraphrase detection, sentiment analysis
or machine translation. Hence, given a sentence
w
1
w
2
. . . w
n
, a compositional distributional model
provides a function f such that:
??
s = f(
??
w
1
,
??
w
2
, . . . ,
??
w
n
) (1)
where
??
w
i
is the distributional vector of the ith
word in the sentence and
??
s the resulting compos-
ite sentential vector.
An interesting question that has attracted the at-
tention of researchers lately refers to the way in
which these models affect ambiguous words; in
other words, given a sentence such as ?a man was
waiting by the bank?, we are interested to know to
what extent a composite vector can appropriately
reflect the intended use of word ?bank? in that con-
text, and how such a vector would differ, for exam-
ple, from the vector of the sentence ?a fisherman
was waiting by the bank?.
Recent experimental evidence (Reddy et al,
2011; Kartsaklis et al, 2013; Kartsaklis and
Sadrzadeh, 2013) suggests that for a number of
compositional models the introduction of a dis-
ambiguation step prior to the actual composi-
tional process results in better composite represen-
tations. In other words, the suggestion is that Eq.
1 should be replaced by:
??
s = f(?(
??
w
1
), ?(
??
w
2
), . . . , ?(
??
w
n
)) (2)
where the purpose of function ? is to return a dis-
ambiguated version of each word vector given the
rest of the context (e.g. all the other words in the
sentence). The composition operation, whatever
that could be, is then applied on these unambigu-
ous representations of the words, instead of the
original distributional vectors.
Until now this idea has been verified on rela-
tively simple compositional functions, usually in-
volving some form of element-wise operation be-
tween the word vectors, such as addition or mul-
tiplication. An exception to this is the work of
Kartsaklis and Sadrzadeh (2013), who apply Eq.
2 on partial tensor-based compositional models.
In a tensor-based model, relational words such
as verbs and adjectives are represented by multi-
linear maps; composition takes place as the ap-
plication of those maps on vectors representing
the arguments (usually nouns). What makes the
models of the above work ?partial? is that the au-
thors used simplified versions of the linear maps,
projected onto spaces of order lower than that re-
quired by the theoretical framework. As a result,
a certain amount of transformational power was
traded off for efficiency.
A potential explanation then for the effective-
ness of the proposed prior disambiguation method
can be sought on the limitations imposed by the
compositional models under test. After all, the
idea of having disambiguation emerge as a direct
212
consequence of the compositional process, with-
out the introduction of any explicit step, seems
more natural and closer to the way the human
mind resolves lexical ambiguities.
The purpose of this paper is to investigate
the hypothesis whether prior disambiguation is
important in a pure tensor-based compositional
model, where no simplifying assumptions have
been made. We create such a model by using lin-
ear regression, and we explain how an explicit dis-
ambiguation step can be introduced to this model
prior to composition. We then proceed by com-
paring the composite vectors produced by this ap-
proach with those produced by the model alone in
a number of experiments. The results show a clear
superiority of the priorly disambiguated models
following Eq. 2, confirming previous research and
suggesting that the reasons behind the success of
this approach are more fundamental than the form
of the compositional function.
2 Composition in distributional models
Compositional distributional models of meaning
vary in sophistication, from simple element-wise
operations between vectors such as addition and
multiplication (Mitchell and Lapata, 2008) to deep
learning techniques based on neural networks
(Socher et al, 2011; Socher et al, 2012; Kalch-
brenner and Blunsom, 2013a). Tensor-based mod-
els, formalized by Coecke et al (2010), comprise
a third class of models lying somewhere in be-
tween these two extremes. Under this setting rela-
tional words such as verbs and adjectives are rep-
resented by multi-linear maps (tensors of various
orders) acting on a number of arguments. An ad-
jective for example is a linear map f : N ? N
(where N is our basic vector space for nouns),
which takes as input a noun and returns a mod-
ified version of it. Since every map of this sort
can be represented by a matrix living in the ten-
sor product space N ? N , we now see that the
meaning of a phrase such as ?red car? is given by
red ?
??
car, where red is an adjective matrix and
? indicates matrix multiplication. The same con-
cept applies for functions of higher order, such as
a transitive verb (a function of two arguments, so
a tensor of order 3). For these cases, matrix mul-
tiplication generalizes to the more generic notion
of tensor contraction. The meaning of a sentence
such as ?kids play games? is computed as:
???
kids
T
? play ?
?????
games (3)
where play here is an order-3 tensor (a ?cube?)
and ? now represents tensor contraction. A con-
cise introduction to compositional distributional
models can be found in (Kartsaklis, 2014).
3 Disambiguation and composition
The idea of separating disambiguation from com-
position first appears in a work of Reddy et al
(2011), where the authors show that the intro-
duction of an explicit disambiguation step prior
to simple element-wise composition is beneficial
for noun-noun compounds. Subsequent work by
Kartsaklis et al (2013) reports very similar find-
ings for verb-object structures, again on additive
and multiplicative models. Finally, in (Kartsaklis
and Sadrzadeh, 2013) these experiments were ex-
tended to include tensor-based models following
the categorical framework of Coecke et al (2010),
where again all ?unambiguous? models present
superior performance compared to their ?ambigu-
ous? versions.
However, in this last work one of the dimen-
sions of the tensors was kept empty (filled in
with zeros). This simplified the calculations but
also weakened the effectiveness of the multi-linear
maps. If, for example, instead of using an order-3
tensor for a transitive verb, one uses some of the
matrix instantiations of Kartsaklis and Sadrzadeh,
Eq. 3 is reduced to one of the following forms:
play  (
???
kids?
?????
games) ,
???
kids (play ?
?????
games)
(
???
kids
T
? play)
?????
games
(4)
where symbol  denotes element-wise multipli-
cation and play is a matrix. Here, the model does
not fully exploit the space provided by the theo-
retical framework (i.e. an order-3 tensor), which
has two disadvantages: firstly, we lose space that
could hold valuable information about the verb in
this case and relational words in general; secondly,
the generally non-commutative tensor contraction
operation is now partly relying on element-wise
multiplication, which is commutative, thus forgets
(part of the) order of composition.
In the next section we will see how to apply lin-
ear regression in order to create full tensors for
verbs and use them for a compositional model that
avoids these pitfalls.
4 Creating tensors for verbs
The essence of any tensor-based compositional
model is the way we choose to create our sentence-
producing maps, i.e. the verbs. In this paper we
adopt a method proposed by Baroni and Zampar-
elli (2010) for building adjective matrices, which
can be generally applied to any relational word.
213
In order to create a matrix for, say, the intransi-
tive verb ?play?, we first collect all instances of
the verb occurring with some subject in the train-
ing corpus, and then we create non-compositional
holistic vectors for these elementary sentences fol-
lowing exactly the same methodology as if they
were words. We now have a dataset with instances
of the form ?
????
subj
i
,
???????
subj
i
play? (e.g. the vector of
?kids? paired with the holistic vector of ?kids play?,
and so on), that can be used to train a linear regres-
sion model in order to produce an appropriate ma-
trix for verb ?play?. The premise of a model like
this is that the multiplication of the verb matrix
with the vector of a new subject will produce a re-
sult that approximates the distributional behaviour
of all these elementary two-word exemplars used
in training.
We present examples and experiments based
on this method, constructing ambiguous and dis-
ambiguated tensors of order 2 (that is, matrices)
for verbs taking one argument. In principle, our
method is directly applicable to tensors of higher
order, following a multi-step process similar to
that of Grefenstette et al (2013) who create order-
3 tensors for transitive verbs using similar means.
Instead of using subject-verb constructs as above
we concentrate on elementary verb phrases of the
form verb-object (e.g. ?play football?, ?admit stu-
dent?), since in general objects comprise stronger
contexts for disambiguating the usage of a verb.
5 Experimental setting
Our basic vector space is trained from the ukWaC
corpus (Ferraresi et al, 2008), originally using as
a basis the 2,000 content words with the highest
frequency (but excluding a list of stop words as
well as the 50 most frequent content words since
they exhibit low information content). We cre-
ated vectors for all content words with at least
100 occurrences in the corpus. As context we
considered a 5-word window from either side of
the target word, while as our weighting scheme
we used local mutual information (i.e. point-wise
mutual information multiplied by raw counts).
This initial semantic space achieved a score of
0.77 Spearman?s ? (and 0.71 Pearson?s r) on the
well-known benchmark dataset of Rubenstein and
Goodenough (1965). In order to reduce the time of
regression training, our vector space was normal-
ized and projected onto a 300-dimensional space
using singular value decomposition (SVD). The
performance of the reduced space on the R&G
dataset was again very satisfying, specifically 0.73
Spearman?s ? and 0.72 Pearson?s r.
In order to create the vector space of the holistic
verb phrase vectors, we first collected all instances
where a verb participating in the experiments ap-
peared at least 100 times in a verb-object relation-
ship with some noun in the corpus. As context of
a verb phrase we considered any content word that
falls into a 5-word window from either side of the
verb or the object. For the 68 verbs participating
in our experiments, this procedure resulted in 22k
verb phrases, a vector space that again was pro-
jected into 300 dimensions using SVD.
Linear regression For each verb we use simple
linear regression with gradient descent directly ap-
plied on matrices X and Y, where the rows of X
correspond to vectors of the nouns that appear as
objects for the given verb and the rows ofY to the
holistic vectors of the corresponding verb phrases.
Our objective function then becomes:
?
W = argmin
W
1
2m
(
?WX
T
?Y
T
?
2
+ ??W?
2
)
(5)
wherem is the number of training examples and ?
a regularization parameter. The matrix W is used
as the tensor for the specific verb.
6 Supervised disambiguation
In our first experiment we test the effectiveness
of a prior disambiguation step for a tensor-based
model in a ?sandbox? using supervised learning.
The goal is to create composite vectors for a num-
ber of elementary verb phrases of the form verb-
object with and without an explicit disambiguation
step, and evaluate which model approximates bet-
ter the holistic vectors of these verb phrases.
The verb phrases of our dataset are based on the
5 ambiguous verbs of Table 1. Each verb has been
combined with two different sets of nouns that ap-
pear in a verb-object relationship with that verb
in the corpus (a total of 343 verb phrases). The
nouns of each set have been manually selected in
order to explicitly represent a different meaning of
the verb. As an example, in the verb ?play? we im-
pose the two distinct meanings of using a musical
instrument and participating in a sport; so the first
Verb Meaning 1 Meaning 2
break violate (56) break (22)
catch capture (28) be on time (21)
play musical instrument (47) sports (29)
admit permit to enter (12) acknowledge (25)
draw attract (64) sketch (39)
Table 1: Ambiguous verbs for the supervised task.
The numbers in parentheses refer to the collected
training examples for each case.
214
set of objects contains nouns such as ?oboe?, ?pi-
ano?, ?guitar?, and so on, while in the second set
we see nouns such as ?football?, ?baseball? etc.
In more detail, the creation of the dataset was
done in the following way: First, all verb entries
with more than one definition in the Oxford Junior
Dictionary (Sansome et al, 2000) were collected
into a list. Next, a linguist (native speaker of En-
glish) annotated the semantic difference between
the definitions of each verb in a scale from 1 (sim-
ilar) to 5 (distinct). Only verbs with definitions
exhibiting completely distinct meanings (marked
with 5) were kept for the next step. For each one
of these verbs, a list was constructed with all the
nouns that appear at least 50 times under a verb-
object relationship in the corpus with the specific
verb. Then, each object in the list was manually
annotated as exclusively belonging to one of the
two senses; so, an object could be selected only if
it was related to a single sense, but not both. For
example, ?attention? was a valid object for the at-
tract sense of verb ?draw?, since it is unrelated to
the sketch sense of that verb. On the other hand,
?car? is not an appropriate object for either sense
of ?draw?, since it could actually appear under both
of them in different contexts. The verbs of Table
1 were the ones with the highest numbers of ex-
emplars per sense, creating a dataset of significant
size for the intended task (each holistic vector is
compared with 343 composite vectors).
We proceed as follows: We apply linear regres-
sion in order to train verb matrices using jointly
the object sets for both meanings of each verb, as
well as separately?so in this latter case we get
two matrices for each verb, one for each sense. For
each verb phrase, we create a composite vector by
matrix-multiplying the verb matrix with the vector
of the specific object. Then we use 4-fold cross
validation to evaluate which version of composite
vectors (the one created by the ambiguous tensors
or the one created by the unambiguous ones) ap-
proximates better the holistic vectors of the verb
phrases in our test set. This is done by comparing
each holistic vector with all the composite ones,
and then evaluating the rank of the correct com-
posite vector within the list of results.
In order to get a proper mixing of objects from
both senses of a verb in training and testing sets,
we set the cross-validation process as follows: We
first split both sets of objects in 4 parts. For each
fold then, our training set is comprised by
3
4
of set
#1 plus
3
4
of set #2, while the test set consists of
the remaining
1
4
of set #1 plus
1
4
of set #2. The
data points of the training set are presented in the
Accuracy MRR Avg Sim
Amb. Dis. Amb. Dis. Amb. Dis.
break 0.19 0.28 0.41 0.50 0.41 0.43
catch 0.35 0.37 0.58 0.61 0.51 0.57
play 0.20 0.28 0.41 0.49 0.60 0.68
admit 0.33 0.43 0.57 0.64 0.41 0.46
draw 0.24 0.29 0.45 0.51 0.40 0.44
Table 2: Results for the supervised task. ?Amb.?
refers to models without the explicit disambigua-
tion step, and ?Dis.? to models with that step.
learning algorithm in random order.
We measure approximation in three different
metrics. The first one, accuracy, is the strictest,
and evaluates in how many cases the composite
vector of a verb phrase is the closest one (the first
one in the result list) to the corresponding holistic
vector. A more relaxed and perhaps more repre-
sentative method is to calculate the mean recipro-
cal rank (MRR), which is given by:
MRR =
1
m
m
?
i=1
1
rank
i
(6)
where m is the number of objects and rank
i
refers
to the rank of the correct composite vector for the
ith object.
Finally, a third way to evaluate the efficiency of
each model is to simply calculate the average co-
sine similarity between every holistic vector and
its corresponding composite vector. The results
are presented in Table 2, reflecting a clear supe-
riority (p < 0.001 for average cosine similarity)
of the prior disambiguation method for every verb
and every metric.
7 Unsupervised disambiguation
In Section 6 we used a controlled procedure to col-
lect genuinely ambiguous verbs and we trained our
models from manually annotated data. In this sec-
tion we briefly outline how the process of creat-
ing tensors for distinct senses of a verb can be au-
tomated, and we test this idea on a generic verb
phrase similarity task.
First, we use unsupervised learning in order to
detect the latent senses of each verb in the corpus,
following a procedure first described by Sch?utze
(1998). For every occurrence of the verb, we cre-
ate a vector representing the surrounding context
by averaging the vectors of every other word in
the same sentence. Then, we apply hierarchical
agglomerative clustering (HAC) in order to cluster
these context vectors, hoping that different groups
of contexts will correspond to the different senses
under which the word has been used in the corpus.
The clustering algorithm uses Ward?s method as
215
inter-cluster measure, and Pearson correlation for
measuring the distance of vectors within a clus-
ter. Since HAC returns a dendrogram embedding
all possible groupings, we measure the quality of
each partitioning by using the variance ratio crite-
rion (Cali?nski and Harabasz, 1974) and we select
the partitioning that achieves the best score (so the
number of senses varies from verb to verb).
The next step is to classify every noun that has
been used as an object with that verb to the most
probable verb sense, and then use these sets of
nouns as before for training tensors for the vari-
ous verb senses. Being equipped with a number of
sense clusters created as above for every verb, the
classification of each object to a relevant sense is
based on the cosine distance of the object vector
from the centroids of the clusters.
1
Every sense
with less than 3 training exemplars is merged to
the dominant sense of the verb. The union of all
object sets is used for training a single unambigu-
ous tensor for the verb. As usual, data points are
presented to learning algorithm in random order.
No objects in our test set are used for training.
We test this system on a verb phase similarity
task introduced in (Mitchell and Lapata, 2010).
The goal is to assess the similarity between pairs
of short verb phrases (verb-object constructs) and
evaluate the results against human annotations.
The dataset consists of 72 verb phrases, paired
in three different ways to form groups of various
degrees of phrase similarity?a total of 108 verb
phrase pairs.
The experiment has the following form: For ev-
ery pair of verb phrases, we construct composite
vectors and then we evaluate their cosine similar-
ity. For the ambiguous regression model, the com-
position is done by matrix-multiplying the am-
biguous verb matrix (learned by the union of all
object sets) with the vector of the noun. For the
disambiguated version, we first detect the most
probable sense of the verb given the noun, again
by comparing the vector of the noun with the
centroids of the verb clusters; then, we matrix-
multiply the corresponding unambiguous tensor
created exclusively from objects that have been
classified as closer to this specific sense of the
verb with the noun. We also test a number
of baselines: the ?verbs-only? model is a non-
compositional baseline where only the two verbs
are compared; ?additive? and ?multiplicative? com-
pose the word vectors of each phrase by applying
simple element-wise operations.
1
In general, our approach is quite close to the multi-
prototype models of Reisinger and Mooney (2010).
Model Spearman?s ?
Verbs-only 0.331
Additive 0.379
Multiplicative 0.301
Linear regression (ambiguous) 0.349
Linear regression (disamb.) 0.399
Holistic verb phrase vectors 0.403
Human agreement 0.550
Table 3: Results for the phrase similarity task. The
difference between the ambiguous and the disam-
biguated version is s.s. with p < 0.001.
The results are presented in Table 3, where
again the version with the prior disambiguation
step shows performance superior to that of the am-
biguous version. There are two interesting obser-
vations that can be made on the basis of Table
3. First of all, the regression model is based on
the assumption that the holistic vectors of the ex-
emplar verb phrases follow an ideal distributional
behaviour that the model aims to approximate as
close as possible. The results of Table 3 confirm
this: using just the holistic vectors of the corre-
sponding verb phrases (no composition is involved
here) returns the best correlation with human an-
notations (0.403), providing a proof that the holis-
tic vectors of the verb phrases are indeed reli-
able representations of each verb phrase?s mean-
ing. Next, observe that the prior disambiguation
model approximates this behaviour very closely
(0.399) on unseen data, with a difference not sta-
tistically significant. This is very important, since
a regression model can only perform as well as its
training dataset alows it; and in our case this is
achieved to a very satisfactory level.
8 Conclusion and future work
This paper adds to existing evidence from previ-
ous research that the introduction of an explicit
disambiguation step before the composition im-
proves the quality of the produced composed rep-
resentations. The use of a robust regression model
rejects the hypothesis that the proposed methodol-
ogy is helpful only for relatively ?weak? composi-
tional approaches. As for future work, an interest-
ing direction would be to see how a prior disam-
biguation step can affect deep learning composi-
tional settings similar to (Socher et al, 2012) and
(Kalchbrenner and Blunsom, 2013b).
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Support
by EPSRC grant EP/F042728/1 is gratefully ac-
knowledged by D. Kartsaklis and M. Sadrzadeh.
216
References
M. Baroni and R. Zamparelli. 2010. Nouns are Vec-
tors, Adjectives are Matrices. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
T. Cali?nski and J. Harabasz. 1974. A Dendrite Method
for Cluster Analysis. Communications in Statistics-
Theory and Methods, 3(1):1?27.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for Distributed Compositional
Model of Meaning. Lambek Festschrift. Linguistic
Analysis, 36:345?384.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
N. Kalchbrenner and P. Blunsom. 2013a. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Compo-
sitionality, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA, October. Association for Computational Lin-
guistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2013.
Separating Disambiguation from Composition in
Distributional Semantics. In Proceedings of 17th
Conference on Computational Natural Language
Learning (CoNLL-2013), Sofia, Bulgaria, August.
Dimitri Kartsaklis. 2014. Compositional operators in
distributional semantics. Springer Science Reviews,
April. DOI: 10.1007/s40362-014-0017-z.
J. Mitchell and M. Lapata. 2008. Vector-based Mod-
els of Semantic Composition. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 705?713.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117. Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of
the ACM, 8(10):627?633.
R. Sansome, D. Reid, and A. Spooner. 2000. The Ox-
ford Junior Dictionary. Oxford University Press.
H. Sch?utze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and
C.D. Manning. 2011. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. Advances in Neural Information Processing
Systems, 24.
R. Socher, B. Huval, C. Manning, and Ng. A.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Conference on Empirical
Methods in Natural Language Processing 2012.
217
Concrete Sentence Spaces for Compositional Distributional
Models of Meaning
Edward Grefenstette?, Mehrnoosh Sadrzadeh?, Stephen Clark?, Bob Coecke?, Stephen Pulman?
?Oxford University Computing Laboratory, ?University of Cambridge Computer Laboratory
firstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.uk
Abstract
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional
semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the
morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional
vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map,
by constructing a corpus-based vector space for the type of sentence. Our construction method is based
on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical
structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun
spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This
enables us to compare meanings of sentences by simply taking the inner product of their vectors.
1 Background
Coecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributional
model of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.
The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector space
semantics. Each word is assigned a grammatical type and a meaning vector in the space corresponding to
its type. The meaning of a sentence is obtained by applying the function corresponding to the grammatical
structure of the sentence to the tensor product of the meanings of the words in the sentence. Based on the
type-logic used, some words will have atomic types and some compound function types. The compound
types live in a tensor space where the vectors are weighted sums (i.e. superpositions) of the pairs of bases
from each space. Compound types are ?applied? to their arguments by taking inner products, in a similar
manner to how predicates are applied to their arguments in Montague semantics.
For the type-logic we use Lambek?s Pregroup grammars [7]. The use of pregoups is not essential, but
leads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).
A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referred
to as an adjoint. It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6]. The
operational difference between a Pregroup and Lambek?s Syntactic Calculus is that, in the latter, the monoid
multiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a left
adjoint, whereas in the pregroup it is the elements themselves which have adjoints. The adjoint types are
used to denote functions, e.g. that of a transitive verb with a subject and object as input and a sentence as
output. In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjoints
of the elements themselves.
As an example, consider the sentence ?dogs chase cats?. We assign the type n (for noun phrase) to ?dog?
and ?cat?, and nrsnl to ?chase?, where nr and nl are the right and left adjoints of n and s is the type of a
125
(declarative) sentence. The type nrsnl expresses the fact that the verb is a predicate that takes two arguments
of type n as input, on its right and left, and outputs the type s of a sentence. The parsing of the sentence is
the following reduction:
n(nrsnl)n ? 1s1 = s
This parse is based on the cancellation of n and nr, and also nl and n; i.e. nnr ? 1 and nln ? 1 for 1
the unit of juxtaposition. The reduction expresses the fact that the juxtapositions of the types of the words
reduce to the type of a sentence.
On the semantic side, we assign the vector space N to the type n, and the tensor space N ?S?N to the
type nrsnl. Very briefly, and in order to introduce some notation, recall that the tensor space A?B has as a
basis the cartesian product of a basis of A with a basis of B. Recall also that any vector can be expressed as
a weighted sum of basis vectors; e.g. if (??v1 , . . . ,??vn) is a basis of A then any vector ??a ? A can be written as
??a =?i Ci??vi where each Ci ? R is a weighting factor. Now for (??v1 , . . . ,??vn) a basis of A and (
??
v?1 , . . . ,
??
v?n)
a basis of B, a vector ??c in the tensor space A ? B can be expressed as follows:
?
ij
Cij (??vi ?
??
v?j )
where the tensor of basis vectors ??vi ?
??
v?j stands for their pair (??vi ,
??
v?j ). In general ??c is not separable into
the tensor of two vectors, except for the case when ??c is not entangled. For non-entangled vectors we can
write ??c = ??a ???b for ??a =
?
i Ci
??vi and
??b =
?
j C ?j
??
v?j ; hence the weighting factor of ??c can be obtained
by simply multiplying the weights of its tensored counterparts, i.e. Cij = Ci ? C ?j . In the entangled case
these weights cannot be determined as such and range over all the possibilities. We take advantage of this
fact to encode meanings of verbs, and in general all words that have compound types and are interpreted as
predicates, relations, or functions. For a brief discussion see the last paragraph of this section. Finally, we
use the Dirac notation to denote the dot or inner product of two vectors ???a | ??b ? ? R defined by
?
i Ci?C ?i.
Returning to our example, for the meanings of nouns we have
???
dogs,??cats ? N , and for the meanings of
verbs we have
????
chase ? N ? S ? N , i.e. the following superposition:
?
ijk
Cijk (??ni ???sj ???nk)
Here ??ni and ??nk are basis vectors of N and ??sj is a basis vector of S. From the categorical translation method
presented in [3] and the grammatical reduction n(nrsnl)n ? s, we obtain the following linear map as the
categorical morphism corresponding to the reduction:
N ? 1s ? N : N ? (N ? S ? N)? N ? S
Using this map, the meaning of the sentence is computed as follows:
???????????
dogs chase cats = (N ? 1s ? N )
(???
dogs ?????chase ???cats
)
= (N ? 1s ? N )
?
?
???
dogs ?
?
?
?
ijk
Cijk(??ni ???sj ???nk)
?
????cats
?
?
=
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats?
The key features of this operation are, first, that the inner-products reduce dimensionality by ?consuming?
tensored vectors and by virtue of the following component function:
N : N ? N ? R :: ??a ?
??b 7? ???a | ??b ?
126
Thus the tensored word vectors
???
dogs ? ????chase ? ??cats are mapped into a sentence space S which is common
to all sentences regardless of their grammatical structure or complexity. Second, note that the tensor product???
dogs?????chase???cats does not need to be calculated, since all that is required for computation of the sentence
vector are the noun vectors and the Cijk weights for the verb. Note also that the inner product operations
are simply picking out basis vectors in the noun space, an operation that can be performed in constant
time. Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which use
the tensor product as a composition operation: first, that the sentence meaning space is high dimensional
and grammatically different sentences have representations with different dimensionalities, preventing them
from being compared directly using inner products; and second, that the space complexity of the tensored
representation grows exponentially with the length and grammatical complexity of the sentence. In constrast,
the model we propose does not require the tensored vectors being combined to be represented explicitly.
Note that we have taken the vector of the transitive verb, e.g.
????
chase, to be an entangled vector in the
tensor space N ? S ?N . But why can this not be a separable vector, in which case the meaning of the verb
would be as follows:
????
chase =
?
i
Ci??ni ?
?
j
C ?j??sj ?
?
k
C ??k??nk
The meaning of the sentence would then become ?1?2
?
j C ?j
??sj for ?1 =
?
i Ci?
???
dogs | ??ni? and ?2 =
?
k C ??k ?
??
cats | ??nk?. The problem is that this meaning only depends on the meaning of the verb and is
independent of the meanings of the subject and object, whereas the meaning from the entangled case,
i.e. ?1?2
?
ijk Cijk
??sj , depends on the meanings of subject and object as well as the verb.
2 From Truth-Theoretic to Corpus-based Meaning
The model presented above is compositional and distributional, but still abstract. To make it concrete, N and
S have to be constructed by providing a method for determining the Cijk weightings. Coecke, Sadrzadeh,
and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework. For
example, assume that N is spanned by all animals and S is the two-dimensional space spanned by ??true and???
false. We use the weighting factor to define a model-theoretic meaning for the verb as follows:
Cijk??sj =
{??
true chase(??ni ,??nk) = true ,???
false o.w.
The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.
So chase(???dogs,???cats) becomes true whenever ?dogs chase cats? is true and false otherwise. This is exactly
how meaning is computed in the model-theoretic view on semantics. One way to generalise this truth-
theoretic meaning is to assume that chase(??ni ,??nk) has degrees of truth, for instance by defining chase as a
combination of run and catch, such as:
chase = 23run+
1
3catch
Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a
worked out example see [3]. But neither of these examples provide a distributional sentence meaning.
Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning
for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes
beyond just composing the meanings of words using a vector operator, such as tensor product, summation
or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as
127
function arguments, according to how the words in the sentence are typed, and uses the syntactic structure
as a guide to determine how the functions are applied to their arguments. The intuition behind this approach
is that syntactic analysis guides semantic vector composition.
The contribution of this paper is to introduce some concrete constructions for a compositional distri-
butional model of meaning. These constructions demonstrate how the mathematical model of [3] can be
implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural
language semantics which is closer to the ideas underlying standard distributional models of word meaning.
We leave full evaluation to future work, in order to determine whether the following method in conjunction
with word vectors built from large corpora leads to improved results on language processing tasks, such as
computing sentence similarity and paraphrase evaluation.
Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are
annotated by ?properties? obtained by combining dependency relations with nouns, verbs and adjectives. For
example, basis vectors might be associated with properties such as ?arg-fluffy?, denoting the argument of
the adjective fluffy, ?subj-chase? denoting the subject of the verb chase, ?obj-buy? denoting the object of the
verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word
has been the argument of ?fluffy?, the subject of ?chase?, the object of ?buy?, and so on.
The framework in [3] offers no guidance as to what the sentence space should consist of. Here we take
the sentence space S to be N ? N , so its bases are of the form ??sj = (??ni ,??nk). The intuition is that, for a
transitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subject
and object.1 The verb vectors Cijk(??ni ,??nk) are built by counting how many times a word that is ni (e.g. has
the property of being fluffy) has been subject of the verb and a word that is nk (e.g. has the property that it?s
bought) has been its object, where the counts are moderated by the extent to which the subject and object
exemplify each property (e.g. how fluffy the subject is). To give a rough paraphrase of the intuition behind
this approach, the meaning of ?dog chases cat? is given by: the extent to which a dog is fluffy and a cat is
something that is bought (for the N ? N property pair ?arg-fluffy? and ?obj-buy?), and the extent to which
fluffy things chase things that are bought (accounting for the meaning of the verb for this particular property
pair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the N ?N
pair ?subj-run? and ?arg-cute?), and the extent to which things that run chase things that are cute (accounting
for the meaning of the verb for this particular property pair); and so on for all noun property pairs.
Adjective Phrases. Adjectives are dealt with in a similar way. We give them the syntactic type nnl and
build their vectors in N ? N . The syntactic reduction nnln ? n associated with applying an adjective to a
noun gives us the map 1N ? N by which we semantically compose an adjective with a noun, as follows:
?????
red fox = (1N ? N )(
??
red ???fox) =
?
ij
Cij??ni???nj |
??
fox?
We can view the Cij counts as determining what sorts of properties the arguments of a particular adjective
typically have (e.g. arg-red, arg-colourful for the adjective ?red?).
Prepositional Phrases. We assign the type nrn to the whole prepositional phrase (when it modifies a noun),
for example to ?in the forest? in the sentence ?dogs chase cats in the forest?. The pregroup parsing is as
follows:
n(nrsnl)n(nrn) ? 1snl1n ? snln ? s1 = s
The vector space corresponding to the prepositional phrase will thus be the tensor space N ? N and the
categorification of the parse will be the composition of two morphisms: (1S?lN )?(rN?1S?1N?rN?1N ).
1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see ?4.
128
The substitution specific to the prepositional phrase happens when computing the vector for ?cats in the
forest? as follows:
?????????????
cats in the forest = (rN ? 1N )
(??
cats ??????????in the forest
)
= (rN ? 1N )
(
??
cats ?
?
lw
Clw??nl ???nk
)
=
?
lw
Clw???cats | ??nl???nw
Here we set the weights Clw in a similar manner to the cases of adjective phrases and verbs with the counts
determining what sorts of properties the noun modified by the prepositional phrase has, e.g. the number of
times something that has attribute nl has been in the forest.
Adverbs. We assign the type srs to the adverb, for example to ?quickly? in the sentence ?Dogs chase cats
quickly?. The pregroup parsing is as follows:
n(nrsnl)n(srs) ? 1s1srs = ssrs ? 1s = s
Its categorification will be a composition of two morphisms (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S). The
substitution specific to the adverb happens after computing the meaning of the sentence without it, i.e. that
of ?Dogs chase cats?, and is as follows:
??????????????????
Dogs chase cats quickly = (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S)
(???
Dogs ?????chase ???cats ??????quickly
)
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?????
quickly
?
?
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?
lw
Clw??sl ???sw
?
?
=
?
lw
Clw
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? | ??sl
?
??sk
The Clw weights are defined in a similar manner to the above cases, i.e. according to the properties the
adverb has, e.g. which verbs it has modified. Note that now the basis vectors ??sl and ??sw are themselves pairs
of basis vectors from the noun space, (??ni ,??nj). Hence, Clw(??ni ,??nj) can be set only for the case when l = i
and w = j; these counts determine what sorts of properties the verbs that happen quickly have (or more
specifically what properties the subjects and objects of such verbs have). By taking the whole sentence into
account in the interpretation of the adverb, we are in a better position to semantically distinguish between
the meaning of adverbs such as ?slowly? and ?quickly?, for instance in terms of the properties that the verb?s
subjects have. For example, it is possible that elephants are more likely to be the subject of a verb which is
happening slowly, e.g. run slowly, and cheetahs are more likely to be the subject of a verb which is happening
quickly.
3 Concrete Computations
In this section we first describe how to obtain the relevant counts from a parsed corpus, and then give some
similarity calculations for some example sentence pairs.
129
Let Cl be the set of grammatical relations (GRs) for sentence sl in the corpus. Define verbs(Cl) to be
the function which returns all instances of verbs in Cl, and subj (and similarly obj ) to be the function which
returns the subject of an instance Vinstance of a verb V , for a particular set of GRs for a sentence:
subj(Vinstance) =
{
noun if Vinstance is a verb with subject noun
?n o.w.
where ?n is the empty string. We express Cijk for a verb V as follows:
Cijk =
{
?
l
?
v?verbs(Cl) ?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? if ??sj = (??ni ,??nk)
0 o.w.
where ?(v, V ) = 1 if v = V and 0 otherwise. Thus we construct Cijk for verb V only for cases where
the subject property ni and the object property nk are paired in the basis ??sj . This is done by counting the
number of times the subject of V has property ni and the object of V has property nk, then multiplying them,
as prescribed by the inner products (which simply pick out the properties ni and nk from the noun vectors
for the subjects and objects).
The procedure for calculating the verb vectors, based on the formulation above, is as follows:
1. For each GR in a sentence, if the relation is subject and the head is a verb, then find the complementary
GR with object as a relation and the same head verb. If none, set the object to ?n.
2. Retrieve the noun vectors
?????subject,????object for the subject dependent and object dependent from previ-
ously constructed noun vectors.
3. For each (ni, nk) ? basis(N)? basis(N) compute the inner-product of ??ni with
?????subject and ??nk with????object (which involves simply picking out the relevant basis vectors from the noun vectors). Multiply
the inner-products and add this to Cijk for the verb, with j such that ??sj = (??ni ,??nk).
The procedure for other grammatical types is similar, based on the definitions of C weights for the semantics
of these types.
We now give a number of example calculations. We first manually define the distributions for nouns,
which in practice would be obtained from a corpus:
bankers cats dogs stock kittens
1. arg-fluffy 0 7 3 0 2
2. arg-ferocious 4 1 6 0 0
3. obj-buys 0 4 2 7 0
4. arg-shrewd 6 3 1 0 1
5. arg-valuable 0 1 2 8 0
We aim to make these counts match our intuitions, in that bankers are shrewd and a little ferocious but not
furry, cats are furry but not typically valuable, and so on.
We also define the distributions for the transitive verbs ?chase?, ?pursue? and ?sell?, again manually
specified according to our intuitions about how these verbs are used. Since in the formalism proposed above,
Cijk = 0 if ??sj 6= (??ni ,??nk), we can simplify the weight matrices for transitive verbs to two dimensional Cik
matrices as shown below, where Cik corresponds to the number of times the verb has a subject with attribute
ni and an object with attribute nk. For example, the matrix below encodes the fact that something ferocious
130
(i = 2) chases something fluffy (k = 1) seven times in the hypothetical corpus from which we might have
obtained these distributions.
Cchase =
?
?
?
?
?
?
1 0 0 0 0
7 1 2 3 1
0 0 0 0 0
2 0 1 0 1
1 0 0 0 0
?
?
?
?
?
?
Cpursue =
?
?
?
?
?
?
0 0 0 0 0
4 2 2 2 4
0 0 0 0 0
3 0 2 0 1
0 0 0 0 0
?
?
?
?
?
?
Csell =
?
?
?
?
?
?
0 0 0 0 0
0 0 3 0 4
0 0 0 0 0
0 0 5 0 8
0 0 1 0 1
?
?
?
?
?
?
These matrices can be used to perform sentence comparisons:
????????????dogs chase cats | ??????????????dogs pursue kittens? =
=
?
?
?
?
ijk
Cchaseijk ?
???
dogs | ??ni???sj ???nk | ??cats?
?
?
?
?
?
?
?
?
?
?
?
ijk
Cpursueijk ?
???
dogs | ??ni???sj ???nk |
?????
kittens?
?
?
?
=
?
ijk
Cchaseijk C
pursue
ijk ?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats????nk |
?????
kittens?
The raw number obtained from the above calculation is 14844. Normalising it by the product of the length
of both sentence vectors gives the cosine value of 0.979.
Consider now the sentence comparison ????????????dogs chase cats | ???????????cats chase dogs?. The sentences in this pair
contain the same words but the different word orders give the sentences very different meanings. The raw
number calculated from this inner product is 7341, and its normalised cosine measure is 0.656, which demon-
strates the sharp drop in similarity obtained from changing sentence structure. We expect some similarity
since there is some non-trivial overlap between the properties identifying cats and those identifying dogs
(namely those salient to the act of chasing).
Our final example for transitive sentences is ????????????dogs chase cats | ????????????bankers sell stock?, as two sentences that
diverge in meaning completely. The raw number for this inner product is 6024, and its cosine measure is
0.042, demonstrating the very low semantic similarity between these two sentences.
Next we consider some examples involving adjective-noun modification. The Cij counts for an adjective
A are obtained in a similar manner to transitive or intransitive verbs:
Cij =
{
?
l
?
a?adjs(Cl) ?(a,A)?
???????
arg-of(a) | ??ni? if ??ni = ??nj
0 o.w.
where adjs(Cl) returns all instances of adjectives in Cl; ?(a,A) = 1 if a = A and 0 otherwise; and
arg-of(a) = noun if a is an adjective with argument noun, and ?n otherwise.
As before, we stipulate the Cij matrices by hand (and we eliminate all cases where i 6= j since Cij = 0
by definition in such cases):
Cfluffy = [9 3 4 2 2] Cshrewd = [0 3 1 9 1] Cvaluable = [3 0 8 1 8]
We compute vectors for ?fluffy dog? and ?shrewd banker? as follows:
???????
fluffy dog = (3 ? 9)???????arg-fluffy+ (6 ? 3)?????????arg-ferocious+ (2 ? 4)??????obj-buys+ (5 ? 2)????????arg-shrewd+ (2 ? 2)?????????arg-valuable
???????????
shrewd banker = (0 ? 0)???????arg-fluffy+ (4 ? 3)?????????arg-ferocious+ (0 ? 0)??????obj-buys+ (6 ? 9)????????arg-shrewd+ (0 ? 1)?????????arg-valuable
Vectors for
???????
fluffy cat and
??????????
valuable stock are computed similarly. We obtain the following similarity mea-
sures:
cosine(???????fluffy dog,???????????shrewd banker) = 0.389 cosine(???????fluffy cat,??????????valuable stock) = 0.184
131
These calculations carry over to sentences which contain the adjective-noun pairings compositionally and
we obtain an even lower similarity measure between sentences:
cosine(????????????????????fluffy dogs chase fluffy cats,?????????????????????????shrewd bankers sell valuable stock) = 0.016
To summarise, our example vectors provide us with the following similarity measures:
Sentence 1 Sentence 2 Degree of similarity
dogs chase cats dogs pursue kittens 0.979
dogs chase cats cats chase dogs 0.656
dogs chase cats bankers sell stock 0.042
fluffy dogs chase fluffy cats shrewd bankers sell valuable stock 0.016
4 Different Grammatical Structures
So far we have only presented the treatment of sentences with transitive verbs. For sentences with intransitive
verbs, the sentence space suffices to be just N . To compare the meaning of a transitive sentence with an
intransitive one, we embed the meaning of the latter from N into the former N ? N , by taking ???n (the
?object? of an intransitive verb) to be
?
i
??ni , i.e. the superposition of all basis vectors of N .
Following the method for the transitive verb, we calculate Cijk for an instransitive verb V and basis pair??sj = (??ni ,??nk) as follows, where l ranges over the sentences in the corpus:
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? =
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni?????n | ??nk?
and ????n | ??ni? = 1 for any basis vector ni.
We can now compare the meanings of transitive and intransitive sentences by taking the inner product of
their meanings (despite the different arities of the verbs) and then normalising it by vector length to obtain
the cosine measure. For example:
????????????dogs chase cats | ????????dogs chase? =
?
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ???cats ?
?
?
?
?
?
?
?
?
?
?
?
ijk
C ?ijk?
???
dogs | ??ni???sj
?
?
?
=
?
ijk
CijkC ?ijk?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats?
The raw number for the inner product is 14092 and its normalised cosine measure is 0.961, indicating high
similarity (but some difference) between a sentence with a transitive verb and one where the subject remains
the same, but the verb is used intransitively.
Comparing sentences containing nouns modified by adjectives to sentences with unmodified nouns is straight-
forward:
?????????????????????fluffy dogs chase fluffy cats | ???????????dogs chase cats? =
?
ij
Cfluffyi C
fluffy
j Cchaseij Cchaseij ?
???dogs | ??ni?2???nj |
???cats?2 = 2437005
132
From the above we obtain the following similarity measure:
cosine(????????????????????fluffy dogs chase fluffy cats,???????????dogs chase cats) = 0.971
For sentences with ditransitive verbs, the sentence space changes to N ? N ? N , on the basis of the verb
needing two objects; hence its grammatical type changes to nrsnlnl. The transitive and intransitive verbs
are embedded in this larger space in a similar manner to that described above; hence comparison of their
meanings becomes possible.
5 Ambiguous Words
The two different meanings of a word can be distinguished by the different properties that they have. These
properties are reflected in the corpus, by the different contexts in which the words appear. Consider the
following example from [4]: the verb ?catch? has two different meanings, ?grab? and ?contract?. They are
reflected in the two sentences ?catch a ball? and ?catch a disease?. The compositional feature of our meaning
computation enables us to realise the different properties of the context words via the grammatical roles they
take in the corpus. For instance, the word ?ball? occurs as argument of ?round?, and so has a high weight
for the base ?arg-round?, whereas the word ?disease? has a high weight for the base ?arg-contagious? and as
?mod-of-heart?. We extend our example corpus from previously to reflect these differences as follows:
ball disease
1. arg-fluffy 1 0
2. arg-ferocious 0 0
3. obj-buys 5 0
4. arg-shrewd 0 0
5. arg-valuable 1 0
6. arg-round 8 0
7. arg-contagious 0 7
8. mod-of-heart 0 6
In a similar way, we build a matrix for the verb ?catch? as follows:
Ccatch =
?
?
?
?
?
?
?
?
?
?
?
?
3 2 3 3 3 8 6 2
3 2 3 0 1 4 7 4
2 4 7 1 1 6 2 2
3 1 2 0 0 3 6 2
1 1 1 0 0 2 0 1
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
?
?
?
?
?
?
?
?
?
?
?
?
The last three rows are zero because we have assumed that the words that can take these roles are mostly
objects and hence cannot catch anything. Given these values, we compute the similarity measure between
the two sentences ?dogs catch a ball? and ?dogs catch a disease? as follows:
?????????????dogs catch a ball | ??????????????dogs catch a disease? = 0
In an idealised case like this where there is very little (or no) overlap between the properties of the objects
associated with one sense of ?catch? (e.g. a disease), and those properties of the objects associated with an-
other sense (e.g. a ball), disambiguation is perfect in that there is no similarity between the resulting phrases.
133
In practice, in richer vector spaces, we would expect even diseases and balls to share some properties. How-
ever, as long as those shared properties are not those typically held by the object of catch, and as long as the
usages of catch play to distinctive properties of diseases and balls, disambiguation will occur by the same
mechanism as the idealised case above, and we can expect low similarity measures between such sentences.
6 Related Work
Mitchell and Lapata introduce and evaluate a multiplicative model for vector composition [8]. The particular
concrete construction of this paper differs from that of [8] in that our framework subsumes truth-theoretic
as well as corpus-based meaning, and our meaning construction relies on and is guided by the grammatical
structure of the sentence. The approach of [4] is more in the spirit of ours, in that extra information about
syntax is used to compose meaning. Similar to us, they use a structured vector space to integrate lexical
information with selectional preferences. Finally, Baroni and Zamparelli model adjective-noun combinations
by treating an adjective as a function from noun space to noun space, represented using a matrix, as we do
in this paper [1].
References
[1] M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun construc-
tions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),
Cambridge, MA, 2010.
[2] S. Clark and S. Pulman. Combining symbolic and distributional models of meaning. In Proceedings of AAAI
Spring Symposium on Quantum Interaction. AAAI Press, 2007.
[3] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical Foundations for a Compositional Dis-
tributional Model of Meaning, volume 36. Linguistic Analysis (Lambek Festschrift), 2010.
http://arxiv.org/abs/1003.4394.
[4] K. Erk and S. Pado?. A structured vector space model for word meaning in context. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-08), pages 897?906, Honolulu, Hawaii, 2008.
[5] G. Grefenstette. Use of syntactic context to produce term association lists for text retrieval. In Nicholas J. Belkin,
Peter Ingwersen, and Annelise Mark Pejtersen, editors, SIGIR, pages 89?97. ACM, 1992.
[6] Z. Harris. Mathematical Structures of Language. Interscience Publishers John Wiley and Sons, 1968.
[7] J. Lambek. From Word to Sentence. Polimetrica, 2008.
[8] J. Mitchell and M. Lapata. Vector-based models of semantic composition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics, pages 236?244, Columbus, OH, 2008.
[9] P. Smolensky and G. Legendre. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar
Vol. I: Cognitive Architecture Vol. II: Linguistic and Philosophical Implications. MIT Press, 2005.
134
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 62?66,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Experimenting with Transitive Verbs in a DisCoCat
Edward Grefenstette
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
edward.grefenstette@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
mehrs@cs.ox.ac.uk
Abstract
Formal and distributional semantic models
offer complementary benefits in modeling
meaning. The categorical compositional dis-
tributional model of meaning of Coecke et al
(2010) (abbreviated to DisCoCat in the title)
combines aspects of both to provide a gen-
eral framework in which meanings of words,
obtained distributionally, are composed using
methods from the logical setting to form sen-
tence meaning. Concrete consequences of
this general abstract setting and applications to
empirical data are under active study (Grefen-
stette et al, 2011; Grefenstette and Sadrzadeh,
2011). In this paper, we extend this study by
examining transitive verbs, represented as ma-
trices in a DisCoCat. We discuss three ways of
constructing such matrices, and evaluate each
method in a disambiguation task developed by
Grefenstette and Sadrzadeh (2011).
1 Background
The categorical distributional compositional model
of meaning of Coecke et al (2010) combines the
modularity of formal semantic models with the em-
pirical nature of vector space models of lexical se-
mantics. The meaning of a sentence is defined to
be the application of its grammatical structure?
represented in a type-logical model?to the kro-
necker product of the meanings of its words, as
computed in a distributional model. The concrete
and experimental consequences of this setting, and
other models that aim to bring together the log-
ical and distributional approaches, are active top-
ics in current natural language semantics research,
e.g. see (Grefenstette et al, 2011; Grefenstette and
Sadrzadeh, 2011; Clark et al, 2010; Baroni and
Zamparelli, 2010; Guevara, 2010; Mitchell and La-
pata, 2008).
In this paper, we focus on our recent concrete Dis-
CoCat model (Grefenstette and Sadrzadeh, 2011)
and in particular on nouns composed with transitive
verbs. Whereby the meaning of a transitive sentence
?sub tverb obj? is obtained by taking the component-
wise multiplication of the matrix of the verb with
the kronecker product of the vectors of subject and
object:
?????????
sub tverb obj = tverb (
??
sub?
??
obj) (1)
In most logical models, transitive verbs are modeled
as relations; in the categorical model the relational
nature of such verbs gets manifested in their ma-
trix representation: if subject and object are each r-
dimensional row vectors in some space N , the verb
will be a r ? r matrix in the space N ? N . There
are different ways of learning the weights of this ma-
trix. In (Grefenstette and Sadrzadeh, 2011), we de-
veloped and implemented one such method on the
data from the British National Corpus. The matrix of
each verb was constructed by taking the sum of the
kronecker products of all of the subject/object pairs
linked to that verb in the corpus. We refer to this
method as the indirect method. This is because the
weight cij is obtained from the weights of the sub-
ject and object vectors (computed via co-occurrence
with bases ??n i and
??n j respectively), rather than di-
rectly from the context of the verb itself, as would
be the case in lexical distributional models. This
construction method was evaluated against an exten-
62
sion of Mitchell and Lapata (2008)?s disambiguation
task from intransitive to transitive sentences. We
showed and discussed how and why our method,
which is moreover scalable and respects the gram-
matical structure of the sentence, resulted in better
results than other known models of semantic vector
composition.
As a motivation for the current paper, note that
there are at least two different factors at work in
Equation (1): one is the matrix of the verb, denoted
by tverb, and the other is the kronecker product of
subject and object vectors
??
sub ?
??
obj. Our model?s
mathematical formulation of composition prohibits
us from changing the latter kronecker product, but
the ?content? of the verb matrices can be built
through different procedures.
In recent work we used a standard lexical distri-
butional model for nouns and engineered our verbs
to have a more sophisticated structure because of
the higher dimensional space they occupy. In par-
ticular, we argued that the resulting matrix of the
verb should represent ?the extent according to which
the verb has related the properties of subjects to the
properties of its objects?, developed a general proce-
dure to build such matrices, then studied their em-
pirical consequences. One question remained open:
what would be the consequence of starting from the
standard lexical vector of the verb, then encoding
it into the higher dimensional space using different
(possibly ad-hoc but nonetheless interesting) mathe-
matically inspired methods.
In a nutshell, the lexical vector of the verb is de-
noted by
???
tverb and similar to vectors of subject and
object, it is an r-dimensional row vector. Since the
kronecker product of subject and object (
??
sub?
??
obj)
is r ? r, in order to make
???
tverb applicable in Equa-
tion 1, i.e. to be able to substitute it for tverb, we
need to encode it into a r ? r matrix in the N ? N
space. In what follows, we investigate the empirical
consequences of three different encodings methods.
2 From Vectors to Matrices
In this section, we discuss three different ways of en-
coding r dimensional lexical verb vectors into r? r
verb matrices, and present empirical results for each.
We use the additional structure that the kronecker
product provides to represent the relational nature
of transitive verbs. The results are an indication that
the extra information contained in this larger space
contributes to higher quality composition.
One way to encode an r-dimensional vector as a
r ? r matrix is to embed it as the diagonal of that
matrix. It remains open to decide what the non-
diagonal values should be. We experimented with
0s and 1s as padding values. If the vector of the verb
is [c1, c2, ? ? ? , cr] then for the 0 case (referred to as
0-diag) we obtain the following matrix:
tverb =
?
?
?
?
?
c1 0 ? ? ? 0
0 c2 ? ? ? 0
...
...
. . .
...
0 0 . . . cr
?
?
?
?
?
For the 1 case (referred to as 1-diag) we obtain the
following matrix:
tverb =
?
?
?
?
?
c1 1 ? ? ? 1
1 c2 ? ? ? 1
...
...
. . .
...
1 1 . . . cr
?
?
?
?
?
We also considered a third case where the vector is
encoded into a matrix by taking the kronecker prod-
uct of the verb vector with itself:
tverb =
???
tverb?
???
tverb
So for
???
tverb = [c1, c2, ? ? ? , cr] we obtain the follow-
ing matrix:
tverb =
?
?
?
?
?
c1c1 c1c2 ? ? ? c1cr
c2c1 c2c2 ? ? ? c2cr
...
...
. . .
...
crc1 crc2 ? ? ? crcr
?
?
?
?
?
3 Degrees of synonymity for sentences
The degree of synonymity between meanings of
two sentences is computed by measuring their ge-
ometric distance. In this work, we used the co-
sine measure. For two sentences ?sub1 tverb1 obj1?
and ?sub2 tverb2 obj2?, this is obtained by taking
the Frobenius inner product of
???????????
sub1 tverb1 obj1 and???????????
sub2 tverb2 obj2. The use of Frobenius product
rather than the dot product is because the calcula-
tion in Equation (1) produces matrices rather than
row vectors. We normalized the outputs by the mul-
tiplication of the lengths of their corresponding ma-
trices.
63
4 Experiment
In this section, we describe the experiment used to
evaluate and compare these three methods. The ex-
periment is on the dataset developed in (Grefenstette
and Sadrzadeh, 2011).
Parameters We used the parameters described by
Mitchell and Lapata (2008) for the noun and verb
vectors. All vectors were built from a lemmatised
version of the BNC. The noun basis was the 2000
most common context words, basis weights were
the probability of context words given the target
word divided by the overall probability of the con-
text word. These features were chosen to enable
easy comparison of our experimental results with
those of Mitchell and Lapata?s original experiment,
in spite of the fact that there may be more sophisti-
cated lexical distributional models available.
Task This is an extension of Mitchell and Lap-
ata (2008)?s disambiguation task from intransitive
to transitive sentences. The general idea behind
the transitive case (similar to the intransitive one) is
as follows: meanings of ambiguous transitive verbs
vary based on their subject-object context. For in-
stance the verb ?meet? means ?satisfied? in the con-
text ?the system met the criterion? and it means
?visit?, in the context ?the child met the house?.
Hence if we build meaning vectors for these sen-
tences compositionally, the degrees of synonymity
of the sentences can be used to disambiguate the
meanings of the verbs in them.
Suppose a verb has two meanings a and b and
that it has occurred in two sentences. Then if in
both of these sentences it has its meaning a, the two
sentences will have a high degree of synonymity,
whereas if in one sentence the verb has meaning a
and in the other meaning b, the sentences will have
a lower degree of synonymity. For instance ?the sys-
tem met the criterion? and ?the system satisfied the
criterion? have a high degree of semantic similarity,
and similarly for ?the child met the house? and ?the
child visited the house?. This degree decreases for
the pair ?the child met the house? and ?the child sat-
isfied the house?.
Dataset The dataset is built using the same guide-
lines as Mitchell and Lapata (2008), using transi-
tive verbs obtained from CELEX1 paired with sub-
jects and objects. We first picked 10 transitive verbs
from the most frequent verbs of the BNC. For each
verb, two different non-overlapping meanings were
retrieved, by using the JCN (Jiang Conrath) infor-
mation content synonymity measure of WordNet to
select maximally different synsets. For instance for
?meet? we obtained ?visit? and ?satisfy?. For each
original verb, ten sentences containing that verb with
the same role were retrieved from the BNC. Exam-
ples of such sentences are ?the system met the crite-
rion? and ?the child met the house?. For each such
sentence, we generated two other related sentences
by substituting their verbs by each of their two syn-
onyms. For instance, we obtained ?the system sat-
isfied the criterion? and ?the system visited the cri-
terion? for the first meaning and ?the child satisfied
the house? and ?the child visited the house? for the
second meaning . This procedure provided us with
200 pairs of sentences.
The dataset was split into four non-identical sec-
tions of 100 entries such that each sentence appears
in exactly two sections. Each section was given to
a group of evaluators who were asked to assign a
similarity score to simple transitive sentence pairs
formed from the verb, subject, and object provided
in each entry (e.g. ?the system met the criterion?
from ?system meet criterion?). The scoring scale for
human judgement was [1, 7], where 1 was most dis-
similar and 7 most identical.
Separately from the group annotation, each pair in
the dataset was given the additional arbitrary classi-
fication of HIGH or LOW similarity by the authors.
Evaluation Method To evaluate our methods, we
first applied our formulae to compute the similar-
ity of each phrase pair on a scale of [0, 1] and then
compared it with human judgement of the same
pair. The comparison was performed by measuring
Spearman?s ?, a rank correlation coefficient ranging
from -1 to 1. This provided us with the degree of
correlation between the similarities as computed by
our model and as judged by human evaluators.
Following Mitchell and Lapata (2008), we also
computed the mean of HIGH and LOW scores.
However, these scores were solely based on the au-
thors? personal judgements and as such (and on their
1http://celex.mpi.nl/
64
own) do not provide a very reliable measure. There-
fore, like Mitchell and Lapata (2008), the models
were ultimately judged by Spearman?s ?.
The results are presented in table 4. The additive
and multiplicative rows have, as composition oper-
ation, vector addition and component-wise multipli-
cation. The Baseline is from a non-compositional
approach; it is obtained by comparing the verb vec-
tors of each pair directly and ignoring their subjects
and objects. The UpperBound is set to be inter-
annotator agreement.
Model High Low ?
Baseline 0.47 0.44 0.16
Add 0.90 0.90 0.05
Multiply 0.67 0.59 0.17
Categorical
Indirect matrix 0.73 0.72 0.21
0-diag matrix 0.67 0.59 0.17
1-diag matrix 0.86 0.85 0.08
v ? v matrix 0.34 0.26 0.28
UpperBound 4.80 2.49 0.62
Table 1: Results of compositional disambiguation.
The indirect matrix performed better than the
vectors encoded in diagonal matrices padded with
0 and 1. However, surprisingly, the kronecker prod-
uct of this vector with itself provided better results
than all the above. The results were statistically sig-
nificant with p < 0.05.
5 Analysis of the Results
Suppose the vector of subject is [s1, s2, ? ? ? , sr] and
the vector of object is
??
obj = [o1, o2, ? ? ? , or], then
the matrix of
??
sub?
??
obj is:
?
?
?
?
?
s1o1 s1o2 ? ? ? s1or
s2o1 s2o2 ? ? ? s2or
...
sro1 sro2 ? ? ? sror
?
?
?
?
?
After computing Equation (1) for each generation
method of tverb, we obtain the following three ma-
trices for the meaning of a transitive sentence:
0-diag :
?
?
?
?
?
c1s1o1 0 ? ? ? 0
0 c2s2o2 ? ? ? 0
...
...
. . .
...
0 0 ? ? ? crsror
?
?
?
?
?
This method discards all of the non-diagonal infor-
mation about the subject and object, for example
there is no occurrence of s1o2, s2o1, etc.
1-diag :
?
?
?
?
?
c1s1o1 s1o2 ? ? ? s1or
s2o1 c2s2o2 ? ? ? s2or
...
...
. . .
...
sro1 sro2 ? ? ? crsror
?
?
?
?
?
This method conserves the information about the
subject and object, but only applies the information
of the verb to the diagonals: s1 and o2, s2 and o1,
etc. are never related to each other via the verb.
v ? v :
?
?
?
?
?
c1c1s1o1 c1c2s1o2 ? ? ? c1crs1or
c2c1s2o1 c2c2s2o2 ? ? ? c2crs2or
...
...
. . .
...
crc1sro1 crc2sro2 ? ? ? crcrsror
?
?
?
?
?
This method not only conserves the information
of the subject and object, but also applies to them
all of the information encoded in the verb. These
data propagate to Frobenius products when comput-
ing the semantic similarity of sentences and justify
the empirical results.
The unexpectedly good performance of the v ? v
matrix relative to the more complex indirect method
is surprising, and certainly demands further inves-
tigation. What is sure is that they each draw upon
different aspects of semantic composition to provide
better results. There is certainly room for improve-
ment and empirical optimisation in both of these
relation-matrix construction methods.
Furthermore, the success of both of these meth-
ods relative to the others examined in Table 1 shows
that it is the extra information provided in the matrix
(rather than just the diagonal, representing the lexi-
cal vector) that encodes the relational nature of tran-
sitive verbs, thereby validating in part the require-
ment suggested in Coecke et al (2010) and Grefen-
stette and Sadrzadeh (2011) that relational word vec-
tors live in a space the dimensionality of which be a
function of the arity of the relation.
65
References
H. Alshawi (ed). 1992. The Core Language Engine.
MIT Press.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices. Proceedings of Conference
on Empirical Methods in Natural Language Processing
(EMNLP).
D. Clarke, R. Lutz and D. Weir. 2010. Semantic
Composition with Quotient Algebras. Proceedings
of Geometric Models of Natural Language Semantics
(GEMS-2010).
S. Clark and S. Pulman. 2007. Combining Symbolic
and Distributional Models of Meaning. Proceedings
of AAAI Spring Symposium on Quantum Interaction.
AAAI Press.
B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-
ical Foundations for Distributed Compositional Model
of Meaning. Lambek Festschrift. Linguistic Analysis
36, 345?384. J. van Benthem, M. Moortgat and W.
Buszkowski (eds.).
J. Curran. 2004. From Distributional to Semantic Simi-
larity. PhD Thesis, University of Edinburgh.
K. Erk and S. Pado?. 2004. A Structured Vector Space
Model for Word Meaning in Context. Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 897?906.
G. Frege 1892. U?ber Sinn und Bedeutung. Zeitschrift
fu?r Philosophie und philosophische Kritik 100.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis.
E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,
S. Pulman. 2011. Concrete Compositional Sentence
Spaces for a Compositional Distributional Model of
Meaning. International Conference on Computational
Semantics (IWCS?11). Oxford.
E. Grefenstette, M. Sadrzadeh. 2011. Experimental Sup-
port for a Categorical Compositional Distributional
Model of Meaning. Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer.
E. Guevara. 2010. A Regression Model of Adjective-
Noun Compositionality in Distributional Semantics.
Proceedings of the ACL GEMS Workshop.
Z. S. Harris. 1966. A Cycling Cancellation-Automaton
for Sentence Well-Formedness. International Compu-
tation Centre Bulletin 5, 69?94.
R. Hudson. 1984. Word Grammar. Blackwell.
J. Lambek. 2008. From Word to Sentence. Polimetrica,
Milan.
T. Landauer, and S. Dumais. 2008. A solution to Platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to information retrieval. Cambridge Uni-
versity Press.
J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, 236?244.
R. Montague. 1974. English as a formal language. For-
mal Philosophy, 189?223.
J. Nivre 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-
tion: The role of distributional cues. Journal of Mem-
ory and Language 35, 606?621.
H. Schuetze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics 24, 97?123.
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Computational Linguistics 46, 1?
2, 159?216.
M. Steedman. 2000. The Syntactic Process. MIT Press.
D. Widdows. 2005. Geometry and Meaning. University
of Chicago Press.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
66
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 41?51,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
The Frobenius Anatomy of Relative Pronouns
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Bob Coecke
University of Oxford
Dept. of Computer Science
coecke@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary, University of London
School of Electronic
Engineering and Computer Science
mehrnoosh.sadrzadeh@eecs.qmul.ac.uk
Abstract
This paper develops a compositional
vector-based semantics of relative pro-
nouns within a categorical framework.
Frobenius algebras are used to formalise
the operations required to model the se-
mantics of relative pronouns, including
passing information between the relative
clause and the modified noun phrase, as
well as copying, combining, and discard-
ing parts of the relative clause. We de-
velop two instantiations of the abstract se-
mantics, one based on a truth-theoretic ap-
proach and one based on corpus statistics.
1 Introduction
Ordered algebraic structures and sequent calculi
have been used extensively in Computer Science
and Mathematical Logic. They have also been
used to formalise and reason about natural lan-
guage. Lambek (1958) used the ordered alge-
bra of residuated monoids to model grammatical
types, their juxtapositions and reductions. Rela-
tional words such as verbs have implicative types
and are modelled using the residuals to the monoid
multiplication. Later, Lambek (1999) simplified
these algebras in favour of pregroups. Here, there
are no binary residual operations, but each element
of the algebra has a left and a right residual.
In terms of semantics, pregroups do not natu-
rally lend themselves to a model-theoretic treat-
ment (Montague, 1974). However, pregroups are
suited to a radically different treatment of seman-
tics, namely distributional semantics (Schu?tze,
1998). Distributional semantics uses vector spaces
based on contextual co-occurrences to model the
meanings of words. Coecke et al (2010) show
how a compositional semantics can be developed
within a vector-based framework, by exploiting
the fact that vector spaces with linear maps and
pregroups both have a compact closed categor-
ical structure (Kelly and Laplaza, 1980; Preller
and Lambek, 2007). Some initial attempts at im-
plementation include Grefenstette and Sadrzadeh
(2011a) and Grefenstette and Sadrzadeh (2011b).
One problem with the distributional approach is
that it is difficult to see how the meanings of some
words ? e.g. logical words such as and, or, and
relative pronouns such as who, which, that, whose
? can be modelled contextually. Our focus in this
paper is on relative pronouns in the distributional
compositional setting.
The difficulty with pronouns is that the contexts
in which they occur do not seem to provide a suit-
able representation of their meanings: pronouns
tend to occur with a great many nouns and verbs.
Hence, if one applies the contextual co-occurrence
methods of distributional semantics to them, the
result will be a set of dense vectors which do
not discriminate between different meanings. The
current state-of-the-art in compositional distribu-
tional semantics either adopts a simple method to
obtain a vector for a sequence of words, such as
adding or mutliplying the contextual vectors of
the words (Mitchell and Lapata, 2008), or, based
on the grammatical structure, builds linear maps
for some words and applies these to the vector
representations of the other words in the string
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011a). Neither of these approaches
produce vectors which provide a good representa-
tion for the meanings of relative clauses.
In the grammar-based approach, one has to as-
sign a linear map to the relative pronoun, for in-
stance a map f as follows:
??????????????
men who like Mary = f(???men,
???????
like Mary)
However, it is not clear what this map should be.
Ideally, we do not want it to depend on the fre-
quency of the co-occurrence of the relative pro-
noun with the relevant basis vectors. But both
41
of the above mentioned approaches rely heavily
on the information provided by a corpus to build
their linear maps. The work of Baroni and Zam-
parelli (2010) uses linear regression and approxi-
mates the context vectors of phrases in which the
target word has occurred, and the work of Grefen-
stette and Sadrzadeh (2011a) uses the sum of Kro-
necker products of the arguments of the target
word across the corpus.
The semantics we develop for relative pronouns
and clauses uses the general operations of a Frobe-
nius algebra over vector spaces (Coecke et al,
2008) and the structural categorical morphisms of
vector spaces. We do not rely on the co-occurrence
frequencies of the pronouns in a corpus and only
take into account the structural roles of the pro-
nouns in the meaning of the clauses. The computa-
tions of the algebra and vector spaces are depicted
using string diagrams (Joyal and Street, 1991),
which depict the interactions that occur among the
words of a sentence. In the particular case of rel-
ative clauses, they visualise the role of the rela-
tive pronoun in passing information between the
clause and the modified noun phrase, as well as
copying, combining, and even discarding parts of
the relative clause.
We develop two instantiations of the abstract se-
mantics, one based on a truth-theoretic approach,
and one based on corpus statistics, where for the
latter the categorical operations are instantiated as
matrix multiplication and vector component-wise
multiplication. As a result, we will obtain the fol-
lowing for the meaning of a subject relative clause:
??????????????
men who like Mary = ???men (love?
????
Mary)
The rest of the paper introduces the categorical
framework, including the formal definitions rel-
evant to the use of Frobenius algebras, and then
shows how these structures can be used to model
relative pronouns within the compositional vector-
based setting.
2 Compact Closed Categories and
Frobenius Algebras
This section briefly reviews compact closed cate-
gories and Frobenius algebras. For a formal pre-
sentation, see (Kelly and Laplaza, 1980; Kock,
2003; Baez and Dolan, 1995), and for an informal
introduction see Coecke and Paquette (2008).
A compact closed category has objects A,B;
morphisms f : A ? B; a monoidal tensor A? B
that has a unit I; and for each objectA two objects
Ar andAl together with the following morphisms:
A?Ar
rA?? I
?rA?? Ar ?A
Al ?A
lA?? I
?lA?? A?Al
These morphisms satisfy the following equalities,
sometimes referred to as the yanking equalities,
where 1A is the identity morphism on object A:
(1A ? 
l
A) ? (?
l
A ? 1A) = 1A
(rA ? 1A) ? (1A ? ?
r
A) = 1A
(lA ? 1A) ? (1Al ? ?
l
A) = 1Al
(1Ar ? 
r
A) ? (?
r
A ? 1Ar) = 1Ar
A pregroup is a partial order compact closed
category, which we refer to as Preg. This means
that the objects of Preg are elements of a par-
tially ordered monoid, and between any two ob-
jects p, q ? Preg there exists a morphism of type
p ? q iff p ? q. Compositions of morphisms
are obtained by transitivity and the identities by
reflexivity of the partial order. The tensor of the
category is the monoid multiplication, and the ep-
silon and eta maps are as follows:
rp = p ? p
r ? 1 ?rp = 1 ? p
r ? p
lp = p
l ? p ? 1 ?lp = 1 ? p ? p
l
Finite dimensional vector spaces and linear
maps also form a compact closed category, which
we refer to as FVect. Finite dimensional vector
spaces V,W are objects of this category; linear
maps f : V ? W are its morphisms with compo-
sition being the composition of linear maps. The
tensor product V ?W is the linear algebraic ten-
sor product, whose unit is the scalar field of vec-
tor spaces; in our case this is the field of reals R.
As opposed to the tensor product in Preg, the ten-
sor between vector spaces is symmetric; hence we
have a naturual isomorphism V ?W ?= W ? V .
As a result of the symmetry of the tensor, the two
adjoints reduce to one and we obtain the following
isomorphism:
V l ?= V r ?= V ?
where V ? is the dual of V . When the basis vectors
of the vector spaces are fixed, it is further the case
that the following isomorphism holds as well:
V ? ?= V
42
Elements of vector spaces, i.e. vectors, are rep-
resented by morphisms from the unit of tensor to
their corresponding vector space; that is??v ? V is
represented by the morphism R
??v
?? V ; by linear-
ity this morphism is uniquely defined when setting
1 7? ??v .
Given a basis {ri}i for a vector space V , the ep-
silon maps are given by the inner product extended
by linearity; i.e. we have:
l = r : V ? ? V ? R
given by:
?
ij
cij ?i ? ?j 7?
?
ij
cij??i | ?j?
Similarly, eta maps are defined as follows:
?l = ?r : R? V ? V ?
and are given by:
1 7?
?
i
ri ? ri
A Frobenius algebra in a monoidal category
(C,?, I) is a tuple (X,?, ?, ?, ?) where, for X
an object of C, the triple (X,?, ?) is an internal
comonoid; i.e. the following are coassociative and
counital morphisms of C:
?: X ? X ?X ? : X ? I
Moreover (X,?, ?) is an internal monoid; i.e. the
following are associative and unital morphisms:
? : X ?X ? X ? : I ? X
And finally the ? and ?morphisms satisfy the fol-
lowing Frobenius condition:
(?? 1X) ? (1X ??) = ? ? ? = (1X ? ?) ? (?? 1X)
Informally, the comultiplication ? decomposes
the information contained in one object into two
objects, and the multiplication ? combines the in-
formation of two objects into one.
Frobenius algebras were originally introduced
in the context of representation theorems for group
theory (Frobenius, 1903). Since then, they have
found applications in other fields of mathematics
and physics, e.g. in topological quantum field the-
ory (Kock, 2003). The above general categorical
definition is due to Carboni and Walters (1987). In
what follows, we use Frobenius algebras that char-
acterise vector space bases (Coecke et al, 2008).
In the category of finite dimensional vector
spaces and linear maps FVect, any vector space V
with a fixed basis {??vi}i has a Frobenius algebra
over it, explicitly given by:
? :: ??vi 7?
??vi ?
??vi ? ::
??vi 7? 1
? :: ??vi ?
??vj 7? ?ij
??vi ? :: 1 7?
?
i
??vi
where ?ij is the Kronecker delta.
Frobenius algebras over vector spaces with or-
thonormal bases are moreover isometric and com-
mutative. A commutative Frobenius Algebra satis-
fies the following two conditions for ? : X?Y ?
Y ?X , the symmetry morphism of (C,?, I):
? ?? = ? ? ? ? = ?
An isometric Frobenius Algebra is one that satis-
fies the following axiom:
? ?? = 1
The vector spaces of distributional models have
fixed orthonormal bases; hence they have isomet-
ric commutative Frobenius algebras over them.
The comultiplication ? of an isometric com-
mutative Frobenius Algebra over a vector space
encodes vectors of lower dimensions into vectors
of higher dimensional tensor spaces; this oper-
ation is referred to as copying. In linear alge-
braic terms, ?(??v ) ? V ? V is a diagonal matrix
whose diagonal elements are weights of ??v ? V .
The corresponding multiplication ? encodes vec-
tors of higher dimensional tensor spaces into lower
dimensional spaces; this operation is referred to
as combining. For ??w ? V ? V , we have that
?(??w ) ? V is a vector consisting only of the diag-
onal elements of ??w .
As a concrete example, take V to be a two di-
mensional space with basis {??v1 ,
??v2}; then the ba-
sis of V ?V is {??v1?
??v1 ,
??v1?
??v2 ,
??v2?
??v1 ,
??v2?
??v2}.
For a vector v = a??v1 + b
??n2 in V we have:
?(v) = ?
(
a
b
)
=
(
a 0
0 b
)
= a??v1?
??v1+b
??v2?
??v2
And for a matrix w = a??v1 ?
??v1 + b
??v1 ?
??v2 +
c??v2 ?
??v1 + d
??v2 ?
??v2 in V ? V , we have:
?(w) = ?
(
a b
c d
)
=
(
a
d
)
= a??v1 + d
??v2
43
3 String Diagrams
The framework of compact closed categories and
Frobenius algebras comes with a complete di-
agrammatic calculus that visualises derivations,
and which also simplifies the categorical and vec-
tor space computations. Morphisms are depicted
by boxes and objects by lines, representing their
identity morphisms. For instance a morphism
f : A ? B, and an object A with the identity ar-
row 1A : A? A, are depicted as follows:
f
A
B
A
The tensor products of the objects and mor-
phisms are depicted by juxtaposing their diagrams
side by side, whereas compositions of morphisms
are depicted by putting one on top of the other;
for instance the object A?B, and the morphisms
f ? g and f ? h, for f : A ? B, g : C ? D, and
h : B ? C, are depicted as follows:
f
A
B D
g
C f
A
B
h
C
A B
The  maps are depicted by cups, ? maps
by caps, and yanking by their composition and
straightening of the strings. For instance, the di-
agrams for l : Al ? A ? I , ? : I ? A ? Al and
(l ? 1A) ? (1A ? ?l) = 1A are as follows:
Al
A Al
A
Al A Al = A
The composition of the  and ? maps with other
morphisms is depicted as before, that is by juxta-
posing them one above the other. For instance the
diagrams for the compositions (1Bl ? f) ? 
l and
?l ? (1Al ? f) are as follows:
B
f
A
Bl
Al A
f
B
As for Frobenius algebras, the diagrams for the
monoid and comonoid morphisms are as follows:
(?, ?) (?, ?)
with the Frobenius condition being depicted as:
= =
The defining axioms guarantee that any picture de-
picting a Frobenius computation can be reduced to
a normal form that only depends on the number of
input and output strings of the nodes, independent
of the topology. These normal forms can be sim-
plified to so-called ?spiders?:
=
? ? ?
? ? ?
???
???
In the category FVect, apart from spaces V,W ,
which are objects of the category, we also have
vectors ??v ,??w . These are depicted by their repre-
senting morphisms and as triangles with a number
of strings emanating from them. The number of
strings of a triangle denote the tensor rank of the
vector; for instance, the diagrams for??v ? V,
??
v? ?
V ?W , and
??
v?? ? V ?W ? Z are as follows:
V W WV ZV
Application of a linear map to a vector is de-
picted using composition of their corresponding
morphisms. For instance, for f : V ? W and
??v ? V , the application f(??v ) is depicted by the
composition I
??v
?? V
f
??W .
V
f
W
44
Applications of the Frobenius maps to vectors
are depicted in a similar fashion; for instance
?(??v ? ??v ) is the composition I ? I
??v ???v
?? V ?
V
?
?? V and ?(??v ) is the composition I
??v
??
V
?
?? I , depicted as follows:
V V
V
V
4 Vector Space Interpretations
The grammatical structure of a language is en-
coded in the category Preg: objects are grammat-
ical types (assigned to words of the language) and
morphisms are grammatical reductions (encoding
the grammatical formation rules of the language).
For instance, the grammatical structure of the sen-
tence ?Men love Mary? is encoded in the assign-
ment of types n to the noun phrases ?men? and
?Mary? and nr ? s? nl to the verb ?love?, and in
the reduction map ln ? 1s ? 
r
n. The application
of this reduction map to the tensor product of the
word types in the sentence results in the type s:
(ln ? 1s ? 
r
n)(n? (n
r ? s? nl)? n)? s
To each reduction map corresponds a string dia-
gram that depicts the structure of reduction:
n nrsnl n
Men love Mary
In Coecke et al (2010) the pregroup types and
reductions are interpreted as vector spaces and lin-
ear maps, achieved via a homomorphic mapping
from Preg to FVect. Categorically speaking, this
map is a strongly monoidal functor:
F : Preg? FVect
It assigns vector spaces to the basic types as fol-
lows:
F (1) = I F (n) = N F (s) = S
and to the compound types by monoidality as fol-
lows; for x, y objects of Preg:
F (x? y) = F (x)? F (y)
Monoidal functors preserve the compact structure;
that is the following holds:
F (xl) = F (xr) = F (x)?
For instance, the interpretation of a transitive verb
is computed as follows:
F (nr ? s? nl) = F (nr)? F (s)? F (nl) =
F (n)? ? F (s)? F (n)? = N ? S ?N
This interpretation means that the meaning vector
of a transitive verb is a vector in N ? S ?N .
The pregroup reductions, i.e. the partial order
morphisms of Preg, are interpreted as linear maps:
whenever p ? q in Preg, we have a linear map
f? : F (p) ? F (q). The  and ? maps of Preg are
interpreted as the  and ? maps of FVect. For in-
stance, the pregroup reduction of a transitive verb
sentence is computed as follows:
F (rn ? 1s ? 
r
n) = F (
r
n)? F (1s)? F (
l
n) =
F (n)
? ? F (1s)? F (n)
? = N ? 1S ? N
The distributional meaning of a sentence is ob-
tained by applying the interpretation of the pre-
group reduction of the sentence to the tensor prod-
uct of the distributional meanings of the words
in the sentence. For instance, the distributional
meaning of ?Men love Mary? is as follows:
F (rn ? 1s ? 
l
n)(
???
Men?
???
love?
????
Mary)
This meaning is depictable via the following string
diagram:
N NSN N
Men love Mary
The next section applies these techniques to the
distributional interpretation of pronouns. The in-
terpretations are defined using:  maps, for appli-
cation of the semantics of one word to another; ?
maps, to pass information around by bridging in-
termediate words; and Frobenius operations, for
copying and combining the noun vectors and dis-
carding the sentence vectors.
5 Modelling Relative Pronouns
In this paper we focus on the subject and object
relative pronouns, who(m), which and that. Ex-
amples of noun phrases with subject relative pro-
nouns are ?men who love Mary?, ?dog which ate
cats?. Examples of noun phrases with object rela-
tive pronouns are ?men whom Mary loves?, ?book
45
that John read?. In the final example, ?book? is the
head noun, modified by the relative clause ?that
John read?. The intuition behind the use of Frobe-
nius algebras to model such cases is the following.
In ?book that John read?, the relative clause acts
on the noun (modifies it) via the relative pronoun,
which passes information from the clause to the
noun. The relative clause is then discarded, and
the modified noun is returned. Frobenius algebras
provide the machinery for all of these operations.
The pregroup types of the relative pronouns are
as follows:
nrnsln (subject)
nrnnllsl (object)
These types result in the following reductions:
nr s nl nn nr n sl n
Subject Rel-Pr Verb Object
nr s nlnn nr n nll sl
Object Rel-Pr Subject Verb
The meaning spaces of these pronouns are com-
puted using the mechanism described above:
F (nrnsln) = F (nr)? F (n)? F (sl)? F (n)
= N ?N ? S ?N
F (nrnnllsl) = F (nr)? F (n)? F (nll)? F (sl)
= N ?N ?N ? S
The semantic roles that these pronouns play are
reflected in their categorical vector space mean-
ings, depicted as follows:
Subj:
N N S N
Obj:
N N SN
with the following corresponding morphisms:
Subj: (1N ? ?N ? ?S ? 1N ) ? (?N ? ?N )
Obj: (1N ? ?N ? 1N ? ?S) ? (?N ? ?N )
The diagram of the meaning vector of the sub-
ject relative clause interacting with the head noun
is as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
The diagram for the object relative clause is:
N S NNN N NN S
Object Rel-Pronoun Subject Verb
These diagrams depict the flow of information in
a relative clause and the semantic role of its rel-
ative pronoun, which 1) passes information from
the clause to the head noun via the ? maps; 2) acts
on the noun via the ? map; 3) discards the clause
via the ? map; and 4) returns the modified noun
via 1N . The  maps pass the information of the
subject and object nouns to the verb and to the rel-
ative pronoun to be acted on. Note that there are
two different flows of information in these clauses:
the ones that come from the grammatical structure
and are depicted by maps (at the bottom of the di-
agrams), and the ones that come from the semantic
role of the pronoun and are depicted by ? maps (at
the top of the diagrams).
The normal forms of these diagrams are:
N S N NN
Subject Verb Object
N S N NN
Subject Verb Object
Symbolically, they correspond to the following
morphisms:
(?N ? ?S ? N )
(?????
Subject?
???
Verb?
????
Object
)
(N ? ?S ? ?N )
(?????
Subject?
???
Verb?
????
Object
)
The simplified normal forms will become useful in
practice when calculating vectors for such cases.
6 Vector Space Instantiations
In this section we demonstrate the effect of the
Frobenius operations using two example instan-
tiations. The first ? which is designed perhaps
46
as a theoretical example rather than a suggestion
for implementation ? is a truth-theoretic account,
similar to Coecke et al (2010) but also allow-
ing for degrees of truth. The second is based on
the concrete implementation of Grefenstette and
Sadrzadeh (2011a).
6.1 Degrees of Truth
Take N to be the vector space spanned by a set
of individuals {??n i}i that are mutually orthogo-
nal. For example, ??n 1 represents the individual
Mary, ??n 25 represents Roger the dog,
??n 10 rep-
resents John, and so on. A sum of basis vec-
tors in this space represents a common noun; e.g.
???man =
?
i
??n i, where i ranges over the basis vec-
tors denoting men. We take S to be the one dimen-
sional space spanned by the single vector
??
1 . The
unit vector spanning S represents truth value 1, the
zero vector represents truth value 0, and the inter-
mediate vectors represent degrees of truth.
A transitive verb w, which is a vector in the
space N ? S ?N , is represented as follows:
w :=
?
ij
??n i ? (?ij
??
1 )???n j
if ??n i w?s
??n j with degree ?ij , for all i, j.
Further, since S is one-dimensional with its
only basis vector being
??
1 , the transitive verb can
be represented by the following element ofN?N :
?
ij
?ij
??n i?
??n j if
??n i w?s
??n j with degree ?ij
Restricting to either ?ij = 1 or ?ij = 0 provides
a 0/1 meaning, i.e. either ??n i w?s
??n j or not.
Letting ?ij range over the interval [0, 1] enables
us to represent degrees as well as limiting cases
of truth and falsity. For example, the verb ?love?,
denoted by love, is represented by:
?
ij
?ij
??n i?
??n j if
??n i loves
??n jwith degree?ij
If we take ?ij to be 1 or 0, from the above we
obtain the following:
?
(i,j)?Rlove
??n i ?
??n j
where Rlove is the set of all pairs (i, j) such that
??n i loves
??n j .
Note that, with this definition, the sentence
space has already been discarded, and so for this
??????????????????
Subject who Verb Object :=
(?N ? N )
(?????
Subject?
???
Verb?
????
Object
)
=
(?N ? N )
?
?
?
k?K
??n k ?(
?
ij
?ij
??n i?
??n j)?
?
l?L
??n l
?
?
=
?
ij,k?K,l?L
?ij?N (
??n k ?
??n i)? N (
??n j ?
??n l)
=
?
ij,k?K,l?L
?ij?ki
??n i?jl
=
?
k?K,l?L
?kl
??n k
Figure 1: Meaning computation with a subject rel-
ative pronoun
instantiation the ? map, which is the part of the
relative pronoun interpretation designed to discard
the relative clause after it has acted on the head
noun, is not required.
For common nouns
?????
Subject =
?
k?K
??n k and
????
Object =
?
l?L
??n l, where k and l range over
the sets of basis vectors representing the respec-
tive common nouns, the truth-theoretic meaning of
a noun phrase modified by a subject relative clause
is computed as in Figure 1. The result is highly in-
tuitive, namely the sum of the subject individuals
weighted by the degree with which they have acted
on the object individuals via the verb. A similar
computation, with the difference that the ? and 
maps are swapped, provides the truth-theoretic se-
mantics of the object relative clause:
?
k?K,l?L
?kl
??n l
The calculation and final outcome is best under-
stood with an example.
Now only consider truth values 0 and 1. Con-
sider the noun phrase with object relative clause
?men whom Mary loves? and takeN to be the vec-
tor space spanned by the set of all people; then the
males form a subspace of this space, where the ba-
sis vectors of this subspace, i.e. men, are denoted
by ??ml, where l ranges over the set of men which
we denote byM . We set ?Mary? to be the individ-
ual
??
f 1, ?men? to be the common noun
?
l?M
??ml,
47
????????????????
men whom Mary loves :=
(N ? ?N )
?
?
??
f 1 ? (
?
(i,j)?Rlove
??
f i ?
??mj)?
?
l?M
??ml
?
?
=
?
l?M,(i,j)?Rlove
N (
??
f 1 ?
??
f i)? ?(
??mj ?
??ml)
=
?
l?M,(i,j)?Rlove
?1i?jl
??mj
=
?
(1,j)?Rlove|j?M
??mj
Figure 2: Meaning computation for example ob-
ject relative clause
and ?love? to be as follows:
?
(i,j)?Rlove
??
f i ?
??mj
The vector corresponding to the meaning of ?men
whom Mary loves? is computed as in Figure 2.
The result is the sum of the men basis vectors
which are also loved by Mary.
The second example involves degrees of truth.
Suppose we have two females Mary
??
f 1 and Jane??
f 2 and four men
??m1,
??m2,
??m3,
??m4. Mary loves
??m1 with degree 1/4 and
??m2 with degree 1/2; Jane
loves ??m3 with degree 1/5; and
??m4 is not loved. In
this situation, we have:
Rlove = {(1, 1), (1, 2), (2, 3)}
and the verb love is represented by:
1/4(
??
f 1?
??m1)+1/2(
??
f 1?
??m2)+1/5(
??
f 2?
??m3)
The meaning of ?men whom Mary loves? is com-
puted by substituting an ?1,j in the last line of Fig-
ure 2, resulting in the men whom Mary loves to-
gether with the degrees that she loves them:
?
(1,j)?Rlove|j?M
?1j
??mj = 1/4
??m1 + 1/2
??m2
?men whom women love? is computed as fol-
lows, where W is the set of women:
?
k?W,l?M,(i,j)?Rlove
?ijN (
??
f k ?
??
f i)? ?(
??mj ?
??ml)
=
?
k?W,l?M,(i,j)?Rlove
?ij?ki?jl
??mj
=
?
(i,j)?Rlove|i?W,j?M
?ij
??mj
= 1/4??m1 + 1/2
??m2 + 1/5
??m3
The result is the men loved by Mary or Jane to-
gether with the degrees to which they are loved.
6.2 A Concrete Instantiation
In the model of Grefenstette and Sadrzadeh
(2011a), the meaning of a verb is taken to be ?the
degree to which the verb relates properties of its
subjects to properties of its object?. Clark (2013)
provides some examples showing how this is an
intuitive defintion for a transitive verb in the cat-
egorical framework. This degree is computed by
forming the sum of the tensor products of the sub-
jects and objects of the verb across a corpus, where
w ranges over instances of the verb:
verb =
?
w
(
??
sbj?
??
obj)w
Denote the vector space of nouns by N ; the above
is a matrix in N ? N , depicted by a two-legged
triangle as follows:
N N
The verbs of this model do not have a sentence
dimension; hence no information needs to be dis-
carded when they are used in our setting, and so no
?map appears in the diagram of the relative clause.
Inserting the above diagram in the diagrams of the
normal forms results in the following for the sub-
ject relative clause (the object case is similar):
N N NN
Subject Verb Object
The abstract vectors corresponding to such dia-
grams are similar to the truth-theoretic case, with
the difference that the vectors are populated from
corpora and the scalar weights for noun vectors
48
are not necessarily 1 or 0. For subject and object
noun context vectors computed from a corpus as
follows:
?????
Subject =
?
k
Ck
??n k
????
Object =
?
l
Cl
??n l
and the verb a linear map:
Verb =
?
ij
Cij
??n i ?
??n j
computed as above, the concrete meaning of a
noun phrase modified by a subject relative clause
is as follows:
?
kijl
CkCijCl?N (
??n k ?
??n i)N (
??n j ?
??n l)
=
?
kijl
CkCijCl?ki
??n k?jl
=
?
kl
CkCklCl
??n k
Comparing this to the truth-theoretic case, we see
that the previous ?kl are now obtained from a cor-
pus and instantiated to CkCklCl. To see how the
above expression represents the meaning of the
noun phrase, decompose it into the following:
?
k
Ck
??n k 
?
kl
CklCl
??n l
Note that the second term of the above, which is
the application of the verb to the object, modifies
the subject via point-wise multiplication. A simi-
lar result arises for the object relative clause case.
As an example, suppose that N has two dimen-
sions with basis vectors ??n 1 and
??n 2, and consider
the noun phrase ?dog that bites men?. Define the
vectors of ?dog? and ?men? as follows:
??
dog = d1
??n 1+d2
??n 2
???men = m1
??n 1+m2
??n 2
and the matrix of ?bites? by:
b11??n 1???n 2+b12??n 1???n 2+b21??n 2???n 1+b22??n 2???n 2
Then the meaning of the noun phrase becomes:
?????????????
dog that bites men :=
d1b11m1??n 1 + d1b12m2??n 1 + d2b21m1??n 2
+ d2b22m2
??n 2 = (d1
??n 1 + d2
??n 2)
((b11m1 + b12m2)
??n 1 + (b21m1 + b22m2)
??n 2)
Using matrix notation, we can decompose the sec-
ond term further, from which the application of the
verb to the object becomes apparent:
(
b11 b12
b21 b22
)
?
(
m1
m2
)
Hence for the whole clause we obtain:
??
dog (bites????men)
Again this result is highly intuitive: assuming
that the basis vectors of the noun space represent
properties of nouns, the meaning of ?dog that bites
men? is a vector representing the properties of
dogs, which have been modified (via multiplica-
tion) by those properties of individuals which bite
men. Put another way, those properties of dogs
which overlap with properties of biting things get
accentuated.
7 Conclusion and Future Directions
In this paper, we have extended the compact cate-
gorical semantics of Coecke et al (2010) to anal-
yse meanings of relative clauses in English from
a vector space point of view. The resulting vec-
tor space semantics of the pronouns and clauses
is based on the Frobenius algebraic operations on
vector spaces: they reveal the internal structure, or
what we call anatomy, of the relative clauses.
The methodology pursued in this paper and the
Frobenius operations can be used to provide se-
mantics for other relative pronouns and also other
closed-class words such as prepositions and deter-
miners. In each case, the grammatical type of the
word and a detailed analysis of the role of these
words in the meaning of the phrases in which they
occur would be needed. In some cases, it may be
necessary to introduce a linear map to represent
the meaning of the word, for instance to distin-
guish the preposition on from in.
The contribution of this paper is best demon-
strated via the string diagrammatic representations
of the vector space meanings of these clauses. A
noun phrase modified by a subject relative clause,
which before this paper was depicted as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
will now include the internal anatomy of its rela-
tive pronoun:
49
N S N NN N NN S
Subject Rel-Pronoun Verb Object
This internal structure shows how the information
from the noun flows through the relative pronoun
to the rest of the clause and how it interacts with
the other words. We have instantiated this vector
space semantics using truth-theoretic and corpus-
based examples.
One aspect of our example spaces which means
that they work particularly well is that the sen-
tence dimension in the verb is already discarded,
which means that the ? maps are not required (as
discussed above). Another feature is that the sim-
ple nature of the models means that the ?map does
not lose any information, even though it takes the
diagonal of a matrix and hence in general throws
information away. The effect of the ? and ? maps
in more complex representations of the verb re-
mains to be studied in future work.
On the practical side, what we offer in this paper
is a method for building appropriate vector repre-
sentations for relative clauses. As a result, when
presented with a relative clause, we are able to
build a vector for it, only by relying on the vector
representations of the words in the clause and the
grammatical role of the relative pronoun. We do
not need to retrieve information from a corpus to
be able to build a vector or linear map for the rela-
tive pronoun, neither will we end up having to dis-
card the pronoun and ignore the role that it plays in
the meaning of the clause (which was perhaps the
best option available before this paper). However,
the Frobenius approach and our claim that the re-
sulting vectors are ?appropriate? requires an empir-
ical evaluation. Tasks such as the term definition
task from Kartsaklis et al (2013) (which also uses
Frobenius algebras but for a different purpose) are
an obvious place to start. More generally, the sub-
field of compositional distributional semantics is
a growing and active one (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Zanzotto et
al., 2010; Socher et al, 2011), for which we argue
that high-level mathematical investigations such
as this paper, and also Clarke (2008), can play a
crucial role.
Acknowledgements
We would like to thank Dimitri Kartsaklis and
Laura Rimell for helpful comments. Stephen
Clark was supported by ERC Starting Grant Dis-
CoTex (30692). Bob Coecke and Stephen Clark
are supported by EPSRC Grant EP/I037512/1.
Mehrnoosh Sadrzadeh is supported by an EPSRC
CAF EP/J002607/1.
References
J.C. Baez and J. Dolan. 1995. Higher-dimensional al-
gebra and topological quantum field theory. Journal
of Mathematical Physics, 36:6073?6105.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
A. Carboni and R. F. C. Walters. 1987. Cartesian bicat-
egories. I. J. Pure and Appied Algebra, 49:11?32.
S. Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359?377. Oxford University Press.
D. Clarke. 2008. Context-theoretic Semantics for Nat-
ural Language: An Algebraic Framework. Ph.D.
thesis, University of Sussex.
B. Coecke and E. Paquette. 2008. Introducing cat-
egories to the practicing physicist. In B. Coecke,
editor, New Structures for Physics, volume 813 of
Lecture Notes in Physics, pages 167?271. Springer.
B. Coecke, D. Pavlovic, and J. Vicary. 2008. A
new description of orthogonal bases. Mathematical
Structures in Computer Science, 1:269?272.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010.
Mathematical foundations for a compositional dis-
tributional model of meaning. Linguistic Analysis,
36:345?384.
F. G. Frobenius. 1903. Theorie der hyperkomplexen
Gro??en. Preussische Akademie der Wissenschaften
Berlin: Sitzungsberichte der Preu?ischen Akademie
der Wissenschaften zu Berlin. Reichsdr.
E. Grefenstette and M. Sadrzadeh. 2011a. Experimen-
tal support for a categorical compositional distribu-
tional model of meaning. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1394?1404.
E. Grefenstette and M. Sadrzadeh. 2011b. Experi-
menting with transitive verbs in a discocat. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics (GEMS).
50
A. Joyal and R. Street. 1991. The Geometry of Tensor
Calculus, I. Advances in Mathematics, 88:55?112.
D. Kartsaklis, M. Sadrzadeh, S. Pulman, and B. Co-
ecke. 2013. Reasoning about meaning in nat-
ural language with compact closed categories and
frobenius algebras. In J. Chubb, A. Eskandar-
ian, and V. Harizanov, editors, Logic and Algebraic
Structures in Quantum Computing and Information,
Association for Symbolic Logic Lecture Notes in
Logic. Cambridge University Press.
G. M. Kelly and M. L. Laplaza. 1980. Coherence for
compact closed categories. Journal of Pure and Ap-
plied Algebra, 19:193?213.
J. Kock. 2003. Frobenius algebras and 2D topological
quantum field theories, volume 59 of London Mathe-
matical Society student texts. Cambridge University
Press.
J. Lambek. 1958. The Mathematics of Sentence Struc-
ture. American Mathematics Monthly, 65:154?170.
J. Lambek. 1999. Type Grammar Revisited Logical
Aspects of Computational Linguistics. In Logical
Aspects of Computational Linguistics, volume 1582
of Lecture Notes in Computer Science, pages 1?27.
Springer Berlin / Heidelberg.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
08, pages 236?244, Columbus, OH.
R. Montague. 1974. English as a formal language.
In R. H. Thomason, editor, Formal philosophy: Se-
lected Papers of Richard Montague, pages 189?223.
Yale University Press.
A. Preller and J. Lambek. 2007. Free compact 2-
categories. Mathematical Structures in Computer
Science, 17:309?340.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, J. Pennington, E. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Edin-
burgh, UK.
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of COLING, Beijing, China.
51
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 114?123,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Separating Disambiguation from Composition
in Distributional Semantics
Dimitri Kartsaklis
University of Oxford
Dept of Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary Univ. of London
School of Electr. Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrs@eecs.qmul.ac.uk
Stephen Pulman
University of Oxford
Dept of Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
stephen.pulman@cs.ox.ac.uk
Abstract
Most compositional-distributional models
of meaning are based on ambiguous vec-
tor representations, where all the senses
of a word are fused into the same vec-
tor. This paper provides evidence that the
addition of a vector disambiguation step
prior to the actual composition would be
beneficial to the whole process, produc-
ing better composite representations. Fur-
thermore, we relate this issue with the
current evaluation practice, showing that
disambiguation-based tasks cannot reli-
ably assess the quality of composition. Us-
ing a word sense disambiguation scheme
based on the generic procedure of Sch?tze
(1998), we first provide a proof of con-
cept for the necessity of separating dis-
ambiguation from composition. Then we
demonstrate the benefits of an ?unambigu-
ous? system on a composition-only task.
1 Introduction
Compositional and distributional semantic mod-
els seem to provide complementary solutions for
solving the same problem, that of assigning a
proper ?meaning? to a text segment. Specifically,
while compositional models deal with the recur-
sive nature of the language, providing a way to
address its inherent ability to create infinite sen-
tences from finite resources (words), they leave
words as unexplained primitives whose meanings
have somehow already been set before the compo-
sitional process. On the other hand, distributional
models have been especially successful in provid-
ing concrete representations for the meaning of
words as vectors in a vector space, created by tak-
ing into account the context in which each word
appears. Despite its success for smaller language
units, the distributional hypothesis does not natu-
rally lend itself to compounds of words. Hence
these models do not canonically scale in tasks re-
quiring the creation of vector representations for
text constituents larger than words, i.e. for phrases
and sentences.
Given the complementary nature of those two
semantic models, it is not surprising that consider-
able research activity has been dedicated on com-
bining them into a single framework that would
benefit from the best of both worlds in a uni-
fied manner: Mitchell and Lapata (2008) exper-
iment with intransitive sentences, applying sim-
ple compositional models based on vector ad-
dition and point-wise multiplication in a disam-
biguation task; Baroni and Zamparelli (2010) and
Guevara (2010) use regression models in order to
build vectors for adjective-noun compounds; Erk
and Pad? (2008) work on transitive sentences us-
ing structured vector spaces; Socher et al (2010,
2011, 2012) use neural networks to combine vec-
tors following the grammatical structure; Grefen-
stette and Sadrzadeh (2011a,b) apply the categori-
cal framework of Coecke et al (2010) on the dis-
ambiguation task of Mitchell and Lapata (2008);
and Kartsaklis et al (2012) and Grefenstette et al
(2013) build upon previous implementations by
adding specific algebraic operations and machine
learning techniques to further improve the con-
crete abilities of the abstract categorical models.
A common strand in all of the above models is
that they are based on ?ambiguous? vector rep-
resentations, where a polysemous word is repre-
sented by a single vector regardless of the number
of its actual senses. For example, the word ?bank?
has at least two meanings (financial institution and
land alongside a river), both of which will be fused
into a single vector representation. And, although
it is generally true that compositional models fol-
lowing the formal semantics view of Montague do
not care about disambiguation (meanings of words
in such models are represented by logical con-
stants explicitly set before the compositional pro-
cess), the story changes when one moves to a vec-
tor space model with ambiguous vector represen-
tations. The main problem is that, when acting on
ambiguous vector spaces, compositional models
114
seem to perform two tasks at the same time, com-
position and disambiguation, leaving the resulting
vector hard to interpret: it is not clear if this vector
is a proper meaning representation for the com-
posed compound or just a disambiguated version
of one of the words therein. This problem escapes
the evaluation schemes, especially when disam-
biguation tasks are used as a criterion for evaluat-
ing compositional models?a common practice in
current research for compositional-distributional
semantics. Indeed, Pulman (2013) argues that al-
though disambiguation can emerge as a welcome
side-effect of the compositional process, it is not
clear if compositionality is either a necessary or
sufficient condition for disambiguation to happen.
On the contrary, it seems that the form of most
current vector space models and the compositional
operations used on them (quite often some form of
vector point-wise multiplication) mainly achieve
disambiguation, but not composition.
The purpose of this paper is to further investi-
gate the potential of a compositional-distributional
model based on disambiguated vector represen-
tations, where each word can have one or more
distinct senses. More specifically, we aim to
show that (a) compositionality is not a neces-
sary condition for disambiguation, so the quite
common practice of using a disambiguation task
as a criterion for evaluating the performance of
compositional-distributional models is question-
able; and (b) the introduction of a separate disam-
biguation step in the compositional process of dis-
tributional models can be indeed beneficial for the
quality of the resulting composed vectors.
We train our models from BNC, a 100-million
words corpus created from samples of written and
spoken English. We perform word sense induc-
tion by following the generic algorithm of Sch?tze
(1998), in which the senses of a word are repre-
sented by distinct clusters created by taking into
account the various contexts in which this specific
word occur in the corpus. For the actual cluster-
ing step we use a combination of hierarchical ag-
glomerative clustering and the Calin?ski-Harabasz
index (Calin?ski and Harabasz, 1974). The param-
eters of the models are fine-tuned on the noun set
of SEMEVAL 2010 Word Sense Induction and Dis-
ambiguation task (Manandhar et al, 2010).
Equipped with a disambiguated vector space,
we use it on a verb disambiguation experiment,
similar in style to that of Mitchell and Lapata
(2008), but applied on a more linguistically mo-
tivated dataset, based on the work of Pickering
and Frisson (2001). We find that the application
of a simple disambiguation algorithm, without any
compositional steps, is proven more effective than
a number of compositional models. We consider
this as an indication for the necessity of separat-
ing disambiguation from composition, since it im-
plies that the latter is not necessary for achiev-
ing the former. Next, we demonstrate that a com-
positional model based on disambiguated vectors
can indeed produce composite vector representa-
tions of better quality, by applying the model on a
phrase similarity task (Mitchell and Lapata, 2010).
The goal here is to evaluate the similarity of short
verb phrases, based on the distance of their com-
posite vectors.
2 Composition in distributional models
The transition from word meaning to sentence
meaning, a task easily done by human subjects
based on the rules of grammar, implies the exis-
tence of a composition operation applied to prim-
itive text units in order to build compound ones.
Various solutions have been proposed with differ-
ent levels of sophistication for this problem in the
context of vector space models of meaning.
At one end of the spectrum the simple models
of Mitchell and Lapata (2008) address composi-
tion as the point-wise multiplication or addition
of the involved word vectors. This bag-of-words
approach has been proven a hard-to-beat baseline
for many of the more sophisticated models. At the
other end, composition in the work of Socher et al
(2010, 2011, 2012) is served by the advanced ma-
chinery of recurring neural networks, where the
output of the network is used again as input in a
recurring fashion, for composing vectors of larger
constituents. Following a different path, the cat-
egorical framework of Coecke et al (2010) ex-
ploits a structural homomorphism between gram-
mar and vector spaces in order to treat words with
special meanings, such as verb and adjectives, as
functions (tensors of rank-n) that apply to their ar-
guments. This application has the form of inner
product, generalising the familiar notion of matrix
multiplication to tensors of higher rank.
Regardless of their level of sophistication, most
of the models which aim to apply composition-
ality on word vector representations fail to ad-
dress the problem of handling the polysemous na-
ture of words. Even more importantly, many of
the models are evaluated on their ability to dis-
ambiguate the meaning of specific words, follow-
ing an idea first introduced by Kintsch (2001) and
later adopted by Mitchell and Lapata (2008) and
others. For example, in this latter work the au-
115
thors test their multiplicative and additive models
as follows: given an ambiguous intransitive verb,
say ?run? (with the two senses to be those of mov-
ing fast and of a liquid dissolving), they examine
to what extent the composition of the verb with
an appropriate subject (e.g. ?horse? or ?colour?)
will disambiguate the intended sense of the verb
within the specific context. Each row in the dataset
consists of a subject (e.g. ?horse?), a verb (?run?),
a high-similarity landmark verb (?gallop?), and a
low-similarity landmark verb (?dissolve?). The
subject is combined with the main verb to form a
simple intransitive sentence, and the vector of this
sentence is then compared with the vectors of the
landmark verbs. The goal is to evaluate the degree
to which the composed sentence vector is closer
to the high landmark than to the vector of the low
landmark, and this is considered an indication of
successful composition.
However, although it is generally true that mul-
tiplying ???run with ????horse will filter out most of the
components of???run that are irrelevant to ?dissolve?
(since the ?dissolve?-related elements of ????horse
should have values close to zero) and will pro-
duce a disambiguated version of this verb under
the context of ?horse?, it is not at all clear if this
vector will also constitute an appropriate repre-
sentation for the meaning of the intransitive sen-
tence ?horse runs?. In other words, here we have
two tasks taking place at the same time: (a) dis-
ambiguation of the ambiguous word given its con-
text; and (b) composition that produces a mean-
ing vector for the whole sentence. The extent to
which the latter is a necessary condition for the
former remains unclear, and constitutes a factor
that complicates the evaluation and assessment of
such systems. In this paper we argue that as long
as the above distinct tasks are interwoven into a
single step, claims of compositionality in distri-
butional systems cannot be reliably assessed. We
therefore propose the addition of a disambiguation
step in the generic methodology of compositional-
distributional models.
3 Related work
Although in general word sense induction is a
popular topic in the natural language processing
literature, little has been done to address poly-
semy specifically in the context of compositional-
distributional models of meaning. In fact, the only
works relevant to ours we are aware of are that of
Erk and Pad? (2008) and Reddy et al (2011). The
structured vector space of Erk and Pad? (2008) is
designed to handle ambiguity in an implicit way,
showing promising results on the Mitchell and
Lapata (2008) task. The work of Reddy et al
(2011) is closer to our research: the authors eval-
uate two word sense disambiguation approaches
on the noun-noun compound similarity task intro-
duced by Mitchell and Lapata (2010), using sim-
ple multiplicative and additive models for compo-
sition. The reported results are also promising,
where at least one of their models performs bet-
ter than the current practice of using ambiguous
vector representations.
Compared to both of the above works, the
scope of the current paper is broader: it does not
solely aim to demonstrate the positive effect of a
?cleaner? vector space on the compositional pro-
cess, but it also proceeds one step further and re-
lates this issue with the current evaluation prac-
tice, showing that a number of verb disambigua-
tion tasks that have been invariantly used for the
assessment of compositional-distributional mod-
els might be in fact based on a wrong criterion.
4 Disambiguation scheme
Our word sense induction method is based on
the effective procedure first presented by Sch?tze
(1998). For the ith occurrence of a target word wt
in the corpus with context Ci = {w1, . . . , wn},
we calculate the centroid of the context as ??ci =
1
n(
??w1 + . . . + ??wn), where ??w is the lexical (or
first order) vector of word w as it is created by the
usual distributional practice (more details in Sec-
tion 5). Then, we cluster these centroids in order
to form a number of sense clusters. Each sense
of the word is represented by the centroid of the
corresponding cluster. Following Sch?tze, we will
refer to these sense vectors as second-order vec-
tors, in order to distinguish them from the lexical
(first-order) vectors. So, in our model each word is
represented by a tuple ???w ,S?, where??w is the 1st-
order vector of the word and S the set of 2nd-order
vectors created by the above procedure.
We are now able to disambiguate the sense of a
target word wt given a context C by calculating a
context vector ??c for C as above, and then com-
paring this with every 2nd-order vector of wt; the
word is assigned to the sense that corresponds to
the closest 2nd-order vector. That is,
???spref = arg min??s ?S
d(??s ,??c ) (1)
where S is the set of 2nd-order vectors for wt and
d(??u ,??v ) the vector distance metric we use.
For the clustering step, we use an iterative
bottom-up approach known as hierarchical ag-
glomerative clustering (HAC). Hierarchical clus-
116
1 0 1 2 3 4 51.0
0.50.0
0.51.0
1.52.0
2.53.0
3.5
23 27 28 20 29 24 22 25 21 26 14 13 16 15 11 19 17 18 10 12 8 1 3 6 9 4 2 7 0 50.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 1: Hierarchical agglomerative clustering.
tering has been invariably applied to unsupervised
word sense induction on a variety of languages,
generally showing good performance?see, for
example, the comparative study of Broda and
Mazur (2012) for English and Polish. Compared
to k-means clustering, this approach has the ma-
jor advantage that it does not require us to define
in advance a specific number of clusters. Com-
pared to more advanced probabilistic techniques,
such as Bayesian mixture models, it is much
more straightforward and simple to implement,
yet powerful enough to demonstrate the necessity
of factoring out ambiguity from compositional-
distributional models.
HAC is a bottom-up method of cluster analy-
sis, starting with each data point (context vector in
our case) forming its own cluster; then, in each it-
eration the two closest clusters are merged into a
new cluster, until all points are finally merged un-
der the same cluster. This process produces a den-
drogram (i.e. a tree diagram), which essentially
embeds every possible clustering of the dataset.
As an example, Figure 1 shows a small dataset
produced by three distinct Gaussian distributions,
and the dendrogram derived by the above algo-
rithm. Implementation-wise, the clustering part in
this work is served by the efficient FASTCLUSTER
library (M?llner, 2013).
Choosing a number of senses In HAC, one still
needs to decide where exactly to cut the tree in or-
der to get the best possible partitioning of the data.
Although the right answer to this problem might
depend on many factors, we can safely assume that
the optimal partitioning is the one that provides
the most compact and maximally separated clus-
ters. One way to measure the quality of a cluster-
ing based on this criterion is the Calin?ski/Harabasz
index (Calin?ski and Harabasz, 1974), also known
as variance ratio criterion (VRC). Given a set ofN
data points and a partitioning of k disjoint clusters,
VRC is computed as follows:
V RCk =
trace(B)
trace(W ) ?
N ? k
k ? 1 (2)
Here, W and B are the intra-cluster and inter-
cluster dispersion matrices, respectively:
W =
k?
i=1
Ni?
l=1
(??xi(l)? x?i)(??xi(l)? x?i)T (3)
B =
k?
i=1
Ni(x?i ? x?)(x?i ? x?)T (4)
where Ni is the number of data points assigned to
cluster i,??xi(l) is the lth point assigned to this clus-
ter, x?i is the centroid of ith cluster (the mean), and
x? is the data centroid of the overall dataset. Given
the above formulas, the trace of B is the sum of
inter-cluster variances, while the trace of W is the
sum of intra-cluster variances. A good partitioning
should have high values for B (which is an indi-
cation for well-separated clusters) and low values
for W (an indication for compact clusters), so the
higher the quality of the partitioning the greater
the value of this ratio.
Compared to other criteria, VRC has been
found to be one of the most effective approaches
for clustering validity?see the comparative stud-
ies of Milligan and Cooper (1985) and Vendramin
et al (2009). Furthermore, it has been previously
applied to word sense discrimination successfully,
returning the best results among a number of other
measures (Savova et al, 2006). For this work, we
calculate VRC for a number of different partition-
ings (ranged from 2 to 10 clusters), and we keep
the partitioning that results in the highest VRC
value as the optimal number of senses for the spe-
cific word. Note that since the HAC dendrogram
already embeds all possible clusterings, the cut-
ting of the tree in order to get a different partition-
ing is performed in constant time.
5 Experimental setting
The choice of our 1st-order vector space is based
on empirical tests, where we found out that a basis
with elements of the form ?word, class? presents
the right balance for our purposes among sim-
pler techniques, such as word-based spaces, and
more complex ones, such as dependency-based
approaches. In our vector space, each word has a
distinct vector representation for every word class
under which occurs in the corpus (e.g. ?suit? will
have a noun vector and a verb vector). As our ba-
sis elements we use the 2000 most frequent con-
tent words in BNC, with weights being calculated
as the ratio of the probability of the context word
given the target word to the probability of the con-
text word overall. The context here is a 5-word
window on both sides of the target word.
The parameters of the clustering scheme are op-
timized on the noun set of SEMEVAL 2010 Word
117
Sense Induction & Disambiguation Task (Man-
andhar et al, 2010). Specifically, when using HAC
one has to decide how to measure the distance
between the clusters, which is the merging crite-
rion applied in every iteration of the algorithm,
as well as the measure between the data points,
i.e. the individual vectors. Based on empirical
tests we limit our options to two inter-cluster mea-
sures: complete-link and Ward?s methods. In the
complete-link method the distance between two
clustersX and Y is the distance between their two
most remote elements:
D(X,Y ) = max
x?X,y?Y
d(x, y) (5)
In Ward?s method, two clusters are selected for
merging if the new partitioning exhibits the mini-
mum increase in the overall intra-cluster variance.
The cluster distance is given by:
D(X,Y ) = 2|X||Y ||X|+ |Y |?
??cX ???cY ?2 (6)
where ??cX and ??cY are the centroids of X and Y .
We test these linkage methods in combination
with three vector distance measures: euclidean,
cosine, and Pearson?s correlation (6 models in to-
tal). The metrics were chosen to represent pro-
gressively more relaxed forms of vector compar-
ison, with the strictest form to be the euclidean
distance and correlation as the most relaxed. For
sense detection we use the disambiguation algo-
rithm described in Section 4, considering as con-
text the whole sentence in which a target word
appears. The distance metric used for the dis-
ambiguation process in each model is identical
to the metric used for the clustering process, so
in the Ward/euclidean model the disambiguation
is based on the euclidean distance, in complete-
link/cosine model on the cosine distance, and so
on. We evaluate the models using V-measure,
an entropy-based metric that addresses the so-
Model V-Meas. Avg clust.
Ward/Euclidean 0.05 1.44
Ward/Correlation 0.14 3.14
Ward/Cosine 0.08 1.94
Complete/Euclidean 0.00 1.00
Complete/Correlation 0.11 2.66
Complete/Cosine 0.06 1.74
Most frequent sense 0.00 1.00
1 cluster/instance 0.36 89.15
Gold standard 1.0 4.46
Table 1: Results on the noun set of SEMEVAL
2010 WSI&D task.
keyboard: 1105 contexts, 2 senses
COMPUTER (665 contexts): program dollar disk power
enter port graphic card option select language drive
pen application corp external editor woman price
page design sun cli amstrad lock interface lcd slot
notebook
MUSIC (440 contexts): drummer instrumental singer
german father fantasia english generation wolfgang
wayne cello body join ensemble mike chamber gary
saxophone sax ricercarus apply form son metal guy
clean roll barry orchestra
Table 2: Derived senses for word ?keyboard?.
called matching problem of F-score (Rosenberg
and Hirschberg, 2007). Table 1 shows the results.
Ward?s method in combination with correla-
tion distance provided the highest V-measure, fol-
lowed by the combination of complete-link with
(again) correlation. Although a direct compari-
son of our models with the models participating
in this task would not be quite sound (since these
models were trained on a special corpus provided
by the organizers, while our model was trained
on the BNC), it is nevertheless enlightening to
mention that the 0.14 V-measure places the Ward-
correlation model at the 4th rank among 28 sys-
tems for the noun set of the task, while at the
same time provides a reasonable average number
of clusters per word (3.14), close to that of the
human-annotated gold standard (4.46). Compare
this, for example, with the best-performing sys-
tem that achieved a V-measure of 0.21, a score
that was largely due to the fact that the model as-
signed the unrealistic number of 11.54 senses per
word on average (since V-measure tends to favour
higher numbers of senses, as the baseline 1 clus-
ter/instance shows in Table 1).1
Table 2 provides an example of the results,
showing the senses for the noun ?keyboard? learnt
by the best model of Ward?s method and correla-
tion measure. Each sense is visualized as a list of
the most dominant words in the cluster, ranked by
their TF-ICF values. Furthermore, Figure 2 shows
the dendrograms produced by four linkage meth-
ods for the word ?keyboard?, demonstrating the su-
periority of Ward?s method.
6 Disambiguation vs composition
A number of models that aim to equip distribu-
tional semantics with compositionality are evalu-
ated on some form of the disambiguation task pre-
sented in Section 2. Versions of this task can be
found, for example, in Mitchell and Lapata (2008),
1The results of SEMEVAL 2010 can be found online at
http://www.cs.york.ac.uk/semeval2010_WSI/task_14
_ranking.html.
118
0.0
0.1
0.2
0.3
0.4
0.5
0.6
keyboard (single/cosine)
(a) Single-link
0.0
0.2
0.4
0.6
0.8
keyboard (average/cosine)
(b) Average-link
0
1
2
3
4
5
keyboard (ward/cosine)
(c) Ward?s method
0.0
0.2
0.4
0.6
0.8
1.0 keyboard (complete/cosine)
(d) Complete-link
Figure 2: Dendrograms produced for word ?key-
board? according to 4 different linkage methods.
Erk and Pad? (2008), Grefenstette and Sadrzadeh
(2011a,b), Kartsaklis et al (2012) and Grefenstette
et al (2013). We briefly remind that the goal is to
assess how well a compositional model can disam-
biguate the meaning of an ambiguous verb, given
a specific context. This kind of evaluation involves
two distinct tasks: the composition of sentence
vectors, and the disambiguation of the verbs. And,
although the evaluation of a model against human
judgements provides some indication for the suc-
cess of the latter task, it leaves unclear to what ex-
tent the former has been achieved. In this section
we perform two experiments in order to address
this question. The first of them aims to support the
following argument: that although disambiguation
can emerge as a side-effect of a compositional pro-
cess, compositionality is not a necessary condition
for this to happen. The second experiment is based
on a more appropriate task that requires genuine
compositional abilities, and demonstrates the good
performance of a compositional model based on
the disambiguated vector space of Section 5.
As our compositional method for the follow-
ing tasks we use the multiplicative and additive
models of Mitchell and Lapata (2008). Despite
the simple nature of these models, there is a num-
ber of reasons that make them good candidates for
demonstrating the main ideas of this paper. First,
for better or worse ?simple? does not necessar-
ily mean ?ineffective?. The comparative study of
Blacoe and Lapata (2012) shows that for certain
tasks these ?baselines? perform equally well or
even better than other more sophisticated models.
And second, it is reasonable to expect that better
compositional models would only work in favour
of our arguments, and not the other way around.
6.1 Evaluating disambiguation
One potential problem with the datasets used for
the disambiguation task of Section 2, similar to
the one of Grefenstette and Sadrzadeh (2011a), is
that ambiguous verbs are usually collected from a
corpus based on some automated method. And,
although they do exhibit variations in their senses
(as most verbs do), in many cases these meanings
are actually related?for example, the meanings of
?write? in G&S dataset are spell and publish. To
overcome this problem, we used the work of Pick-
ering and Frisson (2001), which provides a list of
genuinely ambiguous verbs obtained from careful
manual selection and ranking from human evalu-
ators. The evaluators assessed the relatedness of
each verb?s different meanings using a scale of
0 (totally unrelated) to 7 (highly related). From
these verbs, we picked 10 with an average mark
< 1. An example is ?file?, which means ?smooth?
in ?file nails? and ?register? as in ?file an applica-
tion?. For each verb we picked the 10 most oc-
curring subjects and objects from the BNC (5 for
each landmark). In the case of verb ?file?, for ex-
ample, among these were ?woman? and ?nails? for
landmark ?smooth?, and ?union? and ?lawsuit? for
landmark ?register?. Each subject and object was
modified by its most occurring adjective in the cor-
pus. This resulted in triples of sentences of the
following form:
(1) main: young woman filed long nails
high: young woman smoothed long nails
low: young woman registered long nails
(2) main: monetary union filed civil lawsuit
high: mon. union registered civil lawsuit
low: mon. union smoothed civil lawsuit
The main sentence was paired with both high
and low landmark sentences, creating a dataset2 of
200 sentence pairs (10 main verbs ? 10 contexts
? 2 landmarks)3. These were randomly presented
to 43 human annotators, whose duty was to judge
the similarity between the sentences of each pair.
The human scores were compared with scores pro-
duced by a number of models (Table 3).
The most successful model (M1) does not ap-
ply any form of composition. Instead, the com-
parison of a sentence with a ?landmark? sentence
is simply based on disambiguated versions of the
2The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.
3As a comparison, the Mitchell and Lapata (2008) dataset
consists of 15 main verbs? 4 contexts? 2 landmarks = 120
sentence pairs, while the Grefenstette and Sadrzadeh (2011a)
dataset has the same configuration and size with ours.
119
verbs alone. Specifically, the main verb and the
landmark verb are disambiguated given the con-
text (subjects, objects, and adjectives that mod-
ify them) according to Equation 1; this produces
two 2nd-order vectors, one for the main verb and
one for the landmark. The degree of similarity be-
tween the two sentences is then calculated by mea-
suring the similarity between the two sense vec-
tors of the verbs, without any compositional step.
The score of 0.28 achieved by this model is im-
pressive, given that the inter-annotator agreement
(which serves as an upper-bound) is 0.38.
A number of interesting observations can be
made based on the results of Table 3. First of
all, the ?verbs-only? model outperforms the two
baselines (which use composition but not disam-
biguation) by a large margin, and indeed also the
other compositional models. This is an indica-
tion that this kind of disambiguation task might
not be the best way to evaluate a compositional
model. The fact that the most important condi-
tion for success is the proper disambiguation of
the verb, means that the good performance of a
compositional model demonstrates only this: how
well the model is able to disambiguate an am-
biguous verb. This is different from how well the
composed representation reflects the meaning of
the larger constituent; that is, it has very little to
say about the extent to which an operation like
??????woman???file????nails ( denotes point-wise mul-
tiplication) results in a faithful representation of
the meaning of sentence ?woman filed nails?.
M2 to M5 represent different versions of the
compositional models that use disambiguation in
a distinct step. All these models compose both the
main verb and the landmark with a given context,
and then perform the comparison at sentence level.
In M2 and M3 all words are first disambiguated
prior to composition, while in M4 and M5 the 2nd-
Disambig. Composition ?
M1 Only verbs No 0.282 ?
M2 All words Multiplicative 0.118
M3 All words Additive 0.210
M4 Only verbs Multiplicative 0.110
M5 Only verbs Additive 0.234 ?
B1 No Multiplicative 0.143
B2 No Additive 0.042
Inter-annotator agreement 0.383
? The difference between M1 and M5 is highly
statistically significant with p < 0.0001
Table 3: Spearman?s ? for the Pickering and Fris-
son dataset.
order vector of the verb is composed with the 1st-
order vectors of the other words. The most im-
pressive observation here is that the separation of
disambiguation results in a tremendous improve-
ment for the additive model, from 0.04 to 0.21.
This is not surprising since, when using magni-
tude invariant measures between vectors (such as
cosine distance), the resulting vector is nothing
more than the average of the involved word vec-
tors. The introduction of the disambiguation step
before the composition, therefore, makes a great
difference since it provides much more accurate
starting points to be averaged.
On the other hand, the disambiguated version
of multiplicative model (M2) presents inferior per-
formance compared to the ?ambiguous? version
(B1). We argue that the reason behind this is that
the two models perform different jobs: the result
of B1 is a ?mixing? of composition and disam-
biguation of the most ambiguous word (i.e. the
verb), since this is the natural effect of the point-
wise multiplication operation (see discussion in
Section 2); on the other hand, M2 is designed to
construct an appropriate composite meaning for
the whole sentence. We will try to support this
argument by the experiment of the next section.
6.2 A better test of compositionality
Although there might not exists such a thing
as the best evaluation method for compositional-
distributional semantics, it is safe to assume that
a phrase similarity task avoids many of the pitfalls
of tasks such as the one of Section 6.1. Given pairs
of short phrases, the goal is to assess the similar-
ity of the phrases by constructing composite vec-
tors for them and computing their distance. No as-
sumptions about disambiguation abilities regard-
ing a specific word (e.g. the verb) are made here;
the only criterion is to what extent the composite
vector representing the meaning of a phrase is sim-
ilar or dissimilar to the vector of another phrase.
From this perspective, this task seems the ideal
choice for evaluating a model aiming to provide
appropriate phrasal semantics. The scores given
by the models are compared to those of human
evaluators using Spearman?s ?.
For this experiment, we use the ?verb-object?
part of the dataset presented in the work of
Mitchell and Lapata (2010), which consists of 108
pairs of short verb phrases exhibiting three de-
grees of similarity. A high similarity pair for ex-
ample, is produce effect/achieve result, a medium
one is pour tea/join party, and a low one is close
eye/achieve end. The original dataset alo con-
120
Disambig. Composition ?
M1 Only verbs No 0.318
M2 All words Multiplicative 0.412 ?
M3 All words Additive 0.414 ?
M4 Only verbs Multiplicative 0.352
M5 Only verbs Additive 0.324
B1 No Multiplicative 0.379 ??
B2 No Additive 0.334
Inter-annotator agreement 0.550
? Difference between M2/B1 is stat. sign. with p ? 0.07
? Difference between M3/B1 is stat. sign. with p ? 0.06
Table 4: Phrase similarity results.
tains noun-noun and adjective-noun compounds.
However, the verb-object part serves the pur-
poses of this paper much better, for two reasons.
First, since by definition the proposed methodol-
ogy suits better circumstances involving at least
some level of word ambiguity, a dataset based on
the most ambiguous part of speech (verbs) seems a
reasonable choice. Second, this part of the dataset
allows us to do some meaningful comparisons
with the task of Section 6.1, which is again around
verb structures. The results are shown in Table 4.
This time, the disambiguation step provides
solid benefits for both multiplicative (M2) and
additive (M3) models, with differences that are
statistically significant from the best baseline B1
(with p ? 0.07 and p ? 0.06, respectively).
Note that the ?verbs-only? model (M1), which was
by a large margin the most successful for the
task of Section 6.1, now shows the worst perfor-
mance. For comparison, the best result reported by
Mitchell and Lapata (2010) on a 1st-order space
similar to ours (regarding dimensions and weights)
was 0.38 (?dilation? model).
7 Discussion
This paper is based on the observation that any
compositional operation between two vectors is
essentially a hybrid process consisting of two
?components? that, depending on the form of the
underlying vector space, can have different ?mag-
nitudes?. One of the components results in a cer-
tain amount of disambiguation for the most am-
biguous original word, while the other one works
towards a composite representation for the mean-
ing of the whole phrase or sentence. The tasks of
Section 6 are designed so that each one of them as-
sesses a different aspect of this hybrid process: the
task of Section 6.1 is focused on the disambigua-
tion aspect, while the task of Section 6.2 addresses
the compositionality part. One of our main argu-
ments is the observation that, in order the get bet-
ter compositional representations, it is essential to
first eliminate (or at least reduce as much as pos-
sible the magnitude of) the disambiguation ?com-
ponent? that might show up as a by-product of the
compositional process, so that the result is mainly
a product of pure composition?this is what the
?unambiguous? models do achieve in the task of
Section 6.2. Based on the experimental work con-
ducted in this paper, our first concluding remark is
that the elimination of the ambiguity factor can be
essential for the quality of the composed vectors.
But, if Table 4 provides a proof that the sep-
aration of disambiguation and composition can
indeed produce better compositional representa-
tions, what is the meaning of the inferior perfor-
mance of all ?unambiguous? models (M2 to M5)
compared to verbs-only version (M1) in the task
of Section 6.1? Why disambiguation is not always
effective (as in the case of multiplicative model)
for that task? These are strong indications that the
quality of composition is not crucial for disam-
biguation tasks of this sort, whose only achieve-
ment is that they measure the disambiguation side-
effects generated by the compositional process. In
other words, the practice of evaluating the qual-
ity of composition by using disambiguation tasks
is problematic. As the topic of compositionality
in distributional models of meaning increasingly
gains popularity in the recent years, this second
concluding remark is equally important since it
can contribute towards better evaluation schemes
of such models.
8 Future work
A next step to take in the future is the appli-
cation of these ideas on more complex spaces,
such as those based on the categorical framework
of Coecke et al (2010). The challenge here is
the effective generalization of a disambiguation
scheme on tensors of rank greater than 1. Ad-
ditionally, we would expect this method to bene-
fit from more robust probabilistic clustering tech-
niques. An appealing option is the use of a non-
parametric method, such as a hierarchical Dirich-
let process (Yao and Van Durme, 2011).
Acknowledgements
We would like to thank Daniel M?llner for his
comments on the use of FASTCLUSTER library,
as well as the three anonymous reviewers for their
fruitful suggestions. Support by EPSRC grant EP/
F042728/1 is gratefully acknowledged by the first
two authors.
121
References
Baroni, M. and Zamparelli, R. (2010). Nouns
are Vectors, Adjectives are Matrices. In Pro-
ceedings of Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Blacoe, W. and Lapata, M. (2012). A compari-
son of vector-based representations for seman-
tic composition. In Proceedings of the 2012
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning, pages 546?556,
Jeju Island, Korea. Association for Computa-
tional Linguistics.
Broda, B. and Mazur, W. (2012). Evaluation
of clustering algorithms for word sense disam-
biguation. International Journal of Data Anal-
ysis Techniques and Strategies, 4(3):219?236.
Calin?ski, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications
in Statistics-Theory and Methods, 3(1):1?27.
Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning.
Lambek Festschrift. Linguistic Analysis,
36:345?384.
Erk, K. and Pad?, S. (2008). A Structured Vector-
Space Model for Word Meaning in Context. In
Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 897?906.
Grefenstette, E., Dinu, G., Zhang, Y.-Z.,
Sadrzadeh, M., and Baroni, M. (2013). Multi-
step regression learning for compositional dis-
tributional semantics.
Grefenstette, E. and Sadrzadeh, M. (2011a). Ex-
perimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Grefenstette, E. and Sadrzadeh, M. (2011b). Ex-
perimenting with Transitive Verbs in a DisCo-
Cat. In Proceedings of Workshop on Geomet-
rical Models of Natural Language Semantics
(GEMS).
Guevara, E. (2010). A Regression Model of
Adjective-Noun Compositionality in Distribu-
tional Semantics. In Proceedings of the ACL
GEMS Workshop.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categorical
distributional-compositional semantics: Theory
and experiments. In Proceedings of 24th Inter-
national Conference on Computational Linguis-
tics (COLING 2012): Posters, pages 549?558,
Mumbai, India. The COLING 2012 Organizing
Committee.
Kintsch, W. (2001). Predication. Cognitive Sci-
ence, 25(2):173?202.
Manandhar, S., Klapaftis, I., Dligach, D., and
Pradhan, S. (2010). Semeval-2010 task 14:
Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Associa-
tion for Computational Linguistics.
Milligan, G. and Cooper, M. (1985). An Exami-
nation of Procedures for Determining the Num-
ber of Clusters in a Data Set. Psychometrika,
50(2):159?179.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 236?
244.
Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388?1439.
M?llner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal
of Statistical Software, 9(53):1?18.
Pickering, M. and Frisson, S. (2001). Process-
ing ambiguous verbs: Evidence from eye move-
ments. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 27(2):556.
Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Uni-
versity Press.
Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceed-
ings of 5th International Joint Conference on
Natural Language Processing, pages 705?713.
Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning, pages
410?420.
122
Savova, G., Therneau, T., and Chute, C. (2006).
Cluster Stopping Rules for Word Sense Dis-
crimination. In Proceedings of the workshop
on Making Sense of Sense: Bringing Psy-
cholinguistics and Computational Linguistics
Together, pages 9?16.
Sch?tze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97?
123.
Socher, R., Huang, E., Pennington, J., Ng, A., and
Manning, C. (2011). Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase
Detection. Advances in Neural Information
Processing Systems, 24.
Socher, R., Huval, B., Manning, C., and A., N.
(2012). Semantic compositionality through re-
cursive matrix-vector spaces. In Conference on
Empirical Methods in Natural Language Pro-
cessing 2012.
Socher, R., Manning, C., and Ng, A. (2010).
Learning Continuous Pphrase Representations
and Syntactic Parsing with recursive neural net-
works. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning
Workshop.
Vendramin, L., Campello, R., and Hruschka, E.
(2009). On the Comparison of Relative Clus-
tering Validity Criteria. In Proceedings of the
SIAM International Conference on Data Min-
ing, SIAM, pages 733?744.
Yao, X. and Van Durme, B. (2011). Nonparamet-
ric bayesian word sense induction. ACL HLT
2011, page 10.
123
