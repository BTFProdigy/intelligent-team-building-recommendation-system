Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1918?1927, Dublin, Ireland, August 23-29 2014.
Class-Based Language Modeling for
Translating into Morphologically Rich Languages
Arianna Bisazza and Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{a.bisazza,c.monz}@uva.nl
Abstract
Class-based language modeling (LM) is a long-studied and effective approach to overcome data
sparsity in the context of n-gram model training. In statistical machine translation (SMT), differ-
ent forms of class-based LMs have been shown to improve baseline translation quality when used
in combination with standard word-level LMs but no published work has systematically com-
pared different kinds of classes, model forms and LM combination methods in a unified SMT
setting. This paper aims to fill these gaps by focusing on the challenging problem of translating
into Russian, a language with rich inflectional morphology and complex agreement phenomena.
We conduct our evaluation in a large-data scenario and report statistically significant BLEU im-
provements of up to 0.6 points when using a refined variant of the class-based model originally
proposed by Brown et al. (1992).
1 Introduction
Class-based n-gram modeling is an effective approach to overcome data sparsity in language model (LM)
training. By grouping words with similar distributional behavior into equivalence classes, class-based
LMs have less parameters to train and can make predictions based on longer histories. This makes them
particularly attractive in situations where n-gram coverage is low due to shortage of training data or to
specific properties of the language at hand.
While translation into English has drawn most of the research effort in statistical machine translation
(SMT) so far, there is now a growing interest in translating into languages that are more challenging
for standard n-gram modeling techniques. Notably, morphologically rich languages are characterized by
high type/token ratios (T/T) that reflect in high out-of-vocabulary word rates and frequent backing-off to
low order n-gram estimates, even when large amounts of training data are used. These problems have
been long studied in the field of speech recognition but much less in SMT, although the target LM is a
core component of all state-of-the-art SMT frameworks.
Partly inspired by successful research in the field of speech recognition, various forms of class-based
LMs have been shown to improve the quality of SMT when used in combination with standard word-
level LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008;
Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions
(Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover,
there is no published work that systematically evaluates different kinds of classes, model forms and LM
combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM
combination uses mixtures of multiple word-level LMs for domain adaptation purposes.
This paper aims to fill these gaps by applying various class-based LM techniques to the challenging
problem of translating into a morphologically rich language. In particular we focus on English-Russian,
a language pair for which a fair amount of both parallel data and monolingual data has been provided by
the Workshop of Machine Translation (Bojar et al., 2013). Russian is characterized by a rich inflectional
morphology, with a particularly complex nominal declension (six core cases, three genders and two
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1918
number categories). This results in complex agreement phenomena and an extremely rich vocabulary.
Indeed, by examining our training data (see Section 4), we find the Russian T/T ratio to be almost two
times higher than the English one.
Given this task, we make a number of contributions leading to a better understanding of ways to utilize
class-based language models for translating into morphologically rich languages. We conduct a compar-
ative evaluation of different target LMs along the following axes: (1) Classes: data-driven versus shallow
morphology-based; (2) Model forms: simple class sequence (stream-based) versus original class-based
(Brown et al., 1992); and (3) Combination frameworks: model-level log-linear combination versus word-
level linear interpolation. When comparing the different model forms we pay particular attention to the
role word emission probabilities play in class-based models, which turns out to be a significant factor
for translating into morphologically rich languages. In this context we also evaluate for the first time a
specific form of class-based LM called fullibm (Goodman, 2001) within statistical MT.
2 Class-based language models
As introduced by (Brown et al., 1992), the idea of class-based n-gram language modeling is to group
words with similar distributional behavior into equivalence classes. The word transition probability is
then decomposed into a class transition probability and a word emission probability:
P
class
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) ? p
1
(w
i
|C(w
i
)) (1)
This results in models that are more compact and more robust to data sparsity. Often, in the context of
SMT, the word emission probability is dropped and only the class sequence is modeled. In this work, we
refer to this model form as stream-based n-gram LM:
1
P
stream
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) (2)
Stream-based LMs are used, for instance, in factored SMT (Koehn et al., 2007), and in general many
of the ?class-based LMs? mentioned in the SMT literature are actually of the latter form (2) (Dyer et
al., 2011; Green and DeNero, 2012; Ammar et al., 2013; Chahuneau et al., 2013; Wuebker et al., 2013;
Durrani et al., 2014). One exception is the work of Uszkoreit and Brants (2008), who incorporate word
emission probabilities in their class-based LM used as an additional feature function in the log-linear
combination (cf. Section 3.1). Interestingly, we are not aware of work that compares actual class-based
LMs and stream-based LMs with respect to SMT quality.
While class-based LMs are known to be effective at counteracting data sparsity issues due to rich
vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling.
Thus, grammatical agreement may be improved by a class-based LM approach only within a limited
context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for
n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant
engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based
constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011).
We will now describe different kinds of word-to-class mapping functions used by class-based LMs.
These can be completely data-driven or based on different sorts of linguistic or orthographic features.
2.1 Data-driven classes
The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the
corpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihood
of a training corpus. No linguistic or orthographic features are taken into account while training the
classes.
2
Later work has focused on decreasing the large computational cost of the exchange algorithm
proposed by Brown et al. (1992), either with a distributed algorithm (Uszkoreit and Brants, 2008) or by
using a whole-context distributional vector space model (Sch?utze and Walsh, 2011). In this paper we use
the standard SRILM implementation of Brown clustering.
1
Not to be confused with the incrementally trainable stream-based LMs of Levenberg and Osborne (2009).
2
Och (1999) extends a similar approach to bilingual clustering with the aim of generalizing the applicability of translation
rules in an alignment template SMT framework.
1919
2.2 Linguistic classes
Linguistic knowledge is another way to establish word equivalence classes. Common examples include
lemma, part of speech and morphology-based classes, each of which can capture different aspects of
the word sequence, such as the relative order of syntactic constituents or grammatical agreement. Has-
san et al. (2007) and Birch et al. (2007) went as far as scoring n-grams of Combinatorial Categorial
Grammar supertags. When using linguistic classes, one has to deal with the fact that the same word can
belong to different classes when used in different contexts. Solutions to this problem include tagging
the target word sequence as it is generated (Koehn et al., 2007; Birch et al., 2007; Green and DeNero,
2012), choosing the most probable class sequence for each phrase pair (Monz, 2011) or?even more
lightweight?choosing the most probable class for each word (Bisazza and Federico, 2012).
Alternatively, simpler deterministic class mappings can be derived by using shallow linguistic knowl-
edge, such as suffixes or orthographic features. The former can be obtained with a rule-based stemmer
(as in this work), or, even more simply, by selecting the ? most common word suffixes in a training
corpus and then mapping each word to its longest matching suffix (M?uller et al., 2012). Orthographic
features may include capitalization information or the presence of digits, punctuation or other special
characters (M?uller et al., 2012).
2.3 Hybrid surface/class models
M?uller et al. (2012) obtain the best perplexity reduction when excluding frequent words from the class
mapping. That is, each word with more than ? occurrences in the training corpus is assigned to a singleton
class with word emission probability equal to 1. The frequency threshold ? is determined with a grid
search on a monolingual held-out set. Optimal values for perplexities are shown to vary considerably
among languages. In this work we follow this setup closely.
It is worth noting that Bisazza and Federico (2012) have applied a similar idea to the problem of
style adaptation: they train a hybrid POS/word n-gram LM on an in-domain corpus and use it as an
additional SMT feature function with the goal of counterbalancing the bias towards the style of the large
out-of-domain data. The idea of modeling sequences of mixed granularity (word/subword) was earlier
introduced to speech recognition by Yazgan and Sarac?lar (2004).
The most extensive comparison of distributional, morphological and hybrid classes that we are aware
of is the work by M?uller et al. (2012), but that does not include any SMT evaluation. Looking at perplex-
ity results over a large number of European language pairs (not including Russian), M?uller et al. (2012)
conclude that a hybrid suffix/word class-based LM simply built on frequency-based suffixes performs as
well as a model trained on much more expensive distributional classes. Motivated by this finding, we
evaluate these two kinds of classes in the context of SMT into a morphologically rich language.
2.4 Fullibm language model
As outlined above, the class-based LMs generally used in SMT are in fact stream-based models in the
sense that they only estimate the probability of the class sequence (see Equation 2). However, the clas-
sic form of class-based LM (Brown et al., 1992) also includes a class-to-word emission probability
p
1
(w
i
|C(w
i
)) whose utility has not been properly assessed in the context of SMT.
Besides, we observe that a variety of class-based LM variants have been studied in speech recognition
but not in SMT. In particular, Goodman (2001) presents a generalization of the standard class-based form
where the word emission is also conditioned on the class history rather than on the current class alone.
The resulting model is called fullibm:
P
fullibm
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) ? p
1
(w
i
|C(w
i
i?n+1
)) (3)
We expect this model to yield more refined, context-sensitive word emission distributions which may
result in better target LM probabilities for our SMT system.
1920
3 SMT combining framework
Class-based LMs are rarely used in isolation, but are rather combined with standard word-level models.
There exist at least two ways to combine multiple LMs into a log-linear SMT decoder: (i) as separate
feature functions in the global log-linear combination or (ii) as components of a linear mixture counting
as a single feature function in the global combination.
3.1 Log-linear combination
The standard log-linear approach to SMT allows for the combination of m arbitrary model components
(or feature functions), each weighted by a corresponding weight ?
m
:
p(x|h) =
?
m
p
m
(x|h)
?
m
(4)
In typical SMT settings, p
m
(x|h) are phrase- or word-level translation probabilities, reordering prob-
abilities, and so on. Treating the new LM as an additional feature function has the advantage that its
weight can be directly optimized for SMT quality together with all other feature weights, using standard
parameter tuning techniques (Och, 2003; Hopkins and May, 2011).
3.2 Linear interpolation
The other widely used combining framework is linear interpolation or mixture model:
p(x|h) =
?
q
?
q
p
q
(x|h) (5)
More specifically, word LMs are usually interpolated as a word-level weighted average of the n-gram
probabilities:
p
mixLM
(e) =
n
?
i=1
(
?
q
?
q
p
q
(e
i
|h
i
)
)
(6)
The drawback of this approach is that the linear interpolation weights, or lambdas, cannot be set with
standard SMT tuning techniques. Instead, interpolation weights are typically determined by maximizing
the likelihood of a held-out monolingual data set, but this does not always outperform simple uniform
weighting in terms of translation quality.
3
Despite the lambda optimization issue, linear interpolation with uniform or maximum-likelihood
weights has been shown to work better for SMT than log-linear combination when combining regu-
lar word n-gram LMs (Foster and Kuhn, 2007). However, to the best of our knowledge, the linear
interpolation of word- and class-based LMs has never been tested in SMT.
In their intrinsic evaluation, M?uller et al. (2012) show that linear mixing with hybrid class/surface
models of various kinds consistently decrease the perplexity of a Kneser-Ney smoothed word-level LM,
with relative improvements ranging between 3% (English) and 11% (Finnish). All their models are
interpolated with class-specific lambda weights, according to the following formula:
P
mix
(w
i
|w
i?1
i?n+1
) = ?
C(w
i?1
)
? P
class
(w
i
|w
i?1
i?n+1
) + (1? ?
C(w
i?1
)
) ? P
word
(w
i
|w
i?1
i?n+1
) (7)
where P
word
corresponds to the standard n-gram model using the lexical forms. Equation 7 can be seen
as a generalization of the simple interpolation ?P
class
+ (1 ? ?)P
word
used by Brown et al. (1992).
The class-specific lambdas are estimated by a deleted interpolation algorithm (Bahl et al., 1991). In our
experiments, we test both generic and class-specific lambda interpolation for SMT.
3
Foster and Kuhn (2007) also tried more sophisticated techniques to set interpolation weights but did not obtain significant
improvements.
1921
Corpus Lang. #Sent. #Tok. T/T
paral.train
EN
1.9M
48.9M .0107
RU 45.9M .0204
Wiki dict.
EN/RU 508K ? ?
mono.train
RU 21.0M 390M .0068
newstest12
EN
3K 64K ?
newstest13
3K 56K ?
Table 1: Training and test data statistics: number of sentences, number of tokens and type/token ratio
(T/T). All numbers refer to tokenized, lowercased data.
4 Evaluation
We perform a series of experiments to compare the effectiveness for SMT of various class mapping
functions, different model forms, and different LM combining frameworks.
The task, organized by the Workshop of Machine Translation (WMT, Bojar et al. (2013)), consists
of translating a set of news stories from English to Russian. As shown in Table 1, the available data
includes a fairly large parallel training corpus (1.9M sentences) from various sources, a set of Wikipedia
parallel headlines shared by CMU,
4
and a larger monolingual corpus for model training (21M sentences).
By measuring the type/token ratios of the two sides of a parallel corpus, we can estimate the difference
in morphological complexity between two languages: as shown in Table 1, the Russian T/T is almost
two times higher than the English one (.0204 vs .0107) in the WMT13 parallel training data. As is
usually the case, much more data is available for LM training. Nevertheless we report a rather high
out-of-vocabulary word rate on the devsets? reference translations (2.28%).
4.1 Baseline
Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very
similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Gal-
ley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous
reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distor-
tion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-
level 5-gram target language model trained on all available monolingual data with modified Kneser-Ney
smoothing (Chen and Goodman, 1999). The distortion limit is set to 6 and for each source phrase the top
30 translation candidates are considered.
The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins
and May, 2011) on newstest12. During tuning, 14 PRO parameter estimation runs are performed in paral-
lel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO
runs are then averaged and passed on to the next decoding iteration. Performing weight estimation inde-
pendently for a number of samples corrects for some of the instability that can be caused by individual
samples.
4.2 Language models
The additional LMs are trained with Witten-Bell smoothing (Witten and Bell, 1991), which is a common
choice for class-based LM training as Kneser-Ney smoothing cannot be used for computing discount
factors when the count-of-counts are zero. The main series of experiments employ 5-gram models, but
we also evaluate the usefulness of increasing the order to 7-gram (see Table 3).
5
Data-driven clusters are learned with the standard Brown clustering algorithm, which greedily maxi-
mizes the log likelihood of a class bigram model on the training data. Following Ammar et al. (2013),
we set the number of data-driven clusters to 600. In preliminary experiments we also tested a 256-cluster
setting, but 600 yielded better BLEU scores. For time reasons, we train the clusters on a subset of the
4
http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
5
For this second series of experiments we use the feature weights tuned for the corresponding 5-gram LMs.
1922
LM type smoothing vocab. PP
words Kneser-Ney 2.7M 270
Brown clusters Witten-Bell 600 588
suffixes Witten-Bell 968 2455
suffix/word hybrid (?=5000) Witten-Bell 8530 460
Linear interp.
PP
generic ? class-spec.??s
words + clusters 225 224
words + suffixes 266 265
words + hybrid 243 247
Table 2: Intrinsic evaluation of various types of LMs and their linear interpolations. Perplexity (PP) is
computed on a separate held-out set of 5K Russian sentences. All models are 5-grams.
monolingual data including all the parallel data (news commentary) and the large commoncrawl corpus
for a total of 1M sentences (22M tokens). We then map all monolingual data to the learned clusters and
use that to train all our cluster-based LMs.
For the suffix-based class LMs we closely follow the setup of M?uller et al. (2012) with the only
difference that we use the Russian Snowball stemmer
6
to segment the vocabulary instead of frequency-
based suffixes. The suffix threshold ? (see Section 2.3) is determined by minimizing perplexity on a
separate held-out set (5K sentences): ?=5000 is the optimal setting among {2000, 5000, 10000, 20000}.
7
The same held-out set is used to estimate both the generic and the class-specific lambdas for the linear
interpolation experiments.
Table 2 presents an overview of the LMs used in our experiments. We can see on the left side that
all class-based LMs have notably higher perplexities compared to the word-level, with the fully suffix-
based LM performing worst by far. Nevertheless, all class-based models yield a decrease in perplexity
when they are interpolated with the word-level model (right side). The best improvement is achieved
by the data-driven classes (225 versus 270, that is -17%), but the result of the hybrid LM is also quite
successful (-10%) and much in line with the improvements reported by M?uller et al. (2012) on other
Slavic languages. Because the fully suffix-based LM yields only a modest reduction, we do not to include
it in the SMT evaluation. The right side of Table 2 also shows that using class-specific interpolation
weights is not significantly better, and sometimes is even worse than using only one generic ?, at least
from the point of view of perplexity. Since weight estimation for linear interpolation is still an open
problem for SMT, we decide nevertheless to compare these two interpolation methods in our translation
experiments (see Table 4).
4.3 SMT results
Table 3 shows the results for English to Russian translation using log-linear combination with Brown
clusters and the hybrid suffix/word classes. Translation quality is measured by case-insensitive BLEU
(Papineni et al., 2002) on newstest13 using one reference translation. The relative improvements of the
different class-based LM runs are with respect to the baseline which uses a word-based LM only and
achieves comparable results to the state-of-the-art. We use approximate randomization (Noreen, 1989)
to test for statistically significant differences between runs (Riezler and Maxwell, 2005).
We can see from Table 2(a) that using a stream-based LM as an additional feature, which is log-linearly
interpolated with the other decoder features during parameter estimation, leads to small but statistically
significant improvements. The results also indicate that using a higher n-gram class model (7-gram)
does not yield additional improvements over a 5-gram class model, which is in contrast with the results
reported by Wuebker et al. (2013) on a French-German task.
Since the stream-based models ignore word emission probabilities, one would expect further improve-
ments from the theoretically more correct class-based model which include word emission probabilities
(see Equation 1). Somewhat surprisingly, this is not the case. On the contrary, both 5- and 7-gram
class-based models perform slightly worse than the stream-based models. We suspect that this is due to
the limited context used to estimate the emission probabilities in the original Brown class-based mod-
els. To verify this we compared this to the fullibm model (Equation 3) which conditions word emission
6
http://snowball.tartarus.org/algorithms/russian/stemmer.html
7
Our training corpus is considerably larger than those used by M?uller et al. (2012), therefore we search among higher values.
1923
(a) Brown clusters (600)
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g stream-based 19.1 +0.3
?
24.8 +0.1
7g stream-based 19.1 +0.3
?
24.9 +0.2
? 5g class-based 18.9 +0.1 24.6 ?0.1
7g class-based 18.8 ?0.0 24.7 ?0.0
5g fullibm 19.4 +0.6
?
25.0 +0.3
?
7g fullibm 19.3 +0.5
?
25.0 +0.3
?
(b) Suffixes/words, ? = 5000
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g stream-based 18.9 +0.1 24.6 ?0.1
7g stream-based 18.9 +0.1 24.6 ?0.1
? 5g class-based 19.0 +0.2
?
24.8 +0.1
7g class-based 19.1 +0.3
?
24.7 ?0.0
5g fullibm 19.1 +0.3
?
24.8 +0.1
7g fullibm 19.2 +0.4
?
24.9 +0.2
?
Table 3: SMT translation quality on newstest13 when using different kinds of class-based language mod-
els as additional features in the log-linear combination. The settings used for weight tuning are marked
with ?. Statistically significant differences wrt the baseline are marked with
?
at the p ? .01 level and
?
at the p ? .05 level.
probabilities on the entire n-gram class history of length n ? 1. The fullibm class-based models yield
the biggest statistically significant improvements over the baseline and also compare favorably to the
stream-based and original class-based models. Similarly to stream- and class-based models we do not
observe a difference in performance between 5- and 7-gram models for fullibm.
Table 2(b) shows the results obtained by the shallow morphology-based classes inspired by M?uller
et al. (2012). This form of classes is easy to implement in many languages and computationally much
cheaper than the Brown clusters. Although less than the data-driven class models, the hybrid suffix/word
models also appear to improve translation quality. We can see that fullibm again yields the highest
improvements, but we can also observe more consistent trends where longer n-grams help and class-
based models are preferable to stream-based models without emission probabilities.
When translating into a morphologically rich language, such as Russian, the role of the target lan-
guage model is two-fold. On the one hand, it helps choose the correct meaning from the available phrase
translation candidates, on the other hand, it helps choose the correct surface realization of the trans-
lation candidate that agrees grammatically with the previous target context. For morphologically rich
languages the second aspect plays a considerably larger role than for morphologically poor languages.
To disentangle these two roles of the language model we also evaluated the different language models
with respect to stem-based information only, stripping off any inflectional information using the Snow-
ball stemmer. These results are also reported in Table 3 and in general exhibit the same trend as the
surface-based BLEU scores. Again, fullibm performs best, and the original class-based LMs do not lead
to any improvements over the baseline. As a general observation, we find that the surface-level gains
are most of the time larger than the stem-level ones, which suggests that the additional LMs are mainly
improving the choice of word inflections.
All systems compared in Table 3 use a class language model as an additional feature, which is log-
linearly interpolated with the other decoder features. Alternatively, the word- and the class-based lan-
(a) Brown clusters (600)
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g class, log-linear comb. 18.9 +0.1 24.6 ?0.1
? 5g class, linear (global ?) 18.5 ?0.3 24.4 ?0.3
5g class, linear (class ??s) 18.6 ?0.2 24.5 ?0.2
(b) Suffixes/words, ? = 5000
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g class, log-linear comb. 19.0 +0.2
?
24.8 +0.1
? 5g class, linear (global ?) 18.9 +0.1 24.8 +0.1
5g class, linear (class ??s) 18.6 ?0.1 24.6 ?0.1
Table 4: SMT translation quality on newstest13 when using different LM combining frameworks: ad-
ditional feature in the log-linear combination or linear interpolation with perplexity-tuned weights (one
global lambda or class-specific lambdas).
1924
guage models may be linearly interpolated with weights determined by maximizing the likelihood of a
held-out monolingual data set (see Section 3.2). While linear interpolation often outperforms log-linear
interpolation for combining language models for domain adaptation (Foster and Kuhn, 2007), this does
not seem to be the case for language models for morphologically rich target languages. The results
presented in Table 4 consistently show that linear interpolation under-performs log-linear combination
under all conditions. Even using class-specific interpolation weights as suggested by M?uller et al. (2012)
did not lead to any further improvements.
5 Conclusion
We have presented the first systematic comparison of different forms of class-based LMs and different
class LM combination methods in the context of SMT into a morphologically rich language.
First of all, our results have shown that careful modeling of class-to-word emission probabilities?
often omitted from the models used in SMT?is actually important for improving translation quality.
In particular, we have achieved best results when using a refined variant of the original class-based
LM, called fullibm, which had never been tested for SMT but only for speech recognition (Goodman,
2001). Secondly, we have found that a rather simple LM based on shallow morphology-based classes
can get close, in terms of BLEU, to the performance of more computationally expensive data-driven
classes. Although the reported improvements are modest, they are statistically significant and obtained
in a competitive large-data scenario against a state-of-the-art baseline.
On the downside, and somewhat in contrast with previous findings in domain adaptation, we have
observed that linear interpolation of word- and class-based LMs with perplexity-tuned weights performs
worse than the log-linear combination of models with model-level weights globally tuned for translation
quality. This result was confirmed also when using class-specific lambdas as suggested by M?uller et al.
(2012).
Indeed, modeling morphologically rich languages remains a challenging problem for SMT but, with
our evaluation, we have contributed to assess how far existing language modeling techniques may go
in this direction. Natural extensions of this work include combining multiple LMs based on different,
and possibly complementary, kinds of classes such as data-driven and suffix-based, or using supervised
morphological analyzers instead of a simple stemmer. In a broader perspective, we believe that future re-
search should question the fundamental constraints of n-gram modeling and develop innovative modeling
techniques that conform to the specific requirements of translating into morphologically rich languages.
Acknowledgments
This research was funded in part by the Netherlands Organisation for Scientific Research (NWO) under
project numbers 639.022.213 and 612.001.218. We kindly thank Thomas M?uller for providing code and
support for the weight optimization of linearly interpolated models.
References
Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, Kenton
Murray, Nicola Segall, Alon Lavie, and Chris Dyer. 2013. The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and pseudo-references. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, pages 70?77, Sofia, Bulgaria, August. Association for Computational Linguis-
tics.
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo. 1991. A fast algorithm
for deleted interpolation. In Eurospeech. ISCA.
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine trans-
lation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16, Prague, Czech
Republic, June. Association for Computational Linguistics.
Arianna Bisazza and Marcello Federico. 2012. Cutting the long tail: Hybrid language models for translation style
adaptation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, pages 439?448, Avignon, France, April. Association for Computational Linguistics.
1925
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Simon Carter and Christof Monz. 2011. Syntactic discriminative language model rerankers for statistical machine
translation. Machine Translation, 25(4):317?339.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1677?1687, Seattle, Washington, USA, October. Association for Computational
Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359?393.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012. On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages
200?209, Montr?eal, Canada, June. Association for Computational Linguistics.
Nadir Durrani, Philipp Koehn, Helmut Schmid, and Alexander Fraser. 2014. Investigating the usefulness of gen-
eralized word representations in SMT. In Proceedings of the 25th International Conference on Computational
Linguistics (COLING), Dublin, Ireland, August.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith. 2011. The CMU-ARK German-English
Translation System. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, July. Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages 128?135, Prague, Czech Republic, June. Association for
Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In EMNLP ?08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
848?856, Morristown, NJ, USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pages 773?781, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Joshua T. Goodman. 2001. A bit of progress in language modeling. Computer Speech & Language, 15(4):403?
434.
Spence Green and John DeNero. 2012. A class-based agreement model for generating accurately inflected trans-
lations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ?12,
pages 146?155, Stroudsburg, PA, USA. Association for Computational Linguistics.
Sa?sa Hasan, Oliver Bender, and Hermann Ney. 2006. Reranking translation hypotheses using structural prop-
erties. In Proceedings of the EACL?06 Workshop on Learning Structured Information in Natural Language
Applications, pages 41?48, Trento, Italy, April.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,
Prague, Czech Republic, June. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP?11).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127?133, Edmonton, Canada.
1926
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic. Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-based randomised language models for smt. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 756?764, Stroudsburg, PA, USA. Association for Computational Linguistics.
Christof Monz. 2011. Statistical Machine Translation with Local Language Models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Language Processing, pages 869?879, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thomas M?uller, Hinrich Sch?utze, and Helmut Schmid. 2012. A comparative investigation of morphological
language modeling for the languages of the European Union. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 386?395, Montr?eal, Canada, June. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley-Interscience.
Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the 9th
Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 71?76.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June. Association for Computational
Linguistics.
Hinrich Sch?utze and Michael Walsh. 2011. Half-context language models. Comput. Linguist., 37(4):843?865,
December.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies, pages 620?631, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language
modeling in machine translation. In Proceedings of ACL-08: HLT, pages 755?762, Columbus, Ohio, June.
Association for Computational Linguistics.
Philip Williams and Philipp Koehn. 2011. Agreement constraints for statistical machine translation into german.
In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217?226, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel
events in adaptive text compression. IEEE Trans. Inform. Theory, IT-37(4):1085?1094.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving statistical machine translation
with word class models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1377?1381, Seattle, Washington, USA, October. Association for Computational Linguistics.
Ali Yazgan and Murat Sarac?lar. 2004. Hybrid language models for out of vocabulary word detection in large
vocabulary conversational speech recognition. In Proceedings of ICASSP, volume 1, pages I ? 745?8 vol.1,
may.
1927
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1676?1688,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Word Translation Prediction for Morphologically Rich Languages
with Bilingual Neural Networks
Ke Tran Arianna Bisazza Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{m.k.tran,a.bisazza,c.monz}@uva.nl
Abstract
Translating into morphologically rich lan-
guages is a particularly difficult problem
in machine translation due to the high de-
gree of inflectional ambiguity in the tar-
get language, often only poorly captured
by existing word translation models. We
present a general approach that exploits
source-side contexts of foreign words to
improve translation prediction accuracy.
Our approach is based on a probabilistic
neural network which does not require lin-
guistic annotation nor manual feature en-
gineering. We report significant improve-
ments in word translation prediction accu-
racy for three morphologically rich target
languages. In addition, preliminary results
for integrating our approach into a large-
scale English-Russian statistical machine
translation system show small but statisti-
cally significant improvements in transla-
tion quality.
1 Introduction
The ability to make context-sensitive translation
decisions is one of the major strengths of phrase-
based SMT (PSMT). However, the way PSMT ex-
ploits source-language context has several limita-
tions as pointed out, for instance, by Quirk and
Menezes (2006) and Durrani et al. (2013). First,
the amount of context used to translate a given
input word depends on the phrase segmentation,
with hypotheses resulting from different segmen-
tations competing with one another. Another issue
is that, given a phrase segmentation, each source
phrase is translated independently from the oth-
ers, which can be problematic especially for short
phrases. As a result, the predictive translation of
a source phrase does not access useful linguistic
clues in the source sentence that are outside of the
scope of the phrase.
Lexical weighting tackles the problem of un-
reliable phrase probabilities, typically associated
with long phrases, but does not alleviate the prob-
lem of context segmentation. An important share
of the translation selection task is then left to the
language model (LM), which is certainly very ef-
fective but can only leverage target language con-
text. Moreover, decisions that are taken at early
decoding stages?such as the common practice
of retaining only top n translation options for
each source span?depend only on the translation
models and on the target context available in the
phrase.
Source context based translation models (Gim-
pel and Smith, 2008; Mauser et al., 2009; Jeong
et al., 2010; Haque et al., 2011) naturally ad-
dress these limitations. These models can ex-
ploit a boundless context of the input text, but
they assume that target words can be predicted in-
dependently from each other, which makes them
easy to integrate into state-of-the-art PSMT sys-
tems. Even though the independence assump-
tion is made on the target side, these models have
shown the benefits of utilizing source context, es-
pecially in translating into morphologically rich
languages. One drawback of previous research
on this topic, though, is that it relied on rich
sets of manually designed features, which in turn
required the availability of linguistic annotation
tools like POS taggers and syntactic parsers.
In this paper, we specifically focus on im-
proving the prediction accuracy for word transla-
tions. Achieving high levels of word translation
accuracy is particularly challenging for language
1676
pairs where the source language is morphologi-
cally poor, such as English, and the target lan-
guage is morphologically rich, such as Russian,
i.e., language pairs with a high degree of surface
realization ambiguity (Minkov et al., 2007). To
address this problem we propose a general ap-
proach based on bilingual neural networks (BNN)
exploiting source-side contextual information.
This paper makes a number of contributions:
Unlike previous approaches our models do not re-
quire any form of linguistic annotation (Minkov
et al., 2007; Kholy and Habash, 2012; Chahuneau
et al., 2013), nor do they require any feature en-
gineering (Gimpel and Smith, 2008). Moreover,
besides directly predicting fully inflected forms
as Jeong et al. (2010), our approach can also
model stem and suffix prediction explicitly. Pre-
diction accuracy is evaluated with respect to three
morphologically rich target languages (Bulgarian,
Czech, and Russian) showing that our approach
consistently yields substantial improvements over
a competitive baseline. We also show that these
improvements in prediction accuracy can be ben-
eficial in an end-to-end machine translation sce-
nario by integrating into a large-scale English-
Russian PSMT system. Finally, a detailed analysis
shows that our approach induces a positive bias on
phrase translation probabilities leading to a better
ranking of the translation options employed by the
decoder.
2 Lexical coverage of SMT models
The first question we ask is whether translation
can be improved by a more accurate selection of
the translation options already existing in the SMT
models, as opposed to generating new options.
To answer this question we measure the lexical
coverage of a baseline PSMT system trained on
English-Russian.
1
We choose this language pair
because of the morphological richness on the tar-
get side: Russian is characterized by a highly in-
flectional morphology with a particularly complex
nominal declension (six core cases, three genders
and two number categories). As suggested by
Green and DeNero (2012), we compute the re-
call of reference tokens in the set of target to-
kens that the decoder could produce in a trans-
lation of the source, that is the target tokens of
all phrase pairs that matched the input sentence
1
Training data and SMT setup are described in Section 6.
and that were actually used for decoding.
2
We
call this the decoder?s lexical search space. Then,
we compare the reference/space recall against the
reference/MT-output recall: that is, the percent-
age of reference tokens that also appeared in the
1-best translation output by the SMT system. Re-
sults for the WMT12 benchmark are presented in
Table 1. From the first two rows, we see that only a
rather small part of the correct target tokens avail-
able to the decoder are actually produced in the
1-best MT output (50% against 86%). Although
our word-level analysis does not directly estimate
phrase-level coverage, these numbers suggest that
a large potential for translation improvement lies
in better lexical selection during decoding.
Token recall:
reference/MT-search-space 86.0%
reference/MT-output 50.0%
stem-only reference/MT-output 12.3%
of which reachable 11.2%
Table 1: Lexical coverage analysis of the baseline
SMT system (English-Russian wmt12).
To quantify the importance of morphology, we
count how many reference tokens matched the
MT output only at the stem level
3
and for how
many of those the correct surface form existed
in the search space (reachable matches). These
two numbers represent the upper bound of the im-
provement achievable by a model only predicting
suffixes given the target stems. As shown in Ta-
ble 1, such a model could potentially increase the
reference/MT-output recall by 12.3% with genera-
tion of new inflected forms, and by 11.2% without.
Thus, also when it comes to morphology, gener-
ation seems to be of secondary importance com-
pared to better selection in our experimental setup.
3 Predicting word translations in context
It is standard practice in PSMT to use word-
to-word translation probabilities as an additional
phrase score. More specifically, state-of-the-art
PSMT systems employ the maximum-likelihood
estimate of the context-independent probability
of a target word given its aligned source word
P (t
j
|s
i
) for each word alignment link a
ij
.
2
This corresponds to the top 30 phrases sorted by
weighted phrase, lexical and LM probabilities, for each
source span. Koehn (2004) and our own experience suggest
that using more phrases has little or no impact on MT quality.
3
Word segmentation for this analysis is performed by the
Russian Snowball stemmer, see also Section 5.3.
1677
[?????????????????] [??????? ?????]
constitutionality of the] [indiana law] [.]
[.]
[the
Figure 1: Fragment of English sentence and its in-
correct Russian translation produced by the base-
line SMT system. Square brackets indicate phrase
boundaries.
The main goal of our work is to improve the
estimation of such probabilities by exploiting the
context of s
i
, which in turn we expect will re-
sult in better phrase translation selection. Figure
1 illustrates this idea: the translation of ?law? in
this example has a wrong case?nominative in-
stead of genitive. Due to the rare word ?Indi-
ana/????????, the target LM must backoff to the
bigram history and does not penalize this choice
sufficiently. However, a model that has access to
the word ?of? in the near source context could bias
the translation of ?law? to the correct case.
We then model P (t
j
|c
s
i
) with source context
c
s
i
defined as a fixed-length word sequence cen-
tered around s
i
:
c
s
i
= s
i?k
, ..., s
i
, ..., s
i+k
Our definition of context is similar to the n ? 1
word history used in n-gram LMs. Similarly to
previous work in source context-sensitive trans-
lation modeling (Jeong et al., 2010; Chahuneau
et al., 2013), target words are predicted indepen-
dently from each other, which allows for an ef-
ficient decoding integration. We are particularly
interested in translating into morphologically rich
languages where source context can provide useful
information for the prediction of target translation,
for example, the gender of the subject in a source
sentence constrains the morphology of the transla-
tion of the source verb. Therefore, we integrate the
notions of stem and suffix directly into the model.
We assume the availability of a word segmenta-
tion function g that takes a target word t as in-
put and returns its stem and suffix: g(t) = (?, ?).
Then, the conditional probability p(t
j
|c
s
i
) can be
decomposed into stem probability and suffix prob-
ability:
p(t
j
|c
s
i
) = p(?
j
|c
s
i
)p(?
j
|c
s
i
, ?
j
) (1)
These two probabilities can be estimated sepa-
rately, which yields the two subtasks:
1. predict target stem ? given source context c
s
;
2. predict target suffix ? given source context c
s
and target stem ?.
Based on the results of our analysis, we focus
on the selection of existing translation candidates.
We then restrict our prediction on a set of pos-
sible target candidates depending on the task in-
stead of considering all target words in the vocab-
ulary. More specifically, for each source word s
i
,
our candidate generation function returns the set of
target words T
s
= {t
1
, . . . , t
m
} that were aligned
to s
i
in the parallel training corpus, which in turn
corresponds to the set of target words that the SMT
system can produce for a given source. In practice,
we use a pruned version of T
s
to speed up training
and reduce noise (see details in Section 5).
As for the morphological models, given T
s
and
g, we can obtain L
s
= {?
1
, . . . , ?
k
}, the set of
possible target stem translations of s, and M
?
=
{?
1
, . . . , ?
l
}, the set of possible suffixes for a tar-
get stem ?. The use of L
s
, and M
?
is similar to
stemming and inflection operations in (Toutanova
et al., 2008) while the set T
s
is similar to the GEN
function in (Jeong et al., 2010).
4
Our approach differs crucially from previous
work (Minkov et al., 2007; Chahuneau et al.,
2013) in that it does not require linguistic fea-
tures such as part-of-speech and syntactic tree on
the source side. The proposed models automati-
cally learn features that are relevant for each of the
modeled tasks, directly from word-aligned data.
To make the approach completely language inde-
pendent, the word segmentation function g can be
trained with an unsupervised segmentation tool.
The effects of using different word segmentation
techniques are discussed in Section 5.
4 Bilingual neural networks for
translation prediction
Probabilistic neural network (NN), or continuous
space, language models have received increas-
ing attention over the last few years and have
been applied to several natural language process-
ing tasks (Bengio et al., 2003; Collobert and We-
ston, 2008; Socher et al., 2011; Socher et al.,
2012). Within statistical machine translation, they
4
Note that our suffix generation function M
?
is restricted
to the forms observed in the target monolingual data, but not
to those aligned to a source word s, which opens the possi-
bility of generating inflected forms that are missing from the
translation models. We leave this possibility to future work.
1678
have been used for monolingual target language
modeling (Schwenk et al., 2006; Le et al., 2011;
Duh et al., 2013; Vaswani et al., 2013), n-gram
translation modeling (Son et al., 2012), phrase
translation modeling (Schwenk, 2012; Zou et al.,
2013; Gao et al., 2014) and minimal translation
modeling (Hu et al., 2014). The recurrent neural
network LMs of Auli et al. (2013) are primarily
trained to predict target word sequences. However,
they also experiment with an additional input layer
representing source side context.
Our models differ from most previous work in
neural language modeling in that we predict a tar-
get translation given a source context while pre-
vious models predict the next word given a tar-
get word history. Unlike previous work in phrase
translation modeling with NNs, our models have
the advantage of accessing source context that can
fall outside the phrase boundaries.
We now describe our models in a general set-
ting, predicting target translations given a source
context, where target translations can be either
words, stems or suffixes.
5
4.1 Neural network architecture
Following a common approach in deep learning
for NLP (Bengio et al., 2003; Collobert and We-
ston, 2008), we represent each source word s
i
by
a column vector r
s
i
? R
d
. Given a source con-
text c
s
i
= s
i?k
, ..., s
i
, ..., s
i+k
of k words on the
left and k words on the right of s
i
, the context rep-
resentation r
c
s
i
? R
(2k+1)?d
is obtained by con-
catenating the vector representations of all words
in c
s
i
:
r
c
s
i
= r
s
i?k
 ... r
s
i+k
Our main BNN architecture for word or stem
prediction (Figure 2a) is a feed-forward neural
network (FFNN) with one hidden layer, a matrix
W
1
? R
n?(2k+1)d
connecting the input layer to
the hidden layer, a matrix W
2
? R
|V
t
|?n
connect-
ing the hidden layer to the output layer, and a bias
vector b
2
? R
|V
t
|
where |V
t
| is the size of target
translations vocabulary. The target translation dis-
tribution P
BNN
(t|c
s
i
) for a given source context
c
s
i
is computed by a forward pass:
softmax
(
W
2
?(W
1
r
c
s
i
) + b
2
)
(2)
where ? is a nonlinearity (tanh, sigmoid or rec-
tified linear units). The parameters of the neural
5
The source code of our models is available at https:
//bitbucket.org/ketran
network are ? = {r
s
i
,W
1
,W
2
,b
2
}.
The suffix prediction BNN is obtained by
adding the target stem representation r
?
to the in-
put layer (see Figure 2b).
4.2 Model variants
We encounter two major issues with FFNNs: (i)
They do not provide a natural mechanism to com-
pute word surface conditional probability p(t|c
s
)
given individual stem probability p(?|c
s
) and suf-
fix probability p(?|c
s
, ?), and (ii) FFNNs do not
provide the flexibility to capture long dependen-
cies among words if they lie outside the source
context window. Hence, we consider two BNN
variants: a log-bilinear model (LBL) and a con-
volutional neural network model (ConvNet). LBL
could potentially address (i) by factorizing target
representations into target stem and suffix repre-
sentations whereas ConvNets offer the advantage
of modeling variable input length (ii) (Kalchbren-
ner et al., 2014).
Log-bilinear model. The FFNN models stem
and suffix probabilities separately. A log-bilinear
model instead could directly model word predic-
tion through a factored representation of target
words, similarly to Botha and Blunsom (2014).
Thus, no probability mass would be wasted over
stem-suffix combinations that are not in the candi-
date generation function. The LBL model speci-
fies the conditional distribution for the word trans-
lation t
j
? T
s
i
given a source context c
s
i
:
P?(tj |cs
i
) =
exp(s?(tj , cs
i
))
?
t
?
j
?T
s
i
exp(s?(t
?
j
, c
s
i
))
(3)
We use an additional set of word representations
q
t
j
? R
n
for target translations t
j
. The LBL
model computes a predictive representation q? of a
source context c
s
i
by taking a linear combination
of the source word representations r
s
i+m
with the
position-dependent weight matrices C
m
? R
n?d
:
q? =
k
?
m=?k
C
m
r
s
i+m
(4)
The score function s?(tj , cs
i
) measures the simi-
larity between the predictive representation q? and
the target representation q
t
j
:
s?(tj , cs
i
) = q?
T
q
t
j
+ b
T
h
q
t
j
+ b
t
j
(5)
1679
P?(t|csi)
rsi k rsi rsi+k
W1
W2
 (x)
(a) BNN for word prediction.
P?(?| , csi)
rsi k rsi rsi+k
W1
W2
 (x)
r 
(b) BNN for suffix prediction.
Figure 2: Feed-forward BNN architectures for predicting target translations: (a) word model (similar to
stem model), and (b) suffix model with an additional vector representation r
?
for target stems ?.
Here b
t
j
is the bias term associated with target
word t
j
. b
h
? R
n
are the representation bi-
ases. s?(tj , cs
i
) can be seen as the negative en-
ergy function of the target translation t
j
and its
context c
s
i
. The parameters of the model thus
are ? = {r
s
i
,C
m
,q
t
j
,b
h
, b
t
j
}. Our log-bilinear
model is a modification of the log-bilinear model
proposed for n-gram language modeling in (Mnih
and Hinton, 2007).
Convolutional neural network model. This
model (Figure 3) computes the predictive repre-
sentation q? by applying a sequence of 2k convo-
lutional layers {L
1
, . . . ,L
2k
}. The source context
c
s
i
is represented as a matrix m
c
s
i
? R
d?(2k+1)
:
m
c
s
i
=
[
r
s
i?k
; . . . ; r
s
i+k
]
(6)
q?
rs1 rs2 rs3 rs4 rs5 rs6rs0
Figure 3: Convolutional neural network model.
Edges with the same color indicate the same ker-
nel weight matrix.
Each convolutional layer L
i
consists of a one-
dimensional filter m
i
? R
d?2
. Each row of m
i
is convolved with the corresponding row in the
previous layer resulting in a weight matrix whose
number of columns decreases by one. Thus after
2k convolutional layers, the network transforms
the source context matrix m
c
s
i
to a feature vec-
tor q? ? R
d
. A fully connected layer with weight
matrix W followed by a softmax layer are placed
after the last convolutional layer L
2k
to perform
classification. The parameters of the convolutional
neural network model are ? = {r
s
i
,m
j
,W}.
Here, we focus on a fixed length input, how-
ever convolutional neural networks may be used to
model variable length input (Kalchbrenner et al.,
2014; Kalchbrenner and Blunsom, 2013).
4.3 Training
In training, for each example (t, c
s
), we maximize
the conditional probability P?(t|cs) of a correct
target label t. The contribution of the training ex-
ample (t, c
s
) to the gradient of the log conditional
probability is given by:
?
??
logP?(t|cs) =
?
??
s?(t|cs)
?
?
t
?
?T
s
P?(t
?
|c
s
)
?
??
s?(t
?
, c
s
)
Note that in the gradient, we do not sum over all
target translations T but a set of possible candi-
dates T
s
of a source word s. In practice |T
s
| ? 200
with our pruning settings (see Section 5.1), thus
training time for one example does not depend on
the vocabulary size. Our training criterion can be
seen as a form of contrastive estimation (Smith
and Eisner, 2005), however we explicitly move the
probability mass from competing candidates to the
correct translation candidate, thus obtaining more
reliable estimates of the conditional probabilities.
The BNN parameters are initialized randomly
according to a zero-mean Gaussian. We regularize
the models with L
2
. As an alternative to the L
2
regularizer, we also experiment with dropout (Hin-
ton et al., 2012), where the neurons are randomly
zeroed out with dropout rate p. This technique is
known to be useful in computer vision tasks but
has been rarely used in NLP tasks. In FFNN, we
use dropout after the hidden layer, while in Con-
vNet, dropout applies after the last convolutional
layer. The dropout rate p is set to 0.3 in our exper-
1680
iments. We use rectified nonlinearities
6
in FFNN
and after each convolutional layer in ConvNet. We
train our BNN models with the standard stochastic
gradient descent.
5 Evaluating word translation prediction
In this section, we assess the ability of our BNN
models to predict the correct translation of a word
in context. In addition to English-Russian, we also
consider translation prediction for Czech and Bul-
garian. As members of the Slavic language fam-
ily, Czech and Bulgarian are also characterized by
highly inflectional morphology. Czech, like Rus-
sian, displays a very rich nominal inflection with
as many as 14 declension paradigms. Bulgarian,
unlike Russian, is not affected by case distinctions
but is characterized by a definiteness suffix.
5.1 Experimental setup
The following parallel corpora are used to train the
BNN models:
? English-Russian: WMT13 data (News Com-
mentary and Yandex corpora);
? English-Czech: CzEng 1.0 corpus (Bojar et
al., 2012) (Web Pages and News sections);
? English-Bulgarian: a mix of crawled news
data, TED talks and Europarl proceedings.
Detailed corpus statistics are given in Table 2. For
each language pair, accuracies are measured on a
held-out set of 10K parallel sentences.
To prepare the candidate generation function,
each dataset is first word-aligned with GIZA++,
then a bilingual lexicon with maximum-likelihood
probabilities (P
mle
) is built from the symmetrized
alignment. After some frequency and signifi-
cance pruning,
7
the top 200 translations sorted by
P
mle
(t|s) ? P
mle
(s|t) are kept as candidate word
translations for each source word in the vocabu-
lary. Word alignments are also used to train the
BNN models: each alignment link constitutes a
training sample, with no special treatment of un-
aligned words and 1-to-many alignments.
The context window size k is set to 3 (cor-
responding to 7-gram) and the dimensionality of
6
We find that using rectified linear units gives better re-
sults than sigmoid and tanh.
7
Each lexicon is pruned with minimum word frequency 5,
minimum source-target word pair frequency 2, minimum log
odds ratio 10.
source word representations to 100 in all experi-
ments. The number of hidden units in our feed-
forward neural networks and the target translation
embedding size in LBL models are set to 200. All
models are trained for 10 iterations with learning
rate set to 0.001.
En-Ru En-Cs En-Bg
Sentences 1M 1M 0.8M
Src. tokens 26.5M 19.2M 19.3M
Trg. tokens 24.7M 16.7M 18.9M
Src. T/T .0109 .0105 .0051
Trg. T/T .0247 .0163 .0104
Table 2: BNN training corpora statistics: number
of sentences, tokens, and type/token ratio (T/T).
5.2 Word, stem and suffix prediction
accuracy
We measure accuracy at top-n, i. e. the number
of times the correct translation was in the top n
candidates sorted by a model. For each subtask?
word, stem and suffix prediction?the BNN
model is compared to the context-independent
maximum-likelihood baseline P
mle
(t|s) on which
the PSMT lexical weighting score is based. Note
that this is a more realistic baseline than the uni-
form models sometimes reported in the litera-
ture. The oracle corresponds to the percentage of
aligned source-target word pairs in the held-out set
that are covered by the candidate generation func-
tion. Out of the missing links, about 4% is due
to lexicon pruning. Results for all three language
pairs are presented in Table 3. In this series of
experiments, the morphological BNNs utilize un-
supervised segmentation models trained on each
target language following Lee et al. (2011).
8
As shown in Table 3, the BNN models outper-
form the baseline by a large margin in all tasks and
languages. In particular, word prediction accuracy
at top-1 increases by +6.4%, +24.6% and +9.0%
absolute in English-Russian, English-Czech and
English-Bulgarian respectively, without the use of
any features based on linguistic annotation. While
the baseline and oracle differences among lan-
guages can be explained by different levels of
overlap between training and held-out set, we can-
not easily explain why the Czech BNN perfor-
mance is so much higher. When comparing the
8
We use the C++ implementation available at http://
groups.csail.mit.edu/rbg/code/morphsyn
1681
Model En-Ru En-Cs En-Bg
Word prediction (%):
Baseline 33.0 / 50.1 42.0 / 59.9 47.9 / 66.0
Word BNN
39.4 / 56.6 66.6 / 81.4 56.9 / 74.0
+6.4 / +6.5 +24.6/+21.5 +9.0 / +8.0
Oracle 79.5 / 0.00 90.2 / 0.00 86.9 / 0.00
Stem prediction (%):
Baseline 40.7 / 58.2 46.1 / 64.3 51.9 / 70.1
Stem BNN
45.1 / 62.5 66.1 / 81.6 56.7 / 74.4
+4.4 / +4.3 +20.0/+17.3 +4.8 / +4.3
Suffix prediction (%):
Baseline 71.2 / 85.6 78.8 / 93.2 81.5 / 92.4
Suffix BNN
77.0 / 89.7 91.9 / 97.4 87.7 / 94.9
+5.8 / +4.1 +13.1 /+4.2 +6.2 / +2.5
Table 3: BNN prediction accuracy (top-1/top-3)
compared to a context-independent maximum-
likelihood baseline.
three prediction subtasks, we find that word pre-
diction is the hardest task as expected. Stem pre-
diction accuracies are considerably higher than
word prediction accuracies in Russian, but almost
equal in the other two languages. Finally, base-
line accuracies for suffix prediction are by far
the highest, ranging between 71.2% and 81.5%,
which is primarily explained by a smaller num-
ber of candidates to choose from. Also on this
task, the BNN model achieves considerable gains
of +5.8%, +13.1% and +6.2% at top-1, without the
need of manual feature engineering.
From these figures, it is hard to predict whether
word BNNs or morphological BNNs will have a
better effect on SMT performance. On one hand,
the word-level BNN achieves the highest gain over
the MLE baseline. On the other, the stem- and
suffix-level BNNs provide two separate scoring
functions, whose weights can be directly tuned for
translation quality. A preliminary answer to this
question is given by the SMT experiments pre-
sented in Section 6.
5.3 Effect of word segmentation
This section analyzes the effect of using different
segmentation techniques. We consider two super-
vised tagging methods that produce lemma and in-
flection tag for each token in a context-sensitive
manner: TreeTagger (Sharoff et al., 2008) for Rus-
sian and the Morce tagger (Spoustov? et al., 2007)
for Czech.
9
Finally, we employ the Russian Snow-
ball rule-based stemmer as a light-weight context-
9
Annotation included in the CzEng 1.0 corpus release.
Figure 4: Effect of different word segmentation
techniques (U: unsupervised, S: supervised, R:
rule-based stemmer) on stem and suffix prediction
accuracy. The dark part of each bar stands for top-
1, the light one for top-3 accuracy.
insensitive segmentation technique.
10
As shown in Figure 4, accuracies for both stem
and suffix prediction vary noticeably with the seg-
mentation used. However, higher stem accuracies
corresponds to lower suffix accuracies and vice
versa, which can be mainly due to a general pref-
erence of a tool to segment more or less than an-
other. In summary, the unsupervised segmentation
methods and the light-weight stemmer appear to
perform comparably to the supervised methods.
5.4 Effect of training data size
We examine the predictive power of our models
with respect to the size of training data. Table 4
shows the accuracies of stem and suffix models
trained on 200K and 1M English-Russian sentence
pairs with unsupervised word segmentation. Sur-
prisingly, we observe only a minor loss when we
decrease the training data size, which suggests that
our models are robust even on a small data set.
# Train sent. Stem Acc. Suffix Acc.
1M 45.1 / 62.5 77.0 / 89.7
200K 44.6 / 61.8 75.7 / 88.6
Table 4: Accuracy at top-1/top-3 (%) of stem and
suffix BNNs with different training data sizes.
5.5 Fine-grained evaluation
We evaluate the suffix BNN model at the part-of-
speech (POS) level. Table 5 provides suffix pre-
diction accuracy per POS for En-Ru. For this
analysis, Russian data is segmented by TreeTag-
10
http://snowball.tartarus.org/
algorithms/russian/stemmer.html
1682
ger. Additionally, we report the average number
of suffixes per stem given the part-of-speech.
Our results are consistent with the findings of
Chahuneau et al. (2013):
11
the prediction of ad-
jectives is more difficult than that of other POS
while Russian verb prediction is relatively easier
in spite of the higher number of suffixes per stem.
These differences reflect the importance of source
versus target context features in the prediction of
the target inflection: For instance, adjectives agree
in gender with the nouns they modify, but this may
be only inferred from the target context.
POS A V N M P
Acc. (%) 49.6 61.9 62.8 84.5 64.4
|M
?
| 18.2 18.4 9.2 7.1 13.3
Table 5: Suffix prediction accuracy at top-1 (%),
breakdown by category (A: adjectives, V: verbs,
N: nouns, M: numerals and P: pronouns). |M
?
|
denotes the average number of suffixes per stem.
5.6 Neural Network variants
Table 6 shows the stem and suffix accuracies of
BNN variants on English-Czech. Although none
of the variants outperform our main FFNN archi-
tecture, we observe similar performances by the
LBL on stem prediction, and by the ConvNet on
suffix prediction. This suggests that future work
could exploit their additional flexibilities (see Sec-
tion 4.2) to improve the BNN predictive power.
As for the low suffix accuracy by the LBL, it
can be explained by the absence of nonlinearity
transformation. Nonlinearity is important for the
suffix model where the prediction of target suf-
fix ?
j
often does not depend linearly on s
i
and
?
j
. The predictive representation of target stem
in the LBL stem model, however, mainly depends
on the source representation r
s
i
through a position
dependent weight matrix C
0
. Thus, we observe a
smaller accuracy drop in the stem model than in
the suffix model. Conversely, the ConvNet per-
forms poorly on stem prediction because it cap-
tures the meaning of the whole source context in-
stead of emphasizing the importance of the source
word s
i
as the main predictor of the target transla-
tion t
j
.
11
Chahuneau et al. (2013) report an average accuracy of
63.1% for the prediction of A, V, N, M suffixes. When we
train our model on the same dataset (news-commentary) we
obtain a comparable result (64.7% vs 63.1%).
Unexpectedly, no improvement is obtained by
the use of dropout regularizer (see Section 4.3).
Model Stem Acc Suffix Acc
FFNN 66.1 / 81.6 91.9 / 97.4
FFNN+do 64.6 / 81.1 91.5 / 97.5
LBL 63.6 / 79.6 86.4 / 96.4
ConvNet+do 58.6 / 75.6 90.3 / 96.9
Table 6: Accuracies at top-1/top-3 (%) of stem and
suffix models. +do indicates dropout instead of L
2
regularizer. FFNN is our main architecture.
6 SMT experiments
While the main objective of this paper is to im-
prove prediction accuracy of word translations,
see Section 5, we are also interested in know-
ing to which extent these improvements carry over
within an end-to-end machine translation task. To
this end, we integrate our translation prediction
models described in Section 4 into our existing
English-Russian SMT system.
For each phrase pair matching the input, the
phrase BNN score P
BNN-p
is computed as follows:
P
BNN-p
(s?,
?
t, a) =
|s?|
?
i=1
?
?
?
?
?
1
|{a
i
}|
?
j?{a
i
}
P
BNN
(t
j
|c
s
i
) if |{a
i
}| > 0
P
mle
(NULL|s
i
) otherwise
where a is the word-level alignment of the phrase
pair (s?,
?
t) and {a
i
} is the set of target positions
aligned to s
i
. If a source-target link cannot be
scored by the BNN model, we give it a P
BNN
probability of 1 and increment a separate count
feature ?. Note that the same phrase pair can get
different BNN scores if used in different source
side contexts.
Our baseline is an in-house phrase-based
(Koehn et al., 2003) statistical machine transla-
tion system very similar to Moses (Koehn et al.,
2007). All system runs use hierarchical lexicalized
reordering (Galley and Manning, 2008; Cherry
et al., 2012), distinguishing between monotone,
swap, and discontinuous reordering, all with re-
spect to left-to-right and right-to-left decoding.
Other features include linear distortion, bidirec-
tional lexical weighting (Koehn et al., 2003), word
and phrase penalties, and finally a word-level 5-
gram target LM trained on all available mono-
lingual data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999). The distortion
1683
Corpus Lang. #Sent. #Tok.
paral.train
EN
1.9M
48.9M
RU 45.9M
Wiki dict.
EN/RU 508K ?
mono.train
RU 21.0M 390M
WMT2012
EN
3K 64K
WMT2013
3K 56K
Table 7: SMT training and test data statistics. All
numbers refer to tokenized, lowercased data.
limit is set to 6 and for each source phrase the top
30 translation candidates are considered. When
translating into a morphologically rich language,
data sparsity issues in the target language become
particularly apparent. To compensate for this we
also experiment with a 5-gram suffix-based LM in
addition to the surface-based LM (M?ller et al.,
2012; Bisazza and Monz, 2014).
The BNN models are integrated as additional
log-probability feature functions (logP
BNN-p
):
one feature for the word prediction model or two
features for the stem and suffix models respec-
tively, plus the penalty feature ?.
Table 7 shows the data used to train our English-
Russian SMT system. The feature weights for all
approaches were tuned by using pairwise rank-
ing optimization (Hopkins and May, 2011) on the
wmt12 benchmark (Callison-Burch et al., 2012).
During tuning, 14 PRO parameter estimation runs
are performed in parallel on different samples of
the n-best list after each decoder iteration. The
weights of the individual PRO runs are then av-
eraged and passed on to the next decoding itera-
tion. Performing weight estimation independently
for a number of samples corrects for some of the
instability that can be caused by individual sam-
ples. The wmt13 set (Bojar et al., 2013) was used
for testing. We use approximate randomization
(Noreen, 1989) to test for statistically significant
differences between runs (Riezler and Maxwell,
2005).
Translation quality is measured with case-
insensitive BLEU[%] using one reference trans-
lation. As shown in Table 8, statistically signif-
icant improvements over the respective baseline
(Baseline and Base+suffLM) are marked
N
at the
p < .01 level. Integrating our bilingual neural net-
work approach into our SMT system yields small
but statistically significant improvements of 0.4
BLEU over a competitive baseline. We can also
SMT system wmt12 (dev) wmt13 (test)
Baseline 24.7 18.9
+ stem/suff. BNN 25.1 19.3
N
Base+suffLM 24.5 19.2
+ word BNN 24.5 19.3
+ stem/suff. BNN 24.7 19.6
N
Table 8: Effect of our BNN models on English-
Russian translation quality (BLEU[%]).
see that it is beneficial to add a suffix-based lan-
guage model to the baseline system. The biggest
improvement is obtained by combining the suffix-
based language model and our BNN approach,
yielding 0.7 BLEU over a competitive, state-of-
the-art baseline, of which 0.4 BLEU are due to our
BNNs. Finally, one can see that the BNNs mod-
eling stems and suffixes separately perform bet-
ter than a BNN directly predicting fully inflected
forms.
To better understand the BNN effect on the
SMT system, we analyze the set of phrase pairs
that are employed by the decoder to translate each
sentence. This set is ranked by the weighted com-
bination of phrase translation and lexical weight-
ing scores, target language model score and, if
available, phrase BNN scores. As shown in Ta-
ble 9, the morphological BNN models have a pos-
itive effect on the decoder?s lexical search space
increasing the recall of reference tokens among
the top 1 and 3 phrase translation candidates. The
mean reciprocal rank (MRR) also improves from
0.655 to 0.662. Looking at the 1-best SMT output,
we observe a slight increase of reference/output
recall (50.0% to 50.7%), which is less than the in-
crease we observe for the top 1 translation candi-
dates (57.6% to 59.0%). One possible explanation
is that the new, more accurate translation distribu-
tions are overruled by other SMT model scores,
Token recall (wmt12): Baseline +BNN
reference/MT-search-space [top-1] 57.6% 59.0%
reference/MT-search-space [top-3] 70.7% 70.9%
reference/MT-search-space [top-30] 86.0% 85.0%
reference/MT-search-space [MRR] 0.655 0.662
reference/MT-output 50.0% 50.7%
stem-only reference/MT-output 12.3% 11.5%
of which reachable 11.2% 10.3%
Table 9: Target word coverage analysis of the
English-Russian SMT system before and after
adding the morphological BNN models.
1684
like the target LM, that are based on traditional
maximum-likelihood estimates. While the suffix-
based LMs proved beneficial in our experiments,
we speculate that higher gains could be obtained
by coupling our approach with a morphology-
aware neural LM like the one recently presented
by Botha and Blunsom (2014).
7 Related work
While most relevant literature has been discussed
in earlier sections, the following approaches are
particularly related to ours: Minkov et al. (2007)
and Toutanova et al. (2008) address target inflec-
tion prediction with a log-linear model based on
rich morphological and syntactic features. Their
model exploits target context and is applied to
inflect the output of a stem-based SMT system,
whereas our models predict target words (or pairs
of stem-suffix) independently and are integrated
into decoding. Chahuneau et al. (2013) address
the same problem with another feature-rich dis-
criminative model that can be integrated in decod-
ing, like ours, but they also use it to inflect on-
the-fly stemmed phrases. It is not clear what part
of their SMT improvements is due to the gener-
ation of new phrases or to better scoring. Jeong
et al. (2010) predict surface word forms in con-
text, similarly to our word BNN, and integrate the
scores into the SMT system. Unlike us, they rely
on linguistic feature-rich log-linear models to do
that. Gimpel and Smith (2008) propose a similar
approach to directly predict phrases in context, in-
stead of words.
All those approaches employed features that
capture the global structure of source sentences,
like dependency relations. By contrast, our mod-
els access only local context in the source sen-
tence but they achieve accuracy gains comparably
to models that also use global sentence structure.
8 Conclusions
We have proposed a general approach to predict
word translations in context using bilingual neu-
ral network architectures. Unlike previous NN ap-
proaches, we model word, stem and suffix dis-
tributions in the target language given context in
the source language. Instead of relying on man-
ually engineered features, our models automati-
cally learn abstract word representations and fea-
tures that are relevant for the modeled task directly
from word-aligned parallel data. Our preliminary
results with LBL and ConvNet architectures sug-
gest that potential improvement may be achieved
by factorizing target representations or by dynam-
ically modeling source context size. Evaluated
on three morphologically rich languages, our ap-
proach achieves considerable gains in word, stem
and suffix accuracy over a context-independent
maximum-likelihood baseline. Finally, we have
shown that the proposed BNN models can be
tightly integrated into a phrase-based SMT sys-
tem, resulting in small but statistically significant
BLEU improvement over a competitive, large-
scale English-Russian baseline.
Our analysis shows that the number of correct
target words occurring in highly scored phrase
translation candidates increases after integrating
the morphological BNNs. However, only few of
these end up in the 1-best translation output. Fu-
ture work will investigate the benefits of coupling
our BNN models with target language models that
also exploit abstract word representations, such as
Botha and Blunsom (2014) and Auli et al. (2013).
Acknowledgments
This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
612.001.218. We would like to thank Ekaterina
Garmash for helping with the error analysis of the
English-Russian translations.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October.
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Arianna Bisazza and Christof Monz. 2014. Class-
based language modeling for translating into mor-
phologically rich languages. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics, pages 1918?1927,
Dublin, Ireland.
Ond
?
rej Bojar, Zden
?
ek ?abokrtsk?, Ond
?
rej Du?ek, Pe-
tra Galu?
?
c?kov?, Martin Majli?, David Mare
?
cek, Ji
?
r?
Mar??k, Michal Nov?k, Martin Popel, and Ale? Tam-
chyna. 2012. The joy of parallelism with czeng
1685
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Asso-
ciation.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In Proceedings of the 31st In-
ternational Conference on Machine Learning, Bei-
jing, China, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?al, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1677?1687, Seat-
tle, USA, October.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 200?209, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th Annual International Confer-
ence on Machine Learning, volume 12, pages 2493?
2537.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 678?683, Sofia, Bulgaria, August.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1?11, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 699?709. Associ-
ation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 9?17, Columbus, Ohio,
USA.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?12, pages 146?155, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal Bosch,
and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine Translation, 25(3):239?
285, September.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
USA, October.
1686
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 655?665. Association for
Computational Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in
statistical machine translation. In Proceedings of
the 16th Conference of the European Association for
Machine Translation (EAMT).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133, Ed-
monton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn B. Taylor, editors, Proceedings of the 6th
Conference of the Association for Machine Transla-
tions in the Americas (AMTA 2004), pages 115?124.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Fran?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of Proceedings of ICASSP.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 1?9, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Arne Mauser, Sa?a Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 210?218, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128?135.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three
new graphical models for statistical language mod-
elling. In Proceedings of the 24th International
Conference on Machine Learning, pages 641?648,
New York, NY, USA.
Thomas M?ller, Hinrich Sch?tze, and Helmut Schmid.
2012. A comparative investigation of morphological
language modeling for the languages of the Euro-
pean Union. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 386?395, Montr?al, Canada,
June. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in
statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 9?16, New York
City, USA, June. Association for Computational
Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dechelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL 2006 Conference, pages 723?
730, Sydney, Australia, July. Association for Com-
putational Linguistics.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Design-
ing and evaluating a russian tagset. In Pro-
ceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC?08),
Marrakech, Morocco. European Language Re-
sources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
1687
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Le Hai Son, Alexandre Allauzen, and Fran?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39?48, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Drahom?ra Spoustov?, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, pages 67?74, Prague, Czech Republic, June.
Association for Computational Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of the Associa-
tion for Computational Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387?1392, Seattle, October.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, USA, October.
1688
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 439?448,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Cutting the Long Tail: Hybrid Language Models
for Translation Style Adaptation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
In this paper, we address statistical ma-
chine translation of public conference talks.
Modeling the style of this genre can be very
challenging given the shortage of available
in-domain training data. We investigate the
use of a hybrid LM, where infrequent words
are mapped into classes. Hybrid LMs are
used to complement word-based LMs with
statistics about the language style of the
talks. Extensive experiments comparing
different settings of the hybrid LM are re-
ported on publicly available benchmarks
based on TED talks, from Arabic to English
and from English to French. The proposed
models show to better exploit in-domain
data than conventional word-based LMs for
the target language modeling component of
a phrase-based statistical machine transla-
tion system.
1 Introduction
The translation of TED conference talks1 is an
emerging task in the statistical machine transla-
tion (SMT) community (Federico et al 2011).
The variety of topics covered by the speeches, as
well as their specific language style, make this a
very challenging problem.
Fixed expressions, colloquial terms, figures of
speech and other phenomena recurrent in the talks
should be properly modeled to produce transla-
tions that are not only fluent but that also em-
ploy the right register. In this paper, we propose
a language modeling technique that leverages in-
domain training data for style adaptation.
1http://www.ted.com/talks
Hybrid class-based LMs are trained on text
where only infrequent words are mapped to Part-
of-Speech (POS) classes. In this way, topic-
specific words are discarded and the model fo-
cuses on generic words that we assume more use-
ful to characterize the language style. The factor-
ization of similar expressions made possible by
this mixed text representation yields a better n-
gram coverage, but with a much higher discrimi-
native power than POS-level LMs.
Hybrid LM also differs from POS-level LM in
that it uses a word-to-class mapping to determine
POS tags. Consequently, it doesn?t require the de-
coding overload of factored models nor the tag-
ging of all parallel data used to build phrase ta-
bles. A hybrid LM trained on in-domain data can
thus be easily added to an existing baseline sys-
tem trained on large amounts of background data.
The proposed models are used in addition to
standard word-based LMs, in the framework of
log-linear phrase-based SMT.
The remainder of this paper is organized as fol-
lows. After discussing the language style adapta-
tion problem, we will give an overview of relevant
work. In the following sections we will describe
in detail hybrid LM and its possible variants. Fi-
nally, we will present an empirical analysis of the
proposed technique, including intrinsic evaluation
and SMT experiments.
2 Background
Our working scenario is the translation of TED
talks transcripts as proposed by the IWSLT Eval-
uation Campaign2. This genre covers a variety
of topics ranging from business to psychology.
The available training material ? both parallel and
2http://www.iwslt2011.org
439
Beginning of Sentence: [s] End of Sentence: [/s]
TED NEWS TED NEWS
1st [s] Thank you . [/s] 1st [s] ( AP ) - 1st [s] Thank you . [/s] 1st ? he said . [/s]
2 [s] Thank you very much 2 [s] WASHINGTON ( ... 2 you very much . [/s] 2 ? she said . [/s]
3 [s] I ?m going to 3 [s] NEW YORK ( AP 3 in the world . [/s] 3 , he said . [/s]
4 [s] And I said , 4 [s] ( CNN ) ? 4 and so on . [/s] 4 ? he said . [/s]
5 [s] I don ?t know 5 [s] NEW YORK ( R... 5 , you know . [/s] 5 in a statement . [/s]
6 [s] He said , ? 6 [s] He said : ? 6 of the world . [/s] 6 the United States . [/s]
7 [s] I said , ? 7 [s] ? I don ?t 7 around the world . [/s] 7 to this report . [/s]
8 [s] And of course , 8 [s] It was last updated 8 . Thank you . [/s] 8 ? he added . [/s]
9 [s] And one of the 9 [s] At the same time 9 the United States . [/s] 9 , police said . [/s]
10 [s] And I want to ... 10 all the time . [/s] 10 , officials said . [/s]
11 [s] And that ?s what 69 [s] I don ?t know 11 to do it . [/s] ...
12 [s] We ?re going to 612 [s] I ?m going to 12 and so forth . [/s] 13 in the world . [/s]
13 [s] And I think that 2434 [s] ? I said , 13 don ?t know . [/s] 17 around the world . [/s]
14 [s] And you can see 7034 [s] He said , ? 14 to do that . [/s] 46 of the world . [/s]
15 [s] And this is a 8199 [s] And I said , 15 in the future . [/s] 129 all the time . [/s]
16 [s] And this is the 8233 [s] Thank you very much 16 the same time . [/s] 157 and so on . [/s]
17 [s] And he said , ... 17 , you know ? [/s] 1652 , you know . [/s]
18 [s] So this is a ? [s] Thank you . [/s] 18 to do this . [/s] 5509 you very much . [/s]
Table 1: Common sentence-initial and sentence-final 5-grams, as ranked by frequency, in the TED and NEWS
corpora. Numbers denote the frequency rank.
monolingual ? consists of a rather small collection
of TED talks plus a variety of large out-of-domain
corpora, such as news stories and UN proceed-
ings.
Given the diversity of topics, the in-domain
data alone cannot ensure sufficient coverage to an
SMT system. The addition of background data
can certainly improve the n-gram coverage and
thus the fluency of our translations, but it may also
move our system towards an unsuitable language
style, such as that of written news.
In our study, we focus on the subproblem of
target language modeling and consider two En-
glish text collections, namely the in-domain TED
and the out-of-domain NEWS3, summarized in
Table 2. Because of its larger size ? two orders
of magnitude ? the NEWS corpus can provide a
better LM coverage than the TED on the test data.
This is reflected both on perplexity and on the av-
erage length of the context (or history h) actually
3http://www.statmt.org/wmt11/translation-task.html
LM Data |S| |W | |V | PP h5g
TED-En 124K 2.4M 51K 112 1.7
NEWS-En 30.7M 782M 2.2M 104 2.5
Table 2: Training data and coverage statistics of two
5-gram LMs used for the TED task: number of sen-
tences and tokens, vocabulary size; perplexity and av-
erage word history.
used by these two LMs to score the test?s refer-
ence translations. Note that the latter measure is
bounded at the LM order minus one, and is in-
versely proportional to the number of back-offs
performed by the model. Hence, we use this value
to estimate how well an n-gram LM fits the test
data. Indeed, despite the genre mismatch, the per-
plexity of a NEWS 5-gram LM on the TED-2010
test reference translations is 104 versus 112 for
the in-domain LM, and the average history size is
2.5 versus 1.7 words.
TED NEWS
1st , 1st the
... ...
9 I 40 I
12 you 64 you
90 actually 965 actually
268 stuff 2479 guy
370 guy 2861 stuff
436 amazing 4706 amazing
Table 3: Excerpts from TED and NEWS training vo-
cabularies, as ranked by frequency. Numbers denote
the frequency rank.
Yet we observe that the style of public speeches
is much better represented in the in-domain cor-
pus than in the out-of-domain one. For instance,
let us consider the vocabulary distribution4 of the
4Hesitations and filler words, typical of spoken language,
are not covered in our study because they are generally not
reported in the TED talk transcripts.
440
two corpora (Table 3). The very first forms, as
ranked by frequency, are quite similar in the two
corpora. However, there are important excep-
tions: the pronouns I and you are among the top
20 frequent forms in the TED, while in the NEWS
they are ranked only 40th and 64th respectively.
Other interesting cases are the words actually,
stuff, guy and amazing, all ranked about 10 times
higher in the TED than in the NEWS corpus.
We can also analyze the most typical ways
to start and end a sentence in the two text col-
lections. As shown in Table 1, the frequency
ranking of sentence-initial and sentence-final 5-
grams in the in-domain corpus is notably different
from the out-of-domain one. TED?s most frequent
sentence-initial 5-gram ?[s] Thank you . [/s] ? is
not at all attested in the NEWS corpus. As for
the 4th most common sentence start ?[s] And I
said ,? is only ranked 8199th in the NEWS, and
so on. Notably, the top ranked NEWS 5-grams in-
clude names of cities (Washington, New York) and
of news agency (AP, Reuters). As regards sen-
tence endings, we observe similar contrasts: for
instance, the word sequence ?and so on . [/s] ?
is ranked 4th in the TED and 157th in the NEWS
while ?, you know . [/s] ? is 5th in the TED and
only 1652th in the NEWS.
These figures confirm that the talks have a spe-
cific language style, remarkably different from
that of the written news genre. In summary, talks
are characterized by a massive use of first and sec-
ond persons, by shorter sentences, and by more
colloquial lexical and syntactic constructions.
3 Related Work
The brittleness of n-gram LMs in case of mis-
match between training and task data is a well
known issue (Rosenfeld, 2000). So called do-
main adaptation methods (Bellegarda, 2004) can
improve the situation, once a limited amount
of task specific data become available. Ideally,
domain-adaptive LMs aim to improve model ro-
bustness under changing conditions, involving
possible variations in vocabulary, syntax, content,
and style. Most of the known LM adaption tech-
niques (Bellegarda, 2004), however, address all
these variations in a holistic way. A possible rea-
son for this is that LM adaptation methods were
originally developed under the automatic speech
recognition framework, which typically assumes
the presence of one single LM. The progressive
adoption of the log-linear modeling framework in
many NLP tasks has recently introduced the use
of multiple LM components (features), which per-
mit to naturally factor out and integrate different
aspects of language into one model. In SMT, the
factored model (Koehn and Hoang, 2007), for in-
stance, permits to better tailor the LM to the task
syntax, by complementing word-based n-grams
with a part-of-speech (POS) LM , that can be es-
timated even on a limited amount of task-specific
data. Besides many works addressing holistic LM
domain adaptation for SMT, e.g. Foster and Kuhn
(2007), recently methods were also proposed to
explicitly adapt the LM to the discourse topic of a
talk (Ruiz and Federico, 2011). Our work makes
another step in this direction by investigating hy-
brid LMs that try to explicitly represent the speak-
ing style of the talk genre. As a difference from
standard class-based LMs (Brown et al 1992) or
the more recent local LMs (Monz, 2011), which
are used to predict sequences of classes or word-
class pairs, our hybrid LM is devised to pre-
dict sequences of classes interleaved by words.
While we do not claim any technical novelty in
the model itself, to our knowledge a deep investi-
gation of hybrid LMs for the sake of style adap-
tation is definitely new. Finally, the term hybrid
LM was inspired by Yazgan and Sarac?lar (2004),
which called with this name a LM predicting se-
quences of words and sub-words units, devised to
let a speech recognizer detect out-of-vocabulary-
words.
4 Hybrid Language Model
Hybrid LMs are n-gram models trained on a
mixed text representation where each word is ei-
ther mapped to a class or left as is. This choice
is made according to a measure of word common-
ness and is univocal for each word type.
The rationale is to discard topic-specific words,
while preserving those words that best character-
ize the language style (note that word frequency
is computed on the in-domain corpus only). Map-
ping non-frequent terms to classes naturally leads
to a shorter tail in the frequency distribution, as
visualized by Figure 1. A model trained on such
data has a better n-gram coverage of the test set
and may take advantage of a larger context when
scoring translation hypotheses.
As classes, we use deterministically assigned
POS tags, obtained by first tagging the data with
441
??
??
???
???
????
????
? ??? ??? ??? ??? ??? ???
????
????
Figure 1: Type frequency distribution in the English
TED corpus before and after POS-mapping of words
with less than 500 occurrences (25% of tokens). The
rank in the frequency list (x-axis) is plotted against the
respective frequency in logarithmic scale. Types with
less than 20 occurrences are omitted from the graph.
Tree Tagger (Schmid, 1994) and then choosing
the most likely tag for each word type. In this
way, we avoid the overload of searching for the
best tagging decisions at run-time at the cost of
a slightly higher imprecision (see Section 5.1).
The hybridly mapped data is used to train a high-
order n-gram LM that is plugged into an SMT de-
coder as an additional feature on target word se-
quences. During the translation process, words
are mapped to their class just before querying the
hybrid LM, therefore translation models can be
trained on plain un-tagged data.
As exemplified in Table 4, hybrid LMs can
draw useful statistics on the context of common
words even from a small corpus such as the TED.
To have an idea of data sparseness, consider that
in the unprocessed TED corpus the most frequent
5-gram containing the common word guy occurs
only 3 times. After the mapping of words with
frequency <500, the highest 5-gram frequency
grows to 17, the second one to 9, and so on.
guy 598 actually 3978
a guy VBN NP NP 17 [s] This is actually a 20
guy VBN NP NP , 9 [s] It ?s actually a 17
guy , NP NP , 8 , you can actually VB 13
a guy called NP NP 8 is actually a JJ NN 13
this guy , NP NP 6 This is actually a NN 12
guy VBN NP NP . 6 [s] And this is actually 12
by a guy VBN NP 5 [s] And that ?s actually 10
a JJ guy . [/s] 5 , but it ?s actually 10
I was VBG this guy 4 NN , it ?s actually 9
guy VBN NP . [/s] 4 we?re actually going to 8
Table 4: Most common hybrid 5-grams containing the
words guy and actually, along with absolute frequency.
4.1 Word commonness criteria
The most intuitive way to measure word common-
ness is by absolute term frequency (F ). We will
use this criterion in most of our experiments. A
finer solution would be to also consider the com-
monness of a word across different talks. At this
end, we propose to use the fdf statistics, that is the
product of relative term f requency and document
f requency5:
fdfw =
c(w)
?
w? c(w
?)
?
c(dw)
c(d)
where dw are the documents (talks) containing at
least one occurrence of the word w.
If available, real talk boundaries can be used
to define the documents. Alternatively, we can
simply split the corpus into chunks of fixed size.
In this work we use this approximation.
Another issue is how to set the threshold. In-
dependently from the chosen commonness mea-
sure, we can reason in terms of the ratio of tokens
that are mapped to POS classes (WP ). For in-
stance, in our experiments with English, we can
set the threshold to F=500 and observe that WP
corresponds to 25% of the tokens (and 99% of the
types). In the same corpus, a similar ratio is ob-
tained with fdf=0.012.
In our study, we consider three ratios WP ={.25,
.50, .75} that correspond to different levels of lan-
guage modeling: from a domain-generic word-
level LM to a lexically anchored POS-level LM.
4.2 Handling morphology
Token frequency-based measures may not be suit-
able for languages other than English. When
translating into French, for instance, we have to
deal with a much richer morphology.
As a solution we can use lemmas, univocally
assigned to word types in the same manner as
POS tags. Lemmas can be employed in two ways:
only for word selection, as a frequency measure,
or also for word representation, as a mapping for
common words. In the former, we preserve in-
flected variants that may be useful to model the
language style, but we also risk to see n-gram cov-
erage decrease due to the presence of rare types.
In the latter, only canonical forms and POS tags
5This differs from the tf-idf widely used in information
retrieval, which is used to measure the relevance of a term in
a document. Instead, we measure commonness of a term in
the whole corpus.
442
appear in the processed text, thus introducing a
further level of abstraction from the original text.
Here follows a TED sentence in its original
version (first line) and after three different hy-
brid mappings ? namely WP =.25, WP =.25 with
lemma forms, and WP =.50:
Now you laugh, but that quote has kind of a sting to it, right.
Now you VB , but that NN has kind of a NN to it, right.
Now you VB , but that NN have kind of a NN to it, right.
RB you VB , CC that NN VBZ NN of a NN to it, RB .
5 Evaluation
In this section we perform an intrinsic evaluation
of the proposed LM technique, then we measure
its impact on translation quality when integrated
into a state-of-the-art phrase-based SMT system.
5.1 Intrinsic evaluation
We analyze here a set of hybrid LMs trained on
the English TED corpus by varying the ratio of
POS-mapped words and the word representation
technique (word vs lemma). All models were
trained with the IRSTLM toolkit (Federico et al
2008), using a very high n-gram order (10) and
Witten-Bell smoothing.
First, we estimate an upper bound of the POS
tagging errors introduced by deterministic tag-
ging. At this end, the hybridly mapped data is
compared with the actual output of Tree Tagger on
the TED training corpus (see Table 5). Naturally,
the impact of tagging errors correlates with the ra-
tio of POS-mapped tokens, as no error is counted
on non-mapped tokens. For instance, we note that
the POS error rate is only 1.9% in our primary set-
ting, WP =.25 and word representation, whereas
on a fully POS-mapped text it is 6.6%. Note that
the English tag set used by Tree Tagger includes
43 classes.
Now we focus on the main goal of hybrid text
representation, namely increasing the coverage of
the in-domain LM on the test data. Here too, we
measure coverage by the average length of word
history h used to score the test reference transla-
tions (see Section 2). We do not provide perplex-
ity figures, since these are not directly compara-
ble across models with different vocabularies. As
shown by Table 5, n-gram coverage increases with
the ratio of POS-mapped tokens, ranging from 1.7
on an all-words LM to 4.4 on an all-POS LM. Of
Hybrid 10g LM |V | POS-Err h10g
all words 51299 0.0% 1.7
all lemmas 38486 0.0% 1.9
.25 POS/words 475 1.9% 2.7
.50 POS/words 93 4.1% 3.5
.75 POS/words 50 5.7% 4.1
allPOS 43 6.6% 4.4
.25 POS/lemmas 302 1.8% 2.8
.25 POS/words(fdf) 301 1.9% 2.7
Table 5: Comparison of LMs obtained from different
hybrid mappings of the English TED corpus: vocabu-
lary size, POS error rate, and average word history on
IWSLT?tst2010?s reference translations.
course, the more words are mapped, the less dis-
criminative our model will be. Thus, choosing the
best hybrid mapping means finding the best trade-
off between coverage and informativeness.
We also applied hybrid LM to the French lan-
guage, again using Tree Tagger to create the POS
mapping. The tag set in this case comprises 34
classes and the POS error rate with WP =.25 is
1.2% (compare with 1.9% in English). As previ-
ously discussed, morphology has a notable effect
on the modeling of French. In fact, the vocabu-
lary reduction obtained by mapping all the words
to their most probable lemma is -45% (57959 to
31908 types in the TED corpus), while in English
it is only -25%.
5.2 SMT baseline
Our SMT experiments address the translation of
TED talks from Arabic to English and from En-
glish to French. The training and test datasets
were provided by the organizers of the IWSLT11
evaluation, and are summarized in Table 6.
Marked in bold are the corpora used for hybrid
LM training. Dev and test sets have a single ref-
erence translation.
For both language pairs, we set up com-
petitive phrase-based systems6 using the Moses
toolkit (Koehn et al 2007). The decoder fea-
tures a statistical log-linear model including a
phrase translation model and a phrase reordering
model (Tillmann, 2004; Koehn et al 2005), two
word-based language models, distortion, word
and phrase penalties. The translation and re-
ordering models are obtained by combining mod-
els independently trained on the available paral-
6The SMT systems used in this paper are thoroughly de-
scribed in (Ruiz et al 2011).
443
Corpus |S| |W | `
AR-EN
TED 90K 1.7M 18.9
UN 7.9M 220M 27.8
EN
TED 124K 2.4M 19.5
NEWS 30.7M 782M 25.4
AR test
dev2010 934 19K 20.0
tst2010 1664 30K 18.1
EN-FR
TED 105K 2.0M 19.5
UN 11M 291M 26.5
NEWS 111K 3.1M 27.6
FR
TED 107K 2.2M 20.6
NEWS 11.6M 291M 25.2
EN test
dev2010 934 20K 21.5
tst2010 1664 32K 19.1
Table 6: IWSLT11 training and test data statistics:
number of sentences |S|, number of tokens |W | and
average sentence length `. Token numbers are com-
puted on the target language, except for the test sets.
lel corpora: namely TED and NEWS for Arabic-
English; TED, NEWS and UN for English-
French. To this end we applied the fill-up method
(Nakov, 2008; Bisazza et al 2011) in which out-
of-domain phrase tables are merged with the in-
domain table by adding only new phrase pairs.
Out-of-domain phrases are marked with a binary
feature whose weight is tuned together with the
SMT system weights.
For each target language, two standard 5-gram
LMs are trained separately on the monolingual
TED and NEWS datasets, and log-linearly com-
bined at decoding time. In the Arabic-English
task, we use a hierarchical reordering model (Gal-
ley and Manning, 2008; Hardmeier et al 2011),
while in the English-French task we use a default
word-based bidirectional model. The distortion
limit is set to the default value of 6. Note that
the use of large n-gram LMs and of lexicalized
reordering models was shown to wipe out the im-
provement achievable by POS-level LM (Kirch-
hoff and Yang, 2005; Birch et al 2007).
Concerning data preprocessing we apply stan-
dard tokenization to the English and French text,
while for Arabic we use an in-house tokenizer that
removes diacritics and normalizes special charac-
ters and digits. Arabic text is then segmented with
AMIRA (Diab et al 2004) according to the ATB
scheme7. The Arabic-English system uses cased
7The Arabic Treebank tokenization scheme isolates con-
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
translation models, while the English-French sys-
tem uses lowercased models and a standard re-
casing post-process.
Feature weights are tuned on dev2010 by
means of a minimum error training procedure
(MERT) (Och, 2003). Following suggestions by
Clark et al(2011) and Cettolo et al(2011) on
controlling optimizer instability, we run MERT
four times on the same configuration and use the
average of the resulting weights to evaluate trans-
lation performance.
5.3 Hybrid LM integration
As previously stated, hybrid LMs are trained only
on in-domain data and are added to the log-linear
decoder as an additional target LM. To this end,
we use the class-based LM implementation pro-
vided in Moses and IRSTLM, which applies the
word-to-class mapping to translation hypotheses
before LM querying8. The order of the additional
LM is set to 10 in the Arabic-English evaluation
and 7 in the English-French, as these appeared to
be the best settings in preliminary tests.
Translation quality is measured by BLEU (Pa-
pineni et al 2002), METEOR (Banerjee and
Lavie, 2005) and TER (Snover et al 2006)9. To
test whether differences among systems are statis-
tically significant we use approximate randomiza-
tion as done in (Riezler and Maxwell, 2005)10.
Model variants. The effect on MT quality of
various hybrid LM variants is shown in Table 7.
Note that allPOS and allLemmas refer to deter-
ministically assigned POS tags and lemmas, re-
spectively. Concerning the ratio of POS-mapped
tokens, the best performing values are WP =.25 in
Arabic-English and WP =.50 in English-French.
These hybrid mappings outperform all the uni-
form representations (words, lemmas and POS)
with statistically significant BLEU and METEOR
improvements.
The fdf experiment involves the use of doc-
ument frequency for the selection of common
words. Its performance is very close to that of hy-
8Detailed instructions on how to build and use hybrid
LMs can be found at http://hlt.fbk.eu/people/bisazza.
9We use case-sensitive BLEU and TER, but case-
insensitive METEOR to enable the use of paraphrase tables
distributed with the tool (version 1.3).
10Translation scores and significance tests were com-
puted with the Multeval toolkit (Clark et al 2011):
https://github.com/jhclark/multeval.
444
(a) Arabic to English, IWSLT?tst2010
Added InDomain 10gLM BLEU?MET ? TER ?
.00 POS/words (all words)? 26.1 30.5 55.4
.00 POS/lemmas (all lem.) 26.0 30.5 55.4
1.0 POS/words (all POS)? 25.9 30.6 55.3
.25 POS/words? 26.5 30.6 54.7
.50 POS/words 26.5 30.6 54.9
.75 POS/words 26.3 30.7 55.0
.25 POS/words(fdf) 26.5 30.7 54.7
.25 POS/lemmaF 26.4 30.6 54.8
.25 POS/lemmas 26.5 30.8 54.6
(b) English to French, IWSLT?tst2010
Added InDomain 7gLM BLEU?MET ? TER ?
.00 POS/words (all words) 31.1 52.5 49.9
.00 POS/lemmas (all lem.)? 31.2 52.6 49.7
1.0 POS/words (all POS)? 31.4 52.8 49.8
.25 POS/lemmas? 31.5 52.9 49.7
.50 POS/lemmas 31.9 53.3 49.5
.75 POS/lemmas 31.7 53.2 49.6
.50 POS/lemmas(fdf) 31.9 53.3 49.5
.50 POS/lemmaF 31.6 53.0 49.6
.50 POS/words 31.7 53.1 49.5
Table 7: Comparison of various hybrid LM variants. Translation quality is measured with BLEU, METEOR and
TER (all in percentage form). The settings used for weight tuning are marked with ?. Best models according to
all metrics are highlighted in bold.
brid LMs simply based on term frequency; only
METEOR gains 0.1 points in Arabic-English. A
possible reason for this is that document fre-
quency was computed on fixed-size text chunks
rather than on real document boundaries (see Sec-
tion 4.1). The lemmaF experiment refers to the
use of canonical forms for frequency measuring:
this technique does not seem to help in either lan-
guage pair. Finally, we compare the use of lem-
mas versus surface forms to represent common
words. As expected, lemmas appear to be help-
ful for French language modeling. Interestingly
this is also the case for English, even if by a small
margin (+0.2 METEOR, -0.1 TER).
Summing up, hybrid mapping appears as a
winning strategy compared to uniform map-
ping. Although differences among LM variants
are small, the best model in Arabic-English is
.25-POS/lemmas, which can be thought of as
a domain-generic lemma-level LM. In English-
French, instead, the highest scores are achieved
by .50-POS/lemmas or .50-POS/lemmas(fdf), that
is POS-level LM with few frequently occurring
lexical anchors (vocabulary size 59). An inter-
pretation of this result is that, for French, mod-
eling the syntax is more helpful than modeling
the style. We also suspect that the French TED
corpus is more irregular and diverse with respect
to the style, than its English counterpart. In fact,
while the English corpus include transcripts of
talks given by English speakers, the French one is
mostly a collection of (human) translations. Typi-
cal features of the speech style may have been lost
in this process.
Comparison with baseline. In Table 8 the
best performing hybrid LM is compared against
the baseline that only includes the standard LMs
described in Section 5.2. To complete our eval-
uation, we also report the effect of an in-domain
LM trained on 50 word classes induced from the
corpus by maximum-likelihood based clustering
(Och, 1999).
In the two language pairs, both types of LM
result in consistent improvements over the base-
line. However, the gains achieved by the hybrid
approach are larger and all statistically signifi-
cant. The hybrid approach is significantly bet-
ter than the unsupervised one by TER in Arabic-
English and by BLEU and METEOR in English-
French (these siginificances are not reported in
(a) Arabic to English, IWSLT?tst2010
Added InDomain
BLEU? MET ? TER ?
10g LM
none (baseline) 26.0 30.4 55.6
unsup. classes 26.4? 30.8? 55.1?
hybrid 26.5?(+.5) 30.8?(+.4) 54.6?(-1.0)
(b) English to French, IWSLT?tst2010
Added InDomain
BLEU? MET ? TER ?
7g LM
none (baseline) 31.2 52.7 49.8
unsup. classes 31.5 52.9 49.6
hybrid 31.9?(+.7) 53.3?(+.6) 49.5?(-.3)
Table 8: Final MT results: baseline vs unsupervised
word classes-based LM and best hybrid LM. Statis-
tically significant improvements over the baseline are
marked with ? at the p < .01 and ? at the p < .05 level.
445
the table for clarity). The proposed method ap-
pears to better leverage the available in-domain
data, achieving improvements according to all
metrics: +0.5/+0.4/-1.0 BLEU/METEOR/TER
in Arabic-English and +0.7/-0.6/-0.3 in English-
French, without requiring any bitext annotation or
decoder modification.
Talk-level analysis. To conclude the study,
we analyze the effect of our best hybrid LM
on Arabic-English translation quality, at the sin-
gle talk level. The test used in the experiments
(tst2010) consists of 11 transcripts with an av-
erage length of 151?73 sentences. For each
talk, we compare the baseline BLEU score with
that obtained by adding a .25-POS/lemmas hybrid
LM. Results are presented in Figure 2. The dark
and light columns denote baseline and hybrid-LM
BLEU scores, respectively, and refer to the left y-
axis. Additional data points, plotted on the right
y-axis in reverse order, represent talk-level per-
plexities (PP) of a standard 5-gram LM trained
on TED (?) and those of the .25-POS/lemmas
10-gram hybrid LM (M), computed on reference
translations.
What emerges first is a dramatic variation of
performance among the speeches, with baseline
BLEU scores ranging from 33.95 on talk ?00? to
only 12.42 on talk ?02?. The latter talk appears as
a corner case also according to perplexities (397
by word LM and 111 by hybrid LM). Notably, the
perplexities of the two LMs correlate well with
each other, but the hybrid?s PP is much more sta-
ble across talks: its standard deviation is only 14
?
??
??
??
??
??
??
??
??
?????
???
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ??
?????? ?????? ??????? ?????
Figure 2: Talk-level evaluation on Arabic-English
(IWSLT-tst2010). Left y-axis: BLEU impact of a .25-
POS/lemma hybrid LM. Right y-axis: perplexities by
word LM and by hybrid LM.
points, while that of the word-based PP is 79. The
BLEU improvement given by hybrid LM, how-
ever modest, is consistent across the talks, with
only two outliers: a drop of -0.2 on talk ?00?, and
a drop of -0.7 on talk ?02?. The largest gain (+1.1)
is observed on talk ?10?, from 16.8 to 17.9 BLEU.
6 Conclusions
We have proposed a language modeling technique
that leverages the in-domain data for SMT style
adaptation. Trained to predict mixed sequences
of POS classes and frequent words, hybrid LMs
are devised to capture typical lexical and syntactic
constructions that characterize the style of speech
transcripts.
Compared to standard language models, hy-
brid LMs generalize better to the test data and
partially compensate for the disproportion be-
tween in-domain and out-of-domain training data.
At the same time, hybrid LMs show more dis-
criminative power than merely POS-level LMs.
The integration of hybrid LMs into a competi-
tive phrase-based SMT system is straightforward
and leads to consistent improvements on the TED
task, according to three different translation qual-
ity metrics.
Target language modeling is only one aspect
of the statistical translation problem. Now that
the usability of the proposed method has been as-
sessed for language modeling, future work will
address the extension of the idea to the modeling
of phrase translation and reordering.
Acknowledgments
This work was supported by the T4ME network
of excellence (IST-249119), funded by the DG
INFSO of the European Commission through the
7th Framework Programme. We thank the anony-
mous reviewers for their valuable suggestions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
446
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42(1):93 ? 108.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
9?16, Prague, Czech Republic, June. Association
for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Fed-
erico. 2011. Fill-up versus Interpolation Meth-
ods for Phrase-based SMT Adaptation. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. In MT Summit XIII: the Thir-
teenth Machine Translation Summit, pages 32?39,
Xiamen, China.
Jonathan Clark, Chris Dyer, Alon Lavie, and
Noah Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling
for optimizer instability. In Proceedings of
the Association for Computational Lingustics,
ACL 2011, Portland, Oregon, USA. Associa-
tion for Computational Linguistics. available at
http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic Tagging of Arabic Text: From
Raw Text to Base Phrase Chunks. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 149?152,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, pages 1618?1621, Mel-
bourne, Australia.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian Stu?ker. 2011. Overview of the
IWSLT 2011 Evaluation Campaign. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic, June.
Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,
Marcello Federico, and Mathur Prashant. 2011.
The Uppsala-FBK systems at WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 372?378, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 125?128, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. of the International Workshop on Spoken
Language Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 869?879, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Preslav Nakov. 2008. Improving English-Spanish
Statistical Machine Translation: Experiments in
Domain Adaptation, Sentence Paraphrasing, Tok-
enization, and Recasing. . In Workshop on Statis-
tical Machine Translation, Association for Compu-
tational Linguistics.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the 9th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 71?76.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the
447
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL), pages
311?318, Philadelphia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance
testing for MT. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summariza-
tion, pages 57?64, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: where do we go from here? Pro-
ceedings of the IEEE, 88(8):1270 ?1278.
Nick Ruiz and Marcello Federico. 2011. Topic adap-
tation for lecture translation through bilingual la-
tent semantic models. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
294?302, Edinburgh, Scotland, July. Association
for Computational Linguistics.
Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele
Falavigna, Diego Giuliani, Suhel Jaber, Roberto
Gretter, and Marcello Federico. 2011. FBK @
IWSLT 2011. In International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL).
A. Yazgan and M. Sarac?lar. 2004. Hybrid language
models for out of vocabulary word detection in large
vocabulary conversational speech recognition. In
Proceedings of ICASSP, volume 1, pages I ? 745?8
vol.1, may.
448
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478?487,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modified Distortion Matrices
for Phrase-Based Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
This paper presents a novel method to suggest
long word reorderings to a phrase-based SMT
decoder. We address language pairs where
long reordering concentrates on few patterns,
and use fuzzy chunk-based rules to predict
likely reorderings for these phenomena. Then
we use reordered n-gram LMs to rank the re-
sulting permutations and select the n-best for
translation. Finally we encode these reorder-
ings by modifying selected entries of the dis-
tortion cost matrix, on a per-sentence basis.
In this way, we expand the search space by a
much finer degree than if we simply raised the
distortion limit. The proposed techniques are
tested on Arabic-English and German-English
using well-known SMT benchmarks.
1 Introduction
Despite the large research effort devoted to the mod-
eling of word reordering, this remains one of the
main obstacles to the development of accurate SMT
systems for many language pairs. On one hand, the
phrase-based approach (PSMT) (Och, 2002; Zens et
al., 2002; Koehn et al, 2003), with its shallow and
loose modeling of linguistic equivalences, appears
as the most competitive choice for closely related
language pairs with similar clause structures, both
in terms of quality and of efficiency. On the other,
tree-based approaches (Wu, 1997; Yamada, 2002;
Chiang, 2005) gain advantage, at the cost of higher
complexity and isomorphism assumptions, on lan-
guage pairs with radically different word orders.
Lying between these two extremes are language
pairs where most of the reordering happens locally,
and where long reorderings can be isolated and de-
scribed by a handful of linguistic rules. Notable
examples are the family-unrelated Arabic-English
and the related German-English language pairs. In-
terestingly, on these pairs, PSMT generally pre-
vails over tree-based SMT1, producing overall high-
quality outputs and isolated but critical reordering
errors that undermine the global sentence meaning.
Previous works on this type of language pairs have
mostly focused on source reordering prior to trans-
lation (Xia and McCord, 2004; Collins et al, 2005),
or on sophisticated reordering models integrated into
decoding (Koehn et al, 2005; Al-Onaizan and Pap-
ineni, 2006), achieving mixed results. To merge the
best of both approaches ? namely, access to rich con-
text in the former and natural coupling of reorder-
ing and translation decisions in the latter ? we intro-
ducemodified distortion matrices: a novel method to
seamlessly provide to the decoder a set of likely long
reorderings pre-computed for a given input sentence.
Added to the usual space of local permutations de-
fined by a low distortion limit (DL), this results in a
linguistically informed definition of the search space
that simplifies the task of the in-decoder reordering
model, besides decreasing its complexity.
The paper is organized as follows. After review-
ing a selection of relevant works, we analyze salient
reordering patterns in Arabic-English and German-
English, and describe the corresponding chunk-
based reordering rule sets. In the following sections
we present a reordering selection technique based on
1A good comparison of phrase-based and tree-based ap-
proaches across language pairs with different reordering levels
can be found in (Zollmann et al, 2008).
478
reordered n-gram LMs and, finally, explain the no-
tion of modified distortion matrices. In the last part
of the paper, we evaluate the proposed techniques on
two popular MT tasks.
2 Previous work
Pre-processing approaches to word reordering aim
at permuting input words in a way that minimizes
the reordering needed for translation: determinis-
tic reordering aims at finding a single optimal re-
ordering for each input sentence, which is then
translated monotonically (Xia and McCord, 2004)
or with a low DL (Collins et al, 2005; Habash,
2007); non-deterministic reordering encodes mul-
tiple alternative reorderings into a word lattice and
lets a monotonic decoder find the best path accord-
ing to its models (Zhang et al, 2007; Crego and
Habash, 2008; Elming and Habash, 2009; Niehues
and Kolss, 2009). The latter approaches are ideally
conceived as alternative to in-decoding reordering,
and therefore require an exhaustive reordering rule
set. Two recent works (Bisazza and Federico, 2010;
Andreas et al, 2011) opt instead for a hybrid way:
rules are used to generate multiple likely reorder-
ings, but only for a specific phenomenon ? namely
verb-initial clauses in Arabic. This yields sparse re-
ordering lattices that can be translated with a regular
decoder performing additional reordering.
Reordering rules for pre-processing are either
manually written (Collins et al, 2005) or automat-
ically learned from syntactic parses (Xia and Mc-
Cord, 2004; Habash, 2007; Elming and Habash,
2009), shallow syntax chunks (Zhang et al, 2007;
Crego and Habash, 2008) or part-of-speech labels
(Niehues and Kolss, 2009). Similarly to hybrid ap-
proaches, in this work we use few linguistically in-
formed rules to generate multiple reorderings for se-
lected phenomena but, as a difference, we do not
employ lattices to represent them. We also include a
competitive in-decoding reordering model in all the
systems used to evaluate our methods.
Another large body of work is devoted to the mod-
eling of reordering decisions inside decoding, based
on a decomposition of the problem into a sequence
of basic reordering steps. Existing approaches range
from basic linear distortion to more complex models
that are conditioned on the words being translated.
The linear distortion model (Koehn et al, 2003)
encourages monotonic translations by penalizing
source position jumps proportionally to their length.
If used alone, this model is inadequate for language
pairs with different word orders. Green et al (2010)
tried to improve it with a future distortion cost es-
timate. Thus they were able to preserve baseline
performance at a very high DL, but not to improve
it. Lexicalized phrase orientation models (Tillmann,
2004; Koehn et al, 2005; Zens and Ney, 2006; Gal-
ley and Manning, 2008) predict the orientation of a
phrase with respect to the last translated one. These
models are known to well handle local reordering
and are widely adopted by the PSMT community.
However, they are unsuitable to model long reorder-
ing as they classify as ?discontinuous? every phrase
that does not immediately follow or precede the last
translated one. Lexicalized distortion models pre-
dict the jump from the last translated word to the
next one, with a class for each possible jump length
(Al-Onaizan and Papineni, 2006), or bin of lengths
(Green et al, 2010). These models are conceived to
deal with long reordering, but can easily suffer from
data sparseness, especially for longer jumps occur-
ring less frequently.
Following a typical sequence modeling approach,
Feng et al (2010) train n-gram language models on
source data previously reordered in accordance to
the target language translation. This method does
not directly model reordering decisions, but rather
word sequences produced by them. Despite their
high perplexities, reordered LMs yield some im-
provements when integrated to a PSMT baseline that
already includes a discriminative phrase orientation
model (Zens and Ney, 2006). In this work we use
similar models to rank sets of chunk permutations.
Attempting to improve the reordering space def-
inition, Yahyaei and Monz (2010) train a classifier
to guess the most likely jump length at each source
position, then use its predictions to dynamically set
the DL. Translation improvements are obtained on a
simple task with mostly short sentences (BTEC).
Modifying the distortion function, as proposed in
this paper, makes it possible to expand the pemuta-
tion search space by a much finer degree than vary-
ing the DL does.
479
3 Long reordering patterns
Our study focuses on Arabic-English and German-
English: two language pairs characterized by uneven
distributions of word-reordering phenomena, with
long-range movements concentrating on few pat-
terns. In Arabic-English, the internal order of most
noun phrases needs to be reversed during translation,
which is generally well handled by phrase-internal
reordering or local distortion. At the constituent
level, instead, Arabic admits both SV(O) and VS(O)
orders, the latter causing problematic long reorder-
ings. Common errors due to this issue are the ab-
sence of main verb in the English translation, or the
placement of the main verb before its own subject.
In both cases, adequacy is seriously compromised.
In German-English, the noun phrase structure is
similar between source and target languages. How-
ever, at the constituent level, the verb-second order
of German main clauses conflicts with the rigid SVO
structure of English, as does the clause-final verb
position of German subordinate clauses. As a fur-
ther complication, German compound verbs are split
apart so that the non-finite element (main verb) can
appear long after the inflected auxiliary or modal.
Thanks to sophisticated reordering models, state-
of-the-art PSMT systems are generally good at han-
dling local reordering phenomena that are not cap-
tured by phrase-internal reordering. However, they
typically fail to predict long reorderings. We believe
this is mainly not the fault of the reordering mod-
els, but rather of a too coarse definition of the search
space. To have a concrete idea, consider that a small
change of the DL from 5 to 6 words, in a sentence
of 8, makes the number of explorable permutations
increase from about 9,000 to 22,000. Existing mod-
els cannot be powerful enough to deal with such a
rapidly growing search space.
As a result, decoding at very high DLs is not
a good solution for these language pairs. Indeed,
decent performances are obtained within a low or
mediumDL, but this obviously comes at the expense
of long reorderings, which are often crucial to pre-
serve the general meaning of a translated sentence.
For instance, taking English as the target language,
it is precisely the relative positioning of predicate ar-
guments that determines their role, in the absence of
case markers. Thus, a wrongly reordered verb with
minor impact on automatic scores, can be judged
very badly by a human evaluator.
We will now describe two rule sets aimed at cap-
turing these reordering phenomena.
4 Shallow syntax reordering rules
To compute the source reorderings, we use chunk-
based rules following Bisazza and Federico (2010).
Shallow syntax chunking is indeed a lighter and
simpler task compared to full parsing, and it can
be used to constrain the number of reorderings in
a softer way. While rules based on full parses
are generally deterministic, chunk-based rules are
non-deterministic or fuzzy, as they generate sev-
eral permutations for each matching sequence2. Be-
sides defining a unique segmentation of the sen-
tence, chunk annotation provides other useful infor-
mation that can be used by the rules ? namely chunk
type and POS tags3.
For Arabic-English we apply the rules proposed
by Bisazza and Federico (2010) aimed at transform-
ing VS(O) sentences into SV(O). Reorderings are
generated by moving each verb chunk (VC), alone
or with its following chunk, by 1 to 6 chunks to the
right. The maximum movement of each VC is lim-
ited to the position of the next VC, so that neigh-
boring verb-reordering sequences may not overlap.
This rule set was shown to cover most (99.5%) of
the verb reorderings observed in a parallel news cor-
pus, including those where the verb must be moved
along with an adverbial or a complement.
For German-English we propose a set of three
rules4 aimed at arranging the German constituents
in SVO order:
? infinitive: move each infinitive VC right after a
preceding punctuation;
? subordinate: if a VC is immediately followed
by a punctuation, place it after a preceding sub-
ordinating conjunction (KOUS) or substitutive
relative pronoun (PRELS);
2Chunk annotation does not identify subject and comple-
ment boundaries, nor the relations among constituents that are
needed to deterministically rearrange a sentence in SVO order.
3We use AMIRA (Diab et al, 2004) to annotate Arabic and
Tree Tagger (Schmid, 1994) to annotate German.
4A similar rule set was previously used to produce chunk
reordering lattices in (Hardmeier et al, 2010).
480
????????????????? ??????????????????????????????????
??????????????????????????????????????????????????????
?????????????????????????????????????????????????
?????????????????????????????????? ????????????????????
? ?????????????????
?????????????????????????????????????????????????????
???? ?????????????????????????????????
????????????????????????????????????????????
?????? ?????? ??????? ????????????????????????
? ??????????????????????????????????????????? ?
?? ? ??? ? ? ? ? ? ?? ? ?? ?
?? ? ?? ?? ? ? ? ?? ?
?? ? ? ?? ?
?? ?? ? ? ?
?? ? ? ?? ? ? ?
?? ?? ? ? ? ? ??? ?
?? ?
?? ?
?? ? ?? ?
?? ?
?? ??? ?
?? ??? ??? ?
(a) Arabic VS(O) clause: five permutations
???? ? ??? ?
???? ?
??? ????? ?? ? ? ? ? ?
? ? ? ? ? ?
? ? ? ? ? ? ??? ?
??? ???? ?
??? ?
? ? ???? ? ? ? ? ? ??? ? ??? ?
??????????????????????????????????????????
?????????????????????????????????????????????
????????????????????????????????????????????????????????
??????????????????????????????
??????????????????? ?? ??? ?? ???????????????????? ?????????? ??????
????????????????????????????????????????????????????????????
(b) German broken verb chunk: three permutations
Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunc-
tion, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation.
? broken verb chunk: join each finite VC (auxil-
iary or modal) with the nearest following non-
finite VC (infinitive or participle). Place the re-
sulting block in any position between the orig-
inal position of the finite verb and that of the
non-finite verb5.
The application of chunk reordering rules is illus-
trated by Fig. 1: in the Arabic sentence (a), the sub-
ject ?dozens of militants? is preceded by the main
verb ?took part? and its argument ?to the march?. The
rules generate 5 permutations for one matching se-
quence (chunks 2 to 5), out of which the 5th is the
best for translation. The German sentence (b) con-
tains a broken VC with the inflected auxiliary ?has?
separated from the past participle ?initiated?. Here,
the rules generate 3 permutations for the chunk se-
quence 2 to 5, corresponding to likely locations of
the merged verb phrase, the 1st being optimal.
By construction, both rule sets generate a limited
number of permutations per matching sequence: in
5To bound the number of reorderings, we use the follow-
ing heuristics. In ?infinitive? at most 3 punctuations preceding
the VC are considered. In ?subordinate? 1 to 3 chunks are left
between the conjunction (or pronoun) and the moved VC to ac-
count for the subject. In ?broken VC? if the distance between the
finite and non-finite verb is more than 10 chunks, only the first
5 and last 5 positions of the verb-to-verb span are considered.
Arabic at most 12 for each VC; in German at most 3
for each infinitive VC and for each VC-punctuation
sequence, at most 10 for each broken VC. Empiri-
cally, this yields on average 22 reorderings per sen-
tence in the NIST-MT Arabic benchmark dev06-NW
and 3 on the WMT German benchmark test08. Ara-
bic rules are indeed more noisy, which is not surpris-
ing as reordering is triggered by any verb chunk.
5 Reordering selection
The number of chunk-based reorderings per sen-
tence varies according to the rule set, to the size of
chunks and to the context. A high degree of fuzzi-
ness can complicate the decoding process, leaving
too much work to the in-decoding reordering model.
A solution to this problem is using an external model
to score the rule-generated reorderings and discard
the less probable ones. In such a way, a further part
of reordering complexity is taken out of decoding.
At this end, instead of using a Support Vector Ma-
chine classifier as was done in (Bisazza et al, 2011),
we apply reordered n-gram models that are lighter-
weight and more suitable for a ranking task.
Differently from Feng et al (2010), we train our
models on partially reordered data and at the level of
chunks. Chunks can be represented simply by their
481
type label (such as VC or NC), but also by a com-
bination of the type and head word, to obtain finer
lexicalized distributions. LMs trained on different
chunk representations can also be applied jointly, by
log-linear combination.
We perform reordering selection as follows:
1. Chunk-based reordering rules are applied de-
terministically to the source side of the parallel
training data, using word alignment to choose
the optimal permutation (?oracle reordering?)6.
2. One or several chunk-level 5-gram LMs are
trained on such reordered data, using different
chunk representation modes.
3. Reordering rules are applied to the test sen-
tences and the resulting sets of rule-matching
sequence permutations are scored by the LMs.
The n-best reorderings of each rule-matching
sequence are selected for translation.
In experiments not reported here, we obtained
accurate rankings by scoring source permutations
with a uniformly weighted combination of two LMs
trained on chunk types and on chunk-type+head-
word, respectively. In particular, 3-best reorderings
of each rule-matching sequence yield reordering re-
calls of 77.2% in Arabic and 89.3% in German.
6 Modified distortion matrices
We present here a novel technique to encode likely
long reorderings of an input sentence, which can be
seamlessly integrated into the PSMT framework.
During decoding, the distance between source po-
sitions is used for two main purposes: (i) generating
a distortion penalty for the current hypothesis and
(ii) determining the set of source positions that can
be covered at the next hypothesis expansion. We can
then tackle the coarseness of both distortion penalty
and reordering constraints, by replacing the distance
function with a function defined ad hoc for each in-
put sentence.
Distortion can be thought of as a matrix assigning
a positive integer to any ordered pair of source posi-
tions (sx, sy). In the linear distortion model this is
6Following Bisazza and Federico (2010), the optimal re-
ordering for a source sentence is the one that minimizes dis-
tortion in the word alignment to a target translation, measured
by number of swaps and sum of distortion costs.
defined as:
DL(sx, sy) = |sy ? sx ? 1|
so that moving to the right by 1 position costs 0 and
by 2 positions costs 1. Moving to the left by 1 posi-
tion costs 2 and by 2 positions costs 3, and so on. At
the level of phrases, distortion is computed between
the last word of the last translated phrase and the
first word of the next phrase. We retain this equa-
tion as the core distortion function for our model.
Then, we modify entries in the matrix such that the
distortion cost is minimized for the decoding paths
pre-computed with the reordering rules.
Given a source sentence and its set of rule-
generated permutations, the linear distortion matrix
is modified as follows:
1. non-monotonic jumps (i.e. ordered pairs
(si, si+1) such that si+1? si "= 1) are extracted
from the permutations;
2. then, for each extracted pair, the corresponding
point in the matrix is assigned the lowest possi-
ble distortion cost, that is 0 if si < si+1 and 2
if si > si+1. We call these points shortcuts.
Although this technique is approximate and can
overgenerate minimal-distortion decoding paths7, it
practically works when the number of encoded per-
mutations per sequence is limited. This makes mod-
ifed distortion matrices particularly suitable to en-
code just those reorderings that are typically missed
by phrase-based decoders (see Sect. 3).
Since in this work we use chunk-based rules, we
also have to convert chunk-to-chunk jumps into
word-to-word shortcuts. We propose two ways to
do this, given an ordered pair of chunks (cx,cy):
mode A?A : create a shortcut from each word of
cx to each word of cy;
mode L?F : create only one shortcut from the last
word of cx to the first of cy.
The former solution admits more chunk-internal per-
mutations with the same minimal distortion cost,
whereas the latter implies that the first word of a re-
ordered chunk is covered first and the last is covered
last.
7In fact, any decoding path that includes a jump marked as
shortcut benefits from the same distortion discount in that point.
482
? ? ? ? ? ? ? ? ? ? ??
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
?? ? ? ? ? ? ? ? ? ?
?? ?? ? ? ? ? ? ? ? ?
??
???????
???
???????
??
???
???????
???
?????
????????
? ?
??
? ?
?? ? ?
?? ?
? ?
?? ? ?
??? ??
?? ??????? ??? ??
????? ??? ??? ??????? ??? ????? ???????
?
??
?
?
???
?
?
??
? ? ?
??
?
?
???
??
????? ?? ??? ???? ??? ?? ??? ???? ?
????? ?? ??? ????? ??? ? ??? ??? ?? ??
Figure 2: Modified distortion matrix (mode A?A) of the
German sentence given in Fig. 1. The chunk reordering
shown on top generates three shortcuts corresponding to
the 0?s and 2?s highlighted in the matrix.
Fig. 2 shows the distortion matrix of the German
sentence of Fig. 1, with starting positions as columns
and landing positions as rows. Suppose we want to
encode the reordering shown on top of Fig. 2, cor-
responding to the merging of the broken VC ?hat ...
eingeleitet?. This permutation contains three jumps:
(2,5), (5,3) and (4,6). Converted to word-level in
A?A mode, these yield five word shortcuts8: one
for the onward jump (2,5) assigned 0 distortion; two
for the backward jump (5,3), assigned 2; and two for
the onward jump (4,6), also assigned 0. The desired
reordering is now attainable within a DL of 2 words
instead of 5. The same process is then applied to
other permutations of the sentence.
If compared to the word reordering lattices used
by Bisazza and Federico (2010) and Andreas et al
(2011), modified distortion matrices provide a more
compact, implicit way to encode likely reorderings
in a sentence-specific fashion. Matrix representation
does not require multiplication of nodes for the same
8In L?F mode, instead, each chunk-to-chunk jump would
yield exactly one word shortcut, for a total of three.
source word and is naturally compatible with the
PSMT decoder?s standard reordering mechanisms.
7 Evaluation
In this section we evaluate the impact of modified
distortion matrices on two news translation tasks.
Matrices were integrated into the Moses
toolkit (Koehn et al, 2007) using a sentence-
level XML markup. The list of word shortcuts
for each sentence is provided as an XML tag that
is parsed by the decoder to modify the distortion
matrix just before starting the search. As usual, the
distortion matrix is queried by the distortion penalty
generator and by the hypothesis expander9.
7.1 Experimental setup
For Arabic-English, we use the union of all in-
domain parallel corpora provided for the NIST-MT09
evaluation10 for a total of 986K sentences, 31M En-
glish words. The target LM is trained on the English
side of all available NIST-MT09 parallel data, UN in-
cluded (147M words). For development and test, we
use the newswire sections of the NIST benchmarks,
hereby called dev06-NW, eval08-NW and eval09-
NW: 1033, 813 and 586 sentences, respectively, each
provided with four reference translations.
The German-English system is instead trained
on WMT10 data: namely Europarl (v.5) plus News-
commentary-2010 for a total of 1.6M parallel sen-
tences, 43M English words. The target LM is trained
on the monolingual news data provided for the con-
strained track (1133M words). For development and
test, we use the WMT10 news benchmarks test08,
test09 and test10: 2051, 2525 and 2489 sentences,
respectively, with one reference translation.
Concerning pre-processing, we apply standard to-
kenization to the English data, while for Arabic we
use our in-house tokenizer that removes diacritics
and normalizes special characters. Arabic text is
then segmented with AMIRA (Diab et al, 2004) ac-
cording to the ATB scheme11. German tokenization
9Note that lexicalized reordering models use real word dis-
tances to compute the orientation class of a new hypothesis, thus
they are not affected by changes in the matrix.
10That is everything except the small GALE corpus and the
UN corpus. As reported by Green et al (2010) the removal of
UN data does not affect baseline performances on news test.
11The Arabic Treebank tokenization scheme isolates con-
483
and compound splitting is performed with Tree Tag-
ger (Schmid, 1994) and the Gertwol morphological
analyser (Koskenniemi and Haapalainen, 1994)12.
Using Moses we build competitive baselines on
the training data described above. Word alignment
is produced by the Berkeley Aligner (Liang et al,
2006). The decoder is based on the log-linear com-
bination of a phrase translation model, a lexicalized
reordering model, a 6-gram target language model,
distortion cost, word and phrase penalties. The re-
ordering model is a hierarchical phrase orientation
model (Tillmann, 2004; Koehn et al, 2005; Galley
and Manning, 2008) trained on all the available par-
allel data. We choose the hierarchical variant, as it
was shown by its authors to outperform the default
word-based on an Arabic-English task. Finally, for
German, we enable the Moses option monotone-at-
punctuation which forbids reordering across punc-
tuation marks. The DL is initially set to 5 words
for Arabic-English and to 10 for German-English.
According to our experience, these are the optimal
settings for the evaluated tasks. Feature weights are
optimized by minimum error training (Och, 2003)
on the development sets (dev06-NW and test08).
7.2 Translation quality and efficiency results
We evaluate translations with BLEU (Papineni et al,
2002) and METEOR (Banerjee and Lavie, 2005).
As these scores are only indirectly sensitive to word
order, we also compute KRS or Kendall Reorder-
ing Score (Birch et al, 2010; Bisazza et al, 2011)
which is a positive score based on the Kendall?s
Tau distance between the source-output and source-
reference permutations. To isolate the impact of our
techniques on problematic reordering, we extract
from each test set the sentences that got permuted
by ?oracle reordering? (see Sect. 5). These consti-
tute about a half of the Arabic sentences, and about
a third of the German. We refer to the KRS com-
puted on these test subsets as KRS(R). Statistically
significant differences are assessed by approximate
randomization as in (Riezler and Maxwell, 2005)13.
Tab. 1 reports results obtained by varying the DL
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
12http://www2.lingsoft.fi/cgi-bin/gertwol
13Translation scores and significance tests are computed with
the tools multeval (Clark et al, 2011) and sigf (Pado?, 2006).
and modifying the distortion function. To evalu-
ate the reordering selection technique, we also com-
pare the encoding of all rule-generated reorderings
against only the 3 best per rule-matching sequence,
as ranked by our best performing reordered LM (see
end of Sect. 5). We mark the DL with a ?+? to denote
that some longer jumps are being allowed by modi-
fied distortion. Run times refer to the translation of
the first 100 sentences of eval08-NW and test09 by
a 4-core processor.
Arabic-English. As anticipated, raising the DL
does not improve, but rather worsen performances.
The decrease in BLEU and METEOR reported with
DL=8 is not significant, but the decrease in KRS is
both significant and large. Efficiency is heavily af-
fected, with a 42% increase of the run time.
Results in the row ?allReo? are obtained by encod-
ing all the rule-generated reorderings inL?F chunk-
to-word conversion mode. Except for some gains in
KRS reported on eval08-NW, most of the scores are
lower or equal to the baseline. Such inconsistent be-
haviour is probably due to the low precision of the
Arabic rule set, pointed out in Sect. 4.
Finally, we arrive to the performance of 3-best re-
orderings per sequence. With L?F we obtain sev-
eral improvements, but it?s with A?A that we are
able to beat the baseline according to all metrics.
BLEU andMETEOR improvements are rather small
but significant and consistent across test sets, the
best gain being reported on eval09-NW (+.9 BLEU).
Most importantly, substantial word order improve-
ments are achieved on both full test sets (+.7/+.6
KRS) and selected subsets (+.7/+.6 KRS(R)). Ac-
cording to these figures, word order is affected only
in the sentences that contain problematic reordering.
This is good evidence, suggesting that the decoder
does not get ?confused? by spurious shortcuts.
Looking at run times, we can say that modified
distortion matrices are a very efficient way to ad-
dress long reordering. Even when all the generated
reorderings are encoded, translation time increases
only by 4%. Reordering selection naturally helps to
further reduce decoding overload. As for conversion
modes, A?A yields slightly higher run times than
L?F because it generates more shortcuts.
German-English. In this task we manage to im-
prove translation quality with a setting that is almost
484
(a) Arabic to English
eval08-nw eval09-nw runtime
Distortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)
? plain [baseline] 5 44.5 34.9 81.6 82.9 49.9 38.0 84.1 84.4 1038
plain 8 44.2? 34.8 80.7? 82.2? 49.8 37.9 83.3? 83.5? 1470
? modified: allReo, L?F 5+ 44.4 34.9 82.2? 83.7? 49.9 37.8? 84.3 84.4 1078
modified: 3bestReo, L?F 5+ 44.5 35.1? 82.3? 83.5? 50.7? 38.1 84.8? 85.0? 1052
? modified: 3bestReo, A?A 5+ 44.8? 35.1? 82.3? 83.6? 50.8? 38.2? 84.7? 85.0? 1072
(b) German to English
test09 test10 runtime
Distortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)
? plain [baseline] 10 18.8 27.5 65.8 66.7 20.1 29.4 68.7 68.9 629
plain 20 18.4? 27.4? 63.6? 65.2 ? 19.8? 29.3? 66.3? 66.6? 792
plain 4 18.4? 27.4? 67.3? 66.9 19.6? 29.1? 70.2? 69.6? 345
? modified: allReo, L?F 4+ 19.1? 27.6? 67.6? 68.1? 20.4? 29.4 70.6? 70.7? 352
modified: 3bestReo, L?F 4+ 19.2? 27.7? 67.4? 68.1? 20.4? 29.4 70.4? 70.6? 351
? modified: 3bestReo, A?A 4+ 19.2? 27.7? 67.4? 68.4? 20.6? 29.5? 70.4? 70.7? 357
Table 1: Impact of modified distortion matrices on translation quality, measured with BLEU, METEOR and KRS
(all in percentage form, higher scores mean higher quality). The settings used for weight tuning are marked with ?.
Statistically significant differences wrt the baseline are marked with ? at the p ? .05 level and ? at the p ? .10 level.
twice as fast as the baseline. As shown by the first
part of the table, the best baseline results are ob-
tained with a rather high DL, that is 10 (only KRS
improves with a lower DL). However, with modified
distortion, the best results according to all metrics
are obtained with a DL of 4.
Looking at the rest of the table, we see that re-
ordering selection is not as crucial as in Arabic-
English. This is in line with the properties of the
more precise German reordering rule set (two rules
out of three generate at most 3 reorderings per se-
quence). Considering all scores, the last setting
(3-best reordering and A?A) appears as the best
one, achieving the following gains over the base-
line: +.4/+.5 BLEU, +.2/+.1 METEOR, +1.6/+1.7
KRS and +1.7/+1.8 KRS(R). The agreement ob-
served among such diverse metrics makes us con-
fident about the goodness of the approach.
8 Conclusions
In Arabic-English and German-English, long re-
ordering concentrates on specific patterns describ-
able by a small number of linguistic rules. By
means of non-deterministic chunk reordering rules,
we have generated likely permutations of the test
sentences and ranked them with n-gram LMs trained
on pre-reordered data. We have then introduced the
notion of modified distortion matrices to naturally
encode a set of likely reorderings in the decoder
input. Modified distortion allows for a finer and
more linguistically informed definition of the search
space, which is reflected in better translation outputs
and more efficient decoding.
We expect that further improvements may be
achieved by refining the Arabic reordering rules with
specific POS tags and lexical cues. We also plan
to evaluate modified distortion matrices in conjunc-
tion with a different type of in-decoding reorder-
ing model such as the one proposed by Green et
al. (2010). Finally, we may try to exploit not only
the ranking, but also the scores produced by the re-
ordered LMs, as an additional decoding feature.
Acknowledgments
This work was supported by the T4ME network of
excellence (IST-249119) funded by the European
Commission DG INFSO through the 7th Framework
Programme. We thank Christian Hardmeier for
helping us define the German reordering rules, and
the anonymous reviewers for valuable suggestions.
485
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?
536, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Jacob Andreas, Nizar Habash, and Owen Rambow. 2011.
Fuzzy syntactic reordering for phrase-based statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 227?
236, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 241?249, Upp-
sala, Sweden, July. Association for Computational Lin-
guistics.
Arianna Bisazza, Daniele Pighin, and Marcello Federico.
2011. Chunk-lattices for verb reordering in Arabic-
English statistical machine translation. Machine Trans-
lation, Published Online.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for sta-
tistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the Asso-
ciation for Computational Lingustics, ACL 2011,
Portland, Oregon, USA. Association for Com-
putational Linguistics. accepted; available at
http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment and
reordering for smt. In StatMT ?08: Proceedings of
the Third Workshop on Statistical Machine Translation,
pages 53?61, Morristown, NJ, USA. Association for
Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic Text: From Raw Text to
Base Phrase Chunks. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 149?152, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jakob Elming and Nizar Habash. 2009. Syntactic
reordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statistical
machine translation. In Conference of the Association
for Machine Translation in the Americas (AMTA), Den-
ver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 848?856, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for sta-
tistical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 867?875, Los An-
geles, California. Association for Computational Lin-
guistics.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215?222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lattices
for morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics MATR, pages
88?92, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
486
ings of HLT-NAACL 2003, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings of
the Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Kimmo Koskenniemi and Mariikka Haapalainen, 1994.
GERTWOL ? Lingsoft Oy, chapter 11, pages 121?140.
Roland Hausser, Niemeyer, Tu?bingen.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 206?214, Athens, Greece, March.
Association for Computational Linguistics.
Franz Josef Och. 2002. Statistical Machine Trans-
lation: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 311?318, Philadelphia,
PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL Workshop on In-
trinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 57?64, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Proceed-
ings of the Joint Conference on Human Language Tech-
nologies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics (HLT-NAACL).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for statisti-
cal machine translation. In International Workshop on
Spoken Language Translation (IWSLT), Paris, France.
Kenji Yamada. 2002. A syntax-based translation model.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Southern California, Los Angeles.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Machine
Translation, pages 55?63, New York City, June. Asso-
ciation for Computational Linguistics.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany. Springer Verlag.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sentences
with automatically learned rules for statistical machine
translation. In Proceedings of SSST, NAACL-HLT 2007
/ AMTAWorkshop on Syntax and Structure in Statistical
Translation, pages 1?8, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145?1152, Manchester, UK, August. Coling
2008 Organizing Committee.
487
Dynamically Shaping the Reordering Search Space
of Phrase-Based Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
Defining the reordering search space is a cru-
cial issue in phrase-based SMT between dis-
tant languages. In fact, the optimal trade-
off between accuracy and complexity of de-
coding is nowadays reached by harshly lim-
iting the input permutation space. We pro-
pose a method to dynamically shape such
space and, thus, capture long-range word
movements without hurting translation qual-
ity nor decoding time. The space defined
by loose reordering constraints is dynamically
pruned through a binary classifier that predicts
whether a given input word should be trans-
lated right after another. The integration of
this model into a phrase-based decoder im-
proves a strong Arabic-English baseline al-
ready including state-of-the-art early distor-
tion cost (Moore and Quirk, 2007) and hierar-
chical phrase orientation models (Galley and
Manning, 2008). Significant improvements in
the reordering of verbs are achieved by a sys-
tem that is notably faster than the baseline,
while BLEU and METEOR remain stable, or
even increase, at a very high distortion limit.
1 Introduction
Word order differences are among the most impor-
tant factors determining the performance of statisti-
cal machine translation (SMT) on a given language
pair (Birch et al, 2009). This is particularly true in
the framework of phrase-based SMT (PSMT) (Zens
et al, 2002; Koehn et al, 2003; Och and Ney, 2002),
an approach that remains highly competitive despite
the recent advances of the tree-based approaches.
During the PSMT decoding process, the output
sentence is built from left to right, while the input
sentence positions can be covered in different or-
ders. Thus, reordering in PSMT can be viewed as
the problem of choosing the input permutation that
leads to the highest-scoring output sentence. Due to
efficiency reasons, however, the input permutation
space cannot be fully explored, and is therefore lim-
ited with hard reordering constraints.
Although many solutions have been proposed to
explicitly model word reordering during decoding,
PSMT still largely fails to handle long-range word
movements in language pairs with different syntac-
tic structures1. We believe this is mostly not due to
deficiencies of the existing reordering models, but
rather to a very coarse definition of the reorder-
ing search space. Indeed, the existing reordering
constraints are rather simple and typically based on
word-to-word distances. Moreover, they are uni-
form throughout the input sentence and insensitive
to the actual words being translated. Relaxing this
kind of constraints means dramatically increasing
the size of the search space and making the reorder-
ing model?s task extremely complex. As a result,
even in language pairs where long reordering is reg-
ularly observed, PSMT quality degrades when long
word movements are allowed to the decoder.
We address this problem by training a binary
classifier to predict whether a given input position
should be translated right after another, given the
words at those positions and their contexts. When
this model is integrated into the decoder, its predic-
1For empirical evidence, see for instance (Birch et al, 2009;
Galley and Manning, 2008; Bisazza and Federico, 2012).
327
Transactions of the Association for Computational Linguistics, 1 (2013) 327?340. Action Editor: Philipp Koehn.
Submitted 1/2013; Revised 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
tions can be used not only as an additional feature
function, but also as an early indication of whether
or not a given reordering path should be further ex-
plored. More specifically, at each hypothesis ex-
pansion, we consider the set of input positions that
are reachable according to the usual reordering con-
straints, and prune it based only on the reorder-
ing model score. Then, the hypothesis can be ex-
panded normally by covering the non-pruned posi-
tions. This technique makes it possible to dynami-
cally shape the search space while decoding with a
very high distortion limit, which can improve trans-
lation quality and efficiency at the same time.
The remainder of the paper is organized as fol-
lows. After an overview of the relevant literature,
we describe in detail our word reordering model. In
the following section, we introduce early pruning of
reordering steps as a way to dynamically shape the
input permutation space. Finally, we present an em-
pirical analysis of our approach, including intrinsic
evaluation of the model and SMT experiments on a
well-known Arabic-English news translation task.
2 Previous Work
In this paper, we focus on methods that guide the
reordering search during the phrase-based decoding
process. See for instance (Costa-jussa` and Fonol-
losa, 2009) for a review of pre- and post-reordering
approaches that are not treated here.
Assuming a one-to-one correspondence between
source and target phrases, reordering in PSMT can
be viewed as the problem of searching through a set
of permutations of the input sentence. Thus, two
sub-problems arise: defining the set of allowed per-
mutations (reordering constraints) and scoring the
allowed permutations according to some likelihood
criterion (reordering model). We begin with the lat-
ter, returning to the constraints later in this section.
2.1 Reordering modeling
In its original formulation, the PSMT approach
includes a basic reordering model, called distor-
tion cost, that exponentially penalizes longer jumps
among consecutively translated phrases (f?i?1, f?i):
d(f?i?1, f?i) = e?|start(f?i)? end(f?i?1)? 1|
A number of more sophisticated solutions have
been proposed to explicitly model word reorder-
ing during decoding. These can mostly be grouped
into three families: phrase orientation models, jump
models and source decoding sequence models.
Phrase orientation models (Tillmann, 2004;
Koehn et al, 2005; Zens and Ney, 2006; Galley and
Manning, 2008), also known as lexicalized reorder-
ing models, predict the orientation of a phrase with
respect to the last translated one, by classifying it
as monotone, swap or discontinuous. These mod-
els have proven very useful for short and medium-
range reordering and are among the most widely
used in PSMT. However, their coarse classification
of reordering steps makes them unsuitable to predict
long-range reorderings.
Jump models (Al-Onaizan and Papineni, 2006;
Green et al, 2010; Yahyaei and Monz, 2010) predict
the direction and length of a jump to perform after
a given input word2. Both these works achieve their
best Arabic-English results within a rather small DL:
namely, 8 in (Al-Onaizan and Papineni, 2006) and
5 in (Green et al, 2010), thus failing to capture the
rare but crucial long reorderings that were their main
motivation. A drawback of this approach is that
long jumps are typically penalized because of their
low frequency compared to short jumps. This strong
bias is undesirable, given that we are especially in-
terested in detecting probable long reorderings.
Source decoding sequence models predict which
input word is likely to be translated at a given state
of decoding. For instance, reordered source lan-
guage models (Feng et al, 2010) are smoothed n-
gram models trained on a corpus of source sentences
reordered to match the target word order. When inte-
grated into the SMT system, they assign a probabil-
ity to each newly translated word given the n-1 pre-
viously translated words. Finally, source word pair
reordering models (Visweswariah et al, 2011) esti-
mate, for each pair of input words i and j, the cost
of translating j right after i given various features of
i, j and their respective contexts. Differently from
reordered source LMs, these models are discrimina-
tive and can profit from richer feature sets. At the
same time, they do not employ decoding history-
based features, which allows for more effective hy-
2In this paper, input (or source) word denotes the word at a
given position of the input sentence, rather than a word type.
328
pothesis recombination. The model we are going to
present belongs to this last sub-group, which we find
especially suitable to predict long reorderings.
2.2 Reordering constraints
The reordering constraint originally included in the
PSMT framework and implemented in our reference
toolkit, Moses (Koehn et al, 2007), is called dis-
tortion limit (DL). This consists in allowing the de-
coder to skip, or jump, at most k words from the last
translated phrase to the next one. More precisely, the
limit is imposed on the distortion D between consec-
utively translated phrases (f?i?1, f?i):
D(f?i?1, f?i) =
???start(f?i)? end(f?i?1)? 1
??? ? DL
Limiting the input permutation space is necessary
for beam-search PSMT decoders to function in lin-
ear time. Reordering constraints are also important
for translation quality because the existing models
are typically not discriminative enough to guide the
search over very large sets of reordering hypotheses.
Despite their crucial effects on the complexity of
reordering modeling, though, reordering constraints
have drawn less attention in the literature. The ex-
isting reordering constraints are typically based on
word-to-word distances ? IBM (Berger et al, 1996)
and DL (Koehn et al, 2007) ? or on permutation pat-
terns ? ITG (Wu, 1997). Both kinds of constraints
are uniform throughout the input sentence, and in-
sensitive to the word being translated and to its con-
text. This results in a very coarse definition of the
reordering search space, which is problematic in lan-
guage pairs with different syntactic structures.
To address this problem, Yahyaei and Monz
(2010) present a technique to dynamically set the
DL: they train a classifier to predict the most prob-
able jump length after each input word, and use the
predicted value as the DL after that position. Un-
fortunately, this method can generate inconsistent
constraints leading to decoding dead-ends. As a so-
lution, the dynamic DL is relaxed when needed to
reach the first uncovered position. Translation im-
provements are reported only on a small-scale task
with short sentences (BTEC), over a baseline that in-
cludes a very simple reordering model. In our work
we develop this idea further and use a reordering
model to predict which specific input words, rather
than input intervals, are likely be translated next.
Moreover, our solution is not affected by the con-
straint inconsistency problem (see Sect. 4).
In another related work, Bisazza and Federico
(2012) generate likely reorderings of the input sen-
tence by means of language-specific fuzzy rules
based on shallow syntax. Long jumps are then sug-
gested to the PSMT decoder by reducing the distor-
tion cost for specific pairs of input words. In com-
parison to the dynamic DL, that is a much finer way
to define the reordering space, leading to consistent
improvements of both translation quality and effi-
ciency over a strong baseline. However, the need of
specific reordering rules makes the method harder to
apply to new language pairs.
3 The WaW reordering model
We model reordering as the problem of deciding
whether a given input word should be translated
after another (Word-after-Word). This formulation
is particularly suitable to help the decoder decide
whether a reordering path is promising enough to
be further explored. Moreover, when translating a
sentence, choosing the next source word to translate
appears as a more natural problem than guessing
how much to the left or to the right we should
move from the current source position. The WaW
reordering model addresses a binary decision task
through the following maximum-entropy classifier:
P (Ri,j=Y |fJ1 , i, j) =
exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y )]?
Y ? exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y ?)]
where fJ1 is a source sentence of J words, hm are
feature functions and ?m the corresponding feature
weights. The outcome Y can be either 1 or 0, with
Ri,j=1 meaning that the word at position j is trans-
lated right after the word at position i.
Our WaW reordering model is strongly related to
that of Visweswariah et al (2011) ? hereby called
Travelling Salesman Problem (TSP) model ? with
few important differences: (i) we do not include
in the features any explicit indication of the jump
length, in order to avoid the bias on short jumps;
(ii) they train a linear model with MIRA (Cram-
mer and Singer, 2003) by minimizing the number
329
of input words that get placed after the wrong po-
sition, while we use a maximum-entropy classifier
trained by maximum-likelihood; (iii) they use an
off-the shelf TSP solver to find the best source sen-
tence permutation and apply it as pre-processing to
training and test data. By contrast, we integrate the
maximum-entropy classifier directly into the SMT
decoder and let al its other models (phrase orien-
tation, translation, target LM etc.) contribute to the
final reordering decision.
3.1 Features
Like the TSP model (Visweswariah et al, 2011),
the WaW model builds on binary features similar
to those typically employed for dependency parsing
(McDonald et al, 2005): namely, combinations of
surface forms or POS tags of the words i and j and
their context. Our feature templates are presented in
Table 1. The main novelties with respect to the TSP
model are the mixed word-POS templates (rows 16-
17) and the shallow syntax features. In particular, we
use the chunk types of i, j and their context (18-19),
as well as the chunk head words of i and j (20). Fi-
nally we add a feature to indicate whether the words
i and j belong to the same chunk (21). The jump
orientation ? forward/backward ? is included in the
features that represent the words comprised between
i and j (rows 6, 7, 14, 15). No explicit indication of
the jump length is included in any feature.
3.2 Training data
To generate training data for the classifier, we first
extract reference reorderings from a word-aligned
parallel corpus. Given a parallel sentence, differ-
ent heuristics may be used to convert arbitrary word
alignments to a source permutation (Birch et al,
2010; Feng et al, 2010; Visweswariah et al, 2011).
Similarly to this last work, we compute for each
source word fi the mean ai of the target positions
aligned to fi, then sort the source words according
to this value.3 As a difference, though, we do not
discard unaligned words but assign them the mean
3Using the mean of the aligned indices makes the gener-
ation of reference permutations more robust to alignment er-
rors. Admittedly, this heuristic does not handle well the case of
source words that are correctly aligned to non-consecutive tar-
get words. However, this phenomenon is also not captured by
standard PSMT models, who only learn continuous phrases.
i?2 i?1 i i+1 b j?1 j j+1
1 w w
2 w w w
3 w w w w
4 w w w w
5 w w w w
6 w w w
7 wall w w
8 p p
9 p p p
10 p p p p
11 p p p p
12 p p p p
13 p p p p p p
14 p p p
15 pall p p
16 w p
17 p w
18 c c
19 c c c c c c
20 h h
21 belong to same chunk(i, j)?
w: word identity, p: POS tag, c: chunk type, h: chunk head
Table 1: Feature templates used to learn whether a source
position j is to be translated right after i. Positions com-
prised between i and j are denoted by b and generate two
feature templates: one for each position (6 and 14) and
one for the concatentation of them all (7 and 15).
of their neighbouring words? alignment means, so
that a complete permutation of the source sentence
(?) is obtained. Table 2(a) illustrates this procedure.
Given the reference permutation, we then gener-
ate positive and negative training samples by simu-
lating the decoding process. We traverse the source
positions in the order defined by ?, keeping track of
the positions that have already been covered and, for
each t : 1 ? t ? J , generate:
? one positive sample (R?t,?t+1=1) for the
source position that comes right after it,
? a negative sample (R?t,u=0) for each source
position in {u : ?t??+1 < u < ?t+?+1 ?
u $= ?t+1} that has not yet been translated.
Here, the sampling window ? serves to control the
size of the training data and the proportion between
positive and negative samples. Its value naturally
correlates with the DL used in decoding. The gener-
ation of training samples is illustrated by Table 2(b).
330
(a) Converting word alignments to a permutation:
source words are sorted by their target algnments
mean a. The unaligned word ?D? is assigned the
mean of its neighbouring words? a values (2 +
5)/2 = 3.5 :
(b) Generating binary samples by simulating the
decoding process: shaded rounds represent cov-
ered positions, while dashed arrows represent
negative samples:
Table 2: The classifier?s training data generation process.
3.3 Integration into phrase-based decoding
Rather than using the new reordering model for
data pre-processing as done by (Visweswariah et al,
2011), we directly integrate it into the PSMT de-
coder Moses (Koehn et al, 2007).
Two main computation phases are required by the
WaWmodel: (i) at system initialization time, all fea-
ture weights are loaded into memory, and (ii) before
translating each new sentence, features are extracted
from it and model probabilities are pre-computed
for each pair of source positions (i, j) such that
|j ? i ? 1| ? DL. Note that this efficient solution
is possible because our model does not employ de-
coding history-based features, like the word that was
translated before the last one, or like the previous
jump legth. This is an important difference with re-
spect to the reordered source LM proposed by Feng
et al (2010), which requires inclusion of the last n
translated words in the decoder state.
Fig. 1 illustrates the scoring process: when a par-
tial translation hypothesis H is expanded by cover-
ing a new source phrase f? , the model returns the
log-probability of translating the words of f? in that
particular order, just after the last translated word of
H. In details, this is done by converting the phrase-
internal word alignment4 to a source permutation, in
just the same way it was done to produce the model?s
training examples. Thus, the global score is inde-
pendent from phrase segmentation, and normalized
across outputs of different lengths: that is, the proba-
bility of any complete hypothesis decomposes into J
factors, where J is the length of the input sentence.
The WaW reordering model is fully compatible
with, and complementary to the lexicalized reorder-
ing (phrase orientation) models included in Moses.
Figure 1: Integrating the binary word reordering model
into a phrase-based decoder: when a new phrase is
covered (dashed boxes), the model returns the log-
probability of translating its words in the order defined
by the phrase-internal word alignment.
4 Early pruning of reordering steps
We now explain how the WaW reordering model can
be used to dynamically refine the input permutation
space. This method is not dependent on the particu-
lar classifier described in this paper, but can in prin-
ciple work with any device estimating the probabil-
ity of translating a given input word after another.
The method consists of querying the reordering
model at the time of hypothesis expansion, and fil-
tering out hypotheses solely based on their reorder-
ing score. The rationale is to avoid costly hypoth-
esis expansions for those source positions that the
reordering model considers very unlikely to be cov-
ered at a given point of decoding. In practice, this
works as follows:
? at each hypothesis expansion, we first enumer-
ate the set of uncovered input positions that
are reachable within a fixed DL, and query the
WaW reordering model for each of them5;
4Phrase-internal alignments are provided in the phrase table.
5The score used to prune a new word range f? is the log prob-
ability of translating the first aligned word of f? right after the
last translated word of the current hypothesis. See also Sect. 3.3.
331
? only based on the WaW score, we apply his-
togram and threshold pruning to this set and
proceed to expand the non-pruned positions.
Furthermore, it is possible to ensure that local re-
orderings are always allowed, by setting a so-called
non-prunable-zone of width ? around the last cov-
ered input position.6
According to how the DL, pruning parameters,
and ? are set, we can actually aim at different tar-
gets: with a low DL, loose pruning parameters, and
?=0 we can try to speed up search without sacrific-
ing much translation quality. With a high DL, strict
pruning parameters, and a medium ?, we ensure that
the standard medium-range reordering space is ex-
plored, as well as those few long jumps that are
promising according to the reordering model. In our
experiments, we explore this second option with the
setting DL=18 and ?=5.
The underlying idea is similar to that of early
pruning proposed by Moore and Quirk (2007),
which consisted in discarding possible extensions of
a partial hypothesis based on their estimated score
before computing the exact language model score.
Our technique too has the effect of introducing ad-
ditional points at which the search space is pruned.
However, while theirs was mainly an optimization
technique meant to avoid useless LM queries, we in-
stead aim at refining the search space by exploiting
the fact that some SMT models are more important
than others at different stages of the translation pro-
cess. Our approach actually involves a continuous
alternation of two processes: during hypothesis ex-
pansion the reordering score is combined with all
other scores, while during early pruning some re-
ordering decisions are taken only based on the re-
ordering score. In this way, we try to combine the
benefits of fully integrated reordering models with
those of monolingual pre-ordering methods.
5 Evaluation
We test our approach on an Arabic-English news
translation task where sentences are typically long
and complex. In this language pair, long reorder-
ing errors mostly concern verbs, as all of Subject-
Verb-Object (SVO), VSO and, more rarerly, VOS
6See Bisazza (2013) for technical details on the integration
of word-level pruning with phrase-level hypothesis expansion.
constructions are attested in modern written Ara-
bic. This issue is well known in the SMT field and
was addressed by several recent works, with deep
or shallow parsing-based techniques (Green et al,
2009; Carpuat et al, 2012; Andreas et al, 2011;
Bisazza et al, 2012). We question whether our ap-
proach ? which is not conceived to solve this spe-
cific problem, nor requires manual rules to predict
verb reordering ? will succeed in improving long re-
ordering in a fully data-driven way.
As SMT training data, we use all the in-domain
parallel data provided for the NIST-MT09 evalua-
tion for a total of 986K sentence pairs (31M English
words).7 The target LM used to run the main se-
ries of experiments is trained on the English side of
all available NIST-MT09 parallel data, UN included
(147M words). In the large-scale experiments, the
LM training data also include the sections of the En-
glish Gigaword that best fit to the development data
in terms of perplexity: namely, the Agence France-
Presse, Xinhua News Agency and Associated Press
Worldstream sections (2130M words in total).
For development and test, we use the newswire
sections of the NIST benchmarks: dev06-nw, eval08-
nw, eval09-nw consisting of 1033, 813, 586 sen-
tences respectively. Each set includes 4 reference
translations and the average sentence length is 33
words. To focus the evaluation on problematic re-
ordering, we also consider a subset of eval09-nw
containing only sentences where the Arabic main
verb is placed before the subject (vs-09: 299 sent.).8
As pre-processing, we apply standard tokeniza-
tion to the English data, while the Arabic data is
segmented with AMIRA (Diab et al, 2004) accord-
ing to the ATB scheme9. The same tool also pro-
duces POS tagging and shallow syntax annotation.
7The in-domain parallel data includes all the provided cor-
pora except the UN proceedings, and the non-newswire parts of
the small GALE-Y1-Q4 corpus (that is 9K sentences of audio
transcripts and web data). As reported by Green et al (2010)
the removal of UN data does not affect baseline performances
on the news benchmarks.
8Automatically detected by means of shallow syntax rules.
9The Arabic Treebank tokenization scheme isolates con-
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
332
5.1 Reordering model intrinsic evaluation
Before proceeding to the SMT experiments, we
evaluate the performance of the WaW reorder-
ing model in isolation. All the tested configura-
tions are trained with the freely available MegaM
Toolkit10, implementing the conjugate gradient
method (Hestenes and Stiefel, 1952), in maximum
100 iterations. Training samples are generated
within a sampling window of width ?=10, from a
subset (30K sentences) of the parallel data described
above, resulting in 8M training word pairs11. Test
samples are generated from TIDES-MT04 (1324 sen-
tences, 370K samples with ?=10), one of the corpora
included in our SMT training data. Features with
less than 20 occurrences are ignored.
Classification accuracy. Table 3 presents preci-
sion, recall, and F-score achieved by different fea-
ture subsets, where W stands for word-based, P for
POS-based and C for chunk-based feature templates.
We can see that all feature types contribute to im-
prove the classifier?s performance. The word-based
model achieves the highest precision but a very low
recall, while the POS-based has much more bal-
anced scores. A better performance overall is ob-
tained by combining word-, POS- and mixed word-
POS-based features (62.6% F-score). Finally, the
addition of chunk-based features yields a further im-
provement of about 1 point, reaching 63.8% F-score.
Given these results, we decide to use the W,P,C
model for the rest of the evaluation.
Features (templates) P R F
W [1-7] 73.1 16.4 26.8
P [8-15] 69.5 54.8 61.3
W,P [1-17] 70.2 56.5 62.6
W,P,C [1-21] 70.6 58.1 63.8
Table 3: Classification accuracy of the WaW reordering
model on TIDES-MT04, using different feature subsets.
The template numbers refer to the rows of Table 1.
Ranking accuracy. A more important aspect to
evaluate for our application is how well our model?s
scores can rank a typical set of reordering options.
In fact, the WaW model is not meant to be used as
10http://www.cs.utah.edu/?hal/megam/ (Daume? III, 2004).
11This is the maximum number of samples manageable by
MegaM. However, even scaling from 4M to 8M was only
slightly helpful in our experiments. In the future we plan to test
other learning approaches that scale better to large data sets.
a stand-alone classifier, but as one of several SMT
feature functions. Moreover, for early reordering
pruning to be effective, it is especially important that
the correct reordering option be ranked in the top n
among those available at the time of a given hypoth-
esis expansion. In order to measure this, we simulate
the decoding process by traversing the source words
in target order and, for each of them, we examine
the ranking of all words that may be translated next
(i. e. the uncovered positions within a given DL).
We check how often the correct jump was ranked
first (Top-1) or at most third (Top-3). We also com-
pute the latter score on long reorderings only (Top-
3-long): i. e. backward jumps with distortion D>7
and forward jumps with D>6. In Table 4, results
are compared with the ranking produced by standard
distortion, which always favors shorter jumps. Two
conditions are considered: DL=10 corresponding to
the sampling window ? used to produce the training
data, and DL=18 that is the maximum distortion of
jumps that will be considered in our early-pruning
SMT experiment.
Model DL DL-err Top-1 Top-3 Top-3-longback forw.
Distortion 10 2.4 61.8 79.6 50.7 66.018 0.8 62.0 80.0 18.9 52.3
WaW 10 2.4 71.2 91.2 76.4 69.318 0.8 71.2 91.8 68.0 51.8
Table 4: Word-to-word jump ranking accuracy (%) of
standard distortion and WaW reordering model, in dif-
ferent DL conditions. DL-err is the percentage of correct
jumps beyond DL. The test set consists of 40K reordering
decisions: one for each source word in TIDES-MT04.
We can see that, in terms of overall accuracies, the
WaW reordering model outperforms standard distor-
tion by a large margin (about 10% absolute). This
is an important result, considering that the jump
length, strongly correlating with the jump likeli-
hood, is not directly known to our model. As re-
gards the DL, the higher limit naturally results in a
lower DL-error rate (percentage of correct jumps be-
yond DL): namely 0.8% instead of 2.4%. However,
jump prediction becomes much harder: Top-3 accu-
racy of long jumps by distortion drops from 50.7%
to 18.9% (backward) and from 66.0% to 52.3% (for-
ward). Our model is remarkably robust to this effect
on backward jumps, where it achieves 68.0% accu-
333
racy. Due to the syntactic characteristics of Arabic
and English, the typical long reordering pattern con-
sists in (i) skipping a clause-initial Arabic verb, (ii)
covering a long subject, then finally (iii) jumping
back to translate the verb and (iv) jumping forward
to continue translating the rest of the sentence (see
Fig. 3 for an example).12 Deciding when to jump
back to cover the verb (iii) is the hardest part of
this process, and that is precisely where our model
seems more helpful, while distortion always prefers
to proceed monotonically achieving a very low ac-
curacy of 18.9%. In the case of long forward jumps
(iv), instead, distortion is advantaged as the correct
choice typically corresponds to translating the first
uncovered position, that is the shortest jump avail-
able from the last translated word. Even here, our
model achieves an accuracy of 51.8%, only slightly
lower than that of distortion (52.3%).
In summary, the WaW reordering model signifi-
cantly outperforms distortion in the ranking of long
jumps. In the large majority of cases, it is able to
rank a correct long jump in the top 3 reordering op-
tions, which suggests that it can be effectively used
for early reordering pruning.
5.2 SMT experimental setup
Our SMT systems are built with the Moses toolkit,
while word alignment is produced by the Berke-
ley Aligner (Liang et al, 2006). The baseline de-
coder includes a phrase translation model, a lexi-
calized reordering model, a 6-gram target language
model, distortion cost, word and phrase penalties.
More specifically, the baseline reordering model is a
hierarchical phrase orientation model (Tillmann,
2004; Koehn et al, 2005; Galley and Manning,
2008) trained on all the available parallel data. This
variant was shown to outperform the default word-
based on an Arabic-English task. To make our base-
line even more competitive, we apply early distor-
tion cost, as proposed by Moore and Quirk (2007).
This function has the same value as the standard one
over a complete translation hypothesis, but it antic-
ipates the gradual accumulation of the cost, mak-
ing hypotheses of the same length more compara-
ble to one another. Note that this option has no ef-
12Clearly, we would expect different figures from testing the
model on another language pair like German-English, where the
verb is often postponed in the source with respect to the target.
fect on the distortion limit, but only on the distor-
tion cost feature function. As proposed by Johnson
et al (2007), statistically improbable phrase pairs
are removed from the translation model. The lan-
guage models are estimated by the IRSTLM toolkit
(Federico et al, 2008) with modified Kneser-Ney
smoothing (Chen and Goodman, 1999).
Feature weights are optimized by minimum
BLEU-error training (Och, 2003) on dev06-nw. To
reduce the effects of the optimizer instability, we
tune each configuration four times and use the av-
erage of the resulting weight vectors to translate the
test sets, as suggested by Cettolo et al (2011).
Finally, eval08-nw is used to select the early prun-
ing parameters for the last experiment, while eval09-
nw is always reserved as blind test.
5.3 Evaluation metrics
We evaluate global translation quality with BLEU
(Papineni et al, 2002) and METEOR (Banerjee and
Lavie, 2005). These metrics, though, are only in-
directly sensitive to word order, and especially un-
likely to capture improvements at the level of long-
range reordering. For this reason, we also com-
pute the Kendall Reordering Score or KRS (Birch
et al, 2010) which is a positive score based on the
Kendall?s Tau distance between the source-output
permutation pi and the source-reference permuta-
tions ?:
KRS(pi,?) = (1?
?
K(pi,?)) ? BP
K(pi,?) =
?n
i=1
?n
j=1 d(i, j)
1
2n(n? 1)
d(i, j) =
{
1 if pii < pij and ?i > ?j
0 otherwise
where BP is a sentence-level brevity penalty, similar
to that of BLEU. The KRS is robust to lexical choice
because it performs no comparison between output
and reference words, but only between the positions
of their translations. Besides, it was shown to corre-
late strongly with human judgements of fluency.
Our work specifically addresses long-range re-
ordering phenomena in language pairs where these
are quite rare, although crucial for preserving the
source text meaning. Hence, an improvement at this
level may not be detected by the general-purpose
metrics. We then develop a KRS variant that is only
334
sensitive to the positioning of specific input words.
Assuming that each input word fi is assigned a
weight ?i, the formula above is modified as follows:
d?(i, j) =
{
?i+?j if pii < pij and ?i > ?j
0 otherwise
A similar element-weighted version of Kendall Tau
was proposed by Kumar and Vassilvitskii (2010) to
evaluate document rankings in information retrieval.
Because long reordering errors in Arabic-English
mostly affect verbs, we set the weights to 1 for verbs
and 0 for all other words to only capture verb re-
ordering errors, and call the resulting metric KRS-V.
The source-reference word alignments needed to
compute the reordering scores are generated by the
Berkeley Aligner previously trained on the training
data. Source-output word alignments are instead ob-
tained from the decoder?s trace.
5.4 Results and discussion
To motivate the choice of our baseline setup (early
distortion cost and DL=8), we first compare the per-
formance of standard and early distortion costs un-
der various DL conditions.
???
????
???
???
???
???
??????
???
???
???
???
???
???
???
???
??? ??? ??? ??? ???
??
???
???
?????
Figure 2: Standard vs early distortion cost results on
eval08-nw under different distortion limits (DL), using
the medium-size LM. Best scores are on top-right corner.
As shown in Fig. 2, most results are close to each
other in terms of BLEU and KRS, but early distor-
tion consistently outperforms the standard one (sta-
tistically significant). The most striking difference
appears at a very high distortion limit (18), where
standard distortion scores drop by more than 1 BLEU
point and almost 7 KRS points! Early distortion is
much more robust (only -1 KRS when going from
DL=8 to DL=18), which makes our baseline system
especially strong at the level of reordering.
Table 5 presents the results obtained by integrat-
ing the WaW reordering model as an additional
feature function, and by applying early reordering
pruning. The upper part of the table refers to the
medium-scale evaluation, while the lower part refers
to the large-scale evaluation. In each part, statis-
tical significance is computed against the baseline
[B] by approximate randomization as in (Riezler and
Maxwell, 2005). Run times are obtained by an Intel
Xeon X5650 processor on the first 500 sentences of
eval08-nw, and exclude loading time of all models.
Medium-scale evaluation. Integrating the WaW
model as an additional feature function results in
small but consistent improvements in all DL condi-
tions, which shows that this type of model conveys
information that is missing from the state-of-the-art
reordering models. As regards efficiency, the new
model makes decoding time increase by 8%.
Among the DL settings considered, DL=8 is con-
firmed as the optimal one ? with or without WaW
model. Raising the DL to 18 with no special prun-
ing has a negative impact on both translation quality
and efficiency. The effect is especially visible on the
reordering scores: that is, from 84.7 to 83.9 KRS and
from 86.2 to 85.8 KRS-V on eval09-nw. Run times
are almost doubled: from 87 to 164 and from 94 to
178 ms/word, that is a 89% increase.
We then proceed to the last experiment where the
reordering space is dynamically pruned based on
the WaW model score. As explained in Sect. 4, a
non-prunable-zone of width ?=5 is set around the
last covered position. To set the early pruning pa-
rameters, we perform a grid search over the values
(1, 2, 3, 4, 5) for histogram and (0.5, 0.25, 0.1) for
relative threshold, and select the values that achieve
the best BLEU and KRS on eval08-nw, namely 3 (his-
togram) and 0.1 (threshold). The resulting configu-
ration is then re-optimized by MERT on dev06-nw.
This setting implies that, at a given point of decod-
ing where i is the last covered position, a new word
can be translated only if:
? it lies within a DL of 5 from i, or
? it lies within a DL of 18 from i and its WaW
reordering score is among the top 3 and at least
equal to 1/10 of the best score (in linear space).
As shown in Table 5, early pruning achieves the
best results overall: despite the high DL, we report
335
DL Reo.models eval08-nw eval09-nw vs-09 ms/bleu met krs krs-V bleu met krs krs-V krs-V word
Using the medium-size LM (147M English tokens):
5 hier.lexreo, early disto 44.7 35.1
! 83.0! 84.7! 50.3" 38.1 84.6 85.9 84.7 59
+ WaW model 44.8 35.1 83.7 85.4 51.0# 38.3# 85.1# 86.6$ 85.5# 64
8 hier.lexreo, early disto[B] 44.8 35.2 83.4 85.6 50.6 38.1 84.7 86.2 84.8 87+ WaW model 45.0 35.2 83.7$ 85.9 51.1# 38.3# 85.1# 86.8# 85.8# 94
18
hier.lexreo, early disto 44.7 34.9! 82.4! 84.9! 50.3 38.0" 83.9! 85.8" 84.3" 164
+ WaW model 44.8 35.2 82.7! 85.5 51.0$ 38.3# 84.2" 86.2 85.2 178
+ early reo.pruning(?=5) 45.0 35.3 83.7$ 86.3# 50.9 38.3# 84.9 87.0# 86.2# 68
Using the large interpolated LM (2130M English tokens) and double beam-size:
8 hier.lexreo, early disto[B] 46.3 35.0 83.2 85.0 51.6 38.3 84.5 85.8 84.5 2579
18 hier.lexreo, early disto 45.9
" 34.9! 81.7! 84.1! 51.4 38.1! 83.0! 84.6! 83.1! 5462
+WaW+reo.pruning(?=5) 46.3 35.2 83.4 85.7# 52.8# 38.6# 84.6 86.6# 85.5# 1588
Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with
% BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant
differences with respect to the baseline [B] are marked with #! at the p ? .05 level and $" at the p ? .10 level.
Decoding time is measured in milliseconds per input word.
no loss in BLEU, METEOR and KRS, but we actually
see several improvements. In particular, the gains on
the blind test eval09-nw are +0.3 BLEU, +0.2 ME-
TEOR and +0.2 KRS (only METEOR is significant).
While these gains are admittedly small, we recall
that our techniques affect rare and isolated events
which can hardly emerge from the general-purpose
evaluation metrics. Moreover, to our knowledge,
this is the first time that a PSMT system is shown to
maintain a good performance on this language pair
while admitting very long-range reorderings.
Finally and more importantly, the reordering of
verbs improves significantly on both generic tests
and on the VS- sentence subset (vs-09): namely, in
the latter, we achieve a notable gain of 1.4 KRS-V.
Efficiency is also largely improved by our early
reordering pruning technique: decoding time is re-
duced to 68 ms/word, corresponding to a 22%
speed-up over the baseline.
Large-scale evaluation. We also investigate
whether our methods can be useful in a scenario
where efficiency is less important and more data
is available for training. To this end, we build a
very large LM by interpolating the main LM with
three other LMs trained on different Gigaword sec-
tions (see Sect. 5). Moreover, we relax the decoder?s
beam size from the default value of 200 to 400 hy-
potheses, to reduce the risk of search errors and ob-
tain the best possible baseline performance.
By comparing the large-scale with the medium-
scale baseline in Table 5, we note that the addition
of LM data is especially beneficial for BLEU (+1.5
on eval08-nw and +1.0 on eval09-nw), but not as
much for the other metrics, which challenges the
commonly held idea that more data always improves
translation quality.
Here too, relaxing the DL without special pruning
hurts not only efficiency but also translation qual-
ity: all the scores decrease considerably, showing
that even the stronger LM is not sufficient to guide
search through a very large reordering search space.
As for our enhanced system, it achieves simi-
lar gains as in the medium-scale scenario: that is,
BLEU and METEOR are preserved or slightly im-
proved despite the very high DL, while all the re-
ordering scores increase. In particular, we report sta-
tistically significant improvements in the reordering
of verbs, which is where the impact of our method is
expected to concentrate (+0.7, +0.8 and +1.0 KRS-V
on eval08-nw, eval09-nw and vs-09, respectively).
These results confirm the usefulness of our
method not only as an optimization technique, but
also as a way to improve translation quality on top
of a very strong baseline.
336
SRC
! ! ! " #$!%&' ( )*
+, -. /0$%&' &-1! 2 +1 +03* +045(67!8 +9# +77! 5 :65 &-3*;24<5( &-73! 045( &-=>?@A ( 0B* +CD EF(23*
verb subj. obj. compl.
ywASl sfyr Almmlkp AlErbyp AlsEwdyp ldY lbnAn EbdAlEzyz xwjp tHrk -h fy AtjAh ...
continues ambassador Kingdom Arabian Saudi to Lebanon Abdulaziz Khawja move his in direction
REF The Kingdom of Saudi Arabia ?s ambassador to Lebanon Abdulaziz Khawja continues his moves towards ...
BASE continue to Saudi Arabian ambassador to Lebanon , Abdulaziz Khwja its move in the direction of ...
NEW The Kingdom of Saudi Arabia ?s ambassador to Lebanon , Abdulaziz Khwja continue its move in the direction of ...
SRC
;#7*$G'( H( +0&B5 ( )I( E4 J<K 65# +L M#NL &-O01 P5 )*QR#7*<5( S!
&7=@A ( TU*V3W XY. #8; #?7* +,
adv. verb obj. subj. compl.
fymA dEA -hm r}ys Almktb AlsyAsy l- Hrkp HmAs xAld m$El AlY AltzAm AlHyAd
meanwhile called them head bureau political of movement Hamas Khaled Mashal to necessity neutrality
REF Meanwhile, the Head of the Political Bureau of the Hamas movement, Khaled Mashal, called upon them to remain neutral
BASE The called them, head of Hamas? political bureau, Khalid Mashal, to remain neutral
NEW The head of Hamas? political bureau, Khalid Mashal, called on them to remain neutral
Figure 3: Long reordering examples showing improvements over the baseline system (BASE) when the DL is raised to
18 and early pruning based on WaW reordering scores is enabled (NEW).
Long jumps statistics and examples. To better
understand the behavior of the early-pruning system,
we extract phrase-to-phrase jump statistics from the
decoder log file. We find that 132 jumps beyond the
non-prunable zone (D>5) were performed to trans-
late the 586 sentences of eval09-nw; 38 out of these
were longer than 8 and mostly concentrated on the
VS- sentence subset (27 jumps D>8 performed in
vs-09).13 This and the higher reordering scores sug-
gest that long jumps are mainly carried out to cor-
rectly reorder clause-inital verbs over long subjects.
Fig. 3 shows two Arabic sentences taken from
eval09-nw, that were erroneuously reordered by the
baseline system. The system including the WaW
model and early reordering pruning, instead, pro-
duced the correct translation. The first sentence is
a typical example of VSO order with a long subject:
while the baseline system left the verb in its Ara-
bic position, producing an incomprehensible trans-
lation, the new system placed it rightly between the
English subject and object. This reordering involved
two long jumps: one with D=9 backward and one
with D=8 forward.
The second sentence displays another, less com-
mon, Arabic construction: namely VOS, with a per-
sonal pronoun object. In this case, a backward jump
with D=10 and a forward jump with D=8 were nec-
essary to achieve the correct reordering.
13Statistics computed on the medium-LM system.
6 Conclusions
We have trained a discriminative model to predict
likely reordering steps in a way that is complemen-
tary to state-of-the-art PSMT reordering models. We
have effectively integrated it into a PSMT decoder as
additional feature, ensuring that its total score over a
complete translation hypothesis is consistent across
different phrase segmentations. Lastly, we have pro-
posed early reordering pruning as a novel method
to dynamically shape the input reordering space and
capture long-range reordering phenomena that are
often critical when translating between languages
with different syntactic structures.
Evaluated on a popular Arabic-English news
translation task against a strong baseline, our ap-
proach leads to similar or even higher BLEU, ME-
TEOR and KRS scores at a very high distortion limit
(18), which is by itself an important achievement.
At the same time, the reordering of verbs, measured
with a novel version of the KRS, is consistently im-
proved, while decoding gets significantly faster. The
improvements are also confirmed when a very large
LM is used and the decoder?s beam size is dou-
bled, which shows that our method reduces not only
search errors but also model errors even when base-
line models are very strong.
Word reordering is probably the most difficult as-
pect of SMT and an important factor of both its qual-
ity and efficiency. Given its strong interaction with
the other aspects of SMT, it appears natural to solve
337
word reordering during decoding, rather than before
or after it. To date, however, this objective was only
partially achieved. We believe there is a promising
way to go between fully-integrated reordering mod-
els and monolingual pre-ordering methods. This
work has started to explore it.
Aknowledgments
This work was partially funded by the European
Union FP7 grant agreement 287658 (EU-BRIDGE).
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529?536, Sydney, Australia, July.
Jacob Andreas, Nizar Habash, and Owen Rambow. 2011.
Fuzzy syntactic reordering for phrase-based statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 227?
236, Edinburgh, Scotland, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language translation apparatus and method of
using context-based translation models. United States
Patent, No. 5510981, Apr.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In StatMT ?09: Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 197?
205, Morristown, NJ, USA.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478?487, Jeju Is-
land, Korea, July.
Arianna Bisazza, Daniele Pighin, and Marcello Fed-
erico. 2012. Chunk-lattices for verb reordering in
Arabic-English statistical machine translation. Ma-
chine Translation, Special Issue on MT for Arabic,
26(1-2):85?103.
Arianna Bisazza. 2013. Linguistically Motivated Re-
ordering Modeling for Phrase-Based Statistical Ma-
chine Translation. Ph.D. thesis, University of Trento.
http://eprints-phd.biblio.unitn.it/1019/.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2012.
Improved Arabic-to-English statistical machine trans-
lation by reordering post-verbal subjects for word
alignment. Machine Translation, Special Issue on MT
for Arabic, 26(1-2):105?120.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In MT Summit XIII: the Thirteenth Machine
Translation Summit, pages 32?39, Xiamen, China.
Stanley F. Chen and Joshua Goodman. 1999. An empiri-
cal study of smoothing techniques for language model-
ing. Computer Speech and Language, 4(13):359?393.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2009.
State-of-the-art word reordering approaches in statisti-
cal machine translation: A survey. IEICE TRANSAC-
TIONS on Information and Systems, E92-D(11):2179?
2185.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://pub.hal3.name, implementation avail-
able at http://hal3.name/megam.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic Text: From Raw Text to
Base Phrase Chunks. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 149?152, Boston, Massachusetts, USA.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. In Proceedings of In-
terspeech, pages 1618?1621, Melbourne, Australia.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statisti-
cal machine translation. In Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Denver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 848?856, Morristown, NJ, USA.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP subject detection in verb-initial Ara-
bic clauses. In Proceedings of the Third Workshop
338
on Computational Approaches to Arabic Script-based
Languages (CAASL3), Ottawa, Canada.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL), pages 867?875, Los
Angeles, California.
Magnus R. Hestenes and Eduard Stiefel. 1952. Meth-
ods of conjugate gradients for solving linear systems.
Journal of Research of the National Bureau of Stan-
dards, 49(6):409?436.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Im-
proving translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL 07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
Ravi Kumar and Sergei Vassilvitskii. 2010. General-
ized distances between rankings. In Proceedings of
the 19th international conference on World Wide Web,
pages 571?580, New York, NY, USA. ACM.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine transla-
tion. In In Proceedings of MT Summit XI, pages 321?
327, Copenhagen, Denmark.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 295?302, Philadelhpia, PA.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 311?318, Philadel-
phia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Language
Technologies and the Annual Meeting of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for sta-
tistical machine translation. In International Work-
shop on Spoken Language Translation (IWSLT), Paris,
France.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany. Springer Verlag.
339
340
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88?92,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
FBK at WMT 2010: Word Lattices for
Morphological Reduction and Chunk-based Reordering
Christian Hardmeier, Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
{hardmeier,bisazza,federico}@fbk.eu
Abstract
FBK participated in the WMT 2010
Machine Translation shared task with
phrase-based Statistical Machine Transla-
tion systems based on the Moses decoder
for English-German and German-English
translation. Our work concentrates on ex-
ploiting the available language modelling
resources by using linear mixtures of large
6-gram language models and on address-
ing linguistic differences between English
and German with methods based on word
lattices. In particular, we use lattices to in-
tegrate a morphological analyser for Ger-
man into our system, and we present some
initial work on rule-based word reorder-
ing.
1 System overview
The Human Language Technologies group at Fon-
dazione Bruno Kessler (FBK) participated in the
WMT 2010 Machine Translation (MT) evaluation
with systems for English-German and German-
English translation. While the English-German
system we submitted was relatively simple, we
put some more effort into the inverse translation
direction to make better use of the abundance
of language modelling data available for English
and to address the richness of German morphol-
ogy, which makes it hard for a Statistical Machine
Translation (SMT) system to achieve good vocab-
ulary coverage. In the remainder of this section,
an overview of the common features of our sys-
tems will be given. The next two sections provide
a more detailed description of our approaches to
language modelling, morphological preprocessing
and word reordering.
Both of our systems were based on the Moses
decoder (Koehn et al, 2007). They were simi-
lar to the WMT 2010 Moses baseline system. In-
stead of lowercasing the training data and adding
a recasing step, we retained the data in document
case throughout our system, except for the mor-
phologically normalised word forms described in
section 3. Our phrase tables were trained with the
standard Moses training script, then filtered based
on statistical significance according to the method
described by Johnson et al (2007). Finally, we
used Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) based on the BLEU score (Papineni
et al, 2002).
2 Language modelling
At the 2009 NIST MT evaluation, our system ob-
tained good results using a mixture of linearly in-
terpolated language models (LMs) combining data
from different sources. As the training data pro-
vided for the present evaluation campaign again
included a large set of language modelling corpora
from different sources, especially for English as
a target language, we decided to adopt the same
strategy. The partial corpora for English and their
sizes can be found in table 1. Our base mod-
els of the English Gigaword texts were trained
on version 3 of the corpus (LDC2007T07). We
trained separate language models for the new data
from the years 2007 and 2008 included in ver-
sion 4 (LDC2009T13). Apart from the mono-
lingual English data, we also included language
models trained on the English part of the addi-
tional parallel datasets supplied for the French-
English and Czech-English tasks. All the mod-
els were estimated as 6-gram models with Kneser-
Ney smoothing using the IRSTLM language mod-
elling toolkit (Federico et al, 2008).
For technical reasons, we were unable to use all
the language models during decoding. We there-
fore selected a subset of the models with the fol-
lowing data selection procedure:
1. For a linear mixture of the complete set of
24 language models, we estimated a set of
88
Corpus n-grams
Europarl v5 115,702,157
News 1,437,562,740
News commentary 10 10,381,511
Gigaword v3: 6 models 7,990,828,834
Gigaword 2007/08: 6 models 1,418,281,597
109 fr-en 1,190,593,051
UNDOC fr-en 333,120,732
CzEng: 7 models 153,355,518
Total: 24 models 12,649,826,140
Table 1: Language modelling corpora for English
LMs Perplexity
DEV EVAL
2 188.57 181.38
5 163.68 158.99
10 156.43 151.73
15 154.71 144.98
20 154.39 144.91
24 154.42 144.92
Table 2: Perplexities of LM mixtures
optimal interpolation weights to minimise
the perplexity of the mixture model on the
news-test2008 development set.
2. By sorting the mixture coefficients in de-
scending order, we obtained an ordering of
the language models by their importance with
respect to the development set. We created
partial mixtures by selecting the top n mod-
els according to this order and retraining the
mixture weights with the same algorithm.
Computing the perplexities of these partial
mixtures on the news-test2008 (DEV) and
newstest2009 (EVAL) corpora shows that signif-
icant improvements can be obtained up to a mix-
tures size of about 15 elements. As this size still
turned out to be too large to be managed by our
systems, we used a 5-element mixture in our final
submission (see table 3 for details about the mix-
ture and table 4 for the evaluation results of the
submitted systems).
For the English-German system, the only cor-
pora available for the target language were Eu-
roparl v5, News commentary v10 and the mono-
lingual News corpus. Similar experiments showed
that the News corpus was by far the most impor-
tant for the text genre to be translated and that
including language models trained on the other
Weight Language model
0.368023 News
0.188156 109 fr-en
0.174802 Gigaword v3: NYT
0.144465 Gigaword v3: AFP
0.124553 Gigaword v3: APW
Table 3: 5-element LM mixture used for decoding
BLEU-cased BLEU
en-de
primary 15.5 15.8
secondary 15.3 15.6
primary: only News language model
secondary: linear mixture of 3 LMs
de-en
primary 20.9 21.9
secondary 20.3 21.3
primary: morph. reduction, linear mixture of 5 LMs
secondary: reordering, only News LM
Table 4: Evaluation results of submitted systems
corpora could even degrade system performance.
We therefore decided not to use Europarl or News
commentary for language modelling in our pri-
mary submission. However, we submitted a sec-
ondary system using a mixture of language models
based on all three corpora.
3 Morphological reduction and
decompounding of German
Compounding is a highly productive part of Ger-
man noun morphology. Unlike in English, Ger-
man compound nouns are usually spelt as sin-
gle words, which greatly increases the vocabulary.
For a Machine Translation system, this property
of the language causes a high number of out-of-
vocabulary (OOV) words. It is likely that many
compounds in an input text have not been seen in
the training corpus. We addressed this problem by
splitting compounds in the German source text.
Compound splitting was done using the Gert-
wol morphological analyser (Koskenniemi and
Haapalainen, 1996), a linguistically informed sys-
tem based on two-level finite state morphology.
Since Gertwol outputs all possible analyses of a
word form without taking into account the context,
the output has to be disambiguated. For this pur-
pose, we used part-of-speech (POS) tags obtained
from the TreeTagger (Schmid, 1994) along with
a set of POS-based heuristic disambiguation rules
89
provided to us by the Institute of Computational
Linguistics of the University of Zurich.
As a side effect, Gertwol outputs the base forms
of all words that it processes: Nominative singu-
lar of nouns, infinitive of verbs etc. We decided to
combine the tokens analysed by Gertwol, whether
or not they had been decompounded and lower-
cased, in a further attempt to reduce data sparse-
ness, with their original form in a word lattice
(see fig. 1) and to let the decoder make the choice
between the two according to the translations the
phrase table can provide for each.
Our word lattices are similar to those used by
Dyer et al (2008) for handling word segmentation
in Chinese and Arabic. For each word that was
segmented by Gertwol, we provide exactly one al-
ternative edge labelled with the component words
and base forms as identified by Gertwol, after re-
moving linking morphemes. The edge transition
probabilities are used to identify the source of an
edge: their values are e?1 = 0.36788 for edges de-
riving from Gertwol analysis and e0 = 1 for edges
carrying unprocessed words. Tokens whose de-
compounded base form according to Gertwol is
identical to the surface form in the input are rep-
resented by a single edge with transition proba-
bility e?0.5 = 0.606531. These transition proba-
bilities translate into a binary feature with values
?1, ?0.5 and 0 after taking logarithms in the de-
coder. The feature weight is determined by Min-
imum Error-Rate Training (Och, 2003), together
with the weights of the other feature functions
used in the decoder. During system training, the
processed version of the training corpus was con-
catenated with the unprocessed text.
Experiments show that decompounding and
morphological analysis have a significant impact
on the performance of the MT system. After
these steps, the OOV rate of the newstest2009
test set decreases from 5.88% to 3.21%. Us-
ing only the News language model, the BLEU
score of our development system (measured on
the newstest2009 corpus) increases from 18.77
to 19.31. There is an interesting interaction with
the language models. While using a linear mixture
of 15 language models instead of just the News
LM does not improve the performance of the base-
line system (BLEU score 18.78 instead of 18.77),
the BLEU score of the 15-LM system increases to
20.08 when adding morphological reduction. In
the baseline system, the additional language mod-
els did not have a noticeable effect on translation
quality; however, their impact was realised in the
decompounding system.
4 Word reordering
Current SMT systems are based on the assump-
tion that the word order of the source and the tar-
get languages are fundamentally similar. While
the models permit some local reordering, system-
atic differences in word order involving move-
ments of more than a few words pose major prob-
lems. In particular, Statistical Machine Transla-
tion between German and English is notoriously
impacted by the different fundamental word order
in subordinate clauses, where German Subject?
Object?Verb (SOV) order contrasts with English
Subject?Verb?Object (SVO) order.
In our English-German system, we made the
observation that the verb in an SVO subordi-
nate clause following a punctuation mark fre-
quently gets moved before the preceding punctu-
ation. This movement is triggered by the Ger-
man language model, which prefers verbs pre-
ceding punctuation as consistent with SOV or-
der, and it is facilitated by the fact that the dis-
tance from the verb to the end of the preceding
clause is often smaller than the distance to the end
of the current phrase, so moving the verb back-
wards results in a better score from the distance-
based reordering model. This tendency can be
counteracted effectively by enabling the Moses
decoder?s monotone-at-punctuation feature,
which makes sure that words are not reordered
across punctuation marks. The result is a mod-
est gain from 14.28 to 14.38 BLEU points
(newstest2009).
In the German-English system, we applied a
chunk-based technique to produce lattices repre-
senting multiple permutations of the test sentences
in order to enable long-range reorderings of verb
phrases. This approach is similar to the reorder-
ing technique based on part-of-speech tags pre-
sented by Niehues and Kolss (2009), which re-
sults in the addition of a large number of reorder-
ing paths to the lattices. By contrast, we assume
that verb reorderings only occur between shallow
syntax chunks, and not within them. This makes it
possible to limit the number of long-range reorder-
ing options in an effective way.
We used the TreeTagger to perform shallow
syntax chunking of the German text. By man-
90
Figure 1: Word lattice for morphological reduction
Sonst [drohe]VC , dass auch [weitere L?nder]NC [vom Einbruch]PC [betroffen sein w?rden]VC .
Figure 2: Chunk reordering lattice
BLEU
test-09 test-10
Baseline 18.77 20.1
+ chunk-based reordering 18.94 20.3
Morphological reduction 19.31 20.6
+ chunk-based reordering 19.79 21.1
note: only News LM, case-sensitive evaluation
Table 5: Results with morphological reduction and
chunk reordering on newstest 2009/2010
ual inspection of a data sample, we then identi-
fied a few recurrent patterns of long reorderings
involving the verbs. In particular, we focused on
clause-final verbs in German SOV clauses, which
we move to the left in order to approximate the En-
glish SVO word order. For each sentence a chunk-
based lattice is created, which is then expanded
into a word lattice like the one shown in fig. 2. The
lattice representation provides the decoder with up
to three possible reorderings for a particular verb
chunk. It always retains the original word order as
an alternative input.
For technical reasons, we were unable to pre-
pare a system with reordering, morphological re-
duction and all language models in time for the
shared task. Our secondary submission with re-
ordering is therefore not comparable with our best
system, which includes more language models
and morphological reduction. In subsequent ex-
periments, we combined morphological reduction
with chunk-based reordering (table 5). When mor-
phological reduction is used, the reordering ap-
proach yields an improvement of about 0.5 BLEU
percentage points.
5 Conclusions
There are three important features specific to the
FBK systems at WMT 2010: mixtures of large
language models, German morphological reduc-
tion and decompounding and word reordering.
Our approach to using large language models
proved successful at the 2009 NIST MT evalua-
tion. In the present evaluation, its effectiveness
was reduced by a number of technical problems,
which were mostly due to the limitations of disk
access throughput in our parallel computing en-
vironment. We are working on methods to re-
duce and distribute disk accesses to large lan-
guage models, which will be implemented in the
IRSTLM language modelling toolkit (Federico et
al., 2008). By doing so, we hope to overcome the
current limitations and exploit the power of lan-
guage model mixtures more fully.
The Gertwol-based morphological reduction
and decompounding component we used is a
working solution that results in a significant im-
provement in translation quality. It is an alterna-
tive to the popular statistical compound splitting
methods, such as the one by Koehn and Knight
(2003), incorporating a greater amount of linguis-
tic knowledge and offering morphological reduc-
tion even of simplex words to their base form in
addition. It would be interesting to compare the
relative performance of the two approaches sys-
tematically.
Word reordering between German and English
is a complex problem. Encouraged by the success
of chunk-based verb reordering lattices on Arabic-
English (Bisazza and Federico, 2010), we tried to
adapt the same approach to the German-English
language pair. It turned out that there is a larger
variety of long reordering patterns in this case.
Nevertheless, some experiments performed after
91
the official evaluation showed promising results.
We plan to pursue this work in several directions:
Defining a lattice weighting scheme that distin-
guishes between original word order and reorder-
ing paths could help the decoder select the more
promising path through the lattice. Applying sim-
ilar reordering rules to the training corpus would
reduce the mismatch between the training data and
the reordered input sentences. Finally, it would be
useful to explore the impact of different distortion
limits on the decoding of reordering lattices in or-
der to find an optimal trade-off between decoder-
driven short-range and lattice-driven long-range
reordering.
Acknowledgements
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
References
Arianna Bisazza andMarcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Inter-
speech 2008, pages 1618?1621. ISCA.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
EACL, pages 187?193.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for statistical ma-
chine translation. In Annual meeting of the Associa-
tion for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Kimmo Koskenniemi and Mariikka Haapalainen.
1996. GERTWOL ? Lingsoft Oy. In Roland
Hausser, editor, Linguistische Verifikation. Doku-
mentation zur Ersten Morpholympics 1994, chap-
ter 11, pages 121?140. Niemeyer, T?bingen.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 169?176, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia.
ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
92
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 235?243,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Chunk-based Verb Reordering in VSO Sentences for
Arabic-English Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
In Arabic-to-English phrase-based statis-
tical machine translation, a large number
of syntactic disfluencies are due to wrong
long-range reordering of the verb in VSO
sentences, where the verb is anticipated
with respect to the English word order.
In this paper, we propose a chunk-based
reordering technique to automatically de-
tect and displace clause-initial verbs in the
Arabic side of a word-aligned parallel cor-
pus. This method is applied to preprocess
the training data, and to collect statistics
about verb movements. From this anal-
ysis, specific verb reordering lattices are
then built on the test sentences before de-
coding them. The application of our re-
ordering methods on the training and test
sets results in consistent BLEU score im-
provements on the NIST-MT 2009 Arabic-
English benchmark.
1 Introduction
Shortcomings of phrase-based statistical machine
translation (PSMT) with respect to word reorder-
ing have been recently shown on the Arabic-
English pair by Birch et al (2009). An empiri-
cal investigation of the output of a strong baseline
we developed with the Moses toolkit (Koehn et
al., 2007) for the NIST 2009 evaluation, revealed
that an evident cause of syntactic disfluency is the
anticipation of the verb in Arabic Verb-Subject-
Object (VSO) sentences ? a class that is highly
represented in the news genre1.
Fig. 1 shows two examples where the Arabic
main verb phrase comes before the subject. In
such sentences, the subject can be followed by
adjectives, adverbs, coordinations, or appositions
that further increase the distance between the verb
1In fact, Arabic syntax admits both SVO and VSO orders.
and its object. When translating into English ? a
primarily SVO language ? the resulting long verb
reorderings are often missed by the PSMT decoder
either because of pure modeling errors or because
of search errors (Germann et al, 2001): i.e. their
span is longer than the maximum allowed distor-
tion distance, or the correct reordering hypothesis
does not emerge from the explored search space
because of a low score. In the two examples, the
missed verb reorderings result in different transla-
tion errors by the decoder, respectively, the intro-
duction of a subject pronoun before the verb and,
even worse, a verbless sentence.
In Arabic-English machine translation, other
kinds of reordering are of course very frequent: for
instance, adjectival modifiers following their noun
and head-initial genitive constructions (Idafa).
These, however, appear to be mostly local, there-
fore more likely to be modeled through phrase in-
ternal alignments, or to be captured by the reorder-
ing capabilities of the decoder. In general there is a
quite uneven distribution of word-reordering phe-
nomena in Arabic-English, and long-range move-
ments concentrate on few patterns.
Reordering in PSMT is typically performed
by (i) constraining the maximum allowed word
movement and exponentially penalizing long re-
orderings (distortion limit and penalty), and (ii)
through so-called lexicalized orientation models
(Och et al, 2004; Koehn et al, 2007; Galley
and Manning, 2008). While the former is mainly
aimed at reducing the computational complexity
of the decoding algorithm, the latter assigns at
each decoding step a score to the next source
phrase to cover, according to its orientation with
respect to the last translated phrase. In fact, neither
method discriminates among different reordering
distances for a specific word or syntactic class. To
our view, this could be a reason for their inade-
quacy to properly deal with the reordering pecu-
liarities of the Arabic-English language pair. In
235
src: w AstdEt kl mn AlsEwdyp w lybyA w swryASubj sfrA? hAObj fy AldnmArk .
ref: Each of Saudi Arabia , Libya and SyriaSubj recalled their ambassadorsObj from Denmark .
MT: He recalled all from Saudi Arabia , Libya and Syria ambassadors in Denmark .
src: jdd AlEAhl Almgrby Almlk mHmd AlsAdsSubj dEm hObj l m$rwE Alr}ys Alfrnsy
ref: The Moroccan monarch King Mohamed VISubj renewed his supportObj to the project of French President
MT: The Moroccan monarch King Mohamed VI his support to the French President
Figure 1: Examples of problematic SMT outputs due to verb anticipation in the Arabic source.
this work, we introduce a reordering technique
that addresses this limitation.
The remainder of the paper is organized as fol-
lows. In Sect. 2 we describe our verb reordering
technique and in Sect. 3 we present statistics about
verb movement collected through this technique.
We then discuss the results of preliminary MT ex-
periments involving verb reordering of the training
based on these findings (Sect. 4). Afterwards, we
explain our lattice approach to verb reordering in
the test and provide evaluation on a well-known
MT benchmark (Sect. 5). In the last two sections
we review some related work and draw the final
conclusions.
2 Chunk-based Verb Reordering
The goal of our work is to displace Arabic verbs
from their clause-initial position to a position that
minimizes the amount of word reordering needed
to produce a correct translation. In order to re-
strict the set of possible movements of a verb and
to abstract from the usual token-based movement
length measure, we decided to use shallow syn-
tax chunking of the source language. Full syntac-
tic parsing is another option which we have not
tried so far mainly because popular parsers that are
available for Arabic do not mark grammatical re-
lations such as the ones we are interested in.
We assume that Arabic verb reordering only
occurs between shallow syntax chunks, and not
within them. For this purpose we annotated our
Arabic data with the AMIRA chunker by Diab et
al. (2004)2. The resulting chunks are generally
short (1.6 words on average). We then consider
a specific type of reordering by defining a produc-
tion rule of the kind: ?move a chunk of type T
along with its L left neighbours and R right neigh-
bours by a shift of S chunks?. A basic set of rules
2This tool implies morphological segmentation of the
Arabic text. All word statistics in this paper refer to AMIRA-
segmented text.
that displaces the verbal chunk to the right by at
most 10 positions corresponds to the setting:
T=?VP?, L=0, R=0, S=1..10
In order to address cases where the verb is moved
along with its adverbial, we also add a set of rules
that include a one-chunk right context in the move-
ment:
T=?VP?, L=0, R=1, S=1..10
To prevent verb reordering from overlapping
with the scope of the following clause, we always
limit the maximum movement to the position of
the next verb. Thus, for each verb occurrence, the
number of allowed movements for our setting is at
most 2? 10 = 20.
Assuming that a word-aligned translation of the
sentence is available, the best movement, if any,
will be the one that reduces the amount of distor-
tion in the alignment, that is: (i) it reduces the
number of swaps by 1 or more, and (ii) it mini-
mizes the sum of distances between source posi-
tions aligned to consecutive target positions, i.e.
?
i |ai ? (ai?1 + 1)| where ai is the index of the
foreign word aligned to the ith English word. In
case several movements are optimal according to
these two criteria, e.g. because of missing word-
alignment links, only the shortest good movement
is retained.
The proposed reordering method has been ap-
plied to various parallel data sets in order to per-
form a quantitative analysis of verb anticipation,
and to train a PSMT system on more monotonic
alignments.
3 Analysis of Verb Reordering
We applied the above technique to two parallel
corpora3 provided by the organizers of the NIST-
MT09 Evaluation. The first corpus (Gale-NW)
contains human-made alignments. As these re-
fer to non-segmented text, they were adjusted to
3Newswire sections of LDC2006E93 and LDC2009E08,
respectively 4337 and 777 sentence pairs.
236
Figure 2: Percentage of verb reorderings by maxi-
mum shift (0 stands for no movement).
agree with AMIRA-style segmentation. For the
second corpus (Eval08-NW), we filtered out sen-
tences longer than 80 tokens in order to make
word alignment feasible with GIZA++ (Och and
Ney, 2003). We then used the Intersection of
the direct and inverse alignments, as computed by
Moses. The choice of such a high-precision, low-
recall alignment set is supported by the findings of
Habash (2007) on syntactic rule extraction from
parallel corpora.
3.1 The Verb?s Dance
There are 1,955 verb phrases in Gale-NW and
11,833 in Eval08-NW. Respectively 86% and 84%
of these do not need to be moved according to the
alignments. The remaining 14% and 16% are dis-
tributed by movement length as shown in Fig. 2:
most verb reorderings consist in a 1-chunk long
jump to the right (8.3% in Gale-NW and 11.6% in
Eval08-NW). The rest of the distribution is simi-
lar in the two corpora, which indicates a good cor-
respondence between verb reordering observed in
automatic and manual alignments. By increasing
the maximum movement length from 1 to 2, we
can cover an additional 3% of verb reorderings,
and around 1% when passing from 2 to 3. We
recall that the length measured in chunks doesn?t
necessarily correspond to the number of jumped
tokens. These figures are useful to determine an
optimal set of reordering rules. From now on we
will focus on verb movements of at most 6 chunks,
as these account for about 99.5% of the verb oc-
currences.
Figure 3: Distortion reduction in the GALE-NW
corpus: jump occurrences grouped by length range
(in nb. of words).
3.2 Impact on Corpus Global Distortion
We tried to measure the impact of chunk-based
verb reordering on the total word distortion found
in parallel data. For the sake of reliability, this
investigation was carried out on the manually
aligned corpus (Gale-NW) only. Fig. 3 shows the
positive effect of verb reordering on the total dis-
tortion, which is measured as the number of words
that have to be jumped on the source side in or-
der to cover the sentence in the target order (that
is |ai ? (ai?1 + 1)|). Jumps have been grouped
by length and the relative decrease of jumps per
length is shown on top of each double column.
These figures do not prove as we hoped that
verb reordering resolves most of the long range re-
orderings. Thus we manually inspected a sample
of verb-reordered sentences that still contain long
jumps, and found out that many of these were due
to what we could call ?unnecessary? reordering. In
fact, human translations that are free to some ex-
tent, often display a global sentence restructuring
that makes distortion dramatically increase. We
believe this phenomenon introduces noise in our
analysis since these are not reorderings that an MT
system needs to capture to produce an accurate
and fluent translation.
Nevertheless, we can see from the relative de-
crease percentages shown in the plot, that although
short jumps are by far the most frequent, verb
reordering affects especially medium and long
range distortion. More precisely, our selective
reordering technique solves 21.8% of the 5-to-6-
words jumps, 25.9% of the 7-to-9-words jumps
and 24.2% of the 10-to-14-words jumps, against
237
only 9.5% of the 2-words jumps, for example.
Since our primary goal is to improve the handling
of long reorderings, this makes us think that we
are advancing in a promising direction.
4 Preliminary Experiments
In this section we investigate how verb reordering
on the source language can affect translation qual-
ity. We apply verb reordering both on the training
and the test data. However, while the parallel cor-
pus used for training can be reordered by exploit-
ing word alignments, for the test corpus we need
a verb reordering ?prediction model?. For these
preliminary experiments, we assumed that optimal
verb-reordering of the test data is provided by an
oracle that has access to the word alignments with
the reference translations.
4.1 Setup
We trained a Moses-based system on a subset of
the NIST-MT09 Evaluation data4 for a total of
981K sentences, 30M words. We first aligned the
data with GIZA++ and use the resulting Intersec-
tion set to apply the technique explained in Sect. 2.
We then retrained the whole system ? from word
alignment to phrase scoring ? on the reordered
data and evaluated it on two different versions of
Eval08-NW: plain and oracle verb-reordered, ob-
tained by exploiting word alignments with the first
of the four available English references. The first
experiment is meant to measure the impact of the
verb reordering procedure on training only. The
latter will provide an estimate of the maximum im-
provement we can expect from the application to
the test of an optimal verb reordering prediction
technique. Given our experimental setting, one
could argue that our BLEU score is biased because
one of the references was also used to generate the
verb reordering. However, in a series of exper-
iments not reported here, we evaluated the same
systems using only the remaining three references
and observed similar trends as when all four refer-
ences are used.
Feature weights were optimized through MERT
(Och, 2003) on the newswire section of the NIST-
MT06 evaluation set (Dev06-NW), in the origi-
nal version for the baseline system, in the verb-
reordered version for the reordered system.
4LDC2007T08, 2003T07, 2004E72, 2004T17, 2004T18,
2005E46, 2006E25, 2006E44 and LDC2006E39 ? the two
last with first reference only.
Figure 4: BLEU scores of baseline and reordered
system on plain and oracle reordered Eval08-NW.
Fig. 4 shows the results in terms of BLEU score
for (i) the baseline system, (ii) the reordered sys-
tem on a plain version of Eval08-NW and (iii) the
reordered system on the reordered test. The scores
are plotted against the distortion limit (DL) used
in decoding. Because high DL values (8-10) im-
ply a larger search space and because we want to
give Moses the best possible conditions to prop-
erly handle long reordering, we relaxed for these
conditions the default pruning parameter to the
point that led the highest BLEU score5.
4.2 Discussion
The first observation is that the reordered system
always performs better (0.5?0.6 points) than the
baseline on the plain test, despite the mismatch
between training and test ordering. This may be
due to the fact that automatic word alignments
are more accurate when less reordering is present
in the data, although previous work (Lopez and
Resnik, 2006) showed that even large gains in
alignment accuracy seldom lead to better trans-
lation performances. Moreover phrase extraction
may benefit from a distortion reduction, since its
heuristics rely on word order in order to expand
the context of alignment links.
The results on the oracle reordered test are also
interesting: a gain of at least 1.2 point absolute
over the baseline is reported in all tested DL condi-
tions. These improvements are remarkable, keep-
ing in mind that only 31% of the train and 33% of
the test sentences get modified by verb reordering.
5That is, the histogram pruning maximum stack size was
set to 1000 instead of the default 200.
238
Figure 5: Reordering lattices for Arabic VSO sentences: word-based and chunk-based.
Concerning distortion, although long verb
movements are often observed in parallel corpora,
relaxing the DL to high values does not bene-
fit the translation, even with our ?generous? set-
ting (wider beam search). This is probably due to
the fact that, with weakly constrained distortion,
the risk of search errors increases as the reorder-
ing model fails to properly rank an exponentially
growing set of permutations. Therefore many cor-
rect reordering hypotheses receive low scores and
get lost in pruning or recombination.
5 Verb Reordering Lattices
Having assessed the negative impact of VSO sen-
tences on Arabic-English translation performance,
we now propose a method to improve the handling
of this phenomenon at decoding time. As in real
working conditions word alignments of the input
text are not available, we explore a reordering lat-
tice approach.
5.1 Lattice Construction
Firstly conceived to optimally encode multiple
transcription hypothesis produced by a speech rec-
ognizer, word lattices have later been used to rep-
resent various forms of input ambiguity, mainly at
the level of token boundaries (e.g. word segmenta-
tion, morphological decomposition, word decom-
pounding (Dyer et al, 2008)).
A main problem when dealing with permuta-
tions is that the lattice size can grow very quickly
when medium to long reorderings are represented.
We are particularly concerned with this issue be-
cause our decoding will perform additional re-
ordering on the lattice input. Thanks to the re-
strictions we set on our verb movement reordering
rules described in Sect. 2 ? i.e. only reordering be-
tween chunks and no overlap between consecutive
verb chunks movement ? we are able to produce
quite compact word lattices.
Fig. 5 illustrates how a chunk-based reordering
lattice is generated. Suppose we want to translate
the Arabic sentence ?w >kdt mSAdr rsmyp wjwd
rAbT byn AlAEtdA?At?, whose English meaning is
?Official sources confirmed that there was a link
between the attacks?. The Arabic main verb >kdt
(confirmed) is in pre-subject position. If we ap-
plied word-based rather than chunk-based rules to
move the verb to the right, we would produce the
first lattice of the figure, containing 7 paths (the
original plus 6 verb movements). With the chunk-
based rules, we treat instead chunks as units and
get the second lattice. Then, by expanding each
chunk, we obtain the final, less dense lattice, that
compared to the first does not contain 3 (unlikely)
reordering edges.
To be consistent with the reordering applied to
the training data, we use a set of rules that move
each verb phrase alone or with its following chunk
by 1 to 6 chunks to the right. With this settings,
239
Figure 6: Structure of a chunk-based reordering lattice for verb reordering, before word expansion. Edges
in boldface represent the verbal chunk.
our lattice generation algorithm computes a com-
pact lattice (Fig. 6) that introduces at most 5??S
chunk edges for each verb chunk, where ?S is the
permitted movement range (6 in this case).
Before translation, each edge has to be associ-
ated with a weight that the decoder will use as ad-
ditional feature. To differentiate between the orig-
inal word order and verb reordering we assign a
fixed weight of 1 to the edges of the plain path, and
0.25 to the other edges. As future work we will de-
vise more discriminative weighting schemes.
5.2 Evaluation
For the experiments, we relied on the existing
Moses-implementation of non-monotonic decod-
ing for word lattices (Dyer et al, 2008) with
some fixes concerning the computation of reorder-
ing distance. The translation system is the same
as the one presented in Sect. 4, to which we
added an additional feature function evaluating
the lattice weights (weight-i). Instead of rerun-
ning MERT, we directly estimated the additional
feature-function weight over a suitable interval
(0.002 to 0.5), by running the decoder several
times on the development set. The resulting op-
timal weight was 0.05.
Table 1 presents results on three test sets:
Eval08-NW which was used to calibrate the re-
ordering rules, Reo08-NW a specific test set con-
sisting of the 33% of Eval08-NW sentences that
actually require verb reordering, and Eval09-NW
a yet unseen dataset (newswire section of the
NIST-MT09 evaluation set, 586 sentences). Best
results with lattice decoding were obtained with a
distortion limit (DL) of 4, while best performance
of text decoding was obtained with a DL of 6.
As we hoped, translating a verb reordering lat-
tice yields an additional improvement to the re-
ordering of the training corpus: from 43.67%
to 44.04% on Eval08-NW and from 48.53% to
48.96% on Eval09-NW. The gap between the
baseline and the score obtainable by oracle verb
reordering, as estimated in the preliminary exper-
iments on Eval08-NW (44.36%), has been largely
filled.
On the specific test set ? Reo08-NW ? we ob-
serve a performance drop when reordered models
are applied to non-reordered (plain) input: from
46.90% to 46.64%. Hence it seems that the mis-
match between training and test data is signifi-
cantly impacting on the reordering capabilities of
the system with respect to verbs. We speculate
that such negative effect is diluted in the full test
set (Eval08-NW) and compensated by the positive
influence of verb reordering on phrase extraction.
Indeed, when the lattice technique is applied we
get an improvement of about 0.6 point over the
baseline, which is still a fair result, but not as good
as the one obtained on the general test sets.
Finally, our approach led to an overall gain of
0.8 point BLEU over the baseline, on Eval09-NW.
We believe this is a satisfactory result, given the
fairly good starting performance, and given that
the BLEU metric is known not to be very sensi-
tive to word order variations (Callison-Burch et
al., 2006). For the future, we plan to also use spe-
cific evaluation metrics that will allow us to isolate
the impact of our approach on reordering, like the
ones by Birch et al (2010).
System DL eval08nw reo08nw eval09nw
baseline 6 43.10 46.90 48.13
reord. training +
plain input 6 43.67 46.64 48.53
lattice 4 44.04 47.51 48.96
oracle reord. 4 44.36 48.25 na
Table 1: BLEU scores of baseline and reordered
system on plain test and on reordering lattices.
240
6 Related Work
Linguistically motivated word reordering for
Arabic-English has been proposed in several re-
cent works. Habash (2007) extracts syntactic re-
ordering rules from a word-aligned parallel cor-
pus whose Arabic side has been fully parsed. The
rules involve reordering of syntactic constituents
and are applied in a deterministic way (always
the most probable) as preprocessing of training
and test data. The technique achieves consistent
improvements only in very restrictive conditions:
maximum phrase size of 1 and monotonic decod-
ing, thus failing to enhance the existing reorder-
ing capabilities of PSMT. In (Crego and Habash,
2008; Elming and Habash, 2009) possible in-
put permutations are represented through a word
graph, which is then processed by a monotonic
phrase- or n-gram-based decoder. Thus, these ap-
proaches are conceived as alternatives, rather than
integrations, to PSMT reordering. On the contrary,
we focused on a single type of significant long re-
orderings, in order to integrate class-specific re-
ordering methods into a standard PSMT system.
To our knowledge, the work by Niehues and
Kolss (2009) on German-English is the only ex-
ample of a lattice-based reordering approach be-
ing coupled with reordering at decoding time. In
their paper, discontinuous non-deterministic POS-
based rules learned from a word-aligned corpus
are applied to German sentences in the form of
weighted edges in a word lattice. Their phrase-
based decoder admits local reordering within a
fixed window of 2 words, while, in our work, we
performed experiments up to a distortion limit of
10. Another major difference is that we used shal-
low syntax annotation to effectively reduce the
number of possible permutations. A first attempt
to adapt our technique to the German language is
described in Hardmeier et al (2010).
Our work is also tightly related to the prob-
lem of noun-phrase subject detection, recently ad-
dressed by Green et al (2009). In fact, detect-
ing the extension of the subject can be a suffi-
cient condition to guess the optimal reordering of
the verb. In their paper, a discriminative classi-
fier was trained on a rich variety of linguistic fea-
tures to detect the full scope of Arabic NP subjects
in verb-initial clauses. The authors reported an F-
score of 61.3%, showing that the problem is hard
to solve even when more linguistic information is
available. In order to integrate the output of the
classifier into a PSMT decoder, a specific trans-
lation feature was designed to reward hypotheses
in which the subject is translated as a contiguous
block. Unfortunately, no improvement in transla-
tion quality was obtained.
7 Conclusions
Word reordering remains one of the hardest prob-
lems in statistical machine translation. Based on
the intuition that few reordering patterns would
suffice to handle the most significant cases of long
reorderings in Arabic-English, we decided to fo-
cus on the problem of VSO sentences.
Thanks to simple linguistic assumptions on verb
movement, we developed an efficient, low-cost
technique to reorder the training data, on the one
hand, and to better handle verb reordering at de-
coding time, on the other. In particular, translation
is performed on a compact word lattice that repre-
sents likely verb movements. The resulting system
outperforms a strong baseline in terms of BLEU,
and produces globally more readable translations.
However, the problem is not totally solved because
many verb reorderings are still missed, despite
the suggestions provided by the lattice. Different
factors can explain these errors: poor interaction
between lattice and distortion/orientation models
used by the decoder; poor discriminative power of
the target language model with respect to different
reorderings of the source.
As a first step to improvement, we will devise
a discriminative weighting scheme based on the
length of the reorderings represented in the lat-
tice. For the longer term we are working towards
bringing linguistically informed reordering con-
straints inside decoding, as an alternative to the
lattice solution. In addition, we plan to couple
our reordering technique with more informative
language models, including for instance syntac-
tic analysis of the hypothesis under construction.
Finally we would like to compare the proposed
chunk-based technique with one that exploits full
syntactic parsing of the Arabic sentence to further
decrease the reordering possibilities of the verb.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
241
src: w A$Ar AlsnAtwr AlY dEm h m$rwEA ErD ElY mjls Al$ywx
ref: The Senator referred to his support for a project proposed to the Senate
base MT: The Senator to support projects presented to the Senate
new MT: Senator noted his support projects presented to the Senate
src: mn jAnb h hdd >bw mSEb EbdAlwdwd Amyr AlqAEdp b blAd Almgrb AlAslAmy fy nfs Al$ryT b AlqyAm
b slslp AEtdA?At w >EmAl <rhAbyp Dd AlmSAlH w Alm&ssAt AljzA}ryp fy AlEdyd mn AlmnATq
AljzA}ryp
ref: For his part , Abu Musab Abd al-Wadud , the commander of al-Qaeda in the Islamic Maghreb Countries ,
threatened in the same tape to carry out a series of attacks and terrorist actions against Algerian interests and
organizations in many parts of Algeria
base MT: For his part threatened Abu Musab EbdAlwdwd Amir al-Qaeda Islamic Morocco country in the same tape to
carry out a series of attacks and terrorist acts against the interests and the Algerian institutions in many areas of
Algiers
new MT: For his part , Abu Musab EbdAlwdwd Amir al Qaida threatened to Morocco Islamic country in the same tape
to carry out a series of attacks and terrorist acts against the interests of the Algerian and institutions in many
areas of Algiers
src: w ymtd Alm$rwE 500 km mtr w yrbT Almdyntyn Almqdstyn b mdynp jdp ElY sAHl AlbHr Al>Hmr .
ref: The project is 500 kilometers long and connects the two holy cities with the city of Jeddah on the Red Sea coast.
base MT: It extends the project 500 km and linking the two holy cities in the city of Jeddah on the Red Sea coast .
new MT: The project extends 500 km , linking the two holy cities in the city of Jeddah on the Red Sea coast .
Figure 7: Examples showing MT improvements coming from chunk-based verb-reordering.
References
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phe-
nomena. In StatMT ?09: Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
197?205, Morristown, NJ, USA. Association for
Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, Published online.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of BLEU
in machine translation research. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento,
Italy, April.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment
and reordering for smt. In StatMT ?08: Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 53?61, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic Tagging of Arabic Text: From
Raw Text to Base Phrase Chunks. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 149?152,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jakob Elming and Nizar Habash. 2009. Syntactic re-
ordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for machine translation. In
Proceedings of the 39th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
228?335, Toulouse, France.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP subject detection in verb-initial Ara-
bic clauses. In Proceedings of the Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), Ottawa, Canada.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215?222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lat-
tices for morphological reduction and chunk-based
242
reordering. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
rics MATR, Uppsala, Sweden, July. Association for
Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the
link? In 5th Conference of the Association for Ma-
chine Translation in the Americas (AMTA), Boston,
Massachusetts, August.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technologies and the Annual Meet-
ing of the North American Chapter of the Associ-
ation of Computational Linguistics (HLT-NAACL),
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 160?167.
243
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440?451,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Efficient solutions for word reordering in German-English
phrase-based statistical machine translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
Despite being closely related languages,
German and English are characterized by
important word order differences. Long-
range reordering of verbs, in particular,
represents a real challenge for state-of-the-
art SMT systems and is one of the main
reasons why translation quality is often so
poor in this language pair. In this work,
we review several solutions to improve
the accuracy of German-English word re-
ordering while preserving the efficiency of
phrase-based decoding. Among these, we
consider a novel technique to dynamically
shape the reordering search space and
effectively capture long-range reordering
phenomena. Through an extensive eval-
uation including diverse translation qual-
ity metrics, we show that these solutions
can significantly narrow the gap between
phrase-based and hierarchical SMT.
1 Introduction
Modeling the German-English language pair is
known to be a challenging task for state-of-the-
art statistical machine translation (SMT) methods.
A major factor of difficulty is given by word or-
der differences that yield important long-range re-
ordering phenomena.
Thanks to specific reordering modeling compo-
nents, phrase-based SMT (PSMT) systems (Zens
et al, 2002; Koehn et al, 2003; Och and Ney,
2002) are generally good at handling local re-
ordering phenomena that are not captured inside
phrases. However, they typically fail to predict
long reorderings. On the other hand, hierarchi-
cal SMT (HSMT) systems (Chiang, 2005) can
learn reordering patterns by means of discontinu-
ous translation rules, and are therefore considered
a better choice for language pairs characterized by
massive and hierarchical reordering.
Looking at the results of the Workshop of
Machine Translation?s last edition (WMT12)
(Callison-Burch et al, 2012), no particular SMT
approach appears to be clearly dominating. In
both language directions (official results excluding
the online systems) the rule-based systems outper-
formed all SMT approaches, and among the best
SMT systems we find a variety of approaches:
pure phrase-based, phrase-based and hierarchical
systems combination, n-gram based, a rich syntax-
based approach, and a phrase-based system cou-
pled with POS-based pre-ordering. This gives an
idea of how challenging this language pair is for
SMT and raises the question of which SMT ap-
proach is best suited to model it.
In this work, we aim at answering this ques-
tion by focussing on the word reordering problem,
which is known to be an important factor of SMT
performance (Birch et al, 2008). We hypothe-
size that PSMT can be as successful for German-
English as the more computationally costly HSMT
approach, provided that the reordering-related pa-
rameters are carefully chosen and the best avail-
able reordering models are used. More specifi-
cally, our study covers the following topics: dis-
tortion functions and limits, and dynamic shaping
of the reordering search space based on a discrim-
inative reordering model.
We first review these topics, and then evaluate
them systematically on the WMT task using both
generic and reordering-specific metrics, with the
aim of providing a reference for future system de-
velopers? choices.
2 Background
Word order differences between German and En-
glish are mainly found at the clause (global) level,
as opposed to the phrase (local) level. We refer to
Collins et al (2005) and Gojun and Fraser (2012)
for a detailed description of the German clause
structure. To briefly summarize, we can say that
440
the verb-second order of German main clauses
contrasts with the rigid SVO structure of English,
as does the clause-final verb position of German
subordinate clauses. A further difficulty is given
by the German discontinuous verb phrases, where
the main verb is separated from the inflected auxil-
iary or modal. The distance between the two parts
of a verb phrase can be arbitrarily long as shown
in the following example:
[DE] Jedoch konnten sie Kinder in Teilen von Helmand
und Kandahar im Su?den aus Sicherheitsgrund nicht er-
reichen.
[EN] But they could not reach children in parts of Hel-
mand and Kandahar in the south for security reasons.
Translating this sentence with a PSMT engine
implies performing two very long jumps that are
not even considered by typical systems employing
a distortion limit of 6 or 8 words. At the same
time, increasing the distortion limit to very high
values is known to have a negative impact on both
efficiency and translation quality (cf. results pre-
sented later in this paper).
Because reordering patterns of this kind are
very common between German and English, this
paper focuses on techniques that enable the PSMT
decoder to explore long jumps and thus improve
reordering accuracy without hurting efficiency nor
general translation quality.
2.1 Alternative approaches
German-English reordering in SMT has been
widely studied and is still an open topic. In this
work, we only consider efficient solutions that are
fully integrated into the decoding process, and that
do not require syntactic parsers or manual reorder-
ing rules. Still, it has to be mentioned that sev-
eral alternative solutions were proposed in the lit-
erature. A well-known strategy consists of pre-
ordering the German sentence in an English-like
order by applying a set of manually written rules
to its syntactic parse tree (Collins et al, 2005).1
Other approaches learn the pre-ordering rules au-
tomatically, from syntactic parses (Xia and Mc-
Cord, 2004; Genzel, 2010) or from part-of-speech
labels (Niehues and Kolss, 2009). In the former
case, pre-ordering decisions are typically taken de-
terministically (i. e. one permuation per sentence),
whereas in the latter, multiple alternatives are rep-
resented as word lattices, and the optimal path is
1A similar solution for the opposite translation direction
(English-German) was proposed by Gojun and Fraser (2012).
selected by the decoder at translation time. In
(Tromble and Eisner, 2009), pre-ordering is cast
as a permutation problem and solved by a model
that estimates the probability of reversing the rel-
ative order of any two input words.
In the field of tree-based SMT, positive results
in German-English were achieved by combining
syntactic translation rules with unlabeled hierar-
chical SMT rules (Hoang and Koehn, 2010). More
recently, Braune et al (2012) proposed to improve
the long-range reordering capability of an HSMT
system by integrating constraints based on clausal
boundaries and by manually selecting the rule pat-
terns applicable to long word spans. The paper
did not analyse the impact of the technique on ef-
ficiency.
2.2 Evaluation methods
A large number of previous works on word re-
ordering measured their success with general-
purpose metrics such as BLEU (Papineni et al,
2001) or METEOR (Banerjee and Lavie, 2005).
These metrics, however, are only indirectly sensi-
tive to word order and do not sufficiently penalize
long-range reordering errors, as demonstrated for
instance by Birch et al (2010). While BLEU re-
mains a standard choice for many evaluation cam-
paigns, we believe it is extremely important to
complement it with metrics that are specifically
designed to capture word order differences. In this
work, we adopt two reordering-specific metrics in
addition to BLEU and METEOR:
Kendall Reordering Score (KRS). As pro-
posed by Birch et al (2010), the KRS measures
the similarity between the input-output reordering
and the input-reference reordering. This is done by
converting word alignments to permutations and
computing a permutation distance among them.
When interpolated with BLEU, this score is called
LRscore.2
Verb-specific KRS (KRS-V). The ideal way
to automatically evaluate our systems would be
to use syntax- or semantics-based metrics, as the
impact of long reordering errors is particularly
important at these levels. As a light-weight al-
ternative, we instead concentrate the evaluation
on those word classes that are typically crucial
to guess the general structure of a sentence. To
this end, we adopt a word-weighted version of the
2Thus, our KRS results correspond exactly to the
LRscore(?=1) presented in other papers.
441
KRS and set the weights to 1 for verbs and 0 for
all other words, so that only verb reordering errors
are captured. We call the resulting metric KRS-V.
The KRS-V rates a translation hypothesis as per-
fect (100%) when the translations of all source
verbs are located in their correct position, regard-
less of the other words? ordering.
3 Early distortion cost
In its original formulation, the PSMT approach in-
cludes a basic reordering model, called distortion
cost, that exponentially penalizes longer jumps
among consecutively translated phrases simply
based on their distance. Thus, a completely mono-
tonic translation has a total distortion cost of zero.
A weakness of this model is that it penalizes
long jumps only when they are performed, rather
than accumulating their cost gradually. As an ef-
fect, hypotheses with gaps (i. e. uncovered input
positions) can proliferate and cause the pruning
of more monotonic hypotheses that could lead to
overall better translations.
To solve this problem, Moore and Quirk (2007)
proposed an improved version of the distortion
cost function that anticipates the gradual accumu-
lation of the total distortion cost, making hypothe-
ses with the same number of covered words more
comparable with one another. Early distortion
cost (as called in Moses, or ?distortion penalty es-
timation? in the original paper) is computed by a
simple algorithm that keeps track of the uncovered
input positions. Note that this option affects the
distortion feature function, but not the distortion
limit, which always corresponds to the maximum
distance allowed between consecutively translated
phrases.
Early distortion cost was shown by its authors to
yield similar BLEU scores as the standard one but
with stricter pruning parameters, i. e. faster decod-
ing. Experiments were performed on an English-
French task, with a fixed distortion limit of 5 and
without lexicalized reordering models. Our study
deals with a language pair that is arguably more
difficult at the level of reordering. Moreover, we
start from a stronger baseline and measure the im-
pact of early distortion cost in various distortion
limit settings, using also reordering-specific met-
rics. Results are presented in Section 6.2.
4 Word-after-word reordering
modeling and pruning
Phrase orientation (lexicalized reordering) mod-
els (Tillmann, 2004; Koehn et al, 2005; Galley
and Manning, 2008) have proven very useful for
short and medium-range reordering and are prob-
ably the most widely used in PSMT nowadays.
However, their coarse classification of reordering
steps makes them unsuitable to capture long-range
reordering phenomena, such as those attested in
German-English. Indeed, Galley and Manning
(2008) reported a decrease of translation qual-
ity when the distortion limit was set beyond 6 in
Chinese-English and beyond 4 in Arabic-English.
To address this problem, we have developed a
different reordering model that predicts what in-
put word should be translated at a given decod-
ing state (Bisazza, 2013; Bisazza and Federico,
2013). The model is similar to the one proposed
by Visweswariah et al (2011), however we use
it differently: that is, not simply for data pre-
processing but as an additional feature function
fully integrated in the phrase-based decoder. More
importantly, we propose to use the same model
to dynamically shape the space of reorderings ex-
plored during decoding (cf. Section 4.2), which
was never done before.
Another related work is the source-side decod-
ing sequence model by Feng et al (2010), that is
a generative n-gram model trained on a corpus of
pre-ordered source sentences. Although reminis-
cent of a source-side bigram model, our model has
two important differences: (i) the discriminative
modeling framework enables us to design a much
richer feature set including, for instance, the con-
text of the next word to pick; (ii) all our features
are independent from the decoding history, which
allows for an efficient decoder-integration with no
effect on hypothesis recombination.
Finally, we have to mention the models by Al-
Onaizan and Papineni (2006) and Green et al
(2010), who predict the direction and (binned)
length of a jump to perform after a given input
word. Those models too were only used as ad-
ditional feature functions, and were not shown to
maintain translation quality and efficiency at very
high distortion limits.
4.1 The model
The Word-after-word (WaW) reordering model is
trained to predict whether a given input position
442
should be translated right after another, given the
words at those positions and their contexts. It is
based on the following maximum-entropy binary
classifier:
P (Ri,j=Y |fJ1 , i, j) =
exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y )]?
Y ? exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y ?)]
where fJ1 is a source sentence of J words, hm are
feature functions and ?m the corresponding fea-
ture weights. The outcome Y can be either 1 or 0,
with Ri,j=1 meaning that the word at position j is
translated right after the word at position i.
Training examples are extracted from a corpus
of reference reorderings, obtained by converting
the word-aligned parallel data into a set of source
sentence permutations. A heuristic similar to the
one proposed by Visweswariah et al (2011) is
used to this end. For each input word, we gen-
erate: (i) one positive example for the word that
should be translated right after it; (ii) negative ex-
amples for all the uncovered words that lie within a
given sampling window or ?. The latter parameter
serves to control the proportion between positive
and negative examples.
The WaW model builds on binary features that
are extracted from the local context of positions
i and j, and from the words occurring between
them. In addition to the actual words, the features
may include POS tags and shallow syntax labels
(i. e. chunk types and boundaries). For instance,
one feature may indicate that the last translated
word (wi) is an adjective while the currently trans-
lated one (wj) is a noun:
POS(wi)=adj ? POS(wj)=noun
Other features indicate that a given word or punc-
tuation is found between wi and wj :
wb=?jedoch? ... wb=?.?
or that wi and wj belong to the same shallow syn-
tax chunk.
The WaW reordering model can be seamlessy
integrated into a standard phrase-based decoder
that already includes phrase orientation models.
When a partial hypothesis is expanded with a
given phrase pair, the model returns the log-
probability of translating its words in the order
defined by the phrase-internal word alignment.
Moreover, the global WaW score is independent
from phrase segmentation, and normalized across
outputs of different lengths.
The complete list of features, training data gen-
eration algorithm and other implementation details
are presented in (Bisazza, 2013) and (Bisazza and
Federico, 2013).
4.2 Early reordering pruning
Besides providing an additional feature function
for the log-linear PSMT framework, the WaW
model?s predictions can be used as an early indi-
cation of whether or not a given reordering path
should be further explored. In fact, we have men-
tioned that the existing reordering models are not
capable of guiding the search through very large
reordering search spaces. As a solution, we pro-
pose to decode with loose reordering constraints
(i. e. high distortion limit) but only explore those
long reorderings that are promising according to
the WaW model.
More specifically, at each hypothesis expansion,
we consider the set of input positions that are
reachable within the fixed distortion limit. Only
based on the WaW score, we apply histogram and
threshold pruning to this set and then proceed to
expand only the non-pruned positions.3 Further-
more, it is possible to ensure that local reorderings
are always allowed, by setting a so-called non-
prunable-zone of width ? around the last covered
input position.4 In this way, we can ensure that the
usual space of short to medium-range reordering is
exhaustively explored in addition to few promising
long-range reorderings.
The rationale of this approach is two-fold: First,
to avoid costly hypothesis expansions for very un-
likely reordering steps and thus speed up decod-
ing under loose reordering constraints. Second, to
decrease the risk of model errors by exploiting the
fact that some components of the PSMT log-linear
model are more important than others at different
stages of the translation process.
The WaW model is not the only scoring func-
tion that can be used for early reordering prun-
ing. In principle, even phrase orientation model
scores could be used, but we expect them to per-
form poorly due to the coarse classification of re-
ordering steps (all phrases that are not adjacent to
the current one are treated as discontinuous steps).
3The idea is reminiscent of early pruning by Moore and
Quirk (2007): an optimization technique that consists of dis-
carding hypothesis extensions based on their estimated score
before computing the exact language model score.
4See (Bisazza, 2013) for technical details on the integra-
tion of word-level pruning with phrase-level hypothesis ex-
pansion.
443
5 Reordering in hierarchical SMT
To allow for a fair evaluation of our systems,
we also perform a contrastive experiment using a
tree-based SMT approach: namely, hierarchical
phrase-based SMT (HSMT) (Chiang, 2005).
Reordering in HSMT is not modeled separately
but is embedded in the translation model itself,
which contains lexicalized, non syntactically mo-
tivated rules that are directly learnt from word-
aligned parallel text. The major strength of HSMT
compared to PSMT, is the ability to learn discon-
tinous phrases and long-range lexicalized reorder-
ing rules. However, this modeling power has a cost
in terms of model size and decoding complexity.
To have a concrete idea, consider that the
phrase-table trained on our SMT training data (cf.
Section 6.1) with a maximum phrase length of 7
contains 127 million entries (before phrase table
pruning). The hierarchical rule table trained on the
same data with a comparable span constraint (10)
contains instead 1.2 billion entries ? one order of
magnitude larger.
Furthermore, the HSMT decoder is based on a
chart parsing algorithm, whose complexity is cu-
bic in the input length, and even higher when tak-
ing into account the target language model. This
issue can be partially addressed by different strate-
gies such as cube pruning (Chiang, 2007), which
reduces the LM complexity to a constant, or rule
application constraints. One of such constraints is
the maximum number of source words that may
be covered by non-terminal symbols (span con-
straint). Setting a span constraint ? which is essen-
tial to obtain reasonable decoding times ? means
preventing long-range reordering similarly to set-
ting a distortion limit in PSMT. In our experi-
ments, we consider two settings for this parameter:
10 to capture short to medium-range reorderings,
and 20 to also capture long-range reorderings.
6 Experiments
In this section we evaluate the impact on transla-
tion quality and efficiency of the techniques pre-
sented above. Our main objective is to empiri-
cally verify the hypothesis that better reordering
modeling and better reordering space definition
can significantly improve the accuracy of PSMT in
German-English without sacrificing its efficiency.
6.1 Experimental setup
We choose the WMT German-English news trans-
lation task as our case study. More specifically
we use the WMT10 training data: Europarl (v.5)
plus News-commentary-2010 for a total of 1.6M
parallel sentences, 44M German tokens. The tar-
get LM is trained on the monolingual news data
provided for the constrained track of WMT10
(1133M English tokens). For development we use
theWMT08 news benchmark, while for testing we
use the following data sets:
tests(09-11): the concatentation of three previous
years? benchmarks from 2009 to 2011 (8017
sentences, 21K German tokens).
test12: the latest released benchmark (3003 sen-
tences, 8K German tokens).
Each data set includes one reference translation.
Note that our goal is not to reach the performance
of the best systems participating at the last WMT
edition, but rather to assess the usefulness of our
techniques on a larger and therefore more reliable
test set, while starting from a reasonable baseline.5
For German tokenization and compound split-
ting we use Tree Tagger (Schmid, 1994) and the
Gertwol morphological analyser (Koskenniemi
and Haapalainen, 1994).6
All our SMT systems are built with the Moses
toolkit (Koehn et al, 2007; Hoang et al, 2009),
and word alignments are generated by the Berke-
ley Aligner (Liang et al, 2006). The target lan-
guage model is estimated by the IRSTLM toolkit
(Federico et al, 2008) with modified Kneser-Ney
smoothing (Chen and Goodman, 1999).
The phrase-based baseline decoder includes a
phrase translation model (two phrasal and two lex-
ical probability features), a lexicalized reorder-
ing model (six features), a 6-gram target language
model, distortion cost, word and phrase penalties.
As lexicalized reordering model, we use a hierar-
chical phrase orientation model (Galley and Man-
ning, 2008) trained on all the parallel data using
three orientation classes ? monotone, swap or dis-
continuous ? in bidirectional mode. Statistically
5Our results on test12 are not directly comparable to the
WMT12 submissions due to the different training data: that
is, the WMT12 parallel data includes 50M German tokens
of Europarl data and 4M of news-commentary, as opposed
to the 41M and 2.5M released for WMT10 and used in our
experiments.
6http://www2.lingsoft.fi/cgi-bin/gertwol
444
improbable phrase pairs are pruned from the trans-
lation model as proposed by Johnson et al (2007).
The hierarchical system is trained and tested
using the standard Moses configuration which in-
cludes: a rule table (two phrasal and two lexi-
cal probability features), a 6-gram target language
model, word and rule penalties. We set the span
constraint (cf. Section 5) to the default value of
10 words for rule extraction, while for decoding
we consider two different settings: the default 10
words and a large value of 20 to enable very long-
range reorderings.
Feature weights for all systems are optimized
by minimum BLEU-error training (Och, 2003) on
test08. To reduce the effects of the optimizer insta-
bility, we tune each configuration four times and
use the average of the resulting weight vectors for
testing, as suggested by Cettolo et al (2011).
The source-to-reference word alignments that
are needed to compute the reordering scores are
generated by the Berkeley Aligner previously
trained on the training data. Source-to-output
alignments are obtained from the decoder?s trace.
6.2 Distortion function and limit
We start by measuring the difference between
standard and early distortion cost.7 Figure 1
shows the results in terms of BLEU and KRS, plot-
ted against the distortion limit (DL).
Indeed, early distortion cost (Moore and Quirk,
2007) outperforms the standard one in all the
tested configurations and according to both met-
rics. We can see that the quality of both systems
deteriorates as the distortion limit increases, how-
ever the system with early distortion cost is more
robust to this effect. In particular, when passing
from DL=12 to DL=18, the baseline system loses
1.2 BLEU and no less than 6.8 KRS, whereas the
system with early distortion cost loses 0.8 BLEU
and 4.9 KRS. Given these results, we decide to use
early distortion cost in all the remaining experi-
ments.
6.3 WaW reordering pruning
We have seen that early distortion cost can effec-
tively reduce the loss of translation quality, but
cannot totally prevent it. Moreover, increasing
the distortion limit means exploring many more
7For this first series of experiments, feature weights are
tuned in the DL=8 setting and the two resulting weight vec-
tors (one for standard, one for early distortion) are re-used in
the higher-DL experiments.
Figure 1: Standard vs early distortion cost perfor-
mance measured in terms of BLEU and KRS on
tests(09-11) under different distortion limits.
hypotheses and, consequently, slowing down the
decoding process. With our WaW model-based
reordering pruning technique, we aim at solving
both issues.
We generate the WaW training data from the
first 30K sentences of the News-commentary-
2010 parallel corpus, using a sampling window of
width ?=10. This results in 8 million training sam-
ples, which are fed to the binary classifier imple-
mentation of the MegaM Toolkit8. Features with
less than 20 occurrences are ignored and the max-
imum number of training iterations is set to 100.
Evaluated intrinsically on test08, the model
achieves the following classification accuracy:
67.0% precision, 50.0% recall, 57.2% F-score.
While these figures are rather low, we recall that
the WaW model is not meant to be used as a stand-
alone classifier, but rather as one of several SMT
feature functions and as a way to detect very un-
likely reordering steps. Hence, we also evaluate its
ability to rank a typical set of reordering options
during decoding: that is, we traverse the source
words in target order and, for each of them, we ex-
8http://www.cs.utah.edu/?hal/megam/ (Daume? III, 2004).
445
tests(09-11) test12 ms/
System DL bleu met krs krs-V bleu met krs krs-V word
Allowing only short to medium-range reordering:
PSMT, early disto 8 19.2 28.1 67.4 65.4 19.0 28.1 67.8 66.1
! 202
+WaW (feature only) 19.4! 28.2! 67.6! 65.5! 19.5! 28.3! 67.8 66.2 212
HSMT, max.span=10 20.1! 28.5! 68.4! 66.7! 19.7" 28.4" 68.6! 67.3! 406
Allowing also long-range reordering:
PSMT, early disto
18
18.2 28.0 62.9 62.0 18.2 28.1 63.4 62.5 408
+WaW (feature only) 18.4! 28.0 61.8# 61.3# 18.1 28.1 62.2# 61.7# 428
+WaW reo.pruning (?=5) 19.5! 28.3! 67.9! 66.3! 19.3! 28.4! 67.8! 66.3! 142
HSMT, max.span=20 20.0! 28.5! 68.1! 66.7! 19.7! 28.4 68.2! 67.1! 706
Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality
and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with
% BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically
significant differences with respect to the previous row are marked with !# at the p ? .05 level and "$
at the p ? .10 level. Decoding time is measured in milliseconds per input word.
amine the ranking of all words that may be trans-
lated next (i. e. the uncovered positions within
a given DL). We find that, even when the DL is
very high (18), the correct jump is ranked among
the top 3 reachable jumps in the large majority of
cases (81.4%). If we only consider long jumps ?
i. e. spanning more than 6 words ? the Top-3 accu-
racy is 56.4% while that of a baseline that simply
favors shorter jumps (as the distortion cost does)
is only 26.5%.
For the early reordering pruning experiment, we
set the pruning parameters to 2 for histogram and
0.25 for relative threshold.9 A non-prunable-zone
of width ?=5 is set around the last covered posi-
tion. The resulting configuration is re-optimized
by MERT on test08 for the final experiment.
Table 1 shows the effects of integrating the
WaW reordering model into a PSMT decoder
that already includes a state-of-the-art hierarchi-
cal phrase orientation model. The same table also
presents the results of the HSMT constrastive ex-
periments. Two scenarios are considered: in the
first block, the PSMT distortion limit is set to a
medium value (8) and the HSMT maximum span
constraint is set to 10. Although not directly com-
parable, these settings have the same effect of dis-
allowing long-range reorderings. In the second
block, long-range reorderings are instead allowed
9Pruning parameters were optimized for BLEU with a
grid search over the values (1, 2, 3, 4, 5) for histogram and
(0.5, 0.25, 0.1) for threshold.
with a DL of 18 and a HSMT span constraint of
20.
Feature weights are optimized for each exper-
iment using the procedure described above (four
averaged MERT runs). Statistical significance is
computed for each experiment against the pre-
vious one (i. e. previous row), using approxi-
mate randomization as in (Riezler and Maxwell,
2005). Run times are obtained by an Intel Xeon
X5650 processor on the first 500 sentences of
tests(09-11), excluding loading time of all models.
Medium reordering space. Integrating the
WaW model as an additional feature function
yields small but consistent improvements (second
row of Table 1). Concerning the run time, we no-
tice just a small overload of about 5%: that is, from
202 to 212 ms/word.
In comparison, the tree-based system (third
row) has almost double decoding time but
achieves statistically significant higher translation
quality, especially at the level of reordering.
Large reordering space. As expected, raising
the DL to 18 with no special pruning (fourth row)
results in much slower decoding (from 202 to 408
ms/word) but also in very poor translation qual-
ity. This loss is especially visible on the reordering
scores: e. g. from 67.4 to 62.9 KRS on tests(09-
11). Unfortunately, adding the WaW model as a
feature function (fifth row) does not appear to be
helpful under the high DL condition.
On the other hand, when using the WaW model
446
adv. verbmod subj. obj. compl.
Jedoch konnten sie Kinder in Teilen von Helmand und Kandahar im Su?den aus Sicherheit? grund
SRC however could they children in parts of Helmand and Kandahar in South for security reasons
(de) neg verbinf
nicht erreichen .
not reach
REF But they could not reach children in parts of Helm. and Kand. in the south for security reasons.
BASE-8 However, they were children in parts of Helm. and Kand. in the south, for security reasons.
HIER-10 However, they were children in parts of Helm. and Kand. in the south not reach for security reasons.
BASE-18 However, they were children in parts of Helm. and Kand. in the south do not reach for security reasons.
WAWP-18 However, they could not reach children in parts of Helm. and Kand. in the south for security reasons.
HIER-20 However, they were children in parts of Helm. and Kand. in the south not reach for security reasons.
Table 2: Long-range reordering example showing the behavior of different systems: [BASE-*] are phrase-
based systems with a DL of 8 and 18 respectively; [WAWP-18] refers to the WaW-pruning PSMT system;
[HIER-*] are hierarchical SMT systems with a span constraint of 10 and 20 words respectively.
also for reordering pruning (sixth row) we are able
to recover the performance of the medium-DL
baseline performance and even to slightly improve
it. It is interesting to note that the largest improve-
ment concerns the accuracy of verb reordering on
tests(09-11): from 65.4 to 66.3 KRS-V. Although
the other gains are rather small, we emphasize the
fact that our solutions mostly affect rare and iso-
lated events, which have a limited impact on the
general purpose evaluation metrics but are are es-
sential to produce readable translations. WaW re-
ordering pruning has also a remarkable effect on
efficiency, making decoding time decrease from
428 ms/word to 142 ms/word, that is even faster
than a baseline that does not explore any long-
range reordering at all (202 ms/word).
Finally, we can see from the last row of Ta-
ble 1 that the gap between PSMT and HSMT has
been narrowed significantly. While more work is
needed to reach and outperform the quality of the
HSMT system, we were able to closely approach
it with five times lower decoding time (142 versus
706 ms/word) and about ten times smaller mod-
els (cf. Section 5). Comparing our best system
with the best HSMT system (i. e. span constraint
10), we see that the gap in translation accuracy
is slightly larger and that the decoding speed-up
is smaller (142 versus 406 ms/word). However,
the better performance and efficiency of HSMT-10
comes at the expense of all long-range reorderings.
Thus, our enhanced PSMT appears as an opti-
mal choice in terms of trade-off between transla-
tion quality and efficiency.
Table 3 reports two kinds of decoding statistics
that allow us to explain the very different decod-
ing times observed, and to verify that the WaW-
pruning system actually performs long-range re-
orderings: #hyp/sent is the average number of
partial translation hypotheses created10 per test
sentence; (#jumps/sent)?100 is the average
number of phrase-to-phrase jumps included in
the 1-best translation of every 100 test sentences.
Only medium and long jumps are shown (distor-
tion D?6), divided into three distortion buckets.
System DL #hyp/sent (#jumps/sent)?100D: [6..8] [9..12] [13..18]
baseline 8 600K 90 ? ?
baseline 18 1278K 88 61 48
+WaW r.prun. 18 364K 52 29 17
Table 3: Decoding statistics of three PSMT sys-
tems exploring different reordering search spaces
for the translation of test12.
We can see that the early-pruning system in-
deed performed several long jumps but it explored
a much smaller search space compared to the high-
distortion baseline (364K versus 1278K partial hy-
potheses). As for the lower number of long jumps
(e. g. 29 versus 61 with D in [9..12] and 17 versus
48 in [13..18]) it suggests that the early-pruning
system is more precise, while the high-distortion
baseline is over-reordering.
The output of different systems for our exam-
ple sentence is shown in Table 2. In this sentence,
a jump forward with D=12 and a jump backward
with D=14 were necessary to achieve the correct
reordering of the verb and its negation. Although
10That is, the hypotheses that were scored by all the PSMT
model components and added to a hypothesis stack.
447
Figure 2: Effects of beam size on translation quality measured by BLEU, KRS and KRS-V, in two base-
line PSMT systems (DL=8 and DL=18) and in the WaW early-pruning system (test12). For comparison,
the hierarchical system performance (span constraint 20) is provided as a dotted line.
these jumps were reachable for both the [PSMT-
18] and the [HSMT-20] systems, only the WaW-
pruning PSMT system actually performed them.
6.4 Interaction with beam-search pruning
During the beam-search decoding process, early
reordering pruning interacts with regular hypoth-
esis pruning based on the weighted sum of all
model scores. In particular, all the PSMT systems
presented so far apply a default histogram thresh-
old of 200 to each hypothesis stack. To examine
this interaction, we increase the histogram thresh-
old (beam size) from the default value of 200 up to
800, while keeping all other parameters and fea-
ture weights fixed. The results on test12 are plot-
ted against the beam size and reported in Figure 2.
The dotted line in each plot represents the perfor-
mance of the hierarchical system presented in the
last row of Table 1 (span constraint 20).
We can see that increasing the beam size is more
beneficial for the high-DL baseline (baseDL18)
than for the medium one (baseDL8). This is not
surprising as the risk of search error is higher when
a larger search space is explored with equal mod-
els and pruning parameters. Nevertheless, bas-
eDL18 remains by far the worst performing sys-
tem, even in our largest beam setting (800) corre-
sponding to four times longer decoding time (1582
ms/word). What is remarkable, instead, is that
the larger beam size also results in better perfor-
mances by the WaW-pruning system, which is the
PSMT system that explores by far the smallest
search space (cf. Table 3). The superiority of the
WaW-pruning system over the PSMT baselines is
maintained in all tested settings and according to
all metrics, which confirms the usefulness of our
methods not only as optimization techniques, but
also for reducing model errors of a baseline that
already includes strong reordering models.
With a very large beam size (800) our en-
hanced PSMT system can closely approach the
performance of HSMT-20 in terms of BLEU and
KRS-V, and even surpass it in terms of KRS (sta-
tistically significant) while still remaining faster:
that is, 554 versus 706 ms/word.
Overall HSMT-10 remains the best system, with
slightly higher KRS and KRS-V and lower de-
coding time than our best enhanced PSMT sys-
tem (406 versus 554 ms/word). However, we note
once more that this performance comes at the ex-
pense of all long-range reorderings. For a com-
pletely fair comparison, the HSMT system should
also be enhanced with similar reordering-pruning
techniques ? a research path that we plan to ex-
plore in the future, possibly inspiring from the ap-
proach of Braune et al (2012).
7 Conclusions
We have presented a few techniques that can im-
prove the accuracy of the word reordering per-
formed by a German-English phrase-based SMT
system. In particular, we have shown how long-
range reorderings can be captured without worsen-
ing the general quality of translation and without
renouncing to efficiency. Our best PSMT system
is actually faster than a system that does not even
attempt to perform long-range reordering, and it
448
obtains significantly higher evaluation scores.
In comparison to a more computationally costly
tree-based approach (hierarchical SMT), our en-
hanced PSMT system produces slightly lower
translation quality but in five times lower decod-
ing time when long-range reordering is allowed.
Moreover, when a larger beam size is explored,
the performance of our system can equal that of
the long-reordering hierarchical system, but still
with faster decoding.
In summary, we have shown that an appropri-
ate modeling of the word reordering problem can
lead to narrow or even fill the gap between phrase-
based and hierarchical SMT in this difficult lan-
guage pair. We have also disproved the common
belief that sacrificing long-range reorderings by
setting a low distortion limit is the only way to
obtain well-performing PSMT systems.
Acknowledgments
This work was partially funded by the European
Union under FP7 grant agreement EU-BRIDGE,
Project Number 287658.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 745?
754, Stroudsburg, PA, USA.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2013. Dy-
namically shaping the reordering search space of
phrase-based statistical machine translation. To ap-
pear in Transactions of the ACL.
Arianna Bisazza. 2013. Linguistically Motivated
Reordering Modeling for Phrase-Based Statistical
Machine Translation. Ph.D. thesis, University of
Trento. http://eprints-phd.biblio.unitn.it/1019/.
Fabienne Braune, Anita Gojun, and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
the Annual Conference of the European Associa-
tion for Machine Translation (EAMT), pages 28?30,
Trento, Italy.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. In MT Summit XIII: the Thir-
teenth Machine Translation Summit, pages 32?39,
Xiamen, China.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper avail-
able at http://pub.hal3.name, implementa-
tion available at http://hal3.name/megam.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit
for Handling Large Scale Language Models. In
Proceedings of Interspeech, pages 1618?1621, Bris-
bane, Australia.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statis-
tical machine translation. In Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Denver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
449
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 376?384, Stroudsburg, PA, USA.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 726?735, Avi-
gnon, France, April.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 867?
875, Los Angeles, California.
Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 409?417, Up-
psala, Sweden, July.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most
of the phrasetable. In In Proceedings of EMNLP-
CoNLL 07, pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133, Ed-
monton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. of the International Workshop on Spoken
Language Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Prague, Czech Republic.
Kimmo Koskenniemi and Mariikka Haapalainen,
1994. GERTWOL ? Lingsoft Oy, chapter 11, pages
121?140. Roland Hausser, Niemeyer, Tu?bingen.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June.
Robert C. Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In In Proceedings of MT Summit
XI, pages 321?327, Copenhagen, Denmark.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 295?302, Philadelhpia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL).
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007?1016,
Singapore, August.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July.
450
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508?514, Geneva, Switzerland, Aug 23?Aug
27. COLING.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Con-
ference on Artificial Intelligence (KI2002), pages
18?32, Aachen, Germany. Springer Verlag.
451
