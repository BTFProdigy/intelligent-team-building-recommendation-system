Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 609?616
Manchester, August 2008
Detecting multiple facets of an event using graph-based unsupervised
methods
Pradeep Muthukrishnan
Dept of EECS
University of Michigan
mpradeep@umich.edu
Joshua Gerrish
School of Information
University of Michigan
jgerrish@umich.edu
Dragomir R. Radev
Dept of EECS &
School of Information,
University of Michigan
radev@umich.edu
Abstract
We propose a new unsupervised method
for topic detection that automatically iden-
tifies the different facets of an event. We
use pointwise Kullback-Leibler divergence
along with the Jaccard coefficient to build
a topic graph which represents the com-
munity structure of the different facets.
The problem is formulated as a weighted
set cover problem with dynamically vary-
ing weights. The algorithm is domain-
independent and generates a representa-
tive set of informative and discriminative
phrases that cover the entire event. We
evaluate this algorithm on a large collec-
tion of blog postings about different news
events and report promising results.
1 Introduction
Finding a list of topics that a collection of docu-
ments cover is an important problem in informa-
tion retrieval. Topics can be used to describe or
summarize the collection, or they can be used to
cluster the collection. Topics provide a short and
informative description of the documents that can
be used for quickly browsing and finding related
documents.
Inside a given corpus, there may be multiple top-
ics. Individual documents can also contain multi-
ple topics.
Traditionally, information retrieval systems re-
turn a ranked list of query results based on the
similarity between the user?s query and the docu-
ments. Unfortunately, the results returned will of-
ten be redundant. Users may need to reformulate
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
their search to find the specific topic they are in-
terested in. This active searching process leads to
inefficiencies, especially in cases where queries or
information needs are ambiguous.For example, a
user wants to get an overview of the Virginia tech
shootings, then the first query he/she might try is
?Virginia tech shooting?. Most of the results re-
turned would be posts just mentioning the shoot-
ings and the death toll. But the user might want
a more detailed overview of the shootings. Thus
this leads to continuously reformulating the search
query to discover all the facets of the event.
2 Related Work
Topic detection and tracking was studied exten-
sively on newswire and broadcast collections by
the NIST TDT research program (Allan et al, ).
The large number of people blogging on the web
provides a new source of information for topic de-
tection and tracking.
The TDT task defines topics as ?an event or ac-
tivity, along with all directly related events and ac-
tivities.? In this paper we will stay with this defini-
tion of topic.
Zhai et al proposed several methods for dealing
with a related task, which they called subtopic re-
trieval (Zhai et al, 2003). This is an information
retrieval task where the goal is to retrieve and re-
turn documents that cover the different subtopics
of a given query. As they point out, the utility
of each document is dependent on the other doc-
uments in the ranking, which violates the indepen-
dent relevance assumption traditionally used in IR.
Blei et al (Blei et al, 2003) proposed Latent
Dirichlet Allocation (LDA), a generative model
that allows sets of documents to be explained by
unobserved groups of documents, each based on
a single topic. The LDA model assumes the bag-
609
of-words model and posits that each document is
composed of different topics. Specifically, each
word?s existence is attributed to one of the docu-
ment?s topic. This algorithm outputs a set of n-
grams for each topic whereas our algorithm mod-
els each subtopic using a single n-gram. Due to
limitations of time we were not able to compare
this approach with ours. We plan to have this com-
parison in our future work.
To reduce the complexity of this task, a candi-
date set of subtopics needs to be generated that
cover the document collection. We choose to
use a keyphrase detection algorithm to generate
topic labels. Several keyphrase extraction algo-
rithms have been discussed in the literature, in-
cluding ones based on machine learning methods
(Turney, 2000), (Hulth, 2003) and tf-idf ((Frank
et al, 1999)). Our method uses language models
and pointwise mutual information expressed as the
Kullback-Leibler divergence.
Kullback-Leibler divergence has been found to
be an effective method of finding keyphrases in
text collections. But identification of keyphrases
is not enough to find topics in document. The
keyphrases identified may describe the entire col-
lection, or aspects of the collection. We wish to
summarize subtopics within these collections.
The problem of subtopic detection is also related
to novelty detection in (Allan et al, ). In this prob-
lem, given a set of previously seen documents, the
task is to determine whether a new document con-
tains new or novel content. The TREC 2002 nov-
elty track, the task was to discard sentences that
did not contain new material. This is similar to our
goal of reducing redundancy in the list of returned
subtopics.
In most cases, novelty detection is implemented
as an online algorithm. The system has a set of ex-
isting documents they have seen up until a certain
point. The task is to determine whether a new doc-
ument is novel based on the previous documents.
Once a decision has been made, the status of that
document is fixed. The subtopic detection task dif-
fers from this because it is an offline task. The al-
gorithm typically has access to the entire document
set. Our method differs from this novelty detection
task in that it has access to the entire document col-
lection.
2.1 Existing redundancy measures
Zhang et al examine five different redundancy
measures for adaptive information filtering (Zhang
et al, ). Information filtering systems return rel-
evant documents in a document stream to a user.
Examples of information filtering systems include
traditional information retrieval systems that return
relevant documents depending on the user?s query.
The redundancy measures Zhang et al examine
are based on online analysis of documents. They
identify two methods of measuring redundancy:
? Given n documents, they are considered one
by one, and suppose we have processed i doc-
uments and we have k clusters. Now we need
to process the i+1th document. We compute
the distance of the i + 1th document with the
k clusters and add the document to the clos-
est cluster if the distance is above a certain
threshold, else we create a new cluster with
only the i+ 1th document.
? Measure distance between the new document
and each previously seen document.
They evaluate several measures like set difference,
geometric distance, Distributional similarity and
mixture models. Evaluating the five systems, they
found that cosine similarity was the most effective
measure, followed by the new mixture model mea-
sure.
3 Data
We choose several news events that occurred in
2007 and 2008 based on the popularity in the bl-
ogosphere. We were looking for events that were
widely discussed and commented on. The events
in our collection are the top-level events that we
have gathered. Table 1 lists the events that were
chosen for analysis:
To help illustrate our subtopic detection method,
we will use the Virginia Tech tragedy as an ex-
ample throughout the rest of this paper. People
throughout the blogosphere posted responses ex-
pressing support and condolences for the people
involved, along with their own opinions on what
caused it.
Figures 1 and 2 show two different responses to
the event. The quote in figure 1 shows an example
post from LiveJournal, a popular blogging com-
munity. In this post, the user is discussing his view
on gun control, a hotly debated topic in the after-
math of the shooting. Figure 2 expresses another
person?s emotional response to this event. Both
posts show different aspects of the same story. Our
subtopic detection system seeks to automatically
610
Event Description Posts Dates
iPhone iPhone release hype 48810 June 20 , 2007 - July 7, 2007
petfoodrecall Melamine tainted petfood recall 4285 March 10, 2007 - May 10, 2007
spitzer Eliot Spitzer prostitution scandal 10379 March 6, 2008 - March 23, 2008
vtech Virginia Tech shooting 12256 April 16, 2007 - April 30, 2007
Table 1: Major events summarized
identify these and other distinct discussions that
occur around an event.
After the Virginia Tech murders, there?s
the usual outcry for something to be
done, and in particular, for more gun
control. As usual, I am not persuaded.
The Virginia Tech campus had gun con-
trol, which meant that Cho Seung-Hui
was in violation of the law even before
he started shooting, and also that no law-
abiding citizens were able to draw.
Figure 1: Example blog post from LiveJournal dis-
cussing gun control (Rosen, 2007)
... Predictably, there have been rum-
blings in the media that video games
contributed to Cho Seung-Hui?s mas-
sacre at Virginia Tech. Jack Thomp-
son has come out screaming, referring
to gamers as ?knuckleheads? and calling
video games ?mental masturbation? all
the while referring to himself as an ?ed-
ucator? and ?pioneer? out to ?right? so-
ciety. ...
Figure 2: Example blog post discussing video
games (hoopdog, 2007)
Figure 3 shows a generalized Venn diagram
(Kestler et al, 2005) of the cluster overlap between
different keyphrases from the Virginia Tech event.
3.1 Preprocessing
Data was collected from the Blogocenter blog-
lines database. The Blogocenter group at UCLA
has been retrieving RSS feeds from the Bloglines,
Blogspot, Microsoft Live Spaces, and syndic8 ag-
gregators for the past several years. They currently
have over 192 million blog posts collected.
For each news item, relevant posts were re-
trieved, based on keyword searching and date of
blog post. Posts from the date of occurrence of
the item to two weeks after the event occurred
Figure 3: Generalized Venn diagram of topic over-
lap in the Virginia Tech collection
were gathered, regardless of the actual length of
the event.
Since many RSS feeds indexed by Bloglines are
from commercial news organizations or commer-
cial sites, we had to clean up the retrieved data.
Table 1 lists the event we analyzed along with ba-
sic statistics.
4 Method
Our algorithm should find discriminative labels for
the different topics that exist in a collection of doc-
uments. Taken together, these labels should satisfy
the following conditions:
? Describe a large portion of the collection
? The overlap between the topics should be
minimal
This task is similar to Minimum Set Cover,
which is NP-complete (Garey and Johnson, 1990).
Therefore, trying to find the optimal solution by
enumerating all possible phrases in the corpus
would be impossible, instead we propose a two-
step method for subtopic detection.
The first step is to generate a list of candidate
phrases. These phrases should be informative and
representative of all of the different subtopics. The
second step should select from these phrases con-
sistent with the two conditions stated above.
611
4.1 Generating Candidate Phrases
We want to generate a list of phrases that have a
high probability of covering the document space.
There are many methods that could be used to find
informative keyphrases. One such method is using
the standard information retrieval TF-IDF model
(Salton and McGill, 1986).
Witten et al(Witten et al, 1999) proposed
KEA, an algorithm which generates a list of can-
didate keyphrases using lexical features. They
keyphrases are then selected from these candidates
using a supervised machine learning algorithm.
This approach is not plausible for our purposes be-
cause of the following two reasons.
1. The algorithm is domain-dependent and
needs a training set of documents with anno-
tated keyphrases. But our data sets come from
various domains and it is not a very viable op-
tion to create a training set for each domain.
2. The algorithm generates keyphrases for a sin-
gle document, but for our purposes we need
keyphrases for a corpus.
Another method is using Kullback-Leibler di-
vergence to find informative keyphrases. We found
that KL divergence generated good candidate top-
ics.
Tomokiyo and Hurst (2003) developed a method
of extracting keyphrases using statistical language
models. They considered keyphrases as consisting
of two features, phraseness and informativeness.
Phraseness is described by them as the ?degree to
which a given word sequence is considered to be a
phrase.? For example, collocations could be con-
sidered sequences with a high phraseness. Infor-
mativeness is the extent to which a phrase captures
the key idea or main topic in a set of documents.
To find keyphrases, they compared two lan-
guage models, the target document set and a back-
ground corpus. Pointwise KL divergence was cho-
sen as the method of finding the difference be-
tween two language models.
The KL divergence D(p||q) between two prob-
ability mass functions p(x) and q(x) with alphabet
? is given in equation 1.
D(p||q) =
?
x??
p(x)log
p(x)
q(x)
(1)
KL divergence is an asymmetric function.
D(p||q) may not equal D(q||p).
Pointwise KL divergence is the individual con-
tribution of x to the loss of the entire distribution.
The pointwise KL divergence of a single phrase w
is ?
w
(p||q):
?
w
(p||q) = p(w)log
p(w)
q(w)
(2)
The phraseness of a phrase can be found by
comparing the foreground n-gram language model
against the background unigram model. For ex-
ample, if we were judging the phraseness of ?gun
control?, we would find the pointwise KL diver-
gence of ?gun control? between the foreground bi-
gram language model and the foreground unigram
language model.
?
p
= ?
w
(LM
f
g
N
||LM
f
g
1
) (3)
The informativeness of a phrase can be found by
finding the pointwise KL divergence of the fore-
ground model against the background model.
?
i
= ?
w
(LM
f
g
N
||LM
b
g
N
) (4)
A unified score can be formed by adding the
phraseness and informative score: ? = ?
p
+ ?
i
4.2 Selecting Topic Labels
Once keyphrases have been extracted from the
document set, they are sorted based on their
combined score. We select the top n-ranked
keyphrases as candidate phrases. This step will
hereafter be referred to as ?KL divergence mod-
ule?.
Based on our chosen task conditions regarding
coverage of the documents and minimized overlap
between topics, we need an undirected mapping
between phrases and documents. A natural repre-
sentation for this is a bipartite graph where the two
sets of nodes are phrases and documents. Let the
graph be: G = (W,D,E) where W is the set of
candidate phrases generated by the first step and D
is the entire set of documents. E is the set of edges
between W and D where there is an edge between
a phrase and a document if the document contains
the phrase.
We formulate the task as a variation of Weighted
Set Cover problem in theoretical computer science.
In normal Set Cover we are given a collection of
sets S over a universe U , and the goal is to select a
minimal subset of S such that the whole universe,
U is covered. Unfortunately this problem is NP-
complete (Garey and Johnson, 1990), so we must
612
d1
w3
w1
d4
d2
w2
d5
d3
Figure 4: Bipartite graph representation of topic
document coverage, where the d
i
?s are the docu-
ments and the w
i
?s are the n-grams
settle for an approximate solution. But fortunately
there exist very good ?-approximation algorithms
for this problem (Cui, 2007).
The difference in Weighted Set Cover is that
each set has an associated real-valued weight or
cost and the goal is to find the minimal or maximal
cost subset which covers the universe U .
In our problem, each phrase can be thought of
as a set of the documents which contain it. The
universe is the set of all documents.
4.3 Greedy Algorithm
To solve the above problem, we propose a greedy
algorithm. This algorithm computes a cost for each
node iteratively and selects the node with the low-
est cost at every iteration. The cost of a keyphrase
should be such that we do not choose a phrase with
very high coverage, like ?Virginia? and at the same
time not choose words with very low document
frequency since a very small collection of docu-
ments can not be judged a topic.
Based on these two conditions we have come up
with a linear combination of two cost components,
similar to Maximal Marginal Relevance (MMR)
(Carbonell and Goldstein, 1998).
1. Relative Document Size:
f
1
(w
i
) =
|adj(w
i
)|
N
(5)
where |adj(w
i
)| is the document frequency of
the word.
This factor takes into account that we do not
want to choose words which cover the whole
document collection. For example, phrases
such as ?Virginia? or ?Virginia tech? are bad
subtopics, because they cover most of the
document set.
2. Redundancy Penalty:
We want to choose elements that do not have a
lot of overlap with other elements. One mea-
sure of set overlap is the Jaccard similarity co-
efficient:
J(A,B) =
|A ?B|
|A ?B|
(6)
f
2
(w
i
) = 1?
?
w
j
?W?w
i
J(w
i
, w
j
)
|W | ? 1
(7)
This component is essentially 1? average
Jaccard similarity.
We calculate the pairwise Jaccard coefficient
between the target keyphrase and every other
keyphrase. The pairwise coefficient vector
provides information on how much overlap
there is between a keyphrase and every other
keyphrase. Phrases with a high average Jac-
card coefficient are general facets that cover
the entire collection. Phrases with a low Jac-
card coefficient are facets that cover specific
topics with little overlap.
3. Subtopic Redundancy Memory Effect
Once a keyphrase has been chosen we also
want to penalize other keyphrases that cover
the same content or documents. Equation 8
represents a redundancy ?memory? for each
keyphrase or subtopic. This memory is up-
dated for every step in the greedy algorithm.
R(w
i
) = R(w
i
) + J(w
i
, w
j
) (8)
where w
j
is the newly selected phrase.
A general cost function can be formed from a
linear combination of the three cost components.
We provide two parameters, ? and ? to represent
the trade-off between coverage, cohesiveness and
intersection. For our experiments, we found that
an ? value of 0.7 and a ? value of 0.2 performed
well.
cost(w
i
) = ?? f
1
(w
i
)
+? ? f
2
(w
i
)
+(1? (? + ?))?R(w
i
)
(9)
613
The pseudocode for the greedy algorithm is
given in Figure 5. It should be noted that the al-
gorithm requires the costs to be recomputed af-
ter every iteration. This is because the cost of a
keyphrase may change due to a change in any of
the three components. This is because after select-
ing a keyphrase, it might make another keyphrase
redundant, that is, covering the same content. This
makes the whole problem a dynamic weighted set
cover problem. Hence, the performance guaran-
tees associated with the greedy algorithm for the
Weighted Set Cover problem do not hold true for
the dynamic version.
Algorithm Greedy algorithm for weighted set-cover
Input: Graph G = (W,D,E)
1. N: number of documents to cover
2.
Output: Set of discriminative phrases for the different topics
3. W = {w
1
, w
2
, . . . , w
n
}
4. W
chosen
= ?
5. num docs covered = 0
6. while num docs covered < N
7. do for w
i
? W
8. do cost(w
i
) = ?? f
1
(w
i
)
9. +? ? f
2
(w
i
)
10. +(1? (?+ ?))?R(w
i
)
11. w
selected
= argmax
w
cost(w
i
)
12. for w
i
? W
13. do R(w
i
) = R(w
i
) + J(w
selected
, w
i
)
14. num docs covered = num docs covered +
|adj(w
selected
)|
15. W
chosen
= W
chosen
? {w
selected
}
16. W = W ? {w
selected
}
17. D = D ? adj(selected)
18. return W
chosen
Figure 5: A greedy set-cover algorithm for detect-
ing sub-topics
5 Experiments
As a baseline measure, we extracted the top k
phrases from the word distribution as the topic la-
bels. As a gold standard, we manually annotated
the four different collections of blog posts. Each
annotator generated a list of subtopics.
6 Evaluation
In evaluating topic detection, there exist two cate-
gories of methods, intrinsic and extrinsic (Liddy,
2001). Extrinsic methods evaluate the labels
against a particular task whereas intrinsic methods
measure the quality of the labels directly. We pro-
vide intrinsic and extrinsic evaluations of our algo-
rithm.
To evaluate our facet detection algorithm, we
created a gold standard list of facets for each data
set. A list of the top 300 keyphrases generated by
the KL divergence module was given to two eval-
uators. The evaluators were the first and second
author of this paper. The evaluators labeled each
keyphrase as a positive example of a subtopic or
a negative example of a subtopic. The positive
examples taken together form the gold standard.
For this evaluation process we defined a positive
subtopic as a cohesive collection of documents dis-
cussing the same topic.
Cohen?s Kappa coefficient (Cohen, 1960) was
calculated for the gold standard. Table 6 lists the ?
value for the four data sets.
iPhone petfoodrecall spitzer vtech
0.62 0.86 0.77 0.88
Table 2: Kappa scores for the gold standard
The kappa scores for the petfoodrecall and vtech
datasets showed good agreement among the raters,
while the spitzer data set had only fair agreement.
For the iPhone data set, both evaluators had a large
amount of disagreement on what they considered
subtopics.
A separate group of two evaluators was given
the output from our graph-based algorithm, a list
of the top KL divergence keyphrases of the same
length, and the gold standard for all four data sets.
Evaluators were asked to rate the keyphrases on a
scale from one to five, with one indicating a poor
subtopic, and five indicating a good subtopic. The
number k of subtopics for the algorithm was cutoff
where the f-score is maximized. The same number
of phrases was chosen for KL divergence as well.
Table 3 lists the cutoffs for the four data sets.
iPhone Petfood recall Spitzer Vtech
25 30 24 18
Table 3: Number of generated subtopics for each
collection.
In addition, the precision, F-score, coverage and
average pairwise Jaccard coefficient were calcu-
lated for the four data sets. Precision, recall and
the F-score are given in table 4. The precision,
recall and F-score for the gold standards is one.
The others are shown in table 5. Average pairwise
Jaccard coefficient is calculated by finding the Jac-
card coefficient for every pair of subtopics in the
output and averaging this value. This value is a
measure of the redundancy. The average relevance
is a normalized version of the combined ?phrase-
614
ness? and ?informativeness? score calculated by
the keyphrase detection module. This value is nor-
malized by dividing by the KL divergence for the
entire 300 phrase list. This provides a relevancy
score for the output.
Data set Precision Recall F-score
iphone
KL-Divergence 0.08 0.10 0.09
Graph-based method 0.52 0.60 0.56
petfoodrecall
KL-Divergence 0.37 0.39 0.38
Graph-based method 0.61 0.57 0.59
spitzer
KL-Divergence 0.10 0.08 0.09
Graph-based method 0.79 0.59 0.68
vtech
KL-Divergence 0.05 0.06 0.05
Graph-based method 0.72 0.76 0.74
Table 4: Precision, recall and F-score for the base-
line and graph-based algorithm.
Data set
Coverage Average Normalized Human
pairwise KL rating
JC divergence
iphone
KL-Divergence 40168 0.08 18.19 1.92
Gold standard 12977 0.02 2.81 3.13
Graph-based 9850 0.01 1.98 2.82
petfoodrecall
KL-Divergence 4280 0.18 19.53 1.82
Gold standard 2659 0.05 4.30 3.43
Graph-based 2055 0.01 1.75 2.81
spitzer
KL-Divergence 9291 0.19 22.90 1.33
Gold standard 4036 0.03 2.29 3.31
Graph-based 2468 0.01 1.60 2.88
vtech
KL-Divergence 12215 0.29 24.61 1.61
Gold standard 5058 0.03 2.79 3.76
Graph-based 4342 0.01 1.66 3.28
Table 5: Coverage, overlap and relevance and eval-
uation scores for the gold standard, baseline and
graph-based method.
7 Results
Table 6 shows some of the different subtopics cho-
sen by our algorithm for the different data sets.
There is no manual involvement required in the al-
gorithm except for the intial preprocessing to re-
move commercial news feeds and spam posts. Our
graph-based method performs very well and al-
most achieves the gold standard?s rating. The F-
score for the iPhone data set was only 0.56, but we
believe part of this may be because this data set did
not have clearly defined subtopics, as shown by the
low agreement (0.62) among human evaluators.
Spitzer Petfood recall
Ashley Alexandra Dupre Under Wal-Mart
Oberweis Xuzhou Anying
Emperor?s club People who buy
Governor of New Cuts and Gravy
Spitzer?s resignation Cat and Dog
Dr Laura Cats and Dogs
Mayflower hotel Food and Drug
Sex workers Cyanuric acid
former New york recent pet
High priced prostitution industrial chemical
McGreevey massive pet food
Geraldine Ferraro Royal canin
High priced call Iams and Eukanuba
legally blind Dry food
money laundering
Virginia Tech shooting iPhone
Korean American Photo sent from
Gun Ownership Waiting in line
Holocaust survivor About the iPhone
Mentally ill Unlimited data
Shooting spree From my iPhone
Don Imus cell phones
Video Games Multi-touch
Gun free zone Guided tour
West Ambler Johnston iPhone launch
Columbine High school Walt Mossberg
Self defense Apple Inc
Two hours later Windows Mobile
Gun violence June 29th
Seung Hui Cho Web browser
Second Amendment Activation
South Korean
Table 6: Different topics chosen by the graph-
based algorithm for the different data sets
Figure 6 shows the trade off between coverage
and redundancy. This graph clearly shows that
the overlap between the subtopics increases very
slowly as compared to the number of documents
covered. The slope of the curves increases slowly
when the number of documents to be covered is
small and later increases rapidly. This means that
initially there are a lot of small focused subtopics
and once we have selected all the focused ones the
algorithm is forced to pick the bigger topics and
hence the average pairwise intersection increases.
0 0.5 1 1.5 2 2.5 3 3.5 4
x 104
0
10
20
30
40
50
60
70
80
Number of documents to be covered
Av
er
ag
e 
pa
irw
ise
 in
te
rs
ec
tio
n 
of
 to
pi
cs
 
 
iPhone
Spitzer
petfoodrecall
vtech
Figure 6: Subtopic redundancy vs. coverage
615
8 Conclusion
We have presented a new algorithm based on
weighted set cover for finding subtopics in a
corpus of selected blog postings. The algo-
rithm performs very well in practice compared
to the baseline standard, which outputs the top
keyphrases according to the Kullback-Leibler di-
vergence method. While the baseline standard out-
puts keyphrases which are redundant, in the sense,
they cover the same documents, the graph-based
method outputs keyphrases which have very little
intersection. We provide a new method of ranking
keyphrases that can help users find different facets
of an event.
The identification of facets has many applica-
tions to natural language processing. Once facets
have been identified in a collection, documents can
be clustered based on these facets. These clusters
can be used to generate document summaries or
for visualization of the event space.
The keyphrases themselves provide a succinct
summary of the different subtopics. In future
work, we intend to investigate summarization of
documents based on subtopic clustering using this
method.
9 Acknowledgments
This work was supported by NSF grants IIS
0534323 ?Collaborative Research: BlogoCenter -
Infrastructure for Collecting, Mining and Access-
ing Blogs? awarded to The University of Michigan
and ?iOPENER: A Flexible Framework to Support
Rapid Learning in Unfamiliar Research Domains?,
jointly awarded to U. of Michigan and U. of Mary-
land as IIS 0705832.? Also we would like to thank
Vahed Qazvinian and Arzucan ?Ozgu?r for helping
with the evaluation and their valuable suggestions.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
Allan, James, Courtney Wade, and Alvaro Bolivar. Re-
trieval and novelty detection at the sentence level.
SIGIR 2003, pages 314?321.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022, January.
Carbonell, Jaime G. and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. SIGIR 1998,
pages 335?336.
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measure-
ment, 20:37.
Cui, Peng. 2007. A tighter analysis of set cover greedy
algorithm for test set. In ESCAPE, pages 24?35.
Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. Sixteenth In-
ternational Joint Conference on Artificial Intelli-
gence, pages 668?673.
Garey, Michael R. and David S. Johnson. 1990. Com-
puters and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman.
hoopdog. 2007. Follow-up: Blame game.
http://hoopdogg.livejournal.com/39060.html.
Hulth, Anette. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. EMNLP
2003, pages 216?223.
Kestler, Hans A., Andre Muller, Thomas M. Gress, and
Malte Buchholz. 2005. Generalized venn diagrams:
a new method of visualizing complex genetic set re-
lations. Bioinformatics, 21:1592?1595, April.
Liddy, Elizabeth. 2001. Advances in automatic text
summarization. Information Retrieval, 4:82?83.
Rosen, Nicholas D. 2007. Gun control and mental
health. http://ndrosen.livejournal.com/128715.html.
Salton, G. and M. J. McGill. 1986. Introduction to
Modern Information Retrieval. McGraw-Hill, Inc.
New York, NY, USA.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. ACL
2003 workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, pages 33?40.
Turney, Peter D. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303?
336.
Witten, Ian H., Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In 1st
ACM/IEEE-CS joint conference on Digital libraries,
pages 254?255.
Zhai, Chengxiang, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: meth-
ods and evaluation metrics for subtopic retrieval. SI-
GIR 2003, pages 10?17.
Zhang, Yi, James P. Callan, and Thomas P. Minka.
Novelty and redundancy detection in adaptive filter-
ing. In SIGIR 2002, pages 81?88.
616
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Citations to Generate Surveys of Scientific Paradigms
Saif Mohammad??, Bonnie Dorr???, Melissa Egan??, Ahmed Hassan?,
Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev??, David Zajic?
Institute for Advanced Computer Studies? and Computer Science?, University of Maryland.
Human Language Technology Center of Excellence?. Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu
Department of Electrical Engineering and Computer Science?
School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.edu
Abstract
The number of research publications in var-
ious disciplines is growing exponentially.
Researchers and scientists are increasingly
finding themselves in the position of having
to quickly understand large amounts of tech-
nical material. In this paper we present the
first steps in producing an automatically gen-
erated, readily consumable, technical survey.
Specifically we explore the combination of
citation information and summarization tech-
niques. Even though prior work (Teufel et
al., 2006) argues that citation text is unsuitable
for summarization, we show that in the frame-
work of multi-document survey creation, cita-
tion texts can play a crucial role.
1 Introduction
In today?s rapidly expanding disciplines, scientists
and scholars are constantly faced with the daunting
task of keeping up with knowledge in their field. In
addition, the increasingly interconnected nature of
real-world tasks often requires experts in one dis-
cipline to rapidly learn about other areas in a short
amount of time.
Cross-disciplinary research requires scientists in
areas such as linguistics, biology, and sociology
to learn about computational approaches and appli-
cations, e.g., computational linguistics, biological
modeling, social networks. Authors of journal ar-
ticles and books must write accurate surveys of pre-
vious work, ranging from short summaries of related
research to in-depth historical notes.
Interdisciplinary review panels are often called
upon to review proposals in a wide range of areas,
some of which may be unfamiliar to panelists. Thus,
they must learn about a new discipline ?on the fly?
in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by
combining two currently available technologies: (1)
bibliometric lexical link mining that exploits the
structure of citations and relations among citations;
and (2) summarization techniques that exploit the
content of the material in both the citing and cited
papers.
It is generally agreed upon that manually written
abstracts are good summaries of individual papers.
More recently, Qazvinian and Radev (2008) argue
that citation texts are useful in creating a summary
of the important contributions of a research paper.
The citation text of a target paper is the set of sen-
tences in other technical papers that explicitly refer
to it (Elkiss et al, 2008a). However, Teufel (2005)
argues that using citation text directly is not suitable
for document summarization.
In this paper, we compare and contrast the use-
fulness of abstracts and of citation text in automati-
cally generating a technical survey on a given topic
from multiple research papers. The next section pro-
vides the background for this work, including the
primary features of a technical survey and also the
types of input that are used in our study (full pa-
pers, abstracts, and citation texts). Following this,
we describe related work and point out the advances
of our work over previous work. We then describe
how citation texts are used as a new input for multi-
document summarization to produce surveys of a
given technical area. We apply four different sum-
marization techniques to data in the ACL Anthol-
584
ogy and evaluate our results using both automatic
(ROUGE) and human-mediated (nugget-based pyra-
mid) measures. We observe that, as expected, ab-
stracts are useful in survey creation, but, notably, we
also conclude that citation texts have crucial survey-
worthy information not present in (or at least, not
easily extractable from) abstracts. We further dis-
cover that abstracts are author-biased and thus com-
plementary to the broader perspective inherent in ci-
tation texts; these differences enable the use of a
range of different levels and types of information in
the survey?the extent of which is subject to survey
length restrictions (if any).
2 Background
Automatically creating technical surveys is sig-
nificantly distinct from that of traditional multi-
document summarization. Below we describe pri-
mary characteristics of a technical survey and we
present three types of input texts that we used for
the production of surveys.
2.1 Technical Survey
In the case of multi-document summarization, the
goal is to produce a readable presentation of mul-
tiple documents, whereas in the case of technical
survey creation, the goal is to convey the key fea-
tures of a particular field, basic underpinnings of the
field, early and late developments, important con-
tributions and findings, contradicting positions that
may reverse trends or start new sub-fields, and ba-
sic definitions and examples that enable rapid un-
derstanding of a field by non-experts.
A prototypical example of a technical survey is
that of ?chapter notes,? i.e., short (50?500 word)
descriptions of sub-areas found at the end of chap-
ters of textbook, such as Jurafsky and Martin (2008).
One might imagine producing such descriptions au-
tomatically, then hand-editing them and refining
them for use in an actual textbook.
We conducted a human analysis of these chapter
notes that revealed a set of conventions, an outline
of which is provided here (with example sentences
in italics):
1. Introductory/opening statement: The earliest
computational use of X was in Y, considered by
many to be the foundational work in this area.
2. Definitional follow up: X is def ined as Y.
3. Elaboration of definition (e.g., with an exam-
ple): Most early algorithms were based on Z.
4. Deeper elaboration, e.g., pointing out issues
with initial approaches: Unfortunately, this
model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances /
historical background with citations: Two clas-
sic approaches are described in Q.
7. References to other summaries: R provides a
comprehensive guide to the details behind X.
The notion of text level categories or zoning
of technical papers?related to the survey compo-
nents enumerated above?has been investigated pre-
viously in the work of Nanba and Kan (2004b) and
Teufel (2002). These earlier works focused on the
analysis of scientific papers based on their rhetori-
cal structure and on determining the portions of pa-
pers that contain new results, comparisons to ear-
lier work, etc. The work described in this paper fo-
cuses on the synthesis of technical surveys based on
knowledge gleaned from rhetorical structure not un-
like that of the work of these earlier researchers, but
perhaps guided by structural patterns along the lines
of the conventions listed above.
Although our current approach to survey creation
does not yet incorporate a fully pattern-based com-
ponent, our ultimate objective is to apply these pat-
terns to guide the creation and refinement of the final
output. As a first step toward this goal, we use cita-
tion texts (closest in structure to the patterns iden-
tified by convention 7 above) to pick out the most
important content for survey creation.
2.2 Full papers, abstracts, and citation texts
Published research on a particular topic can be sum-
marized from two different kinds of sources: (1)
where an author describes her own work and (2)
where others describe an author?s work (usually in
relation to their own work). The author?s descrip-
tion of her own work can be found in her paper. How
others perceive her work is spread across other pa-
pers that cite her work. We will refer to the set of
sentences that explicitly mention a target paper Y as
the citation text of Y.
585
Traditionally, technical survey generation has
been tackled by summarizing a set of research pa-
pers pertaining to the topic. However, individual re-
search papers usually come with manually-created
?summaries??their abstracts. The abstract of a pa-
per may have sentences that set the context, state the
problem statement, mention how the problem is ap-
proached, and the bottom-line results?all in 200 to
500 words. Thus, using only the abstracts (instead
of full papers) as input to a summarization system is
worth exploring.
Whereas the abstract of a paper presents what the
authors think to be the important contributions of a
paper, the citation text of a paper captures what oth-
ers in the field perceive as the contributions of the
paper. The two perspectives are expected to have
some overlap in their content, but the citation text
also contains additional information not found in ab-
stracts (Elkiss et al, 2008a). For example, how a
particular methodology (described in one paper) was
combined with another (described in a different pa-
per) to overcome some of the drawbacks of each.
A citation text is also an indicator of what contri-
butions described in a paper were more influential
over time. Another distinguishing feature of citation
texts in contrast to abstracts is that a citation text
tends to have a certain amount of redundant informa-
tion. This is because multiple papers may describe
the same contributions of a target paper. This redun-
dancy can be exploited to determine the important
contributions of the target paper.
Our goal is to test the hypothesis that an ef-
fective technical survey will reflect information on
research not only from the perspective of its au-
thors but also from the perspective of others who
use/commend/discredit/add to it. Before describ-
ing our experiments with technical papers, abstracts,
and citation texts, we first summarize relevant prior
work that used these sources of information as input.
3 Related work
Previous work has focused on the analysis of cita-
tion and collaboration networks (Teufel et al, 2006;
Newman, 2001) and scientific article summarization
(Teufel and Moens, 2002). Bradshaw (2003) used
citation texts to determine the content of articles and
improve the results of a search engine. Citation
texts have also been used to create summaries of sin-
gle scientific articles in Qazvinian and Radev (2008)
and Mei and Zhai (2008). However, there is no pre-
vious work that uses the text of the citations to pro-
duce a multi-document survey of scientific articles.
Furthermore, there is no study contrasting the qual-
ity of surveys generated from citation summaries?
both automatically and manually produced?to sur-
veys generated from other forms of input such as the
abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-
gorization to support a system for writing a survey.
Nanba et al (2004a) automatically categorize cita-
tion sentences into three groups using pre-defined
phrase-based rules. Based on this categorization a
survey generation tool is introduced in Nanba et al
(2004b). They report that co-citation (where both
papers are cited by many other papers) implies sim-
ilarity by showing that the textual similarity of co-
cited papers is proportional to the proximity of their
citations in the citing article.
Elkiss et al (2008b) conducted several exper-
iments on a set of 2,497 articles from the free
PubMed Central (PMC) repository.1 Results from
this experiment confirmed that the cohesion of a ci-
tation text of an article is consistently higher than
the that of its abstract. They also concluded that ci-
tation texts contain additional information are more
focused than abstracts.
Nakov et al (2004) use sentences surrounding ci-
tations to create training and testing data for seman-
tic analysis, synonym set creation, database cura-
tion, document summarization, and information re-
trieval. Kan et al (2002) use annotated bibliogra-
phies to cover certain aspects of summarization and
suggest using metadata and critical document fea-
tures as well as the prominent content-based features
to summarize documents. Kupiec et al (1995) use a
statistical method and show how extracts can be used
to create summaries but use no annotated metadata
in summarization.
Siddharthan and Teufel (2007) describe a new
reference task and show high human agreement as
well as an improvement in the performance of ar-
gumentative zoning (Teufel, 2005). In argumenta-
tive zoning?a rhetorical classification task?seven
1http://www.pubmedcentral.gov
586
classes (Own, Other, Background, Textual, Aim,
Basis, and Contrast) are used to label sentences ac-
cording to their role in the author?s argument.
Our aim is not only to determine the utility of cita-
tion texts for survey creation, but also to examine the
quality distinctions between this form of input and
others such as abstracts and full texts?comparing
the results to human-generated surveys using both
automatic and nugget-based pyramid evaluation
(Lin and Demner-Fushman, 2006; Nenkova and Pas-
sonneau, 2004; Lin, 2004).
4 Summarization systems
We used four summarization systems for our
survey-creation approach: Trimmer, LexRank, C-
LexRank, and C-RR. Trimmer is a syntactically-
motivated parse-and-trim approach. LexRank is a
graph-based similarity approach. C-LexRank and C-
RR use graph clustering (?C? stands for clustering).
We describe each of these, in turn, below.
4.1 Trimmer
Trimmer is a sentence-compression tool that extends
the scope of an extractive summarization system by
generating multiple alternative sentence compres-
sions of the most important sentences in target doc-
uments (Zajic et al, 2007). Trimmer compressions
are generated by applying linguistically-motivated
rules to mask syntactic components of a parse of a
source sentence. The rules can be applied iteratively
to compress sentences below a configurable length
threshold, or can be applied in all combinations to
generate the full space of compressions.
Trimmer can leverage the output of any con-
stituency parser that uses the Penn Treebank con-
ventions. At present, the Stanford Parser (Klein and
Manning, 2003) is used. The set of compressions
is ranked according to a set of features that may in-
clude metadata about the source sentences, details of
the compression process that generated the compres-
sion, and externally calculated features of the com-
pression.
Summaries are constructed from the highest scor-
ing compressions, using the metadata and maximal
marginal relevance (Carbonell and Goldstein, 1998)
to avoid redundancy and over-representation of a
single source.
4.2 LexRank
We also used LexRank (Erkan and Radev, 2004), a
state-of-the-art multidocument summarization sys-
tem, to generate summaries. LexRank first builds a
graph of all the candidate sentences. Two candidate
sentences are connected with an edge if the similar-
ity between them is above a threshold. We used co-
sine as the similarity metric with a threshold of 0.15.
Once the network is built, the system finds the most
central sentences by performing a random walk on
the graph.
The salience of a node is recursively defined on
the salience of adjacent nodes. This is similar to
the concept of prestige in social networks, where the
prestige of a person is dependent on the prestige of
the people he/she knows. However, since random
walk may get caught in cycles or in disconnected
components, we reserve a low probability to jump
to random nodes instead of neighbors (a technique
suggested by Langville and Meyer (2006)).
Note also that unlike the original PageRank
method, the graph of sentences is undirected. This
updated measure of sentence salience is called as
LexRank. The sentences with the highest LexRank
scores form the summary.
4.3 Cluster Summarizers: C-LexRank, C-RR
Two clustering methods proposed by Qazvinian and
Radev (2008)?C-RR and C-LexRank?were used
to create summaries. Both create a fully connected
network in which nodes are sentences and edges are
cosine similarities. A cutoff value of 0.1 is applied
to prune the graph and make a binary network. The
largest connected component of the network is then
extracted and clustered.
Both of the mentioned summarizers cluster the
network similarly but use different approaches to se-
lect sentences from different communities. In C-
RR sentences are picked from different clusters in
a round robin (RR) fashion. C-LexRank first calcu-
lates LexRank within each cluster to find the most
salient sentences of each community. Then it picks
the most salient sentence of each cluster, and then
the second most salient and so forth until the sum-
mary length limit is reached.
587
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer
locator Rinaldi et al 2003; Tomuro, 2003. In addition, number of researchers have built systems to take reading
comprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al
1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ? definition ? or ? other ?
questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples. To better facilitate user
information needs, recent trends in QA research have shifted towards complex, context-based, and interactive
question answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005. [And so on.]
Table 1: First few sentences of the QA citation texts survey generated by Trimmer.
5 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and proceed-
ings of ACL conferences and workshops. It has
almost 11,000 papers. To produce the ACL An-
thology Network (AAN), Joseph and Radev (2007)
manually parsed the references before automatically
compiling the network metadata, and generating ci-
tation and author collaboration networks. The AAN
includes all citation and collaboration data within
the ACL papers, with the citation network consist-
ing of 11,773 nodes and 38,765 directed edges.
Our evaluation experiments are on a set of papers
in the research area of Question Answering (QA)
and another set of papers on Dependency parsing
(DP). The two sets of papers were compiled by se-
lecting all the papers in AAN that had the words
Question Answering and Dependency Parsing, re-
spectively, in the title and the content. There were
10 papers in the QA set and 16 papers in the DP set.
We also compiled the citation texts for the 10 QA
papers and the citation texts for the 16 DP papers.
6 Experiments
We automatically generated surveys for both QA
and DP from three different types of documents: (1)
full papers from the QA and DP sets?QA and DP
full papers (PA), (2) only the abstracts of the QA
and DP papers?QA and DP abstracts (AB), and
(3) the citation texts corresponding to the QA and
DP papers?QA and DP citations texts (CT).
We generated twenty four (4x3x2) surveys,
each of length 250 words, by applying Trimmer,
LexRank, C-LexRank and C-RR on the three data
types (citation texts, abstracts, and full papers) for
both QA and DP. (Table 1 shows a fragment of one
of the surveys automatically generated from QA ci-
tation texts.) We created six (3x2) additional 250-
word surveys by randomly choosing sentences from
the citation texts, abstracts, and full papers of QA
and DP. We will refer to them as random surveys.
6.1 Evaluation
Our goal was to determine if citation texts do in-
deed have useful information that one will want to
put in a survey and if so, how much of this infor-
mation is not available in the original papers and
their abstracts. For this we evaluated each of the
automatically generated surveys using two separate
approaches: nugget-based pyramid evaluation and
ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-
ated from the QA and DP citation texts and abstracts,
respectively:2 (1) We asked two impartial judges to
identify important nuggets of information worth in-
cluding in a survey. (2) We asked four fluent speak-
ers of English to create 250-word surveys of the
datasets. Then we determined how well the differ-
ent automatically generated surveys perform against
these gold standards. If the citation texts have only
redundant information with respect to the abstracts
and original papers, then the surveys of citation texts
will not perform better than others.
6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based
evaluation methodology (Lin and Demner-Fushman,
2006; Nenkova and Passonneau, 2004; Hildebrandt
et al, 2004; Voorhees, 2003). We asked three impar-
tial annotators (knowledgeable in NLP but not affil-
iated with the project) to review the citation texts
and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists
2Creating gold standard data from complete papers is fairly
arduous, and was not pursued.
588
of 2?8 ?nuggets,? or main contributions, supplied
by each paper. Each nugget was assigned a weight
based on the frequency with which it was listed by
annotators as well as the priority it was assigned
in each case. Our automatically generated surveys
were then scored based on the number and weight
of the nuggets that they covered. This evaluation ap-
proach is similar to the one adopted by Qazvinian
and Radev (2008), but adapted here for use in the
multi-document case.
The annotators had two distinct tasks for the QA
set, and one for the DP set: (1) extract nuggets for
each of the 10 QA papers, based only on the citation
texts for those papers; (2) extract nuggets for each
of the 10 QA papers, based only on the abstracts of
those papers; and (3) extract nuggets for each of the
16 DP papers, based only on the citation texts for
those papers.3
We obtained a weight for each nugget by revers-
ing its priority out of 8 (e.g., a nugget listed with
priority 1 was assigned a weight of 8) and summing
the weights over each listing of that nugget.4
To evaluate a given survey, we counted the num-
ber and weight of nuggets that it covered. Nuggets
were detected via the combined use of annotator-
provided regular expressions and careful human re-
view. Recall was calculated by dividing the com-
bined weight of covered nuggets by the combined
weight of all nuggets in the nugget set. Precision
was calculated by dividing the number of distinct
nuggets covered in a survey by the number of sen-
tences constituting that survey, with a cap of 1. F-
measure, the weighted harmonic mean of precision
and recall, was calculated with a beta value of 3 in
order to assign the greatest weight to recall. Recall
is favored because it rewards surveys that include
highly weighted (important) facts, rather than just a
3We first experimented using only the QA set. Then to show
that the results apply to other datasets, we asked human anno-
tators for gold standard data on the DP citation texts. Addi-
tional experiments on DP abstracts were not pursued because
this would have required additional human annotation effort to
establish a point we had already made with the QA set, i.e., that
abstracts are useful for survey creation.
4Results obtained with other weighting schemes that ig-
nored priority ratings and multiple mentions of a nugget by a
single annotator showed the same trends as the ones shown by
the selected weighting scheme, but the latter was a stronger dis-
tinguisher among the four systems.
Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 Average
Input: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413
Table 2: Pyramid F-measure scores of human-created
surveys of QA and DP data. The surveys are evaluated
using nuggets drawn from QA citation texts (QA?CT),
QA abstracts (QA?AB), and DP citation texts (DP?CT).
great number of facts.
Table 2 gives the F-measure values of the 250-
word surveys manually generated by humans. The
surveys were evaluated using the nuggets drawn
from the QA citation texts, QA abstracts, and DP ci-
tation texts. The average of their scores (listed in the
rightmost column) may be considered a good score
to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys
generated by the four automatic summarizers, evalu-
ated using nuggets drawn from the QA citation texts,
QA abstracts, and DP citation texts. The table also
includes results for the baseline random summaries.
When we used the nuggets from the abstracts
set for evaluation, the surveys created from ab-
stracts scored higher than the corresponding surveys
created from citation texts and papers. Further, the
best surveys generated from citation texts outscored
the best surveys generated from papers. When we
used the nuggets from citation sets for evaluation,
the best automatic surveys generated from citation
texts outperform those generated from abstracts and
full papers. All these pyramid results demonstrate
that citation texts can contain useful information that
is not available in the abstracts or the original papers,
and that abstracts can contain useful information that
is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-
mer performed best at this task, in two cases ex-
ceeding the average human performance. Note also
that the random summarizer outscored the automatic
summarizers in cases where the nuggets were taken
from a source different from that used to generate
the survey. However, one or two summarizers still
tended to do well. This indicates a difficulty in ex-
589
System Performance: Pyramid F-measure
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT nuggets 0.321 0.434 0.268 0.295 0.616
QA?AB nuggets 0.305 0.388 0.349 0.320 0.543
Input: QA abstract surveys
QA?CT nuggets 0.452 0.383 0.480 0.441 0.404
QA?AB nuggets 0.623 0.484 0.574 0.606 0.622
Input: QA full paper surveys
QA?CT nuggets 0.239 0.446 0.299 0.190 0.199
QA?AB nuggets 0.294 0.520 0.387 0.301 0.290
Input: DP citation surveys
DP?CT nuggets 0.219 0.231 0.170 0.372 0.136
Input: DP abstract surveys
DP?CT nuggets 0.321 0.301 0.263 0.311 0.312
Input: DP full paper surveys
DP?CT nuggets 0.032 0.000 0.144 * 0.280
Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets
drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).
Human Performance: ROUGE-2human1 human2 human3 human4 average
Input: QA citation surveysQA?CT refs. 0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs. 0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs. 0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs. 0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs. 0.1550 0.1259 0.1200 0.1654 0.1416
Table 4: ROUGE-2 scores obtained for each of the manu-
ally created surveys by using the other three as reference.
ROUGE-1 and ROUGE-L followed similar patterns.
tracting the overlapping survey-worthy information
across the two sources.
6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of
each of human-generated 250-word surveys against
each other. The average (last column) is what the au-
tomatic surveys can aim for. We then evaluated each
of the random surveys and those generated by the
four summarization systems against the references.
Table 5 lists ROUGE scores of surveys when the
manually created 250-word survey of the QA cita-
tion texts, survey of the QA abstracts, and the survey
of the DP citation texts, were used as gold standard.
When we use manually created citation text
surveys as reference, then the surveys gener-
ated from citation texts obtained significantly bet-
ter ROUGE scores than the surveys generated from
abstracts and full papers (p < 0.05) [RESULT 1].
This shows that crucial survey-worthy information
present in citation texts is not available, or hard to
extract, from abstracts and papers alone. Further,
the surveys generated from abstracts performed sig-
nificantly better than those generated from the full
papers (p < 0.05) [RESULT 2]. This shows that ab-
stracts and citation texts are generally denser in sur-
vey worthy information than full papers.
When we use manually created abstract sur-
veys as reference, then the surveys generated
from abstracts obtained significantly better ROUGE
scores than the surveys generated from citation texts
and full papers (p < 0.05) [RESULT 3]. Further, and
more importantly, the surveys generated from cita-
tion texts performed significantly better than those
generated from the full papers (p < 0.05) [RESULT
4]. Again, this shows that abstracts and citation texts
are richer in survey-worthy information. These re-
sults also show that abstracts of papers and citation
texts have some overlapping information (RESULT
2 and RESULT 4), but they also have a signifi-
cant amount of unique survey-worthy information
(RESULT 1 and RESULT 3).
Among the automatic summarizers, C-LexRank
and LexRank perform best. This is unlike the results
found through the nugget-evaluation method, where
Trimmer performed best. This suggests that Trim-
590
System Performance: ROUGE-2
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT refs. 0.11561 0.17013 0.09522 0.13501 0.16984
QA?AB refs. 0.08264 0.11653 0.07600 0.07013 0.10336
Input: QA abstract surveys
QA?CT refs. 0.04516 0.05892 0.06149 0.05369 0.04114
QA?AB refs. 0.12085 0.13634 0.12190 0.20311 0.13357
Input: QA full paper surveys
QA?CT refs. 0.03042 0.03606 0.03599 0.28244 0.03986
QA?AB refs. 0.04621 0.05901 0.04976 0.10540 0.07505
Input: DP citation surveys
DP?CT refs. 0.10690 0.13164 0.08748 0.04901 0.10052
Input: DP abstract surveys
DP?CT refs. 0.07027 0.07321 0.05318 0.20311 0.07176
Input: DP full paper surveys
DP?CT refs. 0.03770 0.02511 0.03433 * 0.04554
Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human
references created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT). These
results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).
mer is better at identifying more useful nuggets of
information, but C-LexRank and LexRank are bet-
ter at producing unigrams and bigrams expected in
a survey. To some extent this may be due to the fact
that Trimmer uses smaller (trimmed) fragments of
source sentences in its summaries.
7 Conclusion
In this paper, we investigated the usefulness of di-
rectly summarizing citation texts (sentences that cite
other papers) in the automatic creation of technical
surveys. We generated surveys of a set of Ques-
tion Answering (QA) and Dependency Parsing (DP)
papers, their abstracts, and their citation texts us-
ing four state-of-the-art summarization systems (C-
LexRank, C-RR, LexRank, and Trimmer). We then
used two separate approaches, nugget-based pyra-
mid and ROUGE, to evaluate the surveys. The re-
sults from both approaches and all four summa-
rization systems show that both citation texts and
abstracts have unique survey-worthy information.
These results also demonstrate that, unlike single
document summarization (where citing sentences
have been suggested to be inappropriate (Teufel
et al, 2006)), multidocument summarization?
especially technical survey creation?benefits con-
siderably from citation texts.
We next plan to generate surveys using both cita-
tion texts and abstracts together as input. Given the
overlapping content of abstracts and citation texts,
discovered in the current study, it is clear that re-
dundancy detection will be an integral component of
this future work. Creating readily consumable sur-
veys is a hard task, especially when using only raw
text and simple summarization techniques. There-
fore we intend to combine these summarization and
bibliometric techniques with suitable visualization
methods towards the creation of iterative technical
survey tools?systems that present surveys and bib-
liometric links in a visually convenient manner and
which incorporate user feedback to produce even
better surveys.
Acknowledgments
This work was supported, in part, by the National
Science Foundation under Grant No. IIS-0705832
(iOPENER: Information Organization for PENning
Expositions on Research) and Grant No. 0534323
(Collaborative Research: BlogoCenter - Infrastruc-
ture for Collecting, Mining and Accessing Blogs),
in part, by the Human Language Technology Cen-
ter of Excellence, and in part, by the Center for Ad-
vanced Study of Language (CASL). Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsors.
591
References
Shannon Bradshaw. 2003. Reference directed indexing:
Redeeming relevance for subject search in citation in-
dexes. In Proceedings of the 7th European Conference
on Research and Advanced Technology for Digital Li-
braries.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 335?336, Melbourne, Australia.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008a. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008b. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Overview of the trec 2003 question-answering track.
In Proceedings of the 2004 Human Language Tech-
nology Conference and the North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT/NAACL 2004).
Mark Joseph and Dragomir Radev. 2007. Citation analy-
sis, centrality, and the ACL Anthology. Technical Re-
port CSE-TR-535-07, University of Michigan. Dept.
of Electrical Engineering and Computer Science.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics (2nd edition). Prentice-Hall.
Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 2002. Using the Annotated Bibliography as a
Resource for Indicative Summarization. In Proceed-
ings of LREC 2002, Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95,
pages 68?73, New York, NY, USA. ACM.
Amy Langville and Carl Meyer. 2006. Google?s PageR-
ank and Beyond: The Science of Search Engine Rank-
ings. Princeton University Press.
Jimmy J. Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to complex
questions. Information Retrieval, 9(5):565?587.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
workshop on Text Summarization Branches Out.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In Workshop on Search and
Discovery in Bioinformatics.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference informa-
tion. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,
and Suguru Saito. 2004a. Bilingual presri: Integration
of multiple research paper databases. In Proceedings
of RIAO 2004, pages 195?211, Avignon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.
2004b. Classification of research papers using cita-
tion links and citation types: Towards automatic re-
view article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
Proceedings of EMNLP, pages 103?110, Australia.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and Af-
fect in Text: Theory and Applications, pages 159?170.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003).
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing and Management (Spe-
cial Issue on Summarization).
592
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 54?61,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
The ACL Anthology Network Corpus 
 
Dragomir R. Radev1,2, Pradeep Muthukrishnan1, Vahed Qazvinian1 
1Department of Electrical Engineering and Computer Science 
2School of Information 
University of Michigan 
{radev,mpradeep,vahed}@umich.edu 
 
Abstract 
We introduce the ACL Anthology Net-
work (AAN), a manually curated net-
worked database of citations, 
collaborations, and summaries in the field 
of Computational Linguistics. We also 
present a number of statistics about the 
network including the most cited authors, 
the most central collaborators, as well as 
network statistics about the paper citation, 
author citation, and author collaboration 
networks. 
1 Introduction 
The ACL Anthology is one of the most 
successful initiatives of the ACL. It was in-
itiated by Steven Bird and is now maintained 
by Min Yen Kan. It includes all papers pub-
lished by ACL and related organizations as 
well as the Computational Linguistics journal 
over a period of four decades. It is available 
at http://www.aclweb.org/anthology-new/ .  
One fundamental problem with the ACL 
Anthology, however, is the fact that it is just 
a collection of papers. It doesn?t include any 
citation information or any statistics about the 
productivity of the various researchers who 
contributed papers to it. We embarked on an 
ambitious initiative to manually annotate the 
entire Anthology in order to make it possible 
to compute such statistics.  
In addition, we were able to use the anno-
tated data for extracting citation summaries of 
all papers in the collection and we also anno-
tated each paper by the gender of the authors 
(and are currently in the process of doing si-
milarly for their institutions) in the goal of 
creating multiple gold standard data sets for 
training automated systems for performing 
such tasks.  
2 Curation 
The ACL Anthology includes 13,739 pa-
pers (excluding book reviews and posters). 
Each of the papers was converted from pdf to 
text using an OCR tool (www.pdfbox.org). 
After this conversion, we extracted the refer-
ences semi-automatically using string match-
ing. The above process outputs all the 
references as a single block so we then ma-
nually inserted line breaks between refer-
ences. These references were then manually 
matched to other papers in the ACL Antholo-
gy using a ?k-best? (with k = 5) string match-
ing algorithm built into a CGI interface. A 
snapshot of this interface is shown in Figure 
1. The matched references were stored to-
gether to produce the citation network. Refer-
ences to publications outside of the AAN 
were recorded but not included in the net-
work. 
 In order to fix the issue of wrong author 
names and multiple author identities we had 
to perform a lot of manual post-processing. 
The first names and the last names were 
swapped for a lot of authors. For example, the 
author name "Caroline Brun" was present as 
"Brun Caroline" in some of her papers. 
Another big source of error was the exclusion 
of middle names or initials in a number of 
papers. For example, Julia Hirschberg had 
two identities as "Julia Hirschberg" and "Julia 
B. Hirschberg". There were a few spelling 
mistakes, like "Madeleine Bates" was miss-
pelled as "Medeleine Bates". 
Finally, many papers included incorrect 
titles in their citation sections. Some used the 
wrong years and/or venues as well. 
 
54
 Figure 1: CGI interface used for matching new references to existing papers 
 
 
Figure 2: Snapshot of the different statistics computed for an author 
55
                  Figure 3: Snapshot of the different statistics for a paper 
 
3 Statistics 
 
Using the metadata and the citations ex-
tracted after curation, we have built three dif-
ferent networks.   
The paper citation network is a directed 
network with each node representing a paper 
labeled with an ACL ID number and the 
edges representing a citation within that paper 
to another paper represented by an ACL ID. 
The paper citation network consists of 13,739 
papers and 54,538 citations.   
The author citation network and the author 
collaboration network are additional networks 
derived from the paper citation network. In 
both of these networks a node is created for 
each unique author. In the author citation 
network an edge is an occurrence of an author 
citing another author. For example, if a paper 
written by Franz Josef Och cites a paper writ-
ten by Joshua Goodman, then an edge is 
created between Franz Josef Och and Joshua 
Goodman. Self citations cause self loops in 
the author citation network. The author cita-
tion network consists of 11,180 unique au-
thors and 332,815 edges (196,905 edges if 
duplicates are removed). 
In the author collaboration network, an 
edge is created for each collaboration. For 
example, if a paper is written by Franz Josef 
Och and Hermann Ney, then an edge is 
created between the two authors.  
Table 1 shows some brief statistics about 
the first two releases of the data set (2006 and 
2007). Table 2 describes the most current re-
lease of the data set (from 2008).  
 
2006 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 8898 7849 7849 
m 8765 137,007 41,362 
2007 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 9767 9421 9421 
m 44,142 158,479 45,878 
       Table 1: Growth of citation volume 
 
 
 
 
Paper 
Citation 
Network  
Author 
Citation 
Network  
Author 
Collaboration 
Network 
Nodes  13,739  10,409 10,409 
Edges  54,538  195,505 57,614 
Diameter  22  10  20 
Average 9.34  43.11  11.07 
56
Degree  
Largest 
Connected 
Component  
11,409  9061  7910 
Watts Stro-
gatz cluster-
ing 
coefficient 
0.18 0.46 0.65 
Newman 
clustering 
coefficient 
0.07 0.14 0.36 
clairlib avg. 
directed 
shortest 
path  
5.91 3.32 5.87 
Ferrer avg. 
directed 
shortest 
path  
5.35 3.29 4.66 
harmonic 
mean geo-
desic dis-
tance  
63.93 5.47 9.40 
harmonic 
mean geo-
desic dis-
tance with 
self-loops 
counted 
63.94 5.47 9.40 
     Table 2: Network Statistics of the cita-
tion and collaboration network. The re-
maining authors (11,180-10,409) are not 
cited and are therefore removed from the 
network analysis
 
 Paper 
Citation 
Network 
Author 
Citation 
Network 
Author 
Collaboratio
n Network 
In-degree Stats 
Power Law 
Exponent 
2.50 2.20 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.00 1.55 2.18 
Out-degree stats 
Power Law 
Exponent 
3.70 2.56 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.12 1.54 2.18 
Total Degree Stats 
Power Law 
Exponent 
2.72 2.27 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
1.81 1.46 2.18 
Table 3: Degree Statistics of the citation 
and collaboration networks 
 
A lot of different statistics have been 
computed based on the data set release in 
2007 by Radev et al The statistics include 
PageRank scores which eliminate PageRank's 
inherent bias towards older papers, Impact 
factor, correlations between different meas-
ures of impact like H-Index, total number of 
incoming citations, PageRank. They also re-
port results from a regression analysis using 
H-Index scores from different sources (AAN, 
Google Scholar) in an attempt to identify 
multi-disciplinary authors.  
4 Sample rankings 
This section shows some of the rankings 
that were computed using AAN. 
57
 Rank Icit Title 
1 590 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
2 444 The Mathematics Of Statistical 
Machine Translation: Parameter 
Estimation 
3 324 Attention Intentions And The 
Structure Of Discourse 
4 271 A Maximum Entropy Approach 
To Natural Language Processing 
5 270  Bleu: A Method For 
Automatic Evaluation Of 
6 246  A Maximum-Entropy-Inspired 
Parser 
7 230 A Stochastic Parts Program And 
Noun Phrase Parser For 
Unrestricted Text 
8 221 A Systematic Comparison Of 
Various  Statistical Alignment 
9 211 A Maximum Entropy Model For 
Part-Of-Speech Tagging 
10 211 Three Generative Lexicalized 
Models For Statistical Parsing 
Table 4: Papers with the most incoming 
citations (icit) 
Rank PR Title 
1 1099.1 A Stochastic Parts Program 
And Noun Phrase Parser For 
Unrestricted Text 
2 943.8 Finding Clauses In Unrestricted 
Text By Finitary And 
Stochastic Methods 
3 568.8 A Stochastic Approach To 
Parsing 4 543.1 A Statistical Approach To 
Machine Translation 
5 414.1 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
6 364.9 The Mathematics Of Statistical 
Machine Translation: Parameter  
Estimation 
7 362.2 The Contribution Of Parsing To 
Prosodic Phrasing In An 
Experimental  
Text-To-Speech System 
8 301.6 Attention Intentions And The 
Structure Of Discourse 
9 250.5 Bleu: A Method For Automatic 
Evaluation Of Machine 
Translation 
10 242.5 A Maximum Entropy Approach 
To Natural Language 
Processing 
 Table 5: Papers with highest PageRank 
(PR) scores 
It must be noted that the PageRank scores 
are not accurate because of the lack of cita-
tions outside AAN. Specifically, out of the 
155,858 total number of citations, only 
54,538 are within AAN. 
 
  Rank Icit Author Name 
1 (1) 3886 (3815) Och, Franz Josef 
2 (2) 3297 (3119) Ney, Hermann 
3 (3) 3067 (3049) Della Pietra, Vincent J. 
4 (5) 2746 (2720) Mercer, Robert L. 
5 (4) 2741 (2724) Della Pietra, Stephen 
A. 6 (6) 2605 (2589) Marcus, Mitchell P. 
7 (8) 2454 (2407) Collins, Michael John 
8 (7) 2451 (2433) Brown, Peter F. 
9 (9) 2428 (2390)  Church, Kenneth Ward 
10 (10) 2047 (1991) Marcu, Daniel 
      Table 6: Authors with most incoming 
citations (the values in parentheses are us-
ing non-self- citations) 
 
Rank h Author Name 
1 18 Knight, Kevin 
2 16 Church, Kenneth Ward 
3 15 Manning, Christopher D. 
3 15 Grishman, Ralph 
3 15 Pereira, Fernando C. N. 
6 14 Marcu, Daniel 
6 14 Och, Franz Josef 
6 14 Ney, Hermann 
6 14 Joshi, Aravind K. 
6 14 Collins, Michael John 
      Table 7: Authors with the highest h-
index 
 
Rank ASP Author Name 
1  2.977 Hovy, Eduard H. 
2  2.989 Palmer, Martha Stone 
3  3.011 Rambow, Owen 
4  3.033 Marcus, Mitchell P. 
5  3.041 Levin, Lori S. 
6  3.052 Isahara, Hitoshi 
7  3.055 Flickinger, Daniel P. 
8  3.071 Klavans, Judith L. 
9  3.073 Radev, Dragomir R. 
10 3.077 Grishman, Ralph 
Table 8: Authors with the least average  
shortest path (ASP) length in the author 
collaboration network 
 
 
58
5 Related phrases 
We have also computed the related phras-
es for every author using the text from the 
papers they have authored, using the simple 
TF-IDF scoring scheme (see Figure 4).  
 
 
Figure 4: Snapshot of the related phrases 
for Franz Josef Och 
6 Citation summaries 
The citation summary of an article, P, is 
the set of sentences that appear in the litera-
ture and cite P. These sentences usually men-
tion at least one of the cited paper?s contribu-
tions. We use AAN to extract the citation 
summaries of all articles, and thus the citation 
summary of P is a self-contained set and only 
includes the citing sentences that appear in 
AAN papers. Extraction is performed auto-
matically using string-based heuristics by 
matching the citation pattern, author names 
and publication year, within the sentences. 
The following example shows the citation 
summary extracted for ?Koo, Terry, Carreras, 
Xavier, Collins, Michael John, Simple Semi-
supervised Dependency Parsing". The cita-
tion summary of (Koo et al, 2008) mentions 
KCC08, dependency parsing, and the use of 
word clustering in semi-supervised NLP. 
 
 
 
 
 
 
 
 
 
 
 
 
C08-1051 1 7:191 Furthermore, recent studies revealed that word clustering is useful for semi-supervised learn-
ing in NLP (Miller et al, 2004; Li and McCallum, 2005; Kazama and Torisawa, 2008; Koo et al, 2008). 
 
D08-1042 2 78:214 There has been a lot of progress in learning dependency tree parsers (McDonald et al, 2005; 
Koo et al, 2008; Wang et al, 2008). 
 
W08-2102 3 194:209 The method shows improvements over the method described in (Koo et al, 2008), which 
is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting 
that the incorporation of constituent structure can improve dependency accuracy. 
 
W08-2102 4 32:209 The model also recovers dependencies with significantly higher accuracy than state-of-the-
art dependency parsers such as (Koo et al, 2008; McDonald and Pereira, 2006). 
 
W08-2102 5 163:209 KCC08 unlabeled is from (Koo et al, 2008), a model that has previously been shown to 
have higher accuracy than (McDonald and Pereira, 2006). 
 
W08-2102 6 164:209 KCC08 labeled is the labeled dependency parser from (Koo et al, 2008); here we only 
evaluate the unlabeled accuracy. 
 
Figure 5: Sample citation summary 
 
59
 Figure 6: Snapshot of the citation summary for a paper 
 
 
The citation text that we have extracted for 
each paper is a good resource to generate 
summaries of the contributions of that paper. 
We have previously developed systems using 
clustering the similarity networks to generate 
short, and yet informative, summaries of in-
dividual papers (Qazvinian and Radev 2008), 
and more general scientific topics, such as 
Dependency Parsing, and Machine Transla-
tion (Radev et al 2009) . 
 
 
7 Gender annotation 
We have manually annotated the gender of 
most authors in AAN using the name of the 
author. If the gender cannot be identified 
without any ambiguity using the name of the 
author, we resorted to finding the homepage 
of the author. We have been able to annotate 
8,578 authors this way: 6,396 male and 2,182 
female.  
 
8 Downloads 
The following files can be downloaded: 
Text files of the paper: The raw text files 
of the papers after converting them from pdf 
to text is available for all papers. The files are 
named by the corresponding ACL ID. 
Metadata: This file contains all the meta-
data associated with each paper. The metada-
ta associated with every paper consists of the 
paper id, title, year, venue. 
Citations: The paper citation network indi-
cating which paper cites which other paper. 
Figure 7 includes some examples.  
 
 
id = {C98-1096} 
author = {Jing, Hongyan; McKeown, Kathleen R.} 
title = {Combining Multiple, Large-Scale Resources in a Reusable Lexicon for Natural Language Genera-
tion} 
venue = {International Conference On Computational Linguistics} 
year = {1998} 
 
id = {J82-3004} 
author = {Church, Kenneth Ward; Patil, Ramesh} 
title = {Coping With Syntactic Ambiguity Or How To Put The Block In  The Box On The Table} 
venue = {American Journal Of Computational Linguistics} 
year = {1982} 
60
 A00-1001 ==> J82-3002 
A00-1002 ==> C90-3057 
C08-1001 ==> N06-1007 
     C08-1001 ==> N06-1008 
 
Figure 7: Sample contents of the downloadable corpus 
 
We also include a large set of scripts 
which use the paper citation network and the 
metadata file to output the auxiliary networks 
and the different statistics. 
The scripts are documented here:  
http://clair.si.umich.edu/ .The data set has 
already been downloaded from 2,775 unique 
IPs since June 2007. Also, the website has 
been very popular based on access statistics. 
There have been more than 2M accesses in 
2009.  
References  
Vahed Qazvinian and Dragomir R. Radev. Scien-
tific paper summarization using citation sum-
mary networks. In COLING 2008, Manchester, 
UK, 2008. 
Dragomir R. Radev, Mark Joseph, Bryan Gibson, 
and Pradeep Muthukrishnan. A Bibliometric 
and Network Analysis of the Field of Computa-
tional Linguistics. JASIST, 2009 to appear. 
61
Proceedings of the TextGraphs-6 Workshop, pages 42?50,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Simultaneous Similarity Learning and Feature-Weight Learning for
Document Clustering
Pradeep Muthukrishnan
Dept of CSE,
University of Michigan
mpradeep@umich.edu
Dragomir Radev
School of Information,
Dept of CSE,
University of Michigan
radev@umich.edu
Qiaozhu Mei
School of Information,
Dept of CSE,
University of Michigan
qmei@umich.edu
Abstract
A key problem in document classification and
clustering is learning the similarity between
documents. Traditional approaches include
estimating similarity between feature vectors
of documents where the vectors are computed
using TF-IDF in the bag-of-words model.
However, these approaches do not work well
when either similar documents do not use the
same vocabulary or the feature vectors are not
estimated correctly.
In this paper, we represent documents and
keywords using multiple layers of connected
graphs. We pose the problem of simultane-
ously learning similarity between documents
and keyword weights as an edge-weight regu-
larization problem over the different layers of
graphs. Unlike most feature weight learning
algorithms, we propose an unsupervised algo-
rithm in the proposed framework to simulta-
neously optimize similarity and the keyword
weights. We extrinsically evaluate the perfor-
mance of the proposed similarity measure on
two different tasks, clustering and classifica-
tion. The proposed similarity measure out-
performs the similarity measure proposed by
(Muthukrishnan et al, 2010), a state-of-the-
art classification algorithm (Zhou and Burges,
2007) and three different baselines on a vari-
ety of standard, large data sets.
1 Introduction
The recent upsurge in the amount of text available
due to the widespread growth of the Internet has led
to the need for large scale, efficient Machine Learn-
ing (ML), Information Retrieval (IR) tools for text
mining. At the heart of many of the ML, IR algo-
rithms is the need for a good similarity measure be-
tween documents. For example, a better similarity
measure almost always leads to better performance
in tasks like document classification, clustering, etc.
Traditional approaches represent documents with
many keywords using a simple feature vector de-
scription. Then, similarity between two documents
is estimated using the dot product between their
corresponding vectors. However, such similarity
measures do not use all the keywords together and
hence, suffer from problems due to sparsity. There
are two major issues in computing similarity be-
tween documents
? Similar documents may not use the same vo-
cabulary.
? Estimating feature weights or weight of a key-
word to the document it is contained in.
For example, consider two publications, X and
Y , in the field of Machine Learning. Let X be a
paper on clustering while Y is on classification. Al-
though the two publications use very different vo-
cabulary, they are semantically similar. Keyword
weights are mostly estimated using the frequency of
the keyword in the document. For example, TF-IDF
based scoring is the most commonly used approach
to compute keyword weights to compute similarity
between documents. However, suppose publications
X and Y mention the keyword ??Machine Learn-
ing?? only once. Although, they are mentioned only
once in the text of the document, for the purposes
of computing semantic similarity between the docu-
42
ments, it would be beneficial to give it a high key-
word weight.
A commonly used approach to estimate seman-
tic similarity between documents is to use an ex-
ternal knowledge source like WordNet (Pedersen
et al, 2004). However, these approaches are do-
main dependent and language dependent. If docu-
ment similarity can not be estimated accurately us-
ing just the text, there have been approaches incor-
porating multiple sources of similarity like link sim-
ilarity, authorship similarity between publications
(Bach et al, 2004; Cortes et al, 2009). (Muthukr-
ishnan et al, 2010) also uses multiple sources of
similarity. In addition to improving similarity es-
timates between documents, it also improves sim-
ilarity estimates between keywords. Co-clustering
(Dhillon et al, 2003) based approaches are used
to alleviate problems due to the sparsity and high-
dimensionality of the data. In co-clustering, the key-
words and the documents are simultaneously clus-
tered by exploiting the duality between them. How-
ever, the approach relies solely on the keyword dis-
tributions to cluster the documents and vice-versa.
It does not take into account the inherent similar-
ity between the keywords (documents) when cluster-
ing the documents (keywords). Also, co-clustering
takes as input the weight of all keywords to corre-
sponding documents.
2 Motivation
First, we explain how similarity learning and fea-
ture weight learning can mutually benefit from each
other using an example. For example, consider the
following three publications in the field of Machine
Translation, (Brown et al, 1990; Gale and Church,
1991; Marcu and Wong, 2002)
Clearly, all the papers belong to the field of Ma-
chine Translation but (Gale and Church, 1991) con-
tains the phrase ??Machine Translation?? only once
in the entire text. However, we can learn to attribute
some similarity between (Brown et al, 1990) and
the second publication using the text in (Marcu and
Wong, 2002). The keywords ??Bilingual Corpora??
and ??Machine Translation?? co-occur in the text in
(Marcu andWong, 2002) which makes the keywords
themselves similar. Now we can attribute some sim-
ilarity between the (Brown et al, 1990) and (Marcu
andWong, 2002) publication since they contain sim-
ilar keywords. This shows how similarity learning
can benefit from important keywords.
Now, assume that ??Machine Translation?? is an
important keyword (high keyword weight) for the
first and third publication while ??Bilingual Cor-
pora?? is an important keyword for the second pub-
lication. We explained how to infer similarity be-
tween the first and second publication using the third
publication as a bridge. Using the newly learned
similarity measure, we can infer that ??Bilingual
Corpora?? is an important keyword for the sec-
ond publication since a similar keyword (??Machine
Translation??) is an important keyword for similar
publications.
Let documents Di and Dj contain keywords Kik
and Kjl. Then intuitively, the similarity between
two documents should be jointly proportional to
? The similarity between keywords Kik and Kjl
? The weights of Kik to Di and Kjl to Dj .
Similarly the weight of a keyword Kik to docu-
ment Di should be jointly proportional to
? The similarity between documents Di and Dj .
? The similarity between keyphrases Kik and
Kjl and weight of Kjl to Dj .
The major contributions of the paper are given be-
low,
? A rich representation model for representing
documents with associated keywords for effi-
ciently estimating document similarity..
? A regularization framework for joint feature
weight (keyword weight) learning and similar-
ity learning.
? An unsupervised algorithm in the proposed
framework to efficiently learn similarity be-
tween documents and the weights of keywords
for each document in a set of documents.
In the next two sections, we formalize and ex-
ploit this observation to jointly optimize similarity
between documents and weight of keywords to doc-
uments in a principled way.
43
3 Problem Formulation
We assume that a set of keywords have been ex-
tracted for the set of documents to be analyzed. The
setup is very general: Documents are represented
by the set of candidate keywords. In addition to
that, we have crude initial similarities estimated
between documents and also between keywords
and the weights of keywords to documents. The
similarities and keyword weights are represented
using two layers of graphs. We formally define the
necessary concepts,
Definition 1: Documents and corresponding
keywords
We have a set of N documents D =
{d1, d2, . . . , dN}. Each document, di has a set
of mi keywords Ki = {ki1, ki2, . . . , kimi}
Definition 2: Document Similarity Graph
The document similarity graph, G1 = (V1, E1),
consists of the set of documents as nodes and the
edge weights represent the initial similarity between
the documents.
Definition 3: Keyword Similarity Graph
The keyword similarity graph, G2 = (V2, E2), con-
sists of the set of keywords as nodes and the edge
weights represent the initial similarity between the
keywords.
The document similarity graph and the keyword
similarity graph can be considered as two layers of
graphs which are connected by the function defined
below
Definition 4: keyword Weights (KW)
There exists an edge between di and kij for 1 ? j ?
mi. Let Z represent the keyword weighting func-
tion, i.e, Zdi,kij represents the weight of keyword
kij t document di.
4 Regularization Framework
?(w,Z) = ?0 ? ISC(w,w?) + ?1 ? IKC(Z,Z?)
+?2 ?KS(w,Z) + ?3 ? SK(Z,w) (1)
where ?0 + ?1 + ?2 + ?3 = 1.
ISC refers to Initial Similarity Criterion and IKC
refers to Initial Keyword weight Criterion. They are
defined as follows
ISC(w,w?) =
?
u,v?G1
(wu,v ? w?u,v)
2 (2)
IKC(Z,Z?) =
?
u?G1,v?G2
(Zu,v ? Z?u,v)
2 (3)
KS refers toKeyword based Similarity and SK refers
to Similarity induced Keyword weight. They are de-
fined as follows
KS(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
Zu1,u2Zv1,v2
(wu1,v1 ? wu2,v2)
2 (4)
and
SK(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
wu1,v1wu2,v2
(Zu1,u2 ? Zv1,v2)
2 (5)
Then the task is to minimize the objective function.
The objective function consists of four parts. The
first and second parts are initial similarity criterion
and initial keyword criterion. They ensure that the
optimized edge weights are close to the initial edge
weights. The weights ?0 and ?1 ensure that the op-
timized weights are close to the initial weights, in
other words, they represent the confidence level in
initial weights.
The significance of the third and the fourth parts
of the objective function are best explained by a sim-
ple example. Consider two graphs, G1 and G2. Let
G1 be the graph containing publications as nodes
and edge weights representing initial similarity val-
ues. Let G2 be the graph corresponding to keywords
and edge weights represent similarity between key-
words. There is an edge from a node u1 in G1 to a
node v1 in G2 if the publication corresponding to u1
contains the keyword corresponding to v1.
According to this example, minimizing the key-
word weight induced similarity part corresponds to
updating similarity values between keywords in pro-
portion to weights of the keywords to the respective
documents they are contained in and the similarity
between the documents. keyword weight induced
similarity part also helps updating similarity values
44
between documents in proportion to weights of key-
words they contain and the similarity between the
contained keywords.
Minimizing the similarity induced keyword part
corresponds to updating keyword weights in propor-
tion to the following
? Similarity between v1 and other keywords v2 ?
G2
? Keyword weight of v2 to documents u2 ? G1
? Similarity between u1 and u2
Therefore, even if frequency of a keyword such
as ??Machine Translation?? in a publication is not
high, it can achieve a high keyword weight if it con-
tains many other similar keywords such as ??Bilin-
gual Corpora?? and ??Word alignment??.
5 Efficient Algorithm
We seek to minimize the objective function using
Alternating Optimization (AO) (Bezdek and Hath-
away, 2002), an approximate optimization method.
Alternating optimization is an iterative procedure for
minimizing (or maximizing) the function f(x) =
f(X1, X2, . . . , Xt) jointly over all variables by al-
ternating restricted minimizations over the individ-
ual subsets of variables X1, . . . , Xt.
In this optimization method, we partition the set
of variables into a set of mutually exclusive, exhaus-
tive subsets. We iteratively perform minimizations
over each subset of variables while maintaining the
other subsets of variables fixed. Formally, let the set
of real-valued variables be X = {X1, X2, . . . , XN}
be partitioned into m subsets, {Y1, Y2, . . . , Ym}.
Also, let si = |Yi|. Then we begin with the ini-
tial set of values {Y 01 , Y20, . . . , Ym0} and make re-
stricted minimizations of the following form,
min
Yi?Rsi
{f(Y1r+1, . . . , Yi?1r+1, Yi, Yi+1r, . . . , Ymr)}
(6)
where i = 1, 2, . . . ,m. The underline notation Yj
indicates that the subset of variables Yj are fixed
with respect to Yi. In the context of our prob-
lem, we update each edge weight while maintaining
other edge weights to be a constant. Then the prob-
lem boils down to the minimization problem over a
single edge weight. For example, let us solve the
minimization problem for edge weight correspond-
ing to (ui, vj) where ui, vj ? G1 (The case where
ui, vj ? G2 is analogous). Clearly the objective
function is a convex function in w(ui, vj). The par-
tial derivative of the objective function with respect
to the edge weight is given below,
??(w,Z)
?wui,vj
= 2?0(wui,vj ? w?ui,vj )
+2?2 ?
?
u2,v2?G2
(wui,vj ? wu2,v2)Zu1,u2Zvj ,v2
+?3 ?
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wui,vjwu2,v2
. (7)
To minimize the above function, we set the partial
derivative to zero which gives us the following ex-
pression,
wuj ,vk =
1
C1
(?0w?ui,vj +
?2
?
u2,v2?G2
Zui,u2 wu2,v2 Zvj ,v2)(8)
where
C1 = ?0 + ?2
?
u2,v2?G2
Zui,u2 Zvj ,v2
+?3
2
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wu2,v2
Similarly, we can derive the update equation for
keyword weights, Zui,uj as below,
Zui,uj =
1
C2
(?1Z?ui,uj +
?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2 Zv1,v2)
(9)
where,
C2 = ?1 + ?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2
+?2
2
?
v1?G1
?
v2?G2
(wui,v1 ? wuj ,v2)
2Zv1,v2
45
The similarity score between two nodes is propor-
tional to the similarity between nodes in the other
layer. For example, the similarity between two doc-
uments (keywords) is proportional to the similarity
between the keywords the documents they contain
(the documents they are contained in). C plays the
role of a normalization constant. Therefore, for sim-
ilarity between two nodes to be high, it is required
that they not only contain a lot of similar nodes in
the other graph but the similar nodes need to be im-
portant to the two target nodes.
Similarly, a particular keyword will have a high
weight to a document if similar keywords have high
weights to similar documents. Also, it is neces-
sary that the similarity between the keywords and
the documents are high.
It can be shown that equations 8 and 9 converge
q? linearly since the minimization problem is con-
vex in each of the variables individually and hence
has a global and unique minimizer (Bezdek and
Hathaway, 2002).
5.1 Layered Random Walk Interpretation
The above algorithm has a very nice intuitive inter-
preation in terms of random walks over the two dif-
ferent graphs. Assume the initial weights are transi-
tion probability values after the graphs are normal-
ized so that each row of the adjacency matrices sums
to 1. Then the similarity between two nodes u and v
in the same graph is computed as sum of two parts.
The first part is ?0 times the initial similarity. This
is necessary so that the optimized similarity values
are not too far away from the initial similarity val-
ues. The second part corresponds to the probability
of a random walk of length 3 starting at u and reach-
ing v through two intermediate nodes from the other
graph.
The semantics of the random walk depends on
whether u, v are documents or keywords. For exam-
ple, if the two nodes are documents, then the simi-
larity between two documents d1 and d2 is the prob-
ability of random walk starting at document d1 and
then moving to a keyword k1 and then moving to
keyword k2 and then to document d2. Note that key-
words k1 and k2 can be the same keyword which
accounts for similarity between documents because
they contain the same keyword.
Also, note that second and higher order depen-
dencies are also taken into account by this algo-
rithm. That is, two papers may become similar be-
cause they contain two keywords which are con-
nected by a path in the keyword graph, whose length
is greater than 1. This is due to the iterative nature
of the algorithm. For example, keywords ??Machine
Translation?? and ??Bilingual corpora?? occur often
together and hence any co-occurrence based simi-
larity measure will assign a high initial similarity
value. Hence two publications which contain these
words will be assigned a non-zero similarity value
after a single iteration. Also, ??Bilingual corpora??
and ??SMT?? (abbreviation for Statistical Machine
Translation) can have a high initial similarity value
which enables assiging a high similarity value be-
tween ??Machine Translation?? and ??SMT??. This
leads to a chain effect as the number of iterations in-
creases which helps assign non-zero similarity val-
ues between semantically similar documents even if
they do not contain common keywords.
6 Experiments
It is very hard to evaluate similarity measures in iso-
lation. Thus, most of the algorithms to compute sim-
ilarity scores are evaluated extrinsically, i.e, the sim-
ilarity scores are used for an external task like clus-
tering or classification and the performance in the
external task is used as the performance measure for
the similarity scores. This also helps demonstrate
the different applications of the computed similar-
ity measure. Thus, we perform a variety of differ-
ent experiments on standard data sets to illustrate
the improved performance of the proposed similar-
ity measure. There are three natural variants of the
algorithm,
? Unified: We compare against the edge-weight
regularization algorithm proposed in (Muthukr-
ishnan et al, 2010). The algorithm has the
same representation as our algorithm but the
optimization is strictly defined over the edge
weights in the two layers of the graph, wij?s
and not on the keyword weights. Therefore,
Zij are maintained constant throughout the al-
gorithm.
? Unified-binary: In this variant, we initialize the
keyword weights to 1, i.e, Zij = 1 whenever
document i contains the keyword j.
46
ACL-ID Paper Title Research Topic
W05-0812 Improved HMM Alignment Models for Languages With Scarce
Resources
Machine Translation
P07-1111 A Re-Examination of Machine Learning Approaches for Sentence-
Level MT Evaluation
Machine Translation
P03-1054 Accurate Unlexicalized Parsing Dependency Parsing
P07-1050 K-Best Spanning Tree Parsing Dependency Parsing
P88-1020 Planning Coherent Multi-Sentential Text Summarization
Table 1: Details of a few sample papers classified according to research topic
? Unified-TFIDF: We initialize the keyword
weights to the TFIDF scores, Zij is set to the
TFIDF score of keyword j for document i.
Experiment Set I: We compare our similarity mea-
sure against other similarity measures in the context
of classification. We also compare against a state
of the art classification algorithm which uses differ-
ent similarity measures due to different feature types
without integrating them into one single similarity
measure. Specifically, we compare our algorithm
against three other similarity baselines in the context
of classification which are listed below.
? Content Similarity: Similarity is computed us-
ing just the feature vector representation using
just the text. We use cosine similarity after pre-
processing each document into a tf.idf vector
for the AAN data set. For all other data sets,
we use the cosine similarity on the binary fea-
ture vector representation that is available.
? Link Similarity: Similarity is computed using
only the links (citations, in the case of publica-
tions). To compute link similarity, we use the
node similarity algorithm proposed by (Harel
and Koren, 2001) using a random walk of
length 3 on the link graph.
? Linear combination: The content similarity
(CS) and link similarity (LS) between docu-
ments x and y are combined in a linear fashion
as ?CS(x, y)+(1??)LS(x, y). We tried dif-
ferent values of ? and report only the best accu-
racy that can be achieved using linear combina-
tion of similarity measures. Note that this is an
upper bound on the accuracy of Multiple Ker-
nel Learning with the restriction of the combi-
nation being affine.
We also compare our algorithm against the follow-
ing algorithms SC-MV: We compare our algorithm
against the spectral classification algorithm for data
with multiple views (Zhou and Burges, 2007). The
algorithm tries to classify data when multiple views
of the data are available. The multiple views are rep-
resented using multiple homogeneous graphs with a
common vertex set. In each graph, the edge weights
represent similarity between the nodes computed us-
ing a single feature type. For our experiments, we
used the link similarity graph and the content simi-
larity graph as described above as the two views of
the same data
We use a semi-supervised graph classification al-
gorithm (Zhu et al, 2003) to perform the classifica-
tion.
Experiment Set II: We illustrate the improved
performance of our similarity measure in the con-
text of clustering. We compare our similarity mea-
sure against the three similarity baselines mentioned
above. We use a spectral graph clustering algorithm
proposed in (Dhillon et al, 2007) to perform the
clustering.
We performed our experiments on three different
data sets. The three data sets are explained below.
? AAN Data: The ACL Anthology is a collec-
tion of papers from the Computational Lin-
guistics journal as well as proceedings from
ACL conferences and workshops and includes
15, 160 papers. To build the ACL Anthology
Network (AAN), (Radev et al, 2009) manu-
ally performed some preprocessing tasks in-
cluding parsing references and building the net-
work metadata, the citation, and the author col-
laboration networks. The full AAN includes
the raw text of all the papers in addition to full
citation and collaboration networks.
We chose a subset of papers in 3 topics (Ma-
47
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  10  20  30  40  50  60  70
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(a) AAN
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(b) Cornell
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(c) Texas
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45  50
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(d) Washington
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(e) Wisconsin
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 50  100  150  200  250  300  350  400  450  500
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(f) Cora
Figure 1: Classification Accuracy on the different data sets. The number of points labeled is plotted along
the x-axis and the y-axis shows the classification accuracy on the unlabeled data.
chine Translation, Dependency Parsing, Sum-
marization) from the ACL anthology. These
topics are three main research areas in Natural
Language Processing (NLP). Specifically, we
collected all papers which were cited by pa-
pers whose titles contain any of the following
phrases, ??Dependency Parsing??, ??Machine
Translation??, ??Summarization??. From this
list, we removed all the papers which contained
any of the above phrases in their title because
48
this would make the clustering task easy. The
pruned list contains 1190 papers. We manually
classified each paper into four classes (Depen-
dency Parsing, Machine Translation, Summa-
rization, Other) by considering the full text of
the paper. The manually cleaned data set con-
sists of 275Machine Translation papers, 73 De-
pendency Parsing papers and 32 Summariza-
tion papers. Table 1 lists a few sample papers
from each class.
WebKB(Sen et al, 2008): The data set con-
sists of a subset of the original WebKB data set.
The corpus consists of 877 web pages collected
from four different universities. Each web page
is represented by a 0/1-valued word vector with
1703 unique words after stemming and remov-
ing stopwords. All words with document fre-
quency less than 10 were removed.
Cora(Sen et al, 2008): The Cora dataset con-
sists of 2708 scientific publications classified
into one of seven classes. The citation network
consists of 5429 links. Each publication in the
dataset is described by a 0/1-valued word vec-
tor indicating the absence/presence of the cor-
responding word from the dictionary. The dic-
tionary consists of 1433 unique words.
For all the data sets, we constructed two graphs,
the kewyord feature graph and the link similarity
graph. The keyword feature layer graph, Gf =
(Vf , Ef , wf ) is a weighted graph where Vf is the
set of all features. The edge weight between key-
words fi and fj represents the similarity between
the features. The edge weights are initialized to the
cosine similarity between their corresponding doc-
ument vectors. The link similarity graph, Go =
(Vo, Eo, wo) is another weighted graph where Vo
is the set of objects. The edge weight represents
the similarity between the documents and is initial-
ized to the similarity between the documents due to
the link structure. The link similarity between two
documents is computed using the similarity mea-
sure proposed by (Harel and Koren, 2001) on the
citation graph. We also performed experiments by
initializing the similarity between documents to the
keyword similarity. Although, our algorithm still
outperforms other algorithms and the baselines (not
shown due to space restrictions), the accuracy using
citation similarity is higher.
7 Results and Discussion
Figure 1 shows the accuracy of the classification ob-
tained using different similarity measures. It can be
seen that the proposed algorithm (both the variants)
performs much better than other similarity measures
by a large margin. The algorithm performs much
better when more information is provided in the
form of TF-IDF scores. We attribute this to the
rich representation of the data. In our algorithm, the
data is represented as a set of heterogeneous graphs
(layers) which are connected together instead of the
normal feature vector representation. Thus, we can
leverage on the similarity between the keywords and
the objects (documents) to iteratively improve sim-
ilarity in both layers. Whereas, in the case of the
algorithm in (Zhou and Burges, 2007) all the graphs
are isolated homogeneous graphs. Hence there is no
information transfer across the different graphs.
For the clustering task, we use Normalized Mu-
tual Information (NMI) (Strehl and Ghosh, 2002)
between the ground truth clusters and the outputted
clustering as the measure of clustering accuracy.
Table 2 shows the Normalized Mutual Informa-
tion scores obtained by the different similarity mea-
sures on the different data sets.
8 Conclusion
In this paper, we have proposed a novel approach
to compute similarity between documents and key-
words iteratively. We formalized the problem of
similarity estimation as an optimization problem in-
duced by a regularization framework over edges in
multiple graphs. We propose an efficient, iterative
algorithm based on Alternating Optimization (AO)
which has a neat, intuitive interpretation in terms
of random walks over multiple graphs. We demon-
strated the improved performance of the proposed
algorithm over many different baselines and a state-
of-the-art classifcation algorithm and a similarity
measure which uses the same information as given
to our algorithm.
49
Similarity Measure AAN Texas Wisconsin Washington Cornell Cora
Content Similarity (Cosine) 0.66 0.34 0.42 0.59 0.63 0.48
Link Similarity 0.45 0.49 0.39 0.52 0.56 0.52
Linear Combination 0.69 0.54 0.46 0.54 0.68 0.54
Unified Similarity 0.78 0.69 0.54 0.66 0.72 0.64
Unified Similarity-Binary 0.80 0.68 0.56 0.69 0.74 0.66
Unified Similarity-TFIDF 0.84 0.70 0.60 0.72 0.78 0.70
Table 2: Normalized Mutual Information scores of the different similarity measures on the different data
sets
References
Francis R. Bach, Gert R. G. Lanckriet, and Michael I.
Jordan. 2004. Multiple kernel learning, conic duality,
and the smo algorithm. In Proceedings of the twenty-
first international conference on Machine learning,
ICML ?04, pages 6?, New York, NY, USA. ACM.
James Bezdek and Richard Hathaway. 2002. Some notes
on alternating optimization. In Nikhil Pal and Michio
Sugeno, editors, Advances in Soft Computing AFSS
2002, volume 2275 of Lecture Notes in Computer Sci-
ence, pages 187?195. Springer Berlin.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics.
Corinna Cortes, Mehryar. Mohri, and Afshin Ros-
tamizadeh. 2009. Learning non-linear combinations
of kernels. In In NIPS.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-theoretic co-
clustering. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?03, pages 89?98, New York, NY,
USA. ACM.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors
a multilevel approach. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 29(11):1944?
1957, November.
William A. Gale and Kenneth Ward Church. 1991. A
program for aligning sentences in bilingual corpora.
In In Proceedings of ACL.
David Harel and Yehuda Koren. 2001. On clustering us-
ing random walks. In Foundations of Software Tech-
nology and Theoretical Computer Science 2245, pages
18?41. Springer-Verlag.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of EMNLP.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over multiple
graphs for similarity learning. In In ICDM.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In In Proceedings of the ACL Workshop on Nat-
ural Language Processing and Information Retrieval
for Digital Libraries.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
Alexander Strehl and Joydeep Ghosh. 2002. Cluster en-
sembles: a knowledge reuse framework for combining
partitionings. In Eighteenth national conference on
Artificial intelligence, pages 93?98, Menlo Park, CA,
USA. American Association for Artificial Intelligence.
Dengyong Zhou and Christopher J. C. Burges. 2007.
Spectral clustering and transductive learning with mul-
tiple views. In ICML ?07, pages 1159?1166, New
York, NY, USA.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In ICML 2003, pages 912?
919.
50
