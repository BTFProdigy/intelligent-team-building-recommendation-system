Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 5?8,
Suntec, Singapore, 3 August 2009.
c
?2009 ACL and AFNLP
LX-Center: a center of online linguistic services
Ant
?
onio Branco, Francisco Costa, Eduardo Ferreira, Pedro Martins,
Filipe Nunes, Jo
?
ao Silva and Sara Silveira
University of Lisbon
Department of Informatics
{antonio.branco, fcosta, eferreira, pedro.martins,
fnunes, jsilva, sara.silveira}@di.fc.ul.pt
Abstract
This is a paper supporting the demonstra-
tion of the LX-Center at ACL-IJCNLP-09.
LX-Center is a web center of online lin-
guistic services aimed at both demonstrat-
ing a range of language technology tools
and at fostering the education, research
and development in natural language sci-
ence and technology.
1 Introduction
This paper is aimed at supporting the demonstra-
tion of a web center of online linguistic services.
These services demonstrate language technology
tools for the Portuguese language and are made
available to foster the education, research and de-
velopment in natural language science and tech-
nology.
This paper adheres to the common format de-
fined for demo proposals: the next Section 2
presents an extended abstract of the technical con-
tent to be demonstrated; Section 3 provides a
script outline of the demo presentation; and the
last Section 4 describes the hardware and internet
requirements expected to be provided by the local
organizer.
2 Extended abstract
The LX-Center is a web center of online linguis-
tic services for the Portuguese language located at
http://lxcenter.di.fc.ul.pt. This is
a freely available center targeted at human users. It
has a counterpart in terms of a webservice for soft-
ware agents, the LXService, presented elsewhere
(Branco et al, 2008).
2.1 LX-Center
The LX-Center encompasses linguistic services
that are being developed, in all or part, and main-
tained at the University of Lisbon, Department of
Informatics, by the NLX-Natural Language and
Speech Group. At present, it makes available the
following functionalities:
? Sentence splitting
? Tokenization
? Nominal lemmatization
? Nominal morphological analysis
? Nominal inflection
? Verbal lemmatization
? Verbal morphological analysis
? Verbal conjugation
? POS-tagging
? Named entity recognition
? Annotated corpus concordancing
? Aligned wordnet browsing
These functionalities are provided by one or
more of the seven online services that integrate
the LX-Center. For instance, the LX-Suite service
accepts raw text and returns it sentence splitted,
tokenized, POS tagged, lemmatized and morpho-
logically analyzed (for both verbs and nominals).
Some other services, in turn, may support only one
of the functionalities above. For instance, the LX-
NER service ensures only named entity recogni-
tion.
These are the services offered by the LX-
Center:
? LX-Conjugator
? LX-Lemmatizer
? LX-Inflector
? LX-Suite
? LX-NER
? CINTIL concordancer
? MWN.PT browser
5
The access to each one of these services is ob-
tained by clicking on the corresponding button on
the left menu of the LX-Center front page.
Each of the seven services integrating the LX-
Center will be briefly presented in a different
subsection below. Fully fledged descriptions are
available at the corresponding web pages and in
the white papers possibly referred to there.
2.2 LX-Conjugator
The LX-Conjugator is an online service for fully-
fledged conjugation of Portuguese verbs. It takes
an infinitive verb form and delivers all the corre-
sponding conjugated forms. This service is sup-
ported by a tool based on general string replace-
ment rules for word endings supplemented by a list
of overriding exceptions. It handles both known
verbs and unknown verbs, thus conjugating neolo-
gisms (with orthographic infinitival suffix).
The Portuguese verbal inflection system is a
most complex part of the Portuguese morphology,
and of the Portuguese language, given the high
number of conjugated forms for each verb (ca. 70
forms in non pronominal conjugation), the num-
ber of productive inflection rules involved and the
number of non regular forms and exceptions to
such rules.
This complexity is further increased when the
so-called pronominal conjugation is taken into ac-
count. The Portuguese language has verbal clitics,
which according to some authors are to be ana-
lyzed as integrating the inflectional suffix system:
the forms of the clitics may depend on the Number
(Singular vs. Plural), the Person (First, Second,
Third or Second courtesy), the Gender (Masculine
vs. Feminine), the grammatical function which
they are in correspondence with (Subject, Direct
object or Indirect object), and the anaphoric prop-
erties (Pronominal vs. Reflexive); up to three cli-
tics (e.g. deu-se-lho / gave-One-ToHim-It) may be
associated with a verb form; clitics may occur in
so called enclisis, i.e. as a final part of the verb
form (e.g. deu-o / gave-It), or in mesoclisis, i.e.
as a medial part of the verb form (e.g. d?a-lo-ia
/ give-it-Condicional) ? when the verb form oc-
curs in certain syntactic or semantic contexts (e.g
in the scope of negation), the clitics appear in pro-
clisis, i.e. before the verb form (ex.: n?ao o deu /
NOT it gave); clitics follow specific rules for their
concatenation.
With LX-Conjugator, pronominal conjugation
can be fully parameterizable and is thus exhaus-
tively handled. Additionally, LX-Conjugator ex-
haustively handles a set of inflection cases which
tend not to be supported together in verbal conju-
gators: Compound tenses; Double forms for past
participles (regular and irregular); Past participle
forms inflected for number and gender (with tran-
sitive and unaccusative verbs); Negative impera-
tive forms; Courtesy forms for second person.
This service handles also the very few cases
where there may be different forms in different
variants: when a given verb has different ortho-
graphic representations for some of its inflected
forms (e.g. arguir in European vs. arg?uir in
American Portuguese), all such representations
will be displayed.
2.3 LX-Lemmatizer
The LX-Lemmatizer is an online service for fully-
fledged lemmatization and morphological analysis
of Portuguese verbs. It takes a verb form and de-
livers all the possible corresponding lemmata (in-
finitive forms) together with inflectional feature
values.
This service is supported by a tool based on
general string replacement rules for word endings
whose outcome is validated by the reverse proce-
dure of conjugation of the output and matching
with the original input. These rules are supple-
mented by a list of overriding exceptions. It thus
handles an open set of verb forms provided these
input forms bear an admissible verbal inflection
ending. Hence, this service processes both lexi-
cally known and unknown verbs, thus coping with
neologisms.
LX-Lemmatizer handles the same range of
forms handled and generated by the LX-
Conjugator. As for pronominal conjugation forms,
the outcome displays the clitic detached from
the lemma. The LX-Lemmatizer and the LX-
Conjugator can be used in ?roll-over? mode. Once
the outcome of say the LX-Conjugator on a given
input lemma is displayed, the user can click over
any one of the verbal forms in that conjugation ta-
ble. This activates the LX-Lemmatizer on that in-
put verb form, and then its possible lemmas, to-
gether with corresponding inflection feature val-
ues, are displayed. Now, any of these lemmas can
also be clicked on, which will activate back the
LX-Conjugator and will make the corresponding
conjugation table to be displayed.
6
2.4 LX-Inflector
The LX-Inflector is an online service for the
lemmatization and inflection of nouns and adjec-
tives of Portuguese. This service is also based on
a tool that relies on general rules for ending string
replacement, supplemented by a list of overrid-
ing exceptions. Hence, it handles both lexically
known and unknown forms, thus handling pos-
sible neologisms (with orthographic suffixes for
nominal inflection).
As input, this service takes a Portuguese nomi-
nal form ? a form of a noun or an adjective, in-
cluding adjectival forms of past participles ?, to-
gether with a bundle of inflectional feature values
? values of inflectional features of Gender and
Number intended for the output.
As output, it returns: inflectional features ?
the input form is echoed with the correspond-
ing values for its inflectional features of Gender
and Number, that resulted from its morphological
analysis; lemmata ? the lemmata (singular and
masculine forms when available) possibly corre-
sponding to the input form; inflected forms ? the
inflected forms (when available) of each lemma in
accordance with the values for inflectional features
entered. LX-Inflector processes both simple, pre-
fixed or non prefixed, and compound forms.
2.5 LX-Suite
The LX-Suite is an online service for the shal-
low processing of Portuguese. It accepts raw
text and returns it sentence splitted, tokenized,
POS tagged, lemmatized and morphologically an-
alyzed.
This service is based on a pipeline of a num-
ber of tools, including those supporting the ser-
vices described above. Those tools, for lemmati-
zation and morphological analysis, are inserted at
the end of the pipeline and are preceded by three
other tools: a sentence splitter, a tokenizer and a
POS tagger.
The sentence splitter marks sentence and para-
graph boundaries and unwraps sentences split over
different lines. An f-score of 99.94% was obtained
when testing it on a 12,000 sentence corpus.
The tokenizer segments the text into lexically
relevant tokens, using whitespace as the separator;
expands contractions; marks spacing around punc-
tuation or symbols; detaches clitic pronouns from
the verb; and handles ambiguous strings (con-
tracted vs. non contracted). This tool achieves an
f-score of 99.72%.
The POS tagger assigns a single morpho-
syntactic tag to every token. This tagger is based
on Hidden Markov Models, and was developed
with the TnT software (Brants, 2000). It scores
an accuracy of 96.87%.
2.6 LX-NER
The LX-NER is an online service for the recog-
nition of expressions for named entities in Por-
tuguese. It takes a segment of Portuguese text and
identifies, circumscribes and classifies the expres-
sions for named entities it contains. Each named
entity receives a standard representation.
This service handles two types of expressions,
and their subtypes. (i) Number-based expressions:
Numbers ? arabic, decimal, non-compliant, ro-
man, cardinal, fraction, magnitude classes; Mea-
sures ? currency, time, scientific units; Time ?
date, time periods, time of the day; Addresses ?
global section, local section, zip code; (ii) Name-
base expressions: Persons; Organizations; Loca-
tions; Events; Works; Miscellaneous.
The number-based component is built upon
handcrafted regular expressions. It was devel-
oped and evaluated against a manually constructed
test-suite including over 300 examples. It scored
85.19% precision and 85.91% recall. The name-
based component is built upon HMMs with the
help of TnT (Brants, 2000). It was trained over
a manually annotated corpus of approximately
208,000 words, and evaluated against an unseen
portion with approximately 52,000 words. It
scored 86.53% precision and 84.94% recall.
2.7 CINTIL Concordancer
The CINTIL-Concordancer is an online concor-
dancing service supporting the research usage of
the CINTIL Corpus.
The CINTIL Corpus is a linguistically inter-
preted corpus of Portuguese. It is composed of 1
Million annotated tokens, each one of which ver-
ified by human expert annotators. The annotation
comprises information on part-of-speech, lemma
and inflection of open classes, multi-word expres-
sions pertaining to the class of adverbs and to the
closed POS classes, and multi-word proper names
(for named entity recognition).
This concordancer permits to search for occur-
rences of strings in the corpus and returns them
together with their window of left and right con-
text. It is possible to search for orthographic forms
7
or through linguistic information encoded in their
tags. This service offers several possibilities with
respect to the format for displaying the outcome
of a given search (e.g. number of occurrences per
page, size of the context window, sorting the re-
sults in a given page, hiding the tags, etc.)
This service is supported by Poliqarp, a free
suite of utilities for large corpora processing
(Janus and Przepi?orkowski, 2006).
2.8 MWN.PT Browser
The MWN.PT Browser is an online service to
browse the MultiWordnet of Portuguese.
The MWN.PT is a lexical semantic network for
the Portuguese language, shaped under the on-
tological model of wordnets, developed by our
group. It spans over 17,200 manually validated
concepts/synsets, linked under the semantic rela-
tions of hyponymy and hypernymy. These con-
cepts are made of over 21,000 word senses/word
forms and 16,000 lemmas from both European
and American variants of Portuguese. They are
aligned with the translationally equivalent con-
cepts of the English Princeton WordNet and, tran-
sitively, of the MultiWordNets of Italian, Spanish,
Hebrew, Romanian and Latin.
It includes the subontologies under the concepts
of Person, Organization, Event, Location, and Art
works, which are covered by the top ontology
made of the Portuguese equivalents to all concepts
in the 4 top layers of the Princeton wordnet and
to the 98 Base Concepts suggested by the Global
Wordnet Association, and the 164 Core Base Con-
cepts indicated by the EuroWordNet project.
This browsing service offers an access point to
the MultiWordnet, browser
1
tailored to the Por-
tuguese wordnet. It offers also the possibility
to navigate the Portuguese wordnet diagrammat-
ically by resorting to Visuwords.
2
3 Outline
This is an outline of the script to be followed.
Step 1 : Presentation of the LX-Center.
Narrative: The text in Section 2.1 above.
Action: Displaying the page at
http://lxcenter.di.fc.ul.pt.
Step 2 : Presentation of LX-Conjugator.
Narrative: The text in Section 2.2 above.
Action: Running an example by selecting
1
http://multiwordnet.itc.it/
2
http://www.visuwords.com/
?see an example? option at the page
http://lxconjugator.di.fc.ul.pt.
Step 3 : Presentation of LX-Lemmatizer.
Narrative: The text in Section 2.3 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxlemmatizer.di.fc.ul.pt;
clicking on one of the inflected forms in the
conjugation table generated; clicking on one
of the lemmas returned.
Step 4 : Presentation of LX-Inflector.
Narrative: The text in Section 2.4 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxinflector.di.fc.ul.pt.
Step 5 : Presentation of LX-Suite.
Narrative: The text in Section 2.5 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxsuite.di.fc.ul.pt.
Step 6 : Presentation of LX-NER.
Narrative: The text in Section 2.6 above.
Action: Running an example by copying one
of the examples in the page
http://lxner.di.fc..ul.pt
and hitting the ?Recognize? button.
Step 7 : Presentation of CINTIL Concordancer.
Narrative: The text in Section 2.7 above.
Action: Running an example by selecting
?see an example? option at the page
http://cintil.ul.pt.
Step 8 : Presentation of MWN.PT Browser.
Narrative: The text in Section 2.8 above.
Action: Running an example by selecting
?see an example? option at the page
http://mwnpt.di.fc.ul.pt/.
4 Requirements
This demonstration requires a computer (a laptop
we will bring along) and an Internet connection.
References
A. Branco, F. Costa, P. Martins, F. Nunes, J. Silva and
S. Silveira. 2008. ?LXService: Web Services of
Language Technology for Portuguese?. Proceed-
ings of LREC2008. ELRA, Paris.
D. Janus and A. Przepi?orkowski. 2006. ?POLIQARP
1.0: Some technical aspects of a linguistic search
engine for large corpora?. Proceedings PALC 2005.
T. Brants. 2000. ?TnT-A Statistical Part-of-speech
Tagger?. Proceedings ANLP2000.
8
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 57?64,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Self- or Pre-Tuning?
Deep linguistic processing of language variants
Anto?nio Branco
Universidade de Lisboa
Antonio.Branco@di.fc.ul.pt
Francisco Costa
Universidade de Lisboa
fcosta@di.fc.ul.pt
Abstract
This paper proposes a design strategy
for deep language processing grammars
to appropriately handle language vari-
ants. It allows a grammar to be re-
stricted as to what language variant it is
tuned to, but also to detect the variant
a given input pertains to. This is eval-
uated and compared to results obtained
with an alternative strategy by which the
relevant variant is detected with current
language identification methods in a pre-
processing step.
1 Introduction
This paper addresses the issue of handling dif-
ferent variants of a given language by a deep
language processing grammar for that language.
In the benefit of generalization and grammar
writing economy, it is desirable that a grammar
can handle language variants ? that share most
grammatical structures and lexicon ? in order to
avoid endless multiplication of individual gram-
mars, motivated by inessential differences.
From the viewpoint of analysis, however, in-
creased variant coverage typically opens the way
to increased spurious overgeneration. Conse-
quently, the ability for the grammar to be tuned
to the relevant dialect of the input is impor-
tant to control overgeneration arising from its
flexibility.
Control on what is generated is also desirable.
In general one wants to be able to parse as much
variants as possible, but at the same time be se-
lective in generation, by consistently generating
only in a given selected variant.
Closely related to the setting issue (addressed
in the next Section 2) is the tuning issue: if a
system can be restricted to a particular variety,
what is the best way to detect the variety of the
input? We discuss two approaches to this issue.
One of them consists in using pre-processing
components that can detect the language variety
at stake. This pre-tuning approach explores the
hypothesis that methods developed for language
identification can be used also to detect language
variants (Section 5).
The other approach is to have the computa-
tional grammar prepared for self-tuning to the
language variant of the input in the course of
processing that input (Section 4).
We evaluate the two approaches and compare
them (last Section 6).
2 Variant-sensitive Grammar
In this Section, we discuss the design options for
a deep linguistic processing grammar allowing
for its appropriate tuning to different language
variants. For the sake of concreteness of the dis-
cussion, we assume the HPSG framework (Pol-
lard and Sag, 1994) and a grammar that handles
two close variants of the same language, Euro-
pean and Brazilian Portuguese. These assump-
tions are merely instrumental, and the results
obtained can be easily extended to other lan-
guages and variants, and to other grammatical
frameworks for deep linguistic processing.
A stretch of text from a language L can dis-
play grammatical features common to all vari-
ants of L, or contain a construction that per-
tains to some or only one of its variants. Hence,
undesirable overgeneration due to the grammar
readiness to cope with all language variants can
57
ep-variant
variant
single-variant bp-variant
european-portuguese portuguese brazilian-portuguese
Figure 1: Type hierarchy under variant.
be put in check by restricting the grammar
to produce variant-?consistent? analyses. More
precisely, if the input string contains an element
that can only be found in variety v
1
and that in-
put string yields ambiguity in a different stretch
but only in varieties v
k
other than v
1
, this ambi-
guity will not give rise to multiple analyses if the
grammar can be designed so that it can be con-
strained to accept strings with marked elements
of at most one variety, v
1
.
The approach we propose seeks to implement
this mode of operation in analysis, with the im-
portant effect of permitting also to control the
variant under which generation should be per-
formed. It relies on the use of a feature VARIANT
to model variation. This feature is appropriate
for all signs and declared to be of type variant.
Given the working language variants assumed
here, its values are presented in Figure 1.
This attribute is constrained to take the ap-
propriate value in lexical items and construc-
tions specific to one of the two varieties. For
example, a hypothetical lexical entry for the lex-
ical item autocarro (bus, exclusive to European
Portuguese) would include the constraint that
the attribute VARIANT has the value ep-variant
and the corresponding Brazilian Portuguese en-
try for o?nibus would constrain the same feature
to bear the value bp-variant. The only two types
that are used to mark signs are ep-variant and
bp-variant. The remaining types presented in
Figure 1 are used to constrain grammar behav-
ior, as explained below.
Lexical items are not the only elements that
can have marked values in the VARIANT fea-
ture. Lexical and syntax rules can have them,
too. Such constraints model constructions that
markedly pertain to one of the dialects.
Feature VARIANT is structure-shared among
all signs comprised in a full parse tree. This
is achieved by having all lexical or syntactic
rules unifying their VARIANT feature with the
VARIANT feature of their daughters.
If two signs (e.g. from lexical items and syn-
tax rules) in the same parse tree have different
values for feature VARIANT (one has ep-variant
and the other bp-variant), they will unify to por-
tuguese, as can be seen from Figure 1. This type
means that lexical items or constructions spe-
cific to two different varieties are used together.
Furthermore, since this feature is shared among
all signs, it will be visible everywhere, for in-
stance in the root node.
It is possible to constrain feature VARIANT in
the root condition of the grammar so that the
grammar works in a variant-?consistent? fash-
ion: this feature just has to be constrained to
be of type single-variant (in root nodes) and
the grammar will accept either European Por-
tuguese or Brazilian Portuguese. Furthermore,
in the non natural condition where the input
string bears marked properties of both vari-
ants, that string will receive no analysis: feature
VARIANT will have the value portuguese in this
case, and there is no unifier for portuguese and
single-variant.
If this feature is constrained to be of type
european-portuguese in the root node, the gram-
mar will not accept any sentence with fea-
tures of Brazilian Portuguese, since they will be
marked to have a VARIANT of type bp-variant,
which is incompatible with european-portuguese.
It is also possible to have the grammar re-
ject European Portuguese (using type brazilian-
portuguese) or to ignore variation completely by
not constraining this feature in the start symbol.
With this grammar design it is thus possi-
ble to control beforehand the mode of operation
for the grammar, either for it to handle only
one variant or several. But it is also possible
to use the grammar to detect to which variety
input happens to belong. This self-tuning of
the grammar to the relevant variant is done by
parsing that input and placing no constraint on
feature VARIANT of root nodes, and then read-
ing the value of attribute VARIANT from the re-
sulting feature structure: values ep-variant and
bp-variant result from parsing text with proper-
ties specific to European Portuguese or Brazilian
Portuguese respectively; value variant indicates
that no marked elements were detected and the
text can be from both variants. Also here where
the language variant of the input is detected by
the grammar, the desired variant-?consistent?
58
behavior of the grammar is enforced.
If the input can be known to be specifically
European or Brazilian Portuguese before it is
parsed, the constraints on feature VARIANT can
be set accordingly to improve efficiency: When
parsing text known to be European Portuguese,
there is no need to explore analyses that are
markedly Brazilian Portuguese, for instance.
It is thus important to discuss what meth-
ods for language variant detection can be put
in place that support a possible pre-processing
step aimed at pre-tuning the grammar for the
relevant variant of the input. It is also impor-
tant to gain insight on the quality of the per-
formance of this method and on how the perfor-
mance of this pre-tuning setup compares with
the self-tuning approach. This is addressed in
the next Sections.
3 Experimental setup
Before reporting on the results obtained with the
experiments on the performance of the two ap-
proaches (self- and pre-tuning), it is important
to introduce the experimental conditions under
which such exercises were conducted.
3.1 Data
To experiment with any of these two approaches
to variant-tuning, two corpora of newspaper text
were used, CETEMPublico (204M tokens) and
CETENFolha (32M tokens). The first contains
text from the European newspaper O Pu?blico,
and the latter from the South American Folha
de Sa?o Paulo. These corpora are only minimally
annotated (paragraph and sentence boundaries,
inter alia), but are very large.
Some preprocessing was carried out: XML-
like tags, like the <s> and </s> tags marking
sentence boundaries, were removed and each in-
dividual sentence was put in a single line.
Some heuristics were also employed to remove
loose lines (parts of lists, etc.) so that only lines
ending in ., ! and ?, and containing more than 5
tokens (whitespace delimited) were considered.
Other character sequences that were judged ir-
relevant and potential misguiders for the pur-
pose at hand were normalized: URLs were re-
placed by the sequence URL, e-mail addresses
by MAIL, hours and dates by HORA and DATA,
etc. Names at the beginning of lines indicating
speaker (in an interview, for instance) were re-
moved, since they are frequent and the grammar
that will be used is not intended to parse name
plus sentence strings.
The remaining lines were ordered by length in
terms of words and the smallest 200K lines from
each of the two corpora were selected. Small
lines were preferred as they are more likely to
receive an analysis by the grammar.
Given the methods we will be employing for
pre-tuning reportedly perform well even with
small training sets (Section 5), only a modest
portion of text from these corpora was needed.
In the benefit of comparability of the two
approaches for grammar tuning, it is impor-
tant that all the lines in the working data are
parsable by the grammar. Otherwise, even if
in the pre-tuning approach the pre-processor
gets the classification right for non parsable sen-
tences, this will be of no use since the grammar
will not produce any result out of that. 90K lines
of text were thus randomly selected from each
corpus and checked as to whether they could be
parsed by the grammar. 25K of parsable lines of
the American corpus and 21K of parsable lines
of the European corpus were obtained (46K lines
out of 180K, representing 26% rate of parsabil-
ity for the grammar used ? more details on this
grammar in the next Section).
It is worth noting that the use of two corpora,
one from an European newspaper and the other
from an American newspaper, without further
annotation, does not allow their appropriate use
in the present set of experiments. The reason
is that if a sentence is found in the European
corpus, one can have almost absolute certainty
that it is possible in European Portuguese, but
one does not know if it is Brazilian Portuguese,
too. The same is true of any sentences in the
American corpus ? it can also be a sentence
of European Portuguese in case it only contains
words and structures common to both variants.
In order to prepare the data, a native speaker
of European Portuguese was asked to manually
decide from sentences found in the American
corpus whether they are markedly Brazilian Por-
tuguese. Conversely, a Brazilian informant de-
tected markedly European Portuguese sentences
from the European corpus.
From these parsed lines we drew around 1800
random lines of text from each corpus, and had
them annotated. The lines coming from the
American corpus were annotated for whether
they are markedly Brazilian Portuguese, and
59
vice-versa for the other corpus. Thus a three-
way classification is obtained: any sentence
was classified as being markedly Brazilian Por-
tuguese, European Portuguese or common to
both variants.
The large majority of the sentences were
judged to be possible in both European and
Brazilian Portuguese. 16% of the sentences in
the European corpus were considered not be-
longing to Brazilian Portuguese, and 21% of the
sentences in the American corpus were judged as
not being European Portuguese.1 Overall, 81%
of the text was common to both varieties.
10KB of text from each one of the three classes
were obtained. 140 lines, approximately 5KB,
were reserved for training and another 140 for
test. In total, the 30 K corpus included 116, 170,
493 and 41 sentence tokens for, respectively, 8,
7, 6 and 5 word length sentence types.
3.2 Variation
These training corpora were submitted to man-
ual inspection in order to identify and quantify
the sources of variant specificity. This is impor-
tant to help interpret the experimental results
and to gain insight on the current coverage of
the grammar used in the experiment.
This analysis was performed over the 140 lines
selected as markedly Brazilian Portuguese, and
assumed that the sources of variant specificity
should have broadly the same distribution in
the other 140K lines markedly European Por-
tuguese.
1. Mere orthographic differences (24%) e.g.
ac?a?o vs. acc?a?o (action)
2. Phonetic variants reflected in orthography
(9.3%) e.g. iro?nico vs. iro?nico (ironic)
1A hypothetical explanation for this asymmetry (16%
vs. 21%) is that one of the most pervasive differences
between European and Brazilian Portuguese, clitic place-
ment, is attenuated in writing: Brazilian text often dis-
plays word order between clitic and verb similar to Euro-
pean Portuguese, and different from oral Brazilian Por-
tuguese. Therefore, European text displaying European
clitic order tends not be seen as markedly European. In
fact, we looked at the European sentences with clitic
placement characteristic of European Portuguese that
were judged possible in Brazilian Portuguese. If they
were included in the markedly European sentences, 23%
of the European text would be unacceptable Brazilian
Portuguese, a number closer to the 21% sentences judged
to be exclusively Brazilian Portuguese in the American
corpus.
3. Lexical differences (26.9% of differences)
(a) Different form, same meaning (22.5%)
e.g. time vs. equipa (team)
(b) Same form, different meaning (4.4%)
e.g. policial (policeman/criminal novel
4. Syntactic differences (39.7%)
(a) Possessives w/out articles (12.2%)
(b) In subcategorization frames (9.8%)
(c) Clitic placement (6.4%)
(d) Singular bare NPs (5.4%)
(e) In subcat and word sense (1.9%)
(f) Universal todo + article (0.9%)
(g) Contractions of Prep+article (0.9%)
(h) Questions w/out SV inversion (0.9%)
(i) Postverbal negation (0.5%)
(j) other (0.5%)
About 1/3 of the differences found would dis-
appear if a unified orthography was adopted.
Differences that are reflected in spelling can be
modeled via multiple lexical entries, with con-
straints on feature VARIANT reflecting the vari-
ety in which the item with that spelling is used.
Interestingly, 40% of the differences are syn-
tactic in nature. These cases are expected to
be more difficult to detect with stochastic ap-
proaches than with a grammar.
4 Self-tuning
4.1 Grammar and baseline
The experiments on the self-tuning approach
were carried out with a computational grammar
for Portuguese developed with the LKB plat-
form (Copestake, 2002) that uses MRS for se-
mantic representation (Copestake et al, 2001)
(Branco and Costa, 2005). At the time of the
experiments reported here, this grammar was
of modest size. In terms of linguistic phenom-
ena, it covered basic declarative sentential struc-
tures and basic phrase structure of all cate-
gories, with a fully detailed account of the struc-
ture of NPs. It contained 42 syntax rules, 37
lexical rules (mostly inflectional) and a total
of 2988 types, with 417 types for lexical en-
tries. There were 2630 hand-built lexical entries,
mostly nouns, with 1000 entries. It was coupled
with a POS tagger for Portuguese, with 97% ac-
curacy (Branco and Silva, 2004).
60
In terms of the sources of variant specificity
identified above, this grammar was specifically
designed to handle the co-occurrence of prenom-
inal possessives and determiners and most of the
syntactic constructions related to clitic-verb or-
der. As revealed by the study of the training
corpus, these constructions are responsible for
almost 20% of marked sentences.
The lexicon contained lexical items markedly
European Portuguese and markedly Brazilian
Portuguese. These were taken from the Por-
tuguese Wiktionary, where this information is
available. Leaving aside the very infrequent
items, around 740 marked lexical items were
coded. Items that are variant specific found in
the training corpora (80 more) were also entered
in the lexicon.
These items, markedly belonging to one vari-
ant, were declined into their inflected forms and
the resulting set Lex
bsl
was used in the following
baseline for dialect tuning: for a sentence s and
N
ep
, resp. N
bp
, the number of tokens of items
in Lex
bsl
markedly European, resp. Brazilian
Portuguese, occurring in s, s is tagged as Euro-
pean Portuguese if N
ep
> N
bp
, or vice-versa, or
else, ?common? Portuguese if N
ep
= N
bp
= 0.
Known Predicted class
class EP BP Common Recall
EP 45 0 95 0.32
BP 3 45 92 0.32
Common 4 4 132 0.94
Precision 0.87 0.98 0.41
Table 1: Baseline: Confusion matrix.
For this baseline, the figure of 0.53 of overall
accuracy was obtained, detailed in Table 1.2
4.2 Results with self-tuning
The results obtained for the self-tuning mode
of operation are presented in Table 2.3 When
the grammar produced multiple analyses for a
2Naturally, extending the operation of this baseline
method beyond the terms of comparability with gram-
mars that handle each sentence at a time, namely by
increasingly extending the number of sentences in the
stretch of text being classified, will virtually lead it to
reach optimal accuracy.
3These figures concern the test corpus, with the three
conditions represented by 1/3 of the sentences, which are
all parsable. Hence, actual recall over a naturally occur-
ring text is expected to be lower. Using the estimate that
only 26% of input receives a parse, that figure for recall
would lie somewhere around 0.15 (= 0.57 x 0.26).
given sentence, that sentence was classified as
markedly European, resp. Brazilian, Portuguese
if all the parses produced VARIANT with type ep-
variant, resp. bp-variant. In all other cases, the
sentence would be classified as common to both
variants.
Known Predicted class
class EP BP Common Recall
EP 53 1 86 0.38
BP 6 61 73 0.44
Common 14 1 125 0.89
Precision 0.73 0.97 0.44
Table 2: Self-tuning: Confusion matrix.
Every sentence in the test data was classified,
and the figure of 0.57 was obtained for over-
all accuracy. The analysis of errors shows that
the sentence belonging to Brazilian Portuguese
or to ?common? Portuguese wrongly classified
as European Portuguese contain clitics follow-
ing the European Portuguese syntax, and some
misspellings conforming to the European Por-
tuguese orthography.
5 Pre-tuning
5.1 Language Detection Methods
Methods have been developed to detect the lan-
guage a given text is written in. They have
also been used to discriminate varieties of the
same language, although less often. (Lins and
Gonc?alves, 2004) look up words in dictionaries
to discriminate among languages, and (Oakes,
2003) runs stochastic tests on token frequencies,
like the chi-square test, in order to differentiate
between European and American English.
Many methods are based on frequency of byte
n-grams in text because they can simultaneously
detect language and character encoding (Li and
Momoi, 2001), and can reliably classify short
portions of text. They have been applied in web
browsers (to identify character encodings) and
information retrieval systems.
We are going to focus on methods based on
character n-grams. Because all information used
for classification is taken from characters, and
they can be found in text in much larger quanti-
ties than words or phrases, problems of scarcity
of data are attenuated. Besides, training data
can also be easily found in large amounts be-
cause corpora do not need to be annotated (it is
61
only necessary to know the language they belong
to). More importantly, methods based on char-
acter n-grams can reliably classify small portions
of text. The literature on automatic language
identification mentions training corpora as small
as 2K producing classifiers that perform with al-
most perfect accuracy for test strings as little as
500 Bytes (Dunning, 1994) and considering sev-
eral languages. With more training data (20K-
50K of text), similar quality can be achieved for
smaller test strings (Prager, 1999).
Many n-gram based methods have been ex-
plored besides the one we opted for.4 Many
can achieve perfect or nearly perfect classifica-
tion with small training corpora on small texts.
In previous work (Branco and Costa, 2007),
we did a comparative study on two classifiers
that use approaches very well understood in
language processing and information retrieval,
namely Vector Space and Bayesian models. We
retain here the latter as this one scored compar-
atively better for the current purposes.
In order to know which language L
i
? L gen-
erated string s, Bayesian methods can be used
to calculate the probabilities P (s|L
i
) of string s
appearing in language L
i
for all L
i
? L, the con-
sidered language set, and decide for the language
with the highest score (Dunning, 1994). That is,
in order to compute P (L
i
|s), we only compute
P (s|L
i
). The Bayes rule allows us to cast the
problem in terms of P (s|Li)P (Li )
P (s)
, but as is stan-
dard practice, the denominator is dropped since
we are only interested here in getting the highest
probability, not its exact value. The prior P (L
i
)
is also ignored, corresponding to the simplify-
ing assumption that all languages are equally
probable for the operation of the classifier. The
way P (s|L
i
) is calculated is also the standard
way to do it, namely assuming independence
and just multiplying the probabilities of charac-
ter c
i
given the preceding n-1 characters (using
n-grams), for all characters in the input (esti-
mated from n-gram counts in the training set).
For our experiments, we implemented the al-
gorithm described in (Dunning, 1994). Other
common strategies were also used, like prepend-
ing n?1 special characters to the input string to
harmonize calculations, summing logs of proba-
bilities instead of multiplying them to avoid un-
4See (Sibun and Reynar, 1996) and (Hughes et al,
2006) for surveys.
derflow errors, and using Laplace smoothing to
reserve probability mass to events not seen in
training.
5.2 Calibrating the implementation
5.2.1 Detection of languages
First of all, we want to check that the lan-
guage identification methods we are using, and
have implemented, are in fact reliable to identify
different languages. Hence, we run the classifier
on three languages showing strikingly different
characters and character sequences. This is a
deliberately easy test to get insight into the ap-
propriate setting of the two parameters at stake
here, size of of the n-gram in the training phase,
and size of the input in the running phase.
For this test, we used the Universal Declara-
tion of Human Rights texts.The languages used
were Finnish, Portuguese and Welsh.5
Several tests were conducted, splitting the
test data in chunks 1, 5, 10 and 20 lines long.
The classifier obtained perfect accuracy on all
test conditions (all chunk sizes), for all values of
n between 1 and 7 (inclusively). For n = 8 and
n = 9 there were errors only when classifying 1
line long items.
The average line length for the test corpora
was 138 characters for Finnish, 141 for Por-
tuguese and 121 for Welsh (133 overall). In the
corpora we will be using in the following experi-
ments, average line length is much lower (around
40 characters per line). To become closer to
our experimental conditions, we also evaluated
this classifiers with the same test corpora, but
truncated each line beyond the first 50 charac-
ters, yielding test corpora with an average line
length around 38 characters (since some were
smaller than that). The results are similar. The
Bayesian classifier performed with less than per-
fect accuracy also with n = 7 when classifying 1
line at a time.
Our classifier was thus performing well at dis-
criminating languages with short values of n,
and can classify short bits of text, even with
incomplete words.
5The Preamble and Articles 1?19 were used for train-
ing (8.1K of Finnish, 6.9K of Portuguese, and 6.1K of
Welsh), and Articles 20?30 for testing (4.6K of Finnish,
4.7K of Portuguese, and 4.0K of Welsh).
62
5.2.2 Detection of originating corpus
In order to study its suitability to discrimi-
nate also the two Portuguese variants, we ex-
perimented our implementation of the Bayesian
classifiers on 200K lines of text from each of the
two corpora. We randomly chose 20K lines for
testing and the remaining 180K for training. A
classification is considered correct if the classi-
fier can guess the newspaper the text was taken
from.
The average line length of the test sentences is
43 characters. Several input lengths were tried
out by dividing the test data into various sets
with varying size. Table 3 summarizes the re-
sults obtained.
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.84 0.99 1 1
n = 3 0.96 0.99 1 1
n = 4 0.96 1 1 1
n = 5 0.94 1 1 1
n = 6 0.92 0.99 1 1
n = 7 0.89 0.98 0.99 1
Table 3: Originating corpora: Accuracy
The accuracy of the classifier is surprisingly
high given that the sentences that cannot be at-
tributed to a single variety are estimated to be
around 81%.
5.2.3 Scaling down the training data
A final check was made with the classifier
to gain further insight on the comparability of
the results obtained under the two tuning ap-
proaches. It was trained on the data prepared
for the actual experiment, made of the 10K
with lines that have the shortest length and are
parsable, but using only the markedly European
and Brazilian Portuguese data (leaving aside the
sentences judged to be common to both). This
way the two setups can be compared, since in
the test of the Subsection just above much more
data was available for training.
Results are in Table 4. As expected, with
a much smaller amount of training data there
is an overall drop in the accuracy, with a no-
ticed bias at classifying items as European Por-
tuguese. The performance of the classifier de-
grades with larger values of n. Nevertheless, the
classifier is still very good with bigrams, with an
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.86 0.98 0.96 1
n = 3 0.82 0.73 0.64 0.5
n = 4 0.68 0.55 0.5 0.5
Table 4: Two-way classification: Accuracy
almost optimal performance, only slightly worse
than the one observed in the previous Subsec-
tion, when it was trained with more data.
From these preliminary tests, we learned that
we could expect a quasi optimal performance of
the classifier we implemented to act as a prepro-
cessor in the pre-tuning approach, when n = 2
and it is run under conditions very close to the
ones it will encounter in the actual experiment
aimed at comparing the two tuning approaches.
5.3 Results with pre-tuning
In the final experiment, the classifier should
discriminate between three classes, deciding
whether the input is either specifically Euro-
pean or Brazilian Portuguese, or else whether
it belongs to both variants. It was trained over
the 15K tokens/420 lines of training data, and
tested over the held out test data of identical
size.
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.59 0.67 0.76 0.76
n = 3 0.55 0.52 0.45 0.33
n = 4 0.48 0.39 0.33 0.33
Table 5: Three-way classification: Accuracy
The results are in Table 5. As expected, the
classifier based in bigrams has the best perfor-
mance for every size of the input, which im-
proves from 0.59 to 0.76 as the size of the input
gets from 1 line to 20 lines.
6 Discussion and conclusions
From the results above for pre-tuning, it is the
value 0.59, obtained for 1 line of input, that can
be put on a par with the value of 0.57 obtained
for self-tuning ? both of them to be appreciated
against the baseline of 0.53.
Interestingly, the performance of both ap-
proaches are quite similar, and quite encour-
aging given the limitations under which the
present pilot exercise was executed. But this is
63
also the reason why they should be considered
with the appropriate grano salis.
Note that there is much room for improve-
ment in both approaches. From the several
sources of variant specificity, the grammar used
was prepared to cope only with grammatical
constructs that are responsible for at most 20%
of them. Also the lexicon, that included a little
more than 800 variant-distinctive items, can be
largely improved.
As to the classifier used for pre-tuning, it im-
plements methods that may achieve optimal ac-
curacy with training data sets of modest size but
that need to be nevertheless larger than the very
scarce 15K tokens used this time. Using backoff
and interpolation will help to improve as well.
Some features potentially distinguish, how-
ever, the pre-tuning based on Bayesian classifier
from the self-tuning by the grammar.
Language detection methods are easy to scale
up with respect to the number of variants used.
In contrast, the size of the type hierarchy under
variant is exponential on the number of language
variants if all combinations of variants are taken
into account, as it seems reasonable to do.
N-grams based methods are efficient and can
be very accurate. On the other hand, like any
stochastic method, they are sensitive to training
data and tend to be much more affected than the
grammar in self-tuning by a change of text do-
main. Also in dialogue settings with turns from
different language variants, hence with small
lengths of texts available to classify and suc-
cessive alternation between language variants,
n-grams are likely to show less advantage than
self-tuning by fully fledged grammars.
These are issues over which more acute insight
will be gained in future work, which will seek
to improve the contributions put forward in the
present paper.
Summing up, a major contribution of the
present paper is a design strategy for type-
feature grammars that allows them to be appro-
priately set to the specific language variant of a
given input. Concomitantly, this design allows
the grammars either to be pre-tuned or to self-
tune to that dialect ? which, to the best of our
knowledge, consists in a new kind of approach to
handling language variation in deep processing.
In addition, we undertook a pilot experiment
which can be taken as setting the basis for a
methodology to comparatively assess the perfor-
mance of these different tuning approaches and
their future improvements.
References
Anto?nio Branco and Francisco Costa. 2005. LX-
GRAM ? deep linguistic processing of Portuguese
with HSPG. Technical report, Dept. of Informat-
ics, University of Lisbon.
Anto?nio Branco and Francisco Costa. 2007. Han-
dling language variation in deep processing. In
Proc. CLIN2007.
Anto?nio Branco and Joa?o Silva. 2004. Evaluat-
ing solutions for the rapid development of state-
of-the-art POS taggers for Portuguese. In Proc.
LREC2004.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan Sag. 2001. Minimal Recursion Semantics:
An introduction. Language and Computation, 3.
Ann Copestake. 2002. Implementing typed feature
structure grammars. CSLI.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State Univ.
Baden Hughes, Timothy Baldwin, Steven Bird,
Jeremy Nicholson, and Andrew MacKinlay. 2006.
Reconsidering language identification for written
language resources. In Proc. LREC2006.
Shanjian Li and Katsuhiko Momoi. 2001. A com-
posite approach to language/encoding detection.
In Proc. 19th International Unicode Conference.
Rafael Lins and Paulo Gonc?alves. 2004. Automatic
language identification of written texts. In Proc.
2004 ACM Symposium on Applied Computing.
Michael P. Oakes. 2003. Text categorization: Auto-
matic discrimination between US and UK English
using the chi-square test and high ratio pairs. Re-
search in Language, 1.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. CSLI.
John M. Prager. 1999. Linguini: Language iden-
tification for multilingual documents. Journal of
Management Information Systems, 16(3).
Penelope Sibun and Jeffrey C. Reynar. 1996. Lan-
guage identification: Examining the issues. In 5th
Symposium on Document Analysis and IR.
64
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 266?275,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Aspectual Type and Temporal Relation Classification
Francisco Costa
Universidade de Lisboa
fcosta@di.fc.ul.pt
Anto?nio Branco
Universidade de Lisboa
Antonio.Branco@di.fc.ul.pt
Abstract
In this paper we investigate the relevance of
aspectual type for the problem of temporal
information processing, i.e. the problems
of the recent TempEval challenges.
For a large list of verbs, we obtain sev-
eral indicators about their lexical aspect by
querying the web for expressions where
these verbs occur in contexts associated
with specific aspectual types.
We then proceed to extend existing solu-
tions for the problem of temporal informa-
tion processing with the information ex-
tracted this way. The improved perfor-
mance of the resulting models shows that
(i) aspectual type can be data-mined with
unsupervised methods with a level of noise
that does not prevent this information from
being useful and that (ii) temporal informa-
tion processing can profit from information
about aspectual type.
1 Introduction
Extracting the temporal information present in a
text is relevant to many natural language process-
ing applications, including question-answering,
information extraction, and even document sum-
marization, as summaries may be more readable
if they follow a chronological order.
Recent evaluation campaigns have focused on
the extraction of temporal information from writ-
ten text. TempEval (Verhagen et al 2007), in
2007, and more recently TempEval-2 (Verhagen
et al 2010), in 2010, were concerned with this
problem. Additionally, they provided data that
can be used to develop and evaluate systems that
can automatically temporally tag natural language
text. These data are annotated according to the
TimeML (Pustejovsky et al 2003) scheme.
Figure 1 shows a small and slightly simpli-
fied fragment of the data from TempEval, with
TimeML annotations. There, event terms, such
as the term referring to the event of releasing the
tapes, are annotated using EVENT tags. States
(such as the situations denoted by verbs like want
or love) are also considered events. Temporal ex-
pressions, such as today, are enclosed in TIMEX3
tags. The attribute value of time expressions
holds a normalized representation of the date or
time they refer to (e.g. the word today denotes the
date 1998-01-14 in this example). The TLINK
elements at the end describe temporal relations
between events and temporal expressions. For in-
stance, the event of the plane going down is anno-
tated as temporally preceding the date denoted by
the temporal expression today.
The major tasks of these two TempEval evalu-
ation challenges were about guessing the type of
temporal relations, i.e. the value of the relType
attribute of the TLINK elements in Figure 1, all
other annotations being given. Temporal relation
classification is also the most interesting problem
in temporal information processing. The other
relevant tasks (identifying and normalizing tem-
poral expressions and events) have a longer re-
search history and show better evaluation results.
TempEval was organized in three tasks
(TempEval-2 has four additional ones, that are not
relevant to this work): task A was concerned with
classifying temporal relations holding between an
event and a time mentioned in the same sentence
(although they could be syntactically unrelated, as
the temporal relation represented by the TLINK
with the lid with the value l1 in Figure 1); task
266
<s>In Washington <TIMEX3 tid="t53" type="DATE"
value="1998-01-14">today</TIMEX3>, the Federal
Aviation Administration <EVENT eid="e1"
class="OCCURRENCE" stem="release"
aspect="NONE" tense="PAST" polarity="POS"
pos="VERB">released</EVENT> air traffic control tapes from
<TIMEX3 tid="t54" type="TIME"
value="1998-XX-XXTNI">the night</TIMEX3> the TWA
Flight eight hundred <EVENT eid="e2"
class="OCCURRENCE" stem="go" aspect="NONE"
tense="PAST" polarity="POS"
pos="VERB">went</EVENT> down.</s>
<TLINK lid="l1" relType="BEFORE" eventID="e2"
relatedToTime="t53"/>
<TLINK lid="l2" relType="OVERLAP"
eventID="e2" relatedToTime="t54"/>
Figure 1: Sample of the data annotated for TempEval,
corresponding to the fragment: In Washington today,
the Federal Aviation Administration released air traf-
fic control tapes from the night the TWA Flight eight
hundred went down.
Task
A B C
Best system 0.62 0.80 0.55
Average of all participants 0.56 0.74 0.51
Majority class baseline 0.57 0.56 0.47
Table 1: Results for English in TempEval (F-measure),
from Verhagen et al(2009)
B focused on the temporal relation between events
and the document?s creation time, which is also
annotated in TimeML (not shown in that Figure);
and task C was about classifying the temporal re-
lation between the main events of two consecu-
tive sentences. The possible values for the type
of temporal relation are BEFORE, AFTER and
OVERLAP.1
Table 1 shows the results of the first TempEval
evaluation. The results of TempEval-2 are fairly
similar (Verhagen et al 2010), but the data used
are similar but not identical.
The best system in TempEval for tasks A and B
(Pus?cas?u, 2007) combined statistical and knowl-
edge based methods to propagate temporal con-
straints along parse trees coming from a syntac-
tic parser. The best system for task C (Min et
1There are the additional disjunctive values
BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and
VAGUE, employed when the annotators could not make a
more specific decision, but these affect a small number of
instances.
al., 2007) also combined rule-based and machine
learning approaches. It employed sophisticated
NLP to compute some of the features used; more
specifically it used syntactic features.
Our goal with this work is to evaluate the im-
pact of information about aspectual type on these
tasks. The TimeML annotations include an at-
tribute class for EVENTs that encodes some as-
pectual information, distinguishing between sta-
tive (annotated with the value STATE) and non-
stative events (value OCCURRENCE). This at-
tribute is relevant to the classification problem at
hand, i.e. it is a useful feature for machine learned
classifiers for the TempEval tasks (although this
class attribute encodes other kinds of informa-
tion as well). However, aspectual distinctions can
be more fine-grained than a mere binary distinc-
tion, and so far no system has explored this sort of
information to help improve the solutions to tem-
poral relation classification.
In this paper we work with Portuguese, but in
principle there is no reason to believe that our
findings would not apply to other languages that
display similar aspectual phenomena, such as En-
glish. Some of the details, such as the material
in Section 4.2, are however language specific and
would need adaptation.
2 Aspectual Type
Distinctions of aspectual type (also referred to as
situation type, lexical aspect or Aktionsart) of the
sort of Vendler (1967) and Dowty (1979) are ex-
pected to improve the existing solutions to the
problem of temporal relation classification. The
major aspectual distinctions are between (i) states
(e.g. to hate beer, to know the answer, to own a
car, to stink), (ii) processes, also called activities
(to work, to eat ice cream, to grow, to play the
piano), (iii) culminated processes, also called ac-
complishments (to paint a picture, to burn down,
to deliver a sermon) and (iv) culminations, also
called achievements (to explode, to win the game,
to find the key). States and processes are atelic
situations in that they do not make salient a spe-
cific instant in time. Culminated processes and
culminations are telic situations: they have an in-
trinsic, instantaneous endpoint, called the culmi-
nation (e.g. in the case of to paint a picture, it is
the moment when the picture is ready; in the case
of to explode, it is the moment of the explosion).
There are several reasons to think aspectual
267
type is relevant to temporal information pro-
cessing. First, these distinctions are related to
how long events last: culminations are punctual,
whereas states can be very prolonged in time.
States are thus more likely to temporally overlap
other temporal entities than culminations, for in-
stance.
Second, there are grammatical consequences
on how events are anchored in time. Consider
the following examples, from Ritchie (1979) and
Moens and Steedman (1988):
(1) When they built the 59th Street bridge,
they used the best materials.
(2) When they built that bridge, I was still a
young lad.
The situation of building the bridge is a cul-
minated processed, composed by the process of
actively building a bridge followed by the culmi-
nation of the bridge being finished. In sentence
(1), the event described in the main clause (that of
using the best materials) is a process, but in sen-
tence (2) it is a state (the state of being a young
lad). Even though the two clauses in each sen-
tence are connected by when, the temporal rela-
tions holding between the events of each clause
are different. On the one hand, in sentence (1)
the event of using the best materials (a process)
overlaps with the process of actively building the
bridge and precedes the culmination of finishing
the bridge. On the other hand, in sentence (2)
the event of being a young lad (which is a state)
overlaps with both the process of actively build-
ing the bridge and the culmination of the bridge
being built. This difference is arguably caused by
the different aspectual types of the main events of
each sentence.
As another example, states overlap with tem-
poral location adverbials, as in (3), while culmi-
nations are included in them, as in (4).
(3) He was happy last Monday.
(4) He reached the top of Mount Everest last
Monday.
In other cases, differences in aspectual type can
disambiguate ambiguous linguistic material. For
instance, the preposition in is ambiguous as it can
be used to locate events in the future but also to
measure the duration of culminated processes; it
is thus ambiguous with culminated processes, as
in he will read the book in three days but not with
other aspectual types, as in he will be living there
in three days.
A factor related to aspectual class, that is not
trivial to account for, is the phenomenon of as-
pectual shift, or aspectual coercion (Moens and
Steedman, 1988; de Swart, 1998; de Swart, 2000).
Many linguistic contexts pose constraints on as-
pectual type. This does not mean, however, that
clashes of aspectual type cause ungrammatical-
ity. What often happens is that phrases associated
with an incompatible aspectual type get their type
changed in order to be of the required type, caus-
ing a change in meaning.
For instance, the progressive construction com-
bines with processes. When it combines with e.g.
a culminated process, the culmination is stripped
off from this culminated process, which is thus
converted into a process. The result is that a sen-
tence like (5) does not say that the bridge was fin-
ished (the event has no culmination), whereas one
such as (6) does say this (the event has a culmina-
tion).
(5) They were building that bridge.
(6) They built that bridge.
Aspectual type is not a property of just words,
but phrases as well. For example, while the
progressive construction just mentioned combines
with processes, the resulting phrase behaves as a
state (cf. the sentence When they built the 59th
Street bridge, they were using the best materi-
als and what was mentioned above about when
clauses).
3 Strategy
Aspectual type is hard to annotate. This is partly
because of what was just mentioned: it is not a
property of just words, but rather phrases, and
different phrases with the same head word can
have different aspectual types; however anno-
tation schemes like TimeML annotate the head
word as denoting events, not full phrases or
clauses.
For this reason, our strategy is to obtain aspec-
tual type information from unannotated data. Be-
cause these data are gradient?an event-denoting
word can be associated with different aspectual
types, depending on word sense?we do not aim
to extract categorical information, but rather nu-
268
meric values for each event term that reflect as-
sociations to aspectual types. These may be seen
as values that are indicative of the frequencies in
which an event term denotes a state, or a process,
etc.
In order to extract these indicators, we resort to
a methodology sometimes referred to as Google
Hits: large amounts of queries are sent to a web
search engine (not necessarily Google), and the
number of search results (the number of web
pages that match the query) is recorded and taken
as a measure of the frequency of the queried ex-
pression.
This methodology is not perfect, since multiple
occurrences of the queried expression in the same
web page are not reflected in the hit count, and
in many cases the hit counts reported by search
engines are just estimates and might not be very
accurate. Additionally, uncarefully formulated
queries can match expressions that are syntacti-
cally and semantically very different from what
was intended. In any case, it has the advantages
of being based on a very large amount of data and
not requiring any manual annotation, which can
introduce errors.
3.1 The Web as a Very Large Corpus
Hearst (1992) is one of the earliest studies where
specific textual patterns are used to extract lexico-
semantic information from very large corpora.
The author?s goal was to extract hyponymy rela-
tions. With the same goal, Kozareva et al(2008)
apply similar textual patterns to the web.
The web has been used as a corpus by many
other authors with the purpose of extracting syn-
tactic or semantic properties of words or re-
lations between them, e.g. Ravichandran and
Hovy (2002), Etzioni et al(2004), etc. Some
of this work is specially relevant to the problem
of temporal information processing. VerbOcean
(Chklovski and Pantel, 2004) is a database of
web mined relations between verbs. Among other
kinds of relations, it includes typical precedence
relations, e.g. sleeping happens before waking up.
This type of information has in fact been used by
some of the participating systems of TempEval-2
(Ha et al 2010), with good results.
More generally, there is a large body of work
focusing on lexical acquisition from corpora. Just
as an example, Mayol et al(2005) learn subcate-
gorization frames of verbs from large amounts of
data. Relevant to our work is that of Siegel and
McKeown (2000). The authors guess the aspec-
tual type of verbs by searching for specific pat-
terns in a one million word corpus that has been
syntactically parsed. They extract several linguis-
tic indicators and combine them with machine
learning algorithms. The indicators that they ex-
tract are naturally different from ours, since they
have access to syntactic structure and we do not,
but our data are based on a much larger corpus.
3.2 Textual Patterns as Indicators of
Aspectual Type
Because of aspectual shift phenomena (see Sec-
tion 2), full syntactic parsing is necessary in order
to determine the aspectual type of a natural lan-
guage expression. However, this can be approxi-
mated by frequencies: it is natural to expect that
e.g. stative verbs occur more frequently in stative
contexts than non-stative verbs, even if there may
be errors in determining these contexts if syntactic
parsing is not a possibility.
If one uses Google Hits, syntactic information
is not accessible. In return for its impreciseness,
Google Hits have the advantage of being based on
very large amounts of data.
4 Scope and Approach
In this study we focus exclusively on verbs, but
events can be denoted by words belonging to
other parts-of-speech. This limitation is linked to
the fact that the textual patterns that are used to
search for specific aspectual contexts are sensitive
to part-of-speech (i.e. what may work for a verb
may not work equally well for a noun).
In order to assess whether aspectual type in-
formation is relevant to the problem of temporal
relation classification, our approach is to check
whether incorporating that kind of information
into existing solutions for this problem can im-
prove their performance. TimeML annotated
data, such as those used for TempEval, can be
used to train machine learned classifiers. These
can then be augmented with attributes encoding
aspectual type information and their performance
compared to the original classifiers.
Additionally, we work with Portuguese data.
This is because our work is part of an effort to
implement a temporal processing system for Por-
tuguese. We briefly describe the data next.
269
<s>Em Washington, <TIMEX3 tid="t53" type="DATE"
value="1998-01-14">hoje</TIMEX3>, a Federal Aviation
Administration <EVENT eid="e1" class="OCCURRENCE"
stem="publicar" aspect="NONE" tense="PPI"
polarity="POS" pos="VERB">publicou</EVENT>
gravac?o?es do controlo de tra?fego ae?reo da <TIMEX3
tid="t54" type="TIME"
value="1998-XX-XXTNI">noite</TIMEX3> em que o voo
TWA800 <EVENT eid="e2" class="OCCURRENCE"
stem="cair" aspect="NONE" tense="PPI"
polarity="POS" pos="VERB">caiu</EVENT>.</s>
<TLINK lid="l1" relType="BEFORE" eventID="e2"
relatedToTime="t53"/>
<TLINK lid="l2" relType="OVERLAP"
eventID="e2" relatedToTime="t54"/>
Figure 2: Sample of the Portuguese data adapted from
the TempEval data, corresponding to the fragment: Em
Washington, hoje, a Federal Aviation Administration
publicou gravac?o?es do controlo de tra?fego ae?reo da
noite em que o voo TWA800 caiu.
4.1 Data
Our experiments used TimeBankPT (Costa and
Branco, 2010; Costa and Branco, 2012; Costa, to
appear). This corpus is an adaptation of the orig-
inal TempEval data to Portuguese, obtained by
translating it and then adapting the annotations.
Figure 2 shows the Portuguese equivalent to the
sample presented above in Figure 1. The two cor-
pora are quite similar, but there is of course the
language difference. TimeBankPT contains a few
corrections to the data (mostly the temporal rela-
tions), but these corrections only changed around
1.2% of the total number of annotated temporal
relations (Costa and Branco, 2012). Although we
did not test our results on English data, we specu-
late that our results carry over to other languages.
Just like the original English corpus for
TempEval, it is divided in a training part and a
testing part. The numbers (sentences, words, an-
notated events, time expressions and temporal re-
lations) are fairly similar for the two corpora (the
English one and the Portuguese one).
4.2 Extracting the Aspectual Indicators
We extracted the 4,000 most common verbs from
a 180 million word corpus of Portuguese news-
paper text, CETEMPu?blico. Because this corpus
is not annotated, we used a part-of-speech tag-
ger and morphological analyzer (Barreto et al
2006; Silva, 2007) to detect verbs and to obtain
their dictionary form. We then used an inflection
tool (Branco et al 2009) to generate the specific
verb forms that are used in the queries. They are
mostly third person singular forms of several dif-
ferent tenses.
The indicators that we used are ratios of Google
Hits. They compare two queries.
Several indicators were tested. We provide ex-
amples with the verb fazer ?do? for the queries
being compared by each indicator. The name of
each indicator reflects the aspectual type being
tested, i.e. states should present high values for
State Indicators 1 and 2, processes should show
high values for Process Indicators 1?4, etc.
? State Indicator 1 (Indicator S1) is about im-
perfective and perfective past forms of verbs.
It compares the number of hits a for an im-
perfective form fazia ?did? to the number of
hits b for a perfective form fez ?did?: aa+b .
Assuming the imperfective past constrains
the entire clause to be a state, and the perfec-
tive past constrains it to be telic, the higher
this value the more frequently the verb ap-
pears in stative clauses in a past tense.2
? State Indicator 2 (Indicator S2) is about the
co-occurrence with acaba de ?has just fin-
ished?. It compares the number of hits a
for acaba de fazer ?has just finished doing?
to the number of hits b for fazer ?to do?:
b
a+b . In Portuguese, this construction does
not seem to be felicitous with states.
? Process Indicator 1 (Indicator P1) is about
past progressive forms and simple past forms
(both imperfective). It compares the num-
ber of hits a for fazia ?did? to the number of
hits b for estava a fazer ?was doing?: ba+b .
Assuming the progressive construction is a
function from processes to states (see Sec-
tion 2), the higher this value, the more likely
the verb can occur with the interpretation of
a process.
2We expect this frequency to be indicative of states be-
cause states can appear in the imperfective past tense with
their interpretation unchanged, whereas non-stative events
have their interpretation shifted to a stative one in that con-
text (e.g. they get a habitual reading). In order to refer to an
event occurring in the past with an on-going interpretation,
non-stative verbs require the progressive construction to be
used in Portuguese, whereas states do not. Therefore, states
should occur more freely in the simple imperfective past.
270
? Process Indicator 2 (Indicator P2) is about
past progressive forms vs. simple past forms
(perfective). It compares the number of hits
a for fez ?did? to the number of hits b for
esteve a fazer ?was doing?: ba+b . Similarly
to the previous indicator, this one tests the
frequency of a verb appearing in a context
typical of processes.
? Process Indicator 3 (Indicator P3) is about
the occurrence of for Adverbials. It com-
pares the number of hits a for fez ?did? to
the number of hits b for fez durante muito
tempo ?did for a long time?: ba+b . This
number is also intended to be an indica-
tion of how frequent a verb can be used
with the interpretation of a process. Note
that Portuguese allows modifiers to occur
freely between a verb and its complements,
so this test should work for transitive verbs
(or any other subcategorization frame involv-
ing complements), not just intransitive ones.
? Process Indicator 4 (Indicator P4) is about
the co-occurrence of a verb with parar de ?to
stop?. It compares the number of hits a for
parou de fazer ?stopped doing? to the num-
ber of hits b for fazer ?to do?: aa+b . Just like
the English verbs stop and finish are sensitive
to the aspectual type of their complement, so
is the Portuguese verb parar, which selects
for processes.
? Atelicity Indicator 1 (Indicator A1) is about
comparing in and for adverbials. It compares
the number of hits a for fez num instante ?did
in an instant? to the number of hits b for fez
durante muito tempo ?did for a long time?:
b
a+b . Processes can be modified by for ad-
verbials, whereas culminated processes are
modified by in adverbials. This indicator
tests the occurrence of a verb in contexts that
require these aspectual types.
? Atelicity Indicator 2 (Indicator A2) is about
comparing for Adverbials with suddenly. It
compares the number of hits a for fez de re-
pente ?did suddenly? to the number of hits
b for fez durante muito tempo ?did for a
long time?: ba+b . De repente ?suddenly?
seems to modify culminations, so this indi-
cator compares process readings with culmi-
nation readings.
? Culmination Indicator1 (Indicator C1) is
about differentiating culminations and cul-
minated processes. It compares the number
of hits a for fez de repente ?did suddenly? to
the number of hits b for fez num instante ?did
in an instant?: aa+b .
For each of the 4,000 verbs, the necessary
queries required by these indicators were gener-
ated and then sent to a search engine. The queries
were enclosed in quotes, so as to guarantee ex-
act matches. The number of hits was recorded for
each query.
We had some problems with outliers for a few
rather infrequent verbs. These could show very
extreme values for some indicators. In order
to minimize their impact, for each indicator we
homogenized the 100 highest values that were
found. More specifically, for each indicator, each
one of the highest 100 values was replaced by the
100th highest value. The bottom 100 values were
similarly changed. This way the top 99 values and
the bottom 99 values are replaced by the 100th
highest value and the 100th lowest value respec-
tively.
Each indicator ranges between 0 and 1 in the-
ory. In practice, we seldom find values close to the
extremes, as this would imply that some queries
would have close to 0 hits, which does not occur
very often (after all, we intentionally used queries
for which we would expect large hit counts, as
these are more likely to be representative of true
language use). For this reason, each indicator is
scaled so that its minimum (actual) value is 0 and
its maximum (actual) value is 1.
5 Evaluation
As mentioned before, in order to assess the use-
fulness of these aspectual indicators for the tasks
of temporal relation classification, we checked
whether they can improve machine learned clas-
sifiers trained for this problem. We next describe
the classifiers that were used as the bases for com-
parison.
5.1 Experimental Setup
In order to obtain bases for comparison, we
trained machine learned classifiers on the Por-
tuguese corpus TimeBankPT, that is adapted from
the TempEval data (see Section 4.1). We took
inspiration in the work of Hepple et al(2007).
271
This was one of the participating systems of
TempEval. It used machine learning algorithms
implemented in Weka (Witten and Frank, 1999).
For our experiments, we used Weka?s implemen-
tation of the C4.5 algorithm, trees.J48 (Quin-
lan, 1993), the RIPPER algorithm as implemented
by Weka?s rules.JRip (Cohen, 1995), a near-
est neighbors classifier, lazy.KStar (Cleary
and Trigg, 1995), a Na??ve Bayes classifier, namely
Weka?s bayes.NaiveBayes (John and Lang-
ley, 1995), and a support vector classifier, Weka?s
functions.SMO (Platt, 1998) . We chose these
algorithms as they are representative of a wide
range of machine learning approaches.
Recall that the tasks of TempEval are to guess
the type of temporal relations. Each train or test
instance thus corresponds to a temporal relation,
i.e. a TLINK element in the TimeML annota-
tions (see Figures 1 and 2). The classification
problem is to determine the value of the attribute
relType of TimeML TLINK elements. These
temporal relations relate an event (referred by the
eventID attribute of TLINK elements) to an-
other temporal entity, that can be a time (pointed
to by the relatedToTime attribute), in the case
of tasks A and B, or, in the case of task C, an-
other event (given by the relatedToEvent at-
tribute).
As for the features that were employed, we also
took inspiration in the approach of Hepple et al
(2007). These authors used as classifier attributes
two types of features. The first group of features
corresponds to TimeML attributes: for instance
the value of the aspect attribute of EVENT el-
ements, for the events involved in the temporal
relation to be classified. The second group of fea-
tures corresponds to simple features that can be
computed with string manipulation and do not re-
quire any kind of natural language processing.
Table 2 shows the features that were tried and
employed.
The event features correspond to attributes
of EVENT elements, with the exception of
the event-string feature, which takes as
value the character data inside the correspond-
ing TimeML EVENT element. In a simi-
lar spirit, the timex3 features are taken from
the attributes of TIMEX3 elements with the
same name. The tlink-relType feature
is the class attribute and corresponds to the
relType attribute of the TimeML TLINK el-
Task
Attribute A B C
event-aspect ? X X
event-polarity X X X
event-POS ? ? X
event-stem ? X ?
event-string X ? ?
event-class X ? X
event-tense X X X
order-event-first X N/A N/A
order-event-between X N/A N/A
order-timex3-between ? N/A N/A
order-adjacent X N/A N/A
timex3-mod X ? N/A
timex3-type ? ? N/A
tlink-relType X X X
Table 2: Feature combinations used in the classifiers
used as comparison bases. Features inspired by the
ones used by Hepple et al(2007) in TempEval.
ement that represents the temporal relation to
be classified. The order features are the at-
tributes computed from the document?s textual
content. The feature order-event-first
encodes whether the event terms precedes in
the text the time expression it is related to by
the temporal relation to classify. The clas-
sifier attribute order-event-between de-
scribes whether any other event is mentioned
in the text between the two expressions for
the entities that are in the temporal relation,
and similarly order-timex3-between is
about whether there is an intervening tempo-
ral expression. Finally, order-adjacent is
true iff both order-timex3-between and
order-event-between are false (even if
other linguistic material occurs between the ex-
pressions denoting the two entities in the temporal
relation).
In order to arrive at the final set of features
(marked with a check mark in Table 2), we per-
formed exhaustive search on all possible combi-
nations of these features for each task, using the
Na??ve Bayes algorithm. They were compared us-
ing 10-fold cross-validation on the training data.
The feature combinations shown in Table 2 are
the optimal combinations arrived at in this way.
These are the classifiers that we used for the
272
comparison with the aspectual type indicators.
We chose this straightforward approach because it
forms a basis for comparison that is easily repro-
ducible: the algorithm implementations that were
used are part of freely available software, and the
features that were employed are easily computed
from the annotated data, with no need to run any
natural language processing tools whatsoever.
As mentioned before in Section 4.1, the data
used are organized in a training set and an evalu-
ation set. The training part is around 60K words
long, the test data containing around 9K words.
When tested on held-out data, these classifiers
present the scores shown in italics in Table 3.
These results are fairly similar to the scores that
the system of Hepple et al(2007) obtained in
TempEval with English data: 0.59 for task A, 0.73
for task B, and 0.54 for task C. They are also not
very far from the best results of TempEval. As
such they represent interesting bases for compar-
ison, as improving their performance is likely to
be relevant to the best systems that have been de-
veloped for temporal information processing.
5.2 Results and Discussion
After obtaining the bases for comparison de-
scribed above, we proceeded to check whether the
aspectual type indicators described in Section 4.2
can improve these results.
For each aspectual indicator, we implemented
a classifier feature that encodes its value for the
event term in the temporal relation (if it is not a
verb, this value is missing). In the case of task C,
two features are added for each indicator, one for
each event term.
We extended each of these classifiers with one
of these features at a time (two in the case of task
C), and checked whether it improved the results
on the test data. So for instance, in order to test
Indicator S1, we extended each of these classifiers
with a feature that encodes the value that this indi-
cator presents for the term that denotes the event
present in the temporal relation to be classified.
In the case of task C, two classifier features are
added, one for each event term, and both for the
same Indicator S1. For instance, for the (train-
ing) instance corresponding to the TLINK in Fig-
ure 2 with the lid attribute that has the value l1,
the classifier feature for Indicator S1 has the value
that was computed for the verb cair ?go down?,
since this is the stem of the word that denotes
Task
Classifier A B C
trees.J48 0.57 0.77 0.53
With best indicator 0.55
rules.JRip 0.60 0.76 0.51
With best indicator 0.61 0.54
lazy.KStar 0.54 0.70 0.52
With best indicator 0.73 0.53
bayes.NaiveBayes 0.50 0.76 0.53
With best indicator 0.53 0.54
functions.SMO 0.55 0.79 0.54
With best indicator 0.56 0.55
Table 3: Evaluation on held-out test data of classi-
fiers trained on full train data. Values for the classi-
fiers used as comparison bases are in italics. Boldface
highlights improvements resulting from incorporating
aspectual indicators as classifier features, and missing
values represent no improvement.
the event that is the first argument of this temporal
relation. After adding each of these features, we
retrained the classifiers on the training data and
tested them on the held-out test data. In order to
keep the evaluation manageable, we did not test
combinations of multiple indicators.
Table 3 shows the overall results. For task
A, the best indicators were P4 (with JRip), A1
(NaiveBayes) and S1 (SMO). For task B the
best one was P4 (KStar). For task C, the best
indicators were P3 (J48), A1 and P3 (JRip),
C1 (KStar), A1 (NaiveBayes) and P2 (SMO).
Each of the indicators S2, P1 and A2 either does
not improve the results or does so but not as much
as another, better indicator for the same task and
algorithm.
It seems clear from Table 3 that some tasks ben-
efit from these indicators more than others. In
particular, task C shows consistent improvements
whereas task B is hardly affected. Since task C
is about relations involving two events, the classi-
fiers may be picking up the sort of linguistic gen-
eralizations mentioned in Section 2 about when
clauses.
J48 and JRip produce human-readable mod-
els. We checked how these classifiers are taking
advantage of the aspectual indicators. For task C,
the induced models are generally associating high
273
values of the indicators A1 and P3 with overlap
relations and low values of these indicators with
other types of relations. This is expected. On the
one end, high values for these indicators are asso-
ciated with atelicity (i.e. the endpoint of the cor-
responding event is not presented). On the other
hand, both indicators are based on queries con-
taining the phrase durante muito tempo ?for a long
time?, which, in addition to picking up events that
can be modified by for adverbials, more specifi-
cally pick up events that happen for a long time
and are thus likely to overlap other events.
For task A, JRip also associates high values of
the indicator P4?which constitute evidence that
the corresponding events are processes (which are
atelic)?with overlap relations. This is a specially
interesting result, considering that the queries on
which this indicator is based reflect a purely as-
pectual constraint.
6 Concluding Remarks
In this paper, we evaluated the relevance of infor-
mation about aspectual type for temporal process-
ing tasks.
Temporal information processing has received
substantial attention recently with the two
TempEval challenges in 2007 and 2010. The most
interesting problem of temporal information pro-
cessing, that of temporal relation classification, is
still affected by high error rates.
Even though a very substantial part of the se-
mantics literature on tense and aspect focuses on
aspectual type, solutions to the problem of auto-
matic temporal relation classification have not in-
corporated this sort of semantic information. In
part this is expected, as aspectual type is very in-
terconnected with syntax (cf. the discussion about
aspectual coercion in Section 2), and the phe-
nomenon of aspect shift can make it hard to com-
pute even when syntactic information is available.
Our contribution with this paper is to incor-
porate this sort of information in existing ma-
chine learned classifiers that tackle this problem.
Even though these classifiers do not have access to
syntactic information, aspectual type information
seemed to be useful in improving the performance
of these models. We hypothesize that combin-
ing aspectual type information with information
about syntactic structure can further improve the
problems of temporal information processing, but
we leave that research to future work.
An interesting question that we hope will be ad-
dressed by future work is how these results extend
to other languages. We cannot provide an answer
to this question, as we do not have the data. How-
ever, this experiment can be replicated for any lan-
guage that has (i) TimeML annotated data, (ii) a
reasonable size of documents on the Web and a
search engine capable of separating them from the
documents in other languages and (iii) an aspec-
tual system similar enough that the question be-
ing addressed in this paper makes sense (and use-
ful patterns for queries can be constructed, even
if not entirely identical to the ones that we used).
The second criterion is met by many, many lan-
guages. The third one also seems to affect many
languages, as the existing literature on aspectual
phenomena indicates that these phenomena are
quite widespread. The second criterion is, at the
moment, the hardest to fulfill as not many lan-
guages have data with rich annotations about time
(i.e. including events and temporal relations). We
speculate that our results can extend to English,
although a different set of query patterns may
have to be used in order to extract the aspectual
indicators that are employed. We believe this be-
cause the two languages largely overlap when it
comes to aspectual phenomena.
References
Florbela Barreto, Anto?nio Branco, Eduardo Ferreira,
Ama?lia Mendes, Maria Fernanda Nascimento, Fil-
ipe Nunes, and Joa?o Silva. 2006. Open resources
and tools for the shallow processing of Portuguese:
the TagShare project. In Proceedings of LREC
2006.
Anto?nio Branco, Francisco Costa, Eduardo Ferreira,
Pedro Martins, Filipe Nunes, Joa?o Silva, and Sara
Silveira. 2009. LX-Center: a center of online lin-
guistic services. In Proceedings of the Demo Ses-
sion, ACL-IJCNLP2009, Singapore.
Timothy Chklovski and Patrick Pantel. 2004. Verb-
Ocean: Mining the Web for fine-grained semantic
verb relations. In In Proceedings of EMNLP-2004,
Barcelona, Spain.
John G. Cleary and Leonard E. Trigg. 1995. K*: An
instance-based learner using an entropic distance
measure. In 12th International Conference on Ma-
chine Learning, pages 108?114.
William W. Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 115?123.
Francisco Costa and Anto?nio Branco. 2010. Tempo-
ral information processing of a new language: Fast
274
porting with minimal resources. In Proceedings of
ACL 2010.
Francisco Costa and Anto?nio Branco. 2012. Time-
BankPT: A TimeML annotated corpus of Por-
tuguese. In Proceedings of LREC2012.
Francisco Costa. to appear. Processing Temporal In-
formation in Unstructured Documents. Ph.D. the-
sis, Universidade de Lisboa, Lisbon.
Henrie?tte de Swart. 1998. Aspect shift and coercion.
Natural Language and Linguistic Theory, 16:347?
385.
Henrie?tte de Swart. 2000. Tense, aspect and coer-
cion in a cross-linguistic perspective. In Proceed-
ings of the Berkeley Formal Grammar conference,
Stanford. CSLI Publications.
David R. Dowty. 1979. Word Meaning and Montague
Grammar: the Semantics of Verbs and Times in
Generative Semantics and Montague?s PTQ. Rei-
del, Dordrecht.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, , Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in Know-
ItAll. In Proceedings of the 13th International Con-
ference on World Wide Web.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling temporal
relations with Markov logic and lexical ontology. In
Proceedings of SemEval 2010.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics,
volume 2, pages 539?545, Nantes, France.
Mark Hepple, Andrea Setzer, and Rob Gaizauskas.
2007. USFD: Preliminary exploration of fea-
tures and classifiers for the TempEval-2007 tasks.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
George H. John and Pat Langley. 1995. Estimating
continuous distributions in Bayesian classifiers. In
Eleventh Conference on Uncertainty in Artificial In-
telligence, pages 338?345, San Mateo.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056, Columbus, Ohio.
Association for Computational Linguistics.
Laia Mayol, Gemma Boleda, and Toni Badia. 2005.
Automatic acquisition of syntactic verb classes with
basic resources. Language Resources and Evalua-
tion, 39(4):295?312.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to
temporal relation identification in news text. pages
219?222.
Marc Moens and Mark Steedman. 1988. Temporal
ontology and temporal reference. Computational
Linguistics, 14(2):15?28.
John Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimiza-
tion. In Bernhard Scho?lkopf, Chris Burges, and
Alexander J. Smola, editors, Advances in Kernel
Methods?Support Vector Learning.
Georgiana Pus?cas?u. 2007. WVALI: Temporal rela-
tion identification by syntactico-semantic analysis.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust specification of
event and temporal expressions in text. In IWCS-
5, Fifth International Workshop on Computational
Semantics.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question an-
swering system. In Proceedings of ACL 2002.
Graeme D. Ritchie. 1979. Temporal clauses in En-
glish. Theoretical Linguistics, 6:87?115.
Eric V. Siegel and Kathleen McKeown. 2000.
Learning methods to combine linguistic indica-
tors: Improving aspectual classification and reveal-
ing linguistic insights. Computational Linguistics,
24(4):595?627.
Joa?o Ricardo Silva. 2007. Shallow processing
of Portuguese: From sentence chunking to nomi-
nal lemmatization. Master?s thesis, Faculdade de
Cie?ncias da Universidade de Lisboa, Lisbon, Portu-
gal.
Zeno Vendler. 1967. Verbs and times. Linguistics in
Philosophy, pages 97?121.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, and James Pustejovsky. 2007.
SemEval-2007 Task 15: TempEval temporal re-
lation identification. In Proceedings of SemEval-
2007.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The TempEval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of SemEval-2010.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann,
San Francisco.
275
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 671?677,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Temporal information processing of a new language:
fast porting with minimal resources
Francisco Costa and Anto?nio Branco
Universidade de Lisboa
Abstract
We describe the semi-automatic adapta-
tion of a TimeML annotated corpus from
English to Portuguese, a language for
which TimeML annotated data was not
available yet. In order to validate this
adaptation, we use the obtained data to
replicate some results in the literature that
used the original English data. The fact
that comparable results are obtained indi-
cates that our approach can be used suc-
cessfully to rapidly create semantically an-
notated resources for new languages.
1 Introduction
Temporal information processing is a topic of nat-
ural language processing boosted by recent eval-
uation campaigns like TERN2004,1 TempEval-1
(Verhagen et al, 2007) and the forthcoming
TempEval-22 (Pustejovsky and Verhagen, 2009).
For instance, in the TempEval-1 competition, three
tasks were proposed: a) identifying the temporal
relation (such as overlap, before or after) hold-
ing between events and temporal entities such as
dates, times and temporal durations denoted by ex-
pressions (i.e. temporal expressions) occurring in
the same sentence; b) identifying the temporal re-
lation holding between events expressed in a doc-
ument and its creation time; c) identifying the tem-
poral relation between the main events expressed
by two adjacent sentences.
Supervised machine learning approaches are
pervasive in the tasks of temporal information pro-
cessing. Even when the best performing sys-
tems in these competitions are symbolic, there are
machine learning solutions with results close to
their performance. In TempEval-1, where there
were statistical and rule-based systems, almost
1http://timex2.mitre.org
2http://www.timeml.org/tempeval2
all systems achieved quite similar results. In the
TERN2004 competition (aimed at identifying and
normalizing temporal expressions), a symbolic
system performed best, but since then machine
learning solutions, such as (Ahn et al, 2007), have
appeared that obtain similar results.
These evaluations made available sets of anno-
tated data for English and other languages, used
for training and evaluation. One natural question
to ask is whether it is feasible to adapt the training
and test data made available in these competitions
to other languages, for which no such data still ex-
ist. Since the annotations are largely of a seman-
tic nature, not many changes need to be done in
the annotations once the textual material is trans-
lated. In essence, this would be a fast way to create
temporal information processing systems for lan-
guages for which there are no annotated data yet.
In this paper, we report on an experiment
that consisted in adapting the English data of
TempEval-1 to Portuguese. The results of ma-
chine learning algorithms over the data thus ob-
tained are compared to those reported for the En-
glish TempEval-1 competition. Since the results
are quite similar, this permits to conclude that
such an approach can rapidly generate relevant and
comparable data and is useful when porting tem-
poral information processing solutions to new lan-
guages.
The advantages of adapting an existing corpus
instead of annotating text from scratch are: i)
potentially less time consuming, if it is faster to
translate the original text than it is to annotate
new text (this can be the case if the annotations
are semantic and complex); b) the annotations can
be transposed without substantial modifications,
which is the case if they are semantic in nature;
c) less man power required: text annotation re-
quires multiple annotators in order to guarantee
the quality of the annotation tags, translation of
the markables and transposition of the annotations
671
in principle do not; d) the data obtained are com-
parable to the original data in all respects except
for language: genre, domain, size, style, annota-
tion decisions, etc., which allows for research to
be conducted with a derived corpus that is compa-
rable to research using the original corpus. There
is of course the caveat that the adaptation process
can introduce errors.
This paper proceeds as follows. In Section 2,
we provide a quick overview of the TimeML an-
notations in the TempEval-1 data. In Section 3,
it is described how the data were adapted to Por-
tuguese. Section 4 contains a brief quantitative
comparison of the two corpora. In Section 5, the
results of replicating one of the approaches present
in the TempEval-1 challenge with the Portuguese
data are presented. We conclude this paper in Sec-
tion 6.
2 Brief Description of the Annotations
Figure 1 contains an example of a document from
the TempEval-1 corpus, which is similar to the
TimeBank corpus (Pustejovsky et al, 2003).
In this corpus, event terms are tagged with
<EVENT>. The relevant attributes are tense,
aspect, class, polarity, pos, stem. The
stem is the term?s lemma, and pos is its part-of-
speech. Grammatical tense and aspect are encoded
in the features tense and aspect. The attribute
polarity takes the value NEG if the event term
is in a negative syntactic context, and POS other-
wise. The attribute class contains several lev-
els of information. It makes a distinction between
terms that denote actions of speaking, which take
the value REPORTING and those that do not.
For these, it distinguishes between states (value
STATE) and non-states (value OCCURRENCE),
and it also encodes whether they create an in-
tensional context (value I STATE for states and
value I ACTION for non-states).
Temporal expressions (timexes) are inside
<TIMEX3> elements. The most important fea-
tures for these elements are value, type and
mod. The timex?s value encodes a normal-
ized representation of this temporal entity, its
type can be e.g. DATE, TIME or DURATION.
The mod attribute is optional. It is used for ex-
pressions like early this year, which are anno-
tated with mod="START". As can be seen in
Figure 1 there are other attributes for timexes
that encode whether it is the document?s creation
time (functionInDocument) and whether its
value can be determined from the expression
alone or requires other sources of information
(temporalFunction and anchorTimeID).
The <TLINK> elements encode temporal re-
lations. The attribute relType represents the
type of relation, the feature eventID is a ref-
erence to the first argument of the relation.
The second argument is given by the attribute
relatedToTime (if it is a time interval or du-
ration) or relatedToEvent (if it is another
event; this is for task C). The task feature is the
name of the TempEval-1 task to which this tempo-
ral relation pertains.
3 Data Adaptation
We cleaned all TimeML markup in the
TempEval-1 data and the result was fed to
the Google Translator Toolkit.3 This tool com-
bines machine translation with a translation
memory. A human translator corrected the
proposed translations manually.
After that, we had the three collections of docu-
ments (the TimeML data, the English unannotated
data and the Portuguese unannotated data) aligned
by paragraphs (we just kept the line breaks from
the original collection in the other collections). In
this way, for each paragraph in the Portuguese data
we know all the corresponding TimeML tags in
the original English paragraph.
We tried using machine translation software (we
used GIZA++ (Och and Ney, 2003)) to perform
word alignment on the unannotated texts, which
would have enabled us to transpose the TimeML
annotations automatically. However, word align-
ment algorithms have suboptimal accuracy, so the
results would have to be checked manually. There-
fore we abandoned this idea, and instead we sim-
ply placed the different TimeML markup in the
correct positions manually. This is possible since
the TempEval-1 corpus is not very large. A small
script was developed to place all relevant TimeML
markup at the end of each paragraph in the Por-
tuguese text, and then each tag was manually repo-
sitioned. Note that the <TLINK> elements always
occur at the end of each document, each in a sep-
arate line: therefore they do not need to be reposi-
tioned.
During this manual repositioning of the anno-
tations, some attributes were also changed man-
3http://translate.google.com/toolkit
672
<?xml version="1.0" ?>
<TempEval>
ABC<TIMEX3 tid="t52" type="DATE" value="1998-01-14" temporalFunction="false"
functionInDocument="CREATION_TIME">19980114</TIMEX3>.1830.0611
NEWS STORY
<s>In Washington <TIMEX3 tid="t53" type="DATE" value="1998-01-14" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t52">today</TIMEX3>, the Federal Aviation Administration <EVENT
eid="e1" class="OCCURRENCE" stem="release" aspect="NONE" tense="PAST" polarity="POS" pos="VERB">released
</EVENT> air traffic control tapes from <TIMEX3 tid="t54" type="TIME" value="1998-XX-XXTNI"
temporalFunction="true" functionInDocument="NONE" anchorTimeID="t52">the night</TIMEX3> the TWA Flight
eight hundred <EVENT eid="e2" class="OCCURRENCE" stem="go" aspect="NONE" tense="PAST" polarity="POS"
pos="VERB">went</EVENT>down.</s>
...
<TLINK lid="l1" relType="BEFORE" eventID="e2" relatedToTime="t53" task="A"/>
<TLINK lid="l2" relType="OVERLAP" eventID="e2" relatedToTime="t54" task="A"/>
<TLINK lid="l4" relType="BEFORE" eventID="e2" relatedToTime="t52" task="B"/>
...
</TempEval>
Figure 1: Extract of a document contained in the training data of the first TempEval-1
ually. In particular, the attributes stem, tense
and aspect of <EVENT> elements are language
specific and needed to be adapted. Sometimes, the
pos attribute also needs to be changed, since e.g.
a verb in English can be translated as a noun in
Portuguese. The attribute class of the same kind
of elements can be different, too, because natural
sounding translations are sometimes not literal.
3.1 Annotation Decisions
When porting the TimeML annotations from En-
glish to Portuguese, a few decisions had to be
made. For illustration purposes, Figure 2 contains
the Portuguese equivalent of the extract presented
in Figure 1.
For <TIMEX3> elements, the issue is that if the
temporal expression to be annotated is a preposi-
tional phrase, the preposition should not be inside
the <TIMEX3> tags according to the TimeML
specification. In the case of Portuguese, this raises
the question of whether to leave contractions of
prepositions with determiners outside these tags
(in the English data the preposition is outside and
the determiner is inside).4 We chose to leave them
outside, as can be seen in that Figure. In this ex-
ample the prepositional phrase from the night/da
noite is annotated with the English noun phrase
the night inside the <TIMEX3> element, but the
Portuguese version only contains the noun noite
inside those tags.
For <EVENT> elements, some of the attributes
are adapted. The value of the attribute stem is
4The fact that prepositions are placed outside of temporal
expressions seems odd at first, but this is because in the orig-
inal TimeBank, from which the TempEval data were derived,
they are tagged as <SIGNAL>s. The TempEval-1 data does
not contain <SIGNAL> elements, however.
obviously different in Portuguese. The attributes
aspect and tense have a different set of
possible values in the Portuguese data, simply
because the morphology of the two languages
is different. In the example in Figure 1 the
value PPI for the attribute tense stands for
prete?rito perfeito do indicativo. We chose to
include mood information in the tense attribute
because the different tenses of the indicative and
the subjunctive moods do not line up perfectly
as there are more tenses for the indicative than
for the subjunctive. For the aspect attribute,
which encodes grammatical aspect, we only
use the values NONE and PROGRESSIVE,
leaving out the values PERFECTIVE and
PERFECTIVE PROGRESSIVE, as in Portuguese
there is no easy match between perfective aspect
and grammatical categories.
The attributes of <TIMEX3> elements carry
over to the Portuguese corpus unchanged, and the
<TLINK> elements are taken verbatim from the
original documents.
4 Data Description
The original English data for TempEval-1 are
based on the TimeBank data, and they are split
into one dataset for training and development and
another dataset for evaluation. The full data are or-
ganized in 182 documents (162 documents in the
training data and another 20 in the test data). Each
document is a news report from television broad-
casts or newspapers. A large amount of the doc-
uments (123 in the training set and 12 in the test
data) are taken from a 1989 issue of the Wall Street
Journal.
The training data comprise 162 documents with
673
<?xml version="1.0" encoding="UTF-8" ?>
<TempEval>
ABC<TIMEX3 tid="t52" type="DATE" value="1998-01-14" temporalFunction="false"
functionInDocument="CREATION_TIME">19980114</TIMEX3>.1830.1611
REPORTAGEM
<s>Em Washington, <TIMEX3 tid="t53" type="DATE" value="1998-01-14" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t52">hoje</TIMEX3>, a Federal Aviation Administration <EVENT
eid="e1" class="OCCURRENCE" stem="publicar" aspect="NONE" tense="PPI" polarity="POS" pos="VERB">publicou
</EVENT> gravaoes do controlo de trfego areo da <TIMEX3 tid="t54" type="TIME" value="1998-XX-XXTNI"
temporalFunction="true" functionInDocument="NONE" anchorTimeID="t52">noite</TIMEX3> em que o voo TWA800
<EVENT eid="e2" class="OCCURRENCE" stem="cair" aspect="NONE" tense="PPI" polarity="POS" pos="VERB">caiu
</EVENT>
.</s>
...
<TLINK lid="l1" relType="BEFORE" eventID="e2" relatedToTime="t53" task="A"/>
<TLINK lid="l2" relType="OVERLAP" eventID="e2" relatedToTime="t54" task="A"/>
<TLINK lid="l4" relType="BEFORE" eventID="e2" relatedToTime="t52" task="B"/>
...
</TempEval>
Figure 2: Extract of a document contained in the Portuguese data
2,236 sentences (i.e. 2236 <s> elements) and
52,740 words. It contains 6799 <EVENT> el-
ements, 1,244 <TIMEX3> elements and 5,790
<TLINK> elements. Note that not all the events
are included here: the ones expressed by words
that occur less than 20 times in TimeBank were
removed from the TempEval-1 data.
The test dataset contains 376 sentences and
8,107 words. The number of <EVENT> elements
is 1,103; there are 165 <TIMEX3>s and 758
<TLINK>s.
The Portuguese data of course contain the same
(translated) documents. The training dataset has
2,280 sentences and 60,781 words. The test data
contains 351 sentences and 8,920 words.
5 Comparing the two Datasets
One of the systems participating in the
TempEval-1 competition, the USFD system
(Hepple et al, 2007), implemented a very
straightforward solution: it simply trained classi-
fiers with Weka (Witten and Frank, 2005), using
as attributes information that was readily available
in the data and did not require any extra natural
language processing (for all tasks, the attribute
relType of <TLINK> elements is unknown and
must be discovered, but all the other information
is given).
The authors? objectives were to see ?whether a
?lite? approach of this kind could yield reasonable
performance, before pursuing possibilities that re-
lied on ?deeper? NLP analysis methods?, ?which
of the features would contribute positively to sys-
tem performance? and ?if any [machine learning]
approach was better suited to the TempEval tasks
than any other?. In spite of its simplicity, they ob-
tained results quite close to the best systems.
For us, the results of (Hepple et al, 2007) are in-
teresting as they allow for a straightforward evalu-
ation of our adaptation efforts, since the same ma-
chine learning implementations can be used with
the Portuguese data, and then compared to their
results.
The differences in the data are mostly due to
language. Since the languages are different, the
distribution of the values of several attributes are
different. For instance, we included both tense
and mood information in the tense attribute of
<EVENT>s, as mentioned in Section 3.1, so in-
stead of seven possible values for this attribute, the
Portuguese data contains more values, which can
cause more data sparseness. Other attributes af-
fected by language differences are aspect, pos,
and class, which were also possibly changed
during the adaptation process.
One important difference between the English
and the Portuguese data originates from the fact
that events with a frequency lower than 20 were
removed from the English TempEval-1 data. Since
there is not a 1 to 1 relation between English event
terms and Portuguese event terms, we do not have
the guarantee that all event terms in the Portuguese
data have a frequency of at least 20 occurrences in
the entire corpus.5
The work of (Hepple et al, 2007) reports on
both cross-validation results for various classifiers
over the training data and evaluation results on the
training data, for the English dataset. We we will
5In fact, out of 1,649 different stems for event terms in the
Portuguese training data, only 45 occur at least 20 times.
674
Task
Attribute A B C
EVENT-aspect ! ! !
EVENT-polarity ! ! ?
EVENT-POS ! ! !
EVENT-stem ! ? ?
EVENT-string ? ? ?
EVENT-class ? ! !
EVENT-tense ? ! !
ORDER-adjacent ! N/A N/A
ORDER-event-first ! N/A N/A
ORDER-event-between ? N/A N/A
ORDER-timex-between ? N/A N/A
TIMEX3-mod ! ? N/A
TIMEX3-type ! ? N/A
Table 1: Features used for the English TempEval-1
tasks. N/A means the feature was not applicable to
the task,!means the feature was used by the best
performing classifier for the task, and ? means it
was not used by that classifier. From (Hepple et
al., 2007).
be comparing their results to ours.
Our purpose with this comparison is to validate
the corpus adaptation. Similar results would not
necessarily indicate the quality of the adapted cor-
pus. After all, a word-by-word translation would
produce data that would yield similar results, but
it would also be a very poor translation, and there-
fore the resulting corpus would not be very inter-
esting. The quality of the translation is not at stake
here, since it was manually revised. But similar
results would indicate that the obtained data are
comparable to the original data, and that they are
similarly useful to tackle the problem for which
the original data were collected. This would con-
firm our hypothesis that adapting an existing cor-
pus can be an effective way to obtain new data for
a different language.
5.1 Results for English
The attributes employed for English by (Hepple et
al., 2007) are summarized in Table 1. The class is
the attribute relType of <TLINK> elements.
The EVENT features are taken from <EVENT>
elements. The EVENT-string attribute is the
character data inside the element. The other at-
tributes correspond to the feature of <EVENT>
with the same name. The TIMEX3 features
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 58.2 76.7 54.0
rules.DecisionTable 53.3 79.0 52.9
functions.SMO 55.1 78.1 55.5
rules.JRip 50.7 78.6 53.4
bayes.NaiveBayes 56.3 76.2 50.7
Table 2: Performance of several machine learn-
ing algorithms on the English TempEval-1 train-
ing data, with cross-validation. The best result
for each task is in boldface. From (Hepple et al,
2007).
also correspond to attributes of the relevant
<TIMEX3> element. The ORDER features are
boolean and computed as follows:
? ORDER-event-first is whether the
<EVENT> element occurs in the text before
the <TIMEX3> element;
? ORDER-event-between is whether an
<EVENT> element occurs in the text between
the two temporal entities being ordered;
? ORDER-timex-between is the same, but
for temporal expressions;
? ORDER-adjacent is whether both
ORDER-event-between and ORDER-
timex-between are false (but other
textual data may occur between the two
entities).
Cross-validation over the training data pro-
duced the results in Table 2. The base-
line used is the majority class baseline, as
given by Weka?s rules.ZeroR implemen-
tation. The lazy.KStar algorithm is a
nearest-neighbor classifier that uses an entropy-
based measure to compute instance similarity.
Weka?s rules.DecisionTable algorithm as-
signs to an unknown instance the majority class
of the training examples that have the same
attribute values as that instance that is be-
ing classified. functions.SMO is an imple-
mentation of Support Vector Machines (SVM),
rules.JRip is the RIPPER algorithm, and
bayes.NaiveBayes is a Naive Bayes classi-
fier.
675
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 57.4 77.7 53.3
rules.DecisionTable 54.2 78.1 51.6
functions.SMO 55.5 79.3 56.8
rules.JRip 52.1 77.6 52.1
bayes.NaiveBayes 56.0 78.2 53.5
trees.J48 55.6 79.0 59.3
Table 3: Performance of several machine learn-
ing algorithms on the Portuguese data for the
TempEval-1 tasks. The best result for each task
is in boldface.
5.2 Attributes
We created a small script to convert the XML an-
notated files into CSV files, that can be read by
Weka. In this process, we included the same at-
tributes as the USFD authors used for English.
For task C, (Hepple et al, 2007) are not very
clear whether the EVENT attributes used were re-
lated to just one of the two events being temporally
related. In any case, we used two of each of the
EVENT attributes, one for each event in the tempo-
ral relation to be determined. So, for instance, an
extra attribute EVENT2-tense is where the tense
of the second event in the temporal relation is kept.
5.3 Results
The majority class baselines produce the same
results as for English. This was expected: the
class distribution is the same in the two datasets,
since the <TLINK> elements were copied to the
adapted corpus without any changes.
For the sake of comparison, we used the same
classifiers as (Hepple et al, 2007), and we used the
attributes that they found to work best for English
(presented above in Table 1). The results for the
Portuguese dataset are in Table 3, using 10-fold
cross-validation on the training data.
We also present the results for Weka?s imple-
mentation of the C4.5 algorithm, to induce deci-
sion trees. The motivation to run this algorithm
over these data is that decision trees are human
readable and make it easy to inspect what deci-
sions the classifier is making. This is also true of
rules.JRip. The results for the decision trees
are in this table, too.
The results obtained are almost identical to the
results for the original dataset in English. The best
performing classifier for task A is the same as for
English. For task B, Weka?s functions.SMO
produced better results with the Portuguese data
than rules.DecisionTable, the best per-
forming classifier with the English data for this
task. In task C, the SVM algorithm was also the
best performing algorithm among those that were
also tried on the English data, but decision trees
produced even better results here.
For English, the best performing classifier for
each task on the training data, according to Ta-
ble 2, was used for evaluation on the test data: the
results showed a 59% F-measure for task A, 73%
for task B, and 54% for task C.
Similarly, we also evaluated the best algorithm
for each task (according to Table 3) with the Por-
tuguese test data, after training it on the entire
training dataset. The results are: in task A the
lazy.KStar classifier scored 58.6%, and the
SVM classifier scored 75.5% in task B and 59.4%
in task C, with trees.J48 scoring 61% in this
task.
The results on the test data are also fairly similar
for the two languages/datasets.
We inspected the decision trees and rule sets
produced by trees.J48 and rules.JRip, in
order to see what the classifiers are doing.
Task B is probably the easiest task to check this
way, because we expect grammatical tense to be
highly predictive of the temporal order between an
event and the document?s creation time.
And, indeed, the top of the tree induced by
trees.J48 is quite interesting:
eTense = PI: OVERLAP (388.0/95.0)
eTense = PPI: BEFORE (1051.0/41.0)
Here, eTense is the EVENT-tense attribute
of <EVENT> elements, PI stands for present in-
dicative, and PPI is past indicative (prete?rito per-
feito do indicativo). In general, one sees past
tenses associated with the BEFORE class and fu-
ture tenses associated with the AFTER class (in-
cluding the conditional forms of verbs). Infini-
tives are mostly associated with the AFTER class,
and present subjunctive forms with AFTER and
OVERLAP. Figure 3 shows the rule set induced by
the RIPPER algorithm.
The classifiers for the other tasks are more dif-
ficult to inspect. For instance, in task A, the event
term and the temporal expression that denote the
entities that are to be ordered may not even be di-
rectly syntactically related. Therefore, it is hard to
676
(eClass = OCCURRENCE) and ( eTense = INF) and ( ePolarity = POS) => lRelType= AFTER
(183.0/77.0)
( eTense = FI) => lRelType= AFTER (55.0/10.0)
(eClass = OCCURRENCE) and ( eTense = IR-PI+INF) => lRelType= AFTER (26.0/4.0)
(eClass = OCCURRENCE) and ( eTense = PC) => lRelType= AFTER (15.0/3.0)
(eClass = OCCURRENCE) and ( eTense = C) => lRelType= AFTER (17.0/2.0)
( eTense = PI) => lRelType= OVERLAP (388.0/95.0)
(eClass = ASPECTUAL) and ( eTense = PC) => lRelType= OVERLAP (9.0/2.0)
=> lRelType= BEFORE (1863.0/373.0)
Figure 3: rules.JRip classifier induced for task B. INF stands for infinitive, FI is future indicative,
IR-PI+INF is an infinitive form following a present indicative form of the verb ir (to go), PC is present
subjunctive, C is conditional, PI is present indicative.
see how interesting the inferred rules are, because
we do not know what would be interesting in this
scenario. In any case, the top of the induced tree
for task A is:
oAdjacent = True: OVERLAP (554.0/128.0)
Here, oAdjacent is the ORDER-adjacent
attribute. Assuming this attribute is an indication
that the event term and the temporal expression are
related syntactically, it is interesting to see that the
typical temporal relation between the two entities
in this case is an OVERLAP relation. The rest of
the tree is much more ad-hoc, making frequent use
of the stem attribute of <EVENT> elements, sug-
gesting the classifier is memorizing the data.
Task C, where two events are to be ordered, pro-
duced more complicated classifiers. Generally the
induced rules and the tree paths compare the tense
and the class of the two event terms, showing some
expected heuristics (such as, if the tense of the first
event is future and the tense of the second event
is past, assign AFTER). But there are also many
several rules for which we do not have clear intu-
itions.
6 Discussion
In this paper, we described the semi-automatic
adaptation of a TimeML annotated corpus from
English to Portuguese, a language for which
TimeML annotated data was not available yet.
Because most of the TimeML annotations are
semantic in nature, they can be transposed to a
translation of the original corpus, with few adap-
tations being required.
In order to validate this adaptation, we used the
obtained data to replicate some results in the liter-
ature that used the original English data.
The results for the Portuguese data are very sim-
ilar to the ones for English. This indicates that our
approach to adapt existing annotated data to a dif-
ferent language is fruitful.
References
David Ahn, Joris van Rantwijk, and Maarten de Ri-
jke. 2007. A cascaded machine learning approach
to interpreting temporal expressions. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 420?427, Rochester, New York,
April. Association for Computational Linguistics.
Mark Hepple, Andrea Setzer, and Rob Gaizauskas.
2007. USFD: Preliminary exploration of fea-
tures and classifiers for the TempEval-2007 tasks.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
James Pustejovsky and Marc Verhagen. 2009.
Semeval-2010 task 13: evaluating events, time ex-
pressions, and temporal relations (tempeval-2). In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 112?116, Boulder, Colorado. Association for
Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of Corpus Linguistics 2003,
pages 647?656.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
and J. Pustejovsky. 2007. SemEval-2007 Task 15:
TempEval temporal relation identification. In Pro-
ceedings of SemEval-2007.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Francisco. second edition.
677
High Precision Analysis of NPs
with a Deep Processing
Grammar
Ant?nio Branco
Francisco Costa
Universidade de Lisboa (Portugal)
email: Antonio.Branco@di.fc.ul.pt
Abstract
In this paper we present LXGram, a general purpose grammar for the
deep linguistic processing of Portuguese that aims at delivering detailed
and high precision meaning representations. LXGram is grounded on the
linguistic framework of Head-Driven Phrase Structure Grammar (HPSG).
HPSG is a declarative formalism resorting to unification and a type sys-
tem with multiple inheritance. The semantic representations that LX-
Gram associates with linguistic expressions use the Minimal Recursion
Semantics (MRS) format, which allows for the underspecification of scope
effects. LXGram is developed in the Linguistic KnowledgeBuilder (LKB)
system, a grammar development environment that provides debugging
tools and efficient algorithms for parsing and generation. The implemen-
tation of LXGram has focused on the structure of Noun Phrases, and LX-
Gram accounts for many NP related phenomena. Its coverage continues
to be increased with new phenomena, and there is active work on ex-
tending the grammar?s lexicon. We have already integrated, or plan to
integrate, LXGram in a few applications, namely paraphrasing, treebank-
ing and language variant detection. Grammar coverage has been tested
on newspaper text.
31
32 Branco and Costa
1 Introduction
In this paper we present LXGram, a hand-built, general purpose computational gram-
mar for the deep linguistic processing of Portuguese, specially geared to high precision
processing of Noun Phrases. This grammar is based on the framework of Head-Driven
Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), one of the most promi-
nent linguistic theories being used in natural language processing. Like several other
computational HPSGs, LXGram uses Minimal Recursion Semantics (MRS; Copes-
take et al (2005)) for the representation of meaning.
LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copes-
take, 2002), a development environment for constraint-based grammars. This envi-
ronment provides a GUI, debugging tools and very efficient algorithms for parsing
and generation with the grammars developed there (Malouf et al, 2000; Carroll et al,
1999).
Several broad-coverage grammars have been developed in the LKB. Currently, the
largest ones are for English (Copestake and Flickinger, 2000), German (M?ller and
Kasper, 2000) and Japanese (Siegel and Bender, 2002). The grammars developed
with the LKB are also supported by the PET parser (Callmeier, 2000), which allows
for faster parsing times due to the fact that the grammars are compiled into a binary
format in a first step. As the LKB grammars for other languages, LXGram is in active
development, and it is intended to be a broad-coverage, open-domain grammar for
Portuguese. At the same time, it produces detailed representations of meaning in
tandem with syntactic structures, making it useful for a wide range of applications.
In Section 2, we describe the framework foundations of the grammar. The major
design features of the grammar are introduced in Section 3. We talk about the coverage
of LXGram in Section 4. Section 5 presents some of the phenomena treated within
the NP domain and shows examples of implemented analyses relating to NP syntax
and semantics. In Section 6, results on the performance of the grammar are reported,
and in Section 7, we discuss applications where the grammar is or is being integrated.
Finally, the paper closes with concluding remarks in Section 8.
2 Foundations
LXGram adopts the HPSG framework, a popular linguistic theory with a large body of
literature covering many natural language phenomena. These insights can be directly
incorporated in the implementation of a computational grammar.
2.1 HPSG
HPSG resorts to a declarative formalism to model linguistic data. It employs a type
system (supporting multiple inheritance) and typed feature structures (recursive data
structures defining ?has-a? relations) in order to describe the properties of linguistic
objects (words, phrases, rules). Unification of types and feature structures is central
to HPSG, used to ensure that the various elements have compatible properties. For
instance, the fact that a transitive verb takes an NP as its complement is captured in
HPSG by defining a lexical type for transitive verbs, say transitive-verb-lex(eme), with
constraints like the following (among others), presented in the Attribute-Value Matrix
(AVM) format widely employed in HPSG:
High Precision Analysis of NPs with a Deep Processing Grammar 33
?
?
?
?
?
?
?
?
transitive-verb-lex
SYNSEM|LOCAL|CAT|VAL|COMPS
?
?
?
?
?
LOCAL|CAT
?
?
?
?
HEAD noun
VAL
[
SPR ??
COMPS ??
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The NP complement of the verb is represented in this AVM as the value of the at-
tribute COMPS. This attribute takes a list as its value (indicated by the angle brackets).
In this case the sole element of this list describes an object with a HEAD feature of the
type noun and empty complements (the attribute COMPS) and specifier (the feature
SPR) (i.e. they have been saturated at the point where the verb combines with this
element), which is the HPSG description of an NP.
2.2 MRS
Minimal Recursion Semantics (MRS) is used as the format of semantic representa-
tions that LXGram associates with expressions from Portuguese. MRS has several
properties that are interesting for applications. A relevant one is the use of pointers to
represent scope effects that are handled via recursion in traditional formal semantics.
This use of pointers (called handles) allows for the underspecification of scope rela-
tions, which avoids listing all scope possibilities for ambiguous sentences (although
they can still be computed on demand with the LKBmachinery). This is a useful prop-
erty: scope does not need to be resolved in all applications (e.g. machine translation
does not require it), but at the same time scoped formulas can be obtained on demand
if required (e.g. for automated inference).
We provide an example MRS representation derived for the sentence ?todas as
equipas podem vencer? (all teams can win) in Figure 1. This MRS describes the
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
_todo_q_rel
LBL h3 h
ARG0 x6 x
RSTR h5 h
BODY h4 h
?
?
?
?
?
?
?
,
?
?
?
_equipa_n_rel
LBL h7 h
ARG0 x6
?
?
?
,
?
?
?
?
?
_poder_v_rel
LBL h8
ARG0 e2
ARG1 h9 h
?
?
?
?
?
,
?
?
?
?
?
_vencer_v_rel
LBL h10
ARG0 e11
ARG1 x6
?
?
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h1
LARG h8
?
?
?
,
?
?
?
qeq
HARG h5
LARG h7
?
?
?
,
?
?
?
qeq
HARG h9
LARG h10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: MRS for the sentence ?Todas as equipas podem vencer? (all teams can win)
two following scoped formulas, where the predicate _todo_q stands for a universal
quantifier:
? _todo_q(x6,_equipa_n(x6),_poder_v(e2,_vencer_v(e11,x6)))
? _poder_v(e2,_todo_q(x6,_equipa_n(x6),_vencer_v(e11,x6)))
34 Branco and Costa
The first reading is the one that says that each team has a chance to win, while the
second reading says that it is possible for there to be a situation in which all teams win
(false assuming common sense knowledge). A single MRS representation is obtained
for these two readings by instantiating the ARG1 feature of the relation _poder_v (can)
with the handle h9, which is related to the handle h10 labeling the relation _vencer_v
(win) via a qeq relation (equality modulo intervening quantifiers). This is the way of
saying that these two handles are the same (first reading) or that there is an intervening
generalized quantifier relation (second reading).
Semantic representations abstract from many grammatical and superficial details
of language, like word order, syntactic structure and morphology. As such, they are
very similar across different natural languages (modulo predicate names). This is also
true of MRS. Furthermore, semantic representations hide grammar implementation.
As such, they are the preferred grammar?s interface for applications, that do not need
any knowledge of the grammatical properties of Portuguese and may not need to look
at syntactic analysis.
The MRS format is also used with several other computational HPSGs, for other
languages. Several applications (e.g. Machine Translation) have been used with other
HPSGs that communicate with these grammars via the MRSs (Bond et al, 2004).
These applications can be easily integrated with grammars for different languages
that also use MRS: they are almost completely language independent.
3 Design Features
Given the foundational options, LXGram adheres to a number of important design
features.
Bidirectionality LXGram is bidirectional. The formalism employed is completely
declarative. It can be used for parsing (yielding syntactic analyses and semantic rep-
resentations from natural language input) and also for generation (yielding natural
language from meaning representations). As such it can be useful for a wide range of
applications.
Precision LXGram aims at high precision of linguistic processing. Modulo bugs,
the grammar cannot parse ungrammatical input. Although this feature may have a
negative impact on robustness, it is an important aspect of the grammar when it is
used for generation, as it means it is not possible to generate ungrammatical strings.1
It is indeed possible to mark some rules and some lexical entries to only be used for
parsing and not for generation. In the configuration files for the LKB one can list these
rules and lexical items. We are currently using this feature in order to be able to parse
input that is not ungrammatical but is marked with respect to register, but preventing
the grammar from generating such strings.
Importantly, the fact that it cannot parse ungrammatical input also means that the
grammar will not produce impossible analyses for grammatical sentences.
Broad Coverage LXGram development is aimed at a broad coverage. We also
seek to make LXGram neutral with respect to regional variation as much as possi-
ble. Currently, the grammar accommodates both European Portuguese and Brazilian
1We believe that dealing with ill-formed input is best done via other means (rather than let the grammar
overgenerate so it can parse more), like partial parsing or the integration with/falling back to other tools.
High Precision Analysis of NPs with a Deep Processing Grammar 35
Portuguese. Aspects of variation that are accounted for include lexical differences
(merely affecting spelling or more substantial ones) as well as syntactic discrepancies
(e.g. definite articles before possessives, word order between clitic pronouns and the
verb).
Efficiency The processors on which LXGram runs (LKB, PET) are very efficient.
In addition, there are grammar engineering techniques that improve efficiency (e.g.
(Flickinger, 2000)) that are also exploited in our implementation.
Robustness The LKB and PET systems provide several ways to combine a grammar
with the output of shallow tools, like part-of-speech taggers. Such integration can
improve grammar coverage, as the grammar needs information about all words in the
input, and some words may be missing in the grammar?s lexicon. We have success-
fully combined LXGram with a part-of-speech tagger and a morphological analyzer
(more in Section 6). The grammar code includes mappings from the input format
(XML) to the feature structures that are manipulated by the grammar.
Availability A version of LXGram is publicly available at http://nlxgroup.di.
fc.ul.pt/lxgram. LXGram can be used by applications without any knowledge of
the grammar?s implementation or internal workings. The LKB allows for applications
to communicate with the grammar via sockets, accepting parser input in XML or raw
text and returning semantic representations in XML, for which a DTD is available. It
is also possible to automatically produce a list of all the predicates known by the gram-
mar together with their arity and argument types (from the lexicon and syntax rules),
that can be manually annotated with comments and examples. The predicates corre-
sponding to lexical items are however quite transparent once the naming conventions
that are used are explained.
4 Coverage
4.1 Lexical Coverage
When one is using a lexicalist framework like HPSG, lexical coverage is a key issue
because all tokens in the input should be known by the grammar in order for the
grammar to produce a parse. Furthermore, the amount of information included in
the lexicon that is used by an HPSG is very large. Part-of-speech and morphological
information is not sufficient. For the correct assignment of semantic representations,
subcategorization frames as well as other information pertaining to semantics must
be correctly associated with every lexical item, something that cannot be known with
sufficient quality by just using shallower tools, like part-of-speech taggers.
In LXGram a hand-crafted lexicon containing several hundreds of nouns, adjectives
and verbs was developed. However, the manual creation of lexica with this amount
of information is time consuming and error prone. We are exploring methods to al-
leviate this problem. An option is to combine the grammar with shallower tools in
order to have access to some of the information needed and assume default values
for the information that cannot be obtained this way. We have already integrated
the grammar with a set of shallow tools (a part-of-speech tagger, a lemmatizer and a
morphological analyzer) in order to guess information about unknown words. Prelim-
inary results indicate an increase in coverage on unrestricted newspaper text from 2%
to 13%. Although this approach cannot guarantee correct semantic representations
36 Branco and Costa
(or even syntactic trees, since subcategorization frames constrain syntactic structure),
it can be useful in applications that only require some restricted amount of linguistic
information.
4.2 Overall Grammatical Coverage
In order to get a quantitative overview of the grammar, it can be characterized as
follows:
? 24,484 lines of code, including comments and excluding the lexicon;
? 53 syntax rules;
? 40 lexical rules, mostly inflectional;
? 3,154 total types;
? 414 types for lexical items;
? 2,718 hand-built lexical entries.
For a qualitative overview, these are the linguistic phenomena covered so far:
? Declarative sentences
? Yes-no questions e.g.: ?Portanto o Estado tem um gosto?? (So does the State
have preferences?)
? Imperative sentences e.g.: ?D?-me um desses bolos.? (Give me one of those
cakes)
? Some subcategorization frames of verbs, nouns and adjectives e.g.: ?a Pol?-
nia empatou com a Fran?a? (Poland tied with France); ?eu j? disse que pode ser
um dos mais baratos? (I told you already that it can be one of the cheapest);
?filho de um professor dos arredores de Viena? (son of a teacher from the out-
skirts of Vienna)
? Comparative constructions e.g.: ?a vida ? maior do que o cinema? (life is
larger than cinema)
? Noun phrase structure, including determiners, possessives, cardinal and
ordinal numerals, prepositional phrases, adjectives, etc. (examples in the
next section)
? Modification of verbal projections by prepositional and adverbial phrases
e.g.: ?No CAPC termina hoje a exposi??o? (the exhibit ends today at CAPC);
? Relative clauses e.g.: ?sete outros suspeitos que a pol?cia ainda procura? (seven
other suspects that the police are still looking for)
? Null subjects and objects e.g.: ?Sa?mos depois do jantar.? ((We) left after
dinner); ?Podemos comer l? perto.? (We can eat near there)
High Precision Analysis of NPs with a Deep Processing Grammar 37
? Floated Quantifiers e.g.: ?os ?ndices subiram todos? (the indices have all gone
up)
The development of the grammar is going on and this grammar is getting its cover-
age increased with important phenomena that are missing. In particular, for the near
future, we are working towards including more subcategorization frames for verbs,
nouns and adjectives, and implementing wh-questions, coordination and adverbial
subordination.
5 Noun Phrases
A special design feature of LXGram is that it includes a comprehensive implementa-
tion of Portuguese Noun Phrase structure, covering:
? Bare noun phrases (i.e. NPs lacking a determiner) e.g.: ?boa gest?o? (good
management); ?imagens da bancada? (images of the seats)
? Determiners and predeterminers e.g.: ?esta sua ?ltima produ??o? (this last
production of his); ?todos estes problemas? (all these problems); ?todos os par-
tidos pol?ticos? (all the political parties), ?aquele tempo todo? (all that time)
? Word order constraints among NP elements e.g.: ?as duas primeiras insti-
tui??es? (the two first institutions); ?os primeiros sete meses deste ano? (the
first seven months of this year); ?sete outros suspeitos que a pol?cia ainda
procura? (seven other suspects that the police are still looking for); ?os outros
tr?s membros do conselho? (the other three members of the council); ?os seus
dois primeiros anos pol?micos na Casa Branca? (his first two polemic years
in the White House); ?o primeiro grande conflito que aportava em Bel?m? (the
first great conflict that reached Berlin); ?outro lugar qualquer? (any other place);
?um lugar qualquer? (any place); ?qualquer outra solu??o? (any other solution)
? Prenominal and postnominal possessives e.g.: ?o seu terceiro maior parceiro
comercial? (its third major commercial partner); ?um adjunto seu que atendeu
ali o telefonema? (an assessor of his who answered the phone call there)
? Modification of adjectives e.g.: ?os escritores mais importantes? (the most
important writers); ?o discurso razoavelmente optimista? (the reasonably opti-
mistic speech)
? Missing nouns e.g.: ?dois que s?o g?meos? (two who are twins)
? Word order between adjectives and complements of nouns e.g.: ?o conhec-
imento essencial das pessoas? (the essential knowledge about people)
? Adjectives with the semantics of arguments of nouns e.g.: ?o veto americano
? renova??o do mandato? (the American veto to the renewal of the position)
Precision was given a lot of attention. For instance, many items are constrained not
to appear more than once in a given NP (determiners, possessives, cardinals, ordinals,
etc.). Scope phenomena are also handled (motivated by semantics), as well as order
constraints. Agreement is enforced.
38 Branco and Costa
We present some examples of phenomena for which LXGram provides interesting
semantic representations and that we have not found in the literature pertaining to
implemented grammars.
5.1 Floated Quantifiers
The first example relates to floated quantifiers. For all the sentences in (1), which are
all grammatical in Portuguese, LXGram provides the MRS equivalent of
all(x, price(x),will(go_up(x))):
(1) a. Todos
all
os
the
pre?os
prices
v?o
will
subir.
go up
b. Os
the
pre?os
prices
todos
all
v?o
will
subir.
go up
c. Os
the
pre?os
prices
v?o
will
todos
all
subir.
go up
d. Os
the
pre?os
prices
v?o
will
subir
go up
todos.
all
In all of these cases we associate empty semantics to the definite article (?os?).
Semantic information is percolated around the syntactic trees so that the universal
quantifier, which can be realized at several different places, ends up being linked to
the semantics of the NP subject in the semantic representations for all these sentences.
We also make sure that definite articles always carry quantifier semantics when no
floated quantifier is present.
The implementation revolves around allowing floated quantifiers to attach to verbs,
resulting in verb-headed nodes that combine with NP subjects lacking quantifier se-
mantics. Raising verbs, like the form ?v?o? in this example, constrain their subject
according to the constraints of the subject of their VP complement. For instance, the
last example (1d) receives a syntactic analysis like the one described by the following
tree:
[
subj-head-phrase
SUBJ ??
]



HH
HH
HH
1
 HH
Os
the
pre?os
prices
?
?
head-comp-phrase
SUBJ
?
1
?
?
?



HH
HH
H
[
SUBJ
?
1
?
]
v?o
will
?
?
floated-quant-phrase
SUBJ
?
1 NP?e,t?
?
?
?


HH
H
[
SUBJ
?
NP
?
]
subir
go up
todos
all
High Precision Analysis of NPs with a Deep Processing Grammar 39
Here, NP abbreviates a feature structure that describes a noun phrase (of the se-
mantic type ??e,t?,t?), and NP?e,t? abbreviates the constraints that describe an NP
introduced by a determiner lacking quantifier semantics (i.e. an NP with an MRS
representation that is similar to that of constituents with the the semantic type ?e,t?).
In HPSG, the SUBJ feature encodes the constraints on the subject that a constituent
selects. We use a dedicated syntax rule to combine ?subir? and ?todos? (floated-quant-
phrase), that creates a node requiring an NP with the semantic type ?e,t? as its subject.
The verb form ?v?o? is treated as a raising verb: in HPSG the syntactic requirements
on the subject of a raising verb are the same as the requirements on the subject of the
VP complement that that verb selects for. This is denoted by the boxed integers in this
tree (which represent unification).
In this example, the VP complement of ?v?o? is the phrase ?subir todos?, as these
two constituents are combined via the head-comp-phrase rule (selection of comple-
ments is represented in a way similar to the selection of subjects, but via the feature
COMPS instead of the feature SUBJ). The subject of the head-comp-phrase is the sub-
ject of its head daughter (?v?o?). The topmost node is the result of applying a syntactic
rule to project subjects to the left of their head (subj-head-phrase).
The example in (1c) is processed in a similar fashion:
[
subj-head-phrase
SUBJ ??
]




HH
HH
HH
H
2
 HH
Os
the
pre?os
prices
?
?
head-comp-phrase
SUBJ
?
2
?
?
?



HH
HH
H
?
?
floated-quant-phrase
SUBJ
?
2 NP?e,t?
?
?
?


HH
H
[
SUBJ
?
1
?
]
v?o
will
todos
all
[
SUBJ
?
1 NP
?
]
subir
go up
In this example, the complement of ?v?o? is the node spanning ?subir?, which
selects for a quantified subject. The subject of the raising verb is accordingly also
a quantified NP. Here, the rule to project a floated quantifier applies lower than the
construction that projects complements, creating a node that requires a non-quantified
subject. The SUBJ feature of head-complement constructions comes from the head
daughter (the floated-quant-phrase node in this example). Therefore, the node pro-
duced by the head-comp-phrase rule also requires a non-quantified subject.
Note that the composition of semantics with MRS in based on the concatenation of
the RELS and HCONS lists associated to the various constituents and passing around
40 Branco and Costa
the values of the features LTOP and INDEX (see Figure 1). It is not based on function
application. The composition of semantics with MRS is quite flexible.
5.2 Scope of Adjectives and Relative Clauses
The second example that we show here relates to the semantic scope between different
elements of noun phrases. In particular, we can see a distinction in the interpretation
of the two following examples:
(2) a. um
a
poss?vel
possible
m?dico
doctor
chin?s
Chinese
a possible Chinese doctor
b. um
a
poss?vel
possible
m?dico
doctor
que
who
?
is
chin?s
Chinese
a possible doctor who is Chinese
In the first NP an entity is described as possibly being a Chinese doctor. The second
NP describes an entity as possibly being a doctor and certainly being Chinese. Ac-
cordingly, LXGram delivers slightly different semantic representations for these two
NPs. The first case produces something similar to
?P. a(x, possible(doctor(x)? chinese(x)),P(x)).
The second NP is treated along the lines of
?P. a(x, possible(doctor(x))? chinese(x),P(x)).
These two different readings are derived simply by constraining the relative syntactic
scope of adjectives and relative clauses. Namely, LXGram forces prenominal adjec-
tives to attach higher than postnominal adjectives (2a) but lower than relative clauses
(2b). In this case, the scope differences in the semantic representations are simply
derived from the differences in syntactic scope:


HH
H
um


HH
H
poss?vel
 HH
m?dico chin?s



HH
HH
H
um


HH
HH
 HH
poss?vel m?dico
que ? chin?s
Of course, the following examples receive equivalent semantics:
(3) a. um
a
m?dico
doctor
chin?s
chinese
a Chinese doctor
b. um
a
m?dico
doctor
que
who
?
is
chin?s
Chinese
a doctor who is Chinese
High Precision Analysis of NPs with a Deep Processing Grammar 41
6 Evaluation
Some evaluation experiments were conducted to test LXGram?s coverage. In one of
them, a corpus with newspaper text (self-reference) was used, with 145 sentences.
For this experiment, we used a part-of-speech tagger and a morphological analyzer
(self-reference) in order to guess some information about out-of-vocabulary words. A
default value was assumed for the missing subcategorization information (all unknown
verbs were treated as transitive verbs). The average sentence length was 22 words. In
this experiment, 13.1% of all sentences received at least one parse by the grammar.2
On the same test corpus, the average time it took for a sentence to parse was 1.1
seconds on a P4 machine at 3GHz. The average amount of memory required to analyze
a sentence was 145.5MB.
In another experiment, with 180,000 short sentences (5 to 9 words) selected ran-
domly from two newspaper corpora (CETEMP?blico and CETENFolha), LXGram
had achieved 26% coverage, using a similar approach to handle unknown words (self-
reference).
During the development of LXGram we maintain several test suites, consisting of
example sentences for the implemented phenomena. The test suites use a controlled
vocabulary. Also, several examples attest several phenomena, in order to test the
interaction of the different modules. They are very useful to test the syntax rules of
the grammar and the semantics that LXGram produces, and for regression testing. The
test suite for NPs contains 851 sentences (429 of which are negative examples, that
the grammar should not parse). The average sentence length is 5.3 words (2?16). On
this test suite LXGram has 100% coverage and 0% overgeneration. The average time
needed to analyze a sentence is 0.11 seconds, with an average memory requirement
of 15.5MB. Plotting parse time by sentence length, we see an approximately linear
increase in parse time with this test suite.
7 Applications and Further Work
We have used LXGram to automatically discriminate between texts written in Euro-
pean Portuguese and Brazilian Portuguese, with encouraging results, which match the
results obtained with other dialect detection methodologies. (self-reference).3
Additionally, we are working towards integrating it with an existing question an-
swering system (self-reference).4 This is in part the reason for the special focus on
NPs, as these constituents are often short answers to factoid questions.
Because the grammar is entirely bidirectional, a paraphraser is gained for free from
the implementation of LXGram: the grammar can simply be used to generate from the
semantic representations that it derives from an input sentence, thus producing para-
phrases of the textual input. We are also working to integrate the grammar, running
under this functionality, into the QA system.
2Note that one of the HPSGs with the broadest coverage at the moment, the ERG, covers 17% of the
British National Corpus. The main cause of parse failure is out-of-vocabulary words.
3In particular, the results obtained with LXGram were quite similar to the results obtained with the
standard methods (based on character n-grams) that are used to identify the language in which a given text
is written, when used for this purpose.
4See (Bobrow et al, 2007) for a similar approach, where an LFG is employed in a question answering
system aiming at high precision.
42 Branco and Costa
On a par with the above lines of research, we are intensively using the grammar to
semi-automatically produce a treebank that contains syntactic representations and se-
mantic descriptions of the sentences in a newspaper corpus. LXGram has also served
in the past to implement and experiment with novel linguistic analyses of interesting
phenomena (self-reference). By making it freely available, we intend to encourage
this sort of experimentation also by other researchers. One can reap important bene-
fits from computationally implementing linguistic analyses: the debugging tools allow
for fast checking of correctness; the impact on other analyses that are already imple-
mented can be immediately assessed via regression testing, making it possible to test
the interaction between linguistic analyses for different phenomena; it is possible to
automatically compare different competing analyses for efficiency, based on test suites
or corpora.
8 Conclusions
In this paper we presented LXGram, a computational grammar for the deep linguistic
processing of Portuguese. LXGram is implemented in a declarative formalism. It
can be used for analysis as well as generation. It produces high precision syntactic
analyses and semantic representations. LXGram supports the two main varieties of
Portuguese: European and Brazilian Portuguese. It is not dependent on a particular
domain or genre.
So far the focus of the implementation was on noun phrases and basic sentence
structure, a coverage that is being extended in ongoingwork. The outcome of different
evaluation experiments shows scores that are in line with those obtained with similar
grammars for other languages.
References
Bobrow, D. G., B. Cheslow, C. Condoravdi, L. Karttunen, T. H. King, R. Nairn,
V. de Paiva, C. Price, and A. Zaenen (2007). PARC?s bridge and question answer-
ing system. In T. H. King and E. M. Bender (Eds.), Proceedings of the GEAF07
Workshop, Stanford, CA, pp. 46?66. CSLI Publications.
Bond, F., S. Fujita, C. Hashimoto, K. Kasahara, S. Nariyama, E. Nichols, A. Ohtani,
T. Tanaka, and S. Amano (2004). The Hinoki treebank: Working toward text un-
derstanding. In S. Hansen-Schirra, S. Oepen, and H. Uszkoreit (Eds.), COLING
2004 5th International Workshop on Linguistically Interpreted Corpora, Geneva,
Switzerland, pp. 7?10. COLING.
Callmeier, U. (2000). PET ? A platform for experimentation with efficient HPSG
processing techniques. Natural Language Engineering 6(1), 99?108. (Special
Issue on Efficient Processing with HPSG).
Carroll, J., A. Copestake, D. Flickinger, and V. Poznan?ski (1999). An efficient chart
generator for (semi-)lexicalist grammars. In Proceedings of the 7th European
Workshop on Natural Language Generation (EWNLG?99), Toulouse, pp. 86?95.
Copestake, A. (2002). Implementing Typed Feature Structure Grammars. Stanford:
CSLI Publications.
High Precision Analysis of NPs with a Deep Processing Grammar 43
Copestake, A. and D. Flickinger (2000). An open-source grammar development envi-
ronment and broad-coverage English grammar using HPSG. In Proceedings of the
Second conference on Language Resources and Evaluation (LREC-2000), Athens,
Greece.
Copestake, A., D. Flickinger, I. A. Sag, and C. Pollard (2005). Minimal Recursion
Semantics: An introduction. Journal of Research on Language and Computa-
tion 3(2?3), 281?332.
Flickinger, D. (2000). On building a more efficient grammar by exploiting types.
Natural Language Engineering 6(1), 15?28. (Special Issue on Efficient Processing
with HPSG).
Malouf, R., J. Carrol, and A. Copestake (2000). Efficient feature structure operations
without compilation. Natural Language Engineering 6(1), 29?46. (Special Issue
on Efficient Processing with HPSG).
M?ller, S. and W. Kasper (2000). HPSG analysis of German. In W. Wahlster (Ed.),
Verbmobil: Foundations of Speech-to-Speech Translation (Artificial Intelligence
ed.)., pp. 238?253. Berlin Heidelberg New York: Springer-Verlag.
Pollard, C. and I. Sag (1994). Head-Driven Phrase Structure Grammar. Stanford:
Chicago University Press and CSLI Publications.
Siegel, M. and E. M. Bender (2002). Efficient deep processing of Japanese. In Pro-
ceedings of the 3rd Workshop on Asian Language Resources and International
Standardization. Coling 2002 Post-Conference Workshop, Taipei, Taiwan, pp. 31?
38.
LXGram in the Shared Task
?Comparing Semantic
Representations?
of STEP 2008
Ant?nio Branco
Francisco Costa
Universidade de Lisboa (Portugal)
email: Antonio.Branco@di.fc.ul.pt
Abstract
LXGram is a hand-built Portuguese computational grammar based on
HPSG (syntax) and MRS (semantics). The LXGram system participated
in the STEP 2008 shared task which aims at comparing semantic repre-
sentations produced by NLP systems such as LXGram. Every partici-
pating team had to contribute a small text. The text that we submitted
for the shared task was originally in Portuguese (an excerpt from a news-
paper) and translated into English, to make a meaningful comparison at
the shared task possible. Likewise, the English texts contributed by the
other participating teams were translated into Portuguese. Because the
LXGram generates many different analyses (mainly due to PP attach-
ment ambiguities), the preferred analysis was selected manually. It was
required to extend LXGram?s lexicon and inventory of syntax rules to be
able to get a reasonable performance on the shared task data. Eventually,
our system was able to produce an analysis for 20 out of the 30 sentences
of the shared task data.
299
300 Branco and Costa
1 Introduction
This paper describes the participation of the Portuguese grammar LXGram in the
Shared Task of STEP 2008 ?Comparing Semantic Representations? (Bos, 2008). This
Shared Task was held in the University of Venice on 22?24 September 2008, with the
purpose of comparing semantic representations produced by different natural language
processing systems. This task had seven participating teams. Each team contributed
with a small text (up to five sentences long) to be processed by all the systems.
LXGram is a hand-built, general purpose computational grammar for the deep lin-
guistic processing of Portuguese. It is developed under the grammatical framework
of Head-Driven Phrase Structure Grammar, HPSG (Pollard and Sag, 1987, 1994; Sag
et al, 2003) and uses Minimal Recursion Semantics, MRS (Copestake et al, 2005) for
the representation of meaning. This grammar implementation is undertaken with the
LKB (Copestake, 2002) grammar development environment and its evaluation and
regression testing is done via [incr tsdb()] (Oepen, 2001). It is also intended to be
compatible with the PET parser (Callmeier, 2000).
The LinGO Grammar Matrix (version 0.9), an open-source kit for the rapid devel-
opment of grammars based on HPSG and MRS, was used as the initial code upon
which to build LXGram. The grammar is implemented in the LKB using the T DL
formalism (Krieger and Sch?fer, 1994), based on unification and on typed feature
structures, and whose types are organized in a multiple inheritance hierarchy.
For more information, please refer to a detailed implementation report (Branco and
Costa, 2008a) or on pages 31?43 of this volume (Branco and Costa, 2008b). A free
version of the grammar can also be obtained at http://nlx.di.fc.ul.pt/lxgram,
under an ELDA research license.
Section 2 introduces the main features of the Minimal Recursion Semantics for-
mat, which is employed in the semantic representations produced by LXGram. In
Section 3, the sample text that the LXGram team submitted is described, together
with an explanation of the representations derived by the grammar. Finally, Section 4
discusses the results for the full data set of the Shared Task.
2 Semantic Formalism
In LXGram, semantic information is encoded following Minimal Recursion Seman-
tics (MRS) format for semantic representation (Copestake et al, 2005). MRS has
several properties that makes it an interesting semantic representation format from the
point of view of computational semantics.
Notoriously, it allows underspecification of the scope of relevant operators, which
permits that a sentence with scope ambiguities can be given a single, underspeci-
fied representation. For some applications, for instance machine translation between
closely related languages from the same language family, the underspecified repre-
sentations may be sufficient and bring the benefit of avoiding possible combinatorial
explosion into as many parses as readings.
In a nutshell, the underspecification of scope is achieved by associating every basic
relation to a handle (in the feature structure for a relation, the feature LBL encodes
this handle) and describing the constraints that hold between these handles (in the
feature HCONS, handle constraints). These constraints can be stated in a way such that
LXGram in the Shared Task ?Comparing Semantic Representations? 301
some scope resolution options are allowed while others are discarded. Nevertheless,
there may applications for which it may be important to get fully specified semantic
representations. In this case, MRS permits that the different scope possibilities be
computed on demand from the underspecified representation.
Also worth referring in this very brief presentation of the gist of MRS, it is the
representation of conjunction with the relative order of conjuncts underspecified, by
giving the same handle to the different conjuncts. This avoids computing associativity
and commutativity of conjunction in situations where spurious overgeneration may
arise.
Please consult Branco and Costa (2008a) in this volume (pages 31?43) for an ex-
ample illustrating quantifier scope ambiguities and underspecification. Due to space
limitations, it is not possible to provide further details on the MRS formalism here.
For the presentation of MRS, please consult Copestake et al (2005).
3 Sample Text
The following sentences are our examples for the shared task:
(1) A
the
primeira
first
escola
school
de
of
treino
training
de
of
c?es-guias
leader dogs
do
of the
Pa?s
country
vai
goes
nascer
to be born
em
in
Mort?gua
Mort?gua
e
and
treinar?
will train
22
22
c?es-guias
leader dogs
por
per
ano.
year
The first school for the training of leader dogs in the country is going to be
created in Mort?gua and will train 22 leader dogs per year.
(2) Em
in
Mort?gua,
Mort?gua
Jo?o
Jo?o
Pedro
Pedro
Fonseca
Fonseca
e
and
Marta
Marta
Gomes
Gomes
coordenam
coordinate
o
the
projecto
project
que
that
sete
seven
pessoas
people
desenvolvem
develop
nesta
in this
escola.
school
In Mort?gua, Jo?o Pedro Fonseca and Marta Gomes coordinate the project
that seven people develop in this school.
(3) Visitaram
they visited
v?rios
several
espa?os
spaces
semelhantes
similar
em
in
Inglaterra
England
e
and
em
in
Fran?a,
France,
e
and
numa
in one
das
of the
escolas
schools
francesas
French
est?o
are
j?
already
em
in
est?gio
internship
duas
two
futuras
future
treinadoras.
trainers
They visited several similar places in England and in France, and two future
trainers are already doing internship in one of the French schools.
(4) Os
the
fundos
funding
comunit?rios
communitarian
asseguram
ensure
a
the
manuten??o
maintenance
da
of the
escola
school
at?
until
1999.
1999
The communitarian funding ensures the operation of the school until 1999.
302 Branco and Costa
(5) Gostar?amos
we would like
que
that
a
the
nossa
our
escola
school
funcionasse
worked
?
to the
semelhan?a
similarity
das
of the
francesas,
French
que
which
vivem
live
de
from
d?divas,
donations
do
from the
merchandising
merchandising
e
and
at?
even
das
from the
rifas
raffles
que
that
as
the
crian?as
children
vendem
sell
nas
in the
escolas.
schools
We would like our school to work similarly to the French ones, which live
from donations, from the merchandising and even from the raffles that chil-
dren sell in school.
These sentences were adapted from newspaper text. We have chosen them because
they display interesting phenomena.
The semantic representations that LXGram produces for these sentences are pre-
sented at the Shared Task website http://www.sigsem.org. An example is included
in Appendix B. Several analyses are obtained for these examples (e.g. one of the sen-
tences got 540 parses), the main reason being PP attachment ambiguity. The semantic
representations we present are the ones associated to the preferred analyses, which
were selected manually.
Note that since the representations could not be displayed in a single page, the value
of the feature RELS was split across multiple pages. To ensure readability, the values
of the other features (LTOP, INDEX and HCONS) are repeated on every page pertaining
to the same representation.
Some comments are in order concerning these representations:
? The morphological person, number and gender are encoded as features (PER-
SON, NUMBER, GENDER) of the relevant index (quantified variable) that is
present there. For indices, the boolean feature DIV is also used, that shows
the value + for plurals and mass nouns.
? Event variables are included for the relations introduced by verbs, adjectives,
prepositions and adverbs (under their ARG0 feature). The morphological in-
formation on the verbs is also encoded as features of these events. This is the
purpose of the features MOOD, TENSE and ASPECT. There is also a feature
SF (sentence force) that represents whether a sentence denotes a proposition, a
question or a command. The feature ELLIPTICAL-PUNCT denotes whether the
sentence ends with an ellipsis (. . .) and is useful in order to constrain what is
generated by the grammar.
? There is a tense_rel relation associated to each verb form. Its ARG0 feature is
the same as the ARG0 of the verb it is associated with. The purpose of this extra
relation is to make an event variable present in the semantic representations for
the copular sentences where the relevant predicate is provided by a noun (none
of these examples). In such cases this event will contain the morphological
information of the copular verb.
? Note that the information about whether adjectives have intersective semantics
(see ?franc?s???French??in sentence (3)) or non-intersective semantics (see
LXGram in the Shared Task ?Comparing Semantic Representations? 303
?futuro???future??in sentence (5)) is visible in the corresponding semantic
representations.
The names of the predicates that correspond to lexical items of several classes
(common nouns, verbs, adjectives, adverbs, prepositions, etc.) follow a naming con-
vention that includes a lemma field, a part-of-speech field and an optional sense field
(often reflecting subcategorization). Table 1 lists the predicates present in these rep-
resentations and provides the corresponding English lemmas. There are other special
relations in these representations:
? udef_q_rel
the quantifier for bare NPs
? proper_q_rel
the quantifier for proper names
? tense_rel
associated to every verbal relation (see discussion above)
? named_rel
associated to proper names
? name-precedes_rel
associated to proper names
? string-equals_rel
equality between strings
? indef_q_rel
associated to some indefinites. In particular it is the quantifier used for NPs that
are introduced by elements that can also follow determiners (e.g. cardinals and
vague quantifiers like ?v?rios???several?)
? cardinal_rel
constrains the cardinality of the set denoted by the expression linked to its ARG1
feature
? greater-or-equal_rel
the integer in its ARG0 is greater than or equal to the integer in its ARG1 feature
? plus_rel
the integer in its ARG0 is the result of summing the two integers in the TERM0
and TERM1 features
? int-equals_rel
equality between integers
? ellipsis-or-generic_n_1_rel
placeholder relation when there are missing nouns
304 Branco and Costa
Table 1: Correspondence of Portuguese MRS relations and English lemmas
MRS Relation English lemma
_ano_n_rel year
_?_semelhan?a_a_-de-_rel similarly
_assegurar_v_rel to ensure
_at?_a_rel even
_at?_p_rel until
_c?o-guia_n_rel leader dog
_comunit?rio_a_rel communitarian
_coordenar_v_rel to coordinate
_crian?a_n_rel child
_d?diva_n_-de-a-_rel donation
_de_p_rel of, from
_desenvolver_v_rel to develop
_e_coord_rel and
_em_p_rel in
_espa?o_n_rel space
_est?gio_n_rel internship
_este_a_rel this
_escola_n_rel school
_franc?s_a_rel French
_funcionar_v_rel to work
_fundo_n_rel funding
_futuro_a_rel future
_gostar_v_rel to like
_ir_v_aux_rel to be going to
_j?_a_rel already
_manuten??o_n_-de-por-_rel maintenance
_merchandising_n_rel merchandising
_nascer_v_rel to be born
_o_q_rel the
_pa?s_n_rel country
_por_p_rel per
_pessoa_n_rel person
_primeiro_a_rel first
_projecto_n_-de-por_rel project
_rifa_n_rel raffle
_semelhante_a_-a-_rel similar
_treinador_n_-de-_rel trainer
_treinar_v_rel to train
_treino_n_-de-por-_rel training
_um_q_rel a
_v?rios_a_scop_rel several
_vender_v_-a-_rel to sell
_visitar_v_rel to visit
_viver_v_rel to live
LXGram in the Shared Task ?Comparing Semantic Representations? 305
Sometimes some details of the semantic representations that are possible to obtain
depend on the features of the system where LXGram is developed and runs. In partic-
ular, for each feature that represents an argument of a relation (ARG0, ARG1, ARG2,
CARG, . . .), it must be stated in the configuration files whether it will contain a constant
(e.g. a string literal). For instance, we must say that the feature CARG always contains
a value, for visualization purposes. This fact sometimes constrains the display of the
semantic representations. It is the reason why the semantics for proper names and for
cardinals is more copious than what would seem necessary at first.
For instance, the semantics associated to ?7 pessoas? (?7 people?) in sentence (2)
is roughly ?x.cardinal_rel(e,_pessoa_n_rel(x), j1)? greater-or-equal_rel( j1, j2)?
int-equals( j2,7) (note that conjunction is denoted in MRS via identical labels for
relations). The information conveyed by the last two predicates could be simply given
by greater-or-equal_rel( j1,7). However, for that to display correctly we would have
to configure the system to display the second argument of the greater-or-equal_rel
relation as a constant. This will not always be the case: in the semantics for ?22? that
argument is the integer that is the result of summing ?20? and ?2? (number expressions
receive compositional semantics), represented with the help of the plus_rel relation.
The LKB does not allow one to compute arithmetic expressions.
These few sentences present some interesting problems for the computation of se-
mantic representation in general.
Typically, one is not able to resolve missing nouns, as this sometimes requires
access to pragmatic information. As a consequence, the semantics produced for sen-
tences with a missing noun (see sentence (5)) includes an ellipsis-or-generic_n_1_rel
instead of the relation corresponding to that noun.
Also, it is very hard if not impossible to recover missing arguments. See for in-
stance the semantics for the adjective ?semelhante? (?similar?) in sentence (3). The
missing argument is given the type r, instead of the type x of quantified variables, so
that we can omit a quantifier for it in the semantics and still be able to ask the system
for scoped solutions (the system would complain about free variables if these elements
were given the type x).
Finally, it is worth noting that there are some limitations of the semantic represen-
tations obtained given that the empirical coverage of the grammar is still in develop-
ment. Currently, the grammar does not make yet any distinction between restrictive
and non-restrictive relative clauses, as we have not focused on the fully-fledged im-
plementation of the semantics of non-restrictive relative clauses yet. This can be seen
in the semantics for the last example, where both relative clauses are semantically
combined with their head in the same way.
4 Performance in the Shared Task
There are seven small texts in the Shared Task. The sample text we submitted is text
4. We translated the other six texts into Portuguese before passing them to the system.
Translation of the Texts
The translations were done by the authors. We tried to make them as literal as possi-
ble in order to support comparability of the different systems taking part in the Shared
Task, but some bits were not literally translated as that would have produced unnatural
306 Branco and Costa
sentences. We also tried not to make the texts easy to parse by the system by simpli-
fying the texts in the translations. We present the translation for the texts 1, 2, 3, 5, 6
and 7 in the Appendix A, with English glosses.
Initial Coverage
When we tried to parse the other six texts of the Shared Task, we got 0% coverage.
The causes for parse failure were missing words in the lexicon and missing syntactic
constructions.
Since the aim of the Shared Task is not to evaluate data coverage but rather to
compare the semantic representations output by different NLP systems, we made an
effort to expand LXGram by enlarging the lexicon and implementing some syntax
rules, with the purpose of producing semantic representations for as many sentences
in the Shared Task data as possible, within the time constraints.
During this grammar expansion, we tried not to tune the grammar to these particular
sentences. We tried to make the implementation of new phenomena general. For this
reason, some phenomena were not implemented deliberately, because we felt that we
would not be able to produce general solutions for them within the time limit. This is
the case of WH- questions (present in the first text), which are not yet supported by
LXGram and whose implementation we did not want to rush.
Grammar Expansion
We added 97 lexical entries to the grammar. For some of these items, we had to create
new lexical types, because they have subcategorization frames for which there was
still no lexical type in the grammar. One example is the noun ?pedido? (order), which
was implemented as having two arguments realized by prepositional phrases, the first
one headed ?de? and the second one headed by ?a?. LXGram already contained lex-
ical types for nouns with two arguments, but introduced by different prepositions.
Although these two arguments of the noun were not present in the example where
this noun occurs (the third sentence of text 3), we nevertheless created a new lexical
type for this subcategorization frame. We could have used an existing lexical type for
nouns with no complements and that particular sentence would have parsed fine, but
the predicate for that noun would not be a two-place predicate in the MRS represen-
tation. We added 10 new lexical types.
The constructions that were implemented in LXGram in order to parse these sen-
tences were:
? the progressive. In European Portuguese, the progressive is expressed via a
form of the verb ?estar? (to be) combined with an infinitive preceded by the
preposition ?a?.
? temporal expressions headed by the verb ?haver? (there to be). The temporal
expression for some time (second sentence of text 2) is expressed in Portuguese
as ?h? algum tempo? (literally: there is some time). The verb form cannot be
analyzed as a preposition, because this sort of expression is syntactically com-
positional. For instance, the verb inflects for tense (it can appear in the imper-
fect if the main verb of the clause is in a past tense) and there can be adverbs
modifying it to its right (?h? j? algum tempo?, there is already some time, i.e.
LXGram in the Shared Task ?Comparing Semantic Representations? 307
for some time now). We created a unary syntax rule that takes as daughter a
clause headed by this verb and produces a mother node with the syntactic char-
acteristics of a clause introduced by a subordinating conjunction and modifying
another clause. This rule adds a relation similar to a relation introduced by
a subordinating conjunction, and it?s called abstract-temporal_x_rel. We take
this relation as having the meaning of ?since?, but with the two arguments re-
versed, and the Portuguese clause for that is known for some time gets analyzed
as meaning roughly there is some time (some time has passed) since that is
known. That is a very literal semantic representation, but it allows us to keep
the semantic composition mechanism completely monotonic.
? the impersonal pronoun ?se?. The most naturally sounding translation of it was
suspected that (last sentence of text 5) is ?suspeitou-se que?, with a verb in the
active voice and its subject being realized by a clitic pronoun. This clitic has to
appear adjacently to the verb, which is atypical for subjects in Portuguese.
? NP appositives. We also implemented a rule to allow NP apposition. This was
because of sentences like the second sentence in text 6.
Additionally, a few preprocessor rules were expanded. For instance, sentences like
the last sentence of text 7 require integer literals to be considered as proper names. We
cannot create lexical entries for all integers, so we added preprocessor rules in order
to contemplate the possibility of integers as proper names.
Final Results
After grammar expansion, 20 sentences out of the 30 sentences in all the texts of the
Shared Task got an analysis. The sentences that could not be parsed are the following:
? Text 1: sentences (c) and (d).
? Text 5: sentences (a), (c) and (d)
? Text 6: all sentences
? Text 7: sentences (a) and (b)
The two sentences of text 1 that could not be parsed contain WH- questions, which
are currently not supported by the system.
The sentence (a) of text 5 could not be parsed because it contains two sentences as
the complement of a verb. LXGram cannot yet combine two independent sentences,
and we chose to not implement this possibility because the combination of an n-way
ambiguous sentence with another m-way ambiguous sentence would be n ? m-way
ambiguous.
The sentence (c) of the same text was not parsed because of a semantically vacuous
clitic (not implemented yet) and a relative clause modifying another clause (also not
covered). LXGram does not support sentence relatives and we chose not to imple-
ment them yet because, if the relative pronoun is filling a subject position (as in that
sentence), the verb has to allow for propositional subjects. In LXGram, we currently
only have subcategorization frames for verbs that take NPs as subjects, and we have
to review all lexical entries for verbs before we can parse that sentence.
308 Branco and Costa
For the remaining sentences without a parse, the reason was efficiency. Several of
the sentences in the Shared Task data translate to Portuguese sentences that are very
long (over 40 words) or have a very high number of prepositions, producing many
attachment possibilities. Note that we were doing exhaustive search. In many cases
the parser would run out of memory. In order to alleviate this problem, we used the
PET parser instead of the LKB parser for the longer sentences. PET is considerably
faster, because it is implemented in C (the LKB is in Lisp), and it precompiles the
grammar into a binary format. Also, the input to PET can be preprocessed by a POS
tagger, in order to reduce lexical ambiguity. We did this preprocessing for some of the
longer sentences.
However, PET dumps MRS representations as text, and choosing the best parse
from this sort of output is not practical, especially for sentences with many readings.
So we exported the results into a format that can be read by [ incr tsdb() ], a tool for
the management of test suites and corpora. With this tool, it is possible to choose
parses by choosing discriminants derived from all analyses. Choosing or rejecting a
single discriminant can eliminate a large number of analyses in one step. However,
[ incr tsdb() ] calls the LKB to reconstruct the trees based on the output of PET (which
includes the names of the rules used and syntactic constituency), when one wants to
choose the best parse. Even though the parse forest has already been built by the
PET parser, the LKB can still run out of memory when it is reconstructing the feature
structures if the number of analyses is sufficiently large (we had a sentence with over
18000 parses).
We also tried commenting out some rules that were not necessary to parse these
sentences, with the purpose of reducing the search space. Examples include robustness
rules, for parsing strings with no verb.
In the near future, we will be working on a stochastic disambiguation module,
which PET supports, in order to constrain the parser?s search space and to keep only
the best n parses, so that we can avoid the efficiency problems that we are facing at
the moment.
Analyses
The semantic representations for the sentences that LXGram parsed successfully are
presented in the appendix. As mentioned before, we performed exhaustive search. We
chose the best parse manually.
We used [ incr tsdb() ] associated to the LKB in order to choose the preferred
reading. After that we exported the MRS representation. The LKB exports LaTeX
directly. We edited the exported LaTeX in order to make the representations fit into
the pages of the appendix. This involved manually adding newlines and page breaks.
We also corrected characters with diacritics, which did not display correctly, and we
removed characterization information: after the name of each predicate, there is a
pair of character positions indicating the substring in the input spanned by the lexical
items or rules associated to that predicate; they were removed because they are not
interpretable by someone who does not know the implementation details, e.g. the
semantics for null subjects span the substring of the entire VP since this piece of
semantics is introduced by a unary rule that takes a VP as daughter.
LXGram in the Shared Task ?Comparing Semantic Representations? 309
Discussion of the Results
We would like to comment on some of the semantic representations obtained with
LXGram.
As we have pointed out before, some details of the semantics are not completely
independent of language. For an example, see the discussion above about temporal
expressions headed by the verb ?haver?.
MRS does not directly support a treatment of intentionality. For instance, sentence
(c) of text 2 contains an intentional context: it does not assert the existence of ?other
cancers caused by viruses?. There is no standard way of representing this sort of
intentionality with MRS.
Also, MRS does not support conjunction of quantifiers. There is no MRS equiva-
lent to a lambda expression like ?P.Quant1(x,P(x))?Quant2(y,P(y)). The usual MRS
representations associated with NP coordination have to include an explicit relation for
the truth function involved (but taking referential indices as arguments), as well as an
extra quantifier relation (the relation used in these cases is called ude f_q_rel, which
is also the name for the quantifier of bare NPs).
Some phenomena are difficult to analyze. An example is in sentence (c) of text 7.
In the Portuguese translation, we have two coordinated NPs at the end of the sentence
(the best sounding translation requires a determiner before each of the two nouns),
which are followed by a PP. The Portuguese translation interprets this PP as realizing
an argument of both nouns (cf. federal government interest and federal government
tax incentives). We could not get this reading, because we do not allow PP arguments
to attach higher than determiners. The analysis that we present leaves the first noun
with this argument underspecified, as this PP attaches directly to the second noun in
the corresponding syntax tree. This possibility of PP attachment seems to be required
for cases of NP coordination like this one, but it can be a source of overgeneration
for NPs that are not coordinated. This phenomenon affects other NP elements, like
adjective phrases, that can also take scope over a coordination of NPs. The current
implementation forces all noun dependents that have a restrictive interpretation to
attach lower than determiners, as that is the place where the restrictor of the quantifier
for that NP is visible in the feature structures.
References
Bos, J. (2008). Introduction to the Shared Task on Comparing Semantic Representa-
tions. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing. STEP 2008
Conference Proceedings, Volume 1 of Research in Computational Semantics, pp.
257?261. College Publications.
Branco, A. and F. Costa (2008a). A computational grammar for deep linguistic pro-
cessing of Portuguese: LXGram, version A.4.1. Technical report, University of
Lisbon, Department of Informatics.
Branco, A. and F. Costa (2008b). High Precision Analysis of NPs with a Deep Pro-
cessing Grammar. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing.
STEP 2008 Conference Proceedings, Volume 1 of Research in Computational Se-
mantics, pp. 31?43. College Publications.
310 Branco and Costa
Callmeier, U. (2000). PET ? A platform for experimentation with efficient HPSG
processing techniques. Natural Language Engineering 6(1), 99?108. (Special
Issue on Efficient Processing with HPSG).
Copestake, A. (2002). Implementing Typed Feature Structure Grammars. Stanford:
CSLI Publications.
Copestake, A., D. Flickinger, I. A. Sag, and C. Pollard (2005). Minimal Recursion
Semantics: An introduction. Journal of Research on Language and Computa-
tion 3(2?3), 281?332.
Krieger, H.-U. and U. Sch?fer (1994). T DL ? A type description language for
constraint-based grammars. In Proceedings of the 15th International Conference
on Computational Linguistics, Kyoto, Japan, pp. 893?899.
Oepen, S. (2001). [incr tsdb()] ? competence and performance laboratory. User
manual. Technical report, Computational Linguistics, Saarland University, Saar-
br?cken, Germany. In preparation.
Pollard, C. and I. Sag (1987). Information-Based Syntax and Semantics, Vol. 1. Num-
ber 13 in CSLI Lecture Notes. Stanford: CSLI Publications.
Pollard, C. and I. Sag (1994). Head-Driven Phrase Structure Grammar. Stanford:
Chicago University Press and CSLI Publications.
Sag, I. A., T. Wasow, and E. M. Bender (2003). Syntactic Theory ? A Formal Intro-
duction (2nd ed.). Stanford: CSLI Publications.
LXGram in the Shared Task ?Comparing Semantic Representations? 311
Appendix A: Translations of the Texts for the Shared Task
Text 1
(1) Um
an
objecto
object
?
is
lan?ado
thrown
com
with
uma
a
velocidade
speed
horizontal
horizontal
de
of
20
20
m/s
m/s
de
from
um
a
penhasco
cliff
que
that
tem
has
125
125
m
m
de
of
altura.
height
An object is thrown with a horizontal speed of 20 m/s from a cliff that is 125 m high.
(2) O
the
objecto
object
cai
falls
pela
for the
altura
height
do
of the
penhasco.
cliff
The object falls for the height of the cliff.
(3) Se
if
a
the
resist?ncia
resistance
do
of the
ar
air
?
is
negligenci?vel,
negligible
quanto
how much
tempo
time
demora
takes
o
the
objecto
object
a
to
cair
fall
ao
to the
ch?o?
ground
If air resistance is negligible, how long does it take the object to fall to the ground?
(4) Qual
what
?
is
a
the
dura??o
duration
da
of the
queda?
fall
What is the duration of the fall?
Text 2
(1) O
the
cancro
cancer
cervical
cervical
?
is
causado
caused
por
by
um
a
v?rus.
virus
Cervical cancer is caused by a virus.
(2) Isso
that
?
is
conhecido
known
h?
there is
algum
some
tempo
time
e
and
levou
led
a
to
uma
a
vacina
vaccine
que
that
parece
seems
preveni-lo.
to prevent it
That has been known for some time and it has led to a vaccine that seems to prevent it.
(3) Os
the
investigadores
researchers
t?m
have
procurado
looked
outros
other
cancros
cancers
que
that
possam
may
ser
be
causados
caused
por
by
v?rus.
viruses
Researchers have been looking for other cancers that may be caused by viruses.
Text 3
(1) O
the
John
John
foi
went
a
to
um
a
restaurante.
restaurant
John went into a restaurant.
(2) Havia
there was
uma
a
mesa
table
no
in the
canto.
corner
There was a table in the corner.
(3) O
the
empregado
waiter
anotou
wrote down
o
the
pedido.
order
The waiter took the order.
(4) A
the
atmosfera
atmosphere
era
was
acolhedora
warm
e
and
simp?tica.
friendly
The atmosphere was warm and friendly.
(5) Ele
he
come?ou
began
a
to
ler
read
o
the
seu
his
livro.
book
He began to read his book.
312 Branco and Costa
Text 5
(1) Enquanto
as
os
the
3
3
canh?es
guns
do
of the
torre?o
Turret
2
2
eram
were
carregados,
loaded
um
a
membro
member
da
of the
equipa
crew
que
who
estava
was
a
to
operar
operate
o
the
canh?c?o
gun
central
central
gritou
yelled
ao
to the
telefone
phone
?Tenho
I have
aqui
here
um
a
problema.
problem.
Ainda
Still
n?o
not
estou
I am
preparado?.
ready
As the 3 guns of Turret 2 were being loaded, a crewman who was operating the center gun yelled
into the phone, ?I have a problem here. I am not ready yet.?
(2) Ent?o
then
o
the
explosivo
propellant
rebentou.
exploded
Then the propellant exploded.
(3) Quando
when
os
the
membros
members
da
of the
equipa
crew
do
of the
canh?o
gun
morreram,
died
estavam
they were
agachados
crouching
de
of
forma
way
n?o
not
natural,
natural
o que
which
sugeria
suggested
que
that
sabiam
they knew
que
that
se
DUMMY CLITIC
daria
would happen
uma
an
explos?o.
explosion
When the gun crew was killed they were crouching unnaturally, which suggested that they knew that
an explosion would happen.
(4) O
the
explosivo
propellant
que
that
foi
was
usado
used
era
was
feito
made
de
from
peda?os
chunks
de
of
nitrocelulose
nitrocellulose
que
that
foram
were
produzidos
produced
durante
during
a
the
Segunda
second
Guerra
world
Mundial
war
e
and
foram
were
reembalados
repackaged
em
in
1987
1987
em
in
sacos
bags
que
that
foram
were
feitos
made
em
in
1945.
1945
The propellant that was used was made from nitrocellulose chunks that were produced during World
War II and were repackaged in 1987 in bags that were made in 1945.
(5) Inicialmente,
initially
suspeitou-se
suspected IMPERSONAL SUBJECT
que
that
este
this
armazenamento
storage
poderia
might
ter
have
reduzido
reduced
a
the
estabilidade
stability
da
of the
p?lvora.
powder
Initially it was suspected that this storage might have reduced the powder?s stability.
Text 6
(1) Entre
amid
as
the
filas
rows
cerradas
tightly packed
de
of
casas
houses
do
of the
norte
north
de
of
Filad?lfia,
Philadelphia
uma
a
quinta
farm
urbana
urban
pioneira
pioneering
est?
is
a
to
produzir
produce
comida
food
local
local
fresca
fresh
para
for
uma
a
comunidade
community
que
that
frequentemente
often
n?o
not
a
it
tem,
has
e
and
a
to
gerar
generate
dinheiro
money
com
with
isso.
it
Amid the tightly packed row houses of North Philadelphia, a pioneering urban farm is providing
fresh local food for a community that often lacks it, and making money in the process.
(2) Greensgrow,
Greensgrow
um
a
terreno
plot
de
of
um
one
acre
acre
de
of
canteiros
beds
elevados
raised
e
and
estufas
greenhouses
no
on the
local
site
de
of
uma
a
antiga
former
f?brica
factory
de
of
galvaniza??o
galvanization
de
of
a?o,
steel
est?
is
a
to
ter
have
lucro
profit
vendendo
selling
os
the
pr?prios
own
vegetais
vegetables
e
and
ervas
herbs
assim como
as well as
uma
a
gama
range
de
of
produtos
products
de
from
agricultores
farmers
locais,
local
e
and
gerindo
managing
um
a
viveiro
nursery
que
that
vende
sells
plantas
plants
e
and
pl?ntulas.
seedlings
Greensgrow, a one-acre plot of raised beds and greenhouses on the site of a former steel-galvanizing
factory, is turning a profit by selling its own vegetables and herbs as well as a range of produce
from local growers, and by running a nursery selling plants and seedlings.
LXGram in the Shared Task ?Comparing Semantic Representations? 313
(3) A
the
quinta
farm
lucrou
earned
cerca de
about
10000
10000
d?lares
dollars
com
with
uma
a
receita
revenue
de
of
450000
450000
d?lares
dollars
em
in
2007,
2007
e
and
espera
hopes
ter
to have
um
a
lucro
profit
de
of
5%
5%
sobre
on
os
the
650000
650000
d?lares
dollars
de
of
receitas
revenue
neste
in this
ano,
year
o
the
seu
its
10??
10th
ano,
year
para
in order
poder
to be able
abrir
to open
outra
another
actividade
operation
noutro
in another
s?tio
place
de
of
Filad?lfia.
Philadelphia
The farm earned about $10,000 on revenue of $450,000 in 2007, and hopes to make a profit of 5
percent on $650,000 in revenue in this, its 10th year, so it can open another operation elsewhere in
Philadelphia.
Text 7
(1) O
the
desenvolvimento
development
moderno
modern
da
of the
tecnologia
techonology
e
and
aplica??es
applications
de
of
energia
energy
e?lica
wind.ADJECTIVE
j?
already
estava
was
numa
in a
fase
phase
avan?ada
advanced
nos
by the
anos
years
30,
30
quando
when
por
by
estimativa
estimation
cerca de
about
600000
600000
moinhos
mills
forneciam
supplied
?reas
areas
rurais
rural
com
with
electricidade
electricity
e
and
servi?os
services
de
of
bombeamento
pumping
de
of
?gua.
water
Modern development of wind-energy technology and applications was well underway by the 1930s,
when an estimated 600,000 windmills supplied rural areas with electricity and water-pumping ser-
vices.
(2) Quando
when
a
the
distribui??o
distribution
em
in
larga
broad
escala
scale
de
of
electricidade
electricity
chegou
arrived
?s
to the
quintas
farms
e
and
?s
to the
terras
small
pequenas,
towns
o
the
uso
use
de
of
energia
energy
e?lica
wind.ADJECTIVE
nos
in the
Estados Unidos
United States
come?ou
started
a
to
diminuir,
subside
mas
but
voltou
it went back
a
to
subir
raise
depois
after
da
of the
falta
shortage
de
of
petr?leo
oil
nos
in the
EUA
US
no
in the
come?o
beginning
dos
of the
anos
years
70.
70
Once broad-scale electricity distribution spread to farms and country towns, use of wind energy in
the United States started to subside, but it picked up again after the U.S. oil shortage in the early
1970s.
(3) Nos
in the
?ltimos
last
30
30
anos,
years
a
the
investiga??o
research
e
and
o
the
desenvolvimento
development
t?m
have
oscilado
fluctuated
de acordo
in accordance
com
with
o
the
interesse
interest
e
and
os
the
benef?cios
benefits
fiscais
fiscal
do
of the
governo
government
federal.
federal
Over the past 30 years, research and development has fluctuated with federal government interest
and tax incentives.
(4) Em
in
meados
middle
dos
of the
anos
years
80,
80
as
the
turbinas
turbines
e?licas
wind.ADJECTIVE
tinham
had
tipicamente
typically
uma
a
pot?ncia
power rating
m?xima
maximum
de
of
150
150
kW.
kW
In the mid-?80s, wind turbines had a typical maximum power rating of 150 kW.
(5) Em
In
2006,
2006
as
the
turbinas
turbines
comerciais
commercial
de
of
grande
large
escala
scale
s?o
are
comummente
commonly
avaliadas
rated
em
at
mais
more
de
than
1
1
MW
MW
e
and
est?o
are
dispon?veis
available
em
in
no
at the
m?ximo
most
4
4
MW
MW
de
of
capacidade.
capacity
In 2006, commercial, utility-scale turbines are commonly rated at over 1 MW and are available in
up to 4 MW capacity.
314 Branco and Costa
Appendix B: MRS Representation for Text 4, Sentence 1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_o_q_rel
LBL h3 h
ARG0 x4
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER feminine
DIV -
?
?
?
?
?
?
?
RSTR h6 h
BODY h5 h
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_primeiro_a_rel
LBL h7 h
ARG0 e8 e
ARG1 h9 h
?
?
?
?
?
,
?
?
?
_escola_n_rel
LBL h9
ARG0 x4
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_de_p_rel
LBL h9
ARG0 e10 e
ARG1 x4
ARG2 x11
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER masculine
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h12 h
ARG0 x11
RSTR h14 h
BODY h13 h
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_treino_n_-de-por-_rel
LBL h15 h
ARG0 x11
ARG1 r17 r
ARG2 x16
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.GENDER masculine
PNG.NUMBER plural
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h18 h
ARG0 x16
RSTR h20 h
BODY h19 h
?
?
?
?
?
?
?
,
?
?
?
_c?o-guia_n_rel
LBL h21 h
ARG0 x16
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_de_p_rel
LBL h9
ARG0 e22 e
ARG1 x4
ARG2 x23
?
?
?
?
?
?
?
x
DIV -
PNG.NUMBER singular
PNG.GENDER masculine
PNG.PERSON 3rd
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
_o_q_rel
LBL h24 h
ARG0 x23
RSTR h26 h
BODY h25 h
?
?
?
?
?
?
?
,
?
?
?
_pa?s_n_rel
LBL h27 h
ARG0 x23
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
tense_rel
LBL h28 h
ARG0 e29
?
?
?
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT -
E.MOOD indicativo
E.TENSE presente
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_ir_v_aux_rel
LBL h28
ARG0 e29
ARG1 h30 h
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
tense_rel
LBL h31 h
ARG0 e32
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT bool
E.MOOD infinitivo-nao-flexionado
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_nascer_v_rel
LBL h31
ARG0 e32
ARG1 x4
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_em_p_rel
LBL h31
ARG0 e33 e
ARG1 e32
ARG2 x34
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER feminine
DIV -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
proper_q_rel
LBL h35 h
ARG0 x34
RSTR h37 h
BODY h36 h
?
?
?
?
?
?
?
,
?
?
?
?
?
named_rel
LBL h38 h
ARG0 x34
ARG1 s39 s
?
?
?
?
?
,
?
?
?
?
?
string-equals_rel
LBL h38
ARG0 s39
CARG mort?gua
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_e_coord_rel
LBL h40 h
C-ARG e2
L-HNDL h28
L-INDEX e29
R-HNDL h42 h
R-INDEX e41
?
?
?
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT -
E.MOOD indicativo
E.TENSE futuro
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
tense_rel
LBL h42
ARG0 e41
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_treinar_v_rel
LBL h42
ARG0 e41
ARG1 x4
ARG2 x43
?
?
?
?
?
?
?
x
PNG.NUMBER plural
PNG.GENDER masculine
PNG.PERSON 3rd
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
indef_q_rel
LBL h44 h
ARG0 x43
RSTR h45 h
BODY h46 h
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
cardinal_rel
LBL h47 h
ARG0 e49 e
ARG1 h50 h
ARG2 j48 j
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
greater-or-equal_rel
LBL h47
ARG0 j48
ARG1 j51 j
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
plus_rel
LBL h47
ARG0 j51
TERM0 j53 j
TERM1 j52 j
?
?
?
?
?
?
?
?
,
?
?
?
?
?
int-equals_rel
LBL h47
ARG0 j53
CARG 20
?
?
?
?
?
,
?
?
?
?
?
int-equals_rel
LBL h47
ARG0 j52
CARG 2
?
?
?
?
?
,
?
?
?
_c?o-guia_n_rel
LBL h50
ARG0 x43
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_por_p_rel
LBL h42
ARG0 e54 e
ARG1 e41
ARG2 x55
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER masculine
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h56 h
ARG0 x55
RSTR h58 h
BODY h57 h
?
?
?
?
?
?
?
,
?
?
?
_ano_n_rel
LBL h59 h
ARG0 x55
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h1
LARG h40
?
?
?
,
?
?
?
qeq
HARG h6
LARG h7
?
?
?
,
?
?
?
qeq
HARG h14
LARG h15
?
?
?
,
?
?
?
qeq
HARG h20
LARG h21
?
?
?
,
?
?
?
qeq
HARG h26
LARG h27
?
?
?
,
?
?
?
qeq
HARG h30
LARG h31
?
?
?
,
?
?
?
qeq
HARG h37
LARG h38
?
?
?
,
?
?
?
qeq
HARG h45
LARG h47
?
?
?
,
?
?
?
qeq
HARG h58
LARG h59
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
