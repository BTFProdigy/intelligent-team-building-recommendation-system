TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Proceedings of the ACL Student Research Workshop, pages 139?144,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Corpus-Oriented Development of Japanese HPSG Parsers
Kazuhiro Yoshida
Department of Computer Science,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033
kyoshida@is.s.u-tokyo.ac.jp
Abstract
This paper reports the corpus-oriented de-
velopment of a wide-coverage Japanese
HPSG parser. We first created an HPSG
treebank from the EDR corpus by us-
ing heuristic conversion rules, and then
extracted lexical entries from the tree-
bank. The grammar developed using this
method attained wide coverage that could
hardly be obtained by conventional man-
ual development. We also trained a statis-
tical parser for the grammar on the tree-
bank, and evaluated the parser in terms of
the accuracy of semantic-role identifica-
tion and dependency analysis.
1 Introduction
In this study, we report the corpus-oriented de-
velopment of a Japanese HPSG parser using the
EDR Japanese corpus (2002). Although several re-
searchers have attempted to utilize linguistic gram-
mar theories, such as LFG (Bresnan and Kaplan,
1982), CCG (Steedman, 2001) and HPSG (Pollard
and Sag, 1994), for parsing real-world texts, such at-
tempts could hardly be successful, because manual
development of wide-coverage linguistically moti-
vated grammars involves years of labor-intensive ef-
fort.
Corpus-oriented grammar development is a gram-
mar development method that has been proposed as
a promising substitute for conventional manual de-
velopment. In corpus-oriented methods, a treebank
of a target grammar is constructed first, and various
grammatical constraints are extracted from the tree-
bank. Previous studies reported that wide-coverage
grammars can be obtained at low cost by using this
method. (Hockenmaier and Steedman, 2002; Miyao
et al, 2004) The treebank can also be used for train-
ing statistical disambiguation models, and hence we
can construct a statistical parser for the extracted
grammar.
The corpus-oriented method enabled us to de-
velop a Japanese HPSG parser with semantic infor-
mation, whose coverage on real-world sentences is
95.3%. This high coverage allowed us to evaluate
the parser in terms of the accuracy of dependency
analysis on real-world texts, the evaluation measure
that is previously used for more statistically-oriented
parsers.
2 HPSG
Head-Driven Phrase Structure Grammar (HPSG) is
classified into lexicalized grammars (Schabes et al,
1988). It attempts to model linguistic phenomena
by interactions between a small number of grammar
rules and a large number of lexical entries. Figure
1 shows an example of an HPSG derivation of a
Japanese sentence ?kare ga shinda,? which means,
?He died.? In HPSG, linguistic entities such as words
and phrases are represented by typed feature struc-
tures called signs, and the grammaticality of a sen-
tence is verified by applying grammar rules to a se-
quence of signs. The sign of a lexical entry encodes
the type and valence (i.e. restriction on the types of
phrases that can appear around the word) of a corre-
sponding word. Grammar rules of HPSG consist of
139
RULE complement_head
SIGN
HEAD verb
SPR
COMPS
HEAD verb
SPR
COMPS 2 PP"ga"
"shinda"
died
RULE specifier_head
SIGN 2
HEAD PP"ga"
SPR
COMPS
HEAD PP"ga"
SPR 1 noun
COMPS
"ga"
NOM
1
HEAD noun
SPR
COMPS
"kare"
he
Figure 1: Example of HPSG analysis.
schemata and principles, the former enumerate pos-
sible patterns of phrase structures, and the latter are
basically for controlling the inheritance of daugh-
ters? features to the parent.
In the current example, the lexical entry for
?shinda? is of the type verb, as indicated in its
HEAD, and its COMPS feature restricts its preced-
ing phrase to be of the type PP?ga?. The HEAD
feature of the root node of the derivation is inher-
ited from the lexical entry for ?shinda?, because
complement-head structures are head-final, and the
head feature principle states that the HEAD feature
of a phrase must be inherited from its head daughter.
There are several implementations of Japanese
HPSG grammars. JACY (Siegel and Bender, 2002)
is a hand-crafted Japanese HPSG grammar that pro-
vides semantic information as well as linguistically
motivated analysis of complex constructions. How-
ever, the evaluation of the grammar has not been
done on domain-independent real-world texts such
as newspaper articles. Although Bond et al (2004)
attempted to improve the coverage of the JACY
grammar through the development of an HPSG tree-
bank, they limited the target of their treebank an-
notation to short sentences from dictionary defini-
tions. SLUNG (Mitsuishi et al, 1998) is an HPSG
grammar whose coverage on real-world sentences
is about 99%, but the grammar is underspecified,
which means that the constraints of the grammar are
not sufficient for conducting semantic analysis. By
employing corpus-oriented development, we aim to
develop a wide-coverage HPSG parser that enables
sign
SYNSEM
synsem
LOCAL
local
CAT
cat
HEAD
head
MOD RIGHT synsemLEFT synsem
BAR phrase/chunk
VAL
SPR local
COMPS
AGENT local
OBJECT local
GOAL local
CONT content
Figure 2: Sign of the grammar.
semantic analysis of real-word texts.
3 Grammar Design
First, we provide a brief description of some char-
acteristics of Japanese. Japanese is head final, and
phrases are typically headed by function words. Ar-
guments of verbs usually have no fixed order (this
phenomenon is called scrambling) and are freely
omitted. Arguments? semantic relations to verbs
are chiefly determined by their head postpositions.
For example, ?boku/I ga/NOM kare/he wo/ACC ko-
roshi/kill ta/DECL? (I killed him) can be paraphrased
as ?kare wo boku ga koroshi ta,? without changing
the meaning.
The case alternation phenomenon must also be
taken into account. Case alternation is caused by
special auxiliaries ?(sa)se? and ?(ra)re,? which are
causative and passive auxiliaries, respectively, and
the verbs change their subcategorization behavior
when they are combined with these auxiliaries.
The following sections describe the design of our
grammar. Especially, treatment of the scrambling
and case alternation phenomena is provided in de-
tail.
3.1 Fundamental Phrase Structures
Figure 2 presents the basic structure of signs of our
grammar. The HEAD feature specifies phrasal cat-
egories, the MOD feature represents restrictions on
the left and right modifiees, and the VAL feature en-
codes valence information. (For the explanation of
the BAR feature, see the description of the promo-
140
Table 1: Schemata and their uses.
schema name common use of the rule
specifier-head PP or NP + postposition
VP + verbal ending
NP + suffix
complement-head argument (PP/NP) + verb
compound-noun NP + NP
modifier-head modifier + head
head-modifier phrase + punctuation
promotion promotes chunks to phrases
tion schema below.) 1 For some types of phrases,
additional features are specified as HEAD features.
Now, we provide a detailed explanation of the de-
sign of the schemata and how the features in Figure
2 work. The following descriptions are also summa-
rized in Table 1.
specifier-head schema Words are first concate-
nated by this schema to construct basic word chunks.
Postpositional phrases (PPs), which consist of post-
positions and preceding phrases, are the most typi-
cal example of specifier-head structures. For post-
positions, we specify a head feature PFORM, with
the postposition?s surface string as its value, in addi-
tion to the features in Figure 2, because differences
of postpositions play a crucial role in disambiguat-
ing semantic-structures of Japanese. For example,
the postposition ?wo? has a PFORM feature whose
value is ?wo,? and it accepts an NP as its specifier.
As a result, a PP such as ?kare wo? inherits the value
of PFORM feature ?wo? from ?wo.?
The schema is also used when VPs are con-
structed from verbs and their endings (or, sometimes
auxiliaries. See also Section 3.2).
complement-head schema This schema is used
for combining VPs with their subcategorized argu-
ments (see Section 3.2 for details).
compound-noun schema Because nouns can be
freely concatenated to form compound nouns, a spe-
cial schema is used for compound nouns.
modifier-head schema This schema is for modi-
fiers and their heads. Binary structures that cannot
be captured by the above three schemata are also
1The CONTENT feature, which should contain information
about the semantic contents of syntactic entities, is ignored in
the current implementation of the grammar.
considered to be modifier-head structures.2
head-modifier schema This schema is used when
the modifier-head schema is not appropriate. In the
current implementation, it is used for a phrase and
its following punctuation.
promotion schema This unary schema changes
the value of the BAR feature from chunk to phrase.
The distinction between these two types of con-
stituents is for prohibiting some kind of spurious
ambiguities. For example, ?kinou/yesterday ko-
roshi/kill ta/DECL? can be analyzed in two differ-
ent ways, i.e. ?(kinou (koroshi ta))? and ?((kinou
koroshi) ta).? The latter analysis is prevented by
restricting ?kinou??s modifiee to be a phrase, and
?ta??s specifier to be a chunk, and by assuming ?ko-
roshi? to be a chunk.
3.2 Scrambling and Case Alternation
Scrambling causes problems in designing a Japanese
HPSG grammar, because original HPSG, designed
for English, specifies the subcategorization frame of
a verb as an ordered list, and the semantic roles of
arguments are determined by their order in the com-
plement list.
Our implementation treats the complement fea-
ture as a list of semantic roles. Semantic roles for
which verbs subcategorize are agent, object, and
goal.3 Correspondingly, we assume three subtypes
of the complement-head schema: the agent-head,
object-head, and goal-head schemata. When verbs
take their arguments, arguments receive semantic
roles which are permitted by the subcategorization
of verbal signs. We do not restrict the order of
application of the three types of complement-head
schemata, so that a single verbal lexical entry can
accept arguments that are scrambled in arbitrary or-
der. In Figure 3, ?kare ga? is a ga-marked PP, so it is
analyzed as an agent of ?koro(su).? 4
Case alternation is caused by special auxiliaries
?(sa)se? and ?(ra)re.? For instance, in ?boku/I
2Current implementation of the grammar treats complex
structures such as relative clause constructions and coordina-
tions just the same as simple modification.
3These are the three roles most commonly found in EDR.
4We assume that a single semantic role cannot be occupied
by more than one syntactic entities. This assumption is some-
times violated in EDR?s annotation, causing failures in grammar
extraction.
141
comp_head
HEAD verb
AGENT 1 PP"ga"
OBJECT PP"wo"
"korosu"
kill
1 HEAD PP"ga"
"kare ga"
he-NOM
Figure 3: Verb and its argument.
HEAD verb
SPR
verb
HEAD PASSIVE plus
COMPS 1
COMPS 1
Figure 4: Lexical sign of ?(ra)re?.
ga/NOM kare/he ni/DAT korosa/kill re/PASSIVE
ta/DECL? (I was killed by him), ?korosa? takes a
?ga?-marked PP as an object and a ?ni?-marked PP
as an agent, though without ?(sa)re,? it takes a ?ga?-
marked PP as an agent and a ?wo?-marked PP as an
object.
We consider auxiliaries as a special type of
verbs which do not have their own subcategoriza-
tion frames. They inherit the subcategorization
frames of verbs.5 To capture the case alternation
phenomenon, each verb has distinct lexical entries
for its passive and causative uses. This distinc-
tion is made by binary valued HEAD features, PAS-
SIVE and CAUSATIVE. The passive (causative) aux-
iliary restricts the value of its specifier?s PASSIVE
(CAUSATIVE) feature to be plus, so that it can only
be combined with properly case-alternated verbal
lexical entries.
Figure 4 presents the lexical sign of the passive
auxiliary ?(ra)re.? Our analysis of an example sen-
tence is presented in Figure 5. Note that the passive
auxiliary ?re(ta)? requires the value of the PASSIVE
feature of its specifier be plus, and hence ?koro(sa)?
cannot take the same lexical entry as in Figure 3.
4 Grammar Extraction from EDR
The EDR Japanese corpus consists of 207802 sen-
tences, mainly from newspapers and magazines.
The annotation of the corpus includes word segmen-
5The control phenomena caused by auxiliaries are currently
unsupported in our grammar.
comp_head
HEAD verb
AGENT PP"ni"
OBJECT 3 PP"ga"
HEAD verb
SPR
verb
HEAD PASSIVE plus
AGENT 1
OBJECT 2
AGENT 1
OBJECT 2
"reta"
PASSIVE
HEAD
verb
PASSIVE plus
AGENT 1 PP"ni"
OBJECT 2 PP"ga"
"korosa"
kill
3 HEAD PP"ga"
"kare ga"
he-NOM
Figure 5: Example of passive construction.
tation, part-of-speech (POS) tags, phrase structure
annotation, and semantic information.
The heuristic conversion of the EDR corpus into
an HPSG treebank consists of the following steps. A
sentence ?((kare/NP-he wo/PP-ACC) (koro/VP-kill
shi/VP-ENDING ta/VP-DECL))? ([I] killed him yes-
terday) is used to provide examples in some steps.
Phrase type annotation Phrase type labels such
as NP and VP are assigned to non-terminal nodes.
Because Japanese is head final, the label of the right-
most daughter of a phrase is usually percolated to its
parent. After this step, the example sentence will be
?((PP kare/NP wo/PP) (VP koro/VP shi/VP ta/VP)).?
Assign head features The types of head features
of terminal nodes are determined, chiefly from their
phrase types. Features specific to some categories,
such as PFORM, are also assigned in this step.
Binarization Phrases for which EDR employs flat
annotation are converted into binary structures. The
binarized phrase structure of the example sentence
will be ?((kare wo) ((koro shi) ta)).?
Assign schema names Schema names are as-
signed according to the patterns of phrase structures.
For instance, a phrase structure which consists of
PP and VP is identified as a complement-head struc-
ture, if the VP?s argument and the PP are coindexed.
In the example sentence, ?kare wo? is annotated as
?koro?s object in EDR, so the object-head schema is
applied to the root node of the derivation.
Inverse schema application The consistency of
the derivation of the obtained HPSG treebank is ver-
142
ified by applying the schemata to each node of the
derivation trees in the treebank.
Lexicon Extraction Lexical entries are extracted
from the terminal nodes of the obtained treebank.
5 Disambiguation Model
We also train disambiguation models for the gram-
mar using the obtained treebank. We employ log-
linear models (Berger et al, 1996) for the disam-
biguation. The probability of a parse   of a sentence

is calculated as follows:

 
	 




 



Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1017?1024,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Retrieval for the Accurate Identification of Relational Concepts
in Massive Textbases
Yusuke Miyao? Tomoko Ohta? Katsuya Masuda? Yoshimasa Tsuruoka?
Kazuhiro Yoshida? Takashi Ninomiya? Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?Information Technology Center, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{yusuke,okap,kmasuda,tsuruoka,kyoshida,ninomi,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper introduces a novel framework
for the accurate retrieval of relational con-
cepts from huge texts. Prior to retrieval,
all sentences are annotated with predicate
argument structures and ontological iden-
tifiers by applying a deep parser and a term
recognizer. During the run time, user re-
quests are converted into queries of region
algebra on these annotations. Structural
matching with pre-computed semantic an-
notations establishes the accurate and effi-
cient retrieval of relational concepts. This
framework was applied to a text retrieval
system for MEDLINE. Experiments on
the retrieval of biomedical correlations re-
vealed that the cost is sufficiently small for
real-time applications and that the retrieval
precision is significantly improved.
1 Introduction
Rapid expansion of text information has motivated
the development of efficient methods of access-
ing information in huge texts. Furthermore, user
demand has shifted toward the retrieval of more
precise and complex information, including re-
lational concepts. For example, biomedical re-
searchers deal with a massive quantity of publica-
tions; MEDLINE contains approximately 15 mil-
lion references to journal articles in life sciences,
and its size is rapidly increasing, at a rate of more
than 10% yearly (National Library of Medicine,
2005). Researchers would like to be able to
search this huge textbase for biomedical correla-
tions such as protein-protein or gene-disease asso-
ciations (Blaschke and Valencia, 2002; Hao et al,
2005; Chun et al, 2006). However, the framework
of traditional information retrieval (IR) has diffi-
culty with the accurate retrieval of such relational
concepts because relational concepts are essen-
tially determined by semantic relations between
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
The present paper demonstrates a framework
for the accurate real-time retrieval of relational
concepts from huge texts. Prior to retrieval, we
prepare a semantically annotated textbase by ap-
plying NLP tools including deep parsers and term
recognizers. That is, all sentences are annotated
in advance with semantic structures and are stored
in a structured database. User requests are con-
verted on the fly into patterns of these semantic
annotations, and texts are retrieved by matching
these patterns with the pre-computed semantic an-
notations. The accurate retrieval of relational con-
cepts is attained because we can precisely describe
relational concepts using semantic annotations. In
addition, real-time retrieval is possible because se-
mantic annotations are computed in advance.
This framework has been implemented for a
text retrieval system for MEDLINE. We first ap-
ply a deep parser (Miyao and Tsujii, 2005) and
a dictionary-based term recognizer (Tsuruoka and
Tsujii, 2004) to MEDLINE and obtain annotations
of predicate argument structures and ontological
identifiers of genes, gene products, diseases, and
events. We then provide a search engine for these
annotated sentences. User requests are converted
into queries of region algebra (Clarke et al, 1995)
extended with variables (Masuda et al, 2006) on
these annotations. A search engine for the ex-
tended region algebra efficiently finds sentences
having semantic annotations that match the input
queries. In this paper, we evaluate this system with
respect to the retrieval of biomedical correlations
1017
Symbol CRP
Name C-reactive protein, pentraxin-related
Species Homo sapiens
Synonym MGC88244, PTX1
Product C-reactive protein precursor, C-reactive
protein, pentraxin-related protein
External links EntrezGene:1401, GDB:119071, . . .
Table 1: An example GENA entry
and examine the effects of using predicate argu-
ment structures and ontological identifiers.
The need for the discovery of relational con-
cepts has been investigated intensively in Infor-
mation Extraction (IE). However, little research
has targeted on-demand retrieval from huge texts.
One difficulty is that IE techniques such as pat-
tern matching and machine learning require heav-
ier processing in order to be applied on the fly.
Another difficulty is that target information must
be formalized beforehand and each system is de-
signed for a specific task. For instance, an IE
system for protein-protein interactions is not use-
ful for finding gene-disease associations. Apart
from IE research, enrichment of texts with vari-
ous annotations has been proposed and is becom-
ing a new research area for information manage-
ment (IBM, 2005; TEI, 2004). The present study
basically examines this new direction in research.
The significant contribution of the present paper,
however, is to provide the first empirical results of
this framework for a real task with a huge textbase.
2 Background: Resources and Tools for
Semantic Annotations
The proposed system for the retrieval of relational
concepts is a product of recent developments in
NLP resources and tools. In this section, ontology
databases, deep parsers, and search algorithms for
structured data are introduced.
2.1 Ontology databases
Ontology databases are collections of words and
phrases in specific domains. Such databases have
been constructed extensively for the systematic
management of domain knowledge by organizing
textual expressions of ontological entities that are
detached from actual sentences.
For example, GENA (Koike and Takagi, 2004)
is a database of genes and gene products that
is semi-automatically collected from well-known
databases, including HUGO, OMIM, Genatlas,
Locuslink, GDB, MGI, FlyBase, WormBase,
Figure 1: An output of HPSG parsing
Figure 2: A predicate argument structure
CYGD, and SGD. Table 1 shows an example of
a GENA entry. ?Symbol? and ?Name? denote
short forms and nomenclatures of genes, respec-
tively. ?Species? represents the organism species
in which this gene is observed. ?Synonym? is a
list of synonyms and name variations. ?Product?
gives a list of products of this gene, such as pro-
teins coded by this gene. ?External links? pro-
vides links to other databases, and helps to obtain
detailed information from these databases. For
biomedical terms other than genes/gene products,
the Unified Medical Language System (UMLS)
meta-thesaurus (Lindberg et al, 1993) is a large
database that contains various names of biomedi-
cal and health-related concepts.
Ontology databases provide mappings be-
tween textual expressions and entities in the real
world. For example, Table 1 indicates that CRP,
MGC88244, and PTX1 denote the same gene con-
ceptually. Hence, these resources enable us to
canonicalize variations of textual expressions of
ontological entities.
2.2 Parsing technologies
Recently, state-of-the-art CFG parsers (Charniak
and Johnson, 2005) can compute phrase structures
of natural sentences at fairly high accuracy. These
parsers have been used in various NLP tasks in-
cluding IE and text mining. In addition, parsers
that compute deeper analyses, such as predicate
argument structures, have become available for
1018
the processing of real-world sentences (Miyao and
Tsujii, 2005). Predicate argument structures are
canonicalized representations of sentence mean-
ings, and express the semantic relations of words
explicitly. Figure 1 shows an output of an HPSG
parser (Miyao and Tsujii, 2005) for the sentence
?A normal serum CRP measurement does not ex-
clude deep vein thrombosis.? The dotted lines ex-
press predicate argument relations. For example,
the ARG1 arrow coming from ?exclude? points
to the noun phrase ?A normal serum CRP mea-
surement?, which indicates that the subject of ?ex-
clude? is this noun phrase, while such relations are
not explicitly represented by phrase structures.
Predicate argument structures are beneficial for
our purpose because they can represent relational
concepts in an abstract manner. For example, the
relational concept of ?CRP excludes thrombosis?
can be represented as a predicate argument struc-
ture, as shown in Figure 2. This structure is univer-
sal in various syntactic expressions, such as pas-
sivization (e.g., ?thrombosis is excluded by CRP?)
and relativization (e.g., ?thrombosis that CRP ex-
cludes?). Hence, we can abstract surface varia-
tions of sentences and describe relational concepts
in a canonicalized form.
2.3 Structural search algorithms
Search algorithms for structured texts have been
studied extensively, and examples include XML
databases with XPath (Clark and DeRose, 1999)
and XQuery (Boag et al, 2005), and region alge-
bra (Clarke et al, 1995). The present study fo-
cuses on region algebra extended with variables
(Masuda et al, 2006) because it provides an ef-
ficient search algorithm for tags with cross bound-
aries. When we annotate texts with various levels
of syntactic/semantic structures, cross boundaries
are inherently nonnegligible. In fact, as described
in Section 3, our system exploits annotations of
predicate argument structures and ontological en-
tities, which include substantial cross boundaries.
Region algebra is defined as a set of operators
on regions, i.e., word sequences. Table 2 shows
operators of the extended region algebra, where
A and B denote regions, and results of operations
are also regions. For example, ?A & B? denotes a
region that includes both A and B. Four contain-
ment operators, >, >>, <, and <<, represent an-
cestor/descendant relations in XML. For example,
?A > B? indicates that A is an ancestor of B. In
[tag] Region covered with ?<tag>?
A > B A containing B
A >> B A containing B (A is not nested)
A < B A contained by B
A << B A contained by B (B is not nested)
A - B Starting with A and ending with B
A & B A and B
A | B A or B
Table 2: Operators of the extended region algebra
[sentence] >>
(([word arg1="$subject"] > exclude) &
([phrase id="$subject"] > CRP))
Figure 3: A query of the extended region algebra
Figure 4: Matching with the query in Figure 3
search algorithms for region algebra, the cost of
retrieving the first answer is constant, and that of
an exhaustive search is bounded by the lowest fre-
quency of a word in a query (Clarke et al, 1995).
Variables in the extended region algebra allow
us to express shared structures and are necessary
in order to describe predicate argument structures.
For example, Figure 3 shows a formula in the ex-
tended region algebra that represents the predicate
argument structure of ?CRP excludes something.?
This formula indicates that a sentence contains a
region in which the word ?exclude? exists, the
first argument (?arg1?) phrase of which includes
the word ?CRP.? A predicate argument relation is
expressed by the variable, ?$subject.? Figure 4
shows a situation in which this formula is satisfied.
Three horizontal bars describe regions covered by
<sentence>, <phrase>, and <word> tags,
respectively. The dotted line denotes the relation
expressed by this variable. Given this formula as a
query, a search engine can retrieve sentences hav-
ing semantic annotations that satisfy this formula.
3 A Text Retrieval System for MEDLINE
While the above resources and tools have been de-
veloped independently, their collaboration opens
up a new framework for the retrieval of relational
concepts, as described below (Figure 5).
Off-line processing: Prior to retrieval, a deep
parser is applied to compute predicate argument
1019
Figure 5: Framework of semantic retrieval
structures, and a term recognizer is applied to cre-
ate mappings from textual expressions into identi-
fiers in ontology databases. Semantic annotations
are stored and indexed in a structured database for
the extended region algebra.
On-line processing: User input is converted into
queries of the extended region algebra. A search
engine retrieves sentences having semantic anno-
tations that match the queries.
This framework is applied to a text retrieval en-
gine for MEDLINE. MEDLINE is an exhaustive
database covering nearly 4,500 journals in the life
sciences and includes the bibliographies of arti-
cles, about half of which have abstracts. Research
on IE and text mining in biomedical science has
focused mainly on MEDLINE. In the present pa-
per, we target al articles indexed in MEDLINE at
the end of 2004 (14,785,094 articles). The follow-
ing sections explain in detail off-/on-line process-
ing for the text retrieval system for MEDLINE.
3.1 Off-line processing: HPSG parsing and
term recognition
We first parsed all sentences using an HPSG parser
(Miyao and Tsujii, 2005) to obtain their predi-
cate argument structures. Because our target is
biomedical texts, we re-trained a parser (Hara et
al., 2005) with the GENIA treebank (Tateisi et
al., 2005), and also applied a bidirectional part-of-
speech tagger (Tsuruoka and Tsujii, 2005) trained
with the GENIA treebank as a preprocessor.
Because parsing speed is still unrealistic for
parsing the entire MEDLINE on a single ma-
chine, we used two geographically separated com-
puter clusters having 170 nodes (340 Xeon CPUs).
These clusters are separately administered and not
dedicated for use in the present study. In order to
effectively use such an environment, GXP (Taura,
2004) was used to connect these clusters and dis-
tribute the load among them. Our processes were
given the lowest priority so that our task would not
disturb other users. We finished parsing the entire
MEDLINE in nine days (Ninomiya et al, 2006).
# entries (genes) 517,773
# entries (gene products) 171,711
# entries (diseases) 148,602
# expanded entries 4,467,855
Table 3: Sizes of ontologies used for term recog-
nition
Event type Expressions
influence effect, affect, role, response, . . .
regulation mediate, regulate, regulation, . . .
activation induce, activate, activation, . . .
Table 4: Event expression ontology
Next, we annotated technical terms, such as
genes and diseases, to create mappings to onto-
logical identifiers. A dictionary-based term recog-
nition algorithm (Tsuruoka and Tsujii, 2004) was
applied for this task. First, an expanded term
list was created by generating name variations of
terms in GENA and the UMLS meta-thesaurus1.
Table 3 shows the size of the original database and
the number of entries expanded by name varia-
tions. Terms in MEDLINE were then identified
by the longest matching of entries in this expanded
list with words/phrases in MEDLINE.
The necessity of ontologies is not limited to
nominal expressions. Various verbs are used for
expressing events. For example, activation events
of proteins can be expressed by ?activate,? ?en-
hance,? and other event expressions. Although the
numbers of verbs and their event types are much
smaller than those of technical terms, verbal ex-
pressions are important for the description of rela-
tional concepts. Since ontologies of event expres-
sions in this domain have not yet been constructed,
we developed an ontology from scratch. We inves-
tigated 500 abstracts extracted from MEDLINE,
and classified 167 frequent expressions, including
verbs and their nominalized forms, into 18 event
types. Table 4 shows a part of this ontology. These
expressions in MEDLINE were automatically an-
notated with event types.
As a result, we obtained semantically annotated
MEDLINE. Table 5 shows the size of the orig-
inal MEDLINE and semantic annotations. Fig-
ure 6 shows semantic annotations for the sentence
in Figure 1, where ?-? indicates nodes of XML,2
1We collected disease names by specifying a query with
the semantic type as ?Disease or Syndrome.?
2Although this example is shown in XML, this textbase
contains tags with cross boundaries because tags for predicate
argument structures and technical terms may overlap.
1020
# papers 14,785,094
# abstracts 7,291,857
# sentences 70,935,630
# words 1,462,626,934
# successfully parsed sentences 69,243,788
# predicate argument relations 1,510,233,701
# phrase tags 3,094,105,383
# terms (genes) 84,998,621
# terms (gene products) 27,471,488
# terms (diseases) 19,150,984
# terms (event expressions) 51,810,047
Size of the original MEDLINE 9.3 GByte
Size of the semantic annotations 292 GByte
Size of the index file for region algebra 954 GByte
Table 5: Sizes of the original and semantically an-
notated MEDLINE textbases
- <sentence sentence_id="e6e525">
- <phrase id="0" cat="S" head="15" lex_head="18">
- <phrase id="1" cat="NP" head="4" lex_head="14">
- <phrase id="2" cat="DT" head="3" lex_head="3">
- <word id="3" pos="DT" cat="DT" base="a" arg1="4">
- A
- <phrase id="4" cat="NP" head="7" lex_head="14">
- <phrase id="5" cat="AJ" head="6" lex_head="6">
- <word id="6" pos="JJ" cat="AJ" base="normal" arg1="7">
- normal
- <phrase id="7" cat="NP" head="10" lex_head="14">
- <phrase id="8" cat="NP" head="9" lex_head="9">
- <word id="9" pos="NN" cat="NP" base="serum" mod="10">
- serum
- <phrase id="10" cat="NP" head="13" lex_head="14">
- <phrase id="11" cat="NP" head="12" lex_head="12">
- <entity_name id="entity-1" type="gene"
gene_id="GHS003134" gene_symbol="CRP"
gene_name="C-reactive protein, pentraxin-related"
species="Homo sapiens"
db_site="EntrezGene:1401|GDB:119071|GenAtlas:CRP">
- <word id="12" pos="NN" cat="NP" base="crp" mod="13">
- CRP
- <phrase id="13" cat="NP" head="14" lex_head="14">
- <word id="14" pos="NN" cat="NP" base="measurement">
- measurement
- <phrase id="15" cat="VP" head="16" lex_head="18">
- <phrase id="16" cat="VP" head="17" lex_head="18">
- <phrase id="17" cat="VP" head="18" lex_head="18">
- <word id="18" pos="VBZ" cat="VP" base="do"
arg1="1" arg2="21">
- does
- <phrase id="19" cat="AV" head="20" lex_head="20">
- <word id="20" pos="RB" cat="AV" base="not" arg1="21">
- not
- <phrase id="21" cat="VP" head="22" lex_head="23">
- <phrase id="22" cat="VP" head="23" lex_head="23">
- <word id="23" pos="VB" cat="VP" base="exclude"
arg1="1" arg2="24">
- exclude
...
Figure 6: A semantically annotated sentence
although the latter half of the sentence is omitted
because of space limitations. Sentences are an-
notated with four tags,3 ?phrase,? ?word,? ?sen-
tence,? and ?entity name,? and their attributes as
given in Table 6. Predicate argument structures are
annotated as attributes, ?mod? and ?argX ,? which
point to the IDs of the argument phrases. For ex-
ample, in Figure 6, the <word> tag for ?exclude?
has the attributes arg1="1" and arg2="24",
which denote the IDs of the subject and object
phrases, respectively.
3Additional tags exist for representing document struc-
tures such as ?title? (details omitted).
Tag Attributes
phrase id, cat, head, lex head
word id, cat, pos, base, mod, argX , rel type
sentence sentence id
entity name id, type, gene id/disease id, gene symbol,
gene name, species, db site
Attribute Description
id unique identifier
cat syntactic category
head head daughter?s ID
lex head lexical head?s ID
pos part-of-speech
base base form of the word
mod ID of modifying phrase
argX ID of the X-th argument of the word
rel type event type
sentence id sentence?s ID
type whether gene, gene prod, or disease
gene id ID in GENA
disease id ID in the UMLS meta-thesaurus
gene symbol short form of the gene
gene name nomenclature of the gene
species species that have this gene
db site links to external databases
Table 6: Tags (upper) and attributes (lower) for
semantic annotations
3.2 On-line processing
The off-line processing described above results in
much simpler on-line processing. User input is
converted into queries of the extended region al-
gebra, and the converted queries are entered into a
search engine for the extended region algebra. The
implementation of a search engine is described in
detail in Masuda et al (2006).
Basically, given subject x, object y, and verb v,
the system creates the following query:
[sentence] >>
([word arg1="$subject" arg2="$object"
base="v"] &
([phrase id="$subject"] > x) &
([phrase id="$object"] > y))
Ontological identifiers are substituted for x, y,
and v, if possible. Nominal keywords, i.e., x and
y, are replaced by [entity_name gene_id="n"]
or [entity_name disease_id="n"], where n is
the ontological identifier of x or y. For verbal key-
words, base="v" is replaced by rel_type="r",
where r is the event type of v.
4 Evaluation
Our system is evaluated with respect to speed and
accuracy. Speed is indispensable for real-time in-
teractive text retrieval systems, and accuracy is key
for the motivation of semantic retrieval. That is,
our motivation for employing semantic retrieval
1021
Query No. User input
1 something inhibit ERK2
2 something trigger diabetes
3 adiponectin increase something
4 TNF activate IL6
5 dystrophin cause disease
6 macrophage induce something
7 something suppress MAP phosphorylation
8 something enhance p53 (negative)
Table 7: Queries for experiments
[sentence] >>
([word rel_type="activation"] &
[entity_name type="gene" gene_id="GHS019685"] &
[entity_name type="gene" gene_id="GHS009426"])
[sentence] >>
([word arg1="$subject" arg2="$object"
rel_type="activation"] &
([phrase id="$subject"] >
[entity_name type="gene" gene_id="GHS019685"]) &
([phrase cat="np" id="$object"] >
[entity_name type="gene" gene_id="GHS009426"]))
Figure 7: Queries of the extended region algebra
for Query 4-3 (upper: keyword search, lower: se-
mantic search)
was to provide a device for the accurate identifica-
tion of relational concepts. In particular, high pre-
cision is desired in text retrieval from huge texts
because users want to extract relevant information,
rather than collect exhaustive information.
We have two parameters to vary: whether to
use predicate argument structures and whether to
use ontological identifiers. The effect of using
predicate argument structures is evaluated by com-
paring ?keyword search? with ?semantic search.?
The former is a traditional style of IR, in which
sentences are retrieved by matching words in a
query with words in sentences. The latter is a
new feature of the present system, in which sen-
tences are retrieved by matching predicate argu-
ment relations in a query with those in a semanti-
cally annotated textbase. The effect of using onto-
logical identifiers is assessed by changing queries
of the extended region algebra. When we use the
term ontology, nominal keywords in queries are
replaced with ontological identifiers in GENA and
the UMLS meta-thesaurus. When we use the event
expression ontology, verbal keywords in queries
are replaced with event types.
Table 7 is a list of queries used in the follow-
ing experiments. Words in italics indicate a class
of words: ?something? indicates that any word
can appear, and disease indicates that any dis-
ease expression can appear. These queries were
selected by a biologist, and express typical re-
lational concepts that a biologist may wish to
find. Queries 1, 3, and 4 represent relations of
genes/proteins, where ERK2, adiponectin, TNF,
and IL6 are genes/proteins. Queries 2 and 5 de-
scribe relations concerning diseases, and Query 6
is a query that is not relevant to genes or diseases.
Query 7 expresses a complex relation concern-
ing a specific phenomena, i.e., phosphorylation,
of MAP. Query 8 describes a relation concerning
a gene, i.e., p53, while ?(negative)? indicates that
the target of retrieval is negative mentions. This is
expressed by ?not? modifying a predicate.
For example, Query 4 attempts to retrieve sen-
tences that mention the protein-protein interaction
?TNF activates IL6.? This is converted into queries
of the extended region algebra given in Figure 7.
The upper query is for keyword search and only
specifies the appearances of the three words. Note
that the keywords are translated into the ontolog-
ical identifiers, ?activation,? ?GHS019685,? and
?GHS009426.? The lower query is for semantic
search. The variables in ?arg1? and ?arg2? indi-
cate that ?GHS019685? and ?GHS009426? are the
subject and object, respectively, of ?activation?.
Table 8 summarizes the results of the experi-
ments. The postfixes of query numbers denote
whether ontological identifiers are used. X-1 used
no ontologies, and X-2 used only the term ontol-
ogy. X-3 used both the term and event expression
ontologies4. Comparison of X-1 and X-2 clarifies
the effect of using the term ontology. Comparison
of X-2 and X-3 shows the effect of the event ex-
pression ontology. The results for X-3 indicate
the maximum performance of the current system.
This table shows that the time required for the se-
mantic search for the first answer, shown as ?time
(first)? in seconds, was reasonably short. Thus,
the present framework is acceptable for real-time
text retrieval. The numbers of answers increased
when we used the ontologies, and this result indi-
cates the efficacy of both ontologies for obtaining
relational concepts written in various expressions.
Accuracy was measured by judgment by a bi-
ologist. At most 100 sentences were retrieved for
each query, and the results of keyword search and
semantic search were merged and shuffled. A bi-
ologist judged the shuffled sentences (1,839 sen-
tences in total) without knowing whether the sen-
4Query 5-1 is not tested because ?disease? requires
the term ontology, and Query 6-2 is not tested because
?macrophage? is not assigned an ontological identifier.
1022
Query Keyword search Semantic search
No. # ans. time (first/all) precision n-precision # ans. time (first/all) precision relative recall
1-1 252 0.00/ 1.5 74/100 (74%) 74/100 (74%) 143 0.01/ 2.5 96/100 (96%) 51/74 (69%)
1-2 348 0.00/ 1.9 61/100 (61%) 61/100 (61%) 174 0.01/ 3.1 89/100 (89%) 42/61 (69%)
1-3 884 0.00/ 3.2 50/100 (50%) 50/100 (50%) 292 0.01/ 5.3 91/100 (91%) 21/50 (42%)
2-1 125 0.00/ 1.8 45/100 (45%) 9/ 27 (33%) 27 0.02/ 2.9 23/ 27 (85%) 17/45 (38%)
2-2 113 0.00/ 2.9 40/100 (40%) 10/ 26 (38%) 26 0.06/ 4.0 22/ 26 (85%) 19/40 (48%)
2-3 6529 0.00/ 12.1 42/100 (42%) 42/100 (42%) 662 0.01/1527.4 76/100 (76%) 8/42 (19%)
3-1 287 0.00/ 1.5 20/100 (20%) 4/ 30 (13%) 30 0.05/ 2.4 23/ 30 (80%) 6/20 (30%)
3-2 309 0.01/ 2.1 21/100 (21%) 4/ 32 (13%) 32 0.10/ 3.5 26/ 32 (81%) 6/21 (29%)
3-3 338 0.01/ 2.2 24/100 (24%) 8/ 39 (21%) 39 0.05/ 3.6 32/ 39 (82%) 8/24 (33%)
4-1 4 0.26/ 1.5 0/ 4 (0%) 0/ 0 (?) 0 2.44/ 2.4 0/ 0 (?) 0/ 0 (?)
4-2 195 0.01/ 2.5 9/100 (9%) 1/ 6 (17%) 6 0.09/ 4.1 5/ 6 (83%) 2/ 9 (22%)
4-3 2063 0.00/ 7.5 5/100 (5%) 5/ 94 (5%) 94 0.02/ 10.5 89/ 94 (95%) 2/ 5 (40%)
5-2 287 0.08/ 6.3 73/100 (73%) 73/100 (73%) 116 0.05/ 14.7 97/100 (97%) 37/73 (51%)
5-3 602 0.01/ 15.9 50/100 (50%) 50/100 (50%) 122 0.05/ 14.2 96/100 (96%) 23/50 (46%)
6-1 10698 0.00/ 42.8 14/100 (14%) 14/100 (14%) 1559 0.01/3014.5 65/100 (65%) 10/14 (71%)
6-3 42106 0.00/3379.5 11/100 (11%) 11/100 (11%) 2776 0.01/5100.1 61/100 (61%) 5/11 (45%)
7 87 0.04/ 2.7 34/ 87 (39%) 7/ 15 (47%) 15 0.05/ 4.2 10/ 15 (67%) 10/34 (29%)
8 1812 0.01/ 7.6 19/100 (19%) 17/ 84 (20%) 84 0.20/ 29.2 73/ 84 (87%) 7/19 (37%)
Table 8: Number of retrieved sentences, retrieval time, and accuracy
tence was retrieved by keyword search or semantic
search. Without considering which words actually
matched the query, a sentence is judged to be cor-
rect when any part of the sentence expresses all of
the relations described by the query. The modality
of sentences was not distinguished, except in the
case of Query 8. These evaluation criteria may be
disadvantageous for the semantic search because
its ability to exactly recognize the participants of
relational concepts is not evaluated. Table 8 shows
the precision attained by keyword/semantic search
and n-precision, which denotes the precision of
the keyword search, in which the same number,
n, of outputs is taken as the semantic search. The
table also gives the relative recall of the semantic
search, which represents the ratio of sentences that
are correctly output by the semantic search among
those correctly output by the keyword search. This
does not necessarily represent the true recall be-
cause sentences not output by keyword search are
excluded. However, this is sufficient for the com-
parison of keyword search and semantic search.
The results show that the semantic search exhib-
ited impressive improvements in precision. The
precision was over 80% for most queries and was
nearly 100% for Queries 4 and 5. This indicates
that predicate argument structures are effective for
representing relational concepts precisely, espe-
cially for relations in which two entities are in-
volved. Relative recall was approximately 30?
50%, except for Query 2. In the following, we
will investigate the reasons for the residual errors.
Table 9 shows the classifications of the errors of
Disregarding of noun phrase structures 45
Term recognition errors 33
Parsing errors 11
Other reasons 8
Incorrect human judgment 7
Nominal expressions 41
Phrasal verb expressions 26
Inference required 24
Coreference resolution required 19
Parsing errors 16
Other reasons 15
Incorrect human judgment 10
Table 9: Error analysis (upper: 104 false positives,
lower: 151 false negatives)
semantic retrieval. The major reason for false pos-
itives was that our queries ignore internal struc-
tures of noun phrases. The system therefore re-
trieved noun phrases that do not directly mention
target entities. For example, ?the increased mor-
tality in patients with diabetes was caused by . . . ?
does not indicate the trigger of diabetes. Another
reason was term recognition errors. For exam-
ple, the system falsely retrieved sentences con-
taining ?p40,? which is sometimes, but not nec-
essarily used as a synonym for ?ERK2.? Ma-
chine learning-based term disambiguation will al-
leviate these errors. False negatives were caused
mainly by nominal expressions such as ?the in-
hibition of ERK2.? This is because the present
system does not convert user input into queries
on nominal expressions. Another major reason,
phrasal verb expressions such as ?lead to,? is also
a shortage of our current strategy of query cre-
ation. Because semantic annotations already in-
1023
clude linguistic structures of these expressions, the
present system can be improved further by creat-
ing queries on such expressions.
5 Conclusion
We demonstrated a text retrieval system for MED-
LINE that exploits pre-computed semantic anno-
tations5. Experimental results revealed that the
proposed system is sufficiently efficient for real-
time text retrieval and that the precision of re-
trieval was remarkably high. Analysis of resid-
ual errors showed that the handling of noun phrase
structures and the improvement of term recogni-
tion will increase retrieval accuracy. Although
the present paper focused on MEDLINE, the NLP
tools used in this system are domain/task indepen-
dent. This framework will thus be applicable to
other domains such as patent documents.
The present framework does not conflict with
conventional IR/IE techniques, and integration
with these techniques is expected to improve the
accuracy and usability of the proposed system. For
example, query expansion and relevancy feedback
can be integrated in a straightforward way in order
to improve accuracy. Document ranking is useful
for the readability of retrieved results. IE systems
can be applied off-line, in the manner of the deep
parser in our system, for annotating sentences with
target information of IE. Such annotations will en-
able us to retrieve higher-level concepts, such as
relationships among relational concepts.
Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Systems
Genomics? (MEXT, Japan), Genome Network
Project (NIG, Japan), and Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
S. Boag, D. Chamberlin, M. F. Ferna?ndez, D. Florescu,
J. Robie, and J. Sime?on. 2005. XQuery 1.0: An
XML query language.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proc. ACL 2005.
5A web-based demo of our system is available on-line at:
http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
J. Clark and S. DeRose. 1999. XML Path Language
(XPath) version 1.0.
C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski.
1995. An algebra for structured text search and a
framework for its implementation. The Computer
Journal, 38(1):43?56.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
T. Hara, Y. Miyao, and J. Tsujii. 2005. Adapting
a probabilistic disambiguation model of an HPSG
parser to a new domain. In Proc. IJCNLP 2005.
IBM, 2005. Unstructed Information Management Ar-
chitecture (UIMA) SDK User?s Guide and Refer-
ence.
A. Koike and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. In Proc.
Biolink 2004, pages 9?16.
D. A. Lindberg, B. L. Humphreys, and A. T. Mc-
Cray. 1993. The Unified Medical Language Sys-
tem. Methods in Inf. Med., 32(4):281?291.
K. Masuda, T. Ninomiya, Y. Miyao, T. Ohta, and
J. Tsujii. 2006. Nested region algebra extended with
variables. In Preparation.
Y. Miyao and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proc. 43rd ACL, pages 83?90.
National Library of Medicine. 2005. Fact Sheet MED-
LINE. Available at http://www.nlm.nih.
gov/pubs/factsheets/medline.html.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, K. Taura, and
J. Tsujii. 2006. Fast and scalable HPSG parsing.
Traitement automatique des langues (TAL), 46(2).
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax annotation for the GENIA corpus. In Proc.
IJCNLP 2005, Companion volume, pages 222?227.
K. Taura. 2004. GXP : An interactive shell for the grid
environment. In Proc. IWIA2004, pages 59?67.
TEI Consortium, 2004. Text Encoding Initiative.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP 2005, pages
467?474.
1024
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
BioNLP 2007: Biological, translational, and clinical language processing, pages 209?216,
Prague, June 2007. c?2007 Association for Computational Linguistics
Reranking for Biomedical Named-Entity Recognition
Kazuhiro Yoshida? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{kyoshida, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper investigates improvement of au-
tomatic biomedical named-entity recogni-
tion by applying a reranking method to the
COLING 2004 JNLPBA shared task of bio-
entity recognition. Our system has a com-
mon reranking architecture that consists of a
pipeline of two statistical classifiers which
are based on log-linear models. The ar-
chitecture enables the reranker to take ad-
vantage of features which are globally de-
pendent on the label sequences, and fea-
tures from the labels of other sentences than
the target sentence. The experimental re-
sults show that our system achieves the la-
beling accuracies that are comparable to the
best performance reported for the same task,
thanks to the 1.55 points of F-score improve-
ment by the reranker.
1 Introduction
Difficulty and potential application of biomedical
named-entity recognition has attracted many re-
searchers of both natural language processing and
bioinformatics. The difficulty of the task largely
stems from a wide variety of named entity expres-
sions used in the domain. It is common for practi-
cal protein or gene databases to contain hundreds of
thousands of items. Such a large variety of vocab-
ulary naturally leads to long names with productive
use of general words, making the task difficult to be
solved by systems with naive Markov assumption of
label sequences, because such systems must perform
their prediction without seeing the entire string of
the entities.
Importance of the treatment of long names might
be implicitly indicated in the performance com-
parison of the participants of JNLPBA shared
task (Kim et al, 2004), where the best perform-
ing system (Zhou and Su, 2004) attains their scores
by extensive post-processing, which enabled the
system to make use of global information of the
entity labels. After the shared task, many re-
searchers tackled the task by using conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), which
seemed to promise improvement over locally opti-
mized models like maximum entropy Markov mod-
els (MEMMs) (McCallum et al, 2000). However,
many of the CRF systems developed after the shared
task failed to reach the best performance achieved
by Zhou et al One of the reasons may be the defi-
ciency of the dynamic programming-based systems,
that the global information of sequences cannot be
incorporated as features of the models. Another rea-
son may be that the computational complexity of
the models prevented the developers to invent ef-
fective features for the task. We had to wait until
Tsai et al (2006), who combine pattern-based post-
processing with CRFs, for CRF-based systems to
achieve the same level of performance as Zhou et al
As such, a key to further improvement of the perfor-
mance of bio-entity recognition has been to employ
global features, which are effective to capture the
features of long names appearing in the bio domain.
In this paper, we use reranking architecture,
which was successfully applied to the task of nat-
ural language parsing (Collins, 2000; Charniak and
209
Johnson, 2005), to address the problem. Reranking
enables us to incorporate truly global features to the
model of named entity tagging, and we aim to real-
ize the state-of-the-art performance without depend-
ing on rule-based post-processes.
Use of global features in named-entity recogni-
tion systems is widely studied for sequence labeling
including general named-entity tasks like CoNLL
2003 shared task. Such systems may be classified
into two kinds, one of them uses a single classifier
which is optimized incorporating non-local features,
and the other consists of pipeline of more than one
classifiers. The former includes Relational Markov
Networks by Bunescu et al (2004) and skip-edge
CRFs by Sutton et al (2004). A major drawback
of this kind of systems may be heavy computational
cost of inference both for training and running the
systems, because non-local dependency forces such
models to use expensive approximate inference in-
stead of dynamic-programming-based exact infer-
ence. The latter, pipelined systems include a re-
cent study by Krishnan et al (2006), as well as
our reranking system. Their method is a two stage
model of CRFs, where the second CRF uses the
global information of the output of the first CRF.
Though their method is effective in capturing var-
ious non-local dependencies of named entities like
consistency of labels, we may be allowed to claim
that reranking is likely to be more effective in bio-
entity tagging, where the treatment of long entity
names is also a problem.
This paper is organized as follows. First, we
briefly overview the JNLPBA shared task of bio-
entity recognition and its related work. Then we ex-
plain the components of our system, one of which is
an MEMM n-best tagger, and the other is a reranker
based on log-linear models. Then we show the ex-
periments to tune the performance of the system us-
ing the development set. Finally, we compare our
results with the existing systems, and conclude the
paper with the discussion for further improvement
of the system.
2 JNLPBA shared task and related work
This section overviews the task of biomedical named
entity recognition as presented in JNLPBA shared
task held at COLING 2004, and the systems that
were successfully applied to the task. The train-
ing data provided by the shared task consisted of
2000 abstracts of biomedical articles taken from the
GENIA corpus version 3 (Ohta et al, 2002), which
consists of the MEDLINE abstracts with publication
years from 1990 to 1999. The articles are annotated
with named-entity BIO tags as an example shown in
Table 1. As usual, ?B? and ?I? tags are for beginning
and internal words of named entities, and ?O? tags
are for general English words that are not named en-
tities. ?B? and ?I? tags are split into 5 sub-labels,
each of which are used to represent proteins, genes,
cell lines, DNAs, cell types, and RNAs. The test
set of the shared task consists of 404 MEDLINE ab-
stracts whose publication years range from 1978 to
2001. The difference of publication years between
the training and test sets reflects the organizer?s in-
tention to see the entity recognizers? portability with
regard to the differences of the articles? publication
years.
Kim et al (Kim et al, 2004) compare the 8 sys-
tems participated in the shared task. The systems
use various classification models including CRFs,
hidden Markov models (HMMs), support vector ma-
chines (SVMs), and MEMMs, with various features
and external resources. Though it is impossible to
observe clear correlation between the performance
and classification models or resources used, an im-
portant characteristic of the best system by Zhou et
al. (2004) seems to be extensive use of rule-based
post processing they apply to the output of their clas-
sifier.
After the shared task, several researchers tack-
led the problem using the CRFs and their ex-
tensions. Okanohara et al (2006) applied semi-
CRFs (Sarawagi and Cohen, 2004), which can treat
multiple words as corresponding to a single state.
Friedrich et al (2006) used CRFs with features from
the external gazetteer. Current state-of-the-art for
the shared-task is achieved by Tsai et al (2006),
whose improvement depends on careful design of
features including the normalization of numeric ex-
pressions, and use of post-processing by automati-
cally extracted patterns.
210
IL-2 gene expression requires reactive oxygen production by 5-lipoxygenase .
B-DNA I-DNA O O O O O O B-protein O
Figure 1: Example sentence from the training data.
State name Possible next state
BOS B-* or O
B-protein I-protein, B-* or O
B-cell type I-cell type, B-* or O
B-DNA I-DNA, B-* or O
B-cell line I-cell line, B-* or O
B-RNA I-RNA, B-* or O
I-protein I-protein, B-* or O
I-cell type I-cell type, B-* or O
I-DNA I-DNA, B-* or O
I-cell line I-cell line, B-* or O
I-RNA I-RNA, B-* or O
O B-* or O
Table 1: State transition of MEMM.
3 N-best MEMM tagger
As our n-best tagger, we use a first order MEMM
model (McCallum et al, 2000). Though CRFs (Laf-
ferty et al, 2001) can be regarded as improved ver-
sion of MEMMs, we have chosen MEMMs because
MEMMs are usually much faster to train compared
to CRFs, which enables extensive feature selection.
Training a CRF tagger with features selected us-
ing an MEMM may result in yet another perfor-
mance boost, but in this paper we concentrate on the
MEMM as our n-best tagger, and consider CRFs as
one of our future extensions.
Table 1 shows the state transition table of our
MEMM model. Though existing studies suggest
that changing the tag set of the original corpus, such
as splitting of O tags, can contribute to the perfor-
mances of named entity recognizers (Peshkin and
Pfefer, 2003), our system uses the original tagset
of the training data, except that the ?BOS? label is
added to represent the state before the beginning of
sentences.
Probability of state transition to the i-th label of a
sentence is calculated by the following formula:
P (li|li?1, S) =
exp(?j ?jfj(li, li?1, S))
?
l exp(
?
j ?jfj(l, li?1, S))
. (1)
Features used Forward tagging Backward tagging
unigrams, bi-
grams and pre-
vious labels
(62.43/71.77/66.78) (66.02/74.73/70.10)
unigrams and
bigrams (61.64/71.73/66.30) (65.38/74.87/69.80)
unigrams and
previous labels (62.17/71.67/66.58) (65.59/74.77/69.88)
unigrams (61.31/71.81/66.15) (65.61/75.25/70.10)
Table 2: (Recall/Precision/F-score) of forward and
backward tagging.
where li is the next BIO tag, li?1 is the previous
BIO tag, S is the target sentence, and fj and lj
are feature functions and parameters of a log-linear
model (Berger et al, 1996). As a first order MEMM,
the probability of a label li is dependent on the pre-
vious label li?1, and when we calculate the normal-
ization constant in the right hand side (i.e. the de-
nominator of the fraction), we limit the range of l to
the possible successors of the previous label. This
probability is multiplied to obtain the probability of
a label sequence for a sentence:
P (l1...n|S) =
?
i
P (li|li?1). (2)
The probability in Eq. 1. is estimated as a single
log-linear model, regardless to the types of the target
labels.
N-best tag sequences of input sentences are ob-
tained by well-known combination of the Viterbi al-
gorithm and A* algorithm. We implemented two
methods for thresholding the best sequences: N -
best takes the sequences whose ranks are higher than
N , and ?-best takes the sequences that have proba-
bility higher than that of the best sequences with a
factor ?, where ? is a real value between 0 and 1. The
?-best method is used in combination with N -best to
limit the maximum number of selected sequences.
3.1 Backward tagging
There remains one significant choice when we de-
velop an MEMM tagger, that is, the direction of tag-
ging. The results of the preliminary experiment with
211
forward and backward MEMMs with word unigram
and bigram features are shown in Table 2. (The eval-
uation is done using the same training and develop-
ment set as used in Section 5.) As can be seen, the
backward tagging outperformed forward tagging by
a margin larger than 3 points, in all the cases.
One of the reasons of these striking differences
may be long names which appear in biomedical
texts. In order to recognize long entity names, for-
ward tagging is preferable if we have strong clues of
entities which appear around their left boundaries,
and backward tagging is preferable if clues appear
at right boundaries. A common example of this ef-
fect is a gene expression like ?XXX YYY gene.? The
right boundary of this expression is easy to detect
because of the word ?gene.? For a backward tagger,
the remaining decision is only ?where to stop? the
entity. But a forward tagger must decide not only
?where to start,? but also ?whether to start? the en-
tity, before the tagger encounter the word ?gene.? In
biomedical named-entity tagging, right boundaries
are usually easier to detect, and it may be the reason
of the superiority of the backward tagging.
We could have partially alleviated this effect by
employing head-word triggers as done in Zhou et
al. (2004), but we decided to use backward tag-
ging because the results of a number of preliminary
experiments, including the ones shown in Table 2
above, seemed to be showing that the backward tag-
ging is preferable in this task setting.
3.2 Feature set
In our system, features of log-linear models are gen-
erated by concatenating (or combining) the ?atomic?
features, which belong to their corresponding atomic
feature classes. Feature selection is done by de-
ciding whether to include combination of feature
classes into the model. We ensure that features in the
same atomic feature class do not co-occur, so that a
single feature-class combination generates only one
feature for each event. The following is a list of
atomic feature classes implemented in our system.
Label features The target and previous labels. We
also include the coarse-grained label distinction to
distinguish five ?I? labels of each entity classes from
the other labels, expecting smoothing effect.
Word-based features Surface strings, base forms,
parts-of-speech (POSs), word shapes1, suffixes and
prefixes of words in input sentence. These features
are extracted from five words around the word to be
tagged, and also from the words around NP-chunk
boundaries as explained bellow.
Chunk-based features Features dependent on the
output of shallow parser. Word-based features of
the beginning and end of noun phrases, and the dis-
tances of the target word from the beginning and end
of noun phrases are used.
4 Reranker
Our reranker is based on a log-linear classifier.
Given n-best tag sequences Li(1 ? i ? n), a log-
linear model is used to estimate the probability
P (Li|S) =
exp(?j ?jfj(Li, S))
?
k exp(
?
j ?jfj(Lk, S))
. (3)
From the n-best sequences, reranker selects a se-
quence which maximize this probability.
The features used by the reranker are explained in
the following sections. Though most of the features
are binary-valued (i.e. the value of fj in Eq. 3. is
exclusively 1 or 0), the logarithm of the probability
of the sequence output by the n-best tagger is also
used as a real-valued feature, to ensure the reranker?s
improvement over the n-best tagger.
4.1 Basic features
Basic features of the reranker are straightforward ex-
tension of the features used in the MEMM tagger.
The difference is that we do not have to care the lo-
cality of the features with regard to the labels.
Characteristics of words that are listed as word-
based features in the previous section is also used
for the reranker. Such features are chiefly extracted
from around the left and right boundaries of entities.
In our experiments, we used five words around the
leftmost and rightmost words of the entities. We also
use the entire string, affixes, word shape, concatena-
tion of POSs, and length of entities. Some of our
1The shape of a word is defined as a sequence of character
types contained in the word. Character types include uppercase
letters, lowercase letters, numerics, space characters, and the
other symbols.
212
features depend on two adjacent entities. Such fea-
tures include the word-based features of the words
between the entities, and the verbs between the en-
tities. Most of the features are used in combination
with entity types.
4.2 N-best distribution features
N-best tags of sentences other than the target sen-
tence is available to the rerankers. This information
is sometimes useful for recognizing the names in
the target sentence. For example, proteins are often
written as ?XXX protein? where XXX is a protein
name, especially when they are first introduced in an
article, and thereafter referred to simply as ?XXX.?
In such cases, the first appearance is easily identified
as proteins only by local features, but the subsequent
ones might not, and the information of the first ap-
pearance can be effectively used to identify the other
appearances.
Our system uses the distribution of the tags of
the 20 neighboring sentences of the target sentence
to help the tagging of the target sentence. Tag
distributions are obtained by marginalizing the n-
best tag sequences. Example of an effective feature
is a binary-valued feature which becomes 1 when
the candidate entity names in the target sentence is
contained in the marginal distribution of the neigh-
boring sentences with a probability which is above
some threshold.
We also use the information of overlapping
named-entity candidates which appear in the target
sentence. When there is an overlap between the en-
tities in the target sequence and any of the named-
entity candidates in the marginal distribution of the
target sentence, the corresponding features are used
to indicate the existence of the overlapping entity
and its entity type.
5 Experiments
We evaluated the performance of the system on the
data set provided by the COLING 2004 JNLPBA
shared-task. which consists of 2000 abstracts from
the MEDLINE articles. GENIA tagger 2, a biomed-
ical text processing tool which automatically anno-
2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/. The
tagger is trained on the GENIA corpus, so it is likely to show
very good performance on both training and development sets,
but not on the test set.
Features used (Recall/Precision/F-score)
full set (73.90/77.58/75.69)
w/o shallow parser (72.63/76.35/74.44)
w/o previous labels (72.06/75.38/73.68)
Table 3: Performance of MEMM tagger.
tates POS tags, shallow parses and named-entity tags
is used to preprocess the corpus, and POS and shal-
low parse information is used in our experiments.
We divided the data into 20 contiguous and
equally-sized sections, and used the first 18 sec-
tions for training, and the last 2 sections for testing
while development (henceforth the training and de-
velopment sets, respectively). The training data of
the reranker is created by the n-best tagger, and ev-
ery set of 17 sections from the training set is used
to train the n-best tagger for the remaining section
(The same technique is used by previous studies
to avoid the n-best tagger?s ?unrealistically good?
performance on the training set (Collins, 2000)).
Among the n-best sequences output by the MEMM
tagger, the sequence with the highest F-score is used
as the ?correct? sequence for training the reranker.
The two log-linear models for the MEMM tagger
and reranker are estimated using a limited-memory
BFGS algorithm implemented in an open-source
software Amis3. In both models, Gaussian prior dis-
tributions are used to avoid overfitting (Chen and
Rosenfeld, 1999), and the standard deviations of the
Gaussian distributions are optimized to maximize
the performance on the development set. We also
used a thresholding technique which discards fea-
tures with low frequency. This is also optimized us-
ing the development set, and the best threshold was
4 for the MEMM tagger, and 50 for the reranker 4.
For both of the MEMM tagger and reranker, com-
binations of feature classes are manually selected to
improve the accuracies on the development set. Our
final models include 49 and 148 feature class combi-
nations for the MEMM tagger and reranker, respec-
tively.
Table 3 shows the performance of the MEMM
tagger on the development set. As reported in many
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/.
4We treated feature occurrences both in positive and nega-
tive examples as one occurrence.
213
Features used (Recall/Precision/F-score)
oracle (94.62/96.07/95.34)
full set (75.46/78.85/77.12)
w/o features that
depend on two
entities
(74.67/77.99/76.29)
w/o n-best distribu-
tion features
(74.99/78.38/76.65)
baseline (73.90/77.58/75.69)
Table 4: Performance of the reranker.
of the previous studies (Kim et al, 2004; Okanohara
et al, 2006; Tzong-Han Tsai et al, 2006), features of
shallow parsers had a large contribution to the per-
formance. The information of the previous labels
was also quite effective, which indicates that label
unigram models (i.e. 0th order Markov models, so
to speak) would have been insufficient for good per-
formance.
Then we developed the reranker, using the results
of 50-best taggers as training data. Table 4 shows the
performance of the reranker pipelined with the 50-
best MEMM tagger, where the ?oracle? row shows
the upper bound of reranker performance. Here, we
can observe that the reranker successfully improved
the performance by 1.43 points from the baseline
(i.e. the one-best of the MEMM tagger). It is also
shown that the global features that depend on two
adjacent entities, and the n-best distribution features
from the outside of the target sentences, are both
contributing to this performance improvement.
We also conducted experimental comparison of
two thresholding methods which are described in
Section 3. Since we can train and test the reranker
with MEMM taggers that use different thresholding
methods, we could make a table of the performance
of the reranker, changing the MEMM tagger used
for both training and evaluation5.
Tables 5 and 6 show the F-scores obtained by
various MEMM taggers, where the ?oracle? column
again shows the performance upper bound. (All
of the ?-best methods are combined with 200-best
thresholding.) Though we can roughly state that the
reranker can work better with n-best taggers which
5These results might not be a fair comparison, because the
feature selection and hyper-parameter tuning are done using a
reranker which is trained and tested with a 50-best tagger.
are more ambiguous than those used for their train-
ing, the differences are so slight to see clear ten-
dencies (For example, the columns for the reranker
trained using the 10-best MEMM tagger seems to be
a counter example against the statement).
We may also be able to say that the ?-best meth-
ods are generally performing slightly better, and it
could be explained by the fact that we have bet-
ter oracle performance with less ambiguity in ?-best
methods.
However, the scores in the column corresponding
to the 50-best training seems to be as high as any of
the scores of the ?-best methods, and the best score
is also achieved in that column. The reason may be
because our performance tuning is done exclusively
using the 50-best-trained reranker. Though we could
have achieved better performance by doing feature
selection and hyper-parameter tuning again using ?-
best MEMMs, we use the reranker trained on 50-
best tags run with 70-best MEMM tagger as the best
performing system in the following.
5.1 Comparison with existing systems
Table 7 shows the performance of our n-best tag-
ger and reranker on the official test set, and the best
reported results on the same task. As naturally ex-
pected, our system outperformed the systems that
cannot accommodate truly global features (Note that
one point of F-score improvement is valuable in this
task, because inter-annotator agreement rate of hu-
man experts in bio-entity recognition is likely to be
about 80%. For example, Krauthammer et al (2004)
report the inter-annotater agreement rate of 77.6%
for the three way bio-entity classification task.) and
the performance can be said to be at the same level as
the best systems. However, in spite of our effort, our
system could not outperform the best result achieved
by Tsai et al What makes Tsai et al?s system per-
form better than ours might be the careful treatment
of numeric expressions.
It is also notable that our MEMM tagger scored
71.10, which is comparable to the results of the sys-
tems that use CRFs. Considering the fact that the
tagger?s architecture is a simple first-order MEMM
which is far from state-of-the-art, and it uses only
POS taggers and shallow parsers as external re-
sources, we can say that simple machine-learning-
based method with carefully selected features could
214
Thresholding method for training
Thresholding
method for
testing
oracle avg. # of an-
swers
10-best 20-best 30-best 40-best 50-best 70-best 100-best
10-best 91.00 10 76.51 76.53 76.85 76.73 77.01 76.68 76.86
20-best 93.31 20 76.40 76.55 76.83 76.62 76.95 76.68 76.85
30-best 94.40 30 76.34 76.52 76.91 76.63 77.06 76.75 76.90
40-best 94.94 40 76.39 76.58 76.91 76.71 77.14 76.75 76.92
50-best 95.34 50 76.37 76.58 76.90 76.65 77.12 76.78 76.92
70-best 95.87 60 76.38 76.57 76.91 76.71 77.16 76.81 76.97
100-best 96.26 70 76.38 76.59 76.95 76.74 77.10 76.82 76.98
Table 5: Comparison of the F-scores of rerankers trained and evaluated with various N -best taggers.
Thresholding method for training
Thresholding
method for
testing
oracle
avg. #
of an-
swers
0.05-best 0.02-best 0.008-best 0.004-best 0.002-best 0.0005-best 0.0002-best
0.05-best 91.65 10.7 76.70 76.80 76.93 76.64 77.02 76.78 76.52
0.02-best 93.45 17.7 76.79 76.91 77.07 76.79 77.09 76.89 76.70
0.008-best 94.81 27.7 76.79 77.01 77.05 76.80 77.14 76.88 76.73
0.004-best 95.55 37.5 76.79 76.98 76.97 76.74 77.12 76.86 76.71
0.002-best 96.09 49.3 76.79 76.98 76.96 76.73 77.13 76.85 76.72
0.0005-best 96.82 77.7 76.79 76.98 76.96 76.73 77.13 76.85 76.70
0.0002-best 97.04 99.2 76.83 77.01 76.96 76.71 77.13 76.88 76.70
Table 6: Comparison of the F-scores of rerankers trained and evaluated with various ?-best taggers.
F-score Method
71.10 MEMMThis paper
72.65 reranking
Tsai et al (2006) 72.98 CRF, post-processing
Zhou et al (2004) 72.55
HMM,
SVM, post-
processing,
gazetteer
Friedrich et al (2006) 71.5 CRF,gazetteer
Okanohara et al (2006) 71.48 semi-CRF
Table 7: Performance comparison on the test set.
be sufficient practical solutions for this kind of tasks.
6 Conclusion
This paper showed that the named-entity recogni-
tion, which have usually been solved by dynamic-
programming-based sequence-labeling techniques
with local features, can have innegligible perfor-
mance improvement from reranking methods. Our
system showed clear improvement over many of the
machine-learning-based systems reported to date,
and also proved comparable to the existing state-of-
the-art systems that use rule-based post-processing.
Our future plans include further sophistication of
features, such as the use of external gazetteers which
is reported to improve the F-score by 1.0 and 2.7
points in (Zhou and Su, 2004) and (Friedrich et
al., 2006), respectively. We expect that reranking
architecture can readily accommodate dictionary-
based features, because we can apply elaborated
string-matching algorithms to the qualified candi-
date strings available at reranking phase.
We also plan to apply self-training of n-best tag-
ger which successfully boosted the performance
of one of the best existing English syntactic
parser (McClosky et al, 2006). Since the test data of
the shared-task consists of articles that represent the
different publication years, the effects of the publi-
cation years of the texts used for self-training would
be interesting to study.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
215
to Natural Language Processing. Computational Lin-
guistics, 22(1).
R. Bunescu and R. Mooney. 2004. Relational markov
networks for collective information extraction. In Pro-
ceedings of ICML 2004.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL 2005.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. In Technical
Report CMUCS.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of 17th In-
ternational Conference on Machine Learning, pages
175?182. Morgan Kaufmann, San Francisco, CA.
Christoph M. Friedrich, Thomas Revillion, Martin Hof-
mann, and Juliane Fluck. 2006. Biomedical and
Chemical Named Entity Recognition with Conditional
Random Fields: The Advantage of Dictionary Fea-
tures. In Proceedings of the Second International Sym-
posium on Semantic Mining in Biomedicine.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the Bio-Entity Recognition Task at JNLPBA. In
Proceedings of the International Workshop on Natu-
ral Language Processing in Biomedicine and its Appli-
cations (JNLPBA-04), pages 70?75, Geneva, Switzer-
land.
Michael Krauthammer and Goran Nenadic. 2004. Term
identification in the biomedical literature. Journal of
Biomedical Informatics, 37(6).
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In Pro-
ceedings of ACL 2006.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of 18th International Conference on Ma-
chine Learning, pages 282?289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Models for
Information Extraction and Segmentation. In ICML
2000.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
NAACL 2006.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA Corpus: an Annotated Research
Abstract Corpus in Molecular Biology Domain. In
Proceedings of the Human Language Technology Con-
ference (HLT 2002), March.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the Scala-
bility of Semi-Markov Conditional Random Fields for
Named Entity Recognition. In Proceedings of ACL
2006, Sydney, Australia, July.
Leonid Peshkin and Avi Pfefer. 2003. Bayesian Infor-
mation Extraction Network. In Proceedings of the
Eighteenth International Joint Conf. on Artificial In-
telligence.
S. Sarawagi and W. Cohen. 2004. Semimarkov con-
ditional random fields for information extraction. In
Proceedings of ICML 2004.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive Segmentation and Labeling of Distant Entities in
Information Extraction. Technical report, University
of Massachusetts. Presented at ICML Workshop on
Statistical Relational Learning and Its Connections to
Other Fields.
Richard Tzong-Han Tsai, Cheng-Lung Sung, Hong-Jie
Dai, Hsieh-Chuan Hung, Ting-Yi Sung, and Wen-Lian
Hsu. 2006. NERBio: using selected word conjunc-
tions, term normalization, and global patterns to im-
prove biomedical named entity recognition. In BMC
Bioinformatics 2006, 7(Suppl 5):S11.
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recognition.
In Proceedings of the International Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-04), pages 96?99.
216
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 118?119,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Raising the Compatibility of Heterogeneous Annotations:
A Case Study on Protein Mention Recognition
Yue Wang? Kazuhiro Yoshida? Jin-Dong Kim? Rune S?tre? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{wangyue, kyoshida, jdkim, rune.saetre, tsujii}@is.s.u-tokyo.ac.jp
Abstract
While there are several corpora which claim
to have annotations for protein references,
the heterogeneity between the annotations is
recognized as an obstacle to develop expen-
sive resources in a synergistic way. Here we
present a series of experimental results which
show the differences of protein mention an-
notations made to two corpora, GENIA and
AImed.
1 Introduction
There are several well-known corpora with protein
mention annotations. It is a natural request to bene-
fit from the existing annotations, but the heterogene-
ity of the annotations remains an obstacle. The het-
erogeneity is caused by different definitions of ?pro-
tein?, annotation conventions, and so on.
It is clear that by raising the compatibility of an-
notations, we can reduce the performance degrada-
tion caused by the heterogeneity of annotations.
In this work, we design several experiments to
observe the effect of removing or relaxing the het-
erogeneity between the annotations in two corpora.
The experimental results show that if we understand
where the difference is, we can raise the compati-
bility of the heterogeneous annotations by removing
the difference.
2 Corpora and protein mention recognizer
We used two corpora: the GENIA corpus (Kim
et al, 2003), and the AImed corpus (Bunescu and
Mooney, 2006). There are 2,000 MEDLINE ab-
stracts and 93,293 entities in the GENIA corpus.
?
??
??
??
??
??
??
??
??
??
?? ?? ?? ?? ??? ??? ??? ???????
???????????????????
???
????
Proceedings of the Workshop on BioNLP: Shared Task, pages 103?106,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
From Protein-Protein Interaction to Molecular Event Extraction
Rune S?tre?, Makoto Miwa?, Kazuhiro Yoshida? and Jun?ichi Tsujii?
{rune.saetre,mmiwa,kyoshida,tsujii}@is.s.u-tokyo.ac.jp
?Department of Computer Science
?Information Technology Center
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
Abstract
This document describes the methods and re-
sults for our participation in the BioNLP?09
Shared Task #1 on Event Extraction. It also
contains some error analysis and a brief dis-
cussion of the results. Previous shared tasks in
the BioNLP community have focused on ex-
tracting gene and protein names, and on find-
ing (direct) protein-protein interactions (PPI).
This year?s task was slightly different, since
the protein names were already manually an-
notated in the text. The new challenge was
to extract biological events involving these
given gene and gene products. We modi-
fied a publicly available system (AkanePPI)
to apply it to this new, but similar, protein
interaction task. AkanePPI has previously
achieved state-of-the-art performance on all
existing public PPI corpora, and only small
changes were needed to achieve competitive
results on this event extraction task. Our of-
ficial result was an F-score of 36.9%, which
was ranked as number six among submissions
from 24 different groups. We later balanced
the recall/precision by including more predic-
tions than just the most confident one in am-
biguous cases, and this raised the F-score on
the test-set to 42.6%. The new Akane program
can be used freely for academic purposes.
1 Introduction
With the increasing number of publications report-
ing on protein interactions, there is also a steadily
increasing interest in extracting information from
Biomedical articles by using Natural Language Pro-
cessing (BioNLP). There has been several shared
tasks arranged by the BioNLP community to com-
pare different ways of doing such Information Ex-
traction (IE), as reviewed in Krallinger et al(2008).
Earlier shared tasks have dealt with Protein-
Protein Interaction (PPI) in general, but this
task focuses on more specific molecular events,
such as Gene expression, Transcription, Pro-
tein catabolism, Localization and Binding, plus
(Positive or Negative) Regulation of proteins or
other events. Most of these events are related to PPI,
so our hypothesis was that one of the best perform-
ing PPI systems would perform well also on this
new event extraction task. We decided to modify a
publicly available system with flexible configuration
scripting (Miwa et al, 2008). Some adjustments had
to be made to the existing system, like adding new
types of Named Entities (NE) to represent the events
mentioned above. The modified AkaneRE (for Re-
lation Extraction) can be freely used in academia1.
2 Material and Methods
The event extraction system is implemented in a
pipeline fashion (Fig. 1).
2.1 Tokenization and Sentence Boundary
Detection
The text was split into single sentences by a sim-
ple sentence detection program, and then each sen-
tence was split into words (tokens). The tokeniza-
tion was done by using white-space as the token-
separator, but since all protein names are known dur-
ing both training and testing, some extra tokeniza-
tion rules were applied. For example, the protein
1http://www-tsujii.is.s.u-tokyo.ac.jp/?satre/akane/
103
Recursive Template  
Output
POS tagging
Parsing
(Enju & GDep)
Event Clueword 
Recognition
Event Template 
Extraction
Machine 
Learning (ML)
Training Data
ML Filtering
POS tagging
Event Clueword 
Recognition
Event Template 
Filling
Test Data
Models with 
Templates
Parsing
(Enju & GDep)
Tokenization Tokenization
Figure 1: System Overview
name ?T cell factor 1? is treated as a single token,
?T cell factor 1?, and composite tokens including a
protein name, like ?(T cell factor 1)?, are split into
several tokens, like ?(?, ?T cell factor 1? and ?)?, by
adding space around all given protein names. Also,
punctuation (commas, periods etc.) were treated as
separate tokens.
2.2 POS-tagging and Parsing
We used Enju2 and GDep3 to parse the text. These
parsers have their own built-in Part-of-Speech (POS)
taggers, and Enju also provides a normalized lemma
form for each token.
2.3 Event Clue-word tagging
Event clue-word detection was performed by a Ma-
chine Learning (ML) sequence labeling program.
This named-entity tagger program is based on a first
order Maximum Entropy Markov Model (MEMM)
and is described in Yoshida and Tsujii (2007). The
clue-word annotation of the shared-task training set
was converted into BIO format, and used to train the
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
3http://www.cs.cmu.edu/?sagae/parser/gdep/
MEMM model. The features used in the MEMM
model was extracted from surface strings and POS
information of the words corresponding to (or ad-
jacent to) the target BIO tags. The clue-word tag-
ger was applied to the development and test sets to
obtain the marginal probability that each word is a
clue-word of a certain category. The probabilities
were obtained by marginalizing the n-best output of
the MEMM tagger. We later also created clue-word
probability annotation of the training set, to enable
the template extraction program to access clue-word
probability information in the training phase.
2.4 Event Template Extraction
The training data was used to determine which
events to extract. As input to the system, a list of
Named Entity (NE) types and the Roles they can
play were provided. The roles can be thought of as
slots for arguments in event-frames, and in this task
the roles were Event (clue), Theme and Cause. In
the original AkanePPI (based on the AIMed corpus),
the only NE type was Protein, and the only role was
Theme (p1 and p2). All the (PPI) events were pair-
wise interactions, and there was no explicit event-
clue role. This means that all the events could be
represented with the single template shown first in
Table 1.
The BioNLP shared task used eight other NE
types, in addition to manually annotated Proteins,
namely Binding, Gene expression, Localization,
Protein catabolism, Transcription, Regulation, Pos-
itive Regulation and Negative Regulation. The first
five events have only Theme slots, which can only
be filled by Proteins, while the last three regulation
events are very diverse. They also have one Theme
slot, but they can have a Cause slot as well, and each
role/slot can be filled with either Proteins, or other
Events. See the first half of Table 1.
148 templates were extracted and clustered into
nine homogeneous groups which were classified
as nine separate sub-problems. The grouping was
based on whether the templates had an Event or a
Protein in the same role-positions. This way of orga-
nizing the groups was motivated by the fact that the
Proteins are 100% certain, while the accuracy of the
clue-word recognizer is only around 50% (estimated
on the training data). The bottom of Table 1 shows
the resulting nine general interaction templates.
104
2.5 Machine Learning with Maximum Entropy
Models
We integrated Maximum Entropy (ME) modeling,
also known as Logistic Regression, into AkaneRE.
This was done by using LIBLINEAR4, which han-
dles multi-class learning and prediction. Gold tem-
plates were extracted during training, and each tem-
plate was matched with all legal combinations of
Named Entities (including gold proteins/clue-words
and other recognized clue-word candidates) in each
sentence. The positive training examples were la-
beled as gold members of the template, and all other
combinations matching a given template were la-
beled as negative examples within that specific tem-
plate class. The templates were grouped into the
nine general templates shown in the bottom of Ta-
ble 1. Using one-vs-rest logistic regression, we
trained one multi-class classifier for each of the nine
groups individually. The ML features are shown in
Table 2.
In the test-phase, we extracted and labeled all re-
lation candidates matching all the templates from the
training-phase. The ML component was automati-
cally run independently for each of the nine groups
listed in the bottom of Table 1. Each time, all the
candidate template-instances in the current group
were assigned a confidence score by the classifier for
that group. This score is the probability that a can-
didate is a true relation, and a value above a certain
threshold means that the extracted relation will be
predicted as a true member of its specific template.
LIBLINEAR?s C-value parameter and the prediction
threshold were selected by hand to produce a good
F-score (according to the strict matching criterion)
on the development-test set.
2.6 Filtering and recursive output of the most
confident template instances
After machine learning, all the template instances
were filtered based on their confidence score. Af-
ter tuning the threshold to the development test-set,
we ended up using 1 as our C-value, and 3.5% as
our confidence threshold. Because the prediction
of Regulation Events were done independent from
the sub-events (or proteins) affected by that event,
some sub-events had to be included for complete-
4http://www.csie.ntu.edu.tw/?cjlin/liblinear/
ness, even if their confidence score was below the
threshold.
3 Results and Discussion
Our final official result was an F-score of 36.9%,
which was ranked as number six among the sub-
missions from 24 different groups. This means that
the AkanePPI system can achieve good results when
used on other PPI-related relation-extraction tasks,
such as this first BioNLP event recognition shared
task. The most common error was in predicting reg-
ulation events with other events as Theme or Cause.
The problem is that these events involve more than
one occurrence of event-trigger words, so the perfor-
mance is more negatively affected by our imperfect
clue-word detection system.
Since the recall was much lower on the test-set
than on the development test-set, we later allowed
the system to predict multiple confident alternatives
for a single event-word, and this raised our score on
the test-set from 36.9% to 42.6%. In hindsight, this
is obvious since there are many such examples in
the training data: E.g. ?over-express? is both posi-
tive regulation and Gene expression. The new sys-
tem, named AkaneRE (for Relation Extraction), can
be used freely for academic purposes.
As future work, we believe a closer integration
between the clue-word recognition and the template
prediction modules can lead to better performance.
Acknowledgments
?Grant-in-Aid for Specially Promoted Research?
and ?Genome Network Project?, MEXT, Japan.
References
Martin Krallinger et al 2008. Evaluation of text-mining
systems for biology: overview of the second biocre-
ative community challenge. Genome Biology, 9(S2).
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining multi-
ple layers of syntactic information for protein-protein
interaction extraction. In Proceedings of SMBM 2008,
pages 101?108, Turku, Finland, September.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of the Workshop on BioNLP 2007, June. Prague,
Czech Republic.
105
Freq Event Theme1 Theme2 Theme3 Theme4 Cause
- PPI Protein Protein
613 Binding Protein
213 Binding Protein Protein
3 Binding Protein Protein Protein
2 Binding Protein Protein Protein Protein
217 Regulation Protein Protein
12 Regulation Binding Protein
48 +Regulation Transcription Protein
4 +Regulation Phosphorylation Binding
5 -Regulation +Regulation Protein
... ... ... ...
Total 148 Templates
Count General Templates Theme1 Theme2 Theme3 Theme4 Cause
9 event templates Protein
1 event template Protein Protein
1 event template Protein Protein Protein
1 event template Protein Protein Protein Protein
3 event templates Protein Protein
12 event templates Protein Event
27 event templates Event
26 event templates Event Protein
68 event templates Event Event
Table 1: Interaction Templates from the training-set. Classic PPI at the top, compared to Binding and Regulation
events in the middle. 148 different templates were automatically extracted from the training data by AkaneRE. At
the bottom, the Generalized Interaction Templates are shown, with proteins distinguished from other Named Entities
(Events)
Feature Example
Text The binding of the most prominent factor, named TCF-1 ( T cell factor 1 ),
is correlated with the proto-enhancer activity of TCEd.
BOW B The
BOW M0 -comma- -lparen- factor most named of prominent PROTEIN the
BOW A -comma- -rparen- activity correlated is of proto-enhancer the TCEd with
Enju PATH (ENTITY1) (<prep arg12arg1) (of) (prep arg12arg2>) (factor)
(<verb arg123arg2) (name) (verb arg123arg3>) (ENTITY2)
pairs (ENTITY1 <prep arg12arg1) (<prep arg12arg1 of) (of prep arg12arg2>) ...
triples (ENTITY1 <prep arg12arg1 of) (<prep arg12arg1 of prep arg12arg2>) ...
GDep PATH (ENTITY1) (<NMOD) (name) (<VMOD) (ENTITY2)
pairs/triples (ENTITY1 <NMOD) (<NMOD name) ... (ENTITY1 <NMOD name) ...
Vector BOW B BOW M0...BOW M4 BOW A Enju PATH GDep PATH
Table 2: Bag-Of-Words (BOW) and shortest-path features for the machine learning. Several BOW feature groups were
created for each template, based on the position of the words in the sentence, relative to the position of the template?s
Named Entities (NE). Specifically, BOW B was made by the words from the beginning of the sentence to the first NE,
BOW A by the words between the last NE and the end of the sentence, and BOW M0 to BOW M4 was made by the
words between the main event clue-word and the NE in slot 0 through 4 respectively. The path features are made from
one, two or three neighbor nodes. We also included certain specific words, like ?binding?, as features.
106
