Addressee Identification in Face-to-Face Meetings
Natasa Jovanovic, Rieks op den Akker and Anton Nijholt
University of Twente
PO Box 217 Enschede
The Netherlands
{natasa,infrieks,A.Nijholt}@ewi.utwente.nl
Abstract
We present results on addressee identifica-
tion in four-participants face-to-face meet-
ings using Bayesian Network and Naive
Bayes classifiers. First, we investigate
how well the addressee of a dialogue
act can be predicted based on gaze, ut-
terance and conversational context fea-
tures. Then, we explore whether informa-
tion about meeting context can aid classi-
fiers? performances. Both classifiers per-
form the best when conversational context
and utterance features are combined with
speaker?s gaze information. The classifiers
show little gain from information about
meeting context.
1 Introduction
Addressing is an aspect of every form of commu-
nication. It represents a form of orientation and
directionality of the act the current actor performs
toward the particular other(s) who are involved in
an interaction. In conversational communication
involving two participants, the hearer is always the
addressee of the speech act that the speaker per-
forms. Addressing, however, becomes a real issue
in multi-party conversation.
The concept of addressee as well as a vari-
ety of mechanisms that people use in addressing
their speech have been extensively investigated
by conversational analysts and social psycholo-
gists (Goffman, 1981a; Goodwin, 1981; Clark and
Carlson, 1982).
Recently, addressing has received consider-
able attention in modeling multi-party interac-
tion in various domains. Research on au-
tomatic addressee identification has been con-
ducted in the context of mixed human-human
and human-computer interaction (Bakx et al,
2003; van Turnhout et al, 2005), human-human-
robot interaction (Katzenmaier et al, 2004), and
mixed human-agents and multi-agents interaction
(Traum, 2004). In the context of automatic anal-
ysis of multi-party face-to-face conversation, Ot-
suka et al (2005) proposed a framework for
automating inference of conversational structure
that is defined in terms of conversational roles:
speaker, addressee and unaddressed participants.
In this paper, we focus on addressee identifica-
tion in a special type of communication, namely,
face-to-face meetings. Moreover, we restrict our
analysis to small group meetings with four partic-
ipants. Automatic analysis of recorded meetings
has become an emerging domain for a range of
research focusing on different aspects of interac-
tions among meeting participants. The outcomes
of this research should be combined in a targeted
application that would provide users with useful
information about meetings. For answering ques-
tions such as ?Who was asked to prepare a presen-
tation for the next meeting?? or ?Were there any
arguments between participants A and B??, some
sort of understanding of dialogue structure is re-
quired. In addition to identification of dialogue
acts that participants perform in multi-party dia-
logues, identification of addressees of those acts is
also important for inferring dialogue structure.
There are many applications related to meeting
research that could benefit from studying address-
ing in human-human interactions. The results
can be used by those who develop communicative
agents in interactive intelligent environments and
remote meeting assistants. These agents need to
recognize when they are being addressed and how
they should address people in the environment.
This paper presents results on addressee identi-
169
fication in four-participants face-to-face meetings
using Bayesian Network and Naive Bayes classi-
fiers. The goals in the current paper are (1) to
find relevant features for addressee classification
in meeting conversations using information ob-
tained from multi-modal resources - gaze, speech
and conversational context, (2) to explore to what
extent the performances of classifiers can be im-
proved by combining different types of features
obtained from these resources, (3) to investigate
whether the information about meeting context
can aid the performances of classifiers, and (4) to
compare performances of the Bayesian Network
and Naive Bayes classifiers for the task of ad-
dressee prediction over various feature sets.
2 Addressing in face-to-face meetings
When a speaker contributes to the conversation, all
those participants who happen to be in perceptual
range of this event will have ?some sort of partic-
ipation status relative to it?. The conversational
roles that the participants take in a given conversa-
tional situation make up the ?participation frame-
work? (Goffman, 1981b).
Goffman (1976) distinguished three basic kinds
of hearers: those who overhear, whether or not
their unratified participation is unintentional or en-
couraged; those who are ratified but are not specif-
ically addressed by the speaker (also called unad-
dressed recipients (Goffman, 1981a)); and those
ratified participants who are addressed. Ratified
participants are those participants who are allowed
to take part in conversation. Regarding hearers?
roles in meetings, we are focused only on ratified
participants. Therefore, the problem of addressee
identification amounts to the problem of distin-
guishing addressed from unaddressed participants
for each dialogue act that speakers perform.
Goffman (1981a) defined addressees as those
?ratified participants () oriented to by the speaker
in a manner to suggest that his words are particu-
larly for them, and that some answer is therefore
anticipated from them, more so than from the other
ratified participants?. According to this, it is the
speaker who selects his addressee; the addressee is
the one who is expected by the speaker to react on
what the speaker says and to whom, therefore, the
speaker is giving primary attention in the present
act.
In meeting conversations, a speaker may ad-
dress his utterance to the whole group of partici-
pants present in the meeting, or to a particular sub-
group of them, or to a single participant in partic-
ular. A speaker can also just think aloud or mum-
ble to himself without really addressing anybody
(e.g.?What else do I want to say?? (while try-
ing to evoke more details about the issue that he is
presenting)). We excluded self-addressed speech
from our study.
Addressing behavior is behavior that speakers
show to express to whom they are addressing their
speech. It depends on the course of the conver-
sation, the status of attention of participants, their
current involvement in the discussion as well as
on what the participants know about each others?
roles and knowledge, whether explicit addressing
behavior is called for. Using a vocative is the ex-
plicit verbal way to address someone. In some
cases the speaker identifies the addressee of his
speech by looking at the addressee, sometimes ac-
companying this by deictic hand gestures. Ad-
dressees can also be designated by the manner of
speaking. For example, by whispering, a speaker
can select a single individual or a group of people
as addressees. Addressees are often designated by
the content of what is being said. For example,
when making the suggestion ?We all have to de-
cide together about the design?, the speaker is ad-
dressing the whole group.
In meetings, people may perform various group
actions (termed as meeting actions) such as pre-
sentations, discussions or monologues (McCowan
et al, 2003). A type of group action that meeting
participants perform may influence the speaker?s
addressing behavior. For example, speakers may
show different behavior during a presentation than
during a discussion when addressing an individ-
ual: regardless of the fact that a speaker has
turned his back to a participant in the audience
during a presentation, he most probably addresses
his speech to the group including that participant,
whereas the same behavior during a discussion, in
many situations, indicates that that participant is
unaddressed.
In this paper, we focus on speech and gaze as-
pects of addressing behavior as well as on con-
textual aspects such as conversational history and
meeting actions.
3 Cues for addressee identification
In this section, we present our motivation for fea-
ture selection, referring also to some existing work
170
on the examination of cues that are relevant for ad-
dressee identification.
Adjacency pairs and addressing - Adjacency
pairs (AP) are minimal dialogic units that con-
sist of pairs of utterances called ?first pair-part?
(or a-part) and the ?second pair-part? (or b-part)
that are produced by different speakers. Examples
include question-answers or statement-agreement.
In the exploration of the conversational organiza-
tion, special attention has been given to the a-parts
that are used as one of the basic techniques for se-
lecting a next speaker (Sacks et al, 1974). For ad-
dressee identification, the main focus is on b-parts
and their addressees. It is to be expected that the
a-part provides a useful cue for identification of
addressee of the b-part (Galley et al, 2004). How-
ever, it does not imply that the speaker of the a-part
is always the addressee of the b-part. For example,
A can address a question to B, whereas B?s reply
to A?s question is addressed to the whole group. In
this case, the addressee of the b-part includes the
speaker of the a-part.
Dialogue acts and addressing When designing
an utterance, a speaker intends not only to per-
form a certain communicative act that contributes
to a coherent dialogue (in the literature referred
to as dialogue act), but also to perform that act to-
ward the particular others. Within a turn, a speaker
may perform several dialogue acts, each of those
having its own addressee ( e.g. I agree with you
[agreement; addressed to a previous speaker] but
is this what we want [information request; ad-
dressed to the group]). Dialogue act types can
provide useful information about addressing types
since some types of dialogue acts -such as agree-
ments or disagreements- tend to be addressed to
an individual rather than to a group. More infor-
mation about the addressee of a dialogue can be
induced by combining the dialogue act informa-
tion with some lexical markers that are used as ad-
dressee ?indicators? (e.g. you, we, everybody, all
of you) (Jovanovic and op den Akker, 2004).
Gaze behavior and addressing Analyzing
dyadic conversations, researchers into social
interaction observed that gaze in social inter-
action is used for several purposes: to control
communication, to provide a visual feedback, to
communicate emotions and to communicate the
nature of relationships (Kendon, 1967; Argyle,
1969).
Recent studies into multi-party interaction em-
phasized the relevance of gaze as a means of ad-
dressing. Vertegaal (1998) investigated to what ex-
tent the focus of visual attention might function as
an indicator for the focus of ?dialogic attention? in
four-participants face-to-face conversations. ?Di-
alogic attention? refers to attention while listening
to a person as well as attention while talking to
one or more persons. Empirical findings show that
when a speaker is addressing an individual, there
is 77% chance that the gazed person is addressed.
When addressing a triad, speaker gaze seems to be
evenly distributed over the listeners in the situa-
tion where participants are seated around the ta-
ble. It is also shown that on average a speaker
spends significantly more time gazing at an indi-
vidual when addressing the whole group, than at
others when addressing a single individual. When
addressing an individual, people gaze 1.6 times
more while listening (62%) than while speaking
(40%). When addressing a triad the amount of
speaker gaze increases significantly to 59%. Ac-
cording to all these estimates, we can expect that
gaze directional cues are good indicators for ad-
dressee prediction.
However, these findings cannot be generalized
in the situations where some objects of interest are
present in the conversational environment, since
it is expected that the amount of time spent look-
ing at the persons will decrease significantly. As
shown in (Bakx et al, 2003), in a situation where
a user interacts with a multimodal information sys-
tem and in the meantime talks to another person,
the user looks most of the time at the system, both
when talking to the system (94%) and when talk-
ing to the user (57%). Also, another person looks
at the system in 60% of cases when talking to the
user. Bakx et al (2003) also showed that some im-
provement in addressee detection can be achieved
by combining utterance duration with gaze.
In meeting conversations, the contribution of
the gaze direction to addressee prediction is also
affected by the current meeting activity and seat-
ing arrangement (Jovanovic and op den Akker,
2004). For example, when giving a presentation,
a speaker most probably addresses his speech to
the whole audience, although he may only look at
a single participant in the audience. A seating ar-
rangement determines a visible area for each meet-
ing participant. During a turn, a speaker mostly
looks at the participants who are in his visible area.
171
Moreover, the speaker frequently looks at a sin-
gle participant in his visual area when addressing
a group. However, when he wants to address a sin-
gle participant outside his visual area, he will often
turn his body and head toward that participant.
In this paper, we explored not only the effec-
tiveness of the speaker?s gaze direction, but also
the effectiveness of the listeners? gaze directions
as cues for addressee prediction.
Meeting context and addressing As Goff-
man (1981a) has noted, ?the notion of a conver-
sational encounter does not suffice in dealing with
the context in which words are spoken; a social
occasion involving a podium event or no speech
event at all may be involved, and in any case, the
whole social situation, the whole surround, must
always be considered?. A set of various meet-
ing actions that participants perform in meetings is
one aspect of the social situation that differentiates
meetings from other contexts of talk such as ordi-
nary conversations, interviews or trials. As noted
above, it influences addressing behavior as well
as the contribution of gaze to addressee identifi-
cation. Furthermore, distributions of addressing
types vary for different meeting actions. Clearly,
the percentage of the utterances addressed to the
whole group during a presentation is expected to
be much higher than during a discussion.
4 Data collection
To train and test our classifiers, we used a small
multimodal corpus developed for studying ad-
dressing behavior in meetings (Jovanovic et al,
2005). The corpus contains 12 meetings recorded
at the IDIAP smart meeting room in the research
program of the M41 and AMI projects2. The
room has been equipped with fully synchronized
multi-channel audio and video recording devices,
a whiteboard and a projector screen. The seating
arrangement includes two participants at each of
two opposite sides of the rectangular table. The
total amount of the recorded data is approximately
75 minutes. For experiments presented in this pa-
per, we have selected meetings from the M4 data
collection. These meetings are scripted in terms of
type and schedule of group actions, but content is
natural and unconstrained.
The meetings are manually annotated with dia-
logue acts, addressees, adjacency pairs and gaze
1http://www.m4project.org
2http://www.amiproject.org
direction. Each type of annotation is described
in detail in (Jovanovic et al, 2005). Additionally,
the available annotations of meeting actions for the
M4 meetings3 were converted into the corpus for-
mat and included in the collection.
The dialogue act tag set employed for the cor-
pus creation is based on the MRDA (Meeting
Recorder Dialogue Act) tag set (Dhillon et al,
2004). The MRDA tag set represents a modifi-
cation of the SWDB-DAMSL tag set (Jurafsky et
al., 1997) for an application to multi-party meet-
ing dialogues. The tag set used for the corpus cre-
ation is made by grouping the MRDA tags into 17
categories that are divided into seven groups: ac-
knowledgments/backchannels, statements, ques-
tions, responses, action motivators, checks and po-
liteness mechanisms. A mapping between this tag
set and the MRDA tag set is given in (Jovanovic
et al, 2005). Unlike MRDA where each utterance
is marked with a label made up of one or more
tags from the set, each utterance in the corpus is
marked as Unlabeled or with exactly one tag
from the set. Adjacency pairs are labeled by mark-
ing dialogue acts that occur as their a-part and b-
part.
Since all meetings in the corpus consist of four
participants, the addressee of a dialogue act is la-
beled as Unknown or with one of the following
addressee tags: individual Px, a subgroup of par-
ticipants Px,Py or the whole audience Px,Py,Pz.
Labeling gaze direction denotes labeling gazed
targets for each meeting participants. As the only
targets of interest for addressee identification are
meeting participants, the meetings were annotated
with the tag set that contains tags that are linked to
each participant Px and the NoTarget tag that is
used when the speaker does not look at any of the
participants.
Meetings are annotated with a set of six meet-
ing actions described in (McCowan et al, 2003):
monologue, presentation, white-board, discussion,
consensus, disagreement and note-taking.
Reliability of the annotation schema As re-
ported in (Jovanovic et al, 2005), gaze annota-
tion has been reproduced reliably (segmentation
80.40% (N=939); classification ? = 0.95). Table
1 shows reliability of dialogue act segmentation
as well as Kappa values for dialogue act and ad-
dressee classification for two different annotation
3http://mmm.idiap.ch/
172
groups that annotated two different sets of meeting
data.
Group Seg(%) N DA(?) ADD(?)
B&E 91.73 377 0.77 0.81
M&R 86.14 367 0.70 0.70
Table 1: Inter-annotator agreement on DA and ad-
dressee annotation: N- number of agreed segments
5 Addressee classification
In this section we present the results on addressee
classification in four-persons face-to-face meet-
ings using Bayesian Network and Naive Bayes
classifiers.
5.1 Classification task
In a dialogue situation, which is an event which
lasts as long as the dialogue act performed by the
speaker in that situation, the class variable is the
addressee of the dialogue act (ADD). Since there
are only a few instances of subgroup addressing in
the data, we removed them from the data set and
excluded all possible subgroups of meeting par-
ticipants from the set of class values. Therefore,
we define addressee classifiers to identify one of
the following class values: individual Px where
x ? {0,1,2,3} and ALLPwhich denotes the whole
group.
5.2 Feature set
To identify the addressee of a dialogue act we
initially used three sorts of features: conversa-
tional context features (later referred to as contex-
tual features), utterance features and gaze features.
Additionally, we conducted experiments with an
extended feature set including a feature that con-
veys information about meeting context.
Contextual features provide information about
the preceding utterances. We experimented with
using information about the speaker, the addressee
and the dialogue act of the immediately preceding
utterance on the same or a different channel (SP-
1, ADD-1, DA-1) as well as information about
the related utterance (SP-R, ADD-R, DA-R). A re-
lated utterance is the utterance that is the a-part of
an adjacency pair with the current utterance as the
b-part. Information about the speaker of the cur-
rent utterance (SP) has also been included in the
contextual feature set.
As utterance features, we used a subset of lex-
ical features presented in (Jovanovic and op den
Akker, 2004) as useful cues for determining
whether the utterance is single or group addressed.
The subset includes the following features:
? does the utterance contain personal pronouns ?we? or
?you?, both of them, or neither of them?
? does the utterance contain possessive pronouns or pos-
sessive adjectives (?your/yours? or ?our/ours?), their
combination or neither of them?
? does the utterance contain indefinite pronouns such as
?somebody?, ?someone?, ?anybody?, ?anyone?, ?ev-
erybody? or ?everyone??
? does the utterance contain the name of participant Px?
Utterance features also include information about
the utterance?s conversational function (DA tag)
and information about utterance duration i.e.
whether the utterance is short or long. In our ex-
periments, an utterance is considered as a short ut-
terance, if its duration is less than or equal to 1
sec.
We experimented with a variety of gaze fea-
tures. In the first experiment, for each participant
Px we defined a set of features in the form Px-
looks-Py and Px-looks-NT where x,y ? {0,1,2,3}
and x 6= y; Px-looks-NT represents that partici-
pant Px does not look at any of the participants.
The value set represents the number of times that
speaker Px looks at Py or looks away during the
time span of the utterance: zero for 0, one for 1,
two for 2 and more for 3 or more times. In the
second experiment, we defined a feature set that
incorporates only information about gaze direction
of the current speaker (SP-looks-Px and SP-looks-
NT) with the same value set as in the first experi-
ment.
As to meeting context, we experimented with
different values of the feature that represents the
meeting actions (MA-TYPE). First, we used a full
set of speech based meeting actions that was ap-
plied for the manual annotation of the meetings in
the corpus: monologue, discussion, presentation,
white-board, consensus and disagreement. As the
results on modeling group actions in meetings pre-
sented in (McCowan et al, 2003) indicate that
consensus and disagreements were mostly mis-
classified as discussion, we have also conducted
experiments with a set of four values for MA-
TYPE, where consensus, disagreement and dis-
cussion meeting actions were grouped in one cat-
egory marked as discussion.
173
5.3 Results and Discussions
To train and test the addressee classifiers, we used
the hand-annotated M4 data from the corpus. Af-
ter we had discarded the instances labeled with
Unknown or subgroup addressee tags, there were
781 instances left available for the experiments.
The distribution of the class values in the selected
data is presented in Table 2.
ALLP P0 P1 P2 P3
40.20% 13.83% 17.03% 15.88% 13.06%
Table 2: Distribution of addressee values
For learning the Bayesian Network structure,
we applied the K2 algorithm (Cooper and Her-
skovits, 1992). The algorithm requires an ordering
on the observable features; different ordering leads
to different network structures. We conducted ex-
periments with several orderings regarding feature
types as well as with different orderings regarding
features of the same type. The obtained classifi-
cation results for different orderings were nearly
identical. For learning conditional probability dis-
tributions, we used the algorithm implemented in
theWEKA toolbox4 that produces direct estimates
of the conditional probabilities.
5.3.1 Initial experiments without meeting
context
The performances of the classifiers are mea-
sured using different feature sets. First, we mea-
sured the performances of classifiers using utter-
ance features, gaze features and contextual fea-
tures separately. Then, we conducted experiments
with all possible combinations of different types of
features. For each classifier, we performed 10-fold
cross-validation. Table 3 summarizes the accura-
cies of the classifiers (with 95% confidence inter-
val) for different feature sets (1) using gaze infor-
mation of all meeting participants and (2) using
only information about speaker gaze direction.
The results show that the Bayesian Network
classifier outperforms the Naive Bayes classifier
for all feature sets, although the difference is sig-
nificant only for the feature sets that include con-
textual features.
For the feature set that contains only informa-
tion about gaze behavior combined with infor-
mation about the speaker (Gaze+SP), both clas-
sifiers perform significantly better when exploit-
ing gaze information of all meeting participants.
4http://www.cs.waikato.ac.nz/ ml/weka/
In other words, when using solely focus of visual
attention to identify the addressee of a dialogue
act, listeners? focus of attention provides valuable
information for addressee prediction. The same
conclusion can be drawn when adding informa-
tion about utterance duration to the gaze feature
set (Gaze+SP+Short), although for the Bayesian
Network classifier the difference is not significant.
For all other feature sets, the classifiers do not per-
form significantly different when including or ex-
cluding the listeners gaze information. Even more,
both classifiers perform better using only speaker
gaze information in all cases except when com-
bined utterance and gaze features are exploited
(Utterance+Gaze+SP).
The Bayesian network and Naive Bayes clas-
sifiers show the same changes in the perfor-
mances over different feature sets. The re-
sults indicate that the selected utterance fea-
tures are less informative for addressee predic-
tion (BN:52.62%, NB:52.50%) compared to con-
textual features (BN:73.11%; NB:68.12%) or fea-
tures of gaze behavior (BN:66.45%, NB:64.53%).
The results also show that adding the informa-
tion about the utterance duration to the gaze fea-
tures, slightly increases the accuracies of the clas-
sifiers (BN:67.73%, NB:65.94%), which confirms
findings presented in (Bakx et al, 2003). Com-
bining the information from the gaze and speech
channels significantly improves the performances
of the classifiers (BN:70.68%; NB:69.78%) in
comparison to performances obtained from each
channel separately. Furthermore, higher accura-
cies are gained when adding contextual features to
the utterance features (BN:76.82%; NB:72.21%)
and even more to the features of gaze behavior
(BN:80.03%, NB:77.59%). As it is expected, the
best performances are achieved by combining all
three types of features (BN:82.59%, NB:78.49%),
although not significantly better compared to com-
bined contextual and gaze features.
We also explored how well the addressee can be
predicted excluding information about the related
utterance (i.e. AP information). The best perfor-
mances are achieved combining speaker gaze in-
formation with contextual and utterance features
(BN:79.39%; NB:76.06%). A small decrease in
the classification accuracies when excluding AP
information (about 3%) indicates that remaining
contextual, utterance and gaze features capture
most of the useful information provided by AP.
174
Baysian Networks Naive Bayes
Feature sets Gaze All Gaze SP Gaze All Gaze SP
All Features 81.05% (?2.75) 82.59% (?2.66) 78.10% (?2.90) 78.49% (?2.88)
Context 73.11% (?3.11) 68.12% (?3.27)
Utterance+SP 52.62% (?3.50) 52.50% (?3.50)
Gaze+SP 66.45% (?3.31) 62.36% (?3.40) 64.53% (?3.36) 59.02% (?3.45)
Gaze+SP+Short 67.73% (?3.28) 66.45% (?3.31) 65.94% (?3.32) 61.46% (?3.41)
Context+Utterance 76.82% (?2.96) 72.21% (?3.14)
Context+Gaze 79.00% (?2.86) 80.03% (?2.80) 74.90% (?3.04) 77.59% (?2.92)
Utterance+Gaze+SP 70.68% (?3.19) 70.04% (?3.21) 69.78% (?3.22) 68.63% (?3.25)
Table 3: Classification results for Bayesian Network and Naive Bayes classifiers using gaze information
of all meeting participants (Gaze All) and using speaker gaze information (Gaze SP)
Error analysis Further analysis of confusion
matrixes for the best performed BN and NB clas-
sifiers, show that most misclassifications were be-
tween addressing types (individual vs. group):
each Px was more confused with ALLP than with
Py. A similar type of confusion is observed be-
tween human annotators regarding addressee an-
notation (Jovanovic et al, 2005). Out of all mis-
classified cases for each classifier, individual types
of addressing (Px) were, in average, misclassified
with addressing the group (ALLP) in 73% cases
for NB, and 68% cases for BN.
5.3.2 Experiments with meeting context
We examined whether meeting context informa-
tion can aid the classifiers? performances. First,
we conducted experiments using the six values
set for the MA-TYPE feature. Then, we exper-
imented with employing the reduced set of four
types of meeting actions (see Section 5.2). The
accuracies obtained by combining the MA-TYPE
feature with contextual, utterance and gaze fea-
tures are presented in Table 4.
Bayesian Networks Naive Bayes
Features Gaze All Gaze SP Gaze All Gaze SP
MA-6+All 81.82% 82.84% 78.74% 79.90%
MA-4+All 81.69% 83.74% 78.23% 79.13%
Table 4: Classification results combining MA-
TYPE with the initial feature set
The results indicate that adding meeting con-
text information to the initial feature set improves
slightly, but not significantly, the classifiers? per-
formances. The highest accuracy (83.74%) is
achieved using the Bayesian Network classifier by
combining the four-values MA-TYPE feature with
contextual, utterance and the speaker?s gaze fea-
tures.
6 Conclusion and Future work
We presented results on addressee classification
in four-participants face-to-face meetings using
Bayesian Network and Naive Bayes classifiers.
The experiments presented should be seen as pre-
liminary explorations of appropriate features and
models for addressee identification in meetings.
We investigated how well the addressee of a di-
alogue act can be predicted (1) using utterance,
gaze and conversational context features alone as
well as (2) using various combinations of these
features. Regarding gaze features, classifiers? per-
formances are measured using gaze directional
cues of the speaker only as well as of all meeting
participants. We found that contextual informa-
tion aids classifiers? performances over gaze in-
formation as well as over utterance information.
Furthermore, the results indicate that selected ut-
terance features are the most unreliable cues for
addressee prediction. The listeners? gaze direc-
tion provides useful information only in the situa-
tion where gaze features are used alone. Combina-
tions of features from various resources increases
classifiers? performances in comparison to perfor-
mances obtained from each resource separately.
However, the highest accuracies for both classi-
fiers are reached by combining contextual and ut-
terance features with speaker?s gaze (BN:82.59%,
NB:78.49%). We have also explored the ef-
fect of meeting context on the classification task.
Surprisingly, addressee classifiers showed little
gain from the information about meeting actions
(BN:83.74%, NB:79.90%). For all feature sets,
the Bayesian Network classifier outperforms the
Naive Bayes classifier.
In contrast to Vertegaal (1998) and Otsuka et
al. (2005) findings, where it is shown that gaze
can be a good predictor for addressee in four-
participants face-to-face conversations, our results
175
show that in four-participants face-to-face meet-
ings, gaze is less effective as an addressee indi-
cator. This can be due to several reasons. First,
they used different seating arrangements which is
implicated in the organization of gaze. Second,
our meeting environment contains attentional ?dis-
tracters? such as whiteboard, projector screen and
notes. Finally, during a meeting, in contrast to an
ordinary conversation, participants perform vari-
ous meeting actions which may influence gaze as
an aspect of addressing behavior.
We will continue our work on addressee identi-
fication on the large AMI data collection that is
currently in production. The AMI corpus con-
tains more natural, scenario-based, meetings that
involve groups focused on the design of a TV re-
mote control. Some initial experiments on the
AMI pilot data show that additional challenges for
addressee identification on the AMI data are: roles
that participants play in the meetings (e.g. project
manager or marketing expert) and additional at-
tentional ?distracters? present in the meeting room
such as, the task object at first place and laptops.
This means that a richer feature set should be ex-
plored to improve classifiers? performances on the
AMI data including, for example, the background
knowledge about participants? roles. We will also
focus on the development of new models that bet-
ter handle conditional and contextual dependen-
cies among different types of features.
Acknowledgments
This work was partly supported by the European
Union 6th FWP IST Integrated Project AMI (Aug-
mented Multi-party Interaction, FP6-506811, pub-
lication AMI-153).
References
M. Argyle. 1969. Social Interaction. London: Tavis-
tock Press.
I. Bakx, K. van Turnhout, and J. Terken. 2003. Facial
orientation during multi-party interaction with infor-
mation kiosks. In Proc. of INTERACT.
H. H. Clark and B. T. Carlson. 1982. Hearers and
speech acts. Language, 58:332?373.
G. Cooper and E. Herskovits. 1992. Bayesian method
for the induction of probabilistic networks from
data. Machine Learning, 9:309?347.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialogue act label-
ing guide. Technical report, ICSI, Berkeley, USA.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and
disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependen-
cies. In Proc. of 42nd Meeting of the ACL.
E. Goffman. 1976. Replies and responses. Language
in Society, 5:257?313.
E. Goffman. 1981a. Footing. In Forms of Talk, pages
124?159. University of Pennsylvania Press.
E. Goffman. 1981b. Forms of Talk. University of
Pennsylvania Press, Philadelphia.
C. Goodwin. 1981. Conversational Organiza-
tion: Interaction Between Speakers and Hearers.
NY:Academic Press.
N. Jovanovic and R. op den Akker. 2004. Towards
automatic addressee identification in multi-party di-
alogues. In Proc of the 5th SIGDial.
N. Jovanovic, R. op den Akker, and A. Nijholt. 2005.
A corpus for studying addressing behavior in face-
to-face meetings. In Proc. of the 6th SIGDial.
D. Jurafsky, L. Shriberg, and D. Biasca. 1997. Switch-
board swbd-damsl shallow-discourse-function an-
notation coders manual, draft 13. Technical report,
University of Colorado, Institute of Cognitive Sci-
ence.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot in-
teractions based on head pose and speech. In Proc.
of ICMI.
A. Kendon. 1967. Some functions of gaze direction in
social interaction. Acta Psychologica, 32:1?25.
I. McCowan, S. Bengio, D. Gatica-Perez, G. Lathoud,
F. Monay, D. Moore, P. Wellner, and H. Bourlard.
2003. Modeling human interactions in meetings. In
Proc. IEEE ICASSP.
K. Otsuka, Y. Takemae, J. Yamato, and H. Murase.
2005. A probabilistic inference of multiparty-
conversation structure based on markov-switching
models of gaze patterns, head directions, and utter-
ances. In Proc. of ICMI.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50:696?735.
D. Traum. 2004. Issues in multi-party dialogues. In
F. Dignum, editor, Advances in Agent Communica-
tion, pages 201?211. Springer-Verlag.
K. van Turnhout, J. Terken, I. Bakx, and B. Eggen.
2005. Identifying the intended addressee in
mixed human-human and human-computer interac-
tion from non-verbal features. In Proc. of ICMI.
R. Vertegaal. 1998. Look who is talking to whom.
Ph.D. thesis, University of Twente, September.
176
Towards automatic addressee identification in multi-party dialogues
Natasa Jovanovic
Department of Computer Science
University of Twente
PO Box 217 Enschede, the Netherlands
natasa@cs.utwente.nl
Rieks op den Akker
Department of Computer Science
University of Twente
PO Box 217 Enschede, the Netherlands
infrieks@cs.utwente.nl
Abstract
The paper is about the issue of addressing in
multi-party dialogues. Analysis of addressing
behavior in face to face meetings results in
the identification of several addressing mech-
anisms. From these we extract several utter-
ance features and features of non-verbal com-
municative behavior of a speaker, like gaze
and gesturing, that are relevant for observers to
identify the participants the speaker is talking
to. A method for the automatic prediction of
the addressee of speech acts is discussed.
1 Introduction
Communication, between humans or between humans
and conversational computer agents, involves address-
ing. Addressing has received attention in the tradition
of conversation analysis (Clark and Carlson, 1992; Clark
and Schaefer, 1992), but not that much in the commu-
nity of computational dialogue systems. One exception is
(Traum, 2003). An explanation for this lack of attention
may be that most research in computational dialogue sys-
tems concerns systems that were designed for interaction
between one human user and one conversational agent.
In dialogues in which only two participants take part ad-
dressing goes without saying. Addressing becomes a real
issue in multi-party conversations and that is the subject
of this paper.
There are a number of application areas that could ben-
efit from studying addressing behavior in human human
interactions. It can provide valuable data for learning
more about human interaction and the way humans in-
teract with intelligent environments. The result can be
used by those who develop communicative agents in in-
teractive intelligent environments, meeting managers and
presentation assistants. If we could induce from recorded
meetings the ?who said what, when and to whom? we
can use this information for making summarizations of
meetings, and for real-time tracking.
Research on small group discussions (Carletta et al,
2002) has shown that there is a noticeable difference in
the interaction patterns between large and small groups
(up to seven participants). A small group discussion
looks like two-way conversations but conversations oc-
cur between all pairs of members and every member can
initiate conversation. A large group discussion is more
like a series of conversations between a group leader and
various individuals with the rest participants present but
silent. We will focus our research on small group discus-
sions in meetings.
In this paper we propose research that aims at the au-
tomatic determination of the addressee of a speaker in
small meetings. Analysis of the mechanisms that people
use in identifying their addressees leads to a model of a
conversation that describes the features that play a role
in these mechanisms. These features can be of several
types: verbal, non-verbal, and features of the situation.
Our research is partly based on analysis of the IDIAP
multi-modal meeting data corpus made available through
the Media File Server 1.
2 Addressee detection - problem overview
One of the question of interest concerning a meeting is:
?Who talked to whom and about what during the meet-
ing??. This question refers to three very important as-
pects of a conversational event: source of the message
(speaker identification), topic of the message (topic de-
tection) and addressee of the message (addressee identi-
fication).
Speaker and addressee roles are the basic conversa-
tional roles. There are different ways to categorize the
audience of a speech act. We use a taxonomy of con-
versational roles proposed in (Clark and Carlson, 1992).
People around an action are divided in those who re-
1http://mmm.idiap.ch
ally participate in the action (active participants) and
those who do not (non-participants). The active partic-
ipants in a conversation include speaker and addressee as
well as other participants taking part in conversation but
currently not being addressed. Clark called them side-
participants. All other listeners who have no rights to
take part in conversation are called overhearers. Over-
hearers are divided in two groups: bystanders and eaves-
droppers. Bystanders are overhearers who are present and
the speaker is aware of their presence. Eavesdroppers are
those who are listening without the speakers awareness.
In determining the conversational roles in a meeting situ-
ation we will focus on the active participants. The prob-
lem of addressee identification amounts to the problem of
distinguishing the addressee from the side participants in
a conversation.
According to dialogue act theory (Bunt, 2000) an ut-
terance can consist of several segments which carry dif-
ferent dialogue acts. Each of these dialogue acts can have
it?s own addressee. The following example is an example
of multi-addressee utterances.
A: We could use Java as a standard?
[suggestion] addressee B,C
B: yes? but what about C++ ?
[agreement]addressee A? [suggestion]addressee A,C
C: Both is OK for me
[accept] addressee A,B
3 Observation analysis - addressee
detection in meetings
Three main questions considering addressee detections
are: 1. What are the relevant sources of information for
the addressee detection in face-to-face meetings? 2. How
does the speaker express who is the addressee of his ut-
terance? 3. How can we combine all this information in
order to determine the addressee of the speaker utterance?
In order to find answers on these questions we ob-
served meetings recorded at the IDIAP and annotated
several of them. For annotation we used the NITE Work-
bench for Windows (NWB3) annotation tool 2. We de-
fined our annotation scheme based on the initial assump-
tions about the information sources that can be used for
the addressee identification. These assumptions are the
result of our meeting observations.
3.1 Sources of information
When two or more people are engaged in interaction
they communicate using verbal and/or non-verbal ele-
ments. The most natural and powerful human commu-
nication is in combined use of words, gaze, facial and
gestural movements, posture, bodily contact, etc.
2http://nite.nis.sdu.dk/download/. NWB The NITE Work-
bench is a general-purpose natural interactivity coding tool
3.1.1 Speech
Speech is the main communication channel used in the
meeting conversation. Therefore, it is the main source
for addressee detection. The most common heuristics that
may guide the addressee recognition process is the search
for linguistic markers in the utterance. Table 1 contains
linguistic markers that can be used as cues for addressee
detection. For instance, you is the personal pronoun that
refers to the meeting participants excluding the speaker of
the utterance. Usage of quantifying determiners, numer-
als and indefinite pronouns may help in distinguish you
as a particular person from you as a group. If an utter-
ance contains noun phrases like some of you, few of you,
most of you, etc., then it is addressed to all meeting par-
ticipants. The speaker doesn?t know who he is actually
addressing (He saw some of you yesterday).
Name detection is a powerful method for addressee
determination. The name in vocative form is used for di-
rect addressing the person with that name (What about
you, John?). Using the name of the participant the
speaker can claim something about the participant ad-
dressing the utterance to the other addressee (John was
not present et the last meeting).
Dialogue acts. There is a relation between addressees
of an utterances and the type of the dialogue act the
speaker performed. Sometimes the description of a di-
alogue act includes the possible addressees of the act.
Therefore, knowledge about the dialog act is used as a cue
for addressee detection. For dialogue act annotation we
use the Meeting Recorder Dialogue Acts (MRDA) tag set
(Dhillon et al, 2003). The MRDA is a tag set for labeling
multiparty face-to-face meetings. The tags in the MRDA
set are organized into thirteen groups according to syn-
tactic, semantic, pragmatic and functional characteristic
of the utterance they mark. For addressee detection pur-
poses we used a large subset of the MRDA tag set but we
organized them at two levels: forward looking function
(FLF) and backward looking function (BLF). FLF rep-
resents the effect that an utterance has on the subsequent
interaction. BLF indicates how an utterance relates to the
previous discourse. If an utterance has both functions the
corresponding addressee is the addressee of the BLF.
When an utterance is marked with a BLF it is related
to one of the previous utterances in the conversation. The
addressee of the associated dialogue act in most cases is
the speaker of the related utterance. However, it is pos-
sible that the speaker of the related utterance is the same
as the current speaker. For instance, a speaker can repeat
or correct himself. The addressees of these utterances are
addressees of the related utterances. Most of the BLFs are
related to the previous utterances of the other speaker (ac-
ceptance tags, answer tags, etc.). In the multiparty case
there is a number of interesting interaction patterns with
respect to addressee identification.
Word classes Example Example
Personal pronouns PP I/me, you, she/her, he/him, we/us What do you think about that?
Quantifying determiners+PP all of you, some of you, few of you He saw some of you yesterday.
Numerals+PP two of you, three of you, last of you Three of you should prepare a presentation.
Indefinite pronouns+PP anyone of you, someone of you Did anyone of you finish the job?
Possessive pronouns mine, yours, hers, his, ours, theirs Is this yours?
Personal adjectives my, your, his, her, our, their I like your style.
Indefinite pronouns everybody, somebody, anyone Does anyone have any question?
Table 1: Linguistic markers
1.
A: We could use Java as a standard [suggestion]
B: I agree [accept]
C: No [reject]
D: For me, it is OK [accept]
2.
A: I think that we should use Java [suggestion]
B: I propose C++ [suggestion]
C: I don?t agree with you [reject]
In the first conversation all responses are related to A?s
proposal. Therefore, A is the addressee of the utterances
expressed by B, C and D. It means the addressee doesn?t
have to be the previous speaker. In the second example it
is not clear whether C rejects A?s or B?s proposal or both
proposals. Additional information obtained from visual
channels can help in resolving the addressee ambiguity.
Unlike BLFs, FLFs do not provide much information
about the addressee of a speaker?s utterance. Yet, some
assumptions about the utterance?s addressee are possible,
especially if we take in consideration the linguistic mark-
ers mentioned above. For instance, the speaker of an ut-
terance marked with some of the question tags directly
addresses the addressee to provide information. In com-
bination with the use of the personal pronoun we these
questions are addressed to a subgroup of participants or to
all participants rather than to a single person. Very often
questions in meeting discussions are open-ended ques-
tions. An open-ended question is a question that does
not seek a specific answer. Rather, it is asked in a broad
sense. An open-ended question is more likely addressed
to all meeting participants. If an open ended question
contains ?you? than a single person is the most probable
addressee (What about C? questions? What about you?).
Linguistic markers and dialogue acts described above
provide us with starting assumptions about the most
likely addressee. These assumptions are mostly related
to a size of the target group i.e. whether the addressee is
a single participant, a group of participant or all partic-
ipants. Therefore, some other communication channels
are used in combination with speech for addressing the
utterance. In the following sections we will describe the
role of non-verbal communication channels in addressee
detection.
3.1.2 Gaze direction
Gaze is an important aspect of social interaction (Ar-
gyle, 1973). One of the functions of gaze is channel-
control. Mutual gaze is important when people want to
establish relationship. Unless the utterance is very short
the speaker very soon breaks the mutual gaze. When fin-
ishing the utterance, the speaker gazes back to a listener.
If the speaker decided to continue to talk at turn transition
points, or even before, he usually gazes away. Need for
feedback effects the speaker?s gaze direction. Gaze direc-
tion shows a participant?s focus of attention. In the meet-
ing scenario where all participants are around the table
the focus of attention of the current speaker are mostly the
other meeting participants. Since it is almost impossible
to record eye gazing of participants, gaze information is
obtained and induced from head movements. In (Stiefel-
hagen and Zhu, 2002) it is shown that we can predict a
participant focus of attention based on head orientation
with a reliability of 88,7 %.
The contribution of gaze information to addressee de-
tection is dependent on the current meeting action (dis-
cussion, presentation, note-taking, etc.), the participants?
location and the utterance length. During a presentation a
speaker most probably addresses utterances to all meeting
participants. Therefore, information about gaze direction
is less relevant for a presentation than for a discussion
meeting action. When the utterance is short a speaker
usually gazes only at one participant or at no one, ad-
dressing the utterance at a group of people or at the whole
audience. Moreover, information about the visible areas
of the participants and hence the relative positions they
have in the meeting is relevant for interpreting the gaze
behavior in terms of focus of attention and it?s contribu-
tion to addressing. During a turn a speaker mostly looks
at the participant who are in his visible area. On the other
hand if he wants to address someone outside his visual
area he will often move his body towards the addressee.
The result of automatic or manual gaze annotation is
a list of gazed participants or objects, together with time
stamps. For the BLFs the first participant in the list is
of interest. If the participant is not in the speaker?s vis-
ible area then the gazed participant is the most likely
addressee of the speaker utterance. If the participant is
in the speaker?s visible area and he is a candidate from
the speech analysis then the likelihood that he is the ad-
dressee of the speaker utterance is greater. For the FLFs
utterance length and structure of the gaze list play a very
important role. For BLFs the last participant in the gazed
list is of interest.
3.1.3 Gesture
Pointing at a person is a way to address a speech act
to a person. It is usually accompanied with gazing at the
person. Still the addressee of a speaker?s utterance is not
necessarily the same as a person that the speaker points
at. When X talks to Y and points at Z, at the same time
X usually verbally refers to Z using a proper noun (name
of people, group name, etc.), a pronoun (he/she/they,
him/her/them, his/her/their, etc.) or using the role of par-
ticipant (boss, chairman, etc.). This means that X talks
to Y about Z. (Yesterday I met him on the street.)
3.1.4 Context
The categories of the context that contribute to ad-
dressee detection are: interaction history, meeting action
history, user context and spatial context. Interaction his-
tory is related to the conversation history and to the non-
verbal interaction history. Conversation history contains
the temporal sequence of speakers, performed dialogue
acts and their addressees. Meeting action history is a se-
quence of previous meeting actions including the current
meeting action. For instance, if a presentation is fol-
lowed by a discussion, the presenter is the more proba-
ble addressee of the other participants? utterances, espe-
cially those that are marked as questions. Spatial con-
text includes participants? location, locations of the en-
vironmental objects, distance between participants, par-
ticipants? visible area. User context includes participants
names, gender, social roles (status roles and closeness),
institutional roles etc.
3.2 Towards an automatic addressee detection
Although participants or outsiders are most of the time
quite sure about the intended addressee of a speaker this
knowledge is essentially error-prone. Using observa-
tional features obtained from different available sources
they can only predict the most probable addressee of an
utterance. Methods for addressee detection will either be
rule based or follow a statistical approach.
A rule-based algorithm used for computing addressee
in the MRE (Mission Rehearsal Exercise) project is
shown in (Traum, 2003). The rule-based method we in-
tend to apply for addressee identification first processes
information obtained from the utterance. This returns a
list of possible addressees with corresponding probabil-
ities. The probabilities are estimations from annotated
meeting data. The idea is first to eliminate cases where
the addressee is completely determined (names in voca-
tive forms, quantifiers and numerals in combination with
?you?, etc.). According to analysis of the relation between
dialogue acts and addressee, different sets of rules are ap-
plied for FLFs and BLFs. For instance, if an utterance
is marked with a BLF that is related to an utterance of a
previous speaker, the addressee is the speaker of the re-
lated utterance with probability P . The following steps
are related to the processing of information from addi-
tional sources (gaze and gesture) adding the additional
probability values to the possible addressee. Contextual
features are used at each level of processing.
Given all available multi-modal information E about
a conversational situation a statistical addressee identifi-
cation method should classify the addressee for each dia-
logue act in the conversation. As a computational model
we will use Bayesian networks (BN). The nodes in the
Bayesian network will include all observable features as
input variable and one unobservable output variable that
represent the addressee. From some preliminary models,
we concluded that Bayesian network used for addressee
classification of FLFs is more complicated than for BLFs.
We therefore consider using separate models for BLFs
and FLFs.
4 Conclusions and future directions
Addressing is an interesting aspect of communication
and the automatic identification of conversational roles in
multi-party dialogues is an open research problem. We
expect that statistical approaches can be applied at this
domain. Our future work will be based primarily on ob-
taining a huge set of data for training and testing the mod-
els. We will also define new scenario?s for new types of
meetings that will show more interesting phenomena re-
lated to addressing behavior.
References
Michael Argyle. 1973. Social Interaction. Tavistock
Publications.
H. Bunt. 2000. Dialogue pragmatics and context specifi-
cation. In Abduction, Belief and Context in Dialogue;
studies in computational pragmatics. John Benjamins,
Amsterdam.
Jean Carletta, Anne H. Anderson, and S. Garrod. 2002.
Seeing eye to eye: an account of grounding and un-
derstanding in work groups. Bulletin of the Japanese
cognitive sciences, 9(1):1?20.
Herbert H. Clark and Thomas B. Carlson. 1992. Hear-
ers and speech acts. In Arenas of Language Use
(H.H.Clark ed.). University of Chicago Press and
CSLI.
Herbert H. Clark and Edward F. Schaefer. 1992. Deal-
ing with overhearers. In Arenas of Language Use
(H.H.Clark ed.). University of Chicago Press and
CSLI.
R. Dhillon, S Bhagat, H Carvey, and E. Shriberg. 2003.
Meeting recorder project:dialogue act labeling guide,
version 3. Technical report, ICSI.
Rainer Stiefelhagen and Jie Zhu. 2002. Head orienta-
tion and gaze direction in meetings. In Conference on
Human Factors in Computing Systems (CHI2002).
David Traum. 2003. Issues in multi-party dialogues. In
Advances in Agent Communication (F. Dignum, ed.).
Springer-Verlag LNCS.
