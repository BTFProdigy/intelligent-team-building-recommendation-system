Proceedings of the EACL 2009 Student Research Workshop, pages 61?69,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
A Comparison of Merging Strategies for Translation of German
Compounds
Sara Stymne
Department of Computer and Information Science
Linko?ping University, Sweden
sarst@ida.liu.se
Abstract
In this article, compound processing for
translation into German in a factored sta-
tistical MT system is investigated. Com-
pounds are handled by splitting them prior
to training, and merging the parts after
translation. I have explored eight merging
strategies using different combinations of
external knowledge sources, such as word
lists, and internal sources that are carried
through the translation process, such as
symbols or parts-of-speech. I show that
for merging to be successful, some internal
knowledge source is needed. I also show
that an extra sequence model for part-of-
speech is useful in order to improve the
order of compound parts in the output.
The best merging results are achieved by a
matching scheme for part-of-speech tags.
1 Introduction
In German, as in many other languages, com-
pounds are normally written as single words with-
out spaces or other word boundaries. Compounds
can be binary, i.e., made up of two parts (1a), or
have more parts (1b). There are also coordinated
compound constructions (1c). In a few cases com-
pounds are written with a hyphen (1d), often when
one of the parts is a proper name or an abbrevia-
tion.
(1) a. Regierungskonferenz
intergovernmental conference
b. Fremdsprachenkenntnisse
knowledge of foreign languages
c. See- und Binnenha?fen
sea and inland ports
d. Kosovo-Konflikt
Kosovo conflict
e. Vo?lkermord
genocide
German compounds can have English trans-
lations that are compounds, written as separate
words (1a), other constructions, possibly with in-
serted function words and reordering (1b), or sin-
gle words (1e). Compound parts sometimes have
special compound forms, formed by addition or
truncations of letters, by umlaut or by a combi-
nation of these, as in (1a), where the letter -s is
added to the first part, Regierung. For an overview
of German compound forms, see Langer (1998).
Compounds are productive and common in Ger-
man and other Germanic languages, which makes
them problematic for many applications includ-
ing statistical machine translation. For translation
into a compounding language, fewer compounds
than in normal texts are often produced, which can
be due to the fact that the desired compounds are
missing in the training data, or that they have not
been aligned correctly. Where a compound is the
idiomatic word choice in the translation, aMT sys-
tem can instead produce separate words, genitive
or other alternative constructions, or only translate
one part of the compound.
The most common way to integrate compound
processing into statistical machine translation is to
split compounds prior to training and translation.
Splitting of compounds has received a lot of focus
in the literature, both for machine translation, and
targeted at other applications such as information
retrieval or speech recognition.
When translating into a compounding language
there is a need to merge the split compounds af-
ter translation. In order to do this we have to
identify which words that should be merged into
compounds, which is complicated by the fact that
the translation process is not guaranteed to pro-
duce translations where compound parts are kept
together.
61
In this article I explore the effects of merging in
a factored phrase-based statistical machine trans-
lation system. The system uses part-of-speech as
an output factor. This factor is used as a knowl-
edge source for merging and to improve word
order by using a part-of-speech (POS) sequence
model.
There are different knowledge sources for
merging. Some are external, such as frequency
lists of words, compounds, and compound parts,
that could be compiled at split-time. It is also
possible to have internal knowledge sources, that
are carried through the translation process, such
as symbols on compound parts, or part-of-speech
tags. Choices made at split-time influence which
internal knowledge sources are available at merge-
time. I will explore and compare three markup
schemes for compound parts, and eight merg-
ing algorithms that use different combinations of
knowledge sources.
2 Related Work
Splitting German compounds into their parts prior
to translation has been suggested by many re-
searchers. Koehn and Knight (2003) presented an
empirical splitting algorithm that is used to im-
prove translation from German to English. They
split all words in all possible places, and consid-
ered a splitting option valid if all the parts are ex-
isting words in a monolingual corpus. They al-
lowed the addition of -s or -es at all splitting
points. If there were several valid splitting options
they chose one based on the number of splits, the
geometric mean of part frequencies or based on
alignment data. Stymne (2008) extended this al-
gorithm in a number of ways, for instance by al-
lowing more compound forms. She found that for
translation into German, it was better to use the
arithmetic mean of part frequencies than the geo-
metric mean. Using the mean of frequencies can
result in no split, if the compound is more frequent
than its parts.
Merging has been much less explored than split-
ting since it is common only to discuss translation
from compounding languages. However, Popovic?
et al (2006) used merging for translation into Ger-
man. They did not mark compound parts in any
way, so the merging is based on two word lists,
with compound parts and full compounds found
at split-time. All words in the translation output
that were possible compound parts were merged
with the next word if it resulted in a known com-
pound. They only discussed merging of binary
compounds. The drawback of this method is that
novel compounds cannot be merged. Neverthe-
less, this strategy led to improved translation mea-
sured by three automatic metrics.
In a study of translation between English and
Swedish, Stymne and Holmqvist (2008) suggested
a merging algorithm based on part-of-speech,
which can be used in a factored translation sys-
tem with part-of-speech as an output factor. Com-
pound parts had special part-of-speech tags based
on the head of the compound, and merging was
performed if that part-of-speech tag matched that
of the following word. When compound forms
had been normalized the correct compound form
was found by using frequency lists of parts and
words compiled at split-time. This method can
merge unseen compounds, and the tendency to
merge too much is reduced by the restriction that
POS-tags need to match. In addition coordinated
compounds were handled by the algorithm. This
strategy resulted in improved scores on automatic
metrics, which were confirmed by an error analy-
sis.
Koehn et al (2008) discussed treatment of hy-
phened compounds in translation into German by
splitting at hyphens and treat the hyphen as a sep-
arate token, marked by a symbol. The impact on
translation results was small.
There are also other ways of using compound
processing to improve SMT into German. Popovic?
et al (2006) suggested using compound splitting
to improve alignment, or to merge English com-
pounds prior to training.
Some work has discussed merging of not only
compounds, but of all morphs. Virpioja et al
(2007) merged translation output that was split
into morphs for Finnish, Swedish and Danish.
They marked split parts with a symbol, and
merged every word in the output which had this
symbol with the next word. If morphs were
misplaced in the translation output, they were
merged anyway, possibly creating non-existent
words. This system was worse than the baseline
on Bleu (Papineni et al, 2002), but an error analy-
sis showed some improvements.
El-Kahlout and Oflazer (2006), discuss merg-
ing of morphs in Turkish. They also mark
morphs with a symbol, and in addition normal-
ize affixes to standard form. In the merging
62
phase, surface forms were generated following
morphographemic rules. They found that morphs
were often translated out of order, and that merg-
ing based purely on symbols gave bad results. To
reduce this risk, they constrained splitting to allow
only morphologically correct splits, and by group-
ing some morphemes. This lead to less ordering
problems in the translation output and gave im-
provements over the baseline.
Compound recombination have also been ap-
plied to German speech recognition, e.g. by
(Berton et al, 1996), who performed a lexical
search to extend the word graph that is output by
the speech recogniser.
3 Compound Processing
German compounds are split in the training data
and prior to translation. After translation, the parts
are merged to form full compounds. The knowl-
edge sources available to the merging process de-
pend on which information is carried through the
translation process.
The splitting algorithm of Stymne (2008) will
be used throughout this study. It is slightly mod-
ified such that only the 10 most common com-
pound forms from a corpus study of Langer (1998)
are allowed, and the hyphen in hyphened com-
pounds is treated as a compound form, analogous
to adding for instance the letter s to a part.
The annotation of compound parts influences
the merging process. Choices have to be made
concerning the form, markup and part-of-speech
of compound parts. For the form two options
have been considered, keeping the original com-
pound form, or normalizing it so that it coincides
with a normal word. Three types of marking have
been investigated, no marking at all (unmarked), a
marking symbol that is concatenated to all parts
but the last (marked), or using a separate sym-
bol between parts (sepmarked). The sepmarked
scheme has different symbols for parts of coordi-
nated compounds than for other compounds. Parts
are normalized in the unmarked and sepmarked
schemes, but left in their compound form in the
marked scheme, since the symbol separates them
from ordinary words in any case.
There is also the issue of which part-of-speech
tag to use for compound parts. The last part of the
compound, the head, always has the same part-of-
speech tag as the full compound. Two schemes
are explored for the other parts. For the marked
and unmarked system, a part-of-speech tag that is
derived from that of the last part of the word is
used. For the sepmarked scheme the most com-
mon part-of-speech tag of the part from the tagged
monolingual corpus is used.
In summary, the three markup schemes use the
following combinations, exemplified by the result
of splitting the word begru??enswert (welcome, lit-
erally worth to welcome)
? Unmarked: no symbol, normalization, spe-
cial POS-tags
begru??en ADJ-PART wert ADJ
? Marked: symbol on parts, no normalization,
special POS-tags
begru??ens# ADJ-PART wert ADJ
? Sepmarked: symbol as separate token, nor-
malization, ordinary POS-tags
begru??en VV @#@ COMP wert ADJ
3.1 Merging
There is no guarantee that compound parts appear
in a correct context in the translation output. This
fact complicates merging, since there is a general
choice between only merging those words that we
know are compounds, and merging all occurrences
of compound parts, which will merge unseen com-
pounds, but probably also merge parts that do not
form well-formed compounds. There is also the
issue of parts possibly being part of coordinated
compounds.
The internal knowledge sources that can be used
for merging depends on the markup scheme used.
The available internal sources are markup sym-
bols, part-of-speech tags, and the special tags for
compound parts. The external resources are fre-
quency lists of words, compounds and parts, pos-
sibly with normalization, compiled at split-time.
For the unmarked and sepmarked scheme, re-
verse normalization, i.e., mapping normalized
compound parts into correct compound forms, has
to be applied in connection with merging. As in
Stymne and Holmqvist (2008), all combinations
of compound forms that are known for each part
are looked up in the word frequency list, and the
most frequent combination is chosen. If there are
no known combinations, the parts are combined
from left to right, at each step choosing the most
frequent combination.
Three main types of merging algorithms are in-
vestigated in this study. The first group, inspired
63
Name Description
word-list Merges all tokens that have been seen as compound parts with the next part if it results in a known
word, from the training corpus
word-list + head-pos As word-list, but only merges words where the last part is a noun, adjective or verb
compound-list As word-list, but for known compounds from split-time, not for all known words
symbol Merges all tokens that are marked with the next token
symbol + head-pos As symbol, but only merges words where the last part is a noun, adjective or verb
symbol + word-list A mix of symbol and word-list, where marked compounds are merged, if it results in a known word
POS-match Merges all tokens with a compound part-of-speech tag, if the tag match the tag of the next token
POS-match + coord As POS-match, but also adds a hyphen to parts that are followed by the conjunction und (and)
Table 1: Merging algorithms
by Popovic? et al (2006), is based only on exter-
nal knowledge sources, frequency lists of words
or compounds, and of parts, compiled at split-
time. Novel compounds cannot be merged by
these algorithms. The second group uses sym-
bols to guide merging, inspired by work on mor-
phology merging (Virpioja et al, 2007). In the
unmarked scheme where compound parts are not
marked with symbols, the special POS-tags are
used to identify parts instead1. The third group
is based on special part-of-speech tags for com-
pounds (Stymne and Holmqvist, 2008), and merg-
ing is performed if the part-of-speech tags match.
This group of algorithms cannot be applied to the
sepmarked scheme.
In addition a restriction that the head of the
compound should have a compounding part-of-
speech, that is, a noun, adjective, or verb, and a
rule to handle coordinated compounds are used.
By using these additions and combinations of the
main algorithms, a total of eight algorithms are ex-
plored, as summarized in Table 1. For all algo-
rithms, compounds can have an arbitrary number
of parts.
If there is a marked compound part that cannot
be combined with the next word, in any of the al-
gorithms, the markup is removed, and the part is
left as a single word. For the sepmarked system,
coordinated compounds are handled as part of the
symbol algorithms, by using the special markup
symbol that indicates them.
3.2 Merging Performance
To give an idea of the potential of the merging al-
gorithms, they are evaluated on the split test refer-
ence corpus, using the unmarked scheme. The cor-
pus has 55580 words, of which 4472 are identified
as compounds by the splitting algorithm. Of these
4160 are known from the corpus, 245 are novel,
1For the marked scheme using POS-tags to identify com-
pound parts is equivalent to using symbols.
and 67 are coordinated. For the methods based
on symbols or part-of-speech, this merging task is
trivial, except for reverse normalization, since all
parts are correctly ordered.
Table 2 shows the number of errors. The POS-
match algorithm with treatment of coordination
makes 55 errors, 4 of which are due to coordinated
compounds that does not use und as the conjunc-
tion. The other errors are due to errors in the re-
verse normalization of novel compounds, which
has an accuracy of 79% on this text. The POS-
match and symbol algorithms make additional er-
rors on coordinated compounds. The head-pos
restriction blocks compounds with an adverb as
head, which gave better results on translation data,
but increased the errors on this evaluation. The
word list method both merges many words that
are not compounds, and do not merge any novel
compounds. Using a list of compounds instead of
words reduces the errors slightly.
4 System Description
The translation system used is a factored phrase-
based translation system. In a factored transla-
tion model other factors than surface form can
be used, such as lemma or part-of-speech (Koehn
and Hoang, 2007). In the current system part-of-
speech is used only as an output factor in the target
language. Besides the standard language model a
sequence model on part-of-speech is used, which
can be expected to lead to better word order in the
translation output. There are no input factors, so
no tagging has to be performed prior to translation,
only the training corpus needs to be tagged. In ad-
dition, the computational overhead is small. One
possible benefit gained by using part-of-speech as
an output factor is that ordering, both in general,
and of compound parts, can be improved. This hy-
pothesis is tested by trying two system setups, with
and without the part-of-speech sequence model.
In addition part-of-speech is used for postprocess-
64
wlist wlist+head-pos clist symbol symbol+head-pos symbol+wlist POS-match POS-match+coord
2393 1656 2257 118 205 330 118 55
Table 2: Number of merging errors on the split reference corpus
Tokens Types
English baseline 15158429 63692
German
baseline 14356051 184215
marked 15674728 93746
unmarked 15674728 81806
sepmarked 17007929 81808
Table 3: Type and token counts for the 701157
sentence training corpus
ing, both for uppercasing German nouns and as a
knowledge source for compound merging.
The tools used are the Moses toolkit (Koehn et
al., 2007) for decoding and training, GIZA++ for
word alignment (Och and Ney, 2003), and SRILM
(Stolcke, 2002) for language models. A 5-gram
model is used for surface form, and a 7-gram
model is used for part-of-speech. To tune feature
weights minimum error rate training is used (Och,
2003), optimized against the Neva metric (Fors-
bom, 2003). Compound splitting is performed on
the training corpus, prior to training. Merging is
performed after translation, both for test, and in-
corporated into the tuning step.
4.1 Corpus
The system is trained and tested on the Europarl
corpus (Koehn, 2005). The training corpus is fil-
tered to remove sentences longer than 40 words
and with a length ratio of more than 1 to 7. The fil-
tered training corpus contains 701157 sentences.
500 sentences are used for tuning and 2000 sen-
tences for testing2. The German side of the train-
ing corpus is part-of-speech tagged using TreeTag-
ger (Schmid, 1994).
The German corpus has nearly three times as
many types, i.e., unique tokens, as the English cor-
pus despite having a somewhat lower token count,
as shown for the training corpus in Table 3. Com-
pound splitting drastically reduces the number of
types, to around half or less, even though it is still
larger than for English. Marking on parts gives
15% more types than no marking.
2The test set is test2007 from the ACL 2008 Workshop on
Statistical Machine Translation, http://www.statmt.
org/wmt08/shared-task.html
5 Evaluation
Two types of evaluation are performed. The in-
fluence of the different merging algorithms on the
overall translation quality is evaluated, using two
automatic metrics. In addition the performance
of the merging algorithms are analysed in some
more detail. In both cases the effect of the POS
sequence model is also discussed. Even when the
POS sequence model is not used, part-of-speech
is carried through the translation process, so that it
can be used in the merging step.
5.1 Evaluation of Translation
Translations are evaluated on two automatic met-
rics: Bleu (Papineni et al, 2002) and PER, posi-
tion independent error-rate (Tillmann et al, 1997).
Case-sensitive versions of the metrics are used.
PER does not consider word order, it evaluates
the translation as a bag-of-word, and thus the sys-
tems without part-of-speech sequence models can
be expected to do well on PER. Note that PER is
an error-rate, so lower scores are better, whereas
higher scores are better for Bleu.
These metrics have disadvantages, for instance
because the same weight is given to all tokens,
both to complex compounds, and to function
words such as und (and). Bleu has been criticized,
see e.g. (Callison-Burch et al, 2006; Chiang et al,
2008).
Table 4 and 5 shows the translation results using
the different merging algorithms. For the systems
with POS sequence models the baseline performs
slightly better on Bleu, than the best systems with
merging. Without the POS sequence model, how-
ever, merging often leads to improvements, by up
to 0.48 Bleu points. For all systems it is advanta-
geous to use the POS sequence model.
For the baseline, the PER scores are higher
for the system without a POS sequence model,
which, compared to the Bleu scores, confirms
the fact that word order is improved by the se-
quence model. The systems with merging are
better than the baseline with the POS sequence
model. In all cases, however, the systems with
merging performs worse when not using a POS
sequence model, indicating that the part-of-speech
65
with POS-model without POS-model
unmarked sepmarked marked unmarked sepmarked marked
word-list 17.93 17.66 18.92 17.70 17.29 18.69
word-list + head-pos 19.34 19.07 19.60 19.13 18.63 19.38
compound-list 18.94 17.77 18.13 18.56 17.40 17.86
symbol 20.02 19.57 20.03 19.66 19.14 19.79
symbol + head-pos 20.02 19.55 20.01 19.75 19.12 19.78
symbol + word-list 20.03 19.72 20.02 19.76 19.29 19.79
POS-match 20.12 ? 20.03 19.84 ? 19.80
POS-match + coord 20.10 ? 19.97 19.85 ? 19.80
Table 4: Translation results for Bleu. Baseline with POS: 20.19, without POS: 19.66. Results that are
better than the baseline are marked with bold face.
with POS-model without POS-model
unmarked sepmarked marked unmarked sepmarked marked
word-list 29.88 28.64 28.19 30.27 29.94 28.71
word-list + head-pos 27.49 26.07 27.26 27.78 27.22 27.84
compound-list 26.92 27.99 29.25 27.46 29.07 29.74
symbol 27.21 26.13 26.95 27.70 27.40 27.61
symbol + head-pos 27.11 26.10 26.92 27.34 27.35 27.54
symbol + word-list 26.86 25.54 26.80 27.15 26.72 27.39
POS-match 26.99 ? 26.93 27.17 ? 27.53
POS-match + coord 27.10 ? 26.93 27.28 ? 27.53
Table 5: Translation results for PER. Baseline with POS: 27.22, without POS: 26.49. Results that are
better than the baseline are marked with bold face.
sequence model improves the order of compound
parts.
When measured by PER, the best results when
using merging are achieved by combining sym-
bols and word lists, but when measured by Bleu,
the POS-based algorithms are best. The simpler
symbol-based methods, often have similar scores,
and in a few cases even better. Adding treatment
of coordinated compounds to the POS-match al-
gorithm changes scores marginally in both direc-
tions. The word list based methods, however, gen-
erally give bad results. Using the head-pos restric-
tion improves it somewhat and using a compound
list instead of a word list gives different results in
the different markup schemes, but is still worse
than the best systems. This shows that some kind
of internal knowledge source, either symbols or
part-of-speech, is needed in order for merging to
be successful.
On both metrics, the marked and unmarked sys-
tem perform similarly. They are better than the
sepmarked system on Bleu, but the sepmarked sys-
tem is a lot better on PER, which is an indication
of that word order is problematic in the sepmarked
system, with its separate tokens to indicate com-
pounds.
5.2 Evaluation of Merging
The results of the different merging algorithms are
analysed to find the number of merges and the type
and quality of the merges. In addition I investigate
the effect of using a part-of-speech model on the
merging process.
Table 6 shows the reduction of words3 achieved
by applying the different algorithms. The word
list based method produces the highest number
of merges in all cases, performing many merges
where the parts are not recognized as such by the
system. The number of merges is greatly reduced
by the head-pos restriction. An investigation of the
output of the word list based method shows that
it often merges common words that incidentally
form a new word, such as bei (at) and der (the)
to beider (both). Another type of error is due to
errors in the corpus, such as the merge of umwelt
(environment) and und (and), which occurs in the
corpus, but is not a correct German word. These
two error types are often prohibited by the head-
pos restrictions. The compound list method avoids
these errors, but it does not merge compounds that
were not split by the splitting algorithm, due to a
high frequency, giving a very low number of splits
in some cases. There are small differences be-
tween the POS-match and symbol algorithms. Not
using the POS sequence model results in a higher
number of merges for all systems.
A more detailed analysis was performed of the
3The reduction of words is higher than the number of pro-
duced compounds, since each compound can have more than
two parts.
66
with POS-model without POS-model
unmarked sepmarked marked unmarked sepmarked marked
word-list 5275 5422 4866 5897 5589 5231
word-list + head-pos 4161 4412 4338 4752 4601 4661
compound-list 4460 4669 3253 5116 4850 3534
symbol 4431 4712 4332 5144 4968 4702
symbol + head-pos 4323 4671 4279 4832 4899 4594
symbol + word-list 4178 4436 4198 4753 4656 4530
POS-match 4363 ? 4310 4867 ? 4618
POS-match + coord 4361 ? 4310 4865 ? 4618
Table 6: Reduction of number of words by using different merging algorithms
with POS-model without POS-model
unmarked sepmarked marked unmarked sepmarked marked
Known 3339 3594 3375 3747 3762 3587
Novel
Good 168 176 105 104 245 93
Bad 20 97 8 10 64 7
Coordinated
Good 43 43 42 42 37 44
Bad 9 9 3 22 7 5
Single part
Good 6 ? 5 136 ? 33
Bad 11 ? 16 52 ? 46
Total 3596 3919 3554 4113 4115 3815
Table 7: Analysis of merged compounds
compounds parts in the output. The result of merg-
ing them are classified into four groups: merged
compounds that are known from the training cor-
pus (2a) or that are novel (2b), parts that were
not merged (2c), and parts of coordinated com-
pounds (2d). They are classified as bad if the com-
pound/part should have been merged with the next
word, does not fit into its context, or has the wrong
form.
(2) a. Naturschutzpolitik
nature protection policy
b. UN-Friedensplan
UN peace plan
c. * West- zulassen
west allow
d. Mittel- und Osteuropa
Central and Eastern Europe
For the unmarked and sepmarked systems, the
classification was based on the POS-match con-
straint, where parts are not merged if the POS-tags
do not match. POS-match cannot be used for the
sepmarked scheme, which has standard POS-tags.
Table 7 shows the results of this analysis. The
majority of the merged compounds are known
from the training corpus for all systems. There
is a marked difference between the two systems
that use POS-match, and the sepmarked system
that does not. The sepmarked system found the
highest number of novel compounds, but also have
the highest error rate for these, which shows that
it is useful to match POS-tags. The other two sys-
tems find fewer novel compounds, but also make
fewer mistakes. The marked system has more er-
rors for single parts than the other systems, mainly
beacuse the form of compound parts were not nor-
malized. Very few errors are due to reverse nor-
malization. In the unmarked system with a POS
sequence model, there were only three such errors,
which is better than the results on split data in Sec-
tion 3.2.
Generally the percentage of bad parts or com-
pounds is lower for the systems with a POS se-
quence model, which shows that the sequence
model is useful for the ordering of compound
parts. The number of single compound parts is
also much higher for the systems without a POS
sequence model. 80% of the merged compounds
in the unmarked system are binary, i.e., have two
parts, and the highest number of parts in a com-
pound is 5. The pattern for the other systems is
similar.
All systems produce fewer compounds than the
4472 in the German reference text. However, there
might also be compounds in the output, that were
not split and merged. These numbers are not di-
rectly comparable to the baseline system, and ap-
plying the POS-based splitting algorithm to trans-
lation output would not give a fair comparison.
An indication of the number of compounds in a
text is the number of long words. In the reference
text there are 351 words with at least 20 characters,
67
which will be used as the limit for long words. A
manual analysis showed that all these words are
compounds. The baseline system produces 209
long words. The systems with merging, discussed
above, all produce more long words than the base-
line, but less than the reference, between 263 and
307, with the highest number in the marked sys-
tem. The trend is the same for the systems with-
out a POS sequence model, but with slightly fewer
long words than for the systems with merging.
6 Discussion
The choice of merging method has a large impact
on the final translation result. For merging to be
successful some internal knowledge source, such
as part-of-speech or symbols is needed. The pure
word list based method performed the worst of
all systems on both metrics in most cases, which
was not surprising, considering the evaluation of
the merging algorithms on split data, where it was
shown that the word-list based methods merged
many parts that were not compounds.
The combination of symbols and word lists gave
good results on the automatic metrics. An advan-
tage of this method is that it is applicable for trans-
lation systems that do not use factors. However,
it has the drawback that it does not merge novel
compounds, and finds fewer compounds than most
other algorithms. The error analysis shows that
many valid compounds are discarded by this algo-
rithm. A method that both find novel compounds,
and that works well is that based on POS-match.
In its current form it needs a decoder that can han-
dle factored translation models. It would, how-
ever, be possible to use more elaborate symbols
with part-of-speech information, which would al-
low a POS-matching scheme, without the need of
factors.
The error analysis of merging performance
showed that merging works well, especially for
the two schemes where POS-matching is possi-
ble, where the proportion of errors is low. It
also showed that using a part-of-speech sequence
model was useful in order to get good results,
specifically since it increased the number of com-
pound parts that were placed correctly in the trans-
lation output.
The sepmarked scheme is best on the PER met-
ric it is worse on Bleu, and the error analysis
shows that it performs worse on merging than the
other systems. This could probably be improved
by the use of special POS-tags and POS-matching
for this scheme as well. It is hard to judge which
is best of the unmarked and marked scheme. They
perform similarly on the metrics, and there is no
clear difference in the error analysis. The un-
marked scheme does produce a somewhat higher
number of novel compounds, though. A disadvan-
tage of the marked scheme is that the compound
form is kept for single parts. A solution for this
could be to normalize parts in this scheme as well,
which could improve performance, since reverse
normalization performance is good on translation
data.
The systems with splitting and merging have
more long words than the baseline, which indi-
cates that they are more successful in creating
compounds. However, they still have fewer long
words than the reference text, indicating the need
of more work on producing compounds.
7 Conclusion and Future Work
In this study I have shown that the strategy used
for merging German compound parts in transla-
tion output influences translation results to a large
extent. For merging to be successful, it needs
some internal knowledge source, carried through
the translation process, such as symbols or part-
of- speech. The overall best results were achieved
by using matching for part-of-speech.
One factor that affects merging, which was not
explored in this work, is the quality of splitting.
If splitting produces less erroneously split com-
pounds than the current method, it is possible
that merging also can produce better results, even
though it was not clear from the error analysis that
bad splits were a problem. A number of more ac-
curate splitting strategies have been suggested for
different tasks, see e.g. Alfonseca et al (2008),
that could be explored in combination with merg-
ing for machine translation.
I have compared the performance of different
merging strategies in one language, German. It
would be interesting to investigate these meth-
ods for other compounding languages as well. I
also want to explore translation between two com-
pounding languages, where splitting and merging
would be performed on both languages, not only
on one language as in this study.
68
References
Enrique Alfonseca, Slaven Bilac, and Stefan Phar-
ies. 2008. Decompounding query keywords from
compounding languages. In Proceedings of ACL-
08: HLT, Short Papers, pages 253?256, Columbus,
Ohio.
Andre? Berton, Pablo Fetter, and Peter Regel-
Brietzmann. 1996. Compound words in large-
vocabulary German speech recognition systems. In
Proceedings of the Fourth International Conference
on Spoken Language Processing (ICSLP), pages
1165?1168, Philadelphia, Pennsylvania, USA.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in
machine translation research. In Proceedings of the
11th Conference of EACL, pages 249?256, Trento,
Italy.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 610?619, Honolulu, Hawaii.
I?lknur Durgar El-Kahlout and Kemal Oflazer. 2006.
Initial explorations in English to Turkish statistical
machine translation. In HLT-NAACL 2006: Pro-
ceedings of the Workshop on Statistical Machine
Translation, pages 7?14, New York, NY.
Eva Forsbom. 2003. Training a super model look-
alike: featuring edit distance, n-gram occurrence,
and one reference translation. In Proceedings of
the Workshop on Machine Translation Evaluation:
Towards Systemizing MT Evaluation, pages 29?36,
New Orleans, Louisiana.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural language
Processing and Computational Natural Language
Learning, pages 868?876, Prague, Czech Republic.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of EACL, pages 187?193, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
demonstration session, Prague, Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In Proceedings of
MT Summit X, Phuket, Thailand.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung natu?rlicher Sprache
(KONVENS), pages 83?97, Bonn, Germany.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of German compound
words. In Proceedings of FinTAL ? 5th Interna-
tional Conference on Natural Language Process-
ing, pages 616?624, Turku, Finland. Springer Ver-
lag, LNCS.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing (ICSLP), pages 901?904, Denver, Col-
orado.
Sara Stymne and Maria Holmqvist. 2008. Process-
ing of Swedish compounds for phrase-based statis-
tical machine translation. In Proceedings of the Eu-
ropean Machine Translation Conference (EAMT08),
pages 180?189, Hamburg, Germany.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Aarne Ranta and
Bengt Nordstro?m, editors, Proceedings of GoTAL ?
6th International Conference on Natural Language
Processing, pages 464?475, Gothenburg, Sweden.
Springer Verlag, LNCS/LNAI.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. In Proceedings of the 5 th Eu-
ropean Conference on Speech Communication and
Technology, pages 2667?2670, Rhodes, Greece.
Sami Virpioja, Jaako J.Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware sta-
tistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings of
MT Summit XI, pages 491?498, Copenhagen, Den-
mark.
69
Proceedings of the Second Workshop on Statistical Machine Translation, pages 181?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Getting to know Moses:  
Initial experiments on German?English factored translation  
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg 
Department of Computer and Information Science 
Link?pings universitet, Sweden 
{marho,sarst,lah}@ida.liu.se 
 
 
 
Abstract 
We present results and experiences from 
our experiments with phrase-based statisti-
cal machine translation using Moses. The 
paper is based on the idea of using an off-
the-shelf parser to supply linguistic infor-
mation to a factored translation model and 
compare the results of German?English 
translation to the shared task baseline sys-
tem based on word form. We report partial 
results for this model and results for two 
simplified setups. Our best setup takes ad-
vantage of the parser?s lemmatization and 
decompounding. A qualitative analysis of 
compound translation shows that decom-
pounding improves translation quality. 
1 Introduction  
One of the stated goals for the shared task of this 
workshop is ?to offer newcomers a smooth start 
with hands-on experience in state-of-the-art statis-
tical machine translation methods?. As our previ-
ous research in machine translation has been 
mainly concerned with rule-based methods, we 
jumped at this offer. 
We chose to work on German-to-English trans-
lation for two reasons. Our primary practical inter-
est lies with translation between Swedish and Eng-
lish, and of the languages offered for the shared 
task, German is the one closest in structure to 
Swedish. While there are differences in word order 
and morphology between Swedish and German, 
there are also similarities, e.g., that both languages 
represent nominal compounds as single ortho-
graphic words. We chose the direction from Ger-
man to English because our knowledge of English 
is better than our knowledge of German, making it 
easier to judge the quality of translation output. 
Experiments were performed on the Europarl data. 
With factored statistical machine translation, 
different levels of linguistic information can be 
taken into account during training of a statistical 
translation system and decoding. In our experi-
ments we combined syntactic and morphological 
factors from an off-the-shelf parser with the fac-
tored translation framework in Moses (Moses, 
2007). We wanted to test the following hypotheses: 
? Translation models based on lemmas will im-
prove translation quality (Popovi? and Ney, 
2004) 
? Decompounding German nominal compounds 
will improve translation quality (Koehn and 
Knight, 2003) 
? Re-ordering models based on word forms and 
parts-of-speech will improve translation qual-
ity (Zens and Ney, 2006). 
2 The parser 
The parser, Machinese Syntax, is a commercially 
available dependency parser from Connexor Oy 1. 
It provides each word with lemma, part-of-speech, 
morphological features and dependency relations 
(see Figure 1). In addition, the lemmas of com-
pounds are marked by a ?#? separating the two 
parts of the compound. For the shared task we only 
used shallow linguistic information: lemma, part-
of-speech and morphology. The compound bound-
ary identification was used to split noun com-
                                                 
1 Connexor Oy, http://www.connexor.com. 
181
pounds to make the German input more similar to 
English text. 
 
1 Mit   mit   pm>2    @PREMARK PREP 
2 Blick blick advl>10 @NH N MSC SG DAT 
3 auf   auf   pm>5    @PREMARK PREP 
 
Figure 1. Example of parser output 
 
We used the parser?s tokenization as given. Some 
common multiword units, such as ?at all? and ?von 
heute?, are treated as single words by the parser 
(cf. Niessen and Ney, 2004). The German parser 
also splits contracted prepositions and determiners 
like ?zum? ? ?zu dem? (?to the?). 
3 System description 
For our experiments with Moses we basically fol-
lowed the shared task baseline system setup to 
train our factored translation models. After training 
a statistical model, minimum error-rate tuning was 
performed to tune the model parameters. All ex-
periments were performed on an AMD 64 Athlon 
4000+ processor with 4 Gb of RAM and 32 bit 
Linux (Ubuntu).  
Since time as well as computer resources were 
limited we designed a model that we hoped would 
make the best use of all available factors. This 
model turned out to be too complex for our ma-
chine and in later experiments we abandoned it for 
a simpler model.  
3.1 Pre-processing 
In the pre-processing step we used the standard 
pre-processing of the shared task baseline system, 
parsed the German and English texts and processed 
the output to obtain four factors: word form, 
lemma, part-of-speech and morphology. Missing 
values for lemma, part-of-speech and morphology 
were replaced with default values. 
Noun compounds are very frequent in German, 
2.9% of all tokens in the tuning corpus were identi-
fied by the parser as noun compounds. Compounds 
tend to lead to sparse data problems and splitting 
them has been shown to improve German-English 
translation (Koehn and Knight, 2003). Thus we 
decided to decompund German noun compounds 
identified as such by our parser.  
We used a simple strategy to remove fillers and 
to correct some obvious mistakes. We removed the 
filler ?-s? that appear before a marked split unless it 
was one of ?-ss?, ?-urs?, ?-eis? or ?-us?. This applied 
to 35% of the noun compounds in the tuning cor-
pus. The fillers were removed both in the word 
form and the lemma (see Figure 2). 
There were some mistakes made by the parser, 
for instance on compounds containing the word 
?nahmen? which was incorrectly split as ?stel-
lungn#ahmen? instead of ?stellung#nahmen? 
(?statement?). These splits were corrected by mov-
ing the ?n? to the right side of the split. 
We then split noun-lemmas on hyphens unless 
there were numbers on either side of it and on the 
places marked by ?#?. Word forms were split in the 
corresponding places as the lemmas. 
The part-of-speech and morphology of the last 
word in the compound is the same as for the whole 
compound. For the other parts we hypothesized 
that part-of-speech is Noun and the morphology is 
unknown, marked by the tag UNK. 
 
Parser output: 
unionsl?nder unions#land N NEU PL ACC 
 
Factored output: 
union|union|N|UNK 
l?nder|land|N|NEU_PL_ACC 
 
Figure 2. Compound splitting for ?unionsl?nder? 
(?countries in the union?) 
 
These strategies are quite crude and could be fur-
ther refined by studying the parser output thor-
oughly to pinpoint more problems.  
3.2 Training translation models with linguis-
tic factors 
After pre-processing, the German?English Eu-
roparl training data contains four factors: 0: word 
form, 1: lemma, 2: part-of-speech, 3: morphology. 
As a first step in training our translation models we 
performed word alignment on lemmas as this could 
potentially improve word alignment. 
3.2.1 First setup 
Factored translation requires a number of decoding 
steps, which are either mapping steps mapping a 
source factor to a target factor or generation steps 
generating a target factor from other target factors. 
Our first setup contained three mapping steps, T0?
T2, and one generation step, G0.  
 
 
182
T0: 0-0 (word ? word) 
T1: 1-1 (lemma ? lemma) 
T2: 2,3-2,3  (pos+morph ? pos+morph) 
G0:  1,2,3-0  (lemma+pos+morph ? word)  
 
With the generation step, word forms that did not 
appear in the training data may still get translated 
if the lemma, part-of-speech and morphology can 
be translated separately and the target word form 
can be generated from these factors. 
Word order varies a great deal between German 
and English. This is especially true for the place-
ment of verbs. To model word order changes we 
included part-of-speech information and created 
two reordering models, one based on word form 
(0), the other on part-of-speech (2): 
 
0-0.msd-bidirectional-fe 
2-2.msd-bidirectional-fe 
 
The decoding times for this setup turned out to be 
unmanageable. In the first iteration of parameter 
tuning, decoding times were approx. 6 
min/sentence. In the second iteration decoding 
time increased to approx. 30 min/sentence.  Re-
moving one of the reordering models did not result 
in a significant change in decoding time. Just trans-
lating the 2000 sentences of test data with untuned 
parameters would take several days. We inter-
rupted the tuning and abandoned this setup. 
3.2.2 Second setup 
Because of the excessive decoding times of the 
first factored setup we resorted to a simpler system 
that only used the word form factor for the transla-
tion and reordering models. This setup differs from 
the shared task baseline in the following ways: 
First, it uses the tokenization provided by the 
parser. Second, alignment was performed on the 
lemma factor. Third, German compounds were 
split using the method described above. To speed 
up tuning and decoding, we only used the first 200 
sentences of development data (dev2006) for tun-
ing and reduced stack size to 50.  
 
T0: 0-0 (word ? word) 
R:  0-0.msd-bidirectional-fe 
3.2.3 Third setup 
To test our hypothesis that word reordering would 
benefit from part-of-speech information we created 
another simpler model. This setup has two map-
ping steps, T0 and T1, and a reordering model 
based on part-of-speech.  
 
T0: 0-0 (word ? word) 
T1: 2,3-2,3 (pos+morph ? pos+morph) 
R: 2-2.msd-bidirectional-fe 
4 Results  
We compared our systems to a baseline system 
with the same setup as the WMT2007 shared task 
baseline system but tuned with our system?s sim-
plified tuning settings (200 instead of 2000 tuning 
sentences, stack size 50). Table 1 shows the Bleu 
improvement on the 200 sentences development 
data from the first and last iteration of tuning. 
 
Dev2006 (200) System 
1st iteration Last iteration 
Baseline 19.56 27.07 
First 21.68 - 
Second 20.43 27.16 
Third 20.72 24.72 
 Table 1. Bleu scores on 200 sentences of tuning 
data before and after tuning 
 
The final test of our systems was performed on the 
development test corpus (devtest2006) using stack 
size 50. The results are shown in Table 2. The low 
Bleu score for the third setup implies that reorder-
ing on part-of-speech is not enough on its own. 
The second setup performed best with a slightly 
higher Bleu score than the baseline. We used the 
second setup to translate test data for our submis-
sion to the shared task.  
 
System Devtest2006 (NIST/Bleu) 
Baseline 6.7415 / 25.94  
First - 
Second  6.8036 / 26.04 
Third 6.5504 / 24.57 
Table 2. NIST and Bleu scores on development 
test data 
4.1 Decompounding 
We have evaluated the decompounding strategy by 
analyzing how the first 75 identified noun com-
pounds of the devtest corpus were translated by our 
second setup compared to the baseline. The sample 
183
excluded doubles and compounds that had no clear 
translation in the reference corpus.  
Out of these 75 compounds 74 were nouns that 
were correctly split and 1 was an adjective that was 
split incorrectly: ?allumfass#ende?. Despite that it 
was incorrectly identified and split it was trans-
lated satisfyingly to ?comprehensive?. 
The translations were grouped into the catego-
ries shown in Table 3. The 75 compounds were 
classified into these categories for our second sys-
tem and the baseline system, as shown in Table 4. 
As can be seen the compounds were handled better 
by our system, which had 62 acceptable transla-
tions (C or V) compared to 48 for the baseline and 
did not leave any noun compounds untranslated.  
 
Table 3. Classification scheme with examples for 
compound translations 
 
Table 4. Classification of 75 compounds from our 
second system and the baseline system 
Decompounding of nouns reduced the number 
of untranslated words, but there were still some 
left. Among these were cases that can be handled 
such as separable prefix verbs like ?aufzeigten? 
(?pointed out?) (Niessen and Ney, 2000) or adjec-
tive compounds such as ?multidimensionale? 
(?multi dimensional?). There were also some noun 
compounds left which indicates that we might need 
a better decompounding strategy than the one used 
by the parser (see e.g. Koehn and Knight, 2003). 
4.2 Experiences and future plans  
With the computer equipment at our disposal, 
training of the models and tuning of the parameters 
turned out to be a very time-consuming task. For 
this reason, the number of system setups we could 
test was small, and much fewer than we had hoped 
for. Thus it is too early to draw any conclusions as 
regards our hypotheses, but we plan to perform 
more tests in the future, also on Swedish?English 
data. The parser's ability to identify compounds 
that can be split before training seems to give a 
definite improvement, however, and is a feature 
that can likely be exploited also for Swedish-to-
English translation with Moses. 
References 
Koehn, Philipp and Kevin Knight, 2003. Empirical 
methods for compound splitting. In Proceedings of 
EACL 2003, 187-194. Budapest, Hungary. 
Moses ? a factored phrase-based beam-search decoder 
for machine translation. 13 April 2007,  URL:  
http://www.statmt.org/moses/ . 
Niessen, Sonja and Hermann Ney, 2004. Statistical ma-
chine translation with scarce resources using mor-
pho-syntactic information. Computational Linguis-
tics, 181-204 . 
Niessen, Sonja and Hermann Ney, 2000. Improving 
SMT Quality with Morpho-syntactic Analysis. In 
Proceedings of Coling 2000. 1081-1085. Saar-
br?cken, Germany.  
Popovi?, Maja and Hermann Ney, 2004.  Improving 
Word Alignment Quality using Morpho-Syntactic In-
formation. In Proceedings of Coling 2004, 310-314, 
Geneva, Switzerland. 
Zens, Richard and Hermann Ney, 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In HLT-NAACL: Proceedings of the Workshop 
on Statistical Machine Translation, 55-63, New York 
City, NY.  
Category Example 
C-correct Regelungsentwurf 
Draft regulation  
Ref: Draft regulation 
V-variant Schlachth?fen 
Abattoirs  
Ref: Slaughter houses  
P-partly correct Anpassungsdruck 
Pressure 
Ref: Pressure for adaption 
F-wrong form L?nderberichte 
Country report  
Ref: Country reports 
W-wrong Erbonkel 
Uncle dna  
Ref: Sugar daddy 
U-untranslated Schlussentwurf 
Schlussentwurf  
Ref: Final draft  
Baseline system 
 C V P W U F Tot 
C 36 1 3  3 1 44 
V 1 9 2 1 5  18 
P   3  2  5 
W    1 2  3 
U       0 
F 1     4 5 S
ec
on
d 
sy
st
em
 
Tot 38 10 8 2 12 5 75 
184
Proceedings of the Third Workshop on Statistical Machine Translation, pages 135?138,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Effects of Morphological Analysis in Translation between German and
English
Sara Stymne, Maria Holmqvist and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
We describe the LIU systems for German-
English and English-German translation sub-
mitted to the Shared Task of the Third Work-
shop of Statistical Machine Translation. The
main features of the systems, as compared
with the baseline, is the use of morphologi-
cal pre- and post-processing, and a sequence
model for German using morphologically rich
parts-of-speech. It is shown that these addi-
tions lead to improved translations.
1 Introduction
Research in statistical machine translation (SMT)
increasingly makes use of linguistic analysis in order
to improve performance. By including abstract cat-
egories, such as lemmas and parts-of-speech (POS),
in the models, it is argued that systems can become
better at handling sentences for which training data
at the word level is sparse. Such categories can be
integrated in the statistical framework using factored
models (Koehn et al, 2007). Furthermore, by pars-
ing input sentences and restructuring based on the
result to narrow the structural difference between
source and target language, the current phrase-based
models can be used more effectively (Collins et al,
2005).
German differs structurally from English in sev-
eral respects (see e.g. Collins et al, 2005). In this
work we wanted to look at one particular aspect
of restructuring, namely splitting of German com-
pounds, and evaluate its effect in both translation di-
rections, thus extending the initial experiments re-
ported in Holmqvist et al (2007). In addition, since
German is much richer in morphology than English,
we wanted to test the effects of using a sequence
model for German based on morphologically sub-
categorized parts-of-speech. All systems have been
specified as extensions of the Moses system pro-
vided for the Shared Task.
2 Part-of-speech and Morphology
For both English and German we used the part-of-
speech tagger TreeTagger (Schmid, 1994) to obtain
POS-tags.
The German POS-tags from TreeTagger were re-
fined by adding morphological information from
a commercial dependency parser, including case,
number, gender, definiteness, and person for nouns,
pronouns, verbs, adjectives and determiners in the
cases where both tools agreed on the POS-tag. If
they did not agree, the POS-tag from TreeTagger
was chosen. This tag set seemed more suitable for
SMT, with tags for proper names and foreign words
which the commercial parser does not have.
3 Compound Analysis
Compounding is common in many languages, in-
cluding German. Since compounding is highly pro-
ductive it increases vocabulary size and leads to
sparse data problems.
Compounds in German are formed by joining
words, and in addition filler letters can be inserted
or letters can be removed from the end of all but the
last word of the compound (Langer, 1998). We have
chosen to allow simple additions of letter(s) (-s, -n,
-en, -nen, -es, -er, -ien) and simple truncations (-e,
135
-en, -n). Example of compounds with additions and
truncations can be seen in (1).
(1) a. Staatsfeind (Staat + Feind)
public enemy
b. Kirchhof (Kirche + Hof)
graveyard
3.1 Splitting compounds
Noun and adjective compounds are split by a mod-
ified version of the corpus-based method presented
by Koehn and Knight (2003). First the German lan-
guage model data is POS-tagged and used to calcu-
late frequencies of all nouns, verbs, adjectives, ad-
verbs and the negative particle. Then, for each noun
and adjective all splits into these known words from
the corpus, allowing filler additions and truncations,
are considered, choosing the splitting option with
the highest arithmetic mean1 of the frequencies of
its parts.
A length limit of each part was set to 4 charac-
ters. For adjectives we restrict the number of parts
to maximum two, since they do not tend to have
multiple parts as often as nouns. In addition we
added a stop list with 14 parts, often mistagged, that
gave rise to wrong adjective splits, such as arische
(?Aryan?) in konsularische (?consular?).
As Koehn and Knight (2003) points out, parts of
compounds do not always have the same meaning
as when they stand alone, e.g. Grundrechte (?basic
rights?), where the first part, Grund, usually trans-
lates as foundation, which is wrong in this com-
pound. To overcome this we marked all compound
parts but the last, with the symbol ?#?. Thus they are
handled as separate words. Parts of split words also
receive a special POS-tag, based on the POS of the
last word of the compound, and the last part receives
the same POS as the full word.
We also split words containing hyphens based on
the same algorithm. Their parts receive a different
POS-tag, and the hyphens are left at the end of all
but the last part.
1We choose the arithmetic mean over the geometric mean
used by Koehn and Knight (2003) in order to increase the num-
ber of splits.
3.2 Merging compounds
For translation into German, the translation output
contains split compounds, which need to be merged.
An algorithm for merging has been proposed by
Popovic? et al (2006) using lists of compounds and
their parts. This method cannot merge unseen com-
pounds, however, so instead we base merging on
POS. If a word has a compound-POS, and the fol-
lowing word has a matching POS, they are merged.
If the next POS does not match, a hyphen is added
to the word, allowing for coordinated compounds as
in (2).
(2) Wasser- und Bodenqualita?t
water and soil quality
4 System Descriptions
The main difference of our system in relation to the
baseline system of the Shared Task2 is the pre- and
post-processing described above, the use of a POS
factor, and an additional sequence model on POS.
We also modified the tuning to include compound
merging, and used a smaller corpus, 600 sentences
picked evenly from the dev2006 corpus, for tuning.
We use the Moses decoder (Koehn et al, 2007) and
SRILM language models (Stolcke, 2002).
4.1 German ? English
We used POS as an output factor, as can be seen in
Figure 1. Using additional factors only on the tar-
get side means that only the training data need to be
POS-tagged, not the tuning data or translation input.
However, POS-tagging is still performed for Ger-
man as input to the pre-processing step. As Figure 1
shows we have two sequence models. A 5-gram lan-
guage model based on surface form using Kneser-
Ney smoothing and in addition a 7-gram sequence
model based on POS using Witten-Bell3 smoothing.
The training corpus was filtered to sentences with
2?40 words, resulting in a total of 1054688 sen-
tences. Training was done purely on Europarl data,
but results were submitted both on Europarl and
2http://www.statmt.org/wmt08/baseline.
html
3Kneser-Ney smoothing can not be used for the POS se-
quence model, since there were counts-of-counts of zero. How-
ever, Witten-Bell smoothing gives good results when the vocab-
ulary is small.
136
7?gram
POSPOS
wordword
Source Target
5?gram
word
Factors Sequence
models
Figure 1: Architecture of the factored system
News data. The news data were submitted to see
how well a pure out-of-domain system could per-
form.
In the pre-processing step compounds were split.
This was done for training, tuning and translation.
In addition German contracted prepositions and de-
terminers, such as zum from zu dem (?to the?), when
identified as such by the tagger, were split.
4.2 English ? German
All features of the German to English system were
used, and in addition more fine-grained German
POS-tags that were sub-categorized for morpholog-
ical features. This was done for training, tuning
and sequence models. At translation time no pre-
processing was needed for the English input, but a
post-processing step for the German output is re-
quired, including the merging of compounds and
contracted prepositions and determiners. The latter
was done in connection with uppercasing, by train-
ing an instance of Moses on a lower cased corpus
with split contractions and an upper-cased corpus
with untouched contractions. The tuning step was
modified so that merging of compounds were done
as part of the tuning.
4.3 Baseline
For comparison, we constructed a baseline accord-
ing to the shared-task description, but with smaller
tuning corpus, and the same sentence filtering for the
translation model as in the submitted system, using
only sentences of length 2-40.
In addition we constructed a factored baseline
system, with POS as an output factor and a se-
quence model for POS. Here we only used the orig-
inal POS-tags from TreeTagger, no additional mor-
phology was added for German.
De-En En-De
Baseline 26.95 20.16
Factored baseline 27.43 20.27
Submitted system 27.63 20.46
Table 1: Bleu scores for Europarl (test2007)
De-En En-De
Baseline 19.54 14.31
Factored baseline 20.16 14.37
Submitted system 20.61 14.77
Table 2: Bleu scores for News Commentary (nc-test2007)
5 Results
Case-sensitive Bleu scores4 (Papineni et al, 2002)
for the Europarl devtest set (test2007) are shown in
table 1. We can see that the submitted system per-
forms best, and that the factored baseline is better
than the pure baseline, especially for translation into
English.
Bleu scores for News Commentary5 (nc-test2007)
are shown in Table 2. Here we can also see that the
submitted system is the best. As expected, Bleu is
much lower on out-of-domain news text than on the
Europarl development test set.
5.1 Compounds
The quality of compound translations were analysed
manually. The first 100 compounds that could be
found by the splitting algorithm were extracted from
the Europarl reference text, test2007, together with
their English translations6 .
System translations were compared to the an-
notated compounds and classified into seven cate-
gories: correct, alternative good translation, correct
but different form, part of the compound translated,
no direct equivalent, wrong and untranslated. Out
of these the first three categories can be considered
good translations.
We performed the error analysis for the submitted
and the baseline system. The result can be seen in
4The %Bleu notation is used in this report
5No development test set for News test were provided, so we
present result for the News commentary, which can be expected
to give similar results.
6The English translations need not be compounds. Com-
pounds without a clear English translation were skipped.
137
De ? En En ? De
Subm Base Subm Base
Correct 50 46 40 39
Alternative 36 26 32 29
Form 5 7 6 8
Part 2 5 10 15
No equivalent 6 2 8 5
Wrong 1 7 1 1
Untranslated ? 7 3 3
Table 3: Results of the error analysis of compound trans-
lations
Table 3. For translation into English the submitted
system handles compound translations considerably
better than the baseline with 91% good translations
compared to 79%. In the submitted system all com-
pounds have a translation, compared to the baseline
system which has 7% of the compounds untrans-
lated. In the other translation direction the difference
is smaller, the biggest difference is that the submit-
ted system has fewer cases of partial translation.
5.2 Agreement in German NPs
To study the effects of using fine-grained POS-tags
in the German sequence model, a similar close study
of German NPs was performed. 100 English NPs
having at least two dependents of the head noun
were selected from a randomly chosen subsection
of the development test set. Their translations in
the baseline and submitted system were then identi-
fied. Translations that were not NPs were discarded.
In about two thirds (62 out of 99) of the cases, the
translations were identical. For the remainder, 12
translations were of equal quality, the submitted sys-
tem had a better translation in 17 cases (46%), and a
worse one in 8 cases (22%). In the majority of cases
where the baseline was better, this was due to word
selection, not agreement.
6 Conclusions
Adding morphological processing improved trans-
lation results in both directions for both text types.
Splitting compounds gave a bigger effect for trans-
lation from German. Marking of compound parts
worked well, with no untranslated parts left in the
sample used for evaluation. The mini-evaluation
of German NPs in English-German translation in-
dicates that the morphologically rich POS-based se-
quence model for German also had a positive effect.
Acknowledgement
We would like to thank Joe Steinhauer for help with
the evaluation of German output.
References
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the ACL, pages
531?540, Ann Arbor, Michigan.
M. Holmqvist, S. Stymne, and L. Ahrenberg. 2007. Get-
ting to know Moses: Initial experiments on German-
English factored translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 181?184, Prague, Czech Republic. Association
for Computational Linguistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of the tenth con-
ference of EACL, pages 187?193, Budapest, Hungary.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL, demonstration ses-
sion, Prague, Czech Republic.
S. Langer. 1998. Zur Morphologie und Semantik von
Nominalkomposita. In Tagungsband der 4. Konferenz
zur Verarbeitung natu?rlicher Sprache (KONVENS),
pages 83?97.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311?318, Philadelphia, Pennsyl-
vania.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical ma-
chine translation of German compound words. In Pro-
ceedings of FinTAL - 5th International Conference on
Natural Language Processing, pages 616?624, Turku,
Finland.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Preoceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP),
pages 901?904, Denver, Colorado.
138
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 120?124,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Improving alignment for SMT by reordering
and augmenting the training corpus
Maria Holmqvist, Sara Stymne, Jody Foo and Lars Ahrenberg
Department of Computer and Information Science
Link?ping University, Sweden
{marho,sarst,jodfo,lah}@ida.liu.se
Abstract
We describe the LIU systems for English-
German and German-English translation
in the WMT09 shared task. We focus on
two methods to improve the word align-
ment: (i) by applying Giza++ in a sec-
ond phase to a reordered training cor-
pus, where reordering is based on the
alignments from the first phase, and (ii)
by adding lexical data obtained as high-
precision alignments from a different word
aligner. These methods were studied in
the context of a system that uses com-
pound processing, a morphological se-
quence model for German, and a part-
of-speech sequence model for English.
Both methods gave some improvements to
translation quality as measured by Bleu
and Meteor scores, though not consis-
tently. All systems used both out-of-
domain and in-domain data as the mixed
corpus had better scores in the baseline
configuration.
1 Introduction
It is an open question whether improved word
alignment actually improves statistical MT. Fraser
and Marcu (2007) found that improved alignments
as measured by AER will not necessarily improve
translation quality, whereas Ganchev et al (2008)
did improve translation quality on several lan-
guage pairs by extending the alignment algorithm.
For this year?s shared task we therefore stud-
ied the effects of improving word alignment in the
context of our system for the WMT09 shared task.
Two methods were tried: (i) applying Giza++ in
a second phase to a reordered training corpus,
where reordering is based on the alignments from
the first phase, and (ii) adding lexical data ob-
tained as high-precision alignments from a differ-
ent word aligner. The submitted system includes
the first method in addition to the processing of
compounds and additional sequence models used
by Stymne et al (2008). Heuristics were used
to generate true-cased versions of the translations
that were submitted, as reported in section 6.
In this paper we report case-insensitive Bleu
scores (Papineni et al, 2002), unless otherwise
stated, calculated with the NIST tool, and case-
insensitive Meteor-ranking scores, without Word-
Net (Agarwal and Lavie, 2008).
2 Baseline system
Our baseline system uses compound split-
ting, compound merging and part-of-
speech/morphological sequence models (Stymne
et al, 2008). Except for these additions it is
similar to the baseline system of the workshop1.
The translation system is a factored phrase-
based translation system that uses the Moses
toolkit (Koehn et al, 2007) for decoding and train-
ing, GIZA++ for word alignment (Och and Ney,
2003), and SRILM (Stolcke, 2002) for language
models. Minimum error rate training was used to
tune the model feature weights (Och, 2003).
Tuning was performed on the news-dev2009a
set with 1025 sentences. All development test-
ing was performed on the news-dev2009b set with
1026 sentences.
2.1 Sequence model based on part-of-speech
and morphology
The translation models were factored with one ad-
ditional output factor. For English we used part-
of-speech tags obtained with TreeTagger (Schmid,
1994). For German we enriched the tags from
TreeTagger with morphological information, such
as case or tense, that we get from a commercial
1http://www.statmt.org/wmt09/baseline.
html
120
dependency parser2.
We used the extra factor in an additional se-
quence model which can improve agreement be-
tween words, and word order. For German this
factor was also used for compound merging.
2.2 Compound processing
Prior to training and translation, compound pro-
cessing was performed using an empirical method
based on (Koehn and Knight, 2003; Stymne,
2008). Words were split if they could be split
into parts that occur in a monolingual corpus. We
chose the split with the highest arithmetic mean
of the corpus frequencies of compound parts. We
split nouns, adjectives and verbs into parts that
were content words or particles. A part had to
be at least 3 characters in length and a stop list
was used to avoid parts that often lead to errors,
such as arische (Aryan) in konsularische (con-
sular). Compound parts sometimes have special
compound suffixes, which could be additions or
truncations of letters, or combinations of these.
We used the top 10 suffixes from a corpus study
of Langer (1998), and we also treated hyphens as
suffixes of compound parts. Compound parts were
given a special part-of-speech tag that matched the
head word.
For translation into German, compound parts
were merged to form compounds, both during test
and tuning. The merging is based on the spe-
cial part-of-speech tag used for compound parts
(Stymne, 2009). A token with this POS-tag is
merged with the next token, either if the POS-tags
match, or if it results in a known word.
3 Domain adaptation
This year three training corpora were available, a
small bilingual news commentary corpus, a rea-
sonably large Europarl corpus, and a very large
monolingual news corpus, see Table 1 for details.
The bilingual data was filtered to remove sen-
tences longer than 60 words. Because the German
news training corpus contained a number of En-
glish sentences, this corpus was cleaned by remov-
ing sentences containing a number of common En-
glish words.
Based on Koehn and Schroeder (2007) we
adapted our system from last year, which was fo-
cused on Europarl, to perform well on test data
2Machinese syntax, from Connexor Oy http://www.
connexor.eu
Corpus German English
news-commentary09 81,141
Europarl 1,331,262
news-train08 9,619,406 21,215,311
Table 1: Number of sentences in the corpora (after
filtering)
Corpus En?De De?En
Bleu Meteor Bleu Meteor
News com. 12.13 47.01 17.21 36.08
Europarl 12.92 47.27 18.53 37.65
Mixed 12.91 47.96 18.76 37.69
Mixed+ 14.62 49.48 19.92 38.18
Table 2: Results of domain adaptation
from the news domain. We used the possibility
to include several translation models in the Moses
decoder by using multiple alternative decoding
paths. We first trained systems on either bilingual
news data or Europarl. Then we trained a mixed
system, with two translation models one from each
corpus, a language model from the bilingual news
data, and a Europarl reordering model. The mixed
system was slightly better than the Europarl only
system. All sequence models used 5-grams for
surface form and 7-grams for part-of-speech. All
scores are shown in Table 2.
We wanted to train sequence models on the
large monolingual corpora, but due to limited
computer resources, we had to use a lower order
for this, than on the small corpus. Thus our se-
quence models on this data has lower order than
those trained on bilingual news or Europarl, with
4-grams for surface form and 6-grams for part-
of-speech. We also used the entropy-based prun-
ing included in the SRILM toolkit, with 10?8 as
a threshold. Using these sequence models in the
mixed model, called mixed+, improved the results
drastically, as shown in Table 2.
The other experiments reported in this paper are
based on the mixed+ system.
4 Improved alignment by reordering
Word alignment with Giza++ has been shown to
improve from making the source and target lan-
guage more similar, e.g., in terms of segmentation
(Ma et al, 2007) or word order.
We used the following simple procedure to im-
prove alignment of the training corpus by reorder-
ing the words in one of the texts according to the
121
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
Re-Src 14.63 49.80 20.54 38.86
Re-Trg 14.51 48.62 20.48 38.73
Table 3: Results of reordering experiments
word order in the other language:
1. Word align the corpus with Giza++.
2. Reorder the German words according to the
order of the English words they are aligned
to. (This is a common step in approaches that
extract reordering rules for translation. How-
ever, this is not what we use it for here.)
3. Word align the reordered German and origi-
nal English corpus with Giza++.
4. Put the reordered German words back into
their original position and adjust the align-
ments so that the improved alignment is pre-
served.
After this step we will have a possibly improved
alignment compared to the original Giza++ align-
ment. A phrase table was extracted from the align-
ment and training was performed as usual. The re-
ordering procedure was carried out on both source
(Re-Src) and target data (Re-Trg) and the results
of translating devtest data using these alignments
are shown in Table 3.
Compared with our baseline (mixed+), Bleu
and Meteor increased for the translation direction
German?English. Both source reordering and tar-
get reordering resulted in a 0.6 increase in Bleu.
For translation into German, source reordering
resulted in a somewhat higher Meteor score, but
overall did not seem to improve translation. Tar-
get reordering in this direction resulted in lower
scores.
It is not clear why reordering improved trans-
lation for German?English and not for English?
German. In all experiments, the heuristic sym-
metrization of directed Giza++ alignments was
performed in the intended translation direction 3.
3Our experiments show that symmetrization in the wrong
translation direction will result in lower translation quality
scores.
5 Augmenting the corpus with an
extracted dictionary
Previous research (Callison-Burch et al, 2004;
Fraser and Marcu, 2006) has shown that includ-
ing word aligned data during training can improve
translation results. In our case we included a dic-
tionary extracted from the news-commentary cor-
pus during the word alignment.
Using a method originally developed for term
extraction (Merkel and Foo, 2007), the news-
commentary09 corpus was grammatically anno-
tated and aligned using a heuristic word aligner.
Candidate dictionary entries were extracted from
the alignments. In order to optimize the qual-
ity of the dictionary, dictionary entry candidates
were ranked according to their Q-value, a metric
specifically designed for aligned data (Merkel and
Foo, 2007). The Q-value is based on the following
statistics:
? Type Pair Frequencies (TPF), i.e. the number
of times where the source and target types are
aligned.
? Target types per Source type (TpS), i.e. the
number of target types a specific source type
has been aligned to.
? Source types per Target type (SpT), i.e. the
number of source types a specific target type
has been aligned to.
The Q-value is calculated as
Q?value= TPFTpS+SpT . A high Q-value indi-
cates a dictionary candidate pair with a relatively
low number of translation variations. The candi-
dates were filtered using a Q-value threshold of
0.333, resulting in a dictionary containing 67287
entries.
For the experiments, the extracted dictionary
was inserted 200 times into the corpus used dur-
ing word alignment. The added dictionary entries
were removed before phrase extraction. Experi-
ments using the extracted dictionary as an addi-
tional phrase table were also run, but did not result
in any improvement of translation quality.
The results can be seen in Table 4. There was
no evident pattern how the inclusion of the dictio-
nary during alignment (DictAl) affected the trans-
lation quality. The inclusion of the dictionary pro-
duced both higher and lower Bleu scores than the
122
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
DictAl 14.73 49.39 18.93 37.71
Table 4: Results of domain adaptation
Corpus En?De De?En
Mixed+ 13.31 17.47
with OOV 13.74 17.96
Table 5: Case-sensitive Bleu scores
baseline system depending on the translation di-
rection. Meteor scores were however consistently
lower than the baseline system.
6 Post processing of out-of-vocabulary
words
In the standard systems all out-of-vocabulary
words are transferred as is from the translation in-
put to the translation output. Many of these words
are proper names, which do not get capitalized
properly, or numbers, which have different for-
matting in German and English. We used post-
processing to improve this.
For all unknown words we capitalized either the
first letter, or all letters, if they occur in that form
in the translation input. For unknown numbers
we switched between the German decimal comma
and the English decimal point for decimal num-
bers. For large numbers, English has a comma
to separate thousands, and German has a period.
These were also switched. This improved case-
sensitive Bleu scores in both translation directions,
see Table 5.
7 Submitted system
For both translation directions De-En and En-De
we submitted a system with two translation mod-
els trained on bilingual news and Europarl. The
alignment was improved by using the reordering
techniques described in section 4. The systems
also use all features described in this paper except
for the lexical augmentation (section 5) which did
not result in significant improvement. The results
of the submitted systems on devtest data are bold-
faced in Table 3.
Corpus En?De De?En
All 14.63 20.54
En-De orig. 19.93 26.82
Other set 11.66 16.17
Table 6: Bleu scores for the reordered systems on
two sections of development set news-dev2009b.
NIST scores show the same distribution.
8 Results on two sections of devtest data
Comparisons of translation output with reference
translations on devtest data showed some surpris-
ing differences, which could be attributed to cor-
responding differences between source and refer-
ence data. The differences were not evenly dis-
tributed but especially frequent in those sections
where the original language was something other
than English or German. To check the homogene-
ity of the devtest data we divided it into two sec-
tions, one for documents of English or German
origin, and the other for the remainder. It turned
out that scores were dramatically different for the
two sections, as shown in Table 6.
The reason for the difference is likely to be that
only the En-De set contains source texts and trans-
lations, while the other section contains parallel
translations from the same source. This suggests
that it would be interesting to study the effects of
splitting the training corpus in the same way be-
fore training.
9 Conclusion
The results of augmenting the training corpus with
an extracted lexicon were inconclusive. How-
ever, the alignment reordering improved transla-
tion quality, especially in the De?En direction.
The result of these reordering experiments indi-
cates that better word alignment quality will im-
prove SMT. The reordering method described in
this paper also has the advantage of only requir-
ing two runs of Giza++, no additional resources or
training is necessary to get an improved alignment.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
123
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In Pro-
ceedings of the 42nd Annual Meeting of ACL, pages
175?182, Barcelona, Spain.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of ACL, pages 769?776, Sydney, Australia.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Kuzman Ganchev, Jo?o de Almeida Varelas Gra?a, and
Ben Taskar. 2008. Better alignments = better trans-
lations? In Proceedings of the 46th Annual Meeting
of ACL, pages 986?993, Columbus, Ohio.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the tenth conference of EACL, pages 187?193, Bu-
dapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
ACL, demonstration session, Prague, Czech Repub-
lic.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung nat?rlicher Sprache
(KONVENS), pages 83?97.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of ACL,
pages 304?311, Prague, Czech Republic.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Con-
ference of Computational Linguistics (NODALIDA-
2007), pages 349?354, Tartu, Estonia.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing (ICSLP), pages 901?904, Denver, Col-
orado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Aarne Ranta and
Bengt Nordstr?m, editors, Proceedings of GoTAL,
6th International Conference on Natural Language
Processing, LNCS/LNAI Volume 5221, pages 464?
475.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of German compounds. In Pro-
ceedings of the EACL09 Student Research Work-
shop, Athens, Greece.
124
Proceedings of the ACL-HLT 2011 Student Session, pages 12?17,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Pre- and Postprocessing for Statistical Machine Translation into Germanic
Languages
Sara Stymne
Department of Computer and Information Science
Linko?ping University, Linko?ping, Sweden
sara.stymne@liu.se
Abstract
In this thesis proposal I present my thesis
work, about pre- and postprocessing for sta-
tistical machine translation, mainly into Ger-
manic languages. I focus my work on four ar-
eas: compounding, definite noun phrases, re-
ordering, and error correction. Initial results
are positive within all four areas, and there are
promising possibilities for extending these ap-
proaches. In addition I also focus on methods
for performing thorough error analysis of ma-
chine translation output, which can both moti-
vate and evaluate the studies performed.
1 Introduction
Statistical machine translation (SMT) is based on
training statistical models from large corpora of hu-
man translations. It has the advantage that it is very
fast to train, if there are available corpora, compared
to rule-based systems, and SMT systems are often
relatively good at lexical disambiguation. A large
drawback of SMT systems is that they use no or lit-
tle grammatical knowledge, relying mainly on a tar-
get language model for producing correct target lan-
guage texts, often resulting in ungrammatical out-
put. Thus, methods to include some, possibly shal-
low, linguistic knowledge seem reasonable.
The main focus for SMT to date has been on
translation into English, for which the models work
relatively well, especially for source languages that
are structurally similar to English. There has been
less research on translation out of English, or be-
tween other language pairs. Methods that are useful
for translation into English have problems in many
cases, for instance for translation into morpholog-
ically rich languages. Word order differences and
morphological complexity of a language have been
shown to be explanatory variables for the perfor-
mance of phrase-based SMT systems (Birch et al,
2008). German and the Scandinavian languages are
a good sample of languages, I believe, since they are
both more morphologically complex than English to
a varying degree, and the word order differ to some
extent, with mostly local differences between En-
glish and Scandinavian, and also long distance dif-
ferences with German, especially for verbs.
Some problems with SMT into German and
Swedish are exemplified in Table 1. In the Ger-
man example, the translation of the verb welcome
is missing in the SMT output. Missing and mis-
placed verbs are common error types, since the
German verb should appear last in the sentence
in this context, as in the reference, begru??en.
There is also an idiomatic compound, redebeitrag
(speech+contribution; intervention) in the refer-
ence, which is produced as the single word beitrag in
the SMT output. In the Swedish example, there are
problems with a definite NP, which has the wrong
gender of the definite article, den instead of det, and
is missing a definite suffix on the noun synsa?tt(et)
((the) approach).
In this proposal I outline my thesis work which
aims to improve statistical machine translation, par-
ticularly into Germanic languages, by using pre- and
postprocessing on one or both language sides, with
an additional focus on error analysis. In section 2 I
present a thesis overview, and in section 3 I briefly
overview MT evaluation techniques, and discuss my
work on MT error analysis. In section 4 I describe
my work on pre- and postprocessing, which is fo-
cused on compounding, definite noun phrases, word
order, and error correction.
12
En source I too would like to welcome Mr Prodi?s forceful and meaningful intervention.
De SMT Ich mo?chte auch herrn Prodis energisch und sinnvollen Beitrag.
De reference Ich mo?chte meinerseits auch den klaren und substanziellen Redebeitrag von Pra?sident Prodi
begru??en.
En source So much for the scientific approach.
Se SMT Sa? mycket fo?r den vetenskapliga synsa?tt.
Se reference Sa? mycket fo?r den vetenskapliga infallsvinkeln.
Table 1: Examples of problematic PBSMT output
2 Thesis Overview
My main research focus is how pre- and postpro-
cessing can be used to improve statistical MT, with
a focus on translation into Germanic languages. The
idea behind preprocessing is to change the training
corpus on the source side and/or on the target side
in order to make them more similar, which makes
the SMT task easier, since the standard SMT mod-
els work better for more similar languages. Post-
processing is needed after the translation when the
target language has been preprocessed, in order to
restore it to the normal target language. Postpro-
cessing can also be used on standard MT output, in
order to correct some of the errors from the MT sys-
tem. I focus my work about pre- and postprocessing
on four areas: compounding, definite noun phrases,
word order, and error correction. In addition I am
making an effort into error analysis, to identify and
classify errors in the MT output, both in order to fo-
cus my research effort, and to evaluate and compare
systems.
My work is based on the phrase-based approach
to statistical machine translation (PBSMT, Koehn et
al. (2003)). I further use the framework of factored
machine translation, where each word is represented
as a vector of factors, such as surface word, lemma
and part-of-speech, rather than only as surface words
(Koehn and Hoang, 2007). I mostly utilize factors to
translate into both words and (morphological) part-
of-speech, and can then use an additional sequence
model based on part-of-speech, which potentially
can improve word order and agreement. I take ad-
vantage of available tools, such as the Moses toolkit
(Koehn et al, 2007) for factored phrase-based trans-
lation.
I have chosen to focus on PBSMT, which is a very
successful MT approach, and have received much
research focus. Other SMT approaches, such as hi-
erarchical and syntactical SMT (e.g. Chiang (2007),
Zhang et al (2007a)) can potentially overcome some
language differences that are problematic for PB-
SMT, such as long-distance word order differences.
Many of these models have had good results, but
they have the drawback of being more complex than
PBSMT, and some methods do not scale well to
large corpora. While these models at least in princi-
ple address some of the drawbacks of the flat struc-
ture in PBSMT, Wang et al (2010) showed that a
syntactic SMT system can still gain from prepro-
cessing such as parse-tree modification.
3 Evaluation and Error Analysis
Machine translation systems are often only evalu-
ated quantitatively by using automatic metrics, such
as Bleu (Papineni et al, 2002), which compares the
system output to one or more human reference trans-
lations. While this type of evaluation has its advan-
tages, mainly that it is fast and cheap, its correla-
tion with human judgments is often low, especially
for translation out of English (Callison-Burch et al,
2009). In order to overcome these problems to some
extent I use several metrics in my studies, instead of
only Bleu. Despite this, metrics only give a single
score per sentence batch and system, which even us-
ing several metrics gives us little information on the
particular problems with a system, or about what the
possible improvements are.
One alternative to automatic metrics is human
judgments, either absolute scores, for instance for
adequacy or fluency, or by ranking sentences or seg-
ments. Such evaluations are a valuable complement
to automatic metrics, but they are costly and time-
consuming, and while they are useful for comparing
systems they also fail to pinpoint specific problems.
I mainly take advantage of this type of evaluation as
part of participating with my research group in MT
13
shared tasks with large evaluation campaigns such
as WMT (e.g. Callison-Burch et al (2009)).
To overcome the limitation of quantitative evalu-
ations, I focus on error analysis (EA) of MT output
in my thesis. EA is the task of annotating and clas-
sifying the errors in MT output, which gives a qual-
itative view. It can be used to evaluate and compare
systems, but is also useful in order to focus the re-
search effort on common problems for the language
pair in question. There have been previous attempts
of describing typologies for EA for MT, but they are
not unproblematic. Vilar et al (2006) suggested a ty-
pology with five main categories: missing, incorrect,
unknown, word order, and punctuation, which have
also been used by other researchers, mainly for eval-
uation. However, this typology is relatively shallow
and mixes classification of errors with causes of er-
rors. Farru?s et al (2010) suggested a typology based
on linguistic categories, such as orthography and se-
mantics, but their descriptions of these categories
and their subcategories are not detailed. Thus, as
part of my research, I am in the progress of design-
ing a fine-grained typology and guidelines for EA.
I have also created a tool for performing MT error
analysis (Stymne, 2011a). Initial annotations have
helped to focus my research efforts, and will be dis-
cussed below. I also plan to use EA as one means of
evaluating my work on pre- and postprocessing.
4 Main Research Problems
In this section I describe the four main problem ar-
eas I will focus on in my thesis project. I summarize
briefly previous work in each area, and outline my
own current and planned contributions. Sample re-
sults from the different studies are shown in Table
2.
4.1 Compounding
In most Germanic languages, compounds are writ-
ten without spaces or other word boundaries, which
makes them problematic for SMT, mainly due to
sparse data problems. The standard method for treat-
ing compounds for translation from Germanic lan-
guages is to split them in both the training data
and translation input (e.g. (Nie?en and Ney, 2000;
Koehn and Knight, 2003; Popovic? et al, 2006)).
Koehn and Knight (2003) also suggested a corpus-
based compound splitting method that has been
much used for SMT, where compounds are split
based on corpus frequencies of its parts.
If compounds are split for translation into Ger-
manic languages, the SMT system produces output
with split compounds, which need to be postpro-
cessed into full compounds. There has been very
little research into this problem. For this process to
be successful, it is important that the SMT system
produces the split compound parts in a correct word
order. To encourage this I have used a factored trans-
lation system that outputs parts-of-speech and uses a
sequence model on parts-of-speech. I extended the
part-of-speech tagset to use special part-of-speech
tags for split compound parts, which depend on the
head part-of-speech of the compound. For instance,
the Swedish noun pa?rontra?d (pear tree) would be
tagged as pa?ron|N-part tra?d|N when split. Using
this model the number of compound parts that were
produced in the wrong order was reduced drastically
compared to not using a part-of-speech sequence
model for translation into German (Stymne, 2009a).
I also designed an algorithm for the merging
task that uses these part-of-speech tags to merge
compounds only when the next part-of-speech tag
matches. This merging method outperforms reim-
plementations and variations of previous merging
suggestions (Popovic? et al, 2006), and methods
adapted from morphology merging (Virpioja et al,
2007) for translation into German (Stymne, 2009a).
It also has the advantage over previous merging
methods that it can produce novel compounds, while
at the same time reducing the risk of merging parts
into non-words. I have also shown that these com-
pound processing methods work equally well for
translation into Swedish (Stymne and Holmqvist,
2008). Currently I am working on methods for fur-
ther improving compound merging, with promising
initial results.
4.2 Definite Noun Phrases
In Scandinavian languages there are two ways to
express definiteness in noun phrases, either by a
definite article, or by a suffix on the noun. This
leads to problems when translating into these lan-
guages, such as superfluous definite articles and
wrong forms of nouns. I am not aware of any
published research in this area, but an unpublished
14
Language pair Corpus Corpus size Testset size In article System Bleu NIST
En-De Europarl 439,513 2,000 Stymne (2008)
BL 19.31 5.727
+Comp 19.73 5.854
En-Se Europarl 701,157 2,000
Stymne and
Holmqvist (2008)
BL 21.63 6.109
+Comp 22.12 6.143
En-Da Automotive 168,046 1,000 Stymne (2009b)
BL 70.91 8.816
+Def 76.35 9.363
En-Se Europarl 701,157 1,000 Stymne (2011b)
BL 21.63 6.109
+Def 22.03 6.178
En-De Europarl 439,513 2,000 Stymne (2011c)
BL 19.32 5.901
+Reo 19.59 5.936
En-Se Europarl 701,157 335
Stymne and
Ahrenberg (2010)
BL 19.44 5.381
+EC 22.12 5.447
Table 2: A selection of results for the four pre- and postprocessing strategies. Corpus sizes are given as number of
sentences. BL is baseline systems, +Comp with compound processing, +Def with definite processing, +Reo with
iterative reordering and alignment and monotone decoding, +EC with grammar checker error correction. The test set
for error correction only contains sentences that are affected by the error correction.
report shows no gain for a simple pre-processing
strategy for translation from German to Swedish
(Samuelsson, 2006). There is similar work on other
phenomena, such as Nie?en and Ney (2000), who
move German separated verb prefixes, to imitate the
English phrasal verb structure.
I address definiteness by preprocessing the source
language, to make definite NPs structurally simi-
lar to target language NPs. The transformations
are rule-based, using part-of-speech tags. Definite
NPs in Scandinavian languages are mimicked in the
source language by removing superfluous definite
articles, and/or adding definite suffixes to nouns. In
an initial study, this gave very good results, with rel-
ative Bleu improvements of up to 22.1% for trans-
lation into Danish (Stymne, 2009b). In Swedish
and Norwegian, the distribution of definite suffixes
is more complex than in Danish, and the basic strat-
egy that worked well for Danish was not successful
(Stymne, 2011b). A small modification to the ba-
sic strategy, so that superfluous English articles were
removed, but no suffixes were added, was success-
ful for translation from English into Swedish and
Norwegian. A planned extension is to integrate the
transformations into a lattice that is fed to the de-
coder, in the spirit of (Dyer et al, 2008).
4.3 Word Order
There has been a lot of research on how to handle
word order differences between languages. Prepro-
cessing approaches can use either hand-written rules
targeting known language differences (e.g. Collins
et al (2005), Li et al (2009)), or automatically learnt
rules (e.g. Xia and McCord (2004), Zhang et al
(2007b)), which are basically language independent.
I have performed an initial study on a language
independent word order strategy where reordering
rule learning and word alignment are performed iter-
atively, since they both depend on the other process
(Stymne, 2011c). There were no overall improve-
ments as measured by Bleu, but an investigation of
the reordering rules showed that the rules learned
in the different iterations are different with regard
to the linguistic phenomena they handle, indicating
that it is possible to learn new information from iter-
ating rule learning and word alignment. In this study
I only choose the 1-best reordering as input to the
SMT system. I plan to extend this by presenting sev-
eral reorderings to the decoder as a lattice, which has
been successful in previous work (see e.g. Zhang et
al. (2007b)).
My preliminary error analysis has shown that
there are two main word order difficulties for trans-
lation between English and Swedish, adverb place-
ment, and V2 errors, where the verb is not placed
in the correct position when it should be placed
before the subject. I plan to design a preprocess-
ing scheme to tackle these particular problems for
English-Swedish translation.
15
4.4 Error Correction
Postprocessing can be used to correct MT output
that has not been preprocessed, for instance in or-
der to improve the grammaticality. There has not
been much research in this area. A few examples
are Elming (2006), who use transformation-based
learning for word substitution based on aligned hu-
man post-edited sentences, and Guzma?n (2007) who
used regular expression to correct regular Spanish
errors. I have applied error correction suggestions
given by a grammar checker to theMT output, show-
ing that it can improve certain types of errors, such
as NP agreement and word order, with a high pre-
cision, but unfortunately with a low recall (Stymne
and Ahrenberg, 2010). Since the recall is low, the
positive effect on metrics such as Bleu is small on
general test sets, but there are improvements on test
sets which only contains sentences that are affected
by the postprocessing. An error analysis showed that
68?74% of the corrections made were useful, and
only around 10% of the changes made were harm-
ful. I believe that this approach could be even more
useful for similar languages, such as Danish and
Swedish, where a spell-checker might also be use-
ful.
The initial error analysis I have performed has
helped to identify common errors in SMT output,
and shown that many of them are quite regular. A
strategy I intend to pursue is to further identify com-
mon and regular problems, and to either construct
rules or to train a machine learning classifier to iden-
tify them, in order to be able to postprocess them. It
might also be possible to use the annotations from
the error analysis as part of the training data for such
a classifier.
5 Discussion
The main focus of my thesis will be on designing
and evaluating methods for pre- and postprocess-
ing of statistical MT, where I will contribute meth-
ods that can improve translation within the four ar-
eas discussed in section 4. The effort is focused
on translation into Germanic languages, including
German, on which there has been much previous
research, and Swedish and other Scandinavian lan-
guages, where there has been little previous re-
search. I believe that both language-pair dependent
and independent methods for pre- and postprocess-
ing can be useful. It is also the case that some
language-pair dependent methods carry over to other
(similar) language pairs with no or little modifica-
tion. So far I have mostly used rule-based process-
ing, but I plan to extend this with investigating ma-
chine learning methods, and compare the two main
approaches.
I strongly believe that it is important for MT re-
searchers to perform qualitative evaluations, both for
identifying problems with MT systems, and for eval-
uating and comparing systems. In my experience it
is often the case that a change to the system to im-
prove one aspect, such as compounding, also leads
to many other changes, in the case of compounding
for instance because of the possibility of improved
alignments, which I think we lack a proper under-
standing of.
My planned thesis contributions are to design a
detailed error typology, guidelines, and a tool, tar-
geted at MT researchers, for performing error anno-
tation, and to improve statistical machine translation
in four problem areas, using several methods of pre-
and postprocessing.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of EMNLP, pages 745?754, Honolulu,
Hawaii, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of WMT, pages 1?28, Athens, Greece.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):202?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, USA.
Christopher Dyer, SmarandaMuresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL, pages 1012?1020, Columbus, Ohio,
USA.
Jakob Elming. 2006. Transformation-based correction
of rule-based MT. In Proceedings of EAMT, pages
219?226, Oslo, Norway.
Mireia Farru?s, Marta R. Costa-jussa`, Jose? B. Marin?o, and
Jose? A. R. Fonollosa. 2010. Linguistic-based evalu-
ation criteria to identify statistical machine translation
16
errors. In Proceedings of EAMT, pages 52?57, Saint
Raphae?l, France.
Rafael Guzma?n. 2007. Advanced automatic MT
post-editing using regular expressions. Multilingual,
18(6):49?52.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP/CoNLL, pages
868?876, Prague, Czech Republic.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL,
pages 187?193, Budapest, Hungary.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48?54, Edmonton, Alberta,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, demonstration session, pages 177?180,
Prague, Czech Republic.
Jin-Ji Li, Jungi Kim, Dong-Il Kim, and Jong-Hyeok
Lee. 2009. Chinese syntactic reordering for ade-
quate generation of Korean verbal phrases in Chinese-
to-Korean SMT. In Proceedings of WMT, pages 190?
196, Athens, Greece.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proceed-
ings of CoLing, pages 1081?1085, Saarbru?cken, Ger-
many.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318, Philadelphia, Pennsylvania, USA.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of German compound
words. In Proceedings of FinTAL ? 5th International
Conference on Natural Language Processing, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Yvonne Samuelsson. 2006. Nouns in statistical ma-
chine translation. Unpublished manuscript: Term pa-
per, Statistical Machine Translation.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175?2181, Valetta, Malta.
Sara Stymne and Maria Holmqvist. 2008. Processing of
Swedish compounds for phrase-based statistical ma-
chine translation. In Proceedings of EAMT, pages
180?189, Hamburg, Germany.
Sara Stymne. 2008. German compounds in factored sta-
tistical machine translation. In Proceedings of Go-
TAL ? 6th International Conference on Natural Lan-
guage Processing, pages 464?475, Gothenburg, Swe-
den. Springer Verlag, LNCS/LNAI.
Sara Stymne. 2009a. A comparison of merging strategies
for translation of German compounds. In Proceedings
of EACL, Student Research Workshop, pages 61?69,
Athens, Greece.
Sara Stymne. 2009b. Definite noun phrases in statistical
machine translation into Danish. In Proceedings of the
Workshop on Extracting and Using Constructions in
NLP, pages 4?9, Odense, Denmark.
Sara Stymne. 2011a. Blast: A tool for error analysis of
machine translation output. In Proceedings of ACL,
demonstration session, Portland, Oregon, USA.
Sara Stymne. 2011b. Definite noun phrases in statistical
machine translation into Scandinavian languages. In
Proceedings of EAMT, Leuven, Belgium.
Sara Stymne. 2011c. Iterative reordering and word
alignment for statistical MT. In Proceedings of the
18th Nordic Conference on Computational Linguis-
tics, Riga, Latvia.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In Proceedings of LREC, pages 697?702,
Genoa, Italy.
Sami Virpioja, Jaako J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of MT Sum-
mit XI, pages 491?498, Copenhagen, Denmark.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In Proceedings of CoLing, pages 508?514,
Geneva, Switzerland.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007a. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535?542, Copen-
hagen, Denmark.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007b.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the International
Workshop on Spoken Language Translation, pages 21?
28, Trento, Italy.
17
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56?61,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
BLAST: A Tool for Error Analysis of Machine Translation Output
Sara Stymne
Department of Computer and Information Science
Linko?ping University, Linko?ping, Sweden
sara.stymne@liu.se
Abstract
We present BLAST, an open source tool for er-
ror analysis of machine translation (MT) out-
put. We believe that error analysis, i.e., to
identify and classify MT errors, should be an
integral part of MT development, since it gives
a qualitative view, which is not obtained by
standard evaluation methods. BLAST can aid
MT researchers and users in this process, by
providing an easy-to-use graphical user inter-
face. It is designed to be flexible, and can be
used with any MT system, language pair, and
error typology. The annotation task can be
aided by highlighting similarities with a ref-
erence translation.
1 Introduction
Machine translation evaluation is a difficult task,
since there is not only one correct translation of a
sentence, but many equally good translation options.
Often, machine translation (MT) systems are only
evaluated quantitatively, e.g. by the use of automatic
metrics, which is fast and cheap, but does not give
any indication of the specific problems of a MT sys-
tem. Thus, we advocate human error analysis of MT
output, where humans identify and classify the prob-
lems in machine translated sentences.
In this paper we present BLAST,1 a graphical tool
for performing human error analysis, from any MT
system and for any language pair. BLAST has a
graphical user interface, and is designed to be easy
1The BiLingual Annotation/Annotator/Analysis Support
Tool, available for download at http://www.ida.liu.
se/?sarst/blast/
and intuitive to work with. It can aid the user by
highlighting similarities with a reference sentence.
BLAST is flexible in that it can be used with out-
put from any MT system, and with any hierarchical
error typology. It has a modular design, allowing
easy extension with new modules. To the best of our
knowledge, there is no other publicly available tool
for MT error annotation. Since we believe that error
analysis is a vital complement to MT evaluation, we
think that BLAST can be useful for many other MT
researchers and developers.
2 MT Evaluation and Error Analysis
Hovy et al (2002) discussed the complexity of MT
evaluation, and stressed the importance of adjusting
evaluation to the purpose and context of the trans-
lation. However, MT is very often only evaluated
quantitatively using a single metric, especially in re-
search papers. Quantitative evaluations can be au-
tomatic, using metrics such as Bleu (Papineni et
al., 2002) or Meteor (Denkowski and Lavie, 2010),
where the MT output is compared to one or more hu-
man reference translations. Metrics, however, only
give a single quantitative score, and do not give any
information about the strengths and weaknesses of
the system. Comparing scores from different met-
rics can give a very rough indication of some major
problems, especially in combination with a part-of-
speech analysis (Popovic? et al, 2006).
Human evaluation is also often quantitative, for
instance in the form of estimates of values such as
adequacy and fluency, or by ranking sentences from
different systems (e.g. Callison-Burch et al (2007)).
A combination of human and automatic metrics is
56
human-targeted metrics such as HTER, where a hu-
man corrects the output of a system to the clos-
est correct translation, on which standard metrics
such as TER is then computed (Snover et al, 2006).
While these types of evaluation are certainly useful,
they are expensive and time-consuming, and still do
not tell us anything about the particular errors of a
system.2
Thus, we think that qualitative evaluation is an
important complement, and that error analysis, the
identification and classification of MT errors, is an
important task. There have been several suggestions
for general MT error typologies (Flanagan, 1994;
Vilar et al, 2006; Farru?s et al, 2010), targeted at
different user groups and purposes, focused on either
evaluation of single systems, or comparison between
systems. It is also possible to focus error analysis at
a specific problem, such as verb form errors (Murata
et al, 2005).
We have not been able to find any other freely
available tool for error analysis of MT. Vilar et al
(2006) mentioned in a footnote that ?a tool for high-
lighting the differences [between the MT system and
a correct translation] also proved to be quite useful?
for error analysis. They do not describe this tool any
further, and do not discuss if it was also used to mark
and store the error annotations themselves.
Some tools for post-editing of MT output, a re-
lated activity to error analysis, have been described
in the literature. Font Llitjo?s and Carbonell (2004)
presented an online tool for eliciting information
from the user when post-editing sentences, in or-
der to improve a rule-based translation system. The
post-edit operations were labeled with error cate-
gories, making it a type of error analysis. This tool
was highly connected to their translation system,
and it required users to post-edit sentences by mod-
ifying word alignments, something that many users
found difficult. Glenn et al (2008) described a post-
editing tool used for HTER calculation, which has
been used in large evaluation campaigns. The tool
is a pure post-editing tool and the edits are not clas-
sified. Graphical tools have also successfully been
used to aid humans in other MT-related tasks, such
as human MT evaluation of adequacy, fluency and
2Though it does, at least in principle, seem possible to mine
HTER annotations for more information
system comparison (Callison-Burch et al, 2007),
and word alignment (Ahrenberg et al, 2003).
3 System Overview
BLAST is a tool for human annotations of bilingual
material. Its main purpose is error analysis for ma-
chine translation. BLAST is designed for use in any
MT evaluation project. It is not tied to the informa-
tion provided by specific MT systems, or to specific
languages, and it can be used with any hierarchi-
cal error typology. It has a preprocessing module
for automatically aiding the annotator by highlight-
ing similarities between the MT output and a refer-
ence. Its modular design allows easy integration of
new modules for preprocessing. BLAST has three
working modes for handling error annotations: for
adding new annotations, for editing existing annota-
tions, and for searching among annotations.
BLAST can handle two types of annotations: er-
ror annotations and support annotations. Error an-
notations are based on a hierarchical error typology,
and are used to annotate errors in MT output. Error
annotations are added by the users of BLAST. Sup-
port annotations are used as a support to the user,
currently to mark similarities in the system and ref-
erence sentences. The support annotations are nor-
mally created automatically by BLAST, but they can
also be modified by the user. Both annotation types
are stored with the indices of the words they apply
to.
Figure 1 shows a screenshot of BLAST. The MT
output is shown to the annotator one segment at a
time, in the upper part of the screen. A segment nor-
mally consists of a sentence and the MT output can
be accompanied by a source sentence, a reference
sentence, or both. Error annotations are marked in
the segments by bold, underlined, colored text, and
support annotations are marked by light background
colors. The bottom part of the tool, contains the er-
ror typology, and controls for updating annotations
and navigation. The error typology is shown using
a menu structure, where submenus are activated by
the user clicking on higher levels.
3.1 Design goals
We created BLAST with the goal that it should be
flexible, and allow maximum freedom for the user,
57
Figure 1: Screenshot of BLAST
based on the following goals:
? Independent of the MT system being analyzed,
particularly not dependent on specific informa-
tion given by a particular MT system, such as
alignment information
? Compatible with any error typology
? Language pair independent
? Possible to mark where in a sentence an error
occurs
? Possible to view either source or reference sen-
tences, or both
? Possible to automatically highlight similarities
between the system and the reference sentences
? Containing a search function for errors
? Simple to understand and use
The current implementation of BLAST fulfils all
these goals, with the possible small limitation that
the error typology has to be hierarchical. We believe
this limitation is minor, however, since it is possible
to have a relatively flat structure if desired, and to
re-use the same submenu in many places, allowing
cross-classification within a hierarchical typology.
The flexibility of the tool gives users a lot of free-
dom in how to use it in their evaluation projects.
However, we believe that it is important within ev-
ery error annotation project to use a set error typol-
ogy and guidelines for annotation, but the annotation
tool should not limit users in making these choices.
3.2 Error Typologies
As described above, BLAST is easily configurable
with new typologies for annotation, with the only
restriction that the typology is hierarchical. BLAST
currently comes with the following implemented ty-
pologies, some of which are general, and some of
which are targeted at specific language (pairs):
? Vilar et al (2006)
? General
? Chinese
? Spanish
? Farru?s et al (2010)
? Catalan?Spanish
? Flanagan (1994) (slightly modified into a hier-
archical structure)
? French
58
? German
? Our own tentative fine-grained typology
? General
? Swedish
The error typologies can be very big, and it is hard
to fit an arbitrarily large typology into a graphical
tool. BLAST thus uses a menu structure which al-
ways shows the categories in the first level of the ty-
pology. Lower subtypologies are only shown when
they are activated by the user clicking on a higher
level. In Figure 1, the subtypologies to Word order
were activated by the user first clicking on Word or-
der, then on Phrase level.
It is important that typologies are easy to extend
and modify, especially in order to cover new target
languages, since the translation problems to some
extent will be dependent on the target language, for
instance with regard to the different agreement phe-
nomena in languages. The typologies that come with
BLAST can serve as a starting point for adjusting ty-
pologies, especially to new target languages.
3.3 Implementation
BLAST is implemented as a Java application using
Swing for the graphical user interface. Using Java
makes it platform independent, and it is currently
tested on Unix, Linux, Mac, and Windows. BLAST
has an object-oriented design, with a particular fo-
cus on modular design, to allow it to be easily ex-
tendible with new modules for preprocessing, read-
ing and writing to different file formats, and present-
ing statistics. Unicode is used in order to allow a
high number of languages, and sentences can be dis-
played both right to left, and left to right. BLAST
is open source and is released under the LGPL li-
cense.3
3.4 File formats
The main file types used in BLAST is the annotation
file, containing the translation segments and annota-
tions, and the typology file. These files are stored
in a simple text file format. There is also a configu-
ration file, which can be used for program settings,
besides using command line options, for instance to
configure color schemes, and to change preprocess-
ing settings. The statistics of an annotation project
3http://www.gnu.org/copyleft/lesser.html
are printed in a text file in a human-readable format
(see Section 4.5).
The annotation file contains the translation seg-
ments for the MT system, and possibly for the
source and reference sentences, and all error and
support annotations. The annotations are stored with
the indices of the word(s) in the segments that were
marked, and a label identifying the error type. The
annotation file is initially created automatically by
BLAST based on sentence aligned files. It is then
updated by BLAST with the annotations added by
the user.
The typology file has a header with main informa-
tion, and then an item for each menu containing:
? The name of the menu
? A list of menu items, containing:
? Display name
? Internal name (used in annotation file, and
internally in BLAST)
? The name of its submenu (if any)
The typology files have to be specified by the user,
but BLAST comes with several typology files, as de-
scribed in Section 3.2.
4 Working with BLAST
BLAST has three different working modes: annota-
tion, edit and search. The main mode is annotation,
which allows the user to add new error annotations.
The edit mode allows the user to edit and remove er-
ror annotations. The search mode allows the user to
search for errors of different types. BLAST can also
create support annotations, that can later be updated
by the user, and calculate and print statistics of an
annotation project.
4.1 Annotation
The annotation mode is the main working mode in
BLAST, and it is active in Figure 1. In annotation
mode a segment is shown with all its current er-
ror annotations. The annotations are marked with
bold and colored text, where the color depends on
the main type of the error. For each new annotation
the user selects the word or words that are wrong,
and selects an error type. In figure 1, the words no
television, and the error type Word order?Phrase
level?Long are selected in order to add a new error
59
annotation. BLAST ignores identical annotations,
and warns the user if they try to add an annotation
for the exact same words as another annotation.
4.2 Edit
In edit mode the user can change existing error an-
notations. In this mode only one annotation at a time
is shown, and the user can switch between them. For
each annotation affected words are highlighted, and
the error typology area shows the type of the error.
The currently shown error can be changed to a dif-
ferent error type, or it can be removed. The edit
mode is useful for revising annotations, and for cor-
recting annotation errors.
4.3 Search
In search mode, it is possible to search for errors of
a certain type. To search, users choose the error type
they want to search for in the error typology, and
then search backwards or forwards for error annota-
tions of that type. It is possible both to search for
specific errors deep in the typology, and to search
for all errors of a type higher in the typology, for
instance, to search for all word order errors, regard-
less of subclassification. Search is active between all
segments, not only for the currently shown segment.
Search is useful for controlling the consistency of
annotations, and for finding instances of specific er-
rors.
4.4 Support annotations
Error annotation is a hard task for humans, and thus
we try to aid it by including automatic preprocess-
ing, where similarities between the system and refer-
ence sentences are marked at different levels of sim-
ilarity. Even if the goal of the error analysis often is
not to compare the MT output to a single reference,
but to the closest correct equivalent, it can still be
useful to be able to see the similarities to one ref-
erence sentence, to be able to identify problematic
parts easier.
For this module we have adapted the code
for alignment used in the Meteor-NEXT metric
(Denkowski and Lavie, 2010) to BLAST. In Meteor-
NEXT the system and reference sentences are
aligned at the levels of exact matching, stemmed
matching, synonyms, and paraphrases. All these
modules work on lower-cased data, so we added a
module for exact matching with the original casing
kept. The exact and lower-cased matching works
for most languages, and stemming for 15 languages.
The synonym module uses WordNet, and is only
available for English. The paraphrase module is
based on an automatic paraphrase induction method
(Bannard and Callison-Burch, 2005), it is currently
trained for five languages, but the Meteor-NEXT
code for training it for additional languages is in-
cluded.
Support annotations are normally only created au-
tomatically, but BLAST allows the user to edit them.
The mechanism for adding, removing or changing
support annotations is separate from error annota-
tions, and can be used regardless of mode.
4.5 Create Statistics
The statistics module prints statistics about the cur-
rently loaded annotation project. The statistics are
printed to a file, in a human-readable format. It con-
tains information about the number of sentences and
errors in the project, average number of errors per
sentence, and how many sentences there are with
certain numbers of errors. The main part of the
statistics is the number and percentage of errors for
each node in the error typology. It is also possible to
get the number of errors for cross-classifications, by
specifying regular expressions for the categories to
cross-classify in the configuration file.
5 Future Extensions
BLAST is under active development, and we plan to
add new features. Most importantly we want to add
the possibility to annotate two MT systems in paral-
lel, which can be useful if the purpose of the annota-
tion is to compare MT systems. We are also working
on refining and developing the existing proposals for
error typologies, which is an important complement
to the tool itself. We intend to define a new fine-
grained general error typology, with extensions to a
number of target languages.
The modularity of BLAST also makes it possible
to add new modules, for instance for preprocess-
ing and to support other file formats. One example
would be to support error annotation of only specific
phenomena, such as verb errors, by adding a prepro-
cessing module for highlighting verbs with support
60
annotations, and a suitable verb-focused error typol-
ogy. We are also working on a preprocessing module
based on grammar checker techniques (Stymne and
Ahrenberg, 2010), that highlights parts of the MT
output that it suspects are non-grammatical.
Even though the main purpose of BLAST is for
error annotation of machine translation output, the
freedom in the use of error typologies and support
annotations also makes it suitable for other tasks
where bilingual material is used, such as for anno-
tations of named entities in bilingual texts, or for
analyzing human translations, e.g. giving feedback
to second language learners, with only the addition
of a suitable typology, and possibly a preprocessing
module.
6 Conclusion
We presented BLAST; a flexible tool for annotation
of bilingual segments, specifically intended for error
analysis of MT. BLAST facilitates the error analysis
task, which we believe is vital for MT researchers,
and could also be useful for other users of MT. Its
flexibility makes it possible to annotate translations
from any MT system and between any language
pairs, using any hierarchical error typology.
References
Lars Ahrenberg, Magnus Merkel, and Michael Petterst-
edt. 2003. Interactive word alignment for language
engineering. In Proceedings of EACL, pages 49?52,
Budapest, Hungary.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604, Ann Arbor, Michigan,
USA.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
WMT, pages 136?158, Prague, Czech Republic, June.
Michael Denkowski and Alon Lavie. 2010. METEOR-
NEXT and the METEOR paraphrase tables: Improved
evaluation support for five target languages. In Pro-
ceedings of WMT and MetricsMATR, pages 339?342,
Uppsala, Sweden.
Mireia Farru?s, Marta R. Costa-jussa`, Jose? B. Marin?o, and
Jose? A. R. Fonollosa. 2010. Linguistic-based evalu-
ation criteria to identify statistical machine translation
errors. In Proceedings of EAMT, pages 52?57, Saint
Raphae?l, France.
Mary Flanagan. 1994. Error classification for MT
evaluation. In Proceedings of AMTA, pages 65?72,
Columbia, Maryland, USA.
Ariadna Font Llitjo?s and Jaime Carbonell. 2004. The
translation correction tool: English-Spanish user stud-
ies. In Proceedings of LREC, pages 347?350, Lisbon,
Portugal.
Meghan Lammie Glenn, Stephanie Strassel, Lauren
Friedman, and Haejoong Lee. 2008. Management
of large annotation projects involving multiple human
judges: a case study of GALE machine translation
post-editing. In Proceedings of LREC, pages 2957?
2960, Marrakech, Morocco.
Eduard Hovy, Margaret King, and Andrei Popescu-Belis.
2002. Principles of context-based machine translation
evaluation. Machine Translation, 17(1):43?75.
Masaki Murata, Kiyotaka Uchimoto, QingMa, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems? errors in tense, aspect,
and modality. In Proceedings of PACLIC 19, pages
155?166, Taipei, Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318, Philadelphia, Pennsylvania, USA.
Maja Popovic?, Adria` de Gisper, Deepa Gupta, Patrik
Lambert, Hermann Ney, Jose? Marin?o, and Rafael
Banchs. 2006. Morpho-syntactic information for au-
tomatic error analysis of statistical machine translation
output. In Proceedings of WMT, pages 1?6, New York
City, New York, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human notation.
In Proceedings of AMTA, pages 223?231, Cambridge,
Massachusetts, USA.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175?2181, Valetta, Malta.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In Proceedings of LREC, pages 697?702,
Genoa, Italy.
61
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193?198,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Docent: A Document-Level Decoder for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Sara Stymne J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We describe Docent, an open-source de-
coder for statistical machine translation
that breaks with the usual sentence-by-
sentence paradigm and translates complete
documents as units. By taking transla-
tion to the document level, our decoder
can handle feature models with arbitrary
discourse-wide dependencies and consti-
tutes an essential infrastructure compon-
ent in the quest for discourse-aware SMT
models.
1 Motivation
Most of the research on statistical machine trans-
lation (SMT) that was conducted during the last
20 years treated every text as a ?bag of sentences?
and disregarded all relations between elements in
different sentences. Systematic research into ex-
plicitly discourse-related problems has only begun
very recently in the SMT community (Hardmeier,
2012) with work on topics such as pronominal
anaphora (Le Nagard and Koehn, 2010; Hard-
meier and Federico, 2010; Guillou, 2012), verb
tense (Gong et al, 2012) and discourse connect-
ives (Meyer et al, 2012).
One of the problems that hamper the develop-
ment of cross-sentence models for SMT is the fact
that the assumption of sentence independence is
at the heart of the dynamic programming (DP)
beam search algorithm most commonly used for
decoding in phrase-based SMT systems (Koehn et
al., 2003). For integrating cross-sentence features
into the decoding process, researchers had to adopt
strategies like two-pass decoding (Le Nagard and
Koehn, 2010). We have previously proposed an
algorithm for document-level phrase-based SMT
decoding (Hardmeier et al, 2012). Our decoding
algorithm is based on local search instead of dy-
namic programming and permits the integration of
document-level models with unrestricted depend-
encies, so that a model score can be conditioned on
arbitrary elements occurring anywhere in the input
document or in the translation that is being gen-
erated. In this paper, we present an open-source
implementation of this search algorithm. The de-
coder is written in C++ and follows an object-
oriented design that makes it easy to extend it with
new feature models, new search operations or dif-
ferent types of local search algorithms. The code
is released under the GNU General Public License
and published on Github1 to make it easy for other
researchers to use it in their own experiments.
2 Document-Level Decoding with Local
Search
Our decoder is based on the phrase-based SMT
model described by Koehn et al (2003) and im-
plemented, for example, in the popular Moses
decoder (Koehn et al, 2007). Translation is
performed by splitting the input sentence into
a number of contiguous word sequences, called
phrases, which are translated into the target lan-
guage through a phrase dictionary lookup and op-
tionally reordered. The choice between different
translations of an ambiguous source phrase and the
ordering of the target phrases are guided by a scor-
ing function that combines a set of scores taken
from the phrase table with scores from other mod-
els such as an n-gram language model. The actual
translation process is realised as a search for the
highest-scoring translation in the space of all the
possible translations that could be generated given
the models.
The decoding approach that is implemented in
Docent was first proposed by Hardmeier et al
(2012) and is based on local search. This means
that it has a state corresponding to a complete, if
possibly bad, translation of a document at every
1https://github.com/chardmeier/docent/wiki
193
stage of the search progress. Search proceeds by
making small changes to the current search state in
order to transform it gradually into a better trans-
lation. This differs from the DP algorithm used in
other decoders, which starts with an empty trans-
lation and expands it bit by bit. It is similar to
previous work on phrase-based SMT decoding by
Langlais et al (2007), but enables the creation of
document-level models, which was not addressed
by earlier approaches.
Docent currently implements two search al-
gorithms that are different generalisations of the
hill climbing local search algorithm by Hardmeier
et al (2012). The original hill climbing algorithm
starts with an initial state and generates possible
successor states by randomly applying simple ele-
mentary operations to the state. After each op-
eration, the new state is scored and accepted if
its score is better than that of the previous state,
else rejected. Search terminates when the decoder
cannot find an acceptable successor state after a
certain number of attempts, or when a maximum
number of steps is reached.
Simulated annealing is a stochastic variant of
hill climbing that always accepts moves towards
better states, but can also accept moves towards
lower-scoring states with a certain probability that
depends on a temperature parameter in order to
escape local maxima. Local beam search gener-
alises hill climbing in a different way by keeping
a beam of a fixed number of multiple states at any
time and randomly picking a state from the beam
to modify at each move. The original hill climb-
ing procedure can be recovered as a special case
of either one of these search algorithms, by call-
ing simulated annealing with a fixed temperature
of 0 or local beam search with a beam size of 1.
Initial states for the search process can be gen-
erated either by selecting a random segmentation
with random translations from the phrase table in
monotonic order, or by running DP beam search
with sentence-local models as a first pass. For
the second option, which generally yields better
search results, Docent is linked with the Moses
decoder and makes direct calls to the DP beam
search algorithm implemented by Moses. In addi-
tion to these state initialisation procedures, Docent
can save a search state to a disk file which can be
loaded again in a subsequent decoding pass. This
saves time especially when running repeated ex-
periments from the same starting point obtained
by DP search.
In order to explore the complete search space
of phrase-based SMT, the search operations in a
local search decoder must be able to change the
phrase translations, the order of the output phrases
and the segmentation of the source sentence into
phrases. The three operations used by Hardmeier
et al (2012), change-phrase-translation, reseg-
ment and swap-phrases, jointly meet this require-
ment and are all implemented in Docent. Addi-
tionally, Docent features three extra operations, all
of which affect the target word order: The move-
phrases operation moves a phrase to another loca-
tion in the sentence. Unlike swap-phrases, it does
not require that another phrase be moved in the
opposite direction at the same time. A pair of
operations called permute-phrases and linearise-
phrases can reorder a sequence of phrases into ran-
dom order and back into the order corresponding
to the source language.
Since the search algorithm in Docent is
stochastic, repeated runs of the decoder will gen-
erally produce different output. However, the vari-
ance of the output is usually small, especially
when initialising with a DP search pass, and it
tends to be lower than the variance introduced
by feature weight tuning (Hardmeier et al, 2012;
Stymne et al, 2013a).
3 Available Feature Models
In its current version, Docent implements a selec-
tion of sentence-local feature models that makes
it possible to build a baseline system with a con-
figuration comparable to that of a typical Moses
baseline system. The published source code
also includes prototype implementations of a few
document-level models. These models should be
considered work in progress and serve as a demon-
stration of the cross-sentence modelling capabilit-
ies of the decoder. They have not yet reached a
state of maturity that would make them suitable
for production use.
The sentence-level models provided by Docent
include the phrase table, n-gram language models
implemented with the KenLM toolkit (Heafield,
2011), an unlexicalised distortion cost model with
geometric decay (Koehn et al, 2003) and a word
penalty cost. All of these features are designed
to be compatible with the corresponding features
in Moses. From among the typical set of baseline
features in Moses, we have not implemented the
194
lexicalised distortion model, but this model could
easily be added if required. Docent uses the same
binary file format for phrase tables as Moses, so
the same training apparatus can be used.
DP-based SMT decoders have a parameter
called distortion limit that limits the difference in
word order between the input and the MT out-
put. In DP search, this is formally considered to
be a parameter of the search algorithm because it
affects the algorithmic complexity of the search
by controlling how many translation options must
be considered at each hypothesis expansion. The
stochastic search algorithm in Docent does not re-
quire this limitation, but it can still be useful be-
cause the standard models of SMT do not model
long-distance reordering well. Docent therefore
includes a separate indicator feature to indicate
a violated distortion limit. In conjunction with a
very large weight, this feature can effectively en-
sure that the distortion limit is enforced. In con-
trast with the distortion limit parameter of a DP de-
coder, the weight of our distortion limit feature can
potentially be tuned to permit occasional distor-
tion limit violations when they contribute to better
translations.
The document-level models included in Docent
include a length parity model, a semantic lan-
guage model as well as a collection of document-
level readability models. The length parity model
is a proof-of-concept model that ensures that all
sentences in a document have either consistently
odd or consistently even length. It serves mostly as
a template to demonstrate how a simple document-
level model can be implemented in the decoder.
The semantic language model was originally pro-
posed by Hardmeier et al (2012) to improve lex-
ical cohesion in a document. It is a cross-sentence
model over sequences of content words that are
scored based on their similarity in a word vector
space. The readability models serve to improve
the readability of the translation by encouraging
the selection of easier and more consistent target
words. They are described and demonstrated in
more detail in section 5.
Docent can read input files both in the NIST-
XML format commonly used to encode docu-
ments in MT shared tasks such as NIST or WMT
and in the more elaborate MMAX format (M?ller
and Strube, 2003). The MMAX format makes
it possible to include a wide range of discourse-
level corpus annotations such as coreference links.
These annotations can then be accessed by the
feature models. To allow for additional target-
language information such as morphological fea-
tures of target words, Docent can handle simple
word-level annotations that are encoded in the
phrase table in the same way as target language
factors in Moses.
In order to optimise feature weights we have
adapted the Moses tuning infrastructure to Do-
cent. In this way we can take advantage of all its
features, for instance using different optimisation
algorithms such as MERT (Och, 2003) or PRO
(Hopkins and May, 2011), and selective tuning of
a subset of features. Since document features only
give meaningful scores on the document level and
not on the sentence level, we naturally perform
optimisation on document level, which typically
means that we need more data than for the op-
timisation of sentence-based decoding. The res-
ults we obtain are relatively stable and competit-
ive with sentence-level optimisation of the same
models (Stymne et al, 2013a).
4 Implementing Feature Models
Efficiently
While translating a document, the local search de-
coder attempts to make a great number of moves.
For each move, a score must be computed and
tested against the acceptance criterion. An over-
whelming majority of the proposed moves will be
rejected. In order to achieve reasonably fast de-
coding times, efficient scoring is paramount. Re-
computing the scores of the whole document at
every step would be far too slow for the decoder
to be useful. Fortunately, score computation can
be sped up in two ways. Knowledge about how
the state to be scored was generated from its pre-
decessor helps to limit recomputations to a min-
imum, and by adopting a two-step scoring proced-
ure that just computes the scores that can be calcu-
lated with little effort at first, we need to compute
the complete score only if the new state has some
chance of being accepted.
The scores of SMT feature models can usu-
ally be decomposed in some way over parts of
the document. The traditional models borrowed
from sentence-based decoding are necessarily de-
composable at the sentence level, and in practice,
all common models are designed to meet the con-
straints of DP beam search, which ensures that
they can in fact be decomposed over even smal-
195
ler sequences of just a few words. For genuine
document-level features, this is not the case, but
even these models can often be decomposed in
some way, for instance over paragraphs, anaphoric
links or lexical chains. To take advantage of this
fact, feature models in Docent always have access
to the previous state and its score and to a list of
the state modifications that transform the previous
state into the next. The scores of the new state are
calculated by identifying the parts of a document
that are affected by the modifications, subtract-
ing the old scores of this part from the previous
score and adding the new scores. This approach
to scoring makes feature model implementation
a bit more complicated than in DP search, but it
gives the feature models full control over how they
decompose a document while still permitting effi-
cient decoding.
A feature model class in Docent implements
three methods. The initDocument method is called
once per document when decoding starts. It
straightforwardly computes the model score for
the entire document from scratch. When a state
is modified, the decoder first invokes the estim-
ateScoreUpdate method. Rather than calculating
the new score exactly, this method is only required
to return an upper bound that reflects the max-
imum score that could possibly be achieved by this
state. The search algorithm then checks this upper
bound against the acceptance criterion. Only if the
upper bound meets the criterion does it call the
updateScore method to calculate the exact score,
which is then checked against the acceptance cri-
terion again.
The motivation for this two-step procedure is
that some models can compute an upper bound ap-
proximation much more efficiently than an exact
score. For any model whose score is a log probab-
ility, a value of 0 is a loose upper bound that can
be returned instantly, but in many cases, we can do
much better. In the case of the n-gram language
model, for instance, a more accurate upper bound
can be computed cheaply by subtracting from the
old score all log-probabilities of n-grams that are
affected by the state modifications without adding
the scores of the n-grams replacing them in the
new state. This approximation can be calculated
without doing any language model lookups at all.
On the other hand, some models like the distor-
tion cost or the word penalty are very cheap to
compute, so that the estimateScoreUpdate method
can simply return the precise score as a tight up-
per bound. If a state gets rejected because of a
low score on one of the cheap models, this means
we will never have to compute the more expensive
feature scores at all.
5 Readability: A Case Study
As a case study we report initial results on how
document-wide features can be used in Docent in
order to improve the readability of texts by encour-
aging simple and consistent terminology (Stymne
et al, 2013b). This work is a first step towards
achieving joint SMT and text simplification, with
the final goal of adapting MT to user groups such
as people with reading disabilities.
Lexical consistency modelling for SMT has
been attempted before. The suggested approaches
have been limited by the use of sentence-level
decoders, however, and had to resort to proced-
ures like post processing (Carpuat, 2009), multiple
decoding runs with frozen counts from previous
runs (Ture et al, 2012), or cache-based models
(Tiedemann, 2010). In Docent, however, we al-
ways have access to a full document translation,
which makes it straightforward to include features
directly into the decoder.
We implemented four features on the document
level. The first two features are type token ra-
tio (TTR) and a reformulation of it, OVIX, which
is less sensitive to text length. These ratios have
been related to the ?idea density? of a text (M?h-
lenbock and Kokkinakis, 2009). We also wanted
to encourage consistent translations of words, for
which we used the Q-value (Del?ger et al, 2006),
which has been proposed to measure term qual-
ity. We applied it on word level (QW) and phrase
level (QP). These features need access to the full
target document, which we have in Docent. In ad-
dition, we included two sentence-level count fea-
tures for long words that have been used to meas-
ure the readability of Swedish texts (M?hlenbock
and Kokkinakis, 2009).
We tested our features on English?Swedish
translation using the Europarl corpus. For train-
ing we used 1,488,322 sentences. As test data, we
extracted 20 documents with a total of 690 sen-
tences. We used the standard set of baseline fea-
tures: 5-gram language model, translation model
with 5 weights, a word penalty and a distortion
penalty.
196
Baseline Readability features Comment
de ?rade ledam?terna (the honourable
Members)
ledam?terna (the members) / ni
(you)
+ Removal of non-essential words
p? ett s?dant s?tt att (in such a way
that)
s? att (so that) + Simplified expression
gemenskapslagstiftningen (the
community legislation)
gemenskapens lagstiftning (the
community?s legislation)
+ Shorter words by changing long
compound to genitive construction
V?rldshandelsorganisationen (World
Trade Organisation)
WTO (WTO) ? Changing long compound to
English-based abbreviation
handlingsplanen (the action plan) planen (the plan) ? Removal of important word
?gnat s?rskild uppm?rksamhet ?t (paid
particular attention to)
s?rskilt uppm?rksam p?
(particular attentive on)
? Bad grammar because of changed
part of speech and missing verb
Table 2: Example translation snippets with comments
Feature BLEU OVIX LIX
Baseline 0.243 56.88 51.17
TTR 0.243 55.25 51.04
OVIX 0.243 54.65 51.00
QW 0.242 57.16 51.16
QP 0.243 57.07 51.06
All 0.235 47.80 49.29
Table 1: Results for adding single lexical consist-
ency features to Docent
To evaluate our system we used the BLEU score
(Papineni et al, 2002) together with a set of read-
ability metrics, since readability is what we hoped
to improve by adding consistency features. Here
we used OVIX to confirm a direct impact on con-
sistency, and LIX (Bj?rnsson, 1968), which is a
common readability measure for Swedish. Unfor-
tunately we do not have access to simplified trans-
lated text, so we calculate the MT metrics against a
standard reference, which means that simple texts
will likely have worse scores than complicated
texts closer to the reference translation.
We tuned the standard features using Moses and
MERT, and then added each lexical consistency
feature with a small weight, using a grid search ap-
proach to find values with a small impact. The res-
ults are shown in Table 1. As can be seen, for in-
dividual features the translation quality was main-
tained, with small improvements in LIX, and in
OVIX for the TTR and OVIX features. For the
combination we lost a little bit on translation qual-
ity, but there was a larger effect on the readability
metrics. When we used larger weights, there was
a bigger impact on the readability metrics, with a
further decrease on MT quality.
We also investigated what types of changes the
readability features could lead to. Table 2 shows a
sample of translations where the baseline is com-
pared to systems with readability features. There
are both cases where the readability features help
and cases where they are problematic. Overall,
these examples show that our simple features can
help achieve some interesting simplifications.
There is still much work to do on how to take
best advantage of the possibilities in Docent in or-
der to achieve readable texts. This attempt shows
the feasibility of the approach. We plan to ex-
tend this work for instance by better feature op-
timisation, by integrating part-of-speech tags into
our features in order to focus on terms rather than
common words, and by using simplified texts for
evaluation and tuning.
6 Conclusions
In this paper, we have presented Docent, an open-
source document-level decoder for phrase-based
SMT released under the GNU General Public Li-
cense. Docent is the first decoder that permits the
inclusion of feature models with unrestricted de-
pendencies between arbitrary parts of the output,
even crossing sentence boundaries. A number of
research groups have recently started to investig-
ate the interplay between SMT and discourse-level
phenomena such as pronominal anaphora, verb
tense selection and the generation of discourse
connectives. We expect that the availability of a
document-level decoder will make it substantially
easier to leverage discourse information in SMT
and make SMT models explore new ground bey-
ond the next sentence boundary.
References
Carl-Hugo Bj?rnsson. 1968. L?sbarhet. Liber, Stock-
holm.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
197
Louise Del?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies: an
approach based on aligned corpora. In International
Congress of the European Federation for Medical
Informatics, pages 747?752, Maastricht, The Neth-
erlands.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 1?10, Avignon,
France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and J?rg
Tiedemann. 2012. Document-wide decoding
for phrase-based statistical machine translation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 1179?1190, Jeju Island, Korea.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Amer-
ican chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague, Czech Republic.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statist-
ical machine translation. In TMI-2007: Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 104?113, Sk?vde, Sweden.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statist-
ical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA), San
Diego, California, USA.
Katarina M?hlenbock and Sofie Johansson Kokkinakis.
2009. LIX 68 revisited ? an extended readability. In
Proceedings of the Corpus Linguistics Conference,
Liverpool, UK.
Christoph M?ller and Michael Strube. 2003. Multi-
level annotation in MMAX. In Proceedings of the
Fourth SIGdial Workshop on Discourse and Dia-
logue, pages 198?207, Sapporo, Japan.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo, Ja-
pan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA.
Sara Stymne, Christian Hardmeier, J?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight optim-
ization for discourse-level SMT. In Proceedings of
the Workshop on Discourse in Machine Translation
(DiscoMT), Sofia, Bulgaria.
Sara Stymne, J?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013), pages 375?386, Oslo,
Norway.
J?rg Tiedemann. 2010. Context adaptation in stat-
istical machine translation using models with ex-
ponentially decaying cache. In Proceedings of the
ACL 2010 Workshop on Domain Adaptation for Nat-
ural Language Processing (DANLP), pages 8?15,
Uppsala, Sweden.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montr?al, Canada.
198
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 183?188,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Vs and OOVs: Two Problems for Translation between German and
English
Sara Stymne, Maria Holmqvist, Lars Ahrenberg
Linko?ping University
Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
In this paper we report on experiments
with three preprocessing strategies for im-
proving translation output in a statistical
MT system. In training, two reordering
strategies were studied: (i) reorder on the
basis of the alignments from Giza++, and
(ii) reorder by moving all verbs to the
end of segments. In translation, out-of-
vocabulary words were preprocessed in a
knowledge-lite fashion to identify a likely
equivalent. All three strategies were im-
plemented for our English?German sys-
tem submitted to the WMT10 shared task.
Combining them lead to improvements in
both language directions.
1 Introduction
We present the Liu translation system for the con-
strained condition of the WMT10 shared transla-
tion task, between German and English in both di-
rections. The system is based on the 2009 Liu sub-
mission (Holmqvist et al, 2009), that used com-
pound processing, morphological sequence mod-
els, and improved alignment by reordering.
This year we have focused on two issues: trans-
lation of verbs, which is problematic for transla-
tion between English and German since the verb
placement is different with German verbs often be-
ing placed at the end of sentences; and OOVs, out-
of-vocabulary words, which are problematic for
machine translation in general. Verb translation
is targeted by trying to improve alignment, which
we believe is a crucial step for verb translation
since verbs that are far apart are often not aligned
at all. We do this mainly by moving verbs to the
end of sentences previous to alignment, which we
also combine with other alignments. We trans-
form OOVs into known words in a post-processing
step, based on casing, stemming, and splitting of
hyphenated compounds. In addition, we perform
general compound splitting for German both be-
fore training and translation, which also reduces
the OOV rate.
All results in this article are for the develop-
ment test set newstest2009, on truecased output.
We report Bleu scores (Papineni et al, 2002) and
Meteor ranking (without WordNet) scores (Agar-
wal and Lavie, 2008), using percent notation. We
also used other metrics, but as they gave similar
results they are not reported. For significance test-
ing we used approximate randomization (Riezler
and Maxwell, 2005), with p < 0.05.
2 Baseline System
The 2010 Liu system is based on the PBSMT base-
line system for the WMT shared translation task1.
We use the Moses toolkit (Koehn et al, 2007) for
decoding and to train translation models, Giza++
(Och and Ney, 2003) for word alignment, and the
SRILM toolkit (Stolcke, 2002) to train language
models. The main difference to the WMT base-
line is that the Liu system is trained on truecased
data, as in Koehn et al (2008), instead of lower-
cased data. This means that there is no need for a
full recasing step after translation, instead we only
need to uppercase the first word in each sentence.
2.1 Corpus
We participated in the constrained task, where we
only trained the Liu system on the news and Eu-
roparl corpora provided for the workshop. The
translation and reordering models were trained us-
ing the bilingual Europarl and news commentary
corpora, which we concatenated.
We used two sets of language models, one
where we first trained two models on Europarl
and news commentary, which we then interpolated
1http://www.statmt.org/wmt10/baseline.
html
183
with more weight given to the news commentary,
using weights from Koehn and Schroeder (2007).
The second set of language models were trained
on monolingual news data. For tuning we used
every second sentence, in total 1025 sentences, of
news-test2008.
2.2 Training with Limited Computational
Resources
One challenge for us was to train the transla-
tion sytem with limited computational resources.
We trained all systems on one Intel Core 2 CPU,
3.0Ghz, 16 Gb of RAM, 64 bit Linux (RedHat)
machine. This constrained the possibilities of us-
ing the data provided by the workshop to the full.
The main problem was training the language mod-
els, since the monolingual data was very large
compared to the bilingual data.
In order to train language models that were both
fast at runtime, and possible to train with the avail-
able memory, we chose to use the SRILM toolkit
(Stolcke, 2002), with entropy-based pruning, with
10?8 as a threshold. To reduce the model size we
also used lower order models for the large corpus;
4-grams instead of 5-grams for words and 6-grams
instead of 7-grams for the morphological models.
It was still impossible to train on the monolingual
English news corpus, with nearly 50 million sen-
tences, so we split that corpus into three equal size
parts, and trained three models, that were interpo-
lated with equal weights.
3 Morphological Processing
We added morphological processing to the base-
line system, by training additional sequence mod-
els on morphologically enriched part-of-speech
tags, and by compound processing for German.
We utilized the factored translation framework
in Moses, to enrich the baseline system with an
additional target sequence model. For English
we used part-of-speech tags obtained using Tree-
Tagger (Schmid, 1994), enriched with more fine-
grained tags for the number of determiners, in or-
der to target more agreement issues, since nouns
already have number in the tagset. For German
we used morphologically rich tags from RFTag-
ger (Schmid and Laws, 2008), that contains mor-
phological information such as case, number, and
gender for nouns and tense for verbs. We used
the extra factor in an additional sequence model
on the target side, which can improve word order
System Bleu Meteor
Baseline 13.42 48.83
+ morph 13.85 49.69
+ comp 14.24 49.41
Table 1: Results for morphological processing,
English?German
System Bleu Meteor
Baseline 18.34 38.13
+ morph 18.39 37.86
+ comp 18.50 38.47
Table 2: Results for morphological processing,
German?English
and agreement between words. For German the
factor was also used for compound merging.
Prior to training and translation, compound pro-
cessing was performed, using an empirical method
(Koehn and Knight, 2003; Stymne, 2008) that
splits words if they can be split into parts that oc-
cur in a monolingual corpus, choosing the split-
ting option with the highest arithmetic mean of its
part frequencies in the corpus. We split nouns,
adjectives and verbs, into parts that are content
words or particles. We imposed a length limit on
parts of 3 characters for translation from German
and of 6 characters for translation from English,
and we had a stop list of parts that often led to
errors, such as arische (Aryan) in konsularische
(consular). We allowed 10 common letter changes
(Langer, 1998) and hyphens at split points. Com-
pound parts were given a special part-of-speech
tag that matches the head word.
For translation into German, compound parts
were merged into full compounds using a method
described in Stymne and Holmqvist (2008), which
is based on matching of the special part-of-speech
tag for compound parts. A word with a compound
POS-tag were merged with the next word, if their
POS-tags were matching.
Tables 1 and 2 show the results of the addi-
tional morphological processing. Adding the se-
quence models on morphologically enriched part-
of-speech tags gave a significant improvement for
translation into German, but similar or worse re-
sults as the baseline for translation into English.
This is not surprising, since German morphology
is more complex than English morphology. The
addition of compound processing significantly im-
proved the results on Meteor for translation into
184
English, and it also reduced the number of OOVs
in the translation output by 20.8%. For translation
into German, compound processing gave a signif-
icant improvement on both metrics compared to
the baseline, and on Bleu compared to the system
with morphological sequence models. Overall, we
believe that both compound splitting and morphol-
ogy are useful; thus all experiments reported in the
sequel are based on the baseline system with mor-
phology models and compound splitting, which
we will call base.
4 Improved Alignment by Reordering
Previous work has shown that translation quality
can be improved by making the source language
more similar to the target language, for instance
in terms of word order (Wang et al, 2007; Xia
and McCord, 2004). In order to harmonize the
word order of the source and target sentence, they
applied hand-crafted or automatically induced re-
ordering rules to the source sentences of the train-
ing corpus. At decoding time, reordering rules
were again applied to input sentences before trans-
lation. The positive effects of such methods seem
to come from a combination of improved align-
ment and improved reordering during translation.
In contrast, we focus on improving the word
alignment by reordering the training corpus. The
training corpus is reordered prior to word align-
ment with Giza++ (Och and Ney, 2003) and then
the word links are re-adjusted back to the original
word positions. From the re-adjusted corpus, we
create phrase tables that allow translation of non-
reordered input text. Consequently, our reordering
only affects the word alignment and the phrase ta-
bles extracted from it.
We investigated two ways of reordering. The
first method is based on word alignments and the
other method is based on moving verbs to sim-
ilar positions in the source and target sentences.
We also investigated different combinations of re-
orderings and alignments. All results for the sys-
tems with improved reordering are shown in Ta-
bles 3 and 4.
4.1 Reordering Based on Alignments
The first reordering method does not require any
syntactic information or rules for reordering. We
simply used symmetrized Giza++ word align-
ments to reorder the words in the source sentences
to reflect the target word order and applied Giza++
System Bleu Meteor
base 14.24 49.41
reorder 14.32 49.58
verb 13.93 49.22
base+verb 14.38 49.72
base+verb+reorder 14.39 49.39
Table 3: Results for improved alignment,
English?German
System Bleu Meteor
base 18.50 38.47
reorder 18.77 38.53
verb 18.61 38.53
base+verb 18.66 38.61
base+verb+reorder 18.73 38.59
Table 4: Results for improved alignment,
German?English
again to the reordered training corpus. The follow-
ing steps were performed to produce the final word
alignment:
1. Word align the training corpus with Giza++.
2. Reorder the source words according to the or-
der of the target words they are aligned to
(store the original source word positions for
later).
3. Word align the reordered source and original
target corpus with Giza++.
4. Re-adjust the new word alignments so that
they align source and target words in the orig-
inal corpus.
The system built on this word alignment (re-
order) had a significant improvement in Bleu score
over the unreordered baseline (base) for transla-
tion into English, and small improvements other-
wise.
4.2 Verb movement
The positions of finite verbs are often very differ-
ent in English and German, where they are often
placed at the end of sentences. In several cases we
noted that finite verbs were misaligned by Giza++.
To improve the alignment of verbs, we moved all
verbs in both English and German to the end of the
sentences prior to word alignment. The reordered
sentences were word aligned with Giza++ and the
185
resulting word links were then re-adjusted to align
words in the original corpus.
The system created from this alignment (verb)
resulted in significantly lower scores than base for
translation into German, and similar scores as base
for translation into English.
4.3 Combination Systems
The alignment based on reordered verbs did not
produce a better alignment in terms of Bleu scores
of the resulting translations, which led us to the
conclusion that the alignment was noisy. How-
ever, it is possible that we did correctly align some
words that were misaligned in the baseline align-
ment. To investigate this issue we concatenated
first the baseline and verb alignments, and then all
three alignments, and extracted phrase tables from
the concatenated training sets.
All scores for both combined systems signifi-
cantly outperformed the unfactored baseline, and
were slightly better than base. For translation into
German it was best to use the combination of only
verb and base, which was significantly better than
base on Meteor. This shows that even though the
verb alignments were not good when used in a sin-
gle system, they still could contribute in a combi-
nation system.
5 Preprocessing of OOVs
Out-of-vocabulary words, words that have not
been seen in the training data, are a problem in
statistical machine translation, since no transla-
tions have been observed for them. The standard
strategy is to transfer them as is to the translation
output, which, naive as it sounds, actually works
well in some cases, since many OOVs are numbers
or proper names (Stymne and Holmqvist, 2008).
However, it still results in incomprehensible words
in the output in many cases. We have investi-
gated several ways of changing unknown words
into similar words that have been seen in the train-
ing data, in a preprocessing step.
We also considered another OOV problem,
number formatting, since it differs between En-
glish and German. To address this, we swapped
decimal points/commas, and other delimeters for
unknown numbers in a post-processing step.
In the preprocessing step, we applied a num-
ber of transformations to each OOV word, accept-
ing the first applicable transformation that led to a
known word:
Type German English
total OOVs 1833 1489
casing 124 26
stemming 270 72
hyphenated words 230 124
end hyphens 24 ?
Table 5: Number of affected words by OOV-
preprocessing
1. Change the word into a known cased ver-
sion (since we trained a truecased system,
this handles cased variations of words)
2. Stem the word, and if we know the stem,
choose the most common realisation of that
stem (using a Porter stemmer)
3. For hyphenated words, split at the hyphen (if
any of the resulting parts are OOVs, they are
recursively treated as well)
4. Remove hyphens at the end of German words
(that could result from compound splitting)
The first two steps were based on frequency lists
of truecased and stemmed words that we compiled
from the monolingual training corpora.
Inspection of the initial results showed that
proper names were often changed into other words
in English, so we excluded them from the prepro-
cessing by not applying it to words with an initial
capital letter. This happened to a lesser extent for
German, but here it was impossible to use the same
simple heuristic for proper names, since German
nouns also have an initial capital letter.
The number of affected words for the baseline
using the final transformations are shown in Table
5. Even though we managed to transform some
words, we still lack a transformation for the ma-
jority of OOVs. Despite this, there is a tendency of
small improvements on both metrics in the major-
ity of cases in both translation directions, as shown
in Tables 6 and 7.
Figure 1 shows an example of how OOV pro-
cessing affects one sentence for translation from
German to English. In this case splitting a hy-
phenated compound gives a better translation,
even though the word opening is chosen rather
than jack. There is also a stemming change,
where the adjective ausgereiftesten (the most well-
engineered), is changed form superlative to posi-
tive. This results in a more understandable trans-
186
DE original Die besten und technisch ausgereiftesten Telefone mit einer 3,5-mm-O?ffnung
fu?r normale Kopfho?rer kosten bis zu fu?nfzehntausend Kronen.
DE preprocessed die besten und technisch ausgereifte Telefone mit einer 3,5 mm O?ffnung fu?r
normale Kopf Ho?rer kosten bis zu fu?nfzehntausend Kronen .
base+verb+reorder The best and technically ausgereiftesten phones with a 3,5-mm-O?ffnung for
normal earphones cost up to fifteen thousand kronor.
base+verb+reorder
+OOV
The best and technologically advanced phones with a 3.5 mm opening for nor-
mal earphones cost up to fifteen thousand kronor.
EN reference The best and most technically well-equipped telephones, with a 3.5 mm jack
for ordinary headphones, cost up to fifteen thousand crowns.
Figure 1: Example of the effects of OOV processing for German?English
System Bleu Meteor
base 14.24 49.41
+ OOV 14.26 49.43
base+verb 14.38 49.72
+ OOV 14.42 49.75
+ MBR 14.41 49.77
Table 6: Results for OOV-processing and MBR,
English?German.
System Bleu Meteor
base 18.50 38.47
+ OOV 18.48 38.59
base+verb+reorder 18.73 38.59
+ OOV 18.81 38.70
+ MBR 18.84 38.75
Table 7: Results for OOV-processing and MBR,
German?English.
lation, which, however, is harmful to automatic
scores, since the preceding word, technically,
which is identical to the reference, is changed into
technologically.
This work is related to work by Arora et al
(2008), who transformed Hindi OOVs by us-
ing morphological analysers, before translation to
Japanese. Our work has the advantage that it is
more knowledge-lite, as it only needs a Porter
stemmer and a monolingual corpus. Mirkin et al
(2009) used WordNet to replace OOVs by syn-
onyms or hypernyms, and chose the best overall
translation partly based on scoring of the source
transformations. Our OOV handling could po-
tentially be used in combination with both these
strategies.
6 Final Submission
For the final Liu shared task submission we
used the base+verb+reorder+OOV system for
German?English and the base+verb+OOV sys-
tem for English?German, which had the best
overall scores considering all metrics. To these
systems we added minimum Bayes risk (MBR)
decoding (Kumar and Byrne, 2004). In standard
decoding, the top suggestion of the translation sys-
tem is chosen as the system output. In MBR de-
coding the risk is spread by choosing the trans-
lation that is most similar to the N highest scor-
ing translation suggestions from the system, with
N = 100, as suggested in Koehn et al (2008).
MBR decoding gave hardly any changes in auto-
matic scores, as shown in Tables 6 and 7. The final
system was significantly better than the baseline in
all cases, and significantly better than base on Me-
teor in both translation directions, and on Bleu for
translation into English.
7 Conclusions
As in Holmqvist et al (2009) reordering by us-
ing Giza++ in two phases had a small, but consis-
tent positive effect. Aligning verbs by co-locating
them at the end of sentences had a largely negative
effect. However, when output from this method
was concatenated with the baseline alignment be-
fore extracting the phrase table, there were con-
sistent improvements. Combining all three align-
ments, however, had mixed effects. Combining re-
ordering in training with a knowledge-lite method
for handling out-of-vocabulary words led to sig-
nificant improvements on Meteor scores for trans-
lation between German and English in both direc-
tions.
187
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of unknown words in phrase-
based statistical machine translation for languages
of rich morphology. In Proceedings of the 1st Inter-
national Workshop on Spoken Languages Technolo-
gies for Under-Resourced Languages, pages 70?75,
Hanoi, Vietnam.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 120?124, Athens,
Greece.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the EACL, pages 187?193,
Budapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting
of the ACL, demonstration session, pages 177?180,
Prague, Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2004 Human Language
Technology Conference of the NAACL, pages 169?
176, Boston, Massachusetts, USA.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung natu?rlicher Sprache
(KONVENS), pages 83?97, Bonn, Germany.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 791?
799, Suntec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT
and/or Summarization at the 43th Annual Meeting of
the ACL, pages 57?64, Ann Arbor, Michigan, USA.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an
application to fine-grained pos tagging. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 777?784, Manchester,
UK.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne and Maria Holmqvist. 2008. Process-
ing of Swedish compounds for phrase-based statis-
tical machine translation. In Proceedings of the
12th Annual Conference of the European Associa-
tion for Machine Translation, pages 180?189, Ham-
burg, Germany.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Proceedings of
GoTAL ? 6th International Conference on Natural
Language Processing, pages 464?475, Gothenburg,
Sweden.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 737?745, Prague, Czech Republic.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
188
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 250?260,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Productive Generation of Compound Words in Statistical Machine
Translation
Sara Stymne
Linko?ping University
Linko?ping, Sweden
sara.stymne@liu.se
Nicola Cancedda
Xerox Research Centre Europe
Meylan, France
nicola.cancedda@xrce.xerox.com
Abstract
In many languages the use of compound
words is very productive. A common practice
to reduce sparsity consists in splitting com-
pounds in the training data. When this is done,
the system incurs the risk of translating com-
ponents in non-consecutive positions, or in the
wrong order. Furthermore, a post-processing
step of compound merging is required to re-
construct compound words in the output. We
present a method for increasing the chances
that components that should be merged are
translated into contiguous positions and in the
right order. We also propose new heuristic
methods for merging components that outper-
form all known methods, and a learning-based
method that has similar accuracy as the heuris-
tic method, is better at producing novel com-
pounds, and can operate with no background
linguistic resources.
1 Introduction
In many languages including most of the Germanic
(German, Swedish etc.) and Uralic (Finnish, Hun-
garian etc.) language families so-called closed com-
pounds are used productively. Closed compounds
are written as single words without spaces or other
word boundaries, as the Swedish:
gatstenshuggare gata + sten + huggare
paving stone cutter street stone cutter
To cope with the productivity of the phenomenon,
any effective strategy should be able to correctly
process compounds that have never been seen in the
training data as such, although possibly their com-
ponents have, either in isolation or within a different
compound.
The extended use of compounds make them prob-
lematic for machine translation. For translation into
a compounding language, often fewer compounds
than in normal texts are produced. This can be due
to the fact that the desired compounds are missing in
the training data, or that they have not been aligned
correctly. When a compound is the idiomatic word
choice in the translation, a MT system can often
produce separate words, genitive or other alternative
constructions, or translate only one part of the com-
pound.
Most research on compound translation in com-
bination with SMT has been focused on transla-
tion from a compounding language, into a non-
compounding one, typically into English. A com-
mon strategy then consists in splitting compounds
into their components prior to training and transla-
tion.
Only few have investigated translation into a com-
pounding language. For translation into a com-
pounding language, the process becomes:
? Splitting compounds on the target (compound-
ing language) side of the training corpus;
? Learn a translation model from this split train-
ing corpus from source (e.g. English) into
decomposed-target (e.g. decomposed-German)
? At translation time, translate using the learned
model from source into decomposed-target.
? Apply a post-processing ?merge? step to recon-
struct compounds.
The merging step must solve two problems: identify
which words should be merged into compounds, and
choose the correct form of the compound parts.
250
The former problem can become hopelessly diffi-
cult if the translation did not put components nicely
side by side and in the correct order. Preliminary
to merging, then, the problem of promoting transla-
tions where compound elements are correctly posi-
tioned needs to be addressed. We call this promoting
compound coalescence.
2 Related work
The first suggestion of a compound merging method
for MT that we are aware of was described by
Popovic? et al (2006). Each word in the translation
output is looked up in a list of compound parts, and
merged with the next word if it results in a known
compound. This method led to improved overall
translation results from English to German. Stymne
(2008) suggested a merging method based on part-
of-speech matching, in a factored translation system,
where compound parts had a special part-of-speech
tag, and compound parts are only merged with the
next word if the part-of-speech tags match. This re-
sulted in improved translation quality from English
to German, and from English to Swedish (Stymne
and Holmqvist, 2008). Another method, based on
several decoding runs, was investigated by Fraser
(2009).
Stymne (2009a) investigated and compared merg-
ing methods inspired by Popovic? et al (2006),
Stymne (2008) and a method inspired by morphol-
ogy merging (El-Kahlout and Oflazer, 2006; Virpi-
oja et al, 2007), where compound parts were anno-
tated with symbols, and parts with symbols in the
translation output were merged with the next word.
3 Promoting coalescence of compounds
If compounds are split in the training set, then there
is no guarantee that translations of components will
end up in contiguous positions and in the correct or-
der. This is primarily a language model problem,
and we will model it as such by applying POS lan-
guage models on specially designed part-of-speech
sets, and by applying language model inspired count
features.
The approach proposed in Stymne (2008) consists
in running a POS tagger on the target side of the cor-
pus, decompose only tokens with some predefined
POS (e.g. Nouns), and then marking with special
POS-tags whether an element is a head or a modi-
fier. As an example, the German compound ?Fremd-
sprachenkenntnisse?, originally tagged as N(oun),
would be decomposed and re-tagged before training
as:
fremd sprachen kenntnisse
N-Modif N-Modif N
A POS n-gram language model using these extended
tagset, then, naturally steers the decoder towards
translations with good relative placement of these
components
We modify this approach by blurring distinctions
among POS not relevant to the formation of com-
pounds, thus further reducing the tagset to only three
tags:
? N-p ? all parts of a split compound except the
last
? N ? the last part of the compound (its head) and
all other nouns
? X ? all other tokens
The above scheme assumes that only noun com-
pounds are treated but it could easily be extended to
other types of compounds. Alternatively, splitting
can be attempted irrespective of POS on all tokens
longer than a fixed threshold, removing the need of
a POS tagger.
3.1 Sequence models as count features
We expect a POS-based n-gram language model on
our reduced tagset to learn to discourage sequences
unseen in the training data, such as the sequence
of compound parts not followed by a suitable head.
Such a generative LM, however, might also have a
tendency to bias lexical selection towards transla-
tions with fewer compounds, since the correspond-
ing tag sequences might be more common in text.
To compensate for this bias, we experiment with in-
jecting a little dose of a-priori knowledge, and add a
count feature, which explicitly counts the number of
occurrences of POS-sequences which we deem good
and bad in the translation output.
Table 1 gives an overview of the possible bigram
combinations, using the three symbol tagset, plus
sentence beginning and end markers, and their judg-
ment as good, bad or neutral.
251
Combination Judgment
N-p N-p Good
N-p N Good
N-p < \s > Bad
N-p X Bad
all other combinations Neutral
Table 1: Tag combinations in the translation output
We define two new feature functions: one count-
ing the number of occurrences of Good sequences
(the Boost model) and the other counting the occur-
rences of Bad sequences (the Punish model). The
two models can be used either in isolation or com-
bined, with or without a further POS n-gram lan-
guage model.
4 Merging compounds
Once a translation is generated using a system
trained on split compounds, a post-processing step
is required to merge components back into com-
pounds. For all pairs of consecutive tokens we have
to decide whether to combine them or not. Depend-
ing on the language and on preprocessing choices,
we might also have to decide whether to apply any
boundary transformation like e.g. inserting an ?s? be-
tween components.
The method proposed in Popovic? et al (2006)
maintains a list of known compounds and compound
modifiers. For any pair of consecutive tokens, if the
first is in the list of known modifiers and the com-
bination of the two is in the list of compounds, than
the two tokens are merged.
A somewhat orthogonal approach is the one pro-
posed in Stymne (2008): tokens are labeled with
POS-tags; compound modifiers are marked with
special POS-tags based on the POS of the head. If
a word with a modifier POS-tag is followed by ei-
ther another modifier POS-tag of the same type, or
the corresponding head POS-tag, then the two to-
kens are merged.
In the following sections we describe how we
modify and combine these two heuristics, and how
we alternatively formulate the problem as a se-
quence labelling problem suitable for a machine
learning approach.
4.1 Improving and combining heuristics
We empirically verified that the simple heuristics in
Popovic? et al (2006) tends to misfire quite often,
leading to too many compounds. We modify it by
adding an additional check: tokens are merged if
they appear combined in the list of compounds, but
only if their observed frequency as a compound is
larger than their frequency as a bigram. This blocks
the merging of many consecutive words, which just
happen to form a, often unrelated, compound when
merged, such as fo?r sma? (too small) into fo?rsma?
(spurn) in Swedish. Compound and bigram frequen-
cies can be computed on any available monolingual
corpus in the domain of interest.
We furthermore observed that the (improved) list-
based heuristic and the method based on POS pat-
terns lead to complementary sets of false negatives.
We thus propose to combine the two heuristics in
this way: we merge two consecutive tokens if they
would be combined by either the list-based heuris-
tic or the POS-based heuristic. We empirically veri-
fied improved performance when combining heuris-
tics in this way (Section 5.2).
4.2 Compound merging as sequence labelling
Besides extending and combining existing heuris-
tics, we propose a novel formulation of compound
merging as a sequence labelling problem. The oppo-
site problem, compound splitting, has successfully
been cast as a sequence labelling problem before
(Dyer, 2010), but here we apply this formulation in
the opposite direction.
Depending on choices made at compound split-
ting time, this task can be either a binary or mul-
ticlass classification task. If compound parts were
kept as-is, the merging task is a simple concatena-
tion of two words, and each separation point must
receive a binary label encoding whether the two to-
kens should be merged. An option at splitting time
is to normalize compound parts, which often have
a morphological form specific to compounds, to a
canonical form (Stymne, 2009b). In this case the
compound form has to be restored before concate-
nating the parts. This can be modeled as a multi-
class classifier that have the possible boundary trans-
formations as its classes.
Consider for instance translating into German the
252
English sentence:
Europe should promote the knowledge of
foreign languages
Assuming that the training corpus did not con-
tain occurrences of the pair (?knowledge of foreign
languages?,?fremdsprachenkenntnisse?) but con-
tained occurrences of (?knowledge?,?kenntnisse?),
(?foreign?,?fremd?) and (?languages?,?sprachen?),
then the translation model from English into
decomposed-German could be able to produce:
Europa sollte fremd sprachen kenntnisse
fo?rdern
We cast the problem of merging compounds as one
of making a series of correlated binary decisions,
one for each pair of consecutive words, each decid-
ing whether the whitespace between the two words
should be suppressed (label ?1?) or not (label ?0?).
In the case above, the correct labelling for the sen-
tence would be {0,0,1,1,0}, reconstructing the cor-
rect German:
Europa sollte fremdsprachenkenntnisse
fo?rdern1
If conversely, components are normalized upon
splitting, then labels are no longer binary, but come
from a set describing all local orthographic transfor-
mations possible for the language under considera-
tion. In this work we limited our attention to the case
when compounds are not normalized upon splitting,
and labels are hence binary.
While in principle one could address each atomic
merging decision independently, it seems intuitive
that a decision taken at one point should influence
merging decisions in neighboring separation points.
For this reason, instead of a simple (binary or n-
ary) classification problem, we prefer a sequence la-
belling formulation.
The array of sequence labelling algorithms po-
tentially suitable to our problem is fairly broad, in-
cluding Hidden Markov Models (HMMs) (Rabiner,
1989), Conditional Random Fields (CRFs) (Lafferty
et al, 2001), structured perceptrons (Collins, 2002),
1Nouns in German are capitalized. This is normally dealt
as a further ?truecasing? postprocessing, and is an orthogonal
problem from the one we deal with here.
and more. Since the focus of this work is on the
application rather than on a comparison among al-
ternative structured learning approaches, we limited
ourselves to a single implementation. Considering
its good scaling capabilities, appropriateness in pres-
ence of strongly redundant and overlapping features,
and widespread recognition in the NLP community,
we chose to use Conditional Random Fields.
4.2.1 Features
Each sequence item (i.e. each separation point be-
tween words) is represented by means of a sparse
vector of features. We used:
? Surface words: word-1, word+1
? Part-of-speech: POS-1, POS+1
? Character n-grams around the merge point
? 3 character suffix of word-1
? 3 character prefix of word+1
? Combinations crossing the merge points:
1+3, 3+1, 3+3 characters
? Normalized character n-grams around the
merge point, where characters are replaced by
phonetic approximations, and grouped accord-
ing to phonetic distribution, see Figure 1 (only
for Swedish)
? Frequencies from the training corpus, binned
by the following method:
f? =
{
10blog10(f)c if f > 1
f otherwise
for the following items:
? bigram, word-1,word+1
? Compound resulting from merging word-
1,word+1
? Word-1 as a true prefix of words in the cor-
pus
? Word+1 as a true suffix of words in the
corpus
? Frequency comparisons of two different fre-
quencies in the training corpus, classified into
four categories: freq1 = freq2 = 0, freq1 <
freq2, freq1 = freq2, freq1 > freq2
253
# vowels (soft versus hard)
$word = s/[aoua?]/a/g;
$word = s/[eiya?o?e?]/e/g;
# consonant combinations and
# spelling alternations
$word = s/ng/N/g;
$word = s/gn/G/g;
$word = s/ck/K/g;
$word = s/[lhgd]j/J/g;
$word = s/?ge/Je/g;
$word = s/?ske/Se/g;
$word = s/?s[kt]?j/S/g;
$word = s/?s?ch/S/g;
$word = s/?tj/T/g;
$word = s/?ke/Te/g;
#consonants grouping
$word = s/[ptk]/p/g;
$word = s/[bdg]/b/g;
$word = s/[lvw]/l/g;
$word = s/[cqxz]/q/g;
Figure 1: Transformations performed for normalizing
Swedish consonants (Perl notation).
? word-1,word+1 as bigram vs compound
? word-1 as true prefix vs single word
? word+1 as true suffix vs single word
where -1 refers to the word before the merge point,
and +1 to the word after.
We aimed to include features representing the
knowledge available to the list and POS heuristics,
by including part-of-speech tags and frequencies for
compounds and bigrams, as well as a comparison
between them. Features were also inspired by pre-
vious work on compound splitting, based on the in-
tuition that features that are useful for splitting com-
pounds, could also be useful for merging. Charac-
ter n-grams has successfully been used for splitting
Swedish compounds, as the only knowledge source
by Brodda (1979), and as one of several knowl-
edge sources by Sjo?bergh and Kann (2004). Friberg
(2007) tried to normalize letters, beside using the
original letters. While she was not successful, we
still believe in the potential of this feature. Larson et
al. (2000), used frequencies of prefixes and suffixes
from a corpus, as a basis of their method for splitting
German compounds.
4.2.2 Training data for the sequence labeler
Since features are strongly lexicalized, a suitably
large training dataset is required to prevent overfit-
ting, ruling out the possibility of manual labelling.
We created our training data automatically, using
the two heuristics described earlier, plus a third one
enabled by the availability, when estimating parame-
ters for the CRF, of a reference translation: merge if
two tokens are observed combined in the reference
translation (possibly as a sub-sequence of a longer
word). We compared multiple alternative combina-
tions of heuristics on a validation dataset. The val-
idation and test data were created by applying all
heuristics, and then manually check all positive an-
notations.
A first possibility to automatically generate a
training dataset consists in applying the compound
splitting preprocessing of choice to the target side of
the parallel training corpus for the SMT system: sep-
aration points where merges should occur are thus
trivially identified. In practice, however, merging
decisions will need be taken on the noisy output of
the SMT system, and not on the clean training data.
To acquire training data that is similar to the test
data, we could have held out from SMT training a
large fraction of the training data, used the trained
SMT to translate the source side of it, and then la-
bel decision points according to the heuristics. This
would, however, imply making a large fraction of
the data unavailable to training of the SMT. We thus
settled for a compromise: we trained the SMT sys-
tem on the whole training data, translated the whole
source, then labeled decision points according to the
heuristics. The translations we obtain are thus bi-
ased, of higher quality than those we should expect
to obtain on unseen data. Nevertheless they are sub-
stantially more similar to what will be observed in
operations than the reference translations.
5 Experiments
We performed experiments on translation from En-
glish into Swedish and Danish on two different cor-
pora, an automotive corpus collected from a propri-
etary translation memory, and on Europarl (Koehn,
2005) for the merging experiments. We used fac-
tored translation (Koehn and Hoang, 2007), with
both surface words and part-of-speech tags on the
254
EU-Sv Auto-Sv Auto-Da
Corpus Europarl Automotive Automotive
Languages English?Swedish English?Swedish English?Danish
Compounds split N, V, Adj N, V, Adj N
POS tag-sets POS POS,RPOS RPOS
Decoder Moses in-house in-house
Training sentences SMT 1,520,549 329,090 168,047
Training words SMT (target) 34,282,247 3,061,282 1,553,382
Training sentences CRF 248,808 317,398 164,702
Extra training sentences CRF 3,000 3,000 163,201
Table 2: Overview of the experimental settings
target side, with a sequence model on part-of-
speech. We used two decoders, Matrax (Simard et
al., 2005) and Moses (Koehn et al, 2007), both stan-
dard statistical phrase based decoders. For parame-
ter optimization we used minimum error rate train-
ing (Och, 2003) with Moses and gradient ascent on
smoothed NIST for the in-house decoder. In the
merging experiments we used the CRF++ toolkit.2
Compounds were split before training using a
corpus-based method (Koehn and Knight, 2003;
Stymne, 2008). For each word we explored all pos-
sible segmentations into parts that had at least 3
characters, and choose the segmentation which had
the highest arithmetic mean of frequencies for each
part in the training corpus. We constrained the split-
ting based on part-of-speech by only allowing split-
ting options where the compound head had the same
tag as the full word. The split compound parts kept
their form, which can be special to compounds, and
no symbols or other markup were added.
The experiment setup is summarized in Table 2.
The extra training sentences for CRF are sentences
that were not also used to train the SMT system. For
tuning, test and validation data we used 1,000 sen-
tence sets, except for Swedish auto, where we used
2,000 sentences for tuning. In the Swedish experi-
ments we split nouns, adjectives and verbs, and used
the full POS-set, except in the coalescence exper-
iments where we compared the full and restricted
POS-sets. For Danish we only split nouns, and
used the restricted POS-set. For frequency calcu-
lations of compounds and compound parts that were
needed for compound splitting and some of the com-
2Available at http://crfpp.sourceforge.net/
pound merging strategies, we used the respective
training data in all cases. Significance testing was
performed using approximate randomization (Rie-
zler and Maxwell, 2005), with 10,000 iterations, and
? < 0.05.
5.1 Experiments: Promoting compound
coalescence
We performed experiments with factored translation
models with the restricted part-of-speech set on the
Danish and Swedish automotive corpus. In these ex-
periments we compared the restricted part-of-speech
set we suggest in this work to several baseline sys-
tems without any compound processing and with
factored models using the extended part-of-speech
set suggested by Stymne (2008). Compound parts
were merged using the POS-based heuristic. Results
are reported on two standard metrics, NIST (Dod-
dington, 2002) and Bleu (Papineni et al, 2002), on
lower-cased data. For all sequence models we use
3-grams.
Results on the two Automotive corpora are sum-
marized in Table 3. The scores are very high, which
is due to the fact that it is an easy domain with many
repetitive sentence types. On the Danish dataset,
we observe significant improvements in BLEU and
NIST over the baseline for all methods where com-
pounds were split before translation and merged af-
terwards. Some of the gain is already obtained us-
ing a language model on the extended part-of-speech
set. Additional gains can however be obtained us-
ing instead a language model on a reduced set of
POS-tags (RPOS), and with a count feature explic-
itly boosting desirable RPOS sequences. The count
feature on undesirable sequences did not bring any
255
improvements over any of the systems with com-
pound splitting.
Results on the Swedish automotive corpus are less
clear-cut than for Danish, with mostly insignificant
differences between systems. The system with de-
composition and a restricted part-of-speech model
is significantly better on Bleu than all other systems,
except the system with decomposition and a stan-
dard part-of-speech model. Not splitting actually
gives the highest NIST score, even though the dif-
ference to the other systems is not significant, ex-
cept for the system with a combination of a trained
RPOS model and a boost model, which also has sig-
nificantly lower Bleu score than the other systems
with compound splitting.
5.2 Experiments: Compound merging
We compared alternative combinations of heuristics
on our three validation datasets, see Figure 2. In
order to estimate the amount of false negatives for
all three heuristics, we inspected the first 100 sen-
tences of each validation set, looking for words that
should be merged, but were not marked by any of
the heuristics. In no case we could find any such
words, so we thus assume that between them, the
heuristics can find the overwhelming majority of all
compounds to be merged.
We conducted a round of preliminary experiments
to identify the best combination of the heuristics
available at training time (modified list-based, POS-
based, and reference-based) to use to create auto-
matically the training data for the CRF. Best results
on the validation data are obtained by different com-
bination of heuristics for the three datasets, as could
be expected by the different distribution of errors
in Figure 2. In the experiments below we trained
the CRF using for each dataset the combination of
heuristics corresponding to leaving out the grey por-
tions of the Venn diagrams. This sort of prelimi-
nary optimization requires hand-labelling a certain
amount of data. Based on our experiments, skipping
this optimization and just using ref?(list?POS) (the
optimal configuration for the Swedish-English Eu-
roparl corpus) seems to be a reasonable alternative.
The validation data was also used to set a fre-
quency cut-off for feature occurrences (set at 3 in
the following experiments) and to tune the regu-
larization parameter in the CRF objective function.
448OK
1212/0
0-
154150/4
1411/3
0-154/11
list
POS ref
Automotive, Swedish
48OK1
8282/0
OO/0
-4OK1
5880/88
-4/83088/8O
list
PKS ref
Europarl, Swedish
488OK
8812
/0/012
-0-012
53/154
//12l3l213
istP
SOr ef?
Automotive, Danish
Figure 2: Evaluation of the different heuristics on valida-
tion files from the three corpora. The number in each re-
gion of the Venn diagrams indicates the number of times
a certain combination of heuristics fired (i.e. the num-
ber of positives for that combination). The two smaller
numbers below indicate the number of true and false pos-
itive, respectively. Venn diagram regions corresponding
to unreliable combinations of heuristics have correspond-
ing figures on a grey background. OK means that a large
fraction of the Venn cell was inspected, and no error was
found.
256
Danish auto Swedish auto
BLEU NIST BLEU NIST
No compound
splitting
Base 70.91 8.8816
Base+POSLM 72.08 8.9338 56.79 9.2674
With
compound
splitting
POSLM 74.11* 9.2341* 57.28 9.1717
RPOSLM 74.26* 9.2767* 58.12* 9.1694
punish model 73.34* 9.1543*
boost model 74.96** 9.3028** 57.31 9.1736
RPOSLM + boost 74.76** 9.3368** 55.82 9.1088
Table 3: Results of experiments with methods for promoting coalescence. Compounds are merged based on the POS
heuristic. Scores that are significantly better than Base+POSLM, are marked ?*?, and scores that are also better than
POSLM with ?**?.
Results are largely insensitive to variations in these
hyper-parameters, especially to the CRF regulariza-
tion parameter.
For the Danish auto corpus we had access to train-
ing data that were not also used to train the SMT
system, that we used to compare the performance
with that on the possibly biased training data that
was also used to train the SMT system. There were
no significant differences between the two types of
training data on validation data, which confirmed
that reusing the SMT training data for CRF training
was a reasonable strategy.
The overall merging results of the heuristics, the
best sequence labeler, and the sequence labeler with-
out POS are shown in Table 4. Notice how the (mod-
ified) list and POS heuristics have complementary
sets of false negatives: when merging on the OR of
the two heuristics, the number of false negatives de-
creases drastically, in general compensating for the
inevitable increase in false positives.
Among the heuristics, the combination of the im-
proved list heuristic and the POS-based heuristic has
a significantly higher recall and F-score than the
POS-based heuristic alone in all cases except on the
validation data for Swedish Auto, and than the list-
based strategy in several cases. The list heuristic
alone performs reasonably well on the two Swedish
data sets, but has a very low recall on the Danish
dataset. In all three cases the SMT training data
has been used for the list used by the heuristic, so
this is unexpected, especially considering the fact
that the Danish dataset is in the same domain as
one of the Swedish datasets. The Danish training
data is smaller than the Swedish data though, which
might be an influencing factor. It is possible that this
heuristic could perform better also for Danish given
more data for frequency calculations.
The sequence labeler is competitive with the
heuristics; on F-score it is only significantly worse
than any of the heuristics once, for Danish auto test
data, and in several cases it has a significantly higher
F-score than some of the heuristics. The sequence
labeler has a higher precision, significantly so in
three cases, than the best heuristic, the combina-
tion heuristic, which is positive, since erroneously
merged compounds are usually more disturbing for
a reader or post-editor than non-merged compounds.
The sequence-labelling approach can be used also
in the absence of a POS tagger, which can be impor-
tant if no such tool of suitable quality is available
for the target language and the domain of interest.
We thus also trained a CRF-based compound merger
without using POS features, and without using the
POS-based heuristic when constructing the training
data. Compared to the CRF with access to POS-tags,
on validation data F-score is significantly worse on
the Europarl Swedish condition and the Automotive
Danish condition, and are unchanged on Automo-
tive Swedish. On test data there are no significant
differences of the two sequence labelers on the two
Automotive corpora. On Swedish Europarl, the CRF
without POS has a higher recall at the cost of a lower
precision. Compared to the list heuristic, which is
the only other alternative strategy that works in the
absence of a POS tagger, the CRF without POS per-
forms significantly better on recall and F-score for
Danish automotive, and mostly comparative on the
two Swedish corpora.
257
Validation data Test data
Precision Recall F-score Precision Recall F-score
Swedish auto
list .9889p,lp .9936p .9912p .9900 .9770 .9835
POS .9757 .9632 .9694 .9916lp .9737 .9826
list?POS .9720 1p .9858p .9822 .9984l,p,c,cp .9902l,p,cp
CRF (ref?list) .9873p,lp .9984p .9928p,lp .9869 .9869 .9869
CRF without POS .9873p,lp .9968p .9920p,lp .9836 .9852 .9844
Swedish Europarl
list .9923lp,c,cp .9819 .9871 .9882lp,cp .9849 .9865
POS .9867lp .9785 .9825 .9893lp .9751 .9822
list?POS .9795 .9958l,p,c,cp .9876p,cp .9782 .9993l,p,c,cp .9886p,cp
CRF (ref?(list?POS)) .9841cp .9916l,p .9879p,cp .9953l,p,lp,cp .9790 .9871p
CRF without POS .9780 .9882p .9831 .9805 .9882p,c .9843
Danish auto
list .9250 .7603 .8346 .9905lp .7640 .8626
POS .9814l,lp .9635l,cp .9724l,lp,cp .9779 .9294l .9538l
list?POS .9251 .9863l,p,cp .9547l .9760 .9878l,p,c .9819l,p,c
CRF (ref?list?POS) .9775l,lp .9932l,p,cp .9853l,p,lp,cp .9778 .9659l,p .9718l,p
CRF without POS .9924l,lp,c .8973l .9424l .9826 .9635l,p .9729l,p
Table 4: Precision, Recall, and F-score for compound merging methods based on heuristics or sequence labelling on
validation data and on held-out test data. The superscripts marks the systems that are significantly worse than the
system in question (l-list, p-POS, lp-list?POS, c-best CRF configuration, cp-CRF without POS).
The sequence labeler has the advantage over
the heuristics that it is able to merge completely
novel compounds, whereas the list strategy can
only merge compounds that it has seen, and the
POS-based strategy can create novel compounds,
but only with known modifiers. An inspection of
the test data showed that there were a few novel
compounds merged by the sequence labeler that
were not identified with either of the heuristics. In
the test data we found knap+start (button start)
and vand+neds?nkning (water submersion) in Dan-
ish Auto, and kvarts sekel (quarter century) and
bostad(s)+ersa?ttning (housing grant) in Swedish
Europarl. This confirms that the sequence labeler,
from automatically labeled data based on heuristics,
can learn to merge new compounds that the heuris-
tics themselves cannot find.
6 Discussion and conclusions
In this article, we described several methods for
promoting coalescence and deciding if and how to
merge word compounds that are either competitive
with, or superior to, any currently known method.
For promoting compound coalescence we exper-
imented with introducing additional LMs based on
a restricted set of POS-tags, and with dedicated
SMT model features counting the number of se-
quences known a priori to be desirable and unde-
sirable. Experiments showed that this method can
lead to large improvements over systems using no
compound processing, and over previously known
compound processing methods.
For merging, we improved an existing list-based
heuristic, consisting in checking whether the first of
two consecutive words has been observed in a cor-
pus as a compound modifier and their combination
has been observed as a compound, introducing the
additional constraint that words are merged only if
their corpus frequency as a compound is larger than
their frequency as a bigram.
We observed that the false negatives of this im-
proved list-based heuristic and of another, known,
heuristic based on part-of-speech tags were comple-
mentary, and proposed a logical OR of them that
generally improves over both.
We furthermore cast the compound merging prob-
258
lem as a sequence labelling problem, opening it to
solutions based on a broad array of models and al-
gorithms. We experimented with one model, Condi-
tional Random Fields, designed a set of easily com-
puted features reaching beyond the information ac-
cessed by the heuristics, and showed that it gives
very competitive results.
Depending on the choice of the features, the se-
quence labelling approach has the potential to be
truly productive, i.e. to form new compounds in
an unrestricted way. This is for instance the case
with the feature set we experimented with. The list-
based heuristic is not productive: it can only form
a compound if this was already observed as such.
The POS-based heuristic presents some limited pro-
ductivity. Since it uses special POS-tags for com-
pound modifiers, it can form a compound provided
its head has been seen alone or as a head, and its
modifier(s) have been seen elsewhere, possibly sep-
arately, as modifier(s) of compounds. The sequence
labelling approach can decide to merge two consec-
utive words even if neither was ever seen before in a
compound.
In this paper we presented results on Swedish and
Danish. We believe that the methods would work
well also for other compounding languages such as
German and Finnish. If the linguistic resources re-
quired to extract some of the features, e.g. a POS
tagger, are unavailable (or are available only at train-
ing time but not in operations) for some language,
the sequence-labelling method can still be applied. It
is competitive or better than the list heuristic, which
is the only heuristic available in that scenario.
Experiments on three datasets show that the im-
proved and combined heuristics perform generally
better than any already known method, and that, be-
sides being fully productive, the sequence-labelling
version is highly competitive, tends to generate
fewer false positives than the combination heuristic,
and can be used flexibly with limited or no linguistic
resources.
References
Benny Brodda. 1979. Na?got om de svenska ordens fono-
tax och morfotax: Iakttagelse med utga?ngspunkt fra?n
experiment med automatisk morfologisk analys. In
PILUS nr 38. Inst. fo?r lingvistik, Stockholms univer-
sitet, Sweden.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California, USA.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Ph.D. thesis,
University of Maryland, USA.
I?lknur Durgar El-Kahlout and Kemal Oflazer. 2006. Ini-
tial explorations in English to Turkish statistical ma-
chine translation. In Proceedings of the Workshop
on Statistical Machine Translation, pages 7?14, New
York City, New York, USA.
Alexander Fraser. 2009. Experiments in morphosyntac-
tic processing for translating to and from German. In
Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 115?119, Athens, Greece.
Karin Friberg. 2007. Decomposing Swedish compounds
using memory-based learning. In Proceedings of the
16th Nordic Conference on Computational Linguistics
(Nodalida?07), pages 224?230, Tartu, Estonia.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868?876, Prague, Czech Republic.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the 10th
Conference of the EACL, pages 187?193, Budapest,
Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, demon-
stration session, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, Williamstown, MA.
259
Martha Larson, Daniel Willett, Joachim Ko?hler, and Ger-
hard Rigoll. 2000. Compound splitting and lexi-
cal unit recombination for improved performance of
a speech recognition system for German parliamen-
tary speeches. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing,
volume 3, pages 945?948, Beijing, China, October.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the ACL, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of German compound
words. In Proceedings of FinTAL ? 5th International
Conference on Natural Language Processing, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of IEEE, 77(2):257?286.
Stefan Riezler and John T. Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the Workshop on Intrin-
sic and Extrinsic Evaluation Measures for MT and/or
Summarization at ACL?05, pages 57?64, Ann Arbor,
Michigan, USA.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-
mada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of the Human Language Technology Conference
and the conference on Empirical Methods in Natu-
ral Language Processing, pages 755?762, Vancouver,
British Columbia, Canada.
Jonas Sjo?bergh and Viggo Kann. 2004. Finding the cor-
rect interpretation of Swedish compounds, a statisti-
cal approach. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04), Lisbon, Portugal.
Sara Stymne and Maria Holmqvist. 2008. Processing of
Swedish compounds for phrase-based statistical ma-
chine translation. In Proceedings of the 12th Annual
Conference of the European Association for Machine
Translation, pages 180?189, Hamburg, Germany.
Sara Stymne. 2008. German compounds in factored sta-
tistical machine translation. In Proceedings of Go-
TAL ? 6th International Conference on Natural Lan-
guage Processing, pages 464?475, Gothenburg, Swe-
den. Springer Verlag, LNCS/LNAI.
Sara Stymne. 2009a. A comparison of merging strategies
for translation of German compounds. In Proceedings
of the EACL 2009 Student Research Workshop, pages
61?69, Athens, Greece.
Sara Stymne. 2009b. Compound processing for phrase-
based statistical machine translation. Licentiate the-
sis, Linko?ping University, Sweden.
Sami Virpioja, Jaako J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of MT Sum-
mit XI, pages 491?498, Copenhagen, Denmark.
260
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 393?398,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Experiments with word alignment, normalization and clause reordering for
SMT between English and German
Maria Holmqvist, Sara Stymne and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
firstname.lastname@liu.se
Abstract
This paper presents the LIU system for the
WMT 2011 shared task for translation be-
tween German and English. For English?
German we attempted to improve the trans-
lation tables with a combination of standard
statistical word alignments and phrase-based
word alignments. For German?English trans-
lation we tried to make the German text more
similar to the English text by normalizing Ger-
man morphology and performing rule-based
clause reordering of the German text. This re-
sulted in small improvements for both transla-
tion directions.
1 Introduction
In this paper we present the LIU system for the
WMT11 shared task, for translation between En-
glish and German in both directions. We added a
number of features that address problems for trans-
lation between German and English such as word or-
der differences, incorrect alignment of certain words
such as verbs, and the morphological complexity
of German compared to English, as well as dealing
with previously unseen words.
In both translation directions our systems in-
clude compound processing, morphological se-
quence models, and a hierarchical reordering model.
For German?English translation we also added mor-
phological normalization, source side reordering,
and processing of out-of-vocabulary words (OOVs).
For English?German translation, we extracted word
alignments with a supervised method and combined
these alignments with Giza++ alignments in various
ways to improve the phrase table. We experimented
with different ways of combining the two alignments
such as using heuristic symmetrization and interpo-
lating phrase tables.
Results are reported on three metrics, BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002) and
Meteor ranking scores (Agarwal and Lavie, 2008)
based on truecased output.
2 Baseline System
This years improvements were added to the LIU
baseline system (Stymne et al, 2010). Our base-
line is a factored phrase based SMT system that uses
the Moses toolkit (Koehn et al, 2007) for transla-
tion model training and decoding, GIZA++ (Och
and Ney, 2003) for word alignment, SRILM (Stol-
cke, 2002) an KenLM (Heafield, 2011) for language
modelling and minimum error rate training (Och,
2003) to tune model feature weights. In addition,
the LIU baseline contains:
? Compound processing, including compound
splitting and for translation into German also
compound merging
? Part-of-speech and morphological sequence
models
All models were trained on truecased data. Trans-
lation and reordering models were trained using the
bilingual Europarl and News Commentary corpora
that were concatenated before training. We created
two language models. The first model is a 5-gram
model that we created by interpolating two language
393
models from bilingual News Commentary and Eu-
roparl with more weight on the News Commentary
model. The second model is a 4-gram model trained
on monolingual News only. All models were cre-
ated using entropy-based pruning with 10?8 as the
threshold.
Due to time constraints, all tuning and evaluation
were performed on half of the provided shared task
data. Systems were tuned on 1262 sentences from
newstest2009 and all results reported in Tables 1 and
2 are based on a devtest set of 1244 sentences from
newstest2010.
2.1 Sequence models with part-of-speech and
morphology
To improve target word order and agreement in the
translation output, we added an extra output factor in
our translation models consisting of tags with POS
and morphological features. For English we used
tags that were obtained by enriching POS tags from
TreeTagger (Schmid, 1994) with additional morpho-
logical features such as number for determiners. For
German, the POS and morphological tags were ob-
tained from RFTagger (Schmid and Laws, 2008)
which provides morphological information such as
case, number and gender for nouns and tense for
verbs. We trained two sequence models for each
system over this output factor and added them as
features in our baseline system. The first sequence
model is a 7-gram model interpolated from models
of bilingual Europarl and News Commentary. The
second model is a 6-gram model trained on mono-
lingual News only.
2.2 Compound processing
In both translation directions we split compounds,
using a modified version of the corpus-based split-
ting method of Koehn and Knight (2003). We split
nouns, verb, and adjective compounds into known
parts that were content words or cardinal numbers,
based on the arithmetic mean of the frequency of
the parts in the training corpus. We allowed 10 com-
mon letter changes (Langer, 1998) and hyphens at
split points. Compound parts were kept in their sur-
face form and compound modifiers received a part-
of-speech tag based on that of the tag of the full com-
pound.
For translation into German, compounds were
merged using the POS-merging strategy of Stymne
(2009). A compound part in the translation output,
identified by the special part-of-speech tags, was
merged with the next word if that word had a match-
ing part-of-speech tag. If the compound part was
followed by the conjunction und (and), we added a
hyphen to the part, to account for coordinated com-
pounds.
2.3 Hierarchical reordering
In our baseline system we experimented with two
lexicalized reordering models. The standard model
in Moses (Koehn et al, 2005), and the hierarchi-
cal model of Galley and Manning (2008). In both
models the placement of a phrase is compared to
that of the previous and/or next phrase. In the stan-
dard model up to three reorderings are distinguished,
monotone, swap, and discontinuous. In the hier-
archical model the discontinuous class can be fur-
ther subdivided into two classes, left and right dis-
continuous. The hierarchical model further differs
from the standard model in that it compares the or-
der of the phrase with the next or previous block of
phrases, not only with the next or previous single
phrase.
We investigated one configuration of each
model. For the standard model we used the msd-
bidirectional-fe setting, which uses three orienta-
tions, is conditioned on both the source and target
language, and considers both the previous and next
phrase. For the hierarchical model we used all four
orientations, and again it is conditioned on both the
source and target language, and considers both the
previous and next phrase.
The result of replacing the standard reordering
model with an hierarchical model is shown in Table
1 and 2. For translation into German adding the hi-
erarchical model led to small improvements as mea-
sured by NIST and Meteor. For translation in the
other direction, the differences on automatic metrics
were very small. Still, we decided to use the hierar-
chical model in all our systems.
3 German?English
For translation from German into English we fo-
cused on making the German source text more sim-
ilar to English by removing redundant morphology
394
and changing word order before training translation
models.
3.1 Normalization
We performed normalization of German words to re-
move distinctions that do not exist in English, such
as case distinctions on nouns. This strategy is sim-
ilar to that of El-Kahlout and Yvon (2010), but we
used a slightly different set of transformations, that
we thought better mirrored the English structure.
For morphological tags we used RFTagger and for
lemmas we used TreeTagger. The morphological
transformations we performed were the following:
? Nouns:
? Replace with lemma+s if plural number
? Replace with lemma otherwise
? Verbs:
? Replace with lemma if present tense, not
third person singular
? Replace with lemma+p if past tense
? Adjectives:
? Replace with lemma+c if comparative
? Replace with lemma+sup if superlative
? Replace with lemma otherwise
? Articles:
? Definite articles:
? Replace with des if genitive
? Replace with der otherwise
? Indefinite articles:
? Replace with eines if genitive
? Replace with ein otherwise
? Pronouns:
? Replace with RELPRO if relative
? Replace with lemma if indefinite, interrog-
ative, or possessive pronouns
? Add +g to all pronouns which are geni-
tive, unless they are possessive
For all word types that are not mentioned in the
list, surface forms were kept.
BLEU NIST Meteor
Baseline 21.01 6.2742 41.32
+hier reo 20.94 6.2800 41.24
+normalization 20.85 6.2370 41.04
+source reordering 21.06 6.3082 41.40
+ OOV proc. 21.22 6.3692 41.51
Table 1: German?English translation results. Results are
cumulative.
We also performed those tokenization and
spelling normalizations suggested by El-Kahlout
and Yvon (2010), that we judged could safely be
done for translation from German without collect-
ing corpus statistics. We split words with numbers
and letters, such as 40-ja?hrigen or 40ja?hrigen (40
year-old), unless the suffix indicates that it is a ordi-
nal, such as 70sten (70th). We also did some spelling
normalization by exchanging ? with ss and replacing
tripled consonants with doubled consonants. These
changes would have been harmful for translation
into German, since they change the language into a
normalized variant, but for translation from German
we considered them safe.
3.2 Source side reordering
To make the word order of German input sen-
tences more English-like a version of the rules of
(Collins et al, 2005) were partially implemented us-
ing tagged output from the RFTagger. Basically,
beginnings of subordinate clauses, their subjects (if
present) and final verb clusters were identified based
on tag sequences, and the clusters were moved to
the beginning of the clause, and reordered so that
the finite verb ended up in the second clause posi-
tion. Also, some common adverbs were moved with
the verb cluster and placed between finite and non-
finite verbs. After testing, we decided to apply these
rules only to subordinate clauses at the end of sen-
tences, since these were the only ones that could be
identified with good precision. Still, some 750,000
clauses were reordered.
3.3 OOV Processing
We also added limited processing of OOVs. In a pre-
processing step we replaced unknown words with
known cased variants if available, removed markup
from normalized words if that resulted in an un-
395
known token, and split hyphened words. We also
split suspected names in cases where we had a pat-
tern with a single upper-case letter in the middle of a
word, such as ConocoPhillips into Conoco Phillips.
In a post-processing step we changed the number
formatting of unknown numbers by changing dec-
imal points and thousand separators, to agree with
English orthography. This processing only affects
a small number of words, and cannot be expected
to make a large impact on the final results. Out
of 884 OOVs in the devtest, 39 had known cased
options, 126 hyphened words were split, 147 cases
had markup from the normalization removed, and 13
suspected names were split.
3.4 Results
The results of these experiments can be seen in Table
1 where each new addition is added to the previous
system. When we compare the new additions with
the baseline with hierarchical reordering, we see that
while the normalization did not seem to have a posi-
tive effect on any metric, both source reordering and
OOV processing led to small increases on all scores.
4 English?German
For translation from English into German we at-
tempted to improve the quality of the phrase table by
adding new word alignments to the standard Giza++
alignments.
4.1 Phrase-based word alignment
We experimented with different ways of com-
bining word alignments from Giza++ with align-
ments created using phrase-based word alignment
(PAL) which previously has been shown to improve
alignment quality for English?Swedish (Holmqvist,
2010). The idea of phrase-based word alignment is
to use word and part-of-speech sequence patterns
from manual word alignments to align new texts.
First, parallel phrases containing a source segment,
a target segment and links between source and target
words are extracted from word aligned texts (Figure
1). In the second step, these phrases are matched
against new parallel text and if a matching phrase
is found, word links from the phrase are added to
the corresponding words in the new text. In order
to increase the number of matching phrases and im-
prove word alignment recall, words in the parallel
En: a typical example
De: ein typisches Beispiel
Links: 0-0 1-1 2-2
En: a JJ example
De: ein ADJA Beispiel
Links: 0-0 1-1 2-2
En: DT JJ NN
De: ART ADJA N
Links: 0-0 1-1 2-2
Figure 1: Examples of parallel phrases used in word
alignment.
BLEU NIST Meteor
Baseline 16.16 6.2742 50.89
+hier reo 16.06 6.2800 51.25
+pal-gdfa 16.14 5.6527 51.10
+pal-dual 15.71 5.5735 50.43
+pal-inter 15.92 5.6230 50.73
Table 2: English?German translation results, results
are cumulative except for the three alternative PAL-
configurations.
segments were replaced by POS/morphological tags
from RFTagger.
Alignment patterns were extracted from 1000 sen-
tences in the manually word aligned sample of
English?German Europarl texts from Pado and Lap-
ata (2006). All parallel phrases were extracted from
the word aligned texts, as when extracting a trans-
lation model. Parallel phrases that contain at least
3 words were generalized with POS tags to form
word/POS patterns for alignment. A subset of these
patterns, with high alignment precision (> 0.80) on
the 1000 sentences, were used to align the entire
training corpus.
We combined the new word alignments with
the Giza++ alignments in two ways. In the first
method, we used a symmetrization heuristic similar
to grow-diag-final-and to combine three word align-
ments into one, the phrase-based alignment and two
Giza++ alignments in different directions. In the
second method we extracted a separate phrase ta-
ble from the sparser phrase-based alignment using
a constrained method of phrase extraction that lim-
ited the number of unaligned words in each phrase
pair. The reason for constraining the phrase table
396
extraction was that the standard extraction method
does not work well for the sparse word alignments
that PAL produces, but we think it could still be
useful for extracting highly reliable phrases. After
some experimentation we decided to allow an unlim-
ited number of internal unaligned words, that is un-
aligned words that are surrounded by aligned words,
but limit the number of external unaligned words,
i.e., unaligned words at the beginning or end of the
phrase, to either one each in the source and target
phrase, or to zero.
We used two ways to include the sparse phrase-
table into the translation process:
? Have two separate phrase-tables, the sparse ta-
ble, and the standard GIZA++ based phrase-
table, and use Moses? dual decoding paths.
? Interpolate the sparse phrase-table with the
standard phrase-table, using the mixture model
formulation of Ueffing et al (2007), with equal
weights, in order to boost the probabilities of
highly reliable phrases.
4.2 Results
We evaluated our systems on devtest data and found
that the added phrase-based alignments did not pro-
duce large differences in translation quality com-
pared to the baseline system with hierarchical re-
ordering as shown in Table 2. The system created
with a heuristic combination of PAL and Giza++
(pal-gdfa) had a small increase in BLEU, but no im-
provement on the other metrics. Systems using a
phrase table extracted from the sparse alignments
did not produce better results than baseline. The sys-
tem using dual decoding paths (pal-dual) produced
worse results than the system using an interpolated
phrase table (pal-inter).
5 Submitted systems
The LIU system participated in German?English
and English?German translation in the WMT 2011
shared task. The new additions were a combina-
tion of unsupervised and supervised word align-
ments, spelling normalization, clause reordering and
OOV processing. Our submitted systems contain
all additions described in this paper. For English-
German we used the best performing method of
BLEU
System Devtest Test
en-de
baseline +hier 16.1 14.5
submitted 16.1 14.8
de-en
baseline +hier 20.9 19.3
submitted 21.2 19.9
Table 3: Summary of devtest results and shared task test
results for submitted systems and LIU baseline with hier-
archical reordering.
word alignment combination which was the method
that uses heuristic combination similar to grow-diag-
final-and.
The results of our submitted systems are shown
in Table 3 where we compare them to the LIU base-
line system with hierarchical reordering models. We
report modest improvements on the devtest set for
both translation directions. We also found small im-
provements of our submitted systems in the official
shared task evaluation on the test set newstest2011.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
Michael Collins, Philipp Koehn, and Ivona Kucerova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California.
I?lknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English statisti-
cal machine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 251?258, Paris, France.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
848?856, Honolulu, Hawaii.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
397
Workshop on Statistical Machine Translation, Edin-
burgh, UK.
Maria Holmqvist. 2010. Heuristic word alignment
with parallel phrases. In Proceedings of the Seventh
Conference on International Language Resources and
Evaluation, pages 744-748, Valletta, Malta.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Tenth Conference of EACL, pages 187?193, Budapest,
Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation, Pittsburgh, Pennsylvania.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, Demon-
stration Session, 177?180, Prague, Czech Republic.
Stefan Langer. 1998. Zur Morphologie und Semantik
von Nominalkomposita. In Tagungsband der 4. Kon-
ferenz zur Verarbeitung natu?rlicher Sprache, pages
83?97, Bonn, Germany.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Sebastian Pado and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 1161?1168, Sydney, Aus-
tralia.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, pages 777?784, Manchester, UK.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 189?194, Uppsala,
Sweden.
Sara Stymne. 2009. A comparison of merging strategies
for translation of German compounds. In Proceedings
of the EACL Student Research Workshop, pages 61?
69, Athens, Greece.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Semi-supervised model adaptation for statistical
machine translation. Machine Translation, 21(2):77?
94.
398
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 470?477,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Spell Checking Techniques for Replacement of Unknown Words and Data
Cleaning for Haitian Creole SMS Translation
Sara Stymne
Department of Computer and Information Science
Linko?ping University, Sweden
sara.stymne@liu.se
Abstract
We report results on translation of SMS mes-
sages from Haitian Creole to English. We
show improvements by applying spell check-
ing techniques to unknown words and creating
a lattice with the best known spelling equiva-
lents. We also used a small cleaned corpus to
train a cleaning model that we applied to the
noisy corpora.
1 Introduction
In this paper we report results on the WMT 2011
featured shared task on translation of SMS messages
from Haitian Creole into English, which featured a
number of challenges. The in-domain data avail-
able is small and noisy, with a lot of non-standard
language. Furthermore, Haitian Creole is a low re-
source language, for which there are few language
technology tools and corpora available.
Our main focus has been to make the best pos-
sible use of the available training data through dif-
ferent ways of cleaning the data, and by replacing
unknown words in the test data by plausible spelling
equivalents. We have also investigated effects of dif-
ferent ways to combine the available data in transla-
tion and language models.
2 Baseline system
We performed all our experiments using a stan-
dard phrase-based statistical machine translation
(PBSMT) system, trained using the Moses toolkit
(Koehn et al, 2007), with SRILM (Stolcke, 2002)
and KenLM (Heafield, 2011) for language model-
ing, and GIZA++ (Och and Ney, 2003) for word
alignment. We also used a lexicalized reordering
model (Koehn et al, 2005). We optimized each
system separately using minimum error rate train-
ing (Och, 2003). The development and devtest data
were available in two versions, as raw, noisy data,
and in a clean version, where the raw data had been
cleaned by human post-editors.
The different subcorpora had different tokeniza-
tions and casing conventions. We normalized punc-
tuation by applying a tokenizer that separated most
punctuation marks into separate tokens, excluding
apostrophes that were suspected to belong to con-
tracted words or Haitian short forms, periods for ab-
breviations, and periods in URLs. There were often
many consecutive punctuation marks; these were re-
placed by only the first of the punctuation marks.
In the English translations of the SMS data there
were often translator?s notes at the end of the transla-
tions. These were removed when introduced by two
standard formulations: Additional Notes or transla-
tor?s note/interpretation. In addition the translation
marker The SMS [ . . . ] were removed.
Case information was inconsistent, especially for
SMS data, and for this reason we lower-cased all
Haitian source data. On the English target side
we wanted to use true-cased data, since we wanted
case distinctions in the translation output. We based
the true-casing on Koehn and Haddow (2009), who
changed the case of the first word in each sentence,
to the most common case variant of that word in the
corpus when it is not sentence initial. In the noisy
SMS data, though, there were many sentences with
all capital letters that would influence this truecasing
method negatively. To address this, we modified the
algorithm to exclude sentences with more than 40%
capital letters when calculating corpus statistics, and
to lowercase all unknown capitalized words.
470
Data Sentences Words TM LM Reo TC
In-domain SMS data 17,192 35k SMS SMS yes yes
Medical domain 1,619 10k other other ? ?
Newswire domain 13,517 30k other other ? yes
Glossary 35,728 85k other other ? ?
Wikipedia parallel sentence 8,476 90k other other ? yes
Wikipedia named entities 10,499 25k other other ? ?
Haitisurf dictionary 1,687 3k other other ? yes
Krengle sentences 658 3k other other ? yes
The Bible 30,715 850k bible bible ? yes
Table 1: Corpora used for training translation models (TM), language models (LM), lexicalized reordering model
(Reo), and true-casing model (TC). All corpora are bilingual English?Haitian Creole.
All translation results are reported for the devtest
corpus, on truecased data. We report results on
three metrics, Bleu (Papineni et al, 2002), NIST
(Doddington, 2002), and Meteor optimized on flu-
ency/adequacy (Lavie and Agarwal, 2007).
3 Corpus Usage
The corpora available for the task was a small
bilingual in-domain corpus of SMT data, a limited
amount of bilingual out-of-domain corpora, such
as dictionaries and the Bible. This is different to
the common situation of domain adaptation, as in
the standard WMT shared tasks, where there is a
small bilingual in-domain corpus, a larger in-domain
monolingual corpus, and possibly several out-of-
domain corpora that can be both monolingual and
bilingual. In such a scenario it is often useful to
use all available training data for both translation
and language models, possibly in separate models
(Koehn and Schroeder, 2007).
Table 1 summarizes how we used the available
corpora, in our different models. For translation
and language models we separated the bilingual data
into three parts, the SMS data, the Bible, and every-
thing else. For our lexicalized reordering model we
only used SMS data, since we believe word order
there is likely to differ from the other corpora. For
the English true-casing model we concatenated the
English side of all bilingual corpora that were not
lower-cased.
Table 2 shows the results of the different model
combinations on the clean devtest data. When we
used only the SMS data in the translation model,
the scores changed only slightly regardless of which
combinations of language models we used. Using
two translation models for the SMS data and the
other bilingual data overall gave better results than
when only using SMS data for the translation model.
With double translation models it was best only to
use the SMS data in the language model. Including
the Bible data had a minor impact. Based on these
experiments we will use all available training data
in two translation models, one for SMS and one for
everything else, but only use SMS data in one lan-
guage model, which corresponds to the line marked
in bold in Table 2, and which we will call the dual
system.
We did not perform model combination experi-
ments for the raw input data, since we believed the
pattern would be similar as for the clean data. The
results for the raw devtest as input are considerably
lower than for the clean data. Using the best model
combination, we got a Bleu score of only 26.25,
which can be compared to 29.90 using the clean
data.
4 Data Cleaning Model
While the training data is noisy, we had access to
cleaned versions of dev, devtest and test data. We
decided to use the dev data to build a model for
cleaning the noisy SMS data. We did this by train-
ing a standard PBSMT model from raw to clean dev
data. When inspecting this translation model we
found that it very often changed the place holders
for names and phone numbers, and thus we filtered
out all entries in the phrase table that did not have
matching place holders. We then used this model to
perform monotone decoding of the raw SMS data,
thus creating a cleaner version of it.
This approach is similar to that of Aw et al
471
TMs LMs Bleu NIST Meteor
SMS SMS 29.04 5.578 52.32
SMS SMS, other 28.76 5.543 51.96
SMS SMS, other+bible 29.18 5.696 51.77
SMS, other SMS 29.78 5.808 52.86
SMS, other+bible SMS 29.90 5.764 52.88
SMS, other+bible SMS, other 29.59 5.742 52.28
SMS, other+bible SMS, other+bible 28.75 5.587 52.52
Table 2: Translation results, with different combinations of translation and language models. Model names separated
by a comma stands for separate models, and names separated with a plus for one model built from concatenated
corpora.
Model Testset Bleu NIST Meteor
Dual clean 29.90 5.764 52.88
Dual+CM clean 29.78 5.740 52.95
Dual raw 26.25 5.231 50.79
Dual raw+CM 26.26 5.348 51.30
Dual+CM raw 25.64 5.120 50.01
Dual+CM raw+CM 26.24 5.362 51.64
Table 3: Translation results, with and without an addi-
tional cleaning model (+CM) on the clean and raw devtest
data
(2006), who trained a model for translation from
English SMS language to standard written English,
with very good results both on this task itself, and
on a task of translating English SMS messages into
Chinese. For training they used up to 5000 sen-
tences, but the results stabilized already when us-
ing 3000 training sentences. Our task is different,
though, since we do not aim at standard written
Haitian, but into cleaned up SMS language, and our
training corpus is a lot smaller, only 900 sentences.
Table 3 shows the results of using the cleaning
model on training data and raw translation input. For
the clean data using the cleaning model on the train-
ing data had very little effect on any of the metrics
used. For the raw data translation results are im-
proved as measured by NIST and Meteor when we
use the filter on the devtest data, compared to using
the raw devtest data. Using the filter on the training
data gives worse results for non-filtered devtest data,
but the overall best results are had by filtering both
training and devtest data for raw translation input.
Based on these experiments we used the cleaning
model both on test and training data for raw input,
but not at all for clean input, marked in bold in Table
3.
5 Spell Checking-based Replacement of
Unknown Words
The SMS data is noisy, and there are often many
spelling variations of the same word. One exam-
ple is the word airport, which occur in the training
corpus in at least six spelling variants: the correct
ayeropo`, and aeoport, ayeopo`, aeroport, aeyopo`t,
and aewopo, and in the devtest in a seventh variant
aye?oport. The non-standardized spelling means that
many unknown words (out-of-vocabulary words,
OOVs) have a known spelling variant in the train-
ing corpus. We thus decided to treat OOVs using a
method inspired by spell-checking techniques, and
applied an approximate string matching technique
to OOVs in the translation input in order to change
them into known spelling variants.
OOV replacement has been proposed by several
researchers, replacing OOVs e.g. by morphological
variants (Arora et al, 2008) or synonyms (Mirkin et
al., 2009). Habash (2008) used several techniques
for expanding OOVs in order to extend the phrase-
table. Yang and Kirchhoff (2006) trained a morpho-
logically based back-off model for OOVs. Bertoldi
et al (2010) created confusion networks as input of
translation input with artificially created misspelled
words, not specifically targetting OOVs, however.
The work most similar to ours is DeNeefe et al
(2008), who also created lattices with spelling alter-
natives for OOVs, which did not improve translation
results, however. Contrary to us, they only consid-
ered one edit per word, and did not weigh edits or
lattice arcs.
Many standard spell checkers are based on the
noisy channel model, which use an error (channel)
model and a source model, which is normally mod-
472
eled by a language model. The error model normally
use some type of approximate string matching, such
as Levenshtein distance (Levenshtein, 1966), which
measures the distance between two strings as the
number of insertions, deletions, and substitutions of
characters. It is often normalized based on the length
of the strings (Yujian and Bo, 2007), and the dis-
tance calculation has also been improved by associ-
ating different costs to individual error operations.
Church and Gale (1991) used a large training corpus
to assign probabilities to each unique error opera-
tion, and also conditioned operations on one consec-
utive character. Brill and Moore (2000) introduced a
model that worked on character sequences, not only
on character level, and was conditioned on where
in the word the sequences occurred. They trained
weights on a corpus of misspelled words with cor-
rections.
Treating OOVs in the SMS corpus as a spell
checking problem differs from a standard spell
checking scenario in that the goal is not necessarily
to change an incorrectly spelled word into a correct
word, but rather to change a word that is not in our
corpus into a spelling variant that we have seen in the
corpus, but which might not necessarily be correctly
spelled. It is also the case that many of the OOVs are
not wrong, but just happen to be unseen; for instance
there are many place names. Thus we must make
sure that our algorithm for finding spelling equiva-
lents is bi-directional, so that it cannot only change
incorrect spellings into correct spellings, but also go
the other way, which could be needed in some cases.
We also need to try not to suggest alternatives for
words that does not have any plausible alternatives
in the corpus, such as unknown place names.
5.1 Approximate String Matching Algorithm
The approximate string matching algorithm we sug-
gest is essentially that of Brill and Moore (2000),
a modified weighted Levenshtein distance, where
we allow error operations on character sequences as
well as on single characters. We based our weight
estimations on the automatically created list of lex-
ical variants that was built as a step in building the
cleaning model, described in section 4. This list is
very noisy, but does also contain some true spelling
equivalents. We implemented two versions of the
algorithm, first a simple version which used manu-
ally identified error operations, then a more complex
variant where error operations and weights were
found automatically.
Manually Assigned Weights
We went through the lexicon list manually to iden-
tify edits that could correct the misspellings that oc-
curred in the list. We identified substitutions lim-
ited to three characters in length, and at the begin-
ning and end of words we also identified letter in-
sertions and deletions. The inspection showed that
it was very common for letters to be replaced by the
same letter but with a diacritic, or with a different
diacritic, for instance to vary between [e, e?, e`]. An-
other common operation was between a single char-
acter and two consecutive occurrences of the same
character. Table 4 shows the 46 identified opera-
tions. To account for the fact that we do not want
our error model to have a directionality from wrong
to correct, we allow operations in both directions.
Since the operations were found manually we did
not have a reliable way to estimate weights, and used
uniform weights for all operations. The operations
in Table 4 have the weights given in the table, sub-
stitution of a letter with a diacritic variant 0, single
to double letters 0.1, insertions and deletions 1 and
substitutions other than those in the table, 1.6.
Automatically Assigned Weights
To automatically train weights from the very noisy
list of lexical variants, we filtered it by applying
the edit distance with the manual weights described
above to phrase pair that did not differ in length by
more than three characters. We used a cut-off thresh-
old of 2.8 for words where both versions had at least
six characters, and 1 for shorter words. This gave
us a list of 587 plausible spelling variants, from the
original list with 1635 word pairs.
To find good character substitutions and assign
weights to them, we used standard PBSMT tech-
niques as implemented in Moses, but on character
level, with the filtered list of word pairs as train-
ing data. We inserted spaces between each character
of the words, and also added beginning and end of
word markers, e.g., the word proble?m was tokenized
as ?B p r o b l e? m E?. Thus we could train a PB-
SMT system that aligned characters using GIZA++,
and extracted and scored phrases, which in this case
473
Manual Automatic
Type Instances Weight Examples+weights Count
mid 1-1 e-i, a-o, i-y, a-e, i-u, s-c, r-w, c-k, j-g, s-z,
n-m
.2 n-m .90, e-c .74, j-g .62 12
mid 1-2 z-sz, i-iy, m-nm, n-nm, y-il, i-ye, s-rs, t-
th, o-an, x-ks, x-kz, e-a,
.2 x-ks .35, i-ue .83, w-rr .74 107
mid 1-3 ? ? e-ait .75 e-eur .66 29
mid 2-2 wa-oi, we-oi, en-un, xs-ks .2 we-oi .67, wo-ro .20, ie-ye .54 103
mid 2-3 wa-oir, ye-ier, an-ent, eo-eyo .2 iv-eve .79, ey-eyi .18 160
mid 3-3 syo-tio, syo-tyo .2 ant-ent .81, dyo,dia .67 116
beg 0-1 -h, -l .2 -n .95, -m .90, -h .50 9
beg 0-2 ? ? -te .95, -pa .82 6
beg 1-1 h-l .2 a-e .89, w-r .67 i-u .33 5
beg 1-2,3 ? ? e-ai .68, a-za .74 k-pak .48 30
beg 2,3-2,3 ? ? wo-ro 0, ex-ekz .65, ens-ins .17 58
end 0-1 -e, -t, -n, -m, -r, -y .2 -r .57 -e .85, -v .75 12
end 0-2 -te, -de, -ue, -le 1 -de .93, -le .75 7
end 1-1 ? ? e-o .74, n-m .86 5
end 1-2,3 ? ? i-li .81, c-se .62 n-nne .66 48
end 2,3-2,3 ? ? sm-me .67, ns-nce .38, wen-oin .36 70
Table 4: Error operations at the middle, beginning and end of words. For manually defined operations all instances are
shown, with their uniform score. For automaticcally identified operations examples are shown with their score, and
the total count of each operation type.
amounts to creating a phrase-table with character se-
quences. The phrase probabilities are given in both
translation directions, P (S|T ) and P (T |S). Since
we do not want our scores to have any direction, we
used the arithmetic mean of these two probabilities
to calculate the score for the pair, which is calcu-
lated as 1 ? ((P (S|T ) + P (T |S))/2), to also con-
vert the probabilities to costs. To compensate for
errors made in the extraction process, we filtered
out phrase pairs where both probabilities were lower
than 0.1.
To get fair scores for character sequences of dif-
ferent lengths we applied the phrase table construc-
tion four times, while increasing the limit of the
maximum phrase length from one to four. From the
first phrase table, with maximum length 1, we ex-
tracted 1-1 substitutions, from the second table 1-2
and 2-2 substitutions, and so on. We used the begin-
ning and end of word markers both to extract sub-
stitutions that were only used at the beginning or
end of sentences, and to extract deletions and inser-
tions used at the beginning and end of words. Again,
we only allowed substitutions up to three characters
in length. The fourth phrase-table, with phrases of
length four, were only used to allow us to extract
substitutions of length three at the beginning and end
of words, since the markers count as tokens. Table 4
shows the types of transformations extracted, some
examples of each with their score, and the count
of each transformation. A total of 777 operations
were found, compared to only 46 manual operations.
There were few substitutions with diacritic variants,
so again we allowed them with a zero cost. The costs
for deletions, additions, and substitutions not given
any weights were the same as before, 1, 1, and 1.6.
For the edit distance with the automatic weights, we
used scores that were normalized by the length of
the shortest string.
Application to OOVs
We applied the edit distance operation on all OOVs
longer than 3 characters, and calculated the distance
to all words in the training corpora that did not dif-
fer in length with more than two characters. We used
the standard dynamic programming implementation
of our edit distance, but extended to check the scores
not only in directly neighbouring cells, but in cells
up to a distance of 3 away, to account for the maxi-
mum length of the character sequence substitutions.
It would have been possible to use a fast trie imple-
474
Clean devtest Raw devtest
System Bleu NIST Meteor Bleu NIST Meteor
No OOV treatment 29.90 5.764 52.88 26.24 5.362 51.64
Manual 1-best 29.76 5.721 52.91 26.60 5.417 52.17
Automatic 1-best 29.90 5.746 52.83 26.26 5.351 51.60
Manual lattice 30.53 5.957 54.06 27.12 5.574 53.27
Automatic lattice 30.94 5.982 54.62 27.27 5.554 52.99
Automatic lattice + LM 30.33 5.912 54.07 27.79 5.555 52.98
Table 5: Translation results, using the approximate string matching algorithm for OOVs. The submitted system is
marked with bold.
mentation (Brill and Moore, 2000), however.
We performed both 1-best substitution of OOVs,
and lattice decoding where we kept the three best
alternatives for each word. In both cases we only re-
placed OOVs if the edit distance scores were below a
threshold of 1.2 for the manual weights, which were
not normalized, and for the normalized automatic
weights below 0.25, or below 0.33 for word pairs
where both words had at least 6 characters. These
thresholds were set by inspecting the results, but re-
sulted in a different number of substitutions:
? clean (total 691)
? manual: 251
? automatic: 222
? raw (total 932)
? manual: 601
? automatic: 437
The lattice arcs were weighted with the edit dis-
tance score, normalized to fall between 0-1. We also
tried to include a source language model score in
the weights in the lattice, to account for the source
model that has been shown to be useful for spelling
correction, but which has not been found useful for
OOV replacement. We trained a 3-gram language
model on the Haitian SMS text, and applied this
model for a five-word context around the replaced
OOV. We used a single lattice weight where half the
score came from the edit distance, and the other half
represented the language model component. A bet-
ter approach though, would probably have been to
use two weights.
5.2 Results
Table 5 shows the results of the OOV treatment.
When using 1-best substitutions there are small dif-
ferences compared to the baseline on both test sets,
except for the system with manual weights on raw
data, which was improved on all metrics. All three
ways of applying the lattice substitutions led to large
improvements on all metrics on both test sets. On
the clean test set it was better to use automatic than
manual weights when not using the language model
score, which made the results worse. On the raw
test set the highest Meteor and NIST scores were
had by using manual weights, whereas the highest
Bleu score was had by using automatic weights with
the language model. The system submitted to the
workshop is the system with a lattice with manual
weights, marked in bold in Table 5, since the auto-
matic weights were not ready in time for the submis-
sion.
6 Conclusion
In this article we presented methods for translat-
ing noisy Haitian Creole SMS messages, which we
believe are generally suitable for small and noisy
corpora and under-resourced languages. We used
an automatically trained cleaning model, trained on
only 900 manually cleaned sentences, that led to im-
provements for noisy translation input. Our main
contribution was to apply methods inspired by spell
checking to suggest known spelling variants of un-
known words, which we presented as a lattice to
the decoder. Several versions of this method gave
consistent improvements over the baseline system.
There are still many questions left about which con-
figuration that is best for weighting and pruning the
lattice, however, which we intend to investigate in
future work. In this work we only considered OOVs
in the translation input, but it would also be interest-
ing to address misspelled words in the training cor-
pus.
475
References
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of unknown words in phrase-based
statistical machine translation for languages of rich
morphology. In Proceedings of the First Interna-
tional Workshop on Spoken Languages Technologies
for Under-resourced languages (SLTU-2008), pages
70?75, Hanoi, Vietnam.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the ACL, poster session, pages 33?40, Syd-
ney, Australia.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.
2010. Statistical machine translation of texts with
misspelled words. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference of
the NAACL, pages 412?419, Los Angeles, California,
USA.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the ACL,
pages 286?293, Hong Kong.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1:93?103.
Steve DeNeefe, Ulf Hermjakob, and Kevin Knight. 2008.
Overcoming vocabulary sparsity in MT using lattices.
In Proceedings of the 8th Conference of the Associa-
tion for Machine Translation in the Americas, pages
89?96, Waikiki, Hawaii, USA.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California, USA.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in Arabic-English statisti-
cal machine translation. In Proceedings of the 46th
Annual Meeting of the ACL: Human Language Tech-
nologies, Short papers, pages 57?60, Columbus, Ohio,
USA.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT 2009 shared task
with reordering and speed improvements to Moses. In
Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 160?164, Athens, Greece.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 224?227, Prague, Czech
Republic, June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation, Pittsburgh, Pennsylvania,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, demon-
stration session, pages 177?180, Prague, Czech Re-
public.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions and reversals.
Soviet Physics Doklady, 10(8):707?710.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translating
unknown terms. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 791?799, Sun-
tec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the ACL, pages 311?318,
Philadelphia, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA.
476
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of the 11th Confer-
ence of the EACL, pages 41?48, Trento Italy.
Li Yujian and Liu Bo. 2007. A normalized Levenshtein
distance metric. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 29(6):1091?1095.
477
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?34,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Clustered Word Classes for Preordering in Statistical Machine
Translation
Sara Stymne
Linko?ping University, Sweden
sara.stymne@liu.se
Abstract
Clustered word classes have been used in
connection with statistical machine transla-
tion, for instance for improving word align-
ments. In this work we investigate if clus-
tered word classes can be used in a pre-
ordering strategy, where the source lan-
guage is reordered prior to training and
translation. Part-of-speech tagging has pre-
viously been successfully used for learn-
ing reordering rules that can be applied
before training and translation. We show
that we can use word clusters for learn-
ing rules, and significantly improve on a
baseline with only slightly worse perfor-
mance than for standard POS-tags on an
English?German translation task. We also
show the usefulness of the approach for
the less-resourced language Haitian Creole,
for translation into English, where the sug-
gested approach is significantly better than
the baseline.
1 Introduction
Word order differences between languages are
problematic for statistical machine translation
(SMT). If the word orders of two languages have
large differences, the standard methods do not
tend to work well, with difficulties in many steps
such as word alignment and modelling of reorder-
ing in the decoder. This can be addressed by ap-
plying a preordering method, that is, to reorder the
source side of the corpus to become similar to the
target side, prior to training and translation. The
rules used for reordering are generally based on
some kind of linguistic annotation, such as part-
of-speech tags (POS-tags).
For many languages in the world, so called less-
resourced languages, however, part-of-speech
taggers, or part-of-speech tagged corpora that can
be used for training a tagger, are not available. In
this study we investigate if it is possible to use
unsupervised POS-tags, in the form of clustered
word classes, as a basis for learning reordering
rules for SMT. Unsupervised tagging methods can
be used for any language where a corpus is avail-
able. This means that we can potentially benefit
from preordering even for languages where tag-
gers are available.
We present experiments on two data sets. First
an English?German test set, where we can com-
pare the results of clustered word classes with
standard tags. We show that both types of tags
beat a baseline without preordering, and that clus-
tered tags perform nearly as well as standard tags.
English and German is an interesting case for re-
ordering experiments, since there are both long
distance movement of verbs and local word or-
der differences, for instance due to differences in
adverb placements. We also apply the method
to translation from the less-resourced language
Haitian Creole into English, and show that it leads
to an improvement over a baseline. The differ-
ences in word order between these two languages
are smaller than for English?German.
Besides potentially improving SMT for less-
resourced languages, the presented approach can
also be used as an extrinsic evaluation method for
unsupervised POS-tagging methods. This is espe-
cially useful for the task of word class clustering
which is hard to evaluate.
2 Unsupervised POS-tagging
There have been several suggestions of clustering
methods for obtaining word classes that are com-
pletely unsupervised, and induce classes from raw
28
text. Brown et al (1992) described a hierarchical
word clustering method which maximizes the mu-
tual information of bigrams. Schu?tze (1995) de-
scribed a distributional clustering algorithm that
uses global context vectors as a basis for clus-
tering. Biemann (2006) described a graph-based
clustering methods for word classes. Goldwa-
ter and Griffiths (2007) used Bayesian reasoning
for word class induction. Och (1999) described
a method for determining bilingual word classes,
used to improve the extraction of alignment tem-
plates through alignments between classes, not
only between words. He also described a mono-
lingual word clustering method, which is based
on a maximum likelihood approach, using the fre-
quencies of unigrams and bigrams in the training
corpus.
The above methods are fully unsupervised, and
produce unlabelled classes. There has also been
work on what Goldwater and Griffiths (2007)
call POS disambiguation, where the learning of
classes is constrained by a dictionary of the al-
lowable tags for each word. Such work has for
instance been based on hidden Markov models
(Merialdo, 1994), log-linear models (Smith and
Eisner, 2005), and Bayesian reasoning (Goldwa-
ter and Griffiths, 2007).
Word clusters have previously been used for
SMT for improving word alignment (Och, 1999),
in a class-based language model (Costa-jussa` et
al., 2007) or for extracting gappy patterns (Gim-
pel and Smith, 2011). To the best of our knowl-
edge this is the first study of applying clustered
word classes for creating pre-translation reorder-
ing rules. The most similar work we are aware
of is Costa-jussa` and Fonollosa (2006) who used
clustered word classes in a strategy they call sta-
tistical machine reordering, where the corpus is
translated into a reordered language using stan-
dard SMT techniques in a pre-processing step.
The addition of word classes led to improvements
over just using surface form, but no comparison
to using POS-tags were shown. Clustered word
classes have also been used in a discriminate re-
ordering model (Zens and Ney, 2006), and were
shown to reduce the classification error rate.
Word clusters have also been used for unsu-
pervised and semi-supervised parsing. Klein and
Manning (2004) used POS-tags as the basis of a
fully unsupervised parsing method, both for de-
pendency and constituency parsing. They showed
that clustered word classes can be used instead of
conventional POS-tags, with some result degra-
dation, but that it is better than several baseline
systems. Koo et al (2008) used features based on
clustered word classes for semi-supervised depen-
dency parsing and showed that using word class
features together with POS-based features led to
improvements, but using word class features in-
stead of POS-based features only degraded results
somewhat.
3 Reordering for SMT
There is a large amount of work on reordering
for statistical machine translation. One way to
approach reordering is by extending the transla-
tion model, either by adding extra models, such
as lexicalized (Koehn et al, 2005) or discrimina-
tive (Zens and Ney, 2006) reordering models or
by directly modelling reordering in hierarchical
(Chiang, 2007) or syntactical translation models
(Yamada and Knight, 2002).
Preordering is another common strategy for
handling reordering. Here the source side of the
corpus is transformed in a preprocessing step to
become more similar to the target side. There
have been many suggestions of preordering strate-
gies. Transformation rules can be handwrit-
ten rules targeting known syntactic differences
(Collins et al, 2005; Popovic? and Ney, 2006),
or they can be learnt automatically (Xia and Mc-
Cord, 2004; Habash, 2007). In these studies the
reordering decision was taken deterministically
on the source side. This decision can be delayed
to decoding time by presenting several reordering
options to the decoder as a lattice (Zhang et al,
2007; Niehues and Kolss, 2009) or as an n-best
list (Li et al, 2007).
Generally reordering rules are applied to the
source language, but there have been attempts at
target side reordering as well (Na et al, 2009).
Reordering rules can be based on different lev-
els of linguistic annotation, such as POS-tags
(Niehues and Kolss, 2009), chunks (Zhang et al,
2007) or parse trees (Xia and McCord, 2004).
Common for all these levels is that a tool like a
tagger or parser is needed for them to work.
In all the above studies, the reordering rules are
applied to the translation input, but they are only
applied to the training data in a few cases, for in-
stance in Popovic? and Ney (2006). Rottmann and
Vogel (2007) compared two strategies for reorder-
29
ing the training corpus, by using alignments, and
by applying the reordering rules to create a lat-
tice from which they extracted the 1-best reorder-
ing. They found that it was better to use the latter
option, to reorder the training data based on the
rules, than to use the original order in the train-
ing data. Using alignment-based reordering was
not successful, however. Another option for us-
ing reorderings in the training data was presented
by Niehues et al (2009), who directly extracted
phrase pairs from reordering lattices, and showed
a small gain over non-reordered training data.
3.1 POS-based Preordering
Our work is based on the POS-based reorder-
ing model described by Niehues and Kolss
(2009), in which POS-based rules are extracted
from a word aligned corpus, where the source
side is part-of-speech tagged. There are two
types of rules. Short-range rules (Rottmann
and Vogel, 2007) contain a pattern of POS-tags,
and a possible reordering to resemble the tar-
get language, such as VVIMP VMFIN PPER ?
PPER VMFIN VVIMP, which moves a personal
pronoun to a position in front of a verb group.
Long-range rules were designed to cover move-
ments over large spans, and also contain gaps
that can match one or several words, such as
VAFIN * VVPP ? VAFIN VVPP *, which
moves the two parts of a German verbs together
past an object of any size, so as to resemble En-
glish.
Short-range rules are extracted by identifying
POS-sequences in the training corpus where there
are crossing alignments. The rules are stored as
the part-of-speech pattern of the source on the left
hand side of the rule, and the pattern correspond-
ing to the target side word order on the right hand
side.
Long-range rules are extracted in a similar way,
by identifying two neighboring POS-sequences
on the source side that have crossed alignments.
Gaps are introduced into the rules by replacing
either the right hand side or the left hand side
by a wild card. In order to constrain the appli-
cation of these rules, the POS-tag to the left of the
rule is included in the rule. Depending on the lan-
guage pair it might be advantageous to use rules
that have wildcards either on the left or right hand
side. For German-to-English translation, the main
long distance movement is that verbs move to the
left, and, as shown by Niehues and Kolss (2009),
it is advantageous to use only long-range rules
with left-wildcards, as in the example rule above.
For the other translation direction, it is important
to move verbs to the right, and thus right-wildcard
rules were better.
The probability of both short and long range
rules is calculated by relative frequencies as the
number of times a rule occurs divided by the num-
ber of times the source side occurs in the training
data.
In a preprocessing step to decoding, all rules
are applied to each input sentence, and when a
rule applies, the alternative word order is added
to a word lattice. To keep lattices of a reason-
able size, Niehues and Kolss (2009) suggested us-
ing a threshold of 0.2 for the probability of short-
range rules, of 0.05 for the probability of long
range rules, and blocked rules that could be ap-
plied more than 5 times to the same sentence. We
adopt these threshold values.
In this work we use the short-range reorder-
ing rules of Rottmann and Vogel (2007) and the
long-range rules of Niehues and Kolss (2009). As
suggested we use only right-wildcard rules for
English?German translation. For Haitian Creole,
we have no prior knowledge of the reordering di-
rection, and thus choose to use both left and right
long-range rules. In previous work only one stan-
dard POS-tagset was explored. In this work we in-
vestigate the effect of different type of annotation
schemes, besides only POS-tags. We use several
types of tags from a parser, and compare them to
using unsupervised tags in the form of clustered
word classes. We also apply the reordering tech-
niques to translation from Haitian Creole, a less-
resourced language for which no POS-tagger is
available.
4 Experimental Setup
We conducted experiments for two language
pairs, English?German and Haitian Creole?
English. We always applied the reordering rules
to the translation input, creating a lattice of pos-
sible reorderings as input to the decoder. For the
training data we applied two strategies. As the
first option we used training data from the base-
line system with original word order. As the sec-
ond option we reordered the training data as well,
using the learnt reordering rules to create reorder-
ing lattices for the training data, from which we
30
ID Form Lemma Dependency Functional tag Syntax POS Morphology
1 Resumption resumption main:>0 @NH %NH N NOM SG
2 of of mod:>1 @<NOM-OF %N< PREP
3 the the attr:>4 @A> %>N DET
4 session session pcomp:>2 @<P %NH N NOM SG
Table 1: Parser output
extracted the 1-best reordering, as suggested by
Rottmann and Vogel (2007).
For the supervised tagging of the English
source side we use a commercial functional de-
pendency parser.1 The main reason for using a
parser instead of a tagger was that we wanted to
explore the effect of different tagging schemes,
which was available from this parser. An example
of a tagged English text can be seen in Table 1.
In this work we used four types of tags extracted
from the parser output, part-of-speech tags (pos),
dependency tags (dep), functional tags (func) and
shallow syntax tags (syntax). The dependency
tags consist of the dependency label of the word
and the POS-tag of its dependent. For the exam-
ple in Table 1, the sequence of dependency tags
is: main TOP mod N attr N pcomp PREP.
The other tag types are directly exemplified in Ta-
ble 1. The tagsets have different sizes, as shown
in Table 2.
For the unsupervised tags, we used clustered
word classes obtained using the mkcls software,2
which implements the approach of Och (1999).
We explored three different numbers of clusters,
50, 125, and 625. The clustering was performed
on the same corpus as the SMT training.
The translation system used is a standard
phrase-based SMT system. The translation model
was trained by first creating unidirectional word
alignments in both directions using GIZA++ (Och
and Ney, 2003), which are then symmetrized
by the grow-diag-final-and method (Koehn et al,
2005). From this many-to-many alignment, con-
sistent phrases of up to length 7 were extracted.
A 5-gram language model was used, produced
by SRILM (Stolcke, 2002). For training and de-
coding we used the Moses toolkit (Koehn et al,
2007) and the feature weights were optimized
using minimum error rate training (Och, 2003).
1http://www.connexor.eu/technology/
machinese/machinesesyntax/
2http://www-i6.informatik.
rwth-aachen.de/web/Software/mkcls.html
Tagset Classes Rules Paths
pos 23 319147 2.1e09
dep 523 328415 2.8e09
func 49 325091 1.5e10
syntax 20 315407 4.5e11
class50 50 303292 6.2e09
class125 125 271348 1.3e07
class625 625 211606 31654
Table 2: Number of tags for each tagset in the English
training corpus, number of rules extracted for each
tagset, and average numbers of paths per sentence in
the testset lattice using each tagset to create rules
The baseline systems were trained using no ad-
ditional preordering, only a distance-based re-
ordering penalty for modelling reordering. For
the Haitian Creole?English experiments we also
added a lexicalized reordering model (Koehn et
al., 2005), both to the baseline and to the re-
ordered systems.
For the English?German experiments, the
translation system was trained and tested using a
part of the Europarl corpus (Koehn, 2005). The
training part contained 439513 sentences and 9.4
million words. Sentences longer than 40 words
were filtered out. The test set has 2000 sentences
and the development set has 500 sentences.
For the Haitian Creole?English experiments
we used the SMS corpus released for WMT11
(Callison-Burch et al, 2011). The corpus con-
tains 17192 sentences and 352326 words. The
test and development data both contain 900 sen-
tences each. Since we know of no POS-tagger for
Haitian Creole, we only compare the clustered re-
sult to a baseline system.
Reordering rules were extracted from the same
corpora that were used for training the SMT sys-
tem. The word alignments needed for reordering
were created using GIZA++ (Och and Ney, 2003),
an implementation of the IBM models (Brown et
al., 1993) of alignment, which is trained in a fully
unsupervised manner based on the EM algorithm
(Dempster et al, 1977).
31
5 Results
Table 2 shows the number of rules, and the av-
erage number of paths for each sentence in the
test data lattice, using each tagset. For the stan-
dard tagsets the number of rules is relatively con-
stant, despite the fact that the number of tags in
the tagsets are quite different. For the clustered
word classes, there are slightly fewer rules with
50 classes than for the standard tags, and the num-
ber of rules decreases with a higher number of
classes. For the average number of lattice paths
per sentence, there are some differences for the
standard tags, but it is not related to tagset size.
Again, the clustering with 50 classes has a simi-
lar number as the standard classes, but here there
is a sharp decrease of lattice paths with a higher
number of classes.
The translation results for the English?German
experiments are shown in Table 3. We report
translation results for two metrics, Bleu (Papineni
et al, 2002) and NIST (Doddington, 2002), and
significance testing is performed using approxi-
mate randomization (Riezler andMaxwell, 2005),
with 10,000 iterations. All the systems with re-
ordering have higher scores than the baseline on
both metrics. This difference is always significant
for NIST, and significant for Bleu in all cases ex-
cept for two systems, one with standard tags and
one with clustered tags. Between most of the sys-
tems with reordering the differences are small and
most of them are not significant. Overall the sys-
tems with standard word classes perform slightly
better than the clustered systems, especially the
func tagset gives consistently high results, and is
significantly better than four of the clustered sys-
tems on Bleu, and than one system on NIST. The
fact that the number of paths were much smaller
for a high number of clustered classes than for the
other tagsets does not seem to have influenced the
translation results.
Clustering of word classes is nondeterministic,
and several runs of the cluster methods give dif-
ferent results, which could influence the transla-
tion results as well. To investigate this, we reran
the experiment with 50 classes and baseline train-
ing data three times. The differences of the re-
sults between these runs were small, Bleu varied
between 20.08?20.19 and NIST varied between
5.99?6.01. This variation is smaller than the dif-
ference between the baseline and the reordering
Baseline training Reordered training
Tagset Bleu NIST Bleu NIST
Baseline 19.84 5.92 ? ?
pos 20.34** 6.05** 20.26** 5.98*
dep 20.11 6.03** 20.25** 6.06**
func 20.40** 6.05** 20.40** 6.06**
syntax 20.29** 6.07** 20.32** 6.06**
class50 20.15* 6.05** 20.15* 5.99**
class125 20.15* 6.03** 20.17* 6.02**
class625 20.19** 6.05** 20.07 6.05**
Table 3: Translation results for English?German. Sta-
tistically significant differences from baseline scores
are marked * (p < 0.05), ** (p < 0.01).
Tagset Classes Rules Paths
class50 50 4588 3.70
class125 125 3554 1.46
class625 625 2388 1.42
Table 4: Number of classes for Haitian Creole, number
of rules extracted for each tagset, and average numbers
of paths per sentence in the testset lattice using each
tagset to create rules
systems, and should not influence the overall con-
clusions.
For the Haitian Creole testset both the average
number of reorderings per sentence, and the num-
ber of rules, are substantially lower than for the
English testset. As shown in Table 4, the trends
are the same, however. With a higher number of
classes there are both fewer rules and fewer rule
applications. That there are few rules and paths
can both depend on the fact that there are fewer
word order differences between these languages,
that the corpus is smaller, and that the sentence
length is shorter.
Even though the number of reorderings is rel-
atively small, there are consistent significant im-
provements for all reordered options on both Bleu
and NIST compared to the baseline, as shown in
Table 5. Between the clustered systems the dif-
ferences are relatively small, and the only sig-
nificant differences are that the system with 50
classes and reordered training data is worse on
Bleu than 50 classes with baseline reordering and
125 classes with reordered training data, at the
0.05-level. The trend for the systems with 125 and
625 classes is in the other direction with slightly
higher results with reordered data. There is hardly
any difference between these two systems, which
is not surprising, seeing that the number of ap-
32
Baseline training Reordered training
Tagset Bleu NIST Bleu NIST
Baseline 29.04 5.58 ? ?
class50 29.59** 5.73** 29.60** 5.69**
class125 29.52** 5.70** 29.78** 5.73**
class625 29.55** 5.70** 29.75** 5.74**
Table 5: Translation results for Haitian Creole?
English. Statistically significant differences from
baseline BLEU score are marked ** (p < 0.01).
plied rules is very similar.
6 Conclusion and Future Work
We have presented experiments of using clustered
word classes as input to a preordering method for
SMT. We showed that the proposed method per-
form better than a baseline and nearly on par with
using standard tags for an English?German trans-
lation task. We also showed that it can improve
results over a baseline when translating from the
less-resourced language Haitian Creole into En-
glish, even though the word order differences be-
tween these languages are relatively small.
The suggested preordering algorithm with
word classes is fully unsupervised, since unsuper-
vised methods are used both for word classes and
word alignments that are the basis of the preorder-
ing algorithm. This means that the method can
be applied to less-resourced languages where no
taggers or parsers are available, which is not the
case for the many preordering methods which are
based on POS-tags or parse trees.
This initial study is quite small, and in the fu-
ture we plan to extend it to larger corpora and
other language pairs. We would also like to com-
pare the performance of different unsupervised
word clustering and POS-tagging methods on this
task.
Acknowledgments
I would like to thank Jan Niehues for sharing his
code, and for his help on the POS-based reorder-
ing, and Joakim Nivre and the anonymous review-
ers for their insightful comments.
References
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, pages 7?12, Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of WMT, pages 22?64, Edinburgh, Scot-
land.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):202?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, USA.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
EMNLP, pages 70?76, Sydney, Australia.
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lam-
bert, Maxim Khalilov, Jose? A. R. Fonollosa, Jose? B.
Marin?o, and Rafael E. Banchs. 2007. Ngram-based
statistical machine translation enhanced with mul-
tiple weighted reordering hypotheses. In Proceed-
ings of WMT, pages 167?170, Prague, Czech Re-
public.
Arthur E. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal
Statistical Society, 39(1):1?38.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology, pages 228?231, San Diego, California,
USA.
Kevin Gimpel and Noah A. Smith. 2011. Genera-
tive models of monolingual and bilingual gappy pat-
terns. In Proceedings of WMT, pages 512?522, Ed-
inburgh, Scotland.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of ACL, pages 744?751,
Prague, Czech Republic.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proceedings of MT
Summit XI, pages 215?222, Copenhagen, Denmark.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
ACL, pages 478?485, Barcelona, Spain.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
33
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of the International Workshop on
Spoken Language Translation, Pittsburgh, Pennsyl-
vania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL, demonstration
session, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603, Columbus,
Ohio.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the ACL, pages 720?727, Prague,
Czech Republic.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?172.
Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-Hyeok
Lee. 2009. Improving fluency by reordering tar-
get constituents using MST parser in English-to-
Japanese phrase-based SMT. In Proceedings of
MT Summit XII, pages 276?283, Ottawa, Ontario,
Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of WMT, pages 206?214, Athens, Greece.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
translation system for the EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 80?84, Athens, Greece.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71?76, Bergen, Norway.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of ACL, pages 311?318, Philadelphia,
Pennsylvania, USA.
Maja Popovic? and Hermann Ney. 2006. POS-based
reorderings for statistical machine translation. In
Proceedings of LREC, pages 1278?1283, Genoa,
Italy.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance
testing for MT. In Proceedings of the Workshop
on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization at ACL?05, pages 57?64,
Ann Arbor, Michigan, USA.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sko?vde, Sweden.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148,
Dublin, Ireland.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL, pages 354?362, Ann
Arbor, Michigan, USA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA.
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proceedings of CoLing, pages
508?514, Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A decoder
for syntax-based statistical MT. In Proceedings of
ACL, pages 303?310, Philadelphia, Pennsylvania,
USA.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings of WMT, pages 55?63, New
York City, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 21?28, Trento, Italy.
34
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225?231,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Tunable Distortion Limits and Corpus Cleaning for SMT
Sara Stymne Christian Hardmeier Jo?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tem for WMT13, for English-to-German
translation. We use the Docent decoder,
a local search decoder that translates at
the document level. We add tunable dis-
tortion limits, that is, soft constraints on
the maximum distortion allowed, to Do-
cent. We also investigate cleaning of the
noisy Common Crawl corpus. We show
that we can use alignment-based filtering
for cleaning with good results. Finally we
investigate effects of corpus selection for
recasing.
1 Introduction
In this paper we present the Uppsala University
submission to WMT 2013. We have submitted one
system, for translation from English to German.
In our submission we use the document-level de-
coder Docent (Hardmeier et al, 2012; Hardmeier
et al, 2013). In the current setup, we take advan-
tage of Docent in that we introduce tunable dis-
tortion limits, that is, modeling distortion limits as
soft constraints instead of as hard constraints. In
addition we perform experiments on corpus clean-
ing. We investigate how the noisy Common Crawl
corpus can be cleaned, and suggest an alignment-
based cleaning method, which works well. We
also investigate corpus selection for recasing.
In Section 2 we introduce our decoder, Docent,
followed by a general system description in Sec-
tion 3. In Section 4 we describe our experiments
with corpus cleaning, and in Section 5 we describe
experiments with tunable distortion limits. In Sec-
tion 6 we investigate corpus selection for recasing.
In Section 7 we compare our results with Docent
to results using Moses (Koehn et al, 2007). We
conclude in Section 8.
2 The Docent Decoder
Docent (Hardmeier et al, 2013) is a decoder for
phrase-based SMT (Koehn et al, 2003). It differs
from other publicly available decoders by its use
of a different search algorithm that imposes fewer
restrictions on the feature models that can be im-
plemented.
The most popular decoding algorithm for
phrase-based SMT is the one described by Koehn
et al (2003), which has become known as stack
decoding. It constructs output sentences bit by
bit by appending phrase translations to an initially
empty hypothesis. Complexity is kept in check,
on the one hand, by a beam search approach that
only expands the most promising hypotheses. On
the other hand, a dynamic programming technique
called hypothesis recombination exploits the lo-
cality of the standard feature models, in particu-
lar the n-gram language model, to achieve a loss-
free reduction of the search space. While this de-
coding approach delivers excellent search perfor-
mance at a very reasonable speed, it limits the
information available to the feature models to an
n-gram window similar to a language model his-
tory. In stack decoding, it is difficult to implement
models with sentence-internal long-range depen-
dencies and cross-sentence dependencies, where
the model score of a given sentence depends on
the translations generated for another sentence.
In contrast to this very popular stack decod-
ing approach, our decoder Docent implements a
search procedure based on local search (Hard-
meier et al, 2012). At any stage of the search pro-
cess, its search state consists of a complete docu-
ment translation, making it easy for feature mod-
els to access the complete document with its cur-
rent translation at any point in time. The search
algorithm is a stochastic variant of standard hill
climbing. At each step, it generates a successor
of the current search state by randomly applying
225
one of a set of state changing operations to a ran-
dom location in the document. If the new state
has a better score than the previous one, it is ac-
cepted, else search continues from the previous
state. The operations are designed in such a way
that every state in the search space can be reached
from every other state through a sequence of state
operations. In the standard setup we use three op-
erations: change-phrase-translation replaces the
translation of a single phrase with another option
from the phrase table, resegment alters the phrase
segmentation of a sequence of phrases, and swap-
phrases alters the output word order by exchang-
ing two phrases.
In contrast to stack decoding, the search algo-
rithm in Docent leaves model developers much
greater freedom in the design of their feature func-
tions because it gives them access to the transla-
tion of the complete document. On the downside,
there is an increased risk of search errors because
the document-level hill-climbing decoder cannot
make as strong assumptions about the problem
structure as the stack decoder does. In prac-
tice, this drawback can be mitigated by initializing
the hill-climber with the output of a stack decod-
ing pass using the baseline set of models without
document-level features (Hardmeier et al, 2012).
Since its inception, Docent has been used to ex-
periment with document-level semantic language
models (Hardmeier et al, 2012) and models to
enhance text readability (Stymne et al, 2013b).
Work on other discourse phenomena is ongoing.
In the present paper, we focus on sentence-internal
reordering by exploiting the fact that Docent im-
plements distortion limits as soft constraints rather
than strictly enforced limitations. We do not in-
clude any of our document-level feature functions.
3 System Setup
In this section we will describe our basic system
setup. We used all corpora made available for
English?German by the WMT13 workshop. We
always concatenated the two bilingual corpora Eu-
roparl and News Commentary, which we will call
EP-NC. We pre-processed all corpora by using
the tools provided for tokenization and we also
lower-cased all corpora. For the bilingual corpora
we also filtered sentence pairs with a length ra-
tio larger than three, or where either sentence was
longer than 60 tokens. Recasing was performed as
a post-processing step, trained using the resources
in the Moses toolkit (Koehn et al, 2007).
For the language model we trained two sepa-
rate models, one on the German side of EP-NC,
and one on the monolingual News corpus. In
both cases we trained 5-gram models. For the
large News corpus we used entropy-based prun-
ing, with 10?8 as a threshold (Stolcke, 1998). The
language models were trained using the SRILM
toolkit (Stolcke, 2002) and during decoding we
used the KenLM toolkit (Heafield, 2011).
For the translation model we also trained two
models, one with EP-NC, and one with Common
Crawl. These two models were interpolated and
used as a single model at decoding time, based on
perplexity minimization interpolation (Sennrich,
2012), see details in Section 4. The transla-
tion models were trained using the Moses toolkit
(Koehn et al, 2007), with standard settings with
5 features, phrase probabilities and lexical weight-
ing in both directions and a phrase penalty. We ap-
plied significance-based filtering (Johnson et al,
2007) to the resulting phrase tables. For decod-
ing we used the Docent decoder with random ini-
tialization and standard parameter settings (Hard-
meier et al, 2012; Hardmeier et al, 2013), which
beside translation and language model features in-
clude a word penalty and a distortion penalty.
Parameter optimization was performed using
MERT (Och, 2003) at the document-level (Stymne
et al, 2013a). In this setup we calculate both
model and metric scores on the document-level
instead of on the sentence-level. We produce k-
best lists by sampling from the decoder. In each
optimization run we run 40,000 hill-climbing it-
erations of the decoder, and sample translations
with interval 100, from iteration 10,000. This
procedure has been shown to give competitive re-
sults to standard tuning with Moses (Koehn et
al., 2007) with relatively stable results (Stymne
et al, 2013a). For tuning data we concate-
nated the tuning sets news-test 2008?2010 and
newssyscomb2009, to get a higher number of doc-
uments. In this set there are 319 documents and
7434 sentences.
To evaluate our system we use newstest2012,
which has 99 documents and 3003 sentences. In
this article we give lower-case Bleu scores (Pap-
ineni et al, 2002), except in Section 6 where we
investigate the effect of different recasing models.
226
Cleaning Sentences Reduction
None 2,399,123
Basic 2,271,912 5.3%
Langid 2,072,294 8.8%
Alignment-based 1,512,401 27.0%
Table 1: Size of Common Crawl after the different
cleaning steps and reduction in size compared to
the previous step
4 Cleaning of Common Crawl
The Common Crawl (CC) corpus was collected
from web sources, and was made available for the
WMT13 workshop. It is noisy, with many sen-
tences with the wrong language and also many
non-corresponding sentence pairs. To make better
use of this resource we investigated two methods
for cleaning it, by making use of language identi-
fication and alignment-based filtering. Before any
other cleaning we performed basic filtering where
we only kept pairs where both sentences had at
most 60 words, and with a length ratio of maxi-
mum 3. This led to a 5.3% reduction of sentences,
as shown in Table 1.
Language Identification For language identifi-
cation we used the off-the-shelf tool langid.py (Lui
and Baldwin, 2012). It is a python library, cover-
ing 97 languages, including English and German,
trained on data drawn from five different domains.
It uses a naive Bayes classifier with a multino-
mial event model, over a mixture of byte n-grams.
As for many language identification packages it
works best for longer texts, but Lui and Bald-
win (2012) also showed that it has good perfor-
mance for short microblog texts, with an accuracy
of 0.89?0.94.
We applied langid.py for each sentence in the
CC corpus, and kept only those sentence pairs
where the correct language was identified for both
sentences with a confidence of at least 0.999. The
total number of sentences was reduced by a further
8.8% based on the langid filtering.
We performed an analysis on a set of 1000 sen-
tence pairs. Among the 907 sentences that were
kept in this set we did not find any cases with
the wrong language. Table 2 shows an analysis
of the 93 sentences that were removed from this
test set. The overall accuracy of langid.py is much
higher than indicated in the table, however, since
it does not include the correctly identified English
and German sentences. We grouped the removed
sentences into four categories, cases where both
languages were correctly identified, but under the
confidence threshold of 0.999, cases where both
languages were incorrectly identified, and cases
where one language was incorrectly identified.
Overall the language identification was accurate
on 54 of the 93 removed sentences. In 18 of the
cases where it was wrong, the sentences were not
translation correspondents, which means that we
only wrongly removed 21 out of 1000 sentences.
It was also often the case when the language was
wrongly identified, that large parts of the sentence
consisted of place names, such as ?Forums about
Conil de la Frontera - Ca?diz.? ? ?Foren u?ber Conil
de la Frontera - Ca?diz.?, which were identified as
es/ht instead of en/de. Even though such sentence
pairs do correspond, they do not contain much use-
ful translation material.
Alignment-Based Cleaning For the alignment-
based cleaning, we aligned the data from the pre-
vious step using GIZA++ (Och and Ney, 2003)
in both directions, and used the intersection of
the alignments. The intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics, like grow-diag-final-and (Koehn et
al., 2005). Our hypothesis was that sentence pairs
with very few alignment points in the intersection
would likely not be corresponding sentences.
We used two types of filtering thresholds based
on alignment points. The first threshold is for the
ratio of the number of alignment points and the
maximum sentence length. The second threshold
is the absolute number of alignment points in a
sentence pair. In addition we used a third thresh-
old based on the length ratio of the sentences.
To find good values for the filtering thresholds,
we created a small gold standard where we man-
ually annotated 100 sentence pairs as being cor-
responding or not. In this set the sentence pairs
did not match in 33 cases. Table 3 show results for
some different values for the threshold parameters.
Overall we are able to get a very high precision
on the task of removing non-corresponding sen-
tences, which means that most sentences that are
removed based on this cleaning are actually non-
corresponding sentences. The recall is a bit lower,
indicating that there are still non-corresponding
sentences left in our data. In our translation sys-
tem we used the bold values in Table 3, since it
gave high precision with reasonable recall for the
removal of non-corresponding sentences, meaning
227
Identification Total Wrong lang. Non-corr Corr Languages identified
English and German < 0.999 15 0 7 8
Both English and German wrong 6 2 2 2 2:na/es, 2:et/et, 1: es/an, 1:es/ht
English wrong 13 1 6 6 5: es 4: fr 1: br, it, de, eo
German wrong 59 51 3 5 51: en 3: es 2:nl 1: af, la, lb
Total 93 54 18 21
Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out of
a 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)
pairs.
Ratio align Min align Ratio length Prec. Recall F Kept
0.1 4 2 0.70 0.77 0.73 70%
0.28 4 2 0.94 0.72 0.82 57%
0.42 4 2 1.00 0.56 0.72 41%
0.28 2 2 0.91 0.73 0.81 59%
0.28 6 2 0.94 0.63 0.76 51%
0.28 4 1.5 0.94 0.65 0.77 52%
0.28 4 3 0.91 0.75 0.82 60%
Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre-
cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of kept
sentence pairs
that we kept most correctly aligned sentence pairs.
This cleaning method is more aggressive than
the other cleaning methods we described. For the
gold standard only 57% of sentences were kept,
but in the full training set it was a bit higher, 73%,
as shown in Table 1.
Phrase Table Interpolation To use the CC cor-
pus in our system we first trained a separate phrase
table which we then interpolated with the phrase
table trained on EP-NC. In this way we could al-
ways run the system with a single phrase table. For
interpolation, we used the perplexity minimization
for weighted counts method by Sennrich (2012).
Each of the four weights in the phrase table, back-
ward and forward phrase translation probabilities
and lexical weights, are optimized separately. This
method minimizes the cross-entropy based on a
held-out corpus, for which we used the concate-
nation of all available News development sets.
The cross-entropy and the contribution of CC
relative to EP-NC, are shown for phrase transla-
tion probabilities in both directions in Table 4. The
numbers for lexical weights show similar trends.
For each cleaning step the cross-entropy is re-
duced and the contribution of CC is increased. The
difference between the basic cleaning and langid is
very small, however. The alignment-based clean-
ing shows a much larger effect. After that cleaning
step the CC corpus has a similar contribution to
EP-NC. This is an indicator that the final cleaned
CC corpus fits the development set well.
p(S|T ) p(T |S)
Cleaning CE IP CE IP
Basic 3.18 0.12 3.31 0.06
Langid 3.17 0.13 3.29 0.07
Alignment-based 3.02 0.47 3.17 0.61
Table 4: Cross-entropy (CE) and relative interpo-
lation weights (IP) compared to EP-NC for the
Common Crawl corpus, with different cleaning
Results In Table 5 we show the translation re-
sults with the different types of cleaning of CC,
and without it. We show results of different corpus
combinations both during tuning and testing. We
see that we get the overall best result by both tun-
ing and testing with the alignment-based cleaning
of CC, but it is not as useful to do the extra clean-
ing if we do not tune with it as well. Overall we
get the best results when tuning is performed in-
cluding a cleaned version of CC. This setup gives
a large improvement compared to not using CC at
all, or to use it with only basic cleaning. There is
little difference in Bleu scores when testing with
either basic cleaning, or cleaning based on lan-
guage ID, with a given tuning, which is not sur-
prising given their small and similar interpolation
weights. Tuning was, however, not successful
when using CC with basic cleaning.
Overall we think that alignment-based corpus
cleaning worked well. It reduced the size of the
corpus by over 25%, improved the cross-entropy
for interpolation with the EP-NC phrase-table, and
228
Testing
Tuning not used basic langid alignment
not used 14.0 13.9 13.9 14.0
basic 14.2 14.5 14.3 14.3
langid 15.2 15.3 15.3 15.3
alignment 12.7 15.3 15.3 15.7
Table 5: Bleu scores with different types of clean-
ing and without Common Crawl
gave an improvement on the translation task. We
still think that there is potential for further improv-
ing this filtering and to annotate larger test sets to
investigate the effects in more detail.
5 Tunable Distortion Limits
The Docent decoder uses a hill-climbing search
and can perform operations anywhere in the sen-
tence. Thus, it does not need to enforce a strict
distortion limit. In the Docent implementation, the
distortion limit is actually implemented as a fea-
ture, which is normally given a very large weight,
which effectively means that it works as a hard
constraint. This could easily be relaxed, however,
and in this work we investigate the effects of using
soft distortion limits, which can be optimized dur-
ing tuning, like other features. In this way long-
distance movements can be allowed when they are
useful, instead of prohibiting them completely. A
drawback of using no or soft distortion limits is
that it increases the search space.
In this work we mostly experiment with variants
of one or two standard distortion limits, but with a
tunable weight. We also tried to use separate soft
distortion limits for left- and right-movement. Ta-
ble 6 show the results with different types of dis-
tortion limits. The system with a standard fixed
distortion limits of 6 has a somewhat lower score
than most of the systems with no or soft distortion
limits. In most cases the scores are similar, and
we see no clear affects of allowing tunable lim-
its over allowing unlimited distortion. The system
that uses two mono-directional limits of 6 and 10
has slightly higher scores than the other systems,
and is used in our final submission.
One possible reason for the lack of effect of al-
lowing more distortion could be that it rarely hap-
pens that an operator is chosen that performs such
distortion, when we use the standard Docent set-
tings. To investigate this, we varied the settings of
the parameters that guide the swap-phrases opera-
tor, and used the move-phrases operator instead of
swap-phrases. None of these changes led to any
DL type Limit Bleu
No DL ? 15.5
Hard DL 6 15.0
One soft DL 6 15.5
8 14.2
10 15.5
Two soft DLs 4,8 15.5
6,10 15.7
Bidirectional soft DLs 6,10 15.5
Table 6: Bleu scores for different distortion limit
(DL) settings
improvements, however.
While we saw no clear effects when using tun-
able distortion limits, we plan to extend this work
in the future to model movement differently based
on parts of speech. For the English?German lan-
guage pair, for instance, it would be reasonable to
allow long distance moves of verb groups with no
or little cost, but use a hard limit or a high cost for
other parts of speech.
6 Corpus Selection for Recasing
In this section we investigate the effect of using
different corpus combinations for recasing. We
lower-cased our training corpus, which means that
we need a full recasing step as post-processing.
This is performed by training a SMT system on
lower-cased and true-cased target language. We
used the Moses toolkit to train the recasing system
and to decode during recasing. We investigate the
effect of using different combinations of the avail-
able training corpora to train the recasing model.
Table 7 show case sensitive Bleu scores, which
can be compared to the previous case-insensitive
scores of 15.7. We see that there is a larger effect
of including more data in the language model than
in the translation model. There is a performance
jump both when adding CC data and when adding
News data to the language model. The results
are best when we include the News data, which
is not included in the English?German translation
model, but which is much larger than the other cor-
pora. There is no further gain by using News in
combination with other corpora compared to using
only News. When adding more data to the trans-
lation model there is only a minor effect, with the
difference between only using EP-NC and using
all available corpora is at most 0.2 Bleu points.
In our submitted system we use the monolingual
News corpus both in the LM and the TM.
There are other options for how to treat recas-
229
Language model
TM EP-NC EP-NC-CC News EP-NC-News EP-NC-CC-News
EP-NC 13.8 14.4 14.8 14.8 14.8
EP-NC-CC 13.9 14.5 14.9 14.8 14.8
News 13.9 14.5 14.9 14.9 14.9
EP-NC-News 13.9 14.5 14.9 14.9 14.9
EP-NC-CC-News 13.9 14.5 14.9 14.9 15.0
Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and
translation model (TM) for recasing
ing. It is common to train the system on true-
cased data instead of lower-cased data, which has
been shown to lead to small gains for the English?
German language pair (Koehn et al, 2008). In this
framework there is still a need to find the correct
case for the first word of each sentence, for which
a similar corpus study might be useful.
7 Comparison to Moses
So far we have only shown results using the Do-
cent decoder on its own, with a random initializa-
tion, since we wanted to submit a Docent-only sys-
tem for the shared task. In this section we also
show contrastive results with Moses, and for Do-
cent initialized with stack decoding, using Moses,
and for different type of tuning.
Previous research have shown mixed results for
the effect of initializing Docent with and with-
out stack decoding, when using the same feature
sets. In Hardmeier et al (2012) there was a drop
of about 1 Bleu point for English?French trans-
lation based on WMT11 data when random ini-
tialization was used. In Stymne et al (2013a),
on the other hand, Docent gave very similar re-
sults with both types of initialization for German?
English WMT13 data. The latter setup is similar
to ours, except that no Common Crawl data was
used.
The results with our setup are shown in Ta-
ble 8. In this case we lose around a Bleu point
when using Docent on its own, without Moses ini-
tialization. We also see that the results are lower
when using Moses with the Docent tuning method,
or when combining Moses and Docent with Do-
cent tuning. This indicates that the document-
level tuning has not given satisfactory results in
this scenario, contrary to the results in Stymne et
al. (2013a), which we plan to explore further in
future work. Overall we think it is important to
develop stronger context-sensitive models for Do-
cent, which can take advantage of the document
context.
Test system Tuning system Bleu
Docent (random) Docent 15.7
Docent (stack) Docent 15.9
Moses Docent 15.9
Docent (random) Moses 15.9
Docent (stack) Moses 16.8
Moses Moses 16.8
Table 8: Bleu scores for Docent initialized ran-
domly or with stack decoding compared to Moses.
Tuning is performed with either Moses or Docent.
For the top line we used tunable distortion lim-
its 6,10 with Docent, in the other cases a standard
hard distortion limit of 6, since Moses does not al-
low soft distortion limits.
8 Conclusion
We have presented the Uppsala University system
for WMT 2013. Our submitted system uses Do-
cent with random initialization and two tunable
distortion limits of 6 and 10. It is trained with the
Common Crawl corpus, cleaned using language
identification and alignment-based filtering. For
recasing we used the monolingual News corpora.
For corpus-cleaning, we present a novel method
for cleaning noisy corpora based on the number
and ratio of word alignment links for sentence
pairs, which leads to a large reduction of corpus
size, and to small improvements on the transla-
tion task. We also experiment with tunable dis-
tortion limits, which do not lead to any consistent
improvements at this stage.
In the current setup the search algorithm of
Docent is not strong enough to compete with
the effective search in standard decoders like
Moses. We are, however, working on developing
discourse-aware models that can take advantage of
the document-level context, which is available in
Docent. We also need to further investigate tuning
methods for Docent.
230
References
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the ACL,
System Demonstrations, pages 25?30, Jeju Island,
Korea.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 16th
Annual Conference of the European Association
for Machine Translation, pages 539?549, Avignon,
France.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274, Landsdowne,
Virginia, USA.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight opti-
mization for discourse-level SMT. In Proceedings
of the ACL 2013 Workshop on Discourse in Machine
Translation (DiscoMT 2013), Sofia, Bulgaria.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
231
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Feature Weight Optimization for Discourse-Level SMT
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann and Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We present an approach to feature weight
optimization for document-level decoding.
This is an essential task for enabling future
development of discourse-level statistical
machine translation, as it allows easy inte-
gration of discourse features in the decod-
ing process. We extend the framework of
sentence-level feature weight optimization
to the document-level. We show experi-
mentally that we can get competitive and
relatively stable results when using a stan-
dard set of features, and that this frame-
work also allows us to optimize document-
level features, which can be used to model
discourse phenomena.
1 Introduction
Discourse has largely been ignored in traditional
machine translation (MT). Typically each sentence
has been translated in isolation, essentially yield-
ing translations that are bags of sentences. It is
well known from translation studies, however, that
discourse is important in order to achieve good
translations of documents (Hatim and Mason,
1990). Most attempts to address discourse-level
issues for statistical machine translation (SMT)
have had to resort to solutions such as post-
processing to address lexical cohesion (Carpuat,
2009) or two-step translation to address pronoun
anaphora (Le Nagard and Koehn, 2010). Recently,
however, we presented Docent (Hardmeier et al,
2012; Hardmeier et al, 2013), a decoder based
on local search that translates full documents. So
far this decoder has not included a feature weight
optimization framework. However, feature weight
optimization, or tuning, is important for any mod-
ern SMT decoder to achieve a good translation
performance.
In previous research with Docent, we used grid
search to find weights for document-level features
while base features were optimized using stan-
dard sentence-level techniques. This approach is
impractical since many values for the extra fea-
tures have to be tried, and, more importantly, it
might not give the same level of performance as
jointly optimizing all parameters. Principled fea-
ture weight optimization is thus essential for re-
searchers that want to use document-level features
to model discourse phenomena such as anaphora,
discourse connectives, and lexical consistency. In
this paper, we therefore propose an approach that
supports discourse-wide features in document-
level decoding by adapting existing frameworks
for sentence-level optimization. Furthermore, we
include a thorough empirical investigation of this
approach.
2 Discourse-Level SMT
Traditional SMT systems translate texts sentence
by sentence, assuming independence between sen-
tences. This assumption allows efficient algo-
rithms based on dynamic programming for explor-
ing a large search space (Och et al, 2001). Be-
cause of the dynamic programming assumptions it
is hard to directly include discourse-level features
into a traditional SMT decoder. Nevertheless,
there have been several attempts to integrate inter-
sentential and long distance models for discourse-
level phenomena into standard decoders, usually
as ad-hoc additions to standard models, address-
ing a single phenomenon.
Several studies have tried to improve pro-
noun anaphora by adding information about the
antecedent, either by using two-step decoding
(Le Nagard and Koehn, 2010; Guillou, 2012) or
by extracting information from previously trans-
lated sentences (Hardmeier and Federico, 2010),
unfortunately without any convincing results. To
address the translation of discourse connectives,
source-side pre-processing has been used to anno-
tate surface forms either in the corpus or in the
60
phrase-table (Meyer and Popescu-Belis, 2012) or
by using factored decoding (Meyer et al, 2012)
to disambiguate connectives, with small improve-
ments. Lexical consistency has been addressed
by the use of post-processing (Carpuat, 2009),
multi-pass decoding (Xiao et al, 2011; Ture et al,
2012), and cache models (Tiedemann, 2010; Gong
et al, 2011). Gong et al (2012) addressed the
issue of tense selection for translation from Chi-
nese, by the use of inter-sentential tense n-grams,
exploiting information from previously translated
sentences. Another way to use a larger context
is by integrating word sense disambiguation and
SMT. This has been done by re-initializing phrase
probabilities for each sentence (Carpuat and Wu,
2007), by introducing extra features in the phrase-
table (Chan et al, 2007), or as a k-best re-ranking
task (Specia et al, 2008). Another type of ap-
proach is to integrate topic modeling into phrase
tables (Zhao and Xing, 2010; Su et al, 2012). For
a more thorough overview of discourse in SMT,
see Hardmeier (2012).
Here we instead choose to work with the re-
cent document-level SMT decoder Docent (Hard-
meier et al, 2012). Unlike in traditional decod-
ing were documents are generated sentence by
sentence, feature models in Docent always have
access to the complete discourse context, even
before decoding is finished. It implements the
phrase-based SMT approach (Koehn et al, 2003)
and is based on local search, where a state con-
sists of a full translation of a document, which is
improved by applying a series of operations to im-
prove the translation. A hill-climbing strategy is
used to find a (local) maximum. The operations
allow changing the translation of a phrase, chang-
ing the word order by swapping the positions of
two phrases, and resegmenting phrases. The initial
state can either be initialized randomly in mono-
tonic order, or be based on an initial run from a
standard sentence-based decoder. The number of
iterations in the decoder is controlled by two pa-
rameters, the maximum number of iterations and
a rejection limit, which stops the decoder if no
change was made in a certain number of iterations.
This setup is not limited by dynamic programming
constraints, and enables the use of the translated
target document to extract features. It is thus easy
to directly integrate discourse-level features into
Docent. While we use this specific decoder in our
experiments, the method proposed for document-
level feature weight optimization is not limited to
it. It can be used with any decoder that outputs
feature values at the document level.
3 Sentence-Level Tuning
Traditionally, feature weight optimization, or tun-
ing, for SMT is performed by an iterative process
where a development set is translated to produce a
k-best list. The parameters are then optimized us-
ing some procedure, generally to favor translations
in the k-best list that have a high score on some
MT metric. The translation step is then repeated
using the new weights for decoding, and optimiza-
tion is continued on a new k-best list, or on a com-
bination of all k-best lists. This is repeated until
some end condition is satisfied, for instance for a
set number of iterations, until there is only very
small changes in parameter weights, or until there
are no new translations in the k-best lists.
SMT tuning is a hard problem in general, partly
because the correct output is unreachable and
also because the translation process includes la-
tent variables, which means that many efficient
standard optimization procedures cannot be used
(Gimpel and Smith, 2012). Nevertheless, there
are a number of techniques including MERT (Och,
2003), MIRA (Chiang et al, 2008; Cherry and
Foster, 2012), PRO (Hopkins and May, 2011),
and Rampion (Gimpel and Smith, 2012). All of
these optimization methods can be plugged into
the standard optimization loop. All of the meth-
ods work relatively well in practice, even though
there are limitations, for instance that many meth-
ods are non-deterministic meaning that their re-
sults are somewhat unstable. However, there are
some important differences. MERT is based on
scores for the full test set, whereas the other meth-
ods are based on sentence-level scores. MERT
also has the drawback that it only works well for
small sets of features. In this paper we are not
concerned with the actual optimization algorithm
and its properties, though, but instead we focus
on the integration of document-level decoding into
the existing optimization frameworks.
In order to adapt sentence-level frameworks to
our needs we need to address the granularity of
scoring and the process of extracting k-best lists.
For document-level features we do not have mean-
ingful scores on the sentence level which are re-
quired in standard optimization frameworks. Fur-
thermore, the extraction of k-best lists is not as
61
Input: inputDocs, refDocs, init weights ?0, max decoder iters max, sample start ss, sample interval si,
Output: learned weights ?
1: ? ? ?0
2: Initialize empty klist
3: run? 1
4: repeat
5: Initialize empty klistrun
6: for doc? 1, inputDocs.size do Initialize decoder state randomly for inputDocs[doc]
7: for iter? 1,max do
8: Perform one hill-climbing step for inputDocs[doc]
9: if iter >= ss & iter mod si == 0 then
10: Add translation for inputDocs[doc] to klistrun
11: end if
12: end for
13: end for
14: Merge klistrun with klist
15: modelScoresdoc ? ComputeModelScores(klist)
16: metricStatsdoc ? ComputeMetricStats(klist, refDocs)
17: ?run ? ?
18: ? ? Optimize(?run,modelScoresdoc,metricStatsdoc)
19: run? run + 1
20: until Done(run, ?, ?run)
Figure 1: Document-level feature weight optimization algorithm
straightforward in our hill-climbing decoder as in
standard sentence-level decoders such as Moses
(Koehn et al, 2007) where such a list can be ap-
proximated easily from the internal beam search
strategy. Working on output lattices is another op-
tion in standard approaches (Cherry and Foster,
2012) which is also not applicable in our case.
In the following section we describe how we
can address these issues in order to adapt sentence-
level frameworks for our purposes.
4 Document-Level Tuning
To allow document-level feature weight optimiza-
tion, we make some small changes to the sentence-
level framework. Figure 1 shows the algorithm we
use. It assumes access to an optimization algo-
rithm, Optimize, and an end criterion, Done.
The changes from standard sentence-level opti-
mization is that we compute scores on the docu-
ment level, and that we sample translations instead
of using standard k-best lists.
The main challenge is that we need meaning-
ful scores which we do not have at the sentence
level in document decoding. We handle this by
simply computing all scores (model scores and
metric scores) exclusively at the document level.
Remember that all standard MT metrics based on
sentence-level comparisons with reference trans-
lations can be aggregated for a complete test set.
Here we do the same for all sentences in a given
document. This can actually be an advantage com-
pared to optimization methods that use sentence-
level scores, which are known to be unreliable
(Callison-Burch et al, 2012). Document-level
scores should thus be more stable, since they are
based on more data. A potential drawback is that
we get fewer data points with a test set of the same
size, which might mean that we need more data to
achieve as good results as with sentence-level op-
timization. We will see the ability of our approach
to optimize weights with reasonable data sets in
our experiments further down.
The second problem, the extraction of k-best
lists can be addressed in several ways. It is pos-
sible to get a k-best list from Docent by extract-
ing the results from the last k iterations. However,
since Docent operates on the document-level and
does not accept updates in each iteration, there will
be many identical and/or very similar hypotheses
with such an approach. Another option would be
to extract the translations from the k last differ-
ent iterations, which would require some small
changes to the decoder. Instead, we opt to use k-
lists, lists of translations sampled with some inter-
val, which contains k translations, but not neces-
sarily all the k best translations that could be found
by the decoder. A k-best list is of course a k-list,
which we get with a sample interval of 1.
We also choose to restart Docent randomly in
each optimization iteration, since it allows us to
explore a larger part of the search space. We
empirically found that this strategy worked better
than restarting the decoder from the previous best
state.
62
German?English English?Swedish
Type Sentences Documents Type Sentences Documents
Training
Europarl 1.9M ? Europarl 1.5M ?
News Commentary 178K ? ? ? ?
Tuning
News2009 2525 111 Europarl (Moses) 2000 ?
News2008-2010 7567 345 Europarl (Docent) 1338 100
Test News2012 3003 99 Europarl 690 20
Table 1: Domain and number of sentences and documents for the corpora
As seen in Figure 1, there are some additional
parameters in our procedure: the sample start iter-
ation and the sample interval. We also need to set
the number of decoder iterations to run. In Sec-
tion 5 we empirically investigate the effect of these
parameters.
Compared to sentence-level optimization, we
also have a smaller number of units to get scores
from, since we use documents as units, and not
sentences. The importance of this depends on the
optimization algorithm. MERT calculates metric
scores over the full tuning set, not for individual
sentences, and should not be affected too much
by the change in granularity. Many other opti-
mization algorithms, like PRO, work on the sen-
tence level, and will likely be more affected by
the reduction of units. In this work we focus on
MERT, which is the most commonly used opti-
mization procedure in the SMT community, and
which tends to work quite well with relatively few
features. However, we also show contrastive re-
sults for PRO (Hopkins and May, 2011). A fur-
ther issue is that Docent is non-deterministic, i.e.,
it can give different results with the same param-
eter weights. Since the optimization process is al-
ready somewhat unstable this is a potential issue
that needs to be explored further, which we do in
Section 5.
Implementation-wise we adapted Docent to out-
put k-lists and adapted the infrastructure available
for tuning in the Moses decoder (Koehn et al,
2007) to work with document-level scores. This
setup allows us to use the variety of optimization
procedures implemented there.
5 Experiments
In this section we report experimental results
where we investigate several issues in connec-
tion with document-level feature weight optimiza-
tion for SMT. We first describe the experimental
setup, followed by baseline results using sentence-
level optimization. We then present validation ex-
periments with standard sentence-level features,
which can be compared to standard optimization.
Finally, we report results with a set of document-
level features that have been proposed for joint
translation and text simplification (Stymne et al,
2013).
5.1 Experimental Setup
Most of our experiments are for German-to-
English news translation using data from the
WMT13 workshop.1 We also show results with
document-level features for English-to-Swedish
Europarl (Koehn, 2005). The size of the training,
tuning, and test sets are shown in Table 1. First of
all, we need to extract documents for tuning and
testing with Docent. Fortunately, the news data al-
ready contain document markup, corresponding to
individual news articles. For Europarl we define a
document as a consecutive sequence of utterances
from a single speaker. To investigate the effect of
the size of the tuning set, we used different subsets
of the available tuning data.
All our document-level experiments are car-
ried out with Docent but we also contrast with
the Moses decoder (Koehn et al, 2007). For the
purpose of comparison, we use a standard set of
sentence-level features used in Moses in most of
our experiments: five translation model features,
one language model feature, a distance-based re-
ordering penalty, and a word count feature. For
feature weight optimization we also apply the
standard settings in the Moses toolkit. We opti-
mize towards the Bleu metric, and optimization
ends either when no weights are changed by more
than 0.00001, or after 25 iterations. MERT is used
unless otherwise noted.
Except for one of our baselines, we always run
Docent with random initialization. For test we run
the document decoder for a maximum of 227 iter-
ations with a rejection limit of 100,000. In our
experiments, the decoder always stopped when
reaching the rejection limit, usually between 1?5
1http://www.statmt.org/wmt13/
translation-task.html
63
million iterations.
We show results on the Bleu (Papineni et al,
2002) and NIST (Doddington, 2002) metrics. For
German?English we show the average result and
standard deviation of three optimization runs, to
control for optimizer instability as proposed by
Clark et al (2011). For English?Swedish we re-
port results on single optimization runs, due to
time constraints.
5.2 Baselines
Most importantly, we would like to show the effec-
tiveness of the document-level tuning procedure
described above. In order to do this, we created
a baseline using sentence-level optimization with
a tuning set of 2525 sentences and the News2009
corpus for evaluation. Increasing the tuning set is
known to give only modest improvements (Turchi
et al, 2012; Koehn and Haddow, 2012).
The feature weights optimized with the stan-
dard Moses decoder can then directly be used in
our document-level decoder as we only include
sentence-level features in our baseline model. As
expected, these optimized weights also lead to
a better performance in document-level decoding
compared to an untuned model as shown in Ta-
ble 2. Note, that Docent can be initialized in
two ways, by Moses and randomly. Not surpris-
ingly, the result for the runs initialized with Moses
are identical with the pure sentence-level decoder.
Initializing randomly gives a slightly lower Bleu
score but with a larger variation than with Moses
initialization, which is also expected. Docent is
non-deterministic, and can give somewhat varying
results with the same weights. However, this vari-
ation has been shown experimentally to be very
small (Hardmeier et al, 2012).
Our goal now is to show that document-level
tuning can perform equally well in order to verify
our approach. For this, we set up a series of ex-
periments looking at varying tuning sets and dif-
ferent parameters of the decoding and optimiza-
tion procedure. With this we like to demonstrate
the stability of the document-level feature weight
optimization approach presented above. Note that
the most important baselines for comparison with
the results in the next sections are the ones with
Docent and random initialization.
5.3 Sentence-Level Features
In this section we present validation results where
we investigate different aspects of document-
System Tuning Bleu NIST
Moses None 17.7 6.25
Docent-M None 17.7 6.25
Docent-R None 15.2 (0.05) 5.88 (0.00)
Moses Moses 18.3 (0.04) 6.22 (0.01)
Docent-M Moses 18.3 (0.04) 6.22 (0.01)
Docent-R Moses 18.1 (0.13) 6.23 (0.01)
Table 2: Baseline results, where Docent-M is ini-
tialized with Moses and Docent-R randomly
Docs Sent. Min Max Bleu NIST
111 2525 3 127 18.0 (0.11) 6.19 (0.04)
345 7567 3 127 18.1 (0.14) 6.25 (0.02)
100 1921 8 40 18.0 (0.05) 6.25 (0.10)
200 3990 8 40 17.9 (0.25) 6.20 (0.09)
100 2394 8 100 18.0 (0.12) 6.27 (0.07)
200 4600 8 100 18.1 (0.29) 6.26 (0.10)
300 6852 8 100 18.2 (0.13) 6.27 (0.03)
Table 3: Results for German?English with varying
sizes of tuning set, where the number of sentences
and documents are varied, as well as the minimum
and maximum number of sentences per document
level feature weight optimization with standard
sentence-level features. In this way we can com-
pare the results directly to standard sentence-level
optimization, and to the results of Moses.
Corpus size We investigate how tuning is af-
fected by corpus size. The corpus size was var-
ied in two ways, by changing the number of docu-
ments in the tuning set, and by changing the length
of documents in the tuning sets. In this exper-
iment we run 20000 decoder iterations per opti-
mization iteration, and use a k-list of size 101,
with sample interval 100. Table 3 shows the re-
sults with varying tuning set sizes for German?
English. There is very little variation between the
scores, and no clear tendencies. All results are of
similar quality to the baseline with random initial-
ization and sentence-level tuning, and better than
not using any tuning. The top line in Table 3 is
News2009, the same tuning set as for the base-
lines. The scores are somewhat more unstable than
the baseline scores, but stability is not related to
corpus size. In the following sections we will use
the tuning set with 200 documents, size 8-40.
Number of decoder iterations and k-list sam-
pling Two issues that are relevant for feature
weight optimization with the document-level de-
coder is the number of decoder hill-climbing iter-
ations in each optimization iteration, and the set-
tings for k-list sampling. These choices affect the
64
Iterations K-list UTK Bleu NIST
20000 101 55.6 17.9 (0.25) 6.20 (0.09)
30000 201 67.2 17.9 (0.06) 6.21 (0.01)
40000 301 79.9 18.2 (0.11) 6.28 (0.09)
50000 401 86.9 18.1 (0.20) 6.22 (0.05)
75000 651 99.2 17.8 (0.15) 6.13 (0.03)
100000 901 106.8 17.9 (0.17) 6.16 (0.03)
30000 101 21.6 18.0 (0.15) 6.21 (0.02)
40000 101 12.6 17.7 (0.53) 6.12 (0.15)
50000 101 8.2 17.9 (0.24) 6.18 (0.06)
Table 4: Results for German?English with a vary-
ing number of iterations and k-list size (UTK is
the average number of unique translations per doc-
ument in the k-lists)
quality of the translations in each optimization it-
eration, and the spread in the k-list. We will report
the average number of unique translations per doc-
ument in the k-lists, UTK, during feature weight
optimization, in this section.
The top half of Table 4 shows results with a
different number of iterations, when we sample
k-lists from iteration 10000 with interval 100 for
German?English, which means that the size of the
k-lists also changes. The differences on MT met-
rics are very small. The number of new unique
translations in the k-lists decrease with the number
of decoder iterations. With 20K iterations, 55%
of the k-lists entries are unique, which could be
compared to only 12% with 100K iterations. The
majority of the unique translations are thus found
in the beginning of the decoding, which is not sur-
prising.
The bottom half of Table 4 shows results with
a different number of decoder iterations, but a set
k-list size. In this setting the number of unique
hypotheses in the k-lists obviously decreases with
the number of decoder iterations. Despite this,
there are mostly small result differences, except
for 40K iterations, which has more unstable results
than the other settings. It does not seem useful to
increase the number of decoder iterations without
also increasing the size of the k-list. An even bet-
ter strategy might be to only include unique entries
in the k-lists. We will explore this in future work.
We also ran experiments where we did not
restart the decoder with a random state in each iter-
ation, but instead saved the previous state and con-
tinued decoding with the new weights from there.
This, however, was largely unsuccessful, and gave
very low scores. We believe that the reason for this
is mainly that a much smaller part of the search
space is explored when the decoder is not restarted
Interval Start UTK Bleu NIST
1 19900 1.4 18.2 (0.07) 6.25 (0.04)
10 19000 5.2 18.1 (0.08) 6.22 (0.03)
100 10000 55.6 17.9 (0.25) 6.20 (0.09)
200 0 82.2 17.9 (0.19) 6.15 (0.05)
Table 5: Results with different k-list-sample inter-
vals for k-lists size 101 (UTK is the average num-
ber of unique translations per document in the k-
lists)
with a new seed repeatedly. The fact that a higher
overall quality can be achieved with a higher num-
ber of iterations (see Figure 2) can apparently not
compensate for this drawback.
Finally, we investigate the effect of the sam-
ple interval for the k-lists. To get k-lists of equal
size, 101, we start the sampling at different itera-
tions. Table 5 shows the results, and we can see
that with a small sample interval, the number of
unique translations decreases drastically. Despite
this, there are no large result differences. There
is actually a slight trend that a smaller sample in-
terval is better. This does not confirm our intuition
that it is important with many different translations
in the k-list. Especially for interval 1 it is surpris-
ing, since there is often only 1 unique translation
for a single document. We believe that the fact that
k-lists from different iterations are joined, can be
part of the explanation for these results. We think
more work is needed in the future, to further ex-
plore these settings, and the interaction with the
total number of decoder iteration, and the k-list
sampling.
To further shed some light to these results, we
show learning curves from the optimization. Fig-
ure 2 shows Bleu scores for the system optimized
with 100K decoder iterations after different num-
bers of iterations, for the last three iterations in
each of the three optimization runs. As shown in
Hardmeier et al (2012), the translation quality in-
creases fast at first, but start to level out at around
40K iterations. Despite this, the optimization re-
sults are good even with 20K iterations, which is
somewhat surprising. Figures 3 and 4 show the
Bleu scores after each tuning iteration for the sys-
tems in Tables 4 and 5. As is normal for SMT tun-
ing, the convergence is slow, and there are some
oscillations even late in the optimization. Over-
all systems with many iterations seem somewhat
more stable.
Overall, the results are better than the untuned
65
 
14
 
14.5 15
 
15.5 16
 
16.5 17
 
17.5
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90 
100
Bleu
Dec
ode
r ite
ratio
ns (*
1000
)
1-23 1-24 1-25 2-23 2-24 2-25 3-23 3-24 3-25
Figure 2: Bleu scores during 100000 Docent iter-
ations during feature weight optimization
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s20K 30K 40K 50K 75K 100
K
Figure 3: Bleu scores during feature weight opti-
mization for systems with different number of de-
coder iterations and k-list sizes.
baseline and on par with the sentence-level tuning
baselines in all settings, with a relatively modest
variation, even across settings. In fact, if we cal-
culate the total scores of all 36 systems in Tables 4
and 5, we get a Bleu score of 18.0 (0.23) and a
NIST score of 6.19 (0.07), with a variation that is
not higher than for many of the different settings.
Optimization method In this section we com-
pare the performance of the MERT optimiza-
tion algorithm with that of PRO, and a combi-
nation that starts MERT with weights initialized
with PRO (MERT+PRO), suggested by Koehn and
Haddow (2012). Here we run 30000 decoder it-
erations. Table 6 shows the results. Initializing
MERT with PRO did not affect the scores much.
The scores with only PRO, however, are slightly
lower than for MERT, and have a much larger
score variation. This could be because PRO is
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s1 (20
K)
10 (2
0K)
100
 (20K
)
100
 (30K
)
100
 (40K
)
100
 (50K
)
200
 (20K
)
Figure 4: Bleu scores during feature weight opti-
mization for systems with different k-list sample
interval and number of decoder iterations.
Bleu NIST
MERT 17.9 (0.06) 6.21 (0.01)
PRO 17.5 (0.41) 6.15 (0.20)
MERT+PRO 18.0 (0.12) 6.18 (0.06)
Table 6: Results with different optimization algo-
rithms for German?English
likely to need more data, since it calculates met-
ric scores on individual units, sentences or docu-
ments, not across the full tuning set, like MERT.
This likely means that 200 documents are too few
for stable results with optimization methods that
depend on unit-level metric scores.
5.4 Document-Level Features
In this section we investigate the effect of opti-
mization with a number of document-level fea-
tures. We use a set of features proposed in Stymne
et al (2013), in order to promote the readability
of texts. In this scenario, however, we use these
features in a standard SMT setting, where they
can potentially improve the lexical consistency of
translations. The features are:
? Type token ratio (TTR) ? the ratio of types,
unique words, to tokens, total number of
words
? OVIX ? a reformulation of TTR that has tra-
ditionally been used for Swedish and that is
less sensible to text length than TTR, see
Eq. 1
? Q-value, phrase level (QP) - The Q-value was
developed as a measure for bilingual term
quality (Dele?ger et al, 2006), to promote
common and consistently translated terms.
See Eq. 2, where f(st) is the frequency of
66
German?English English?Swedish
System Optimization Bleu NIST Bleu NIST
Moses Sentence 18.3 (0.04) 6.22 (0.01) 24.3 6.12
Docent Sentence 18.1 (0.13) 6.23 (0.01) 24.1 6.06
Docent Document 17.9 (0.25) 6.20 (0.09) 23.4 6.01
TTR Document 18.3 (0.16) 6.33 (0.04) 23.6 6.15
OVIX Document 18.3 (0.13) 6.30 (0.03) 23.4 5.99
QW Document 18.1 (0.14) 6.22 (0.03) 24.2 6.11
QP Document 18.0 (0.10) 6.23 (0.05) 21.2 5.70
Table 7: Results when using document-level features
the phrase pair, n(s) is the number of unique
target phrases which the source phrase is
aligned to in the document, and n(t) is the
same for the target phrase. Here the Q-value
is applied on the phrase level.
? Q-value, word level (QW) - Same as above,
but here we apply the Q-value for source
words and their alignments on the target side.
OVIX =
log(count(tokens))
log
(
2?
log(count(types))
log(count(tokens))
) (1)
Q-value =
f(st)
n(s) + n(t)
(2)
We added these features one at a time to the
standard feature set. Optimization was performed
with 20000 decoder iterations, and a k-list of size
101. As shown in the previous sections, there
are slightly better settings, which could have been
used to boost the results somewhat.
The results are shown in Table 7. For German?
English, the results are generally on par with the
baselines for Bleu and slightly higher on NIST for
OVIX and TTR. For English?Swedish, we used a
smaller tuning set on the document level than on
the sentence level, see Table 1, due to time con-
straints. This is reflected in the scores, which are
generally lower than for sentence-level decoding.
Using the QW feature, however, we receive com-
petitive scores to the sentence-based baselines,
which indicates that it can be meaningful to use
document-level features with the suggested tuning
approach.
While the results do not improve much over
the baselines, these experiments still show that
we can optimize discourse-level features with
our approach. We need to identify more useful
document-level features in future work, however.
6 Conclusion
We have shown how the standard feature weight
optimization workflow for SMT can be adapted to
document-level decoding, which allows easy inte-
gration of discourse-level features into SMT. We
modified the standard framework by calculating
scores on the document-level instead of the sen-
tence level, and by using k-lists rather than k-best
lists.
Experimental results show that we can achieve
relatively stable results, on par with the results for
sentence-level optimization and better than with-
out tuning, with standard features. This is de-
spite the fact that we use the hill-climbing de-
coder without initialization by a standard decoder,
which means that it is somewhat unstable, and
is not guaranteed to find any global maximum,
even according to the model. We also show that
we can optimize document-level features success-
fully. We investigated the effect of a number of
parameters relating to tuning set size, the number
of decoder iterations, and k-list sampling. There
were generally small differences relating to these
parameters, however, indicating that the suggested
approach is robust. The interaction between pa-
rameters does need to be better explored in future
work, and we also want to explore better sampling,
without duplicate translations.
This is the first attempt of describing and exper-
imentally investigating feature weight optimiza-
tion for direct document-level decoding. While we
show the feasibility of extending sentence-level
optimization to the document level, there is still
much more work to be done. We would, for in-
stance, like to investigate other optimization pro-
cedures, especially for systems with a high num-
ber of features. Most importantly, there is a large
need for the development of useful discourse-level
features for SMT, which can now be optimized.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
67
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 61?72, Prague, Czech Republic.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 33?40, Prague,
Czech Republic.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of Human
Language Technologies: The 2008 Annual Con-
ference of the NAACL, pages 224?233, Honolulu,
Hawaii.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the ACL: Human Language Tech-
nologies, pages 176?181, Portland, Oregon, USA.
Louise Dele?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies:
an approach based on aligned corpora. In Inter-
national Congress of the European Federation for
Medical Informatics, pages 747?752, Maastricht,
The Netherlands.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology, pages 228?231, San Diego, California,
USA.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the NAACL: Human Language Technologies, pages
221?231, Montre?al, Canada.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the EACL 2012 Student Research Workshop, pages
1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Longman, London, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
68
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing sense-labeled discourse connectives for statisti-
cal machine translation. In Proceedings of the Joint
Workshop on Exploiting Synergies between Informa-
tion Retrieval and Machine Translation (ESIRMT)
and Hybrid Approaches to Machine Translation
(HyTra), pages 129?138, Avignon, France.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the 10th Biennial Conference of the Association for
Machine Translation in the Americas, San Diego,
California, USA.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Lucia Specia, Baskaran Sankaran, and Maria das
Grac?as Volpe Nunes. 2008. N-best reranking for
the efficient integration of word sense disambigua-
tion and statistical machine translation. In Proceed-
ings of the 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLING), pages 399?410, Haifa, Israel.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the ACL,
pages 459?468, Jeju Island, Korea.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the ACL
2010 Workshop on Domain Adaptation for Natural
Language Processing (DANLP), pages 8?15, Upp-
sala, Sweden.
Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello
Cristianini. 2012. Learning to translate: A statis-
tical and computational analysis. Advances in Arti-
ficial Intelligence, 2012. Article ID 484580.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the
NAACL: Human Language Technologies, pages
417?426, Montre?al, Canada.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of MT Summit
XIII, pages 131?138, Xiamen, China.
Bing Zhao and Eric P. Xing. 2010. HM-BiTAM: Bilin-
gual topic exploration, word alignment,and transla-
tion. In Advances in Neural Information Processing
Systems 20 (NIPS), pages 1689?1696, Cambridge,
Massachusetts, USA.
69
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122?129,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Anaphora Models and Reordering for Phrase-Based SMT
Christian Hardmeier Sara Stymne J
?
org Tiedemann Aaron Smith Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tems for WMT14. We look at the integra-
tion of a model for translating pronomi-
nal anaphora and a syntactic dependency
projection model for English?French. Fur-
thermore, we investigate post-ordering and
tunable POS distortion models for English?
German.
1 Introduction
In this paper we describe the Uppsala University
systems for WMT14. We present three different
systems. Two of them are based on the document-
level decoder Docent (Hardmeier et al., 2012; Hard-
meier et al., 2013a). In our English?French sys-
tem we extend Docent to handle pronoun anaphora,
and in our English?German system we add part-
of-speech phrase-distortion models to Docent. For
German?English we also have a system based on
Moses (Koehn et al., 2007). Again the focus is
on word order, this time by using pre- and post-
reordering.
2 Document-Level Decoding
Traditional SMT decoders translate texts as bags
of sentences, assuming independence between sen-
tences. This assumption allows efficient algorithms
for exploring a large search space based on dy-
namic programming (Och et al., 2001). Because of
the dynamic programming assumptions it is hard to
directly include discourse-level and long-distance
features into a traditional SMT decoder.
In contrast to this very popular stack decoding
approach, our decoder Docent (Hardmeier et al.,
2012; Hardmeier et al., 2013a) implements a search
procedure based on local search. At any stage of
the search process, its search state consists of a
complete document translation, making it easy for
feature models to access the complete document
with its current translation at any point in time. The
search algorithm is a stochastic variant of standard
hill climbing. At each step, it generates a successor
of the current search state by randomly applying
one of a set of state changing operations to a ran-
dom location in the document, and accepts the new
state if it has a better score than the previous state.
The operations are to change the translation of a
phrase, to change the word order by swapping the
positions of two phrases or moving a sequence of
phrases, and to resegment phrases. The initial state
can either be initialized randomly, or be based on
an initial run from Moses. This setup is not limited
by dynamic programming constraints, and enables
the use of the full translated target document to
extract features.
3 English?French
Our English?French system is a phrase-based SMT
system with a combination of two decoders, Moses
(Koehn et al., 2007) and Docent (Hardmeier et al.,
2013a). The fundamental setup is loosely based
on the system submitted by Cho et al. (2013) to
the WMT 2013 shared task. Our phrase table is
trained on data taken from the News commentary,
Europarl, UN, Common crawl and 10
9
corpora.
The first three of these corpora were included in-
tegrally into the training set after filtering out sen-
tences of more than 80 words. The Common crawl
and 10
9
data sets were run through an additional
filtering step with an SVM classifier, closely fol-
lowing Mediani et al. (2011). The system includes
three language models, a regular 6-gram model
with modified Kneser-Ney smoothing (Chen and
Goodman, 1998) trained with KenLM (Heafield,
2011), a 4-gram bilingual language model (Niehues
et al., 2011) with Kneser-Ney smoothing trained
with KenLM and a 9-gram model over Brown clus-
ters (Brown et al., 1992) with Witten-Bell smooth-
ing (Witten and Bell, 1991) trained with SRILM
(Stolcke, 2002).
122
The latest version released in March is equipped with . . . It is sold at . . .
La derni`ere version lanc?ee en mars est dot?ee de . . . ? est vendue . . .
Figure 1: Pronominal Anaphora Model
Our baseline system achieved a cased BLEU
score of 33.2 points on the newstest2014 data set.
Since the anaphora model used in our submission
suffered from a serious bug, we do not discuss the
results of the primary submission in more detail.
3.1 Pronominal Anaphora Model
Our pronominal anaphora model is an adaptation
of the pronoun prediction model described by Hard-
meier et al. (2013b) to SMT. The model consists
of a neural network that discriminatively predicts
the translation of a source language pronoun from
a short list of possible target language pronouns us-
ing features from the context of the source language
pronouns and from the translations of possibly re-
mote antecedents. The objective of this model is to
handle situations like the one depicted in Figure 1,
where the correct choice of a target-language pro-
noun is subject to morphosyntactic agreement with
its antecedent. This problem consists of several
steps. To score a pronoun, the system must decide
if a pronoun is anaphoric and, if so, find potential
antecedents. Then, it can predict what pronouns
are likely to occur in the translation. Our pronoun
prediction model is trained on both tasks jointly,
including anaphora resolution as a set of latent vari-
ables. At test time, we split the network in two
parts. The anaphora resolution part is run sepa-
rately as a preprocessing step, whereas the pronoun
prediction part is integrated into the document-level
decoder with two additional feature models.
The features correspond to two copies of the neu-
ral network, one to handle the singular pronoun it
and one to handle the plural pronoun they. Each net-
work just predicts a binary distinction between two
cases, il and elle for the singular network and ils
and elles for the plural network. Unlike Hardmeier
et al. (2013b), we do not use an OTHER category to
capture cases that should not be translated with any
of these options. Instead, we treat all other cases in
the phrase table and activate the anaphora models
only if one of their target pronouns actually occurs
in the output.
To achieve this, we generate pronouns in two
steps. In the phrase table training corpus, we re-
place all pronouns that should be handled by the
classifier, i.e. instances of il and elle aligned to it
and instances of ils and elles aligned to they, with
special placeholders. At decoding time, if a place-
holder is encountered in a target language phrase,
the applicable pronouns are generated with equal
translation model probability, and the anaphora
model adds a score to discriminate between them.
To reduce the influence of the language model
on pronoun choice and give full control to the
anaphora model, our primary language model is
trained on text containing placeholders instead of
pronouns. Since all output pronouns can also be
generated without the interaction of the anaphora
model if they are not aligned to a source language
pronoun, we must make sure that the language
model sees training data for both placeholders and
actual pronouns. However, for the monolingual
training corpora we have no word alignments to
decide whether or not to replace a pronoun by a
placeholder. To get around this problem, we train a
6-gram placeholder language model on the target
language side of the Europarl and News commen-
tary corpora. Then, we use the Viterbi n-gram
model decoder of SRILM (Stolcke, 2002) to map
pronouns in the entire language model training set
to placeholders where appropriate. No substitu-
tions are made in the bilingual language model or
the Brown cluster language model.
3.2 Dependency Projection Model
Our English?French system also includes a depen-
dency projection model, which uses source-side
dependency structure to model target-side relations
between words. This model assigns a score to each
dependency arc in the source language by consider-
ing the target words aligned to the head and the de-
pendent. In Figure 2, for instance, there is an nsub-
jpass arc connecting dominated to production. The
head is aligned to the target word domin?ee, while
the dependent is aligned to the set {production,de}.
The score is computed by a neural network taking
as features the head and dependent words and their
part-of-speech tags in the source language, the tar-
get word sets aligned to the head and dependent,
the label of the dependency arc, the distance be-
tween the head and dependent word in the source
language as well as the shortest distance between
any pair of words in the aligned sets. The network
is a binary classifier trained to discriminate positive
examples extracted from human-made reference
123
Domestic meat production is dominated by chicken .
amod
nn
nsubjpass
auxpass prep pobj
punct
La production int?erieure de viande est domin?ee par le poulet .
Figure 2: Dependency projection model
translations from negative examples extracted from
n-best lists generated by a baseline SMT system.
4 English?German
For English?German we have two systems, one
based on Moses, and one based on Docent. In both
cases we have focused on word order, particularly
for verbs and particles.
Both our systems are trained on the same data
made available by WMT. The Common crawl data
was filtered using the method of Stymne et al.
(2013). We use factored models with POS tags
as a second output factor for German. The possi-
bility to use language models for different factors
has been added to our Docent decoder. Language
models include an in-domain news language model,
an out-of-domain model trained on the target side
of the parallel training data and a POS language
model trained on tagged news data. The LMs are
trained in the same way as for English?French.
All systems are tuned using MERT (Och, 2003).
Phrase-tables are filtered using entropy-based prun-
ing (Johnson et al., 2007) as implemented in Moses.
All BLEU scores are given for uncased data.
4.1 Pre-Ordered Alignment and
Post-Ordered Translation
The use of syntactic reordering as a separate pre-
processing step has already a long tradition in sta-
tistical MT. Handcrafted rules (Collins et al., 2005;
Popovi?c and Ney, 2006) or data-driven models (Xia
and McCord, 2004; Genzel, 2010; Rottmann and
Vogel, 2007; Niehues and Kolss, 2009) for pre-
ordering training data and system input have been
explored in numerous publications. For certain
language pairs, such as German and English, this
method can be very effective and often improves
the quality of standard SMT systems significantly.
Typically, the source language is reordered to better
match the syntax of the target language when trans-
lating between languages that exhibit consistent
word order differences, which are difficult to handle
by SMT systems with limited reordering capabil-
ities such as phrase-based models. Preordering is
often done on the entire training data as well to op-
timize translation models for the pre-ordered input.
Less common is the idea of post-ordering, which
refers to a separate step after translating source lan-
guage input to an intermediate target language with
corrupted (source-language like) word order (Na et
al., 2009; Sudoh et al., 2011).
In our experiments, we focus on the translation
from English to German. Post-ordering becomes
attractive for several reasons: One reason is the
common split of verb-particle constructions that
can lead to long distance dependencies in German
clauses. Phrase-based systems and n-gram lan-
guage models are not able to handle such relations
beyond a certain distance and it is desirable to keep
them as connected units in the phrase translation
tables. Another reason is the possible distance of
finite and infinitival verbs in German verb phrases
that can lead to the same problems described above
with verb-particle constructions. The auxiliary or
modal verb is placed at the second position but
the main verb appears at the end of the associated
verb phrase. The distances can be arbitrarily long
and long-range dependencies are quite frequent.
Similarly, negation particles and adverbials move
away from the inflected verb forms in certain con-
structions. For more details on specific phenomena
in German, we refer to (Collins et al., 2005; Go-
jun and Fraser, 2012). Pre-ordering, i.e. moving
English words into German word order does not
seem to be a good option as we loose the con-
nection between related items when moving par-
ticles and main verbs away from their associated
elements. Hence, we are interested in reordering
the target language German into English word or-
der which can be beneficial in two ways: (i) Re-
ordering the German part of the parallel training
data makes it possible to improve word alignment
(which tends to prefer monotonic mappings) and
subsequent phrase extraction which leads to better
translation models. (ii) We can explore a two-step
procedure in which we train a phrase-based SMT
model for translating English into German with
English word order first (which covers many long-
distance relations locally) and then apply a second
system that moves words into place according to
correct German syntax (which may involve long-
range distortion).
For simplicity, we base our experiments on hand-
124
crafted rules for some of the special cases discussed
above. For efficiency reasons, we define our rules
over POS tag patterns rather than on full syntac-
tic parse trees. We rely on TreeTagger and apply
rules to join verbs in discontinuous verb phrases
and to move verb-finals in subordinate clauses, to
move verb particles, adverbials and negation par-
ticles. Table 1 shows two examples of reordered
sentences together with the original sentences in
English and German. Our rules implement rough
heuristics to identify clause boundaries and word
positions. We do not properly evaluate these rules
but focus on the down-stream evaluation of the MT
system instead.
It is therefore dangerous to extrapolate from short-term trends.
Daher ist es gef?ahrlich, aus kurzfristigen Trends Prognosen abzuleiten.
Daher ist gef?ahrlich es, abzuleiten aus kurzfristigen Trends Prognosen.
The fall of Saddam ushers in the right circumstances.
Der Sturz von Saddam leitet solche richtigen Umst?ande ein.
Der Sturz von Saddam ein leitet solche richtigen Umst?ande.
Table 1: Two examples of pre-ordering outputs.
The first two lines are the original English and
German sentences and the third line shows the re-
ordered sentence.
We use three systems based on Moses to com-
pare the effect of reordering on alignment and trans-
lation. All systems are case-sensitive phrase-based
systems with lexicalized reordering trained on data
provided by WMT. Word alignment is performed
using fast align (Dyer et al., 2013). For tuning we
use newstest2011. Additionally, we also test paral-
lel data from OPUS (Tiedemann, 2012) filtered by
a method adopted from Mediani et al. (2011).
To contrast our baseline system, we trained a
phrase-based model on parallel data that has been
aligned on data pre-ordered using the reordering
rules for German, which has been restored to the
original word order after word alignment and be-
fore phrase extraction (similar to (Carpuat et al.,
2010; Stymne et al., 2010)). We expect that the
word alignment is improved by reducing crossings
and long-distance links. However, the translation
model as such has the same limitations as the base-
line system in terms of long-range distortions. The
final system is a two-step model in which we apply
translation and language models trained on pre-
ordered target language data to perform the first
step, which also includes a reordered POS language
model. The second step is also treated as a transla-
tion problem as in Sudoh et al. (2011), and in our
case we use a phrase-based model here with lexical-
ized reordering and a rather large distortion limit
of 12 words. Another possibility would be to apply
another rule set that reverts the misplaced words
to the grammatically correct positions. This, how-
ever, would require deeper syntactic information
about the target language to, for example, distin-
guish main from subordinate clauses. Instead, our
model is trained on parallel target language data
with the pre-ordered version as input and the orig-
inal version as output language. For this model,
both sides are tagged and a POS language model
is used again as one of the target language factors
in decoding. Table 2 shows the results in terms of
BLEU scores on the newstest sets from 2013 and
2014.
newstest2013 newstest2014
baseline 19.3 19.1
pre 19.4 19.3
post 18.6 18.7
baseline+OPUS 19.5 19.3
pre+OPUS 19.5 19.3
post+OPUS 19.7 18.8
Table 2: BLEU4 scores for English-German sys-
tems (w/o OPUS): Standard phrase-based (base-
line); phrase-based with pre-ordered parallel cor-
pus used for word alignment (pre); two-step phrase-
based with post-reordering (post)
The results show that pre-ordering has some ef-
fect on word alignment quality in terms of support-
ing better phrase extractions in subsequent steps.
Our experiments show a consistent but small im-
provement for models trained on data that have
been prepared in this way. In contrast, the two-step
procedure is more difficult to judge in terms of au-
tomatic metrics. On the 2013 newstest data we can
see another small improvement in the setup that
includes OPUS data but in most cases the BLEU
scores go down, even below the baseline. The
short-comings of the two-step procedure are ob-
vious. Separating translation and reordering in a
pipeline adds the risk of error propagation. Fur-
thermore, reducing the second step to single-best
translations is a strong limitation and using phrase-
based models for the final reordering procedure is
probably not the wisest decision. However, manual
inspections reveals that many interesting phenom-
ena can be handled even with this simplistic setup.
Table 3 illustrates this with a few selected out-
comes of our three systems. They show how verb-
particle constructions with long-range distortion
125
reference Schauspieler Orlando Bloom hat sich zur Trennung von seiner Frau , Topmodel Miranda Kerr , ge?au?ert .
baseline Schauspieler Orlando Bloom hat die Trennung von seiner Frau , Supermodel Miranda Kerr .
pre-ordering Schauspieler Orlando Bloom hat angek?undigt , die Trennung von seiner Frau , Supermodel Miranda Kerr .
post-ordering Schauspieler Orlando Bloom hat seine Trennung von seiner Frau angek?undigt , Supermodel Miranda Kerr .
reference Er gab bei einer fr?uheren Befragung den Kokainbesitz zu .
baseline Er gab den Besitz von Kokain in einer fr?uheren Anh?orung .
pre-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung .
post-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung ein .
reference Borussia Dortmund k?undigte daraufhin harte Konsequenzen an .
baseline Borussia Dortmund k?undigte an , es werde schwere Folgen .
pre-ordering Borussia Dortmund hat angek?undigt , dass es schwerwiegende Konsequenzen .
post-ordering Borussia Dortmund k?undigte an , dass es schwere Folgen geben werde .
Table 3: Selected translation examples from the newstest 2014 data; the human reference translation; the
baseline system, pre-ordering for word alignment and two-step translation with post-ordering.
such as ?r?aumte ... ein? can be created and how
discontinuous verb phrases can be handled (?hat ...
angek?undigt?) with the two-step procedure. The
model is also often better in producing verb finals
in subordinate clauses (see the final example with
?geben werde?). Note that many of these improve-
ments do not get any credit by metrics like BLEU.
For example the acceptable expression ?r?aumte ein?
which is synonymous to ?gab zu? obtains less credit
then the incomplete baseline translation. Interest-
ing is also to see the effect of pre-ordering when
used for alignment only in the second system. The
first example in Table 3, for example, includes a
correct main verb which is omitted in the baseline
translation, probably because it is not extracted as
a valid translation option.
4.2 Part-of-Speech Phrase-Distortion Models
Traditional SMT distortion models consist of two
parts. A distance-based distortion cost is based
on the position of the last word in a phrase, com-
pared to the first word in the next phrase, given the
source phrase order. A hard distortion limit blocks
translations where the distortion is too large. The
distortion limit serves to decrease the complexity
of the decoder, thus increasing its speed.
In the Docent decoder, the distortion limit is not
implemented as a hard limit, but as a feature, which
could be seen as a soft constraint. We showed in
previous work (Stymne et al., 2013) that it was
useful to relax the hard distortion limit by either
using a soft constraint, which could be tuned, or
removing the limit completely. In that work we
still used the standard parametrization of distortion,
based on the positions of the first and last words in
phrases.
Our Docent decoder, however, always provides
us with a full target translation that is step-wise im-
proved, which means that we can apply distortion
measures on the phrase-level without resorting to
heuristics, which, for instance, are needed in the
case of the lexicalized reordering models in Moses
(Koehn et al., 2005). Because of this it is possible
to use phrase-based distortion, where we calculate
distortion based on the order of phrases, not on the
order of some words. It is possible to parametrize
phrase-distortion in different ways. In this work we
use the phrase-distortion distance and a soft limit
on the distortion distance, to mimic the word-based
distortion. In our experiments we always set the
soft limit to a distance of four phrases. In addition
we use a measure based on how many crossings
a phrase order gives rise to. We thus have three
phrase-distortion features.
As captured by lexicalized reordering models,
different phrases have different tendencies to move.
To capture this to some extent, we also decided
to add part-of-speech (POS) classes to our mod-
els. POS has previously successfully been used
in pre-reordering approaches (Popovi?c and Ney,
2006; Niehues and Kolss, 2009). The word types
that are most likely to move long distances in
English?German translation are verbs and parti-
cles. Based on this observation we split phrases
into two classes, phrases that only contains verbs
and particles, and all other phrases. For these two
groups we use separate phrase-distortion features,
thus having a total of six part-of-speech phrase-
distortion features. All of these features are soft,
and are optimized during tuning.
In our system we initialize Docent by running
Moses with a standard distortion model and lexi-
calized reordering, and then continuing the search
with Docent including our part-of-speech phrase-
distortion features. Tuning was done separately for
the two components, first for the Moses component,
and then for the Docent component initialized by
126
reference Laut Dmitrij Kislow von der Organisation ?Pravo na oryzhie? kann man eine Pistole vom Typ Makarow f?ur 100 bis 300 Dollar kaufen.
baseline Laut Dmitry Kislov aus der Rechten zu Waffen, eine Makarov Gun-spiele erworben werden k?onnen f?ur 100-300 Dollar.
POS+phrase Laut Dmitry Kislov von die Rechte an Waffen, eine Pistole Makarov f?ur 100-300 Dollar erworben werden k?onnen.
reference Die Waffen gelangen ?uber mehrere Kan?ale auf den Schwarzmarkt.
baseline Der ?Schwarze? Markt der Waffen ist wieder aufgef ?ullt ?uber mehrere Kan?ale.
POS+phrase Der ?Schwarze? Markt der Waffen durch mehrere Kan?ale wieder aufgef ?ullt ist.
reference Mehr Kameras k?onnten m?oglicherweise das Problem l?osen...
baseline M?oglicherweise k?onnte das Problem l?osen, eine gro?e Anzahl von Kameras...
POS+phrase M?oglicherweise, eine gro?e Anzahl von Kameras k?onnte das Problem l?osen...
Table 4: Selected translation examples from the newstest2013 data; the human reference translation; the
baseline system (Moses with lexicalized reordering) and the system with a POS+phrase distortion model.
Moses with lexicalized reordering with its tuned
weights. We used newstest2009 for tuning. The
training data was lowercased for training and de-
coding, and recasing was performed using a sec-
ond Moses run trained on News data. As baselines
we present two Moses systems, without and with
lexicalized reordering, in addition to standard dis-
tortion features.
Table 5 shows results with our different distor-
tion models. Overall the differences are quite small.
The clearest difference is between the two Moses
baselines, where the lexicalized reordering model
leads to an improvement. With Docent, both the
word distortion and phrase distortion without POS
do not help to improve on Moses, with a small de-
crease in scores on one dataset. This is not very
surprising, since lexical distortion is currently not
supported by Docent, and the distortion models are
thus weaker than the ones implemented in Moses.
For our POS phrase distortion, however, we see a
small improvement compared to Moses, despite the
lack of lexicalized distortion. This shows that this
distortion model is actually useful, and can even
successfully replace lexicalized reordering. In fu-
ture work, we plan to combine this method with a
lexicalized reordering model, to see if the two mod-
els have complementary strengths. Our submitted
system uses the POS phrase-distortion model.
System Distortion newstest2013 newstest2014
Moses word 19.4 19.3
Moses word+LexReo 19.6 19.6
Docent word 19.5 19.6
Docent phrase 19.5 19.6
Docent POS+phrase 19.7 19.7
Table 5: BLEU4 scores for English?German sys-
tems with different distortion models.
If we inspect the translations, most of the differ-
ences between the Moses baseline and the system
with POS+phrase distortion are actually due to lex-
ical choice. Table 4 shows some examples where
there are word order differences. The result is quite
mixed with respect to the placement of verbs. In
the first example, both systems put the verbs to-
gether but in different positions, instead of splitting
them like the reference suggests. In the second
example, our system erroneously put the verbs at
the end, which would be fine if the sentence had
been a subordinate clause. In the third example,
the baseline system has the correct placement of
the auxiliary ?k?onnte?, while our system is bet-
ter at placing the main verb ?l?osen?. In general,
this indicates that our system is able to support
long-distance distortion as it is needed in certain
cases but sometimes overuses this flexibility. A
better model would certainly need to incorporate
syntactic information to distinguish main from sub-
ordinate clauses. However, this would add a lot of
complexity to the model.
5 Conclusion
We have described the three Uppsala University
systems for WMT14. In the English?French sys-
tem we extend our document-level decoder Do-
cent (Hardmeier et al., 2013a) to handle pronoun
anaphora and introduced a dependency projection
model. In our two English?German system we
explore different methods for handling reordering,
based on Docent and Moses. In particular, we look
at post-ordering as a separate step and tunable POS
phrase distortion.
Acknowledgements
This work forms part of the Swedish strategic re-
search programme eSSENCE. We also acknowl-
edge the use of the Abel cluster, owned by the
University of Oslo and the Norwegian metacenter
for High Performance Computing (NOTUR) and
operated by the Department for Research Comput-
ing at USIT, under project nn9106k. Finally, we
would also like to thank Eva Pettersson, Ali Basirat,
and Eva Martinez for help with human evaluation.
127
References
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical report, Computer Sci-
ence Group, Harvard University, Cambridge, Mas-
sachusetts, USA.
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan
Niehues, Teresa Herrmann, Isabel Slawik, and Alex
Waibel. 2013. The Karlsruhe Institute of Technol-
ogy translation systems for the WMT 2013. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 104?108, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michi-
gan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the 2013
Conference of the NAACL: Human Language Tech-
nologies, pages 644?648, Atlanta, Georgia, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 376?
384, Beijing, China.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English?to?
German SMT. In Proceedings of the 13th Confer-
ence of the EACL, pages 726?735, Avignon, France.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1179?
1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, J?org Tiedemann,
and Joakim Nivre. 2013a. Docent: A document-
level decoder for phrase-based statistical machine
translation. In Proceedings of the 51st Annual Meet-
ing of the ACL, Demonstration session, pages 193?
198, Sofia, Bulgaria.
Christian Hardmeier, J?org Tiedemann, and Joakim
Nivre. 2013b. Latent anaphora resolution for cross-
lingual pronoun prediction. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 380?391, Seattle,
Washington, USA. Association for Computational
Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The
KIT English?French translation systems for IWSLT
2011. In Proceedings of the International Workshop
on Spoken Language Translation, pages 73?78, San
Francisco, California, USA.
Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-Hyeok
Lee. 2009. Improving fluency by reordering tar-
get constituents using MST parser in English-to-
Japanese phrase-based SMT. In Proceedings of
MT Summit XII, pages 276?283, Ottawa, Ontario,
Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 198?206, Edinburgh, Scot-
land. Association for Computational Linguistics.
128
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167,
Sapporo, Japan.
Maja Popovi?c and Hermann Ney. 2006. POS-based re-
orderings for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
1278?1283, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion limits
and corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Translation,
pages 225?231, Sofia, Bulgaria.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proceedings of
MT Summit XIII, pages 316?323, Xiamen. China.
J?org Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC?12), pages 2214?2218, Istanbul,
Turkey.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
129
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275?286,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Estimating Word Alignment Quality for SMT Reordering Tasks
Sara Stymne J
?
org Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
Previous studies of the effect of word
alignment on translation quality in SMT
generally explore link level metrics only
and mostly do not show any clear connec-
tions between alignment and SMT qual-
ity. In this paper, we specifically inves-
tigate the impact of word alignment on
two pre-reordering tasks in translation, us-
ing a wider range of quality indicators
than previously done. Experiments on
German?English translation show that re-
ordering may require alignment models
different from those used by the core trans-
lation system. Sparse alignments with
high precision on the link level, for trans-
lation units, and on the subset of cross-
ing links, like intersected HMM models,
are preferred. Unlike SMT performance
the desired alignment characteristics are
similar for small and large training data
for the pre-reordering tasks. Moreover,
we confirm previous research showing that
the fuzzy reordering score is a useful and
cheap proxy for performance on SMT re-
ordering tasks.
1 Introduction
Word alignment is a key component in all state-of-
the-art statistical machine translation (SMT) sys-
tems, and there has been some work exploring the
connection between word alignment quality and
translation quality (Och and Ney, 2003; Fraser and
Marcu, 2007; Lambert et al., 2012). The standard
way to evaluate word alignments in this context is
by using metrics like alignment error rate (AER)
and F-measure on the link level, and the general
conclusion appears to be that translation quality
benefits from alignments with high recall (rather
than precision), at least for large training data. Al-
though many other ways of measuring alignment
quality have been proposed, such as working on
translation units (Ahrenberg et al., 2000; Ayan and
Dorr, 2006; S?gaard and Kuhn, 2009) or using link
degree and related measures (Ahrenberg, 2010),
these methods have not been used to study the re-
lation between alignment and translation quality,
with the exception of Lambert et al. (2012).
Word alignment is also used for many other
tasks besides translation, including term bank
creation (Merkel and Foo, 2007), cross-lingual
annotation projection for part-of-speech tagging
(Yarowsky et al., 2001), semantic roles (Pado and
Lapata, 2005), pronoun anaphora (Postolache et
al., 2006), and cross-lingual clustering (T?ackstr?om
et al., 2012). Even within SMT itself, there are
tasks such as reordering that often make crucial
use of word alignments. For instance, source lan-
guage reordering commonly relies on rules learnt
automatically from word-aligned data (e.g., Xia
and McCord (2004)). As far as we know, no one
has studied the impact of alignment quality on
these additional tasks, and it seems to be tacitly
assumed that alignments that are good for transla-
tion are also good for other tasks.
In this paper we set out to explore the impact
of alignment quality on two pre-reordering tasks
for SMT. In doing so, we employ a wider range of
quality indicators than is customary, and for refer-
ence these indicators are used also to assess over-
all translation quality. To allow an in-depth explo-
ration of the connections between several aspects
of word alignment and reordering, we limit our
study to one language pair, German?English. We
think this is a suitable language pair for studying
reordering since it has both short range and long
range reorderings. Our main focus is on using rel-
atively large training data, 2M sentences, but we
also report results with small training data, 170K
sentences. The main conclusion of our study is
that alignments that are optimal for translation are
not necessarily optimal for reordering, where pre-
275
cision is of greater importance than recall. For
SMT the best alignments are different depending
on corpus size, but for the reordering tasks results
are stable across training data size.
In section 2 we discuss previous work related
to word alignment and SMT. In section 3, we in-
troduce the word alignment quality indicators we
use, and show experimental results for a number
of alignment systems on an SMT task. In sec-
tion 4, we turn to reordering for SMT and use
the same quality indicators to study the impact of
alignment quality on reordering quality. In section
5 we briefly describe results using small training
data. In section 6, we conclude and suggest direc-
tions for future work.
2 Word Alignment and SMT
Word alignment is the task of relating words
in one language to words in the translation in
another language, see an example in Figure 1.
Word alignment models can be learnt automati-
cally from large corpora of sentence aligned data.
Brown et al. (1993) proposed the so-called IBM
models, which are still widely used. These five
models estimate alignments from corpora using
the expectation-maximization algorithm, and each
model adds some complexity. Model 4 is com-
monly used in SMT systems. There have been
many later suggestions of alternatives to these
models. These are often alternatives to model 2,
such as the HMM model (Vogel et al., 1996) and
fast align (Dyer et al., 2013).
All these generative models produce directional
alignments where one word in the source can be
linked to many target words (1?m links) but not
vice versa. It is generally desirable to also allow
n?1 and n?m links, and to achieve this it is com-
mon practice to perform word alignment in both
directions and to symmetrize them using some
heuristic. A number of common symmetrization
strategies are described in Table 1 (Koehn et al.,
2005). There are also other alternatives, such as
the refined method (Och and Ney, 2003), or link
deletion from the union (Fossum et al., 2008).
There is also a wide range of alternative ap-
proaches to word alignment. For example, various
discriminative models have been proposed in the
literature (Liu et al., 2005; Moore, 2005; Taskar
et al., 2005). Their advantage is that they may
integrate a wide range of features that may lead
to improved alignment quality. However, most of
Symmetrization Description
int: intersection A
TS
?A
ST
uni: union A
TS
?A
ST
gd: grow-diag intersection plus adjacent links
from the union if both linked
words are unaligned
gdf: grow-diag-final gd with links from the union
added in a final step if either
linked word is unaligned
gdfa:
grow-diag-final-and
gd with links from the union
added in a final step if both linked
words are unaligned
Table 1: Symmetrization strategies for word align-
ments A
TS
and A
ST
in two directions
these models require external tools (for creating
linguistic features) and manually aligned training
data, which we do not have for our data sets (be-
sides the data we need for evaluation). Investigat-
ing these types of models are outside the scope of
our current work.
Word alignments are used as an important
knowledge source for training SMT systems. In
word-based SMT, the parameters of the gener-
ative word alignment models are essentially the
translation model of the system. In phrase-based
SMT (PBSMT) (Koehn et al., 2003), which is
among the state-of-the-art systems today, word
alignments are used as a basis for extracting
phrases and estimating phrase alignment probabil-
ities. Similarly, word alignments are also used for
estimating rule probabilities in various kinds of hi-
erarchical and syntactic SMT (Chiang, 2007; Ya-
mada and Knight, 2002; Galley et al., 2004).
Intrinsic evaluation of word alignment is gener-
ally based on a comparison to a gold standard of
human alignments. Based on the gold standard,
metrics like precision, recall and F-measure can
be calculated for each alignment link, see Eqs. 1?
2, where A are hypothesized alignment links and
G are gold standard links. Another common met-
ric is alignment error rate (AER) (Och and Ney,
2000), which is based on a distinction between
sure, S, and possible, P , links in the gold stan-
dard. 1?AER is identical to balanced F-measure
when the gold standard does not make a distinc-
tion between S and P.
Precision(A,G) =
|G ?A|
|A|
(1)
Recall(A,G) =
|G ?A|
|G|
(2)
AER = 1?
|P ?A|+ |S ?A|
|S|+ |A|
(3)
276
Crossing = 8
SKDT =
?
8/66 ? 0.65
6 1?1 links
3 multi links
0 null links
Figure 1: An example alignment illustrating n?1, 1?m and crossing links.
The relation between word alignment qual-
ity and PBSMT has been studied by some re-
searchers. Och and Ney (2000) looked at the im-
pact of IBM and HMM models on the alignment
template approach (Och et al., 1999) in terms of
AER. They found that AER correlates with human
evaluation of sentence level quality, but not with
word error rate. Fraser and Marcu (2007) found
that there is no correlation between AER and Bleu
(Papineni et al., 2002), especially not when the P -
set is large. They found that a balanced F-measure
is a better indicator of Bleu, but that a weighted
F-measure is even better (see Eq. 4) mostly with
a higher weight for recall than for precision. This
weight, however, needs to be optimized for each
data set, language pair, and gold standard align-
ment separately.
F(A,G, ?) =
(
?
Precision(A,G)
+
1? ?
Recall(A,G)
)
?1
(4)
Ayan and Dorr (2006) on the other hand found
some evidence for the importance of precision
over recall. However, they used much smaller
training data than Fraser and Marcu (2007). They
also suggested using a measure called consistent
phrase error-rate (CPER), but found that it was
hard to assess the impact of alignment on MT, both
with AER and CPER. Lambert et al. (2012) per-
formed a study where they investigated the effect
of word alignment on MT using a large number of
word alignment indicators. They found that there
was a difference between large and small datasets
in that alignment precision was more important
with small data sets, and recall more important
with large data sets. Overall they did not find any
indicator that was significant over two language
pairs and different corpus sizes. There were more
significant indicators for large datasets, however.
Most researchers who propose new alignment
models perform both a gold standard evalua-
tion and an SMT evaluation (Liang et al., 2006;
Ganchev et al., 2008; Junczys-Dowmunt and Sza?,
2012; Dyer et al., 2013). The relation between the
two types of evaluation is often quite weak. Sev-
eral of these studies only show AER on their gold
standard, despite its well-known shortcomings.
Even though many studies have shown some
relation between translation quality and AER or
weighted F-measure, it has rarely been investi-
gated thoroughly in its own right, and, as far as we
are aware, not for other tasks than SMT. Further-
more, most of these studies considers nothing else
but link level agreement. In this paper we take a
broader view on alignment quality and explore the
effect of other types of quality indicators as well.
3 Word Alignment Quality Indicators
We investigate four groups of quality indicators.
The first group is the classic group where met-
rics are calculated on the alignment link level,
which has been used in several studies. In our
experiments we use a gold standard that does not
make use of distinctions between sure and possible
links, as suggested by Fraser and Marcu (2007).
With this, we can calculate the standard metrics
P(recision) R(ecall) and F(-measure). We will
mainly use balanced F-measure, but occasionally
also report weighted F-measure. As noted before,
1?AER is equivalent to balanced F when only
sure links are used, and will thus not be reported
separately.
S?gaard and Kuhn (2009) and S?gaard and Wu
(2009) suggested working on the translation unit
(TU) level, instead of the link level. A translation
unit, or cept (Goutte et al., 2004), is defined as
a maximally connected subgraph of an alignment.
In Figure 1, the twelve links form nine translation
units. S?gaard and Wu (2009) suggest the metric
TUER, translation unit error rate, shown in Eq. 5,
where A
U
are hypothesized translation units, and
G
U
are gold standard translation units.
1
They use
TUER to establish lower bounds for the cover-
age of alignments from different formalisms, not
to evaluate SMT. While they only use TUER, it
1
TUER is similar to CPER (Ayan and Dorr, 2006), which
measures the error rate of extracted phrases. Due to how
phrase extraction handle null links, there are differences,
however.
277
is also possible to define Precision, Recall and F-
measure over translation units in the same way as
for alignment links. We will use these three mea-
sures to get a broader picture of TUs in alignment
evaluation. Also in this case, 1?TUER is equiva-
lent to F-measure.
TUER(A,G) = 1?
2|A
U
?G
U
|
|A
U
|+ |G
U
|
(5)
The TU metrics are quite strict, since they re-
quire exact matching of TUs. Tiedemann (2005)
suggested the MWU metrics for word alignment
evaluation, which also consider partial matches
of annotated multi-word units, which is a similar
concept to TUs. In those metrics, precision and
recall grow proportionally to the number of cor-
rectly aligned words within translation units. Pro-
posed links are in this way scored according to
their overlap with translation units in the gold stan-
dard. Precision and recall are defined in Eqs. 6?7,
where overlap(X
U
, Y ) is the number of source
and target words in X
U
that overlap with transla-
tion units in Y normalized by the size of X
U
(in
terms of source and target words). Note, that TUs
need to overlap in source and target. Otherwise,
their overlap will be counted as zero.
P
MWU
=
?
A
U
?A
overlap(A
U
, G)
|A|
(6)
R
MWU
=
?
G
U
?G
overlap(G
U
, A)
|G|
(7)
There have also been attempts at classifying
alignments in other ways, not related to a gold
standard. Ahrenberg (2010) proposed several
ways to categorize human alignments, including
link degree, reordering of links, and structural cor-
respondence. He used these indicators to profile
hand-aligned corpora from different domains. We
will not use structural correspondence, which re-
quires a dependency parser, and which we believe
is error prone when performed automatically. We
will use what we call link degree, i.e., how many
alignment links each word obtains. Ahrenberg
(2010) used a fine-grained scheme of the percent-
age for different degrees, including isomorphism
1?1, deletion 0?1, reduction m?1, and paraphrase
m?n. Similar link degree classes were used by
Lambert et al. (2012). In this work we will re-
duce these classes into three: 1?1 links, null links,
which combine the 0?1 and 1?0 cases, and multi
links where there are many words on at least one
side.
Ahrenberg (2010) also proposed to measure re-
orderings. He does this by calculating the percent-
age of links with crossings of different lengths. To
define this he only considers adjacent links in the
source using the distance between corresponding
target words, which means that his metric becomes
a directional measure. Reorderings of alignments
was also used by Genzel (2010), who used cross-
ing score, the number of crossing links, to rank
reordering rules. This is non-directional and sim-
pler to calculate than Ahrenberg (2010)?s metrics,
and implicitly covers length since a long distance
reordering leads to a higher number of pairwise
crossing links. Birch and Osborne (2011) sug-
gest using squared Kendall ? distance (SKTD), see
Eq. 8, where n is the number of links, as a basis
of LR-score, an MT metric that takes reordering
into account. They found that squaring ? better
explained reordering, than using only ? . In this
study we will use both, crossing score and SKTD.
Figure 1 shows these scores for an example sen-
tence. These two measures only tell us how much
reordering there is. To quantify this relative to the
gold standard we also report the absolute differ-
ence between the number of gold standard cross-
ings and system crossings, which we call Crossd-
iff. To account for the quality of crossings, to some
extent, we will also report precision, recall, and F-
measure for the subset of translation units that are
involved in a crossing.
SKTD =
?
|crossing link pairs|
(n
2
? n)/2
(8)
3.1 Alignment Experiments
We perform all our experiments for German?
English. The alignment indicators are calculated
on a corpus of 987 hand aligned sentences (Pado
and Lapata, 2005). The gold standard contains
explicit null links, which the symmetrized auto-
matic alignments do not. To allow a straightfor-
ward comparison we consistently remove all null
links when comparing system alignments to the
gold standard.
For creating the automatic alignments we used
GIZA++ (Och and Ney, 2003) to compute direc-
tional alignments for model 2?4 and the HMM
model, and fast align (fa) (Dyer et al., 2013) as
newer alternatives to model 2. These models re-
quire large amounts of data to be estimated reli-
ably. To achieve this we concatenated the gold
standard with the large SMT training data (see
278
A
l
i
g
n
m
e
n
t
l
i
n
k
s
T
r
a
n
s
l
a
t
i
o
n
u
n
i
t
s
M
W
U
L
i
n
k
d
e
g
r
e
e
L
i
n
k
c
r
o
s
s
i
n
g
s
T
o
t
a
l
P
R
F
T
o
t
a
l
P
R
F
P
R
F
1
-
1
n
u
l
l
m
u
l
t
i
T
o
t
a
l
S
K
T
D
P
R
F
C
r
o
s
s
d
i
f
f
g
o
l
d
2
2
6
2
9
?
?
?
1
7
0
6
8
?
?
?
?
?
?
.
5
4
2
.
3
2
8
.
1
3
0
3
0
1
6
3
.
2
9
2
?
?
?
0
2
-
i
n
t
1
5
3
6
2
.
8
5
0
.
5
7
7
.
6
8
7
1
5
3
6
2
.
7
0
1
.
6
3
1
.
6
6
4
.
8
4
9
.
7
1
2
.
7
7
4
.
5
0
0
.
5
0
0
.
0
0
0
1
0
0
6
4
.
2
6
7
.
5
5
1
.
4
6
3
.
5
0
3
2
0
0
9
9
3
-
i
n
t
1
6
5
7
3
.
8
6
0
.
6
3
0
.
7
2
7
1
6
5
7
3
.
7
0
7
.
6
8
6
.
6
9
7
.
8
5
7
.
7
7
6
.
8
1
4
.
5
6
1
.
4
3
9
.
0
0
0
1
2
6
8
2
.
2
7
4
.
5
5
3
.
5
2
1
.
5
3
7
1
7
4
8
1
4
-
i
n
t
1
6
5
2
9
.
9
0
3
.
6
6
0
.
7
6
3
1
6
5
2
9
.
7
4
3
.
7
2
0
.
7
3
1
.
9
0
1
.
8
1
3
.
8
5
5
.
5
5
9
.
4
4
1
.
0
0
0
1
1
2
2
9
.
2
5
1
.
6
6
3
.
5
2
2
.
5
8
4
1
8
9
3
4
H
M
M
-
i
n
t
1
4
8
7
1
.
9
2
2
.
6
0
6
.
7
3
1
1
4
8
7
1
.
7
6
8
.
6
6
9
.
7
1
5
.
9
2
0
.
7
5
0
.
8
2
7
.
4
7
6
.
5
2
4
.
0
0
0
8
0
7
7
.
2
2
1
.
7
0
9
.
4
1
7
.
5
2
5
2
2
0
8
6
f
a
-
i
n
t
1
5
9
9
7
.
8
5
7
.
6
0
6
.
7
1
0
1
5
9
9
7
.
6
9
6
.
6
5
2
.
6
7
3
.
8
5
4
.
7
4
2
.
7
9
4
.
5
3
1
.
4
6
9
.
0
0
0
9
7
2
4
.
2
4
6
.
5
6
8
.
4
7
1
.
5
1
5
2
0
4
3
9
2
-
g
d
2
2
8
8
2
.
7
0
2
.
7
1
0
.
7
0
6
1
6
5
1
1
.
5
9
9
.
5
7
9
.
5
8
9
.
8
0
6
.
8
2
7
.
8
1
6
.
5
2
4
.
2
8
9
.
1
8
6
2
1
8
2
3
.
2
7
0
.
4
4
6
.
4
4
4
.
4
4
5
8
3
4
0
3
-
g
d
2
1
9
6
1
.
7
5
7
.
7
3
4
.
7
4
5
1
7
6
4
4
.
6
5
0
.
6
7
2
.
6
6
1
.
8
1
7
.
8
5
5
.
8
3
6
.
6
0
8
.
2
7
0
.
1
2
2
2
1
8
8
6
.
2
7
8
.
4
9
2
.
5
2
3
.
5
0
7
8
2
7
7
4
-
g
d
2
2
7
5
4
.
7
6
8
.
7
7
2
.
7
7
0
1
7
6
1
1
.
6
7
0
.
6
9
2
.
6
8
1
.
8
3
9
.
8
8
6
.
8
6
2
.
6
0
5
.
2
4
7
.
1
4
8
2
1
9
6
6
.
2
5
9
.
5
8
3
.
5
1
7
.
5
4
8
8
1
9
7
H
M
M
-
g
d
1
9
4
3
0
.
8
1
2
.
6
9
8
.
7
5
1
1
5
8
3
1
.
7
0
9
.
6
5
8
.
6
8
2
.
8
7
8
.
8
2
0
.
8
4
8
.
4
9
9
.
4
0
7
.
0
9
4
1
4
3
3
4
.
2
3
1
.
6
2
1
.
4
1
1
.
4
9
5
1
5
8
2
9
f
a
-
g
d
2
3
1
4
8
.
7
0
2
.
7
1
9
.
7
1
0
1
7
0
4
3
.
5
8
9
.
5
8
8
.
5
8
8
.
8
0
2
.
8
3
9
.
8
2
0
.
5
4
8
.
2
5
8
.
1
9
4
1
8
5
7
8
.
2
4
2
.
4
5
4
.
4
4
7
.
4
5
0
1
1
5
8
5
2
-
g
d
f
a
2
3
8
4
0
.
6
8
7
.
7
2
4
.
7
0
5
1
7
4
6
9
.
5
7
5
.
5
8
8
.
5
8
2
.
7
8
0
.
8
4
1
.
8
0
9
.
5
9
0
.
2
1
6
.
1
9
4
2
5
6
1
6
.
2
7
9
.
4
1
9
.
4
7
3
.
4
4
4
6
7
1
8
3
-
g
d
f
a
2
3
0
4
9
.
7
3
6
.
7
4
9
.
7
4
2
1
8
7
3
2
.
6
2
1
.
6
8
1
.
6
5
0
.
7
8
6
.
8
7
0
.
8
2
6
.
6
8
4
.
1
8
8
.
1
2
8
2
7
1
1
9
.
2
9
4
.
4
5
1
.
5
6
1
.
5
0
0
4
5
4
7
4
-
g
d
f
a
2
3
7
0
4
.
7
5
1
.
7
8
7
.
7
6
9
1
8
5
6
1
.
6
4
5
.
7
0
1
.
6
7
2
.
8
1
3
.
9
0
1
.
8
5
5
.
6
7
3
.
1
7
2
.
1
5
4
2
6
9
7
7
.
2
7
5
.
5
2
9
.
5
6
2
.
5
4
5
3
0
4
4
H
M
M
-
g
d
f
a
2
0
5
5
4
.
7
9
9
.
7
2
6
.
7
6
1
1
6
9
5
5
.
6
8
5
.
6
8
1
.
6
8
3
.
8
5
7
.
8
5
1
.
8
5
4
.
5
6
5
.
3
3
7
.
0
9
8
1
7
3
9
9
.
2
4
6
.
5
8
4
.
4
7
5
.
5
2
4
1
2
7
6
4
f
a
-
g
d
f
a
2
3
7
1
7
.
6
9
3
.
7
2
6
.
7
1
0
1
7
6
1
2
.
5
7
5
.
5
9
4
.
5
8
4
.
7
8
5
.
8
4
6
.
8
1
5
.
5
8
7
.
2
1
4
.
1
9
9
2
0
3
8
4
.
2
4
7
.
4
3
9
.
4
6
5
.
4
5
2
9
7
7
9
2
-
g
d
f
2
9
0
5
0
.
5
9
1
.
7
5
8
.
6
6
4
1
7
0
8
9
.
5
1
1
.
5
1
2
.
5
1
2
.
7
6
1
.
8
7
6
.
8
1
4
.
6
2
5
.
0
0
2
.
3
7
3
5
9
5
9
2
.
3
3
8
.
3
2
1
.
4
3
8
.
3
7
0
2
9
4
2
9
3
-
g
d
f
2
6
5
7
5
.
6
6
0
.
7
7
5
.
7
1
3
1
8
3
5
4
.
5
8
8
.
6
3
2
.
6
0
9
.
7
7
8
.
8
9
1
.
8
3
1
.
7
1
2
.
0
6
4
.
2
2
5
5
0
8
3
4
.
3
4
4
.
3
8
7
.
5
5
2
.
4
5
5
2
0
6
7
1
4
-
g
d
f
2
6
5
2
9
.
6
9
3
.
8
1
2
.
7
4
8
1
8
2
6
9
.
6
2
8
.
6
7
3
.
6
5
0
.
8
1
0
.
9
2
2
.
8
6
2
.
7
0
6
.
0
7
0
.
2
2
3
4
7
2
1
6
.
3
2
2
.
4
5
9
.
5
8
5
.
5
1
4
1
7
0
5
3
H
M
M
-
g
d
f
2
3
8
8
6
.
7
2
5
.
7
6
5
.
7
4
4
1
6
6
6
0
.
6
5
1
.
6
3
5
.
6
4
3
.
8
5
1
.
8
8
7
.
8
6
9
.
5
7
9
.
2
5
1
.
1
6
9
3
6
8
8
1
.
3
0
9
.
4
7
3
.
4
9
9
.
4
8
6
6
7
1
8
f
a
-
g
d
f
2
6
7
2
4
.
6
3
3
.
7
4
8
.
6
8
6
1
7
4
5
4
.
5
2
4
.
5
3
6
.
5
3
0
.
7
6
9
.
8
6
5
.
8
1
4
.
5
8
9
.
1
0
1
.
3
1
0
3
4
3
0
9
.
3
7
9
.
3
5
1
.
4
4
5
.
3
9
2
4
1
4
6
2
-
u
n
i
3
0
7
1
2
.
5
6
6
.
7
6
9
.
6
5
2
1
5
8
6
4
.
5
0
3
.
4
6
8
.
4
8
5
.
7
7
4
.
8
6
9
.
8
1
8
.
5
8
4
.
0
0
2
.
4
1
3
7
1
2
2
3
.
3
4
9
.
3
0
5
.
3
9
6
.
3
4
5
4
1
0
6
0
3
-
u
n
i
2
8
0
9
3
.
6
3
6
.
7
8
9
.
7
0
4
1
7
3
9
1
.
5
9
2
.
6
0
3
.
5
9
7
.
7
9
1
.
8
8
9
.
8
3
7
.
6
8
4
.
0
6
7
.
2
4
9
6
1
8
2
3
.
3
5
5
.
3
8
1
.
5
2
3
.
4
4
1
3
1
6
6
0
4
-
u
n
i
2
7
9
2
0
.
6
7
0
.
8
2
7
.
7
4
0
1
7
4
1
1
.
6
3
6
.
6
4
9
.
6
4
2
.
8
2
6
.
9
2
1
.
8
7
1
.
6
8
2
.
0
7
4
.
2
4
4
5
7
4
0
8
.
3
3
3
.
4
5
6
.
5
6
4
.
5
0
4
2
7
2
4
5
H
M
M
-
u
n
i
2
4
7
1
2
.
7
0
7
.
7
7
2
.
7
3
8
1
5
9
8
0
.
6
4
9
.
6
0
8
.
6
2
8
.
8
5
7
.
8
8
1
.
8
6
9
.
5
6
1
.
2
6
0
.
1
8
0
4
2
2
6
4
.
3
1
9
.
4
5
9
.
4
7
5
.
4
6
7
1
2
1
0
1
f
a
-
u
n
i
2
7
9
5
1
.
6
1
2
.
7
5
6
.
6
7
6
1
6
3
8
5
.
5
1
2
.
4
9
1
.
5
0
4
.
7
8
1
.
8
6
7
.
8
2
2
.
5
4
8
.
1
1
1
.
3
4
6
3
8
2
8
5
.
3
9
6
.
3
3
6
.
4
0
7
.
3
6
8
8
1
2
2
T
a
b
l
e
2
:
V
a
l
u
e
s
f
o
r
a
l
i
g
n
m
e
n
t
q
u
a
l
i
t
y
i
n
d
i
c
a
t
o
r
s
f
o
r
t
h
e
d
i
f
f
e
r
e
n
t
a
l
i
g
n
m
e
n
t
s
,
w
h
e
r
e
2
?
4
,
H
M
M
,
a
n
d
f
a
a
r
e
a
l
i
g
n
m
e
n
t
m
o
d
e
l
s
,
a
n
d
s
y
m
m
e
t
r
i
z
a
t
i
o
n
s
t
r
a
t
e
g
i
e
s
r
e
f
e
r
t
o
T
a
b
l
e
1
279
Section 3.2) of 2M sentences during alignment.
For symmetrization we used all methods in Table
1, as implemented in the Moses toolkit (Koehn et
al., 2007) and in fast align (Dyer et al., 2013).
Based on the automatically aligned gold stan-
dard, we calculated all alignment indicators for all
settings. The complete results can be found in
Table 2, where we have ordered the symmetriza-
tion methods with the most sparse, intersection, on
top. Overall we can see that while several of the
alignment methods create a much higher number
of alignment links than the gold standard, they do
not produce many more translation units. This is
very interesting and indicates why link level statis-
tics may not be accurate enough to predict the per-
formance of certain downstream applications. As
expected, the metric scores for translation units
are lower than for link level metrics. This is
partly due to the fact that these measures do not
count any partially correct links; the MWU met-
rics which considers partial matches often have
higher scores than link level metrics. Another
finding is that the number of crossings vary a lot
with more than twice as many as the reference for
model2+union, and less than three times as many
for HMM+intersection. The HMM and fa models
have fewer reorderings than the IBM models.
We are now interested in the relation between
alignment evaluation on the link level and on the
translation unit level, which has not been thor-
oughly investigated before. Table 3 shows the cor-
relations between the various metrics. Both preci-
sion and F-measure at the link level have signifi-
cant correlations to all TU metrics. Link level re-
call, on the other hand, is significantly negatively
correlated with TU precision, but not significantly
correlated to any other TU metric, not even TU re-
call. Link level precision is thus highly important
for matching translation units. We can also note
here that while there is a trade-off between preci-
sion and recall on link level, this is not the case for
translation units, which can have both high pre-
cision and high recall. The same is not true for
MWU, that allows partial matching, where we also
see at least some precision/recall trade-off.
3.2 SMT Experiments
For reference, we first study the impact of align-
ment on SMT performance. Our SMT system
is a standard PBSMT system trained on WMT13
Translation unit
Link level ? P R F
P .95 .77 .90
R ?.57 ?.22 ?.42
F .70 .90 .83
Table 3: Pearson correlations between gold stan-
dard word alignment evaluation on the link level
and on translation unit level. Significant correla-
tions are marked with bold (< 0.01).
data.
2
We trained a German?English system on
2M sentences from Europarl and News Commen-
tary. We used the target side of the parallel corpus
and the SRILM toolkit (Stolcke, 2002) to train a 5-
gram language model. For training the translation
model and for decoding we used the Moses toolkit
(Koehn et al., 2007). We applied a standard feature
set consisting of a language model feature, four
translation model features, word penalty, phrase
penalty, and distortion cost. For tuning we used
minimum error-rate training (Och, 2003). In or-
der to minimize the risk of tuning influencing the
results, we used a fixed set of weights for each
experiment, tuned on a model 4+gdfa alignment.
3
For tuning we used newstest2009 with 2525 sen-
tences, and for testing we used newstest2013 with
3000 sentences. Evaluation was performed using
the Bleu metric (Papineni et al., 2002). The same
system setup was used for the SMT systems with
reordering.
Table 4 shows the results on the SMT task.
Model 3 and 4 with gd/gdfa symmetrization yield
the highest scores. There is a larger difference be-
tween systems with different symmetrization than
between systems with different alignment models.
The sparse intersection symmetrization gives the
poorest results. The top row in Table 5 shows
correlations between Bleu and all word alignment
quality indicators. There are significant correla-
tions with link level recall. A weighted link level
F-measure with ? = 0.3 gives a significant corre-
lation of .72, which confirms the results of Fraser
and Marcu (2007). There are no significant corre-
lations with the TU metrics but a positive correla-
tion with the number of TUs. For the MWU met-
rics the correlations are similar to the link level,
2
http://www.statmt.org/wmt13/
translation-task.html
3
This could have disfavored the other alignments, so we
also performed control experiments where we ran separate
tunings for each alignment. While the absolute results varied
somewhat, the correlations with alignment indicators were
stable.
280
m2 m3 m4 HMM fa
inter 18.1 19.1 19.3 18.8 18.9
gd 20.4 20.9 20.9 20.5 20.6
gdfa 20.4 20.7 20.8 20.5 20.5
gdf 19.4 19.7 20.1 19.9 20.0
union 19.2 19.6 19.8 19.7 20.0
Table 4: Baseline Bleu scores for different sym-
metrization heuristics
suggesting that they measure similar things. Intu-
itively it seems important for SMT to match full
translation units, but it might be the case that the
phrase extraction strategy is robust as long as there
are partial matches. There are no significant cor-
relations with link degree or link crossings, ex-
cept a negative correlation with Crossdiff, which
means that it is good to have a similar number of
crossings as the baseline. These results confirm
results from previous studies that link level mea-
sures, especially recall and weighted F-measure
show some correlation with SMT quality whereas
precision does not.
4 Reordering Tasks for SMT
Reordering is an important part of any SMT sys-
tem. One way to address it is to add reorder-
ing models to standard PBSMT systems, for in-
stance lexicalized reordering models (Koehn et al.,
2005), or to directly model reordering in hierarchi-
cal (Chiang, 2007) or syntactic translation models
(Yamada and Knight, 2002). Another type of ap-
proach is preordering, where the source side is re-
ordered to mimic the target side before translation.
There have also been approaches where reordering
is modeled as part of the evaluation of MT systems
(Birch and Osborne, 2011).
We can distinguish two main types of ap-
proaches to preordering in SMT, either by using
hand-written rules, which often operate on syn-
tactic trees (Collins et al., 2005), or by reordering
rules that are learnt automatically based on a word
aligned corpus (Xia and McCord, 2004). The lat-
ter approach is of interest to us, since it is based
on word alignments.
There has been much work on automatic learn-
ing of reordering rules, which can be based on dif-
ferent levels of annotation, such as part-of-speech
tags (Rottmann and Vogel, 2007; Niehues and
Kolss, 2009; Genzel, 2010), chunks (Zhang et
al., 2007) or parse trees (Xia and McCord, 2004).
In general, all these approaches lead to improve-
ments of translation quality. The reordering is
always applied on the translation input. It can
also be applied on the source side of the train-
ing corpora, which sometimes improves the results
(Rottmann and Vogel, 2007), but sometimes does
not make a difference (Stymne, 2012). When pre-
ordering is performed on the translation input, it
can be presented to the decoder as a 1-best reorder-
ing (Xia and McCord, 2004), as an n-best list (Li
et al., 2007), or as a lattice of possible reorderings
(Rottmann and Vogel, 2007; Zhang et al., 2007).
In the preordering studies cited above it is often
not even stated which alignment model was used.
A few authors mention the alignment tool that has
been applied but no comparison between different
alignment models is performed in any of the pa-
pers we are aware of. Li et al. (2007), for exam-
ple, simply state that they used GIZA++ and gdf
symmetrization and that they removed less proba-
ble multi links. Lerner and Petrov (2013) use the
intersection of HMM alignments and claims that
model 4 did not add much value. Genzel (2010)
did mention that using a standard model 4 was
not successful for his rule learning approach. In-
stead he used filtered model-1-alignments, which
he claims was more successful. However, there
are no further analyses or comparisons between
the alignments reported in any of these papers.
Another type of approach to reordering is to
only reorder the data in order to improve word
alignments, and to restore the original word or-
der before training the SMT system. This type
of approach has the advantage that no modifica-
tions are needed for the translation input. This ap-
proach has also been used both with hand-written
rules (Carpuat et al., 2010; Stymne et al., 2010)
and with rules based on initial word alignments on
non-reordered texts (Holmqvist et al., 2009). For
the latter approach a small study of the effect of gd
and gdfa symmetrizations was presented, which
only showed small variations in quality scores
(Holmqvist et al., 2012).
Below we present the two tasks that we study
in this paper: part-of-speech-based reordering for
creating input lattices for SMT and alignment-
based reordering for improving phrase-tables. We
evaluate the performance of these tasks in rela-
tion to the use of different alignment models and
symmetrization heuristics. For these tasks we are
mainly interested in the full translation task, for
which we report Bleu scores. In addition we also
show fuzzy reordering score (FRS), which focuses
281
Alignment links Translation units MWU
Total P R F Total P R F P R F
SMT, Bleu .33 ?.25 .56 .46 .65 ?.20 .16 ?.02 ?.29 .59 .44
POSReo, FRS ?.80 .87 ?.49 .75 ?.23 .90 .81 .89 .82 ?.45 .22
POSReo, Bleu ?.64 .74 ?.27 .85 .05 .80 .80 .86 .67 ?.23 .35
AlignReo, FRS ?.77 .88 ?.43 .84 ?.11 .90 .88 .92 .81 ?.37 .31
AlignReo, Bleu ?.81 .83 ?.58 .61 ?.24 .75 .64 .72 .71 ?.53 .04
Link degree Link crossings
1-1 null multi Total SKTD P R F Crossdiff
SMT, Bleu .33 ?.30 .21 ?.05 ?.14 ?.09 .25 .07 ?.63
POSReo, FRS ?.41 .84 ?.89 ?.81 ?.70 .90 .21 .86 ?.41
POSReo, Bleu ?.17 .66 ?.80 ?.71 ?.60 .79 .42 .89 ?.49
AlignReo, FRS ?.32 .77 ?.86 ?.80 ?.73 .94 .27 .92 ?.38
AlignReo, Bleu ?.57 .83 ?.79 ?.93 ?.91 .86 ?.07 .69 ?.52
Table 5: Pearson correlations between different alignment characteristics and scores for the translation
and reordering tasks. Significant correlations are marked with bold (< 0.01).
only on the reordering component (Talbot et al.,
2011). It compares a system reordering to a refer-
ence reordering, by measuring how many chunks
that have to be moved to get an identical word or-
der, see Eq. 9, where C is the number of con-
tiguously aligned chunks, and M the number of
words. To find the reference ordering we apply
the method of Holmqvist et al. (2009), described
in Section 4.2, to the gold standard alignment.
FRS = 1?
C ? 1
M ? 1
(9)
4.1 Part-of-Speech-Based Reordering
Our first reordering task is a part-of-speech-based
preordering method described by Rottmann and
Vogel (2007) and Niehues and Kolss (2009),
which was successfully used for German?English
translation. Rules are learnt from a word aligned
POS-tagged corpus. Based on the alignments, tag
patterns are identified that give rise to specific re-
orderings. These patterns are then scored based
on relative frequency.
4
The rules are then applied
to the translation input to create a reordering lat-
tice, with normalized edge scores based on rule
scores. In our experiments we only use rules with
a score higher than 0.2, to limit the size of the lat-
tices. For calculating FRS, we pick the highest
scoring 1-best word order from the lattices.
We learn rules from our entire SMT training
corpus varying alignment models and symmetriza-
tion. To investigate only the effect of word align-
ment for creating reordering rules, we do not
4
Note that we do not use words (Rottmann and Vogel,
2007) or wild cards (Niehues and Kolss, 2009) in our rules.
m2 m3 m4 HMM fa
inter .577 .575 .581 .596 .567
gd .555 .559 .570 .589 .546
gdfa .540 .540 .559 .579 .539
gdf .439 .499 .542 .560 .495
union .442 .492 .544 .563 .486
Table 6: Fuzzy reordering scores for part-of-
speech-based reordering for different alignments
m2 m3 m4 HMM fa
inter 21.4 21.6 21.8 21.6 21.6
gd 21.5 21.6 21.6 21.7 21.5
gdfa 21.4 21.5 21.7 21.7 21.4
gdf 20.3 21.0 21.4 21.5 21.0
union 20.3 21.5 21.6 21.5 20.8
Table 7: Bleu scores for part-of-speech-based re-
ordering for different alignments
change the SMT system, which is trained based
on model 4+gdfa alignments. The only thing that
varies for the translation task is thus the input lat-
tice given to this SMT system.
The results are shown in Tables 6 and 7. Most
Bleu scores are better than using the same SMT
system without preordering, with a Bleu score of
20.8. The results on FRS and Bleu are highly cor-
related at .94, despite the fact that we use a lattice
as SMT input, and the 1-best order for FRS. For
both metrics sparse symmetrization like intersec-
tion and gd performs best. Model 4 and HMM
perform best with similar Bleu scores, but FRS is
better for the HMM model.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled POSReo.
There are strong correlations with all TU metrics,
contrary to the SMT task. There are also signifi-
cant correlations with link level precision and bal-
282
anced F-measure. The correlation with weighted
link level F-measure is even higher, .91 for ? =
0.6. This is an indication that this algorithm is
more sensitive to precision than the SMT task. As
for the SMT task, the correlation patterns are simi-
lar for the MWU metrics as for link level. For link
degree, null alignments are correlated, but there is
a negative correlation for multi links. The correla-
tions with the number of crossings and SKTD are
negative, which means that it is better to have a
low number of crossings. This may seem counter-
intuitive, but note in Table 1 that many alignments
have a much higher number of crossings than the
baseline. The precision of the crossing links is
highly correlated with performance on this task,
while the recall is not. This tells us that it is impor-
tant that the crossings we find in the alignment are
good, but that it is less important that we find all
crossings. This makes sense since the rule learner
can then learn at least a subset of all existing cross-
ings well.
4.2 Reordering for Alignment
In our second reordering task we investigate
alignment-based reordering for improving phrase-
tables (Holmqvist et al., 2009; Holmqvist et al.,
2012). This strategy first performs a word align-
ment, based on which the source text is reordered
to remove all crossings. A second alignment is
trained on the reordered data, which is then re-
stored to the original order before training the
full SMT system. In Holmqvist et al. (2012) it
was shown that this strategy leads to improve-
ments in link level recall and F-measure as well
as small translation improvements for English?
Swedish. It also led to small improvements for
German?English translation.
Similar to the previous experiments, we now
vary alignment models and symmetrization that
are used for reordering during the first step. The
second step is kept the same using model 4+gdfa
in order to focus on the reordering step in our com-
parisons. Tables 8 and 9 show the results of these
experiments. In this case the reordering strat-
egy was not successful, always producing lower
Bleu scores than the baseline of 20.8. However,
there are some interesting differences in these out-
comes. On this task as well, FRS and Bleu scores
are highly correlated at .89, which was expected,
since this method directly uses the reordered data
to train phrase tables. For the best systems, the
m2 m3 m4 HMM fa
inter .583 .604 .669 .654 .598
gd .548 .583 .646 .642 .561
gdfa .532 .564 .633 .645 .553
gdf .422 .482 .571 .574 .474
union .395 .455 .552 .545 .452
Table 8: Fuzzy reordering scores for alignment-
based reordering for different alignments
m2 m3 m4 HMM fa
inter 19.5 19.5 19.9 20.2 19.4
gd 19.3 19.5 19.8 20.2 19.3
gdfa 19.1 19.2 19.6 20.0 19.2
gdf 18.3 18.2 18.6 19.0 18.9
union 17.4 17.8 18.4 18.8 18.8
Table 9: Bleu scores for alignment-based reorder-
ing for different alignments
FRS scores are higher than for the previous task,
see Table 6, which shows that reordering directly
based on alignments is easier than learning and ap-
plying rules based on them, given suitable align-
ments. On this task, again, the sparser alignments
are the most successful on both tasks. Here, how-
ever, the HMM model gives the best Bleu scores,
and similar FRS scores to model 4.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled Align-
Reo. The correlation patterns are very similar
to the previous task. A few more indicators are
significantly negatively correlated with alignment-
based reordering than with the other reordering
tasks and metrics. The performance on our two
reordering tasks are significantly correlated at .76.
Again alignments with good scores on TU met-
rics, link level precision and crossing link preci-
sion are preferable. For this task, the best correla-
tion with weighted link level F-measure is .86 for
? = 0.8. Again, we thus see that sparse align-
ments with high precision on all measures includ-
ing the crossing subset, are important.
5 Small Training Data
Since previous work has suggested that training
data size influences the relation between align-
ment and SMT quality for small and large training
data (Lambert et al., 2012), we investigated this is-
sue also for our reordering tasks. We repeated all
our experiments on a small dataset, only the News
Commentary data from WMT13, with 170K sen-
tences. Due to space constraints we cannot show
all results in the paper, but the main findings are
283
summarized in this section.
To acquire alignment results we realigned the
gold standard concatenated with the smaller data,
to reflect the actual quality of alignment with a
small dataset. As expected the quality scores tend
to be lower with less data. Overall the same sys-
tems tend to perform good on each metric with the
small and large data, even though there is some
variation in the ranking between systems. On the
SMT task as well, the Bleu scores are lower, as
expected. In this case fast align is doing best fol-
lowed by model 4 and 3. The best symmetrization
is again gd and gdfa. There are also some differ-
ences in the correlation profile. Link recall and
number of translation units are no longer signifi-
cantly correlated, whereas the number of crossings
and SKTD are. The highest correlation for link
level F-measure is .60 for balanced F-measure,
showing that precision is equally important to re-
call with less data.
For the reordering tasks the scores are again
lower. The POS-based reorderings again help over
the baseline SMT, whereas the alignment-based
reordering leads to slightly lower scores. The cor-
relation profile look exactly the same for Bleu
for POS-based reordering. FRS for both tasks
and Bleu for alignment-based reordering have the
same correlation profiles as Bleu for alignment-
based reordering on large data. There are thus
very small differences in the word alignment qual-
ity indicators that are relevant with large and small
training data, while there are some differences on
the SMT task. For weighted link level F-measure,
the highest correlations are found with ? = 0.6?
0.7 on the different metrics, again showing that
precision is more important than recall. For FRS
on both tasks and Bleu for alignment-based re-
ordering, model4 and HMM with intersection and
gd still perform best. For Bleu for POS-based re-
ordering, gdfa and model 3 also give good results.
6 Conclusion and Future Work
We have shown that the best combination of align-
ment and symmetrization models for SMT are not
the best models for reordering tasks in our ex-
perimental setting. For SMT, high recall is more
important than precision with large training data,
while precision and recall are of equal impor-
tance with small training data. This finding sup-
ports previous research (Fraser and Marcu, 2007;
Lambert et al., 2012). Translation unit metrics
are not predictive of SMT performance. For the
large data condition model 3 and 4 with gd and
gdfa symmetrization gave the best results, whereas
fast align with gd and gdfa was best with small
training data.
For the two preordering tasks we investigated,
however, link level weighted F-measure that gave
more weight to precision was important, as well as
all TU metrics. It was also important to have high
precision for the crossing subset of TUs. Hence,
it is more important to reliably find some cross-
ings than to find all crossings. This make sense
since the extracted rules or performed reorderings
are likely good in such cases, even if we are not
able to find all possible reorderings. In conclu-
sion, based on this study, we recommend intersec-
tion symmetrization with model 4 and HMM for
SMT reordering tasks.
We have studied two relatively different re-
ordering tasks with two training data sizes, but
found that they to a large extent prefer the same
types of alignments. Moreover, the results on
these two reordering tasks correlates strongly with
FRS, which is much cheaper to calculate than
SMT metrics that may even require retraining of
full SMT systems. This is consistent with Tal-
bot et al. (2011) who suggested FRS for preorder-
ing tasks. We thus would encourage developers
of alignment methods to not only give results for
SMT, but also for FRS, as a proxy for reordering
tasks. Furthermore, it is also useful to give results
on TU metrics in addition to link level metrics to
complement the evaluation.
In this paper, we have looked at existing genera-
tive alignment and symmetrization models. In fu-
ture work, we would also like to investigate other
models, including the removal of low-confidence
links, which has previously been proposed for pre-
reordering (Li et al., 2007; Genzel, 2010). Given
the results, it also seems motivated to develop
or adapt the existing models in general, to bet-
ter fit the properties of specific auxiliary tasks.
Furthermore, we need to validate our findings on
other language pairs, especially for non-related
languages with even more diverse word order.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
284
References
Lars Ahrenberg, Magnus Merkel, Anna S?agvall Hein,
and J?org Tiedemann. 2000. Evaluation of word
alignment systems. In Proceedings of LREC, vol-
ume III, pages 1255?1261, Athens, Greece.
Lars Ahrenberg. 2010. Alignment-based profiling of
Europarl data in an English-Swedish parallel corpus.
In Proceedings of LREC, pages 3398?3404, Valetta,
Malta.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
Coling and ACL, pages 9?16, Sydney, Australia.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing metrics for MT. In Proceedings of ACL, pages
1027?1035, Portland, Oregon, USA.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of ACL, Short Papers,
pages 178?183, Uppsala, Sweden.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):202?228.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL,
pages 644?648, Atlanta, Georgia, USA.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of WMT, pages 44?52, Columbus, Ohio.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL, pages 273?280, Boston,
Massachusetts, USA.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proceedings of ACL, pages 986?993, Columbus,
Ohio, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of Coling, pages 376?384,
Beijing, China.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Pro-
ceedings of ACL, pages 502?509, Barcelona, Spain.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of WMT, pages 120?124, Athens,
Greece.
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and
Magnus Merkel. 2012. Alignment-based reordering
for SMT. In Proceedings of LREC, Istanbul, Turkey.
Marcin Junczys-Dowmunt and Arkadiusz Sza?. 2012.
SyMGiza++: Symmetrized word alignment models
for statistical machine translation. In International
Joint Conference of Security and Intelligent Infor-
mation Systems, pages 379?390, Warsaw, Poland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48?54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177?180, Prague, Czech Republic.
Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and
Andy Way. 2012. What types of word alignment
improve statistical machine translation? Machine
Translation, 26(4):289?323.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP, pages 513?523, Seattle, Wash-
ington, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the ACL, pages 720?727, Prague, Czech
Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL,
pages 104?111, New York City, New York, USA.
285
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
ACL, pages 459?466, Ann Arbor, Michigan, USA.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Confer-
ence on Computational Linguistics, pages 349?354,
Tartu, Estonia.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
HLT and EMNLP, pages 81?88, Vancouver, British
Columbia, Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of WMT, pages 206?214, Athens, Greece.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of Coling, pages 1086?
1090, Saarbr?ucken, Germany.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of the
Joint Conference of EMNLP and Very Large Cor-
pora, pages 20?28, College Park, Maryland, USA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Sebastian Pado and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information.
In Proceedings of HLT and EMNLP, pages 859?866,
Vancouver, British Columbia, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, Pennsylvania,
USA.
Oana Postolache, Dan Cristea, and Constantin Or?asan.
a. 2006. Transferring coreference chains through
word alignment. In Proceedings of LREC, pages
889?892, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Anders S?gaard and Jonas Kuhn. 2009. Empirical
lower bounds on alignment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation, pages 19?27, Boulder, Colorado,
USA.
Anders S?gaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceedings
of 11th International Conference on Parsing Tech-
nologies, pages 33?36, Paris, France.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of
WMT and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of ROBUS-UNSUP 2012: Joint Workshop
on Unsupervised and Semi-Supervised Learning in
NLP, pages 28?34, Avignon, France.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL, pages 477?487, Montr?eal, Quebec, Canada.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A lightweight evaluation framework for machine
translation reordering. In Proceedings of WMT,
pages 12?21, Edinburgh, Scotland.
Ben Taskar, Lacoste-Julien Simon, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT and EMNLP,
pages 73?80, Vancouver, British Columbia, Canada.
J?org Tiedemann. 2005. Optimisation of word
alignment clues. Natural Language Engineering,
11(03):279?293. Special Issue on Parallel Texts.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical
translation. In Proceedings of Coling, pages 836?
841, Copenhagen, Denmark.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling, pages
508?514, Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL,
pages 303?310, Philadelphia, Pennsylvania, USA.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology, pages 1?8, San
Diego, California, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 21?28, Trento, Italy.
286
