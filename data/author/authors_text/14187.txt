Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 139?147,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Collecting Image Annotations Using Amazon?s Mechanical Turk
Cyrus Rashtchian Peter Young Micah Hodosh Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
201 North Goodwin Ave, Urbana, IL 61801-2302
{crashtc2, pyoung2, mhodosh2, juliahmr}@illinois.edu
Abstract
Crowd-sourcing approaches such as Ama-
zon?s Mechanical Turk (MTurk) make it pos-
sible to annotate or collect large amounts of
linguistic data at a relatively low cost and high
speed. However, MTurk offers only limited
control over who is allowed to particpate in
a particular task. This is particularly prob-
lematic for tasks requiring free-form text en-
try. Unlike multiple-choice tasks there is no
correct answer, and therefore control items
for which the correct answer is known can-
not be used. Furthermore, MTurk has no ef-
fective built-in mechanism to guarantee work-
ers are proficient English writers. We de-
scribe our experience in creating corpora of
images annotated with multiple one-sentence
descriptions on MTurk and explore the effec-
tiveness of different quality control strategies
for collecting linguistic data using Mechani-
cal MTurk. We find that the use of a qualifi-
cation test provides the highest improvement
of quality, whereas refining the annotations
through follow-up tasks works rather poorly.
Using our best setup, we construct two image
corpora, totaling more than 40,000 descriptive
captions for 9000 images.
1 Introduction
Although many generic NLP applications can be de-
veloped by using existing corpora or text collections
as test and training data, there are many areas where
NLP could be useful if there was a suitable corpus
available. For example, computer vision researchers
are becoming interested in developing methods that
can predict not just the presence and location of cer-
tain objects in an image, but also the relations be-
tween objects, their attributes, or the actions and
events they participate in. Such information can
neither be obtained from standard computer vision
data sets such as the COREL collection nor from
the user-provided keyword tag annotations or cap-
tions on photo-sharing sites such as Flickr. Simi-
larly, although the text near an image on a website
may provide cues about the entities depicted in the
image, an explicit description of the image content
itself is typically only provided if it is not immedi-
ately obvious to a human what is depicted (in which
case we may not expect a computer vision system
to be able to recognize the image content either).
We therefore set out to collect a corpus of images
annotated with simple full-sentence descriptions of
their content. To obtain these descriptions, we used
Amazon?s Mechanical Turk (MTurk).1 MTurk is
an online framework that allows researchers to post
annotation tasks, called HITs (?Human Intelligence
Task?), then, for a small fee, be completed by thou-
sands of anonymous non-expert users (Turkers). Al-
though MTurk has been used for a variety of tasks in
NLP, our use of MTurk differs from other research
in NLP that uses MTurk mostly for annotation of
existing text. Similar to crowdsourcing-based an-
notation, quality control is an essential component
of crowdsourcing-based data collection efforts, and
needs to be factored into the overall costs. For us,
the quality of the text produced by the Turkers is
particularly important since we are interested in us-
1All of our experiments on Mechanical Turk were adminis-
tered and paid for through the services offered by Dolores Labs.
139
ing this corpus for future research at the intersection
of computer vision and natural language processing.
However, MTurk provides limited ways to imple-
ment such quality control directly. For example, our
initial experiments yielded a data set that contained
many sentences that were clearly not written by na-
tive speakers. We learned that several steps must be
taken to ensure that Turkers both understand the task
and produce quality data.
This paper describes our experiences with Turk
(based on data collection efforts in spring and sum-
mer 2009), comparing two different approaches to
quality control. Although we did not set out to run a
scientific experiment comparing different strategies
of how to collect linguistic data on Turk, our expe-
rience points towards certain recommendations for
how to collect linguistic data on Turk.
2 The core task: image annotation
The PASCAL Data Set Every year, the Pat-
tern Analysis, Statistical Modeling, and Computa-
tional Learning (PASCAL) organization hosts the
Visual Object Classes Challenge (Everingham et al,
2008). This is a competition similar to the shared
tasks familiar to the ACL community, where a com-
mon data set of images with classification and de-
tection information is released, and computer vision
researchers compete to create the best classification,
detection, and segmentation systems. We chose to
use this collection of images because it is a standard
resource for computer vision, and will therefore fa-
ciliate further research.
The VOC2008 development and training set con-
tains around 6000 images. It is categorized by ob-
jects that appear in the image, with some images ap-
pearing in multiple categories.2. The images con-
tain a wide variety of actions and scenery. Our cor-
pus consists of 1000 of these images, fifty randomly
chosen from each of the twenty categories.
MTurk setup We asked Turkers to write one de-
scriptive sentence for each of ten images. An ex-
ample annotation screen is shown in Figure 1. We
2The twenty categories include people, various animals,
vehicles and other objects: person, bird, cat, cow,
dog, horse, sheep, aeroplane, bicycle,
boat, bus, car, motorbike, train, bottle,
chair, dining table, potted plant, sofa,
tv/monitor
Figure 1: Screenshot of the image annotation task.
first showed the Turkers a list of instructive guide-
lines describing the task (Figure 6). The instruc-
tions told them to write ten complete but simple sen-
tences, to include adjectives if possible, to describe
the main characters, the setting, or the relation of
the objects in the image, to pay attention to gram-
mar and spelling, and to try to be concise. These
instructions were meant to both explain the task and
to prepare Turkers to write quality sentences. We
then showed each Turker a set of ten images, chosen
randomly from the 1000 total images, and displayed
one at a time. The Turkers navigated using ?Next?
buttons through the ten annotation screens, each dis-
playing one image and one text-box. We allowed
Turkers ten minutes to complete one task.3 We re-
stricted the task to Turkers who have previously had
at least 95% of their results approved. We paid $0.10
to complete one task. The total cost for all 5000 de-
scriptions was $50 (plus Amazon?s 10% fee).
2.1 Results
On average, Turkers wrote the ten sentences in a to-
tal of four minutes. The average pay rate was $1.30
per hour, and the whole experiment finished in under
two days. Five different people described each im-
age, and in the end, most of the Turkers completed
the task successfully, although 2.5% of the 5000 sen-
tences were empty strings. Turkers varied in the time
they took to complete the experiment, in the length
of their sentences, and in the level of detail they in-
cluded about the image. An example captioned im-
age is shown in Figure 2.
Problems with the data The quality of descrip-
tions varied greatly. We were hoping to collect sim-
ple sentences, written in correct English, describ-
ing the entities and actions in the images. More-
3This proved to be more than enough time for the task.
140
Figure 2: An image along with the five captions that were written by Turkers.
over, these are explicitly the types of descriptions we
asked for in the MTurk task instructions. Although
we found the descriptions acceptable more than half
of the time, a large number of the remaining descrip-
tions had at least one of the following two problems:
1. Some descriptions did not mention the salient
entities in the image, some were simply noun
phrases (or less), and some were humorous or
speculative.4 We find all of these to be prob-
lems because future computer vision and nat-
ural language processing research will require
accurate and consistent image captions.
2. A number of Turkers were not sufficiently pro-
ficient in English. Many descriptions contained
grammar and spelling errors, and some in-
cluded very awkward constructions. For exam-
ple, the phrase ?X giving pose? showed up sev-
eral times in descriptions of images containing
people (e.g. ?The lady and man giving pose.?).
Such spelling and grammar errors will pose dif-
ficulties for any standard text-processing algo-
rithms trained on native English.
Spell checking Due to the large number of mis-
spellings in in the initial data set, we first ran the sen-
tences first through our spell checker before putting
them up on Turk to assess their quality. We tok-
enized the captions with OpenNLP, and first checked
a manually created list of spelling corrections for
each token. These included canonicalizations (cor-
recting ?surf board? as ?surfboard?), words our au-
tomatic spell checker did not recognize (?mown?),
and the most common misspellings in our data set
4For example, some Turkers commented on the feelings of
animals (e.g. ?the dog is not very happy next to the dumpster?),
and others made jokes about the content of the image (e.g. ?The
goat is ready for hair cut?)
(?shepard? to ?shepherd?). If the token was not in
our manual list, we passed the word to aspell. From
aspell?s candidate corrections, we selected the most
frequent word that appeared either in other captions
of the same image, of images of the same topic, or
any caption in our data set.
3 Post-hoc quality control
Because our initial data collection efforts resulted in
relatively noisy data, we created a new set of MTurk
tasks designed to provide post-hoc quality control.
Our aim was to filter out captions containing mis-
spellings and incorrect grammar.
MTurk setup Each HIT consisted of fifty differ-
ent image descriptions and asked Turkers to decide
for each of them whether they contained correct
grammar and spelling or not. At the beginning of
each HIT, we included a brief training phase, where
we showed the Turkers five example descriptions la-
beled as ?correct? or ?incorrect? (Figure 7). In the
HIT itself, the fifty descriptions were displayed in
blocks of five (albeit not for the same image) , and
each description was followed by two radio buttons
labeled ?correct? and ?incorrect?. We did not show
the corresponding images. A screenshot is shown in
Figure 3. Each block of five captions contained one
control item that we use for later assessment of the
Turkers? spell-checking ability. We wrote these con-
trol captions ourselves, modeling them after actual
image descriptions. We paid $0.08 for one task, and
three people completed each task.
3.1 Results
On average, Turkers completed a HIT (judging fifty
sentences) in four minutes, at an average hourly rate
of $1.04. Each sentence in our data set was judged
by three Turkers. The whole experiment finished
141
Figure 3: Screenshot from the grammar/spelling checking task. This is a block of five sentences that Turkers had
to label as using correct or incorrect grammar and spelling. The first sentence is a control item that we included to
monitor the Turkers? performance, and the other four are captions generated by other Turkers in a previous task.
Data set Quality control % Votes for ?correct English?
produced by... performed by... 0 1 2 3
Unqualified writers three Turkers 18.9% 31.2% 26.4% 23.5%
Unqualified writers three experts 11.8% 12.7% 15.3% 60.2%
Qualified writers three experts 0.5% 2.5% 15.0% 82.0%
Table 1: Quality control by Turkers and Experts. The three experts judged 600 sentences from each data set. 565
sentences produced by unqualified workers were also judged by three Turkers.
in under two days, at a total cost of $28.80 (plus
Amazon?s 10% fee). We also selected randomly
600 spell-checked sentences for expert annotation.
Three members of our team (all native speakers of
English) judged each of these sentences in the same
manner as the Turkers. Each sentence could there-
fore get between 0 and 3 Turker votes and between
0 and 3 expert votes for good English. The top two
rows of Table 1 show the distribution of votes in
each of the two groups. We also assess whether the
judgments of the Turkers correlate with our own ex-
pert judgments. Table 2(a) shows the overall agree-
ment between Turkers and expert annotators. The
rest of Table 2 shows how performance of the Turk-
ers on the control items affected agreement with ex-
pert judgments. We define the performance of a
Turker in terms of the average the number of con-
trol items that they got right in each HIT they took.
For each threshold in Tables 2(a)-(d), we considered
only those images for which we have three quality
judgments by workers whose performance is above
the specified threshold.
Our results show that the effectiveness of using
Turkers to filter for grammar and spelling issues is
limited. Overall, the Turker judgments were overly
harsh. The majority Turker vote agrees with the ma-
jority vote of the trained annotators on only 65.1%
of the sentences. Manual inspection of the differ-
ences reveals that the Turkers marked many per-
fectly grammatical English sentences as incorrect
(although they also marked a few which we had
missed). Agreement with experts decreases among
those Turkers that performed better on the control
sentences, with only 56.7% agreement for Turkers
that got all the controls right. In addition, the Turk-
ers are significantly more likely to report false nega-
tives over false positives and this also increases with
performance on the control sentences. (Overall, the
Turkers marked 29.9% of the sentences as false neg-
atives, whereas the Turkers that scored perfectly on
the controls marked 39.3% as false negatives.) Ex-
amination of the areas of high disagreement reveal
that the Turkers were much more likely to vote down
noun phrases than the experts were. The correct ex-
ample captions provided in the instructions of the
quality control test were complete sentences. Some
of the control captions were noun phrases, but all
of the noun phrase controls had some other error
in them. Thus it was possible to either believe that
noun phrases were correct or incorrect, and still be
consistent with the provided examples, and provide
correct judgments on the control sentences.
142
(a) ? 0 controls correct: 565 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.4% 3.7% 3.9%
1 3.2% 5.7% 5.0% 17.3%
2 1.8% 2.8% 3.5% 18.2%
3 0.0% 0.4% 2.5% 20.7%
(b) ? 5 controls correct: 553 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.5% 3.8% 4.0%
1 3.1% 5.4% 5.1% 17.5%
2 1.8% 2.7% 3.6% 18.4%
3 0.0% 0.4% 2.5% 20.3%
(c) ? 7 controls correct: 331 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 6.3% 3.9% 5.1%
1 3.0% 4.5% 5.1% 24.5%
2 1.8% 1.8% 2.4% 15.1%
3 0.0% 0.0% 2.1% 17.2%
(d) ? 9 controls correct: 127 sentences
Turk Expert votes
votes 0 1 2 3
0 7.9% 6.3% 3.1% 6.3%
1 1.6% 4.7% 6.3% 23.6%
2 0.8% 3.1% 1.6% 15.7%
3 0.0% 0.0% 1.6% 17.3%
Table 2: Quality control: Agreement between Turker and Expert votes, depending on the average number of control
items the Turker voters got right.
4 Quality control through pre-screening
Quality control can also be imposed through a pre-
screening of the Turkers allowed to take the HIT. We
collected another set of five descriptions per image,
but restricted participation to Turkers residing in the
US5, and created a brief qualification test to check
their English. We would like to be able to restrict our
tasks to Turkers who are native speakers and com-
petent spellers and writers of English, regardless of
their country of residence. However, this seems to
be difficult to verify within the current MTurk setup.
Qualification Test Design The qualification test
consists of forty binary questions: fifteen testing
spelling, fifteen testing grammar, and ten testing the
ability to identify good image descriptions.
In all three cases, we started the section with a
set of instructions displaying examples of positive
and negative answers to the tasks. Each spelling
question consisted of a single sentence, and Turk-
ers were asked to determine if all of the words in
the sentence were spelled correctly and if the correct
word was being used (?lose? versus ?loose?). Each
grammar question consisted of a single sentence that
was either correct or included a grammatical error.
Both spelling and grammar checking questions were
based on common mistakes made by foreign English
5As of March 2010, 46.80% of Turkers reside in the U.S
(http://behind-the-enemy-lines.blogspot.
com/ 03/09/2010)
Figure 4: Average caption length (5000 images)
speakers and on grammatical or spelling errors that
occurred in our initial set of image captions. The
grammar and spelling questions are listed in Table
3. The image description questions consisted of one
image shown with two actual captions, and the Turk-
ers were asked which caption better described the
image. In order to pass the qualification test, we
required each annotator to correctly answer at least
twenty-four spelling and grammar questions and at
least eight image description questions. To prevent
Turkers from using the number of question they got
correct to do a brute force search for the correct an-
swers, we simply told them if they passed (?1?) or
failed (?0?). Currently, 1504 people have taken the
qualification test, with a 67.2% passing rate. Since
this qualification test was only required for our HITs
that were restricted to US residents, we assume (but
are not able to verify) that most, if not all, of the
people who took this test are actually US residents.
143
MTurk Set-up We use the same MTurk set-up as
before, but to encourage Turkers to complete the
task even though they first have to pass a qualifica-
tion test, we pay them $0.10 to annotate five images.
4.1 Results
We found that the Turkers who passed the qualifica-
tion provided much better captions for the images.
The average time spent on each image was longer
(four minutes per ten images for the non-qualified
workers versus five minutes per ten images for the
qualified workers). On average, qualified Turk-
ers produced slightly longer sentences (avg. 10.7
words) than non-qualified workers (avg. 10.0 words)
(Figure 4), and the awkward constructions produced
by the unqualified workers were mostly absent. The
entire corpus was annotated in 253 hours at a cost of
$100.00 (plus Amazon?s 10% fee).
We also looked at the rate of misspellings (ap-
proximated by how often our spell-checker indicated
a misspelling). Without the qualification test, Out
of the 600 sentences produced without the qualifica-
tion test, 78 contained misspellings, whereas only 25
sentences out of the 600 produced by the qualified
workers contained misspellings. Furthermore, mis-
spellings in the no-qualification group include many
genuine errors (?the boys are playing in tabel?,
?bycycles?, ?eatting?), whereas misspellings in the
qualification group are largely typos (e.g. Ywo for
Two, tableclothe, chari for chair). Furthermore, the
spell checker corrected all 25 misspellings in the
qualified data set to the intended word, but 27 out of
the 78 misspellings in the data produced by the un-
qualified workers got changed to some other word.
The same three members of our team rated again
the English of 600 randomly selected sentences writ-
ten by Turkers residing in the US who passed our
test. We found a significant improvement in quality
(Table 1, bottom row), with the majority expert vote
accepting over 97% of the sentences. This is also
corroborated by qualitative analysis of the data (see
Figure 5 for examples). Inspection reveals that sen-
tences that are deemed ungrammatical by the experts
typically contain some undetected typo, and would
be correct if these typos could be fixed. Without a
qualification test, there is a significantly greater per-
centage of nonsensical responses such as: ?Is this a
bird squirrel?? and ?thecentury?. In addition, gram-
matically correct but useless fragments such as ?very
dark? and ?peace? only appear without a test. After
requiring the qualification test, the major reasons for
rejection by Turkers are typos such as in ?The two
dogs blend in with the stuff animals? or missing de-
terminers such as in ?a train on tracks in town?.
Overall cost effectiveness Using the no qualifica-
tion test approach, we first paid $50.00 to get 5000
sentences written by unqualified Turkers (which re-
sulted in 4851 non-empty sentences). This resulted
in low-quality data which required further verifica-
tion. Since this is too time-consuming for expert an-
notators, we then paid another $28.80 to get each of
these sentences subsequently checked by three Turk-
ers for grammaticality, resulting in 2222 sentences
which received at least two positive votes for gram-
maticality. With the qualification test approach, we
paid $100.00 to get 5000 sentences written. Based
on our experiments on the set of 600 sentences, ex-
perts would judge over 97% of these sentences as
correct, thus obviating the immediate need for fur-
ther control. That is, it effectively costs more for
non-qualified Turkers to produce sentences that are
judged to be good than for qualified Turkers. Fur-
thermore, their sentences will probably be of lower
quality even after they have been judged acceptable.
5 A corpus of captions for Flickr photos
Encouraged by the success of the qualification test
approach, we extended our corpus to contain 8000
images collected from Flickr. We again paid the
Turkers $0.10 to annotate five images. Our data set
consists of 8108 hand-selected images from Flickr,
depicting actions and events (rather than images de-
picting scenery and mood). These images are more
likely to require full sentence descriptions than the
PASCAL images. We chose six large Flickr groups6
and downloaded a few thousand images from each,
giving us a total of 15,000 candidate images. We re-
moved all black and white or sepia images as well as
images containing photographer signatures or seals.
Next, we manually identified pictures that depicted
the actions of people or animals. For example, we
kept images of people walking in parks, but not of
6The groups: strangers!, Wild-Child (Kids in Action), Dogs
in Action (Read the Rules), Outdoor Activities, Action Photog-
raphy and Flickr-Social (two or more people in the photo)
144
Without qualification test
(1) lady with birds
(2) Some parrots are have speaking skill.
(3) A lady in their dining table with birds on her shoulder and head.
(4) Asian woman with two cockatiels, on shoulder
head, room with oak cabinets.,
(5) The lady loves the parrot
With qualification test
(1) A woman has a bird on her shoulder, and another bird on her head
(2) A woman with a bird on her head and a bird on her shoulder.
(3) A women sitting at a dining table with two small birds sitting on her.
(4) A young Asian woman sitting at a kitchen
table with a bird on her head and another on her shoulder.
(5) Two birds are perched on a woman sitting in a kitchen.
Figure 5: Comparison of captions written by Turkers with and without qualification test
empty parks; we kept several people posing, but not
a close-up of a single person.7 Each HIT asked
Turkers to describe five images. We required the
qualification test and US residency. Average com-
pletion time was a little above 3 minutes for 5 sen-
tences. The corpus was annotated in 284 hours8, at
a total cost of $812.00 (plus Amazon?s 10% fee).
6 Related work and conclusions
Related work MTurk has been used for many dif-
ferent NLP and vision tasks (Tietze et al, 2009;
Zaidan and Callison-Burch, 2009; Snow et al, 2008;
Sorokin and Forsyth, 2008). Due to the noise in-
herent in non-expert annotations, many other at-
tempts at quality control have been made. Kit-
tur et al (2008) solicit ratings about different as-
pects of Wikipedia articles. At first they receive
very noisy results, due to Turkers? not paying at-
tention when completing the task or specifically try-
ing to cheat the requester. They remade the task,
this time starting by asking the Turkers verifiable
questions, speculating that the users would produce
better quality responses when they suspect their an-
swers will be checked. They also added a question
that required the Turkers to comprehend the con-
tent of the Wikipedia article. With this new set-
up, they find that the quality greatly increases and
carelessness is reduced. Kaisser and Lowe (2008)
7Our final data set consists of 1482 pictures from action pho-
tography, 1904 from dogs, 776 from flickr-social, 916 from out-
door, 1257 from strangers and 1773 from wild-child.
8Note that the annotation process scaled pretty well, con-
sidering that annotating more than eight times the number of
images took only 31 hours longer.
collected question and answer pairs by presenting
Turkers with a question and telling them to copy and
paste from a document of text they know to contain
the answer. They achieve a good but far from per-
fect interannotator agreement based on the extracted
answers. We speculate that the quality would be
much worse if the Turkers wrote the sentences them-
selves. Callison-Burch (2009) asks Turkers to pro-
duce translations when given reference sentences in
other languages. Overall, he finds find that Turk-
ers produce better translations than machine transla-
tion systems. To eliminate translations from Turkers
who simply put the reference sentence into an online
translation website, he performs a follow-up task,
where he asks other Turkers to vote on if they believe
that sentences were generated using an online trans-
lation system. Mihalcea and Strapparava (2009) ask
Turkers to produce 4-5 sentence opinion paragraphs
about the death penalty, about abortion and describ-
ing a friend. They report that aside from a small
number of invalid responses, all of the paragraphs
were of good quality and followed their instructions.
Their success is surprising to us because they do not
report using a qualification test, and when we did
this our responses contained a large amount of in-
correct English spelling and grammar.
The TurKit toolkit (Little et al, 2009) provides
another approach to improving the quality of MTurk
annotations. Their iterative framework allows the
requester to set up a series of tasks that first solic-
its text annotations from Turkers and then asks other
Turkers to improve the annotations. They report suc-
cessful results using this methodology, but we chose
145
to stick with simply using the qualification test be-
cause it achieves the desired results already. Fur-
thermore, although using TurKit would have proba-
bly done away with our few remaining grammar and
spelling mistakes, it may have caused the captions
for an image to be a little too similar, and we value
a diversity in the use of words and points of view.
Our experiences We have described our experi-
ences in using Amazon?s Mechanical Turk in the
first half of 2009 to create a corpus of images anno-
tated with descriptive sentences. We implemented
two different approaches to quality control: first, we
did not impose any restrictions on who could write
image descriptions. This was then followed by a sec-
ond set of MTurk tasks where Turkers had to judge
the quality of the sentences generated in our initial
Turk experiments. This approach to quality control
would be cost-effective if the initial data were not
too noisy and the subsequent judgments were ac-
curate and cheap. However, this was not the case,
and quality control on the judgments in the form of
control items turned out to result in even lower ac-
curacy. We then repeated our data collection effort,
but required that Turkers live in the US and take a
brief qualification test that we created to test their
English. This is cost-effective if English proficiency
can be accurately assessed in such a brief qualifica-
tion test. We found that the latter approach was in-
deed far cheaper, and produced significantly better
data. We did not set out to run a scientific experi-
ment comparing different strategies of how to col-
lect linguistic data on Turk, and therefore there may
be multiple explanations for the effects we observe.
Nevertheless, our experience indicates strongly that
even very simple prescreening measures can provide
very effective quality control.
We also extended our corpus to include 8000 im-
ages collected from Flickr. We hope to release this
data to the public for future natural language pro-
cessing and computer vision research.
Recommended practices for usingMTurk in NLP
Our experience indicates that with simple prescreen-
ing, linguistic data can be elicited fairly cheaply and
rapidly from crowd-sourcing services such as Me-
chanical Turk. However, many applications may re-
quire more control over where the data comes from.
Even though NLP data collection differs fundamen-
tally from psycholinguistic experiments that may
elicit production data, our community will typically
also need to know whether data was produced by na-
tive speakers or not. Until MTurk provides a better
mechanism to check the native language of its work-
ers, linguistic data collection on MTurk will have to
rely on potentially very noisy input.
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Meaning
of Images. We are grateful for David Forsyth?s ad-
vice and for Alex Sorokin?s support with MTurk.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP 2009.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop/.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with amazons mechanical turk. In LREC 2008.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk. In
Proceedings of SIGCHI 2008.
Greg Little, Lydia B. Chilton, Max Goldman, and
Robert C. Miller. 2009. Turkit: tools for iterative tasks
on mechanical turk. In HCOMP ?09: Proceedings of
the ACM SIGKDD Workshop on Human Computation.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP 2008.
Alexander Sorokin and David Forsyth. 2008. Utility data
annotation with amazon mechanical turk. In Com-
puter Vision and Pattern Recognition Workshop.
Martin I. Tietze, Andi Winterboer, and Johanna D.
Moore. 2009. The effect of linguistic devices in infor-
mation presentation messages on comprehension and
recall. In Proceedings of ENLG 2009.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009.
146
Are all of the words correctly spelled and correctly used? Is the sentence grammatically correct?
A group of children playing with thier toys (N) A man giving pose to camera. (N)
He accepts the crowd?s praise graciously. (Y) The white sheep walks on the grass. (Y)
The coffee is kept at a very hot temperture. (N) She is good woman. (N)
A green car is parked in front of a resturant. (N) He should have talk to him. (N)
An orange cat sleeping with a dog that is much larger then it. (N) He has many wonderful toy. (N)
I ate a tasty desert after lunch. (N) He sended the children home to their parents. (N)
A group of people getting ready for a surprise party. (Y) The passage through the hills was narrow. (Y)
A small refrigerator filled with colorful fruits and vegetables. (Y) A sleeping dog. (Y)
Two men fly by in a red plain. (N) The questions on the test was difficult. (N)
A causal picture of a man and a woman. (N) In Finland, we are used to live in a cold climate. (N)
Three men are going out for a special occasion. (Y) Three white sheeps graze on the grassy field. (N)
Woman eatting lots of food. (N) Between you and me, this is wrong. (Y)
Dyning room with chairs. (N) They are living there during six months. (N)
A woman recieving a package. (N) I was given lots of advices about buying new furnitures. (N)
This is a relatively uncommon occurance. (Y) A horse being led back to it?s stall. (N)
Table 3: The spelling and grammar portions of the qualification test. The test may be found on MTurk by searching
for the qualification entitled ?Image Annotation Qualification?.
Figure 6: Screenshot of the image annotation instruc-
tions: guidelines (top) and examples (bottom).
Figure 7: Screenshot of the quality control test instruc-
tions: guidelines (top) and examples (bottom).
147
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 162?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Cross-caption coreference resolution for automatic image understanding
Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu
Abstract
Recent work in computer vision has aimed
to associate image regions with keywords
describing the depicted entities, but ac-
tual image ?understanding? would also re-
quire identifying their attributes, relations
and activities. Since this information can-
not be conveyed by simple keywords, we
have collected a corpus of ?action? photos
each associated with five descriptive cap-
tions. In order to obtain a consistent se-
mantic representation for each image, we
need to first identify which NPs refer to
the same entities. We present three hierar-
chical Bayesian models for cross-caption
coreference resolution. We have also cre-
ated a simple ontology of entity classes
that appear in images and evaluate how
well these can be recovered.
1 Introduction
Many photos capture a moment in time, telling a
brief story of people, animals and objects, their at-
tributes, and their relationship to each other. Al-
though different people may give different inter-
pretations to the same picture, people can read-
ily interpret photos and describe the entities and
events they perceive in complex sentences. This
level of image understanding still remains an elu-
sive goal for computer vision: although current
methods may be able to identify the overall scene
(Quattoni and Torralba, 2009) or some specific
classes of entities (Felzenszwalb et al, 2008), they
are only starting to be able to identify attributes
of entities (Farhadi et al, 2009), and are far from
recovering a complete semantic interpretation of
the depicted situation. Like natural language pro-
cessing, computer vision requires suitable training
data, and there are currently no publicly available
data sets that would enable the development of
such systems.
Photo sharing sites such as Flickr allow users
to annotate images with keywords and other de-
scriptions, and vision researchers have access to
large collections of images annotated with key-
words (e.g. the Corel collection). A lot of recent
work in computer vision has been aimed at pre-
dicting these keywords (Blei et al, 2003; Barnard
et al, 2003; Feng and Lapata, 2008; Deschacht
and Moens, 2007; Jeon et al, 2003). But key-
words alone are not expressive enough to capture
relations between entities. Some research has used
the text that surrounds an image in a news arti-
cle as a proxy (Feng and Lapata, 2008; Deschacht
and Moens, 2007). However, in many cases, the
surrounding text or a user-provided caption does
not simply describe what is depicted in the image
(since this is usually obvious to the human reader
for which this text is intended), but provides ad-
ditional information. We have collected a corpus
of 8108 images associated with several simple de-
scriptive captions. In contrast to the text near an
image on the web, the captions in our corpus pro-
vide direct, if partial and slightly noisy, descrip-
tions of the image content. Our data set differs
from paraphrase corpora (Barzilay and McKeown,
2001; Dolan et al, 2004) in that the different cap-
tions of an image are produced independently by
different writers. There are many ways of describ-
ing the same image, because it is often possible
to focus on different aspects of the depicted situ-
ation, and because certain aspects of the situation
may be unclear to the human viewer.
One of our goals is to use these captions to
obtain a semantic representation of each image
that is consistent with all of its captions. In or-
der to obtain such a representation, it is neces-
sary to identify the entities that appear in the im-
age, and to perform cross-caption coreference res-
olution, i.e. to identify all mentions of the same
entity in the five captions associated with an im-
age. In this paper, we compare different meth-
162
A golden retriever (ANIMAL) is playing with a smaller black and brown dog(ANIMAL) in a pink collar (CLOTHING).
A smaller black dog (ANIMAL) is fighting with a larger brown dog (ANIMAL) in a forest (NAT_BACKGROUND).
A smaller black and brown dog (ANIMAL) is jumping on a large orange dog (ANIMAL).
Brown dog (ANIMAL) with mouth (BODY_PART) open near head(BODY_PART) of black and tan dog (ANIMAL).
Two dogs (ANIMAL) playing near the woods (NAT_BACKGROUND).
Figure 1: An image with five captions from our corpus. Coreference chains and ontological classes are
indicated in color.
ods of cross-caption coreference resolution on our
corpus. In order to facilitate further computer vi-
sion research, we have also defined a set of coarse-
grained ontological classes that we use to automat-
ically categorize the entities in our data set.
2 A corpus of action images and captions
Image collection and sentence annotation We
have constructed a corpus consisting of 8108 pho-
tographs from Flickr.com, each paired with five
one-sentence descriptive captions written by Ama-
zon?s Mechanical Turk1 workers. We downloaded
a few thousand images from each of six selected
Flickr groups2. To facilitate future computer vi-
sion research on our data, we filtered out images in
black-and-white or sepia, as well as images with
watermarks, signatures, borders or other obvious
editing. Since our collection focuses on images
depicting actions, we then filtered out images of
scenery, portraits, and mood photography. This
was done independently by two members of our
group and adjudicated by a third.
We paid Turk workers $0.10 to write one de-
scriptive sentence for each of five distinct and ran-
domly chosen images that were displayed one at
a time. We required a small qualification test
that examined the workers? English grammar and
spelling and we restricted the task to U.S. work-
ers (see Rashtchian et al (2010) for more details).
Our final corpus contains five sentences for each
of our 8108 images, totaling 478,317 word tokens,
and an average sentence length of 11.8 words.
We first spell-checked3 these sentences, and used
OpenNLP4 to POS-tag them. We identified NPs
using OpenNLP?s chunker, followed by a semi-
1https://www.mturk.com
2The groups:?strangers!?, ?Wild-Child (Kids in Action)?,
?Dogs in Action (Read the Rules)?, ?Outdoor Activities?,
?Action Photography?, ?Flickr-Social (two or more people in
the photo)?.
3We used Unix?s aspell to generate possible correc-
tions and chose between them based on corpus frequencies.
4http://opennlp.sourceforge.net
automatic procedure to correct for a number of
systematic chunking errors that could easily be
corrected. We randomly selected 200 images for
further manual annotation, to be used as test and
development data in our experiments.
Gold standard coreference annotation We
manually annotated NP chunks, ontological
classes, and cross-caption coreference chains for
each of the 200 images in our test and development
data. Each image was annotated independently by
two annotators and adjudicated by a third.5 The
development set contains 1604 mentions. On av-
erage, each caption has 3.2 mentions, and each im-
age has 5.9 coreference chains (distinct entities).
Ontological annotation of entities In order to
understand the role entities mentioned in the sen-
tences play in the image, we have defined a simple
ontology of entity classes (Table 1). We distin-
guish entities that constitute the background of an
image from those that appear in the foreground.
These entities can be animate (people or animals)
or inanimate. For inanimate objects, we distin-
guish static objects from ?movable? objects. We
also distinguish man-made from natural objects
and backgrounds, since this matters for computer
vision algorithms. We have labeled the entity
mentions in our test and development data with
classes from this ontology. Again, two of us an-
notated each image?s mentions, and adjudication
was performed by a single person. Our ontology
is similar to, but smaller than the one proposed
by Hollink and Worring (2005) for video retrieval,
which in turn is based on Hoogs et al (2003) and
Hunter (2001).
3 Predicting image entities from captions
Figure 1 shows an image from our corpus. Dif-
ferent captions use different words to refer to the
5We used MMAX2 (Mu?ller and Strube, 2006) both for
annotation and adjudication.
163
Ontological Class Examples
animal dog, horse, cow
background man-made street, pool, carpet
background natural ocean, field, air
body part hair, mouth, arms
clothing shirt, hat, sunglasses
event trick, sunset, game
fixed object man-made furniture, statue, ramp
fixed object natural rock, puddle, bush
image attribute camera, picture, closeup
material man-made paint, frosting
material natural water, snow, dirt
movable man-made ball, toy, bowl
movable natural leaves, snowball
nondepictable something, Batman
orientation front, top, [the] distance
part of edge, side, top, tires
person family, skateboarder
property of shadow, shade, theme
vehicle surfboard, bike, boat
writing graffiti, map
Table 1: Our ontology for entities in images.
same entity, or even seemingly contradictory mod-
ifiers (?orange? vs. ?brown? dog). In order to
predict what entities appear in an image from its
captions, we need to identify how many entities
each sentence describes, and what role these enti-
ties play in the image (e.g. person, animal, back-
ground). Because we have five sentences asso-
ciated with each image, we also need to identify
which noun phrases in the different captions of
the same image refer to the same entity. Because
the captions were generated independently, there
are no discourse cues such as anaphora to identify
coreference. This creates problems for standard
coreference resolution systems trained on regular
text. Our data also differs from standard corefer-
ence data sets in that entities are rarely referred to
by proper nouns.
Our first task is to identify which noun phrases
may refer to the same entity. We do this by identi-
fying the set of entity types that each NP may refer
to. We use WordNet (Fellbaum, 1998) to iden-
tify the possible entity types (WordNet synsets) of
each head noun. Since the salient entities in each
image are likely to be mentioned by more than one
caption writer, we then aim to restrict those types
to those that may be shared by some head nouns in
the other captions of the same image. This gives
us an inventory of entity types for each mention,
which we use to identify coreferences, restricted
by the constraint that all coreferent mentions refer
to an entity of the same type.
4 Using WordNet to identify entity types
WordNet (Fellbaum, 1998) provides a rich ontol-
ogy of entity types that facilitates our coreference
task.6 We use WordNet to obtain a lexicon of pos-
sible entity types for each mention (based on their
lexical heads, assumed to be the last word with a
nominal POS tag7). We first generate a set of can-
didate synsets based solely on the lexical heads,
and then generate lexicon entries based on rela-
tions between the candidates.
WordNet synsets provide us with synonyms,
and hypernym/hyponym relations. For each men-
tion, we generate a list of candidate synsets.
We require that the candidates are one of the
first four synsets reported and that their fre-
quency is to be at least one-tenth of the most
frequent synset. We limit candidates to ones
with ?physical entity#n#1?, ?event#n#1?, or ?vi-
sual property#n#1? as a hypernym, in order to en-
sure that the synset describes something that is de-
pictable. To avoid word senses that refer to a per-
son in a metaphorical fashion, (e.g. pig meaning
slovenly person or red meaning communist), we
ignore synsets that refer to people if the word has
a synset that is an animal or color.8
In general, we would like for mentions to be
able to take on more specific word senses. For ex-
ample, we would like to be able to identify that
?woman? and ?person? may refer to the same
entity, whereas ?man? and ?woman? typically
would not. However, we also do not want a type
inventory that is too large or too fine-grained.
Once the candidate synsets are generated, we
consider all pairs of nouns (n1, n2) that occur in
different captions of the same image and exam-
ine all corresponding pairs of candidate synsets
(s1, s2). If s2 is a synonym or hypernym of s1, it
is possible that two captions have different words
describing the same entity, so we add s1 and s29
to the lexicon of n1. Adding s2 to n1?s lexicon al-
lows it to act as an umbrella sense covering other
nouns describing the same entity.10 We add s2 to
6For the prediction of ontological classes, we use our own
ontology because WordNet is too fine-grained for this pur-
pose.
7If there are two NP chunks that form a ?[NP ... group] of
[NP... ]? construction, we only use the second NP chunk.
8An exception list handles cases (diver, blonde), where the
human sense is more likely than the animal or color sense.
9We don?t add s2 if it is ?object#n#1? or ?clothing#n#1?.
10This is needed when captions use different aspects of
the entity to describe it (for example, ?skier? and ?a skiing
man?).
164
the lexicon of n2 (since if n1 is using the sense s1,
then n2 must be using the sense s2) and if n1 oc-
curs at least five times in the corpus, we add s1 to
the lexicon of n2.
5 A heuristic coreference algorithm
Based on WordNet candidate synsets, we define
a heuristic algorithm that finds the optimal entity
assignment for the mentions associated with each
image. This algorithm is based on the principles
driving our generative model described below, and
on the observation that salient entities will be men-
tioned in many captions and that captions tend to
use similar words to describe the same entity.
Simple heuristic algorithm:
1. For each noun, choose the synset that appears
in the most number of captions of an image,
and break ties by choosing the synset that
covers the fewest distinct lemmatized nouns.
2. Group all of the noun phrase chunks that
share a synset into a single coreference chain.
6 Bayesian coreference models
Since we cannot afford to manually annotate our
entire data set with coreference information, we
follow Haghighi and Klein (2007)?s work on un-
supervised coreference resolution, and develop a
series of generative Bayesian models for our task.
6.1 Model 0: Simple Mixture Model
In our first model, based on Haghighi and Klein?s
baseline Dirichlet Process model, each image i
corresponds to the set of observed mentions wi
from across its captions. Image i has a hidden
global topic Ti, drawn from a distribution with a
GEM prior with hyperparameter ? as explained by
Teh et al (2006). In a Dirichlet process, the GEM
distribution is an infinite analog of the Dirich-
let distribution, allowing for a potentially infinite
number of mixture components. P (Ti = t) is pro-
portional to ? if t is a new component, or to the
number of times t has been drawn before other-
wise. Given a topic choice Ti = t, entity type
assignments Zj for all mentions wj in image i
are in turn drawn from a topic-specific multino-
mial ?t over all possible entity types E that was
drawn from a Dirichlet prior with hyperparameter
?. Similarly, given an entity type Zi = z, each
corresponding (observed) head word wj is drawn
from an entity type-specific multinomial ?z over
all possible words V, drawn from a finite Dirich-
let prior with hyperparameter ?. The set of all im-
ages belonging to the same topic is analogous to
an individual document in Haghighi and Klein?s
baseline model.11 All headwords of the same en-
tity type are assumed to be coreferent, similar to
Haghighi and Klein?s model. As described in sec-
tion 4, we use WordNet to identify the subset of
types that can actually produce the given words.
Therefore, similar to the way Andrzejewski and
Zhu (2009) handled a priori knowledge of topics,
we will define an indicator variable ?ij that is 1
iff the WordNet information allows word i to be
produced from entity set j and 0 otherwise.
6.1.1 Sampling Model 0
We find argmaxZ,TP (Z,T|X) with Gibbs sam-
pling. Here, Z and T are the collection of type
and topic assignments, with Z?j = Z? {Zj} and
T?i = T ? {Ti}. This style of notation will be
extended analogously to other variables. Let ne,x
represent the number of times word x is produced
from entity e across all topics and let pj be the
number of images assigned to topic j. Let mt,e
represent the number of times entity type e is
generated by topic t. Each iteration consists of
two steps: first, each Zi is resampled, fixing T;
and then each Ti is resampled based on Z12.
1. Sampling Zj:
P (Zj=e|wj ? w
i,Z?j ,T) ? P (wj |Zj=e)P (Zj=e|Ti)
P (wj = x|Zj = e) ?
?
n?je,x + ?
P
x? n
?j
e,x? + ?
?
?xe
P (Zj = e|Ti = t) =
m?jt,e + ?
P
e? m
?j
t,e? + ?
2. Sampling Ti:
P (Ti=j|w,Z,T
?i) ? P (Ti=j|T
?i)P (Z|Ti=j,T
?i)
? P (Ti = j|T
?i)
Y
k?wi
P (Zk|Ti = j)
= P (Ti = j|T
?i)
Y
k?wi
m?ij,Zk + ?P
e? m
?i
j,e? + ?
P (Ti = j|T
?i) ?
(
?, If its a new topic
pj Otherwise
11Since we do not have multiple images of the same well-
known people or places, referred to by their names, we do not
perform any cross-image coreference
12Sampling on the exponentiated posterior to find the mode
as Haghighi and Klein (2007) did was found to not signifi-
cantly affect results on our tasks
165
Caption 1: {x21,1:a golden retriever; x21,2 :a smaller black and brown dog; x21,3:a pink collar}
Caption 2: {x21,4:a smaller black dog; x21,5:a larger brown dog; x21,6:a forest}
Caption 3: {x21,7:small black and brown dog; x21,8:a large orange dog}
Caption 4: {x21,9:brown dog; x21,10:mouth; x21,11:head; x21,12:black and tan dog}
Caption 5: {x21,13:two dogs; x21,14:the woods}
DOG
attr:5
DOG
attr:3
CLOTHING
attr:2
FOREST
attr:8
Image 21:
x21,1
x21,5
x21,8
x21,9
MOUTH
attr:0 ...
x21,2 
x21,4 
x21,7 
x21,12 
x21,3 x21,6
x21,14
x21,10 Restaurant 211
Figure 2: Models 1 and 2 as Chinese restaurant franchises: each image topic is a franchise, each image
is a restaurant, each entity is a table, each mention is a customer. Model 2 adds attributes (in italics).
6.2 Model 1: Explicit Entities
Model 0 does not have an explicit representation
of entities beyond their type and thus cannot dis-
tinguish multiple entities of the same type in an
image. Although Model 1 also represents men-
tions only by their head words (and thus cannot
distinguish black dog from brown dog), it creates
explicit entities based on the Chinese restaurant
franchise interpretation of the hierarchical Dirich-
let Process model (Teh et al, 2006). Figure 2 (ig-
noring the modifiers / attributes for now) illustrates
the Chinese restaurant franchise interpretation of
our model. Using this metaphor, there are a se-
ries of restaurants (= images), each consisting of
a potentially infinite number of tables (= entities),
that are frequented by customers (= entity men-
tions) who will be seated at the tables. Restau-
rants belong to franchises (= image topics). Each
table is served one dish (= entity type, e.g. DOG,
CLOTHING) shared by all the customers. The head
word of a mention xi,j is generated in the follow-
ing manner: customer j enters restaurant i (be-
longing to franchise Ti) and sits down at one of
the existing tables with probability proportional to
the number of other customers there, or sits at a
new table with probability proportional to a con-
stant. A dish eia (DOG) from a potentially infinite
menu is served to each new table a, with probabil-
ity proportional to the total number of tables it is
served at in the franchise Ti (or to a constant if it
is a new dish). The (observed) head word of the
mention xj,i (dog, retriever) is then drawn from
the multinomial distribution over words defined by
the entity type (DOG) at the table. The menu (set
of dishes) available to each restaurant and table is
restricted by our lexicon of WordNet synsets for
each mention. More formally, each image topic
t defines a distribution over entities drawn from a
global GEM prior with hyperparameter ?. There-
fore, the probability of an entity a is proportional
to the number of its existing mentions in images of
the same topic, or to ?, if it is previoulsy unmen-
tioned. The type of each entity, ea, is drawn from
a topic-dependent multinomial with global Dirich-
let prior. The head words of mentions are gener-
ated by their entity type as in Model 0. Mentions
assigned to the same entity are considered to be
coreferent. Based on the nature of our corpus, we
again assume that two words cannot be coreferent
within a sentence, restrict the distribution to not
allow inter-sentence coreference and renormalize
the values accordingly.
6.2.1 Sampling Model 1
There are three parts to our resampling procedure:
resampling the entity assignment for each word,
resampling the entity type for each entity, and re-
sampling the topic of each image. The kth word
of image i, sentence j, will now be wij,k; e
i
a is the
entity type of entity a in image i; aij,k is the en-
tity that word k of sentence j is produced from in
image i, and Zij,k represents that entity?s type. a
is the set of all current entity assignments and e
are the type assignments for entities. m is now de-
fined as the number of entities of a certain type be-
ing drawn for an image, n is defined as before and
ci,a is the number of times entity a is expressed in
image i. Topics are resampled as in Model 0.
Entity Assignment Resampling Entity assign-
ments for words are resampled one sentence at a
time in the order the headwords appear in the sen-
tence. For each word in the sentence, entity as-
signments are defined by the distribution of Fig-
ure 3. The headword is assigned to an existing
entity with probability proportional to the number
of entities already assigned to that entity and the
probability that the entity emits that word. The
word is assigned to a new entity with a newly
166
Model 1:
P (eia=e|a,w, Ti = t, e
?i,a) ? (m?e
i,a
t,e + ?)
Q
{wij,k=x|a
i
j,k=a}
n?e
i,a
e,x + ?
P
y(n
?ei,a
e,y + ?)
?x,e
P (aij,k=a|a
?(i,j,k?|k??k), e,T) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,a, if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k)), o/w
With:
P (wij,k=x|Z
i
j,k=e,a
?(i,j,k?|k??k)) ?
n?(i,j,k
?|k??k)
e,x + ?
P
y(n
?(i,j,k?|k??k)
e,y + ?)
?x,e
P (eia=e|e
?i,a, Ti = t) ?
m?e
i,a
t,e + ?
P
e?(m
?ei,a
t,e? + ?)
Model 2:
P (aij,k=a|a
?(i,j,k?|k??k), e,T,b) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,aP (d
i
j,k|b
i
a), if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))P (bia)P (d
i
a|b
i
a), o/w
P (bia=b|D
i
a,b
?i,a)?P (bia=b|b
?i,a)
Q
d?Dia
(s?i,ab,d + ?)
P
d?(s
?i,a
b,d? + ?)
Figure 3: Sampling equations for Models 1 and 2
drawn entity type with probability proportional ?,
the probability that the entity type is for an im-
age of the given topic (normalized over WordNet?s
possible entities for the word), and the probability
the drawn type produces the word. ?ij,a = 1 iff
entity a of image i does not appear in sentence j
and ?ij,a = 0 otherwise. a
?(i,j,k?|k??k) represents
removing the kth or later words in sentence j of
image i
Entity Type Resampling Fixing the assign-
ments, the type of each entity is redrawn based
on the distribution in Figure 3. It is proportional
to the probability that a certain entity type is in an
image of a given topic and, independently for each
of the words, the probability that the given word
expresses the type. n?e
i,a
e,x is the number of times
entity type e is expressed as word x not counting
the words attached to the currently entity being re-
sampled and m?e
i,a
t,e is the number of times an en-
tity of type e appears in an image of topic t not
counting the current entity being resampled. The
probability of a given image belonging to a topic is
proportional the number of images already in the
topic (or ?) followed by the probability that each
of the entities in the image were drawn from that
topic.
6.3 Model 2: Explicit Entities and Modifiers
Certain entities cannot be distinguished simply by
head word alone, such in the example in Figure 2.
Model 2 augments Model 1 with the ability to gen-
erate modifiers. In addition to an entity type, each
entity draws an attribute from a global distribution
drawn from a GEM distribution with hyperparam-
eter ?. An attribute is a multinomial distribution,
on possible modifier words, drawn from a Dirich-
let prior with parameter ?. From the attribute, each
modifier word is drawn independently. There-
fore given an attribute b and a set of modifiers d:
P (d|b) ?
?
d?d(sd + ?) where sd is number of
times modifier d is produced by attribute b. In ad-
dition, the probability of a certain attribute b given
all other assignments is given by:
P (bia = b|b
?i,a) ?
(
?, If its a new attribute
rb, Otherwise
where rb is the number of entities with at-
tribute b. As in Model 1, mentions assigned to
the same entity are considered coreferent. Con-
sider the ?smaller black dog? mention in Figure 2.
When the mention is being resampled, the at-
tribute choice for each table will bias the probabil-
ity distribution towards the table whose attribute
is more likely to produce ?smaller? and ?black?.
Therefore, the model can now better distinguish
the two dogs in the image.
6.3.1 Sampling Model 2
The addition of modifiers only directly effects the
distribution when resampling entity assignments
since attributes are independent of entity types,
image topics, and headwords of noun phrases. The
sampling distribution are again shown in Figure 3.
In a separate sampling step, it is now necessary to
resample the attribute assigned to each entity: The
probability of drawing a certain attribute is illus-
trated in Figure 3 with Dia as the set of all the mod-
ifiers of all the noun phrases currently assigned to
167
entity a of image i, and s?i,ab,d as the number of
times attribute b produces modifier d without the
current assignment of entity a of image i.
6.4 Implementation
The topic assignments for each image are initial-
ized to correspond to the Flickr groups the images
were taken from. Each mention was initialized as
its own entity with type and attribute sampled from
a uniform distribution.
As our training is unsupervised, each of the
models were ran on the entire dataset. For Model
0, after burn-in, 20 samples of Z were taken
spaced 200 iterations apart, while for Model 1
samples were taken spaced 100 apart, and 25 apart
for Model 2. The implementation of Model 2 ran
too slow to effectively judge when burn in oc-
curred, impacting the results.
The values of parameters ?, ?, ?, ?, ?, ?, and
the number of initial attributes were hand-tuned
based on the average performance on our anno-
tated development subset of 100 images.13
7 Evaluation of coreference resolution
We evaluate each of the generative models and the
heuristic coreference algorithm on the annotated
test subset of our corpus consisting of 100 images
with both the OpenNLP chunking and the gold
standard chunking. We report our scores based
on the MUC evaluation metric. The results are
reported in Table 2 as the average scores across
all the samples of two independent runs of each
model. We also present results on Model 0 with-
out using WordNet where every word can be an
expression of one of 200 fake entity sets. The
same table also shows the performance of a base-
line model and the upper bound on performance
imposed by WordNet.
A baseline model: In our baseline model, two
noun phrases in captions of the same image are
coreferent if they share the same head noun.
Upper bound on performance: Although
WordNet synsets provide a good indication of
whether two mentions can refer to the same
entity or not, they may also be overly restrictive
in other cases. We measure the upper bound
on performance that our reliance on WordNet
imposes by finding the best-scoring coreference
assignment that is consistent with our lexicon.
13(0.1, 0.1, 100, 0.001875, 100, 0.0002, 20) respectively.
This achieves an F-score of 90.2 on the test data
with gold chunks.
Performance increases in each subsequent
model. The heuristic beats each of the models, but
in some sense it is an extreme version of Model
1. Both it and Model 1 attempt to produce en-
tity sets that cover as many captions as possible,
while minimizing the number of distinct words in-
volved. The heuristic locally forces this case, at
the expense of no longer being a generative model.
8 Ontological Class Prediction
As a further step towards understanding the se-
mantics of images, we develop a model that labels
each entity with one of the ontological classes de-
fined in section 2. The immediate difficulty of this
task is that our ontology includes not only seman-
tic distinctions, but also spatial and visual ones.
While it may be easy to tell which words are an-
imals and which are people, there is only a fine
distinction at the language level whether an object
is movable, fixed, or part of the background.14
8.1 Model and Features
We define our task as a classification problem,
where each entity must be assigned to one of
twenty classes defined by our ontology. We use a
Maximum Entropy classifier, implemented in the
MALLET toolkit (McCallum, 2002), and define
the following text features:
NP Chunk: We include all the words in the NP
chunk, unfiltered.
WordNet Synsets and Hypernyms: The most
likely synset is either the first one that appears in
WordNet or one of the ones predicted by our coref-
erence system. For each of these possibilities, we
include all of that synset?s hypernyms.
Syntactic Role: We parsed our captions with the
C&C parser (Clark and Curran, 2007), and record
whether the word appears as a direct object of a
verb, as the object of a preposition, as the subject
of the sentence, or as a modifier. If it is a modi-
fier, we also add the head word of the phrase being
modified.
14For example, we deem bowls and silverware to be mov-
able objects; furniture, fixed; and carpets, background. More-
over, in all three cases, we must correctly distinguish that
these objects are man-made and not found in nature.
168
Model OpenNLP chunks Gold chunks
Rec. Prec. F1 Rec. Prec. F1
Baseline 57.3 89.5 69.9 64.1 96.2 77.0
Upper bound 82.1 100 90.2
WN Heuristic 70.6 84.8 77.0 80.4 90.6 85.2
Model 0 w/o WN 79.7 59.8 68.4 85.1 62.7 72.2
Model 0 66.8 83.1 74.1 75.9 90.3 82.5
Model 1 69.6 83.8 76.0 78.0 90.8 83.9
Model 2 69.2 84.4 76.1 77.9 91.5 84.1
Table 2: Coreference resolution results (MUC scores; Models 0-2 are averaged over all samples)
8.2 Experiments
We use two baselines. The naive baseline catego-
rizes words by selecting the most frequent class
of the word. If no instances of the word have oc-
curred, it uses the overall most frequent class. The
WordNet baseline works by finding the most fre-
quent class amongst the most relevant synsets for
a word. It calculates the class frequency for each
synset by assuming each word has the sense of its
first synset and incrementing the frequency of the
first synset and its hypernyms. When categorizing
a word, it finds the set of closest hypernyms of the
word that have a non-zero frequency, and chooses
the class with the greatest sum of frequency counts
amongst those hypernyms.
We train the MaxEnt classifier using semi-
supervised learning. Initially, we train a classifier
using the 500 sentence gold standard development
set. For each class, we use the top 5%15 of the la-
bels to label the unlabeled data and provide addi-
tional training data. We then retrain the classifier
on the newly labeled examples and the develop-
ment set, and run it on the test set. For each coref-
erence chain in the test set, we relabel all of the
mentions in the chain to use the majority class, if
a clear majority exists. If no such majority exists,
we leave the labels as is. The MaxEnt classifier
experiments were conducted by varying the source
of the synset assigned to each word. For each of
our coreference systems, we report two scores (Ta-
ble 3). The first is the average accuracy when us-
ing the output from two runs of each model with
about 20 samples per run, and the second uses the
output that performs best on the coreference task
when scored on the development data.
Discussion Although we use WordNet to clas-
sify our entity mentions, we designed our ontology
by considering only the images and their captions,
with no particular mapping to WordNet in mind.
15This was tuned using 10-fold cross-validation of the de-
velopment set.
Classifier (synset prediction) Accuracy (gold chunks)
Naive Baseline 72.0
WordNet Baseline 81.0
MaxEnt (1st-synset) 84.4
MaxEnt (WN heuristic) 82.7
Avg. ? Best-Coref
MaxEnt (Model 1) 83.9 0.5 84.5
MaxEnt (Model 2) 84.1 0.4 85.3
Table 3: Prediction of ontological classes
Therefore, these experiments provide of a proof of
concept for the semi-supervised labeling of a cor-
pus using any semantic/visual ontology.
Overall, Model 2 had the best performance for
this task. This demonstrates that the additional
features of Model 2 force synset selections that are
consistent across the entire corpus, and are sen-
sitive to the modifiers appearing with them. The
WordNet heuristic selects synsets in a fairly ar-
bitrary manner - all other things being equal, the
synsets are chosen without reference to what other
synsets are chosen by similar clusters of nouns.
9 Evaluating entity prediction
Together, the coreference resolution algorithm and
ontology classification model provide us with a set
of distinct, ontologically-categorized entities ap-
pearing in each image. We perform a final experi-
ment to evaluate how well our models can recover
the mentioned entities and their ontological types
for each image. We now represent each entity as a
tuple (L, c), where L is its coreference chain, and
c is the ontological class of these mentions. 16
We compute the precision and recall between
the predicted and gold standard tuples for each im-
age. We consider a tuple (L?, c?) correctly pre-
dicted only when a copy of (L?, c?) occurs both
in the set of predicted tuples and the set of gold
standard tuples.17 Then, as usual, for precision we
16Note that for each image, the tuples of all entities corre-
spond to a partition of the set of the head-word mentions in
an image.
17We assign no partial credit because incorrect typing or
169
Model Recall Precision F-score
Baseline 28.4 20.6 23.9
WordNet Heuristic 48.3 43.9 46.0
Model 1 (avg) 51.7 42.8 46.8
Model 1 (best-coref) 50.9 45.4 48.0
Model 2 (avg) 52.2 42.7 47.0
Model 2 (best-coref) 52.3 46.0 49.0
Table 4: Overall entity recovery. We measure
how many entities we identify correctly (requiring
complete recovery of their coreference chains and
correct prediction of their ontological class.
normalize the number of overlapping tuples by the
number of predicted tuples, and for recall, by the
number of gold standard tuples. We report average
precision and recall over all images in our test set.
We report scores for four different pairs of on-
tological class and coreference chain predictions.
As a baseline, we use the ontological classes pre-
dicted by the our naive baseline and the chains pre-
dicted by the ?same-words-are-coreferent? coref-
erence resolution baseline.
We also report results using the classes and
chains predicted by Model 1, Model 2, and the
WordNet Heuristic Algorithm. The influence of
the different coreference algorithms comes from
the entity types that are used to determine corefer-
ence chains, and that also correspond to WordNet
candidate synsets. In other words, although the
final coreference chain may be predicted by two
different models, the synsets they use to do so may
differ, affecting the synset and hypernym features
used for ontological prediction. We present results
in Table 4 for these four different set-ups.
The synsets chosen by the different corefer-
ence algorithms clearly have different applicabil-
ity when it comes to ontological class prediction.
Although Model 2 performs comparably to Model
1 and does worse than the WordNet heuristic al-
gorithm for coreference chain prediction, it cer-
tainly does better on this task. Since our end goal
is creating a unified semantic representation, this
final task judges the effectiveness of our models to
capture the most detailed entity information. The
success of Model 2 means that the incorporation
of adjectives informs the proper choice of synsets
that are useful in predicting ontological classes.
10 Conclusion
As a first step towards automatic image under-
standing, we have collected a corpus of images as-
incomplete coreference chaining both completely change the
semantics of an image.
sociated with several simple descriptive captions,
which provide more detailed information about
the image than simple keywords. We plan to make
this data set available for further research in com-
puter vision and natural language processing. In
order to enable the creation of a semantic repre-
sentation of the image content that is consistent
with the captions in our data set, we use Word-
Net and a series of Bayesian models to perform
cross-caption coreference resolution. Similar to
Haghighi and Klein (2009), who find that linguis-
tic heuristics can provide very strong baselines for
standard coreference resultion, relatively simple
heuristics based on WordNet alne perform sur-
prisingly well on our task, although they are out-
performed by our Bayesian models for overall en-
tity prediction. Since our generative models are
based on Dirichlet Process priors, they are de-
signed to favor a small number of unique entities
per image. In the heuristic algorithm, this bias
is built in explicitly, resulting in slightly higher
performance on the coreference resolution task.
However, while the generative models can use
global information to learn what entity type each
word is likely to represent, the heuristic is unable
to capture any non-local information about the en-
tities, and thus provides less useful input for the
prediction of ontological classes.
Future work will aim to improve on these re-
sults by overcoming the upper bound on perfor-
mance imposed by WordNet, and through a more
sophisticated model of modifiers. We will also in-
vestigate how image features can be incorporated
into our model to improve performance on entity
detection. Ultimately, identifying the depicted en-
tities from multiple image captions will require
novel ways to correctly handle the semantics of
plural NPs (i.e. that one caption?s ?two dogs? con-
sist of another?s ?golden retreiver? and ?smaller
black dog?). We foresee similar challenges when
dealing with verbs and events.
The creation of an actual semantic representa-
tion of the image content is a challenging problem
in itself, since the different captions often focus
on different aspects of the depicted situation, or
provide different interpretation of ambiguous sit-
uations. We believe that this poses many inter-
esting challenges for natural language processing,
and will ultimately require ways to integrate the
information conveyed in the caption with visual
features extracted from the image.
170
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Mean-
ing of Images. We are grateful for David Forsyth
and Dan Roth?s advice, and for Alex Sorokins sup-
port with MTurk.
References
David Andrzejewski and Xiaojin Zhu. 2009. Latent
Dirichlet alocation with topic-in-set knowledge. In
NAACL HLT 2009 Workshop on Semi-Supervised
Learning for Natural Language Processing, pages
43?48.
Kobus Barnard, Pinar Duygulu, David Forsyth,
Nando De Freitas, David M. Blei, and Michael I.
Jordan. 2003. Matching words and pictures. Jour-
nal of Machine Learning Research, 3:1107?1135.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th annual meeting of the Asso-
ciation for Computational Linguistics, pages 50?57,
Toulouse, France, July.
David M. Blei, Michael I, David M. Blei, and Michael
I. 2003. Modeling annotated data. In Proceedings
of the 26th International ACM SIGIR Conference,
pages 127?134.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of Coling 2004, pages 350?356,
Geneva, Switzerland, August. COLING.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing objects by their attributes. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 1778?1785, June.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
P. Felzenszwalb, D. McAllester, and D. Ramanan.
2008. A discriminatively trained, multiscale, de-
formable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1?8,
June.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of ACL-08: HLT, pages 272?280,
Columbus, Ohio, June.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161, Singapore, August. Associ-
ation for Computational Linguistics.
L. Hollink and M. Worring. 2005. Building a vi-
sual ontology for video retrieval. In MULTIMEDIA
?05: Proceedings of the 13th annual ACM interna-
tional conference on Multimedia, pages 479?482,
New York, NY, USA. ACM.
A. Hoogs, J. Rittscher, G. Stein, and J. Schmiederer.
2003. Video content annotation using visual anal-
ysis and a large semantic knowledgebase. In IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, volume 2, pages II?327 ?
II?334 vol.2, June.
Jane Hunter. 2001. Adding multimedia to the semantic
web - building an mpeg-7 ontology. In In Interna-
tional Semantic Web Working Symposium (SWWS,
pages 261?281.
Lavrenko Manmatha Jeon, V. Lavrenko, R. Manmatha,
and J. Jeon. 2003. A model for learning the seman-
tics of pictures. In Seventeenth Annual Conference
on Neural Information Processing Systems (NIPS).
MIT Press.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Christoph Mu?ller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Sabine Braun et al editor, Corpus Technology and
Language Pedagogy, pages 197?214. Peter Lang,
Frankfurt a.M., Germany.
Ariadna Quattoni and Antonio B. Torralba. 2009.
Recognizing indoor scenes. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
413?420. IEEE.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazons mechanical turk. In NAACL
Workshop Creating Speech and Language Data With
Amazons Mechanical Turk.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
171
