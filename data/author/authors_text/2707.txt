Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1?9,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Third PASCAL Recognizing Textual Entailment Challenge 
 
Danilo Giampiccolo 
CELCT 
Via alla Cascata 56/c 
38100 POVO TN 
giampiccolo@celct.it 
Bernardo Magnini 
FBK-ITC 
Via Sommarive 18, 
38100 Povo TN 
magnini@itc.it  
Ido Dagan 
Computer Science Department 
Bar-Ilan University 
Ramat Gan 52900, Israel 
dagan@macs.biu.ac.il 
Bill Dolan 
Microsoft Research 
Redmond, WA, 98052, USA 
billdol@microsoft.com 
Abstract 
This paper presents the Third PASCAL 
Recognising Textual Entailment Chal-
lenge (RTE-3), providing an overview of 
the dataset creating methodology and the 
submitted systems. In creating this 
year?s dataset, a number of longer texts 
were introduced to make the challenge 
more oriented to realistic scenarios. Ad-
ditionally, a pool of resources was of-
fered so that the participants could share 
common tools. A pilot task was also set 
up, aimed at differentiating unknown en-
tailments from identified contradictions 
and providing justifications for overall 
system decisions. 26 participants submit-
ted 44 runs, using different approaches 
and generally presenting new entailment 
models and achieving higher scores than 
in the previous challenges. 
1.1 The RTE challenges 
 
The goal of the RTE challenges has been to cre-
ate a benchmark task dedicated to textual en-
tailment ? recognizing that the meaning of one 
text is entailed, i.e. can be inferred, by another1. 
In the recent years, this task has raised great in-
terest since applied semantic inference concerns 
many practical Natural Language Processing 
(NLP) applications, such as Question Answering 
(QA), Information Extraction (IE), Summariza-
tion, Machine Translation and Paraphrasing, and 
certain types of queries in Information Retrieval 
(IR). More specifically, the RTE challenges 
have aimed to focus research and evaluation on 
this common underlying semantic inference task 
and separate it from other problems that differ-
ent NLP applications need to handle. For exam-
ple, in addition to textual entailment, QA sys-
tems need to handle issues such as answer re-
trieval and question type recognition.  
By separating out the general problem of tex-
tual entailment from these task-specific prob-
lems, progress on semantic inference for many 
application areas can be promoted. Hopefully, 
research on textual entailment will finally lead to 
the development of entailment ?engines?, which 
can be used as a standard module in many appli-
cations (similar to the role of part-of-speech tag-
gers and syntactic parsers in current NLP appli-
cations). 
In the following sections, a detailed descrip-
tion of RTE-3 is presented. After a quick review 
                                                 
1
 The task was first defined by Dagan and Glickman 
(2004). 
1
of the previous challenges (1.2), section 2 de-
scribes the preparation of the dataset. In section 
3 the evaluation process and the results are pre-
sented, together with an analysis of the perform-
ance of the participating systems. 
1.2  The First and Second RTE Challenges 
 
The first RTE challenge2 aimed to provide the 
NLP community with a new benchmark to test 
progress in recognizing textual entailment, and 
to compare the achievements of different groups. 
This goal proved to be of great interest, and the 
community's response encouraged the gradual 
expansion of the scope of the original task. 
The Second RTE challenge3 built on the suc-
cess of the first, with 23 groups from around the 
world (as compared to 17 for the first challenge) 
submitting the results of their systems. Repre-
sentatives of participating groups presented their 
work at the PASCAL Challenges Workshop in 
April 2006 in Venice, Italy. The event was suc-
cessful and the number of participants and their 
contributions to the discussion demonstrated that 
Textual Entailment is a quickly growing field of 
NLP research. In addition, the workshops 
spawned an impressive number of publications 
in major conferences, with more work in pro-
gress. Another encouraging sign of the growing 
interest in the RTE challenge was represented by 
the increase in the number of downloads of the 
challenge datasets, with about 150 registered 
downloads for the RTE-2 development set. 
1.3 The Third Challenge 
 
RTE-3 followed the same basic structure of the 
previous campaigns, in order to facilitate the 
participation of newcomers and to allow "veter-
ans" to assess the improvements of their systems 
in a comparable test exercise. Nevertheless, 
some innovations were introduced, on the one 
hand to make the challenge more stimulating 
and, on the other, to encourage collaboration 
between system developers. In particular, a lim-
ited number of longer texts, i.e. up to a para-
graph in length, were incorporated in order to 
move toward more comprehensive scenarios, 
                                                 
2
 http://www.pascal-network.org/Challenges/RTE/. 
3
 http://www.pascal-network.org/Challenges/RTE2./ 
which incorporate the need for discourse analy-
sis. However, the majority of examples re-
mained similar to those in the previous chal-
lenges, providing pairs with relatively short 
texts.  
Another innovation was represented by a re-
source pool4, where contributors had the possi-
bility to share the resources they used. In fact, 
one of the key conclusions at the second RTE 
Challenge Workshop was that entailment model-
ing requires vast knowledge resources that cor-
respond to different types of entailment reason-
ing. Moreover, entailment systems also utilize 
general NLP tools such as POS taggers, parsers 
and named-entity recognizers, sometimes posing 
specialized requirements to such tools. In re-
sponse to these demands, the RTE Resource 
Pool was built, which may serve as a portal and 
forum for publicizing and tracking resources, 
and reporting on their use.  
In addition, an optional pilot task, called "Ex-
tending the Evaluation of Inferences from Texts" 
was set up by the US National Institute of Stan-
dards and Technology (NIST), in order to ex-
plore two other sub-tasks closely related to tex-
tual entailment: differentiating unknown entail-
ments from identified contradictions and provid-
ing justifications for system decisions. In the 
first sub-task, the idea was to drive systems to 
make more precise informational distinctions, 
taking a three-way decision between "YES", 
"NO" and "UNKNOWN?, so that a hypothesis 
being unknown on the basis of a text would be 
distinguished from a hypothesis being shown 
false/contradicted by a text. As for the other sub-
task, the goal for providing justifications for de-
cisions was to explore how eventual users of 
tools incorporating entailment can be made to 
understand how decisions were reached by a 
system, as users are unlikely to trust a system 
that gives no explanation for its decisions. The 
pilot task exploited the existing RTE-3 Chal-
lenge infrastructure and evaluation process by 
using the same test set, while utilizing human 
assessments for the new sub-tasks. 
                                                 
4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail 
ment_Resource_Pool. 
2
 Table 1: Some examples taken from the Development Set. 
 
2 The RTE-3 Dataset 
2.1 Overview 
 
The textual entailment recognition task required the 
participating systems to decide, given two text 
snippets t and h, whether t entails h. Textual en-
tailment is defined as a directional relation between 
two text fragments, called text (t, the entailing 
text), and hypothesis (h, the entailed text), so that a 
human being, with common understanding of lan-
guage and common background knowledge, can 
infer that h is most likely true on the basis of the 
content of t. 
As in the previous challenges, the RTE-3 dataset 
consisted of 1600 text-hypothesis pairs, equally 
divided into a development set and a test set. While 
the length of the hypotheses (h) was  the same as in 
the past datasets, a certain number of texts (t) were 
longer than in previous datasets, up to a paragraph. 
The longer texts were marked as L, after being se-
lected automatically when exceeding 270 bytes. In 
the test set they were about 17% of the total.  
As in RTE-2, four applications ? namely IE, IR, 
QA and SUM ? were considered as settings or con-
texts for the pairs generation (see 2.2 for a detailed 
description). 200 pairs were selected for each ap-
plication in each dataset. Although the datasets 
were supposed to be perfectly balanced, the num-
ber of negative examples were slightly higher in 
both development and test sets (51.50% and 
51.25% respectively; this was unintentional). Posi-
tive entailment examples, where t entailed h, were 
annotated YES; the negative ones, where entailment 
did not hold, NO. Each pair was annotated with its 
TASK TEXT HYPOTHESIS ENTAILMENT 
IE At the same time the Italian digital rights group, Elec-
tronic Frontiers Italy, has asked the nation's government 
to investigate Sony over its use of anti-piracy software. 
Italy's govern-
ment investigates 
Sony. 
NO 
IE Parviz Davudi was representing Iran at a meeting of the 
Shanghai Co-operation Organisation (SCO), the fledg-
ling association that binds Russia, China and four for-
mer Soviet republics of central Asia together to fight 
terrorism 
China is a mem-
ber of SCO. 
YES 
IR Between March and June, scientific observers say, up to 
300,000 seals are killed. In Canada, seal-hunting means 
jobs, but opponents say it is vicious and endangers the 
species, also threatened by global warming 
Hunting endan-
gers seal species. 
YES 
IR The Italian parliament may approve a draft law allow-
ing descendants of the exiled royal family to return 
home. The family was banished after the Second World 
War because of the King's collusion with the fascist 
regime, but moves were introduced this year to allow 
their return. 
Italian royal fam-
ily returns home. 
NO 
QA Aeschylus is often called the father of Greek tragedy; 
he wrote the earliest complete plays which survive from 
ancient Greece. He is known to have written more than 
90 plays, though only seven survive. The most famous 
of these are the trilogy known as Orestia. Also well-
known are The Persians and Prometheus Bound. 
"The Persians" 
was written by 
Aeschylus. 
YES 
SUM A Pentagon committee and the congressionally char-
tered Iraq Study Group have been preparing reports for 
Bush, and Iran has asked the presidents of Iraq and 
Syria to meet in Tehran. 
Bush will meet 
the presidents of 
Iraq and Syria in 
Tehran. 
NO 
3
related task (IE/IR/QA/SUM) and entailment 
judgment (YES/NO, obviously released only in the 
development set). Table 1 shows some examples 
taken from the development set. 
The examples in the dataset were based mostly 
on outputs (both correct and incorrect) of Web-
based systems. In order to avoid copyright prob-
lems, input data was limited to either what had al-
ready been publicly released by official competi-
tions or else was drawn from freely available 
sources such as WikiNews and Wikipedia. 
In choosing the pairs, the following judgment 
criteria and guidelines were considered: 
 
? As entailment is a directional relation, the 
hypothesis must be entailed by the given 
text, but the text need not be entailed by 
the hypothesis. 
? The hypothesis must be fully entailed by 
the text. Judgment must be NO if the hy-
pothesis includes parts that cannot be in-
ferred from the text. 
? Cases in which inference is very probable 
(but not completely certain) were judged as 
YES.  
? Common world knowledge was assumed, 
e.g. the capital of a country is situated in 
that country, the prime minister of a state is 
also a citizen of that state, and so on. 
2.2 Pair Collection 
 
As in RTE-2, human annotators generated t-h pairs 
within 4 application settings.  
 
The IE task was inspired by the Information Ex-
traction (and Relation Extraction) application, 
where texts and structured templates were replaced 
by t-h pairs. As in the 2006 campaign, the pairs 
were generated using four different approaches: 
1) Hypotheses were taken from the relations 
tested in the ACE-2004 RDR task, while 
texts were extracted from the outputs of ac-
tual IE systems, which were provided with 
relevant news articles. Correctly extracted  
instances were used to generate positive 
examples and incorrect instances to gener-
ate negative examples. 
2) The same procedure was followed using 
output of IE systems on the dataset of the 
MUC-4 TST3 task, in which the events are 
acts of terrorism. 
3) The annotated MUC-4 dataset and the 
news articles were also used to manually 
generate entailment pairs based on ACE re-
lations.  
4) Hypotheses corresponding to relations not 
found in the ACE and MUC datasets  were 
used both to be given to IE systems and to 
manually generate t-h pairs from collected 
news articles. Examples of these relations, 
taken from various semantic fields, were 
?X beat Y?, ?X invented Y?, ?X steal Y? 
etc. 
 
The common aim of all these processes was to 
simulate the need of IE systems to recognize that 
the given text indeed entails the semantic relation 
that is expected to hold between the candidate tem-
plate slot fillers.  
 
In the IR (Information Retrieval) application set-
ting, the hypotheses were propositional IR queries, 
which specify some statement, e.g. ?robots are 
used to find avalanche victims?. The hypotheses 
were adapted and simplified from standard IR 
evaluation datasets (TREC and CLEF). Texts (t) 
that did or did not entail the hypotheses were se-
lected from documents retrieved by different search 
engines (e.g. Google, Yahoo and MSN) for each 
hypothesis. In this application setting it was as-
sumed that relevant documents (from an IR per-
spective) should entail the given propositional hy-
pothesis. 
 
For the QA (Question Answering) task, annotators 
used questions taken from the datasets of official 
QA competitions, such as TREC QA and 
QA@CLEF datasets, and the corresponding an-
swers extracted from the Web by actual QA sys-
tems. Then they transformed the question-answer 
pairs into t-h pairs as follows: 
 
? An answer term of the expected answer 
type was picked from the answer passage -
either a correct or an incorrect one.  
? The question was turned into an affirma-
tive sentence plugging in the answer term. 
? t-h pairs were generate, using the affirma-
tive sentences as hypotheses (h?s) and the 
original answer passages as texts (t?s).  
4
For example, given the question ?How high is 
Mount Everest?? and a text (t) ?The above men-
tioned expedition team comprising of 10 members 
was permitted to climb 8848m. high Mt. Everest 
from Normal Route for the period of 75 days from 
15 April, 2007 under the leadership of Mr. Wolf 
Herbert of Austria?, the annotator, extracting the 
piece of information ?8848m.? from the text, 
would turn the question into an the affirmative sen-
tence ?Mount Everest is 8848m high?, generating a 
positive entailment pair. This process simulated the 
need of a QA system to verify that the retrieved 
passage text actually entailed the provided answer. 
 
In the SUM (Summarization) setting, the 
entailment pairs were generated using two proce-
dures. 
In the first one, t?s and h?s were sentences taken 
from a news document cluster, a collection of news 
articles that describe the same news item. Annota-
tors were given the output of multi-document 
summarization systems -including the document 
clusters and the summary generated for each clus-
ter. Then they picked sentence pairs with high lexi-
cal overlap, preferably where at least one of the 
sentences was taken from the summary (this sen-
tence usually played the role of t). For positive ex-
amples, the hypothesis was simplified by removing 
sentence parts, until it was fully entailed by t. 
Negative examples were simplified in a similar 
manner. In alternative, ?pyramids? produced for 
the experimental evaluation mehod in DUC 2005 
(Passonneau et al 2005) were exploited. In this 
new evaluation method, humans select sub-
sentential content units (SCUs) in several manually 
produced summaries on a subject, and collocate 
them in a ?pyramid?, which has at the top the 
SCUs with the higher frequency, i.e. those which 
are present in most summaries. Each SCU is identi-
fied by a label, a sentence in natural language 
which expresses the content. Afterwards, the anno-
tators individuate the SCUs present in summaries 
generated automatically (called peers), and link 
them to the ones present in the pyramid, in order to 
assign each peer a weight. In this way, the SCUs in 
the automatic summaries linked to the SCUs in the 
higher tiers of the pyramid are assigned a heavier 
weight than those at the bottom. For the SUM set-
ting, the RTE-3 annotators selected relevant pas-
sages from the peers and used them as T?s, mean-
while the labels of the corresponding SCUs were 
used as H?s. Small adjustments were allowed, 
whenever the texts were not grammatically accept-
able. This process simulated the need of a summa-
rization system to identify information redundancy, 
which should be avoided in the summary. 
2.3 Final dataset  
 
Each pair of the dataset was judged by three anno-
tators. As in previous challenges, pairs on which 
the annotators disagreed were filtered-out.  
On the test set, the average agreement between 
each pair of annotators who shared at least 100 ex-
amples was 87.8%, with an average Kappa level of 
0.75, regarded as substantial agreement according 
to Landis and Koch (1997).  
19.2 % of the pairs in the dataset were removed 
from the test set due to disagreement. The dis-
agreement was generally due to the fact that the h 
was more specific than the t, for example because 
it contained more information, or made an absolute 
assertion where t proposed only a personal opinion. 
In addition, 9.4 % of the remaining pairs were dis-
carded, as they seemed controversial, too difficult, 
or too similar when compared to other pairs.  
As far as the texts extracted from the web are 
concerned, spelling and punctuation errors were 
sometimes fixed by the annotators, but no major 
change was allowed, so that the language could be 
grammatically and stylistically imperfect. The hy-
potheses were finally double-checked by a native 
English speaker. 
3 The RTE-3 Challenge 
3.1 Evaluation measures 
 
The evaluation of all runs submitted in RTE-3 was 
automatic. The judgments (classifications) returned 
by the system were compared to the Gold Standard 
compiled by the human assessors. The main 
evaluation measure was accuracy, i.e. the percent-
age of matching judgments. 
For systems that provided a confidence-ranked 
list of the pairs, in addition to the YES/NO judg-
ment, an Average Precision measure was also 
computed. This measure evaluates the ability of 
systems to rank all the T-H pairs in the test set ac-
cording to their entailment confidence (in decreas-
ing order from the most certain entailment to the 
least certain). Average precision is computed as the 
5
average of the system's precision values at all 
points in the ranked list in which recall increases, 
that is at all points in the ranked list for which the 
gold standard annotation is YES, or, more for-
mally:  
 
?
=
?n
i i
iUpToPairEntailmentiE
R 1
)(#)(1
          (1) 
 
where n is the number of the pairs in the test set, R 
is the total number of positive pairs in the test set, 
E(i) is 1 if the i-th pair is positive and 0 otherwise, 
and i ranges over the pairs, ordered by their rank-
ing.  
In other words, the more the system was confi-
dent that t entails h, the higher was the ranking of 
the pair. A perfect ranking would have placed all 
the positive pairs (for which the entailment holds) 
before all the negative ones, yielding an average 
precision value of 1. 
3.2 Submitted systems 
 
Twenty-six teams participated in the third chal-
lenge, three more than in previous year. Table 2 
presents the list of the results of each submitted 
runs and the components used by the systems. 
Overall, we noticed a move toward deep ap-
proaches, with a general consolidation of ap-
proaches based on the syntactic structure of Text 
and Hypothesis. There is an evident increase of 
systems using some form of logical inferences (at 
least seven systems). However, these approaches, 
with few notably exceptions, do not seem to be 
consolidated enough, as several systems show re-
sults  not still at the state of art (e.g. Natural Logic 
introduced by Chambers et al). For many systems 
an open issue is the availability and integration of 
different and complex semantic resources-  
A more extensive and fine grained use of spe-
cific semantic phenomena is also emerging. As an 
example, Tatu and Moldovan carry on a sophisti-
cated analysis of named entities, in particular Per-
son names, distinguishing first names from last 
names. Some form of relation extraction, either 
through manually built patterns (Chambers et al) 
or through the use of an information extraction sys-
tem (Hickl and Bensley) have been introduced this 
year, even if still on a small scale (i.e. few rela-
tions).  
On the other hand, RTE-3 confirmed that both 
machine learning using lexical-syntactic features 
and transformation-based approaches on depend-
ency representations are well consolidated tech-
niques to address textual entailment. The extension 
of transformation-based approaches toward prob-
abilistic settings is an interesting direction investi-
gated by some systems (e.g. Harmeling). On the 
side of ?light? approaches to textual entailment, 
Malakasiotis and Androutpoulos provide a useful 
baseline for the task (0.61%) using only POS tag-
ging and then applying string-based measures to 
estimate the similarity between Text and Hypothe-
sis. 
As far as resources are concerned, lexical data-
bases (mostly WordNet and DIRT) are still widely 
used. Extended WordNet is also a common re-
source (for instance in Iftene and Balahur-
Dobrescu) and the Extended Wordnet Knowledge 
Base has been successfully used in (Tatu and 
Moldovan). Verb-oriented resources are also 
largely present in several systems, including Fra-
menet (e.g. Burchardt et al), Verbnet (Bobrow et 
al.) and Propbank (e.g. Adams et al). It seems that 
the use of the Web as a resource is more limited 
when compared to the previous RTE workshop. 
However, as in RTE-2, the use of large semantic 
resources is still a crucial factor affecting the per-
formance of systems (see, for instance, the use of a 
large corpus of entailment examples in Hickl and 
Bensley).  
Finally, an interesting aspect is that, stimulated 
by the percentage of longer texts included this year, 
a number of participating systems addressed anaph-
ora resolution (e.g. Delmonte, Bar-Haim et al, 
Iftene and Balahur-Dobrescu). 
3.3 Results 
 
The accuracy achieved by the participating sys-
tems ranges from 49% to 80% (considering the best 
run of each group), while most of the systems ob-
tained a score in between 59% and 66%. One sub-
mission, Hickl and Bensley achieved 80% accu-
racy, scoring 8% higher than the second system 
(Tatu and Moldovan, 72%), and obtaining the best 
absolute result achieved in the three RTE chal-
lenges. 
6
 Table 2: Submission results and components of the systems.
 . 
System Components 
First Author Accuracy 
Average 
precision L
ex
ic
al
 
R
el
at
io
n
,
 
W
o
rd
N
et
 
 
n
-
gr
am
\w
o
rd
 
sim
ila
rit
y 
Sy
n
ta
ct
ic
 
M
at
ch
-
in
g\
A
lig
n
in
g 
Se
m
an
tic
 
R
o
le
 
La
be
lin
g\
 
Fr
am
en
et
\P
ro
ba
n
k,
 
V
er
bn
et
 
Lo
gi
ca
l I
n
fe
re
n
ce
 
Co
rp
u
s/ 
W
eb
-
ba
se
d 
St
at
ist
ic
s,
 
LS
A
 
M
L 
Cl
as
sif
ic
at
io
n
 
A
n
ap
ho
ra
 
re
so
lu
tio
n
 
 
En
ta
ilm
en
t 
Co
rp
o
ra
 
?
 
D
IR
T 
Ba
ck
gr
o
u
n
d 
K
n
o
w
le
dg
e 
Adams 0.6700  X X    X X   
0.6112 0.6118 X  X   X  X X Bar-Haim 
0.5837 0.6093  X  X   X  X  
Baral 0.4963 0.5364 X    X    X 
0.6050 0.5897 X  X    X   Blake 
  0.6587 0.6096 X  X    X   
0.5112 0.5720  X   X X     Bobrow 
  0.5150 0.5807 X   X X     
0.6250  X  X X      Burchardt 
0.6262           
0.5500   X    X    Burek 
0.5500 0.5514          
0.6050 0.6341 X  X  X  X X  Chambers 
  0.6362 0.6527 X  X  X  X X  
0.5088 0.4961  X   
 
 X    X Clark  
0.4725 0.4961  X    X    X 
Delmonte 0.5875 0.5830 X  X X X   X  
0.6563  X X X       Ferrandez 
0.6375           
0.6062  X X     X   Ferr?s 
0.6150  X X     X   
0.5600 0.5813 X  X    X   Harmling 
0.5775 0.5952 X  X    X   
Hickl 0.8000 0.8815 X X   X  X X X 
0.6913  X  X      X Iftene 
0.6913  X  X      X 
0.6400  X X     X   Li 
0.6488           
Litkowski   0.6125           
Malakasiotis  0.6175 0.6808  X     X   
Marsi 0.5913    X      X 
0.5888  X X X    X   Montejo-R?ez 
0.6038  X X X    X   
0.6238  X X X    X   Rodrigo 
0.6312  X X X    X   
0.6262  X X       X Roth 
0.5975    X     X  
0.6100 0.6195 X X     X   Settembre 
  0.6262 0.6274 X X     X   
0.7225 0.6942 X    X   X X Tatu 
  0.7175 0.6797 X    X   X  
0.6650    X    X   Wang  
0.6687           
0.6675 0.6674 X  X    X   Zanzotto 
  0.6575 0.6732 X  X    X   
7
As far as the per-task results are concerned, the 
trend registered in RTE-2 was confirmed, in that 
there was a marked difference in the performances 
obtained in different task settings. 
In fact, the average accuracy achieved in the QA 
setting (0.71) was 20 points higher than that 
achieved in the IE setting (0.52); the average accu-
racy in the IR and Sum settings was 0.66 and 0.58 
respectively. In RTE-2 the best results were 
achieved in SUM, while the lower score was al-
ways recorded in IE. As already pointed out by 
Bar-Haim (2006), these differences should be fur-
ther investigated, as they could lead to a sensible 
improvement of the performance. 
As for the LONG pairs, which represented a 
new element of this year?s challenge, no substan-
tial difference was noted in the systems? perform-
ances: the average accuracy over the long pairs 
was 58.72%, compared to 61.93% over the short 
ones.  
4 Conclusions and future work 
 
At its third round, the Recognizing Textual En-
tailment task has reached a noticeable level of ma-
turity, as the very high interest in the NLP commu-
nity and the continuously increasing number of 
participants in the challenges demonstrate. The 
relevance of Textual Entailment Recognition to 
different applications, such as the AVE5 track at 
QA at CLEF6, has also been acknowledged. Fur-
thermore, the debates and the numerous publica-
tions about the Textual Entailment have contrib-
uted to the better understanding the task and its 
nature.  
To keep a good balance between the consoli-
dated main task and the need for moving forward, 
longer texts were introduced in the dataset, in order 
to make the task more challenging, and a pilot task 
was proposed. The Third RTE Challenge have also 
confirmed that the methodology for the creation of 
the datasets, developed in the first two campaigns, 
is robust. Overall, the transition of the challenge 
coordination from Bar-Ilan ?which organized the 
first two challenges- to CELCT was successful, 
though some problems were encountered, espe-
cially in the preparation of the data set. The sys-
                                                 
5
 http://nlp.uned.es/QA/ave/. 
6
 http://clef-qa.itc.it/. 
tems which took part in RTE-3 showed that the 
technology applied to Entailment Recognition has 
made significant progress, confirmed by the results, 
which were generally better than last year. In par-
ticular, visible progress in defining several new 
principled scenarios for RTE was represented, such 
as Hickl?s commitment-based approach, Bar 
Haim?s proof system, Harmeling?s probabilistic 
model, and Standford?s use of Natural Logic. 
If, on the one hand, the success that RTE has 
had so far is very encouraging, on the other, it in-
cites to overcome certain current limitations, and to 
set realistic and, at the same time, stimulating goals 
for the future. First at all, theoretical refinements 
both of the task and the models applied to it need 
to be developed. In particular, more efforts are re-
quired to improve knowledge acquisition, as little 
progress has been made on this front so far. Also 
the data set generation and the evaluation method-
ology  need to be refined and extended. A major 
problem in the current setting of the data collection 
is that the distribution of the examples is arbitrary 
to a large extent, being determined by manual se-
lection. Therefore new evaluation methodologies, 
which can reflect realistic distributions should be 
investigated, as well as the possibility of evaluating 
Textual Entailment Recognition within additional 
concrete application scenarios, following the spirit 
of the QA Answer Validation Exercise.  
 
 
Acknowledgments 
 
The following sources were used in the preparation 
of the data: 
 
? PowerAnswer question answering system, from 
Language Computer Corporation, provided by Dan 
Moldovan and Marta Tatu. 
http://www.languagecomputer.com/solutions/question answer-
ing/power answer/ 
 
? Cicero Custom and Cicero Relation information 
extraction systems, from Language Computer Cor-
poration, provided by Sanda M. Harabagiu, An-
drew Hickl, John Lehmann and  and Paul Aarseth. 
http://www.languagecomputer.com/solutions/information_ext
action/cicero/index.html 
 
? Columbia NewsBlaster multi-document summa-
rization system, from the Natural Language Proc-
8
essing group at Columbia University?s Departmen-
tof Computer Science. 
http://newsblaster.cs.columbia.edu/ 
 
? NewsInEssence multi-document summarization 
system provided by Dragomir R. Radev and Jahna 
Otterbacher from the Computational Linguistics 
and Information Retrieval research group, Univer-
sity of Michigan. 
http://www.newsinessence.com 
 
? New York University?s information extraction 
system, provided by Ralph Grishman, Department 
of Computer Science, Courant Institute of Mathe-
matical Sciences, New York University. 
 
? MUC-4 information extraction dataset, from the 
National Institute of Standards and Technology 
(NIST).  
http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 
 
? ACE 2004 information extraction templates, 
from the National Institute of Standards and Tech-
nology (NIST). 
http://www.nist.gov/speech/tests/ace/ 
 
? TREC IR queries and TREC-QA question collec-
tions, from the National Institute of Standards and 
Technology (NIST). 
http://trec.nist.gov/ 
 
? CLEF IR queries and CLEF-QA question collec-
tions, from DELOS Network of Excellence  
for Digital Libraries. 
 http://www.clef-campaign.org/, http://clef-qa.itc.it/ 
 
? DUC 2005 annotated peers, from Columbia Uni-
versity, NY, provided by Ani Nenkova. 
http://www1.cs.columbia.edu/~ani/DUC2005/ 
 
We would like to thank the people and organiza-
tions that made these sources available for the 
challenge. In addition, we thank Idan Szpektor and 
Roy Bar Haim from Bar-Ilan University  for their 
assistance and advice, and Valentina Bruseghini 
from CELCT for managing the RTE-3 website. 
 
We would also like to acknowledge the people 
and organizations involved in creating and annotat-
ing the data: Pamela Forner, Errol Hayman, Cam-
eron Fordyce from CELCT and Courtenay 
Hendricks, Adam Savel and Annika Hamalainen 
from the Butler Hill Group, which was funded by 
Microsoft Research. 
 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. We wish to thank the managers of the 
PASCAL challenges program, Michele Sebag and 
Florence d?Alche-Buc, for their efforts and sup-
port, which made this challenge possible. We also 
thank David Askey, who helped manage the RTE 3 
website.  
 
References 
 
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, 
Danilo Giampiccolo, Bernardo Magnini and Idan 
Szpektor. 2006. The Second PASCAL Recognizing 
Textual Entailment Challenge. In Proceedings of the 
Second PASCAL Challenges Workshop on Recog-
nizing Textual Entailment, Venice, Italy. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognizing Textual Entailment 
Challenge. In Qui?onero-Candela et al, editors, 
MLCW 2005, LNAI Volume 3944, pages 177-190. 
Springer-Verlag. 
J. R. Landis and G. G. Koch. 1997. The measurements 
of observer agreement for categorical data. Biomet-
rics, 33:159?174. 
Rebecca Passonneau, Ani Nenkova., Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the 
pyramid method in DUC 2005. In Proceedings of the 
Document Understanding Conference (DUC 05), 
Vancouver, B.C., Canada. 
Ellen M. Voorhees and Donna Harman. 1999. Overview 
of the seventh text retrieval conference. In Proceed-
ings of the Seventh Text Retrieval Conference 
(TREC-7). NIST Special Publication. 
 
 
9
The Italian Lexical Sample Task at SENSEVAL-3
Bernardo Magnini, Danilo Giampiccolo and Alessandro Vallin
ITC-Irst, Istituto per la Ricerca Scientifica e Tecnologica
Via Sommarive, 18 ? 38050 Trento, Italy
{magnini, giampiccolo, vallin}@itc.it
Abstract
The Italian lexical sample task at
SENSEVAL-3 provided a framework to
evaluate supervised and semi-supervised
WSD systems. This paper reports on the
task preparation ? which offered the op-
portunity to review and refine the Italian
MultiWordNet ? and on the results of the
six participants, focussing on both the
manual and automatic tagging procedures.
1 Introduction
The task consisted in automatically determining
the correct meaning of a word within a given con-
text (i.e. a short text snippet). Systems? results
were compared on the one hand to those achieved
by human annotators (upper bound), and on the
other hand to those returned by a basic algorithm
(baseline).
In the second section of this paper an overview
of the task preparation is given and in the follow-
ing one the main features of the participating sys-
tems are briefly outlined and the results of the
evaluation exercise are presented.
In the conclusions we give an overall judgement
of the outcome of the task, suggesting possible im-
provements for the next campaign.
2 Manual Annotation
A collection of manually labeled instances was
built for three main reasons:
1. automatic evaluation (using the Scorer2 pro-
gram) required a Gold Standard list of senses
provided by human annotators;
2. supervised WSD systems need a labeled set of
training data, that in our case was twice larger
than the test set;
3. manual semantic annotation is a time-
consuming activity, but SENSEVAL repre-
sents the framework to build reusable bench-
mark resources. Besides, manual sense tagging
entails the revision of the sense inventory,
whose granularity does not always satisfy an-
notators.
2.1 Corpus and Words Choice
The document collection from which the anno-
tators selected the text snippets containing the
lemmata to disambiguate was the macro-balanced
section of the Meaning Italian Corpus (Bentivogli
et al, 2003). This corpus is an open domain col-
lection of newspaper articles that contains about 90
million tokens covering a time-spam of 4 years
(1998-2001). The corpus was indexed in order to
browse it with the Toolbox for Lexicographers
(Giuliano, 2002), a concordancer that enables tag-
gers to highlight the occurrences of a token within
a context.
Two taggers chose 45 lexical entries (25 nouns,
10 adjectives and 10 verbs) according to their
polysemy in the sense inventory, their polysemy in
the corpus and their frequency (Edmonds, 2000).
The words that had already been used at
SENSEVAL-2 were avoided. Ten words were
shared with the Spanish, Catalan and Basque lexi-
cal sample tasks.
Annotators were provided with a formula that
indicated the number of labeled instances for each
lemma1, so they checked that the words were con-
1 No. of labeled instances for each lemma = 75 + (15*no. of attested senses) +
(7* no. of attested multiwords), where 75 is a fixed number of examples distrib-
uted over all the attested senses.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
siderably frequent and polysemous before starting
to tag and save the instances.
As a result, average polysemy attested in the la-
beled data turned out to be quite high: six senses
for the nouns, six for the adjectives and seven for
the verbs.
2.2 Sense Inventory and Manual Tagging
Differently from the Italian lexical sample task
at SENSEVAL-2, where the instances were tagged
according to ItalWordNet (Calzolari et al, 2002),
this year annotators used the Italian MultiWord-
Net, (hereafter MWN) developed at ITC-Irst (Pi-
anta, 2002). This lexical-semantic database
includes about 42,000 lemmata and 60,000 word
senses, corresponding to 34,000 synsets. Instead of
distributing to participants the senses of each
lemma and a limited hierarchical data structure of
the semantic relations of the senses (as happened at
SENSEVAL-2), the entire resource was made
available. Nevertheless, none of the six participat-
ing systems, being supervised, actually needed
MWN.
The annotators? task was to tag one occurrence
of each selected word in all the saved instances,
assigning only one sense drawn from the Italian
MWN. The Toolbox for Lexicographers enabled
annotators to browse the document collection and
to save the relevant text snippets, while a graphical
interface2 was used to annotate the occurrences,
storing them in a database. Generally, instances
consisted of the sentence containing the ambiguous
lemma, with a preceding and a following sentence.
Nevertheless, annotators tended to save the mini-
mal piece of information that a human would need
to disambiguate the lemma, which was often
shorter than three sentences.
The two annotators were involved simultane-
ously: firstly, each of them saved a part of the in-
stances and tagged the occurrences, secondly they
tagged the examples that had been chosen by the
other one.
More importantly, they interacted with a lexi-
cographer, who reviewed the sense inventory
whenever they encountered difficulties. Sometimes
there was an overlap between two or more word
senses, while in other cases MWN needed to be
enriched, adding new synsets, relations or defini-
2 This tool was designed and developed by Christian Girardi at ITC-Irst, Trento,
Italy.
tions. All the 45 lexical entries we considered were
thoroughly reviewed, so that word senses were as
clear as possible to the annotators. On the one
hand, the revision of MWN made manual tagging
easier, while on the other hand it led to a high Inter
Tagger Agreement (that ranged between 73 and 99
per cent), consequently reflected in the K statistics
(that ranged between 0.68 and 0.99).
Table 1 below summarizes the results of the
manual tagging.
Table 1. Manual Annotation Results
Once the instances had been collected and
tagged by both the annotators, we asked them to
discuss the examples about which they disagreed
and to find a definitive meaning for them.
Since the annotators built the corpus while tag-
ging, they tended to choose occurrences whose
meaning was immediately straightforward, avoid-
ing problematic cases. As a consequence, the ITA
turned out to be so high and the distribution of the
senses in the labeled data set did not reflect the
actual frequency in the Italian language, which
may have affected the systems? performance.
Annotators assigned different senses to 674 in-
stances over a total of 7584 labeled examples.
Generally, disagreement depended on trivial mis-
takes, and in most cases one of the two assigned
meanings was chosen as the final one. Neverthe-
less, in 46 cases the third and last annotation was
different from the previous two, which could dem-
onstrate that a few word senses were not com-
pletely straightforward even after the revision of
the sense inventory.
For example, the following instance for the
lemma ?vertice? (vertex, acme, peak) was anno-
tated in three different ways:
La struttura lavorativa ? spiega Grandi ? ha un carattere paramilita-
re. Al vertice della piramide c?? il direttore, poi i manager, quelli con
la cravatta e la camicia a mezze maniche.
Annotator 1 tagged with sense 2 (Factotum,
?the highest point of something?), while annotator
2 decided for sense 4 (Geometry, ?the point of in-
Average
polysemy
in MWN
Average
polysemy in
the labeled set
I.T.A.
Average
K
# training
examples
# test
examples
25 nouns 10 6 0.9 2835 1343
10 adjectives 8 6 0.89 1111 524
10 verbs 9 7 0.89 1199 572
tersection of lines or the point opposite the base of
a figure?) because the text refers to the vertex of a
pyramid. Actually, the snippet reported this ab-
stract image to describe the structure of an enter-
prise, so in the end the two taggers opted for sense
5 (Administration, ?the group of the executives of
a corporation?). Therefore, subjectivity in manual
tagging was considerably reduced by adjusting the
sense repository and selecting manually each sin-
gle instance, but it could not be eliminated.
3 Automatic Annotation
We provided participants with three data sets:
labeled training data (twice larger than the test set),
unlabeled training data (about 10 times the labeled
instances) and test data. In order to facilitate par-
ticipation, we PoS-tagged the labeled data sets us-
ing an Italian version of the TnT PoS-tagger
(Brants, 2000), trained on the Elsnet corpus.
3.1 Participants? results
Three groups participated in the Italian lexical
sample task, testing six systems: two developed by
ITC-Irst - Italy - (IRST-Kernels and IRST-Ties),
three by Swarthmore College - U.S.A. - (swat-hk-
italian, Italian-swat_hk-bo and swat-italian) and
one by UNED - Spain.
Table 2 below reports the participants? results,
sorted by F-measure.
system precision recall attempted F-measure
IRST-Kernels 0.531 0.531 100% 0.531
swat-hk-italian 0.515 0.515 100% 0.515
UNED 0.498 0.498 100% 0.498
italian-swat_hk-bo 0.483 0.483 100% 0.483
swat-italian 0.465 0.465 100% 0.465
IRST-Ties 0.552 0.309 55.92% 0.396
baseline 0.183 0.183 100% 0.183
Table 2. Automatic Annotation Results (fine-grained score)
The baseline results were obtained running a sim-
ple algorithm that assigned to the instances of the
test set the most frequent sense of each lemma in
the training set. All the systems outperformed the
baseline and obtained similar results. Compared to
the baseline of the other Lexical Sample tasks, ours
is much lower because we interpreted the formula
described above (see footnote 1), and tagged the
same number of instances for all the senses of each
lemma disregarding their frequency in the docu-
ment collection. As a result, the distribution of the
examples over the attested senses did not reflect
the one in natural language, which may have af-
fected the systems? performance.
While at SENSEVAL-2 test set senses were
clustered in order to compute mixed- and coarse-
grained scores, this year we decided to return just
the fine-grained measure, where an automatically
tagged instance is correct only if the sense corre-
sponds to the one assigned by humans, and wrong
otherwise (i.e. one-to-one mapping).
There are different sense clustering methods,
but grouping meanings according to some sort of
similarity is always an arbitrary decision. We in-
tended to calculate a domain-based coarse-grained
score, where word senses were clustered according
to the domain information provided in WordNet
Domains (Magnini and Cavagli?, 2000). Unfortu-
nately, this approach would have been significant
with nouns, but not with adjectives and verbs, that
belong mostly to the generic Factotum domain, so
we discarded the idea.
All the six participating systems were super-
vised, which means they all used the training data
set and no one utilized either unlabelled instances
or the lexical database. UNED used also SemCor
as an additional source of training examples.
IRST-Kernels system exploited Kernel methods
for pattern abstraction and combination of different
knowledge sources, in particular paradigmatic and
syntagmatic information, and achieved the best F-
measure score.
IRST-Ties, a generalized pattern abstraction
system originally developed for Information Ex-
traction tasks and mainly based on the boosted
wrapper induction algorithm, used only lemma and
POS as features. Proposed as a ?baseline? system
to discover syntagmatic patterns, it obtained a quite
low recall (about 55 per cent), which affected the
F-measure, but proved to be the most precise sys-
tem.
Swarthmore College wrote three supervised
classifiers: a clustering system based on cosine
similarity, a decision list system and a naive bayes
classifier. Besides, Swarthmore group took advan-
tage of two systems developed at the Hong Kong
Polytechnic University: a maximum entropy classi-
fier and system which used boosting (Italian-
swat_hk-bo). The run swat-hk-italian joined all the
five classifiers according to a simple majority-vote
scheme, while swat-hk-italian did the same using
only the three classifiers developed in Swarthmore.
The system presented by the UNED group em-
ployed similarity as a learning paradigm, consid-
ering the co-occurrence of different nouns and
adjectives.
3.2 General Remarks on Task Complexity
As we mentioned above, the 45 words for the
Italian lexical sample task were chosen according
to their polysemy and frequency. We addressed
difficult words, that had at least 5 senses in MWN.
Actually, polysemy does not seem to be directly
related to systems? results (Calzolari, 2002), in fact
the average F-measure of our six runs for the
nouns (0.512) was higher than for adjectives
(0.472) and verbs (0.448), although the former had
more attested senses in the labeled data.
Complexity in returning the correct sense seems
to depend on the blurred distinction between simi-
lar meanings rather than on the number of senses
themselves. If we consider the nouns ?attacco?
(attack) and ?esecuzione? (performance, execu-
tion), for which the systems obtained the worst and
one of the best average results respectively, we
notice that the 4 attested senses of ?esecuzione?
were clearly distinguished and referred to different
domains (Factotum, Art, Law and Politics), while
the 6 attested senses of ?attacco? were more subtly
defined. Senses 2, 7 and 11 were very difficult to
discriminate and often appeared in metaphorical
contexts. Senses 5 and 6, for their part, belong to
the Sport domain and are not always easy to dis-
tinguish.
4 Conclusions
The results of the six systems participating in
the evaluation exercise showed some improve-
ments compared to the average performance at
SENSEVAL-2, though data sets and sense reposi-
tories were considerably different.
We are pleased with the successful outcome of
the experiments in terms of participation, although
regrettably no system exploited the unlabeled
training set, which was intended to offer a less
time-consuming resource. On the other hand, the
labeled instances that have been collected represent
a useful and reusable benchmark.
As a final remark we think it could be interest-
ing to consider the actual distribution of word
senses in Italian corpora in collecting the examples
for the next campaign.
Acknowledgements
We would like to thank Christian Girardi and
Oleksandr Vagin for their technical support;
Claudio Giuliano and the Ladin Cultural Centre
for the use of their Toolbox for Lexicographers;
Pamela Forner, Daniela Andreatta and Elisabetta
Fauri for the revision of the Italian MWN and on
the semantic annotation of the examples; and Luisa
Bentivogli and Emanuele Pianta for their precious
suggestions during the manual annotation.
References
Luisa Bentivogli, Christian Girardi and Emanuele
Pianta. 2003. The MEANING Italian Corpus. In Pro-
ceedings of the Corpus Linguistics 2003 conference,
Lancaster, UK: 103-112.
Francesca Bertagna, Claudia Soria and Nicoletta Cal-
zolari. 2001. The Italian Lexical Sample Task. In
Proceedings of SENSEVAL-2: Second International
Workshop on Evaluating Word Sense Disambigua-
tion Systems, Toulouse, France: 29-32.
Thorsten Brants. 2000. TnT - a Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA: 224-231.
Nicoletta Calzolari, Claudia Soria, Francesca Bertagna
and Francesco Barsotti. 2002. Evaluating lexical re-
sources using SENSEVAL. Natural Language Engi-
neering, 8(4): 375-390.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2.
(http://www.sle.sharp.co.uk/SENSEVAL2/archive/in
dex.htm)
Claudio Giuliano. 2002. A Toolbox for Lexicographers
In Proceedings of the tenth EURALEX International
Congress, Copenhagen, Denmark: 113-118.
Bernardo Magnini and Gabriela Cavagli?. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC-2000, Athens, Greece: 1413-1418.
Emanuele Pianta, Luisa Bentivogli and Christian
Girardi. 2002. MultiWordNet: developing an aligned
multilingual database. In Proceedings of the First
International Conference on Global WordNet, My-
sore, India: 293-302.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Divide and Conquer:
Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Abstract
We address the creation of cross-lingual tex-
tual entailment corpora by means of crowd-
sourcing. Our goal is to define a cheap and
replicable data collection methodology that
minimizes the manual work done by expert
annotators, without resorting to preprocess-
ing tools or already annotated monolingual
datasets. In line with recent works empha-
sizing the need of large-scale annotation ef-
forts for textual entailment, our work aims to:
i) tackle the scarcity of data available to train
and evaluate systems, and ii) promote the re-
course to crowdsourcing as an effective way
to reduce the costs of data collection without
sacrificing quality. We show that a complex
data creation task, for which even experts usu-
ally feature low agreement scores, can be ef-
fectively decomposed into simple subtasks as-
signed to non-expert annotators. The resulting
dataset, obtained from a pipeline of different
jobs routed to Amazon Mechanical Turk, con-
tains more than 1,600 aligned pairs for each
combination of texts-hypotheses in English,
Italian and German.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of Textual Entailment
(Dagan and Glickman, 2004). The task consists of
deciding, given a text (T) and an hypothesis (H) in
different languages, if the meaning of H can be in-
ferred from the meaning of T. As in other NLP appli-
cations, both for monolingual and cross-lingual TE,
the availability of large quantities of annotated data
is an enabling factor for systems development and
evaluation. Until now, however, the scarcity of such
data on the one hand, and the costs of creating new
datasets of reasonable size on the other, have repre-
sented a bottleneck for a steady advancement of the
state of the art.
In the last few years, monolingual TE corpora for
English and other European languages have been
created and distributed in the framework of sev-
eral evaluation campaigns, including the RTE Chal-
lenge1, the Answer Validation Exercise at CLEF2,
and the Textual Entailment task at EVALITA3. De-
spite the differences in the design of the tasks, all
the released datasets were collected through simi-
lar procedures, always involving expensive manual
work done by expert annotators. Moreover, in the
data creation process, large amounts of hand-crafted
T-H pairs often have to be discarded in order to re-
tain only those featuring full agreement, in terms of
the assigned entailment judgements, among multiple
annotators. The amount of discarded pairs is usually
high, contributing to increase the costs of creating
textual entailment datasets4.
The issues related to the shortage of datasets and
the high costs for their creation are more evident
1http://www.nist.gov/tac/2011/RTE/
2http://nlp.uned.es/clef-qa/ave/
3http://www.evalita.it/2009/tasks/te
4For instance, in the first five RTE Challenges, the aver-
age effort needed to create 1,000 pairs featuring full agreement
among 3 annotators was around 2.5 person-months. Typically,
around 25% of the original pairs had to be discarded during the
process, due to low inter-annotator agreement (Bentivogli et al,
2009).
670
in the CLTE scenario, where: i) the only dataset
currently available is an English-Spanish corpus ob-
tained by translating the RTE-3 corpus (Negri and
Mehdad, 2010), and ii) the application of the stan-
dard methods adopted to build RTE pairs requires
proficiency in multiple languages, thus significantly
increasing the costs of the data creation process.
To address these issues, in this paper we devise
a cost-effective methodology to create cross-lingual
textual entailment corpora. In particular, we focus
on the following problems:
(1) Is it possible to collect T-H pairs minimizing
the intervention of expert annotators? To address
this question, we explore the feasibility of crowd-
sourcing the corpus creation process. As a contri-
bution beyond the few works on TE/CLTE data ac-
quisition, we define an effective methodology that:
i) does not involve experts in the most complex (and
costly) stages of the process, ii) does not require pre-
processing tools, and iii) does not rely on the avail-
ability of already annotated RTE corpora.
(2) How can we guarantee good quality of the col-
lected data at a low cost? We address the quality
control issue through the decomposition of a com-
plex task (i.e. creating and annotating entailment
pairs) into smaller sub-tasks. Complex tasks are usu-
ally hard to explain in a simple way understandable
to non-experts, difficult to accomplish, and not suit-
able for the application of the quality-check mecha-
nisms provided by current crowdsourcing services.
Our ?divide and conquer? solution represents the
first attempt to address a complex task involving
content generation and labelling through the defini-
tion of a cheap and reliable pipeline of simple tasks
which are easy to define, accomplish, and control.
(3) Can we adapt such methodology to collect
cross-lingual T-H pairs? We tackle this question
by separating the problem of creating and annotating
TE pairs from the issues related to the multilingual
dimension. Our solution builds on the assumption
that entailment annotations can be projected across
aligned T-H pairs in different languages. In this
case, a complex multilingual task is reduced to a se-
quence of simpler subtasks where the most difficult
one, the generation of entailment pairs, is entirely
monolingual. Besides ensuring cost-effectiveness,
our solution allows us to overcome the problem of
finding workers that are proficient in multiple lan-
guages. Moreover, since the core monolingual tasks
of the process are carried out by manipulating En-
glish texts, we are able to address the very large
community of English speaking workers, with a
considerable reduction of costs and execution time.
Finally, as a by-product of our method, the acquired
pairs are fully aligned for all language combinations,
thus enabling meaningful comparisons between sce-
narios of different complexity (monolingual TE, and
CLTE between close or distant languages).
We believe that, in the same spirit of recent works
promoting large-scale annotation efforts around en-
tailment corpora (Sammons et al, 2010; Bentivogli
et al, 2010), the proposed approach and the resulting
dataset5 will contribute to meeting the strong need
for resources to develop and evaluate novel solutions
for textual entailment.
2 Related Works
Crowdsourcing services, such as Amazon Mechan-
ical Turk6 (MTurk) and CrowdFlower7, have been
recently used with success for a variety of NLP ap-
plications (Callison-Burch and Dredze, 2010). The
idea is that the acquisition and annotation of large
amounts of data needed to train and evaluate NLP
tools can be carried out in a cost-effective manner
by defining simple Human Intelligence Tasks (HITs)
routed to a crowd of non-expert workers (aka ?Turk-
ers?) hired through on-line marketplaces.
As regards textual entailment, the first work ex-
ploring the use of crowdsourcing services for data
annotation is described in (Snow et al, 2008), which
shows high agreement between non-expert annota-
tions of the RTE-1 dataset and existing gold standard
labels assigned by expert labellers.
Focusing on the actual generation of monolingual
entailment pairs, (Wang and Callison-Burch, 2010)
experiments the use of MTurk to collect facts and
counter facts related to texts extracted from an ex-
isting RTE corpus annotated with named entities.
Taking a step beyond the task of annotating exist-
5The CLTE corpora described in this paper will be made
freely available for research purposes through the website of
the funding EU Project CoSyne (http://www.cosyne.eu/).
6https://www.mturk.com/
7Although MTurk is directly accessible only to US citizens,
the CrowdFlower service (http://crowdflower.com/) provides an
interface to MTurk for non-US citizens.
671
ing datasets, and showing the feasibility of involving
non-experts also in the generation of TE pairs, this
approach is more relevant to our objectives. How-
ever, at least two major differences with our work
have to be remarked. First, they still use avail-
able RTE data to obtain a monolingual TE corpus,
whereas we pursue the more ambitious goal of gen-
erating from scratch aligned CLTE corpora for dif-
ferent language combinations. To this aim, we do
not resort to already annotated data, nor language-
specific preprocessing tools. Second, their approach
involves qualitative analysis of the collected data
only a posteriori, after manual removal of invalid
and trivial generated hypotheses. In contrast, our
approach integrates quality control mechanisms at
all stages of the data collection/annotation process,
thus minimizing the recourse to experts to check the
quality of the collected material.
Related research in the CLTE direction is re-
ported in (Negri and Mehdad, 2010), which de-
scribes the creation of an English-Spanish corpus
obtained from the RTE-3 dataset by translating the
English hypotheses into Spanish. Translations have
been crowdsourced adopting a methodology based
on translation-validation cycles, defined as separate
HITs. Although simplifying the CLTE corpus cre-
ation problem, which is recast as the task of translat-
ing already available annotated data, this solution is
relevant to our work for the idea of combining gold
standard units and ?validation HITS? as a way to
control the quality of the collected data at runtime.
3 Quality Control of Crowdsourced Data
The design of data acquisition HITs has to take into
account several factors, each having a considerable
impact on the difficulty of instructing the workers,
the quality and quantity of the collected data, the
time and overall costs of the acquisition. A major
distinction has to be made between jobs requiring
data annotation, and those involving content gener-
ation. In the former case, Turkers are presented with
the task of labelling input data referring to a fixed
set of possible values (e.g. making a choice between
multiple alternatives, assigning numerical scores to
rank the given data). In the latter case, Turkers are
faced with creative tasks consisting in the production
of textual material (e.g. writing a correct translation,
or a summary of a given text).
The ease of controlling the quality of the acquired
data depends on the nature of the job. For annotation
jobs, quality control mechanisms can be easily set up
by calculating Turkers? agreement, by applying vot-
ing schemes, or by adding hidden gold units to the
data to be annotated8. In contrast, the quality of the
results of content generation jobs is harder to assess,
due to the fact that multiple valid results are accept-
able (e.g. the same content can be expressed, trans-
lated, or summarized in different ways). In such sit-
uations the standard quality control mechanisms are
not directly applicable, and the detection of errors
requires either costly manual verification at the end
of the acquisition process, or more complex and cre-
ative solutions integrating HITs for quality check.
Most of the approaches to content generation pro-
posed so far rely on post hoc verification to fil-
ter out undesired low-quality data (Mrozinski et al,
2008; Mihalcea and Strapparava, 2009; Wang and
Callison-Burch, 2010). The few solutions integrat-
ing validation HITs address the translation of sin-
gle sentences, a task that is substantially different
from ours (Negri and Mehdad, 2010; Bloodgood and
Callison-Burch, 2010). Compared to sentence trans-
lation, the task of creating CLTE pairs is both harder
to explain without recurring to notions that are dif-
ficult to understand to non-experts (e.g. ?seman-
tic equivalence?, ?unidirectional entailment?), and
harder to execute without mastering these notions.
To tackle these issues the ?divide and conquer? ap-
proach described in the next section consists in the
decomposition of a difficult content generation job
into easier subtasks that are: i) self-contained and
easy to explain, ii) easy to execute without any NLP
expertise, and iii) suitable for the integration of a va-
riety of runtime control mechanisms (regional qual-
ifications, gold units, ?validation HITs?) able to en-
sure a good quality of the collected material.
8Both MTurk and CrowdFlower provide means to check
workers? reliability, and weed out untrusted ones without money
waste. These include different types of qualification mecha-
nisms, the possibility of giving work only to known trusted
Turkers (only with MTurk), and the possibility of adding hid-
den gold standard units in the data to be annotated (offered as a
built-in mechanism only by CrowdFlower).
672
4 CLTE Corpus Creation Methodology
Our approach builds on a pipeline of HITs routed to
MTurk?s workforce through the CrowdFlower inter-
face. The objective is to collect aligned T-H pairs
for different language combinations, reproducing an
RTE-like annotation style. However, our annotation
is not limited to the standard RTE framework, where
only unidirectional entailment from T to H is con-
sidered. As a useful extension, we annotate any pos-
sible entailment relation between the two text frag-
ments, including: i) bidirectional entailment (i.e.
semantic equivalence between T and H), ii) unidi-
rectional entailment from T to H, and iii) unidirec-
tional entailment from H to T. The resulting pairs
can be easily used to generate not only standard RTE
datasets9, but also general-purpose collections fea-
turing multi-directional entailment relations.
4.1 Data Acquisition and Annotation
We collect large amounts of CLTE pairs carrying out
the most difficult part of the process (the creation of
entailment-annotated pairs) at a monolingual level.
Starting from a set of parallel sentences in n lan-
guages, (e.g. L1, L2, L3), n entailment corpora are
created: one monolingual (L1/L1), and n-1 cross-
lingual (L1/L2, and L1/L3).
The monolingual corpus is obtained by modify-
ing the sentences only in one language (L1). Orig-
inal and modified sentences are then paired and an-
notated to form an entailment dataset for L1. The
CLTE corpora are obtained by combining the mod-
ified sentences in L1 with the original sentences in
L2 and L3, and projecting to the multilingual pairs
the annotations assigned to the monolingual pairs.
In principle, only two stages of the process re-
quire crowdsourcing multilingual tasks, but do not
concern entailment annotations. The first one, at the
beginning of the process, aims to obtain a set of par-
allel sentences to start with, and can be done in dif-
ferent ways (e.g. crowdsourcing the translation of
a set of sentences). The second one, at the end of
the process, consists of translating the modified L1
sentences into other languages (e.g. L2) in order to
extend the corpus to cover new language combina-
9With the positive examples drawn from bidirectional and
unidirectional entailments from T to H, and the negative ones
drawn from unidirectional entailments from H to T.
tions (e.g. L2/L2, L2/L3).
The execution of the two ?multilingual? stages is
not strictly necessary but depends on: i) the avail-
ability of parallel sentences to start the process, and
ii) the actual objectives in terms of language combi-
nations to be covered10.
As regards the first stage, in this work we started
from a set of 467 English/Italian/German aligned
sentences extracted from parallel documents down-
loaded from the Cafebabel European Magazine11.
Concerning the second multilingual stage, we per-
formed only one round of translations from En-
glish to Italian to extend the 3 combinations ob-
tained without translations (ENG/ENG, ENG/ITA,
and ENG/GER) with the new language combina-
tions ITA/ITA, ITA/ENG, and ITA/GER.
STEP1:	 ?Sentence	 ?modifica?on	 ?(monolingual)	 ?
STEP3:	 ?Transla?on	 ?(mul?lingual)	 ?
GER	 ? ENG	 ?
ENG1	 ?
ITA	 ?
ITA1	 ? ITA	 ?ENG	 ? ENG1	 ?
STEP2:	 ?TE	 ?annota?on	 ?(monolingual)	 ?
Monolingual	 ?TE	 ?corpus	 ?
Cross-??lingual	 ?TE	 ?corpus	 ?
ENG1	 ?GER	 ?
ENG1	 ?ITA	 ?
TE	 ?annota?ns	 ?projec?n	 ?	 ?	 ?
ITA1	 ? GER	 ?
ITA1	 ? ENG	 ?
Figure 1: Corpus creation process.
The main steps of our corpus creation process,
depicted in Figure 1, can be summarized as follows:
Step1: Sentence modification. The original
English sentences (ENG) are modified through
(monolingual) generation HITs asking Turkers to:
i) preserve the meaning of the original sentences
using different surface forms, or ii) slightly change
their meaning by adding or removing content. Our
assumption, in line with (Bos et al, 2009), is that
10Starting from parallel sentences in n languages, the n cor-
pora obtained without recurring to translations can be aug-
mented, by means of translation HITs, to create the full set of
language combinations. Each round of translation adds 1 mono-
lingual corpus, and n-1 CLTE corpora.
11http://www.cafebabel.com/
673
another way to think about entailment is to consider
whether one text T1 adds new information to the
content of another text T: if so, then T is entailed by
T1.
The result of this phase is a set of texts (ENG1)
that can be of three types:
1. Paraphrases of the original ENG texts, that will
be used to create bidirectional entailment pairs
(ENG?ENG1);
2. More specific sentences (the outcome of
content addition operations), used to create
ENG?ENG1 unidirectional entailment pairs;
3. More general sentences (the outcome of
content removal operations), used to create
ENG?ENG1 unidirectional entailment pairs.
Step2: TE Annotation. Entailment pairs com-
posed of the original sentences (ENG) and the modi-
fied ones (ENG1) are used as input of (monolingual)
annotation HITs asking Turkers to decide which of
the two texts contains more information. As a re-
sult, each ENG/ENG1 pair is annotated as an ex-
ample of uni-/bidirectional entailment, and stored in
the monolingual English corpus. Since the original
ENG texts are aligned with the ITA and GER texts,
the entailment annotations of ENG/ENG1 pairs can
be projected to the other language pairs and the
ITA/ENG1 and GER/ENG1 pairs are stored in the
CLTE corpus. The possibility of projecting TE an-
notations is based on the assumption that the seman-
tic information is mostly preserved during the trans-
lation process. This particularly holds at the deno-
tative level (i.e. regarding the truth values of the
sentence) which is crucial to semantic inference. At
other levels (e.g. lexical) there might be slight se-
mantic variations which, however, are very unlikely
to play a crucial role in determining entailment rela-
tions.
Step3: Translation. The modified sentences
(ENG1) are translated into Italian (ITA1) through
(multilingual) generation HITs reproducing the ap-
proach described in (Negri and Mehdad, 2010). As
a result, three new datasets are produced by au-
tomatically projecting annotations: the monolin-
gual ITA/ITA1, and the cross-lingual ENG/ITA1 and
GER/ITA1.
Since the solution adopted for sentence transla-
tion does not present novelty factors, the remainder
of this paper will omit further details on it. Instead,
the following sections will focus on the more chal-
lenging tasks of sentence modification and TE anno-
tation.
4.2 Crowdsourcing Sentence Modification and
TE Annotation
Sentence modification and TE annotation have been
decomposed into a pipeline of simpler monolingual
English sub-tasks. Such pipeline, depicted in Figure
2, involves several types of generation/annotation
HITs designed to be easily understandable to non-
experts. Each HIT consists of: i) a set of instruc-
tions for a specific task (e.g. paraphrasing a text),
ii) the data to be manipulated (e.g. an English sen-
tence), and iii) a test to check workers? reliability.
To cope with the quality control issues discussed in
Section 3, such tests are realized using gold stan-
dard units, either hidden in the data to be annotated
(annotation HITs) or defined as test questions that
workers must correctly answer (generation HITs).
Moreover, regional qualifications are applied to all
HITs. As a further quality check, all the annotation
HITs consider Turkers? agreement as a way to filter
out low quality results (only annotations featuring
agreement among 4 out of 5 workers are retained).
The six HITs defined for each subtask can be de-
scribed as follows:
1. Paraphrase (generation). Modify an En-
glish text (ENG), in order to produce a semantically
equivalent variant (ENG1). As a reliability test, be-
fore creating the paraphrase workers are asked to
judge if two English sentences contain the same in-
formation.
2. Grammaticality (annotation). Decide if an
English sentence is grammatically correct. This val-
idation HIT represents a quality check of the out-
put of each generation task (i.e. paraphrasing, and
add/remove information HITs).
3. Bidirectional Entailment (annotation). De-
cide whether two English sentences, the original
ENG and the modified ENG1, contain the same in-
formation (i.e. are semantically equivalent).
4a. Add Information (generation). Modify an
English text to create a more specific one by adding
content. As a reliability test, before generating the
674
Figure 2: Sentence modification and TE annotation pipeline.
new sentence workers are asked to judge which of
two given English sentences is more detailed.
4b. Remove Information (generation). Mod-
ify an English text to create a more general one by
removing part of its content. As a reliability test, be-
fore generating the new sentence workers are asked
to judge which of two given English sentences is less
detailed.
5. Unidirectional Entailment (annotation). De-
cide which of two English sentences (the original
ENG, and a modified ENG1) provides more infor-
mation.
These HITs are combined in an iterative pro-
cess that alternates text generation, grammaticality
check, and entailment annotation steps. As a result,
for each original ENG text we obtain multiple ENG1
variants of the three types (paraphrases, more gen-
eral texts, and more specific texts) and, in turn, a set
of annotated monolingual (ENG/ENG1) TE pairs.
As described in Section 4.1, the resulting mono-
lingual English TE corpus (ENG/ENG1) is used to
create the following mono/cross-lingual TE corpora:
? ITA/ENG1, and GER/ENG1 (by projecting TE
annotations)
? ITA/ITA1, GER/ITA1, and ENG/ITA1 (by
translating the ENG1 texts into Italian, and pro-
jecting TE annotations)
5 The Resulting CLTE Corpora
This section provides a quantitative and qualita-
tive analysis of the results of our corpus creation
methodology, focusing on the collected ENG-ENG1
monolingual dataset. It has to be remarked that, as
an effect of the adopted methodology, all the obser-
vations and the conclusions drawn hold for the col-
lected CLTE corpora as well.
5.1 Quantitative Analysis
Table 1 provides some details about each step of the
pipeline shown in Figure 2. For each HIT the table
presents: i) the number of items (sentences, or pairs
of sentences) given in input, ii) the number of items
(sentences or annotations) produced as output, iii)
the number of items discarded when the agreement
threshold was not reached, iv) the number of entail-
ment pairs added to the corpus, v) the time (days and
hours) required by the MTurk workforce to complete
the job, and vi) the cost of the job.
In HIT-1 (Paraphrase) 1,414 paraphrases were
collected asking three different meaning-preserving
modifications of each of the 467 original sen-
tences12. From a practical point of view, such redun-
dancy aims to ensure a sufficient number of gram-
matically correct and semantically equivalent mod-
ified sentences. From a theoretical point of view,
12Often, crowdsourced jobs return a number of output items
that is slightly larger than required, due to the labour distribution
mechanism internal to MTurk.
675
HIT # Input items # Output items # Discarded items # Pairs to corpus MTurk time Cost ($)
1. Paraphrase 467 1,414 5d+10.5h 45.48
2. Grammaticality 1,414 1,326 88 (6.22%) 1d+15h 56.88
3. Bidirectional Ent. 1,326 1,213 113 (8.52%) 301 3d+2h 53.47
(yes=1,205 no=8)
4a. Add Info 452 916 3d 37.02
4b. Remove Info 452 923 2d+22h 29.73
2. Grammaticality 1,839 1,749 90 (4.89%) 2d+5h 64.37
3. Bidirectional Ent. 1,749 1,438 311 (17.78%) 148 3d+20.5h 70.52
(yes=148 no=1,290)
5. Unidirectional Ent. 1,298 1,171 127 (9.78%) 1,171 8.5h 78.24
(491 + 680)
TOTAL 721 1,620 22d+11h 435.71
Table 1: The monolingual dataset creation pipeline.
collecting many variants of a small pool of origi-
nal sentences aims to create pairs featuring different
entailment relations with similar superficial forms.
This, in principle, should allow to obtain a dataset
which requires TE systems to focus more on deeper
semantic phenomena than on the surface realization
of the pairs.
The collected paraphrases were sent as input to
HIT-2 (Grammaticality). After this validation HIT,
the number of acceptable paraphrases was reduced
to 1,326 (with 88 discarded sentences, correspond-
ing to 6.22% of the total).
The retained paraphrases were paired with their
corresponding original sentences, and sent to HIT-3
(Bidirectional Entailment) to be judged for semantic
equivalence. The pairs marked as bidirectional en-
tailments (1,205) were divided in three groups: 25%
of the pairs (301) were directly stored in the final
corpus, while the ENG1 paraphrases of the remain-
ing 75% (904) were equally distributed to the next
modification steps.
In both HIT-4a (Add Information) and HIT-4b
(Remove information) two new modified sentences
were asked for each of the 452 paraphrases received
as input. The sentences collected in these generation
tasks were respectively 916 and 923.
The new modified sentences were sent back to
HIT-2 (Grammaticality) and HIT-3 (Bidirectional
Entailment). As a result 1,438 new pairs were cre-
ated; out of these, 148 resulted to be bidirectional
entailments and were stored in the corpus.
Finally, the 1,298 entailment pairs judged as non-
bidirectional in the two previously completed HIT-
3 (8+1,290) were given as input to HIT-5 (Unidi-
rectional Entailment). The pairs which passed the
agreement threshold were classified according to the
judgement received, and stored in the corpus as uni-
directional entailment pairs.
The analysis of Table 1 allows to formulate
some considerations. First, the percentage of dis-
carded items confirms the effectiveness of decom-
posing complex generation tasks into simpler sub-
tasks that integrate validation HITs and quality
checks based on non-experts? agreement. In fact, on
average, around 9.5% of the generated items were
discarded without experts? intervention13. Second,
the amount of discarded items gives evidence about
the relative difficulty of each HIT. As expected,
we observe lower rejection rates, corresponding to
higher inter-annotator agreement, for grammatical-
ity HITs (5.55% on average) than for more complex
entailment-related tasks (12.02% on average).
Looking at costs and execution time, it is hard
to draw definite conclusions due to several factors
that influence the progress of the crowdsourced jobs
(e.g. the fluctuations of Turkers? performances, the
time of the day at which jobs are posted, the dif-
ficulty to set the optimal cost for a given HIT14).
On the one hand, as expected, the more creative
?Add Info? task proved to be more demanding than
the ?Remove Info?: even though it was paid more,
13Moreover, it is worthwhile noticing that around 20% of the
collected items were automatically rejected (and not paid) due
to failures on the gold standard controls created both for gener-
ation and annotation tasks.
14The payment for each HIT was set on the basis of a pre-
vious feasibility study aimed at determining the best trade-off
between cost and execution time. However, replicating our ap-
proach would not necessarily result in the same costs.
676
it still took little more time to be completed. On
the other hand, although the ?Unidirectional Entail-
ment? task was expected to be more difficult and
thus rewarded more than the ?Bidirectional Entail-
ment? one, in the end it took notably less time to
be completed. Nevertheless, the overall figures (435
USD, and about 22.5 days of MTurk work to com-
plete the process)15 clearly demonstrate the effec-
tiveness of the approach. Even considering the time
needed for an expert to manage the pipeline (i.e. one
week to prepare gold units, and to handle the I/O of
each HIT), these figures show that our methodology
provides a cheaper and faster way to collect entail-
ment data in comparison with the RTE average costs
reported in Section 1.
As regards the amount of data collected, the re-
sulting corpus contains 1,620 pairs with the fol-
lowing distribution of entailment relations: i) 449
bidirectional entailments, ii) 491 ENG?ENG1 uni-
directional entailments, and iii) 680 ENG?ENG1
unidirectional entailments.
It must be noted that our methodology does not
lead to the creation of pairs where some information
is provided in one text and not in the other, and vice-
versa, as Example 1 shows:
Example 1.
ENG: New theories were emerging in the field of psychology.
ENG1: New theories were rising, which announced a kind of
veiled racism.
These negative examples in both directions repre-
sent a natural extension of the dataset, relevant also
for specific application-oriented scenarios, and their
creation will be addressed in future work.
Besides the achievement of our primary objec-
tives, the adopted approach led to some interesting
by-products. First, the generated corpora are per-
fectly suitable to produce entailment datasets simi-
lar to those used in the traditional RTE evaluation
framework. In particular, considering any possible
entailment relation between two text fragments, our
annotation subsumes the one proposed in RTE cam-
paigns. This allows for the cost-effective genera-
tion of RTE-like annotations from the acquired cor-
15Although by projecting annotations the ENG1/ITA and
ENG1/GER CLTE corpora came for free, the ITA1/ITA,
ITA1/ENG, and ITA1/GER combinations created by crowd-
sourcing translations added 45 USD and approximately 5 days
to these figures.
pora by combining ENG?ENG1 and ENG?ENG1
pairs to form 940 positive examples (449+491),
keeping the 680 ENG?ENG1 as negative exam-
ples. Moreover, by swapping ENG and ENG1 in the
unidirectional entailment pairs, 491 additional nega-
tive examples and 680 positive examples can be eas-
ily obtained.
Finally, the output of HITs 1-2-3 in Table 1 rep-
resents per se a valuable collection of 1,205 para-
phrases. This suggests the great potential of crowd-
sourcing for paraphrase acquisition.
5.2 Qualitative Analysis
Through manual verification of more than 50% of
the corpus (900 pairs), a total number of 53 pairs
(5.9%) were found incorrect. The different errors
were classified as follows:
Type 1: Sentence modification errors. Generation
HITs are a minor source of errors, being responsible
for 10 problematic pairs. These errors are either in-
troduced by generating a false statement (Example
2), or by forming a not fully understandable, awk-
ward, or non-natural sentence (Example 3).
Example 2.
ENG: Kosovo was the subject of major riots in 1989.
ENG1: The Russian city of Kosovo was the subject of ...
Example 3.
ENG: Balat is the Kurdish-Armenian district of Instanbul.
ENG1: Balat is a place, which is the Kurdish-Armenian ...
Type 2: TE annotation errors. The notion of con-
taining more/less information, used in the ?Unidi-
rectional Entailment? HIT, can mostly be applied
straightforwardly to the entailment definition. How-
ever, the concept of ?more/less detailed?, which gen-
erally works for factual statements, in some cases is
not applicable. In fact, the MTurk workers have reg-
ularly interpreted the instructions about the amount
of information as concerning the quantity of con-
cepts contained in a sentence. This is not always cor-
responding to the actual entailment relation between
the sentences. As a consequence, 43 pairs featur-
ing wrong entailment annotations were encountered.
These errors can be classified as follows:
a) 13 pairs, where the added/removed information
changes the meaning of the sentence. In these cases,
the modified sentence was judged more/less specific
677
than the original one, leading to unidirectional en-
tailment annotation. On the contrary, in terms of
the standard entailment definition, the correct anno-
tation is ?no entailment? (as in Example 4, which
was annotated as ENG?ENG1):
Example 4.
ENG: If you decide to live in Bulgaria, you have to like
difficulties because they are not difficulties, they are challenges.
ENG1: You have to like difficulties as they are not difficulties,
they are challenges.
b) 10 pairs where the incorrect annotation is due to
a coreference problem, as in:
Example 5.
ENG: John Smith is the new CEO of the company.
ENG1: He is the new CEO of the company.
These pairs were labelled as unidirectional entail-
ments (in the example above ENG?ENG1), under
the assumption that a proper name is more specific
and informative than a pronoun. However, adher-
ing to the TE definition, co-referring expressions are
equivalent, and their realization does not play any
role in the entailment decision. This implies that the
correct entailment annotation is ?bidirectional?.
c) 9 pairs where the sentences are semantically
equivalent, but contain a piece of information which
is explicit in one sentence, and implicit in the other.
In these cases, Turkers judged the sentence contain-
ing the explicit mention as more specific, and thus
the pair was annotated as unidirectional entailment.
Example 6.
ENG: I hear the click of the trigger and the burst of bullets
reach me immediately.
ENG1: I hear the trigger and the burst of bullets reach me
instantly.
In Example 6, the expression ?the trigger? in ENG1
implicitly means ?the click of the trigger?, mak-
ing the two sentences equivalent, and the entailment
bidirectional (instead of ENG?ENG1).
d) 7 pairs where the information removed from or
added to the sentence is not relevant to the entail-
ment relation. In these cases, the modified sen-
tence was judged less/more specific than the origi-
nal one (and thus considered as unidirectional entail-
ment), even though the correct judgement is ?bidi-
rectional?, as in:
Example 7.
ENG: At the same time, AKP is struggling with its approach to
the EU.
ENG1: AKP is struggling with its approach to the European
Union.
e) 4 pairs where the added/removed information
concerns universally quantified general statements,
about which the interpretation of ?more/less spe-
cific? given by Turkers resulted in the wrong anno-
tation.
Example 8.
ENG: I think the success of multicultural couples depends on
the size of the cultural gap between the two partners
ENG1: I believe the success of the couples depends on the size
of the cultural gap between the 2 partners.
In Example 8, the additional information (?mul-
ticultural?) restricts the set to which it refers
(?couples?) making ENG entailed by ENG1, and
not vice versa as resulted from Turkers? annotation.
In light of this analysis, we conclude that the sen-
tence modification methodology proved to be suc-
cessful, as the low number of Type 1 errors shows.
Considering that the most expensive phase in the
creation of a TE dataset is the generation of the
pairs, this is a significant achievement. Differently,
the entailment assessment phase appears to be more
problematic, accounting for the majority of errors.
As shown by Type 2 errors, this is due to a par-
tial misalignment between the instructions given in
our HITs, and the formal definition of textual en-
tailment. For this reason, further experimentation
will explore different ways to instruct workers (e.g.
asking to consider proper names and pronouns as
equivalent) in order to reduce the amount of errors
produced. As a final remark, considering that in the
creation of a TE dataset the manual check of the an-
notated pairs represents a minor cost, even the in-
volvement of experts to filter out wrong annotations
would not decrease the cost-effectiveness of the pro-
posed methodology.
6 Conclusions
There is an increasing need of annotated data to
develop new solutions to the Textual Entailment
problem, explore new entailment-related tasks, and
set up experimental frameworks targeting real-world
applications. Following the recent trends promot-
ing annotation efforts that go beyond the estab-
lished RTE Challenge framework (unidirectional en-
tailment between monolingual T-H pairs), in this
678
paper we addressed the multilingual dimension of
the problem. Our primary goal was the creation of
large-scale collections of entailment pairs for differ-
ent language combinations. Besides that, we consid-
ered cost effectiveness and replicability as additional
requirements. To achieve our objectives, we devel-
oped a ?divide and conquer? methodology based on
crowdsourcing. Our approach presents several key
innovations with respect to the related works on TE
data acquisition. These include the decomposition
of a complex content generation task in a pipeline
of simpler subtasks accessible to a large crowd of
non-experts, and the integration of quality control
mechanisms at each stage of the process. The result
of our work is the first large-scale dataset contain-
ing both monolingual and cross-lingual corpora for
several combinations of texts-hypotheses in English,
Italian, and German. Among the advantages of our
method it is worth mentioning: i) the full alignment
between the created corpora, ii) the possibility to
easily extend the dataset to new languages, and iii)
the feasibility of creating general-purpose corpora,
featuring multi-directional entailment relations, that
subsume the traditional RTE-like annotation.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The au-
thors would like to thank Emanuele Pianta for the
helpful discussions, and Giovanni Moretti for the
valuable support in the creation of the CLTE dataset.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of TAC 2009.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of LREC 2010.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to Build Machine Translation
Evaluation Sets. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
Johan Bos, Fabio Massimo Zanzotto, and Marco Pennac-
chiotti. 2009. Textual Entailment at EVALITA 2009.
Proceedings of EVALITA 2009.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazons Me-
chanical Turk. Proceedings NAACL-2010 Workshop
on Creating Speech and Language Data With Amazons
Mechanical Turk.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of NAACL-HLT 2010.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. Proceedings of ACL-HLT
2011.
Rada Mihalcea and Carlo Strapparava. 2009. The Lie
Detector: Explorations in the Automatic Recognition
of Deceptive Language. Proceedings of ACL 2009.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a Why-Question Corpus for Devel-
opment and Evaluation of an Automatic QA-System.
Proceedings of ACL 2008.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk.
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. Ask Not What Textual Entailment Can Do for
You... Proceedings of ACL 2010.
Rion Snow, Brendan O?Connor, Daniel Jurafsky and An-
drew Y. Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. Proceedings of EMNLP 2008.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
679
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 399?407,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Semeval-2012 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
FBK-irst
Trento, Italy
mehdad@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the first round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2012. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (10 teams,
92 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization.
Cross-linguality represents a dimension of the TE
recognition problem that has been so far only par-
tially investigated. The great potential for integrat-
ing monolingual TE recognition components into
NLP architectures has been reported in several ar-
eas, including question answering, information re-
trieval, information extraction, and document sum-
marization. However, mainly due to the absence of
cross-lingual textual entailment (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. The
CLTE task aims at prompting research to fill this
gap. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set up MT systems. We
believe that all these resources can positively con-
tribute to develop inference mechanisms for multi-
lingual data.
Content synchronization represents a challenging
application scenario to test the capabilities of ad-
vanced NLP systems. Given two documents about
the same topic written in different languages (e.g.
Wiki pages), the task consists of automatically de-
tecting and resolving differences in the information
they provide, in order to produce aligned, mutually
enriched versions of the two documents. Towards
this objective, a crucial requirement is to identify the
information in one page that is either equivalent or
novel (more informative) with respect to the content
of the other. The task can be naturally cast as an
entailment recognition problem, where bidirectional
and unidirectional entailment judgments for two text
fragments are respectively mapped into judgments
about semantic equivalence and novelty. Alterna-
tively, the task can be seen as a machine translation
evaluation problem, where judgments about seman-
tic equivalence and novelty depend on the possibility
to fully or partially translate a text fragment into the
other.
399
Figure 1: ?bidirectional?, ?forward?, ?backward? and
?no entailment? judgments for SP/EN CLTE pairs.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as useful evidence to
draw entailment decisions), the standard sentence
and word alignment programs used in SMT offer a
strong baseline for CLTE. However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and large
room for mutual improvement.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments (see Figure 1 for
Spanish/English examples of each judgment):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in both direc-
tions;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset created for
the first round of the task.
3 Dataset description
Four CLTE corpora have been created for the fol-
lowing language combinations: Spanish/English
(SP-EN), Italian/English (IT-EN), French/English
(FR-EN), German/English (DE-EN). The datasets
are released in the XML format shown in Figure 1.
3.1 Data collection and annotation
The dataset was created following the crowdsourc-
ing methodology proposed in (Negri et al, 2011),
which consists of the following steps:
1. First, English sentences were manually ex-
tracted from copyright-free sources (Wikipedia
and Wikinews). The selected sentences repre-
sent one of the elements (T1) of each entail-
ment pair;
2. Next, each T1 was modified through crowd-
sourcing in various ways in order to ob-
tain a corresponding T2 (e.g. introduc-
ing meaning-preserving lexical and syntactic
changes, adding and removing portions of
text);
3. Each T2 was then paired to the original T1,
and the resulting pairs were annotated with one
of the four entailment judgments. In order to
reduce the correlation between the difference
in sentences? length and entailment judgments,
400
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.1 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgments, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were manually checked and corrected
when necessary. Only pairs with agreement between
two expert annotators were retained. The final result
is a multilingual parallel entailment corpus, where
T1s are in 5 different languages (i.e. English, Span-
ish, German, Italian, and French), and T2s are in En-
glish. It?s worth mentioning that the monolingual
English corpus, a by-product of our data collection
methodology, will be publicly released as a further
contribution to the research community.2
3.2 Dataset statistics
Each dataset consists of 1,000 pairs (500 for training
and 500 for test), balanced across the four entail-
ment judgments (bidirectional, forward, backward,
and no entailment).
For each language combination, the distribu-
tion of the four entailment judgments according to
length diff is shown in Figure 2. Vertical bars rep-
resent, for each length diff value, the proportion
of pairs belonging to the four entailment classes.
As can be seen, the length diff constraint applied
to the length difference in the monolingual English
1Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
2The cross-lingual datasets are already available for research
purposes at http://www.celct.it/resourcesList.
php. The monolingual English dataset will be publicly released
to non participants in July 2012.
pairs (step 3 of the creation process) is substantially
reflected in the cross-lingual datasets for all lan-
guage combinations. In fact, as shown in Table 1,
the majority of the pairs is always included in the
same length diff range (approximately [-5,+5]) and,
within this range, the distribution of the four classes
is substantially uniform. Our assumption is that such
data distribution makes entailment judgments based
on mere surface features such as sentence length in-
effective, thus encouraging the development of alter-
native, deeper processing strategies.
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
Table 1: CLTE pairs distribution within the -5/+5
length diff range.
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgments re-
turned by each system with those manually assigned
by human annotators. The metric used for systems?
ranking is accuracy over the whole test set, i.e. the
number of correct judgments out of the total number
of judgments in the test set. Additionally, we calcu-
lated precision, recall, and F1 measures for each of
the four entailment judgment categories taken sep-
arately. These scores aim at giving participants the
possibility to gain clearer insights into their system?s
behavior on the entailment phenomena relevant to
the task.
For each language combination, two baselines
considering the length difference between T1 and T2
have been calculated (besides the trivial 0.25 accu-
racy score obtained by assigning each test pair in the
balanced dataset to one of the four classes):
? Composition of binary judgments (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary entailment
decisions (?YES?, ?NO?). The classifier uses
length(T1)/length(T2) as a single feature to
check for entailment from T1 to T2, and
length(T2)/length(T1) for the opposite direc-
tion. For each test pair, the unidirectional
401
010
20
30
40
50
60
70
80
-21 -18 -15 -12 -9 -6 -3 0 3 6 9
no_entailmentforwardbidirectionalbackward
(a) SP-EN
0
10
20
30
40
50
60
70
80
-17 -14 -11 -8 -5 -2 1 4 7 10
no_entailmentforwardbidirectionalbackward
(b) IT-EN
0
10
20
30
40
50
60
70
80
90
-21 -18 -15 -12 -9 -6 -3 0 3 6 9 12
no_entailmentforwardbidirectionalbackward
(c) FR-EN
0
10
20
30
40
50
60
70
80
90
-14 -10 -7 -4 -1 2 5 8 11 14 17
no_entailmentforwardbidirectionalbackward
(d) DE-EN
Figure 2: CLTE pairs distribution for different length diff values across all datasets.
judgments returned by the two classifiers are
composed into a single multi-directional judg-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgments.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using a
linear kernel with default parameters. Baseline re-
sults are reported in Table 2.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages.
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.34 0.39 0.39 0.40
Multi-class 0.43 0.44 0.42 0.42
Table 2: Baseline accuracy results.
5 Submitted runs and results
Participants were allowed to submit up to five runs
for each language combination. A total of 17 teams
registered to participate in the task and downloaded
the training set. Out of them, 12 downloaded the
test set and 10 (including one of the task organizers)
submitted valid runs. Eight teams produced submis-
sions for all the language combinations, while two
teams participated only in the SP-EN task. In total,
92 runs have been submitted and evaluated (29 for
SP-EN, and 21 for each of the other language pairs).
402
Despite the novelty and the difficulty of the problem,
these numbers demonstrate the interest raised by the
task, and the overall success of the initiative.
System name SP-EN IT-EN FR-EN DE-EN
BUAP run1 0.350 0.336 0.334 0.330
BUAP run2 0.366 0.344 0.342 0.268
celi run1 0.276 0.278 0.278 0.280
celi run2 0.336 0.338 0.300 0.352
celi run3 0.322 0.334 0.298 0.350
celi run4 0.268 0.280 0.280 0.274
DirRelCond3 run1 0.300 0.280 0.362 0.336
DirRelCond3 run2 0.300 0.284 0.360 0.336
DirRelCond3 run3 0.300 0.338 0.384 0.364
DirRelCond3 run4 0.344 0.316 0.384 0.374
FBK run1* 0.502 - - -
FBK run2* 0.490 - - -
FBK run3* 0.504 - - -
FBK run4* 0.500 - - -
HDU run1 0.630 0.554 0.564 0.558
HDU run2 0.632 0.562 0.570 0.552
ICT run1 0.448 0.454 0.456 0.460
JU-CSE-NLP run1 0.274 0.316 0.288 0.262
JU-CSE-NLP run2 0.266 0.326 0.294 0.296
JU-CSE-NLP run3 0.272 0.314 0.296 0.264
Sagan run1 0.342 0.352 0.346 0.342
Sagan run2 0.328 0.352 0.336 0.310
Sagan run3 0.346 0.356 0.330 0.332
Sagan run4 0.340 0.330 0.310 0.310
SoftCard run1 0.552 0.566 0.570 0.550
UAlacant run1 LATE 0.598 - - -
UAlacant run2 0.582 - - -
UAlacant run3 LATE 0.510 - - -
UAlacant run4 0.514 - - -
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 3: Accuracy results (92 runs) over the 4 lan-
guage combinations. Highest, average, median and low-
est scores are calculated considering the best run for each
team (*task organizers? system).
Accuracy results are reported in Table 3. As can
be seen from the table, overall accuracy scores are
quite different across language pairs, with the high-
est result on SP-EN (0.632), which is considerably
higher than the highest score on DE-EN (0.558).
This might be due to the fact that most of the partic-
ipating systems rely on a ?pivoting? approach that
addresses CLTE by automatically translating T1 in
the same language of T2 (see Section 6). Regard-
ing the DE-EN dataset, pivoting methods might be
penalized by the lower quality of MT output when
German T1s are translated into English.
The comparison with baselines results leads to in-
teresting observations. First of all, while all systems
significantly outperform the lowest 1-class baseline
(0.25), both other baselines are surprisingly hard to
beat. This shows that, despite the effort in keep-
ing the distribution of the entailment classes uni-
form across different length diff values, eliminating
the correlation between sentences? length and cor-
rect entailment decisions is difficult. As a conse-
quence, although disregarding semantic aspects of
the problem, features considering such information
are quite effective.
In general, systems performed better on the SP-
EN dataset, with most results above the binary base-
line (8 out of 10), and half of the systems above the
multi-class baseline. For the other language pairs
the results are lower, with only 3 out of 8 partici-
pants above the two baselines in all datasets. Aver-
age results reflect this situation: the average scores
are always above the binary baseline, whereas only
the SP-EN average result is higher than the multi-
class baseline(0.44 vs. 0.43).
To better understand the behaviour of each sys-
tem (also in relation to the different language com-
binations), Table 4 provides separate precision, re-
call, and F1 scores for each entailment judgment,
calculated over the best runs of each participating
team. Overall, the results suggest that the ?bidi-
rectional? and ?no entailment? categories are more
problematic than ?forward? and ?backward? judg-
ments. For most datasets, in fact, systems? perfor-
mance on ?bidirectional? and ?no entailment? is sig-
nificantly lower, typically on recall. Except for the
DE-EN dataset (more problematic on ?forward?),
also average F1 results on these judgments are lower.
This might be due to the fact that, for all datasets, the
vast majority of ?bidirectional? and ?no entailment?
judgments falls in a length diff range where the dis-
tribution of the four classes is more uniform (see
Figure 2).
Similar reasons can justify the fact that ?back-
ward? entailment results are consistently higher on
all datasets. Compared with ?forward? entailment,
these judgments are in fact less scattered across the
entire length diff range (i.e. less intermingled with
the other classes).
403
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods as-
sign entailment judgments without preliminary
translation.
? Composition of binary judgments vs. Multi-
class classification. Compositional approaches
map unidirectional entailment decisions taken
separately into single judgments (similar to the
Binary baseline in Section 4). Methods based
on multi-class classification directly assign one
of the four entailment judgments to each test
pair (similar to our Multi-class baseline).
Concerning the former dimension, most of the
systems (6 out of 10) adopted a pivoting approach,
relying on Google Translate (4 systems), Microsoft
Bing Translator (1), or a combination of Google,
Bing, and other MT systems (1) to produce English
T2s. Regarding the latter dimension, the composi-
tional approach was preferred to multi-class classi-
fication (6 out of 10). The best performing system
relies on a ?hybrid? approach (combining monolin-
gual and cross-lingual alignments) and a compo-
sitional strategy. Besides the frequent recourse to
MT tools, other resources used by participants in-
clude: on-line dictionaries for the translation of sin-
gle words, word alignment tools, part-of-speech tag-
gers, NP chunkers, named entity recognizers, stem-
mers, stopwords lists, and Wikipedia as an external
multilingual corpus. More in detail:
BUAP [pivoting, compositional] (Vilarin?o et al,
2012) adopts a pivoting method based on translating
T1 into the language of T2 and vice versa (Google
Translate3 and the OpenOffice Thesaurus4). Simi-
larity measures (e.g. Jaccard index) and rules are
3http://translate.google.com/
4http://extensions.services.openoffice.
org/en/taxonomy/term/233
respectively used to annotate the two resulting sen-
tence pairs with entailment judgments and combine
them in a single decision.
CELI [cross lingual, compositional & multi-
class] (Kouylekov, 2012) uses dictionaries for word
matching, and a multilingual corpus extracted from
Wikipedia for term weighting. Word overlap and
similarity measures are then used in different ap-
proaches to the task. In one run (Run 1), they are
used to train a classifier that assigns separate en-
tailment judgments for each direction. Such judg-
ments are finally composed into a single one for each
pair. In the other runs, the same features are used for
multi-class classification.
DirRelCond3 [cross lingual, compositional]
(Perini, 2012) uses bilingual dictionaries (Freedict5
and WordReference6) to translate content words into
English. Then, entailment decisions are taken com-
bining directional relatedness scores between words
in both directions (Perini, 2011).
FBK [cross lingual, compositional & multi-
class] (Mehdad et al, 2012a) uses cross-lingual
matching features extracted from lexical phrase ta-
bles, semantic phrase tables, and dependency rela-
tions (Mehdad et al, 2011; Mehdad et al, 2012b;
Mehdad et al, 2012c). The features are used for
multi-class and binary classification using SVMs.
HDU [hybrid, compositional] (Wa?schle and
Fendrich, 2012) uses a combination of binary clas-
sifiers for each entailment direction. The classifiers
use both monolingual alignment features based on
METEOR (Banerjee and Lavie, 2005) alignments
(translations obtained from Google Translate), and
cross-lingual alignment features based on GIZA++
(Och and Ney, 2000) (word alignments learned on
Europarl).
ICT [pivoting, compositional] (Meng et al,
2012) adopts a pivoting method (using Google
Translate and an in-house hierarchical MT system),
and the open source EDITS system (Kouylekov and
Negri, 2010) to calculate similarity scores between
monolingual English pairs. Separate unidirectional
entailment judgments obtained from binary classi-
fier are combined to return one of the four valid
CLTE judgments.
5http://www.freedict.com/
6http://www.wordreference.com/
404
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211
celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312
DirRelCond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264
FBK spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460
HDU spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608
ICT spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474
JU-CSE-NLP spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297
Sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464
SoftCard spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504
UAlacant spa-eng run1 LATE 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559
AVG. 0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268
celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299
DirRelCond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234
HDU ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490
ICT ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496
JU-CSE-NLP ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304
Sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426
SoftCard ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492
AVG. 0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374
celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246
DirRelCond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243
HDU fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494
ICT fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462
JU-CSE-NLP fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237
Sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355
SoftCard fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500
AVG. 0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321
celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303
DirRelCond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305
HDU deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522
ICT deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489
JU-CSE-NLP deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119
Sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387
SoftCard deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500
AVG. 0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368
Table 4: precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
JU-CSE-NLP [pivoting, compositional] (Neogi
et al, 2012) uses Microsoft Bing translator7 to pro-
duce monolingual English pairs. Separate lexical
mapping scores are calculated (from T1 to T2 and
vice-versa) considering different types of informa-
tion and similarity metrics. Binary entailment de-
7http://www.microsofttranslator.com/
cisions are then heuristically combined into single
decisions.
Sagan [pivoting, multi-class] (Castillo and Car-
denas, 2012) adopts a pivoting method using Google
Translate, and trains a monolingual system based on
a SVM multi-class classifier. A CLTE corpus de-
rived from the RTE-3 dataset is also used as a source
of additional training material.
405
SoftCard [pivoting, multi-class] (Jimenez et al,
2012) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity (computed with
edit-distance)
UAlacant [pivoting, multi-class] (Espla`-Gomis
et al, 2012) exploits translations obtained from
Google Translate, Microsoft Bing translator, and the
Apertium open-source MT platform (Forcada et al,
2011).8 Then, a multi-class SVM classifier is used
to take entailment decisions using information about
overlapping sub-segments as features.
7 Conclusion
Despite the novelty of the problem and the diffi-
culty to capture multi-directional entailment rela-
tions across languages, the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
tion task organized within SemEval-2012 was a suc-
cessful experience. This year a new interesting chal-
lenge has been proposed, a benchmark for four lan-
guage combinations has been released, baseline re-
sults have been proposed for comparison, and a
monolingual English dataset has been produced as
a by-product which can be useful for monolingual
TE research. The interest shown by participants
was encouraging: 10 teams submitted a total of 92
runs for all the language pairs proposed. Overall,
the results achieved on all datasets are encourag-
ing, with best systems significantly outperforming
the proposed baselines. It is worth observing that the
nature of the task, which lies between semantics and
machine translation, led to the participation of teams
coming from both these communities, showing in-
teresting opportunities for integration and mutual
improvement. The proposed approaches reflect this
situation, with teams traditionally working on MT
now dealing with entailment, and teams tradition-
ally participating in the RTE challenges now dealing
with cross-lingual alignment techniques. Our ambi-
tion, for the future editions of the CLTE task, is to
further consolidate the bridge between the semantics
and MT communities.
8http://www.apertium.org/
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The
authors would also like to acknowledge Giovanni
Moretti from CELCT for evaluation scripts and
technical assistance, and the volunteer translators
that contributed to the creation of the dataset:
Mar??a Sol Accossato, Laura Barthe?le?my, Clau-
dia Biacchi, Jane Brendler, Amandine Chantrel,
Hanna Cheda Patete, Ellen Clancy, Rodrigo Damian
Tejeda, Daniela Dold, Valentina Frattini, Debora
Hedy Amato, Geniz Hernandez, Be?ne?dicte Jean-
nequin, Beate Jones, Anne Kauffman, Marcia Laura
Zanoli, Jasmin Lewis, Alicia Lo?pez, Domenico Los-
eto, Sabrina Luja?n Sa?nchez, Julie Mailfait, Gabriele
Mark, Nunzio Pruiti, Lourdes Rey Cascallar, Sylvie
Martlew, Aleane Salas Velez, Monica Scalici, An-
dreas Schwab, Marianna Sicuranza, Chiara Sisler,
Stefano Tordazzi, Yvonne.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72.
Julio Castillo and Marina Cardenas. 2012. Sagan: A
Cross Lingual Textual Entailment system based on
Machine Traslation. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Mikel L. Forcada, Ginest??-Rosell Mireia, Nordfalk Jacob,
O?Regan Jim, Ortiz-Rojas Sergio, Pe?rez-Ortiz Juan A.,
Sa?nchez-Mart??nez Felipe, Ram??rez-Sa?nchez Gema,
406
and Tyers Francis M. 2011. Apertium: a Free/Open-
Source Platform for Rule-Based Machine Translation.
Machine Translation, 25(2):127?144. Special Issue:
Free/Open-Source Machine Translation.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual En-
tailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2012. CELI: An Experiment with
Cross Language Textual Entailment. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? G. C. de Souza.
2012a. FBK: Cross-Lingual Textual Entailment With-
out Translation. In Proceedings of the 6th Interna-
tional Workshop on Semantic Evaluation (SemEval
2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012c. Match without a Referee: Evaluating MT Ade-
quacy without Reference Translations. In Proceedings
of the 7th Workshop on Statistical Machine Translation
(WMT 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Snehasis Neogi, Partha Pakray, Sivaji Bandyopadhyay,
and Alexander Gelbukh. 2012. JU-CSE-NLP: Lan-
guage Independent Cross-lingual Textual Entailment
System. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Franz J. Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL 2000).
Alpa?r Perini. 2011. Detecting textual entailment
with conditions on directional text relatedness scores.
Studia Universitatis Babes-Bolyai Series Informatica,
LVI(2):13?18.
Alpa?r Perini. 2012. DirRelCond3: Detecting Textual
Entailment Across Languages With Conditions On Di-
rectional Text Relatedness Scores. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n,
and Esteban Castillo. 2012. BUAP: Lexical and
Semantic Similarity for Cross-lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
407
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
UBC
Vancouver, Canada
mehdad@cs.ubc.ca
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al, 2011; Bronner et al, 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al, 2011;
25
Figure 1: Example of SP-EN CLTE pairs.
Mehdad et al, 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in either di-
rection;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
3 Dataset description
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year?s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
3.1 Data collection and annotation
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al, 2011). The main differ-
ence with last year?s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
26
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences? length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
3.2 Dataset statistics
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
27
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
Table 1: Training set pair distribution within the -5/+5
length diff range.
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
Table 2: Test set pair distribution within the -5/+5
length diff range.
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
28
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems? ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system?s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
? Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (?YES?, ?NO?). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year?s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the ?no-entailment? examples in all
datasets (F1no?entailment = 0).
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
Table 3: Baseline accuracy results.
5 Submitted runs and results
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
?pivoting?, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
4Including the task organizers.
29
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 ? ? ? 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers? system).
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it?s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the ?bidirectional?
and ?no entailment? categories consistently proved
to be more problematic than ?forward? and ?back-
ward? judgements, this year?s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of ?bidirectional?
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for ?no entailment?
are lower for two of them (SP-EN and DE-EN).
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
As regards the comparison with the baselines,
this year?s results confirm that the length diff -based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. ?no-entailment?), which are
better handled by participating systems.
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
? Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (?YES?/?NO?)
30
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year?s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al, 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilarin?o et al, 2013) adopts a pivoting
method based on translating T1 into the language of
31
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
CELI [cross-lingual, meta-classifier]
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (?YES?/?NO?) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al, 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
7 Conclusion
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
5http://translate.google.com/
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and ?simpler? solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria D??az, Bianca
Jeremias, Anne Kauffman, Laura Lo?pez Ortiz, Julie
Mailfait, Laura Mora?n Iglesias, Andreas Schwab.
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
32
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types. . In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazons? Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314?323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659?2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilarin?o, David Pinto, Saul Leo?n, Yuridiana
Alema?n, and Helena Go?mez-Adorno. 2013. BUAP:
N -gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
33
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
