Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 731?740, Dublin, Ireland, August 23-29 2014.
Low-Dimensional Manifold Distributional Semantic Models
Georgia Athanasopoulou
School of Electronic &
Computer Engineering
T.U.C. Chania, Greece
gathanasopoulou@isc.tuc.gr
Elias Iosif
Athena Research and
Innovation Center,
15125 Maroussi, Greece
iosif.elias@gmail.com
Alexandros Potamianos
School of Electrical &
Computer Engineering
N.T.U.A, Athens, Greece
apotam@gmail.com
Abstract
Motivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributed
semantic model (DSM) that consists of low-dimensional manifolds built on semantic neighbor-
hoods. Each semantic neighborhood is sparsely encoded and mapped into a low-dimensional
space. Global operations are decomposed into local operations in multiple sub-spaces; results
from these local operations are fused to come up with semantic relatedness estimates. Manifold
DSM are constructed starting from a pairwise word-level semantic similarity matrix. The pro-
posed model is evaluated on semantic similarity estimation task significantly improving on the
state-of-the-art.
1 Introduction
The estimation of semantic similarity between words, sentences and documents is a fundamental problem
for many research disciplines including computational linguistics (Malandrakis et al., 2011), semantic
web (Corby et al., 2006), cognitive science and artificial intelligence (Resnik, 2011; Budanitsky and
Hirst, 2001). In this paper, we study the geometrical structure of the lexical space in order to extract se-
mantic relations among words. In (Karlgren et al., 2008), the high-dimensional lexical space is assumed
to consist of manifolds of very low dimensionality that are embedded in this high dimensional space.
The manifold hypothesis is compatible with evidence from psycholinguistics and cognitive science. In
(Tenenbaum et al., 2011), the question ?How does the mind work?? is answered as follows: cognitive
organization is based on domains with similar items connected to each other and lexical information
is represented hierarchically, i.e., a domain that consists of similar lexical entries may be represented
by a more abstract concept. An example of such a domain is {blue, red, yellow, pink, ...} that corre-
sponds by the concept of color. An inspiring analysis about the geometry of thought, as well as cognitive
evidence for the low-dimensional manifold assumption can be found in (Gardenfors, 2000), e.g., the
domain of color is argued to be cognitively represented as an one-dimensional manifold. Following the
low-dimensional manifold hypothesis we propose to extend distributional semantic models (DSMs) into
a hierarchical model of domains (or concepts) that contain semantically similar words. Global operations
on the lexical space are decomposed into local operations on the low-dimensional domain sub-manifolds.
Our goal is to exploit this hierarchical low-rank model to estimate relations between words, such as se-
mantic similarity.
There has been much research interest on devising data-driven approaches for estimating semantic
similarity between words. DSMs (Baroni and Lenci, 2010) are based on the distributional hypothesis
of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap
of their linguistic contexts. DSMs are typically constructed from co-occurrence statistics of word tuples
that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and
Potamianos, 2013), general-purpose, language-agnostic algorithms were proposed for estimating seman-
tic similarity using no linguistic resources other than a corpus created via web queries. The key idea of
this work was the construction of semantic networks and semantic neighborhoods that capture smooth
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are
added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
731
co-occurrence and context similarity statistics. The majority of DSMs adopt high-dimensional represen-
tations, while the underlying space geometry is not explicitly taken into consideration during the design
of algorithms aimed for performing several semantic tasks.
We propose the construction of a low-dimensional manifold DSM consting of four steps: 1) identify
the domains that correspond to the low-dimensional manifolds, 2) run the dimensionality reduction al-
gorithm for each domain, 3) construct a DSM for each domain, and 4) combine the manifold DSMs to
come up with global measures of lexical relations. A variety of algorithms can be found in the literature
for projecting a set of tokens into low dimensional sub-spaces, given a token similarity or dissimilarity
matrix. Depending on the nature of the dataset, these projection algorithms may or may not preserve
the local geometries of the original dataset. Most dimensionality reduction algorithms make the implicit
assumption that the underlying space is metric, e.g., Multidimensional Scaling (MDS) (Torgerson, 1952)
or Principal Component Analysis (PCA) (Jolliffe, 2005) or the ones using non-negative matrix factor-
ization (Tsuge et al., 2001) and typically fail to capture the geometry of manifolds embedded in high
dimensional spaces. A variety of dimensionality reduction algorithms have been developed that respect
the local geometry. Some examples are the Isomap algorithm (Tenenbaum et al., 2000) that performs
the projection based on a weighted neighborhood graph, Local Linear Embedings (LLE) (Roweis and
Saul, 2000) that assigns neighbors to each data point, Random Projections (Baraniuk and Wakin, 2009),
(Li et al., 2006) that preserves the manifold geometry by executing random linear projections and oth-
ers (Hessian Eigenmaps (HLLE) (Donoho and Grimes, 2003); Maximum Variance Unfolding (MVU)
(Wang, 2011)). The manifold hypothesis has also been studied by the representation learning commu-
nity where the local geometry is disentangled from the global geometry mainly by using neighborhood
graphs (Weston et al., 2012) or coding schemes (Yu et al., 2009). For a review see (Bengio et al., 2013).
A fundamental problem with all aforementioned methods when applied to lexical semantic spaces is
that they do not account for ambiguous tokens, i.e., word senses. The main assumption of dimensionality
reduction and manifold unfolding algorithms is that each token (word) belongs to a single sub-manifold.
This in fact is not true for polysemous words, for example the word ?green? could belong both to the
domain colors, as well as to the domain plants. In essence, lexical semantic spaces are manifolds that
have singularities: the manifold collapses in the neighborhood of polysemous words that can be thought
of semantic black holes that can instantaneously transfer you from one domain to another. Our proposed
solution to this problem is to allow words to live in multiple sub-manifolds.
The algorithms proposed in this paper build on recent research work on distributional semantic models
and manifold representational learning. Manifold DSMs can be trained directly from a corpus and do
not require a-priori knowledge or any human-annotated resources (just like DSMs). We show that the
proposed low-dimensional, sparse and hierarchical manifold representation significantly improves on the
state-of-the-art for the problem of semantic similarity estimation.
2 Metrics of Semantic Similarity
Semantic similarity metrics can be broadly divided into the following types: (i) metrics that rely on
knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge
source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and
Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954). DSMs can be distin-
guished into (i) unstructured: use bag-of-words model (Iosif and Potamianos, 2010) and (ii) structured:
exploitation of syntactic relationships between words (Grefenstette, 1994; Baroni and Lenci, 2010). The
vector space model (VSM) constitutes the main implementation for both unstructured and structured
DSMs. Cosine similarity constitutes a measurement of word similarity that is widely used on top of
the VSM. The similarity between two words is estimated as the cosine of their respective vectors whose
elements correspond to corpus-based co-occurrence statistics. In essence, the similarity between words
is computed via second-order co-occurrences.
Direct (i.e., first-order) co-occurrences can be also used for the estimation of semantic similarity (Bol-
legala et al., 2007; Gracia et al., 2006). The exploitation of first-order co-occurrence statistics constitutes
the simplest form of unstructured DSMs. A key parameter for such models is the definition of the context
in which the words of interest co-occur: from entire documents (Bollegala et al., 2007) to paragraphs
732
(V?eronis, 2004) and sentences (Iosif and Potamianos, 2013). The effect of co-occurrence context for
the task of similarity computation between nouns is discussed in (Iosif and Potamianos, 2013). The
underlying assumption is that two words that co-occur in a specified context are semantically related.
3 Collapsed Manifold Hypothesis, Low-Dimensionality and Sparsity
The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it
is organized in such a way that interesting semantic relations can be exported from manifolds of much
lower dimensionality embedded in this high dimensional space (Karlgren et al., 2008). We assume that
(at least some of) these sub-manifolds contain semantically similar words (or word senses). For example,
a potential sub-manifold in the lexical space could be the one that contains the colors (e.g., red, blue,
green). But in fact many words, such as book, green, fruit, are expected to belong simultaneously in
semantically different manifolds because they have multiple meanings.
A simple way to bootstrap the manifold recreation process is to build a domain around each word,
i.e., the semantic neighborhood of each word defines a domain. For example, in Figure 1 we show
the semantic neighborhood of fruit. The connections between words indicate high semantic similarity,
i.e., this is a pruned semantic similarity graph of all words in the semantic neighborhood of the word
?fruit?. It is clear from this example that in a typical neighborhood there exist word pairs that should be
native
genus
b
shurb
b
plant
flowering
b
tree
b
species
b
b
garden
b
soil
b
animal
b
water
b
seed
flower
b
b
fruit
b
vegetable
b
apple
juice
daiquiri
b
orange
b
drink
b
zest
b
lemon
sugar
salt
b
flour
b
cream
b
butter
b
b
b
milk
b
corn
b
honey
b
tomato
b
b b
b
Figure 1: Visualization of the semantic neighborhood of the word ?fruit?.
?connected? to each other because they have close semantic relation, like {flower, plant} and others that
should not be ?connected? because they are semantically apart, like {garden, salt}. A sparse encoding of
the semantic similarity relations in a neighborhood is needed in order to achieve (via multi-dimensional
scaling) a parsimonious representation with good geometric properties
1
.
The graph connectivity or sparseness matrix identifies the word pairs that should be encoded in a
neighborhood is defined as
?
S ? {0, 1}
n?n
, where value
?
S(i, j) = 1 indicates that the i
th
, j
th
word
pair is encoded, while
?
S(i, j) = 0 indicates that the pair is ignored (n is the number of words and
i, j = 1, .., n in the neighborhood). We define the degree of sparseness of matrix
?
S as the percentage of
0?s in the matrix.
4 Dimensionality Reduction
In this section, the Sparse Projection (SP) algorithm is described (see also Algorithm 1). SP is the core
algorithm for constructing manifold DSMs presented in Section 5. SP is a dimensionality reduction
algorithm that projects a set of n words into a vector space of d dimensions. The input to the algorithm
is a dissimilarity or semantic distance matrix P ? R
n?n
, where element P(i, j) encodes the degree
of dissimilarity between words w
i
and w
j
. The output of SP are the d-dimensional coordinate vectors
of the n projected words that form a matrix X ? R
n?d
. Each row x
i
? R
1?d
of matrix X ? R
n?d
corresponds to the coordinates of the i
th
word w
i
. Once X is estimated the dissimilarity matrix is
recomputed and updated to new values, as discussed next. Each paragraph that follows corresponds to a
module in Algorithm 1.
1
Compare for example with Isomap (Tenenbaum et al., 2000) were a short- and long-distance metric is used. When using
sparse encoding the long-distance metric is set to a very large fixed number (similarity set to 0). In both cases, the underlying
manifold is unfolded and low-dimensional representation with (close to) metric properties are discovered.
733
Semantic Distance Re-estimation: Given the matrix X ? R
n?d
containing the vector projections of
words in the d-dimensional space, the dissimilarity matrix is re-estimated using the Euclidean distance
2
.
Let
?
P ? R
n?n
be the matrix with the new dissimilarity scores then the new dissimilarity score between
words w
i
and w
j
is simply:
?
P(i, j) = ?x
i
? x
j
?
2
, where x
i
, x
j
are the vectors corresponding to words
w
i
, w
j
respectively, i, j = 1, .., n and ?.?
2
is the Euclidean norm.
Connectivity Graph and Sparsity: As discussed in Section 3, given a set of words only a small
subset of lexical relations should be explicitly encoded between pairs of these words. Therefore,
the SP algorithm should only take into account strongly related word pairs and ignore the rest. This
is the main difference between our approach compared to the generic MDS algorithm proposed in
(Torgerson, 1952). In order to apply the sparseness constraint, we first construct the connectivity
matrix
?
S ? {0, 1}
n?n
. Word pairs (w
i
, w
j
) with small similarity values (or equivalently large semantic
distance) are penalized: zero values are assigned to their corresponding position (i, j) in
?
S matrix. In
essence, the matrix
?
S is obtained by hard {0, 1} thresholding on the dissimilarity matrix P: all values
that are under a threshold are set to 0, while all values equal or greater to the threshold are set to 1.
Let n be the number of words under investigation, then the number of word pairs is p =
n?(n?1)
2
. The
degree of sparseness is defined as the number of unordered word pairs (w
i
, w
j
), i 6= j where
?
S(i, j) = 0
normalized over the total number of pairs p
3
.
Error Criterion: The algorithm employs a local and a global error criterion defined as follows:
1. The local error corresponds to the projection error for each individual word w
i
e ? R
n?1
, where
i = 1...n and is defined as the sum of the dissimilarity matrix errors before and after projection
computed only for the words that are ?connected? to w
i
, as follows:
e
i
=
n
?
j=1
?
S(i, j) ?
(
?
P(i, j)?P(i, j)
)
2
(1)
2. The global error of the projection is simply the sum over local errors for all words: e
tot
=
?
n
i=1
e
i
Algorithm 1 Sparse projection (SP)
Require: v // Vocabulary: vector of n words
Require: P // n?n dissimilarity matrix
1:
?
S? ComputeConnectivityMatrix(S)
2: for each word w
i
? v do
3: X
i
? RandomInitialization(X
i
)
4: end for
5: k = 0 // Iteration counter: initialization
6: e
k
tot
= inf // Global error: initialization
7: repeat
8: k = k + 1
9: for each word w
i
? v do
10: for each direction z do
11: X?MoveWordToDirection(w
i
, z)
12: e
z
i
? ComputeLocalError(
?
S,P,X,i)
13: end for
14: z?
i
? FindDirectionOfMinLocalError(e
z
i
)
15: X = MoveWordToDirection(w
i
, z?
i
)
16: end for
17: e
k
tot
? UpdateGlobalError(
?
S,P,X)
18: until e
k?1
tot
< e
k
tot
// Stopping condition
19:
?
P? SemanticDistanceReestimation(X)
20:
?
P? SparseDistanceNormalizedRanges(
?
P,
?
S)
21: return X // n?d matrix with coordinates;
22: return
?
S // n?n matrix with connections;
23: return
?
P // n?n updated dissimilarity matrix;
24: return
?
P // n?n sparse-normalized distances;
Random Walk SP: In function MoveWordToDirection(?) of Algorithm 1, the pseudo-variable direction
z refers to a standard set of perturbations of each word in the d-dimensional space. For example, if the
dimension of the projection is d = 2 then the coordinates of each word are modeled as (k
1
, k
2
), where
k
1
, k
2
? R. A potential set of perturbations are the following: (k
1
, k
2
+ s), (k
1
, k
2
? s), (k
1
+ s, k
2
)
and (k
1
? s, k
2
), where s is the perturbation step parameter of the algorithm. For coordinates systems
normalized in [0, 1]
d
we chose a value of s equal to 0.1. Good convergence properties to global maxima
have been experimentally shown for this algorithm for multiple runs on (noisy) randomly generated data.
2
Other metrics, e.g., cosine similarity, have also been tested out but results are not shown here due to lack of space. Euclidean
distance performed somewhat better that cosine similarity for the semantic similarity estimation task.
3
The SP algorithm with 0% degree of sparseness is equivalent to the MDS algorithm.
734
Sparse Semantic Distance Normalized Ranges: This function normalizes all the distance scores of
?
P
in a range of values, [0 r
1
], where r
1
? R
+
is an arbitrary positive constant and also it imposes the
sparsity constraint as follows: if
?
S(i, j) = 0 then
?
P(i, j) = r
1
. If
?
S(i, j) = 1 then
?
P(i, j) = r
2
?
?
P(i,j)
r
3
,
where r
3
is the maximum distance over all ?connected? pairs, i.e. r
3
, max{
?
P 
?
S}, with  denoting
the Hadamard product, and r
2
? R
+
can be either equal to r
1
or slightly smaller than r
1
. The assignment
of r
2
< r
1
aims to differentiate the ?unconnected? pairs from the ?connected? but dissimilar ones
4
.
5 Low-Dimensional Manifold DSMs
The end-to-end low-dimensional manifold DSM (LDMS) system is depicted in Figure 2. Note that
v
1
, v
2
, ..., v
|V|
? V are the domains or sub-manifolds of the LDMS, for each domain v
i
a separate DSM
is built. V is the set of domains (concept vocabulary) and |V| denotes to the cardinality of V. The input
Figure 2: LDMS system.
to LDMS is a (global) similarity matrix S ? R
n?n
, where n is the total number of tokens (words) in
the LDMS model. Note that S can be estimated using any of the baseline semantic similarity metrics
5
presented in Section 2. Since the SP algorithm uses as input a dissimilarity or semantic distance matrix,
the pairwise word similarity matrix S ? R
n?n
is transformed to a semantic distance (or dissimilarity)
matrix P ? R
n?n
as: P(i, j) = c
1
? e
?c
2
?S(i,j)
where c
1
, c
2
? R are constants and the i, j indexes run
from 1 to n. In this work, we used c
1
=c
2
=20. The transformation defined by (5) was selected in order
to non-linearly scale and increase the relative distance of dissimilar words compared to similar ones
6
.
The steps followed by the LDMS system are the following:
1. Domain Selection: The domains v
1
, v
2
, ..., v
|V|
are created as follows: for each word w
i
in our
model we create a corresponding domain v
i
that consists of all the words that are semantically
similar to w
i
, i.e., the ith domain is the semantic neighborhood of word w
i
. Thus in our model
the vocabulary size is equal to the domain set cardinality, i.e., n = |V|. Domain v
i
is created by
selecting the top N most semantically similar words to w
i
based on the (global) similarity matrix
S ? R
n?n
. We have experimented with various domain sizes N ranging between 20 and 200
neighbors; note that each word in the LDMS may belong to multiple domains.
2. Sparse Projections on Domains: Following the selection of domain v
i
? V the (local) dissimilarity
matrix for each domain P
v
i
? R
N?N
is defined as a submatrix of P ? R
n?n
. Then, the SP
algorithm is applied to each domain separately, resulting in i = 1, .., |V| re-estimated bounded
semantic distance matrices
?
P
v
i
.
3. Fusion: To reach a decision on the strength of the semantic relation between words w
i
and w
j
the
semantic distance matrices from each domain
?
P
v
i
must be combined. Only domains were both
words w
i
and w
j
appear are relevant in this fusion process. This procedure is described next.
4
We experimented with various values for r
1
and r
2
achieving comparable performance; we selected r
2
? 0.9r
1
that had
slightly better performance. The value of r
1
can be chosen arbitrary, the results reported here were obtained for r
1
= 20 and
r
2
= 18.
5
Here, the Google-based Semantic Relatedness was employed using a corpus of web-harvested document snippets.
6
Similar nonlinear scaling function from similarity to distance can be found in the literature, e.g., (Borg, 2005)
735
5.1 Fusion
Motivation: Given a set of words L = {w
1
, w
2
, ...w
n
} we assume that their corresponding set of word
senses
7
is M = {s
11
, s
12
, .., s
1n
1
, .., .., s
n1
, s
n2
, .., s
nn
n
}. The set of senses is defined as M = ?
n
i=1
M
i
,
where M
i
= {s
i1
, s
i2
, ..., s
in
i
} is the set of senses for word w
i
. Let S(.) be a metric of semantic similar-
ity, e.g., the metric defined in Section 2, which is symmetric, i.e., S(x, y) ? S(y, x). The notations S
w
(.)
and S
s
(.) are used in order to distinguish the similarity at word and sense level, respectively. According to
the maximum sense similarity assumption (Resnik, 1995), the similarity between w
i
and w
j
, S
w
(w
i
, w
j
),
is defined as the pairwise maximum similarity between their corresponding senses S
s
(s
ik
, s
jl
):
S
w
(w
i
, w
j
) ? S
s
(s
ik
, s
jl
), where (k, l) = argmax
(p?M
i
,r?M
j
)
S
s
(s
ip
, s
jr
).
Note that the maximum pairwise similarity metric (or equivalently the minimum pairwise distance
metric) is also known as the ?common sense? set similarity (or distance) employed by human cognition
when evaluating the similarity (or distance) between two sets.
Fusion of local dissimilarity scores: Next we describe a domain fusion model that follows the min-
imum pairwise distance (dissimilarity) principle motivated by human cognition. The steps for the re-
computation of the (global) dissimilarity between words w
i
and w
j
are:
1. Search for all the domains where w
i
and w
j
co-exist.
2. Let U ? V be the subset of domains from the previous step. The distances between words w
i
and
w
j
are retrieved from domain dissimilarity matrices
?
P
u
for all u ? U . The distances are stored into
vector d ? R
|U |?1
.
3. Motivated by the maximum sense similarity assumption (see above) the dissimilarity between w
i
and w
j
is defined as
8
:
?
P(i, j) = min
k=1..|U |
{d
k
} (2)
4. If words w
i
and w
j
do not co-exist in any domain then r
1
is assigned as their dissimilarity score,
where r
1
is the upper bound of
?
P
u
matrices as defined in the previous section.
For example, let one pair of words (w
1
, w
2
) co-exists in |U | = 3 different domains with corresponding
local distances d = [9 20 11] then the global distance of (w
1
, w
2
) is 9.
6 Evaluation
In this section, we evaluate the performance of the proposed approach with respect to the task of simi-
larity judgment between nouns. Results are reported with respect to several domain/neighborhood sizes,
sparse percentages and domain dimensions.
The performance of similarity metrics were evaluated against human ratings from three standard
datasets of noun pairs, namely WS353 (Finkelstein et al., 2001), RG (Rubenstein and Goodenough,
1965) MC (Miller and Charles, 1991). The first and the second datasets consist of the subset of 272 and
57 pairs, respectively, that are also included in SemCor3
9
corpus, while the third dataset consists of 28
noun pairs. The Pearson?s correlation coefficient was selected as evaluation metric to compare estimated
similarities against the ground truth.
The similarity matrix computed using the Google-based Semantic Relatedness (Gracia et al., 2006)
was used as baseline, as well as to bootstrap the LDMS global similarity matrix S, for a list of 8752 nouns
extracted from the SemCor3 corpus
10
. The performance of the proposed LDMS approach is presented
in Table 1. In addition, the performance of other unsupervised similarity estimation algorithms are
reported for comparison purposes: 1) SEMNET is an alternative implementation of unstructured DSMs
based on the idea of semantic neighborhoods and networks (Iosif and Potamianos, 2013) 2) WikiRelate!
includes various taxonomy-based metrics that are typically applied to the WordNet hierarchy; the basic
7
This is a simplification. In reality, some of the word senses will be the same, so strictly speaking this is not a set definition.
8
Other fusion methods have also been evaluated, e.g., (weighted) average. Results are omitted here due to lack of space.
Minimum pairwise distance fusion outperformed other fusion schemes.
9
http://www.cse.unt.edu/
?
rada/downloads.html
10
The baseline similarity matrix and the 8752 nouns are public available in:
http://www.telecom.tuc.gr/
?
iosife/downloads.html
736
idea behind WikiRelate! is to adapt these metrics to a hierarchy extracted from the links between the
pages of the English Wikipedia (Strube and Ponzetto, 2006) . 3) TypeDM is a structured DSM (Baroni
and Lenci, 2010), 4) AAHKPS1 constitutes an unstructured paradigm of DSM development using four
billion web documents that were acquired via crawling (Agirre et al., 2009), 5) Moreover, two well-
established dimensionality reduction algorithms (Isomap and LLE) that support the manifold hypothesis,
were applied to the task of semantic similarity computation
11
. LDMS, Isomap and LLE were given as
input the matrix P ? R
n?n
, where n = 8752 is the number of words in our models. Isomap and LLE
used dimensionality reduction down to d = 5 and neighborhood size equal to N = 120. SEMNET was
run for neighborhood size equal to N = 100. While LDMS run for dimensionality down to d = 5,
domain/neighborhood size equal to N = 140 and degree of sparseness 90%. The proposed LDMS
system surpassed the performance of the baseline system for all three datasets, as well as the performance
of the other corpus-based approaches for the WS353 and MC datasets. The dimensionality reduction
algorithms (Isomap - LLE) are shown to perform poorly for this particular task.
Datasets Algorithm
Baseline SEMNET WikiRelate! TypeDM AAHKPS1 Isomap LLE LDMS
WS353 0.61 0.64 0.48 - - 0.14 0.04 0.69
RG 0.81 0.87 0.53 0.82 - 0.04 0 0.86
MC 0.85 0.91 0.45 - 0.89 -0.04 -0.04 0.94
Table 1: Performance of various algorithms for the task of similarity judgment.
The performance (Pearson correlation) of the LDMS approach is shown in Figures 3a, 3b and 4a as
a function of neighborhood size and degree of sparseness. Results are presented for all three datasets:
WS353, MC, and RG. The baseline performance is also plotted (dotted line). For all three datasets,
we see a clear relationship between neighborhood size, degree of sparseness and performance. Sparse
representations achieve peak performance for larger neighborhood sizes. High degree of sparseness
between 80 and 90% achieves the best results for domain/neighborhood sizes between 100 and 140. The
figures show that there is potential for even better performance by fine-tuning the LDMS parameters.
The performance of LDMS is shown in Figure 4b as a function of the projection dimension d. The de-
gree of sparseness is fixed at 80% and the domain/neighborhood size is equal to 100 for all experiments.
It is observed that the performance for all three datasets remains relatively constant when at least d = 3
is used. In fact results are slightly better for d = 3 than for higher dimensions but the differences in
performance are not significant. The results suggest that even a 3D sub-space is adequate for accurately
representing the semantics of each underlying domain.
20 40 60 80 100 120 140 160 180 2000.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.705
Neighborhood size
Corr
elati
on
 
 Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 0.947
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
Figure 3: Performance as a function of domain size N and sparseness percentage for the (a) WS353
dataset and (b) MC dataset.
11
LDMS is not directly comparable with Isomap-LLE algorithms because it represents only the domains in low-dimensional
spaces and not the whole dataset.
737
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.865
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
2 3 4 5 6 7 80.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Dimensions
Corr
elati
on
 
 MCRGWS353
Figure 4: Performance for the (a) RG dataset as a function of domain size N and sparseness percentage
and (b) WS353, MC, RG datasets as a function of projection dimension d.
7 Conclusions
In this work, we proposed a novel, hierarchical DSM that was applied to semantic relation estimation
task obtaining very good results. The proposed representation consists of low-dimensional manifolds
that are derived from sparse projections of semantic neighborhoods. The core idea of low dimensional
subspaces was motivated by cognitive models of conceptual spaces. The validity of this motivation was
experimentally verified via the estimation of semantic similarity between nouns. The proposed approach
was found to be (at least) competitive with other state-of-the-art DSM approaches that adopt flat feature
representations and do not explicitly include the sparsity and dimensionality as a key design parameter.
The poor performance of Isomap and LLE can be attributed to the nature of the specific application,
i.e., word semantics. A key characteristic of this application is the ambiguity of word senses. These
algorithms assume only one sense for each word (i.e., a word is represented as a single point in a high-
dimensional space). Although the disambiguation task is not explicitly addressed, LDMS approach
handles the ambiguity of words by isolating each word?s senses in different domains.
Our initial intuition regarding the semantic fragmentation of lexical neighborhoods due to singularities
introduced by word senses was supported by the high performance when large (i.e., 80% - 90%) degree of
sparseness was imposed. The hypothesis of low-dimensional representation was validated by the finding
that as little as three dimensions are adequate for representing domain/neighborhood semantics. It was
also observed that the parameters of the LDMS model, i.e., number of dimensions, neighborhoodsize
and degree of sparseness, are interrelated: very sparse projections achieve best results with very low
dimensionality when large neighborhood sizes are used.
This is only a first step toward using ensembles of low-dimensional DSMs for semantic relation esti-
mation. As future work we plan to further investigate the creation of domains based on more complex
geometric properties of the underlying space (Kreyszig, 2007). A more formal investigation of the re-
lation between sparseness, dimensionality and performance is also needed. Finally, creating multi-level
hierarchical representations that are consistent with cognitive organization is an important challenge that
can further improve manifold DSM performance.
Acknowledgments
This work has been partially funded by two projects supported by the EU Seventh Framework Pro-
gramme (FP7): 1) PortDial, grant number 296170 and 2) SpeDial, grant number 611396.
738
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies, pages
19?27. Association for Computational Linguistics.
R. G Baraniuk and M. B Wakin. 2009. Random projections of smooth manifolds. Foundations of computational
mathematics, 9(1):51?77.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
Y. Bengio, A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Measuring semantic similarity between words using web search
engines. In Proc. of International Conference on World Wide Web, pages 757?766, Banff, Alberta, Canada.
Ingwer Borg. 2005. Modern multidimensional scaling: Theory and applications. Springer.
A. Budanitsky and G. Hirst. 2001. Semantic distance in wordnet: An experimental, application-oriented evalua-
tion of five measures. In Workshop on WordNet and Other Lexical Resources.
O. Corby, R. Dieng-Kuntz, F. Gandon, and C. Faron-Zucker. 2006. Searching the semantic web: Approximate
query processing based on ontologies. Intelligent Systems, IEEE, 21(1):20?27.
D. L Donoho and C. Grimes. 2003. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591?5596.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in
context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages
406?414. ACM.
P. Gardenfors. 2000. Conceptual spaces: The geometry of thought. Cambridge, Massachusetts: USA. ISBN,
262071991.
J. Gracia, R. Trillo, M. Espinoza, and E. Mena. 2006. Querying the web: A multiontology disambiguation method.
In Proc. of International Conference on Web Engineering, pages 241?248, Palo Alto, California, USA.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell,
MA, USA.
Z. Harris. 1954. Distributional structure. Word, 10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web
documents. Knowledge and Data Engineering, IEEE Transactions on, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2013. Similarity computation using semantic networks created from web-harvested
data. Natural Language Engineering (DOI: 10.1017/S1351324913000144).
I. Jolliffe. 2005. Principal component analysis. Wiley Online Library.
J. Karlgren, A. Holst, and M. Sahlgren. 2008. Filaments of meaning in word space. In Advances in Information
Retrieval, pages 531?538. Springer.
E. Kreyszig. 2007. Introductory functional analysis with applications. Wiley. com.
P. Li, T. J Hastie, and K. W Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 287?296. ACM.
N. Malandrakis, A. Potamianos, E. Iosif, and S. S Narayanan. 2011. Kernel models for affective lexicon creation.
In INTERSPEECH, pages 2977?2980.
G. A Miller and W. G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive
processes, 6(1):1?28.
P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International
Joint Conference for Artificial Intelligence, pages 448?453.
739
P. Resnik. 2011. Semantic similarity in a taxonomy: An information-based measure and its application to prob-
lems of ambiguity in natural language. arXiv preprint arXiv:1105.5444.
S. T Roweis and L. K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323?2326.
H. Rubenstein and J. B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM,
8(10):627?633.
Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia.
In AAAI, pages 1419?1424.
J. B Tenenbaum, V. De Silva, and J. C Langford. 2000. A global geometric framework for nonlinear dimensional-
ity reduction. Science, 290(5500):2319?2323.
J. B Tenenbaum, C. Kemp, T. L Griffiths, and N. D Goodman. 2011. How to grow a mind: Statistics, structure,
and abstraction. science, 331(6022):1279?1285.
Warren S Torgerson. 1952. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401?419.
S. Tsuge, M. Shishibori, S. Kuroiwa, and K. Kita. 2001. Dimensionality reduction using non-negative matrix
factorization for information retrieval. In Systems, Man, and Cybernetics, 2001 IEEE International Conference
on, volume 2, pages 960?965 vol.2.
J. V?eronis. 2004. Hyperlex: Lexical cartography for information retrieval. Computer Speech and Language,
18(3):223?252.
Jianzhong Wang. 2011. Maximum variance unfolding. In Geometric Structure of High-Dimensional Data and
Dimensionality Reduction, pages 181?202. Springer.
J. Weston, F. Ratle, H. Mobahi, and R. Collobert. 2012. Deep learning via semi-supervised embedding. In Neural
Networks: Tricks of the Trade, pages 639?655. Springer.
K. Yu, T. Zhang, and Y. Gong. 2009. Nonlinear learning using local coordinate coding. In Advances in Neural
Information Processing Systems, pages 2223?2231.
740
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668?672,
Dublin, Ireland, August 23-24, 2014.
tucSage: Grammar Rule Induction for Spoken Dialogue Systems via
Probabilistic Candidate Selection
Arodami Chorianopoulou
?
, Georgia Athanasopoulou
?
, Elias Iosif
? ?
,
Ioannis Klasinas
?
, Alexandros Potamianos
?
?
School of ECE, Technical University of Crete, Chania 73100, Greece
?
School of ECE, National Technical University of Athens, Zografou 15780, Greece
?
?Athena? Research Center, Marousi 15125, Greece
{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.gr
iosife@telecom.tuc.gr, apotam@gmail.com
Abstract
We describe the grammar induction sys-
tem for Spoken Dialogue Systems (SDS)
submitted to SemEval?14: Task 2. A sta-
tistical model is trained with a rich fea-
ture set and used for the selection of can-
didate rule fragments. Posterior probabil-
ities produced by the fragment selection
model are fused with estimates of phrase-
level similarity based on lexical and con-
textual information. Domain and language
portability are among the advantages of
the proposed system that was experimen-
tally validated for three thematically dif-
ferent domains in two languages.
1 Introduction
A critical task for Spoken Dialogue Systems
(SDS) is the understanding of the transcribed user
input, that utilizes an underlying domain grammar.
An obstacle to the rapid deployment of SDS to
new domains and languages is the time-consuming
development of grammars that require human ex-
pertise. Machine-assisted grammar induction has
been an open research area for decades (K. Lari
and S. Young, 1990; S. F. Chen, 1995) aiming
to lower this barrier. Induction algorithms can
be broadly distinguished into resource-based, e.g.,
(A. Ranta, 2004), and data-driven, e.g., (H. Meng
and K.-C. Siu, 2002). The main drawback of
the resource-based paradigm is the requirement of
pre-existing knowledge bases. This is addressed
by the data-driven paradigm that relies (mostly)
on plain corpora. SDS grammars are built by uti-
lizing low- and high-level rules. Low-level rules
This work is licenced under a Creative Commons Attri-
bution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
are similar to gazetteers consisting of terminal en-
tries, e.g., list of city names. High-level rules can
be lexicalized as textual fragments (or chunks),
which are semantically defined on top of low-
level rules, e.g., ?depart from <City>?.
The data-driven induction of low-level rules is a
well-researched area enabled by various technolo-
gies including web harvesting for corpora creation
(Klasinas et al., 2013), term extraction (K. Frantzi
and S. Ananiadou, 1997), word-level similarity
computation (Pargellis et al., 2004) and cluster-
ing (E. Iosif and A. Potamianos, 2007). High-level
rule induction is a less researched area that poses
two main challenges: 1) the extraction and selec-
tion of salient candidate fragments from a corpus
that convey semantics relevant to the domain of in-
terests and 2) the organization of such fragments
(e.g., via clustering) according to their semantic
similarity. Despite the recent interest on phrase (J.
Mitchell and M. Lapata, 2010) and sentence simi-
larity, each respective problem remains open.
Next, our submission
1
for the Se-
mEval?14: Task2 is briefly described, which
constitutes a data-driven approach for inducing
high-level SDS grammar rules. At the system?s
core lies a statistical model for the selection of
textual fragments based on a rich set of features.
This set includes various lexical features, aug-
mented with statistics from n-gram language
models, as well as with heuristic features. The
candidate selection model posterior is fused
with a phrase-level semantic similarity metric.
Two different approaches are used for similarity
computation relying on the overlap of character
bigrams or context-based similarity according
to the distributional hypothesis of meaning.
The domain and language portability of the
proposed system is demonstrated by its successful
application across three different domains and
1
Please note that the last three authors of this submission
are among the organizers of this task.
668
two languages. All the four subtasks defined by
the organizers were completed with very good
performance that exceeds the baseline.
2 System Description
The basic functionality of the proposed system
is the mapping (assignment) of unknown textual
fragments into known high-level grammar rules.
Let E be the set of unknown fragments, while the
set of known rules is denoted byR. Each unknown
fragment f ?E is allowed to be mapped to a sin-
gle high-level rule r
s
?R, where 1? s? m and
m is the total number of rules in the grammar.
Figure 1: Overview of system architecture.
The system consists of three major components as
shown at the system architecture diagram in Fig.
1, specifically: 1) candidate selection: a set of
classifiers is built, one for each r
s
to select whether
f ? E is a candidate member of the specific rule
2
,
2) similarity computation between f and r
s
, and
3) mapping f to a high-level rule r
s
(denoted as
f 7? r
s
) according to the following model:
argmax
s
{p(r
s
|f)
w
S(f, r
s
)} : f 7? r
s
(1)
where p(r
s
|f) stands for the probability of f
belonging to rule r
s
and it is estimated via the
respective classifier. The similarity between
f and r
s
is denoted by S(f |r
s
), while w is
a fixed weight taking values in the interval
[0 ?). The fusion weight w controls the rela-
tive importance of the candidate selection and
semantic similarity modules, e.g., for w = 0
only the similarity metric S(f, r
s
) is used in the
decision. For example, consider the fragment f
?leaving <City>?. Also, assume two high-
level rules, namely, <ArrCity>={?arrive
2
The requirement for building a classifier for each gram-
mar rule is realistic for the case of SDS, especially for the typ-
ical iterative human-in-the-loop grammar development sce-
nario.
at <City>?,...} and <DepCity>=
{?depart <City>?,...}. According to (1)
f is mapped to the <DepCity> rule.
2.1 Candidate Selection
In this section, the features used for building the
candidate selection module for each r
s
? R are
briefly described. Given a pair (f ,r
s
) a two-class
statistical classification model that corresponds to
r
s
is used for estimating p(r
s
|f) in (1).
Definitions. A high-level rule r
s
can be con-
sidered as a set of fragments, e.g.,?depart
<City>?, ?leaving <City>?. For each
fragment there are two types of constituents,
namely, lexical (e.g., ?depart?,?leaving?)
and low-level rules (e.g., ?<City>?). The fol-
lowing features are extracted for r
s
considering its
respective fragments, as well as for f .
Shallow features. 1) the number of constituents
(i.e., tokens), 2) the count of lexical constituents
to the number of tokens, 3) the count of low-level
rules to the number of tokens, 4) the count of lex-
ical constituents that follow the right-most low-
level rule of the fragment, and 5) the count of low-
level rules that appear twice in a fragment.
Perplexity-based features. A fragment
?
f can
be represented as a sequence of tokens as
w
1
w
2
... w
z
. The perplexity of
?
f is defined as
PP (
?
f)=2
H(
?
f)
, where H(
?
f)=
1
z
log(p(
?
f)). p(
?
f)
stands for the probability of
?
f estimated using an
n-gram language model. Two PP values were
used as features computed for n=2, 3.
Features of lexical similarity. Four scores of lex-
ical similarity computed between f and r
s
were
used as features. Let N
s
denote the set of frag-
ments that are included in the training set of each
rule r
s
. The following metrics were employed
for computing the similarity between the unknown
fragment f and a fragment f
s
? N
s
: 1) the nor-
malized longest common subsequence (Stoilos et
al., 2005) denoted as S
C
, 2) the normalized over-
lap in character bigrams that is denoted as S
B
and
it is defined in (2), 3) a proposed variation of the
Levenshtein distance, S
L
, defined as S
L
(f, f
s
) =
l
1
?L(f,f
s
)
l
1
+d
, where l
1
and l
2
are the lengths (in char-
acters) of the lengthiest and the shortest fragment
between f and f
s
, respectively, while d= l
1
? l
2
.
L(.) stands for the Levenshtein distance (V. I. Lev-
enshtein, 1966; R. A. Wagner and M. J. Fischer,
1974). 4) if f and f
s
differ by one token exactly
S
L
is applied, otherwise their similarity is set to
0. Regarding S
C
and S
B
, the similarity between
669
f and r
s
was estimated as the maximum similarity
yielded when computing the similarities between
f and each f
s
?N
s
. For the rest metrics, the sim-
ilarity between f and r
s
was estimated by averag-
ing the |N
s
| similarities computed between f and
each f
s
?N
s
.
Heuristic features. Considering an unknown
fragment f and the set of training fragments N
s
corresponding to rule r
s
, in total nine features
were used: 1) the difference between the aver-
age length (in tokens) of fragments in N
s
and the
length of f , 2) the difference between the average
number of low-level rules in N
s
and the number
of low-level rules in f , 3) as 2) but considering
the lexical constituents instead of low-level rules,
4) the number of low-level rules shared between
N
s
and f , 5) as 4) but considering the lexical con-
stituents instead of low-level rules, 6) a boolean
function that equals 1 if f is a substring of at least
one f
s
? N
s
, 7) a boolean function that equals 1 if
f shares the same lexical constituents at least one
f
s
? N
s
, 8) a boolean function that equals 1 if f
is shorter by one token compared to any f
s
? N
s
,
9) a boolean function that equals 1 if f is lengthier
by one token compared to any f
s
? N
s
.
Selection. The aforementioned features are used
for building a binary classifier for each r
s
? R,
where 1 ? s ? m, for deciding whether f can
be regarded as a candidate member of r
s
or not.
Given an unknown fragment f these classifiers are
employed for estimating in total m probabilities
p(r
s
|f).
2.2 Similarity Metrics
Here, two types of similarity metrics are defined,
which are used for estimating S(f, r
s
) in (1).
String-based similarity. Consider two fragments
f
i
and f
j
whose sets of character bigrams are de-
noted as M
i
and M
j
, respectively. Also, M
min
=
min(|M
i
|, |M
j
|) and M
max
= max(|M
i
|, |M
j
|
). The similarity between f
i
and f
j
is based on
the overlap of their respective character bigrams
defined as (Jimenez et al., 2012):
S
B
(f
i
, f
j
) =
|M
i
?M
j
|
?M
max
+ (1? ?)M
min
, (2)
where 0??? 1, while, here we use ?=0.5. The
similarity between a fragment f and a rule r
s
is
computed by averaging the similarities computed
between f and each f
s
?N
s
.
Context-based similarity. This is a corpus-based
metric relying on the distributional hypothesis of
meaning suggesting that similarity of context im-
plies similarity of meaning (Z. Harris, 1954). A
contextual window of size 2K+1 words is cen-
tered on the fragment of interest f
i
and lexical
features are extracted. For every instance of f
i
in
the corpus the K words left and right of f
i
for-
mulate a feature vector v
i
. For a given value of K
the context-based semantic similarity between two
fragments, f
i
and f
j
, is computed as the cosine of
their feature vectors: S
K
(f
i
, f
j
) =
v
i
.v
j
||v
i
|| ||v
j
||
. The
elements of feature vectors can be weighted ac-
cording various schemes (E. Iosif and A. Potami-
anos, 2010), while, here we use a binary scheme.
The similarity between a fragment f and a rule
r
s
is computed by averaging the similarities com-
puted between f and each f
s
?N
s
.
2.3 Mapping of Unknown Fragments
The output of the described system is the mapping
of a fragment f to a single (i.e., one-to-one assign-
ment) high-level rule r
s
? R, where 1 ? s ? m.
This is achieved by applying (1). The p(r
s
|f)
probabilities were estimated as described in Sec-
tion 2.1. The S(f, r
s
) similarities were estimated
using either S
K
or S
B
defined in Section 2.2.
3 Datasets and Experiments
Datasets. The data was organized with respect to
three different domains: 1) air travel (flight book-
ing, car rental etc.), 2) tourism (information for
city guide), and 3) finance (currency exchange). In
total, there are four separate datasets: two datasets
for the air travel domain in English (EN) and
Greek (GR), one dataset for the tourism domain
in English, and one dataset for the finance domain
in English.
The number of high-level rules for each dataset
Domain #rules #train frag. #test frag.
Travel:EN 32 982 284
Travel:GR 35 956 324
Tourism:EN 24 1004 285
Finance:EN 9 136 37
Table 1: Number of rules and train/test fragments.
are shown in Table 1, along with the number
of fragments included in training and test data.
Experiments. Regarding the computation of
perplexity-based features (defined in Section 2.1)
the SRILM toolkit (A. Stolcke, 2002) was used.
The n-gram probabilities were estimated over a
corpus that was created by aggregating all the
670
valid fragments included in the training data.
For the computation of the context-based similar-
ity metric S
K
(defined in Section 2.2) a corpus
of web-harvested data was created for each do-
main/language. The context window size K was
Domain # sentences
Travel:EN 5721
Travel:GR 6359
Tourism:EN 829516
Finance:EN 168380
Table 2: Size of corpora used in S
K
metric.
set to 1. The size of the used corpora are presented
Table 2, while the process of corpus creation is
detailed in (Klasinas et al., 2013). The classifiers
used for the candidate selection module, described
in Section 2.1 were random forests with 50 trees
(L. Breiman, 2001).
4 Evaluation Metrics and Results
The proposed model defined by (1) was evaluated
in terms of weighted F-measure, (FM ). Initially,
we run our system using the training and develop-
ment set provided by the task organizers, in order
to tune the w and K parameters. The tuning was
conducted on the Travel English domain, while the
respective evaluation results are shown in Table 3
in terms of FM . We observe that the best re-
Weight w 0 1 50 500
FM 0.68 0.72 0.70 0.72
Table 3: Results for the tuning of w.
sults are achieved for w = 1 and w = 500. In
the case where w = 0 the rule mapping relies only
on the similarity metric. In addition, we exper-
imented with various values the context window
size K of the context-based similarity metric S
K
:
K = 1, 3, 7. For all values of K similar perfor-
mance was obtained (0.70). Given the aforemen-
Domains Baseline Run 1 Run 2 Run 3
Travel:EN 0.51 0.66 0.65 0.68
Travel:GR 0.26 0.52 0.49 0.49
Tourism:EN 0.87 0.86 0.85 0.86
Finance:EN 0.60 0.70 0.63 0.58
UA 0.56 0.69 0.66 0.65
WA 0.52 0.66 0.64 0.65
Table 4: Official results.
tioned tuning the following values were selected
for the official runs: w = 1, w = 500 and K = 1.
In total, three system runs were submitted:
Run 1. The character bigram similarity metric was
used, while w was set to 1.
Run 2. The context-based similarity metrics was
used with K = 1, while w was set to 1.
Run 3. The character bigram similarity metric was
used, while w was set to 500.
The results for the aforementioned runs, along
with the baseline performance are shown in Ta-
ble 4. An overview of the participating systems
suggests that our submission achieved the high-
est performance for almost all domains and lan-
guages. The weighted (WA) and unweighted (UA)
average across the 4 datasets are also presented,
where the weight depends on the number of rules
in the dataset. Using these measures, our main
run (Run 1) obtained the best results. We ob-
serve that the performance is consistently worse
for Runs 2 and 3, with the exception of the Travel
English dataset. Comparing the performance of
Runs 1 and 2, we observe that the character bigram
metric consistently outperforms the context-based
one. For individual datasets, our system underper-
forms for the Finance (in Run 3) and the Tourism
domain (in all Runs). For the case of the Finance
domain this may be attributed to the relatively lim-
ited training data.
5 Conclusions
We proposed a supervised grammar induction sys-
tem using the fusion of a grammar fragment se-
lection and similarity estimation modules. The
best configuration of our system was Run 1 which
achieved the highest performance compared to
other submissions, in almost all domains. To sum-
marize, 1) the selection module boost the sys-
tem?s performance significanlty, 2) the high per-
formance in different domains is a promising indi-
cator for domain and language portability. Future
work should involve the implementation of more
complex features for the candidate selection algo-
rithm and further investigation of phrase level sim-
ilarity metrics.
Acknowledgements
This work has been partially funded by the
projects: 1) SpeDial, and 2) PortDial, supported
by the EU Seventh Framework Programme (FP7),
with grant number 611396 and 296170, respec-
tively.
671
References
Elias Iosif and Alexandros Potamianos. 2010. Un-
supervised semantic similarity computation between
terms using web documents. IEEE Transactions on
Knowledge and Data Engineering, 22(11), pp. 1637-
1647.
Sergio Jimenez, Claudia Becerra and Alexander Gel-
bukh. 2012. Soft Cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pp. 449-453
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spyros Georgiladakis and Gianluka Mameli. 2013.
Web data harvesting for speech understanding
grammar induction. in Proceedings of the Inter-
speech.
Helen M. Meng and Kai-Chung Siu 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1), pp. 172-181.
PortDial Project free data deliverable D3.1.
https://sites.google.com/site/portdial2/deliverables-
publication
Andreas Stolcke 2002 Srilm-an extensible language
modeling toolkit in Proceedings of the Interspeech
2002
Karim Lari and Steve J. Young 2002. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1), pp. 35-56.
Stanley F. Chen 1995. Bayesian grammar induction
for language modeling. in Proceedings of the 33rd
annual meeting of ACL
Zellig Harris 1954. Distributional structure. Word,
10(23), pp. 146-162.
Rebecca Hwa 1999. Supervised grammar induction
using training data with limited constituent informa-
tion. in Proceedings of the 37th annual meeting of
ACL
Matthew Lease, Eugene Charniak, and Mark Johnson
2005. Parsing and its applications for conversa-
tional speech. in Proceedings of Acoustics, Speech,
and Signal Processing (ICASSP)
Vladimir I. Levenshtein 1966. Binary codes capable
of correcting deletions, insertions and reversals. in
Soviet physics doklady, 10(8), pp. 707-710.
Leo Breiman 2001. Random forests. in Machine
Learning, 45(1), pp. 5-32.
Dan Jurafsky and James H. Martin 2009. Speech
and language processing an introduction to natural
language processing, computational linguistics, and
speech. Pearson Education Inc
Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias
2005. A string metric for ontology alignment. in
The Semantic WebISWC, pp. 624637
Robert A. Wagner and Michael J. Fisher 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1), pp. 168-173
Katerina Frantzi and Sophia Ananiadou 1997. Au-
tomatic term recognition using contextual cues. in
Proceedings of International Joint Conferences on
Artificial Intelligence
Elias Iosif and Alexandros Potamianos 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. in Proceedings of Interspeech
Jeffrey Mitchell and Mirela Lapata 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388-1429.
Ye-Yi Wang and Alex Acero 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3), pp. 360-416.
Eric Brill 1992. A simple rule-based part of speech
tagger. in Proceedings of the workshop on Speech
and Natural Language
Alexander Clark 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. in Proceedings of the 2001 work-
shop on Computational Natural Language Learning
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
2009. Unsupervised multilingual grammar induc-
tion. in Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL
Aarne Ranta 2009. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming: 14(2), pp. 145-189
Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,
Alexandros Potamianos and Augustine Tsai 2009.
Auto-induced Semantic Classes. Speech Communi-
cation: 43(3), pp. 183-203
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. in Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*Sem), pp. 385-393
672
