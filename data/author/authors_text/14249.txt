Proceedings of NAACL HLT 2009: Short Papers, pages 185?188,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
MICA: A Probabilistic Dependency Parser
Based on Tree Insertion Grammars
Application Note
Srinivas Bangalore Pierre Boulllier
AT&T Labs ? Research INRIA
Florham Park, NJ, USA Rocquencourt, France
srini@research.att.com Pierre.Boullier@inria.fr
Alexis Nasr Owen Rambow Beno??t Sagot
Aix-Marseille Universite? CCLS, Columbia Univserity INRIA
Marseille, France New York, NY, USA Rocquencourt, France
alexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.fr
Abstract
MICA is a dependency parser which returns
deep dependency representations, is fast, has
state-of-the-art performance, and is freely
available.
1 Overview
This application note presents a freely avail-
able parser, MICA (Marseille-INRIA-Columbia-
AT&T).1 MICA has several key characteristics that
make it appealing to researchers in NLP who need
an off-the-shelf parser.
? MICA returns a deep dependency parse, in
which dependency is defined in terms of lex-
ical predicate-argument structure, not in terms
of surface-syntactic features such as subject-verb
agreement. Function words such as auxiliaries
and determiners depend on their lexical head, and
strongly governed prepositions (such as to for give)
are treated as co-heads rather than as syntactic heads
in their own right. For example, John is giving books
to Mary gets the following analysis (the arc label is
on the terminal).
giving
John
arc=0
is
arc=adj
books
arc=1
to
arc=co-head
Mary
arc=2
The arc labels for the three arguments John,
books, and Mary do not change when the sentence
is passivized or Mary undergoes dative shift.
1We would like to thank Ryan Roth for contributing the
MALT data.
? MICA is based on an explicit phrase-structure
tree grammar extracted from the Penn Treebank.
Therefore, MICA can associate dependency parses
with rich linguistic information such as voice, the
presence of empty subjects (PRO), wh-movement,
and whether a verb heads a relative clause.
?MICA is fast (450 words per second plus 6 sec-
onds initialization on a standard high-end machine
on sentences with fewer than 200 words) and has
state-of-the-art performance (87.6% unlabeled de-
pendency accuracy, see Section 5).
? MICA consists of two processes: the supertag-
ger, which associates tags representing rich syntac-
tic information with the input word sequence, and
the actual parser, which derives the syntactic struc-
ture from the n-best chosen supertags. Only the su-
pertagger uses lexical information, the parser only
sees the supertag hypotheses.
? MICA returns n-best parses for arbitrary n;
parse trees are associated with probabilities. A
packed forest can also be returned.
? MICA is freely available2, easy to install under
Linux, and easy to use. (Input is one sentence per
line with no special tokenization required.)
There is an enormous amount of related work,
and we can mention only the most salient, given
space constraints. Our parser is very similar to the
work of (Shen and Joshi, 2005). They do not em-
ploy a supertagging step, and we do not restrict our
trees to spinal projections. Other parsers using su-
pertagging include the LDA of Bangalore and Joshi
(1999), the CCG-based parser of Clark and Curran
(2004), and the constraint-based approach of Wang
2http://www1.ccls.columbia.edu/?rambow/mica.html
185
and Harper (2004). Widely used dependency parsers
which generate deep dependency representations in-
clude Minipar (Lin, 1994), which uses a declarative
grammar, and the Stanford parser (Levy and Man-
ning, 2004), which performs a conversion from a
standard phrase-structure parse. All of these systems
generate dependency structures which are slightly
different from MICA?s, so that direct comparison
is difficult. For comparison purposes, we therefore
use the MALT parser generator (Nivre et al, 2004),
which allows us to train a dependency parser on our
own dependency structures. MALT has been among
the top performers in the CoNLL dependency pars-
ing competitions.
2 Supertags and Supertagging
Supertags are elementary trees of a lexicalized
tree grammar such as a Tree-Adjoining Gram-
mar (TAG) (Joshi, 1987). Unlike context-free gram-
mar rules which are single level trees, supertags are
multi-level trees which encapsulate both predicate-
argument structure of the anchor lexeme (by includ-
ing nodes at which its arguments must substitute)
and morpho-syntactic constraints such as subject-
verb agreement within the supertag associated with
the anchor. There are a number of supertags for each
lexeme to account for the different syntactic trans-
formations (relative clause, wh-question, passiviza-
tion etc.). For example, the verb give will be associ-
ated with at least these two trees, which we will call
tdi and tdi-dat. (There are also many other trees.)
tdi tdi-dat
S
NP0 ? VP
V? NP1 ? PP
P
to
NP2 ?
S
NP0 ? VP
V? NP2 ?NP1 ?
Supertagging is the task of disambiguating among
the set of supertags associated with each word in
a sentence, given the context of the sentence. In
order to arrive at a complete parse, the only step
remaining after supertagging is establishing the at-
tachments among the supertags. Hence the result of
supertagging is termed as an ?almost parse? (Banga-
lore and Joshi, 1999).
The set of supertags is derived from the Penn
Treebank using the approach of Chen (2001). This
extraction procedure results in a supertag set of
4,727 supertags and about one million words of su-
pertag annotated corpus. We use 950,028 annotated
words for training (Sections 02-21) and 46,451 (Sec-
tion 00) annotated words for testing in our exper-
iments. We estimate the probability of a tag se-
quence directly as in discriminative classification
approaches. In such approaches, the context of the
word being supertagged is encoded as features for
the classifier. Given the large scale multiclass la-
beling nature of the supertagging task, we train su-
pertagging models as one-vs-rest binary classifica-
tion problems. Detailed supertagging experiment re-
sults are reported in (Bangalore et al, 2005) which
we summarize here. We use the lexical, part-of-
speech attributes from the left and right context
in a 6-word window and the lexical, orthographic
(e.g. capitalization, prefix, suffix, digit) and part-
of-speech attributes of the word being supertagged.
Crucially, this set does not use the supertags for the
words in the history. Thus during decoding the su-
pertag assignment is done locally and does not need
a dynamic programming search. We trained a Max-
ent model with such features using the labeled data
set mentioned above and achieve an error rate of
11.48% on the test set.
3 Grammars and Models
MICA grammars are extracted in a three steps pro-
cess. In a first step, a Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995) is extracted from the
treebank, along with a table of counts. This is the
grammar that is used for supertagging, as described
in Section 2. In a second step, the TIG and the count
table are used to build a PCFG. During the last step,
the PCFG is ?specialized? in order to model more
finely some lexico-syntactic phenomena. The sec-
ond and third steps are discussed in this section.
The extracted TIG is transformed into a PCFG
which generates strings of supertags as follows. Ini-
tial elementary trees (which are substituted) yield
rules whose left hand side is the root category of
the elementary tree. Left (respectively right) aux-
iliary trees (the trees for which the foot node is the
186
left (resp. right) daughter of the root) give birth to
rules whose left-hand side is of the form Xl (resp.
Xr), where X is the root category of the elementary
tree. The right hand side of each rule is built during
a top down traversal of the corresponding elemen-
tary tree. For every node of the tree visited, a new
symbol is added to the right hand side of rule, from
left to right, as follows:
? The anchor of the elementary tree adds the su-
pertag (i.e., the name of the tree), which is a terminal
symbol, to the context-free rule.
? A substitution node in the elementary tree adds
its nonterminal symbol to the context-free rule.
? A interior node in the elementary tree at which
adjunction may occur adds to the context-free rule
the nonterminal symbol X ?r or X ?l , where X is the
node?s nonterminal symbol, and l (resp. r) indicates
whether it is a left (resp. right) adjunction. Each
interior node is visited twice, the first time from the
left, and then from the right. A set of non-lexicalized
rules (i.e., rules that do not generate a terminal sym-
bol) allow us to generate zero or more trees anchored
by Xl from the symbol X ?l . No adjunction, the first
adjunction, and the second adjunction are modeled
explicitly in the grammar and the associated prob-
abilistic model, while the third and all subsequent
adjunctions are modeled together.
This conversion method is basically the same as
that presented in (Schabes and Waters, 1995), ex-
cept that our PCFG models multiple adjunctions at
the same node by positions (a concern Schabes and
Waters (1995) do not share, of course). Our PCFG
construction differs from that of Hwa (2001) in that
she does not allow multiple adjunction at one node
(Schabes and Shieber, 1994) (which we do since we
are interested in the derivation structure as a repre-
sentation of linguistic dependency). For more in-
formation about the positional model of adjunction
and a discussion of an alternate model, the ?bigram
model?, see (Nasr and Rambow, 2006).
Tree tdi from Section 2 gives rise to the following
rule (where tdi and tCO are terminal symbols and
the rest are nonterminals): S ? S?l NP VP?l V?l tdiV?r NP PP?l P?l tCO P?r NP PP?r VP?r S?r
The probabilities of the PCFG rules are estimated
using maximum likelihood. The probabilistic model
refers only to supertag names, not to words. In the
basic model, the probability of the adjunction or sub-
stitution of an elementary tree (the daughter) in an-
other elementary tree (the mother) only depends on
the nonterminal, and does not depend on the mother
nor on the node on which the attachment is per-
formed in the mother elementary tree. It is well
known that such a dependency is important for an
adequate probabilistic modelling of syntax. In order
to introduce such a dependency, we condition an at-
tachment on the mother and on the node on which
the attachment is performed, an operation that we
call mother specialization. Mother specialization is
performed by adding to all nonterminals the name of
the mother and the address of a node. The special-
ization of a grammar increase vastly the number of
symbols and rules and provoke severe data sparse-
ness problems, this is why only a subset of the sym-
bols are specialized.
4 Parser
SYNTAX (Boullier and Deschamp, 1988) is a sys-
tem used to generate lexical and syntactic analyzers
(parsers) (both deterministic and non-deterministic)
for all kind of context-free grammars (CFGs) as
well as some classes of contextual grammars. It
has been under development at INRIA for several
decades. SYNTAX handles most classes of determin-
istic (unambiguous) grammars (LR, LALR, RLR)
as well as general context-free grammars. The
non-deterministic features include, among others,
an Earley-like parser generator used for natural lan-
guage processing (Boullier, 2003).
Like most SYNTAX Earley-like parsers, the archi-
tecture of MICA?s PCFG-based parser is the follow-
ing:
? The Earley-like parser proper computes a shared
parse forest that represents in a factorized (polyno-
mial) way all possible parse trees according to the
underlying (non-probabilistic) CFG that represents
the TIG;
? Filtering and/or decoration modules are applied
on the shared parse forest; in MICA?s case, an n-
best module is applied, followed by a dependency
extractor that relies on the TIG structure of the CFG.
The Earley-like parser relies on Earley?s algo-
rithm (Earley, 1970). However, several optimiza-
tions have been applied, including guiding tech-
niques (Boullier, 2003), extensive static (offline)
187
computations over the grammar, and efficient data
structures. Moreover, Earley?s algorithm has been
extended so as to handle input DAGs (and not only
sequences of forms). A particular effort has been
made to handle huge grammars (over 1 million
symbol occurrences in the grammar), thanks to ad-
vanced dynamic lexicalization techniques (Boullier
and Sagot, 2007). The resulting efficiency is satisfy-
ing: with standard ambiguous NLP grammars, huge
shared parse forest (over 1010 trees) are often gener-
ated in a few dozens of milliseconds.
Within MICA, the first module that is applied on
top of the shared parse forest is SYNTAX?s n-best
module. This module adapts and implements the al-
gorithm of (Huang and Chiang, 2005) for efficient
n-best trees extraction from a shared parse forest. In
practice, and within the current version of MICA,
this module is usually used with n = 1, which iden-
tifies the optimal tree w.r.t. the probabilistic model
embedded in the original PCFG; other values can
also be used. Once the n-best trees have been ex-
tracted, the dependency extractor module transforms
each of these trees into a dependency tree, by ex-
ploiting the fact that the CFG used for parsing has
been built from a TIG.
5 Evaluation
We compare MICA to the MALT parser. Both
parsers are trained on sections 02-21 of our de-
pendency version of the WSJ PennTreebank, and
tested on Section 00, not counting true punctuation.
?Predicted? refers to tags (PTB-tagset POS and su-
pertags) predicted by our taggers; ?Gold? refers to
the gold POS and supertags. We tested MALT using
only POS tags (MALT-POS), and POS tags as well
as 1-best supertags (MALT-all). We provide unla-
beled (?Un?) and labeled (?Lb?) dependency accu-
racy (%). As we can see, the predicted supertags do
not help MALT. MALT is significantly slower than
MICA, running at about 30 words a second (MICA:
450 words a second).
MICA MALT-POS MALT-all
Pred Gold Pred Gold Pred Gold
Lb 85.8 97.3 86.9 87.4 86.8 96.9
Un 87.6 97.6 88.9 89.3 88.5 97.2
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?266.
Srinivas Bangalore, Patrick Haffner, and Gae?l Emami.
2005. Factoring global inference by enriching local rep-
resentations. Technical report, AT&T Labs ? Reserach.
Pierre Boullier and Philippe Deschamp.
1988. Le syste`me SYNTAXTM ? manuel
d?utilisation et de mise en ?uvre sous UNIXTM.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Beno??t Sagot. 2007. Are very large
grammars computationnaly tractable? In Proceedings of
IWPT?07, Prague, Czech Republic.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 7th International Workshop on =20 Pars-
ing Technologies, pages 43?54, Nancy, France.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D. the-
sis, University of Delaware.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL?04.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, Vancouver, Canada.
Rebecca Hwa. 2001. Learning Probabilistic Lexicalized
Grammars for Natural Language Processing. Ph.D. the-
sis, Harvard University.
Aravind K. Joshi. 1987. An introduction to Tree Ad-
joining Grammars. In A. Manaster-Ramer, editor, Math-
ematics of Language. John Benjamins, Amsterdam.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Correct-
ing the surface dependency approximation. In ACL?04.
Dekang Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Coling?94.
Alexis Nasr and Owen Rambow. 2006. Parsing with
lexicalized probabilistic recursive transition networks. In
Finite-State Methods and Natural Language Processing,
Springer Verlag Lecture Notes in Commputer Science.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL-2004.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computational
Linguistics, 1(20):91?124.
Yves Schabes and Richard C. Waters. 1995. Tree Inser-
tion Grammar. Computational Linguistics, 21(4).
Libin Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP?05.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proceed-
ings of the ACL Workshop on Incremental Parsing.
188
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 329?336,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Error mining in parsing results
Beno?t Sagot and ?ric de la Clergerie
Projet ATOLL - INRIA
Domaine de Voluceau, B.P. 105
78153 Le Chesnay Cedex, France
{benoit.sagot,eric.de_la_clergerie}@inria.fr
Abstract
We introduce an error mining technique
for automatically detecting errors in re-
sources that are used in parsing systems.
We applied this technique on parsing re-
sults produced on several million words by
two distinct parsing systems, which share
the syntactic lexicon and the pre-parsing
processing chain. We were thus able to
identify missing and erroneous informa-
tion in these resources.
1 Introduction
Natural language parsing is a hard task, partly be-
cause of the complexity and the volume of infor-
mation that have to be taken into account about
words and syntactic constructions. However, it
is necessary to have access to such information,
stored in resources such as lexica and grammars,
and to try and minimize the amount of missing
and erroneous information in these resources. To
achieve this, the use of these resources at a large-
scale in parsers is a very promising approach (van
Noord, 2004), and in particular the analysis of sit-
uations that lead to a parsing failure: one can learn
from one?s own mistakes.
We introduce a probabilistic model that allows
to identify forms and form bigrams that may be
the source of errors, thanks to a corpus of parsed
sentences. In order to facilitate the exploitation of
forms and form bigrams detected by the model,
and in particular to identify causes of errors, we
have developed a visualization environment. The
whole system has been tested on parsing results
produced for several multi-million-word corpora
and with two different parsers for French, namely
SXLFG and FRMG.
However, the error mining technique which
is the topic of this paper is fully system- and
language-independent. It could be applied with-
out any change on parsing results produced by any
system working on any language. The only infor-
mation that is needed is a boolean value for each
sentence which indicates if it has been success-
fully parsed or not.
2 Principles
2.1 General idea
The idea we implemented is inspired from (van
Noord, 2004). In order to identify missing and er-
roneous information in a parsing system, one can
analyze a large corpus and study with statistical
tools what differentiates sentences for which pars-
ing succeeded from sentences for which it failed.
The simplest application of this idea is to look
for forms, called suspicious forms, that are found
more frequently in sentences that could not be
parsed. This is what van Noord (2004) does, with-
out trying to identify a suspicious form in any sen-
tence whose parsing failed, and thus without tak-
ing into account the fact that there is (at least)
one cause of error in each unparsable sentence.1
On the contrary, we will look, in each sentence
on which parsing failed, for the form that has
the highest probability of being the cause of this
failure: it is the main suspect of the sentence.
This form may be incorrectly or only partially de-
scribed in the lexicon, it may take part in construc-
tions that are not described in the grammar, or it
may exemplify imperfections of the pre-syntactic
processing chain. This idea can be easily extended
to sequences of forms, which is what we do by tak-
1Indeed, he defines the suspicion rate of a form f as the
rate of unparsable sentences among sentences that contain f .
329
ing form bigrams into account, but also to lemmas
(or sequences of lemmas).
2.2 Form-level probabilistic model
We suppose that the corpus is split in sentences,
sentences being segmented in forms. We denote
by si the i-th sentence. We denote by oi,j, (1 ?
j ? |si|) the occurrences of forms that constitute
si, and by F (oi,j) the corresponding forms. Fi-
nally, we call error the function that associates to
each sentence si either 1, if si?s parsing failed, and
0 if it succeeded.
Let Of be the set of the occurrences of a form
f in the corpus: Of = {oi,j|F (oi,j) = f}. The
number of occurrences of f in the corpus is there-
fore |Of |.
Let us define at first the mean global suspicion
rate S, that is the mean probability that a given oc-
currence of a form be the cause of a parsing fail-
ure. We make the assumption that the failure of
the parsing of a sentence has a unique cause (here,
a unique form. . . ). This assumption, which is not
necessarily exactly verified, simplifies the model
and leads to good results. If we call occtotal the
total amount of forms in the corpus, we have then:
S = ?ierror(si)occtotal
Let f be a form, that occurs as the j-th form of
sentence si, which means that F (oi,j) = f . Let us
assume that si?s parsing failed: error(si) = 1. We
call suspicion rate of the j-th form oi,j of sentence
si the probability, denoted by Si,j , that the occur-
rence oi,j of form form f be the cause of the si?s
parsing failure. If, on the contrary, si?s parsing
succeeded, its occurrences have a suspicion rate
that is equal to zero.
We then define the mean suspicion rate Sf of
a form f as the mean of all suspicion rates of its
occurrences:
Sf =
1
|Of |
?
?
oi,j?Of
Si,j
To compute these rates, we use a fix-point al-
gorithm by iterating a certain amount of times the
following computations. Let us assume that we
just completed the n-th iteration: we know, for
each sentence si, and for each occurrence oi,j of
this sentence, the estimation of its suspicion rate
Si,j as computed by the n-th iteration, estimation
that is denoted by S(n)i,j . From this estimation, we
compute the n + 1-th estimation of the mean sus-
picion rate of each form f , denoted by S(n+1)f :
S(n+1)f =
1
|Of |
?
?
oi,j?Of
S(n)i,j
This rate2 allows us to compute a new estima-
tion of the suspicion rate of all occurrences, by
giving to each occurrence if a sentence si a sus-
picion rate S(n+1)i,j that is exactly the estimation
S(n+1)f of the mean suspicion rate of Sf of the cor-
responding form, and then to perform a sentence-
level normalization. Thus:
S(n+1)i,j = error(si) ?
S(n+1)F (oi,j)
?
1?j?|si| S
(n+1)
F (oi,j)
At this point, the n+1-th iteration is completed,
and we can resume again these computations, un-
til convergence on a fix-point. To begin the whole
process, we just say, for an occurrence oi,j of sen-
tence si, that S(0)i,j = error(si)/|si|. This means
that for a non-parsable sentence, we start from a
baseline where all of its occurrences have an equal
probability of being the cause of the failure.
After a few dozens of iterations, we get stabi-
lized estimations of the mean suspicion rate each
form, which allows:
? to identify the forms that most probably cause
errors,
? for each form f , to identify non-parsable sen-
tences si where an occurrence oi,j ? Of of f
is a main suspect and where oi,j has a very
2We also performed experiment in which Sf was esti-
mated by an other estimator, namely the smoothed mean sus-
picion rate, denoted by S?(n)f , that takes into account the num-
ber of occurrences of f . Indeed, the confidence we can have
in the estimation S(n)f is lower if the number of occurrences
of f is lower. Hence the idea to smooth S(n)f by replacing it
with a weighted mean S?(n)f between S
(n)
f and S, where the
weights ? and 1 ? ? depend on |Of |: if |Of | is high, S?(n)f
will be close from S(n)f ; if it is low, it will be closer from S:
S?(n)f = ?(|Of |) ? S
(n)
f + (1 ? ?(|Of |)) ? S.
In these experiments, we used the smoothing function
?(|Of |) = 1 ? e??|Of | with ? = 0.1. But this model,
used with the ranking according to Mf = Sf ? ln |Of | (see
below), leads results that are very similar to those obtained
without smoothing. Therefore, we describe the smoothing-
less model, which has the advantage not to use an empirically
chosen smoothing function.
330
high suspicion rate among all occurrences of
form f .
We implemented this algorithm as a perl script,
with strong optimizations of data structures so as
to reduce memory and time usage. In particu-
lar, form-level structures are shared between sen-
tences.
2.3 Extensions of the model
This model gives already very good results, as we
shall see in section 4. However, it can be extended
in different ways, some of which we already im-
plemented.
First of all, it is possible not to stick to forms.
Indeed, we do not only work on forms, but on cou-
ples made out of a form (a lexical entry) and one
or several token(s) that correspond to this form in
the raw text (a token is a portion of text delimited
by spaces or punctuation tokens).
Moreover, one can look for the cause of the fail-
ure of the parsing of a sentence not only in the
presence of a form in this sentence, but also in the
presence of a bigram3 of forms. To perform this,
one just needs to extend the notions of form and
occurrence, by saying that a (generalized) form is
a unigram or a bigram of forms, and that a (gen-
eralized) occurrence is an occurrence of a gener-
alized form, i.e., an occurrence of a unigram or a
bigram of forms. The results we present in sec-
tion 4 includes this extension, as well as the previ-
ous one.
Another possible generalization would be to
take into account facts about the sentence that are
not simultaneous (such as form unigrams and form
bigrams) but mutually exclusive, and that must
therefore be probabilized as well. We have not yet
implemented such a mechanism, but it would be
very interesting, because it would allow to go be-
yond forms or n-grams of forms, and to manipu-
late also lemmas (since a given form has usually
several possible lemmas).
3 Experiments
In order to validate our approach, we applied
these principles to look for error causes in pars-
ing results given by two deep parsing systems for
French, FRMG and SXLFG, on large corpora.
3One could generalize this to n-grams, but as n gets
higher the number of occurrences of n-grams gets lower,
hence leading to non-significant statistics.
3.1 Parsers
Both parsing systems we used are based on deep
non-probabilistic parsers. They share:
? the Lefff 2 syntactic lexicon for French
(Sagot et al, 2005), that contains 500,000 en-
tries (representing 400,000 different forms) ;
each lexical entry contains morphological in-
formation, sub-categorization frames (when
relevant), and complementary syntactic infor-
mation, in particular for verbal forms (con-
trols, attributives, impersonals,. . . ),
? the SXPipe pre-syntactic processing chain
(Sagot and Boullier, 2005), that converts a
raw text in a sequence of DAGs of forms that
are present in the Lefff ; SXPipe contains,
among other modules, a sentence-level seg-
menter, a tokenization and spelling-error cor-
rection module, named-entities recognizers,
and a non-deterministic multi-word identifier.
But FRMG and SXLFG use completely different
parsers, that rely on different formalisms, on dif-
ferent grammars and on different parser builder.
Therefore, the comparison of error mining results
on the output of these two systems makes it possi-
ble to distinguish errors coming from the Lefff or
from SXPipe from those coming to one grammar
or the other. Let us describe in more details the
characteristics of these two parsers.
The FRMG parser (Thomasset and Villemonte
de la Clergerie, 2005) is based on a compact TAG
for French that is automatically generated from
a meta-grammar. The compilation and execution
of the parser is performed in the framework of
the DYALOG system (Villemonte de la Clergerie,
2005).
The SXLFG parser (Boullier and Sagot, 2005b;
Boullier and Sagot, 2005a) is an efficient and ro-
bust LFG parser. Parsing is performed in two
steps. First, an Earley-like parser builds a shared
forest that represents all constituent structures that
satisfy the context-free skeleton of the grammar.
Then functional structures are built, in one or more
bottom-up passes. Parsing efficiency is achieved
thanks to several techniques such as compact data
representation, systematic use of structure and
computation sharing, lazy evaluation and heuristic
and almost non-destructive pruning during pars-
ing.
Both parsers implement also advanced error re-
covery and tolerance techniques, but they were
331
corpus #sentences #success (%) #forms #occ S (%) Date
MD/FRMG 330,938 136,885 (41.30%) 255,616 10,422,926 1.86% Jul. 05
MD/SXLFG 567,039 343,988 (60.66%) 327,785 14,482,059 1.54% Mar. 05
EASy/FRMG 39,872 16,477 (41.32%) 61,135 878,156 2.66% Dec. 05
EASy/SXLFG 39,872 21,067 (52.84%) 61,135 878,156 2.15% Dec. 05
Table 1: General information on corpora and parsing results
useless for the experiments described here, since
we want only to distinguish sentences that receive
a full parse (without any recovery technique) from
those that do not.
3.2 Corpora
We parsed with these two systems the following
corpora:
MD corpus : This corpus is made out of 14.5
million words (570,000 sentences) of general
journalistic corpus that are articles from the
Monde diplomatique.
EASy corpus : This is the 40,000-sentence cor-
pus that has been built for the EASy parsing
evaluation campaign for French (Paroubek et
al., 2005). We only used the raw corpus
(without taking into account the fact that a
manual parse is available for 10% of all sen-
tences). The EASy corpus contains several
sub-corpora of varied style: journalistic, lit-
eracy, legal, medical, transcription of oral, e-
mail, questions, etc.
Both corpora are raw in the sense that no clean-
ing whatsoever has been performed so as to elimi-
nate some sequences of characters that can not re-
ally be considered as sentences.
Table 1 gives some general information on these
corpora as well as the results we got with both
parsing systems. It shall be noticed that both
parsers did not parse exactly the same set and the
same number of sentences for the MD corpus, and
that they do not define in the exactly same way the
notion of sentence.
3.3 Results visualization environment
We developed a visualization tool for the results of
the error mining, that allows to examine and an-
notate them. It has the form of an HTML page
that uses dynamic generation methods, in particu-
lar javascript. An example is shown on Figure 1.
To achieve this, suspicious forms are ranked ac-
cording to a measure Mf that models, for a given
form f , the benefit there is to try and correct the
(potential) corresponding error in the resources. A
user who wants to concentrate on almost certain
errors rather than on most frequent ones can visu-
alize suspicious forms ranked according to Mf =
Sf . On the contrary, a user who wants to concen-
trate on most frequent potential errors, rather than
on the confidence that the algorithm has given to
errors, can visualize suspicious forms ranked ac-
cording to4 Mf = Sf |Of |. The default choice,
which is adopted to produce all tables shown in
this paper, is a balance between these two possi-
bilities, and ranks suspicious forms according to
Mf = Sf ? ln |Of |.
The visualization environment allows to browse
through (ranked) suspicious forms in a scrolling
list on the left part of the page (A). When the suspi-
cious form is associated to a token that is the same
as the form, only the form is shown. Otherwise,
the token is separated from the form by the sym-
bol ? / ?. The right part of the page shows various
pieces of information about the currently selected
form. After having given its rank according to the
ranking measure Mf that has been chosen (B), a
field is available to add or edit an annotation as-
sociated with the suspicious form (D). These an-
notations, aimed to ease the analysis of the error
mining results by linguists and by the developers
of parsers and resources (lexica, grammars), are
saved in a database (SQLITE). Statistical informa-
tion is also given about f (E), including its number
of occurrences occf , the number of occurrences of
f in non-parsable sentences, the final estimation
of its mean suspicion rate Sf and the rate err(f)
of non-parsable sentences among those where f
appears. This indications are complemented by a
brief summary of the iterative process that shows
the convergence of the successive estimations of
Sf . The lower part of the page gives a mean to
identify the cause of f -related errors by showing
4Let f be a form. The suspicion rate Sf can be considered
as the probability for a particular occurrence of f to cause
a parsing error. Therefore, Sf |Of | models the number of
occurrences of f that do cause a parsing error.
332
AB
C
D
E
F
G
H
Figure 1: Error mining results visualization environment (results are shown for MD/FRMG).
f ?s entries in the Lefff lexicon (G) as well as non-
parsable sentences where f is the main suspect
and where one of its occurrences has a particularly
high suspicion rate5 (H).
The whole page (with annotations) can be sent
by e-mail, for example to the developer of the lex-
icon or to the developer of one parser or the other
(C).
4 Results
In this section, we mostly focus on the results of
our error mining algorithm on the parsing results
provided by SXLFG on the MD corpus. We first
present results when only forms are taken into ac-
count, and then give an insight on results when
both forms and form bigrams are considered.
5Such an information, which is extremely valuable for the
developers of the resources, can not be obtained by global
(form-level and not occurrence-level) approaches such as the
err(f)-based approach of (van Noord, 2004). Indeed, enu-
merating all sentences which include a given form f , and
which did not receive a full parse, is not precise enough:
it would show at the same time sentences wich fail be-
cause of f (e.g., because its lexical entry lacks a given sub-
categorization frame) and sentences which fail for an other
independent reason.
4.1 Finding suspicious forms
The execution of our error mining script on
MD/SXLFG, with imax = 50 iterations and when
only (isolated) forms are taken into account, takes
less than one hour on a 3.2 GHz PC running
Linux with a 1.5 Go RAM. It outputs 18,334 rele-
vant suspicious forms (out of the 327,785 possible
ones), where a relevant suspicious form is defined
as a form f that satisfies the following arbitrary
constraints:6 S(imax)f > 1, 5 ? S and |Of | > 5.
We still can not prove theoretically the conver-
gence of the algorithm.7 But among the 1000 best-
ranked forms, the last iteration induces a mean
variation of the suspicion rate that is less than
0.01%.
On a smaller corpus like the EASy corpus, 200
iterations take 260s. The algorithm outputs less
than 3,000 relevant suspicious forms (out of the
61,125 possible ones). Convergence information
6These constraints filter results, but all forms are taken
into account during all iterations of the algorithm.
7However, the algorithms shares many common points
with iterative algorithm that are known to converge and that
have been proposed to find maximum entropy probability dis-
tributions under a set of constraints (Berger et al, 1996).
Such an algorithm is compared to ours later on in this paper.
333
is the same as what has been said above for the
MD corpus.
Table 2 gives an idea of the repartition of sus-
picious forms w.r.t. their frequency (for FRMG on
MD), showing that rare forms have a greater prob-
ability to be suspicious. The most frequent suspi-
cious form is the double-quote, with (only) Sf =
9%, partly because of segmentation problems.
4.2 Analyzing results
Table 3 gives an insight on the output of our algo-
rithm on parsing results obtained by SXLFG on the
MD corpus. For each form f (in fact, for each cou-
ple of the form (token,form)), this table displays its
suspicion rate and its number of occurrences, as
well as the rate err(f) of non-parsable sentences
among those where f appears and a short manual
analysis of the underlying error.
In fact, a more in-depth manual analysis of the
results shows that they are very good: errors are
correctly identified, that can be associated with
four error sources: (1) the Lefff lexicon, (2) the
SXPipe pre-syntactic processing chain, (3) imper-
fections of the grammar, but also (4) problems re-
lated to the corpus itself (and to the fact that it
is a raw corpus, with meta-data and typographic
noise).
On the EASy corpus, results are also relevant,
but sometimes more difficult to interpret, because
of the relative small size of the corpus and because
of its heterogeneity. In particular, it contains e-
mail and oral transcriptions sub-corpora that in-
troduce a lot of noise. Segmentation problems
(caused both by SXPipe and by the corpus itself,
which is already segmented) play an especially
important role.
4.3 Comparing results with results of other
algorithms
In order to validate our approach, we compared
our results with results given by two other relevant
algorithms:
? van Noord?s (van Noord, 2004) (form-level
and non-iterative) evaluation of err(f) (the
rate of non-parsable sentences among sen-
tences containing the form f ),
? a standard (occurrence-level and iterative)
maximum entropy evaluation of each form?s
contribution to the success or the failure of
a sentence (we used the MEGAM package
(Daum? III, 2004)).
As done for our algorithm, we do not rank forms
directly according to the suspicion rate Sf com-
puted by these algorithms. Instead, we use the Mf
measure presented above (Mf = Sf ?ln |Of |). Us-
ing directly van Noord?s measure selects as most
suspicious words very rare words, which shows
the importance of a good balance between suspi-
cion rate and frequency (as noted by (van Noord,
2004) in the discussion of his results). This remark
applies to the maximum entropy measure as well.
Table 4 shows for all algorithms the 10 best-
ranked suspicious forms, complemented by a man-
ual evaluation of their relevance. One clearly sees
that our approach leads to the best results. Van
Noord?s technique has been initially designed to
find errors in resources that already ensured a very
high coverage. On our systems, whose develop-
ment is less advanced, this technique ranks as most
suspicious forms those which are simply the most
frequent ones. It seems to be the case for the stan-
dard maximum entropy algorithm, thus showing
the importance to take into account the fact that
there is at least one cause of error in any sentence
whose parsing failed, not only to identify a main
suspicious form in each sentence, but also to get
relevant global results.
4.4 Comparing results for both parsers
We complemented the separated study of error
mining results on the output of both parsers by
an analysis of merged results. We computed for
each form the harmonic mean of both measures
Mf = Sf ? ln |Of | obtained for each parsing sys-
tem. Results (not shown here) are very interest-
ing, because they identify errors that come mostly
from resources that are shared by both systems
(the Lefff lexicon and the pre-syntactic processing
chain SXPipe). Although some errors come from
common lacks of coverage in both grammars, it
is nevertheless a very efficient mean to get a first
repartition between error sources.
4.5 Introducing form bigrams
As said before, we also performed experiments
where not only forms but also form bigrams are
treated as potential causes of errors. This approach
allows to identify situations where a form is not in
itself a relevant cause of error, but leads often to
a parse failure when immediately followed or pre-
ceded by an other form.
Table 5 shows best-ranked form bigrams (forms
that are ranked in-between are not shown, to em-
334
#occ > 100 000 > 10 000 > 1000 > 100 > 10
#forms 13 84 947 8345 40 393
#suspicious forms (%) 1 (7.6%) 13 (15.5%) 177 (18.7%) 1919 (23%) 12 022 (29.8%)
Table 2: Suspicious forms repartition for MD/FRMG
Rank Token(s)/form S(50)f |Of | err(f) Mf Error cause
1 _____/_UNDERSCORE 100% 6399 100% 8.76 corpus: typographic noise
2 (...) 46% 2168 67% 2.82 SXPipe: should be treated as skippable words
3 2_]/_NUMBER 76% 30 93% 2.58 SXPipe: bad treatment of list constructs
4 priv?es 39% 589 87% 2.53 Lefff : misses as an adjective
5 Haaretz/_Uw 51% 149 70% 2.53 SXPipe: needs local grammars for references
6 contest? 52% 122 90% 2.52 Lefff : misses as an adjective
7 occup?s 38% 601 86% 2.42 Lefff : misses as an adjective
8 priv?e 35% 834 82% 2.38 Lefff : misses as an adjective
9 [...] 44% 193 71% 2.33 SXPipe: should be treated as skippable words
10 faudrait 36% 603 85% 2.32 Lefff : can have a nominal object
Table 3: Analysis of the 10 best-ranked forms (ranked according to Mf = Sf ? ln |Of |)
this paper global maxent
Rank Token(s)/form Eval Token(s)/form Eval Token(s)/form Eval
1 _____/_UNDERSCORE ++ * + pour -
2 (...) ++ , - ) -
3 2_]/_NUMBER ++ livre - ? -
4 priv?es ++ . - qu?il/qu? -
5 Haaretz/_Uw ++ de - sont -
6 contest? ++ ; - le -
7 occup?s ++ : - qu?un/qu? +
8 priv?e ++ la - qu?un/un +
9 [...] ++ ??trang?res - que -
10 faudrait ++ lecteurs - pourrait -
Table 4: The 10 best-ranked suspicious forms, according the the Mf measure, as computed by different
algorithms: ours (this paper), a standard maximum entropy algorithm (maxent) and van Noord?s rate
err(f) (global).
Rank Tokens and forms Mf Error cause
4 Toutes/toutes les 2.73 grammar: badly treated pre-determiner adjective
6 y en 2,34 grammar: problem with the construction il y en a. . .
7 in ? 1.81 Lefff : in misses as a preposition, which happends before book titles (hence the ?)
10 donne ? 1.44 Lefff : donner should sub-categorize ?-vcomps (donner ? voir. . . )
11 de demain 1.19 Lefff : demain misses as common noun (standard adv are not preceded by prep)
16 ( 22/_NUMBER 0.86 grammar: footnote references not treated
16 22/_NUMBER ) 0.86 as above
Table 5: Best ranked form bigrams (forms ranked inbetween are not shown; ranked according to Mf =
Sf ? ln |Of |). These results have been computed on a subset of the MD corpus (60,000 sentences).
335
phasize bigram results), with the same data as in
table 3.
5 Conclusions and perspectives
As we have shown, parsing large corpora allows
to set up error mining techniques, so as to identify
missing and erroneous information in the differ-
ent resources that are used by full-featured pars-
ing systems. The technique described in this pa-
per and its implementation on forms and form bi-
grams has already allowed us to detect many errors
and omissions in the Lefff lexicon, to point out in-
appropriate behaviors of the SXPipe pre-syntactic
processing chain, and to reveal the lack of cover-
age of the grammars for certain phenomena.
We intend to carry on and extend this work.
First of all, the visualization environment can be
enhanced, as is the case for the implementation of
the algorithm itself.
We would also like to integrate to the model
the possibility that facts taken into account (to-
day, forms and form bigrams) are not necessar-
ily certain, because some of them could be the
consequence of an ambiguity. For example, for
a given form, several lemmas are often possible.
The probabilization of these lemmas would thus
allow to look for most suspicious lemmas.
We are already working on a module that will
allow not only to detect errors, for example in
the lexicon, but also to propose a correction. To
achieve this, we want to parse anew all non-
parsable sentences, after having replaced their
main suspects by a special form that receives
under-specified lexical information. These infor-
mation can be either very general, or can be com-
puted by appropriate generalization patterns ap-
plied on the information associated by the lexicon
with the original form. A statistical study of the
new parsing results will make it possible to pro-
pose corrections concerning the involved forms.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximun entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):pp. 39?
71.
Pierre Boullier and Beno?t Sagot. 2005a. Analyse syn-
taxique profonde ? grande ?chelle: SXLFG. Traite-
ment Automatique des Langues (T.A.L.), 46(2).
Pierre Boullier and Beno?t Sagot. 2005b. Efficient
and robust LFG parsing: SxLfg. In Proceedings of
IWPT?05, Vancouver, Canada, October.
Hal Daum? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://www.isi.edu/~hdaume/docs/
daume04cg-bfgs.ps, implementation avail-
able at http://www.isi.edu/~hdaume/
megam/.
Patrick Paroubek, Louis-Gabriel Pouillot, Isabelle
Robba, and Anne Vilnat. 2005. EASy : cam-
pagne d??valuation des analyseurs syntaxiques. In
Proceedings of the EASy workshop of TALN 2005,
Dourdan, France.
Beno?t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing.
In Proceedings of L&TC 2005, Poznan?, Pologne.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de la
Clergerie, and Pierre Boullier. 2005. Vers un
m?ta-lexique pour le fran?ais : architecture, acqui-
sition, utilisation. Journ?e d??tude de l?ATALA sur
l?interface lexique-grammaire et les lexiques syntax-
iques et s?mantiques, March.
Fran?ois Thomasset and ?ric Villemonte de la Clerg-
erie. 2005. Comment obtenir plus des m?ta-
grammaires. In Proceedings of TALN?05, Dourdan,
France, June. ATALA.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proc. of ACL
2004, Barcelona, Spain.
?ric Villemonte de la Clergerie. 2005. DyALog: a
tabular logic programming based environment for
NLP. In Proceedings of 2nd International Work-
shop on Constraint Solving and Language Process-
ing (CSLP?05), Barcelona, Spain, October.
336
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 1?10,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficient and robust LFG parsing: SXLFG
Pierre Boullier and Beno?t Sagot
INRIA-Rocquencourt, Projet Atoll,
Domaine de Voluceau, Rocquencourt B.P. 105
78 153 Le Chesnay Cedex, France
{pierre.boullier, benoit.sagot}@inria.fr
Abstract
In this paper, we introduce a new parser,
called SXLFG, based on the Lexical-
Functional Grammars formalism (LFG).
We describe the underlying context-free
parser and how functional structures are
efficiently computed on top of the CFG
shared forest thanks to computation shar-
ing, lazy evaluation, and compact data
representation. We then present vari-
ous error recovery techniques we imple-
mented in order to build a robust parser.
Finally, we offer concrete results when
SXLFG is used with an existing gram-
mar for French. We show that our parser
is both efficient and robust, although the
grammar is very ambiguous.
1 Introduction
In order to tackle the algorithmic difficulties of
parsers when applied to real-life corpora, it is nowa-
days usual to apply robust and efficient methods
such as Markovian techniques or finite automata.
These methods are perfectly suited for a large num-
ber of applications that do not rely on a complex rep-
resentation of the sentence. However, the descriptive
expressivity of resulting analyses is far below what
is needed to represent, e.g., phrases or long-distance
dependencies in a way that is consistent with seri-
ous linguistic definitions of these concepts. For this
reason, we designed a parser that is compatible with
a linguistic theory, namely LFG, as well as robust
and efficient despite the high variability of language
production.
Developing a new parser for LFG (Lexical-
Functional Grammars, see, e.g., (Kaplan, 1989)) is
not in itself very original. Several LFG parsers al-
ready exist, including those of (Andrews, 1990) or
(Briffault et al, 1997). However, the most famous
LFG system is undoubtedly the Xerox Linguistics
Environment (XLE) project which is the successor
of the Grammars Writer?s Workbench (Kaplan and
Maxwell, 1994; Riezler et al, 2002; Kaplan et al,
2004). XLE is a large project which concentrates a
lot of linguistic and computational technology, relies
on a similar point of view on the balance between
shallow and deep parsing, and has been successfully
used to parse large unrestricted corpora.
Nevertheless, these parsers do not always use in
the most extensive way all existing algorithmic tech-
niques of computation sharing and compact infor-
mation representation that make it possible to write
an efficient LFG parser, despite the fact that the LFG
formalism, as many other formalisms relying on uni-
fication, is NP-hard. Of course our purpose is not to
make a new XLE system but to study how robust-
ness and efficiency can be reached in LFG parsing
on raw text.
Building constituent structures (c-structures) does
not raise any particular problem in theory,1 be-
cause they are described in LFG by a context-free
grammar (CFG), called (CF) backbone in this pa-
per. Indeed, general parsing algorithms for CFGs
are well-known (Earley, GLR,. . . ). On the other
hand, the efficient construction of functional struc-
tures (f-structures) is much more problematic. The
first choice that a parser designer must face is that
of when f-structures are processed: either during CF
1In practice, the availability of a good parser is sometimes
less straightforward.
1
parsing (interleaved method) or in a second phase
(two-pass computation). The second choice is be-
tween f-structures evaluation on single individual
[sub-]parses ([sub-]trees) or on a complete represen-
tation of all parses. We choose to process all phrasal
constraints by a CF parser which produces a shared
forest2 of polynomial size in polynomial time. Sec-
ond, this shared forest is used, as a whole, to de-
cide which functional constraints to process. For
ambiguous CF backbones, this two pass computa-
tion is more efficient than interleaving phrasal and
functional constraints.3 Another advantage of this
two pass vision is that the CF parser may be easily
replaced by another one. It may also be replaced
by a more powerful parser.4 We choose to evalu-
ate functional constraints directly on the shared for-
est since it has been proven (See (Maxwell and Ka-
plan, 1993)), as one can easily expect, that tech-
niques which evaluate functional constraints on an
enumeration of the resulting phrase-structure trees
are a computational disaster. This article explores
the computation of f-structures directly (without un-
folding) on shared forests. We will see how, in some
cases, our parser allows to deal with potential com-
binatorial explosion. Moreover, at all levels, error
recovering mechanisms turn our system into a robust
parser.
Our parser, called SXLFG, has been evaluated
with two large-coverage grammars for French, on
corpora of various genres. In the last section of this
paper, we present quantitative results of SXLFG us-
ing one of these grammars on a general journalistic
corpus.
2 The SXLFG parser: plain parsing
This section describes the parsing process for fully
grammatical sentences. Error recovery mechanisms,
that are used when this is not the case, are described
in the next section.
2Informally, a shared forest is a structure which can repre-
sent a set of parses (even an unbounded number) in a way that
shares all common sub-parses.
3This fact can be easily understood by considering that func-
tional constraints may be constructed in exponential time on
a sub-forest that may well be discarded later on by (future)
phrasal constraints.
4For example, we next plan to use an RCG backbone (see
(Boullier, 2004) for an introduction to RCGs), with the func-
tional constraints being evaluated on the shared forest output by
an RCG parser.
2.1 Architecture overview
The core of SXLFG is a general CF parser that pro-
cesses the CF backbone of the LFG. It is an Earley-
like parser that relies on an underlying left-corner
automaton and is an evolution of (Boullier, 2003).
The set of analyses produced by this parser is rep-
resented by a shared parse forest. In fact, this parse
forest may itself be seen as a CFG whose produc-
tions are instantiated productions of the backbone.5
The evaluation of the functional equations is per-
formed during a bottom-up left-to-right walk in this
forest. A disambiguation module, which discards
unselected f-structures, may be invoked on any node
of the forest including of course its root node.
The input of the parser is a word lattice (all words
being known by the lexicon, including special words
representing unknown tokens of the raw text). This
lattice is converted by the lexer in a lexeme lattice
(a lexeme being here a CFG terminal symbol asso-
ciated with underspecified f-structures).
2.2 The context-free parser
The evolutions of the Earley parser compared to that
described in (Boullier, 2003) are of two kinds: it ac-
cepts lattices (or DAGs) as input and it has syntac-
tic error recovery mechanisms. This second point
will be examined in section 3.1. Dealing with DAGs
as input does not require, at least from a theoreti-
cal standpoint, considerable changes in the Earley
algorithm.6 Since the Earley parser is guided by
a left-corner finite automaton that defines a regular
super-set of the CF backbone language, this automa-
ton also deals with DAGs as input (this corresponds
to an intersection of two finite automata).
5If A is a non-terminal symbol of the backbone, A
ij
is an in-
stantiated non-terminal symbol if and only if A
ij
+
?
G
a
i+1
. . . a
j
where w = a
1
. . . a
n
is the input string and
+
?
G
the transitive
closure of the derives relation.
6If i is a node of the DAG and if we have a transition on the
terminal t to the node j (without any loss in generality, we can
suppose that j > i) and if the Earley item [A ? ?.t?, k] is
an element of the table T [i], then we can add to the table T [j]
the item [A ? ?t.?, k] if it is not already there. One must
take care to begin a PREDICTOR phase in a T [j] table only if
all Earley phases (PREDICTOR, COMPLETOR and SCANNER)
have already been performed in all tables T [i], i < j.
2
2.3 F-Structures computation
As noted in (Kaplan and Bresnan, 1982), if the num-
ber of CF parses (c-structures) grows exponentially
w.r.t. the length of the input, it takes exponential
time to build and check the associated f-structures.
Our experience shows that the CF backbone for large
LFGs may be highly ambiguous (cf. Section 4).
This means that (full) parsing of long sentences
would be intractable. Although in CF parsing an ex-
ponential (or even unbounded) number of parse trees
can be computed and packed in polynomial time in
a shared forest, the same result cannot be achieved
with f-structures for several reasons.7 However, this
intractable behavior (and many others) may well not
occur in practical NL applications, or some tech-
niques (See Section 2.4) may be applied to restrict
this combinatorial explosion.
Efficient computation of unification-based struc-
tures on a shared forest is still a evolving research
field. However, this problem is simplified if struc-
tures are monotonic, as is the case in LFG. In such
a case the support (i.e., the shared forest) does not
need to be modified during the functional equation
resolution. If we adopt a bottom-up left-to-right
traversal strategy in the shared forest, information
in f-structures is cumulated in a synthesized way.
This means that the evaluation of a sub-forest8 is
only performed once, even when this sub-forest is
shared by several parent nodes. In fact, the effect
of a complete functional evaluation is to associate
to each node of the parse forest a set of partial f-
structures which only depends upon the descendants
of that node (excluding its parent or sister nodes).
The result of our LFG parser is the set of (com-
plete and consistent, if possible) main f-structures
(i.e., the f-structures associated to the root of the
shared forest), or, when a partial analysis occurs,
7As an example, it is possible, in LFG, to define f-structures
which encode individual parses. If a polynomial sized shared
forest represents an exponential number of parses, the number
of different f-structures associated to the root of the shared for-
est would be that exponential number of parses. In other words,
there are cases where no computational sharing of f-structures
is possible.
8If the CF backbone G is cyclic (i.e., ?A s.t. A +?
G
A), the
forest may be a general graph, and not only a DAG. Though our
CF parser supports this case, we exclude it in SXLFG. Of course
this (little) restriction does not mean that cyclic f-structures are
also prohibited. SXLFG does support cyclic f-structures, which
can be an elegant way to represent some linguistic relations.
the sets of (partial) f-structures which are associated
with maximal internal nodes). Such sets of (partial
or not) f-structures could be factored out in a single
f-structure containing disjunctive values, as in XLE.
We decided not to use these complex disjunctive val-
ues, except for some atomic types, but rather to asso-
ciate to any (partial) f-structure a unique identifier:
two identical f-structures will always have the same
identifier throughout the whole process. Experi-
ments (not reported here) show that this strategy is
worth using and that the total number of f-structures
built during a complete parse remains very tractable,
except maybe in some pathological cases.
As in XLE, we use a lazy copying strategy during
unification. When two f-structures are unified, we
only copy their common parts which are needed to
check whether these f-structures are unifiable. This
restricts the quantity of copies between two daughter
nodes to the parts where they interact. Of course,
the original daughter f-structures are left untouched
(and thus can be reused in another context).
2.4 Internal and global disambiguation
Applications of parsing systems often need a dis-
ambiguated result, thus calling for disambiguation
techniques to be applied on the ambiguous output
of parsers such as SXLFG. In our case, this im-
plies developing disambiguation procedures in order
to choose the most likely one(s) amongst the main f-
structures. Afterwards, the shared forest is pruned,
retaining only c-structures that are compatible with
the chosen main f-structure(s).
On the other hand, on any internal node of the for-
est, a possibly huge number of f-structures may be
computed. If nothing is done, these numerous struc-
tures may lead to a combinatorial explosion that pre-
vents parsing from terminating in a reasonable time.
Therefore, it seems sensible to allow the grammar
designer to point out in his or her grammar a set of
non-terminal symbols that have a linguistic property
of (quasi-)saturation, making it possible to apply
on them disambiguation techniques.9 Hence, some
non-terminals of the CF backbone that correspond
9Such an approach is indeed more satisfying than a blind
skimming that stops the full processing of the sentence when-
ever the amount of time or memory spent on a sentence exceeds
a user-specified limit, replacing it by a partial processing that
performs a bounded amount of work on each remaining non-
terminal (Riezler et al, 2002; Kaplan et al, 2004).
3
to linguistically saturated phrases may be associ-
ated with an ordered list of disambiguation meth-
ods, each of these non-terminals having its own list.
This allows for swift filtering out on relevant internal
nodes of f-structures that could arguably only lead
to inconsistent and/or incomplete main f-structures,
or that would be discarded later on by applying the
same method on the main f-structures. Concomi-
tantly, this leads to a significant improvement of
parsing times. This view is a generalization of the
classical disambiguation method described above,
since the pruning of f-structures (and incidentally
of the forest itself) is not reserved any more to the
axiom of the CF backbone. We call global disam-
biguation the pruning of the main f-structures, and
internal disambiguation the same process applied
on internal nodes of the forest. It must be noticed
that neither disambiguation type necessarily leads
to a unique f-structure. Disambiguation is merely
a shortcut for partial or total disambiguation.
Disambiguation methods are generally divided
into probabilistic and rule-based techniques. Our
parsing architecture allows for implementing both
kinds of methods, provided the computations can
be performed on f-structures. It allows to asso-
ciate a weight with all f-structures of a given in-
stantiated non-terminal.10 Applying a disambigua-
tion rule consists in eliminating of all f-structures
that are not optimal according to this rule. Each op-
tional rule is applied in a cascading fashion (one can
change the order, or even not apply them at all).
After this disambiguation mechanism on f-
structures, the shared forest (that represent c-
structures) is filtered out so as to correspond exactly
to the f-structure(s) that have been kept. In partic-
ular, if the disambiguation is complete (only one f-
structure has been kept), this filtering yields in gen-
eral a unique c-structure (a tree).
10See (Kinyon, 2000) for an argumentation on the impor-
tance of performing disambiguation on structures such as TAG
derivation trees or LFG f-structures and not constituent(-like)
structures.
3 Techniques for robust parsing
3.1 Error recovery in the CF parser
The detection of an error in the Earley parser11 can
be caused by two different phenomena: the CF back-
bone has not a large enough coverage or the input is
not in its language. Of course, although the parser
cannot make the difference between both causes,
parser and grammar developers must deal with them
differently. In both cases, the parser has to be able
to perform recovery so as to resume parsing as well
as, if possible, to correctly parse valid portions of
incorrect inputs, while preserving a sensible relation
between these valid portions. Dealing with errors
in parsers is a field of research that has been mostly
addressed in the deterministic case and rarely in the
case of general CF parsers.
We have implemented two recovery strategies in
our Earley parser, that are tried one after the other.
The first strategy is called forward recovery, the
second one backward recovery.12 Both generate a
shared forest, as in the regular case.
The mechanism is the following. If, at a certain
point, the parsing is blocked, we then jump forward
a certain amount of terminal symbols so as to be able
to resume parsing. Formally, in an Earley parser
whose input is a DAG, an error is detected when,
whatever the active table T [j], items of the form
I = [A ? ?.t?, i] in this table are such that in the
DAG there is no out-transition on t from node j. We
say that a recovery is possible in k on ? if in the suf-
fix ? = ?
1
X?
2
there exists a derived phrase from
the symbol X which starts with a terminal symbol
r and if there exists a node k in the DAG, k ? j,
with an out-transition on r. If it is the case and if
this possible recovery is selected, we put the item
[A ? ?t?
1
.X?
2
, i] in table T [k]. This will ensure
11Let us recall here that the Earley algorithm, like the GLR
algorithm, has the valid prefix property. This is still true when
the input is a DAG.
12The combination of these two recovery techniques leads to
a more general algorithm than the skipping of the GLR* algo-
rithm (Lavie and Tomita, 1993). Indeed, we can not only skip
terminals, but in fact replace any invalid prefix by a valid pre-
fix (of a right sentential form) with an increased span. In other
words, both terminals and non-terminals may be skipped, in-
serted or changed, following the heuristics described later on.
However, in (Lavie and Tomita, 1993), considering only the
skipping of terminal symbols was fully justified since their aim
was to parse spontaneous speech, full of noise and irrelevances
that surround the meaningful words of the utterance.
4
S2
S
NP
N
pn
Jean
V
v
essaye
PVP
prep
de
VP
?
spunct
...
Figure 1: Simplified constituents structure for in-
complete sentence Jean essaye de... (?Jean tries
to...?). The derivation of VP in the empty string is
the result of a forward recovery, and will lead to
an incomplete functional structure (no ?pred? in the
sub-structure corresponding to node VP).
at least one valid transition from T [k] on r. The ef-
fect of such a recovery is to assume that between
the nodes j and k in the DAG there is a path that is a
phrase generated by t?
1
. We select only nodes k that
are as close as possible to j. This economy principle
allows to skip (without analysis) the smallest pos-
sible number of terminal symbols, and leads pretty
often to no skipping, thus deriving ?
1
into the empty
string and producing a recovery in k = j. This re-
covery mechanism allows the parsing process to go
forward, hence the name forward recovery.
If this strategy fails, we make use of backward
recovery.13 Instead of trying to apply the current
item, we jump backward over terminal symbols that
have already been recognized by the current item,
until we find its calling items, items on which we
try to perform a forward recovery at turn. In case
of failure, we can go up recursively until we suc-
ceed. Indeed, success is guaranteed, but in the worst
case it is obtained only at the axiom. In this ex-
treme case, the shared forest that is produced is only
a single production that says that the input DAG
is derivable from the axiom. We call this situation
trivial recovery. Formally, let us come back to the
item I = [A ? ?.t?, i] of table T [j]. We know
that there exists in table T [i] an item J of the form
[B ? ?.A?, h] on which we can hazard a forward
13This second strategy could be also used before or even in
parallel with the forward recovery.
recovery in l on ?, where i ? j ? l. If this fails, we
go on coming back further and further in the past,
until we reach the initial node of the DAG and the
root item [S? ? .S$, 0] of table T [0] ($ is the end-
of-sentence mark and S? the super-axiom). Since
any input ends with an $ mark, this strategy always
succeeds, leading in the worst case to trivial recov-
ery.
An example of an analysis produced is shown in
Figure 1: in this case, no out-transition on spunct is
available after having recognized prep. Hence a for-
ward recovery is performed that inserts an ?empty?
VP after the prep, so as to build a valid parse.
3.2 Inconsistent or partial f-structures
The computation of f-structures fails if and only
if no consistent and complete main f-structure is
found. This occurs because unification constraints
specified by functional equations could not have
been verified or because resulting f-structures are
inconsistent or incomplete. Without entering into
details, inconsistency mostly occurs because sub-
categorization constraints have failed.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pred = ?essayer <subj, de-vcomp>?, v[2..3]
subj =
?
?
?
pred = ?Jean <(subj)>?, pn[1..2]
det = +
hum = +
A
ij
=
{
R182
9
, R177
26
, R170
28
}
?
?
?
F68
de-vcomp =
?
?
?
?
?
pred = ?de <obj|...>?, prep[3..4]
vcomp =
[
subj = []
F68
A
ij
=
{
R162
84
}
2
]
F69
pcase = de
A
ij
= {}
2
?
?
?
?
?
F70
number = sg
person = 3
mode = indicative
tense = present
A
ij
=
{
R130
33
, R119
48
, R134
49
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Simplified incomplete functional structure
for incomplete sentence Jean essaye de... (?Jean
tries to...?). Sub-structure identifiers are indicated as
subscripts (like F70). In the grammar, a rule can tell
the parser to store the current instantiated production
in the special field A
ij
of its associated left-hand
side structure. Hence, atoms of the form Rq
p
rep-
resent instantiated production, thus allowing to link
sub-structures to non-terminals of the c-structure.
5
A first failure leads to a second evaluation of f-
structures on the shared forest, during which consis-
tency and completeness checks are relaxed (an ex-
ample thereof is given in Figure 2). In case of suc-
cess, we obtain inconsistent or incomplete main f-
structures. Of course, this second attempt can also
fail. We then look in the shared forest for a set of
maximal nodes that have f-structures (possibly in-
complete or inconsistent) and whose mother nodes
have no f-structures. They correspond to partial
disjoint analyses. The disambiguation process pre-
sented in section 2.4 applies to all maximal nodes.
3.3 Over-segmentation of unparsable sentences
Despite all these recovery techniques, parsing some-
times fails, and no analysis is produced. This can
occur because a time-out given as parameter has ex-
pired before the end of the process, or because the
Earley parser performed a trivial recovery (because
of the insufficient coverage of the grammar, or be-
cause the input sentence is simply too far from being
correct: grammatical errors, incomplete sentences,
too noisy sentences, . . . ).
For this reason, we developed a layer over SXLFG
that performs an over-segmentation of ungrammat-
ical sentences. The idea is that it happens fre-
quently that portions of the input sentence are ana-
lyzable as sentences, although the full input sentence
is not. Therefore, we split in segments unparsable
sentences (level 1 segmentation); then, if needed,
we split anew unparsable level 1 segments14 (level
2 segmentation), and so on with 5 segmentation lev-
els.15 Such a technique supposes that the grammar
recognizes both chunks (which is linguistically jus-
tified, e.g., in order to parse nominal sentences) and
isolated terminals (which is linguistically less rele-
vant). In a way, it is a generalization of the use of a
FRAGMENT grammar as described in (Riezler et al,
2002; Kaplan et al, 2004).
14A sentence can be split into two level 1 segments, the first
one being parsable. Then only the second one will be over-
segmented anew into level 2 segments. And only unparsable
level 2 segments will be over-segmented, and so on.
15The last segmentation level segments the input string into
isolated terminals, in order to guarantee that any input is parsed,
and in particular not to abandon parsing on sentences in which
some level 1 or 2 segments are parsable, but in which some parts
are only parsable at level 5.
4 Quantitative results
4.1 Grammar, disambiguation rules, lexicon
To evaluate the SXLFG parser, we used our system
with a grammar for French that is an adaptation of an
LFG grammar originally developed by Cl?ment for
his XLFG system (Cl?ment and Kinyon, 2001). In
its current state, the grammar has a relatively large
coverage. Amongst complex phenomena covered
by this grammar are coordinations (without ellip-
sis), juxtapositions (of sentences or phrases), inter-
rogatives, post-verbal subjects and double subjects
(Pierre dort-il ?), all kinds of verbal kernels (in-
cluding clitics, auxiliaries, passive, negation), com-
pletives (subcategorized or adjuncts), infinitives (in-
cluding raising verbs and all three kinds of control
verbs), relatives or indirect interrogatives, including
when arbitrarily long-distance dependencies are in-
volved. However, comparatives, clefts and elliptical
coordinations are not specifically covered, inter alia.
Moreover, we have realized that the CF backbone is
too ambiguous (see below).
Besides the grammar itself, we developed a set of
disambiguation heuristics. Following on this point
(Cl?ment and Kinyon, 2001), we use a set of rules
that is an adaptation and extension of the three sim-
ple principles they describe and that are applied
on f-structures, rather than a stochastic model.16
Our rules are based on linguistic considerations and
can filter out functional structures associated to a
given node of the forest. This includes two special
rules that eliminate inconsistent and incomplete f-
structures either in all cases or when consistent and
complete structures exist (these rules are not applied
during the second pass, if any). As explained above,
some non-terminals of the CF backbone, that corre-
spond to linguistically saturated phrases, have been
associated with an ordered list of these rules, each of
these non-terminal having its own list.17.
16As sketched before, this could be easily done by defining a
rule that uses a stochastic model to compute a weight for each
f-structure (see e.g., (Miyao and Tsujii, 2002)) and retains only
those with the heaviest weights (Riezler et al, 2002; Kaplan
et al, 2004). However, our experiments show that structural
rules can be discriminative enough to enable efficient parsing,
without the need for statistical data that have to be acquired on
annotated corpora that are rare and costly, in particular if the
considered language is not English.
17Our rules, in their order of application on main f-structures,
i.e. on the axiom of the backbone, are the following (note that
6
 0
 100
 200
 300
 400
 500
 600
 700
 800
 0  20  40  60  80  100N
um
be
r o
f s
en
te
nc
es
 in
 th
e 
sa
m
e 
cl
as
s
(le
ng
th
 b
et
w
ee
n 
10
i a
nd
 1
0(
i+
1)
-1
)
Sentence length (number of transitions in the corresponding word lattice)
Figure 3: Repartition of sentences of the test corpus
w.r.t. their length. We show the cardinal of classes
of sentences of length 10i to 10(i + 1) ? 1, plotted
with a centered x-coordinate (10(i + 1/2)).
The lexicon we used is the latest version of
Lefff (Lexique des formes fl?chies du fran?ais18),
which contains morphosyntactic and syntactic infor-
mation for more than 600,000 entries corresponding
to approximately 400,000 different tokens (words or
components of multi-word units).
The purpose of this paper is not however to val-
idate the grammar and these disambiguation rules,
since the grammar has only the role of enabling eval-
uation of parsing techniques developed in the current
work.
4.2 Results
As for any parser, the evaluation of SXLFG has been
carried out by testing it in a real-life situation. We
used the previously cited grammar on a raw journal-
istic corpus of 3293 sentences, not filtered and pro-
when used on other non-terminal symbols than the axiom, some
rules may not be applied, or in a different order):
Rule 1: Filter out inconsistent and incomplete structures, if
there is at least one consistent and complete structure.
Rule 2: Prefer analyses that maximize the sum of the weights
of involved lexemes; amongst lexical entries that have a
weight higher than normal are multi-word units.
Rule 3: Prefer nominal groups with a determiner.
Rule 4: Prefer arguments to modifiers, and auxiliary-
participle relations to arguments (the computation is
performed recursively on all (sub-)structures).
Rule 5: Prefer closer arguments (same remark).
Rule 6: Prefer deeper structures.
Rule 7: Order structures according to the mode of verbs (we
recursively prefer structures with indicative verbs, sub-
junctive verbs, and so on).
Rule 8: Order according to the category of adverb governors.
Rule 9: Choose one analysis at random (to guarantee that the
output is a unique analysis).
18Lexicon of French inflected forms
 1
 100000
 1e+10
 1e+15
 1e+20
 1e+25
 1e+30
 0  20  40  60  80  100
N
um
be
r o
f t
re
es
 in
 th
e 
C
FG
 p
ar
se
 fo
re
st
 (l
og
 s
ca
le
)
Sentence length (number of transitions in the corresponding word lattice)
Median number of trees
Number of trees at percentile rank 90
Number of trees at percentile rank 10
Figure 4: CFG ambiguity (medians are computed on
classes of sentences of length 10i to 10(i+1)?1 and
plotted with a centered x-coordinate (10(i + 1/2)).
cessed by the SXPipe pre-parsing system described
in (Sagot and Boullier, 2005). The repartition of sen-
tences w.r.t. their length is plotted in Figure 3.
In all Figures, the x-coordinate is bounded so as
to show results only on statistically significant data,
although we parse all sentences, the longest one be-
ing of length 156.
However, in order to evaluate the performance of
our parser, we had to get rid of, as much as possible,
the influence of the grammar and the corpus in the
quantitative results. Indeed, the performance of the
SXLFG parser does not depend on the quality and
the ambiguity of the grammar, which is an input for
SXLFG. On the contrary, our aim is to develop a
parser which is as efficient and robust as possible
given the input grammar, and in spite of its (possibly
huge) ambiguity and of its (possibly poor) intrinsic
coverage.
4.2.1 CFG parser evaluation
Therefore, Figure 4 demonstrates the level of am-
biguity of the CF backbone by showing the median
number of CF parses given the number of transitions
in the lattice representing the sentence. Although
the number of trees is excessively high, Figure 5
shows the efficiency of our CF parser19 (the max-
imum number of trees reached in our corpus is as
high as 9.12 1038 for a sentence of length 140, which
19Our experiments have been performed on a AMD Athlon
2100+ (1.7 GHz).
7
 0
 100
 200
 300
 400
 500
 0  20  40  60  80  100
C
FG
 p
ar
si
ng
 ti
m
e 
(m
ill
is
ec
on
ds
)
Sentence length (number of transitions in the corresponding word lattice)
Median CFG parsing time
CFG parsing time at percentile rank 90
CFG parsing time at percentile rank 10
Figure 5: CF parsing time (same remark as for Fig. 4).
is parsed in only 0.75 s). Moreover, the error re-
covery algorithms described in section 3.1 are suc-
cessful in most cases where the CF backbone does
not recognize the input sentences: out of the 3292
sentences, 364 are not recognized (11.1%), and the
parser proposes a non-trivial recovery for all but 13
(96.3%). We shall see later the relevance of the pro-
posed recovered forests. We should however notice
that the ambiguity of forests is significantly higher
in case of error recovery.
4.2.2 Evaluation of f-structures computation
Although the CF backbone is massively ambigu-
ous, results show that our f-structures evaluation
system is pretty efficient. Indeed, with a timeout of
20 seconds, it takes only 6 301 seconds to parse the
whole corpus, and only 5, 7% of sentences reach the
timeout before producing a parse. These results can
be compared to the result with the same grammar on
the same corpus, but without internal disambigua-
tion (see 2.4), which is 30 490 seconds and 41.2%
of sentences reaching the timeout.
The coverage of the grammar on our corpus with
internal disambiguation is 57.6%, the coverage be-
ing defined as the proportion of sentences for which
a consistent and complete main f-structure is output
by the parser. This includes cases where the sen-
tence was agrammatical w.r.t. the CF backbone, but
for which the forest produced by the error recov-
ery techniques made it possible to compute a consis-
tent and complete main f-structure (this concerns 86
sentences, i.e., 2.6% of all sentences, and 24.5% of
all agrammatical sentences w.r.t. the backbone; this
shows that CF error recovery gives relevant results).
The comparison with the results with the same
grammar but without internal disambiguation is in-
teresting (see Table 1): in this case, the high propor-
tion of sentences that reach the timeout before being
parsed leads to a coverage as low as 40.2%. Amid
the sentences covered by such a system, 94.6% are
also covered by the full-featured parser (with inter-
nal disambiguation), which means that only 72 sen-
tences covered by the grammar are lost because of
the internal disambiguation. This should be com-
pared with the 645 sentences that are not parsed be-
cause of the timeout when internal disambiguation
is disabled, but that are covered by the grammar and
correctly parsed if internal disambiguation is used:
the risk that is taken by pruning f-structures during
the parsing process is much smaller than the benefit
it gives, both in terms of coverage and parsing time.
Since we do not want the ambiguity of the CF
backbone to influence our results, Figure 6 plots the
total parsing time, including the evaluation of fea-
tures structures, against the number of trees pro-
duced by the CF parser.
8
Results With internal Without internal
disambiguation disambiguation
Total number of sentences 3293
Recognized by the backbone 2929 88.9%
CF parsing with non-trivial recovery 351 10.6%
CF parsing with trivial recovery 13 0.4%
Consistent and complete main f-structure 1896 57.6% 1323 40.2%
Inconsistent and incomplete main f-structure 734 22.3% 316 9.6%
Partial f-structures 455 13.8% 278 8.4%
No f-structure 6 0.2% 6 0.2%
No result (trivial recovery) 13 0.4% 13 0.4%
Timeout (20 s) 189 5.7% 1357 40.2%
Table 1: Coverage results with and without internal ranking, with the same grammar and corpus.
 10
 100
 1000
 10000
 1  100000  1e+10  1e+15  1e+20
To
ta
l p
ar
si
ng
 ti
m
e 
(m
ill
is
ec
on
ds
)
Number of trees in the forest
Median total parsing time
Total parsing time at percentile rank 90
Total parsing time at percentile rank 10
Figure 6: Total parsing time w.r.t. the number of trees in the forest produced by the CF backbone (medians
are computed on classes of sentences whose number of trees lies between 102i and 102i+2 ? 1 and plotted
with a centered x-coordinate (102i+1)).
9
5 Conclusion
This paper shows several important results.
It shows that wide-coverage unification-based
grammars can be used to define natural languages
and that their parsers can, in practice, analyze raw
text.
It shows techniques that allow to compute fea-
ture structures efficiently on a massively ambiguous
shared forest.
It also shows that error recovery is worth doing
both at the phrasal and functional levels. We have
shown that a non-negligible portion of input texts
that are not in the backbone language can neverthe-
less, after CF error recovery, be qualified as valid
sentences for the functional level.
Moreover, the various robustness techniques that
are applied at the functional level allow to gather
(partial) useful information. Note that these ro-
bust techniques, which do not alter the overall ef-
ficiency of SXLFG, apply in the two cases of incom-
plete grammar (lack of covering) and agrammati-
cal phrases (w.r.t. the current definition), though it
seems to be more effective in this latter case.
References
Avery Andrews. 1990. Functional closure in LFG. Tech-
nical report, The Australian National University.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies (IWPT?03), pages 43?54, Nancy, France,
April.
Pierre Boullier. 2004. Range concatenation grammars.
In New developments in parsing technology, pages
269?289. Kluwer Academic Publishers.
Xavier Briffault, Karim Chibout, G?rard Sabah, and
J?r?me Vapillon. 1997. An object-oriented lin-
guistic engineering environment using LFG (Lexical-
Functional Grammar) and CG (Conceptual Graphs).
In Proceedings of Computational Environments for
Grammar Development and Linguistic Engineering,
ACL?97 Workshop.
Lionel Cl?ment and Alexandra Kinyon. 2001. XLFG ?
an LFG parsing scheme for French. In Proceedings of
LFG?01, Hong Kong.
Ronald Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: a formal system for grammatical
representation. In J. Bresnan, editor, The Mental Rep-
resentation of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
Ronald M. Kaplan and John T. Maxwell. 1994. Gram-
mar writer?s workbench, version 2.0. Technical report,
Xerox Corporation.
Ronald Kaplan, Stefan Riezler, Tracey King, John
Maxwell, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of HLT/NAACL,
Boston, Massachusetts.
Ronald Kaplan. 1989. The formal architecture of lexical
functionnal grammar. Journal of Informations Science
and Engineering.
Alexandra Kinyon. 2000. Are structural principles use-
ful for automatic disambiguation ? In Proceedings
of in COGSCI?00, Philadelphia, Pennsylvania, United
States.
Alon Lavie and Masaru Tomita. 1993. GLR* ? an effi-
cient noise-skipping parsing algorithm for context-free
grammars. In Proceedings of the Third International
Workshop on Parsing Technologies, pages 123?134,
Tilburg, Netherlands and Durbuy, Belgium.
John Maxwell and Ronald Kaplan. 1993. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings of
HLT, San Diego, California.
Stefan Riezler, Tracey King, Ronald Kaplan, Richard
Crouch, John Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and discriminative estimation
techniques. In Proceedings of the Annual Meeting of
the ACL, University of Pennsylvania.
Beno?t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing. In
Proceedings of L&TC 2005, Poznan?, Pologne.
10
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 147?152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modeling and Analysis of Elliptic Coordination by Dynamic Exploitation
of Derivation Forests in LTAG parsing
Djam? Seddah (1) & Beno?t Sagot (2)
(1) NCLT - Dublin City University - Ireland
djame.seddah@computing.dcu.ie
(2) Projet ATOLL - INRIA - France
benoit.sagot@inria.fr
Abstract
In this paper, we introduce a generic ap-
proach to elliptic coordination modeling
through the parsing of Ltag grammars. We
show that erased lexical items can be re-
placed during parsing by informations ga-
thered in the other member of the coordi-
nate structure and used as a guide at the
derivation level. Moreover, we show how
this approach can be indeed implemented
as a light extension of the LTAG formalism
throuh a so-called ?fusion? operation and
by the use of tree schemata during parsing
in order to obtain a dependency graph.
1 Introduction
The main goal of this research is to provide a
way of solving elliptic coordination through the
use of Derivation Forests. The use of this de-
vice implies that the resolution mechanism de-
pends on syntactic information, therefore we will
not deal with anaphoric resolutions and scope mo-
difier problems. We show how to generate a de-
rivation forest described by a set of context free
rules (similar to (Vijay-Shanker and Weir, 1993))
augmented by a stack of current adjunctions when
a rule describes a spine traversal. We first briefly
discuss the linguistic motivations behind the reso-
lution mechanism we propose, then introduce the
fusion operation and show how it can be compa-
red to the analysis of (Dalrymple et al, 1991) and
(Steedman, 1990) and we show how it differs from
(Sarkar and Joshi, 1996). We assume that the rea-
der is familiar with the Lexicalized Tree Adjoining
Grammars formalism ((Joshi and Schabes, 1992)).
2 Linguistic Motivations : a parallelism
of Derivation
The LTAG formalism provides a derivation tree
which is strictly the history of the operations nee-
ded to build a constituent structure, the derived
tree. In order to be fully appropriate for seman-
tic inference 1, the derivation tree should display
every syntactico-semantic argument and therefore
should be a graph. However to obtain this kind
of dependency structure when it is not possible to
rely on lexical information, as opposed to (Seddah
and Gaiffe, 2005a), is significantly more compli-
cated. An example of this is provided by elliptic
coordination.
Consider the sentences Figure 3. They all can be
analyzed as coordinations of S categories2 with
one side lacking one mandatory argument. In (4),
one could argue for VP coordination, because the
two predicates share the same continuum (same
subcategorization frame and semantic space). Ho-
wever the S hypothesis is more generalizable and
supports more easily the analysis of coordination
of unlike categories (?John is a republican and
proud of it? becomes ?Johni isj a republican and
?i ?j proud of it?).
The main difficulty is to separate the cases when
a true co-indexation occurs ((2) and (4)) from the
cases of a partial duplication (in (1), the predicate
is not shared and its feature structures could dif-
fer on aspects, tense or number3). In an elliptic
construction, some words are unrealized. There-
fore, their associated syntactic structures are also
non-realized, at least to some extent. However, our
aim is to get, as a result of the parsing process,
the full constituency and dependency structures of
the sentence, including erased semantic items (or
units) and their (empty) syntactic positions. Since
their syntactic realizations have been erased, the
construction of the dependency structure can not
1As elementary trees are lexicalized and must have a mi-
nimal semantic meaning (Abeill?, 1991), the derivation tree
can be seen as a dependency tree with respect to the restric-
tions defined by (Rambow and Joshi, 1994) and (Candito and
Kahane, 1998) to cite a few.
2P for Phrase in french, in Figures given in annex
3see ?John lovesi Mary and childreni their gameboy?
147
be anchored to lexical items. Instead, it has to be
anchored on non-realized lexical items and gui-
ded by the dependency structure of the reference
phrase. Indeed, it is because of the parallelism bet-
ween the reference phrase and the elliptical phrase
that an ellipsis can be interpreted.
3 The Fusion Operation
In this research, we assume that every coordina-
tor, which occurs in elided sentences, anchors an
initial tree ?conj rooted by P and with two sub-
stitution nodes of category P (Figure 1). The fu-
P?conj
P?conjG? et P?conjD?
FIG. 1 ? Initial Tree ?conj
sion operation replaces the missing derivation of
any side of the coordinator by the corresponding
ones from the other side. It shall be noted that the
fusion provide proper node sharing when it is syn-
tactically decidable (cf. 6.4). The implementation
relies on the use of non lexicalized trees (ie tree
schemes) called ghost trees. Their purpose is to
be the support for partial derivations which will
be used to rebuild the derivation walk in the eli-
ded part. We call the partial derivations ghost deri-
vations. The incomplete derivations from the tree
? are shown as a broken tree in Figure 2. The
ghost derivations are induced by the inclusion of
the ghost tree ?? which must be the scheme of the
tree ?. When the two derivation structures from
? and ?? are processed by the fusion operation, a
complete derivation structure is obtained.
?conj
? ?
       
       
       
       
       
       
       
       








??
?conj
??
     
     
     



       
       
       
       
       
       
       
       








Derivations before the Fusion After the Fusion
FIG. 2 ? Derivation sketch of the Fusion Operation
4 examples anylysis
Let us go back to the following sentences :
(1) Jean aimei Marie et Paul ?i Virginie
John loves Mary and Paul Virginia
(2) Pauli aime Virginie et ?i d?teste Marie
Paul loves Virginia and hates Mary
Obviously (1) can have as a logical formula :
aime?(jean?,Marie?) ? aime?(paul?, virginie?)
whereas (2) is rewritten by eat(paul?, apple?) ?
buy?(Paul?, cherries?). The question is to diffe-
rentiate the two occurrence of aime? in (1) from
the paul? ones. Of course, the second should be
noted as a sharing of the same argument when the
first is a copy of the predicate aime?. Therefore
in order to represent the sharing, we will use the
same node in the dependency graph while a ghos-
ted node (noted by ghost(?) in our figures) will be
used in the other case. This leads to the analysis
figure 4. The level of what exactly should be co-
pied, speaking of level of information, is outside
the scope of this paper, but our intuition is that
a state between a pure anchored tree and an tree
schemata is probably the correct answer. As we
said, aspect, tense and in most case diathesis for 4
are shared, as it is showed by the following sen-
tences :
(3)*Paul killed John and Bill by Rodger
(4)*Paul ate apple and Mary will pears
As opposed to (4), we believe ?Paul ate apples
and Mary will do pears? to be correct but in
this case, we do not strictly have an ellipsis but
a semi-modal verb which is susbsumed by its
co-referent. Although our proposition focuses on
syntax-semantic interface, mainly missing syntac-
tic arguments.
5 Ghost Trees and Logical Abstractions
Looking either at the approach proposed by
(Dalrymple et al, 1991) or (Steedman, 1990) for
the treatment of sentences with gaps, we note that
in both cases5 one wants to abstract the realized
element in one side of the coordination in order to
instantiate it in the other conjunct using the coor-
dinator as the pivot of this process. In our analy-
sis, this is exactly the role of ghost trees to support
such abstraction (talking either about High Order
Variable or ?-abstraction). In this regard, the fu-
sion operation has only to check that the deriva-
tions induced by the ghost tree superimpose well
with the derivations of the realized side.
This is where our approach differs strongly from
(Sarkar and Joshi, 1996). Using the fusion opera-
tion involves inserting partial derivations, which
are linked to already existing ones (the realized
derivation), into the shared forest whereas using
4w.r.t to the examples of (Dalrymple et al, 1991), i.e ?It
is possible that this result can be derived (..) but I know of no
theory that does so.?
5Footnote n?3, page 5 for (Dalrymple et al, 1991), and
pages 41-42 for (Steedman, 1990).
148
the conjoin operation defined in (Sarkar and Joshi,
1996) involves merging nodes from different trees
while the tree anchored by a coordinator acts si-
milarly to an auxiliary tree with two foot nodes.
This may cause difficulties to derive the now dag
into a linear string. In our approach, we use empty
lexical items in order to leave traces in the deriva-
tion forest and to have syntacticly motivated deri-
ved tree (cf fig. 5) if we extract only the regular
LTAG ?derivation item? from the forest.
6 LTAG implementation
6.1 Working on shared forest
A shared forest is a structure which combines
all the information coming from derivation trees
and from derived trees. Following (Vijay-Shanker
and Weir, 1993; Lang, 1991), each tree anchored
by the elements of the input sentence is described
by a set of rewriting rules. We use the fact that
each rule which validates a derivation can infer
a derivation item and has access to the whole
chart in order to prepare the inference process.
The goal is to use the shared forest as a guide for
synchronizing the derivation structures from both
parts of the coordinator.
This forest is represented by a context free
grammar augmented by a stack containing the
current adjunctions (Seddah and Gaiffe, 2005a),
which looks like a Linear Indexed Grammar (Aho,
1968).
Each part of a rule corresponds to an
item ? la Cock Kasami Younger described
by (Shieber et al, 1995), whose form is
< N,POS, I, J, STACK > with N a node
of an elementary tree, POS the situation relative
to an adjunction (marked ? if an adjunction is
still possible, ? otherwise). This is marked on
figure 5 with a bold dot in high position, ?, or a
bold dot in low position, ?). I and J are the start
and end indices of the string dominated by the N
node. STACK is the stack containing all the call
of the subtrees which has started an adjunction et
which must be recognized by the foot recognition
rules. We used S as the starting symbol of the
grammar and n is the length of the initial string.
Only the rules which prove a derivation are shown
in figure 6.
The form of a derivation item is
Name :< Node?to , ?from, ?to, T ype, ?ghost >
where Name is the derivation, typed Type6, of
the tree ?from to the node Node of ?to.7
6.2 Overview of the process
We refer to a ghost derivation as any derivation
which occurs in a tree anchored by an empty
element, and ghost tree as a tree anchored by
this empty element. As we can see in figure 5,
we assume that the proper ghost tree has been
selected. So the problem remains to know which
structure we have to use in order to synchronize
our derivation process.
Elliptic substitution of an initial ghost tree
on a tree ?conj : Given a tree ?conj (see Fig.
1) anchored by a coordinator and an initial tree
?1 of root P to be substituted in the leftmost P
node of ?conj . Then the rule corresponding to
the traversal of the Leftmost P node would be
P?conjG(?, i, j,?,?) ?? P?1(?, i, j,?,?) .
So if this rule is validated, then we infer a deriva-
tion item called D1 :<P?conjG,?1,?conj ,subst,-> .
Now, let us assume that the node situated to the
right of the coordinating conjunction dominates a
phrase whose verb has been erased (as in et Paul _
Virginie) and that there exists a tree of Root P with
two argument positions (a quasi tree like N0VN1
in LTAG literature for example). This ghost tree
is anchored by an empty element and is called
?ghost. We have a rule, called Call-subst-ghost,
describing the traversal of this node :
P?conjD(?,j+1,n,-,-) ?? P?ghost(?,j+1,n,-,-) .
For the sake of readability, let us call D1? the
pseudo-derivation of call-subst-ghost :
D1? :< P?conjD, ? , ?conj , subst, ?ghost > ,
where the non-instantiated variable, ? , indicates
the missing information in the synchronized tree.
If our hypothesis is correct, this tree will be ancho-
red by the anchor of ?1. So we have to prepare this
anchoring by performing a synchronization with
existing derivations. This leads us to infer a ghost
substitution derivation of the tree ?1 on the node
P?conjD. The inference rule which produces the
6which can be an adjunction (type = adj), a substitu-
tion (subst), an axiom (ax), an anchor which is usually an
implicit derivation in an LTAG derivation tree (anch) or a
?ghosted? one (adjg ,substg ,anchg)
7?ghost is here to store the name of the ?ghost tree? if the
Node belongs to one or ? otherwise.
149
item called ghost(?1) on Figure 5, is therefore :
D1? :< P?conjD, ? , ?conj , subst, ?ghost >
D1 :< P?conjR, ?1, ?conj , subst,? >
Ghost?D1 :< P?conjR, ?1, ?conj , substg, ?ghost >
The process which is almost the same for the
remaining derivations, is described section 6.4.
6.3 Ghost derivation and Item retrieving
In the last section we have described a ghost
derivation as a derivation which deals with a tree
anchored by an empty element, either it is the
source tree or the destination tree. In fact we need
to keep marks on the shared forest between what
we are really traversing during the parsing process
and what we are synchronizing, that is why we
need to have access to all the needed informations.
But the only rule which really knows which tree
will be either co-indexed or duplicated is the rule
describing the substitution of the realized tree.
So, we have to get this information by accessing
the corresponding derivation item. If we are in a
two phase generation process of a shared forest8
we can generate simultaneously the substitution
rules for the leftmost and rightmost nodes of the
tree anchored by a coordination and then we can
easily get the right synchronized derivation from
the start. Here we have to fetch from the chart this
item using unification variables through the path
of the derivations leading to it.
Let us call ?climbing? the process of going
from a leaf node N of a tree ? to the node
belonging to the tree anchored by a coordi-
nator (?conj) and which dominates this node.
This ?climbing? gives us a list of linked deri-
vations (ie. [< ?x(N), ?y, ?x, T ype, IsGhost >
,< ?z(N), ?x, ?z, T ype1, IsGhost1 >, ..] where
?(N) is the node of the tree ? where the derivation
takes place9). The last returned item is the one who
has an exact counterpart in the other conjunct, and
which is easy to recover as shown by the inference
rule in the previous section. Given this item, we
start the opposite process, called ?descent?, which
use the available data gathered by the climbing
(the derivation starting nodes, the argumental po-
sition marked by an index on nodes in TAG gram-
8The first phase is the generation of the set of rules,
(Vijay-Shanker and Weir, 1993), and the second one is the fo-
rest traversal (Lang, 1992). See (Seddah and Gaiffe, 2005b)
for a way to generate a shared derivation forest where each
derivation rule infers its own derivation item, directly prepa-
red during the generation phase.
9The form of a derivation item is defined section 6.1
mars..) to follow a parallel path. Our algorithm can
be considered as taking the two resulting lists as a
parameter to produce the correct derivation item.
If we apply a two step generation process (shared
forest generation then extraction), the ?descent?
and the ?climbing? phase can be done in parallel
in the same time efficient way than(2005a).
6.4 Description of inference rules
In this section we will describe all of the infe-
rences relative to the derivation in the right part,
resp. left, of the coordination, seen in figure 5.
In the remainder of this paper, we describe the
inference rules involved in so called predicative
derivations (substitutions and ghost substitutions).
Indeed, the status of adjunction is ambiguous. In
the general case, when an adjunct is present on one
side only of the conjunct, there are two possible
readings : one reading with an erased (co-indexed)
modifier on the other side, and one reading with no
such modifier at all on this other side. In the rea-
ding with erasing, there is an additionnal question,
which occurs in the substitution case as well : in
the derivation structure, shall we co-index the era-
sed node with its reference node, or shall we per-
form a (partial) copy, hence creating two (partially
co-indexed) nodes ? The answer to this question
is non-trivial, and an appropriate heuristics is nee-
ded. A first guess could be the following : any fully
erased node (which spans an empty range) is fully
co-indexed, any partially erased node is copied
(with partial co-indexation). In particular, erased
verbs are always copied, since they can not occur
without non-erased arguments (or modifiers).
Elliptic substitution of an initial tree ? on a
ghost tree ?ghost : If a tree ? substituted in
a node Ni of a ghost tree ?ghost (ie. Derivation
g-Der2? on figure 5), where i is the traditional
index of an argumental position (N0,N1...) of this
tree ; and if there exists a ghost derivation of a
substitution of the tree ?ghost into a coordination
tree ?conj (Der. g-Der1) and therefore if this
ghost derivation pertains to a tree ?X where
a substitution derivation exists node Ni,(Der.
Der2) then we infer a ghost derivation indicating
the substitution of ? on the forwarded tree ?X
through the node Ni of the ghost tree ?ghost (Der.
Ghost-Der2).
150
g-Der2?:< N i? , ?, ? , substg, ?ghost >
g-Der1:< P?conjD, ?X , ?conj , substg, ?ghost
Der2:< N i?X ,?, ?X , subst,? >
ghost-Der2:< N i? , ?, ghost(?X), substg, ?ghost >
This is the mechanism seen in the analysis of
?Jean aime Marie et Pierre Virginie? to provide the
derivation tree.
Elliptic substitution of a initial ghost tree ?ghost
on a tree ? substituted on an tree ?conj : We
are here on a kind of opposite situation, we have
a realized subtree which lacks one of its argument
such as Jeani dormit puis ?i mourut (Johni slept
then ?i died). So we have to first let a mark in the
shared forest, then fetch the tree substituted on
the left part of the coordination, and get the tree
which has substituted on its ith node, then we will
be able to infer the proper substitution. We want
to create a real link, because as opposed to the last
case, it?s really a link, so the resulting structure
would be a graph with two links out of the tree
anchored by Jean, one to [dormir] (to sleep) and
one to [mourir] (to die).
If a ghost tree ?ghost substituted on a node Ni
of a tree ? (Der. g-Der1?), if this tree ? has been
substituted on a substitution node,PconjD, in the
rightmost part of a tree ?conj , (Der. Der1) ancho-
red by a coordinating conjunction, if the leftmost
part node, PconjL, of ?conj received a substitu-
tion of a tree ?s, (Der. Der2) and if this tree has
a substitution of a tree ?final on its ith node, (Der.
Der3) then we infer an item indicating a derivation
between the tree ?final and the tree ? on its node
Ni, (Der. g-Der1)10.
g-Der1?:< N i?ghost , ? , ?, substg, ?ghost >
Der1:< P?conjD, ?, ?conj , subst,? >
Der2:< P?conjL, ?s, ?conj , subst,? >
Der3:< N i?s , ?final, ?s, subst,? >
g-Der1:< N i? , ?final, ?, subst, ?ghost >
7 Conclusion
We presented a general framework to model and
to analyze elliptic constructions using simple me-
chanisms namely partial sharing and partial dupli-
cation through the use of a shared derivation fo-
rest in the LTAG framework. The main drawback
of this approach is the use of tree schemata as part
of parsing process because the anchoring process
10This mechanism without any restriction in the general
case, can lead to a exponential complexity w.r.t to the length
of the sentence.
must have a extremely good precision choose al-
gorithm when selecting the relevant trees. For the
best of our knowledge it is one of the first time that
merging tree schemata, shared forest walking and
graph induction, i.e., working with three different
levels of abstraction, is proposed. The mechanism
we presented is powerful enough to model much
more than the ellipsis of verbal heads and/or some
of their arguments. To model elliptic coordinations
for a given langage, the introduction of a specific
saturation feature may be needed to prevent over-
generation (as we presented in (Seddah and Sagot,
2006)). But the same mechanism can be used to go
beyond standard elliptic coordinations. Indeed, the
use of strongly structured anchors (e.g., with a dis-
tinction between the morphological lemma and the
lexeme) could allow a fine-grained specification of
partial value sharing phenomena (e.g. zeugmas).
Apart from an actual large scale implementation
of our approach (both in grammars and parsers),
future work includes applying the technique des-
cribed here to such more complex phenomena.
References
Anne Abeill?. 1991. Une grammaire lexicalis?e
d?arbres adjoints pour le fran?ais. Ph.D. thesis, Pa-
ris 7.
Alfred V. Aho. 1968. Indexed grammars-an extension
of context-free grammars. J. ACM, 15(4) :647?671.
Marie-H?l?ene Candito and Sylvain Kahane. 1998.
Can the TAG derivation tree represent a semantic
graph ? In Proceedings TAG+4, Philadelphie, pages
21?24.
Mary Dalrymple, Stuart M. Shieber, and Fernando
C. N. Pereira. 1991. Ellipsis and higher-order unifi-
cation. Linguistics and Philosophy, 14(4) :399?452.
Aravind K. Joshi and Yves Schabes. 1992. Tree Adjoi-
ning Grammars and lexicalized grammars. In Mau-
rice Nivat and Andreas Podelski, editors, Tree auto-
mata and languages. Elsevier Science.
Bernard Lang. 1991. Towards a Uniform Formal Fra-
mework for Parsing. In M. Tomita, editor, Current
Issues in Parsing Technology. Kluwer Academic Pu-
blishers.
Bernard Lang. 1992. Recognition can be harder than
parsing. In Proceeding of the Second TAG Work-
shop.
Owen Rambow and Aravind K. Joshi. 1994. A Formal
Look at Dependency Grammar and Phrase Structure
Grammars, with Special consideration of Word Or-
der Phenomena. Leo Wanner, Pinter London, 94.
Anoop Sarkar and Aravind Joshi. 1996. Coordination
in tree adjoining grammars : Formalization and im-
plementation. In COLING?96, Copenhagen, pages
610?615.
151
Djam? Seddah and Bertrand Gaiffe. 2005a. How to
build argumental graphs using TAG shared forest :
a view from control verbs problematic. In Proc.
of the 5th International Conference on the Logical
Aspect of Computional Linguistic - LACL?05, Bor-
deaux, France, Apr.
Djam? Seddah and Bertrand Gaiffe. 2005b. Using both
derivation tree and derived tree to get dependency
graph in derivation forest. In Proc. of the 6th In-
ternational Workshop on Computational Semantics
- IWCS-6, Tilburg, The Netherlands, Jan.
Djam? Seddah and Beno?t Sagot. 2006. Mod?lisation
et analyse des coordinations elliptiques via l?exploi-
tation dynamique des for?ts de d?rivation. In Proc.
of Traitement automatique des Langues Naturelle -
TALN 06 - louveau, Belgium, Apr.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24 :3?36.
Marc Steedman. 1990. Gapping as constituant coordi-
nation. Linguistic and Philosophy, 13 :207?264.
K. Vijay-Shanker and D. Weir. 1993. The use of sha-
red forests in tree adjoining grammar parsing. In
EACL ?93, pages 384?393.
8 Figures
1) Jean aimei Marie et Paul ?i Virginie
John loves Mary and Paul Virginia
Predicate elision
2) Pauli mange une pomme et ?i ach?te des cerises
Paul eats an apple and buys cherries
Right subject elision
3) Marie cuit ?i et Pierre vend des cr?pesi
Mary cooks and Peter sells pancakes
Left object elision
4)Mariei cuit ?j et ?i vend des cr?pesj
Mary cooks and sells pancakes
Left object and right subject elision
FIG. 3 ? Exemples of elliptic constructions
? ii
V
aime
N1
Virginie
V
d?teste
N1
Marie
P
etP P
N0
Paul
N0
Paul Virginie
D?testerAimer
Marie
Et
Derived tree
i iJean
P
etP
N1
Marie
P
N0
Paul
V N1
Virginie
V
?aime
N0
ghost(Aimer)Aimer
Et
Jean Marie VirginiePaul
FIG. 4 ? Gapping and Forword Conjunction reduc-
tion
?1
?1
?1?1
?1
?1
?2
?2
?3
?3 ?4 ?5
S
Conj(et)
?1
?2 ?3
?4 ?5
ghost(?1)
Ghost Der. 1
Ghost Der. 2
Ghost Der. 3
Der. 2
Der. 0
Der. 1
Shared forest Dependency graph
?5?4
P
P
VN0
V
N1
N
N
N
N N
N
N
N
Pconj
Pconj
Pconj_G
Jean Marie
Pconj_D
Pg
Pg
N0g
Paul
Vg
Vg
N1g
Virginieaime ?
et
FIG. 5 ? Shared forest and relative dependancy
graph for ?Jean aime Marie et Paul Virginie?( John
loves Mary and Paul Virginie)
call transition rules
Call subst < ?, N? , i, j,?,?, R, Stack > ?< ?, N?, i, j,?,?, R, Stack >
Call adj < ?, N? , i, j,?,?, R, Stack > ?< ?, N? , i, j,?,?, R, [N? |Stack] >
Call axiom S ?< ?, N?, 0, n,?,?, ?, ? >
Call no subs < ?, N? , i, j,?,?, R, Stack > ?true
Call foot < ?, ?N? , i, j,?,?, R, [N? |Stack] > ?< ?, N? , i, j,?,?, R, [Stack] >
The ?Call subst? rule is the rule which starts the recognition
of a substitution of the initial tree ? on the node N of the tree
? between the indices i and j. ?Call adj? starts the recogni-
tion of the adjunction of the auxiliary tree ? on the node N
of an elementary tree ? between i and j. ?Call axiom? starts
the recognition ? of an elementary tree spawning the whole
string. ?Call no subs? starts the recognition of a node N of
a elementary tree ? dominating the empty node between the
indices i and j. ?Call foot? starts the recognition of a subtree
dominated by the node N? between the indices i and j, the
node Ngamma was the start of the adjunction of the auxi-
liary tree ? and ?N? its foot node.
In order to avoid the ?call adj? rule to be over generating, we
control the size of the stack by the number of possible ad-
junctions at a given state : if the automata has no cycle and
if each state of the automata goes forward (j always superior
to i), the number of possible adjunctions on a spine (the path
between the root of an auxiliary tree and its foot) is bounded
by the length of the string to be analyzed.
FIG. 6 ? Shared forest derivation inference rules
152
Proceedings of the 10th Conference on Parsing Technologies, pages 94?105,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Are Very Large Context-Free Grammars Tractable?
Pierre Boullier & Beno??t Sagot
INRIA-Rocquencourt
Domaine de Voluceau, Rocquencourt BP 105
78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
Abstract
In this paper, we present a method which, in
practice, allows to use parsers for languages
defined by very large context-free grammars
(over a million symbol occurrences). The
idea is to split the parsing process in two
passes. A first pass computes a sub-grammar
which is a specialized part of the large gram-
mar selected by the input text and various
filtering strategies. The second pass is a tra-
ditional parser which works with the sub-
grammar and the input text. This approach
is validated by practical experiments per-
formed on a Earley-like parser running on
a test set with two large context-free gram-
mars.
1 Introduction
More and more often, in real-word natural lan-
guage processing (NLP) applications based upon
grammars, these grammars are no more written by
hand but are automatically generated, this has sev-
eral consequences. This paper will consider one of
these consequences: the generated grammars may
be very large. Indeed, we aim to deal with grammars
that have, say, over a million symbol occurrences
and several hundred thousands rules. Traditional
parsers are not usually prepared to handle them,
either because these grammars are simply too big
(the parser?s internal structures blow up) or the time
spent to analyze a sentence becomes prohibitive.
This paper will concentrate on context-free gram-
mars (CFG) and their associated parsers. However,
virtually all Tree Adjoining Grammars (TAG, see
e.g., (Schabes et al, 1988)) used in NLP applica-
tions can (almost) be seen as lexicalized Tree In-
sertion Grammars (TIG), which can be converted
into strongly equivalent CFGs (Schabes and Waters,
1995). Hence, the parsing techniques and tools de-
scribed here can be applied to most TAGs used for
NLP, with, in the worst case, a light over-generation
which can be easily and efficiently eliminated in a
complementary pass. This is indeed what we have
achieved with a TAG automatically extracted from
(Villemonte de La Clergerie, 2005)?s large-coverage
factorized French TAG, as we will see in Section 4.
Even (some kinds of) non CFGs may benefit from
the ideas described in this paper.
The reason why the run-time of context-free (CF)
parsers for large CFGs is damaged relies on a theo-
retical result. A well-known result is that CF parsers
may reach a worst-case running time ofO(|G|?n3)
where |G| is the size of the CFG and n is the length
of the source text.1 In typical NLP applications
which mainly work at the sentence level, the length
of a sentence does not often go beyond a value of
say 100, while its average length is around 20-30
words.2 In these conditions, the size of the grammar,
despite its linear impact on the complexity, may be
the prevailing factor: in (Joshi, 1997), the author re-
marks that ?the real limiting factor in practice is the
size of the grammar?.
The idea developed in this paper is to split the
parsing process in two passes. A first pass called
filtering pass computes a sub-grammar which is the
1These two notions will be defined precisely later on.
2At least for French, English and similar languages.
94
sub-part of the large input grammar selected by the
input sentence and various filtering strategies. The
second pass is a traditional parser which works with
the sub-grammar and the input sentence. The pur-
pose is to find a filtering strategy which, in typical
practical situations, minimizes on the average the
total run-time of the filtering pass followed by the
parser pass.
A filtering pass may be seen as a (filtering) func-
tion that uses the input sentence to select a sub-
grammar out of a large input CFG. Our hope, us-
ing such a filter, is that the time saved by the parser
pass which uses a (smaller) sub-grammar will not
totally be used by the filter pass to generate this sub-
grammar.
It must be clear that this method cannot improve
the worst-case parse-time because there exists gram-
mars for which the sub-grammar selected by the fil-
tering pass is the input grammar itself. In such a
case, the filtering pass is simply a waste of time. Our
purpose in this paper is to argue that this technique
may profit from typical grammars used in NLP. To
do that we put aside the theoretical view point and
we will consider instead the average behaviour of
our processors.
More precisely we will study on two large NL
CFGs the behaviour of our filtering strategies on a
set of test sentences. The purpose being to choose
the best filtering strategy, if any. By best, we mean
the one which, on the average, minimizes the total
run-time of both the filtering pass followed by the
parsing pass.
Useful formal notions and notations are recalled
in Section 2. The filtering strategies are presented
in Section 3 while the associated experiments are
reported in Section 4. This paper ends with some
concluding remarks in Section 5.
2 Preliminaries
2.1 Context-free grammars
A CFG G is a quadruple (N,T, P, S) where N is
a non-empty finite set of nonterminal symbols, T is
a finite set of terminal symbols, P is a finite set of
(context-free rewriting) rules (or productions) and
S is a distinguished nonterminal symbol called the
axiom. The sets N and T are disjoint and V = N?T
is the vocabulary. The rules in P have the form A?
?, with A ? N and ? ? V ?.
For a given string ? ? V ?, its size (length)
is noted |?|. As an example, for the input string
w = a1 ? ? ? an, ai ? T , we have |w| = n. The empty
string is denoted ? and we have |?| = 0. The size |G|
of a CFG G is defined by |G| = ?A???P |A?|.
For G, on strings of V ?, we define the binary re-
lation derive, noted ?, by ?1A?2 A???G ?1??2 if
A ? ? ? P and ?1, ?2 ? V ?. The subscript G
or even the superscript A ? ? may be omitted. As
usual, its transitive (resp. reflexive transitive) clo-
sure is noted +?
G
(resp. ??
G
). We call derivation any
sequence of the form ?1 ?G ? ? ? ?G ?2. A complete
derivation is a derivation which starts with the ax-
iom and ends with a terminal string w. In that case
we have S ??
G
? ??
G
w, and ? is a sentential form.
The string language defined (generated, recog-
nized) by G is the set of all the terminal strings that
are derived from the axiom: L(G) = {w | S +?
G
w,w ? T ?}. We say that a CFG is empty iff its
language is empty.
A nonterminal symbol A is nullable iff it can de-
rive the empty string (i.e., A +?
G
?). A CFG is ?-free
iff its nonterminal symbols are non-nullable.
A CFG is reduced iff every symbol of every pro-
duction is a symbol of at least one complete deriva-
tion. A reduced grammar is empty iff its production
set is empty (P = ?). We say that a non-empty
reduced grammar is in canonical form iff its vocab-
ulary only contains symbols that appear in the pro-
ductions of P .3,4
Two CFGs G and G? are weakly equivalent iff
they generate the same string language. They are
strongly equivalent iff they generate the same set of
structural descriptions (i.e., parse trees). It is a well
known result (See Section 3.2) that every CFG G
can be transformed in time linear w.r.t. |G| into a
strongly equivalent (canonical) reduced CFG G?.
For a given input string w ? T ?, we define its
3We may say that the canonical form of the empty reduced
grammar is ({S}, ?, ?, S) though the axiom S does not appear
in any production.
4Note that the pair (P, S) completely defines a reduced CFG
G = (N,T, P, S) in canonical form since we have N = {X0 |
X0 ? ? ? P} ? {S}, T = {Xi | X0 ? X1 ? ? ?Xp ?
P ?1 ? i ? p}?N . Thus, in the sequel, we often note simply
G = (P, S) grammars in canonical form.
95
ranges as the set Rw = {[i..j] | 1 ? i ? j ?
|w| + 1}. If w = w1tw3 ? T ? is a terminal string,
and if t ? T ? {?} is a (terminal or empty) sym-
bol, the instantiation of t in w is the triple noted
t[i..j] where [i..j] is a range with i = |w1| + 1 and
j = i + |t|. More generally, the instantiation of the
terminal string w2 in w1w2w3 is noted w2[i..j] with
i = |w1| + 1 and j = i + |w2|. Obviously, the in-
stantiation of w itself is then w[1..1 + |w|].
Let us consider an input string w = w1w2w3
and a CFG G. If we have a complete derivation
d = S ??
G
w1Aw3 A???G w1?w3
??
G
w1w2w3, we
see that A derives w2 (we have A +?G w2). More-
over, in this complete derivation, we also know a
range in Rw, namely [i..j], which covers the sub-
string w2 which is derived by A (i = |w1| + 1
and j = i + |w2|). This is represented by the in-
stantiated nonterminal symbol A[i..j]. In fact, each
symbol which appears in a complete derivation may
be transformed into its instantiated counterpart. We
thus talk of instantiated productions or (complete)
instantiated derivations. For a given input text w,
and a CFG G, let PwG be the set of instantiated pro-
ductions that appears in all complete instantiated
derivations.5 The pair (PwG , S[1..|w|+1]) is the (re-
duced) shared parse forest in canonical form.6
2.2 Finite-state automata
A finite-state automaton (FSA) is the 5-tuple A =
(Q,?, ?, q0, F ) where Q is a non empty finite set
of states, ? is a finite set of terminal symbols, ? is
the transition relation ? = {(qi, t, qj)|qi, qj ? Q ?
t ? T ? {?}}, q0 is a distinguished element of Q
called the initial state and F is a subset of Q whose
elements are called final states. The size of A is
defined by |A| = |?|.
As usual, we define both a configuration as an ele-
ment of Q?T ? and derive a binary relation between
5For example, in the previous complete derivation
d, let the right-hand side ? be the (vocabulary) string
X1 ? ? ?Xk ? ? ?Xp in which each symbol Xk derives the ter-
minal string xk ? T ? (we have Xk ??
G
xk and w2 =
x1 ? ? ?xk ? ? ?xp), then the instantiated production A[i0..ip] ?
X1[i0..i1] ? ? ?Xk[ik?1..ik] ? ? ?Xp[ip?1..ip] with i0 = |w1| +
1, i1 = i0 + |x1|, . . . , ik = ik?1 + |xk| . . . and ip = i0 + |w2|
is an element of PwG .
6The popular notion of shared forests mainly comes from
(Billot and Lang, 1989).
configurations, noted ?
A
by (q, tx) ?
A
(q?, x), iff
(q, t, q?) ? ?. If w?w?? ? T ?, we call derivation any
sequence of the form (q?, w?w??) ?
A
? ? ? ?
A
(q??, w??).
If w ? T ?, the initial configuration is noted c0 and
is the pair (q0, w). A final configuration is noted cf
and has the form (qf , ?) with qf ? F . A complete
derivation is a derivation which starts with c0 and
ends in a final configuration cf . In that case we have
c0
?
?
A
cf .
The language L(A) defined (generated, recog-
nized) by the FSA A is the set of all terminal strings
w for which there exists a complete derivation. We
say that an FSA is empty iff its language is empty.
Two FSAs A and A? are equivalent iff they defined
the same language.
An FSA is ?-free iff its transition relation has the
form ? = {(qi, t, qj)|qi, qj ? Q, t ? ?}, except per-
haps for a distinguished transition, the ?-transition
which has the form (q0, ?, qf ), qf ? F and allows
the empty string ? to be in L(A). Every FSA can be
transformed into an equivalent ?-free FSA.
An FSA A = (Q,?, ?, q0, F ) is reduced iff every
element of ? appears in a complete derivation. A
reduced FSA is empty iff we have ? = ?. We say
that a non-empty reduced FSA is in canonical form
iff its set of states Q and its set of terminal symbols
? only contain elements that appear in the transition
relation ?.7 It is a well known result that every FSA
A can be transformed in time linear with |A| into an
equivalent (canonical) reduced FSA A?.
2.3 Input strings and input DAGs
In many NLP applications8 the source text cannot
be considered as a single string of terminal symbols
but rather as a finite set of terminal strings. These
sets are finite languages which can be defined by
particular FSAs. These particular type of FSAs are
called directed-acyclic graphs (DAGs). In a DAG
w = (Q,?, ?, q0, F ), the initial state q0 is 1 and we
assume that there is a single final state f (F = {f}),
Q is a finite subset of the positive integers less than
or equal to f : Q = {i|1 ? i ? f}, ? is the set of
terminal symbols. For the transition relation ?, we
7We may say that the canonical form of the empty reduced
FSA is ({q0}, ?, ?, q0, ?) though the initial state q0 does not
appear in any transition.
8Speech processing, lexical ambiguity representation, . . .
96
require that its elements (i, t, j) are such that i < j
(there are no loops in a DAG). Without loss of gen-
erality, we will assume that DAGs are ?-free reduced
FSAs in canonical form and that any DAG w is noted
by a triple (?, ?, f) since its initial state is always 1
and its set of states is {i | 1 ? i ? f}.
For a given CFG G, the recognition of an input
DAG w is equivalent to the emptiness of its inter-
section with G. This problem can be solved in time
linear in |G| and cubic in |Q| the number of states of
w.
If the input text is a DAG, the previous notions of
range, instantiations and parse forest easily general-
ize: the indices i and j which in the string case locate
the positions of substrings are changed in the DAG
case into DAG states. For example if A[i0..ip] ?
X1[i0..i1] ? ? ?Xp[ip?1..ip] is an instantiated produc-
tion of the parse forest for G = (N,T, P, S) and
w = (?, ?, f), we have A ? X1 ? ? ?Xp ? P and
there is a path in the input DAG from state i0 to state
ip via states i1, . . . , ip?1.
Of course, any nonempty terminal string w ? T+,
may be viewed as a DAG (?, ?, f) where ? = {t |
w = w1tw2 ? t ? T}, ? = {(i, t, i + 1) | w =
w1tw2?t ? T?i = 1+|w1|} and f = 1+|w|. If the
input string w is the empty string ?, the associated
DAG is (?, ?, f) where ? = ?, ? = {(1, ?, 2)} and
f = 2. Thus, in the sequel, we will assume that the
inputs of our parsers are not strings but DAGs. As a
consequence the size (or length) of a sentence is the
size of its DAG (i.e., its number of transitions).
3 Filtering Strategies
3.1 Gold Strategy
Let G = (N,T, P, S) be a CFG, w = (?, ?, f)
be an input DAG of size n = |?| and ?Fw? =
(?Pw?, S[1..f ]) be the reduced output parse for-
est in canonical form. From ?Pw?, it is pos-
sible to extract a set of (reduced) uninstanti-
ated productions P gw = {A ? X1 ? ? ?Xp |
A[i0..ip] ? X1[i0..i1]X2[i1..i2] ? ? ?Xp[ip?1..ip] ?
?Pw?}, which, together with the axiom S, defines a
new reduced CFG Ggw = (P gw, S) in canonical form.
This grammar is called the gold grammar of G for
w, hence the superscript g. Now, if we use Ggw to
reparse the same input DAG w, we will get the same
output forest ?Fw?. But in that case, we are sure that
every production in P gw is used in at least one com-
plete derivation. Now, if this process is viewed as
a filtering strategy that computes a filtering function
as introduced in Section 1, it is clear that this strat-
egy is size-optimal in the sense that P gw is of minimal
size, we call it the gold strategy and the associated
gold filtering function is noted g. Since we do not
want that a filtering strategy looses parses, the result
Gfw = (P fw , S) of any filtering function f must be
such that, for every sentence w, P fw is a superset of
P gw. In other words the recall score of any filtering
function f must be of 100%. We can note that the
parsing pass which generates Ggw may be led by any
filtering strategy f .
As usual, the precision score (precision for short)
of a filtering strategy f (w.r.t. the gold case) is, for
a given w, defined by the quotient |P
g
w|
|P fw|
which ex-
presses the number of useful productions selected by
f on w (for some G).
However, it is clear that we are interested in strate-
gies that are time-optimal and size-optimal strategies
are not necessarily also time-optimal: the time taken
at filtering-time to get a smaller grammar will not
necessarily be won back at parse-time.
For a given CFG G, an input DAG w and a filter-
ing strategy c, we only have to plot the times taken
by the filtering pass and by the parsing pass to make
some estimations on their average (median, decile)
parse times and then to decide which is the winner.
However, it may well happens that a strategy which
has not received the award (with the sample of CFGs
and the test sets tried) would be the winner in an-
other context!
All the following filtering strategies exhibit nec-
essary conditions that any production must hold in
order to be in a parse.
3.2 The make-a-reduced-grammar Algorithm
An algorithm which takes as input any CFG
G = (N,T, P, S) and generates as output a
strongly equivalent reduced CFG G? and which
runs in O(|G|) can be found in many text books
(See (Hopcroft and Ullman, 1979) for example).
So as to eliminate from all our intermediate sub-
grammars all useless productions, each filtering
strategy will end by a call to such an algorithm
named make-a-reduced-grammar.
97
The make-a-reduced-grammar algorithm works
as follows. It first finds all productive9 symbols. Af-
terwards it finds all reachable10 symbols. A symbol
is useful (otherwise useless) if it is both productive
and reachable. A production A? X1 ? ? ?Xp is use-
ful (otherwise useless) iff all its symbols are useful.
A last scan over the grammar erases all useless pro-
duction and leaves the reduced form. The canonical
form is reached in only retaining in the nonterminal
and terminal sets of the sub-grammar the symbols
which occur in the (useful) production set.
3.3 Basic Filtering Strategy: b-filter
The basic filtering strategy (b-filter for short) which
is described in this section will always be tried the
first. Thus, its input is the couple (G,w) where
G = (N,T, P, S) is the large initial CFG and the in-
put sentence w is a reduced DAG in canonical form
w = (?, ?, f) of size n. It generates a reduced CFG
in canonical form noted Gb = (P b, S) in which the
references to both G and w are assumed. Besides
this b-filter, we will examine in Sections 3.4 and 3.5
two others filtering strategies named a and d. These
filters will always have as input a couple (Gc, w)
where Gc = (P c, S) is a reduced CFG in canonical
form which has already been filtered by a previous
sequence of strategies noted c. They generate a re-
duced CFG in canonical form noted Gcf = (P cf , S)
with f = a or f = d respectively. Of course it may
happens that Gcf is identical to Gc if the f -filter is
not effective. A filtering strategy or a combination of
filtering strategies may be applied several times and
lead to a filtered grammar of the form say Gba2da
in which the sequence ba2da explicits the order in
which the filtering strategies have been performed.
We may even repeatedly apply a until a fixed point
is reached before applying d, and thus get something
of the form Gba?d.
The idea behind the b-filter is very simple and has
largely been used in lexicalized formalisms parsing,
in particular in LTAG (Schabes et al, 1988) parsing.
The filter rejects productions of P which contain ter-
minal symbols that do not occur in ? (i.e., that are
not terminal symbols of the DAG w) and thus takes
9X ? V is productive iff we have X ??
G
w,w ? T ?.
10X ? V is reachable iff we have S ??
G
w1Xw2, w1w2 ?
T ?.
S ? AB (1)
S ? BA (2)
A ? a (3)
A ? ab (4)
B ? b (5)
B ? bc (6)
Table 1: A simple grammar
O(|G|) time if we assume that the access to the ele-
ments of the terminal set ? is performed in constant
time. Unlexicalized productions whose right-hand
sides are in N? are kept. It also rejects productions
in which several terminal symbol occurs, in an order
which is not compatible with the linear order of the
input.
Consider for example the set of productions
shown in Table 1 and assume that the source text
is the terminal string ab. It is clear that the b-filter
will erase production 6 since c is not in the source
text.
The execution of the b-filter produces a (non-
reduced) CFG G? such that |G?| ? |G|. However, it
may be the case that some productions of G? are use-
less, it will thus be the task of the make-a-reduced-
grammar algorithm to transform G? into its reduced
canonical form Gb in time O(|G?|). The worst-case
total running time of the whole b-filter pass is thus
O(|G| ? n).
We can remark that, after the execution of the b-
filter, the set of terminal symbols of Gb is a subset
of T ? ?.
3.4 Adjacent Filtering Strategy: a-filter
As explained before, we assume that the input to
the adjacent filtering strategy (a-filter for short) de-
scribed in this section is a couple (Gc, w) where
Gc = (N c, T c, P c, S) is a reduced CFG in canon-
ical form. However, the a-filter would also work
for a non-reduced CFG. As usual, we define the
symbols of Gc as the elements of the vocabulary
V c = N c ? T c.
The idea is to erase productions that cannot be
part of any parses for w in using an adjacency crite-
ria: if two symbols are adjacent in a rule, they must
98
derive terminal symbols that are also adjacent in w.
To give a (very) simple practical idea of what we
mean by adjacency criteria, let us consider again the
source string ab and the grammar defined in Table 1
in which the last production has already been erased
by the b-filter.
The fact that the B-production ends with a b and
that the A-productions all start with an a, implies
that production 2 is in a complete parse only if the
source text is such that b is immediately followed
by a. Since it is not the case, production 2 can be
erased.
More generally, consider a production of the form
A ? ? ? ?XY ? ? ? . If for each couple (a, b) ? T 2 in
which a is a terminal symbol that can terminate (the
terminal strings generated by) X and b is a terminal
symbol that can lead (the terminal strings generated
by) Y , there is no transition on b that can follow a
transition on a in the DAG w, it is clear that the pro-
duction A? ? ? ?XY ? ? ? can be safely erased.
Now assume that we have the following (left)
derivation Y ?? Y1?1 ?? Yi?i ? ? ? ?1 ??
? ? ? Yp?1??pYp?p? ?pYp?p ? ? ? ?1 ?? Yp?p ? ? ? ?1,
with ?p ?? ?. If for each couple (a, b?) in which
a has the previous definition and b? is a terminal
symbol that can lead (the terminal strings gener-
ated by) Yp, there is no transition on b? that can fol-
low a transition on a in the DAG w, the production
Yp?1 ? ?pYp?p can be erased if it is not valid in
another context.
Moreover, consider a (right) derivation of the
form X ?? ?1X1 ?? ?1 ? ? ??iXi ??
? ? ? Xp?1??pXp?p? ?1 ? ? ??pXp?p ?? ?1 ? ? ??pXp,
with ?p ?? ?. If for each couple (a?, b) in which b
has the previous definition and a? is a terminal sym-
bol that can terminate (the terminal strings gener-
ated by) Xp, there is no transition on b that can fol-
low a transition on a? in the DAG w, the production
Xp?1 ? ?pXp?p can be erased if it is not valid in
another context.
In order to formalize these notions we define sev-
eral binary relations together with their (reflexive)
transitive closure.
Within a CFG G = (N,T, P, S), we first define
left-corner noted x. Left-corner (Nederhof, 1993;
Moore, 2000), hereafter LC, is a well-known rela-
tion since many parsing strategies are based upon it.
We say that X is in the LC of A and we write A x X
iff (A,X) ? {(B,Y ) | B ? ?Y ? ? P ? ? ??
G
?}.
We can write A x
A??X?
X to enforce how the cou-
ple (A,X) may be produced.
For its dual relation, right-corner, noted y, we say
that X is in the right corner of A and we write X y A
iff (X,A) ? {(Y,B) | B ? ?Y ? ? P ? ? ??
G
?}. We can write X y
A??X?
A to enforce how the
couple (X,A) may be produced.
We also define the first (resp. last) relation noted
??t (resp. ?? t) by ??t= {(X, t) | X ? V ? t ?
T ?X ??
G
tx ? x ? T ?} (resp. ?? t= {(X, t) | X ?
V ? t ? T ?X ??
G
xt ? x ? T ?}).
We define the adjacent ternary relation on V ?
N? ? V noted ? and we write X ?? Y iff
(X,?, Y ) ? {(U, ?, V ) | A? ?U?V ? ? P ?? ??
G
?}. This means that X and Y occur in that order in
the right-hand side of some production and are sep-
arated by a nullable string ?. Note that X or Y may
or may not be nullable.
On the input DAG w = (?, ?, f), we define the
immediately precede relation noted < and we write
a < b for a, b ? ? iff w1abw3 ? L(w), w1, w3 ?
??.
We also define the precede relation noted ? and
we write a ? b for a, b ? ? iff w1aw2bw3 ?
L(w), w1, w2, w3 ? ??.We can note that ? is not
the transitive closure of <.11
For each production A ? ?X0X1 ? ? ?Xp?1Xp?
in P c and for each symbol pairs (X0,Xp) of non-
nullable symbols s.t. X1 ? ? ?Xp?1 ??Gc ?, we com-
pute two sets A1 and A2 of couples (a, b), a, b ? T c
defined by A1 = ?0<i?p = {(a, b) | a ?? t
X0
X1???Xi?1? Xi ??t b} and A2 = ?0?i<p =
{(a, b) | a ?? t Xi
Xi+1???Xp?1? Xp ??t b}. Any
11Consider the source string bcab for which we have a
+
< c,
but not a ? c.
99
pair (a, b) of A1 is such that the terminal symbol
a may terminate a phrase of X0 while the terminal
symbol b may lead a phrase of X1 ? ? ?Xp. Since
X0 and Xp are not nullable, A1 is not empty. If
none of its elements (a, b) is such that a < b, the
production A ? ?X0X1 ? ? ?Xp?1Xp? is useless
and can be erased. Analogously, any pair (a, b) of
A2 is such that the terminal symbol a may termi-
nate a phrase of X0X1 ? ? ?Xp?1 while the terminal
symbol b may lead a phrase of Xp. Since X0 and
Xp are not nullable, A2 is not empty. If none of
its elements (a, b) is such that a < b, the produc-
tion A ? ?X0X1 ? ? ?Xp?1Xp? is useless and can
be erased. Of course if X1 ? ? ?Xp?1 = ?, we have
A1 = A2.12
The previous method has checked some adjacent
properties inside the right-hand sides of productions.
The following will perform some analogous checks
but at the beginning and at the end of the right-hand
sides of productions.
Let us go back to Table 1 to illustrate our pur-
pose. Recall that, with source text ab, productions 6
and 2 have already been erased. Consider produc-
tion 4 whose left-hand side is an A, the terminal
string ab that it generates ends by b. If we look for
the occurrences of A in the right-hand sides of the
(remaining) productions, we only find production 1
which indicates that A is followed by B. Since the
phrases of B all start with b (See production 5) and
since in the source text b does not immediately fol-
low another b, production 4 can be erased.
In order to check that the input sentence w starts
and ends by valid terminal symbols, we augment
the adjacent relation with two elements ($, ?, S) and
(S, ?, $) where $ is a new terminal symbol which is
supposed to start and to end every sentence.13
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If X is a non-nullable
symbol, we compute the set L = {(a, b) | a ?? t
X ?? Y ?x Z x
Z??U?
U ??t b}. Since Gc is reduced
and since $ < S, we are sure that the set X ?? Y ?x
12It can be shown that the previous check can be performed
on (Gc, w) in worst-case timeO(|Gc|?|?|3) (recall that |?| ?
n). This time reduces to O(|Gc| ? |?|2) if the input sentence
is not a DAG but a string.
13This is equivalent to assume the existence in the grammar
of a super-production whose right-hand side has the form $S$.
Z is non-empty, thus L is also non-empty.14
We can associate with each couple (a, b) ?
L at least one (left) derivation of the form
X?Y ??
Gc
w0aw1?Y ??Gc w0aw1w2Y
??
Gc
w0aw1w2w3Z?2
Z??U??
Gc
w0aw1w2w3?U??2 ??Gc
w0aw1w2w3w4U??2 ??Gc w0aw1w2w3w4w5b?1??2
in which w1w2w3w4w5 ? T c?. These derivations
contains all possible usages of the production Z ?
?U? in a parse. If for every couple (a, b) ? L, the
statement a? b does not hold, we can conclude that
the production Z ? ?U? is not used in any parse
and can thus be deleted.
Analogously, we can check that the order of ter-
minal symbols is compatible with both a production
and its right grammatical context.
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If Y is a non-nullable
symbol, we compute the set R = {(a, b) | a ?? t
U y
Z??U?
Z ?y X ?? Y ??t b}. Since Gc is reduced
and since S < $, we are sure that the set Z ?y X ??
Y is non-empty, thus R is also non-empty.14
To each couple (a, b) ? R we can asso-
ciate at least one (right) derivation of the form
X?Y ??
Gc
X?w1bw0 ??Gc Xw2w1bw0
??
Gc
?1Zw3w2w1bw0
Z??U??
Gc
?1?U?w3w2w1bw0 ??Gc
?1?Uw4w3w2w1bw0 ??Gc ?1??2aw5w4w3w2w1bw0
in which w5w4w3w2w1 ? T c?. These deriva-
tions contains all possible usages of the production
Z ? ?U? in a partial parse. If for every couple
(a, b) ? L, the statement a ? b does not hold, we
can conclude that the production Z ? ?U? is not
used in any parse and can thus be deleted.
Now, a call to the make-a-reduced-grammar al-
gorithm produces a reduced CFG in canonical form
named Gca = (N ca, T ca, P ca, S).
14This statement does not hold any more if we exclude from
P c the productions that have been previously erased during the
current a-filter. In that case, an empty set indicates that the
production Z ? ?U? can be erased.
100
3.5 Dynamic Set Automaton Filtering
Strategy: d-filter
In (Boullier, 2003) the author has presented a
method that takes a CFG G and computes a FSA
that defines a regular superset of L(G). However his
method would produce intractable gigantic FSAs.
Thus he uses his method to dynamically compute
the FSA at parse time on a given source text. Based
on experimental results, he shows that his method
called dynamic set automaton (DSA) is tractable.
He uses it to guide an Earley parser (See (Ear-
ley, 1970)) and shows improvements over the non
guided version. The DSA method can directly be
used as a filtering strategy since the states of the un-
derlying FSA are in fact sets of items. For a CFG
G = (N,T, P, S), an item (or dotted production)
is an element of {[A ? ?.?] | A ? ?? ? P}.
A complete item has the form [A ? ?.], it indi-
cates that the production A ? ? has been, in some
sense, recognized. Thus, the complete items of the
DSA states gives the set of productions selected by
the DSA. This selection can be further refined if we
also use the mirror DSA which processes the source
text from right to left and if we only select complete
items that both belong to the DSA and to its mirror.
Thus, if we assume that the input to the DSA fil-
tering strategy (d-filter) is a couple (Gc, w) where
Gc = (P c, S) is a reduced CFG in canonical form,
we will eventually get a set of productions which is
a subset of P c. If it is a strict subset, we then ap-
ply the make-a-reduced-grammar algorithm which
produces a reduced CFG in canonical form named
Gcd = (P cd, S).
The Section 4 will give measures that may help to
compare the practical merits of the a and d-filtering
strategies.
4 Experiments
The measures presented in this section have been
taken on a 1.7GHz AMD Athlon PC with 1.5 Gb
of RAM running Linux. All parsers are written in C
and have been compiled with gcc 2.96 with the O2
optimization flag.
4.1 Grammars and corpus
We have performed experiments with two large
grammars described below. The first one is an auto-
matically generated CFG, the other one is the CFG
equivalent of a TIG automatically extracted from a
factorized TAG.
The first grammar, named GT>N , is a variant of
the CFG backbone of a large-coverage LFG gram-
mar for French used in the French LFG parser de-
scribed in (Boullier and Sagot, 2005). In this vari-
ant, the set T of terminal symbols is the whole set of
French inflected forms present in the Lefff , a large-
coverage syntactic lexicon for French (Sagot et al,
2006). This leads to as many as 407,863 different
terminal symbols and 520,711 lexicalized produc-
tions (hence, the average number of categories ?
which are here non-terminal symbols ? for an in-
flected form is 1.27). Moreover, this CFG entails
a non-neglectible amount of syntactic constraints
(including over-generating sub-categorization frame
checking), which implies as many as |Pu| = 19, 028
non-lexicalized productions. All in all, GT>N has
539,739 productions.
The second grammar, named GTIG, is a CFG
which represents a TIG. To achieve this, we applied
(Boullier, 2000)?s algorithm on the unfolded version
of (Villemonte de La Clergerie, 2005)?s factorized
TAG. The number of productions in GTIG is com-
parable to that of GT>N . However, these two gram-
mars are completely different. First, GTIG has much
less terminal and non-terminal symbols than GT>N .
This means that the basic filter may be less efficient
on GTIG than on GT>N . Second, the size of GTIG
is enormous (more than 10 times that of GT>N ),
which shows that right-hand sides of GTIG?s pro-
ductions are huge (the average number of right-hand
side symbols is more than 24). This may increase
the usefulness of a- and d-filtering strategies.
Global quantitative data about these grammars is
shown in Table 2.
Both grammars, as evoked in the introduction,
have not been written by hand. On the contrary, they
are automatically generated from a more abstract
and more compact level (a meta-level over LFG for
GT>N , and a metagrammar for GTIG). These gram-
mars are not artificial grammars set up only for this
experiment. On the contrary, they are automatically
generated huge real-life CFGs that are variants of
grammars used in real NLP applications.
Our test suite is a set of 3093 French journalistic
sentences. These sentences are the general lemonde
101
G |N | |T | |P | |Pu| |G|
GT>N 7,862 407,863 539,739 19,028 1,123,062
GTIG 448 173 493,408 4,338 12,455,767
Table 2: Sizes of the grammars GT>N and GTIG
used in our experiments
part of the EASy parsing evaluation campaign cor-
pus. Raw sentences have been turned into DAGs
of inflected forms known by both grammar/lexicon
couples.15 This step has been achieved by the pre-
syntactic processing chain SXPipe (Sagot and Boul-
lier, 2005). They are all recognized by both gram-
mars.16 The resulting DAGs have a median size of
28 and an average size of 31.7.
Before entering into details, let us give here the
first important result of these experiments: it was
actually possible to build parsers out of GT>N and
GTIG and to parse efficiently with the resulting
parsers (we shall detail later on efficiency results).
Given the fact that we are dealing with grammars
whose sizes are respectively over 1,000,000 and over
12,000,000, this is in itself a very satisfying result.
4.2 Precision results
Let us recall informally that the precision of a filter-
ing strategy is the proportion of productions in the
resulting sub-grammar that are in the gold grammar,
i.e., that have effectively instantiated counterparts in
the final parse forest.
We have applied different strategies so as to com-
pare their precisions. The results on GT>N and
GTIG are summed up in Table 3. These results give
several valuable results. First, as we expected, the
basic b-filter drastically reduces the size of the gram-
mar. The result is even better on GT>N thanks to its
large number of terminal symbols. Second, both the
adjacency a-filter and the DSA d-filter efficiently re-
duce the size of the grammar: on GT>N , the a-filter
eliminates 20% of the productions they receive as
input, a bit less for the d-filter. Indeed, the a-filter
performs better than the d-filter introduced in (Boul-
15As seen above, inflected forms are directly terminal sym-
bols of GT>N , while GTIG uses a lexicon to map these in-
flected forms into its own terminal symbols, thereby possibly
introducing lexical ambiguity.
16Approx. 15% of the original set of sentences were not rec-
ognized, and required error recovery techniques; we decided to
discard them for this experiment.
Strategy Average precision
GT>N GTIG
no filter 0.04% 0.03%
b 62.87% 39.43%
bd 74.53% 66.56%
ba 77.31% 66.94%
ba? 77.48% 67.48%
bad 80.27% 77.16%
ba?d 80.30% 77.41%
gold 100% 100%
Table 3: Average precision of six different filtering
strategies on our test corpus with GT>N and GTIG.
lier, 2003), at least as precision is concerned. We
shall see later that this is still the case on global
parsing times. However, applying the d-filter after
the a-filter still removes a non-neglectible amount
of productions:17 each technique is able to eliminate
productions that are kept by the other one. The result
of these filters is suprisingly good: in average, after
all filters, only approx. 20% of the productions that
have been kept will not be successfully instantiated
in the final parse forest. Third, the adjacency filter
can be used in its one-pass mode, since almost all
the benefit from the full (fix-point) mode is already
reached after the first application. This is practically
a very valuable result, since the one-pass mode is
obviously faster than the full mode.
However, all these filters do require computing
time, and it is necessary to evaluate not only the pre-
cision of these filters, but also their execution time
as well as the influence they have on the global (in-
cluding filtering) parsing time .
4.3 Parsing time and best filter
Filter execution times for the six filtering strategies
introduced in Table 3 are illustrated for GT>N in
Figure 1. These graphics show three extremely valu-
able pieces of information. First, filtering times are
extremely low: the average filtering time for the
slowest filter (ba?d, i.e., basic plus full adjacency
plus DSA) on 40-word sentences is around 20 ms.
Second, on small sentences, filtering times are virtu-
ally zero. This is important, since it means that there
17Although not reported here, applying the a before d leads
to the same conclusion.
102
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
b-filter bd-filter ba-filter
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
ba?-filter bad-filter ba?d-filter
Figure 1: Filtering times for six different strategies with GT>N
is almost no fixed cost to pay when we use these
filters (let us recall that without any filter, building
efficient parsers for such a huge grammar is highly
problematic). Third, all these filters, at least when
used with GT>N , are executed in a time which is
linear w.r.t. the size of the input sentence (i.e., the
size of the input DAG).
The results on GTIG lead to the same conclusions,
with one exception: with this extremely huge gram-
mar with so long right-hand sides, the basic filter
is not as fast as on GT>N (and not as precise, as
we will see below, which slows down the make-a-
reduced-grammar algorithm since it is applied on
a larger filtered grammars). For example, the me-
dian execution time for the basic filter on sentences
whose size is approximately 40 is 0.25 seconds,
to be compared with the 0.00 seconds reached on
GT>N (this zero value means a median time strictly
lower than 0.01 seconds, which is the granularity of
our time measurments).
Figure 2 and 3 show the global (filtering+parsing)
execution time for the 6 different filters. We only
show median times computed on classes of sen-
tences of length 10i to 10(i + 1) ? 1 and plotted
with a centered x-coordinate (10(i + 1/2)), but re-
sults with other percentiles or average times on the
same classes draw the same overall picture.
 0
 0.05
 0.1
 0.15
 0.2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 2: Global (filtering+parsing) times for six
different strategies with GT>N
One can see that the results are completely differ-
ent, showing a strong dependency on the character-
istics of the grammar. In the case of GT>N , the huge
number of terminal symbols and the reasonable av-
erage size of right-hand sides of productions, the ba-
sic filtering strategy is the best strategy: although it
is fast because relatively simple, it reduces the gram-
mar extremely efficiently (it has a 60.56% precision,
to be compared with the precision of the void filter
which is 0.04%). Hence, for GT>N , our only result
103
 0
 0.5
 1
 1.5
 2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 3: Global (filtering+parsing) times for six
different strategies with GTIG
is that this basic filter does allow us to build an effi-
cient parser (the most efficient one), but that refined
additionnal filtering strategies are not useful.
The picture is completely different with GTIG.
Contrary to GT>N , this grammar has comparatively
very few terminal and non-terminal symbols, and
very long right-hand sides. These two facts lead
to a lower precision of the basic filter (39.43%),
which keeps many more productions when applied
on GTIG than when applied on GT>N , and leads,
when applied alone, to the less efficient parser. This
gives to the adjacency filter much more opportunity
to improve the global execution time. However, the
complexity of the grammar makes the construction
of the DSA filter relatively costly despite its preci-
sion, leading to the following conclusion: on GTIG
(and probably on any grammar with similar charac-
teristics), the best filtering strategy is the one-pass
adjacency strategy. In particular, this leads to an im-
provement over the work of (Boullier, 2003) which
only introduced the DSA filter. Incidentally, the
extreme size of GTIG leads to much higher pars-
ing times, approximately 10 times higher than with
GT>N , which is consistent with the ratio between
the sizes of both involved grammars.
5 Conclusion
It is a well known result in optimization techniques
that the key to practically improve these processes is
to reduce their search space. This is also the case in
parsing and in particular in CF parsing.
Many parsers process their inputs from left to
right but we can find in the literature other parsing
strategies. In particular, in NLP, (van Noord, 1997)
and (Satta and Stock, 1994) propose bidirectional al-
gorithms. These parsers have the reputation to have
a better efficiency than their left-to-right counterpart.
This reputation is not only based upon experimental
results (van Noord, 1997) but also upon mathemat-
ical arguments in (Nederhof and Satta, 2000). This
is specially true when the productions of the CFG
strongly depend on lexical information. In that case
the parsing search space is reduced because the con-
straints associated to lexical elements are evaluated
as early as possible. We can note that our filtering
strategies try to reach the same purpose by a totally
different mean: we reduce the parsing search space
by eliminating as many productions as possible, in-
cluding possibly non-lexicalized productions whose
irrelevance to parse the current input can not be di-
rectly deduced from that input.
We can also remark that our results are not in con-
tradiction with the claims of (Nederhof and Satta,
2000) in which they argue that ?Earley algorithm
and related standard parsing techniques [. . . ] can-
not be directly extended to allow left-to-right and
correct-prefix-property parsing in acceptable time
bound?. First, as already noted in Section 1, our
method does not work for any large CFG. In order
to work well, the first step of our basic strategy must
filter out a great amount of (lexicalized) productions.
To do that, it is clear that the set of terminals in the
input text must select a small ratio of lexicalized pro-
ductions. To give a more concrete idea we advo-
cate that the selected productions produce roughly a
grammar of normal size out of the large grammar.
Second, our method as a whole clearly does not pro-
cess the input text from left-to-right and thus does
not enter in the categories studied in (Nederhof and
Satta, 2000). Moreover, the authors bring strong evi-
dences that in case of polynomial-time off-line com-
pilation of the grammar, left-to-right parsing cannot
be performed in polynomial time, independently of
the size of the lexicon. Once again, if our filter pass
is viewed as an off-line processing of the large input
grammar, our output is not a compilation of the large
grammar, but a (compilation of a) smaller grammar,
specialized in (some abstractions of) the source text
only. In other words their negative results do not
104
necessarily apply to our specific case.
The experiment campaign as been conducted in
using an Earley-like parser.18 We have also success-
fuly tried the coupling of our filtering strategies with
a CYK parser (Kasami, 1967; Younger, 1967) as
post-processor. However the coupling with a GLR
parser (See (Satta, 1992) for example) is perhaps
more problematic since the time taken to build up
the underlying nondeterministic LR automaton from
the sub-grammar can be prohibitive.
Though no definitive answer can be made to the
question asked in the title, we have shown that, in
some cases, the answer is certainly yes.
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Meeting of
the Association for Computational Linguistics, pages
143?151.
Pierre Boullier and Beno??t Sagot. 2005. Efficient and ro-
bust LFG parsing: SxLfg. In Proceedings of IWPT?05,
pages 1?10, Vancouver, Canada.
Pierre Boullier. 2000. On TAG parsing. Traitement Au-
tomatique des Langues (T.A.L.), 41(3):759?793.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT 03, pages 43?54, Nancy, France.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Jeffrey D. Hopcroft and John E. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Compu-
tation. Addison-Wesley, Reading, Mass.
Aravind Joshi. 1997. Parsing techniques. In Sur-
vey of the state of the art in human language tech-
nology, pages 351?356. Cambridge University Press,
New York, NY, USA.
Tadao Kasami. 1967. An efficient recognition and syntax
algorithm for context-free languages. Scientific Re-
port AFCRL-65?758, Air Force Cambridge Research
Laboratory, Bedford, Massachusetts, USA.
Robert C. Moore. 2000. Improved left-corner
chart parsing for large context-free gram-
mars. In Proceedings of IWPT 2000, pages
18Contrarily to classical Earley parsers, its predictor phase
uses a pre-computed structure which is roughly an LC relation.
Note that this feature forces our filters to compute an LC rela-
tion on the generated sub-grammar. This also shows that LC
parsers may also benefit from our filtering techniques.
171?182, Trento, Italy. Revised version at
http://www.cogs.susx.ac.uk/lab/nlp/
carroll/cfg-resources/iwpt2000-rev2.ps.
Mark-Jan Nederhof and Giorgio Satta. 2000. Left-to-
right parsing and bilexical context-free grammars. In
Proceedings of the first conference on North American
chapter of the ACL, pages 272?279, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Mark-Jan Nederhof. 1993. Generalized left-corner pars-
ing. In Proceedings of the sixth conference on Euro-
pean chapter of the ACL, pages 305?314, Morristown,
NJ, USA. ACL.
Beno??t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing. In
Proceedings of L&TC 2005, pages 348?351, Poznan?,
Poland.
Beno??t Sagot, Lionel Cle?ment, ?Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2 syn-
tactic lexicon for french: architecture, acquisition, use.
In Proc. of LREC?06.
Giorgio Satta and Oliviero Stock. 1994. Bidirectional
context-free grammar parsing for natural language
processing. Artif. Intell., 69(1-2):123?164.
Giorgio Satta. 1992. Review of ?generalized lr parsing?
by masaru tomita. kluwer academic publishers 1991.
Comput. Linguist., 18(3):377?381.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: Cubic-time, parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21(4):479?513.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Comput.
Linguist. (COLING?88), Budapest, Hungary.
Gertjan van Noord. 1997. An efficient implementation of
the head-corner parser. Comput. Linguist., 23(3):425?
456.
?Eric Villemonte de La Clergerie. 2005. From metagram-
mars to factorized TAG/TIG parsers. In Proceedings
of IWPT?05, pages 190?191, Vancouver, Canada.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
105
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 633?640
Manchester, August 2008
Computer aided correction and extension of a syntactic wide-coverage
lexicon
Lionel NICOLAS
?
, Beno??t SAGOT
?
, Miguel A. MOLINERO
?
,
Jacques FARR
?
E
?
,
?
Eric DE LA CLERGERIE
?
?
Team RL, Laboratory I3S - UNSA + CNRS, 06903 Sophia Antipolis, France
{lnicolas, jf}@i3s.unice.fr
?
Project ALPAGE, INRIA Rocquencourt + Paris 7, 78153 Le Chesnay, France
{benoit.sagot, Eric.De La Clergerie}@inria.fr
?
Grupo LYS, Univ. de A Coru?na, 15001 A Coru?na, Espa?na
mmolinero@udc.es
Abstract
The effectiveness of parsers based on man-
ually created resources, namely a grammar
and a lexicon, rely mostly on the quality
of these resources. Thus, increasing the
parser coverage and precision usually im-
plies improving these two resources. Their
manual improvement is a time consuming
and complex task : identifying which re-
source is the true culprit for a given mis-
take is not always obvious, as well as find-
ing the mistake and correcting it.
Some techniques, like van Noord (2004)
or Sagot and Villemonte de La Clergerie
(2006), bring a convenient way to automat-
ically identify forms having potentially er-
roneous entries in a lexicon. We have in-
tegrated and extended such techniques in a
wider process which, thanks to the gram-
mar ability to tell how these forms could
be used as part of correct parses, is able to
propose lexical corrections for the identi-
fied entries.
We present in this paper an implementa-
tion of this process and discuss the main re-
sults we have obtained on a syntactic wide-
coverage French lexicon.
1 Introduction
Increasing the coverage and precision of non-
trained parsers based on manually created gram-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mar and lexicon, relies mostly on the improvement
of these two resources.
The manual improvement of wide coverage lin-
guistic resources is a labour-extensive, complex
and error prone task, requiring an important human
expert work.
In order to minimize human intervention, sim-
plify the process and increase its relevance, auto-
matic or semi-automatic tools can be used. We
present one such tool, using raw inputs, which de-
tects shortcomings of a lexicon and helps correct
them by proposing relevant corrections.
Detecting forms erroneously or incompletely
described in the lexicon is achieved by applying
two techniques which exhibit suspicious forms and
associate them with a set of non parsable sen-
tences.
Proposing relevant corrections relies on the fol-
lowing assumption: when studying the expecta-
tions of a grammar for a suspicious form in various
non-parsable sentences, we can observe expected
use patterns for this form. Those patterns can be
regarded as possible corrections for the lexicon. In
a metaphorical way, we believe the problem to be
due to the lexicon
1
, and we ask the grammar to ex-
press possible corrections for the lexicon.
The set of techniques we present here is fully
system and language independent: it can be eas-
ily applied to most existing lexica and unification-
grammar based parsers. The only condition is to
provide lexically and grammatically valid inputs,
such as law texts or newspapers, in order to ensure
that the rejection of a sentence is only due to errors
1
We will discuss later lexical forms incorrectly suspected
because of errors in other components of the parsing system,
notably the grammar.
633
in some components on which the parser relies on.
This paper is organized as follows. We start by
giving a global view of the whole process (Sect. 2)
that we later detail step by step (Sect. 3, 4, 5, 6
and 7). We then compare our work with previ-
ously publicated ones (Sect. 8) right before expos-
ing the practical context and the results obtained
(Sect. 9). Finally, we outline the planned improve-
ments (Sect. 10) and conclude (Sect. 11).
2 Global view
A lexical form is generally described in a lexicon
with one or more entries including different kinds
of information: the POS (part of speech), morpho-
logic features, syntactic features and sometimes
semantic features.
A form will cause a parsing failure if its descrip-
tion in the lexicon leads to a conflict with the gram-
mar expectations for this form, i.e., if grammar and
lexicon do not agree on a particular instance of the
form in a given sentence.
For practical reasons, we make a difference be-
tween conflicts concerning the POS, that we now
call POS defect, and conflicts concerning the fea-
tures, that we now call overspecification. POS de-
fect generally happen with homonyms, i.e., with
forms related to frequent lemmas while a seldom
used one is missing in the lexicon. Overspecifica-
tion is usually caused by the difficulty of describ-
ing exhaustively all the subcategorization frames
of a lemma (optional arguments, polysemy, etc.).
Therefore, if for a given lemma the most restrictive
frames are also the most frequent, some entries can
be overspecified and induce such conflicts.
We generate lexical corrections according to the
following process.
1. We first detect suspicious forms and associate
them with a set of non-parsable sentences in
which the form is suspected to be responsible
of the sentences? parsing failures.
2. We get as close as possible to the set of parses
that the grammar would have allowed with an
error-free lexicon. We achieve this goal by
underspecifying the suspicious form, i.e., we
increase the set of its possible POS (that is,
by virtually adding new entries to the lexi-
con) and/or underspecify the morphological
and syntactic informations of a given exist-
ing entry. A full underspecification can be
simulated in the following way: during the
parsing process, each time a lexical informa-
tion is checked about the suspicious form, the
lexicon is bypassed and all the constraints are
considered as satisfied. We actually achieved
this operation by replacing the suspicious
form in the associated sentences with special
forms called wildcards.
3. If the suspicious form has been correctly de-
tected, such underspecification increases the
parsing rate (except for sentences for which
the form was not the only problem). In the
newly successful parses, the form became
whatever the grammar wanted it to be, i.e., it
has matched any morphological, syntactic or
semantic pattern required. Those patterns are
the data we use to generate the corrections.
We thus extract the instances of the wildcard
in the newly producted parses, and after rank-
ing, we propose them as corrections.
4. Finally, we manually validate and apply the
corrections.
We will now explain with details how each step
is achieved, starting with the detection of suspi-
cious forms.
3 Detection of suspicious forms
In order to detect erroneous entries in a lexicon, we
have developed and implemented two techniques :
a shallow technique to identify POS defects and an
extended version of an existing parser-based tech-
nique to (mostly) identify overspecification. Both
provide for the form a suspicion rate and a set of
associated non-parsable sentences.
3.1 Tagger-based detection of POS defects
This technique is based on a stochastic tagger. The
underlying idea is to generate new POS for forms
in the input corpus by using an especially con-
figured stochastic tagger (Molinero et al, 2007).
Such a tagger considers every form belonging to
open POS (adjectives, common nouns, adverbs,
verbs and proper nouns) as unknown. Candidate
POS for unknown forms are then proposed by the
tagger?s guesser and the most likely to be correct
are selected by the tagging process itself. Thus,
new POS arise for some forms present in the in-
put.
To obtain such a tagger, we have used two train-
ing sets. One is a training corpus composed of
634
manually tagged sentences (330k words) extracted
from the French Paris 7 Treebank (Abeill?e, 2003),
and the other one is composed of a small list of
lemmas belonging to closed POS (prepositions,
determiners, pronouns and punctuation marks).
The tagger was modified so that only lemmas
present in the second set are considered as known.
After applying the tagger on the input corpus,
we extracted the produced pairs of form/POS and
checked their presence in the lexicon. Every non
present pair has been proposed as POS defect can-
didate. The emergence of false positives has been
smoothed by sorting the POS defect candidates ac-
cording to the following measure:
(n
wt
/n
w
) ? log(n
wt
),
where n
wt
is the number of occurrences of the
form w tagged as t and n
w
is the total number of
occurrences of the form w.
3.2 Parsing-based suspicious forms detection
The technique described hereafter extends the
ideas exposed in (Sagot and Villemonte de La
Clergerie, 2006), in which suspicious forms are de-
tected through a statistical analysis of the parsing
success and errors produced by a parser.
This error mining technique relies on the follow-
ing idea.
? When the parsing of a sentence known to
be lexically and grammatically correct fails,
there is no automatic and unquestionable way
to decide if this rejection is caused by an error
in the lexicon or by a flaw in another compo-
nent of the parsing system (grammar, etc.).
? Given the parsing results of a large corpus
of reliable sentences, the more often a lex-
ical form appears in non-parsable sentences
and not in parsable ones, the more likely it
is that its lexical entries are erroneous. This
suspicion is reinforced if it appears in non-
parsable sentences together with forms that
appear mostly in parsable ones.
The statistical computation establishes a rele-
vant list of lexical forms that are likely to be in-
correctly or incompletely described in the lexicon.
As such, the main drawback of this approach
is the dependence to the quality of the grammar
used. Indeed, if a specific form is naturally tied
with some syntactic construction non-handled by
the grammar, this form will always be found in
rejected sentences and will thus be unfairly sus-
pected. Nevertheless, we limited this drawback by
applying two extensions.
The first, already described in (Sagot and Ville-
monte de La Clergerie, 2006), mixes the detection
results obtained from various parsers with different
grammars, hence with different shortcomings.
The second extension detects short-range rep-
resentative syntactic patterns non-handled by the
grammar and filters the non-parsable sentences
where they appear. To do so, we reduce every sen-
tence to a single POS sequence through the use
of a tagger and train a maximum entropy clas-
sifier (Daum?e III, 2004) with the different possi-
ble trigrams and the corresponding parse failure
or success. Even if non-perfect (the tagger or the
maximum entropy classifier might be mistaken at
some point), this pre-filtering has proved to notice-
ably increase the quality of the suspicious forms
provided.
We will now explain how we manage to permit
the parsing process of the associated non-parsable
sentences in order to extract afterwise the correc-
tions hypotheses.
4 Parsing originally non-parsable
sentences with wildcards
As explained in Sect. 2, in order to generate lexical
corrections, we first need to get as close as possible
to the set of parses that the grammar would have al-
lowed with an error-free lexicon. We achieve this
goal by replacing in the associated sentences ev-
ery suspicious forms with special underspecified
forms called wildcards.
The simplest way would be to use totally un-
derspecified wildcards. Indeed, this would have
the benefit to cover all kinds of conflicts and thus,
it would notably increase the parsing coverage.
However, as observed by (Fouvry, 2003), it in-
troduces an unnecessary ambiguity which usually
leads to a severe overgeneration of parses or to no
parses at all because of time or memory shortage.
In a metaphorical way, we said that we wanted
the grammar to tell us what lexical information it
would have accepted for the suspicious form. Well,
by introducing a totally underspecified wildcard,
either the grammar has so many things to say that
it is hard to know what to listen to, or it has so
many things to think about that it stutters and does
not say anything at all.
Therefore, we refined the wildcard by introduc-
635
ing some data. For technical, linguistic and read-
ability reasons, we added POS information.
When facing a POS defect, we need the parser
to explore other grammar rules than those already
visited during the failed parses. We thus generate
wildcards with different POS than those already
present in the lexicon for the suspicious form.
When facing an overspecification, we need the
parser to explore the same grammar rules without
being stopped by unification failures. We thus gen-
erate wildcards with the same POS than those al-
ready present in the lexicon, but with no feature-
level constraints.
When suspicious forms were correctly detected,
such exchanges usually increases the parsing rate
of the associated sentences. Those parses place
the wildcards in grammatical contexts/patterns
which clearly express what lexical informations
the grammar would have accepted for the suspi-
cious forms.
We will now explain how we extract the cor-
rection hypotheses from the newly obtained parses
and how we rank them.
5 Extracting corrections hypotheses
The extraction directly depends on how one planes
to use the correction hypotheses. In a previous
work (Nicolas et al, 2007), we extracted the cor-
rections proposals in the parser?s output format.
Such a way to process had three important draw-
backs :
? one needed to understand the parser?s output
format before being able to study the correc-
tions;
? merging results produced by various parsers
was difficult, although it is an efficient solu-
tion to tackle most limitations of the process
(see Sect. 6.2);
? some parts of the correction proposals were
using representations that are not easy to re-
late with the format used by the lexicon (spe-
cific tagsets, under- or overspecified informa-
tion w.r.t. the lexicon, etc.).
We therefore developed for each of the parsers
used a conversion module in order to extract from
a given parse the instantiated lexical entry of each
wildcard in the format used by the lexicon.
6 Ranking corrections hypotheses
Natural languages are ambiguous, and so need to
be the grammars that model them. For example, in
many romance languages, an adjective can be used
as a noun and a noun as an adjective.
Consequently, an inadequate wildcard may per-
fectly lead to new parses and provide irrelevant
corrections. We thus separate the correction hy-
potheses according to their corresponding wild-
card before ranking them. Afterwards, the parsing
rate induced by each type of wildcard and the as-
sociated parsed sentences allows to easily identify
which wildcard is the correct one.
When only one parser is used to generate cor-
rection hypotheses, ranking correction hypotheses
proves straightforward, but, as we will explain, the
results heavily depend on the quality of the gram-
mar. We thus put together correction hypotheses
obtained thanks to different parsers in order to rank
them in a more sophisticated way.
6.1 Baseline ranking: single parser mode
The correction hypotheses obtained after introduc-
ing a wildcard are generally irrelevant, i.e., most
of them are parasitic hypotheses resulting from the
ambiguity brought by the wildcards. Nevertheless,
among all these hypotheses, some are valid, or at
least close to valid. In the scope of only one sen-
tence, there is no reliable way to determine which
corrections are the valid ones. But, if we consider
simultaneously various sentences that contain the
same suspicious form embedded in different syn-
tactic structures, we usually observe a strong vari-
ability of the noisy correction hypotheses. On the
opposite, if some stable correction hypothesis is
proposed for various sentences, it is likely to be
valid, i.e, to represent the correct sense of the form
according to the grammar. We thus simply rank
correction hypotheses according to the number of
sentences that have produced them.
6.2 Reducing grammar influence:
multi-parser mode
Using various parsers not only improves the suspi-
cious forms detection (Sect. 3.2), it also allows to
merge correction hypotheses in order to minimize
the influence of the shortcomings of the grammars.
When some form is naturally related to syntactic
constructions that are not correctly handled by the
grammar, this form is always found in rejected sen-
tences, and therefore is always suspected. Replac-
636
ing it by wildcards will only produce incorrect cor-
rections or no correction at all because the problem
is not related to the lexicon.
Having various sets of non-parsable sentences
for a given suspicious form f , and various sets of
correction hypotheses for f , one can discard (or
consider less relevant) correction hypotheses ac-
cording to the following three statements:
? If any form in a sentence is actually incor-
rectly described in the lexicon, then this sen-
tence should be non-parsable for both parsers.
Correction hypotheses produced from sen-
tences that are non-parsable for only one
parser should be discarded.
? For the same reason, correction hypotheses
produced with sentences in which only one
parser made f identified as a suspicious form
should be avoided.
? Finally, correction hypotheses proposed by
only one of both parsers (or proposed much
more often by one parser than by the other
one) might just be the consequence of the am-
biguity of one grammar. Afterall, both gram-
mar describe the same language, they should
find an agreement about the uses of a form.
In our experiments, we decided to apply the fol-
lowing ranking scheme: for a given suspicious
form, we only keep the corrections hypotheses
that are obtained from sentences that were orig-
inally non-parsable and parsable after a wildcard
introduction for both parsers. Afterwards, we sep-
arately rank the correction hypotheses for each
parser and merge the results.
We will now explain how we manually validate
the ranked correction hypotheses.
7 Manual validation of the corrections
When studying the ranked corrections for a given
wildcard, there might be three cases:
1. There are no corrections at all: the form was
unfairly suspected or the generation of wild-
cards was inadequate. It also happens when
the erroneous entries of the suspicious form
are not the only reasons for all the parsing
failures.
2. There are relevant corrections: the form was
correctly detected, the generation of wild-
cards was adequate and the form was the only
reason for various parsing failures.
3. There are irrelevant corrections: the ambi-
guity introduced by the relevant or irrelevant
wildcards opened the path to irrelevant parses
providing irrelevant corrections.
It is truly important to note that an incorrectly
suspected form may perfectly lead to irrelevant
corrections brought by the ambiguity introduced.
Consequently, unless the grammar used, the de-
tection of suspicious form and the generation of
wildcards are perfect, such a correcting process
should always be semi-automatic (manually vali-
dated) and not automatic.
Now that the whole system has been explained
with details, we will expose the similarities and
differences of our methods with previously pub-
licated ones.
8 Related works
Since efficient and linguistically relevant lexical
and grammatical formalisms have been developed,
the acquisition/extension/correction of linguistic
ressources has been an active research field, espe-
cially during the last decade.
The idea to infer lexical data from the grammat-
ical context first appeared in 1990 (Erbach, 1990).
The combination with error mining/detection tech-
nique, such as van Noord (2004), begun in 2006
(van de Cruys, 2006; Yi and Kordoni, 2006). Ex-
cept in our previous work (2007), nobody has com-
bined it with the technique described in Sagot and
Villemonte de La Clergerie (2006). The idea of
prefiltering the sentences (Sec. 3.2) to improve the
error mining performance has never been applied
so far.
The wildcards generation started to be refined
with Barg and Walther (1998). Since then,
the wildcards are partially underspecified and re-
strained to open class POS. In Yi and Kordoni
(2006), the authors use an elegant technique based
on an entropy classifier to select the most adequate
wildcards.
The way to rank the corrections is usually based
on a trained tool (van de Cruys, 2006; Yi and Kor-
doni, 2006), such as an entropy classifier. Surpris-
ingly, the evaluation of hypotheses on various sen-
tences for a same suspicious form in order to dis-
criminate the irrelevant ones has never been con-
sidered so far.
Finally, all the previous works were achieved
with HPSG parsers and no results has been ex-
posed until 2005. van de Cruys (2006) expose its
637
results for each POS and one can clearly observe,
for complex lemmas like verbs, the impossibility
to apply such set of techniques in an automatic way
without harming the quality of the lexicon. The re-
sults would be even worse if applied to corpus with
sentences non-covered by the grammar because no
relevant corrections could be generated but irrele-
vant ones might perfectly be.
9 Results
We now expose the results of our experiments by
describing the practical context, giving some cor-
rection examples and discussing the effectiveness
of the correction process through the parsing rate
increases we have obtained.
9.1 Practical context
The lexicon we are improving is called the Lefff.
2
This wide-coverage morphological and syntactic
French lexicon has been built partly automati-
cally (Sagot et al, 2006) and is under constant de-
velopment. At the time these lines are written, it
contains more than 520 000 entries. The less data
an entry has, the more specified it is.
We used two parsers based on two different
grammars in order to improve the quality of our
corrections.
? The FRMG (French Meta-Grammar) gram-
mar is generated in an hybrid TAG/TIG form
from a more abstract meta-grammar with
highly factorized trees (Thomasset and Ville-
monte de La Clergerie, 2005).
? The SXLFG-FR grammar (Boullier and
Sagot, 2006), is an efficient deep non-
probabilistic LFG grammar.
The corpus used is extracted from the French
politics newspaper Le monde diplomatique. This
corpora is composed with around 280 000 sen-
tences of 25 or less elements and 4,3 million of
words in total.
9.2 Examples of corrections
9.2.1 POS corrections
Most of the POS corrections performed were
about missing forms or about adjectives that could
be used as noun and vice versa. Here are some
examples :
2
Lexique des formes fl?echies du franc?ais/Lexicon of in-
flected forms of French.
? isra?elien (israeli) as an adjective,
? portugais (portuguese) as an adjective,
? politiques (politic) as a common noun,
? parabolique (parabolic) as an adjective,
? pittoresque (picturesque) as an adjective,
? minutieux (meticulous) as an adjective.
9.2.2 Features corrections
As one can expect, most of the features correc-
tions performed were about lemmas with complex
subcategorization frames / features, i.e., essentially
verbs.
? ?revenir? (to come back) did not handle con-
structions like to come back from or to come
back in
? ?se partager? (to share) did not handle con-
structions like to share something between.
? ?aimer? (to love) was described as always ex-
pecting a direct object and an attribute.
? ?livrer? (to deliver) did not handle construc-
tions like to deliver to somebody.
9.3 Correction process relevance
As explained earlier (Sect. 7), this process might
generate erroneous corrections, especially if gen-
eral corpora with sentences non-covered by the
grammar are used and various correction sessions
are made. Globally, the accuracy of the correc-
tions goes decreasing after each session. Indeed,
there are less and less lexical mistakes to correct
after each session. Anyway, we are more inter-
ested in improving efficiently our lexicon. We thus
prove the relevance of the whole process by show-
ing the gains of parsing rate obtained during our
experiments. One must keep in mind that the cor-
rections are manually validated, i.e, the noticeable
increases of parsing coverage (Figure 1) are mostly
due to the improvement of the quality of the lexi-
con.
Table 1 lists the number of lexical forms updated
at each session.
Except for the second session, all correction
sessions have been achieved with the error mining
and the hypothesis generation modules. The sec-
ond session has been achieved with the POS defect
mining module only (Sect. 3.1). We planned to
638
 150000
 151000
 152000
 153000
 154000
 155000
 156000
 157000
 158000
0 1 2 3
Su
cce
ssf
ul p
ars
es
Session number
FrmgSxlfg
Figure 1: Number of sentences successfully parsed
after each session.
Session 1 2 3 total
nc 30 99 1 130
adj 66 694 27 787
verbs 1183 0 385 1568
adv 1 7 0 8
total 1280 800 413 2493
Table 1: Lexical forms updated at each session
interface it with the hypothesis generation module
but we could not finish it on time. Nevertheless,
the suspicious form list provided was good and
simple enough (mostly proper nouns, adjectives
and common nouns) to be reviewed without the
help of the hypothesis generation module.
As expected, we were quickly limited by the
quality of the grammars and by the corpus used.
Indeed, the lexicon and the grammars have been
developed together for the last few years, using
this same corpus for test. Thus, the error mining
technique came, after few corrections sessions, to
provide us irrelevant suspicious forms. The tagger-
based detection of POS defects can only be used
once on each corpus. Further correction and ex-
tension sessions make sense only after grammar
improvements or obtention of new corpora.
Nevertheless, we have already detected and cor-
rected 254 lemmas corresponding to 2493 forms.
The coverage rate (percentage of sentences for
which a full parse is found) has undergone an ab-
solute increase of 3,41% (5141 sentences) for the
FRMG parser and 1,73% (2677 sentences) for the
SXLFG parser. Thoses results were achieved in
only few hours of manual work on the lexicon !
9.4 Discussion
This set of techniques has two major qualities.
The first one, as one can observe through our
results, it allows to improve significantly a lexicon
in a short amount of time.
The second one is related to the main drawback
of our approach: the dependence to the grammars
used. If in a non-parsable sentence, none of the
suspicious forms is a true culprit (there are no rel-
evant correction), then this sentence can be consid-
ered as lexically correct w.r.t. the current state of
the grammar. It thus exhibits shortcomings of the
grammar and can be used to improve it.
A cycling process which alternatively and incre-
mentally improves both the lexicon and the gram-
mar can then be elaborated. This data is even more
important considering the fact that nowadays, large
scale French TreeBank are rare.
10 Future developments
The whole system has globally proved to be ma-
ture. Nevertheless, we are planning the following
improvements to continue our investigation.
? We need to interface the POS defect mining
module with the hypothesis generation one.
? The tagger-based detection of POS defects is
still young and can be improved.
? We will refine the wildcard generation in
a similar way as done in (Yi and Kordoni,
2006).
? In order to pursue the corrections of the lexi-
con, we will improve our grammars accord-
ing to the corpus of failed sentences. It is
now globally representative of shortcomings
of the grammars, thus we are thinking about
developing some detection techniques in or-
der to emphasize cases of error for the gram-
mar. The entropy model built by the maxi-
mum entropy classifier should be a good start-
ing point.
11 Conclusion
The path taken, highlighted by the dependence on
the grammar, seems to be a promising one. It will
allow to develop a cycling process which alterna-
tively and incrementally improves both the lexicon
and the grammar.
639
The correction process of lexicon presented here
is now globally mature and has proved to be rele-
vant and effective in practice. Indeed, noticeable
improvements of the lexicon could be achieved in
a few amount of manual work.
The time spend to validate the corrections gener-
ated has also confirmed our doubts about evolving
such process to an automatic one.
We will definitively continue the correction ses-
sions after upgrading some components.
Acknowledgement We thank the COLE team
(University of Vigo, Spain) for granting us access
to their computers.
We would like to thanks Sylvain Schmitz and
the reviewers for their valuable comments.
The POS mining technique could be achieved
partially thanks to the support of Ministerio de
Educaci?on y Ciencia of Spain (HUM2007-66607-
C04-02) and the Xunta de Galicia (?Galician Net-
work for Language Processing and Information
Retrieval? 2006-2009).
References
Abeill?e, Anne. 2003. Annotation
morpho-syntaxique. Paper available at
http://www.llf.cnrs.fr/Gens/Abeille/guide-morpho-
synt.02.pdf, January.
Barg, Petra and Markus Walther. 1998. Processing un-
konwn words in hpsg. In Proceedings of the 36th
Conference of the ACL and the 17th International
Conference on Computational Linguistics.
Boullier, Pierre and Beno??t Sagot. 2006. Efficient pars-
ing of large corpora with a deep LFG parser. In Pro-
ceedings of LREC?06.
Daum?e III, Hal. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Erbach, Gregor. 1990. Syntactic processing of un-
known words. In IWBS Report 131.
Fouvry, Frederik. 2003. Lexicon acquisition with a
large coverage unification-based grammar. In Com-
panion to the 10th of EACL.
Molinero, Miguel A., Fco. Mario Barcala, Juan Otero,
and Jorge Gra?na. 2007. Practical application of one-
pass viterbi algorithm in tokenization and pos tag-
ging. Recent Advances in Natural Language Pro-
cessing (RANLP). Proceedings, pp. 35-40.
Nicolas, Lionel, Jacques Farr?e, and
?
Eric Villemonte de
La Clergerie. 2007. Correction mining in parsing
results. In Proceedings of LTC?07.
Sagot, Beno??t and
?
Eric Villemonte de La Clergerie.
2006. Error mining in parsing results. In Proceed-
ings of ACL/COLING?06, pages 329?336. Associa-
tion for Computational Linguistics.
Sagot, Beno??t, Lionel Cl?ement,
?
Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for french: architecture, acquisi-
tion, use. In Proceedings of LREC?06.
Thomasset, Franc?ois and
?
Eric Villemonte de La Clerg-
erie. 2005. Comment obtenir plus des m?eta-
grammaires. In Proceedings of TALN?05.
van de Cruys, Tim. 2006. Automatically extending the
lexicon for parsing. In Proceedings of the eleventh
ESSLLI student session.
van Noord, Gertjan. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
ACL 2004.
Yi, Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of LREC-2006.
640
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117?128,
Paris, October 2009. c?2009 Association for Computational Linguistics
Constructing parse forests that include exactly the n-best PCFG trees
Pierre Boullier1, Alexis Nasr2 and Beno??t Sagot1
1. Alpage, INRIA Paris-Rocquencourt & Universite? Paris 7
Domaine de Voluceau ? Rocquencourt, BP 105 ? 78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
2. LIF, Univ. de la Me?diterranne?e
163, avenue de Luminy - Case 901 ? 13288 Marseille Cedex 9, France
Alexis.Nasr@lif.univ-mrs.fr
Abstract
This paper describes and compares two al-
gorithms that take as input a shared PCFG
parse forest and produce shared forests
that contain exactly the n most likely trees
of the initial forest. Such forests are
suitable for subsequent processing, such
as (some types of) reranking or LFG f-
structure computation, that can be per-
formed ontop of a shared forest, but that
may have a high (e.g., exponential) com-
plexity w.r.t. the number of trees contained
in the forest. We evaluate the perfor-
mances of both algorithms on real-scale
NLP forests generated with a PCFG ex-
tracted from the Penn Treebank.
1 Introduction
The output of a CFG parser based on dynamic
programming, such as an Earley parser (Earley,
1970), is a compact representation of all syntac-
tic parses of the parsed sentence, called a shared
parse forest (Lang, 1974; Lang, 1994). It can rep-
resent an exponential number of parses (with re-
spect to the length of the sentence) in a cubic size
structure. This forest can be used for further pro-
cessing, as reranking (Huang, 2008) or machine
translation (Mi et al, 2008).
When a CFG is associated with probabilistic in-
formation, as in a Probabilistic CFG (PCFG), it
can be interesting to process only the n most likely
trees of the forest. Standard state-of-the-art algo-
rithms that extract the n best parses (Huang and
Chiang, 2005) produce a collection of trees, los-
ing the factorization that has been achieved by the
parser, and reproduce some identical sub-trees in
several parses.
This situation is not satisfactory since post-
parsing processes, such as reranking algorithms
or attribute computation, cannot take advantage
of this lost factorization and may reproduce some
identical work on common sub-trees, with a com-
putational cost that can be exponentally high.
One way to solve the problem is to prune the
forest by eliminating sub-forests that do not con-
tribute to any of the n most likely trees. But this
over-generates: the pruned forest contains more
than the n most likely trees. This is particularly
costly for post-parsing processes that may require
in the worst cases an exponential execution time
w.r.t. the number of trees in the forest, such as
LFG f-structures construction or some advanced
reranking techniques. The experiments detailed
in the last part of this paper show that the over-
generation factor of pruned sub-forest is more or
less constant (see 6): after pruning the forest so as
to keep the n best trees, the resulting forest con-
tains approximately 103n trees. At least for some
post-parsing processes, this overhead is highly
problematic. For example, although LFG parsing
can be achieved by computing LFG f-structures
on top of a c-structure parse forest with a reason-
able efficiency (Boullier and Sagot, 2005), it is
clear that a 103 factor drastically affects the overall
speed of the LFG parser.
Therefore, simply pruning the forest is not an
adequate solution. However, it will prove useful
for comparison purposes.
The new direction that we explore in this pa-
per is the production of shared forests that con-
tain exactly the n most likely trees, avoiding both
the explicit construction of n different trees and
the over-generation of pruning techniques. This
can be seen as a transduction which is applied on
a forest and produces another forest. The trans-
duction applies some local transformations on the
structure of the forest, developing some parts of
the forest when necessary.
The structure of this paper is the following. Sec-
tion 2 defines the basic objects we will be dealing
with. Section 3 describes how to prune a shared
117
forest, and introduces two approaches for build-
ing shared forests that contain exactly the n most
likely parses. Section 4 describes experiments that
were carried out on the Penn Treebank and sec-
tion 5 concludes the paper.
2 Preliminaries
2.1 Instantiated grammars
Let G = ?N ,T ,P, S? be a context-free grammar
(CFG), defined in the usual way (Aho and Ullman,
1972). Throughout this paper, we suppose that we
manipulate only non-cyclic CFGs,1 but they may
(and usually do) include ?-productions. Given a
production p ? P, we note lhs(p) its left-hand
side, rhs(p) its right-hand side and |p| the length
of rhs(p). Moreover, we note rhsk(p), with 1 ?
k ? |p|, the kth symbol of rhs(p). We call A-
production any production p ? P of G such that
lhs(p) = A.
A complete derivation of a sentence w =
t1 . . . t|w| (?i ? |w|, ti ? T ) w.r.t. G is of the form
S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w. By def-
inition, A ? X1X2 . . . Xr is a production of G.
Each of A, X1, X2, . . . , Xr spans a unique oc-
currence of a substring ti+1 . . . tj of w, that can
be identified by the corresponding range, noted
i..j. A complete derivation represents a parse tree
whose yield is w, in which each symbol X of
range i..j roots a subtree whose yield is ti+1 . . . tj
(i.e., a derivation of the form X ??
G,w
ti+1 . . . tj).
Let us define the w-instantiation operation (or
instantiation). It can be applied to symbols and
productions of G, and to G itself, w.r.t. a string
w. It corresponds to the well-known intersection
of G with the linear automaton that corresponds
to the string w. We shall go into further detail for
terminology, notation and illustration purposes.
An instantiated non terminal symbol is a triple
noted Ai..j where A ? N and 0 ? i ? j ? |w|.
Similarly, an instantiated terminal symbol is a
triple noted Ti..j where T ? T and 0 ? i ? j =
i + 1 ? |w|. An instantiated symbol, terminal or
non terminal, is noted Xi..j . For any instantiated
symbol Xi..j , i (resp. j) is called its lower bound
1Actually, cyclic CFG can be treated as well, but not
cyclic parse forests. Therefore, if using a cyclic CFG which,
on a particular sentence, builds a cyclic parse forest, cycles
have to be removed before the algorithms descibed in the next
sections are applied. This is the case in the SYNTAX system
(see below).
(resp. upper bound), and can be extracted by the
operator lb() (resp. ub()).
An instantiated production (or instantiated
rule) is a context-free production Ai..j ?
X1i1..j1X2i2..j2 . . . Xrir ..jr whose left-hand side is an
instantiated non terminal symbol and whose right-
hand side is a (possibly empty) sequence of in-
stantiated (terminal or non terminal) symbols, pro-
vided the followings conditions hold:
1. the indexes involved are such that i = i1, j =
jr , and ?l such that 1 ? l < r, jl = il+1;
2. the corresponding non-instantiated produc-
tion A ? X1X2 . . . Xr is a production of
G.
If lhs(p) = Ai..j , we set lb(p) = i and ub(p) = j.
In a complete derivation S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w, any symbol X that spans
the range i..j can be replaced by the instantiated
symbols Xi..j . For example, the axiom S can be
replaced by the instantiated axiom S0..|w| in the
head of the derivation. If applied to the whole
derivation, this operation creates an instantiated
derivation, whose rewriting operations define a
particular set of instantiated productions. Given
G and w, the set of all instantiated productions in-
volved in at least one complete derivation of w is
unique, and noted Pw. An instantiated derivation
represents an instantiated parse tree, i.e., a parse
tree whose node labels are instantiated symbols.
In an instantiated parse tree, each node label is
unique, and therefore we shall not distinguish be-
tween a node in an instantiated parse tree and its
label (i.e., an instantiated symbol).
Then, the w-instantiated grammar Gw for G
and w is a CFG ?Nw,Tw,Pw, S0..|w|? such that:
1. Pw is defined as explained above;
2. Nw is a set of instantiated non terminal sym-
bols;
3. Tw is a set of instantiated terminal symbols.
It follows from the definition of Pw that (instan-
tiated) symbols of Gw have the following prop-
erties: Ai..j ? Nw ? A ??G,w ti+1 . . . tj , and
Ti..j ? Tw ? T = tj .
The w-instantiated CFG Gw represents all parse
trees for w in a shared (factorized) way. It is the
grammar representation of the parse forest of w
118
w.r.t. G.2 In fact, L(Gw) = {w} and the set
of parses of w with respect to Gw is isomorphic
to the set of parses of w with respect to G, the
isomorphism being the w-instantiation operation.
The size of a forest is defined as the size of the
grammar that represents it, i.e., as the number of
symbol occurrences in this grammar, which is de-
fined as the number of productions plus the sum of
the lengths of all right-hand sides.
Example 1: First running example.
Let us illustrate these definitions by an example.
Given the sentence w = the boy saw a man with a
telescope and the grammar G (that the reader has
in mind), the instantiated productions of Gw are:
Det0..1 ? the0..1 N1..2 ? boy1..2
NP0..2 ? Det0..1 N1..2 V2..3 ? saw2..3
Det3..4 ? a3..4 N4..5 ? man4..5
NP3..5 ? Det3..4 N4..5 Prep5..6 ? with5..6
Det6..7 ? a6..7 N7..8 ? telescope7..8
NP6..8 ? Det6..7 N7..8 PP5..8 ? Prep5..6 NP6..8
NP3..8 ? NP3..5 PP5..8 VP2..8 ? V2..3 NP3..8
VP2..5 ? V2..3 NP3..5 VP2..8 ? VP2..5 PP5..8
S0..8 ? NP0..2 VP2..8
They represent the parse forest of w according to
G. This parse forest contains two trees, since there
is one ambiguity: VP2..8 can be rewritten in two
different ways.
The instantiated grammar Gw can be repre-
sented as an hypergraph (as in (Klein and Man-
ning, 2001) or (Huang and Chiang, 2005)) where
the instantiated symbols of Gw correspond to the
vertices of the hypergraph and the instantiated pro-
ductions to the hyperarcs.
We define the extension of an instantiated sym-
bol Xi..j , noted E(Xi..j), as the set of instantiated
parse trees that have Xi..j as a root. The set of all
parse trees of w w.r.t. G is therefore E(S0..|w|). In
the same way, we define the extension of an in-
stantiated production Xi..j ? ? to be the subset
of E(Xi..j) that corresponds to derivations of the
form Xi..j ?G,w ?
??
G,w
ti+1 . . . tj (i.e., trees rooted
in Xi..j and where the daughters of the node Xi..j
are the symbols of ?).
2.2 Forest traversals
Let us suppose that we deal with non-cyclic
forests, i.e., we only consider forests that are rep-
2In particular, if G is a binary grammar, its w-instantation
(i.e., the parse forest of w) has a size O(|w|3), whereas it rep-
resents a potentially exponential number of parse trees w.r.t
|w| since we manipulate only non-cyclic grammars.
resented by a non-recursive instantiated CFG. In
this case, we can define two different kinds of for-
est traversals.
A bottom-up traversal of a forest is a traversal
with the following constraint: an Ai..j-production
is visited if and only if all its instantiated right-
hand side symbols have already been visited; the
instantiated symbol Ai..j is visited once all Ai..j-
productions have been visited. The bottom-up
visit starts by visiting all instantiated productions
with right-hand sides that are empty or contain
only (instantiated) terminal symbols.
A top-down traversal of a forest is a traversal
with the following constraint: a node Ai..j is vis-
ited if and only if all the instantiated productions
in which it occurs in right-hand side have already
been visited; once an instantiated production Ai..j
has been visited, all its Ai..j-productions are vis-
ited as well. Of course the top-down visit starts by
the visit of the axiom S0..|w|.
2.3 Ranked instantiated grammar
When an instantiated grammar Gw =
?Nw,Tw,Pw, S0..|w|? is built on a PCFG, ev-
ery parse tree in E(S0..|w|) has a probability that
is computed in the usual way (Booth, 1969). We
might be interested in extracting the kth most
likely tree of the forest represented by Gw,3 with-
out unfolding the forest, i.e., without enumerating
trees. In order to do so, we need to add some
extra structure to the instantiated grammar. The
augmented instantiated grammar will be called a
ranked instantiated grammar.
This extra structure takes the form of n-best ta-
bles that are associated with each instantiated non
terminal symbol (Huang and Chiang, 2005), thus
leading to ranked instantiated non terminal sym-
bols, or simply instantiated symbols when the con-
text is non ambiguous. A ranked instantiated non
terminal symbol is written ?Ai..j,T (Ai..j)?, where
T (Ai..j) is the n-best table associated with the in-
stantiated symbol Ai..j .
T (Ai..j) is a table of at most n entries. The
k-th entry of the table, noted e, describes how to
build the k-th most likely tree of E(Ai..j). This
tree will be called the k-th extention of Ai..j , noted
Ek(Ai..j). More precisely, e indicates the instanti-
ated Ai..j-production p such that Ek(Ai..j) ? E(p).
It indicates furthermore which trees of the exten-
3In this paper, we shall use the kth most likely tree and the
tree of rank k as synonyms.
119
sions of p?s right-hand side symbols must be com-
bined together in order to build Ek(Ai..j).
We also define the m,n-extension of Ai..j as
follows: Em,n(Ai..j) = ?m?k?nEk(Ai..j).
Example 2: n-best tables for the first running
example.
Let us illustrate this idea on our first running ex-
ample. Recall that in Example 1, the symbol VP2..8
can be rewritten using the two following produc-
tions :
VP2..8 ? V2..3 NP3..8
VP2..8 ? VP2..5 PP5..8
T (VP2..8) has the following form:
1 P1 VP2..8 ? V2..3 NP3..8 ?1, 1? 1
2 P2 VP2..8 ? VP2..5 PP5..8 ?1, 1? 1
This table indicates that the most likely tree
associated with VP2..8 (line one) has probability
P1 and is built using the production VP2..8 ?
V2..3 NP3..8 by combining the most likely tree of
E(V2..3) (indicated by the first 1 in ?1, 1?) with the
most likely tree of E(NP3..8) (indicated by the sec-
ond 1 in ?1, 1?). It also indicates that the most
likely tree of E(VP2..8) is the most likely tree of
E(VP2..8 ? V2..3 NP3..8) (indicated by the pres-
ence of 1 in the last column of entry 1) and the
second most likely tree of E(VP2..8) is the most
likely tree of E(VP2..8 ? VP2..5 PP5..8). This last
integer is called the local rank of the entry.
More formally, the entry T (Ai..j)[k] is defined
as a 4-tuple ?Pk, pk, ~vk, lk? where k is the rank
of the entry, Pk is the probability of the tree
Ek(Ai..j), pk is the instantiated production such
that Ek(Ai..j) ? E(pk), ~vk is a tuple of |rhs(pk)|
integers and lk is the local rank.
The tree Ek(Ai..j) is rooted by Ai..j , and its
daughters root N = |rhs(pk)| subtrees that are
E ~vk[1](rhs1(pk)), . . . , E ~vk [N ](rhsN (pk)).
Given an instantiated symbol Ai..j and an in-
stantitated production p ? P (Ai..j), we define
the n-best table of p to be the table composed
of the entries ?Pk, pk, ~vk, lk? of T (Ai..j) such that
pk = p.
Example 3: Second running example.
The following is a standard PCFG (probabili-
ties are shown next to the corresponding clauses).
S ? A B 1
A ? A1 0.7 A1 ? a 1
A ? A2 0.3 A2 ? a 1
B ? B1 0.6 B1 ? b 1
B ? B2 0.4 B2 ? b 1
The instantiation of the underlying (non-
probabilistic) CFG grammar by the input text
w = a b is the following.
S1..3 ? A1..2 B2..3
A1..2 ? A11..2 A11..2 ? a1..2
A1..2 ? A21..2 A21..2 ? a1..2
B2..3 ? B12..3 B12..3 ? b2..3
B2..3 ? B22..3 B22..3 ? b2..3
This grammar represents a parse forest that con-
tains four different trees, since on the one hand one
can reach (parse) the instantiated terminal symbol
a1..2 through A1 or A2, and on the other hand one
can reach (parse) the instantiated terminal sym-
bol b1..2 through B1 or B2. Therefore, when dis-
cussing this example in the remainder of the paper,
each of these four trees will be named accordingly:
the tree obtained by reaching a through Ai and b
through Bj (i and j are 1 or 2) shall be called
Ti,j .
The corresponding n-best tables are trivial
(only one line) for all instantiated symbols but
A1..2, B2..3 and S1..3. That of A1..2 is the follow-
ing 2-line table.
1 0.7 A ? A1 ?1? 1
2 0.3 A ? A2 ?1? 1
The n-best table for B2..3 is similar. The n-best
table for S1..3 is:
1 0.42 S1..3 ? A1..2 B2..3 ?1, 1? 1
2 0.28 S1..3 ? A1..2 B2..3 ?1, 2? 2
3 0.18 S1..3 ? A1..2 B2..3 ?2, 1? 3
4 0.12 S1..3 ? A1..2 B2..3 ?2, 2? 4
Thanks to the algorithm sketched in section 2.4,
these tables allow to compute the following obvi-
ous result: the best tree is T1,1, the second-best
tree is T1,2, the third-best tree is T2,1 and the worst
tree is T2,2.
If n = 3, the pruned forest over-generates: all
instantiated productions take part in at least one
of the three best trees, and therefore the pruned
forest is the full forest itself, which contains four
trees.
We shall use this example later on so as to il-
lustrate both methods we introduce for building
forests that contain exactly the n best trees, with-
out overgenerating.
2.4 Extracting the kth-best tree
An efficient algorithm for the extraction of the n-
best trees is introduced in (Huang and Chiang,
2005), namely the authors? algorithm 3, which
120
is a re-formulation of a procedure originally pro-
posed by (Jime?nez and Marzal, 2000). Contrar-
ily to (Huang and Chiang, 2005), we shall sketch
this algorithm with the terminology introduced
above (whereas the authors use the notion of hy-
pergraph). The algorithm relies on the n-best ta-
bles described above: extracting the kth-best tree
consists in extending the n-best tables as much as
necessary by computing all lines in each n-best ta-
ble up to those that concern the kth-best tree.4
The algorithm can be divided in two sub-
algorithms: (1) a bottom-up traversal of the for-
est for extracting the best tree; (2) a top-down
traversal for extracting the kth-best tree provided
the (k ? 1)th-best has been already extracted.
The extraction of the best tree can be seen as a
bottom-up traversal that initializes the n-best ta-
bles: when visiting a node Ai..j , the best probabil-
ity of each Ai..j-production is computed by using
the tables associated with each of their right-hand
side symbols. The best of these probabilities gives
the first line of the n-best table for Ai..j (the result
for other productions are stored for possible later
use). Once the traversal is completed (the instanti-
ated axiom has been reached), the best tree can be
easily output by following recursively where the
first line of the axiom?s n-best table leads to.
Let us now assume we have extracted all k?-best
trees, 1 ? k? < k, for a given k ? n. We want
to extract the kth-best tree. We achieve this recur-
sively by a top-down traversal of the forest. In or-
der to start the construction of the kth-best tree, we
need to know the following:
? which instantiated production p must be used
for rewriting the instantiated axiom,
? for each of p?s right-hand side symbols Ai..j ,
which subtree rooted in Ai..j must be used;
this subtree is identified by its local rank
kAi..j , i.e., the rank of its probability among
all subtrees rooted in Ai..j.
This information is given by the kth line of the n-
best table associated with the instantiated axiom.
If this kth line has not been filled yet, it is com-
puted recursively.5 Once the kth line of the n-best
4In the remainder of this paper, we shall use ?extracting
the kth-best tree? as a shortcut for ?extending the n-best ta-
bles up to what is necessary to extract the kth-best tree? (i.e.,
we do not necessarily really build or print the kth-best tree).
5Because the k ? 1th-best tree has been computed, this n-
best table is filled exactly up to line k?1. The kth line is then
table is known, i.e., p and all kAi..j ?s are known,
the rank k is added to p?s so-called rankset, noted
?(p). Then, the top-down traversal extracts recur-
sively for each Ai..j the appropriate subtree as de-
fined by kAi..j . After having extracted the n-th
best tree, we know that a given production p is in-
cluded in the kth-best tree, 1 ? k ? n, if and only
if k ? ?(p).
3 Computing sub-forests that only
contain the n best trees
Given a ranked instantiated grammar Gw, we are
interested in building a new instantiated grammar
which contains exactly the n most likely trees of
E(Gw). In this section, we introduce two algo-
rithms that compute such a grammar (or forest).
Both methods rely on the construction of new
symbols, obtained by decorating instantiated sym-
bols of Gw.
An empirical comparison of the two methods is
described in section 4. In order to evaluate the
size of the new constructed grammars (forests),
we consider as a lower bound the so-called pruned
forest, which is the smallest sub-grammar of the
initial instantiated grammar that includes the n
best trees. It is built simply by pruning produc-
tions with an empty rankset: no new symbols
are created, original instantiated symbols are kept.
Therefore, it is a lower bound in terms of size.
However, the pruned forest usually overgenerates,
as illustrated by Example 3.
3.1 The ranksets method
The algorithm described in this section builds an
instantiated grammar Gnw by decorating the sym-
bols of Gw. The new (decorated) symbols have
the form A?i..j where ? is a set of integers called
a rankset. An integer r is a rank iff we have
1 ? r ? n.
The starting point of this algorithm is set of n-
best tables, built as explained in section 2.4, with-
out explicitely unfolding the forest.
computed as follows: while constructing the k?th-best trees
for each k? between 1 and k?1, we have identified many pos-
sible rewritings of the instantiated axiom, i.e., many (produc-
tion, right-hand side local ranks) pairs; we know the proba-
bility of all these rewritings, although only some of them con-
situte a line of the instantiated axiom?s n-best table; we now
identify new rewritings, starting from known rewritings and
incrementing only one of their local ranks; we compute (re-
cursively) the probability of these newly identified rewritings;
the rewriting that has the best probability among all those that
are not yet a line of the n-best table is then added: it is its kth
line.
121
A preliminary top-down step uses these n-best
tables for building a parse forest whose non-
terminal symbols (apart from the axiom) have the
form A?i..j where ? is a singleton {r}: the sub-
forest rooted in A{r}i..j contains only one tree, that
of local rank r. Only the axiom is not decorated,
and remains unique. Terminal symbols are not af-
fected either.
At this point, the purpose of the algorithm is to
merge productions with identical right-hand sides,
whenever possible. This is achieved in a bottom-
up fashion as follows. Consider two symbols A?1i..j
and A?2i..j , which differ only by their underlying
ranksets. These symbols correspond to two dif-
ferent production sets, namely the set of all A?1i..j-
productions (resp. A?2i..j-productions). Each of
these production sets define a set of right-hand
sides. If these two right-hand side sets are iden-
tical we say that A?1i..j and A
?2
i..j are equivalent. In
that case introduce the rankset ? = ?1 ? ?2 and
create a new non-terminal symbol A?i..j . We now
simply replace all occurrences of A?1i..j and A
?2
i..j
in left- and right-hand sides by A?i..j . Of course
(newly) identical productions are erased. After
such a transformation, the newly created symbol
may appear in the right-hand side of productions
that now only differ by their left-hand sides; the
factorization spreads to this symbol in a bottom-
up way. Therefore, we perform this transforma-
tion until no new pair of equivalent symbols is
found, starting from terminal leaves and percolat-
ing bottom-up as far as possible.
Example 4: Applying the ranksets method to
the second running example.
Let us come back to the grammar of Example 3,
and the same input text w = a b as before. As
in Example 3, we consider the case when we are
interested in the n = 3 best trees.
Starting from the instantiated grammar and the
n-best tables given in Example 3, the preliminary
top-down step builds the following forest (for clar-
ity, ranksets have not been shown on symbols that
root sub-forests containing only one tree):
S1..3 ? A{1}1..2 B
{1}
2..3
S1..3 ? A{1}1..2 B
{2}
2..3
S1..3 ? A{2}1..2 B
{1}
2..3
A{1}1..2 ? A11..2 A11..2 ? a1..2
A{2}1..2 ? A21..2 A21..2 ? a1..2
B{1}2..3 ? B12..3 B12..3 ? b2..3
B{2}2..3 ? B22..3 B22..3 ? b2..3
In this example, the bottom-up step doesn?t fac-
torize out any other symbols, and this is therefore
the final output of the ranksets method. It con-
tains 2 more productions and 3 more symbols than
the pruned forest (which is the same as the origi-
nal forest), but it contains exactly the 3 best trees,
contrarily to the pruned forest.
3.2 The rectangles method
In this section only, we assume that the grammar
G is binary (and therefore the forest, i.e., the gram-
mar Gw, is binary). Standard binarization algo-
rithms can be found in the litterature (Aho and Ull-
man, 1972).
The algorithm described in this section per-
forms, as the preceding one, a decoration of the
symbols of Gw. The new (decorated) symbols
have the form Ax,yi..j , where x and y denote ranks
such that 1 ? x ? y ? n. The semantics of the
decoration is closely related to the x, y extention
of Ai..j , introduced in 2.3:
E(Ax,yi..j) = Ex,y(Ai..j)
It corresponds to ranksets (in the sense of the
previous section) that are intervals: Ax,yi..j is equiv-
alent to the previous section?s A{x,x+1,...,y?1,y}i..j . In
other words, the sub-forest rooted with Ax,yi..j con-
tains exactly the trees of the initial forest, rooted
with Ai..j , which rank range from x to y.
The algorithm performs a top-down traversal of
the initial instantiated grammar Gw. This traver-
sal also takes as input two parameters x and y. It
starts with the symbol S0..|w| and parameters 1 and
n. At the end of the traversal, a new decorated for-
est is built which contains exactly n most likely
the parses. During the traversal, every instantiated
symbol Ai..j will give birth to decorated instanti-
ated symbols of the form Ax,yi..j where x and y are
determined during the traversal. Two different ac-
tions are performed depending on whether we are
122
visiting an instantiated symbol or an instantiated
production.
3.2.1 Visiting an instantiated symbol
When visiting an instantiated symbol Ai..j with
parameters x and y, a new decorated instan-
tiated symbol Ax,yi,j is created and the traver-
sal continues on the instantiated productions of
P (Ai..j) with parameters that have to be com-
puted. These parameters depend on how the el-
ements of Ex,y(Ai..j) are ?distributed? among the
sets E(p) with p ? P (Ai..j). In other words, we
need to determine xk?s and yk?s such that:
Ex,y(Ai..j) =
?
pk?P (Ai..j)
Exk,yk(pk)
The idea can be easily illustrated on an exam-
ple. Suppose we are visiting the instantiated sym-
bol Ai..j with parameters 5 and 10. Suppose also
that Ai..j can be rewritten using the two instanti-
ated productions p1 and p2. Suppose finally that
the 5 to 10 entries of T (Ai..j) are as follows6:
5 p1 4
6 p2 2
7 p2 3
8 p1 5
9 p2 4
10 p1 6
This table says that E5(Ai..j) = E4(p1) i.e. the
5th most likely analysis of E(Ai..j) is the 4th most
likely analysis of E(p1) and E6(Ai..j) = E2(p2)
and so on. From this table we can deduce that:
E5,10(Ai..j) = E4,6(p1) ? E2,4(p2)
The traversal therefore continues on p1 and p2
with parameters 4, 6 and 2, 4.
3.2.2 Visiting an instantiated production
When visiting an instantiated production p of the
form Ai..j ? Bi..l Cl..j with parameters x and y,
a collection of q instantiated productions pr of the
form Ax,yi..j ? B
x1r,x2r
i..l C
y1r ,y2r
l..j , with 1 ? r ? q,
are built, where the parameters x1r, x2r , y1r , y2r and
q have to be computed.
Once the parameters q and x1r, x2r , y1r , y2r with
1 ? r ? q, have been computed, the traversal
continues independently on Bi..l with parameters
x1r and x2r and on Cl..j with parameters y1r and y2r .
6Only the relevant part of the table have been kept in the
figure.
The computation of the parameters x1r, x2r , y1r
and y2r for 1 ? r ? q, is the most complex part of
the algorithm, it relies on the three notions of rect-
angles, q-partitions and n-best matrices, which are
defined below.
Given a 4-tuple of parameters x1r , x2r, y1r , y2r ,
a rectangle is simply a pairing of the form
??x1r , x2r?, ?y1r , y2r ??. A rectangle can be interpreted
as a couple of rank ranges : ?x1r , y1r ? and ?x2r , y2r?.
It denotes the cartesian product
[
x1r, x2r
]?[y1r , y2r
]
.
Let ??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q ??
be a collection of q rectangles. It will be called a
q-partition of the instantiated production p iff the
following is true:
Ex,y(p) =
?
1?r?q
E(Ax,yi..j ? Bx
1
r,x2r
i..l C
y1r ,y2r
l..j )
To put it differently, this definition means that
??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q?? is a q
partition of p if any tree of E(Bx1r,x2ri..l ) combined
with any tree of E(Cy1r ,y2rl..j ) is a tree of Ex,y(p) and,
conversely, any tree of Ex,y(p) is the combination
of a tree of E(Bx1r,x2ri..l ) and a tree of E(Cy
1
r ,y2r
l..j ).
The n-best matrix associated with an instanti-
ated production p, introduced in (Huang and Chi-
ang, 2005), is merely a two dimensional represen-
tation of the n-best table of p. Such a matrix, rep-
resents how the n most likely trees of E(p) are
built. An example of an n-best matrix is repre-
sented in figure 1. This matrix says that the first
most likely tree of p is built by combining the
tree E1(Bi..l) with the tree E1(Cl..j) (there is a 1
in the cell of coordinate ?1, 1?). The second most
likely tree is built by combining the tree E1(Bi..l)
and E2(Cl..j) (there is a 2 in the cell of coordinate
?1, 2?) and so on.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Cl..j
Bi..l
Figure 1: n-best matrix
An n-best matrix M has, by construction, the
remarkable following properties:
123
M(i, y) < M(x, y) ?i 1 ? i < x
M(x, j) < M(x, y) ?j 1 ? j < y
Given an n-best matrix M of dimensions d =
X ? Y and two integers x and y such that 1 ? x <
y ? d, M can be decomposed into three regions:
? the lower region, composed of the cells
which contain ranks i with 1 ? i < x
? the intermediate region, composed of the
cells which contain ranks i with x ? i ? y
? the upper region, composed of the cells
which contain ranks i such that y < i ? d.
The three regions of the matrix of figure 1, for
x = 4 and y = 27 have been delimited with bold
lines in figure 2.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Bi..l
Cl..j
Figure 2: Decomposition of an n-best matrix into
a lower, an intermediate and an upper region with
parameters 4 and 27.
It can be seen that a rectangle, as introduced
earlier, defines a sub-matrix of the n-best matrix.
For example the rectangle ??2, 5?, ?2, 5?? defines
the sub-matrix which north west corner is M(2, 2)
and south east corner is M(5, 5), as represented in
figure 3.
When visiting an instantiated production p, hav-
ing M as an n-best matrix, with the two parame-
ters x and y, the intermediate region of M , with
respect to x and y, contains, by definition, all the
ranks that we are interested in (the ranks rang-
ing from x to y). This region can be partitioned
into a collection of disjoint rectangular regions.
Each such partition therefore defines a collection
of rectangles or a q-partition.
The computation of the parameters x1r, y1r , x2r
and y2r for an instantiated production p therefore
boils down to the computation of a partition of the
intermediate region of the n-best matrix of p.
9
10
11
12
13
17
18
20 21
5
24
26
2 5
2
5 19 22 2725
Cl..j
Bi..l
Figure 3: The sub-matrix corresponding to the
rectangle ??2, 5?, ?2, 5??
We have represented schematically, in figure 4,
two 4-partitions and a 3-partition of the interme-
diate region of the matrix of figure 2. The left-
most (resp. rightmost) partition will be called the
vertical (resp. horizontal) partition. The middle
partition will be called an optimal partition, it de-
composes the intermediate region into a minimal
number of sub-matrices.
   
   
   



III
IV
I
II
   
   
   



I
III
II
   
   
   



II
I 
III
IV
Figure 4: Three partitions of an n-best matrix
The three partitions of figure 4 will give birth to
the following instantiated productions:
? Vertical partition
A4,27i..j ? B3,6i..l C
1,1
l..j A
4,27
i..j ? B2,5i..l C
2,2
l..j
A4,27i..j ? B1,5i..l C
3,5
l..j A
4,27
i..j ? B1,1i..l C
6,6
l..j
? Optimal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,5i..l C
2,5
l..j
A4,27i..j ? B3,6i..l C
1,1
l..j
? Horizontal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,2i..l C
2,5
l..j
A4,27i..j ? B3,5i..l C
1,5
l..j A
4,27
i..j ? B6,6i..l C
1,1
l..j
Vertical and horizontal partition of the interme-
diate region of a n-best matrix can easily be com-
puted. We are not aware of an efficient method that
computes an optimal partition. In the implemen-
tation used for experiments described in section 4,
124
a simple heuristic has been used which computes
horizontal and vertical partitions and keeps the
partition with the lower number of parts.
The size of the new forest is clearly linked to
the partitions that are computed: a partition with
a lower number of parts will give birth to a lower
number of decorated instantiated productions and
therefore a smaller forest. But this optimization
is local, it does not take into account the fact that
an instantiated symbol may be shared in the initial
forest. During the computation of the new forest,
an instantiated production p can therefore be vis-
ited several times, with different parameters. Sev-
eral partitions of p will therefore be computed. If
a rectangle is shared by several partitions, this will
tend to decrease the size of the new forest. The
global optimal must therefore take into account all
the partitions of an instantiated production that are
computed during the construction of the new for-
est.
Example 5: Applying the rectangles method to
the second running example.
We now illustrate more concretely the rectan-
gles method on our second running example intro-
duced in Example 3. Let us recall that we are in-
terested in the n = 3 best trees, the original forest
containing 4 trees.
As said above, this method starts on the instan-
tiated axiom S1..3. Since it is the left-hand side
of only one production, this production is visited
with parameters 1, 3. Moreover, its n-best table is
the same as that of S1..3, given in Example 3. We
show here the corresponding n-best matrix, with
the empty lower region, the intermediate region
(cells corresponding to ranks 1 to 3) and the upper
region:
4
1 2
3
2
2
1
1A1..2
B2..3
As can be seen on that matrix, there are two op-
timal 2-partitions, namely the horizontal and the
vertical partitions, illustrated as follows:
II
I
II I
Let us arbitrarily chose the vertical partition. It
gives birth to two S 1..3-productions, namely:
S 1,31..3 ? A1,21..2 B1,12..3
S 1,31..3 ? A1,11..2 B2,22..3
Since this is the only non-trivial step while apply-
ing the rectangles algorithm to this example, we
can now give its final result, in which the axiom?s
(unnecessary) decorations have been removed:
S1..3 ? A1,21..2 B
{1,1}
2..3
S1..3 ? A1,11..2 B
{2,2}
2..3
A1,21..2 ? A11..2 A11..2 ? a1..2
A1,21..2 ? A21..2 A21..2 ? a1..2
B1,22..3 ? B12..3 B12..3 ? b2..3
B2,22..3 ? B22..3 B22..3 ? b2..3
Compared to the forest built by the ranksets algo-
rithm, this forest has one less production and one
less non-terminal symbol. It has only one more
production than the over-generating pruned for-
est.
4 Experiments on the Penn Treebank
The methods described in section 3 have been
tested on a PCFG G extracted from the Penn Tree-
bank (Marcus et al, 1993). G has been extracted
naively: the trees have been decomposed into bi-
nary context free rules, and the probability of ev-
ery rule has been estimated by its relative fre-
quency (number of occurrences of the rule divided
by the number of occurrences of its left hand side).
Rules occurring less than 3 times and rules with
probabilities lower than 3? 10?4 have been elim-
inated. The grammar produced contains 932 non
terminals and 3, 439 rules.7
The parsing has been realized using the SYN-
TAX system which implements, and optimizes, the
Earley algorithm (Boullier, 2003).
The evaluation has been conducted on the 1, 845
sentences of section 1, which constitute our test
set. For every sentence and for increasing values
of n, an n-best sub-forest has been built using the
rankset and the rectangles method.
The performances of the algorithms have been
measured by the average compression rate they
7We used this test set only to generate practical NLP
forests, with a real NLP grammar, and evaluate the perfor-
mances of our algorithms for constucting sub-forests that
contain only the n-best trees, both in terms of compression
rate and execution time. Therefore, the evaluation carried out
here has nothing to do with the usual evaluation of the pre-
cision and recall of parsers based on the Penn Treebank. In
particular, we are not interested here in the accuracy of such
a grammar, its only purpose is to generate parse forests from
which n-best sub-forests will be built.
125
0e+00
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
7e+05
8e+05
9e+05
 0  100  200  300  400  500  600  700  800  900 1000
av
g.
 n
b 
of
 t
re
es
 i
n 
th
e 
pr
un
ed
 f
or
es
t
n
Figure 5: Overgeneration of the pruned n-best forest
 1
 10
 100
 1000
 1  10  100  1000
co
mp
re
ss
io
n 
ra
te
n
pruned forest
rectangles
ranksets
Figure 6: Average compression rates
achieve for different values of n. The compres-
sion rate is obtained by dividing the size of the
n-best sub-forest of a sentence, as defined in sec-
tion 2, by the size of the (unfolded) n-best forest.
The latter is the sum of the sizes of all trees in the
forest, where every tree is seen as an instantiated
grammar, its size is therefore the size of the corre-
sponding instantiated grammar.
The size of the n-best forest constitutes a natu-
ral upper bound for the representation of the n-best
trees. Unfortunately, we have no natural lower
bound for the size of such an object. Neverthe-
less, we have computed the compression rates of
the pruned n-best forest and used it as an imperfect
lower bound. As already mentioned, its imper-
fection comes from the fact that a pruned n-best
forest contains more trees than the n best ones.
This overgeneration appears clearly in Figure 5
which shows, for increasing values of n, the av-
erage number of trees in the n-best pruned forest
for all sentences in our test set.
Figure 6 shows the average compression rates
achieved by the three methods (forest pruning,
rectangles and ranksets) on the test set for increas-
ing values of n. As predicted, the performances lie
between 1 (no compression) and the compression
of the n-best pruned forest. The rectangle method
outperforms the ranksets algorithm for every value
of n.
The time needed to build an 100-best forest with
the rectangle and the ranksets algorithms is shown
in Figure 7. This figure shows the average parsing
126
 0
 200
 400
 600
 800
 1000
 1200
 5  10  15  20  25  30  35  40  45
ti
me
 i
n 
mi
ll
is
ec
on
ds
sentence length
parsing
ranksets
rectangles
Figure 7: Processing time
time for sentences of a given length, as well as the
average time necessary for building the 100-best
forest using the two aforementioned algorithms.
This time includes the parsing time i.e. it is the
time necessary for parsing a sentence and build-
ing the 100-best forest. As shown by the figure,
the time complexities of the two methods are very
close.
5 Conclusion and perspectives
This work presented two methods to build n-
best sub-forests. The so called rectangle meth-
ods showed to be the most promising, for it al-
lows to build efficient sub-forests with little time
overhead. Future work will focus on computing
optimized partitions of the n-best matrices, a cru-
cial part of the rectangle method, and adapting the
method to arbitrary (non binary) CFG. Another
line of research will concentrate on performing
re-ranking of the n-best trees directly on the sub-
forest.
Acknowledgments
This research is supported by the French National
Research Agency (ANR) in the context of the
SEQUOIA project (ANR-08-EMER-013).
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Taylor L. Booth. 1969. Probabilistic representation of
formal languages. In Tenth Annual Symposium on
Switching and Automata Theory, pages 74?81.
Pierre Boullier and Philippe Deschamp. 1988.
Le syste`me SYNTAXTM - manuel d?utilisation.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Benot Sagot. 2005. Efficient and
robust LFG parsing: SXLFG. In Proceedings of
IWPT?05, Vancouver, Canada.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT?03, pages 43?54.
Jay Earley. 1970. An efficient context-free parsing
algorithm. Communication of the ACM, 13(2):94?
102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL?08, pages 586?594.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, United Kingdom. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT?01.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In J. Loeckx, ed-
itor, Proceedings of the Second Colloquium on Au-
tomata, Languages and Programming, volume 14 of
Lecture Notes in Computer Science, pages 255?269.
Springer-Verlag.
127
Bernard Lang. 1994. Recognition can be harder then
parsing. Computational Intelligence, 10:486?494.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
128
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 254?265,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Directed Acyclic Graphs
with Range Concatenation Grammars
Pierre Boullier and Beno??t Sagot
Alpage, INRIA Paris-Rocquencourt & Universite? Paris 7
Domaine de Voluceau ? Rocquencourt, BP 105 ? 78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
Abstract
Range Concatenation Grammars (RCGs)
are a syntactic formalism which possesses
many attractive properties. It is more pow-
erful than Linear Context-Free Rewriting
Systems, though this power is not reached
to the detriment of efficiency since its sen-
tences can always be parsed in polynomial
time. If the input, instead of a string, is a
Directed Acyclic Graph (DAG), only sim-
ple RCGs can still be parsed in polyno-
mial time. For non-linear RCGs, this poly-
nomial parsing time cannot be guaranteed
anymore. In this paper, we show how the
standard parsing algorithm can be adapted
for parsing DAGs with RCGs, both in the
linear (simple) and in the non-linear case.
1 Introduction
The Range Concatenation Grammar (RCG)
formalism has been introduced by Boullier ten
years ago. A complete definition can be
found in (Boullier, 2004), together with some
of its formal properties and a parsing algorithm
(qualified here of standard) which runs in
polynomial time. In this paper we shall only
consider the positive version of RCGs which
will be abbreviated as PRCG.1 PRCGs are
very attractive since they are more powerful
than the Linear Context-Free Rewriting Systems
(LCFRSs) by (Vijay-Shanker et al, 1987). In fact
LCFRSs are equivalent to simple PRCGs which
are a subclass of PRCGs. Many Mildly Context-
Sensitive (MCS) formalisms, including Tree
Adjoining Grammars (TAGs) and various kinds
of Multi-Component TAGs, have already been
1Negative RCGs do not add formal power since both
versions exactly cover the class PTIME of languages
recognizable in deterministic polynomial time (see (Boullier,
2004) for an indirect proof and (Bertsch and Nederhof, 2001)
for a direct proof).
translated into their simple PRCG counterpart in
order to get an efficient parser for free (see for
example (Barthe?lemy et al, 2001)).
However, in many Natural Language Process-
ing applications, the most suitable input for a
parser is not a sequence of words (forms, ter-
minal symbols), but a more complex representa-
tion, usually defined as a Direct Acyclic Graph
(DAG), which correspond to finite regular lan-
guages, for taking into account various kinds of
ambiguities. Such ambiguities may come, among
others, from the output of speech recognition sys-
tems, from lexical ambiguities (and in particular
from tokenization ambiguities), or from a non-
deterministic spelling correction module.
Yet, it has been shown by (Bertsch and
Nederhof, 2001) that parsing of regular languages
(and therefore of DAGs) using simple PRCGs is
polynomial. In the same paper, it is also proven
that parsing of finite regular languages (the DAG
case) using arbitrary RCGs is NP-complete.
This papers aims at showing how these
complexity results can be made concrete in a
parser, by extending a standard RCG parsing
algorithm so as to handle input DAGs. We
will first recall both some basic definitions and
their notations. Afterwards we will see, with a
slight modification of the notion of ranges, how
it is possible to use the standard PRCG parsing
algorithm to get in polynomial time a parse forest
with a DAG as input.2 However, the resulting
parse forest is valid only for simple PRCGs. In
the non-linear case, and consistently with the
complexity results mentioned above, we show that
the resulting parse forest needs further processing
for filtering out inconsistent parses, which may
need an exponential time. The proposed filtering
algorithm allows for parsing DAGs in practice
with any PRCG, including non-linear ones.
2The notion of parse forest is reminiscent of the work
of (Lang, 1994).
254
2 Basic notions and notations
2.1 Positive Range Concatenation Grammars
A positive range concatenation grammar (PRCG)
G = (N,T, V, P, S) is a 5-tuple in which:
? T and V are disjoint alphabets of terminal
symbols and variable symbols respectively.
? N is a non-empty finite set of predicates of
fixed arity (also called fan-out). We write
k = arity(A) if the arity of the predicate A is
k. A predicate A with its arguments is noted
A(~?) with a vector notation such that |~?| = k
and ~?[j] is its jth argument. An argument is a
string in (V ? T )?.
? S is a distinguished predicate called the start
predicate (or axiom) of arity 1.
? P is a finite set of clauses. A clause c
is a rewriting rule of the form A0( ~?0) ?
A1( ~?1) . . . Ar( ~?r) where r, r ? 0 is its
rank, A0( ~?0) is its left-hand side or LHS,
and A1( ~?1) . . . Ar( ~?r) its right-hand side or
RHS. By definition c[i] = Ai(~?i), 0 ? i ? r
where Ai is a predicate and ~?i its arguments;
we note c[i][j] its jth argument; c[i][j] is of
the form X1 . . . Xnij (the Xk?s are terminal
or variable symbols), while c[i][j][k], 0 ?
k ? nij is a position within c[i][j].
For a given clause c, and one of its predicates
c[i] a subargument is defined as a substring of an
argument c[i][j] of the predicate c[i]. It is denoted
by a pair of positions (c[i][j][k], c[i][j][k? ]), with
k ? k?.
Let w = a1 . . . an be an input string in T ?,
each occurrence of a substring al+1 . . . au is a pair
of positions (w[l], w[u]) s.t. 0 ? l ? u ? n
called a range and noted ?l..u?w or ?l..u? when
w is implicit. In the range ?l..u?, l is its lower
bound while u is its upper bound. If l = u,
the range ?l..u? is an empty range, it spans an
empty substring. If ?1 = ?l1..u1?, . . . and
?m = ?lm..um? are ranges, the concatenation of
?1, . . . , ?m noted ?1 . . . ?m is the range ? = ?l..u?
if and only if we have ui = li+1, 1 ? i < m,
l = l1 and u = um.
If c = A0( ~?0) ? A1( ~?1) . . . Ar( ~?r) is a
clause, each of its sub-
arguments (c[i][j][k], c[i][j][k? ]) may take a range
? = ?l..u? as value: we say that it is instantiated
by ?. However, the instantiation of a subargument
is subjected to the following constraints.
? If the subargument is the empty string (i.e.,
k = k?), ? is an empty range.
? If the subargument is a terminal symbol (i.e.,
k + 1 = k? and Xk? ? T ), ? is such that
l + 1 = u and au = Xk? . Note that several
occurrences of the same terminal symbol
may be instantiated by different ranges.
? If the subargument is a variable symbol
(i.e., k + 1 = k? and Xk? ? V ),
any occurrence (c[i?][j?][m], c[i?][j?][m?]) of
Xk? is instantiated by ?. Thus, each
occurrence of the same variable symbol must
be instantiated by the same range.
? If the subargument is the string Xk+1 . . . Xk? ,
? is its instantiation if and only if we have
? = ?k+1 . . . ?k? in which ?k+1, . . . , ?k? are
respectively the instantiations of Xk+1, . . . ,
Xk? .
If in c we replace each argument by its
instantiation, we get an instantiated clause noted
A0(~?0) ? A1(~?1) . . . Ar(~?r) in which each
Ai(~?i) is an instantiated predicate.
A binary relation called derive and noted ?
G,w
is
defined on strings of instantiated predicates. If ?1
and ?2 are strings of instantiated predicates, we
have
?1 A0(~?0) ?2 ?G,w ?1 A1(~?1) . . . Am( ~?m) ?2
if and only if A0(~?0) ? A1(~?1) . . . Am( ~?m) is an
instantiated clause.
The (string) language of a PRCG G is the
set L(G) = {w | S(?0..|w|?w) +?G,w ?}. In
other words, an input string w ? T ?, |w| =
n is a sentence of G if and only there exists a
complete derivation which starts from S(?0..n?)
(the instantiation of the start predicate on the
whole input text) and leads to the empty string
(of instantiated predicates). The parse forest of w
is the CFG whose axiom is S(?0..n?) and whose
productions are the instantiated clauses used in all
complete derivations.3
We say that the arity of a PRCG is k, and we
call it a k-PRCG, if and only if k is the maximum
3Note that this parse forest has no terminal symbols (its
language is the empty string).
255
arity of its predicates (k = maxA?N arity(A)).
We say that a k-PRCG is simple, we have a simple
k-PRCG, if and only if each of its clause is
? non-combinatorial: the arguments of its RHS
predicates are single variables;
? non-erasing: each variable which occur in
its LHS (resp. RHS) also occurs in its RHS
(resp. LHS);
? linear: there are no variables which occur
more than once in its LHS and in its RHS.
The subclass of simple PRCGs is of importance
since it is MCS and is the one equivalent to
LCFRSs.
2.2 Finite Automata
A non-deterministic finite automaton (NFA) is
the 5-tuple A = (Q,?, ?, q0, F ) where Q is a
non empty finite set of states, ? is a finite set
of terminal symbols, ? is the ternary transition
relation ? = {(qi, t, qj)|qi, qj ? Q? t ? ??{?}},
q0 is a distinguished element of Q called the initial
state and F is a subset of Q whose elements are
called final states. The size of A, noted |A|, is its
number of states (|A| = |Q|).
We define the ternary relation ?? on Q????Q
as the smallest set s.t. ?? = {(q, ?, q) | q ? Q} ?
{(q1, xt, q3) | (q1, x, q2) ? ?? ? (q2, t, q3) ? ?}. If
(q, x, q?) ? ??, we say that x is a path between q
and q?. If q = q0 and q? ? F , x is a complete path.
The language L(A) defined (generated, recog-
nized, accepted) by the NFA A is the set of all its
complete paths.
We say that a NFA is empty if and only if its
language is empty. Two NFAs are equivalent if
and only if they define the same language. A
NFA is ?-free if and only if its transition relation
does not contain a transition of the form (q1, ?, q2).
Every NFA can be transformed into an equivalent
?-free NFA (this classical result and those recalled
below can be found, e.g., in (Hopcroft and Ullman,
1979)).
As usual, a NFA is drawn with the following
conventions: a transition (q1, t, q2) is an arrow
labelled t from state q1 to state q2 which are
printed with a surrounded circle. Final states are
doubly circled while the initial state has a single
unconnected, unlabelled input arrow.
A deterministic finite automaton (DFA) is a
NFA in which the transition relation ? is a
transition function, ? : Q ? ? ? Q. In
other words, there are no ?-transitions and if
(q1, t, q2) ? ?, t 6= ? and ?(q1, t, q?2) ? ? with
q?2 6= q2. Each NFA can be transformed by
the subset construction into an equivalent DFA.
Moreover, each DFA can be transformed by a
minimization algorithm into an equivalent DFA
which is minimal (i.e., there is no other equivalent
DFA with fewer states).
2.3 Directed acyclic graphs
Formally, a directed acyclic graph (DAG) D =
(Q,?, ?, q0, F ) is an NFA for which there exists
a strict order relation < on Q such that (p, t, q) ?
? ? p < q. Without loss of generality we may
assume that < is a total order.
Of course, as NFAs, DAGs can be transformed
into equivalent deterministic or minimal DAGs.
3 DAGs and PRCGs
A DAG D is recognized (accepted) by a PRCG
G if and only if L(D) ? L(G) 6= ?. A trivial
way to solve this recognition (or parsing) problem
is to extract the complete paths of L(D) (which
are in finite number) one by one and to parse
each such string with a standard PRCG parser, the
(complete) parse forest for D being the union of
each individual forest.4 However since DAGs may
define an exponential number of strings w.r.t. its
own size,5 the previous operation would take an
exponential time in the size of D, and the parse
forest would also have an exponential size.
The purpose of this paper is to show that
it is possible to directly parse a DAG (without
any unfolding) by sharing identical computations.
This sharing may lead to a polynomial parse time
for an exponential number of sentences, but, in
some cases, the parse time remains exponential.
3.1 DAGs and Ranges
In many NLP applications the source text cannot
be considered as a sequence of terminal symbols,
but rather as a finite set of finite strings. As
4These forests do not share any production (instantiated
clause) since ranges in a particular forest are all related
to the corresponding source string w (i.e., are all of the
form ?i..j?w). To be more precise the union operation on
individual forests must be completed in adding productions
which connect the new (super) axiom (say S?) with each root
and which are, for each w of the form S? ? S(?0..|w|?w).
5For example the language (a|b)n, n > 0 which contains
2n strings can be defined by a minimal DAG whose size is
n + 1.
256
mentioned in th introduction, this non-unique
string could be used to encode not-yet-solved
ambiguities in the input. DAGs are a convenient
way to represent these finite sets of strings by
factorizing their common parts (thanks to the
minimization algorithm).
In order to use DAGs as inputs for PRCG
parsing we will perform two generalizations.
The first one follows. Let w = t1 . . . tn be a
string in some alphabet ? and let Q = {qi | 0 ?
i ? n} be a set of n + 1 bounds with a total order
relation <, we have q0 < q1 < . . . < qn. The
sequence ? = q0t1q1t2q2 . . . tnqn ? Q?(??Q)n
is called a bounded string which spells w. A range
is a pair of bounds (qi, qj) with qi < qj noted
?pi..pj?pi and any triple of the form (qi?1tiqi)
is called a transition. All the notions around
PRCGs defined in Section 2.1 easily generalize
from strings to bounded strings. It is also the case
for the standard parsing algorithm of (Boullier,
2004).
Now the next step is to move from bounded
strings to DAGs. Let D = (Q,?, ?, q0, F ) be a
DAG. A string x ? ?? s.t. we have (q1, x, q2) ?
?? is called a path between q1 and q2 and a string
? = qt1q1 . . . tpqp ? Q ? (? ? {?} ? Q)? is a
bounded path and we say that ? spells t1t2 . . . tp.
A path x from q0 to f ? F is a complete path
and a bounded path of the form q0t1 . . . tnf with
f ? F is a complete bounded path. In the
context of a DAG D, a range is a pair of states
(qi, qj) with qi < qj noted ?qi..qj?D. A range
?qi..qj?D is valid if and only if there exists a
path from qi to qj in D. Of course, any range
?p..q?D defines its associated sub-DAG D?p..q? =
(Q?p..q?,??p..q?, ??p..q?, p, {q}) as follows. Its
transition relation is ??p..q? = {(r, t, s) | (r, t, s) ?
? ? (p, x?, r), (s, x??, q) ? ??}. If ??p..q? = ?
(i.e., there is no path between p and q), D?p..q? is
the empty DAG, otherwise Q?p..q? (resp. ??p..q?)
are the states (resp. terminal symbols) of the
transitions of ??p..q?. With this new definition of
ranges, the notions of instantiation and derivation
easily generalize from bounded strings to DAGs.
The language of a PRCG G for a DAG
D is defined by
?
L (G,D) = ?f?F {x |
S(?q0..f?D) +?G,D ?}. Let x ? L(D), it is not very
difficult to show that if x ? L(G) then we have
x ?
?
L (G,D). However, the converse is not true
(see Example 1), a sentence of L(D)? ?L (G,D)
may not be in L(G). To put it differently, if we
use the standard RCG parser, with the ranges of
a DAG, we produce the shared parse-forest for
the language
?
L (G,D) which is a superset of
L(D) ? L(G).
However, if G is a simple PRCG, we have
the equality L(G) = ?D is a DAG
?
L (G,D).
Note that the subclass of simple PRCGs is of
importance since it is MCS and it is the one
equivalent to LCFRSs. The informal reason of
the equality is the following. If an instantiated
predicate Ai(~?i) succeeds in some RHS, this
means that each of its ranges ~?i[j] = ?k..l?D has
been recognized as being a component of Ai, more
precisely their exists a path from k to l in D which
is a component of Ai. The range ?k..l?D selects
in D a set ??k..l?D of transitions (the transitions
used in the bounded paths from k to l). Because
of the linearity of G, there is no other range in that
RHS which selects a transition in ??k..l?D . Thus
the bounded paths selected by all the ranges of that
RHS are disjoints. In other words, any occurrence
of a valid instantiated range ?i..j?D selects a set of
paths which is a subset of L(D?i..j?).
Now, if we consider a non-linear PRCG, in
some of its clauses, there is a variable, say X,
which has several occurrences in its RHS (if we
consider a top-down non-linearity). Now assume
that for some input DAG D, an instantiation of
that clause is a component of some complete
derivation. Let ?p..q?D be the instantiation of X
in that instantiated clause. The fact that a predicate
in which X occurs succeeds means that there exist
paths from p to q in D?p..q?. The same thing stands
for all the other occurrences of X but nothing
force these paths to be identical or not.
Example 1.
Let us take an example which will be used
throughout the paper. It is a non-linear 1-PRCG
which defines the language anbncn, n ? 0 as
the intersection of the two languages a?bncn and
anbnc?. Each of these languages is respectively
defined by the predicates a?bncn and anbnc?; the
start predicate is anbncn.
257
1
2
3
4
a b
b c
Figure 1: Input DAG associated with ab|bc.
anbncn(X) ? a?bncn(X) anbnc?(X)
a?bncn(aX) ? a?bncn(X)
a?bncn(X) ? bncn(X)
bncn(bXc) ? bncn(X)
bncn(?) ? ?
anbnc?(Xc) ? anbnc?(X)
anbnc?(X) ? anbn(X)
anbn(aXb) ? anbn(X)
anbn(?) ? ?
If we use this PRCG to parse the DAG of
Figure 1 which defines the language {ab, bc},
we (erroneously) get the non-empty parse for-
est of Figure 2 though neither ab nor bc is in
anbncn.6 It is not difficult to see that the problem
comes from the non-linear instantiated variable
X?1..4? in the start node, and more precisely from
the actual (wrong) meaning of the three differ-
ent occurrences of X?1..4? in anbncn(X?1..4?) ?
a?bncn(X?1..4?) anbnc?(X?1..4?). The first occur-
rence in its RHS says that there exists a path in
the input DAG from state 1 to state 4 which is an
a?bncn. The second occurrence says that there
exists a path from state 1 to state 4 which is an
anbnc?. While the LHS occurrence (wrongly) says
that there exists a path from state 1 to state 4 which
is an anbncn. However, if the two X?1..4??s in the
RHS had selected common paths (this is not pos-
sible here) between 1 and 4, a valid interpretation
could have been proposed.
With this example, we see that the difficulty of
DAG parsing only arises with non-linear PRCGs.
If we consider linear PRCGs, the sub-class of
the PRCGs which is equivalent to LCFRSs, the
6In this forest oval nodes denote different instantiated
predicates, while its associated instantiated clauses are
presented as its daughter(s) and are denoted by square nodes.
The LHS of each instantiated clause shows the instantiation
of its LHS symbols. The RHS is the corresponding sequence
of instantiated predicates. The number of daughters of each
square node is the number of its RHS instantiated predicates.
standard algorithm works perfectly well with input
DAGs, since a valid instantiation of an argument
of a predicate in a clause by some range ?p..q?
means that there exists (at least) one path between
p and q which is recognized.
The paper will now concentrate on non-linear
PRCGs, and will present a new valid parsing
algorithm and study its complexities (in space and
time).
In order to simplify the presentation we
introduce this algorithm as a post-processing pass
which will work on the shared parse-forest output
by the (slightly modified) standard algorithm
which accepts DAGs as input.
3.2 Parsing DAGs with non-linear PRCGs
The standard parsing algorithm of (Boullier, 2004)
working on a string w can be sketched as follows.
It uses a single memoized boolean function
predicate(A, ~?) where A is a predicate and ~? is a
vector of ranges whose dimension is arity(A). The
initial call to that function has the form predicate
(S, ?0..|w|?). Its purpose is, for each A0-clause, to
instantiate each of its symbols in a consistant way.
For example if we assume that the ith argument of
the LHS of the current A0-clause is ??iXaY ???i and
that the ith component of ~?0 is the range ?pi..qi? an
instantiation of X, a an Y by the ranges ?pX ..qX?,
?pa..qa? and ?pY ..qY ? is such that we have pi ?
pX ? qX = pa < qa = pa + 1 = pY ? qY ? qi
and w = w?aw?? with |w?| = pa. Since the PRCG
is non bottom-up erasing, the instantiation of all
the LHS symbols implies that all the arguments
of the RHS predicates Ai are also instantiated and
gathered into the vector of ranges ~?i. Now, for
each i (1 ? i ? |RHS|), we can call predicate
(Ai, ~?i). If all these calls succeed, the instantiated
clause can be stored as a component of the shared
parse forest.7
In the case of a DAG D = (Q,?, ?, q0, F ) as
input, there are two slight modifications, the ini-
tial call is changed by the conjunctive call pred-
icate(S, ?q0..f1?) ? . . .? predicate (S, ?q0..f|F |?)
with fi ? F 8 and the terminal symbol a can be in-
stantiated by the range ?pa..qa?D only if (pa, a, qa)
7Note that such an instantiated clause could be
unreachable from the (future) instantiated start symbol which
will be the axiom of the shared forest considered as a CFG.
8Technically, each of these calls produces a forest. These
individual forests may share subparts but their roots are all
different. In order to have a true forest, we introduce a
new root, the super-root whose daughters are the individual
forests.
258
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
bncn?1..4?
bncn(b?1..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
bncn?3..3?
bncn(??3..3?) ? ?
anbn?2..2?
anbn(??2..2?) ? ?
Figure 2: Parse forest for the input DAG ab|bc.
is a transition in ?. The variable symbol X can
be instantiated by the range ?pX ..qX?D only if
?pX ..qX?D is valid.
3.3 Forest Filtering
We assume here that for a given PRCG G we
have built the parse forest of an input DAG D as
explained above and that each instantiated clause
of that forest contains the range ?pX ..qX?D of
each of its instantiated symbols X. We have seen
in Example 1 that this parse forest is valid if G is
linear but may well be unvalid if G is non-linear.
In that latter case, this happens because the range
?pX ..qX?D of each instantiation of the non-linear
variable X selects the whole sub-DAG D?pX ..qX?
while each instantiation should only select a sub-
language of L(D?pX ..qX?). For each occurrence of
X in the LHS or RHS of a non-linear clause, its
sub-languages could of course be different from
the others. In fact, we are interested in their
intersections: If their intersections are non empty,
this is the language which will be associated with
?pX ..qX?D, otherwise, if their intersections are
empty, then the instantiation of the considered
clause fails and must thus be removed from the
forest. Of course, we will consider that the
language (a finite number of strings) associated
with each occurrence of each instantiated symbol
is represented by a DAG.
The idea of the forest filtering algorithm
is to first compute the DAGs associated with
each argument of each instantiated predicate
during a bottom-up walk. These DAGs are
called decorations. This processing will perform
DAG compositions (including intersections, as
suggested above), and will erase clauses in which
empty intersections occur. If the DAG associated
with the single argument of the super-root is
empty, then parsing failed.
Otherwise, a top-down walk is launched
(see below), which may also erase non-valid
instantiated clauses. If necessary, the algorithm
is completed by a classical CFG algorithm which
erase non productive and unreachable symbols
leaving a reduced grammar/forest.
In order to simplify our presentation we will
assume that the PRCGs are non-combinatorial
and bottom-up non-erasing. However, we
can note that the following algorithm can be
generalized in order to handle combinatorial
PRCGs and in particular with overlapping
arguments.9 Moreover, we will assume that the
forest is non cyclic (or equivalently that all cycles
have previously been removed).10
9For example the non-linear combinatorial clause
A(XY Z) ? B(XY ) B(Y Z) has overlapping arguments.
10By a classical algorithm from the CFG technology.
259
3.3.1 The Bottom-Up Walk
For this principle algorithm, we assume that for
each instantiated clause in the forest, a DAG
will be associated with each occurrence of each
instantiated symbol. More precisely, for a given
instantiated A0-clause, the DAGs associated with
the RHS symbol occurrences are composed (see
below) to build up DAGs which will be associated
with each argument of its LHS predicate. For each
LHS argument, this composition is directed by the
sequence of symbols in the argument itself.
The forest is walked bottom-up starting from its
leaves. The constraint being that an instantiated
clause is visited if and only if all its RHS
instantiated predicates have already all been
visited (computed). This constraint can be
satisfied for any non-cyclic forest.
To be more precise, consider an instantiation
c? = A0(~?0) ? A1(~?1) . . . Ap( ~?p) of the clause
c = A0( ~?0) ? A1( ~?1) . . . Am( ~?m), we perform
the following sequence:
1. If the clause is not top-down linear (i.e.,
there exist multiple occurrences of the same
variables in its RHS arguments), for such
variable X let the range ?pX ..qX? be its
instantiation (by definition, all occurrences
are instantiated by the same range), we
perform the intersection of the DAGs
associated with each instantiated predicate
argument X. If one intersection results in
an empty DAG, the instantiated clause is
removed from the forest. Otherwise, we
perform the following steps.
2. If a RHS variable Y is linear, it occurs once in
the jth argument of predicate Ai. We perform
a brand new copy of the DAG associated with
the jth argument of the instantiation of Ai.
3. At that moment, all instantiated variables
which occur in c? are associated with a DAG.
For each occurrence of a terminal symbol t
in the LHS arguments we associate a (new)
DAG whose only transition is (p, t, q) where
p and q are brand new states with, of course,
p < q.
4. Here, all symbols (terminals or variables) are
associated with disjoints DAGs. For each
LHS argument ~?0[i] = Xi1 . . . Xij . . . Xipi ,
we associate a new DAG which is the
concatenation of the DAGs associated with
the symbols Xi1, . . . , Xij , . . . and Xipi .
5. Here each LHS argument of c? is associated
with a non empty DAG, we then report
the individual contribution of c? into the
(already computed) DAGs associated with
the arguments of its LHS A0(~?0). The DAG
associated with the ith argument of A0(~?0) is
the union (or a copy if it is the first time) of its
previous DAG value with the DAG associated
with the ith argument of the LHS of c?.
This bottom-up walk ends on the super-root with a
final decoration say R. In fact, during this bottom-
up walk, we have computed the intersection of the
languages defined by the input DAG and by the
PRCG (i.e., we have L(R) = L(D) ? L(G)).
Example 2.
1 2 3 4a
b
b c
b
Figure 3: Input DAG associated with abc|ab|bc.
With the PRCG of Example 1 and the input
DAG of Figure 3, we get the parse forest of
Figure 4 whose transitions are decorated by the
DAGs computed by the bottom-up algorithm.11
The crucial point to note here is the intersection
which
is performed between {abc, bc} and {abc, ab} on
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4? . The
non-empty set {abc} is the final result assigned to
the instantiated start symbol. Since this result is
non empty, it shows that the input DAG D is rec-
ognized by G. More precisely, this shows that the
sub-language of D which is recognized by G is
{abc}.
However, as shown in the previous example, the
(undecorated) parse forest is not the forest built
for the DAG L(D) ? L(G) since it may contain
non-valid parts (e.g., the transitions labelled {bc}
or {ab} in our example). In order to get the
11For readability reasons these DAGs are represented by
their languages (i.e., set of strings). Bottom-up transitions
from instantiated clauses to instantiated predicates reflects
the computations performed by that instantiated clause
while bottom-up transitions from instantiated predicates to
instantiated clauses are the union of the DAGs entering that
instantiated predicate.
260
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
a?bncn(a?1..2? X?2..4?) ? a?bncn?2..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
anbnc?(X?1..3? c?3..4? ) ? anbnc??1..3?
bncn?1..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
a?bncn?2..4?
a?bncn(X?2..4?) ? bncn?2..4?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
anbnc??1..3?
anbnc?(X?1..3?) ? anbn?1..3?
bncn?3..3?
bncn(??3..3?) ? ?
bncn?2..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?2..2?
anbn(??2..2?) ? ?
anbn?1..3?
anbn(a?1..2? X?2..2? b?2..3?) ? anbn?2..2?
{abc}
{abc, bc} {abc, ab}
{bc}
{abc}
{ab}
{abc}
{bc} {abc} {ab} {abc}
{bc} {bc} {ab} {ab}
{?}
{bc}
{?}
{ab}
{?}
{bc}
{?}
{ab}
{?} {?}
Figure 4: Bottom-up decorated parse forest for the input DAG abc|ab|bc.
261
right forest (i.e., to get a PRCG parser ? not
a recognizer ? which accepts a DAG as input)
we need to perform another walk on the previous
decorated forest.
3.3.2 The Top-Down Walk
The idea of the top-down walk on the parse
forest decorated by the bottom-up walk is to
(re)compute all the previous decorations starting
from the bottom-up decoration associated with
the instantiated start predicate. It is to be noted
that (the language defined by) each top-down
decoration is a subset of its bottom-up counterpart.
However, when a top-down decoration becomes
empty, the corresponding subtree must be erased
from the forest. If the bottom-up walk succeeds,
we are sure that the top-down walk will not
result in an empty forest. Moreover, if we
perform a new bottom-up walk on this reduced
forest, the new bottom-up decorations will denote
the same language as their top-down decorations
counterpart.
The forest is walked top-down starting from
the super-root. The constraint being that an
instantiated A(~?)-clause is visited if and only if all
the occurrences of A(~?) occurring in the RHS of
instantiated clauses have all already been visited.
This constraint can be satisfied for any non-cyclic
forest.
Initially, we assume that each argument of each
instantiated predicate has an empty decoration,
except for the argument of the super-root which is
decorated by the DAG R computed by the bottom-
up pass.
Now, assume that a top-down decoration has
been (fully) computed for each argument of
the instantiated predicate A0(~?0). For each
instantiated clause of the form c? = A0(~?0) ?
A1(~?1) . . . Ai(~?i) . . . Am( ~?m), we perform the
following sequence:12
1. We perform the intersection of the top-down
decoration of each argument of A0(~?0) with
the decoration computed by the bottom-up
pass for the same argument of the LHS
predicate of c?. If the result is empty, c? is
erased from the forest.
2. For each LHS argument, the previous results
are dispatched over the symbols of this
12The decoration of each argument of Ai(~?i) is either
initially empty or has already been partially computed.
argument.13 Thus, each instantiated LHS
symbol occurrence is decorated by its own
DAG. If the considered clause has several
occurrences of the same variable in the LHS
arguments (i.e., is bottom-up non-linear),
we perform the intersection of these DAGs
in order to leave a single decoration per
instantiated variable. If an intersection results
in an empty DAG, the current clause is erased
from the forest.
3. The LHS instantiated variable decorations
are propagated to the RHS arguments. This
propagation may result in DAG concatena-
tions when a RHS argument is made up of
several variables (i.e., is combinatorial).
4. At last, we associate to each argument
of Ai(~?i) a new decoration which is
computed as the union of its previous top-
down decoration with the decoration just
computed.
Example 3. When we apply the previous al-
gorithm to the bottom-up parse forest of Exam-
ple 2, we get the top-down parse forest of Fig-
ure 5. In this parse forest, erased parts are
laid out in light gray. The more noticable points
w.r.t. the bottom-up forest are the decorations be-
tween anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
and its RHS predicates a?bncn?1..4? and
anbnc??1..4? which are changed both to {abc}
instead of {abc, bc} and {abc, ab}. These two
changes induce the indicated erasings.
13Assume that ~?0[k] = ?p..q?D, that the decoration DAG
associated with the kth argument of A0( ~?0) is D??p..q? =
(Q??p..q?,??p..q?, ???p..q?, p?, F ??p..q?) (we have L(D??p..q?) ?
L(D?p..q?)) and that ~?0[k] = ?1kX?2k and that ?i..j?D is the
instantiation of the symbol X in c?. Our goal is to extract
from D??p..q? the decoration DAG D??i..j? associated with
that instantiated occurrence of X. This computation can be
helped if we maintain, associated with each decoration DAG
a function, say d, which maps each state of the decoration
DAG to a set of states (bounds) of the input DAG D. If, as we
have assumed, D is minimal, each set of states is a singleton,
we can write d(p?) = p, d(f ?) = q for all f ? ? F ??p..q?
and more generally d(i?) ? Q if i? ? Q?. Let I ? = {i? |
i? ? Q??p..q? ? d(i?) = i} and J ? = {j? | j? ? Q??p..q? ?
d(j?) = j}. The decoration DAG D??i..j? is such that
L(D??i..j?) =
S
i??I?,j??J?{x | x is a path from i? to j?}.
Of course, together with the construction of D??i..j? , its
associated function d must also be built.
262
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
a?bncn(a?1..2? X?2..4?) ? a?bncn?2..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
anbnc?(X?1..3? c?3..4? ) ? anbnc??1..3?
bncn?1..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
a?bncn?2..4?
a?bncn(X?2..4?) ? bncn?2..4?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
anbnc??1..3?
anbnc?(X?1..3?) ? anbn?1..3?
bncn?3..3?
bncn(??3..3?) ? ?
bncn?2..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?2..2?
anbn(??2..2?) ? ?
anbn?1..3?
anbn(a?1..2? X?2..2? b?2..3?) ? anbn?2..2?
{abc}
{abc} {abc}
?
{abc}
?
{abc}
? {abc} ? {abc}
? {bc} ? {ab}
?
{bc}
?
{ab}
{?}
{bc}
{?}
{ab}
{?} {?}
Figure 5: Top-down decorated parse forest for the input DAG abc|ab|bc.
263
3.4 Time and Space Complexities
In this Section we study the time and size
complexities of the forest filtering algorithm.
Let us consider the sub-DAG D?p..q? of the
minimal input DAG D and consider any (finite)
regular language L ? L(D?p..q?), and let DL be
the minimal DAG s.t. L(DL) = L. We show, on
an example, that |DL| may be an exponential w.r.t.
|D?p..q?|.
Consider, for a given h > 0, the language
(a|b)h. We know that this language can be
represented by the minimal DAG with h+1 states
of Figure 6.
Assume that h = 2k and consider the
sub-language L2k of (a|b)2k (nested well-
parenthesized strings) which is defined by
1. L2 = {aa, bb} ;
2. k > 1, L2k = {axa, bxb | x ? L2k?2},
It is not difficult to see that the DAG in Figure 7
defines L2k and is minimal, but its size 2k+2 ? 2
is an exponential in the size 2k+1 of the minimal
DAG for the language (a|b)2k .
This results shows that, there exist cases in
which some minimal DAGs D? that define sub-
languages of minimal DAGs D may have a
exponential size (i.e., |D?| = O(2|D|). In other
words, when, during the bottom-up or top-down
walk, we compute union of DAGs, we may fall
on these pathologic DAGs that will induce a
combinatorial explosion in both time and space.
3.5 Implementation Issues
Of course, many improvements may be brought
to the previous principle algorithms in practical
implementations. Let us cite two of them. First it
is possible to restrict the number of DAG copies:
a DAG copy is not useful if it is the last reference
to that DAG.
We shall here devel the second point on a little
more: if an argument of a predicate is never
used in ant non-linearity, it is only a waste of
time to compute its decoration. We say that Ak,
the kth argument of the predicate A is a non-
linear predicate argument if there exists a clause
c in which A occurs in the RHS and whose
kth argument has at least one common variable
another argument Bh of some predicate B of
the RHS (if B = A, then of course k and h
must be different). It is clear that Bh is then
non-linear as well. It is not difficult to see that
decorations needs only to be computed if they are
associated with a non-linear predicate argument. It
is possible to compute those non-linear predicate
arguments statically (when building the parser)
when the PRCG is defined within a single module.
However, if the PRCG is given in several modules,
this full static computation is no longer possible.
The non-linear predicate arguments must thus
be identified at parse time, when the whole
grammar is available. This rather trivial algorithm
will not be described here, but it should be
noted that it is worth doing since in practice it
prevents decoration computations which can take
an exponential time.
4 Conclusion
In this paper we have shown how PRCGs can
handle DAGs as an input. If we consider the linear
PRCG, the one equivalent to LCFRS, the parsing
time remains polynomial. Moreover, input DAGs
necessitate only rather cosmetic modifications in
the standard parser.
In the non-linear case, the standard parser may
produce illegal parses in its output shared parse
forest. It may even produce a (non-empty) shared
parse forest though no sentences of the input DAG
are in the language defined by our non-linear
PRCG. We have proposed a method which uses
the (slightly modified) standard parser but prunes,
within extra passes, its output forest and leaves all
and only valid parses. During these extra bottom-
up and top-down walks, this pruning involves
the computation of finite languages by means of
concatenation, union and intersection operations.
The sentences of these finite languages are always
substrings of the words of the input DAG D.
We choose to represent these intermediate finite
languages by DAGs instead of sets of strings
because the size of a DAG is, at worst, of the same
order as the size of a set of strings but it could, in
some cases, be exponentially smaller.
However, the time taken by this extra pruning
pass cannot be guaranteed to be polynomial,
as expected from previously known complexity
results (Bertsch and Nederhof, 2001). We have
shown an example in which pruning takes an
exponential time and space in the size of D. The
deep reason comes from the fact that if L is a
finite (regular) language defined by some minimal
DAG D, there are cases where a sub-language of
264
0 1 2 h? 1 h
a
b
a
b
a
b
Figure 6: Input DAG associated with the language (a|b)h, h > 0.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
.
.
.
.
.
.
.
.
.
.
.
.
2k+2 ? 4
2k+2 ? 3
2k+2 ? 2
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
Figure 7: DAG associated with the language of nested well-parenthesized strings of length 2k.
L may require to be defined by a DAG whose size
is an exponential in the size of D. Of course this
combinatorial explosion is not a fatality, and we
may wonder whether, in the particular case of NLP
it will practically occur?
References
Franois Barthe?lemy, Pierre Boullier, Philippe De-
schamp, and ?Eric de la Clergerie. 2001. Guided
parsing of range concatenation languages. In Pro-
ceedings of the 39th Annual Meeting of the Associ-
ation for Comput. Linguist. (ACL?01), pages 42?49,
University of Toulouse, France.
Eberhard Bertsch and Mark-Jan Nederhof. 2001. On
the complexity of some extensions of rcg parsing. In
Proceedings of IWPT?01, Beijing, China.
Pierre Boullier, 2004. New Developments in Pars-
ing Technology, volume 23 of Text, Speech and
Language Technology, chapter Range Concatena-
tion Grammars, pages 269?289. Kluwer Academic
Publishers, H. Bunt, J. Carroll, and G. Satta edition.
Jeffrey D. Hopcroft and John E. Ullman. 1979.
Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley, Reading, Mass.
Bernard Lang. 1994. Recognition can be harder than
parsing. Computational Intelligence, 10(4):486?
494.
K. Vijay-Shanker, David Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings of the 25th Meeting of the Association
for Comput. Linguist. (ACL?87), pages 104?111,
Stanford University, CA.
265
Proceedings of NAACL-HLT 2013, pages 239?247,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Enforcing Subcategorization Constraints in a Parser Using Sub-parses
Recombining
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Beno??t Sagot
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
Alpage, INRIA & Universite? Paris-Diderot, Paris, France
?Computer Engineering Department, Faculty of Engineering,
University of Guilan, Rasht, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
benoit.sagot@inria.fr)
Abstract
Treebanks are not large enough to adequately
model subcategorization frames of predica-
tive lexemes, which is an important source of
lexico-syntactic constraints for parsing. As
a consequence, parsers trained on such tree-
banks usually make mistakes when selecting
the arguments of predicative lexemes. In this
paper, we propose an original way to correct
subcategorization errors by combining sub-
parses of a sentence S that appear in the list
of the n-best parses of S. The subcatego-
rization information comes from three differ-
ent resources, the first one is extracted from
a treebank, the second one is computed on a
large corpora and the third one is an existing
syntactic lexicon. Experiments on the French
Treebank showed a 15.24% reduction of er-
roneous subcategorization frames (SF) selec-
tions for verbs as well as a relative decrease of
the error rate of 4% Labeled Accuracy Score
on the state of the art parser on this treebank.
1 Introduction
Automatic syntactic parsing of natural languages
has witnessed many important changes in the last
fifteen years. Among these changes, two have mod-
ified the nature of the task itself. The first one is
the availability of treebanks such as the Penn Tree-
bank (Marcus et al, 1993) or the French Treebank
(Abeille? et al, 2003), which have been used in the
parsing community to train stochastic parsers, such
as (Collins, 1997; Petrov and Klein, 2008). Such
work remained rooted in the classical language the-
oretic tradition of parsing, generally based on vari-
ants of generative context free grammars. The sec-
ond change occurred with the use of discriminative
machine learning techniques, first to rerank the out-
put of a stochastic parser (Collins, 2000; Charniak
and Johnson, 2005) and then in the parser itself (Rat-
naparkhi, 1999; Nivre et al, 2007; McDonald et al,
2005a). Such parsers clearly depart from classical
parsers in the sense that they do not rely anymore on
a generative grammar: given a sentence S, all pos-
sible parses for S1 are considered as possible parses
of S. A parse tree is seen as a set of lexico-syntactic
features which are associated to weights. The score
of a parse is computed as the sum of the weights of
its features.
This new generation of parsers allows to reach
high accuracy but possess their own limitations. We
will focus in this paper on one kind of weakness
of such parser which is their inability to properly
take into account subcategorization frames (SF) of
predicative lexemes2, an important source of lexico-
syntactic constraints. The proper treatment of SF is
actually confronted to two kinds of problems: (1)
the acquisition of correct SF for verbs and (2) the
integration of such constraints in the parser.
The first problem is a consequence of the use of
treebanks for training parsers. Such treebanks are
composed of a few thousands sentences and only a
small subpart of acceptable SF for a verb actually
1Another important aspect of the new parsing paradigm is
the use of dependency trees as a means to represent syntactic
structure. In dependency syntax, the number of possible syn-
tactic trees associated to a sentence is bounded, and only de-
pends on the length of the sentence, which is not the case with
syntagmatic derivation trees.
2We will concentrate in this paper on verbal SF.
239
occur in the treebank.
The second problem is a consequence of the pars-
ing models. For algorithmic complexity as well as
data sparseness reasons, the parser only considers
lexico-syntactic configurations of limited domain of
locality (in the parser used in the current work, this
domain of locality is limited to configurations made
of one or two dependencies). As described in more
details in section 2, SF often exceed in scope such
domains of locality and are therefore not easy to in-
tegrate in the parser. A popular method for intro-
ducing higher order constraints in a parser consist in
reranking the n best output of a parser as in (Collins,
2000; Charniak and Johnson, 2005). The reranker
search space is restricted by the output of the parser
and high order features can be used. One draw-
back of the reranking approach is that correct SF for
the predicates of a sentence can actually appear in
different parse trees. Selecting complete trees can
therefore lead to sub-optimal solutions. The method
proposed in this paper merges parts of different trees
that appear in an n best list in order to build a new
parse.
Taking into account SF in a parser has been a ma-
jor issue in the design of syntactic formalisms in the
eighties and nineties. Unification grammars, such
as Lexical Functional Grammars (Bresnan, 1982),
Generalized Phrase Structure Grammars (Gazdar et
al., 1985) and Head-driven Phrase Structure Gram-
mars (Pollard and Sag, 1994), made SF part of the
grammar. Tree Adjoining Grammars (Joshi et al,
1975) proposed to extend the domain of locality of
Context Free Grammars partly in order to be able
to represent SF in a generative grammar. More
recently, (Collins, 1997) proposed a way to intro-
duce SF in a probabilistic context free grammar and
(Arun and Keller, 2005) used the same technique
for French. (Carroll et al, 1998), used subcate-
gorization probabilities for ranking trees generated
by unification-based phrasal grammar and (Zeman,
2002) showed that using frame frequency in a de-
pendency parser can lead to a significant improve-
ment of the performance of the parser.
The main novelties of the work presented here is
(1) the way a new parse is built by combining sub-
parses that appear in the n best parse list and (2)
the use of three very different resources that list the
possible SF for verbs.
The organization of the paper is the following: in
section 2, we will briefly describe the parsing model
that we will be using for this work and give accuracy
results on a French corpus. Section 3 will describe
three different resources that we have been using to
correct SF errors made by the parser and give cov-
erage results for these resources on a development
corpus. Section 4 will propose three different ways
to take into account, in the parser, the resources de-
scribed in section 3 and give accuracy results. Sec-
tion 5 concludes the paper.
2 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005b) imple-
mentation of (Bohnet, 2010). The parser was trained
on the French Treebank (Abeille? et al, 2003) which
was transformed into dependency trees by (Candito
et al, 2009). The size of the treebank and its de-
composition into train, development and test sets are
represented in table 1.
nb of sentences nb of tokens
TRAIN 9 881 278 083
DEV 1 239 36 508
TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The parser gave state of the art results for parsing
of French, reported in table 2. Table 2 reports the
standard Labeled Accuracy Score (LAS) and Unla-
beled Accuracy Score (UAS) which is the ratio of
correct labeled (for LAS) or unlabeled (for UAS) de-
pendencies in a sentence. We also defined a more
specific measure: the SF Accuracy Score (SAS)
which is the ratio of verb occurrences that have been
paired with the correct SF by the parser. We have
introduced this quantity in order to measure more
accurately the impact of the methods described in
this paper on the selection of a SF for the verbs of a
sentence.
TEST DEV
SAS 80.84 79.88
LAS 88.88 88.53
UAS 90.71 90.37
Table 2: Subcategorization Frame Accuracy, Labeled and
Unlabeled Accuracy Score on TEST and DEV.
240
We have chosen a second order graph parser in
this work for two reasons. The first is that it is the
parsing model that obtained the best results on the
French Treebank. The second is that it allows us
to impose structural constraints in the solution of
the parser, as described in (Mirroshandel and Nasr,
2011), a feature that will reveal itself precious when
enforcing SF in the parser output.
3 The Resources
Three resources have been used in this work in order
to correct SF errors. The first one has been extracted
from a treebank, the second has been extracted from
an automatically parsed corpus that is several order
of magnitude bigger than the treebank. The third one
has been extracted from an existing lexico-syntactic
resource. The three resources are respectively de-
scribed in sections 3.2, 3.3 and 3.4. Before describ-
ing the resources, we describe in details, in section
3.1 our definition of SF. In section 3.5, we evalu-
ate the coverage of these resources on the DEV cor-
pus. Coverage is an important characteristic of a re-
source: in case of an SF error made by the parser, if
the correct SF that should be associated to a verb, in
a sentence, does not appear in the resource, it will be
impossible to correct the error.
3.1 Subcat Frames Description
In this work, a SF is defined as a couple (G,L)
where G is the part of speech tag of the element that
licenses the SF. This part of speech tag can either
be a verb in infinitive form (VINF), a past participle
(VPP), a finite tense verb (V) or a present participle
(VPR). L is a set of couples (f, c) where f is a syn-
tactic function tag chosen among a set F and c is
a part of speech tag chosen among the set C. Cou-
ple (f, c) indicates that function f can be realized as
part of speech tag c. Sets F and C are respectively
displayed in top and bottom tables of figure 1. An
anchored SF (ASF) is a couple (v, S) where v is a
verb lemma and S is a SF, as described above.
A resource is defined as a collection of ASF
(v, S), each associated to a count c, to represent the
fact that verb v has been seen with SF S c times. In
the case of the resource extracted form an existing
lexicon (section 3.4), the notion of count is not ap-
plicable and we will consider that it is always equal
SUJ subject
OBJ object
A OBJ indirect object introduced by the preposition a`
DE OBJ indirect object introduced by the preposition de
P OBJ indirect object introduced by another preposition
ATS attribute of the subject
ATO attribute of the direct object
ADJ adjective
CS subordinating conjunction
N noun
V verb finite tense
VINF verb infinitive form
VPP verb past participle
VPR verb present participle
Figure 1: Syntactic functions of the arguments of the SF
(top table). Part of speech tags of the arguments of the SF
(bottom table)
to one.
Below is an example of three ASF for the french
verb donner (to give). The first one is a transitive SF
where both the subject and the object are realized as
nouns as in Jean donne un livre (Jean gives a book.).
The second one is ditransitive, it has both a direct
object and an indirect one introduced by the prepo-
sition a` as in Jean donne un livre a` Marie. (Jean
gives a book to Marie). The third one corresponds
to a passive form as in le livre est donne? a` Marie par
Jean (The book is given to Marie by Jean).
(donner,(V,(suj,N),(obj,N)))
(donner,(V,(suj,N),(obj,N),(a_obj,N)))
(donner,(VPP,(suj,N),(aux_pass,V),
(a_obj,N),(p_obj,N)))
One can note that when an argument corresponds
to an indirect dependent of the verb (introduced ei-
ther by a preposition or a subordinating conjunc-
tion), we do not represent in the SF, the category
of the element that introduces the argument, but the
category of the argument itself, a noun or a verb.
Two important choices have to be made when
defining SF. The first one concerns the dependents
of the predicative element that are in the SF (argu-
ment/adjunct distinction) and the second is the level
of abstraction at which SF are defined.
In our case, the first choice is constrained by the
treebank annotation guidelines. The FTB distin-
guishes seven syntactic functions which can be con-
sidered as arguments of a verb. They are listed in
the top table of figure 1. Most of them are straight-
241
forward and do not deserve an explanation. Some-
thing has to be said though on the syntactic function
P OBJ which is used to model arguments of the verb
introduced by a preposition that is neither a` nor de,
such as the agent in passive form, which is intro-
duced by the preposition par.
We have added in the SF two elements that do not
correspond to arguments of the verb: the reflexive
pronoun, and the passive auxiliary. The reason for
adding these elements to the SF is that their pres-
ence influences the presence or absence of some ar-
guments of the verb, and therefore the SF.
The second important choice that must be made
when defining SF is the level of abstraction, or, in
other words, how much the SF abstracts away from
its realization in the sentence. In our case, we have
used two ways to abstract away from the surface re-
alization of the SF. The first one is factoring sev-
eral part of speech tags. We have factored pronouns,
common nouns and proper nouns into a single cat-
egory N. We have not gathered verbs in different
modes into one category since the mode of the verb
influences its syntactic behavior and hence its SF.
The second means of abstraction we have used is
the absence of linear order between the arguments.
Taking into account argument order increases the
number of SF and, hence, data sparseness, without
adding much information for selecting the correct
SF, this is why we have decided to to ignore it. In
our second example above, each of the three argu-
ments can be realized as one out of eight parts of
speech that correspond to the part of speech tag N
and the 24 possible orderings are represented as one
canonical ordering. This SF therefore corresponds
to 12 288 possible realizations.
3.2 Treebank Extracted Subcat Frames
This resource has been extracted from the TRAIN
corpus. At a first glance, it may seen strange to ex-
tract data from the corpus that have been used for
training our parser. The reason is that, as seen in
section 1, SF are not directly modeled by the parser,
which only takes into account subtrees made of, at
most, two dependencies.
The extraction procedure of SF from the treebank
is straightforward : the tree of every sentence is vis-
ited and, for every verb of the sentence, its daughters
are visited, and, depending whether they are consid-
ered as arguments of the verb (with respect to the
conventions or section 3.1), they are added to the SF.
The number of different verbs extracted, as well as
the number of different SF and the average number
of SF per verb are displayed in table 3. Column T
(for Train) is the one that we are interested in here.
T L A0 A5 A10
nb of verbs 2058 7824 23915 4871 3923
nb of diff SF 666 1469 12122 2064 1355
avg. nb of SF 4.83 52.09 14.26 16.16 13.45
Table 3: Resources statistics
The extracted resource can directly be compared
with the TREELEX resource (Kupsc and Abeille?,
2008), which has been extracted from the same tree-
bank. The result that we obtain is different, due to
the fact that (Kupsc and Abeille?, 2008) have a more
abstract definition of SF. As a consequence, they de-
fine a smaller number of SF: 58 instead of 666 in
our case. The smaller number of SF yields a smaller
average number of SF per verb: 1.72 instead of 4.83
in our case.
3.3 Automatically computed Subcat Frames
The extraction procedure described above has been
used to extract ASF from an automatically parsed
corpus. The corpus is actually a collection of three
corpora of slightly different genres. The first one
is a collection of news reports of the French press
agency Agence France Presse, the second is a col-
lection of newspaper articles from a local French
newspaper : l?Est Re?publicain. The third one is
a collection of articles from the French Wikipedia.
The size of the different corpora are detailed in ta-
ble 4.
The corpus was first POS tagged with the MELT
tagger (Denis and Sagot, 2010), lemmatized with the
MACAON tool suite (Nasr et al, 2011) and parsed
in order to get the best parse for every sentence.
Then the ASF have been extracted.
The number of verbs, number of SF and average
number of SF per verb are represented in table 3,
in column A0 (A stands for Automatic). As one
can see, the number of verbs and SF are unrealis-
tic. This is due to the fact that the data that we ex-
tract SF from is noisy: it consists of automatically
produced syntactic trees which contain errors (recall
242
CORPUS Sent. nb. Tokens nb.
AFP 2 041 146 59 914 238
EST REP 2 998 261 53 913 288
WIKI 1 592 035 33 821 460
TOTAL 5 198 642 147 648 986
Table 4: sizes of the corpora used to collect SF
that the LAS on the DEV corpus is 88, 02%). There
are two main sources of errors in the parsed data: the
pre-processing chain (tokenization, part of speech
tagging and lemmatization) which can consider as
a verb a word that is not, and, of course, parsing
errors, which tend to create crazy SF. In order to
fight against noise, we have used a simple thresh-
olding: we only collect ASF that occur more than a
threshold i. The result of the thresholding appears
in columns A5 and A10 , where the subscript is the
value of the threshold. As expected both the number
of verbs and SF decrease sharply when increasing
the value of the threshold.
Extracting SF for verbs from raw data has been
an active direction of research for a long time, dat-
ing back at least to the work of (Brent, 1991) and
(Manning, 1993). More recently (Messiant et al,
2008) proposed such a system for French verbs. The
method we use for extracting SF is not novel with
respect to such work. Our aim was not to devise
new extraction techniques but merely to evaluate the
resource produced by such techniques for statistical
parsing.
3.4 Using an existing resource
The third resource that we have used is the Lefff
(Lexique des formes fle?chies du franc?ais ? Lexicon
of French inflected form), a large-coverage syntac-
tic lexicon for French (Sagot, 2010). The Lefff was
developed in a semi-automatic way: automatic tools
were used together with manual work. The latest
version of the Lefff contains 10,618 verbal entries
for 7,835 distinct verbal lemmas (the Lefff covers all
categories, but only verbal entries are used in this
work).
A sub-categorization frame consists in a list of
syntactic functions, using an inventory slightly more
fine-grained than in the French Treebank, and for
each of them a list of possible realizations (e.g.,
noun phrase, infinitive clause, or null-realization if
the syntactic function is optional).
For each verbal lemma, we extracted all sub-
categorization frames for each of the four verbal
part-of-speech tags (V, VINF, VPR, VPP), thus cre-
ating an inventory of SFs in the same sense and for-
mat as described in Section 3.1. Note that such SFs
do not contain alternatives concerning the way each
syntactic argument is realized or not: this extraction
process includes a de-factorization step. Its output,
hereafter L, contains 801,246 distinct (lemma, SF)
pairs.
3.5 Coverage
In order to be able to correct SF errors, the three
resources described above must possess two impor-
tant characteristics: high coverage and high accu-
racy. Coverage measures the presence, in the re-
source, of the correct SF of a verb, in a given sen-
tence. Accuracy measures the ability of a resource
to select the correct SF for a verb in a given context
when several ones are possible.
We will give in this section coverage result, com-
puted on the DEV corpus. Accuracy will be de-
scribed and computed in section 4. The reason why
the two measures are not described together is due
to the fact that coverage can be computed on a ref-
erence corpus while accuracy must be computed on
the output of a parser, since it is the parser that will
propose different SF for a verb in a given context.
Given a reference corpus C and a resource R,
two coverage measures have been computed, lexi-
cal coverage, which measures the ratio of verbs of C
that appear in R and syntactic coverage, which mea-
sures the ratio of ASF of C that appear in R. Two
variants of each measures are computed: on types
and on occurrences. The values of these measures
computed on the DEV corpus are summarized in ta-
ble 5.
T L A0 A5 A10
Lex. types 89.56 99.52 99.52 98.56 98.08
cov. occ 96.98 99.85 99.85 99.62 99.50
Synt. types 62.24 78.15 95.78 91.08 88.84
cov. occ 73.54 80.35 97.13 93.96 92.39
Table 5: Lexical and syntactic coverage of the three re-
sources on DEV
The figures of table 5 show that lexical cover-
age of the three resources is quite high, ranging
243
from 89.56 to 99.52 when computed on types and
from 96.98 to 99.85 when computed on occurrences.
The lowest coverage is obtained by the T resource,
which does not come as a surprise since it is com-
puted on a rather small number of sentences. It
is also interesting to note that lexical coverage of
A does not decrease much when augmenting the
threshold, while the size of the resource decreases
dramatically (as shown in table 3). This validates
the hypothesis that the resource is very noisy and
that a simple threshold on the occurrences of ASF is
a reasonable means to fight against noise.
Syntactic coverage is, as expected, lower than lex-
ical coverage. The best results are obtained by A0:
95.78 on types and 97.13 on occurrences. Thresh-
olding on the occurrences of anchored SF has a big-
ger impact on syntactic coverage than it had on lexi-
cal coverage. A threshold of 10 yields a coverage of
88.84 on types and 92.39 on occurrences.
4 Integrating Subcat Frames in the Parser
As already mentioned in section 1, SF usually ex-
ceed the domain of locality of the structures that are
directly modeled by the parser. It is therefore dif-
ficult to integrate directly SF in the model of the
parser. In order to circumvent the problem, we have
decided to work on the n-best output of the parser:
we consider that a verb v, in a given sentence S,
can be associated to any of the SF that v licenses in
one of the n-best trees. The main weakness of this
method is that an SF error can be corrected only if
the right SF appears at least in one of the n-best parse
trees.
In order to estimate an upper bound of the SAS
that such methods can reach (how many SF errors
can actually be corrected), we have computed the
oracle SAS on the 100 best trees of the DEV corpus
DEV (for how many verbs the correct SF appears
in at least one of the n-best parse trees). The oracle
score is equal to 95.16, which means that for 95.16%
of the verb occurrences of the DEV, the correct SF
appears somewhere in the 100-best trees. 95.16 is
therefore the best SAS that we can reach. Recall
that the baseline SAS is equal to 79.88% the room
for progress is therefore equal to 15.28% absolute.
Three experiments are described below. In the
first one, section 4.1, a simple technique, called Post
Processing is used. Section 4.2 describes a second
technique, called Double Parsing, which is a is a
refinement of Post Processing. Both sections 4.1
and 4.2 are based on single resources. Section 4.3
proposes a simple way to combine the different re-
sources.
4.1 Post Processing
The post processing method (PP) is the simplest one
that we have tested. It takes as input the different
ASF that occur in the n-best output of the parser as
well as a resource R. Given a sentence, let?s note
T1 . . . Tn the trees that appear in the n-best output
of the parser, in decreasing order of their score. For
every verb v of the sentence, we note S(v) the set
of all the SF associated to v that appear in the trees
T1 . . . Tn.
Given a verb v and a SF s, we define the following
functions:
C(v, s) is the number of occurrences of the ASF
(v, s) in the trees T1 . . . Tn.
F(v) is the SF associated to v in T1
CR(v, s) the number of occurrences of the ASF
(v, s) in the resource R.
We define a selection function as a function that
selects a SF for a given verb in a given sentence.
A selection function has to take into account the in-
formation given by the resource (whether an SF is
acceptable/frequent for a given verb) as well as the
information given by the parser (whether the parser
has a strong preference to associate a given SF to a
given verb).
In our experiments, we have tested two simple
selection functions. ?R which selects the first SF
s ? S(v), such that CR(v, s) > 0 when traversing
the trees T1 . . . Tn in the decreasing order of score
(best tree first).
The second function, ?R(v) compares the most
frequent SF for v in the resourceRwith the SF of the
first parse. If the ratio of the number of occurrences
in the n-best of the former and the latter is above a
threshold ?, the former is selected. More formally:
?R(v) =
?
????
????
s? = argmaxs?S(v) CR(v, s)
if C(v,s?)C(v,F(v)) > ?
F(v)
otherwise
244
The coefficient? has been optimized on DEV cor-
pus. Its value is equal to 2.5 for the Automatic re-
source, 2 for the Train resource and 1.5 for the Lefff.
The construction of the new solution proceeds as
follows: for every verb v of the sentence, a SF is se-
lected with the selection function. It is important to
note, at this point, that the SF selected for different
verbs of the sentence can pertain to different parse
trees. The new solution is built based on tree T1. For
every verb v, its arguments are potentially modified
in agreement with the SF selected by the selection
function. There is no guarantee at this point that the
solution is well formed. We will return to this prob-
lem in section 4.2.
We have evaluated the PP method with different
selection functions on the TEST corpus. The results
of applying function ?R were more successful. As
a result we just report the results of this function in
table 6. Different levels of thresholding for resource
A gave almost the same results, we therefore used
A10 which is the smallest one.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.14 89.03 89.03
UAS 90.71 90.91 90.81 90.82
Table 6: LAS and UAS on TEST using PP
The results of table 6 show two interesting facts.
First, the SAS is improved, it jumps from 80.84 to
83.11. PP therefore corrects some SF errors made
by the parser. It must be noted however that this im-
provement is much lower than the oracle score. The
second interesting fact is the very moderate increase
of both LAS and UAS. This is due to the fact that
the number of dependencies modified is small with
respect to the total number of dependencies. The
impact on LAS and UAS is therefore weak.
The best results are obtained with resource T . Al-
though the coverage of T is low, the resource is very
close to the train data, this fact probably explains the
good results obtained with this resource.
It is interesting, at this point, to compare our
method with a reranking approach. In order to do so,
we have compared the upper bound of the number of
SF errors that can be corrected when using rerank-
ing and our approach. The results of the comparison
computed on a list of 100 best trees is reported in
table 7 which shows the ratio of subcat frame errors
that could be corrected with a reranking approach
and the ratio of errors sub-parse recombining could
reach.
DEV TEST
reranking 53.9% 58.5%
sub-parse recombining 75.5% 76%
Table 7: Correction rate for subcat frames errors with dif-
ferent methods
Table 7 shows that combining sub-parses can, in
theory, correct a much larger number of wrong SF
assignments than reranking.
4.2 Double Parsing
The post processing method shows some improve-
ment over the baseline. But it has an important draw-
back: it can create inconsistent parses. Recall that
the parser we are using is based on a second order
model. In other words, the score of a dependency
depends on some neighboring dependencies. When
building a new solution, the post processing method
modifies some dependencies independently of their
context, which may give birth to very unlikely con-
figurations.
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies. The new solution
is therefore the optimal solution that preserves the
dependencies modified by the PP method.
The double parsing (DP) method is therefore a
three stage method. First, sentence S is parsed, pro-
ducing the n-best parses. Then, the post processing
method is used, modifying the first best parse. Let?s
note D the set of dependencies that were changed in
this process. In the last stage, a new parse is pro-
duced, that preserves D.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.30 89.25 89.31
UAS 90.71 91.07 91.05 91.08
Table 8: LAS and UAS on TEST using DP
245
The results of DP on TEST are reported in table
8. SAS did not change with respect to PP, because
DP keeps the SF selected by PP. As expected DP
does increase LAS and UAS. Recomputing an op-
timal solution therefore increases the quality of the
parses. Table 8 also shows that the three resources
get alost the same LAS and UAS although SAS is
better for resource T.
4.3 Combining Resources
Due to the different generation techniques of our
three resources, another direction of research is
combining them. We did different experiments con-
cerning all possible combination of resources: A and
L (AL), T and L (TL), T and A (TA), and all tree
(TAL) resources. The results of these combinations
for PP and DP methods are shown in tables 9 and
10, respectively.
The resource are combined in a back-off schema:
we search for a candidate ASF in a first resource. If
it is found, the search stops. Otherwise, the next re-
source(s) are probed. One question that arises is:
which sequence is the optimal one for combining
the resources. To answer this question, we did sev-
eral experiments on DEV set. Our experiments have
shown that it is better to search T resource, then
A, and, eventually, L. The results of this combining
method, using PP are reported in table 9. The best
results are obtained for the TL combination. The
SAS jumps from 83.11 to 83.76. As it was the case
with single resources, the LAS and UAS increase is
moderate.
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.03 89.22 89.19 89.19
UAS 90.71 90.79 90.98 90.95 90.95
Table 9: LAS and UAS on TEST using PP with resource
combination
With DP (table 9), the order of resource combina-
tion is exactly the same as with PP. As was the case
with single resources, DP has a positive, but moder-
ate, impact on LAS and UAS.
The results of tables 9 and 10 do not show con-
siderable improvement over single resources. This
might be due to the large intersection between our
resources. In other words, they do not have comple-
mentary information, and their combination will not
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.22 89.31 89.34 89.34
UAS 90.71 91.02 91.05 91.08 91.09
Table 10: LAS and UAS on TEST using DP with resource
combination
introduce much information. Another possible rea-
son for this result is the combination technique used.
More sophisticated techniques might yield better re-
sults.
5 Conclusions
Subcategorization frames for verbs constitute a rich
source of lexico-syntactic information which is hard
to integrate in graph based parsers. In this paper, we
have used three different resources for subcatego-
rization frames. These resources are from different
origins with various characteristics. We have pro-
posed two different methods to introduce the useful
information from these resources in a second order
model parser. We have conducted different exper-
iments on French Treebank that showed a 15.24%
reduction of erroneous SF selections for verbs. Al-
though encouraging, there is still plenty of room
for better results since the oracle score for 100 best
parses is equal to 95.16% SAS and we reached
83.76%. Future work will concentrate on more elab-
orate selection functions as well as more sophisti-
cated ways to combine the different resources.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
EDYLEX (ANR-08-CORD-009).
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 306?313.
Association for Computational Linguistics.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
246
Michael Brent. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceedings
of ACL.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
J. Carroll, G. Minnen, and T. Briscoe. 1998. Can sub-
categorisation probabilities help a statistical parser?
Arxiv preprint cmp-lg/9806013.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure Gram-
mar. Harvard University Press.
Aravind Joshi, Leon Levy, and M Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10:136?163.
Anna Kupsc and Anne Abeille?. 2008. Treelex: A subcat-
egorisation lexicon for french verbs. In Proceedings of
the First International Conference on Global Interop-
erability for Language Resources.
Christopher Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of ACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
C. Messiant, A. Korhonen, T. Poibeau, et al 2008.
Lexschem: A large subcategorization lexicon for
french verbs. In Proceedings of the Language Re-
sources and Evaluation Conference.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Slav Petrov and Dan Klein. 2008. Discriminative Log-
Linear Grammars with Latent Variables. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20 (NIPS),
pages 1153?1160, Cambridge, MA. MIT Press.
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammmar. CSLI Series. University of
Chicago Press.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1):151?175.
Beno??t Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for french. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 2744?2751, Valletta, Malta.
D. Zeman. 2002. Can subcategorization help a statistical
dependency parser? In Proceedings of the 19th in-
ternational conference on Computational linguistics-
Volume 1, pages 1?7. Association for Computational
Linguistics.
247
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 525?533,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimal rank reduction
for Linear Context-Free Rewriting Systems with Fan-Out Two
Benot Sagot
INRIA & Universite? Paris 7
Le Chesnay, France
benoit.sagot@inria.fr
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
Linear Context-Free Rewriting Systems
(LCFRSs) are a grammar formalism ca-
pable of modeling discontinuous phrases.
Many parsing applications use LCFRSs
where the fan-out (a measure of the dis-
continuity of phrases) does not exceed 2.
We present an efficient algorithm for opti-
mal reduction of the length of production
right-hand side in LCFRSs with fan-out at
most 2. This results in asymptotical run-
ning time improvement for known parsing
algorithms for this class.
1 Introduction
Linear Context-Free Rewriting Systems
(LCFRSs) have been introduced by Vijay-
Shanker et al (1987) for modeling the syntax
of natural language. The formalism extends the
generative capacity of context-free grammars, still
remaining far below the class of context-sensitive
grammars. An important feature of LCFRSs is
their ability to generate discontinuous phrases.
This has been recently exploited for modeling
phrase structure treebanks with discontinuous
constituents (Maier and S?gaard, 2008), as well as
non-projective dependency treebanks (Kuhlmann
and Satta, 2009).
The maximum number f of tuple components
that can be generated by an LCFRS G is called
the fan-out of G, and the maximum number r of
nonterminals in the right-hand side of a production
is called the rank of G. As an example, context-
free grammars are LCFRSs with f = 1 and r
given by the maximum length of a production
right-hand side. Tree adjoining grammars (Joshi
and Levy, 1977) can also be viewed as a special
kind of LCFRS with f = 2, since each auxil-
iary tree generates two strings, and with r given
by the maximum number of adjunction and sub-
stitution sites in an elementary tree. Beyond tree
adjoining languages, LCFRSs with f = 2 can
also generate languages in which pair of strings
derived from different nonterminals appear in so-
called crossing configurations. It has recently been
observed that, in this way, LCFRSs with f = 2
can model the vast majority of data in discontinu-
ous phrase structure treebanks and non-projective
dependency treebanks (Maier and Lichte, 2009;
Kuhlmann and Satta, 2009).
Under a theoretical perspective, the parsing
problem for LCFRSs with f = 2 is NP-complete
(Satta, 1992), and in known parsing algorithms
the running time is exponentially affected by the
rank r of the grammar. Nonetheless, in natu-
ral language parsing applications, it is possible to
achieve efficient, polynomial parsing if we suc-
ceed in reducing the rank r (number of nontermi-
nals in the right-hand side) of individual LCFRSs?
productions (Kuhlmann and Satta, 2009). This
process is called production factorization. Pro-
duction factorization is very similar to the reduc-
tion of a context-free grammar production into
Chomsky normal form. However, in the LCFRS
case some productions might not be reducible to
r = 2, and the process stops at some larger value
for r, which in the worst case might as well be the
rank of the source production (Rambow and Satta,
1999).
Motivated by parsing efficiency, the factoriza-
tion problem for LCFRSs with f = 2 has at-
tracted the attention of many researchers in recent
years. Most of the literature has been focusing on
binarization algorithms, which attempt to find a re-
duction to r = 2 and return a failure if this is not
possible. Go?mez-Rodr??guez et al (2009) report a
general binarization algorithm for LCFRS which,
in the case of f = 2, works in time O(|p|7), where
|p| is the size of the input production. A more ef-
ficient binarization algorithm for the case f = 2 is
presented in (Go?mez-Rodr??guez and Satta, 2009),
working in time O(|p|).
525
In this paper we are interested in general factor-
ization algorithms, i.e., algorithms that find factor-
izations with the smallest possible rank (not nec-
essarily r = 2). We present a novel technique that
solves the general factorization problem in time
O(|p|2) for LCFRSs with f = 2.
Strong generative equivalence results between
LCFRS and other finite copying parallel rewrit-
ing systems have been discussed in (Weir, 1992)
and in (Rambow and Satta, 1999). Through these
equivalence results, we can transfer the factoriza-
tion techniques presented in this article to other
finite copying parallel rewriting systems.
2 LCFRSs
In this section we introduce the basic notation for
LCFRS and the notion of production factoriza-
tion.
2.1 Definitions
Let ?T be a finite alphabet of terminal symbols.
As usual, ? ?T denotes the set of all finite strings
over ?T , including the empty string ?. For in-
teger k ? 1, (? ?T )k denotes the set of all tuples
(w1, . . . , wk) of strings wi ? ? ?T . In what follows
we are interested in functions mapping several tu-
ples of strings in ? ?T into tuples of strings in ? ?T .
Let r and f be two integers, r ? 0 and f ? 1.
We say that a function g has rank r if there exist
integers fi ? 1, 1 ? i ? r, such that g is defined
on (? ?T )f1 ? (? ?T )f2 ? ? ? ? ? (? ?T )fr . We also say
that g has fan-out f if the range of g is a subset of
(? ?T )f . Let yh, xij , 1 ? h ? f , 1 ? i ? r and
1 ? j ? fi, be string-valued variables. A func-
tion g as above is said to be linear regular if it is
defined by an equation of the form
g(?x11, . . . , x1f1?, . . . , ?xr1, . . . , xrfr?) =
= ?y1, . . . , yf ?, (1)
where ?y1, . . . , yf ? represents some grouping into
f sequences of all and only the variables appear-
ing in the left-hand side of (1) (without repeti-
tions) along with some additional terminal sym-
bols (with possible repetitions).
For a mathematical definition of LCFRS we re-
fer the reader to (Weir, 1992, p. 137). Informally,
in a LCFRS every nonterminal symbol A is asso-
ciated with an integer ?(A) ? 1, called its fan-out,
and it generates tuples in (? ?T )?(A). Productions
in a LCFRS have the form
p : A ? g(B1, B2, . . . , B?(p)),
where ?(p) ? 0, A and Bi, 1 ? i ? ?(p), are non-
terminal symbols, and g is a linear regular func-
tion having rank ?(p) and fan-out ?(A), defined
on (? ?T )?(B1) ?? ? ?? (? ?T )?(B?(p)) and taking val-
ues in (? ?T )?(A). The basic idea underlying the
rewriting relation associated with LCFRS is that
production p applies to any sequence of string tu-
ples generated by the Bi?s, and provides a new
string tuple in (? ?T )?(A) obtained through function
g. We say that ?(p) = ?(A) is the fan-out of p,
and ?(p) is the rank of p.
Example 1 Let L be the language L =
{anbnambmanbnambm |n,m ? 1}. A LCFRS
generating L is defined by means of the nonter-
minals S, ?(S) = 1, and A, ?(A) = 2, and the
productions in figure 1. Observe that nonterminal
A generates all tuples of the form ?anbn, anbn?. 2
Recognition and parsing for a given LCFRS
can be carried out in polynomial time on the length
of the input string. This is usually done by exploit-
ing standard dynamic programming techniques;
see for instance (Seki et al, 1991).1 However, the
polynomial degree in the running time is a mono-
tonically strictly increasing function that depends
on both the rank and the fan-out of the productions
in the grammar. To optimize running time, one can
then recast the source grammar in such a way that
the value of the above function is kept to a min-
imum. One way to achieve this is by factorizing
the productions of a LCFRS, as we now explain.
2.2 Factorization
Consider a LCFRS production of the form
p : A ? g(B1, B2, . . . , B?(p)), where g is
specified as in (1). Let alo C be a subset of
{B1, B2, . . . , B?(p)} such that |C| 6= 0 and |C| 6=
?(p). We let ?C be the alphabet of all variables
xij defined as in (1), for all values of i and j such
that Bi ? C and 1 ? j ? fi. For each i with
1 ? i ? f , we rewrite each string yi in (1) in a
form yi = y?i0zi1y?i1 ? ? ? y?idi?1zidiy
?
idi , with di ? 0,
such that the following conditions are all met:
? each zij , 1 ? j ? di, is a string with one or
more occurrences of variables, all in ?C ;
? each y?ij , 1 ? j ? di ? 1, is a non-empty
string with no occurrences of symbols in ?C ;
? y?0j and y?0di are (possibly empty) strings with
no occurrences of symbols in ?C .
1In (Seki et al, 1991) a syntactic variant of LCFRS is
used, called multiple context-free grammars.
526
S ? gS(A,A), gS(?x11, x12?, ?x21, x22?) = ?x11x21x12x22?;
A ? gA(A), gA(?x11, x12?) = ?ax11b, ax12b?;
A ? g?A(), g?A() = ?ab, ab?.
Figure 1: A LCFRS for language L = {anbnambmanbnambm |n,m ? 1}.
Let c = |C| and c = ?(p) ? |C|. Assume that
C = {Bh1 , . . . , Bhc}, and {B1, . . . , B?(p)} ? C =
{Bh?1 , . . . , Bh?c}. We introduce a fresh nontermi-
nal C with ?(C) = ?fi=1 di and replace pro-
duction p in our grammar by means of the two
new productions p1 : C ? g1(Bh1 , . . . , Bhc) and
p2 : A ? g2(C,Bh?1 , . . . , Bh?c). Functions g1 and
g2 are defined as:
g1(?xh11, . . . , xh1fh1 ?, . . . , ?xhc1, . . . , xhcfhc ?)
= ?z11, ? ? ? , z1d1 , z21, ? ? ? , zfdf ?;
g2(?xh?11, . . . , xh?1fh?1 ?, . . . , ?xh?c1, . . . , xh?cfh?c ?)
= ?y?10, . . . , y?1d1 , y?20, . . . , y?fdf ?.
Note that productions p1 and p2 have rank strictly
smaller than the source production p. Further-
more, if it is possible to choose set C in such a
way that
?f
i=0 di ? f , then the fan-out of p1 and
p2 will be no greater than the fan-out of p.
We can iterate the procedure above as many
times as possible, under the condition that the fan-
out of the productions does not increase.
Example 2 Let us consider the following produc-
tion with rank 4:
A ? gS(B,C,D,E),
gA(?x11, x12?, ?x21, x22?, ?x31, x32?, ?x41, x42?)
= ?x11x21x31x41x12x42, x22x32?.
Applyng the above procedure twice, we obtain a
factorization consisting of three productions with
rank 2 (variables have been renamed to reflect our
conventions):
A ? gA(A1, A2),
gA(?x11, x12?, ?x21, x22?)
= ?x11x21x12, x22?;
A1 ? gA1(B,E),
gA1(?x11, x12?, ?x21, x22?) = ?x11, x21x12x22?;
A2 ? gA2(C,D),
gA2(?x11, x12?, ?x21, x22?) = ?x11x21, x12x22?.
2
The factorization procedure above should be ap-
plied to all productions of a LCFRS with rank
larger than two. This might result in an asymptotic
improvement of the running time of existing dy-
namic programming algorithms for parsing based
on LCFRS.
The factorization technique we have discussed
can also be viewed as a generalization of well-
known techniques for casting context-free gram-
mars into binary forms. These are forms where no
more than two nonterminal symbols are found in
the right-hand side of productions of the grammar;
see for instance (Harrison, 1978). One important
difference is that, while production factorization
into binary form is always possible in the context-
free case, for LCFRS there are worst case gram-
mars in which rank reduction is not possible at all,
as shown in (Rambow and Satta, 1999).
3 A graph-based representation for
LCFRS productions
Rather than factorizing LCFRS productions di-
rectly, in this article we work with a more abstract
representation of productions based on graphs.
From now on we focus on LCFRS whose non-
terminals and productions all have fan-out smaller
than or equal to 2. Consider then a production p :
A ? g(B1, B2, . . . , B?(p)), with ?(A), ?(Bi) ?
2, 1 ? i ? ?(p), and with g defined as
g(?x11, . . . , x1?(B1)?, . . .
. . . , ?x?(p)1, . . . , x?(p)?(B?(p))?)
= ?y1, . . . , y?(A)?.
In what follows, if ?(A) = 1 then ?y1, . . . , y?(A)?
should be read as ?y1? and y1 ? ? ? y?(A) should be
read as y1. The same convention applies to all
other nonterminals and tuples.
We now introduce a special kind of undirected
graph that is associated with a linear order defined
over the set of its vertices. The p-graph associated
with production p is a triple (Vp, Ep,?p) such that
? Vp = {xij | 1 ? i ? ?(p), ?(Bi) = 2, 1 ?
j ? ?(Bi)} is a set of vertices;2
2Here we are overloading symbols xij . It will always be
clear from the context whether xij is a string-valued variable
or a vertex in a p-graph.
527
? Ep = {(xi1, xi2) |xi1, xi2 ? Vp} is a set of
undirected edges;
? for x, x? ? Vp, x ?p x? if x 6= x? and the
(unique) occurrence of x in y1 ? ? ? y?(A) pre-
cedes the (unique) occurrence of x?.
Note that in the above definition we are ignor-
ing all string-valued variables xij associated with
nonterminals Bi with ?(Bi) = 1. This is be-
cause nonterminals with fan-out one can always
be treated as in the context-free grammar case, as
it will be explained later.
Example 3 The p-graph associated with the
LCFRS production in Example 2 is shown in Fig-
ure 2. Circled sets of edges indicate the factoriza-
tion in that example. 2
x21 x31 x41x11
B
CD
E
A1
A2
x42x12 x22 x32
Figure 2: The p-graph associated with the LCFRS
production in Example 2.
We close this section by introducing some ad-
ditional notation related to p-graphs that will be
used throughout this paper. Let E ? Ep be some
set of edges. The cover set for E is defined as
V (E) = {x | (x, x?) ? E} (recall that our edges
are unordered pairs, so (x, x?) and (x?, x) denote
the same edge). Conversely, let V ? Vp be some
set of vertices. The incident set for V is defined
as E(V ) = {(x, x?) | (x, x?) ? Ep, x ? V }.
Assume ?(p) = 2, and let x1, x2 ? Vp. If x1
and x2 do not occur both in the same string y1 or
y2, then we say that there is a gap between x1 and
x2. If x1 ?p x2 and there is no gap between x1
and x2, then we write [x1, x2] to denote the set
{x1, x2} ? {x |x ? Vp, x1 ?p x ?p x2}. For x ?
Vp we also let [x, x] = {x}. A set [x, x?] is called a
range. Let r and r? be two ranges. The pair (r, r?)
is called a tandem if the following conditions are
both satisfied: (i) r?r? is not a range, and (ii) there
exists some edge (x, x?) ? Ep with x ? r and
x? ? r?. Note that the first condition means that r
and r? are disjoint sets and, for any pair of vertices
x ? r and x? ? r?, either there is a gap between x
and x? or else there exists some xg ? Vp such that
x ?p xg ?p x? and xg 6? r ? r?.
A set of edges E ? Ep is called a bundle with
fan-out one if V (E) = [x1, x2] for some x1, x2 ?
Vp, i.e., V (E) is a range. Set E is called a bundle
with fan-out two if V (E) = [x1, x2] ? [x3, x4] for
some x1, x2, x3, x4 ? Vp, and ([x1, x2], [x3, x4])
is a tandem. Note that if E is a bundle with fan-out
two with V (E) = [x1, x2] ? [x3, x4], then neither
E([x1, x2]) nor E([x3, x4]) are bundles with fan-
out one, since there is at least one edge incident
upon a vertex in [x1, x2] and a vertex in [x3, x4].
We also use the term bundle to denote a bundle
with fan-out either one or two.
Intuitively, in a p-graph associated with a
LCFRS production p, a bundle E with fan-out f
and with |E| > 1 identifies a set of nonterminals
C in the right-hand side of p that can be factorized
into a new production. The nonterminals in C are
then replaced in p by a fresh nonterminal C with
fan-out f , as already explained. Our factorization
algorithm is based on efficient methods for the de-
tection of bundles with fan-out one and two.
4 The algorithm
In this section we provide an efficient, recursive
algorithm for the decomposition of a p-graph into
bundles, which corresponds to factorizing the rep-
resented LCFRS production.
4.1 Overview of the algorithm
The basic idea underlying our graph-based algo-
rithm can be described as follows. We want to
compute an optimal hierarchical decomposition of
an input bundle with fan-out 1 or 2. This decom-
position can be represented by a tree, in which
each node N corresponds to a bundle (the root
node corresponds to the input bundle) and the
daughters of N represent the bundles in which N
is immediately decomposed. The decomposition
is optimal in so far as the maximum arity of the
decomposition tree is as small as possible. As
already explained above, this decomposition rep-
resents a factorization of some production p of a
LCFRS, resulting in optimal rank reduction. All
the internal nodes in the decomposition represent
fresh nonterminals that will be created during the
factorization process.
The construction of the decomposition tree is
carried out recursively. For a given bundle with
fan-out 1 or 2, we apply a procedure for decom-
posing this bundle in its immediate sub-bundles
with fan-out 1 or 2, in an optimal way. Then,
528
we recursively apply our procedure to the obtained
sub-bundles. Recursion stops when we reach bun-
dles containing only one edge (which correspond
to the nonterminals in the right-hand side of the
input production). We shall prove that the result is
an optimal decomposition.
The procedure for computing an optimal de-
composition of a bundle F into its immediate sub-
bundles, which we describe in the first part of this
section, can be sketched as follows. First, we iden-
tify and temporarily remove all maximal bundles
with fan-out 1 (Section 4.3). The result is a new
bundle F ? which is a subset of the original bundle,
and has the same fan-out. Next, we identify all
sub-bundles with fan-out 2 in F ? (Section 4.4). We
compute the optimal decomposition of F ?, rest-
ing on the hypothesis that there are no sub-bundles
with fan-out 1. Each resulting sub-bundle is later
expanded with the maximal sub-bundles with fan-
out 1 that have been previously removed. This re-
sults in a ?first level? decomposition of the original
bundle F . We then recursively decompose all in-
dividual sub-bundles of F , including the bundles
with fan-out 1 that have been later attached.
4.2 Backward and forward quantities
For a set V ? Vp of vertices, we write max(V )
(resp. min(V )) the maximum (resp. minimum)
vertex in V w.r.t. the ?p total order.
Let r = [x1, x2] be a range. We write r.left =
x1 and r.right = x2. The set of backward edges
for r is defined as Br = {(x, x?) | (x, x?) ?
Er, x ?p r.left , x? ? r}. The set of for-
ward edges for r is defined symmetrically as Fr =
{(x, x?) | (x, x?) ? Er, x ? r, r.right ?p
x?}. For E ? {Br, Fr} we also define L(E) =
{x | (x, x?) ? E, x ?p x?} and R(E) =
{x? | (x, x?) ? E, x ?p x?}.
Let us assume Br 6= ?. We write r.b.left =
min(L(Br)). Intuitively, r.b.left is the leftmost
vertex of the p-graph that is located at the left
of range r and that is connected to some ver-
tex in r through some edge. Similarly, we write
r.b.right = max(L(Br)). If Br = ?, then we set
r.b.left = r.b.right = ?. Quantities r.b.left and
r.b.right are called backward quantities.
We also introduce local backward quanti-
ties, defined as follows. We write r.lb.left =
min(R(Br)). Intuitively, r.lb.left is the leftmost
vertex among all those vertices in r that are con-
nected to some vertex to the left of r. Similarly,
we write r.lb.right = max(R(Br)). If Br = ?,
then we set r.lb.left = r.lb.right = ?.
We define forward and local forward quanti-
ties in a symmetrical way.
The backward quantities r.b.left and r.b.right
and the local backward quantities r.lb.left and
r.lb.right for all ranges r in the p-graph can
be computed efficiently as follows. We process
ranges in increasing order of size, expanding each
range r by one unit at a time by adding a new
vertex at its right. Backward and local backward
quantities for the expanded range can be expressed
as a function of the same quantities for r. There-
fore if we store our quantities for previously pro-
cessed ranges, each new range can be annotated
with the desired quantities in constant time. This
algorithm runs in time O(n2), where n is the num-
ber of vertices in Vp. This is an optimal result,
since O(n2) is also the size of the output.
We compute in a similar way the forward quan-
tities r.f .left and r.f .right and the local forward
quantities r.lf .left and r.lf .right , this time ex-
panding each range by one unit at its left.
4.3 Bundles with fan-out one
The detection of bundles with fan-out 1 within the
p-graph can be easily performed in O(n2), where
n is the number of its vertices. Indeed, the incident
set E(r) of a range r is a bundle with fan-out one
if and only if r.b.left = r.f .left = ?. This imme-
diately follows from the definitions given in Sec-
tion 4.2. It is therefore possible to check all ranges
the one after the other, once the backward and
forward properties have been computed. These
checks take constant time for each of the ?(n2)
ranges, hence the quadratic complexity.
We now remove from F all bundles with fan-out
1 from the original bundle F . The result is the new
bundle F ?, that has no sub-bundles with fan-out 1.
4.4 Bundles with fan-out two
Efficient detection of bundles with fan-out two in
F ? is considerably more challenging. A direct gen-
eralization of the technique proposed for detecting
bundles with fan-out 1 would use the following
property, that is also a direct corollary of the def-
initions in Section 4.2: the incident set E(r ? r?)
of a tandem (r, r?) is a bundle with fan-out two if
and only if all of the following conditions hold:
(i) r.b.left = r?.f .left = ?, (ii) r.f .left ? r?,
r.f .right ? r?, (iii) r?.b.left ? r, r?.b.right ? r.
529
However, checking all O(n4) tandems the one af-
ter the other would require time O(n4). Therefore,
preserving the quadratic complexity of the overall
algorithm requires a more complex representation.
From now on, we assume that Vp =
{x1, . . . , xn}, and we write [i, j] as a shorthand
for the range [xi, xj].
First, we need to compute an additional data
structure that will store local backward figures in
a convenient way. Let us define the expansion ta-
ble T as follows: for a given range r? = [i?, j?],
T (r?) is the set of all ranges r = [i, j] such that
r.lb.left = i? and r.lb.right = j?, ordered by in-
creasing left boundary i. It turns out that the con-
struction of such a table can be achieved in time
O(n2). Moreover, it is possible to compute in
O(n2) an auxiliary table T ? that associates with r
the first range r?? in T ([r.f.left, r.f.right]) such
that r??.b.right ? r. Therefore, either (r, T ?(r))
anchors a valid bundle, or there is no bundle E
such that the first component of V (E) is r.
We now have all the pieces to extract bundles
with fan-out 2 in time O(n2). We proceed as fol-
lows. For each range r = [i, j]:
? We first retrieve r? = [r.f.left, r.f.right] in
constant time.
? Then, we check in constant time whether
r?.b.left lies within r. If it doesn?t, r is not
the first part of a valid bundle with fan-out 2,
and we move on to the next range r.
? Finally, for each r?? in the ordered set
T (r?), starting with T ?(r), we check whether
r??.b.right is inside r. If it is not, we stop and
move on to the next range r. If it is, we out-
put the valid bundle (r, r??) and move on to
the next element in T (r?). Indeed, in case of
a failure, the backward edge that relates a ver-
tex in r?? with a vertex outside r will still be
included in all further elements in T (r?) since
T (r?) is ordered by increasing left boundary.
This step costs a constant time for each suc-
cess, and a constant time for the unique fail-
ure, if any.
This algorithm spends a constant time on each
range plus a constant time on each bundle with
fan-out 2. We shall prove in Section 5 that there
are O(n2) bundles with fan-out 2. Therefore, this
algorithm runs in time O(n2).
Now that we have extracted all bundles, we
need to extract an optimal decomposition of the in-
put bundle F ?, i.e., a minimal size partition of all
n elements (edges) in the input bundle such that
each of these partition is a bundle (with fan-out 2,
since bundles with fan-out 1 are excluded, except
for the input bundle). By definition, a partition has
minimal size if there is no other partition it is a
refinment of.3
4.5 Extracting an optimal decomposition
We have constructed the set of all (fan-out 2) sub-
bundles of F ?. We now need to build one optimal
decomposition of F ? into sub-bundles. We need
some more theoretical results on the properties of
bundles.
Lemma 1 Let E1 and E2 be two sub-bundles of
F ? (with fan-out 2) that have non-empty intersec-
tion, but that are not included the one in the other.
Then E1 ? E2 is a bundle (with fan-out 2).
PROOF This lemma can be proved by considering
all possible respective positions of the covers of
E1 and E2, and discarding all situations that would
lead to the existence of a fan-out 1 sub-bundle. 
Theorem 1 For any bundle E, either it has at
least one binary decomposition, or all its decom-
positions are refinements of a unique optimal one.
PROOF Let us suppose that E has no bi-
nary decomposition. Its cover corresponds to
the tandem (r, r?) = ([i, j], [i?, j?]). Let
us consider two different decompositions of
E, that correspond respectively to decomposi-
tions of the range r in two sets of sub-ranges
of the form [i, k1], [k1 + 1, k2], . . . , [km, j] and
[i, k?1], [k?1 + 1, k?2], . . . , [k?m? , j]. For simplifying
the notations, we write k0 = k?0 = i and km+1 =
km?+1 = j. Since k0 = k?0, there exist an in-
dex p > 0 such that for any l < p, kl = k?l, but
kp 6= k?p: p is the index that identifies the first
discrepancy between both decomposition. Since
km+1 = km?+1, there must exist q ? m and
q? ? m? such that q and q? are strictly greater
than p and that are the minimal indexes such that
kq = k?q? . By definition, all bundles of the form
E[kl?1,kl] (p ? l ? q) have a non-empty intersec-
tion with at least one bundle of the form E[k?l?1,k?l]
3The term ?refinement? is used in the usual way concern-
ing partitions, i.e., a partition P1 is a refinement of another
one P2 if all constituents in P1 are constituents of P2, or be-
longs to a subset of the partition P1 that is a partition of one
element of P2.
530
(p ? l ? q?). The reverse is true as well. Ap-
plying Lemma 1, this shows that E([kp+1, kq]) is
a bundle with fan-out 2. Therefore, by replacing
all ranges involved in this union in one decom-
position or the other, we get a third decomposi-
tion for which the two initial ones are strict refine-
ments. This is a contradiction, which concludes
the proof. 
Lemma 2 Let E = V (r ? r?) be a bundle, with
r = [i, j]. We suppose it has a unique (non-binary)
optimal decomposition, which decomposes [i, j]
into [i, k1], [k1 + 1, k2], . . . , [km, j]. There exist
no range r?? ? r such that (i) Er?? is a bundle and
(ii) ?l, 1 ? l ? m such that [kl, kl+1] ? r??.
PROOF Let us consider a range r?? that would con-
tradict the lemma. The union of r?? and of the
ranges in the optimal decomposition that have a
non-empty intersection with r?? is a fan-out 2 bun-
dle that includes at least two elements of the opti-
mal decomposition, but that is strictly included in
E because the decomposition is not binary. This
is a contradiction. 
Lemma 3 Let E = V (r, r?) be a bundle, with r =
[i, j]. We suppose it has a binary (optimal) decom-
position (not necessarily unique). Let r?? = [i, k]
be the largest range starting in i such that k < j
and such that it anchors a bundle, namely E(r??).
Then E(r??) and E([k + 1, j]) form a binary de-
composition of E.
PROOF We need to prove that E([k + 1, j]) is a
bundle. Each (optimal) binary decomposition of
E decomposes r in 1, 2 or 3 sub-ranges. If no opti-
mal decomposition decomposes r in at least 2 sub-
ranges, then the proof given here can be adapted
by reasoning on r? instead of r. We now sup-
pose that at least one of them decomposes r in at
least 2 sub-ranges. Therefore, it decomposes r in
[i, k1] and [k1 + 1, j] or in [i, k1], [k1 + 1, k2] and
[k2 + 1, j]. We select one of these optimal decom-
position by taking one such that k1 is maximal.
We shall now distinguish between two cases.
First, let us suppose that r is decomposed
into two sub-ranges [i, k1] and [k1 + 1, j] by
the selected optimal decomposition. Obviously,
E([i, k1]) is a ?crossing? bundle, i.e., the right
component of its cover is is a sub-range of r?.
Since r is decomposed in two sub-ranges, it is
necessarily the same for r?. Therefore, E([i, k1])
has a cover of the form [i, k1] ? [i?, k?1] or [i, k1] ?
[k?1 + 1, j]. Since r?? includes [i, k1], E(r??) has a
cover of the form [i, k]?[i?, k?] or [i, k]?[k? + 1, j].
This means that r? is decomposed by E(r??) in
only 2 ranges, namely the right component of
E(r??)?s cover and another range, that we can call
r???. Since r \ r?? = [k + 1, j] may not anchor
a bundle with fan-out 1, it must contain at least
one crossing edge. All such edges necessarily fall
within r???. Conversely, any crossing edge that
falls inside r??? necessarily has its other end inside
[k + 1, j]. Which means that E(r??) and E(r???)
form a binary decomposition of E. Therefore, by
definition of k1, k = k1.
Second, let us suppose that r is decomposed
into 3 sub-ranges by the selected original decom-
position (therefore, r? is not decomposed by this
decomposition). This means that this decompo-
sition involves a bundle with a cover of the form
[i, k1]?[k2 + 1, j] and another bundle with a cover
of the form [k1 + 1, k2] ? r? (this bundle is in fact
E(r?)). If k ? k2, then the left range of both mem-
bers of the original decomposition are included in
r??, which means that E(r??) = E, and therefore
r?? = r which is excluded. Note that k is at least
as large as k1 (since [i, k1] is a valid ?range start-
ing in i such that k < j and such that it anchors
a bundle?). Therefore, we have k1 ? k < k2.
Therefore, E([i, k1]) ? E(r??), which means that
all edges anchored inside [k2 + 1, j]) are included
in E(r??). Hence, E(r??) can not be a crossing bun-
dle without having a left component that is [i, j],
which is excluded (it would mean E(r??) = E).
This means that E(r??) is a bundle with a cover
of the form [i, k] ? [k? + 1, j]. Which means
that E(r?) is in fact the bundle whose cover is
[k + 1, k? + 1]? r?. Hence, E(r??) and E(r?) form
a binary decomposition of E. Hence, by definition
of k1, k = k1. 
As an immediate consequence of Lemmas 2
and 3, our algorithm for extracting the optimal de-
composition for F ? consists in applying the fol-
lowing procedure recursively, starting with F ?,
and repeating it on each constructed sub-bundle E,
until sub-bundles with only one edge are reached.
Let E = E(r, r?) be a bundle, with r = [i, j].
One optimal decomposition of E can be obtained
as follows. One selects the bundle with a left com-
ponent starting in i and with the maximum length,
and iterating this selection process until r is cov-
ered. The same is done with r?. We retain the opti-
mal among both resulting decompositions (or one
of them if they are both optimal). Note that this
531
decomposition is unique if and only if it has four
components or more; it can not be ternary; it may
be binary, and in this case it may be non-unique.
This algorithm gives us a way to extract an op-
timal decomposition of F ? in linear time w.r.t. the
number of sub-bundles in this optimal decomposi-
tion. The only required data structure is, for each
i (resp. k), the list of bundles with a cover of the
form [i, j]? [k, l] ordered by decreasing j (resp. l).
This can trivially be constructed in time O(n2)
from the list of all bundles we built in time O(n2)
in the previous section. Since the number of bun-
dles is bounded by O(n2) (as mentioned above
and proved in Section 5), this means we can ex-
tract an optimal decomposition for F ? in O(n2).
Similar ideas apply to the simpler case of the
decomposition of bundles with fan-out 1.
4.6 The main decomposition algorithm
We now have to generalize our algorithm in or-
der to handle the possible existence of fan-out 1
bundles. We achieve this by using the fan-out 2
algorithm recursively. First, we extract and re-
move (maximal) bundles with fan-out 1 from F ,
and recursively apply to each of them the com-
plete algorithm. What remains is F ?, which is a
set of bundles with no sub-bundles with fan-out 1.
This means we can apply the algorithm presented
above. Then, for each bundle with fan-out 1, we
group it with a randomly chosen adjacent bundle
with fan-out 2, which builds an expanded bundle
with fan-out 2, which has a binary decomposition
into the original bundle with fan-out 2 and the bun-
dle with fan-out 1.
5 Time complexity analysis
In Section 4, we claimed that there are no more
than O(n2) bundles. In this section we sketch the
proof of this result, which will prove the quadratic
time complexity of our algorithm.
Let us compute an upper bound on the num-
ber of bundles with fan-out two that can be found
within the p-graph processed in Section 4.5, i.e., a
p-graph with no fan-out 1 sub-bundle.
Let E,E? ? Ep be bundles with fan-out two. If
E ? E?, then we say that E? expands E. E? is
said to immediately expand E, written E ? E?,
if E? expands E and there is no bundle E?? such
that E?? expands E and E? expands E??.
Let us represent bundles and the associated im-
mediate expansion relation by means of a graph.
Let E denote the set of all bundles (with fan-out
two) in our p-graph. The e-graph associated with
our LCFRS production p is the directed graph
with vertices E and edges defined by the relation
?. For E ? E , we let out(E) = {E? |E ? E?}
and in(E) = {E? |E? ? E}.
Lack of space prevents us from providing the
proof of the following property. For any E ? E
that contains more than one edge, |out(E)| ? 2
and |in(E)| ? 2. This allows us to prove our up-
per bound on the size of E .
Theorem 2 The e-graph associated with an
LCFRS production p has at most n2 vertices,
where n is the rank of p.
PROOF Consider the e-graph associated with pro-
duction p, with set of vertices E . For a vertex
E ? E , we define the level of E as the number
|E| of edges in the corresponding bundle from the
p-graph associated with p. Let d be the maximum
level of a vertex in E . We thus have 1 ? d ? n.
We now prove the following claim. For any inte-
ger k with 1 ? k ? d, the set of vertices in E with
level k has no more than n elements.
For k = 1, since there are no more than n edges
in such a p-graph, the statement holds.
We can now consider all vertices in E with level
k > 1 (k ? d). Let E(k?1) be the set of all ver-
tices in E with level smaller than or equal to k?1,
and let us call T (k?1) the set of all edges in the e-
graph that are leaving from some vertex in E(k?1).
Since for each bundle E in E(k?1) we know that
|out(E)| ? 2, we have |T (k?1)| ? 2|E(k?1)|.
The number of vertices in E(k) with level larger
than one is at least |E(k?1)| ? n. Since for each
E ? E(k?1) we know that |in(E)| ? 2, we con-
clude that at least 2(|E(k?1)| ? n) edges in T (k?1)
must end up at some vertex in E(k). Let T be the
set of edges in T (k?1) that impinge on some ver-
tex in E \ E(k). Thus we have |T | ? 2|E(k?1)| ?
2(|E(k?1)|?n) = 2n. Since the vertices of level k
in E must have incoming edges from set T , and be-
cause each of them have at least 2 incoming edges,
there cannot be more than n such vertices. This
concludes the proof of our claim.
Since the the level of a vertex in E is necessarily
lower than n, this completes the proof. 
The overall complexity of the complete algo-
rithm can be computed by induction. Our in-
duction hypothesis is that for m < n, the time
complexity is in O(m2). This is obviously true
for n = 1 and n = 2. Extracting the bundles
532
with fan-out 1 costs O(n2). These bundles are of
length n1 . . . nm. Extracting bundles with fan-out
2 costs O((n? n1 ? . . .? nm)2). Applying re-
cursively the algorithm to bundles with fan-out 1
costs O(n21) + . . . +O(n2m). Therefore, the com-
plexity is in O(n2)+O((n ? n1 ? . . .? nm)2)+
?n
i=1 O(ni) = O(n2) +O(
?n
i=1 ni) = O(n2).
6 Conclusion
We have introduced an efficient algorithm for opti-
mal reduction of the rank of LCFRSs with fan-out
at most 2, that runs in quadratic time w.r.t. the rank
of the input grammar. Given the fact that fan-out 1
bundles can be attached to any adjacent bundle in
our factorization, we can show that our algorithm
also optimizes time complexity for known tabular
parsing algorithms for LCFRSs with fan-out 2.
As for general LCFRS, it has been shown by
Gildea (2010) that rank optimization and time
complexity optimization are not equivalent. Fur-
thermore, all known algorithms for rank or time
complexity optimization have an exponential time
complexity (Go?mez-Rodr??guez et al, 2009).
Acknowledgments
Part of this work was done while the second author
was a visiting scientist at Alpage (INRIA Paris-
Rocquencourt and Universite? Paris 7), and was fi-
nancially supported by the hosting institutions.
References
Daniel Gildea. 2010. Optimal parsing strategies for
linear context-free rewriting systems. In Human
Language Technologies: The 11th Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics; Proceedings of
the Main Conference, Los Angeles, California. To
appear.
Carlos Go?mez-Rodr??guez and Giorgio Satta. 2009.
An optimal-time binarization algorithm for linear
context-free rewriting systems with fan-out two. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 985?993, Suntec, Singapore,
August. Association for Computational Linguistics.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduc-
tion of rule length in linear context-free rewriting
systems. In Proceedings of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies Confer-
ence (NAACL?09:HLT), Boulder, Colorado. To ap-
pear.
Michael A. Harrison. 1978. Introduction to Formal
Language Theory. Addison-Wesley, Reading, MA.
Aravind K. Joshi and Leon S. Levy. 1977. Constraints
on local descriptions: Local transformations. SIAM
Journal of Computing
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL 2009), Athens, Greece. To
appear.
Wolfgang Maier and Timm Lichte. 2009. Character-
izing discontinuity in constituent treebanks. In Pro-
ceedings of the 14th Conference on Formal Gram-
mar (FG 2009), Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Philippe
de Groote, editor, Proceedings of the 13th Confer-
ence on Formal Grammar (FG 2008), pages 61?76,
Hamburg, Germany. CSLI Publications.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223:87?120.
Giorgio Satta. 1992. Recognition of linear context-free
rewriting systems. In Proceedings of the 30th Meet-
ing of the Association for Computational Linguistics
(ACL?92), Newark, Delaware.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191?
229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th Meeting of the Association for
Computational Linguistics (ACL?87).
David J. Weir. 1992. Linear context-free rewriting
systems and deterministic tree-walk transducers. In
Proceedings of the 30th Meeting of the Association
for Computational Linguistics (ACL?92), Newark,
Delaware.
533
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383?387,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervized Word Segmentation:
the case for Mandarin Chinese
Pierre Magistry
Alpage, INRIA & Univ. Paris 7,
175 rue du Chevaleret,
75013 Paris, France
pierre.magistry@inria.fr
Beno?t Sagot
Alpage, INRIA & Univ. Paris 7,
175 rue du Chevaleret,
75013 Paris, France
benoit.sagot@inria.fr
Abstract
In this paper, we present an unsupervized seg-
mentation system tested on Mandarin Chi-
nese. Following Harris's Hypothesis in Kempe
(1999) and Tanaka-Ishii's (2005) reformulation,
we base our work on the Variation of Branching
Entropy. We improve on (Jin and Tanaka-Ishii,
2006) by adding normalization and viterbi-
decoding. This enable us to remove most of
the thresholds and parameters from their model
and to reach near state-of-the-art results (Wang
et al, 2011) with a simpler system. We provide
evaluation on different corpora available from
the Segmentation bake-off II (Emerson, 2005)
and define a more precise topline for the task
using cross-trained supervized system available
off-the-shelf (Zhang and Clark, 2010; Zhao and
Kit, 2008; Huang and Zhao, 2007)
1 Introduction
The Chinese script has no explicit ?word? bound-
aries. Therefore, tokenization itself, although the
very first step of many text processing systems, is
a challenging task. Supervized segmentation sys-
tems exist but rely on manually segmented corpora,
which are often specific to a genre or a domain and
use many different segmentation guidelines. In order
to deal with a larger variety of genres and domains,
or to tackle more theoretic questions about linguistic
units, unsupervized segmentation is still an impor-
tant issue. After a short review of the corresponding
literature in Section 2, we discuss the challenging is-
sue of evaluating unsupervized word segmentation
systems in Section 3. Section 4 and Section 5 present
the core of our system. Finally, in Section 6, we de-
tail and discuss our results.
2 State of the Art
Unsupervized word segmentation systems tend to
make use of three different types of information: the
cohesion of the resulting units (e.g., Mutual Infor-
mation, as in (Sproat and Shih, 1990)), the degree of
separation between the resulting units (e.g., Acces-
sor Variety, see (Feng et al, 2004)) and the proba-
bility of a segmentation given a string (Goldwater et
al., 2006; Mochihashi et al, 2009).
A recently published work by Wang et al (2011)
introduce ESA: ?Evaluation, Selection, Adjust-
ment.? This method combines cohesion and separa-
tion measures in a ?goodness? metric that is maxi-
mized during an iterative process. This work is the
current state-of-the-art in unsupervized segmenta-
tion of Mandarin Chinese data.
The main drawbacks of ESA are the need to iterate
the process on the corpus around 10 times to reach
good performance levels and the need to set a param-
eter that balances the impact of the cohesion measure
w.r.t. the separation measure. Empirically, a corre-
lation is found between the parameter and the size of
the corpus but this correlation depends on the script
used in the corpus (it changes if Latin letters and
Arabic numbers are taken into account during pre-
processing or not). Moreover, computing this cor-
relation and finding the best value for the parameter
(i.e., what the authors call the proper exponent) re-
quires a manually segmented training corpus. There-
fore, this proper exponent may not be easily available
in all situations. However, if we only consider their
experiments using settings similar to ours, their re-
sults consistently lie around an f-score of 0.80.
An older approach, introduced by Jin and Tanaka-
Ishii (2006), solely relies on a separation measure
383
that is directly inspired by a linguistic hypothesis for-
mulated by Harris (1955). In Tanaka-Ishii (2005)
(following Kempe (1999)) who use Branching En-
tropy (BE), this hypothesis goes as follows: if se-
quences produced by human language were random,
we would expect the Branching Entropy of a se-
quence (estimated from the n-grams in a corpus)
to decrease as we increase the length of the se-
quence. Therefore the variation of the branching en-
tropy (VBE) should be negative. When we observe
that it is not the case, Harris hypothesizes that we
are at a linguistic boundary. Following this hypoth-
esis, (Jin and Tanaka-Ishii, 2006) propose a system
that segments when BE is rising or when it reach a
certain maximum.
The main drawback of Jin and Tanaka-Ishii (2006)
model is that segmentation decisions are taken very
locally1 and do not depend on neighboring cuts.
Moreover, this system also also relies on parameters,
namely the threshold on the VBE above which the
system decides to segment (in their system, this is
when VBE? 0). In theory, we could expect a de-
creasing BE and look for a less decreasing value (or
on the contrary, rising at least to some extent). A
threshold of 0 can be seen as a default value. Fi-
nally, Jin and Tanaka-Ishii do not take in account that
VBE of n-gram may not be directly comparable to
the VBE of m-grams if m ?= n. A normalization is
needed (as in (Cohen et al, 2002)).
Due to space constraints, we shall not describe
here other systems than those by Wang et al (2011)
and Jin and Tanaka-Ishii (2006). A more compre-
hensive state of the art can be found in (Zhao and
Kit, 2008) and (Wang et al, 2011).
In this paper we will show that we can correct the
drawbacks of Jin and Tanaka-Ishii (2006) model and
reach performances comparable to those of Wang et
al. (2011) with as simpler system.
3 Evaluation
In this paper, in order to be comparable with
Wang et al (2011), we evaluate our system against
the corpora from the Second International Chi-
nese Word Segmentation Bakeoff (Emerson, 2005).
These corpora cover 4 different segmentation guide-
lines from various origins: Academia Sinica (AS),
City-University of Hong-Kong (CITYU), Microsoft
Research (MSR) and Peking University (PKU).
1Jin (2007) uses self-training with MDL to address this issue.
Evaluating unsupervized systems is a challenge by
itself. As an agreement on the exact definition of
what a word is remains hard to reach, various seg-
mentation guidelines have been proposed and fol-
lowed for the annotation of different corpora. The
evaluation of supervized systems can be achieved on
any corpus using any guidelines: when trained on
data that follows particular guidelines, the resulting
system will follow as well as possible these guide-
lines, and can be evaluated on data annotated accord-
ingly. However, for unsupervized systems, there is
no reason why a system should be closer to one ref-
erence than another or even not to lie somewhere
in between the different existing guidelines. Huang
and Zhao (2007) propose to use cross-training of a
supervized segmentation system in order to have an
estimation of the consistency between different seg-
mentation guidelines, and therefore an upper bound
of what can be expected from an unsupervized sys-
tem (Zhao and Kit, 2008). The average consistency
is found to be as low as 0.85 (f-score). Therefore
this figure can be considered as a sensible topline for
unsupervized systems. The standard baseline which
consists in segmenting each character leads to a base-
line around 0.35 (f-score) ? almost half of the to-
kens in a manually segmented corpus are unigrams.
Per word-length evaluation is also important as
units of various lengths tend to have different distri-
butions. We used ZPAR (Zhang and Clark, 2010) on
the four corpora from the Second Bakeoff to repro-
duce Huang and Zhao's (2007) experiments, but also
to measure cross-corpus consistency at a per-word-
length level. Our overall results are comparable to
what Huang and Zhao (2007) report. However, the
consistency is quickly falling for longer words: on
unigrams, f-scores range from 0.81 to 0.90 (the same
as the overall results). We get slightly higher figures
on bigrams (0.85?0.92) but much lower on trigrams
with only 0.59?0.79. In a segmented Chinese text,
most of the tokens are uni- and bigrams but most of
the types are bi- and trigrams (as unigrams are often
high frequency grammatical words and trigrams the
result of more or less productive affixations). There-
fore the results of evaluations only based on tokens
do not suffer much from poor performances on tri-
grams even if a large part of the lexicon may be in-
correctly processed.
Another issue about the evaluation and compari-
son of unsupervized systems is to try and remain fair
384
in terms of preprocessing and prior knowledge given
to the systems. For example, Wang et al (2011)
used different levels of preprocessing (which they
call ?settings?). In their settings 1 and 2, Wang et
al. (2011) try not to rely on punctuation and char-
acter encoding information (such as distinguishing
Latin and Chinese characters). However, they opti-
mize their parameter for each setting. We therefore
consider that their system does take into account the
level of processing which is performed on Latin char-
acters and Arabic numbers, and therefore ?knows?
whether to expect such characters or not. In set-
ting 3 they add the knowledge of punctuation as clear
boundaries and in setting 4 they preprocess Arabic
and Latin and obtain better, more consistent and less
questionable results.
As we are more interested in reducing the amount
of human labor needed than in achieving by all
means fully unsupervized learning, we do not re-
frain from performing basic and straightforward pre-
processing such as detection of punctuation marks,
Latin characters and Arabic numbers.2 Therefore,
our experiments rely on settings similar to their set-
tings 3 and 4, and are evaluated against the same
corpora.
4 Normalized Variation of Branching
Entropy (nVBE)
Our system builds upon Harris's (1955) hypothesis
and its reformulation by Kempe (1999) and Tanaka-
Ishii (2005). Let us now define formally the notions
underlying our system.
Given an n-gram x0..n = x0..1 x1..2 . . . xn?1..n
with a left context ??, we define its Right Branching
Entropy (RBE) as:
h?(x0..n) = H(?? | x0..n)
= ?
?
x???
P (x | x0..n) logP (x | x0..n).
The Left Branching Entropy (LBE) is defined in a
symmetric way: if we note ?? the right context of
x0..n, its LBE is defined as:
h?(x0..n) = H(?? | x0..n).
The RBE (resp. LBE) can be considered as x0..n's
Branching Entropy (BE) when reading from left to
right (resp. right to left).
2Simple regular expressions could also be considered to deal
with unambiguous cases of numbers and dates in Chinese script.
From h?(x0..n) and h?(x0..n?1) on the one hand,
and from h?(x0..n) and h?(x1..n) we estimate the
Variation of Branching Entropy (VBE) in both direc-
tions, defined as follows:
?h?(x0..n) = h?(x0..n) ? h?(x0..n?1)
?h?(x0..n) = h?(x0..n) ? h?(x1..n).
The VBEs are not directly comparable for strings
of different lengths and need to be normalized. In
this work, we recenter them around 0 with respect to
the length of the string by substracting the mean of
the VBEs of the strings of the same length. Writing
??h?(x) and ??h?(x). The normalized VBEs for the
string x, or nVBEs, are then defined as follow (we
only defined ??h?(x) for clarity reasons): for each
length k and each k-gram x such that len(x) = k,
??h?(x) = ?h?(x)???,k, where ??,k is the mean
of the values of ?h?(x) of all k-grams x.
Note that we use and normalize the variation of
branching entropy and not the branching entropy it-
self. Doing so would break the Harris's hypothesis as
we would not expect h?(x0..n) < h?(x0..n?1) in non-
boundary situation anymore. Many studies use di-
rectly the branching entropy (normalized or not) and
report results that are below state-of-the-art systems
(Cohen et al, 2002).
5 Decoding algorithm
If we follow Harris's hypothesis and consider com-
plex morphological word structures, we expect a
large VBE at the boundaries of interesting units and
more unstable variations inside ?words.? This expec-
tation was confirmed by empirical data visualization.
For different lengths of n-grams, we compared the
distributions of the VBEs at different positions inside
the n-gram and at its boundaries. By plotting density
distributions for words vs. non-words, we observed
that the VBE at both boundaries were the most dis-
criminative value. Therefore, we decided to take in
account the VBE only at the word-candidate bound-
aries (left and right) and not to consider the inner val-
ues. Two interesting consequences of this decision
are: first, all ??h(x) can be precomputed as they do
not depend on the context. Second, best segmenta-
tion can be computed using dynamic programming.
Since we consider the VBE only at words bound-
ary, we can define for any n-gram w its autonomy as
a(x) = ???h(x) + ??h?(x). The more an n-gram is
autonomous, the more likely it is to be a word.
385
With this measure, we can redefine the sentence
segmentation problem as the maximization of the au-
tonomy measure of its words. For a character se-
quence s, if we call Seg(s) the set of all the possible
segmentations, then we are looking for:
arg max
W?Seg(s)
?
wi?W
a(wi) ? len(wi),
where W is the segmentation corresponding to the
sequence of words w0w1 . . . wm, and len(wi) is the
length of a word wi used here to be able to com-
pare segmentations resulting in a different number
of words. This best segmentation can be computed
easily using dynamic programming.
6 Results and discussion
We tested our system against the data from the 4 cor-
pora of the Second Bakeoff, in both settings 3 and 4,
as described in Section 3. Overall results are given
in Table 1 and per-word-length results in Table 2.
Our results (nVBE) show significant improve-
ments over Jin's (2006) strategy (VBE > 0) and
are closely competing with ESA. But contrarily to
ESA (Wang et al, 2011), it does not require multi-
ple iterations on the corpus and it does not rely on
any parameters. This shows that we can rely solely
on a separation measure and get high segmentation
scores. When maximized over a sentence, this mea-
sure captures at least in part what can be modeled by
a cohesion measure without the need for fine-tuning
the balance between the two.
The evolution of the results w.r.t. word length is
consistent with the supervized cross-evaluation re-
sults of the various segmentation guidelines as per-
formed in Section 3.
Due to space constraints, we cannot detail here a
qualitative analysis of the results. We can simply
mention that the errors we observed are consistent
with previous systems based on Harris's hypothesis
(see (Magistry and Sagot, 2011) and Jin (2007) for a
longer discussion). Many errors are related to dates
and Chinese numbers. This could and should be
dealt with during preprocessing. Other errors often
involve frequent grammatical morphemes or produc-
tive affixes. These errors are often interesting for lin-
guists and could be studied as such and/or corrected
in a post-processing stage that would introduce lin-
guistic knowledge. Indeed, unlike content words,
grammatical morphemes belongs to closed classes,
System AS CITYU PKU MSR
Setting 3
ESA worst 0.729 0.795 0.781 0.768
ESA best 0.782 0.816 0.795 0.802
nVBE 0.758 0.775 0.781 0.798
Setting 4
VBE > 0 0.63 0.640 0.703 0.713
ESA worst 0.732 0.809 0.784 0.784
ESA best 0.786 0.829 0.800 0.818
nVBE 0.766 0.767 0.800 0.813
Table 1: Evaluation on the Second Bakeoff data with
Wang et al's (2011) settings. ?Worst? and ?best? give the
range of the reported results with differents values of the
parameter in Wang et al's system. VBE > 0 correspond
to a cut whenever BE is raising. nVBE corresponds to our
proposal, based on normalized VBE with maximization at
word boundaries. Recall that the topline is around 0.85
Corpus overall unigrams bigrams trigrams
AS 0.766 0.741 0.828 0.494
CITYU 0.767 0.739 0.834 0.555
PKU 0.800 0.789 0.855 0.451
MSR 0.813 0.823 0.856 0.482
Table 2: Per word-length details of our results with our
nVBE algorithm and setting 4. Recall that the toplines
are respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)
therefore introducing this linguistic knowledge into
the system may be of great help without requiring
to much human effort. A sensible way to go in that
direction would be to let unsupervized system deal
with open classes and process closed classes with a
symbolic or supervized module.
One can also observe that our system performs bet-
ter on PKU and MSR corpora. As PKU is the small-
est corpus and AS the biggest, size alone cannot ex-
plain this result. However, PKU is more consistent
in genre as it contains only articles from the Peo-
ple's Daily. On the other end, AS is a balanced cor-
pus with a greater variety in many aspects. CITYU
Corpus is almost as small as PKU but contains arti-
cles from newspapers of various Mandarin Chinese
speaking communities where great variation is to be
expected. This suggest that consistency of the input
data is as important as the amount of data. This hy-
pothesis has to be confirmed in futur studies. If it is,
automatic clustering of the input data may be an im-
portant pre-processing step for this kind of systems.
386
References
Paul Cohen, Brent Heeringa, and Niall Adams. 2002.
An unsupervised algorithm for segmenting categorical
timeseries into episodes. Pattern Detection and Dis-
covery, page 117?133.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing, volume 133.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weiming
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, page 673?680.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Changning. Huang and Hai Zhao. 2007. ??????
?? (Chinese word segmentation: A decade review).
Journal of Chinese Information Processing, 21(3):8?
20.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branching
entropy. In Proceedings of the COLING/ACL on Main
conference poster sessions, page 428?435.
Zhihui Jin. 2007. A Study On Unsupervised Segmenta-
tion Of Text Using Contextual Complexity. Ph.D. the-
sis, University of Tokyo.
Andr? Kempe. 1999. Experiments in unsupervised
entropy-based corpus segmentation. In Workshop of
EACL in Computational Natural Language Learning,
page 7?13.
Pierre Magistry and Beno?t Sagot. 2011. Segmentation
et induction de lexique non-supervis?es du mandarin.
In TALN'2011 - Traitement Automatique des Langues
Naturelles, Montpellier, France, June. ATALA.
Daichi Mochihashi, Takeshi. Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1-Volume 1, page 100?108.
Richard W. Sproat and Chilin Shih. 1990. A statis-
tical method for finding word boundaries in Chinese
text. Computer Processing of Chinese and Oriental
Languages, 4(4):336?351.
Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries: An experiment using a web search
engine. In IJCNLP, page 93?105.
Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong
Fan. 2011. A new unsupervised approach to word
segmentation. Computational Linguistics, 37(3):421?
454.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using a
single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, page 843?852.
Hai Zhao and Chunyu Kit. 2008. An empirical compar-
ison of goodness measures for unsupervised Chinese
word segmentation with a unified framework. In The
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP2008), Hyderabad, India.
387
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 56?63,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Influence of Pre-annotation on POS-tagged Corpus Development
Kare?n Fort
INIST CNRS / LIPN
Nancy / Paris, France.
karen.fort@inist.fr
Beno??t Sagot
INRIA Paris-Rocquencourt / Paris 7
Paris, France.
benoit.sagot@inria.fr
Abstract
This article details a series of carefully de-
signed experiments aiming at evaluating
the influence of automatic pre-annotation
on the manual part-of-speech annotation
of a corpus, both from the quality and the
time points of view, with a specific atten-
tion drawn to biases. For this purpose, we
manually annotated parts of the Penn Tree-
bank corpus (Marcus et al, 1993) under
various experimental setups, either from
scratch or using various pre-annotations.
These experiments confirm and detail the
gain in quality observed before (Marcus et
al., 1993; Dandapat et al, 2009; Rehbein
et al, 2009), while showing that biases do
appear and should be taken into account.
They finally demonstrate that even a not
so accurate tagger can help improving an-
notation speed.
1 Introduction
Training a machine-learning based part-of-speech
(POS) tagger implies manually tagging a signifi-
cant amount of text. The cost of this, in terms of
human effort, slows down the development of tag-
gers for under-resourced languages.
One usual way to improve this situation is to
automatically pre-annotate the corpus, so that the
work of the annotators is limited to the validation
of this pre-annotation. This method proved quite
efficient in a number of POS-annotated corpus de-
velopment projects (Marcus et al, 1993; Danda-
pat et al, 2009), allowing for a significant gain
not only in annotation time but also in consistency.
However, the influence of the pre-tagging quality
on the error rate in the resulting annotated corpus
and the bias introduced by the pre-annotation has
been little examined. This is what we propose to
do here, using different parts of the Penn Treebank
to train various instances of a POS tagger and ex-
periment on pre-annotation. Our goal is to assess
the impact of the quality (i.e., accuracy) of the
POS tagger used for pre-annotating and to com-
pare the use of pre-annotation with purely manual
tagging, while minimizing all kinds of biases. We
quantify the results in terms of error rate in the re-
sulting annotated corpus, manual annotation time
and inter-annotator agreement.
This article is organized as follows. In Sec-
tion 2, we mention some related work, while Sec-
tion 3 describes the experimental setup, followed
by a discussion on the obtained results (Section 4)
and a conclusion.
2 Related Work
2.1 Pre-annotation for POS Tagging
Very few manual annotation projects give details
about the campaign itself. One major exception is
the Penn Treebank project (Marcus et al, 1993),
that provided detailed information about the man-
ual annotation methodology, evaluation and cost.
Marcus et al (1993) thus showed that manual tag-
ging took twice as long as correcting pre-tagged
text and resulted in twice the inter-annotator dis-
agreement rate, as well as an error rate (using a
gold-standard annotation) about 50% higher. The
pre-annotation was done using a tagger trained on
the Brown Corpus, which, due to errors introduced
by an automatic mapping of tags from the Brown
tagset to the Penn Treebank tagset, had an error
rate of 7?9%. However, they report neither the in-
fluence of the training of the annotators on the po-
tential biases in correction, nor that of the quality
of the tagger on the correction time and the ob-
tained quality.
Dandapat et al (2009) went further and showed
that, for complex POS-tagging (for Hindi and
Bangla), pre-annotation of the corpus allows for
a gain in time, but not necessarily in consis-
56
tency, which depends largely on the pre-tagging
quality. They also noticed that untrained annota-
tors were more influenced by pre-annotation than
the trained ones, who showed ?consistent perfor-
mance?. However, this very complete and inter-
esting experiment lacked a reference allowing for
an evaluation of the quality of the annotations. Be-
sides, it only took into account two types of pre-
tagging quality, high accuracy and low accuracy.
2.2 Pre-annotation in Other Annotation
Tasks
Alex et al (2008) led some experiments in the
biomedical domain, within the framework of a
?curation? task of protein-protein interaction. Cu-
ration consists in reading through electronic ver-
sion of papers and entering retrieved information
into a template. They showed that perfectly pre-
annotating the corpus leads to a reduction of more
than 1/3 in curation time, as well as a better recall
from the annotators. Less perfect pre-annotation
still leads to a gain in time, but less so (a little less
than 1/4th). They also tested the effect of higher
recall or precision of pre-annotation on one anno-
tator (curator), who rated recall more positively
than precision. However, as they notice, this result
can be explained by the curation style and should
be tested on more annotators.
Rehbein et al (2009) led quite thorough ex-
periments on the subject, in the field of semantic
frame assignment annotation. They asked 6 an-
notators to annotate or correct frame assignment
using a task-specific annotation tool. Here again,
pre-annotation was done using only two types of
pre-tagging quality, state-of-the-art and enhanced.
The results of the experiments are a bit disappoint-
ing as they could not find a direct improvement of
annotation time using pre-annotation. The authors
reckon this might be at least partly due to ?an inter-
action between time savings from pre-annotation
and time savings due to a training effect.? For
the same reason, they had to exclude some of the
annotation results for quality evaluation in order
to show that, in line with (Marcus et al, 1993),
quality pre-annotation helps increasing annotation
quality. They also found that noisy and low qual-
ity pre-annotation does not overall corrupt human
judgment.
On the other hand, Fort et al (2009) claim that
pre-annotation introduces a bias in named entity
annotation, due to the preference given by anno-
tators to what is already annotated, thus prevent-
ing them from noticing entities that were not pre-
annotated. This particular type of bias should not
appear in POS-tagging, as all the elements are to
be annotated, but a pre-tagging could influence
the annotators, preventing them from asking them-
selves questions about a specific pre-annotation.
In a completely different field, Barque et
al. (2010) used a series of NLP tools, called
MACAON, to automatically identify the central
component and optional peripheral components of
dictionary definitions. This pre-processing gave
disappointing results as compared to entirely man-
ual annotation, as it did not allow for a significant
gain in time. The authors consider that the bad
results are due to the quality of the tool that they
wish to improve as they believe that ?an automatic
segmentation of better quality would surely yield
some gains.?
Yet, the question remains: is there a quality
threshold for pre-annotation to be useful? and if
so, how can we evaluate it? We tried to answer
at least part of these questions for a quite simple
task for which data is available: POS-tagging in
English.
3 Experimental Setup
The idea underlying our experiments is the follow-
ing. We split the Penn Treebank corpus (Marcus et
al., 1993) in a usual manner, namely we use Sec-
tions 2 to 21 to train various instances of a POS
tagger, and Section 23 to perform the actual ex-
periments. In order to measure the impact of the
POS tagger?s quality, we trained it on subcorpora
of increasing sizes, and pre-annotated Section 23
with these various POS taggers. Then, we man-
ually annotated parts of Section 23 under various
experimental setups, either from scratch or using
various pre-annotations, as explained below.
3.1 Creating the Taggers
We used the MElt POS tagger (Denis and Sagot,
2009), a maximum-entropy based system that is
able to take into account both information ex-
tracted from a training corpus and information ex-
tracted from an external morphological lexicon.1
It has been shown to lead to a state-of-the-art POS
tagger for French. Trained on Sections 2 to 21
1MElt is freely available under LGPL license, on the web
page of its hosting project (http://gforge.inria.
fr/projects/lingwb/) .
57
of the Penn Treebank (MEltALLen ), and evaluated
on Section 23, MElt exhibits a 96.4% accuracy,
which is reasonably close to the state-of-the-art
(Spoustova? et al (2009) report 97.4%). Since it is
trained without any external lexicon, MEltALLen is
very close to the original maximum-entropy based
tagger (Ratnaparkhi, 1996), which has indeed a
similar 96.6% accuracy.
We trained MElt on increasingly larger parts of
the POS-tagged Penn Treebank,2 thus creating dif-
ferent taggers with growing degrees of accuracy
(see table 1). We then POS-tagged the Section 23
with each of these taggers, thus obtaining for each
sentence in Section 23 a set of pre-annotations,
one from each tagger.
Tagger Nb train. sent. Nb tokens Acc. (%)
MElt10en 10 189 66.5
MElt50en 50 1,254 81.6
MElt100en 100 2,774 86.7
MElt500en 500 12,630 92.1
MElt1000en 1,000 25,994 93.6
MElt5000en 5,000 126,376 95.8
MElt10000en 10,000 252,416 96.2
MEltALLen 37,990 944,859 96.4
Table 1: Accuracy of the created taggers evaluated
on Section 23 of the Penn Treebank
3.2 Experiments
We designed different experimental setups to
evaluate the impact of pre-annotation and pre-
annotation accuracy on the quality of the resulting
corpus. The subparts of Section 23 that we used
for these experiments are identified by sentence
ids (e.g., 1?100 denotes the 100 first sentences in
Section 23).
Two annotators were involved in the experi-
ments. They both have a good knowledge of lin-
guistics, without being linguists themselves and
had only little prior knowledge of the Penn Tree-
bank POS tagset. One of them had previous exper-
tise in POS tagging (Annotator1). It should also
be noticed that, though they speak fluent English,
they are not native speakers of the language. They
were asked to keep track of their annotation time,
noting the time it took them to annotate or correct
each series of 10 sentences. They were also asked
to use only a basic text editor, with no macro or
specific feature that could help them, apart from
2More precisely, MEltien is trained on the i first sentences
of the overall training corpus, i.e. Sections 2 to 21.
the usual ones, like Find, Replace, etc. The set
of 36 tags used in the Penn Treebank and quite
a number of particular cases is a lot to keep in
mind. This implies a heavy cognitive load in short-
term memory, especially as no specific interface
was used to help annotating or correcting the pre-
annotations.
It was demonstrated that training improves
the quality of manual annotation in a significant
way as well as allows for a significant gain in
time (Marcus et al, 1993; Dandapat et al, 2009;
Mikulova? and S?te?pa?nek, 2009). In particular, Mar-
cus et al (1993) observed that it took the Penn
Treebank annotators 1 month to get fully efficient
on the POS-tagging correction task, reaching a
speed of 20 minutes per 1,000 words. The speed of
annotation in our experiments cannot be compared
to this, as our annotators only annotated and cor-
rected small samples of the Penn Treebank. How-
ever, the annotators? speed and correctness did
improve with practice. As explained below, we
took this learning curve into account, as previous
work (Rehbein et al, 2009) showed it has an sig-
nificant impact on the results.
Also, during each experiment, sentences were
annotated sequentially. Moreover, the experiments
were conducted in the order we describe them be-
low. For example, both annotators started their
first annotation task (sentences 1?100) with sen-
tence 1.
We conducted the following experiments:
1. Impact of the pre-annotation accuracy on
precision and inter-annotator agreement:
In this experiment, we used sentences 1?
400 with random pre-annotation: for each
sentence, one pre-annotation is randomly
selected among its possible pre-annotations
(one for each tagger instance). The aim of
this is to eliminate the bias caused by the an-
notators? learning curve. Annotation time for
each series of 10 consecutive sentences was
gathered, as well as precision w.r.t. the refer-
ence and inter-annotator agreement (both an-
notators annotated sentences 1?100 and 301?
400, while only one annotated 101?200 and
the other 201?300).
2. Impact of the pre-annotation accuracy on
annotation time: This experiment is based
on sentences 601?760, with pre-annotation.
We divided them in series of 10 sentences.
58
For each series, one pre-annotation is se-
lected (i.e., the pre-annotation produced by
one of the 8 taggers), in such a way that each
pre-annotation is used for 2 series. We mea-
sured the manual annotation time for each se-
ries and each annotator.
3. Bias induced by pre-annotation: In this
experiment, both annotators annotated sen-
tences 451?500 fully manually.3 Later,
they annotated sentences 451?475 with the
pre-annotation from MEltALLen (the best tag-
ger) and sentences 476?500 with the pre-
annotation from MElt50en (the second-worst
tagger). We then compared the fully man-
ual annotations with those based on pre-
annotations to check if and how they diverge
from the Penn Treebank ?gold-standard?; we
also compared annotation times, in order to
get a confirmation of the gain in time ob-
served in previous experiments.
4 Results and Discussion
4.1 Impact of the Pre-annotation Accuracy
on Precision and Inter-annotator
Agreement
The quality of the annotations created during ex-
periment 1 was evaluated using two methods.
First, we considered the original Penn Treebank
annotations as reference and calculated a simple
precision as compared to this reference. Figure 1
gives an overview of the obtained results (note that
the scale is not regular).
However, this is not sufficient to evaluate the
quality of the annotation as, actually, the reference
annotation is not perfect (see below). We therefore
evaluated the reliability of the annotation, calcu-
lating the inter-annotator agreement between An-
notator1 and Annotator2 on the 100-sentence se-
ries they both annotated. We calculated this agree-
ment on some of the subcorpora using pi, aka Car-
letta?s Kappa (Carletta, 1996)4. The results of this
are shown in table 2.
3During this manual annotation step (with no pre-
annotation), we noticed that the annotators used the
Find/Replace all feature of the text editor to fasten
the tagging of some obvious tokens like the or Corp., which
partly explains that the first groups of 10 sentences took
longer to annotate. Also, as no specific interface was use to
help annotating, a (very) few typographic errors were made,
such as DET instead of DT.
4For more information on the terminology issue, refer to
the introduction of (Artstein and Poesio, 2008).
Subcorpus pi
1-100 0.955
301-400 0.963
Table 2: Inter-annotator agreement on subcorpora
The results show a very good agreement accord-
ing to all scales (Krippendorff, 1980; Neuendorf,
2002; Krippendorff, 2004) as pi is always superior
to 0.9. Besides, it improves with training (from
0.955 at the beginning to 0.963 at the end).
We also calculated pi on the corpus we used to
evaluate the pre-annotation bias (Experiment 3).
The results of this are shown in table 3.
Subcorpus Nb sent. pi
No pre-annotation 50 0.947
MElt50en 25 0.944
MEltALLen 25 0.983
Table 3: Inter-annotator agreement on subcorpora
used to evaluate bias
Here again, the results are very good, though a
little bit less so than at the beginning of the mixed
annotation session. They are almost perfect with
MEltALLen .
Finally, we calculated pi throughout Experi-
ment 2. The results are given in Figure 2 and,
apart from a bizarre peak at MElt50en, they show a
steady progression of the accuracy and the inter-
annotator agreement, which are correlated. As for
the MElt50en peak, it does not appear in Figure 1, we
therefore interpret it as an artifact.
4.2 Impact of the Pre-annotation Accuracy
on Annotation Time
Before discussing the results of Experiment 2, an-
notation time measurements during Experiment 3
confirm that using a good quality pre-annotation
(say, MEltALLen ) strongly reduces the annotation
time as compared with fully manual annotation.
For example, Annotator1 needed an average time
of approximately 7.5 minutes to annotate 10 sen-
tences without pre-annotation (Experiment 3),
whereas Experiment 2 shows that it goes down to
approximately 2.5 minutes when using MEltALLen
pre-annotation. For Annotator2, the correspond-
ing figures are respectively 11.5 and 2.5 minutes.
Figure 3 shows the impact on the pre-annotation
type on annotation times. Surprisingly, only the
worst tagger (MElt10en) produces pre-annotations
that lead to a significantly slower annotation. In
59
Figure 1: Accuracy of annotation
other words, a 96.4% accurate pre-annotation does
not significantly speed up the annotation process
with respect to a 81.6% accurate pre-annotation.
This is very interesting, since it could mean that
the development of a POS-annotated corpus for a
new language with no POS tagger could be drasti-
cally sped up. Annotating approximately 50 sen-
tences could be sufficient to train a POS tagger
such as MElt and use it as a pre-annotator, even
though its quality is not yet satisfying.
One interpretation of this could be the follow-
ing. Annotation based on pre-annotations involves
two different tasks: reading the pre-annotated sen-
tence and replacing incorrect tags. The reading
task takes a time that does not really depends on
the pre-annotation quality. But the correction task
takes a time that is, say, linear w.r.t. the num-
ber of pre-annotation errors. Therefore, when the
number of pre-annotation errors is below a cer-
tain level, the correction task takes significantly
less time than the reading task. Therefore, be-
low this level, variations in the pre-annotation er-
ror rate do not lead to significant overall annota-
tion time. Apparently, this threshold is between
66.5% and 81.6% pre-annotation accuracy, which
can be reached with a surprisingly small training
corpus.
4.3 Bias Induced by Pre-annotation
We evaluated both the bias induced by a pre-
annotation with the best tagger, MEltALLen , and the
one induced by one of the least accurate taggers,
MElt50en. The results are given in table 4 and 5, re-
spectively.
They show a very different bias according to
the annotator. Annotator2?s accuracy raises from
94.6% to 95.2% with a 81.6% accuracy tagger
(MElt50en) and from 94.1% to 97.1% with a 96.4%
accuracy tagger (MEltALLen ). Therefore, Annota-
tor2, whose accuracy is less than that of Annota-
tor1 under all circumstances (see figure 1), seems
to be positively influenced by pre-annotation,
whether it be good or bad. The gain is however
much more salient with the best pre-annotation
(plus 3 points).
As for Annotator1, who is the most accurate an-
notator (see figure 1), the results are more surpris-
ing as they show a significant degradation of ac-
curacy, from 98.1 without pre-annotation to 95.8
with pre-annotation using MElt50en, the less accu-
rate tagger. Examining the actual results allowed
us to see that, first, Annotator1 non pre-annotated
version is better than the reference, and second,
the errors made in the pre-annotated version with
MElt50en are so obvious that they can only be due to
a lapse in concentration.
The results, however, remain stable with pre-
annotation using the best tagger (from 98.4 to
98.2), which is consistent with the results obtained
by Dandapat et al (2009), who showed that bet-
ter trained annotators are less influenced by pre-
annotation and show stable performance.
When asked about it, both annotators say
they felt they concentrated more without pre-
60
Figure 2: Annotation accuracy and pi depending on the type of pre-annotation
Annotator No pre-annotation with MEltALLen
Annotator1 98.4 98.2
Annotator2 94.1 97.1
Table 4: Accuracy with or without pre-annotation
with MEltALLen (sentences 451-475)
Annotator No pre-annotation with MElt50en
Annotator1 98.1 95.8
Annotator2 94.6 95.2
Table 5: Accuracy with or without pre-annotation
with MElt50en (sentences 476-500)
annotation. It seems that the rather good results
of the taggers cause the attention of the annotators
to be reduced, even more so as the task is repeti-
tive and tedious. However, annotators also had the
feeling that fully manual annotation could be more
subject to oversights.
These impressions are confirmed by the com-
parison of the contingency tables, as can be seen
from Tables 6, 7 and 8 (in these tables, lines cor-
respond to tags from the annotation and columns
to reference tags; only lines containing at least
one cell with 2 errors or more are shown, with
all corresponding columns). For example, Anno-
tator1 makes more random errors when no pre-
annotation is available and more systematic er-
rors when MEltALLen pre-annotations are used (typ-
ically, JJ instead of VBN, i.e., adjective instead of
past participle, which corresponds to a systematic
trend in MEltALLen ?s results).
JJ VBN
JJ 36 4
(Annotator 1)
JJ NN NNP NNPS VB VBN
JJ 36 4
NN 1 68 2
NNP 24 2
(Annotator 2)
Table 6: Excerpts of the contingency tables for
sentences 451?457 (512 tokens) with MEltALLen
pre-annotation
IN JJ NN NNP NNS RB VBD VBN
JJ 30 2 2
NNS 1 2 40
RB 2 16
VBD 1 17 2
WDT 2
(Annotator 1)
JJ NN RB VBN
JJ 28 3
NN 2 75 1
RB 2 16
VBN 2 10
(Annotator 2)
Table 7: Excerpts of the contingency tables for
sentences 476?500 (523 tokens) with MElt50en pre-
annotation
61
Figure 3: Annotation time depending on the type of pre-annotation
CD DT JJ NN NNP NNS
CD 30 2
JJ 2 72
NN 2 148
NNS 3 68
(Annotator 1)
CD DT IN JJ JJR NN NNP NNS RB VBN
IN 104 2
JJ 2 61 2 1 9
NN 1 4 145
NNPS 2
NNS 1 2 68
RBR 2
(Annotator 2)
Table 8: Excerpts of the contingency tables for
sentences 450?500 (1,035 tokens) without pre-
annotation
5 Conclusion and Further Work
The series of experiments we detailed in this arti-
cle confirms that pre-annotation allows for a gain
in quality, both in terms of accuracy w.r.t. a ref-
erence and in terms of inter-annotator agreement,
i.e., reliability. We also demonstrated that this
comes with biases that should be identified and
notified to the annotators, so that they can be extra
careful during correction. Finally, we discovered
that a surprisingly small training corpus could be
sufficient to build a pre-annotation tool that would
help drastically speeding up the annotation.
This should help developing taggers for under-
resourced languages. In order to check that, we
intend to use this method in a near future to de-
velop a POS tagger for Sorani Kurdish.
We also want to experiment on other, more
precision-driven, annotation tasks, like complex
relations annotation or definition segmentation,
that are more intrinsically complex and for which
there exist no automatic tool as accurate as for
POS tagging.
Acknowledgments
This work was partly realized as part of the
Quaero Programme5, funded by OSEO, French
State agency for innovation.
References
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Stu-
art Roebuck, Richard Tobin, and Xinglong Wang.
2008. Assisted Curation: Does Text Mining Really
Help? In Pacific Symposium on Biocomputing.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the Definitions of the Tre?sor de la Langue
Franc?aise to a Semantic Database of the French Lan-
guage. In Proceedings of the 14th EURALEX Inter-
national Congress, Leeuwarden.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: the Kappa Statistic. Computational
Linguistics, 22:249?254.
5http://quaero.org/
62
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic
Annotation - No Easy Way Out! a Case from Bangla
and Hindi POS Labeling Tasks. In Proceedings of
the third ACL Linguistic Annotation Workshop.
Pascal Denis and Beno??t Sagot. 2009. Coupling an An-
notated Corpus and a Morphosyntactic Lexicon for
State-of-the-art POS Tagging with Less Human Ef-
fort. In Proceedings of PACLIC 2009, Hong-Kong,
China.
Kare?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Vers une me?thodologie d?annotation des
entite?s nomme?es en corpus ? In Actes de la
16e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles 2009 Traitement Automa-
tique des Langues Naturelles 2009, Senlis, France.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12. Sage, Bev-
erly Hills, CA.
Klaus Krippendorff, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marie Mikulova? and Jan S?te?pa?nek. 2009. Annotation
Quality Checking and its Implications for Design
of Treebank (in Building the Prague Czech-English
Dependency Treebank). In Proceedings of the Eight
International Workshop on Treebanks and Linguistic
Theories, volume 4-5, Milan, Italy, December.
Kimberly Neuendorf. 2002. The Content Analysis
Guidebook. Sage, Thousand Oaks CA.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of International Conference on Empirical Methods
in Natural Language Processing, pages 133?142.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the Benefits of Partial
Automatic Pre-labeling for Frame-semantic Anno-
tation. In Proceedings of the Third Linguistic Anno-
tation Workshop, pages 19?26, Suntec, Singapore,
August. Association for Computational Linguistics.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
Training for the Averaged Perceptron POS Tagger.
In EACL ?09: Proceedings of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 763?771, Morristown,
NJ, USA.
63
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 52?60,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A Joint Named Entity Recognition and Entity Linking System
Rosa Stern,1,2 Beno??t Sagot1 and Fre?de?ric Be?chet3
1Alpage, INRIA & Univ. Paris Diderot, Sorbonne Paris Cite? / F-75013 Paris, France
2AFP-Medialab / F-75002 Paris, France
3Univ. Aix Marseille, LIF-CNRS / Marseille, France
Abstract
We present a joint system for named entity
recognition (NER) and entity linking (EL),
allowing for named entities mentions ex-
tracted from textual data to be matched to
uniquely identifiable entities. Our approach
relies on combined NER modules which
transfer the disambiguation step to the EL
component, where referential knowledge
about entities can be used to select a correct
entity reading. Hybridation is a main fea-
ture of our system, as we have performed
experiments combining two types of NER,
based respectively on symbolic and statis-
tical techniques. Furthermore, the statisti-
cal EL module relies on entity knowledge
acquired over a large news corpus using a
simple rule-base disambiguation tool. An
implementation of our system is described,
along with experiments and evaluation re-
sults on French news wires. Linking ac-
curacy reaches up to 87%, and the NER F-
score up to 83%.
1 Introduction
1.1 Textual and Referential Aspects of
Entities
In this work we present a system designed for the
extraction of entities from textual data. Named
entities (NEs), which include person, location,
company or organization names1 must therefore
be detected using named entity recognition (NER)
techniques. In addition to this detection based
on their surface forms, NEs can be identified by
mapping them to the actual entity they denote,
in order for these extractions to constitute use-
ful and complete information. However, because
1The set of possible named entities varies from restric-
tive, as in our case, to wide definitions; it can also include
dates, event names, historical periods, etc.
of name variation, which can be surfacic or en-
cyclopedic, an entity can be denoted by several
mentions (e.g., Bruce Springsteen, Springsteen,
the Boss); conversely, due to name ambiguity, a
single mention can denote several distinct entities
(Orange is the name of 22 locations in the world;
in French, M. Obama can denote both the US
president Barack Obama (M. is an abbreviation of
Monsieur ?Mr?) or his spouse Michelle Obama; in
this case ambiguity is caused by variation). Even
in the case of unambiguous mentions, a clear link
should be established between the surface men-
tion and a uniquely identifiable entity, which is
achieved by entity linking (EL) techniques.
1.2 Entity Approach and Related Work
In order to obtain referenced entities from raw
textual input, we introduce a system based on
the joint application of named entity recognition
(NER) and entity linking (EL), where the NER out-
put is given to the linking component as a set of
possible mentions, preserving a number of am-
biguous readings. The linking process must there-
after evaluate which readings are the most proba-
ble, based on the most likely entity matches in-
ferred from a similarity measure with the context.
NER has been widely addressed by symbolic,
statistical as well as hybrid approaches. Its major
part in information extraction (IE) and other NLP
applications has been stated and encouraged by
several editions of evaluation campaigns such
as MUC (Marsh and Perzanowski, 1998),
the CoNLL-2003 NER shared task
(Tjong Kim Sang and De Meulder, 2003) or
ACE (Doddington et al, 2004), where NER
systems show near-human performances for
the English language. Our system aims at
benefitting from both symbolic and statistical
NER techniques, which have proven efficient
52
but not necessarily over the same type of data
and with different precision/recall tradeoff. NER
considers the surface form of entities; some
type disambiguation and name normalization
can follow the detection to improve the result
precision but do not provide referential infor-
mation, which can be useful in IE applications.
EL achieves the association of NER results with
uniquely identified entities, by relying on an
entity repository, available to the extraction
system and defined beforehand in order to serve
as a target for mention linking. Knowledge about
entities is gathered in a dedicated knowledge base
(KB) to evaluate each entity?s similarity to a given
context. After the task of EL was initiated with
Wikipedia-based works on entity disambiguation,
in particular by Cucerzan (2007) and Bunescu
and Pasca (2006), numerous systems have been
developed, encouraged by the TAC 2009 KB
population task (McNamee and Dang, 2009).
Most often in EL, Wikipedia serves both as an
entity repository (the set of articles referring to
entities) and as a KB about entities (derived from
Wikipedia infoboxes and articles which contain
text, metadata such as categories and hyperlinks).
Zhang et al (2010) show how Wikipedia, by
providing a large annotated corpus of linked
ambiguous entity mentions, pertains efficiently
to the EL task. Evaluated EL systems at TAC
report a top accuracy rate of 0.80 on English data
(McNamee et al, 2010).
Entities that are unknown to the reference
database, called out-of-base entities, are also con-
sidered by EL, when a given mention refers to
an entity absent from the available Wikipedia ar-
ticles. This is addressed by various methods,
such as setting a threshold of minimal similarity
for an entity selection (Bunescu and Pasca, 2006),
or training a separate binary classifier to judge
whether the returned top candidate is the actual
denotation (Zheng et al, 2010). Our approach
of this issue is closely related to the method of
Dredze et al in (2010), where the out-of-base en-
tity is considered as another entry to rank.
Our task differs from EL configurations out-
lined previously, in that its target is entity extrac-
tion from raw news wires from the news agency
Agence France Presse (AFP), and not only link-
ing relying on gold NER annotations: the input
of the linking system is the result of an auto-
matic NER step, which will produce errors of var-
ious kinds. In particular, spans erroneously de-
tected as NEs will have to be discarded by our EL
system. This case, which we call not-an-entity,
contitute an additional type of special situations,
together with out-of-base entities but specific to
our setting. This issue, as well as others of our
task specificities, will be discussed in this paper.
In particular, we use resources partially based on
Wikipedia but not limited to it, and we experiment
on the building of a domain specific entity KB in-
stead of Wikipedia.
Section 2 presents the resources used through-
out our system, namely an entity repository and
an entity KB acquired over a large corpus of news
wires, used in the final linking step. Section 3
states the principles on which the NER compo-
nents of our system relies, and introduces the two
existing NER modules used in our joint architec-
ture. The EL component and the methodology ap-
plied are presented in section 4. Section 5 illus-
trates this methodology with a number of experi-
ments and evaluation results.
2 Entity Resources
Our system relies on two large-scale resources
which are very different in nature:
? the entity database Aleda, automatically
extracted from the French Wikipedia and
Geonames;
? a knowledge base extracted from a large cor-
pus of AFP news wires, with distributional
and contextual information about automati-
cally detected entites.
2.1 Aleda
The Aleda entity repository2 is the result of an ex-
traction process from freely available resources
(Sagot and Stern, 2012). We used the French
Aleda databased, extracted the French Wikipedia3
and Geonames4. In its current development, it pro-
vides a generic and wide coverage entity resource
accessible via a database. Each entity in Aleda is
associated with a range of attributes, either refer-
ential (e.g., the type of the entity among Person,
Location, Organization and Company, the popu-
lation for a location or the gender of a person, etc.)
2Aleda is part of the Alexina project and freely available
at https://gforge.inria.fr/projects/alexina/.
3
www.fr.wikipedia.org
4
www.geonames.org
53
or formal, like the entity?s URI from Wikipedia or
Geonames; this enables to uniquely identify each
entry as a Web resource.
Moreover, a range of possible variants (men-
tions when used in textual content) are associ-
ated to entities entries. Aleda?s variants include
each entity?s canonical name, Geonames location
labels, Wikipedia redirection and disambiguation
pages aliases, as well as dynamically computed
variants for person names, based in particular on
their first/middle/last name structure. The French
Aleda used in this work comprises 870,000 entity
references, associated with 1,885,000 variants.
The main informative attributes assigned to
each entity in Aleda are listed and illustrated by
examples of entries in Tab. 1. The popularity at-
tribute is given by an approximation based on the
length of the entity?s article or the entity?s popu-
lation, from Wikipedia and Geonames entries re-
spectively. Table 1 also details the structure of
Aleda?s variants entries, each of them associated
with one or several entities in the base.
Unlike most EL systems, Wikipedia is not the
entity base we use in the present work; rather,
we rely on the autonomous Aleda database. The
collect of knowledge about entities and their us-
age in context will also differ in that our target
data are news wires, for which the adaptability of
Wikipedia can be questioned.
2.2 Knowledge Acquisition over AFP news
The linking process relies on knowledge about en-
tities, which can be acquired from their usage in
context and stored in a dedicated KB. AFP news
wires, like Wikipedia articles, have their own
structure and formal metadata: while Wikipedia
articles each have a title referring to an entity, ob-
ject or notion, a set of categories, hyperlinks, etc.,
AFP news wires have a headline and are tagged
with a subject (such as Politics or Culture) and
several keywords (such as cinema, inflation or
G8), as well as information about the date, time
and location of production. Moreover, the distri-
bution of entities over news wires can be expected
to be significantly different from Wikipedia, in
particular w.r.t. uniformity, since a small set of
entities forms the majority of occurrences. Our
particular context can thus justify the need for a
domain specific KB.
As opposed to Wikipedia where entities are
identifiable by hyperlinks, AFP corpora provide
no such indications. Wikipedia is in fact a corpus
where entity mentions are clearly and uniquely
linked, whereas this is what we aim at achiev-
ing over AFP?s raw textual data. The acquisi-
tion of domain specific knowledge about enti-
ties from AFP corpora must circumvent this lack
of indications. In this perspective we use an
implementation of a naive linker described in
(Stern and Sagot, 2010). For the main part, this
system is based on heuristics favoring popular en-
tities in cases of ambiguities. An evaluation of
this system showed good accuracy of entity link-
ing (0.90) over the subset of correctly detected en-
tity mentions:5 on the evaluation data, the result-
ing NER reached a precision of 0.86 and a recall
of 0.80. Therefore we rely on the good accuracy
of this system to identify entities in our corpus,
bearing in mind that it will however include cases
of false detections, while knowledge will not be
available on missed entities. It can be observed
that by doing so, we aim at performing a form of
co-training of a new system, based on supervised
machine learning. In particular, we aim at pro-
viding a more portable and systematic method for
EL than the heuristics-based naive linker which
is highly dependent on a particular NER system,
SXPipe/NP, described later on in section 3.2.
The knowledge acquisition was conducted over
a large corpus of news wires (200,000 news items
of the years 2009, 2010 and part of 2011). For
each occurrence of an entity identified as such by
the naive linker, the following features are col-
lected, updated and stored in the KB at the en-
tity level: (i) entity total occurrences and occur-
rences with a particular mention; (ii) entity oc-
currence with a news item topics and keywords,
most salient words, date and location; (iii) entity
co-occurrence with other entity mentions in the
news item. These features are collected for both
entities identified by the naive linker as Aleda?s
entities and mentions recognized by NER pat-
tern based rules; the latter account for out-of-
base entities, approximated by a cluster of all
mentions whose normalization returns the same
string. For instance, if the mentions John Smith
and J. Smith were detected in a document but not
linked to an entity in Aleda, it would be assumed
5This subset is defined by a strict span and type correct
detection, and among the sole entities for which a match in
Aleda or outside of it was identified; the evaluation data is
presented in section 5.1.
54
Entities
ID Type CanonicalName Popularity URI
20013 Loc Kingdom of Spain 46M geon:2510769
10063 Per Michael Jordan 245 wp:Michael Jordan
20056 Loc Orange (California) 136K geon:5379513
10039 Comp Orange 90 wp:Orange (entreprise)
Variants
ID Variant FirstName MidName LastName
20013 Espagne ? ? ?
10063 Jordan ? ? Jordan
10029 George Walker Bush George Walker Bush
10039 Orange ? ? ?
20056 Orange ? ? ?
Table 1: Structure of Entities Entries and Variants in Aleda
that they co-refer to an entity whose normalized
name would be John Smith; this anonymous en-
tity would therefore be stored and identified via
this normalized name in the KB, along with its oc-
currence information.
3 NER Component
3.1 Principles
One challenging subtask of NER is the correct de-
tection of entity mentions spans among several
ambiguous readings of a segment. The other usual
subtask of NER consists in the labeling or classi-
fication of each identified mention with a type; in
our system, this functionality is used as an indica-
tion rather than a final attribute of the denoted en-
tity. The type assigned to each mention will in the
end be the one associated with the matching en-
tity. The segment Paris Hilton can for instance be
split in two consecutive entity mentions, Paris and
Hilton, or be read as a single one. Whether one
reading or the other is more likely can be inferred
from knowledge about entities possibly denoted
by each of these three mentions: depending on the
considered document?s topic, it can be more prob-
able for this segment to be read as the mention
Paris Hilton, denoting the celebrity, rather than
the sequence of two mentions denoting the cap-
ital of France and the hotel company. Based on
this consideration, our system relies on the ability
of the NER module to preserve multiple readings
in its output, in order to postpone to the linker the
appropriate decisions for ambiguous cases. Two
NER systems fitted with this ability are used in our
architecture.
Figure 1: Ambiguous NER output for the segment
Paris Hilton in SXPipe/NP
3.2 Symbolic NER: SXPipe/NP
NP is part of the SXPipe surface processing chain
(Sagot and Boullier, 2008). It is based on a se-
ries of recognition rules and on a large coverage
lexicon of possible entity variants, derived from
the Aleda entity repository presented in section
2.1. As an SXPipe component, NP formalizes the
text input in the form of directed acyclic graphs
(DAGs), in which each possible entity mention
is represented as a distinct transition, as illus-
trated in Figure 1. Possible mentions are labeled
with types among Person, Location, Organization
and Company, based on the information available
about the entity variant in Aleda and on the type
of the rule applied for the recognition.
Figure 1 also shows how an alternative transi-
tion is added to each mention reading of a seg-
ment, in order to account for a possible non-entity
reading (i.e., for a false match returned by the
NER module). When evaluating the adequacy of
each reading, the following EL module will in
fact consider a special not-an-entity candidate as
a possible match for each mention, and select it
as the most probable if competing entity readings
prove insufficiently adequate w.r.t. the considered
context.
55
3.3 Statistical NER: LIANE
The statistical NER system LIANE
(Bechet and Charton, 2010) is based on (i) a
generative HMM-based process used to predict
part-of-speech and semantic labels among Per-
son, Location, Organization and Product for each
input word6, and (ii) a discriminative CRF-based
process to determine the entity mentions? spans
and overall type. The HMM and CRF models
are learnt over the ESTER corpus, consisting in
several hundreds of hours of transcribed radio
broadcast (Galliano et al, 2009), annotated with
the BIO format (table 2). The output of LIANE
investiture NFS O
aujourd?hui ADV B-TIME
a` PREPADE O
Bamako LOC B-LOC
Mali LOC B-LOC
Table 2: BIO annotation for LIANE training
consists in an n-best lists of possible entity
mentions, along with a confidence score assigned
to each result. Therefore it also provides several
readings of some text segments, with alternatives
of entity mention readings.
As shown in (Bechet and Charton, 2010), the
learning model of LIANE makes it particularly
robust to difficult conditions such as non capital-
ization and allows for a good recall rate on various
types of data. This is in opposition with manually
handcrafted systems such as SXPipe/NP, which
can reach high precision rates over the develop-
ment data but prove less robust otherwise. These
considerations, as well as the benefits of a coop-
erations between these two types of systems are
explored in (Be?chet et al, 2011).
By coupling LIANE and SXPipe/NP to perform
the NER step of our architecture, we expect to
benefit from each system?s best predictions and
improving the precision and recall rates. This
is achieved by not enforcing disambiguation of
spans and types at the NER level but by transfer-
ring this possible source of errors to the linking
step, which will rely on entity knowledge rather
than mere surface forms to determine the best
readings, along with the association of mentions
with entity references.
6For the purpose of type consistency across both NER
modules, the NP type Company is merged with Organiza-
tion, and the LIANE mentions typed as Product are ignored
since they are not yet supported by the overall architecture.
Figure 2: Possible readings of the segment Paris
Hilton and ordered candidates
4 Linking Component
4.1 Methodology for Best Reading Selection
As previously outlined, the purpose of our joint
architecture is to infer best entity readings from
contextual similarity between entities and docu-
ments rather than at the surface level during NER.
The linking component will therefore process am-
biguous NER outputs in the following way, illus-
trated by Fig. 2.
1. For each mention returned by the NER mod-
ule, we aim at finding the best fitting entity
w.r.t. the context of the mention occurrence,
i.e., at the document level. This results in
a list of candidate entities associated with
each mention. This candidates set alays in-
cludes the not-an-entity candidate in order to
account for possible false matches returned
by the NER modules.
2. The list of candidates is ordered using a
pointwise ranking model, based on the max-
imum entropy classifier megam.7 The best
scored candidate is returned as a match for
the mention; it can be either an entity present
in Aleda, i.e., a known entity, or an anony-
mous entity, seen during the KB acquisition
but not resolved to a known reference and
identified by a normalized name, or the spe-
cial not-an-entity candidate, which discards
the given mention as an entity denotation.
3. Each reading is assigned a score depending
on the best candidates? scores in the reading.
The key steps of this process are the selection
of candidates for each mention, which must reach
a sufficient recall in order to ensure the reference
resolution, and the building of the feature vec-
tor for each mention/entity pair, which will be
evaluated by the candidate ranker to return the
7http://www.cs.utah.edu/
?
hal/megam/
56
most adequate entity as a match for the mention.
Throughout this process, the issues usually raised
by EL must be considered, in particular the ability
for the model to learn cases of out-of-base enti-
ties, which our system addresses by forming a set
of candidates not only from the entity reference
base (i.e., Aleda), but also from the dedicated KB
where anonymous entities are also collected. Fur-
thermore, unlike the general configuration of EL
tasks, such as the TAC KB population task (sec-
tion 1.2), our input data does not consist in men-
tions to be linked but in multiple possibilities of
mention readings, which adds to our particular
case the need to identify false matches among the
queries made to the linker module.
4.2 Candidates Selection
For each mention detected in the NER output, the
mention string or variant is sent as a query to
the Aleda database. Entity entries associated with
the given variant are returned as candidates. The
set of retrieved entities, possibly empty, consti-
tutes the candidate set for the mention. Because
the knowledge acquisition included the extraction
of unreferenced entities identified by normalized
names (section 2.2), we can send the normaliza-
tion of the mention as an additional query to our
KB. If a corresponding anonymous entity is re-
turned, we can create an anonymous candidate
and add it to the candidate set. Anonymous candi-
dates account for the possibility of an out-of-base
entity denoted by the given mention, with respec-
tively some and no information about the potential
entity they might stand for. Finally, the set is aug-
mented with the special not-an-entity candidate.
4.3 Features for Candidates Ranking
For each pair formed by the considered mention
and each entity from the candidate set, we com-
pute a feature vector which will be used by our
model for assessing the probability that it repre-
sents a correct mention/entity linking. The vec-
tor contains attributes pertaining to the mention,
the candidate and the document themselves, and
to the relations existing between them.
Entity attributes Entity attributes present in
Aleda and the KB are used as features: Aleda pro-
vides the entity type, a popularity indication and
the number of variants associated with the entity.
We retrieve from the KB the entity frequency over
the corpus used for knowledge acquisition.
Mention attributes At the mention level, the
feature set considers the absence or presence of
the mention as a variant in Aleda (for any en-
tity), its occurrence frequency in the document,
and whether similar variants, possibly indicating
name variation of the same entity, are present in
the document (similar variants can have a string
equal to the mention?s string, longer or shorter
than the mention?s string, included in the men-
tion?s string or including it). In the case of a
mention returned by LIANE, the associated con-
fidence score is also included in the feature set.
Entity/mention relation The comparison be-
tween the surface form of the entity?s canonical
name and the mention gives a similarity rate fea-
ture. Also considered as features are the relative
occurrence frequency of the entity w.r.t. the whole
candidate set, the existence of the mention as a
variant for the entity in Aleda, the presence of
the candidate?s type (retrieved from Aleda) in the
possible mention types provided by the NER. The
KB indicates frequency of its occurrences with the
considered mention, which adds another feature.
Document/entity similarity Document metadata
(in particular topics and keywords) are inherited
by the mention and can thus characterize the en-
tity/mention pair. Equivalent information was col-
lected for entities and stored in the KB, which al-
lows to compute a cosine similarity between the
document and the candidate. Moreover, the most
salient words of the document are compared to the
ones most frequently associated with the entity in
the KB. Several atomic and combined features are
derived from these similarity measures.
Other features pertain to the NER output con-
figuration, as well as possible false matches:
NER combined information One of the two
available NER modules is selected as the base
provider for entity mentions. For each mention
which is also returned by the second NER mod-
ule, a feature is instanciated accordingly.
Non-entity features In order to predict cases of
not-an-entity readings of a mention, we use a
generic lexicon of French forms (Sagot, 2010)
where we check for the existence of the mention?s
variant, both with and without capitalization. If
the mention?s variant is the first word of the sen-
tence, this information is added as a feature.
These features represent attributes of the en-
tity/mention pair which can either have a boolean
value (such as variant presence or absence in
57
Aleda) or range throughout numerical values
(e.g., entity frequencies vary from 0 to 201,599).
In the latter case, values are discretized. All fea-
tures in our model are therefore boolean.
4.4 Best Candidate Selection
Given the feature vector instanciated for an (can-
didate entity, mention) pair, our model assigns it a
score. All candidates in the subset are then ranked
accordingly and the first candidate is returned as
the match for the current mention/entity linking.
Anonymous and not-an-entity candidates, as de-
fined earlier and accounting respectively for po-
tential out-of-base entity linking and NER false
matches, are included in this ranking process.
4.5 Ranking of Readings
The last step of our task consists in the ranking
of multiple readings and has yet to be achieved in
order to obtain an output where entity mentions
are linked to adequate entities. In the case of a
reading consisting in a single transition, i.e., a sin-
gle mention, the score is equal to the best candi-
date?s score. In case of multiple transitions and
mentions, the score is the minimum among the
best candidates? scores, which makes a low entity
match probability in a mention sequence penaliz-
ing for the whole reading. Cases of false matches
returned by the NER module can therefore be dis-
carded as such in this step, if an overall non-entity
reading of the whole path receives a higher score
than the other entity predictions.
5 Experiments and Evaluation
5.1 Training and Evaluation Data
We use a gold corpus of 96 AFP news items in-
tended for both NER and EL purposes: the manual
annotation includes mention boundaries as well as
an entity identifier for each mention, correspond-
ing to an Aleda entry when present or the normal-
ized name of the entity otherwise. This allows for
the model learning to take into account cases of
out-of-base entities. This corpus contains 1,476
mentions, 437 distinct Aleda?s entries and 173 en-
tities absent from Aleda. All news items in this
corpus are dated May and June 2009.
In order for the model to learn from cases of
not-an-entity, the training examples were aug-
mented with false matches from the NER step, as-
sociated with this special candidate and the pos-
itive class prediction, while other possible candi-
dates were associated with the negative class. Us-
ing a 10-fold cross-validation, we used this corpus
for both training and evaluation of our joint NER
and EL system.
It should be observed that the learning step con-
cerns the ranking of candidates for a given men-
tion and context, while the final purpose of our
system is the ranking of multiple readings of sen-
tences, which takes place after the application of
our ranking model for mention candidates. Thus
our system is evaluated according to its ability to
choose the right reading, considering both NER re-
call and precision and EL accuracy, and not only
the latter.
5.2 Task Specificities
As outlined in section 1.2, the input for the stan-
dard EL task consists in sets of entity mentions
from a number of documents, sent as queries to a
linking system. Our current task differs in that we
aim at both the extraction and the linking of enti-
ties in our target corpus, which consists in unan-
notated news wires. Therefore, the results of our
system are comparable to previous work when
considering a setting where the NER output is in
fact the gold annotation of our evaluation data,
i.e., when all mention queries should be linked to
an entity. Without modifying the parameters of
our system (i.e., no deactivation of false matches
predictions), we obtain an accuracy of 0.76, in
comparison with a TAC top accuracy of 0.80 and
a median accuracy of 0.70 on English data.8
It is important to observe that our data con-
sists only in journalistic content, as opposed to the
TAC dataset which included various types of cor-
pora. This difference can lead to unequally diffi-
culty levels w.r.t. the EL task, since NER and EL
in journalistic texts, and in particular news wires,
tend to be easier than on other types of corpora.
This comes among other things from the fact that
a small number of popular entities constitute the
majority of NE mention occurrences.
In most systems, EL is performed over noisy
8As explained previously, these figures, as well as the
ones presented later on, cannot be compared with the 0.90
score obtained by the naive linker which we used for the en-
tity KB acquisition. This score is obtained only on mentions
identified by the SXPipe/NP system with the correct span and
type, whereas our system does not consider the mention type
as a contraint for the linking process, and on correct identifi-
cation of a match in or outside of Aleda.
58
Setting NER EL Joint NER+EL
Precision Recall f-measure Accuracy Precision Recall f-measure
SXPipe/NP 0.849 0.768 0.806 0.871 0.669 0.740 0.702
LIANE 0.786 0.891 0.835 0.820 0.730 0.645 0.685
SXPipe/NP- NL 0.775 0.726 0.750 0.875 0.635 0.678 0.656
LIANE- NL 0.782 0.886 0.831 0.818 0.725 0.640 0.680
SXPipe/NP & 2 0.812 0.747 0.778 0.869 0.649 0.705 0.676
LIANE & SXPipe/NP 0.803 0.776 0.789 0.859 0.667 0.689 0.678
Table 3: Joint NER and EL results. Each EL accuracy covers a different set of correctly detected mentions
NER output and participates to the final decisions
about NEs extractions. Therefore the ability of
our system to correctly detect entity mentions in
news content is estimated by computing its pre-
cision, recall and f-measure.9 The EL accuracy,
i.e., the rate of correctly linked mentions, is mea-
sured over the subset of mentions whose reading
was adequately selected by the final ranking. The
evaluation of our system has been conducted over
the corpus described previously with settings pre-
sented in the next section.
5.3 Settings and results
We used each of the two available NER modules
as a provider for entity mentions, either on its
own or together with the second system, used
as an indicator. For each of these settings, we
tried a modified setting in which the prediction
of the naive linker (NL) used to build the en-
tity KB (section 2.2) was added as a feature to
each mention/candidate pair (settings SXPipe/NP-
NL and LIANE-NL). These experiments? results
are reported in Table 3 and are given in terms of:
? NER precision, recall and f-measure;
? EL accuracy over correctly recognized enti-
ties; therefore, the different figures in col-
umn EL Accuracy are not directly compara-
ble to one another, as they are not obtained
over the same set of mentions;
? joint NER+EL precision, recall and f-
measure; the precision/recall is computed as
the product of the NER precision/recall by the
EL accuracy.
9Only mention boundaries are considered for NER evalu-
ation, while other settings require correct type identification
for validating a fully correct detection. In our case, NER is
not a final step, and entity typing is derived from the entity
linking result.
As expected, SXPipe/NP performs better as far
as NER precision is concerned, and LIANE per-
forms better as far as NER recall is concerned.
However, the way we implemented hybridation
at the NER level does not seem to bring improve-
ments. Using the output of the naive linker as a
feature leads to similar or slightly lower NER pre-
cision and recall. Finally, it is difficult to draw
clear-cut comparative conclusions at this stage
concerning the joint NER +EL task.
6 Conclusion and Future Work
We have described and evaluated various settings
for a joint NER and EL system which relies on the
NER systems SXPipe/NP and LIANE for the NER
step. The EL step relies on a hybrid model, i.e., a
statistical model trained on a manually annotated
corpus. It uses features extracted from a large cor-
pus automatically annotated and where entity dis-
ambiguations and matches were computed using
a basic heuristic tool. The results given in the pre-
vious section show that the joint model allows for
good NER results over French data. The impact of
the hybridation of the two NER modules over the
EL task should be further evaluated. In particu-
lar, we should investigate the situations where an
mention was incorrectly detected (e.g., the span is
not fully correct) although the EL module linked it
with the correct entity. Moreover, a detailed eval-
uation of out-of-base linkings vs. linking in Aleda
remains to be performed.
In the future, we aim at exploring various addi-
tional features in the EL system, in particular more
combinations of the current features. The adapta-
tion of our learning model to NER combinations
should also be improved. Finally, a larger set of
training data should be considered. This shall be-
come possible with the recent manual annotation
of a half-million word French journalistic corpus.
59
References
F. Bechet and E Charton. 2010. Unsupervised knowl-
edge acquisition for extracting named entities from
speech. In 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
F. Be?chet, B. Sagot, and R. Stern. 2011.
Coope?ration de me?thodes statistiques et sym-
boliques pour l?adaptation non-supervise?e d?un
syste`me d?e?tiquetage en entite?s nomme?es. In Actes
de la Confe?rence TALN 2011, Montpellier, France.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 2007, pages 708?716.
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The automatic content extraction (ace)
program-tasks, data, and evaluation. In Proceed-
ings of LREC - Volume 4, pages 837?840.
M. Dredze, P. McNamee, D. Rao, A. Gerber, and
T. Finin. 2010. Entity disambiguation for knowl-
edge base population. In Proceedings of the 23rd
International Conference on Computational Lin-
guistics, pages 277?285.
S. Galliano, G. Gravier, and L. Chaubard. 2009. The
Ester 2 Evaluation Campaign for the Rich Tran-
scription of French Radio Broadcasts. In Inter-
speech 2009.
E. Marsh and D. Perzanowski. 1998. Muc-7 eval-
uation of ie technology: Overview of results. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7) - Volume 20.
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC).
P. McNamee, H.T. Dang, H. Simpson, P. Schone, and
S.M. Strassel. 2010. An evaluation of technologies
for knowledge base population. Proc. LREC2010.
B. Sagot and P. Boullier. 2008. SXPipe 2 : ar-
chitecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues
(T.A.L.), 49(2):155?188.
B. Sagot and R. Stern. 2012. Aleda, a free large-
scale entity database for French. In Proceedings of
LREC. To appear.
B. Sagot. 2010. The Lefff , a freely available and
large-coverage morphological and syntactic lexicon
for French. In Proceedings of the 7th Language
Resources and Evaluation Conference (LREC?10),
Vallette, Malta.
R. Stern and B. Sagot. 2010. De?tection et re?solution
d?entite?s nomme?es dans des de?pe?ches d?agence.
In Actes de la Confe?rence TALN 2010, Montre?al,
Canada.
E. F. Tjong Kim Sang and F. De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of CoNLL, pages 142?147, Edmonton, Canada.
W. Zhang, J. Su, C.L. Tan, and W.T. Wang. 2010. En-
tity linking leveraging: automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1290?1298.
Z. Zheng, F. Li, M. Huang, and X. Zhu. 2010. Learn-
ing to link entities with knowledge base. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 483?491.
60
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?61,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Statistical Parsing of Spanish and Data Driven Lemmatization
Joseph Le Roux? Beno?t Sagot? Djam? Seddah?,?
? Laboratoire d?Informatique Paris Nord, Universit? Paris Nord, UMR CNRS 7030
?Alpage, INRIA & Universit? Paris Diderot
? Universit? Paris Sorbonne
leroux@univ-paris13.fr, benoit.sagot@inria.fr, djame.seddah@paris-sorbonne.fr
Abstract
Although parsing performances have greatly
improved in the last years, grammar inference
from treebanks for morphologically rich lan-
guages, especially from small treebanks, is
still a challenging task. In this paper we in-
vestigate how state-of-the-art parsing perfor-
mances can be achieved on Spanish, a lan-
guage with a rich verbal morphology, with a
non-lexicalized parser trained on a treebank
containing only around 2,800 trees. We rely
on accurate part-of-speech tagging and data-
driven lemmatization to provide parsing mod-
els able to cope lexical data sparseness. Pro-
viding state-of-the-art results on Spanish, our
methodology is applicable to other languages
with high level of inflection.
1 Introduction
Grammar inference from treebanks has become the
standard way to acquire rules and weights for pars-
ing devices. Although tremendous progress has
been achieved in this domain, exploiting small tree-
banks is still a challenging task, especially for lan-
guages with a rich morphology. The main difficulty
is to make good generalizations from small exam-
ple sets exhibiting data sparseness. This difficulty
is even greater when the inference process relies
on semi-supervised or unsupervised learning tech-
niques which are known to require more training ex-
amples, as these examples do not explicitly contain
all the information.
In this paper we want to explore how we can cope
with this difficulty and get state-of-the-art syntac-
tic analyses with a non-lexicalized parser that uses
modern semisupervised inference techniques. We
rely on accurate data-driven lemmatization and part-
of-speech tagging to reduce data sparseness and ease
the burden on the parser. We try to see how we
can improve parsing structure predictions solely by
modifying the terminals and/or the preterminals of
the trees. We keep the rest of the tagset as is.
In order to validate our method, we perform ex-
periments on the Cast3LB constituent treebank for
Spanish (Castillan). This corpus is quite small,
around 3,500 trees, and Spanish is known to have
a rich verbal morphology, making the tag set quite
complex and difficult to predict. Cowan and Collins
(2005) and Chrupa?a (2008) already showed inter-
esting results on this corpus that will provide us with
a comparison for this work, especially on the lexical
aspects as they used lexicalized frameworks while
we choose PCFG-LAs.
This paper is structured as follows. In Section 2
we describe the Cast3LB corpus in details. In Sec-
tion 3 we present our experimental setup and results
which we discuss and compare in Section 4. Finally,
Section 5 concludes the presentation.
2 Data Set
The Castillan 3LB treebank (Civit and Mart?, 2004)
contains 3,509 constituent trees with functional an-
notations. It is divided in training (2,806 trees), de-
velopment (365 trees) and test (338 trees).
We applied the transformations of Chrupa?a
(2008) to the corpus where CP and SBAR nodes
are added to the subordinate and relative clauses but
we did not perform any other transformations, like
the coordination modification applied by Cowan and
Collins (2005).
The Cast3LB tag set is rich. In particular part-of-
speech (POS) tags are fine-grained and encode pre-
cise morphological information while non-terminal
tags describe subcategorization and function labels.
55
Without taking functions into account, there are 43
non-terminal tags. The total tag set thus comprises
149 symbols which makes the labeling task chal-
lenging.
The rich morphology of Spanish can be observed
in the treebank through word form variation. Table 1
shows some figures extracted from the corpus (train-
ing, development and test). In particular the word
form/lemma ratio is 1.54, which is similar to other
Romance language treebanks (French FTB and Ital-
ian ITB).
# of tokens 94 907
# of unique word forms 17 979
# of unique lemmas 11 642
ratio word form/lemma 1.54
Table 1: C3LB properties
Thus, we are confronted with a small treebank
with a rich tagset and a high word diversity. All
these conditions make the corpus a case in point for
building a parsing architecture for morphologically-
rich languages.
3 Experiments
We conducted experiments on the Cast3LB develop-
ment set in order to test various treebank modifica-
tions, that can be divided in two categories: (i) mod-
ification of the preterminal symbols of the treebank
by using simplified POS tagsets; (ii) modification of
the terminal symbols of the treebank by replacing
word tokens by lemmas.
3.1 Experimental Setup
In this section we describe the parsing formalism
and POS tagging settings used in our experiments.
PCFG-LAs To test our hypothesis, we use the
grammatical formalism of Probabilistic Context-
Free Grammars with Latent Annotations (PCFG-
LAs) (Matsuzaki et al, 2005; Petrov et al, 2006).
These grammars depart from the standard PCFGs by
automatically refining grammatical symbols during
the training phase, using unsupervised techniques.
They have been applied successfully to a wide range
of languages, among which French (Candito and
Seddah, 2010), German (Petrov and Klein, 2008),
Chinese and Italian (Lavelli and Corazza, 2009).
For our experiments, we used the LORG PCFG-
LA parser implementing the CKY algorithm. This
software also implements the techniques from Attia
et al (2010) for handling out-of-vocabulary words,
where interesting suffixes for part-of-speech tagging
are collected on the training set, ranked according
to their information gain with regards to the part-
of-speech tagging task. Hence, all the experiments
are presented in two settings. In the first one, called
generic, unknown words are replaced with a dummy
token UNK, while in the second one, dubbed IG, we
use the collected suffixes and typographical infor-
mation to type unknown words.1 We retained the 30
best suffixes of length 1, 2 and 3.
The grammar was trained using the algorithm
of Petrov and Klein (2007) using 3 rounds of
split/merge/smooth2 . For lexical rules, we applied
the strategy dubbed simple lexicon in the Berkeley
parser. Rare words ? words occurring less than 3
times in the training set ? are replaced by a special
token, which depends on the OOV handling method
(generic or IG), before collecting counts.
POS tagging We performed parsing experiments
with three different settings regarding POS infor-
mation provided as an input to the parser: (i) with
no POS information, which constitutes our base-
line; (ii) with gold POS information, which can be
considered as a topline for a given parser setting;
(iii) with POS information predicted using the MElt
POS-tagger (Denis and Sagot, 2009), using three
different tagsets that we describe below.
MElt is a state-of-the-art sequence labeller that
is trained on both an annotated corpus and an ex-
ternal lexicon. The standard version of MElt relies
on Maximum-Entropy Markov models (MEMMs).
However, in this work, we have used a multiclass
perceptron instead, as it allows for much faster train-
ing with very small performance drops (see Table 2).
For training purposes, we used the training section
of the Cast3LB (76,931 tokens) and the Leffe lexi-
con (Molinero et al, 2009), which contains almost
800,000 distinct (form, category) pairs.3
We performed experiments using three different
1Names generic and IG originally come from Attia et al
(2010).
2We tried to perform 4 and 5 rounds but 3 rounds proved to
be optimal on this corpus.
3Note that MElt does not use information from the exter-
56
TAGSET baseline reduced2 reduced3
Nb. of tags 106 42 57
Multiclass Perceptron
Overall Acc. 96.34 97.42 97.25
Unk. words Acc. 91.17 93.35 92.30
Maximum-Entropy Markov model (MEMM)
Overall Acc. 96.46 97.42 97.25
Unk. words Acc. 91.57 93.76 92.87
Table 2: MElt POS tagging accuracy on the Cast3LB
development set for each of the three tagsets. We pro-
vide results obtained with the standard MElt algorithm
(MEMM) as well as with the multiclass perceptron, used
in this paper, for which training is two orders of magni-
tude faster. Unknown words represent as high as 13.5 %
of all words.
tagsets: (i) a baseline tagset which is identical
to the tagset used by Cowan and Collins (2005)
and Chrupa?a (2008); with this tagset, the training
corpus contains 106 distinct tags;
(ii) the reduced2 tagset, which is a simplification
of the baseline tagset: we only retain the first two
characters of each tag from the baseline tagset; with
this tagset, the training corpus contains 42 distinct
tags;
(iii) the reduced3 tagset, which is a variant of
the reduced2 tagset: contrarily to the reduced2
tagset, the reduced3 tagset has retained the mood
information for verb forms, as it proved relevant
for improving parsing performances as shown by
(Cowan and Collins, 2005); with this tagset, the
training corpus contains 57 distinct tags.
Melt POS tagging accuracy on the Cast3LB de-
velopment set for these three tagsets is given in Ta-
ble 2, with overall figures together with figures com-
puted solely on unknown words (words not attested
in the training corpus, i.e., as high as 13.5 % of all
tokens).
3.2 Baseline
The first set of experiments was conducted with the
baseline POS tagset. Results are summarized in Ta-
ble 3. This table presents parsing statistics on the
Cast3LB development set in the 3 POS settings in-
nal lexicon as constraints, but as features. Therefore, the set of
categories in the external lexicon need not be identical to the
tagset. In this work, the Leffe categories we used include some
morphological information (84 distinct categories).
troduced above (i) no POS provided, (ii) gold POS
provided and (iii) predicted POS provided. For each
POS tagging setting it shows labeled precision, la-
beled recall, labeled F1-score, the percentage of ex-
act match and the POS tagging accuracy. The latter
needs not be the same as presented in Section 3.1 be-
cause (i) punctuation is ignored and (ii) if the parser
cannot use the information provided by the tagger,
it is discarded and the parser performs POS-tagging
on its own.
MODEL LP LR F1 EXACT POS
Word Only
Generic 81.42 81.04 81.23 14.47 90.89
IG 80.15 79.60 79.87 14.19 85.01
Gold POS
Generic 87.83 87.49 87.66 30.59 99.98
IG 86.78 86.53 86.65 27.96 99.98
Pred. POS
Generic 84.47 84.39 84.43 22.44 95.82
IG 83.60 83.66 83.63 21.78 95.82
Table 3: Baseline PARSEVAL scores on Cast3LB dev. set
(? 40 words)
As already mentioned above, this tagset contains
106 distinct tags. On the one hand it means that POS
tags contain useful information. On the other hand it
also means that the data is already sparse and adding
more sparseness with the IG suffixes and typograph-
ical information is detrimental. This is a major dif-
ference between this POS tagset and the two follow-
ing ones.
3.3 Using simplified tagsets
We now turn to the modified tagsets and measure
their impact on the quality of the syntactic analyses.
Results are summarized in Table 4 for the reduced2
tagset and in Table 5 for reduced3. In these two set-
tings, we can make the following remarks.
? Parsing results are better with reduced3, which
indicates that verbal mood is an important fea-
ture for correctly categorizing verbs at the syn-
tactic level.
? When POS tags are not provided, using suffixes
and typographical information improves OOV
word categorization and leads to a better tag-
ging accuracy and F1 parsing score (78.94 vs.
81.81 for reduced2 and 79.69 vs. 82.44 for re-
duced3).
57
? When providing the parser with POS tags,
whether gold or predicted, both settings show
an interesting difference w.r.t. to unknown
words handling. When using reduced2, the IG
setting is better than the generic one, whereas
the situation is reversed in reduced3. This indi-
cates that reduced2 is too coarse to help finely
categorizing unknown words and that the re-
finement brought by IG is beneficial, however
the added sparseness. For reduced3 it is diffi-
cult to say whether it is the added richness of
the POS tagset or the induced OOV sparseness
that explains why IG is detrimental.
MODEL LP LR F1 EXACT POS
Word Only
Generic 78.86 79.02 78.94 15.23 88.18
IG 81.89 81.72 81.81 16.17 92.19
Gold POS
Generic 86.56 85.90 86.23 26.64 100.00
IG 86.90 86.63 86.77 29.28 100.00
Pred. POS
Generic 84.16 83.81 83.99 21.05 96.76
IG 84.57 84.32 84.45 21.38 96.76
Table 4: PARSEVAL scores on Cast3LB development set
with reduced2 tagset (? 40 words)
MODEL LP LR F1 EXACT POS
Word Only
Generic 79.61 79.78 79.69 14.90 87.29
IG 82.57 82.31 82.44 14.24 91.63
Gold POS
Generic 88.08 87.69 87.89 30.59 100.00
IG 87.56 87.31 87.43 29.61 100.00
Pred. POS
Generic 85.56 85.38 85.47 23.03 96.56
IG 85.32 85.24 85.28 23.36 96.56
Table 5: PARSEVAL scores on Cast3LB development set
with reduced3 tagset (? 40 words)
3.4 Lemmatization Impact
Being a morphologically rich language, Spanish ex-
hibits a high level of inflection similar to several
other Romance languages, for example French and
Italian (gender, number, verbal mood). Furthermore,
Spanish belongs to the pro-drop family and clitic
pronouns are often affixed to the verb and carry
functional marks. This makes any small treebank
of this language an interesting play field for statis-
tical parsing. In this experiment, we want to use
lemmatization as a form of morphological cluster-
ing. To cope with the loss of information, we pro-
vide the parser with predicted POS. Lemmatization
is carried out by the morphological analyzer MOR-
FETTE, (Chrupa?a et al, 2008) while POS tagging
is done by the MElt tagger. Lemmatization perfor-
mances are on a par with previously reported results
on Romance languages (see Table 6)
TAGSET ALL SEEN UNK (13.84%)
baseline 98.39 99.01 94.55
reduced2 98.37 98.88 95.18
reduced3 98.24 98.88 94.23
Table 6: Lemmatization performance on the Cast3LB.
To make the parser less sensitive to lemmatization
and tagging errors, we train both tools on a 20 jack-
kniffed setup4. Resulting lemmas and POS tags are
then reinjected into the train set. The test corpora
is itself processed with tools trained on the unmod-
ified treebank. Results are presented Table 7. They
show an overall small gain, compared to the previ-
ous experiments but provide a clear improvement on
the richest tagset, which is the most difficult to parse
given its size (106 tags).
First, we remark that POS tagging accuracy with
the baseline tagset when no POS is provided is lower
than previously observed. This can be easily ex-
plained: it is more difficult to predict POS with mor-
phological information when morphological infor-
mation is withdrawn from input.
Second, and as witnessed before, reduction of the
POS tag sparseness using a simplified tagset and in-
crease of the lexical sparseness by handling OOV
words using typographical information have adverse
effects. This can be observed in the generic Pre-
dicted POS section of Table 7 where the baseline
tagset is the best option. On the other hand, in IG
Predicted POS, using the reduced3 is better than
baseline and reduced2. Again this tagset is a trade-
off between rich information and data sparseness.
4The training set is split in 20 chunks and each one is pro-
cessed with a tool trained on the 19 other chunks. This enables
the parser to be less sensitive to lemmatization and/or pos tag-
ging errors.
58
TAGSET LR LP F1 EX POS
Word Only ? Generic
baseline 79.70 80.51 80.1 15.23 74.04
reduced2 79.19 79.78 79.48 15.56 89.25
reduced3 79.92 80.03 79.97 13.16 87.67
Word Only ? IG
baseline 80.67 81.32 80.99 15.89 75.02
reduced2 80.54 81.3 80.92 15.13 90.93
reduced3 80.52 80.94 80.73 15.13 88.53
Pred. POS ? Generic
baseline 85.03 85.57 85.30 23.68 95.68
reduced2 83.98 84.73 84.35 23.36 96.78
reduced3 84.93 85.19 85.06 21.05 96.60
Pred. POS ? IG
baseline 84.60 85.06 84.83 23.68 95.68
reduced2 84.29 84.82 84.55 21.71 96.78
reduced3 84.86 85.39 85.12 22.70 96.60
Table 7: Lemmmatization Experiments
In all cases reduced2 is below the other tagsets
wrt. to Parseval F1 although tagging accuracy is bet-
ter. We can conclude that it is too poor from an in-
formational point of view.
4 Discussion
There is relatively few works actively pursued on
statistical constituency parsing for Spanish. The ini-
tial work of Cowan and Collins (2005) consisted
in a thorough study of the impact of various mor-
phological features on a lexicalized parsing model
(the Collins Model 1) and on the performance gain
brought by the reranker of Collins and Koo (2005)
used in conjunction with the feature set developed
for English. Direct comparison is difficult as they
used a different test set (approximately, the concate-
nation of our development and test sets). They report
an F-score of 85.1 on sentences of length less than
40.5
However, we are directly comparable with Chru-
pa?a (2008)6 who adapted the Collins Model 2 to
Spanish. As he was focusing on wide coverage LFG
grammar induction, he enriched the non terminal an-
notation scheme with functional paths rather than
trying to obtain the optimal tagset with respect to
pure parsing performance. Nevertheless, using the
5See http://pauillac.inria.fr/~seddah/
spmrl-spanish.html for details on comparison with that
work.
6We need to remove CP and SBAR nodes to be fairly com-
parable.
same split and providing gold POS, our system pro-
vides better performance (around 2.3 points better,
see Table 8).
It is of course not surprising for a PCFG-LA
model to outperform a Collins? model based lexi-
calized parser. However, it is a fact that, on such
small treebank configurations, PCFG-LA are cru-
cially lacking annotated data. It is only by greatly
reducing the POS tagset and using either a state-of-
the-art tagger or a lemmatizer (or both), that we can
boost our system performance.
The sensitivity of PCFG-LA models to lexical data
sparseness was also shown on French by Seddah
et al (2009). In fact they showed that perfor-
mance of state-of-the-art lexicalized parsers (Char-
niak, Collins models, etc.) were crossing that
of Berkeley parsers when the training set contains
around 2500?3000 sentences. Here, with around
2,800 sentences of training data, we are probably
in a setting where both parser types exhibit simi-
lar performances, as we suspect French and Spanish
to behave in the same way. It is therefore encour-
aging to notice that our approach, which relies on
accurate POS tagging and lemmatization, provides
state-of-the-art performance. Let us add that a simi-
lar method, involving only MORFETTE, was applied
with success to Italian within a PCFG-LA frame-
work and French with a lexicalized parser, both lead-
ing to promising results (Seddah et al, 2011; Seddah
et al, 2010).
5 Conclusion
We presented several experiments reporting the im-
pact of lexical sparseness reduction on non lexical-
ized statistical parsing. We showed that, by using
state-of-the-art lemmatization and POS tagging on
a reduced tagset, parsing performance can be on a
par with lexicalized models that manage to extract
more information from a small corpus exhibiting a
rich lexical diversity. It remains to be seen whether
applying the same kind of simplifications to the rest
of the tagset, i.e. on the internal nodes, can further
improve parse structure quality. Finally, the methods
we presented in this paper are not language specific
and can be applied to other languages if similar re-
sources exist.
59
TAGSET MODE TOKENS ALL ? 70 ? 40
reduced3 Gen. pred. POS 83.92 84.27 85.08
eval. w/o CP/SBAR 84.02 84.37 85.24
baseline IG pred. lemma & POS 84.15 84.40 85.26
eval. w/o CP/SBAR 84.34 84.60 85.45
reduced3 Gen. gold POS 86.21 86.63 87.84
eval. w/o CP/SBAR 86.35 86.77 88.01
baseline gold POS 83.96 84.58 ?
(Chrupa?a, 2008)
Table 8: PARSEVAL F-score results on the Cast3LB test set
Acknowledgments
Thanks to Grzegorz Chrupa?a and Brooke Cowan for
answering our questions and making data available
to us. This work is partly funded by the French Re-
search Agency (EDyLex, ANR-09-COORD-008).
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for arabic, english and
french. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
M. Civit and M. A. Mart?. 2004. Building cast3lb: A
spanish treebank. Research on Language and Compu-
tation, 2(4):549 ? 574.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of spanish. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 795?802. Association for Computa-
tional Linguistics.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings of PACLIC 2009, Hong-Kong, China.
Alberto Lavelli and Anna Corazza. 2009. The berkeley
parser at the evalita 2009 constituency parsing task. In
EVALITA 2009 Workshop on Evaluation of NLP Tools
for Italian.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages 75?
82.
Miguel A. Molinero, Beno?t Sagot, and Lionel Nicolas.
2009. A morphological and syntactic wide-coverage
lexicon for spanish: The leffe. In Proceedings of the
International Conference RANLP-2009, pages 264?
269, Borovets, Bulgaria, September. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2008. Parsing german with
latent variable grammars. In Proceedings of the ACL
Workshop on Parsing German.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
60
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Djam? Seddah, Joseph Le Roux, and Beno?t Sagot. 2011.
Towards using data driven lemmatization for statisti-
cal constituent parsing of italian. In Working Notes of
EVALITA 2011, Rome, Italy, December.
61
