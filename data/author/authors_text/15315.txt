Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613?1624,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data
(and Nothing Else)
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
We present a new language pair agnostic ap-
proach to inducing bilingual vector spaces
from non-parallel data without any other re-
source in a bootstrapping fashion. The pa-
per systematically introduces and describes all
key elements of the bootstrapping procedure:
(1) starting point or seed lexicon, (2) the confi-
dence estimation and selection of new dimen-
sions of the space, and (3) convergence. We
test the quality of the induced bilingual vec-
tor spaces, and analyze the influence of the
different components of the bootstrapping ap-
proach in the task of bilingual lexicon extrac-
tion (BLE) for two language pairs. Results re-
veal that, contrary to conclusions from prior
work, the seeding of the bootstrapping pro-
cess has a heavy impact on the quality of the
learned lexicons. We also show that our ap-
proach outperforms the best performing fully
corpus-based BLE methods on these test sets.
1 Introduction
Bilingual lexicons serve as an indispensable source
of knowledge for various cross-lingual tasks such
as cross-lingual information retrieval (Lavrenko et
al., 2002; Levow et al, 2005) or statistical machine
translation (Och and Ney, 2003). Additionally, they
are a crucial component in cross-lingual knowledge
transfer, where the knowledge about utterances in
one language may be transferred to another. The
utility of the transfer or annotation projection by
means of bilingual lexicons has already been proven
in various tasks such as semantic role labeling (Pado?
and Lapata, 2009; van der Plas et al, 2011), parsing
(Zhao et al, 2009; Durrett et al, 2012; Ta?ckstro?m et
al., 2013b), POS tagging (Yarowsky and Ngai, 2001;
Das and Petrov, 2011; Ta?ckstro?m et al, 2013a), etc.
Techniques for automatic bilingual lexicon ex-
traction (BLE) from parallel corpora on the basis
of word alignment models are well established (Och
and Ney, 2003). However, due to a relative scarce-
ness of parallel data for many language pairs and
domains, alternative approaches that rely on compa-
rable corpora have also gained much interest (e.g.,
Fung and Yee (1998); Rapp (1999)).
The models that rely on non-parallel data typ-
ically represent each word by a high-dimensional
vector in a feature vector space, where the dimen-
sions of the vector are its context features. The con-
text features are typically words co-occurring with
the word in a predefined context.1 The similar-
ity of two words, wS1 given in the source language
LS with vocabulary V S and wT2 in the target lan-
guage LT with vocabulary V T is then computed as
sim(wS1 , wT2 ) = SF (cv(wS1 ), cv(wT2 )). cv(wS1 ) =
[scS1 (c1), . . . , scS1 (cN )] is a context vector for wS1
with N context features ck, where scS1 (ck) denotes
the score for wS1 associated with context feature ck
(similar for wT2 ). SF is a similarity function (e.g.,
cosine, the Kullback-Leibler divergence, the Jaccard
index) operating on the context vectors (Lee, 1999).
When operating with 2 languages, the context fea-
tures cannot be compared directly. Therefore, in
order to compare the feature vectors cv(wS1 ) and
cv(wT2 ), the context features need to span a shared
1The context may be a document, a paragraph, a window of
predefined size around each occurrence of wSi in CS , etc. For
an overview, see, e.g., (Tamura et al, 2012).
1613
bilingual vector space. The standard way of build-
ing a bilingual vector space is to use bilingual lex-
icon entries (Rapp, 1999; Fung and Cheung, 2004;
Gaussier et al, 2004) as dimensions of the space.
However, there seems to be an apparent flaw in
logic, since the methods assume that there exist
readily available bilingual lexicons that are then
used to induce bilingual lexicons! Therefore, the fo-
cus of the researchers has turned to designing BLE
methods that do not rely on any external translation
resources such as machine-readable bilingual lex-
icons and parallel corpora (Haghighi et al, 2008;
Vulic? et al, 2011).
In order to circumvent this issue, one line of re-
cent work aims to bootstrap high-quality bilingual
vector spaces from a small initial seed lexicon. The
seed lexicon is constructed by harvesting identical
or similarly spelled words across languages (Koehn
and Knight, 2002; Peirsman and Pado?, 2010), and it
spans the initial bilingual vector space. The space is
then gradually enriched with new dimensions/axes
during the bootstrapping procedure. The bootstrap-
ping process has already proven its validity in induc-
ing bilingual lexicons for closely similar languages
such as Spanish-Portuguese or Croatian-Slovene
(Fis?er and Ljubes?ic?, 2011), but it still lacks further
generalization to more distant language pairs.
The main goal of this paper is to shed new light
on the bootstrapping approaches to bilingual lexicon
extraction, and to construct a language pair agnos-
tic bootstrapping method that is able to build high-
quality bilingual vector spaces that consequently
lead to high-quality bilingual lexicons for more dis-
tant language pairs where orthographic similarity is
not sufficient to seed bilingual vector spaces. We
aim to answer the following key questions:
? How to seed bilingual vector spaces besides us-
ing only orthographically similar words?
? Is it better to seed bilingual spaces with trans-
lation pairs/dimensions that are frequent in the
corpus, and does the frequency matter at all?
Does the size of the initial seed lexicon matter?
? How to enrich bilingual vector spaces with only
highly reliable dimensions in order to prevent
semantic drift?
With respect to these questions, the main contribu-
tions of this article are:
? We present a complete overview of the frame-
work of bootstrapping bilingual vector spaces
from non-parallel data without any additional
resources. We dissect the bootstrapping pro-
cess and describe all its key components.
? We introduce a new way of seeding the boot-
strapping procedure that does not rely on any
orthographic clues and that yields bilingual
vector spaces of higher quality. We analyze the
impact of different seed lexicons on the quality
of induced bilingual vector spaces.
? We show that in the setting without any ex-
ternal translation resources, our bootstrapping
approach yields lexicons that outperform the
best performing corpus-based BLE methods on
standard test datasets for 2 language pairs.
2 Boostrapping Bilingual Vector Spaces: A
General Overview
This section presents the complete bootstrapping
procedure that starts with an initial seed lexicon
which spans the initial bilingual vector space, while
as the output in each iteration of the procedure it pro-
duces an updated bilingual vector space that can be
used to extract a bilingual lexicon.
2.1 General Framework
We assume that we are solely in possession of a
(non-parallel) bilingual corpus C that is composed
of a sub-corpus CS given in the source language LS ,
and a sub-corpus CT in the target language LT . All
word types that occur in CS constitute a set V S . All
word types in CT constitute a set V T . The goal is to
build a bilingual vector space using only corpus C.
Assumption 1. Dimensions of the bilingual vector
space are one-to-one word translation pairs. For in-
stance, dimensions of a Spanish-English space are
pairs like (perro, dog), (ciencia, science), etc. The
one-to-one constraint (Melamed, 2000), although
not valid in general, simplifies the construction of
the bootstrapping procedure. Z denotes the set of
translation pairs that are the dimensions of the space.
Computing cross-lingual word similarity in a
bilingual vector space. Now, assume that our bilin-
gual vector space consists of N one-to-one word
translation pairs ck = (cSk , cTk ), k = 1, . . . , N . For
each word wSi ? V S , we compute the similarity of
1614
that word with each word wTj ? V T by computing
the similarity between their context vectors cv(wSi )
and cv(wTj ), which are actually their representations
in the N -dimensional bilingual vector space.
The cross-lingual similarity is computed follow-
ing the standard procedure (Gaussier et al, 2004):
(1) For each source word wSi ? V S , build its N -
dimensional context vector cv(wSi ) that consists of
association scores scSk (cSk ), that is, we compute the
strength of association with the ?source? part of each
dimension ck that constitutes the N -dimensional
bilingual space. The association is dependent on the
co-occurrence of wSi and cSk in a predefined context.
Various functions such as the log-likelihood ratio
(LLR) (Rapp, 1999; Ismail and Manandhar, 2010),
TF-IDF (Fung and Yee, 1998), or pointwise mu-
tual information (PMI) (Bullinaria and Levy, 2007;
Shezaf and Rappoport, 2010) are typically used as
weighting functions to quantify the strength of the
association.
(2) Repeat step (1) for each target word wTj ? V T
and build context vectors cv(wTj ) that consist of
scores scTk (cTk ).
(3) Since cSk and cTk address the same dimension
ck in the bilingual vector space for each k =
1, . . . , N , we are able to compute the similarity be-
tween cv(wSi ) and cv(wTj ) using any similarity mea-
sure such as the Jaccard index, the Kullback-Leibler
or the Jensen-Shannon divergence, the cosine mea-
sure, or others (Lee, 1999; Cha, 2007).
The similarity score for two words wSi and wTj
is sim(wSi , wTj ). For each source word wSi , we can
build a ranked listRL(wSi ) that consists of all words
wTj ? V T ranked according to their respective sim-
ilarity scores sim(wSi , wTj ). In the similar fashion,
we can build a ranked list RL(wTj ), for each target
word wTj . We call the top scoring target word wTj
for some source word wSi its translation candidate,
and write TC(wSi ) = wTj . Additionally, we label
the ranked list RL(wSi ) that is pruned at position M
as RLM (wSi ).
Bootstrapping. The key idea of the bootstrapping
approach relies on an insight that highly reliable
translation pairs (wS1 , wT2 ) that are encountered us-
ing the N -dimensional bilingual vector space might
be added as new dimensions of the space. By adding
these new dimensions, it might be possible to extract
more highly reliable translation pairs that were pre-
viously not used as dimensions of the space, and the
iterative procedure repeats until no new dimensions
are found. The induced bilingual vector space may
then be observed as a bilingual lexicon per se, but it
may also be used to find translation equivalents for
other words which are not used to span the space.
Algorithm 1: Bootstrapping a bilingual vector space
Input : Bilingual corpus C = CS ? CT
Initialize: (1) Obtain a one-to-one seed lexicon. The
entries from the lexicon are initial dimensions of the
space: Z0; (2) s = 0;
Bootstrap:
repeat
1: For each wSi ? V
S : compute RL(wSi ) using Zs ;
2: For each wTj ? V
T : compute RL(wTj ) using Zs ;
3: For each wSi ? V
S and wTj ? V
T : score each
translation pair (wSi , TC(w
S
i )) and (TC(w
T
j ), w
T
j )
and add them to a pool of candidate dimensions ;
4: Choose the best candidates from the pool and add
them as new dimensions: Zs+1 ? Zs ? {best} ;
5: Resolve collisions in Zs+1;
6: s? s + 1 ;
until no new dimensions are found (convergence) ;
Output: One-to-one translation pairs? Dimensions of a
bilingual vector space Zfinal
The overview of the procedure as given by alg. 1
reveals these crucial points in the procedure: (Q1)
how to provide initial dimensions of the space? (the
initialization step), (Q2) how to score each trans-
lation pair, estimate their confidence, and how to
choose the best candidates from the pool of candi-
dates? (steps 3 and 4), and (Q3) how to resolve
potential collisions that violate the one-to-one con-
straint? (step 5). We will discuss (Q1) and (Q2) in
more detail later, while we resolve (Q3) following a
simple heuristic as follows:
Assumption 2. In case of collision, dimen-
sions/pairs that are found at later stages of the boot-
strapping process overwrite previous dimensions.
The intuition here is that we expect for the quality of
the space to increase at each stage of the bootstrap-
ping process, and newer translation pairs should be
more confident than the older ones. For instance, if 2
out of N dimensions of a Spanish-English bilingual
space are pairs (piedra,wall) and (tapia,stone), but
then if during the bootstrapping process we extract a
new candidate pair (piedra,stone), we will delete the
former two dimensions and add the latter.
1615
2.2 Initializing Bilingual Vector Spaces
Seeding or initializing a bootstrapping procedure is
often a critical step regardless of the actual task
(McIntosh and Curran, 2009; Kozareva and Hovy,
2010), and it decides whether the complete process
will end as a success or a failure. However, Peirsman
and Pado? (2011) argue that the initialization step is
not crucial when dealing with bootstrapping bilin-
gual vector spaces. Here, we present two different
strategies of initializing the bilingual vector space.
Identical words and cognates. Previous work re-
lies exclusively on identical and similarly spelled
words to build the initial set of dimensions Z0
(Koehn and Knight, 2002; Peirsman and Pado?, 2010;
Fis?er and Ljubes?ic?, 2011). This strategy yields
promising results for closely similar language pairs,
but is of limited use for other language pairs.
High-frequency seeds. Another problem with us-
ing only identical words and cognates as seeds lies in
the fact that many of them might be infrequent in the
corpus, and as a consequence the expressiveness of a
bilingual vector space might be limited. On the other
hand, high-frequency words offer a lot of evidence
in the corpus that could be exploited in the boot-
strapping approach. In order to induce initial trans-
lation pairs, we rely on the framework of multilin-
gual probabilistic topic modeling (MuPTM) (Boyd-
Graber and Blei, 2009; De Smet and Moens, 2009;
Mimno et al, 2009; Zhang et al, 2010), that does
not require a bilingual lexicon, it operates with non-
parallel data, and is able to produce highly confident
translation pairs for high-frequency words (Mimno
et al, 2009; Vulic? and Moens, 2013).2 Therefore,
we can construct the initial seed lexicon as follows:
(1) Train a multilingual topic model on the corpus.
(2) Obtain one-to-one translation pairs using any of
the MuPTM-based models of cross-lingual similar-
ity, e.g., (Vulic? et al, 2011; Vulic? and Moens, 2013).
(3) Retain only symmetric translation pairs. This
step ensures that only highly confident pairs are used
as seed translation pairs.
(4) Rank translation pairs according to their fre-
quency in the corpus and use a subset of the most
2One can also use other models that are similar to MuPTM
such as (Haghighi et al, 2008; Daume? III and Jagarlamudi,
2011) to produce the initial seed lexicon, but that analysis is
beyond the scope of this work.
frequent symmetric pairs as seeds.
2.3 Estimating Confidence of New Dimensions
Another crucial step in the bootstrapping proce-
dure is the estimation of confidence in a translation
pair/candidate dimension. Errors in the early stages
of the procedure may negatively affect the learning
process and even cause semantic drift (Riloff and
Shepherd, 1999; McIntosh and Curran, 2009). We
therefore impose the constraint which requires trans-
lation pairs to be symmetric in order to qualify as po-
tential new dimensions of the space. In other words,
given the current set of dimensions Zs, a transla-
tion pair (wSi , wTj ) has a possibility to be chosen as
a new dimension from the pool of candidate dimen-
sions if and only if it holds: TC(wSi ) = wTj and
TC(wTj ) = wSi . This symmetry constraint should
ensure a relative reliability of translation pairs.
In each iteration of the bootstrapping process, we
may add all symmetric pairs from the pool of candi-
dates as new dimensions, or we could impose addi-
tional selection criteria that quantify the degree of
confidence in translation pairs. We are then able
to rank the symmetric candidate translation pairs in
the pool of candidates according to their confidence
scores (step 3 of alg. 1), and choose only the best
B candidates from the pool in each iteration (step 4)
as done in (Thelen and Riloff, 2002; McIntosh and
Curran, 2009; Huang and Riloff, 2012). By picking
only a subset of the B most confident candidates in
each iteration, we hope to further prevent a possibil-
ity of semantic drift, i.e., ?poisoning? the bootstrap-
ping process that might happen if we include incor-
rect translation pairs as dimensions of the space.
In this paper, we investigate 3 different confidence
estimation functions:3
(1) Absolute similarity score. Confidence of a
translation pair CF (wSi , TC(wSi )) is simply the ab-
solute similarity value sim(wSi , TC(wSi ))
(2) M-Best confidence function. It contrasts the
score of the translation candidate with the average
score over the first M most similar words in the
ranked list. The larger the difference, the more con-
fidence we have in the translation candidate. Given
a word wSi ? V S and a ranked list RLM (wSi ), the
3A symmetrized version of the confidence functions is com-
puted as the geometric mean of source-to-target and target-to-
source confidence scores.
1616
average score of the best M words is computed as:
simM (wSi ) =
1
M
?
wTj ?RLM (w
S
i )
sim(wSi , wTj )
The final confidence score is then:
CF (wSi , TC(wSi )) = sim(wSi , TC(wSi ))? simM (wSi )
(3) Entropy-based confidence function. We adapt
the well-known entropy-based confidence (Smith
and Eisner, 2007; Tu and Honavar, 2012) to this par-
ticular task. First, we need to define a distribution:
p(wTj |wSi ) =
esim(wSi ,wTj )
?
wTl ?V
T esim(wSi ,wTl )
The confidence function is then minus the entropy
of the probability distribution p:
CF (wSi , TC(wSi )) =
?
wTl ?V T
p(wTl |wSi ) log p(wTl |wSi )
3 Experimental Setup
Data collections. We investigate our bootstrapping
approach on the BLE task for 2 language pairs:
Spanish-English (ES-EN) and Italian-English (IT-
EN), and work with the following corpora previ-
ously used by Vulic? and Moens (2013): (i) a col-
lection of 13, 696 Spanish-English Wikipedia arti-
cle pairs (Wiki-ES-EN), (ii) 18, 898 Italian-English
Wikipedia article pairs (Wiki-IT-EN).4
Following (Koehn and Knight, 2002; Haghighi et
al., 2008; Prochasson and Fung, 2011; Vulic? and
Moens, 2013), we use TreeTagger (Schmid, 1994)
for POS-tagging and lemmatization of the corpora,
and then retain only nouns that occur at least 5 times
in the corpus. We record the lemmatized form when
available, and the original form otherwise. Our fi-
nal vocabularies consist of 9, 439 Spanish nouns and
4Vulic? and Moens (2013) also worked with Dutch-English
(NL-EN), but we have decided to leave out the results obtained
for that language pair due to space constraints, high similarity
between the two languages, and the fact that the results obtained
for that language pair are qualitatively similar to the results we
report for ES-EN and IT-EN. Hence including the results for
NL-EN would not contribute to the paper with any new impor-
tant insight and conclusion.
12, 945 nouns for ES-EN, and 7, 160 Italian nouns
and 9, 116 English nouns for IT-EN.
Ground truth. The goal of the BLE task is to ex-
tract a bilingual lexicon of one-to-one translations.
In order to test the quality of bilingual vector spaces
induced by our bootstrapping approach, we evaluate
it on standard 1000 ground truth one-to-one trans-
lation pairs built for the Wiki-ES-EN and Wiki-IT-
EN datasets (Vulic? and Moens, 2013). Note that
we do not explicitly test the bilingual vector space
as a bilingual lexicon, but rather its ability to find
semantically similar words and translations also for
words that are not used as dimensions of the space
(see sect. 2.1).
Evaluation metrics. We measure the performance
on the BLE task using a standard Top M accuracy
(AccM ) metric. It denotes the number of source
words wSi from ground truth translation pairs whose
list RLM (wSi ) contains the correct translation ac-
cording to our ground truth over the total number
of ground truth translation pairs (=1000) (Gaussier
et al, 2004; Tamura et al, 2012).5 Additionally,
we report the mean reciprocal rank (MRR) scores
(Voorhees, 1999) for some experimental runs.
Multilingual topic model. We utilize a straightfor-
ward multilingual extension of the standard Blei et
al.?s LDA model (Blei et al, 2003) called bilingual
LDA (Mimno et al, 2009; Ni et al, 2009; De Smet
and Moens, 2009). BiLDA training follows the pro-
cedure from (Vulic? and Moens, 2013), that is, the
training method is Gibbs sampling with the number
of topics set to K = 2000. Hyperparameters of the
model are set to standard values (Steyvers and Grif-
fiths, 2007): ? = 50/K and ? = 0.01.
Building initial seed lexicons. To produce the lists
of one-to-one translation pairs that are used as seeds
for the bootstrapping approach (see sect. 2.2), we
experiment with the TopicBC and the ResponseBC
methods from (Vulic? and Moens, 2013), which are
the MuPTM-based models of cross-lingual seman-
tic similarity that obtain the best results in the BLE
task on these datasets. In short, the TopicBC method
computes the similarity of two words according to
the similarity of their conditional topic distributions
(Griffiths et al, 2007; Vulic? et al, 2011) using
5We can build a one-to-one bilingual lexicon by harvesting
one-to-one translation pairs (wSi , TC(w
S
i )), and the quality of
that lexicon is best reflected in the Acc1 score.
1617
the Bhattacharyya coefficient (BC) (Kazama et al,
2010) as the similarity function. ResponseBC is a
second-order similarity method. It first computes
initial similarity scores between all words cross-
lingually and monolingually using the cross-lingual
topical space and, in the second step, it computes the
similarity between 2 words as the similarity between
their word vectors that now contain the initial word-
to-word similarity scores with all source and target
words. The similarity function is again BC.
We use these models of similarity as a black box
to acquire seeds for the bootstrapping approach, but
we encourage the interested reader to find more de-
tails about the methods in the relevant literature.
These two models also serve as our baseline models,
and our goal is to test whether we are able to obtain
bilingual lexicons of higher quality using bootstrap-
ping that starts from the output of these models.
Weighting and similarity functions. We have
experimented with different families of weighting
(e.g., PMI, LLR, TF-IDF, chi-square) and similar-
ity functions (e.g., cosine, Dice, Kullback-Leibler,
Jensen-Shannon) (Lee, 1999; Turney and Pantel,
2010). In this paper, we present results obtained
by positive pointwise mutual information (PPMI)
(Niwa and Nitta, 1994) as a weighting function,
which is a standard choice in vector space seman-
tics (Turney and Pantel, 2010), and (combined with
cosine) yields the best results over a group of seman-
tic tasks according to (Bullinaria and Levy, 2007).
We use a smoothed version of PPMI as presented
in (Pantel and Lin, 2002; Turney and Pantel, 2010).
Again, based on the results reported in the relevant
literature (Bullinaria and Levy, 2007; Laroche and
Langlais, 2010; Turney and Pantel, 2010), we opt
for the cosine similarity as a standard choice for SF .
We have also experimented with different window
sizes ranging from 3 to 15 in both directions around
the pivot word, but we have not detected any major
qualitative difference in the results and their inter-
pretation. Therefore, all results reported in the paper
are obtained by setting the window size to 6.
4 Results and Discussion
4.1 Are Seeds Important?
In recent work, Peirsman and Pado? (2010; 2011)
report that ?the size and quality of the (seed) lex-
icon are not of primary importance given that the
bootstrapping procedure effectively helped filter out
incorrect translation pairs and added more newly
identified mutual nearest neighbors.? According to
their findings, (1) noisy translation pairs are cor-
rected in later stages of the bootstrapping process,
since the quality of bilingual vector spaces gradu-
ally increases, (2) the size of the seed lexicon does
not matter since the bootstrapping approach is able
to learn translation pairs that were previously not
present in the seed lexicon. Additionally, they do not
provide any insight whether the frequency of seeds
in the corpus influences the quality of induced bilin-
gual vector spaces. In this paper, we question these
claims with a series of BLE experiments.
All experiments conducted in this section do not
rely on any extra confidence estimation except for
the symmetry constraint, that is, in each step we en-
rich the bilingual vector space with all new symmet-
ric translation pairs (see alg. 1 and sect. 2.3).
Exp. I: Same size, different seedings? The goal
of this experiment is to test whether the quality of
seeds plays an important role in the bootstrapping
approach. We experiment with 3 different seed lex-
icons: (1) Following (Peirsman and Pado?, 2010;
Fis?er and Ljubes?ic?, 2011), we harvest identically
spelled words across 2 languages and treat them
as one-to-one translations. This procedure results
in 459 seed translation pairs for ES-EN, and 431
pairs for IT-EN (SEED-ID), (2) We obtain symmet-
ric translation pairs using the TopicBC method (see
sect. 3) and use 459 pairs that have the highest fre-
quency in the Wiki-ES-EN corpus as seeds for ES-
EN (similarly 431 pairs for IT-EN) (SEED-TB), (3)
As in (2), but we now use the ResponseBC method to
acquire seeds (SEED-RB). The frequency of a one-
to-one translation pair is simply computed as the ge-
ometric mean of the frequencies of words that con-
stitute the translation pair.
Fig. 1(a) and 1(b) display the progress of the same
bootstrapping procedure using the 3 different seed
lexicons. We derive several interesting conclusions:
(i) Regardless of the actual choice of the seeding
method, the bootstrapping process proves its valid-
ity and utility since we observe that the quality of
induced bilingual vector spaces increases over time
for all 3 seeding methods. The bootstrapping proce-
dure converges quickly. The increase is especially
1618
0.2
0.4
0.6
0.8
Acc M
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
Acc1 (SEED-ID)Acc1 (SEED-TB)Acc1 (SEED-RB)Acc10 (SEED-ID)Acc10 (SEED-TB)Acc10 (SEED-RB)
(a) Spanish to English
0.2
0.4
0.6
0.8
Acc M
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
Acc1 (SEED-ID)Acc1 (SEED-TB)Acc1 (SEED-RB)Acc10 (SEED-ID)Acc10 (SEED-TB)Acc10 (SEED-RB)
(b) Italian to English
500
750
1000
1250
1500
1750
Num
ber
ofd
imen
sion
s
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Iteration
SEED-ID (ES-EN)SEED-TB (ES-EN)SEED-RB (ES-EN)SEED-ID (IT-EN)SEED-TB (IT-EN)SEED-RB (IT-EN)
(c) # Dimensions over iterations
Figure 1: Results with 3 different seeding methods as starting points of the bootstrapping process: (i) identical words
only (SEED-ID), (ii) the TopicBC method (SEED-TB), (iii) the ResponseBC method (SEED-RB). (a)AccM scores for
ES-EN; (b) AccM scores for IT-EN; (c) the number of dimensions in the space with the 3 different seeding methods
in each iteration for ES-EN and IT-EN. The bootstrapping procedure typically converges after a few iterations.
0
0.2
0.4
0.6
0.8
1
Acc
M
0 1 2 3 4 5 6 7 8 9 10
Iteration
Acc1 (HF-SEED)Acc1 (MF-SEED)Acc1 (LF-SEED)Acc10 (HF-SEED)Acc10 (MF-SEED)Acc10 (LF-SEED)
(a) Spanish to English
0
0.2
0.4
0.6
0.8
1
Acc
M
0 1 2 3 4 5 6 7 8 9 10
Iteration
Acc1 (HF-SEED)Acc1 (MF-SEED)Acc1 (LF-SEED)Acc10 (HF-SEED)Acc10 (MF-SEED)Acc10 (LF-SEED)
(b) Italian to English
Figure 2: Results on the BLE task with SEED-RB when using seed translation pairs of different frequency: (i) high-
frequency (HF-SEED), (ii) medium-frequency (MF-SEED), (iii) low-frequency (LF-SEED).
prominent in the first few iterations, when the ap-
proach learns more new dimensions (see fig. 1(c)).
(ii) The seeding method is important. A bootstrap-
ping approach that starts with a better seed lexicon
is able to extract bilingual lexicons of higher quality
as reflected in Acc1 scores. Although the bootstrap-
ping approach seems more beneficial when dealing
with noisier seed lexicons (226% increase in terms
of Acc1 for ES-EN and 177% increase for IT-EN
when starting with SEED-ID, compared to 35% in-
crease for ES-EN, and 15% for IT-EN with SEED-
RB), when starting from a noisy seed lexicon such
as SEED-ID the method is unable to reach the same
level of performance. Starting with SEED-ID, the
approach is able to recover noisy dimensions from
an initial bilingual vector space, but it is still unable
to match the results that are obtained when starting
from a better initial space (e.g., SEED-RB).
(iii) SEED-RB produces slightly better results than
SEED-TB (e.g., the final Acc1 of 0.649 for SEED-
RB compared to 0.626 for SEED-TB for IT-EN, and
0.572 compared to 0.553 for ES-EN). This finding is
in line with the results reported in (Vulic? and Moens,
2013) where ResponseBC proved to be a more ro-
bust and a more effective method when applied to
the BLE task directly. In all further experiments we
use ResponseBC to acquire seed pairs, i.e., the seed-
ing method is SEED-RB.
Exp. II: Does the frequency of seeds matter? In
the next experiment, we test whether the frequency
of seeds in the corpus plays an important role in
the bootstrapping process. The intuition is that by
using highly frequent and highly confident transla-
tion pairs the bootstrapping method has more reli-
able clues that help extract new dimensions in sub-
sequent iterations. On the other hand, low-frequency
1619
pairs (although potentially correct one-to-one trans-
lations) do not occur in the corpus and in the con-
texts of other words frequently enough, and are
therefore not sufficient to extract reliable new di-
mensions of the space.
To test the hypothesis, we again obtain all sym-
metric translation pairs using ResponseBC and then
sort them in descending order based on their fre-
quency in the corpus. In total, we retrieve a sorted
list of 2031 symmetric translation pairs for ES-EN,
and 1689 pairs for IT-EN. Following that, we split
the list in 3 parts of equal size: (i) the top third com-
prises translation pairs with the highest frequency in
the corpus (HF-SEED), (ii) the middle third com-
prises pairs of ?medium? frequency (MF-SEED),
(iii) the bottom third are low-frequency pairs (LF-
SEED). We then use these 3 different seed lexicons
of equal size to seed the bootstrapping approach.
Fig. 2(a) and 2(b) show the progress of the boot-
strapping process using these 3 seed lexicons. We
again observe several interesting phenomena:
(i) High-frequency seed translation pairs are better
seeds, and that finding is in line with our hypothesis.
Although the bootstrapping approach again displays
a positive trend regardless of the actual choice of
seeds (we observe an increase even when using LF-
SEED), high-frequency seeds lead to better overall
results in the BLE task. Besides its high presence in
contexts of other words, another advantage of high-
frequency seed pairs is the fact that an initial sim-
ilarity method will typically acquire more reliable
translation candidates for such words (Pekar et al,
2006). For instance, 89.5% of ES-EN pairs in HF-
SEED are correct one-to-one translations, compared
to 65.1% in MF-SEED, and 44.3% in LF-SEED.
(ii) The difference in results between HF-SEED and
MF-SEED is more visible in Acc1 scores. Although
both seed lexicons for all test words provide ranked
lists which contain words that exhibit some semantic
relation to the given word, the reliability and the fre-
quency of translation pairs are especially important
for detecting the relation of cross-lingual word syn-
onymy, that is, the translational equivalence that is
exploited in building one-to-one bilingual lexicons.
Exp. III: Does size matter? The following exper-
iment investigates whether bilingual vector spaces
may be effectively bootstrapped from small high-
quality seed lexicons, and if larger seed lexicons
necessarily lead to bilingual vector spaces of higher
quality as reflected in BLE results. We again retrieve
a sorted list of symmetric translation pairs as in Exp.
II. Following that, we build seed lexicons of vari-
ous sizes by retaining only the first N pairs from
the list, where we vary N from 200 to 1400 in steps
of 200. We also use the entire sorted list as a seed
lexicon (All), and compare the results on the BLE
task with the results obtained by applying the Re-
sponseBC and TopicBC methods directly (Vulic? and
Moens, 2013). The results are summarized in tables
1 and 2. We observe the following:
(i) If we provide a seed lexicon with sufficient en-
tries, the bootstrapping procedure provides compa-
rable results regardless of the seed lexicon size, al-
though results tend to be higher for larger seed lex-
icons (e.g., compare results when starting with 600
and 1200 lexicon entries). When starting with the
size of 600, the bootstrapping approach is able to
find dimensions that were already in the seed lexi-
con of size 1200. The consequence is that, although
bootstrapping with a smaller seed lexicon displays a
slower start (see the difference in results at iteration
0), the performances level after convergence.
(ii) Regardless of the seed lexicon size, the boot-
strapping approach is valuable. It consistently im-
proves the quality of the induced bilingual vector
space, and consequently, the quality of bilingual lex-
icons extracted using that vector space. The positive
impact is more prominent for smaller seed lexicons,
i.e., we observe an increase of 78% for ES-EN when
starting with only 200 seed pairs, compared to an
increase of 15% when starting with 800 seed pairs,
and 10% when starting with 1400 seed pairs.
(iii) The bootstrapping approach outperforms Re-
sponseBC and TopicBC in terms of Acc1 and MRR
scores for both language pairs when the seed lexi-
con provides a sufficient number of entries. How-
ever, in terms of Acc10, TopicBC and ResponseBC
still exhibit comparable (for IT-EN) or even better
(ES-EN) results. Both TopicBC and ResponseBC
are MuPTM-based methods that, due to MuPTM
properties, model the similarity of two words at the
level of documents as contexts, while the bootstrap-
ping approach is a window-based approach that nar-
rows down the context to a local neighborhood of a
word. The MuPTM-based models are better suited
to detect a general topical similarity of words, and
1620
Iteration: 0 2 5 10
Seed lexicon Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
200(?1617) 0.274 0.352 0.525 0.446 0.534 0.713 0.481 0.569 0.753 0.488 0.576 0.752
400(?1563) 0.416 0.499 0.663 0.518 0.602 0.774 0.542 0.620 0.787 0.545 0.625 0.788
600(?1554) 0.459 0.539 0.707 0.550 0.630 0.787 0.573 0.650 0.803 0.578 0.654 0.802
800(?1582) 0.494 0.572 0.728 0.548 0.631 0.799 0.563 0.644 0.802 0.567 0.646 0.806
1000(?1636) 0.516 0.591 0.744 0.563 0.644 0.805 0.578 0.656 0.813 0.581 0.658 0.817
1200(?1740) 0.536 0.613 0.764 0.586 0.661 0.804 0.588 0.664 0.812 0.591 0.667 0.814
1400(?1888) 0.536 0.620 0.776 0.583 0.659 0.808 0.589 0.666 0.815 0.588 0.666 0.818
All-2031(?2437) 0.543 0.625 0.785 0.589 0.667 0.813 0.597 0.675 0.818 0.599 0.677 0.820
TopicBC 0.433 0.576 0.843 ? ? ? ? ? ? ? ? ?
ResponseBC 0.517 0.635 0.891 ? ? ? ? ? ? ? ? ?
Table 1: ES-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number
of dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB.
Iteration: 0 2 5 10
Seed lexicon Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
200(?1255) 0.394 0.469 0.703 0.515 0.595 0.757 0.548 0.621 0.782 0.555 0.628 0.787
400(?1265) 0.546 0.618 0.757 0.623 0.690 0.831 0.639 0.704 0.840 0.644 0.709 0.844
600(?1309) 0.585 0.657 0.798 0.653 0.718 0.856 0.664 0.726 0.859 0.672 0.734 0.862
800(?1365) 0.602 0.672 0.813 0.657 0.723 0.857 0.663 0.726 0.865 0.665 0.730 0.867
1000(?1416) 0.616 0.688 0.828 0.629 0.706 0.853 0.636 0.709 0.857 0.642 0.714 0.861
1200(?1581) 0.628 0.700 0.840 0.655 0.724 0.869 0.664 0.733 0.877 0.668 0.736 0.883
1400(?1749) 0.626 0.701 0.851 0.654 0.727 0.867 0.656 0.728 0.867 0.661 0.733 0.874
All-1689(?2008) 0.616 0.695 0.850 0.643 0.716 0.860 0.653 0.724 0.862 0.654 0.726 0.866
TopicBC 0.578 0.667 0.834 ? ? ? ? ? ? ? ? ?
ResponseBC 0.622 0.729 0.882 ? ? ? ? ? ? ? ? ?
Table 2: IT-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number of
dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB.
are therefore not always able to push the real cross-
lingual synonyms higher in the ranked list of seman-
tically similar words, while the window-based boot-
strapping approach is better tailored to model the
relation of cross-lingual synonymy, i.e., to extract
one-to-one translation pairs (as reflected in Acc1
scores). A similar conclusion for monolingual set-
tings is drawn by Baroni and Lenci (2010).
(iv) Since our bootstrapping approach utilizes Re-
sponseBC or TopicBC as a preprocessing step, it is
obvious that the approach leads to an increased com-
plexity. On top of the initial complexity of Respon-
seBC and TopicBC, the bootstrapping method re-
quires |V S ||V T | comparisons at each iteration, but
given the fact that each wSi ? V S may be processed
independently of any other wSj ? V S in each itera-
tion, the bootstrapping method is trivially paralleliz-
able. That makes the method computationally fea-
sible even for vocabularies larger than the ones re-
ported in the paper.
4.2 Is Confidence Estimation Important?
According to the results from tables 1 and 2, re-
gardless of the seed lexicon size, the bootstrapping
approach does not suffer from semantic drift, i.e.,
if we seed the process with high-quality symmetric
translation pairs, it is able to recover more pairs and
add them as new dimensions of the bilingual vector
space. However, we also study the influence of ap-
plying different confidence estimation functions on
top of the symmetry constraint (see sect 2.3), but we
do not observe any improvement in the BLE results,
regardless of the actual choice of a confidence esti-
mation function. The only observed phenomenon,
as illustrated by fig. 3, is the slower convergence
rate when setting the parameter B to lower values.
The symmetry constraint alone seems to be sufficient
to prevent semantic drift, but it might also be a too
strong and a too conservative assumption, since only
a small portion of all possible translation pairs is
used to span the bilingual vector space (for instance,
1621
0.4
0.45
0.5
0.55
0.6
Ac
c 1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Iteration
B = 30B = 50B = 100B = 150B = 200B = All
Figure 3: The effect of learning rate B on bootstrapping.
Language pair: ES-EN, seed lexicon: SEED-RB with
600 pairs, confidence function: symmetrized M-Best.
when starting with 600 entries for ES-EN, the final
bilingual vector space consists of only 1554 pairs,
while the total number of ES nouns is 9439). One
line of future work will address the construction of
bootstrapping algorithms that also enable the usage
of highly reliable asymmetric pairs as dimensions,
and the confidence estimation functions might have
a more important role in that setting.
5 Conclusion
We have presented a new bootstrapping approach to
inducing bilingual vector spaces from non-parallel
data, and have shown the utility of the induced space
in the BLE task. The approach is fully corpus-based
and, unlike previous work, it does not rely on the
availability of machine-readable translation dictio-
naries or predefined concept categories. We have
systematically described, analyzed and evaluated all
key components of the bootstrapping approach. Re-
sults reveal that, contrary to conclusions from prior
work, the initialization of the bilingual vector space
is especially important. We have presented a novel
approach to initializing the bootstrapping procedure,
and have shown that better results in the BLE task
are obtained by starting from seed lexicons that com-
prise highly reliable high-frequent translation pairs.
The bootstrapping framework presented in the pa-
per is completely language pair independent, which
makes it effectively applicable to any language pair.
In future work, we will investigate other models
of similarity besides TopicBC and ResponseBC (e.g,
the method from (Haghighi et al, 2008)) that could
be used as preliminary models for constructing an
initial bilingual vector space. Furthermore, we plan
to study other confidence functions and explore if
asymmetric translation candidates could also con-
tribute to the bootstrapping method. Finally, we also
plan to test the robustness of our fully corpus-based
bootstrapping approach by porting it to more lan-
guage pairs.
Acknowledgments
We would like to thank the anonymous reviewers for
their useful suggestions. This research has been car-
ried out in the framework of the TermWise Knowl-
edge Platform (IOF-KP/09/001) funded by the In-
dustrial Research Fund, KU Leuven, Belgium.
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?
721.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of UAI, pages 75?82.
John A. Bullinaria and Joseph P. Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39(3):510?526.
Sung-Hyuk Cha. 2007. Comprehensive survey on
distance/similarity measures between probability den-
sity functions. International Journal of Mathematical
Models and Methods in Applied Sciences, 1(4):300?
307.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, pages 600?609.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of ACL-HLT, pages 407?
412.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the Web using in-
terlingual topic modeling. In Proceedings of the CIKM
2009 Workshop on Social Web Search and Mining,
pages 57?64.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings
of EMNLP-CoNLL, pages 1?11.
1622
Darja Fis?er and Nikola Ljubes?ic?. 2011. Bilingual lexicon
extraction from comparable corpora for closely related
languages. In Proceedings of RANLP, pages 125?131.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING, pages 414?420.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of ACL, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proceedings
of EACL, pages 286?295.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora using
in-domain terms. In Proceedings of COLING, pages
481?489.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of ACL, pages 247?256.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL Workshop on Unsupervised Lexi-
cal Acquisition, pages 9?16.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Proceedings of NAACL-HLT, pages 618?
626.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
COLING, pages 617?625.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceedings
of SIGIR, pages 175?182.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of ACL, pages 25?32.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management, 41(3):523?547.
Tara McIntosh and James R. Curran. 2009. Reducing se-
mantic drift with bagging and distributional similarity.
In Proceedings of ACL, pages 396?404.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of EMNLP,
pages 880?889.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of WWW, pages 1155?1156.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-occurrence
vectors from corpora vs. distance vectors from dictio-
naries. In Proceedings of COLING, pages 304?309.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD, pages 613?
619.
Yves Peirsman and Sebastian Pado?. 2010. Cross-
lingual induction of selectional preferences with bilin-
gual vector spaces. In Proceedings of NAACL-HLT,
pages 921?929.
Yves Peirsman and Sebastian Pado?. 2011. Semantic re-
lations in bilingual lexicons. ACM Transactions on
Speech and Language Processing, 8(2):article 3.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247?266.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of ACL, pages 1327?1335.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Ellen Riloff and Jessica Shepherd. 1999. A corpus-based
bootstrapping algorithm for semi-automated semantic
lexicon construction. Natural Language Engineering,
5(2):147?156.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual lex-
icon generation using non-aligned signatures. In Pro-
ceedings of ACL, pages 98?107.
1623
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors.
In Proceedings of EMNLP-CoNLL, pages 667?677.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013a. Token and type
constraints for cross-lingual part-of-speech tagging.
Transactions of ACL, 1:1?12.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.
2013b. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of NAACL-HLT,
pages 1061?1071.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of
EMNLP-CoNLL, pages 24?36.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of EMNLP,
pages 214?221.
Kewei Tu and Vasant Honavar. 2012. Unambiguity reg-
ularization for unsupervised learning of probabilistic
grammars. In Proceedings of EMNLP-CoNLL, pages
1324?1334.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
Journal of Artifical Intelligence Research, 37(1):141?
188.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In Proceedings of ACL-HLT, pages
299?304.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC, pages 77?
82.
Ivan Vulic? and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
NAACL-HLT, pages 106?116.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from comparable
corpora using latent topic models. In Proceedings of
ACL-HLT, pages 479?484.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL, pages 200?207.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceedings
of ACL, pages 1128?1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of ACL, pages 55?
63.
1624
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336?344,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Skip N-grams and Ranking Functions for Predicting Script Events
Bram Jans
KU Leuven
Leuven, Belgium
bram.jans@gmail.com
Steven Bethard
University of Colorado Boulder
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ivan Vulic?
KU Leuven
Leuven, Belgium
ivan.vulic@cs.kuleuven.be
Marie Francine Moens
KU Leuven
Leuven, Belgium
sien.moens@cs.kuleuven.be
Abstract
In this paper, we extend current state-of-the-
art research on unsupervised acquisition of
scripts, that is, stereotypical and frequently
observed sequences of events. We design,
evaluate and compare different methods for
constructing models for script event predic-
tion: given a partial chain of events in a
script, predict other events that are likely
to belong to the script. Our work aims
to answer key questions about how best
to (1) identify representative event chains
from a source text, (2) gather statistics from
the event chains, and (3) choose ranking
functions for predicting new script events.
We make several contributions, introducing
skip-grams for collecting event statistics, de-
signing improved methods for ranking event
predictions, defining a more reliable evalu-
ation metric for measuring predictiveness,
and providing a systematic analysis of the
various event prediction models.
1 Introduction
There has been recent interest in automatically ac-
quiring world knowledge in the form of scripts
(Schank and Abelson, 1977), that is, frequently
recurring situations that have a stereotypical se-
quence of events, such as a visit to a restaurant.
All of the techniques so far proposed for this task
share a common sub-task: given an event or partial
chain of events, predict other events that belong
to the same script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Chambers and Ju-
rafsky, 2011; Manshadi et al 2008; McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010; Regneri
et al 2010). Such a model can then serve as input
to a system that identifies the order of the events
within that script (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009) or that generates
a story using the selected events (McIntyre and
Lapata, 2009; McIntyre and Lapata, 2010).
In this article, we analyze and compare tech-
niques for constructing models that, given a partial
chain of events, predict other events that belong to
the script. In particular, we consider the following
questions:
? How should representative chains of events
be selected from the source text?
? Given an event chain, how should statistics
be gathered from it?
? Given event n-gram statistics, which ranking
function best predicts the events for a script?
In the process of answering these questions, this
article makes several contributions to the field of
script and narrative event chain understanding:
? We explore for the first time the use of skip-
grams for collecting narrative event statistics,
and show that this approach performs better
than classic n-gram statistics.
? We propose a new method for ranking events
given a partial script, and show that it per-
forms substantially better than ranking meth-
ods from prior work.
? We propose a new evaluation procedure (us-
ing Recall@N) for the cloze test, and advo-
cate its usage instead of average rank used
previously in the literature.
? We provide a systematic analysis of the in-
teractions between the choices made when
constructing an event prediction model.
336
Section 2 gives an overview of the prior work
related to this task. Section 3 lists and briefly de-
scribes different approaches that try to provide
answers to the three questions posed in this intro-
duction, while Section 4 presents the results of our
experiments and reports on our findings. Finally,
Section 5 provides a conclusive discussion along
with ideas for future work.
2 Prior Work
Our work is primarily inspired by the work of
Chambers and Jurafsky, which combined a depen-
dency parser with coreference resolution to col-
lect event script statistics and predict script events
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). For each document in their training
corpus, they used coreference resolution to iden-
tify all the entities, and a dependency parser to
identify all verbs that had an entity as either a sub-
ject or object. They defined an event as a verb plus
a dependency type (either subject or object), and
collected for each entity, the chain of events that
it participated in. They then calculated pointwise
mutual information (PMI) statistics over all the
pairs of events that occurred in the event chains in
their corpus. To predict a new script event given
a partial chain of events, they selected the event
with the highest sum of PMIs with all the events
in the partial chain.
The work of McIntyre and Lapata followed in
this same paradigm, (McIntyre and Lapata, 2009;
McIntyre and Lapata, 2010), collecting chains of
events by looking at entities and the sequence of
verbs for which they were a subject or object. They
also calculated statistics over the collected event
chains, though they considered both event bigram
and event trigram counts. Rather than predicting
an event for a script however, they used these sim-
ple counts to predict the next event that should be
generated for a children?s story.
Manshadi and colleagues were concerned about
the scalability of running parsers and coreference
over a large collection of story blogs, and so used
a simplified version of event chains ? just the main
verb of each sentence (Manshadi et al 2008).
Rather than rely on an ad-hoc summation of PMIs,
they apply language modeling techniques (specifi-
cally, a smoothed 5-gram model) over the sequence
of events in the collected chains. However, they
only tested these language models on sequencing
tasks (e.g. is the real sequence better than a ran-
dom sequence?) rather than on prediction tasks
(e.g. which event should follow these events?).
In the current article, we attempt to shed some
light on these previous works by comparing differ-
ent ways of collecting and using event chains.
3 Methods
Models that predict script events typically have
three stages. First, a large corpus is processed to
find event chains in each of the documents. Next,
statistics over these event chains are gathered and
stored. Finally, the gathered statistics are used to
create a model that takes as input a partial script
and produces as output a ranked list of events for
that script. The following sections give more de-
tails about each of these stages and identify the
decisions that must be made in each step, and an
overview of the whole process with an example
source text is displayed in Figure 1.
3.1 Identifying Event Chains
Event chains are typically defined as a sequence
of actions performed by some actor. Formally, an
event chain C for some actor a, is a partially or-
dered set of events (v, d) where each v is a verb
that has the actor a as its dependency d. Following
prior work (Chambers and Jurafsky, 2008; Cham-
bers and Jurafsky, 2009; McIntyre and Lapata,
2009; McIntyre and Lapata, 2010), these event
chains are identified by running a coreference sys-
tem and a dependency parser. Then for each en-
tity identified by the coreference system, all verbs
that have a mention of that entity as one of their
dependencies are collected1. The event chain is
then the sequence of (verb, dependency-type) tu-
ples. For example, given the sentence A Crow
was sitting on a branch of a tree when a Fox ob-
served her, the event chain for the Crow would be
(sitting, SUBJECT), (observed, OBJECT).
Once event chains have been identified, the most
appropriate event chains for training the model
must be selected. The goal of this process is to
select the subset of the event chains identified by
the coreference system and the dependency parser
that look to be the most reliable. Both the coref-
erence system and the dependency parser make
some errors, so not all event chains are necessarily
useful for training a model. The three strategies
we consider for this selection process are:
1Also following prior work, we consider only the depen-
dencies subject and object.
337
John woke up. He opened his eyes and yawned. Then he crossed the room and walked to the door.There he saw Mary. Mary smiled and kissed him. Then they both blushed.JOHN(woke, SUBJ)(opened, SUBJ)(yawned, SUBJ)(crossed, SUBJ)(walked, SUBJ)(saw, SUBJ)(kissed, OBJ)(blushed, SUBJ) MARY(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)(blushed, SUBJ)all chains, long chains,the longest chain all chains 1. Identifying event chains... [(saw, OBJ), (smiled, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(smiled, SUBJ), (blushed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(saw, OBJ), (blushed, SUBJ)]...[(kissed, SUBJ), (blushed, SUBJ)]regular bigrams 2-skip bigrams1-skip bigrams 2. Gathering event chain statistics(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)_________ (missing event)constructing a partial script (cloze test)1. (looked, OBJ)2. (gave, SUBJ)3. (saw, SUBJ)... 1. (kissed, OBJ)2. (looked, OBJ)3. (waited, SUBJ)... 1. (blushed, SUBJ)2. (kissed, OBJ)3. (smiled, SUBJ)C&J PMIOrdered PMIBigram prob. 3. Predicting script events
Figure 1: An overview of the whole linear work flow showing the three key steps ? identifying event chains,
collecting statistics out of the chains and predicting a missing event in a script. The figure also displays how a
partial script for evaluation (Section 4.3) is constructed. We show the whole process for Mary?s event chain only,
but the same steps are followed for John?s event chain.
? Select all event chains, that is, all sequences
of two or more events linked by common
actors. This strategy will produce the largest
number of event chains to train a model from,
but it may produce noisier training data as
the very short chains included by this strategy
may be less likely to represent real scripts.
? Select all long event chains consisting of 5
or more events. This strategy will produce a
smaller number of event chains, but as they
are longer, they may be more likely to repre-
sent scripts.
? Select only the longest event chain. This
strategy will produce the smallest number of
event chains from a corpus. However, they
may be of higher quality, since this strategy
looks for the key actor in each story, and only
uses the events that are tied together by that
key actor. Since this is the single actor that
played the largest role in the story, its actions
may be the most likely to represent a real
script.
3.2 Gathering Event Chain Statistics
Once event chains have been collected from the
corpus, the statistics necessary for constructing
the event prediction model must be gathered. Fol-
lowing prior work (Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009; Manshadi et al
2008; McIntyre and Lapata, 2009; McIntyre and
Lapata, 2010), we focus on gathering statistics
about the n-grams of events that occur in the
collected event chains. Specifically, we look at
strategies for collecting bigram statistics, the most
common type of statistics gathered in prior work.
We consider three strategies for collecting bigram
statistics:
? Regular bigrams. We find all pairs of
events that are adjacent in an event chain
and collect the number of times each event
pair was observed. For example, given the
chain of events (saw, SUBJ), (kissed, OBJ),
(blushed, SUBJ), we would extract the two
event bigrams: ((saw, SUBJ), (kissed, OBJ))
338
and ((kissed, OBJ), (blushed, SUBJ)). In addi-
tion to the event pair counts, we also collect
the number of times each event was observed
individually, to allow for various conditional
probability calculations. This strategy fol-
lows the classic approach for most language
models.
? 1-skip bigrams. We collect pairs of events
that occur with 0 or 1 events intervening be-
tween them. For example, given the chain
(saw, SUBJ), (kissed, OBJ), (blushed, SUBJ),
we would extract three bigrams: the two regu-
lar bigrams ((saw, SUBJ), (kissed, OBJ)) and
((kissed, OBJ), (blushed, SUBJ)), plus the 1-
skip-bigram, ((saw, SUBJ), (blushed, SUBJ)).
This approach to collecting n-gram statistics
is sometimes called skip-gram modeling, and
it can reduce data sparsity by extracting more
event pairs per chain (Guthrie et al 2006).
It has not previously been applied in the task
of predicting script events, but it may be
quite appropriate to this task because in most
scripts it is possible to skip some events in
the sequence.
? 2-skip bigrams. We collect pairs of events
that occur with 0, 1 or 2 intervening events,
similar to what was done in the 1-skip bi-
grams strategy. This will extract even more
pairs of events from each chain, but it is pos-
sible the statistics over these pairs of events
will be noisier.
3.3 Predicting Script Events
Once statistics over event chains have been col-
lected, it is possible to construct the model for
predicting script events. The input of this model
will be a partial script c of n events, where c =
c1c2 . . . cn = (v1, d1), (v2, d2), . . . , (vn, dn), and
the output of this model will be a ranked list of
events where the highest ranked events are the ones
most likely to belong to the event sequence in the
script. Thus, the key issue for this model is to de-
fine the function f for ranking events. We consider
three such ranking functions:
? Chambers & Jurafsky PMI. Chambers and
Jurafsky (2008) define their event ranking
function based on pointwise mutual infor-
mation. Given a partial script c as defined
above, they consider each event e = (v?, d?)
collected from their corpus, and score it as
the sum of the pointwise mutual informations
between the event e and each of the events in
the script:
f(e, c) =
n?
i
log
P (ci, e)
P (ci)P (e)
Chambers and Jurafsky?s description of this
score suggests that it is unordered, such that
P (a, b) = P (b, a). Thus the probabilities
must be defined as:
P (e1, e2) =
C(e1, e2) + C(e2, e1)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
where C(e1, e2) is the number of times that
the ordered event pair (e1, e2) was counted in
the training data, and C(e) is the number of
times that the event e was counted.
? Ordered PMI. A variation on the approach
of Chambers and Jurafsky is to have a score
that takes the order of the events in the chain
into account. In this scenario, we assume that
in addition to the partial script of events, we
are given an insertion point, m, where the
new event should be added. The score is then
defined as:
f(e, c) =
m?
k=1
log
P (ck, e)
P (ck)P (e)
+
n?
k=m+1
log
P (e, ck)
P (e)P (ck)
where the probabilities are defined as:
P (e1, e2) =
C(e1, e2)
?
ei
?
ej
C(ei, ej)
P (e) =
C(e)
?
e? C(e
?)
This approach uses pointwise mutual infor-
mation but also models the event chain in the
order it was observed.
? Bigram probabilities. Finally, a natural
ranking function, which has not been applied
to the script event prediction task (but has
339
been applied to related tasks (Manshadi et
al., 2008)) is to use the bigram probabilities
of language modeling rather than pointwise
mutual information scores. Again, given an
insertion point m for the event in the script,
we define the score as:
f(e, c) =
m?
k=1
logP (e|ck) +
n?
k=m+1
logP (ck|e)
where the conditional probability is defined
as2:
P (e1|e2) =
C(e1, e2)
C(e2)
This approach scores an event based on the
probability that it was observed following all
the events before it in the chain and preceding
all the events after it in the chain. This ap-
proach most directly models the event chain
in the order it was observed.
4 Experiments
Our experiments aimed to answer three questions:
Which event chains are worth keeping? How
should event bigram counts be collected? And
which ranking method is best for predicting script
events? To answer these questions we use two
corpora, the Reuters Corpus and the Andrew Lang
Fairy Tale Corpus, to evaluate our three differ-
ent chain selection methods, {all chains, long
chains, the longest chain}, our three different bi-
gram counting methods, {regular bigrams, 1-skip
bigrams, 2-skip bigrams}, and our three different
ranking methods, {Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}.
4.1 Corpora
We consider two corpora for evaluation:
? Reuters Corpus, Volume 1 3 (Lewis et
al., 2004) ? a large collection of 806, 791
news stories written in English concerning
a number of different topics such as politics,
2Note that predicted bigram probabilities are calculated
in this way for both classic language modeling and skip-gram
modeling. In skip-gram modeling, skips in the n-grams are
only used to increase the size of the training data; prediction
is performed exactly as in classic language modeling.
3http://trec.nist.gov/data/reuters/reuters.html
economics, sports, etc., strongly varying in
length, topics and narrative structure.
? Andrew Lang Fairy Tale Corpus 4 ? a
small collection of 437 children stories with
an average length of 125 sentences, and used
previously for story generation by McIntyre
and Lapata (2009).
In general, the Reuters Corpus is much larger and
allows us to see how well script events can be
predicted when a lot of data is available, while the
Andrew Lang Fairy Tale Corpus is much smaller,
but has a more straightforward narrative structure
that may make identifying scripts simpler.
4.2 Corpus Processing
Constructing a model for predicting script events
requires a corpus that has been parsed with a de-
pendency parser, and whose entities have been
identified via a coreference system. We there-
fore processed our corpora by (1) filtering out
non-narrative articles, (2) applying a dependency
parser, (3) applying a coreference resolution sys-
tem and (4) identifying event chains via entities
and dependencies.
First, articles that had no narrative content were
removed from the corpora. In the Reuters Corpus,
we removed all files solely listing stock exchange
values, interest rates, etc., as well as all articles
that were simply summaries of headlines from dif-
ferent countries or cities. After removing these
files, the Reuters corpus was reduced to 788, 245
files. Removing files from the Fairy Tale corpus
was not necessary ? all 437 stories were retained.
We then applied the Stanford Parser (Klein and
Manning, 2003) to identify the dependency struc-
ture of each sentence in each article in the corpus.
This parser produces a constitutent-based syntactic
parse tree for each sentence, and then converts this
tree to a collapsed dependency structure via a set
of tree patterns.
Next we applied the OpenNLP coreference en-
gine5 to identify the entities in each article, and the
noun phrases that were mentions of each entity.
Finally, to identify the event chains, we took
each of the entities proposed by the coreference
system, walked through each of the noun phrases
associated with that entity, retrieved any subject
4http://www.mythfolklore.net/andrewlang/
5http://incubator.apache.org/opennlp/
340
or object dependencies that linked a verb to that
noun phrase, and created an event chain from the
sequence of (verb, dependency-type) tuples in the
order that they appeared in the text.
4.3 Evaluation Metrics
We follow the approach of Chambers and Jurafsky
(2008), evaluating our models for predicting script
events in a narrative cloze task. The narrative
cloze task is inspired by the classic psychological
cloze task in which subjects are given a sentence
with a word missing and asked to fill in the blank
(Taylor, 1953). Similarly, in the narrative cloze
task, the system is given a sequence of events from
a script where one event is missing, and asked
to predict the missing event. The difficulty of a
cloze task depends a lot on the context around
the missing item ? in some cases it may be quite
predictable, but in many cases there is no single
correct answer, though some answers are more
probable than others. Thus, performing well on a
cloze task is more about ranking the missing event
highly, and not about proposing a single ?correct?
event.
In this way, narrative cloze is like perplexity
in a language model. However, where perplexity
measures how good the model is at predicting a
script event given the previous events in the script,
narrative cloze measures how good the model is
at predicting what is missing between events in
the script. Thus narrative cloze is somewhat more
appropriate to our task, and at the same time sim-
plifies comparisons to prior work.
Rather than manually constructing a set of
scripts on which to run the cloze test, we follow
Chambers and Jurafsky in reserving a section of
our parsed corpora for testing, and then using the
event chains from that section as the scripts for
which the system must predict events. Given an
event chain of length n, we run n cloze tests, with
a different one of the n events removed each time
to create a partial script from the remaining n? 1
events (see Figure 1). Given a partial script as
input, an accurate event prediction model should
rank the missing event highly in the guess list that
it generates as output.
We consider two approaches to evaluating the
guess lists produced in response to narrative cloze
tests. Both are defined in terms of a test collection
C, consisting of |C| partial scripts, where for each
partial script c with missing event e, ranksys(c) is
the rank of e in the system?s guess list for c.
? Average rank. The average rank of the miss-
ing event across all of the partial scripts:
1
|C|
?
c?C
ranksys(c)
This is the evaluation metric used by Cham-
bers and Jurafsky (2008).
? Recall@N. The fraction of partial scripts
where the missing event is ranked N or less6
in the guess list.
1
|C|
|{c : c ? C ? ranksys(c) ? N}|
In our experiments we use N = 50, but re-
sults are roughly similar for lower and higher
values of N .
Recall@N has not been used before for evaluat-
ing models that predict script events, however we
suggest that it is a more reliable metric than Av-
erage rank. When calculating the average rank,
the length of the guess lists will have a significant
influence on results. For instance, if a small model
is trained with only a small vocabulary of events,
its guess lists will usually be shorter than a larger
model, but if both models predict the missing event
at the bottom of the list, the larger model will get
penalized more. Recall@N does not have this is-
sue ? it is not influenced by length of the guess
lists.
An alternative evaluation metric would have
been mean average precision (MAP), a metric
commonly used to evaluate information retrieval.
Mean average precision reduces to mean recipro-
cal rank (MRR) when there?s only a single answer
as in the case of narrative cloze, and would have
scored the ranked lists as:
1
|C|
?
c?C
1
ranksys(c)
Note that mean reciprocal rank has the same issues
with guess list length that average rank does. Thus,
since it does not aid us in comparing to prior work,
and it has the same deficiencies as average rank,
we do not report MRR in this article.
6Rank 1 is the event that the system predicts is most prob-
able, so we want the missing event to have the smallest rank
possible.
341
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 502 0.5179
long chains 549 0.4951
the longest chain 546 0.4984
Table 1: Chain selection methods for the Reuters corpus
- comparison of average ranks and Recall@50.
2-skip + bigram prob.
Chain selection Av. rank Recall@50
all chains 1650 0.3376
long chains 452 0.3461
the longest chain 1534 0.3376
Table 2: Chain selection methods for the Fairy Tale
corpus - comparison of average ranks and Recall@50.
4.4 Results
We considered all 27 combinations of our chain
selection methods, bigram counting methods, and
ranking methods: {all chains, long chains, the
longest chain}x{regular bigrams, 1-skip bigrams,
2-skip bigrams}x{Chambers & Jurafsky PMI, or-
dered PMI, bigram probabilities}. The best among
these 27 combinations for the Reuters corpus was
{all chains}x{2-skip bigrams}x{bigram probabil-
ities} achieving an average rank of 502 and a Re-
call@50 of 0.5179.
Since viewing all the combinations at once
would be confusing, instead the following sec-
tions investigate each decision (selection, counting,
ranking) one at a time. While one decision is var-
ied across its three choices, the other decisions are
held to their values in the best model above.
4.4.1 Identifying Event Chains
We first try to answer the question: How should
representative chains of events be selected from
the source text? Tables 1 and 2 show perfor-
mance when we vary the strategy for selecting
event chains, while fixing the counting method to
2-skip bigrams, and fixing the ranking method to
bigram probabilities.
For the Reuters collection, we see that using all
chains gives a lower average rank and a higher
Recall@50 than either of the strategies that select
a subset of the event chains. The explanation is
probably simple: using all chains produces more
than 700,000 bigrams from the Reuters corpus,
while using only the long chains produces only
around 300,000. So more data is better data for
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 789 0.4886
1-skip bigrams 630 0.4951
2-skip bigrams 502 0.5179
Table 3: Event bigram selection methods for the
Reuters corpus - comparison of average ranks and Re-
call@50.
all chains + bigram prob.
Bigram selection Av. rank Recall@50
regular bigrams 2363 0.3227
1-skip bigrams 1690 0.3418
2-skip bigrams 1650 0.3376
Table 4: Event bigram selection methods for the Fairy
Tales corpus - comparison of average ranks and Re-
call@50.
predicting script events.
For the Fairy Tale collection, long chains gives
the lowest average rank and highest Recall@50. In
this collection, there is apparently some benefit to
filtering the shorter event chains, probably because
the collection is small enough that the noise in-
troduced from dependency and coreference errors
plays a larger role.
4.4.2 Gathering Event Chain Statistics
We next try to answer the question: Given an
event chain, how should statistics be gathered from
it? Tables 3 and 4 show performance when we vary
the strategy for counting event pairs, while fixing
the selecting method to all chains, and fixing the
ranking method to bigram probabilities.
For the Reuters corpus, 2-skip bigrams achieves
the lowest average rank and the highest Recall@50.
For the Fairy Tale corpus, 1-skip bigrams and 2-
skip bigrams perform similarly, and both have
lower average rank and higher Recall@50 than
regular bigrams.
Skip-grams probably outperform regular n-
grams on both of these corpora because the skip-
grams provide many more event pairs over which
to calculate statistics: in the Reuters corpus, regu-
lar bigrams extracts 737,103 bigrams, while 2-skip
bigrams extracts 1,201,185 bigrams. Though skip-
grams have not been applied to predicting script
events before, it seems that they are a good fit,
and better capture statistics about narrative event
chains than regular n-grams do.
342
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 2052 0.1954
ordered PMI 3584 0.1694
bigram prob. 502 0.5179
Table 5: Ranking methods for the Reuters corpus -
comparison of average ranks and Recall@50.
all bigrams + 2-skip
Ranking method Av. rank Recall@50
C&J PMI 1455 0.1975
ordered PMI 2460 0.0467
bigram prob. 1650 0.3376
Table 6: Ranking methods for the Fairy Tale corpus -
comparison of average ranks and Recall@50.
4.4.3 Predicting Script Events
Finally, we try to answer the question: Given
event n-gram statistics, which ranking function
best predicts the events for a script? Tables 5 and
6 show performance when we vary the strategy for
ranking event predictions, while fixing the selec-
tion method to all chains, and fixing the counting
method to 2-skip bigrams.
For both Reuters and the Fairy Tale corpus, Re-
call@50 identifies bigram probabilities as the best
ranking function by far. On the Reuters corpus
the Chambers & Jurafsky PMI ranking method
achieves Recall@50 of only 0.1954, while bigram
probabilities ranking method achieves 0.5179. The
gap is also quite large on the Fairy Tales corpus:
0.1975 vs. 0.3376.
On the Reuters corpus, average rank also identi-
fies bigram probabilities as the best ranking func-
tion, yet for the Fairy Tales corpus, Chambers &
Jurafsky PMI and bigram probabilities have simi-
lar average ranks. This inconsistency is probably
due to the flaws in the average rank evaluation
measure that were discussed in Section 4.3 ? the
measure is overly sensitive to the length of the
guess list, particularly when the missing event is
ranked lower, as it is likely to be when training on
a smaller corpus like the Fairy Tales corpus.
5 Discussion
Our experiments have led us to several important
conclusions. First, we have introduced skip-grams
and proved their utility for acquiring script knowl-
edge ? our models that employ skip bigrams score
consistently higher on event prediction. By follow-
ing the intuition that events do not have to appear
strictly one after another to be closely semantically
related, skip-grams decrease data sparsity and in-
crease the size of the training data.
Second, our novel bigram probabilities ranking
function outperforms the other ranking methods.
In particular, it outperforms the state-of-the-art
pointwise mutual information method introduced
by Chambers and Jurafsky (2008), and it does so
by a large margin, more than doubling the Re-
call@50 on the Reuters corpus. The key insight
here is that, when modeling events in a script, a
language-model-like approach better fits the task
than a mutual information approach.
Third, we have discussed why Recall@N is a
better and more consistent evaluation metric than
Average rank. However, both evaluation metrics
suffer from the strictness of the narrative cloze test,
which accepts only one event being the correct
event, while it is sometimes very difficult, even
for humans, to predict the missing events, and
sometimes more solutions are possible and equally
correct. In future research, our goal is to design
a better evaluation framework which is more suit-
able for this task, where credit can be given for
proposed script events that are appropriate but not
identical to the ones observed in a text.
Fourth, we have observed some differences in
results between the Reuters and the Fairy Tale
corpora. The results for Reuters are consistently
better (higher Recall@50, lower average rank), al-
though fairy tales contain a plainer narrative struc-
ture, which should be more appropriate to our task.
This again leads us to the conclusion that more
data (even with more noise as in Reuters) leads to
a greater coverage of events, better overall models
and, consequently, to more accurate predictions.
Still, the Reuters corpus seems to be far from a
perfect corpus for research in the automatic acqui-
sition of scripts, since only a small portion of the
corpus contains true narratives. Future work must
therefore gather a large corpus of true narratives,
like fairy tales and children?s stories, whose sim-
ple plot structures should provide better learning
material, both for models predicting script events,
and for related tasks like automatic storytelling
(McIntyre and Lapata, 2009).
One of the limitations of the work presented
here is that it takes a fairly linear, n-gram-based ap-
proach to characterizing story structure. We think
such an approach is useful because it forms a natu-
343
ral baseline for the task (as it does in many other
tasks such as named entity tagging and language
modeling). However, story structure is seldom
strictly linear, and future work should consider
models based on grammatical or discourse links
that can capture the more complex nature of script
events and story structure.
Acknowledgments
We would like to thank the anonymous reviewers
for their constructive comments. This research
was carried out as a master thesis in the frame-
work of the TERENCE European project (EU FP7-
257410).
References
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 602?610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 976?986.
David Guthrie, Ben Allison, W. Liu, Louise Guthrie,
and Yorick Wilks. 2006. A closer look at skip-gram
modelling. In Proceedings of the Fifth international
Conference on Language Resources and Evaluation
(LREC), pages 1222?1225.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for
text categorization research. Journal of Machine
Learning Research, 5:361?397.
Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proceed-
ings of the Twenty-First International Florida Artifi-
cial Intelligence Research Society Conference.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 217?225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1562?1572.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979?988.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: an inquiry into
human knowledge structures. Lawrence Erlbaum
Associates.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readibility. Journalism Quarterly,
30:415?433.
344
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449?459,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Detecting Highly Confident Word Translations from Comparable
Corpora without Any Prior Knowledge
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
In this paper, we extend the work on using
latent cross-language topic models for iden-
tifying word translations across compara-
ble corpora. We present a novel precision-
oriented algorithm that relies on per-topic
word distributions obtained by the bilin-
gual LDA (BiLDA) latent topic model.
The algorithm aims at harvesting only the
most probable word translations across lan-
guages in a greedy fashion, without any
prior knowledge about the language pair,
relying on a symmetrization process and
the one-to-one constraint. We report our re-
sults for Italian-English and Dutch-English
language pairs that outperform the current
state-of-the-art results by a significant mar-
gin. In addition, we show how to use the al-
gorithm for the construction of high-quality
initial seed lexicons of translations.
1 Introduction
Bilingual lexicons serve as an invaluable resource
of knowledge in various natural language pro-
cessing tasks, such as dictionary-based cross-
language information retrieval (Carbonell et al
1997; Levow et al 2005) and statistical machine
translation (SMT) (Och and Ney, 2003). In or-
der to construct high quality bilingual lexicons for
different domains, one usually needs to possess
parallel corpora or build such lexicons by hand.
Compiling such lexicons manually is often an ex-
pensive and time-consuming task, whereas the
methods for mining the lexicons from parallel cor-
pora are not applicable for language pairs and do-
mains where such corpora is unavailable or miss-
ing. Therefore the focus of researchers turned to
comparable corpora, which consist of documents
with partially overlapping content, usually avail-
able in abundance. Thus, it is much easier to build
a high-volume comparable corpus. A representa-
tive example of such a comparable text collection
is Wikipedia, where one may observe articles dis-
cussing the similar topic, but strongly varying in
style, length and vocabulary, while still sharing a
certain amount of main concepts (or topics).
Over the years, several approaches for min-
ing translations from non-parallel corpora have
emerged (Rapp, 1995; Fung and Yee, 1998; Rapp,
1999; Diab and Finch, 2000; De?jean et al 2002;
Chiao and Zweigenbaum, 2002; Gaussier et al
2004; Fung and Cheung, 2004; Morin et al 2007;
Haghighi et al 2008; Shezaf and Rappoport,
2010; Laroche and Langlais, 2010), all sharing
the same Firthian assumption, often called the
distributionial hypothesis (Harris, 1954), which
states that words with a similar meaning are likely
to appear in similar contexts across languages.
All these methods have examined different rep-
resentations of word contexts and different meth-
ods for matching words across languages, but they
all have in common a need for a seed lexicon of
translations to efficiently bridge the gap between
languages. That seed lexicon is usually crawled
from the Web or obtained from parallel corpora.
Recently, Li et al(2011) have proposed an ap-
proach that improves precision of the existing
methods for bilingual lexicon extraction, based
on improving the comparability of the corpus un-
der consideration, prior to extracting actual bilin-
gual lexicons. Other methods such as (Koehn and
Knight, 2002) try to design a bootstrapping algo-
rithm based on an initial seed lexicon of transla-
tions and various lexical evidences. However, the
quality of their initial seed lexicon is disputable,
449
since the construction of their lexicon is language-
pair biased and cannot be completely employed
on distant languages. It solely relies on unsatis-
factory language-pair independent cross-language
clues such as words shared across languages.
Recent work from Vulic? et al2011) utilized
the distributional hypothesis in a different direc-
tion. It attempts to abrogate the need of a seed lex-
icon as a prerequisite for bilingual lexicon extrac-
tion. They train a cross-language topic model on
document-aligned comparable corpora and intro-
duce different methods for identifying word trans-
lations across languages, underpinned by per-
topic word distributions from the trained topic
model. Due to the fact that they deal with compa-
rable Wikipedia data, their translation model con-
tains a lot of noise, and some words are poorly
translated simply because there are not enough
occurrences in the corpus. The goal of this work is
to design an algorithm which will learn to harvest
only the most probable translations from the per-
word topic distributions. The translations learned
by the algorithm then might serve as a highly ac-
curate, precision-based initial seed lexicon, which
can then be used as a tool for translating source
word vectors into the target language. The key ad-
vantage of such a lexicon lies in the fact that there
is no language-pair dependent prior knowledge
involved in its construction (e.g., orthographic
features). Hence, it is completely applicable to
any language pair for which there exist sufficient
comparable data for training of the topic model.
Since comparable corpora often construct a
very noisy environment, it is of the utmost impor-
tance for a precision-oriented algorithm to learn
when to stop the process of matching words, and
which candidate pairs are surely not translations
of each other. The method described in this paper
follows this intuition: while extracting a bilingual
lexicon, we try to rematch words, keeping only
the most confident candidate pairs and disregard-
ing all the others. After that step, the most con-
fident candidate pairs might be used with some
of the existing context-based techniques to find
translations for the words discarded in the pre-
vious step. The algorithm is based on: (1) the
assumption of symmetry, and (2) the one-to-one
constraint. The idea of symmetrization has been
borrowed from the symmetrization heuristics in-
troduced for word alignments in SMT (Och and
Ney, 2003), where the intersection heuristics is
employed for a precision-oriented algorithm. In
our setting, it basically means that we keep a
translation pair (wSi , wTj ) if and only if, after the
symmetrization process, the top translation candi-
date for the source word wSi is the target word wTi
and vice versa. The one-to-one constraint aims
at matching the most confident candidates during
the early stages of the algorithm, and then exclud-
ing them from further search. The utility of the
constraint for parallel corpora has already been
evaluated by Melamed (2000).
The remainder of the paper is structured as
follows. Section 2 gives a brief overview of
the methods, relying on per-topic word distribu-
tions, which serve as the tool for computing cross-
language similarity between words. In Section
3, we motivate the main assumptions of the al-
gorithm and describe the full algorithm. Sec-
tion 4 justifies the underlying assumptions of
the algorithm by providing comparisons with a
current-state-of-the-art system for Italian-English
and Dutch-English language pairs. It also con-
tains another set of experiments which inves-
tigates the potential of the algorithm in build-
ing a language-pair unbiased seed lexicon, and
compares the lexicon with other seed lexicons.
Finally, Section 5 lists conclusion and possible
paths of future work.
2 Calculating Initial Cross-Language
Word Similarity
This section gives a quick overview of the Cue
method, the TI method, and their combination,
described by Vulic? et al2011), which proved to
be the most efficient and accurate for identify-
ing potential word translations once the cross-
language BiLDA topic model is trained and the
associated per-topic distributions are obtained for
both source and target corpora. The BiLDA
model we use is a natural extension of the stan-
dard LDA model and, along with the definition of
per-topic word distributions, has been presented
in (Ni et al 2009; De Smet and Moens, 2009;
Mimno et al 2009). BiLDA takes advantage of
the document alignment by using a single variable
that contains the topic distribution ?. This vari-
able is language-independent, because it is shared
by each of the paired bilingual comparable doc-
uments. Topics for each document are sampled
from ?, from which the words are then sampled
in conjugation with the vocabulary distribution ?
450
zSji wSji
? ?
zTji wTji
?
?
?
MS
MT
D
Figure 1: Plate model for bilingual Latent Dirichlet Allocation
1
Figure 1: The bilingual LDA (BiLDA) model
(for language S) and ? (for language T).
2.1 Cue Method
A straightforward approach to express similarity
between words tries to emphasize the associative
relation in a natural way - modeling the proba-
bility P (wT2 |wS1 ), i.e. the probability that a tar-
get word wT2 will be generated as a response to a
cue source word wS1 , where the link between the
words is established via the shared topic space:
P (wT2 |wS1 ) =
?K
k=1 P (wT2 |zk)P (zk|wS1 ), where
K denotes the number of cross-language topics.
2.2 TI Method
This approach constructs word vectors over a
shared space of cross-language topics, where val-
ues within vectors are the TF-ITF scores (term
frequency - inverse topic frequency), computed
in a completely analogical manner as the TF-
IDF scores for the original word-document space
(Manning and Schu?tze, 1999). Term frequency,
given a source word wSi and a topic zk, measures
the importance of the word wSi within the particu-
lar topic zk, while inverse topical frequency (ITF)
of the word wSi measures the general importance
of the source word wSi across all topics. The fi-
nal TF-ITF score for the source word wSi and the
topic zk is given by TF ?ITFi,k = TFi,k ?ITFi.
The TF-ITF scores for target words associated
with target topics are calculated in an analogical
manner and the standard cosine similarity is then
used to find the most similar target word vectors
for a given source word vector.
2.3 Combining the Methods
Topic models have the ability to build clusters of
words which might not always co-occur together
in the same textual units and therefore add ex-
tra information of potential relatedness. These
two methods for automatic bilingual lexicon ex-
traction interpret and exploit underlying per-topic
word distributions in different ways, so combin-
ing the two should lead to even better results. The
two methods are linearly combined, with the over-
all score given by:
SimTI+Cue(wS1 , wT2 ) = ?SimTI(wS1 , wT2 )
+ (1? ?)SimCue(wS1 , wT2 ) (1)
Both methods posses several desirable proper-
ties. According to Griffiths et al(2007), the con-
ditioning for the Cue method automatically com-
promises between word frequency and semantic
relatedness since higher frequency words tend to
have higher probability across all topics, but the
distribution over topics P (zk|wS1 ) ensures that se-
mantically related topics dominate the sum. The
similar phenomenon is captured by the TI method
by the usage of TF, which rewards high frequency
words, and ITF, which assigns a higher impor-
tance for words semantically more related to a
specific topic. These properties are incorporated
in the combination of the methods. As the final
result, the combined method provides, for each
source word, a ranked list of target words with as-
sociated scores that measure the strength of cross-
language similarity. The higher the score, the
more confident a translation pair is. We will use
this observation in the next section during the al-
gorithm construction.
The lexicon constructed by solely applying the
combination of these methods without any addi-
tional assumptions will serve as a baseline in the
results section.
3 Constructing the Algorithm
This section explains the underlying assumptions
of the algorithm: the assumption of symmetry
and the one-to-one assumption. Finally, it pro-
vides the complete outline of the algorithm.
3.1 Assumption of Symmetry
First, we start with the intuition that the assump-
tion of symmetry strengthens the confidence of a
translation pair. In other words, if the most prob-
able translation candidate for a source word wS1 is
a target word wT2 and, vice versa, the most prob-
able translation candidate of the target word wT2
451
is the source word wS1 , and their TI+Cue scores
are above a certain threshold, we can claim that
the words wS1 and wT2 are a translation pair. The
definition of the symmetric relation can also be
relaxed. Instead of observing only one top can-
didate from the lists, we can observe top N can-
didates from both sides and include them in the
search space, and then re-rank the potential candi-
dates taking into account their associated TI+Cue
scores and their respective positions in the list.
We will call N the search space depth. Here is
the outline of the re-ranking method if the search
space consists of the top N candidates on both
sides:
1. Given is a source word wSs , for which we ac-
tually want to find the most probable trans-
lation candidate. Initialize an empty list
Finals = {} in which target language
candidates with their recalculated associated
scores will be stored.
2. Obtain TI+Cue scores for all target words.
Keep only N best scoring target candidates:
{wTs,1, . . . , wTs,N} along with their respective
scores.
3. For each target candidate from
{wTs,1, . . . , wTs,N} acquire TI+Cue scores
over the entire source vocabulary. Keep only
N best scoring source language candidates.
Each word wTs,i ? {wTs,1, . . . , wTs,N} now
has a list of N source language candidates
associated with it: {wSi,1, wSi,2 . . . , wSi,N}.
4. For each target candidate word wTs,i ?
{wTs,1, . . . , wTs,N}, do as follows:
(a) If one of the words from the associated
list is the given source word wSs , re-
member: (1) the position m, denoting
how high in the list the word wSs was
found, and (2) the associated TI+Cue
score SimTI+Cue(wTs,i, wSi,m = wSs ).
Calculate:
(i) G1,i = SimTI+Cue(wSs , wTs,i)/i
(ii) G2,i = SimTI+Cue(wTs,i, wSi,m)/m
Following that, calculate GMi, the ge-
ometric mean of the values G1,i and
G2,i1: GMi =
?
G1,i ?G2,i. Add a tu-
1Scores G1,i and G2,i are structured in such a way to
balance between positions in the ranked lists and the TI+Cue
scores, since they reward candidate words which have high
TI+Cue scores associated with them, and penalize words if
they are found lower in the list of potential candidates.
ple (wTs,i, GMi) to the list Finals.
(b) If we have reached the end of the list
for the target candidate word wTs,i with-
out finding the given source word wSs ,
and i < N , continue with the next word
wTs,i+1. Do not add any tuple to Finals
in this step.
5. If the list Finals is not empty, sort the tuples
in the list in descending order according to
their GMi scores. The first element of the
sorted list contains a word wTs,high, the final
translation candidate of the source word wSs .
If the list Finals is not empty, the final re-
sult of this process will be the cross-language
word translation pair (wSs , wTs,high).
We will call this symmetrization process the
symmetrizing re-ranking. It attempts at push-
ing the correct cross-language synonym to the top
of the candidates list, taking into account both
the strength of similarities defined through the
TI+Cue scores in both directions, and positions
in ranked lists. A blatant example depicting how
this process helps boost precision is presented in
Figure 2. We can also design a thresholded variant
of this procedure by imposing an extra constraint.
When calculating target language candidates for
the source word wSs in Step 2, we proceed fur-
ther only if the first target candidate scores above
a certain threshold P and, additionally, in Step 3,
we keep lists of N source language candidates
for only those target words for which the first
source language candidate in their respective list
scored above the same threshold P . We will call
this procedure the thresholded symmetrizing re-
ranking, and this version will be employed in the
final algorithm.
3.2 One-to-one Assumption
Melamed (2000) has already established that most
source words in parallel corpora tend to translate
to only one target word. That tendency is modeled
by the one-to-one assumption, which constrains
each source word to have at most one translation
on the target side. Melamed?s paper reports that
this bias leads to a significant positive impact on
precision and recall of bilingual lexicon extraction
from parallel corpora. This assumption should
also be reasonable for many types of comparable
corpora such as Wikipedia or news corpora, which
are topically aligned or cover similar themes. We
452
abdij monasterymonkabbey kloostermonnikbenedictijnkloostermonnikabdijabdijmonnikklooster0.22370.15860.1155 0.30490.17400.13380.22660.14940.11310.25490.14960.1288
Figure 2: An example where the assumption of symmetry and the one-to-one assumption clearly help boost
precision. If we keep top Nc = 3 candidates from both sides, the algorithm is able to detect that the correct
Dutch-English translation pair is (abdij, abbey). The TI+Cue method without any assumptions would result with
an indirect association (abdij, monastery). If only the one-to-one assumption was present, the algorithm would
greedily learn the correct direct association (monastery, klooster), remove those words from their respective
vocabularies and then again result with another indirect association (abdij, monk). By additionally employing
the assumption of symmetry with the re-ranking method from Subsection 3.1, the algorithm correctly learns
the translation pair (abdij, abbey). Correct translation pairs (klooster, monastery) and (monnik, monk) are also
obtained. Again here, the pair (monnik, monk) would not be obtained without the one-to-one assumption.
will prove that the assumption leads to better pre-
cision scores even for bilingual lexicon extraction
from such comparable data. The intuition be-
hind introducing this constraint is fairly simple.
Without the assumption, the similarity scores be-
tween source and target words are calculated in-
dependently of each other. We will illustrate the
problem arising from the independence assump-
tion with an example.
Suppose we have an Italian word arcipelago,
and we would like to detect its correct English
translation (archipelago). However, after the
TI+Cue method is employed, and even after the
symmetrizing re-ranking process from the previ-
ous step is used, we still acquire a wrong transla-
tion candidate pair (arcipelago, island). Why is
that so? The word (arcipelago) (or its translation)
and the acquired translation (island) are semanti-
cally very close, and therefore have similar distri-
butions over cross-language topics, but island is a
much more frequent term. The TI+Cue method
concludes that two words are potential trans-
lations whenever their distributions over cross-
language topics are much more similar than ex-
pected by chance. Moreover, it gives a preference
to more frequent candidates, so it will eventually
end up learning an indirect association2 between
words arcipelago and island. The one-to-one as-
sumption should mitigate the problem of such in-
direct associations if we design our algorithm in
such a way that it learns the most confident direct
associations2 first:
2A direct association, as defined in (Melamed, 2000), is
an association between two words (in this setting found by
the TI+Cue method) where the two words are indeed mutual
translations. Otherwise, it is an indirect association.
453
1. Learn the correct direct association pair
(isola, island).
2. Remove the words isola and island from
their respective vocabularies.
3. Since island is not in the vocabulary, the
indirect association between arcipelago and
island is not present any more. The algo-
rithm learns the correct direct association
(arcipelago, archipelago).
3.3 The Algorithm
3.3.1 One-Vocabulary-Pass
First, we will provide a version of the algorithm
with a fixed threshold P which completes only
one pass through the source vocabulary. Let V S
denote a given source vocabulary, and let V T de-
note a given target vocabulary. We need to define
several parameters of the algorithm. Let N0 be
the initial maximum search space depth for the
thresholded symmetrizing re-ranking procedure.
In Figure 2, the current depth Nc is 3, while the
maximum depth might be set to a value higher
than 3. The algorithm with the fixed threshold P
proceeds as follows:
1. Initialize the maximum search space depth
NM = N0. Initialize an empty lexicon L.
2. For each source word wSs ? V S do:
(a) Set the current search space depthNc =
1.3
(b) Perform the thresholded symmetrizing
re-ranking procedure with the current
search space set toNc and the threshold
P . If a translation pair (wSs , wTs,high) is
found, go to the Sub-step 2(d).
(c) If a translation pair is not found, and
Nc < NM , increment the current
search space Nc = Nc+1 and return to
the previous Sub-step 2(b). If a trans-
lation pair is not found and Nc = NM ,
return to Step 2 and proceed with the
next word.
(d) For the found translation pair
(wSs , wTs,high), remove words wSs
and wTs,high from their respective
3The intuition here is simple ? we are trying to detect
a direct association as high as possible in the list. In other
words, if the first translation candidate for the source word
isola is the target word island, and, vice versa, the first
translation candidate for the target word island is isola, we
do not need to expand our search depth, because these two
words are the most likely translations.
vocabularies: V S = V S ? {wSs } and
V T = V T ? {wTs,high} to satisfy the
one-to-one constraint. Add the pair
(wSs , wTs,high) to the lexicon L.
We will name this procedure the one-
vocabulary-pass and employ it later in an iter-
ative algorithm with a varying threshold and a
varying maximum search space depth.
3.3.2 The Final Algorithm
Let us now define P0 as the initial threshold, let
Pf be the threshold at which we stop decreas-
ing the value for threshold and start expanding
our maximum search space depth for the thresh-
olded symmetrizing re-ranking, and let decp be a
value for which we decrease the current threshold
in each step. Finally, let Nf be the limit for the
maximum search space depth, andNM denote the
current maximum search space depth. The final
algorithm is given by:
1. Initialize the maximum search space depth
NM = N0 and the starting threshold P =
P0. Initialize an empty lexicon Lfinal.
2. Check the stopping criterion: If NM > Nf ,
go to Step 5, otherwise continue with Step 3.
3. Perform the one-vocabulary-pass with the
current values of P and NM . Whenever a
translation pair is found, it is added to the
lexicon Lfinal. Additionally, we can also
save the threshold and the depth at which that
pair was found.
4. Decrease P : P = P ? decp, and check
if P < Pf . If still not P < Pf , go to
Step 3 and perform the one-vocabulary-pass
again. Otherwise, if P < Pf and there are
still unmatched words in the source vocab-
ulary, reset P : P = P0, increment NM :
NM = NM + 1 and go to Step 2.
5. Return Lfinal as the final output of the algo-
rithm.
The parameters of the algorithm model its be-
havior. Typically, we would like to setP0 to a high
value, and N0 to a low value, which makes our
constraints strict and narrows our search space,
and consequently, extracts less translation pairs
in the first steps of the algorithm, but the set
of those translation pairs should be highly accu-
rate. Once it is not possible to extract any more
pairs with such strict constraints, the algorithm re-
454
laxes them by lowering the threshold and expand-
ing the search space by incrementing the max-
imum search space depth. The algorithm may
leave some of the source words unmatched, which
is also dependent on the parameters of the algo-
rithm, but, due to the one-to-one assumption, that
scenario also occurs whenever a target vocabulary
contains more words than a source vocabulary.
The number of operations of the algorithm also
depends on the parameters, but it mostly depends
on the sizes of the given vocabularies. The com-
plexity isO(|V S ||V T |), but the algorithm is com-
putationally feasible even for large vocabularies.
4 Results and Discussion
4.1 Training Collections
The data used for training of the models is col-
lected from various sources and varies strongly in
theme, style, length and its comparableness. In
order to reduce data sparsity, we keep only lem-
matized non-proper noun forms.
For Italian-English language pair, we use
18, 898 Wikipedia article pairs to train BiLDA,
covering different themes with different scopes
and subtopics being addressed. Document align-
ment is established via interlingual links from the
Wikipedia metadata. Our vocabularies consist of
7, 160 Italian nouns and 9, 116 English nouns.
For Dutch-English language pair, we use 7, 602
Wikipedia article pairs, and 6, 206 Europarl doc-
ument pairs, and combine them for training.4 Our
final vocabularies consist of 15, 284 Dutch nouns
and 12, 715 English nouns.
Unlike, for instance, Wikipedia articles, where
document alignment is established via interlin-
gual links, in some cases it is necessary to perform
document alignment as the initial step. Since our
work focuses on Wikipedia data, we will not get
into detail with algorithms for document align-
ment. An IR-based method for document align-
ment is given in (Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005), and a feature-based
method can be found in (Vu et al 2009).
4.2 Experimental Setup
All our experiments rely on BiLDA training
with comparable data. Corpora and software for
4In case of Europarl, we use only the evidence of docu-
ment alignment during the training and do not benefit from
the parallelness of the sentences in the corpus.
BiLDA training are obtained from Vulic? et al
(2011). We train the BiLDA model with 2000
topics using Gibbs sampling, since that number
of topics displays the best performance in their
paper. The linear interpolation parameter for the
combined TI+Cue method is set to ? = 0.1.
The parameters of the algorithm, adjusted on a
set of 500 randomly sampled Italian words, are set
to the following values in all experiments, except
where noted different: P0 = 0.20, Pf = 0.00,
decp = 0.01, N0 = 3, and Nf = 10.
The initial ground truth for our source vocab-
ularies has been constructed by the freely avail-
able Google Translate tool. The final ground truth
for our test sets has been established after we
have manually revised the list of pairs obtained by
Google Translate, deleting incorrect entries and
adding additional correct entries. All translation
candidates are evaluated against this benchmark
lexicon.
4.3 Experiment I: Do Our Assumptions Help
Lexicon Extraction?
With this set of experiments, we wanted to test
whether both the assumption of symmetry and
the one-to-one assumption are useful in improv-
ing precision of the initial TI+Cue lexicon extrac-
tion method. We compare three different lexicon
extraction algorithms: (1) the basic TI+Cue ex-
traction algorithm (LALG-BASIC) which serves
as the baseline algorithm5, (2) the algorithm from
Section 3, but without the one-to-one assump-
tion (LALG-SYM), meaning that if we find a
translation pair, we still keep words from the
translation pair in their respective vocabularies,
and (3) the complete algorithm from Section 3
(LALG-ALL). In order to evaluate these lexicon
extraction algorithms for both Italian-English and
Dutch-English, we have constructed a test set of
650 Italian nouns, and a test set of 1000 Dutch
nouns of high and medium frequency. Precision
scores for both language pairs and for all lexicon
extraction algorithms are provided in Table 1.
Based on these results, it is clearly visible that
both assumptions our algorithm makes are valid
5We have also tested whether LALG-BASIC outperforms
a method modeling direct co-occurrence, that uses cosine
to detect similarity between word vectors consisting of TF-
IDF scores in the shared document space (Cimiano et al
2009). Precision using that method is significantly lower,
e.g. 0.5538 vs. 0.6708 of LALG-BASIC for Italian-English.
455
LEX Algorithm Italian-English Dutch-English
LALG-BASIC 0.6708 0.6560
LALG-SYM 0.6862 0.6780
LALG-ALL 0.7215 0.7170
Table 1: Precision scores on our test sets for the 3 dif-
ferent lexicon extraction algorithms.
and contribute to better overall scores. Therefore
in all further experiments we will use the LALG-
ALL extraction algorithm.
4.4 Experiment II: How Does Thresholding
Affect Precision?
The next set of experiments aims at exploring how
precision scores change while we gradually de-
crease threshold values. The main goal of these
experiments is to detect when to stop with the ex-
traction of translation candidates in order to pre-
serve a lexicon of only highly accurate transla-
tions. We have fixed the maximum search space
depth N0 = Nf = 3. We used the same test sets
from Experiment I. Figure 3 displays the change
of precision in relation to different threshold val-
ues, where we start harvesting translations from
the threshold P0 = 0.2 down to Pf = 0.0. Since
our goal is to extract as many correct translation
pairs as possible, but without decreasing the pre-
cision scores, we have also examined what impact
this gradual decrease of threshold also has on the
number of extracted translations. We have opted
for the F? measure (van Rijsbergen, 1979):
F? = (1 + ?2)
Precision ?Recall
?2 ? Precision+Recall (2)
Since our task is precision-oriented, we have set
? = 0.5. F0.5 measure values precision as twice
as important as recall. The F0.5 scores are also
provided in Figure 3.
4.5 Experiment III: Building a Seed Lexicon
Finally, we wanted to test how many accurate
translation pairs our best scoring LALG-ALL al-
gorithm is able to acquire from the entire source
vocabulary, with very high precision still remain-
ing paramount. The obtained highly-precise seed
lexicon then might be employed for an additional
bootstrapping procedure similar to (Koehn and
Knight, 2002; Fung and Cheung, 2004) or sim-
ply for translating context vectors as in (Gaussier
et al 2004).
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Pre
cisi
on/
F-s
cor
e
00.050.10.150.2
Threshold
IT-EN Precision
IT-EN F-score
NL-EN Precision
NL-EN F-score
Pre
cisi
on/
F-s
cor
e
Pre
cisi
on/
F-s
cor
e
Pre
cisi
on/
F-s
cor
e
Figure 3: Precision and F0.5 scores in relation to
threshold values. We can observe that the algorithm
retrieves only highly accurate translations for both lan-
guage pairs while the threshold goes down from value
0.2 to 0.1, while precision starts to drop significantly
after the threshold of 0.1. F0.5 scores also reach their
peaks within that threshold region.
If we do not know anything about a given lan-
guage pair, we can only use words shared across
languages as lexical clues for the construction of
a seed lexicon. It often leads to a low precision
lexicon, since many false friends are detected.
For Italian-English, we have found 431 nouns
shared between the two languages, of which 350
were correct translations, leading to a precision
of 0.8121. As an illustration, if we take the
first 431 translation pairs retrieved by LALG-
ALL, there are 427 correct translation pairs, lead-
ing to a precision of 0.9907. Some pairs do
not share any orthographic similarities: (uccello,
bird), (tastiera, keyboard), (salute, health), (terre-
moto, earthquake) etc.
Following Koehn and Knight (2002), we have
also employed simple transformation rules for the
adoption of words from one language to another.
The rules specific to the Italian-English transla-
tion process that have been employed are: (R1) if
an Italian noun ends in?ione, but not in?zione,
strip the final e to obtain the corresponding En-
glish noun. Otherwise, strip the suffix ?zione,
and append ?tion; (R2) if a noun ends in ?ia,
but not in ?zia or ?fia, replace the suffix ?ia
with ?y. If a noun ends in ?zia, replace the suf-
fix with ?cy and if a noun ends in ?fia, replace
456
Italian-English Dutch-English
Lexicon # Correct Precision F0.5 # Correct Precision F0.5
LEX-1 350 0.8121 0.1876 898 0.8618 0.2308
LEX-2 766 0.8938 0.3473 1376 0.9011 0.3216
LEX-LALG 782 0.8958 0.3524 1106 0.9559 0.2778
LEX-1+LEX-LALG 1070 0.8785 0.4290 1860 0.9082 0.3961
LEX-R+LEX-LALG 1141 0.9239 0.4548 1507 0.9642 0.3500
LEX-2+LEX-LALG 1429 0.8926 0.5102 2261 0.9217 0.4505
Table 2: A comparison of different lexicons. For lexicons employing our LALG-ALL algorithm, only translation
candidates that scored above the threshold P = 0.11 have been kept.
it with ?phy. Similar rules have been introduced
for Dutch-English: the suffix ?tie is replaced by
?tion, ?sie by ?sion, and ?teit by ?ty.
Finally, we have compared the results of the
following constructed lexicons:
? A lexicon containing only words shared
across languages (LEX-1).
? A lexicon containing shared words and trans-
lation pairs found by applying the language-
specific transformation rules (LEX-2).
? A lexicon containing only translation pairs
obtained by the LALG-ALL algorithm that
score above a certain threshold P (LEX-
LALG).
? A combination of the lexicons LEX-1 and
LEX-LALG (LEX-1+LEX-LALG). Non-
matching duplicates are resolved by taking
the translation pair from LEX-LALG as the
correct one. Note that this lexicon is com-
pletely language-pair independent.
? A lexicon combining only translation pairs
found by applying the language-specific
transformation rules and LEX-LALG (LEX-
R+LEX-LALG).
? A combination of the lexicons LEX-2 and
LEX-LALG, where non-matching dupli-
cates are resolved by taking the translation
pair from LEX-LALG if it is present in
LEX-1, and from LEX-2 otherwise (LEX-
2+LEX-LALG).
According to the results from Table 2, we can
conclude that adding translation pairs extracted
by our LALG-ALL algorithm has a major posi-
tive impact on both precision and coverage. Ob-
taining results for two different language pairs
proves that the approach is generic and appli-
cable to any other language pairs. The previ-
ous approach relying on work from Koehn and
Knight (2002) has been outperformed in terms of
precision and coverage. Additionally, we have
shown that adding simple translation rules for lan-
guages sharing same roots might lead to even bet-
ter scores (LEX-2+LEX-LALG). However, it is
not always possible to rely on such knowledge,
and the usefulness of the designed LALG-ALL
algorithm really comes to the fore when the algo-
rithm is applied on distant language pairs which
do not share many words and cognates, and word
translation rules cannot be easily established. In
such cases, without any prior knowledge about the
languages involved in a translation process, one is
left with the linguistically unbiased LEX-1+LEX-
LALG lexicon, which also displays a promising
performance.
5 Conclusions and Future Work
We have designed an algorithm that focuses on ac-
quiring and keeping only highly confident trans-
lation candidates from multilingual comparable
corpora. By employing the algorithm we have
improved precision scores of the methods rely-
ing on per-topic word distributions from a cross-
language topic model. We have shown that the al-
gorithm is able to produce a highly reliable bilin-
gual seed lexicon even when all other lexical clues
are absent, thus making our algorithm suitable
even for unrelated language pairs. In future work,
we plan to further improve the algorithm and use
it as a source of translational evidence for differ-
ent alignment tasks in the setting of non-parallel
corpora.
Acknowledgments
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research
Fund K.U. Leuven, Belgium.
457
References
Jaime G. Carbonell, Jaime G. Yang, Robert E. Fred-
erking, Ralf D. Brown, Yibing Geng, Danny Lee,
Yiming Frederking, Robert E, Ralf D. Geng, and
Yiming Yang. 1997. Translingual information re-
trieval: A comparative evaluation. In Proceedings
of the 15th International Joint Conference on Arti-
ficial Intelligence, pages 708?714.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics, pages 1?5.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus
latent concept models for cross-language informa-
tion retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artifical Intelligence,
pages 1513?1518.
Wim De Smet and Marie-Francine Moens. 2009.
Cross-language linking of news stories on the Web
using interlingual topic modeling. In Proceedings
of the CIKM 2009 Workshop on Social Web Search
and Mining, pages 57?64.
Herve? De?jean, E?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics, pages 1?7.
Mona T. Diab and Steve Finch. 2000. A statis-
tical translation model using comparable corpora.
In Proceedings of the 6th Triennial Conference on
Recherche d?Information Assiste?e par Ordinateur
(RIAO), pages 1500?1508.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 414?420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geomet-
ric view on bilingual lexicon extraction from com-
parable corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic represen-
tation. Psychological Review, 114(2):211?244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 771?779.
Zellig S. Harris. 1954. Distributional structure. Word
10, (23):146?162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 617?625.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management, 41:523?547.
Bo Li, Eric Gaussier, and Akiko Aizawa. 2011. Clus-
tering comparable corpora for bilingual lexicon ex-
traction. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 473?478.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, MA, USA.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26:221?249.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880?889.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 664?671.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics, 31:477?504.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
Wikipedia. In Proceedings of the 18th International
World Wide Web Conference, pages 1155?1156.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 320?322.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
458
Meeting of the Association for Computational Lin-
guistics, pages 519?526.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 98?
107.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 72?79.
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworth.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2009. Feature-
based method for document alignment in compara-
ble news corpora. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 843?851.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 479?484.
459
Proceedings of NAACL-HLT 2013, pages 106?116,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Semantic Similarity of Words as the Similarity of Their
Semantic Word Responses
Ivan Vulic? and Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
Abstract
We propose a new approach to identifying
semantically similar words across languages.
The approach is based on an idea that two
words in different languages are similar if they
are likely to generate similar words (which in-
cludes both source and target language words)
as their top semantic word responses. Se-
mantic word responding is a concept from
cognitive science which addresses detecting
most likely words that humans output as free
word associations given some cue word. The
method consists of two main steps: (1) it uti-
lizes a probabilistic multilingual topic model
trained on comparable data to learn and quan-
tify the semantic word responses, (2) it pro-
vides ranked lists of similar words accord-
ing to the similarity of their semantic word
response vectors. We evaluate our approach
in the task of bilingual lexicon extraction
(BLE) for a variety of language pairs. We
show that in the cross-lingual settings without
any language pair dependent knowledge the
response-based method of similarity is more
robust and outperforms current state-of-the art
methods that directly operate in the semantic
space of latent cross-lingual concepts/topics.
1 Introduction
Cross-lingual semantic word similarity addresses
the task of detecting words that refer to similar se-
mantic concepts and convey similar meanings across
languages. It ultimately boils down to the automatic
identification of translation pairs, that is, bilingual
lexicon extraction (BLE). Such lexicons and seman-
tically similar words serve as important resources
in cross-lingual knowledge induction (e.g., Zhao et
al. (2009)), statistical machine translation (Och and
Ney, 2003) and cross-lingual information retrieval
(Ballesteros and Croft, 1997; Levow et al, 2005).
From parallel corpora, semantically similar words
and bilingual lexicons are induced on the basis of
word alignment models (Brown et al, 1993; Och
and Ney, 2003). However, due to a relative scarce-
ness of parallel texts for many language pairs and
domains, there has been a recent growing interest in
mining semantically similar words across languages
on the basis of comparable data readily available on
the Web (e.g., Wikipedia, news stories) (Haghighi et
al., 2008; Hassan and Mihalcea, 2009; Vulic? et al,
2011; Prochasson and Fung, 2011).
Approaches to detecting semantic word similarity
from comparable corpora are most commonly based
on an idea known as the distributional hypothesis
(Harris, 1954), which states that words with sim-
ilar meanings are likely to appear in similar con-
texts. Each word is typically represented by a high-
dimensional vector in a feature vector space or a so-
called semantic space, where the dimensions of the
vector are its context features. The semantic similar-
ity of two words, wS1 given in the source language
LS with vocabulary V S and wT2 in the target lan-
guage LT with vocabulary V T is then:
Sim(wS1 , w
T
2 ) = SF (cv(w
S
1 ), cv(w
T
2 )) (1)
cv(wS1 ) = [sc
S
1 (c1), . . . , sc
S
1 (cN )] denotes a context
vector for wS1 with N context features ck, where
scS1 (ck) denotes the score for w
S
1 associated with
context feature ck (similar for wT2 ). SF is a sim-
ilarity function (e.g., cosine, the Kullback-Leibler
106
divergence, the Jaccard index) operating on the con-
text vectors (Lee, 1999; Cha, 2007).
In order to compute cross-lingual semantic word
similarity, one needs to design the context features
of words given in two different languages that span
a shared cross-lingual semantic space. Such cross-
lingual semantic spaces are typically spanned by:
(1) bilingual lexicon entries (Rapp, 1999; Gaussier
et al, 2004; Laroche and Langlais, 2010; Tamura
et al, 2012), or (2) latent language-independent se-
mantic concepts/axes (e.g., latent cross-lingual top-
ics) induced by an algebraic model (Dumais et al,
1996), or more recently by a generative probabilis-
tic model (Haghighi et al, 2008; Daume? III and Ja-
garlamudi, 2011; Vulic? et al, 2011). Context vec-
tors cv(wS1 ) and cv(w
T
2 ) for both source and target
words are then compared in the semantic space in-
dependently of their respective languages.
In this work, we propose a new approach to con-
structing the shared cross-lingual semantic space
that relies on a paradigm of semantic word respond-
ing or free word association. We borrow that con-
cept from the psychology/cognitive science litera-
ture. Semantic word responding addresses a task
that requires participants to produce first words that
come to their mind that are related to a presented cue
word (Nelson et al, 2000; Steyvers et al, 2004).
The new cross-lingual semantic space is spanned
by all vocabulary words in the source and the target
language. Each axis in the space denotes a semantic
word response. The similarity between two words is
then computed as the similarity between the vectors
comprising their semantic word responses using any
of existing SF -s. Two words are considered seman-
tically similar if they are likely to generate similar
semantic word responses and assign similar impor-
tance to them.
We utilize a shared semantic space of latent cross-
lingual topics learned by a multilingual probabilistic
topic model to obtain semantic word responses and
quantify the strength of association between any cue
word and its responses monolingually and across
languages, and, consequently, to build semantic re-
sponse vectors. That effectively translates the task
of word similarity from the semantic space spanned
by latent cross-lingual topics to the semantic space
spanned by all vocabulary words in both languages.
The main contributions of this article are:
? We propose a new approach to modeling cross-
lingual semantic similarity of words based on
the similarity of their semantic word responses.
? We present how to estimate and quantify se-
mantic word responses by means of a multilin-
gual probabilistic topic model.
? We demonstrate how to employ our novel
paradigm that relies on semantic word respond-
ing in the task of bilingual lexicon extraction
(BLE) from comparable data.
? We show that the response-based model of sim-
ilarity is more robust and obtains better results
for BLE than the models that operate in the se-
mantic space spanned by latent semantic con-
cepts, i.e., cross-lingual topics directly.
The following sections first review relevant prior
work and provide a very short introduction to multi-
lingual probabilistic topic modeling, then describe
our response-based approach to modeling cross-
lingual semantic word similarity, and finally present
our evaluation and results on the BLE task for a va-
riety of language pairs.
2 Related Work
When dealing with the cross-lingual semantic word
similarity, the focus of the researchers is typically
on BLE, since usually the most similar words across
languages are direct translations of each other. Nu-
merous approaches emerged over the years that try
to induce bilingual word lexicons on the basis of
distributional information. Especially challenging
is the task of mining semantically similar words
from comparable data without any external knowl-
edge source such as machine-readable seed bilin-
gual lexicons used in (Fung and Yee, 1998; Rapp,
1999; Fung and Cheung, 2004; Gaussier et al, 2004;
Morin et al, 2007; Andrade et al, 2010; Tamura
et al, 2012), predefined explicit ontology or cate-
gory knowledge used in (De?jean et al, 2002; Hassan
and Mihalcea, 2009; Agirre et al, 2009), or ortho-
graphic clues as used in (Koehn and Knight, 2002;
Haghighi et al, 2008; Daume? III and Jagarlamudi,
2011). This work addresses that particularly difficult
setting which does not assume any language pair de-
pendent background knowledge. It makes methods
107
developed in such a setting applicable even on dis-
tant language pairs with scarce resources.
Recently, Griffiths et al (2007), and Steyvers and
Griffiths (2007) proposed models of free word asso-
ciation and semantic word similarity in the monolin-
gual settings based on per-topic word distributions
from probabilistic topic models such as pLSA (Hof-
mann, 1999) and LDA (Blei et al, 2003). Addition-
ally, Vulic? et al (2011) constructed several models
that utilize a shared cross-lingual topical space ob-
tained by a multilingual topic model (Mimno et al,
2009; De Smet and Moens, 2009; Boyd-Graber and
Blei, 2009; Ni et al, 2009; Jagarlamudi and Daume?
III, 2010; Zhang et al, 2010) to identify potential
translation candidates in the cross-lingual settings
without any background knowledge. In this paper,
we show that a transition from their semantic space
spanned by cross-lingual topics to a semantic space
spanned by all vocabulary words yields more robust
models of cross-lingual semantic word similarity.
3 Modeling Word Similarity as the
Similarity of Semantic Word Responses
This section contains a detailed description of our
semantic word similarity method that relies on se-
mantic word responses. Since the method utilizes
the concept of multilingual probabilistic topic mod-
eling, we first provide a very short overview of that
concept, then present the intuition behind the ap-
proach, and finally describe our method in detail.
3.1 Multilingual Probabilistic Topic Modeling
Assume that we are given a multilingual corpus
C of l languages, and C is a set of text collec-
tions {C1, . . . , Cl} in those languages. A multi-
lingual probabilistic topic model (Mimno et al,
2009; De Smet and Moens, 2009; Boyd-Graber
and Blei, 2009; Ni et al, 2009; Jagarlamudi and
Daume? III, 2010; Zhang et al, 2010) of a mul-
tilingual corpus C is defined as a set of semanti-
cally coherent multinomial distributions of words
with values Pj(w
j
i |zk), j = 1, . . . , l, for each vo-
cabulary V 1, . . . , V j , . . . , V l associated with text
collections C1, . . . , Cj , . . . , Cl ? C given in lan-
guages L1, . . . , Lj , . . . , Ll. Pj(w
j
i |zk) is calculated
for eachwji ? V
j . The probability scores Pj(w
j
i |zk)
build per-topic word distributions, and they consti-
tute a language-specific representation (e.g., a prob-
ability value is assigned only for words from V j)
of a language-independent cross-lingual latent con-
cept, that is, latent cross-lingual topic zk ? Z .
Z = {z1, . . . , zK} represents the set of all K la-
tent cross-lingual topics present in the multilingual
corpus. Each document in the multilingual corpus
is thus considered a mixture of K cross-lingual top-
ics from the set Z . That mixture for some docu-
ment dji ? Cj is modeled by the probability scores
Pj(zk|d
j
i ) that altogether build per-document topic
distributions.
Each cross-lingual topic from the set Z can be
observed as a latent language-independent concept
present in the multilingual corpus, but each lan-
guage in the corpus uses only words from its own
vocabulary to describe the content of that concept.
For instance, having a multilingual collection in En-
glish, Spanish and Dutch and discovering a topic
on Soccer, that cross-lingual topic would be repre-
sented by words (actually probabilities over words)
{player, goal, coach, . . .} in English, {balo?n (ball),
futbolista (soccer player), goleador (scorer), . . .}
in Spanish, and {wedstrijd (match), elftal (soccer
team), doelpunt (goal), . . .} in Dutch. We have
?
wji?V
j Pj(w
j
i |zk) = 1, for each vocabulary V
j
representing language Lj , and for each topic zk ?
Z . Therefore, the latent cross-lingual topics also
span a shared cross-lingual semantic space.
3.2 The Intuition Behind the Approach
Imagine the following thought experiment. A group
of human subjects who have been raised bilingually
and thus are native speakers of two languages LS
and LT , is playing a game of word associations.
The game consists of possibly an infinite number of
iterations, and each iteration consists of 4 rounds.
In the first round (the S-S round), given a word in
the language LS , the subject has to generate a list
of words in the same language LS that first occur
to her/him as semantic word responses to the given
word. The list is in descending order, with more
prominent word responses occurring higher in the
list. In the second round (the S-T round), the sub-
ject repeats the procedure, and generates the list of
word responses to the same word from LS , but now
in the other language LT . The third (the T-T round)
108
and the fourth round (the T-S round) are similar to
the first and the second round, but now a list of word
responses in both LS and LT has to be generated for
some cue word from LT . The process of generating
the lists of semantic responses then continues with
other cue words and other human subjects.
As the final result, for each word in the source
language LS , and each word in the target language
LT , we obtain a single list of semantic word re-
sponses comprising words in both languages. All
lists are sorted in descending order, based on some
association score that takes into account both the
number of times a word has occurred as an asso-
ciative response, as well as the position in the list
in each round. We can now measure the similarity
of any two words, regardless of their corresponding
languages, according to the similarity of their cor-
responding lists that contain their word responses.
Words that are equally likely to trigger the same as-
sociative responses in the human brain, and more-
over assign equal importance to those responses, as
provided in the lists of associative responses, are
very likely to be closely semantically similar. Addi-
tionally, for a given word wS1 in the source language
LS , some word wT2 in LT that has the highest simi-
larity score among all words inLT should be a direct
word-to-word translation of wS1 .
3.3 Modeling Semantic Word Responses via
Cross-Lingual Topics
Cross-lingual topics provide a sound framework to
construct a probabilistic model of the aforemen-
tioned experiment. To model semantic word re-
sponses via the shared space of cross-lingual top-
ics, we have to set a probabilistic mass that quan-
tifies the degree of association. Given two words
w1, w2 ? V S ? V T , a natural way of expressing the
asymmetric semantic association is by modeling the
probability P (w2|w1) (Griffiths et al, 2007), that is,
the probability to generate word w2 as a response
given word w1. After the training of a multilin-
gual topic model on a multilingual corpus, we obtain
per-topic word distributions with scores PS(wSi |zk)
and PT (wTi |zk) (see Sect. 3.1).
1 The probability
1A remark on notation throughout the paper: Since the
shared space of cross-lingual topics allows us to construct a
uniform representation for all words regardless of a vocabulary
they belong to, due to simplicity and to stress the uniformity,
P (w2|w1) is then decomposed as follows:
Resp(w1, w2) = P (w2|w1) =
K?
k=1
P (w2|zk)P (zk|w1) (2)
The probability scores P (w2|zk) select words that
are highly descriptive for each particular topic. The
probability scores P (zk|w1) ensure that topics zk
that are semantically relevant to the given word
w1 dominate the sum, so the overall high score
Resp(w1, w2) of the semantic word response is as-
signed only to highly descriptive words of the se-
mantically related topics. Using the shared space
of cross-lingual topics, semantic response scores can
be derived for any two words w1, w2 ? V S ? V T .1
The generative model closely resembles the ac-
tual process in the human brain - when we gener-
ate semantic word responses, we first tend to as-
sociate that word with a related semantic/cognitive
concept, in this case a cross-lingual topic (the factor
P (zk|w1)), and then, after establishing the concept,
we output a list of words that we consider the most
prominent/descriptive for that concept (words with
high scores in the factor P (w2|zk)) (Nelson et al,
2000; Steyvers et al, 2004). Due to such modeling
properties, this model of semantic word responding
tends to assign higher association scores for high
frequency words. It eventually leads to asymmet-
ric associations/responses. We have detected that
phenomenon both monolingually and across lan-
guages. For instance, the first response to Span-
ish word mutacio?n (mutation) is English word gene.
Other examples include caldera (boiler)-steam, de-
portista (sportsman)-sport, horario (schedule)-hour
or pescador (fisherman)-fish. In the other associa-
tion direction, we have detected top responses such
as merchant-comercio (trade) or neologism-palabra
(word). In the monolingual setting, we acquire
English pairs such as songwriter-music, discipline-
sport, or Spanish pairs gripe (flu)-enfermedad (dis-
ease), cuenca (basin)-r??o (river), etc.
3.4 Response-Based Model of Similarity
Eq. (2) provides a way to measure the strength of
semantic word responses. In order to establish the
we sometimes use notation P (wi|zk) and P (zk|wi) instead of
PS(wi|zk) or PS(zk|wi) (similar for subscript T ). However,
the reader must be aware that, for instance, P (wi|zk) actually
means PS(wi|zk) if wi ? V S , and PT (wi|zk) if wi ? V T .
109
Semantic responses Response-based similarity
dramaturgo (playwright) play playwright dramaturgo
obra (play) .101 play .142 play .122 playwright
escritor (writer) .083 obra (play) .111 escritor (writer) .087 dramatist
play .066 player .033 obra (play) .073 tragedy
writer .050 escena (scene) .031 writer .060 play
poet .047 jugador (player) .026 poeta (poet) .055 essayist
autor (author) .041 adaptation .025 poet .053 novelist
poeta (poet) .039 stage .024 autor (author) .046 drama
teatro (theatre) .030 game .022 teatro (theatre) .043 tragedian
drama .026 juego (game) .021 tragedy .031 satirist
contribution .025 teatro (theatre) .019 drama .026 writer
Table 1: An example of top 10 semantic word responses and the final response-based similarity for some Spanish and
English words. The responses are estimated from Spanish-English Wikipedia data by bilingual LDA. We can observe
several interesting phenomena: (1) High-frequency words tend to appear higher in the lists of semantic responses
(e.g., play and obra for all 3 words), (2) Due to the modeling properties that give preference to high-frequency words
(Sect. 3.3), a word might not generate itself as the top semantic response (e.g., playwright-play), (3) Both source
and target language words occur as the top responses in the lists, (4) Although play is the top semantic response in
English for both dramaturgo and playwright, its list of top semantic responses is less similar to the lists of those two
words, (5) Although the English word playwright does not appear in the top 10 semantic responses to dramaturgo,
and dramaturgo does not appear in the top 10 responses to playwright, the more robust response-based similarity
method detects that the two words are actually very similar based on their lists of responses, (6) dramaturgo and
playwright have very similar lists of semantic responses which ultimately leads to detecting that playwright is the
most semantically similar word to dramaturgo across the two languages (the last column), i.e., they are direct one-to-
one translations of each other, (7) Another English word dramatist very similar to Spanish dramaturgo is also pushed
higher in the final list, although it is not found in the list of top semantic responses to dramaturgo.
final similarity between two words, we have to com-
pare their semantic response vectors, that is, their
semantic response scores over all words in both
vocabularies. The final model of word similarity
closely mimics our thought experiment. First, for
each word wSi ? V
S , we generate probability scores
P (wSj |w
S
i ) for all words w
S
j ? V
S (the S-S rounds).
Note that P (wSi |w
S
i ) is also defined by Eq. (2).
Following that, for each word wSi ? V
S , we gen-
erate probability scores P (wTj |w
S
i ), for all words
wTj ? V
T (the S-T rounds). Similarly, we calcu-
late probability scores P (wTj |w
T
i ) and P (w
S
j |w
T
i ),
for each wTi , w
T
j ? V
T , and for each wSj ? V
S (the
T-T and T-S rounds).
Now, each word wi ? V S ? V T may be repre-
sented by a (|V S |+ |V T |)-dimensional context vec-
tor cv(wi) as follows:2
[P (wS1 |wi), . . . , P (w
S
|V S ||wi), . . . , P (w
T
|V T ||wi)].
We have created a language-independent cross-
2We assume that the two sets V S and V T are disjunct. It
means that, for instance, Spanish word pie (foot) from V S and
English word pie from V T are treated as two different word
types. In that case, it holds |V S ? V T | = |V S |+ |V T |.
lingual semantic space spanned by all vocabulary
words in both languages. Each feature corresponds
to one word from vocabularies V S and V T , while
the exact score for each feature in the context
vector cv(wi) is precisely the probability that this
word/feature will be generated as a word response
given word wi. The degree of similarity between
two words is then computed on the basis of similar-
ity between their feature vectors using some of the
standard similarity functions (Cha, 2007).
The novel response-based approach of similarity
removes the effect of high-frequency words that tend
to appear higher in the lists of semantic word re-
sponses. Therefore, the real synonyms and trans-
lations should occur as top candidates in the lists
of similar words obtained by the response-based
method. That property may be exploited to identify
one-to-one translations across languages and build a
bilingual lexicon (see Table 1).
4 Experimental Setup
4.1 Data Collections
We work with the following corpora:
110
? IT-EN-W: A collection of 18, 898 Italian-
English Wikipedia article pairs previously used
by Vulic? et al (2011).
? ES-EN-W: A collection of 13, 696 Spanish-
English Wikipedia article pairs.
? NL-EN-W: A collection of 7, 612 Dutch-
English Wikipedia article pairs.
? NL-EN-W+EP: The NL-EN-W corpus aug-
mented with 6,206 Dutch-English document
pairs from Europarl (Koehn, 2005). Although
Europarl is a parallel corpus, no explicit use is
made of sentence-level alignments.
All corpora are theme-aligned, that is, the aligned
document pairs discuss similar subjects, but are
in general not direct translations (except the Eu-
roparl document pairs). NL-EN-W+EP serves to test
whether better semantic responses could be learned
from data of higher quality, and to measure how it
affects the response-based similarity method and the
quality of induced lexicons. Following (Koehn and
Knight, 2002; Haghighi et al, 2008; Prochasson and
Fung, 2011), we consider only noun word types. We
retain only nouns that occur at least 5 times in the
corpus. We record the lemmatized form when avail-
able, and the original form otherwise. Again follow-
ing their setup, we use TreeTagger (Schmid, 1994)
for POS tagging and lemmatization.
4.2 Multilingual Topic Model
The multilingual probabilistic topic model we use
is a straightforward multilingual extension of the
standard Blei et al?s LDA model (Blei et al, 2003)
called bilingual LDA (Mimno et al, 2009; Ni et
al., 2009; De Smet and Moens, 2009). For the de-
tails regarding the modeling assumptions, generative
story, training and inference procedure of the bilin-
gual LDA model, we refer the interested reader to
the aforementioned relevant literature. The poten-
tial of the model in the task of bilingual lexicon ex-
traction was investigated before (Mimno et al, 2009;
Vulic? et al, 2011), and it was also utilized in other
cross-lingual tasks (e.g., Platt et al (2010); Ni et
al. (2011)). We use Gibbs sampling for training.
In a typical setting for mining semantically similar
words using latent topic models in both monolingual
(Griffiths et al, 2007; Dinu and Lapata, 2010) and
cross-lingual setting (Vulic? et al, 2011), the best re-
sults are obtained with the number of topics set to
a few thousands (? 2000). Therefore, our bilingual
LDA model on all corpora is trained with the number
of topics K = 2000. Other parameters of the model
are set to the standard values according to Steyvers
and Griffiths (2007): ? = 50/K and ? = 0.01.
We are aware that different hyper-parameter settings
(Asuncion et al, 2009; Lu et al, 2011), might have
influence on the quality of learned cross-lingual top-
ics, but that analysis is out of the scope of this paper.
4.3 Compared Methods
We evaluate and compare the following word simi-
larity approaches in all our experiments:
1) The method that regards the lists of semantic
word responses across languages obtained by Eq.
(2) directly as the lists of semantically similar words
(Direct-SWR).
2) The state-of-the-art method that employs a simi-
larity function (SF) on theK-dimensional word vec-
tors cv(wi) in the semantic space of latent cross-
lingual topics. The dimensions of the vectors are
conditional topic distribution scores P (zk|wi) that
are obtained by the multilingual topic model directly
(Steyvers and Griffiths, 2007; Vulic? et al, 2011). We
have tested different SF-s (e.g., the Kullback-Leibler
and the Jensen-Shannon divergence, the cosine mea-
sure), and have detected that in general the best
scores are obtained when using the Bhattacharyya
coefficient (BC) (Bhattacharyya, 1943; Kazama et
al., 2010) (Topic-BC).
3) The best scoring similarity method from Vulic?
et al (2011) named TI+Cue. This state-of-the-art
method also operates in the semantic space of latent
cross-lingual concepts/topics.
4) The response-based similarity described in Sect.
3. As for Topic-BC, we again use BC as the simi-
larity function, but now on |V S ? V T |-dimensional
context vectors in the semantic space spanned by
all words in both vocabularies that represent seman-
tic word responses (Response-BC). Given two N -
dimensional word vectors cv(wS1 ) and cv(w
T
2 ), the
BC or the fidelity measure (Cha, 2007) is defined as:
BC(cv(wS1 ), cv(w
T
2 )) =
N?
n=1
?
scS1 (cn) ? sc
T
2 (cn) (3)
111
Corpus: IT-EN-W ES-EN-W NL-EN-W NL-EN-W+EP
Method Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
Direct-SWR .501 .576 .740 .332 .437 .675 .186 .254 .423 .344 .450 .652
Topic-BC .578 .667 .834 .433 .576 .843 .237 .314 .489 .534 .630 .836
TI+Cue .597 .702 .897 .429 .569 .828 .225 .296 .459 .446 .569 .808
Response-BC .622 .729 .882 .517 .635 .891 .236 .320 .511 .574 .653 .864
Table 2: BLE performance of all the methods for Italian-English, Spanish-English and Dutch-English (with 2 different
corpora utilized for the training of bilingual LDA and the estimation of semantic word responses for Dutch-English).
For the Topic-BC method N = K, while N =
|V S ? V T | for Response-BC. Additionally, since
P (zk|wi) > 0 and P (wk|wi) > 0 for each zk ? Z
and each wk ? V S ? V T , a lot of probability mass
is assigned to topics and semantic responses that
are completely irrelevant to the given word. Re-
ducing the dimensionality of the semantic repre-
sentation a posteriori to only a smaller number of
most important semantic axes in the semantic spaces
should decrease the effects of that statistical noise,
and even more firmly emphasize the latent corre-
lation among words. The utility of such semantic
space truncating or feature pruning in monolingual
settings (Reisinger and Mooney, 2010) was also de-
tected previously for LSA and LDA-based models
(Landauer and Dumais, 1997; Griffiths et al, 2007).
Therefore, unless noted otherwise, we perform all
our calculations over the best scoring 200 cross-
lingual topics and the best scoring 2000 semantic
word responses.3
4.4 Evaluation
Ground truth translation pairs.4 Since our task
is bilingual lexicon extraction, we designed a set
of ground truth one-to-one translation pairs for all
3 language pairs as follows. For Dutch-English
and Spanish-English, we randomly sampled a set
of Dutch (Spanish) nouns from our Wikipedia cor-
pora. Following that, we used the Google Trans-
late tool plus an additional annotator to translate
those words to English. The annotator manually
revised the lists and retained only words that have
3The values are set empirically. Calculating similarity
Sim(wS1 , w
T
2 ) may be interpreted as: ?Given word w
S
1 detect
how similar word wT2 is to the word w
S
1 .? Therefore, when
calculating Sim(wS1 , w
T
2 ), even when dealing with symmetric
similarity functions such as BC, we always consider only the
scores P (?|wS1 ) for truncating.
4Available online: http://people.cs.kuleuven.be
/?ivan.vulic/software/
their corresponding translation in the English vo-
cabulary. Additionally, only one possible translation
was annotated as correct. When more than 1 trans-
lation is possible, the annotator marked as correct
the translation that occurs more frequently in the En-
glish Wikipedia data. Finally, we built a set of 1000
one-to-one translation pairs for Dutch-English and
Spanish-English. The same procedure was followed
for Italian-English, but there we obtained the ground
truth one-to-one translation pairs for 1000 most fre-
quent Italian nouns in order to test the effect of word
frequency on the quality of semantic word responses
and the overall lexicon quality.
Evaluation metrics. All the methods under con-
sideration actually retrieve ranked lists of semanti-
cally similar words that could be observed as poten-
tial translation candidates. We measure the perfor-
mance on BLE as Top M accuracy (AccM ). It de-
notes the number of source words from ground truth
translation pairs whose top M semantically simi-
lar words contain the correct translation according
to our ground truth over the total number of ground
truth translation pairs (=1000) (Tamura et al, 2012).
Additionally, we compute the mean reciprocal rank
(MRR) scores (Voorhees, 1999).
5 Results and Discussion
Table 2 displays the performance of each compared
method on the BLE task. It shows the difference in
results for different language pairs and different cor-
pora used to extract latent cross-lingual topics and
estimate the lists of semantic word responses. Ex-
ample lists of semantically similar words over all 3
language pairs are shown in Table 3. Based on these
results, we are able to derive several conclusions:
(i) Response-BC performs consistently better than
the other 3 methods over all corpora and all language
pairs. It is more robust and is able to find some
cross-lingual similarities omitted by the other meth-
112
Italian-English (IT-EN) Spanish-English (ES-EN) Dutch-English (NL-EN)
(1) affresco (2) spigolo (3) coppa (1) caza (2) discurso (3) comprador (1) behoud (2) schroef (3) spar
(fresco) (edge) (cup) (hunting) (speech) (buyer) (conservation) (screw) (fir)
fresco polyhedron club hunting rhetoric purchase conservation socket conifer
mural polygon competition hunt oration seller preservation wire pine
nave vertices final hunter speech tariff heritage wrap firewood
wall diagonal champion hound discourse market diversity wrench seedling
testimonial edge football safari dialectic bidding emphasis screw weevil
apse vertex trophy huntsman rhetorician auction consequence pin chestnut
rediscovery binomial team wildlife oratory bid danger fastener acorn
draughtsman solid relegation animal wisdom microeconomics contribution torque girth
ceiling graph tournament ungulate oration trade decline pipe lumber
palace modifier soccer chase persuasion listing framework routing bark
Table 3: Example lists of top 10 semantically similar words across all 3 language pairs according to our Response-BC
similarity method, where the correct translation word is: (col. 1) found as the most similar word, (2) contained lower
in the list, and (3) not found in the top 10 words.
IT-EN ES-EN NL-EN
direttore-director flauta-flute kustlijn-coastline
radice-root eficacia-efficacy begrafenis-funeral
sintomo-symptom empleo-employment mengsel-mixture
perdita-loss descubierta-discovery lijm-glue
danno-damage desalojo-eviction kijker-viewer
battaglione-battalion miedo-fear oppervlak-surface
Table 4: Example translations found by the Response-BC
method, but missed by the other 3 methods.
ods (see Table 4). The overall quality of the cross-
lingual word similarities and lexicons extracted by
the method is dependent on the quality of estimated
semantic response vectors. The quality of these
vectors is of course further dependent on the qual-
ity of multilingual training data. For instance, for
Dutch-English, we may observe a rather spectacular
increase in overall scores (the tests are performed
over the same set of 1000 words) when we aug-
ment Wikipedia data with Europarl data (compare
the scores for NL-EN-W and NL-EN-W+EP).
(ii) A transition from a semantic space spanned by
cross-lingual topics (Topic-BC) to a semantic space
spanned by vocabulary words (Response-BC) leads
to better results over all corpora and language pairs.
The difference is less visible when using training
data of lesser quality (the scores for NL-EN-W).
Moreover, since the shared space of cross-lingual
topics is used to obtain and quantify semantic word
responses, the quality of learned cross-lingual topics
influences the quality of semantic word responses.
If the semantic coherence of the cross-lingual top-
ical space is unsatisfying, the method is unable to
generate good semantic response vectors, and ul-
timately unable to correctly identify semantically
similar words across languages.
(iii) Due to its modeling properties that assign more
importance to high-frequency words, Direct-SWR
produces reasonable results in the BLE task only for
high-frequency words (see results for IT-EN-W). Al-
though Eq. (2) models the concept of semantic word
responding in a sound way (Griffiths et al, 2007),
using the semantic word responses directly is not
suitable for the actual BLE task.
(iv) The effect of word frequency is clearly visi-
ble when comparing the results obtained on IT-EN-
W with the results obtained on the other Wikipedia
corpora. High-frequency words produce more re-
dundancies in training data that are captured by sta-
tistical models such as latent topic models. High-
frequency words then obtain better estimates of their
semantic response vectors which consequently leads
to better overall scores. The effect of word fre-
quency on statistical methods in the BLE task was
investigated before (Pekar et al, 2006; Prochasson
and Fung, 2011; Tamura et al, 2012), and we also
confirm their findings.
(v) Unlike (Koehn and Knight, 2002; Haghighi et
al., 2008), our response-based method does not rely
on any orthographic features such as cognates or
words shared across languages. It is a pure statis-
tical method that only relies on word distributions
over a multilingual corpus. Based on these distribu-
tions, it performs the initial shallow semantic analy-
sis of the corpus by means of a multilingual prob-
abilistic model. The method then builds, via the
concept of semantic word responding, a language-
113
independent semantic space spanned by all vocabu-
lary words/responses in both languages. That makes
the method portable to distant language pairs. How-
ever, for similar languages, including more evidence
such as orthographic clues might lead to further in-
crease in scores, but we leave that for future work.
6 Conclusion
We have proposed a new statistical approach to iden-
tifying semantically similar words across languages
that relies on the paradigm of semantic word re-
sponding previously defined in cognitive science.
The proposed approach is robust and does not make
any additional language-pair dependent assumptions
(e.g., it does not rely on a seed lexicon, orthographic
clues or predefined concept categories). That effec-
tively makes it applicable to any language pair. Our
experiments on the task of bilingual lexicon extrac-
tion for a variety of language pairs have proved that
the response-based approach is more robust and out-
performs the methods that operate in the semantic
space of latent concepts (e.g., cross-lingual topics)
directly.
Acknowledgments
We would like to thank Steven Bethard and the
anonymous reviewers for their useful suggestions.
This research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund,
KU Leuven, Belgium.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
NAACL-HLT, pages 19?27.
Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsujii.
2010. Robust measurement and comparison of context
similarity for finding translation pairs. In Proceedings
of COLING, pages 19?27.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In Proceedings of UAI, pages 27?34.
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of SI-
GIR, pages 84?91.
A. Bhattacharyya. 1943. On a measure of divergence be-
tween two statistical populations defined by their prob-
ability distributions. Bulletin of the Calcutta Mathe-
matical Society, 35:199?209.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of UAI, pages 75?82.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Sung-Hyuk Cha. 2007. Comprehensive survey on
distance/similarity measures between probability den-
sity functions. International Journal of Mathematical
Models and Methods in Applied Sciences, 1(4):300?
307.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of ACL, pages 407?412.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the Web using in-
terlingual topic modeling. In CIKM Workshop on So-
cial Web Search and Mining (SWSM), pages 57?64.
Herve? De?jean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of COLING, pages 1?7.
Georgiana Dinu and Mirella Lapata. 2010. Topic models
for meaning similarity in context. In Proceedings of
COLING, pages 250?258.
Susan T. Dumais, Thomas K. Landauer, and Michael
Littman. 1996. Automatic cross-linguistic informa-
tion retrieval using Latent Semantic Indexing. In Pro-
ceedings of the SIGIR Workshop on Cross-Linguistic
Information Retrieval, pages 16?23.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP, pages 57?63.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING, pages 414?420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of ACL, pages 526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
114
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proceedings of EMNLP, pages 1192?1201.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proceedings of SIGIR, pages 50?57.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Proceedings of ECIR, pages 444?456.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of ACL, pages 247?256.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition, pages
9?16.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit, pages 79?86.
Thomas K. Landauer and Susan T. Dumais. 1997. Solu-
tions to Plato?s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
COLING, pages 617?625.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of ACL, pages 25?32.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management, 41:523?547.
Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14(2):178?203.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of EMNLP,
pages 880?889.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of ACL, pages 664?671.
Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-
nis. 2000. What is free association and what does it
measure? Memory and Cognition, 28:887?899.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of WWW, pages 1155?1156.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2011. Cross lingual text classification by mining mul-
tilingual topics from Wikipedia. In Proceedings of
WSDM, pages 375?384.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247?266.
John C. Platt, Kristina Toutanova, and Wen-Tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of EMNLP,
pages 251?261.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of ACL, pages 1327?1335.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Joseph Reisinger and Raymond J. Mooney. 2010. A
mixture model with sharing for lexical semantics. In
Proceedings of EMNLP, pages 1173?1182.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nel-
son. 2004. Word association spaces for predicting se-
mantic similarity effects in episodic memory. In Ex-
perimental Cognitive Psychology and Its Applications,
pages 237?249.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of
EMNLP, pages 24?36.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC, pages 77?
82.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from comparable
corpora using latent topic models. In Proceedings of
ACL, pages 479?484.
115
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceedings
of ACL, pages 1128?1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of ACL, pages 55?
63.
116
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479?484,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Word Translations from Comparable Corpora Using Latent
Topic Models
Ivan Vulic?, Wim De Smet and Marie-Francine Moens
Department of Computer Science
K.U. Leuven
Celestijnenlaan 200A
Leuven, Belgium
{ivan.vulic,wim.desmet,sien.moens}@cs.kuleuven.be
Abstract
A topic model outputs a set of multinomial
distributions over words for each topic. In
this paper, we investigate the value of bilin-
gual topic models, i.e., a bilingual Latent
Dirichlet Allocation model for finding trans-
lations of terms in comparable corpora with-
out using any linguistic resources. Experi-
ments on a document-aligned English-Italian
Wikipedia corpus confirm that the developed
methods which only use knowledge from
word-topic distributions outperform methods
based on similarity measures in the original
word-document space. The best results, ob-
tained by combining knowledge from word-
topic distributions with similarity measures in
the original space, are also reported.
1 Introduction
Generative models for documents such as Latent
Dirichlet Allocation (LDA) (Blei et al, 2003) are
based upon the idea that latent variables exist which
determine how words in documents might be gener-
ated. Fitting a generative model means finding the
best set of those latent variables in order to explain
the observed data. Within that setting, documents
are observed as mixtures of latent topics, where top-
ics are probability distributions over words.
Our goal is to model and test the capability of
probabilistic topic models to identify potential trans-
lations from document-aligned text collections. A
representative example of such a comparable text
collection is Wikipedia, where one may observe arti-
cles discussing the same topic, but strongly varying
in style, length and even vocabulary, while still shar-
ing a certain amount of main concepts (or topics).
We try to establish a connection between such latent
topics and an idea known as the distributional hy-
pothesis (Harris, 1954) - words with a similar mean-
ing are often used in similar contexts.
Besides the obvious context of direct co-
occurrence, we believe that topic models are an ad-
ditional source of knowledge which might be used
to improve results in the quest for translation can-
didates extracted without the availability of a trans-
lation dictionary and linguistic knowledge. We de-
signed several methods, all derived from the core
idea of using word distributions over topics as an
extra source of contextual knowledge. Two words
are potential translation candidates if they are often
present in the same cross-lingual topics and not ob-
served in other cross-lingual topics. In other words,
a wordw2 from a target language is a potential trans-
lation candidate for a word w1 from a source lan-
guage, if the distribution of w2 over the target lan-
guage topics is similar to the distribution of w1 over
the source language topics.
The remainder of this paper is structured as fol-
lows. Section 2 describes related work, focusing on
previous attempts to use topic models to recognize
potential translations. Section 3 provides a short
summary of the BiLDA model used in the experi-
ments, presents all main ideas behind our work and
gives an overview and a theoretical background of
the methods. Section 4 evaluates and discusses ini-
tial results. Finally, section 5 proposes several ex-
tensions and gives a summary of the current work.
479
2 Related Work
The idea to acquire translation candidates based
on comparable and unrelated corpora comes from
(Rapp, 1995). Similar approaches are described in
(Diab and Finch, 2000), (Koehn and Knight, 2002)
and (Gaussier et al, 2004). These methods need
an initial lexicon of translations, cognates or simi-
lar words which are then used to acquire additional
translations of the context words. In contrast, our
method does not bootstrap on language pairs that
share morphology, cognates or similar words.
Some attempts of obtaining translations using
cross-lingual topic models have been made in the
last few years, but they are model-dependent and do
not provide a general environment to adapt and ap-
ply other topic models for the task of finding trans-
lation correspondences. (Ni et al, 2009) have de-
signed a probabilistic topic model that fits Wikipedia
data, but they did not use their models to obtain po-
tential translations. (Mimno et al, 2009) retrieve
a list of potential translations simply by selecting
a small number N of the most probable words in
both languages and then add the Cartesian product
of these sets for every topic to a set of candidate
translations. This approach is straightforward, but it
does not catch the structure of the latent topic space
completely.
Another model proposed in (Boyd-Graber and
Blei, 2009) builds topics as distributions over bilin-
gual matchings where matching priors may come
from different initial evidences such as a machine
readable dictionary, edit distance, or the Point-
wise Mutual Information (PMI) statistic scores from
available parallel corpora. The main shortcoming is
that it introduces external knowledge for matching
priors, suffers from overfitting and uses a restricted
vocabulary.
3 Methodology
In this section we present the topic model we used
in our experiments and outline the formal framework
within which three different approaches for acquir-
ing potential word translations were built.
3.1 Bilingual LDA
The topic model we use is a bilingual extension
of a standard LDA model, called bilingual LDA
(BiLDA), which has been presented in (Ni et al,
2009; Mimno et al, 2009; De Smet and Moens,
2009). As the name suggests, it is an extension
of the basic LDA model, taking into account bilin-
guality and designed for parallel document pairs.
We test its performance on a collection of compara-
ble texts which are document-aligned and therefore
share their topics. BiLDA takes advantage of the
document alignment by using a single variable that
contains the topic distribution ?, that is language-
independent by assumption and shared by the paired
bilingual comparable documents. Topics for each
document are sampled from ?, from which the words
are sampled in conjugation with the vocabulary dis-
tribution ? (for language S) and ? (for language
T). Algorithm 3.1 summarizes the generative story,
while figure 1 shows the plate model.
Algorithm
3.1: GENERATIVE STORY FOR BILDA()
for each document pair dj
do
?
???????
???????
for each word position i ? djS
do
{
sample zSji ?Mult(?)
sample wSji ?Mult(?, zSji)
for each word position i ? djT
do
{
sample zTji ?Mult(?)
sample wTji ?Mult(?, zTji)
D
N M
?
?
?
?
?
zSji zTji
wSji wTji
Figure 1: The standard bilingual LDA model
Having one common ? for both of the related doc-
uments implies parallelism between the texts. This
observation does not completely hold for compara-
ble corpora with topically aligned texts. To train the
480
model we use Gibbs sampling, similar to the sam-
pling method for monolingual LDA, with param-
eters ? and ? set to 50/K and 0.01 respectively,
where K denotes the number of topics. After the
training we end up with a set of ? and ? word-topic
probability distributions that are used for the calcu-
lations of the word associations.
If we are given a source vocabulary WS , then the
distribution ? of sampling a new token as word wi ?
WS from a topic zk can be obtained as follows:
P (wi|zk) = ?k,i =
n(wi)k + ?
?|WS |
j=1 n
(wj)
k +WS?
(1)
where, for a word wi and a topic zk, n(wi)k denotes
the total number of times that the topic zk is assigned
to the word wi from the vocabulary WS , ? is a sym-
metric Dirichlet prior,
?|WS |
j=1 n
(wj)
k is the total num-
ber of words assigned to the topic zk, and |WS | is
the total number of distinct words in the vocabulary.
The formula for a set of ? word-topic probability
distributions for the target side of a corpus is com-
puted in an analogical manner.
3.2 Main Framework
Once we derive a shared set of topics along with
language-specific distributions of words over topics,
it is possible to use them for the computation of the
similarity between words in different languages.
3.2.1 KL Method
The similarity between a source word w1 and a tar-
get word w2 is measured by the extent to which
they share the same topics, i.e., by the extent that
their conditional topic distributions are similar. One
way of expressing similarity is the Kullback-Leibler
(KL) divergence, already used in a monolingual set-
ting in (Steyvers and Griffiths, 2007). The simi-
larity between two words is based on the similar-
ity between ?(1) and ?(2), the similarity of con-
ditional topic distributions for words w1 and w2,
where ?(1) = P (Z|w1)1 and ?(2) = P (Z|w2). We
have to calculate the probabilities P (zj |wi), which
describe a probability that a given word is assigned
to a particular topic. If we apply Bayes? rule, we
get P (Z|w) = P (w|Z)P (Z)P (w) , where P (Z) and P (w)
1P (Z|w1) refers to a set of all conditional topic distributions
P (zj |w1)
are prior distributions for topics and words respec-
tively. P (Z) is a uniform distribution for the BiLDA
model, whereas this assumption clearly does not
hold for topic models with a non-uniform topic prior.
P (w) is given by P (w) = P (w|Z)P (Z). If the
assumption of uniformity for P (Z) holds, we can
write:
P (zj |wi) ?
P (wi|zj)
Norm?
= ?j,iNorm?
(2)
for an English word wi, and:
P (zj |wi) ?
P (wi|zj)
Norm?
= ?j,iNorm?
(3)
for a French word wi, where Norm? denotes the
normalization factor
?K
j=1 P (wi|zj), i.e., the sum
of all probabilities ? (or probabilities ? forNorm?)
for the currently observed word wi.
We can then calculate the KL divergence as fol-
lows:
KL(?(1), ?(2)) ?
K?
j=1
?j,1
Norm?
log ?j,1/Norm??j,2/Norm?
(4)
3.2.2 Cue Method
An alternative, more straightforward approach
(called the Cue method) tries to express similarity
between two words emphasizing the associative re-
lation between two words in a more natural way. It
models the probability P (w2|w1), i.e., the probabil-
ity that a target word w2 will be generated as a re-
sponse to a cue source word w1. For the BiLDA
model we can write:
P (w2|w1) =
K?
j=1
P (w2|zj)P (zj |w1)
=
K?
j=1
?j,2
?j,1
Norm?
(5)
This conditioning automatically compromises be-
tween word frequency and semantic relatedness
(Griffiths et al, 2007), since higher frequency words
tend to have higher probabilities across all topics,
but the distribution over topics P (zj |w1) ensures
that semantically related topics dominate the sum.
481
3.2.3 TI Method
The last approach borrows an idea from information
retrieval and constructs word vectors over a shared
latent topic space. Values within vectors are the
TF-ITF (term frequency - inverse topic frequency)
scores which are calculated in a completely ana-
logical manner as the TF-IDF scores for the orig-
inal word-document space (Manning and Schu?tze,
1999). If we are given a source word wi, n(wi)k,S de-
notes the number of times the word wi is associated
with a source topic zk. Term frequency (TF) of the
source word wi for the source topic zk is given as:
TFi,k =
n(wi)k,S
?
wj?WS
n(wj)k,S
(6)
Inverse topical frequency (ITF) measures the gen-
eral importance of the source word wi across all
source topics. Rare words are given a higher im-
portance and thus they tend to be more descriptive
for a specific topic. The inverse topical frequency
for the source word wi is calculated as2:
ITFi = log
K
1 + |k : n(wi)k,S > 0|
(7)
The final TF-ITF score for the source wordwi and
the topic zk is given by TF?ITFi,k = TFi,k ?ITFi.
We calculate the TF-ITF scores for target words as-
sociated with target topics in an analogical man-
ner. Source and target words share the same K-
dimensional topical space, where K-dimensional
vectors consisting of the TF-ITF scores are built
for all words. The standard cosine similarity met-
ric is then used to find the most similar word vectors
from the target vocabulary for a source word vec-
tor. We name this method the TI method. For in-
stance, given a source word w1 represented by a K-
dimensional vector S1 and a target word w2 repre-
sented by a K-dimensional vector T 2, the similarity
between the two words is calculated as follows:
2Stronger association with a topic is modeled by setting a
higher threshold value in n(wi)k,S > threshold, where we have
chosen 0.
cos(w1, w2) =
?K
k=1 S1k ? T 2k?
?K
k=1 (S1k)
2 ?
?
?K
k=1 (T 2k )
2
(8)
4 Results and Discussion
As our training corpus, we use the English-Italian
Wikipedia corpus of 18, 898 document pairs, where
each aligned pair discusses the same subject. In or-
der to reduce data sparsity, we keep only lemmatized
noun forms for further analysis. Our Italian vocabu-
lary consists of 7, 160 nouns, while our English vo-
cabulary contains 9, 166 nouns. The subset of the
650 most frequent terms was used for testing. We
have used the Google Translate tool for evaluations.
As our baseline system, we use the cosine similar-
ity between Italian word vectors and English word
vectors with TF-IDF scores in the original word-
document space (Cos), with aligned documents.
Table 1 shows the Precision@1 scores (the per-
centage of words where the first word from the list
of translations is the correct one) for all three ap-
proaches (KL, Cue and TI), for different number
of topics K. Although KL is designed specifically
to measure the similarity of two distributions, its re-
sults are significantly below those of the Cue and TI,
whose performances are comparable. Whereas the
latter two methods yield the highest results around
the 2, 000 topics mark, the performance of KL in-
creases linearly with the number of topics. This is
an undesirable result as good results are computa-
tionally hard to get.
We have also detected that we are able to boost
overall scores if we combine two methods. We have
opted for the two best methods (TI+Cue), where
overall score is calculated by Score =??ScoreCue+
ScoreTI .3 We also provide the results obtained by
linearly combining (with equal weights) the cosine
similarity between TF-ITF vectors with that between
TF-IDF vector (TI+Cos).
In a more lenient evaluation setting we employ the
mean reciprocal rank (MRR) (Voorhees, 1999). For
a source word w, rankw denotes the rank of its cor-
rect translation within the retrieved list of potential
translations. MRR is then defined as follows:
3The value of ? is empirically set to 10
482
K KL Cue TI TI+Cue TI+Cos
200 0.3015 0.1800 0.3169 0.2862 0.5369
500 0.2846 0.3338 0.3754 0.4000 0.5308
800 0.2969 0.4215 0.4523 0.4877 0.5631
1200 0.3246 0.5138 0.4969 0.5708 0.5985
1500 0.3323 0.5123 0.4938 0.5723 0.5908
1800 0.3569 0.5246 0.5154 0.5985 0.6123
2000 0.3954 0.5246 0.5385 0.6077 0.6046
2200 0.4185 0.5323 0.5169 0.5908 0.6015
2600 0.4292 0.4938 0.5185 0.5662 0.5907
3000 0.4354 0.4554 0.4923 0.5631 0.5953
3500 0.4585 0.4492 0.4785 0.5738 0.5785
Table 1: Precision@1 scores for the test subset of the IT-
EN Wikipedia corpus (baseline precision score: 0.5031)
MRR = 1
|V |
?
w?V
1
rankw
(9)
where V denotes the set of words used for evalu-
ation. We kept only the top 20 candidates from the
ranked list. Table 2 shows the MRR scores for the
same set of experiments.
K KL Cue TI TI+Cue TI+Cos
200 0.3569 0.2990 0.3868 0.4189 0.5899
500 0.3349 0.4331 0.4431 0.4965 0.5808
800 0.3490 0.5093 0.5215 0.5733 0.6173
1200 0.3773 0.5751 0.5618 0.6372 0.6514
1500 0.3865 0.5756 0.5562 0.6320 0.6435
1800 0.4169 0.5858 0.5802 0.6581 0.6583
2000 0.4561 0.5841 0.5914 0.6616 0.6548
2200 0.4686 0.5898 0.5753 0.6471 0.6523
2600 0.4763 0.5550 0.5710 0.6268 0.6416
3000 0.4848 0.5272 0.5572 0.6257 0.6465
3500 0.5022 0.5199 0.5450 0.6238 0.6310
Table 2: MRR scores for the test subset of the IT-EN
Wikipedia corpus (baseline MRR score: 0.5890)
Topic models have the ability to build clusters of
words which might not always co-occur together in
the same textual units and therefore add extra infor-
mation of potential relatedness. Although we have
presented results for a document-aligned corpus, the
framework is completely generic and applicable to
other topically related corpora.
Again, the KL method has the weakest perfor-
mance among the three methods based on the word-
topic distributions, while the other two methods
seem very useful when combined together or when
combined with the similarity measure used in the
original word-document space. We believe that the
results are in reality even higher than presented in
the paper, due to errors in the evaluation tool (e.g.,
the Italian word raggio is correctly translated as ray,
but Google Translate returns radius as the first trans-
lation candidate).
All proposed methods retrieve lists of semanti-
cally related words, where synonymy is not the only
semantic relation observed. Such lists provide com-
prehensible and useful contextual information in the
target language for the source word, even when the
correct translation candidate is missing, as might be
seen in table 3.
(1) romanzo (2) paesaggio (3) cavallo
(novel) (landscape) (horse)
writer tourist horse
novella painting stud
novellette landscape horseback
humorist local hoof
novelist visitor breed
essayist hut stamina
penchant draftsman luggage
formative tourism mare
foreword attraction riding
author vegetation pony
Table 3: Lists of the top 10 translation candidates, where
the correct translation is not found (column 1), lies hidden
lower in the list (2), and is retrieved as the first candidate
(3); K=2000; TI+Cue.
5 Conclusion
We have presented a generic, language-independent
framework for mining translations of words from
latent topic models. We have proven that topical
knowledge is useful and improves the quality of
word translations. The quality of translations de-
pends only on the quality of a topic model and its
ability to find latent relations between words. Our
next steps involve experiments with other topic mod-
els and other corpora, and combining this unsuper-
vised approach with other tools for lexicon extrac-
tion and synonymy detection from unrelated and
comparable corpora.
Acknowledgements
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund
K.U. Leuven, Belgium, and the Flemish SBO-IWT
project AMASS++ (SBO-IWT 0060051).
483
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ?09, pages 75?82.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the web using
interlingual topic modelling. In Proceedings of the
CIKM 2009 Workshop on Social Web Search and Min-
ing, pages 57?64.
Mona T. Diab and Steve Finch. 2000. A statistical trans-
lation model using comparable corpora. In Proceed-
ings of the 2000 Conference on Content-Based Multi-
media Information Access (RIAO), pages 1500?1508.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, pages
526?533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Zellig S. Harris. 1954. Distributional structure. In Word
10 (23), pages 146?162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition - Volume 9, ULA ?02, pages 9?16.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA, USA.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880?889.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of the 18th International World Wide Web
Conference, pages 1155?1156.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?95, pages 320?322.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of the Eighth TExt
Retrieval Conference (TREC-8).
484
