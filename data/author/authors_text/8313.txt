Ext ract ing  the Names  of Genes  and Gene Products  w i th  a 
H idden Markov  Mode l  
Nige l  Co l l i e r ,  Ch ikash i  Nobata  and J un - i ch i  Tsu j i i  
l )el)artm(mt of Information Science 
(h'aduate School of Science 
University of Tokyo, Hongo-7-3-1 
Bunkyo-ku,  Tokyo 113, .Japan 
E-maih {n ige l ,  nova, t su j  ??}@?s. s. u - tokyo ,  ac. jp 
Abst ract  
\~e report the results of a study into the use 
of a linear interpolating hidden Marker model 
(HMM) for the task of extra.('ting lxw\]mi(:al |;er- 
minology fl:om MEDLINE al)stra('ts and texl;s 
in the molecular-bioh)gy domain. Tiffs is the 
first stage isl a. system that will exl;ra('l; evenl; 
information for automatically ut)da.ting 1)ioh)gy 
databases. We trained the HMM entirely with 
1)igrams based (m lexical and character fea- 
tures in a relatively small corpus of 100 MED- 
LINE abstract;s that were ma.rked-ul) l)y (lo- 
main experts wil;h term (:lasses u(:h as t)rol;eins 
and DNA. I.Jsing cross-validation methods we 
a(:\]fieved a,n \].e-score of 0.73 and we (',xmnine the 
('ontrilmtion made by each 1)art of the interl)o- 
lation model to overconfing (la.ta Sl)arsen('.ss. 
1 In t roduct ion  
Ill the last few ye~trs there has t)een a great in- 
vestment in molecula.r-l)iology resear(:h. This 
has yielded many results l;\]la.1;, 1;ogel;her wil;h 
a migration of m:c\]fival mal;erial to the inter- 
net, has resulted in an exl)losion in l;tm nuns- 
\])el7 of research tmbli('ations aa~ailat)le in online 
databases. The results in these 1)al)ers how- 
ever arc not available ill a structured fornmt and 
have to 1)e extracted and synthesized mammlly. 
Updating databases such as SwissProt (Bairoch 
mid Apweiler, 1.997) this way is time (:onsmning 
and nmans l;h~tt he resull;s are not accessible so 
conveniently to he11) researchers in their work. 
Our research is aimed at autonmti(:ally ex- 
tra(:ting facts Kern scientific abstracts and flfll 
papers ill the molecular-biology domain and us- 
ing these to update databases. As the tirst stage 
in achieving this goal we have exl)lored th(; use 
of a generalisable, supervised training method 
based on hidden Markov models (ItMMs) (Ra- 
biner and .\]uang, 1986) fbr tim identification mid 
classitieation of technical expressions ill these 
texts. This task can 1)e considered to be similar 
to the named c.ntity task in the MUC evaluation 
exercises (MUC, 1995). 
In our current work we are using abstracts 
available fl:om PubMed's MEDLINE (MED- 
\],INE, 1999). The MEDLINE (lnta.l)ase is an 
online collection of al)straets for pul)lished jour- 
nal articles in biology mid medicine and con- 
tains more than nine million articles. 
With the rapid growth in the mlmbcr of tmb- 
\]ished l)al)ers in the field of moh;('ular-biolog 3, 
there has been growing interest in the at)pli- 
cation of informa.tion extra(:tion, (Sekimizu et 
al., 1998) (Collier et al, 1999)(Thomas et al, 
1999) (Craven and Kmnlien, 1999), to help solve 
souse (sf the t)robhmss that are associated with 
information overload. 
In the remainder of this i)aper we will first 
of all (ratline the t)ackground to the task and 
then d(~s('ril)e t;hc basics of ItMMs and the fi)r- 
real model wc are using. The following sections 
give an outline of a. lse\v tagged ('orlms (Ohta et 
al., 1999) thnt our team has deveh)i)ed using al)- 
stra('ts taken from a sub-domain of MEDLINF, 
and the results of our experinmnts on this cor- 
lmS. 
2 Background 
Ileeent studies into the use of SUl)ervised 
learning-t)ased models for the n~mled entity task 
in the miero-lsioh)gy domain have. shown that 
lnodels based on HMMs and decision trees such 
as (Nol)al;~t et al, 1999) ~,r(; much more gener- 
alisable and adaptable to slew classes of words 
than systems based on traditional hand-lmilt 
1)attexns a.nd domain specific heuristic rules 
such as (Fukuda et al, 1998), overcoming the 
1)rol)lems associated with data sparseness with 
the help of sophisticated smoothing algorithms 
201 
(Chen and Goodman, 1996). 
HMMs can be considered to be stochastic fi- 
nite state machines and have enjoyed success 
in a number of felds including speech recogni- 
tion and part-of-speech tagging (Kupiec, 1992). 
It has been natural therefore that these mod- 
els have been adapted tbr use in other word- 
class prediction tasks such as the atoned-entity 
task in IE. Such models are often based on n- 
grams. Although the assumption that a word's 
part-of speech or name class can be predicted 
by the previous n-1 words and their classes is 
counter-intuitive to our understanding of lin- 
guistic structures and long distance dependen- 
cies, this simple method does seem to be highly 
effective ill I)ractice. Nymble (Bikel et al, 
1997), a system which uses HMMs is one of the 
most successflfl such systems and trains on a 
corpus of marked-up text, using only character 
features in addition to word bigrams. 
Although it is still early days for the use of 
HMMs for IE, we can see a number of trends 
in the research. Systems can be divided into 
those which use one state per class such as 
Nymble (at the top level of their backoff model) 
and those which automatically earn about the 
model's tructure such as (Seymore t al., 1999). 
Additionally, there is a distinction to be made 
in the source of the knowledge for estimating 
transition t)robabilities between models which 
are built by hand such as (Freitag and McCal- 
lure, 1999) and those which learn fl'om tagged 
corpora in the same domain such as the model 
presented in this paper, word lists and corpora 
in different domains - so-called distantly-labeled 
data (Seymore t al., 1999). 
2.1 Challenges of name finding in 
molecu lar -b io logy  texts  
The names that we are trying to extract fall into 
a number of categories that are often wider than 
the definitions used for the traditional named- 
entity task used in MUC and may be considered 
to share many characteristics of term recogni- 
tion. 
The particular difficulties with identit)dng 
and elassit~qng terms in the molecular-biology 
domain are all open vocabulary and irrgeular 
naming conventions as well as extensive cross- 
over in vocabulary between classes. The irreg- 
ular naming arises in part because of the num- 
ber of researchers from difli;rent fields who are 
TI - Activation of <PROTEIN> JAK kinases 
</PROTEIN> and <PROTEIN>STAT pTvteins 
</PR, OTEIN> by <PROTEIN> interlcukin - 2 
</PROTEIN> and <PROTEIN> intc~fc~vn alph, a
</PROTEIN> , but not the <PROTEIN> T cell 
antigen receptor <~PROTEIN> , in <SOURCE.ct> 
h, uman T lymphoeytes </SOURCE.et> . 
AB The activation of <PROTEIN> Janus 
protein t,.flvsine kinascs </PROTEIN> ( 
<PROTEIN> JAI(s </PROTEIN> ) and 
<PROTEIN> signal transducer and ac- 
tivator of transcription </PROTEIN> ( 
<PROTEIN> STAT </PROTEIN> ) pro- 
reins by <PROTEIN> intcrIcukin ( IL ) 2 
</PROTEIN> , thc  <PROTEIN> T cell antigen 
receptor </PROTEIN> ( <PROTEIN> TCR 
</PROTEIN> ) and <PROTEIN> intc~fcrvn 
( IFN)  alpha </PROTEIN> was czplorcd in 
<SOURCE.ct> human periph, cral blood- derived 
T cclls </SOURCE.et> and the <SOURCE.el> 
leukemic T cell line Kit225 </SOURCE.el> .
Figure 1: Example MEDLINE sentence marked 
up in XML for lfiochemical named-entities. 
working on the same knowledge discovery area 
as well as the large number of substances that 
need to be named. Despite the best, etforts of 
major journals to standardise the terminology, 
there is also a significant problem with syn- 
onymy so that often an entity has more tlm.n 
one name that is widely used. The class cross- 
over of terms arises because nla l ly  prot(:ins are 
named after DNA or RNA with which they re- 
act. 
All of the names which we mark up must be- 
long to only one of the name classes listed in 
Table 1. We determined that all of these name 
classes were of interest o domain experts and 
were essential to our domain model for event 
extraction. Example sentences from a nmrked 
ut) abstract are given in Figure 1. 
We decided not to use separate states ibr 
pre- and post-class words as had been used in 
some other systems, e.g. (Freitag and McCal- 
lure, 1999). Contrary to our expectations, we 
observed that our training data provided very 
poor maximum-likelihood probabilities for these 
words as class predictors. 
We found that protein predictor words had 
the only significant evidence and even this was 
quite weak, except in tlm case of post-class 
words which included a mmfi)er of head nouns 
such as "molecules" or "heterodimers". In our 
202 
Class ~/: Examl)le l)escription 
P1K)TEIN 21.25 .MK ki'n,a.se 
\])NA 358 IL-2 \]rlvmotcr 
\]{NA 30 771I?, 
S()UI{CF,.cl 93 le'ukemic T cell line Kit225 
S()UI\],CE.(:t 417 h,'wm, an T lymphocytes 
SOURCE.too 21 ,%hizosacch, aromyces pombc 
S()URCE.mu 64 mice 
SOURCE.vi 90 ItJV-1 
S()UI{CE.sl 77 membrane 
S()UI{CE.ti 37 central 'ner,vo'us system 
UNK t,y~vsine ph, osphovylal, ion 
t)ro{xfiils~ protein groups, 
families~ cOral)loxes and Slll)Sl;I'llCI;lll'eS. 
I)NAs I)NA groups, regions and genes 
RNAs I~NA groups, regions and genes 
cell line 
(:ell type 
lllOll()-organism 
multiorganism 
viruses 
sublocat;ion 
tissue 
lmckground words 
Table l: Named (mtilsy (:lasses. ~/: indi(:at(ts tsfic ~mmt)cr of XMI, tagged terms in our (:orpus of 100 
abstracts. 
early experiments using I IMMs that in(:orpo- 
rated pro- and 1)ost-c\]ass tates we \[imnd tha.t 
pcrforlnance was signiticantly worse than wil;h- 
Ollt; sll(;h si;at;cs an(l st) w('. formulated the ~uodcl 
as g,~ivcll i l S(;(;\[;iOll :/. 
~,.f(Qi,..~,l < _,Ffi,..~.,, >) + 
(1) 
and for all other words and their name classes 
as tbllows: 
3 Mx.'tzho d 
The lmrl)osc of our mod(;1 is Io lind t;hc n,osl: 
likely so(tilth, liCe of name classes (C) lbr a given 
se(tucncc of wor(ls (W). The set of name ('lasses 
inchutcs the 'Unk' name (:lass whi('h we use li)r 
1)ackgromM words not 1)elonging to ally ()\[ the 
interesting name classes given in Tal)lc 1 and 
t;hc given st;qu(m(:e of words which w(~ ,>('. spans 
a single s(,Jd;cn('c. The task is thcrcfor(~ 1(} max- 
intize Pr((TIH:). \?c iml)lem(mt a I \ ]MM to es- 
t imate this using th('. Markov assuml)tion that 
P r (C I I?  ) can be t'(mnd from t)igrams of ha.me 
classes. 
In th('. following model we (:onsid(u" words to 
1)c ordered pairs consisting of a. surface word, 
W, and a. word tbature, 1", given as < W, F >. 
The word features thcms('Jvcs arc discussed in 
Section 3.1. 
As is common practice, we need to (:alculatc 
the 1)rol)abilities for a word sequence for the 
first; word's name class and every other word 
diflbrently since we have no initial nalnt>class 
to make a transit ion frolll. Accordingly we use 
l;he R)llowing equation to (:alculatc the ilfitial 
name (:lass probability, 
~,,J'(Cz,..~,,I < wi~,..~, 19~,.,~,, >) + 
I',,.( G 
)~o.1' ( G 
A ,./' (G 
;v~.f (G 
5:I./'(G 
), ~./' (G 
),,~.I(G) 
< Wt,l,} >,< l lS,_,, l , i  ~ >,G J) :- 
< 1'15., I ~,, >, < I,V~_~, l )_j >, G.-~) + 
< _, l'i >, < 115_ l, Ft,- ~ >, Ct-., ) + 
< 115, Fi >, < _, P~,_~ >, G ~) + 
< _, l,) >, < ._, 1% ~ >, C~__~) + 
(2) 
whc,:c f(I) is ('alculatcd with nmxinluln- 
likelihood estimates from counts on training 
data, so that tbr example, 
.f(G,I < 1,~5,1,i >,< I,t,~_,, F~_~ >,G-~)  - 
T(< I lS, 1,~ >, G., < 1'15_,, 1~}_~ >, G.-@, 
T(< l'lZj,,l~J >,< \ [ 'Vt- l ,Ft- I  >,Ct- l )  ~3) 
Where T() has been found from counting the 
events in thc training cortms. In our current 
sysl;oln \vc SC\[; t;tlc C()llSt;&lltS ~i }lJld o- i \])y halld 
all(l let ~ ai = 1.0, ~ Ai = 1.0, a0 > al k O-2, 
A0 > A I . . .  _> As. Tile current name-class Ct 
is conditioned oil the current word and fea- 
t;llrc~ thc I)rcviolls name-class, ~*t--l: and t)rc- 
vious word an(t tbaturc. 
Equations 1 and 2 implement a linear- 
interpolating HMM that  incorporates a mmfl)cr 
203 
of sub-models (rethrred to fl'om now by their 
A coefficients) designed to reduce the effects of 
data sparseness. While we hope to have enough 
training data to provide estimates tbr all model 
parameters, in reality we expect to encounter 
highly fl'agmented probability distributions. In 
the worst case, when even a name class pair 
has not been observed beibre in training, the 
model defaults at A5 to an estimate of name 
class unigrams. We note here that the bigram 
language model has a non-zero probability asso- 
ciated with each bigram over the entire vocal)- 
ulary. 
Our model differs to a backoff ormulation be- 
cause we tbund that this model tended to suffer 
fl'om the data sparseness problem on our small 
training set. Bikel et alfor example consid- 
ers each backoff model to be separate models, 
starting at the top level (corresl)onding approx- 
imately to our Ao model) and then falling back 
to a lower level model when there not enough 
evidence. In contrast, we have combined these 
within a single 1)robability calculation tbr state 
(class) transitions. Moreover, we consider that 
where direct bigram counts of 6 or more occur 
in the training set, we can use these directly to 
estimate the state transition probability and we 
nse just the ,~0 model in this case. For counts 
of less than 6 we smooth using Equation 2; this 
can be thought of as a simt)le form of q)nck- 
eting'. The HMM models one state per name 
(:lass as well as two special states tbr the start 
and end o fa  sentence. 
Once the state transition l)rol)abilities have 
been calcnlated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of 1)ossible name class as- 
signments. This is done in linear time, O(MN 2) 
for 54 the nunfl)er of words to be classified and 
N the number of states, to find the highest prob- 
ability path, i.e. to maxinfise Pr(W,  C). In our 
exl)eriments 5/i is the length of a test sentence. 
The final stage of our algorithm that is used 
after name-class tagging is complete is to use 
~ clean-up module called Unity. This creates a 
frequency list of words and name-classes tbr a 
docmnent and then re-tags the document using 
the most frequently nsed name class assigned by 
the HMM. We have generally tbund that this 
improves F-score performance by al)out 2.3%, 
both tbr re-tagging spuriously tagged words and 
Word Feature Exmnl)le 
DigitNmnber 15 
SingleCap M 
GreekLetter alpha 
CapsAndDigits I2 
TwoCaps RalGDS 
LettersAndDigits p52 
hfitCap Interleukin 
LowCaps ka,t)paB 
Lowercase kinases 
IIyphon 
Backslash / 
OpenSquare \[ 
CloseSquare \] 
Colon 
SemiColon 
Percent % 
Oi) enParen ( 
CloseParen ) 
Comma 
FullStop 
Deternliner the 
Conjmmtion and 
Other * + 
Table 2: Word tbatures with examples 
tbr finding untagged words in mlknown contexts 
that had been correctly tagged elsewhere in the 
text. 
3.1 Word  features  
Table 2 shows the character t'eatnres that we 
used which are based on those given for Nymble 
and extended to give high pertbrmance in both 
molecular-biology and newswire domains. The 
intnition is that such features provide evidence 
that helps to distinguish nmne classes of words. 
Moreover we hyt)othesize that such featnres 
will help the model to find sinfilarities between 
known words that were tbnnd in the training 
set and unknown words (of zero frequency in 
the training set) and so overcome the unknown 
word t)rol)lem. To give a simple example: if we 
know that LMP - 1 is a member of PROTEIN  
and we encounter AP - 1 for the first time in 
testing, we can make a fairly good guess about 
the category of the unknown word 'LMP' based 
on its sharing the same feature TwoCaps  with 
the known word 'AP' and 'AP's known relation- 
ship with '- 1'. 
Such unknown word evidence is captured in 
submodels A1 through ),3 in Equation 2. \?e 
204 
consider that character information 1)rovides 
more mealfingflll distinctions between name 
(;\]asses than for examI)le part-of-speech (POS), 
since POS will 1)redominmltly 1)e noun fi)r all 
name-class words. The t'catures were chosen 
to be as domain independent as possit)le, with 
the exception of I lyphon and Greel,:Letter which 
have t)articular signitieance for the terminology 
in this dolnain. 
4 Exper iments  
4.1 Tra in ing  and  tes t ing  set 
The training set we used in our experiments 
('onsisted of 100 MEI)II, INI~ al)stra(:ts, marked 
Ul) ill XS/\[L l)y a (lonmin ext)ert for the name 
('lasses given in Tal)le 1. The mmfl)er of NEs 
that were marked u 1) by class are also given in 
Tfl)le 1 and the total lmmber of words in the 
corlms is 299/\]:0. The al)stracts were chosen from 
a sul)(lomain of moleeular-1)iology that we for- 
mulated by s(',ar(;hing under the terms h/uman, 
blood cell, trav,.scription ,/'actor in the 1)utiMed 
datal)asc, This yiel(l('.(t al)t)roximately 33(10 al/- 
stracts.  
4.2 Resu l ts  
The results are given as F-scores, a (;Ollllll()ll 
measurement for a(:(:ura(:y in tlw, MUC con- 
ferences that eonfl)ines r(;(:all and 1)re(:ision. 
These are eah:ulated using a standard MUC tool 
(Chinchor, 1995). F-score is d('.iin(~d as 
'2 x lS"(eci.sion x l~cc, ll 
F - .~cor.  = (4) 
l)'rccisio~, + \]?,cc(dl 
The tirst set ot7 experiments we did shows the 
effectiveness of the mode.1 for all name (:lasses 
and is smnmarized in Table 3. We see that data 
sparseness does have an etfe('t~ with 1)roteins - 
the most mlmerous (;lass in training - getting 
the best result and I/,NA - the snmllc, st training 
(:lass - getting the worst result. The tal)le also 
shows the ett'eetiveness of the character feature 
set, whi('h in general adds 10.6% to the F-score. 
This is mainly due to a t)ositive effect on words 
in the 1)R,OTEIN and DNA elases, but we also 
see that memt)ers of all SOURCE sul)-('lasses 
sufl'er from featurization. 
We have atteml)ted to incorl)orate generali- 
sation through character t'eatm:es and linear in- 
teri)olation, which has generally \])een quite su(:- 
cessful. Nevertheless we were (:urious to see just 
Class Base llase-l'eatures 
PROTEIN 0.759 0.670 (-11.7%) 
DNA 0.472 0.376 (-20.3%) 
\]~NA 0.025 0.OOO (-leo.o%) 
SOURCE(all) 0.685 0.697 (+1.8%) 
S()UI{CE.cl 0.478 0.503 (+5.2%) 
SOURCE.el 0.708 0.752 (+6.2%) 
SOURCE.me 0.200 0.311 (+55.5%) 
SOURCE.mu 0.396 0.402 (+1.5%) 
SOURCE.vi 0.676 0.713 (+5.5%) 
S()URCI,Lsl 0.540 0.549 (+1.7%) 
SOURCE.ti 0.206 0.216 (+4.9%) 
All classes 0.728 0.651 (-10.6%) 
q)d)le 3: Named entity acquisition results us- 
ing 5-fi)ld cross validation on 100 XML tagged 
MEI)I~INE al/stra(:ts, 80 for training and 20 fin. 
testing, l\]ase-J'(',at'urc.s u es no character feature 
inibrmation. 
)~ Mode\[ No. 
# Texts 0 1 2 3 4 5 
80 
40 
20 
10 
5 
0.06 0.22 0.10 0.67 0.93 1.0 
0.06 0.19 0.10 0.63 0.94 1.0 
().()~l 0.15 0.09 0.59 0.89 1.0 
0.03 0.12 0.08 0.52 0.83 1.0 
0.02 0.09 0.06 0.41 0.68 1.0 
Tal)le 4: M(',an lmml)er of successflll calls to sul)- 
m(i(t(;ls during testing as a fl'aetion of total mnn- 
1)er (If stale transitions in the Viterl)i latti(:e, g/: 
T(!xis indicates the mmfl)er of al)stra(:ts used ill 
training. 
whi(:h t)arts of the model were contributing to 
the bigram s(:ores. Table 4 shows the l)ercent- 
age of bigranls which could be mat('hed against 
training t)igrams. The result indicate tha~ a 
high 1)ereentage of dire(:t bigrams in the test 
eorl)uS never al)t)(;ar in the training (:oft)us and 
shows tha, t our HMM model is highly depel> 
(l(mt on smoothing through models ~kl and )~:~. 
\?e can take another view of the training data 
1)y 'salalni-slieing' the model so that only evi- 
(tenee from 1)art of the model is used. Results 
are shown in Tat)le 5 and support the eonchl- 
sion that models Al, A2 and Aa are. crucial at 
this sir,(; of training data, although we would 
expect their relative ilnportance to fifil as we 
have more (tircct observations of bigrams with 
larger training data sets. 
Tal)le 6 shows the rolmstness of the model 
205 
I Backoff models 
\[ F-score (all classes) 0.728 0.722 0.644 0.572 0.576 \] 
Table 5: F-scores using different nfixtures of models tested on 100 abstracts, 80 training and 20 
testing. 
I # Texts 80 40 20 10 5 \] 
I F-score 0.728 0.705 0.647 0.594 0.534\] 
Table 6: 
training 
stracts). 
F-score for all classes agMnst size of 
corpus (in number of MEDLINE ab- 
for data sparseness, so that even with only 10 
training texts the model can still make sensible 
decisions about term identification and classi- 
fication. As we would expect;, the table ;flso 
clearly shows that more training data is better, 
and we have not yet reached a peak in pertbr- 
i nance .  
5 Conc lus ion  
HMMs are proving their worth for various 
tasks in inibrmation extraction and the results 
here show that this good performance can be 
achieved across domains, i.e. in molecular- 
biology as well as rising news paper reports. The 
task itself', while being similar to named entity 
in MUC, is we believe more challenging due to 
the large nunfl)er of terms which are not proper 
nouns, such as those in the source  sub-classes as 
well as the large lexieal overlap between classes 
such as PROTEIN  and DNA. A usefifl line of 
work in the future would be to find empirical 
methods for comparing difficulties of domains. 
Unlike traditional dictionary-based lnethods, 
the method we have shown has the advantage of 
being portable and no hand-made patterns were 
used. Additiolmlly, since the character tbatures 
are quite powerful, yet very general, there is lit- 
tle need for intervention to create domain spe- 
cific features, although other types of features 
could be added within the interpolation frame- 
work. Indeed the only thing that is required is 
a quite small corpus of text containing entities 
tagged by a domain expert. 
Currently we have optinfized the ,k constants 
by hand but clearly a better way would be to do 
this antomatically. An obvious strategy to use 
would be to use some iterative learning method 
such as Expectation Maximization (Dempster 
et al, 1977). 
The model still has limitations, most obvi- 
ously when it needs to identity, term boundaries 
for phrases containing potentially ambiguous lo- 
cal structures uch as coordination and pa.ren- 
theses. For such cases we will need to add post- 
processing rules. 
There are of course many NF, models that 
are not based on HMMs that have had suc- 
cess in the NE task at the MUC conferences. 
Our main requirement in implementing a model 
for the domain of molecular-biology has been 
ease of development, accuracy and portability 
to other sub-domains since molecular-biology it-
self is a wide field. HMMs seemed to be the 
most favourable option at this time. Alterna- 
tives that have also had considerable success 
are decision trees, e.g. (Nobata et al, 1.999) 
and maximum-entropy. The maximum entropy 
model shown in (Borthwick et al, 1998) in par- 
ticular seems a promising approach because of 
its ability to handle overlapping and large fea- 
ture sets within n well founded nmthenmtical 
ti'amework. However this implementation of the 
method seems to incorporate a number of hand- 
coded domain specitic lexical Datures and dic- 
tionary lists that reduce portability. 
Undoubtedly we could incorporate richer tba- 
tures into our model and based on the evidence 
of others we would like to add head nouns as 
one type of feature in the future. 
Acknowledgements  
We would like to express our gratitude to Yuka 
Tateishi and Tomoko Ohta of the Tsujii labora- 
tory for their efforts to produce the tagged cor- 
tins used in these experiments and to Sang-Zoo 
Lee also of the Tsujii laboratory tbr his com- 
ments regarding HMMs. We would also like to 
thank the anonymous retirees tbr their helpflfl 
comments. 
206 
\]~{,eferences 
A. Bairoch and R. Apweiler. 1997. The SWISS- 
PF\[OT 1)r{)t{~in sequence data bank and its 
new SUl)l)lement 15:EMBL. Nucleic Acids Re- 
search, 25:31-36. 
D. Bikel, S. Miller, I:L Schwartz, and 
R. Wesichedel. 1997. Nymble: a high- 
t)ertbrmanee l arning \]mlne-tin(ler. In Pro- 
ceedings of the Fifth Co~@rcrcncc on Applied 
Natural Langua9 e \])~vcessi'n,g, pages 194 201. 
A. Borthwick, J. Sterling, E. Agichtein, and 
ll,. Grishman. 1998. Ext}l{}iting div(:rse 
knowledge sour(:es via lllaXillllllll (mtrol}y in 
named entity recogniti{}n. In P'mcccdings 
of the Worlcshop on Very Lar.qc Corpora 
(WVLC'98). 
S. Chert and J. Goodman. 1996. An empirical 
study of smoothing te{:hmfiques tbr language 
motleling. 3/tst Annual Meeting of tlt,(: Associ- 
ation of Computational Linguistics, Calffof 
nia, USA, 24-27 .hme. 
N. Chin{:h{}r. 1995. MUC-5 ewduati{m etrics. 
In In Pwcecdings of th, c i"ffl, h, Mc.ss(u.le Un- 
dcrstandin 9 Cou:fe'rencc (MUC-5), Baltimore,, 
Maryland, USA., 1)ages 69 78. 
N. Collier, It.S. Park, N. Ogata, Y. Tateishi, 
C. Nol}ata, 'F. Ohta, T. Sekimizu, H. \]mai, 
and J. Tsujii. 1999. The GENIA 1}r{)je(:t: 
corlms-1)ascd kn(}wlcdge acquisitio\], and in- 
forlnal, ion extra('tion f\]'Olll genome r{',sear(:h 
t)al)ers, in Proccediu, fl.s of the A n',,'aal M(',eting 
of the European ch, aptcr of the Association for 
Computational Lingu'istic,s (EA (/\]3 '99), 3 uuc. 
M. Craven and 3, Kumlien. 1999. Construct- 
ing bioh}gical knowh;{tg{; t}ases t)y extracting 
information from text sour(:es. In \]}~vc(:(,Aings 
of the 7th, hl, tcrnational CoTff(:rence on Intelli- 
gent Systcmps for Molecular Biology (ISMB- 
99), Heidellmrg, Germmly, August 6 10. 
A.P. Dempster, N.M. Laird, and D.B. Rubins. 
1977. Maximmn likelihood from incoml)lete 
data via the EM algorithm. ,\]ou'rnal of the 
Royal Statistical Society (B), 39:1-38. 
l). Freitag and A. McCMlum. 1999. Intbrma- 
tion extraction with HMMs and shrinkage. 
In Proceedings of the AAAl'99 Worl~.~h, op ou, 
Machine Learning for IT~:formation Extrac- 
tion, Orlando, Florida, July 19th. 
K. Fuku(la, T. Tsunoda, A. 2)mmra, and 
T. Takagi. 1998. ~12)ward intbrmation extrac- 
tion: identifying l)rotein names from biologi- 
eal papers. Ill PTvcccdings of thc Pac'lific Sym- 
posium on Biocomp'uting'98 (PSB'98), .Jan- 
1uAYy. 
.1. Kupiec. 1992. l/obust Imrt-ofspeech tag- 
ging using a hidden markov model. Computer 
Speech and Lang'aagc, 6:225-242. 
MEI)LINE. 1999. The PubMed 
datal)ase can be t'(mnd at:. 
httt)://www.ncbi.nhn.nih.gov/Pul}Med/. 
DAIIPA. 1995. l}roceeding.s o.fl th, c Sixth, 
Message Understanding Cou:fcrcnce(MUC-6), 
Cohmdfia, MI), USA, Nove, nfl}er. Morgan 
Nail\['\] l lal l l l .  
C. Nobata, N. Collier, and J. Tsu.iii. 1999. Au- 
tomatic term identification and classification 
in 1}iology texts. In Proceeding.s" of the Nat- 
u'ral Lang,lmgc Pacific Rim Symposium (NL- 
PRS'2000), November. 
Y. Ohta, Y. Tateishi, N. Collie'r, C. No- 
1)ata, K. II}ushi, and J. Tsujii. 1999. A 
senmntieally annotated cort)us from MED- 
L\]\[NE al)sl;ra{:l;s. In l}'rocccd,bu.l s of th.c ~:nth. 
Workshop on Go'home I~fformatics. Universal 
A{:ademy Press, Inc., 14 15 Deccntl)er. 
l~. llabiner and B..\]uang. 1!)86. An intro{tu{:- 
ti(m to hidden Markov too(Ms. H'2EE ASSP 
Magazi',,(',, 1}ages d 16, Jammry. 
T. Sekilnizu, H. Park, and J. 'l'sujii. 1998. 
I{lenti\[ying l;he interaction 1)etween genes an{1 
gOlle i}ro(lucts \]}ase(l on f\]'e(lue\]My seen verbs 
in n\]e{tline al)si;rael;s. Ill ~(:'li,()?ll,('~ \]~ffor'm, al, ics'. 
Univcrsa,1 Academy Press, Inc. 
K. Seymore, A. MeCallum, and l{. I{oscnfeld. 
1999. Learning hidden Markove strucl:ure 
for informati{m (,xtraction. In \])wcccdings of 
the AAAl'99 Workshop on Macfli'n,(: Lcarni'n 9 
for l',fo'rmation E:draction, Orland{}, Flori{ta., 
July 19th. 
.J. Thomas, D. Milward, C. Ouzounis, S. Pul- 
man, and M. Carroll. 1999. Automatic ex- 
traction of 1)rotein interactions fl'om s{'ien- 
tific abstracts. In Proceedings of the I}ac'll/ic 
Symposium on Biocomputing'99 (PSB'99), 
Hawaii, USA, Jmmary 4-9. 
A. 3. Vit(;rbi. 1967. Error l){mnds for {:onvolu- 
tions e{}{les and an asyml)totically optimum 
deco(ling algorithm. IEEE Tran,s'actiou,.s' on
I~formation Theory, IT-13(2):260 269. 
207 
A Method of Measuring Term Representativeness 
- Baseline Method Using Co-occurrence Distribution - 
Toru Hisamitsu,* Yoshiki Niwa,* and Jun-ichi Tsujii * 
$ Central Research Laboratory, Hitachi, Ltd. 
Akanuma 2520, Hatoyama, Saitama 350-0395, Japan 
{hisamitu, yniwa} @harl.hitachi.co.jp 
Abstract 
This paper introduces a scheme, which we call 
the baseline method, to define a measure of term 
representativeness and measures defined by using 
the scheme. The representativeness of a term is 
measured by a normalized characteristic value 
defined for a set of all documents that contain the 
term. Normalization is done by comparing the 
original characteristic value with the 
characteristic value defined for a randomly 
chosen document set of the same size. The latter 
value is estimated by a baseline function obtained 
by random sampling and logarithmic linear 
approximation. We found that the distance 
between the word distribution in a document set 
and the word distribution in a whole corpus is an 
effective characteristic value to use for the 
baseline method. Measures defined by the 
baseline method have several advantages 
including that they can be used to compare the 
representativeness of two terms with very 
different frequencies, and that they have 
well-defined threshold values of being 
representative. In addition, the baseline function 
for a corpus is robust against differences in 
corpora; that is, it can be used for normalization 
in a different corpus that has a different size or is 
in a different domain. 
1 Introduction 
Measuring the representativeness (i.e., the 
informativeness or domain specificity) of a term ~ is 
essential to various tasks in natural language 
processing (NLP) and information retrieval (IR). It 
is particularly crucial when applied to an IR 
interface to help a user find informative terms. For 
instance, when the number of retrieved ocuments i
intractably large, an overview of representative 
words in the documents i needed to understand the 
contents. To enable this, an IR system, called 
DualNAVI, that has two navigation windows where 
one displays a graph of representative words in the 
retrieved ocuments, was developed (Nishioka et al 
1997). This window helps users grasp the contents 
of retrieved ocuments, but it also exposes problems 
concerning existing representativeness measures. 
Figure l shows an example of a graph for the 
query '~Y-'~'~(-~ (electronic money), with Nihon 
A term is a word or a word sequence.  
{ Graduate School of Science, the University of Tokyo 
7-3-I Hongo, Bunkyo-ku, Tokyo 113-8654, Japan 
tsujii@is.s.u-tokyo.ac.j p 
Keizai Sl#mbun (a financial newspaper) 1996 as the 
corpus. Frequently appearing words are displayed in 
the upper part of the window, and words are selected 
by a tf-idf-like measure (Niwa et al 1997). Typical 
non-representative words are filtered out by using a 
stop-word list. 
me n e y . ~ v ~ ~ ~ \ ] ~ - - - - - - ~   
electronic ---- /tmu,.~ \] ~ . / / ' f -~year  
~ / \  ~L~I2~-- month 
read c pher 
Figure 1 
A topic word graph when the query is 
N~-e  ~---(etectronic money). 
One problem is the difficulty of suppressing 
uninformative words such as ~V- (year), -- (one), 
and )\] (month) because classical measures, uch as 
tf-idf are too sensitive to word frequency and no 
established method to automatically construct a 
stop-word list has bcen developed. 
Another problem is that the difference in the 
representativeness of words is not sufficiently 
\[In ~j indicated. In the exarnple above, highlighting " .... 
(cipher) over less representative words such as ~'U~ 
k_ 5 (read) would be useful. Most classical 
measures based on only term frequency and 
document frequency cannot overcome this problem. 
To define a more elaborate measure, atternpts 
to incorporate more precise co-occurrence 
information have been made. Caraballo et al (1999) 
tried to define a measure for "specificity" of a noun 
by using co-occurrence intbrmation of a noun, but it 
was not very successful in the sense that the 
measure did not particularly outperformed the term 
frequency. 
Hisamitsu et al (1999) developed a measure 
of the representativeness of a term by using 
co-occurrence information and a normalization 
320 
technique. Fhe measure is based on the distance 
between tile word distribution in the documenls 
containing a term and the word distribution ill the 
whole corpus. Their measure overcomes previously 
mentioned problems and preliminary experiments 
showed that this measure worked better than 
existing measures in picking out 
representative/non-representative terms. Since tile 
normalizatio11 technique plays a crucial part of 
constructing tile nleasure, issl_lcs related to the 
normalization need more study. 
In this paper we review Hisamitsu's measure 
and introduce a generic scheme - which we call the 
baseline method for convenience - that can be used 
to define wu'ious measures including the above. A 
characteristic value of all documents containing a 
term Y is normalized by using a baseline fimction 
that estimates the characteristic value of a randomly 
chosen document set of the slune size. Tile 
normalized value is then used to measure tile 
representativeness of  the term 77. A measure defined 
by the baseline.-method has several advantages 
compared to classical measures. 
We compare four measures (two classical 
ones and two newly defined ones) from w.trious 
viewpoints, and show the superiority of the measure 
based on the normalized distance between two word 
distributions. Another important finding is that the 
baseline function is substantially portable, that is, 
one defined for a corpus can be used for a different 
corpus even it" the two corpora Mvc considerably 
different sizes or arc in different domains. 
2. E,isting measures of representative~kess 
2.1 Overview 
Various methods for mea:.;uring the inforlnativcness 
or domain specificity of a word have been proposed 
in the donmins of IR and term extraction in NLP 
(see the survey paper by Kageura 1996). Ill 
characterizing a term, Kagcura introduced the 
concepts of "unithood" and "termhood": unithood is 
"the degree of strength or stability of syntagnmtic 
combinations or collocations," and termhood is "tile 
degree to which a linguistic unit is related to (or 
more straightR)rwardly, represents) domain-specific 
concepts." Kageura's termhood is therefore what we 
call representativeness here. 
Representativeness lneasurcs were first 
introduced in till IR domain for determining 
indexing words. The simplest measure is calculated 
fi'om only word frequency within a document, For 
example, tile weight 1 o of word w~ in document d/ is 
defined by 
./~./ 
/ r . .  _ ___  , 
~ >Z/< .It-i 
wherc./ii is tilt: frequency of word wi in document (\]i 
(Sparck-Jolms 1973, Noreauh ct al. 1977). More 
elaborate measures for tcrmhood combine word 
frequency within a document and word occurrence 
over a whole corpus. For instance, (/:/4/; the most 
comlnonly used measure, was originally defined its 
N IoIcl\[ \[,i = ./;, x log( - - ) ,  
where iV, and N,,,,~ are, respectively, tile number of  
documents containing word wg and the total number 
of documents (Salton et al 1973). There are a 
wlriety or" definitions of ?idJl but its basic feature is 
that a word appearing more flequently in fewer 
documents i assigned a higher value. If documents 
are categorized beforehand, we can use a more 
sophisticated measure based on the X-' test of the 
hypothesis that an occurrence of" the target word is 
independent of categories (Nagao et al 1976). 
Research on automatic term extraction in 
NLP domains has led to several measures for 
weighting terms mainly by considering the unithood 
of a word sequence. For instance, mutual 
information (Church ct al. 1990) and the 
log-likelihood (Dunning 1993) methods for 
extracting word bigrams have been widely used. 
Other measures for calculating the unithood of 
n-grains have also been proposed (Frantzi et al 
1996, Nakagawa et al 1998, Kita et al 1994). 
2.2 Problems 
Existing measures uffer from at least one of the 
following problems: 
(1) Classical measures sucll as t/-idjare so sensitive 
to term frequencies that they fail to avoid very 
frequent non-informative words. 
(2) Methods using cross-category word distributions 
(such as the Z-' method) can be applied only if 
documents in a corpus are categorized. 
(3) Most lneasures in NLP domains cannot reat 
single word terms because they use the unithood 
strength of multiple words. 
The threshold wdue lbr being representative is 
dcfincd in all ad hoc manner. 
constructs 
(4) 
The scheme that we describe here 
measures that are free of these problems. 
3. Baseline method for defining 
representativeness measures 
3.1 Basic idea 
This subsection describes the method we developed 
for defining a measure of  term representativeness. 
Our basic idea is smmnarized by tile lhmous quote 
(Firth 1957) : 
"You shall k~ow a wo~zt l~y the coml)alT); ir 
Iwup.v." 
We interpreted this as the following working 
hypolhesis: 
321 
For any term T, if the term is 
representat ive ,  D(T), the seto fa l l  
documents  conta in ing  T, shou ld  have 
some character i s t i c  p roper ty  
compared  to the "average" 
To apply this hypothesis, we need to specify a 
measure to obtain some "property" of a document 
set and the concept of "average". Thus, we 
converted this hypothesis into the following 
procedure: 
Choose a measure  M character i z ing  
adocumentset .  For termT,  ca lcu la te  
M(D(T)), the va lue of the measure  
for D(T). Then compare M(D(T)) with  
B~(#D(T)), where #D(T)is the number  
of words  conta ined  in #D(T), and B,~ 
est imates  the va lue  of M(D) when D 
is a randomly  chosen document  set 
of s ize #D(T). 
Here, M measures the property and BM estinmtes the 
average. The size of a document set is defined as the 
number of words it contains. 
We tried two measures as M. One was the 
number of different words (referred to here as 
DIFFNUM) appearing in a document set. Teramoto 
conducted an experiment with a snmll corpus and 
reported that DIFFNUM was useful for flicking out 
important words (Teramoto et al 1999) under the 
hypothesis that the number of different words 
co-occurring with a topical (representative) word is 
snmllcr than that with a generic word. The other 
measure was the distance between the word 
distribution in D(T) and the word distribution in the 
whole corpus Do. The distance between the two 
distributions can be measured in various ways, and 
we used the log-likelihood ratio as in Hisalnitsu et al 
1999, and denote this rneasure as LLR. Figure 2 
plots (#D, M(D))s when M is DIFFNUM or LLR, 
where D varies over sets of randomly selected 
documents of various sizes from the articles in 
Nikkei-Shinbun 1996. 
For measure M, we define Rep(T, M), the 
representativeness of T, by normalizing M(D(T)) by 
BM(#D(T)). The next subsection describes the 
construction of By and the normalization. 
3.2 Base l ine  funct ion  and  normal i za t ion  
Using the case of LLR as an example, this 
subsection explains why nornmlization is necessary 
and describes the construction of a baseline 
function. 
Figure 3 superimposes coordinates {(#D(7), 
LLR(D(T))} s onto the graph of LLR where T varies 
2 With Teramoto's method, eight paranaeters must be ttmed to 
normalize D1FFNUM( D( T) ), but the details of how this was 
done were not disclosed. 
I000000 
100000 
10000 
I000 
100 
10 
I 
100 100000 100000000 
#D: Size of randomly chosen documents 
F igure  2 
Values of DIFFNUM and LLR for 
randomly chosen document set. 
II ~ ,~ ' '  i " " ~ . . . .  over -ytc pner), qi(year), )J (month), i~cJ~-ll~7~ 
(read), -- (one), j -  ~ (do), and ~}: i>~/(economy). 
Figure 3 shows that, for example, LLR(D(J-~)) is 
smaller than LLR(D( ~,~ }J5 )), which reflects our 
linguistic intuition that words co-occurring with 
"economy" are more biased than those with "do". 
However, LLR(DOI~-',3-)) is smaller than LLR(D(.?J/- 
I~6))  and smaller even than LLR(D@O-~)). This 
contradicts our linguistic intuition, and is why 
values of LLR are not dircctly used to compare the 
representativeness of terms. This phenomenon arises 
because LLR(D(~) generally increases as #\])(7) 
increases. We therefore need to use some form of 
normalization to offset this underlying tendency. 
We used a baseline function to normalize the 
values. In this case, Bu,(o) was designed so that it 
approximates the curve in Fig. 3. From the 
definition of the distance, it is obvious that Bu.t~(0) = 
Bu.R(#Do) = 0. At the limit when #1)(~--+ o% Bu.R(') 
becomes a monotonously increasing function. 
The curve could be approxinmted precisely 
through logarithmic linear approximation near (0, 0). 
~lb make an approximation, up to 300 documents are 
randomly sampled at a time. (Let each randomly 
chosen document set be denoted by D. The number 
of sampled ocuments are increased from one to 300, 
repeating each number up to five times.) Each (#D, 
LLR(D)) is converted to (log(#D), Iog(LLR(D))). 
The curve formulated by the (log(#D), log(LLR(D))) 
values, which is very close to a straight line, is 
further divided into nmltiple parts and is part-wise 
approximated by a linear function. For instance, in 
the interval I = {x \[ 10000 _<x < 15,000}, 
Iog(LLR(D)) could be approximated by 1.103 + 
1.023 x log(#D) with R e = 0.996. 
For LLR, we define Rep(T, LLR), the 
representativeness of T by normalizing LLR(D(7)) 
by Bu.R(#D(7)) asfollows: 
Rep(r, LLR) = 100 x (Iog(LLR(D(T))) _ 1). 
"log(Bu, (# D(T))) 
322 
For instance, when we used Nihon Keizai 
Shimbun 1996, The average of I OOx(log(LLR(D)) 
~log(BLue (#D)) - 1), Avr, was -0.00423 and the 
standard deviation, cs, was about 0.465 when D 
varies over randomly selected octuncnt sets. l';very 
observed wflue fell within Avs'4-4er and 99% ot' 
observed values fell within Avl?3cs. This hapfmlled 
in all corpora (7 orpora) we tested. Theretbrc, we 
can de:fine the threshold of being representative as, 
say, Aw" + 40. 
umoooo ~:} f ' i (economy)  . _ _  _ h.. .  J J~n ion lh )  
! ;~i'~;i/.Jl).~) ( read)  i 
i 
., (cipher) \ !! ~ , j ~  ! & (do) 
)~ 10000 
1000 
1 O0 1000 10000 100000 1000000 10000000 I \[ = {}S 
#1) and lid (T) 
Figure 3 
Baseline and sample word distribution 
3.3 Treatment of very frequent erms 
So \['ar we have been unable to treat extremely 
frequent terms, such as -~-~ (do). We therefore 
used random sampling to calculalc tile 1@1)(77 LLR) 
of a very li'cquent lerm T. II' the munbcr ot' 
documents in D(7) is larger than a threshold wdue N, 
which was calculated froln the average number of 
words contained in a document, N docnmcnts arc 
randomly chosen from D(2) (we used N = 150). This 
subset is denoted D(T) and Re/)(7; LLR) is delined 
by 100 x (log(LLR(D(7))) /log(BL~,Se (#1)(7))) -- 1). 
This is effcctivc because wc can use a 
well-approximated part of the baseline curve; it also 
reduccs thc amount of calctflation required. 
By using Rel)(77 LLR) detSned above, wc 
obtained Rel)(-'F g), LLR) = -0.573, Rel)(a')&TJ, llk 7~), 
LLR) = 4.08, and , * .... Re\])(llil-o, LLR) = 6.80, which 
reflect our linguistic intuition. 
3.4 Features of Rep(T, M) 
Rep(T, M) has the t bllowing advantages by virtue of 
its definition: 
(1) Its definition is mathematically clear. 
(2) It can compare high-frequency terms with low- 
ficqucncy terms. 
(3) The threshold value of being representative can 
be defined systematically. 
(4) It can be applied to n-gram terms for any n. 
4. Experiments 
4.1 Ewfluation of monograms 
Taldng topic-word selection for a navigation 
window for IR (see Fig. 1) into account, we 
cxamined the relation bctwecn the value of Rel)(7, 
M) and a manual classification of words 
(monograms) extracted from 158,000 articles 
(excluding special-styled non-sentential rticles such 
as company-personnel-aflhir articles) in the 1996 
issties of the Nildcei Shinbun. 
4.1.1 Preparation 
We randolnly chose 20,000 words from 86,000 
words having doculnent ficquencies larger than 2, 
thcn randomly chose 2,000 of them and classified 
these into thrce groups: class a (acceptable) words 
uscfill for the navigation window, class d (delete) 
words not usethl for the navigation window, ,and 
class u (uncertain) words whose usefulness in the 
navigation window was either neulral or difficult to 
judge. In the classification process, a judge used the 
DualNA VI system and examined the informativeness 
of each word as guidance. Classification into class d 
words was done conservatively because the 
consequences of removing informative words from 
lhc window are more serious than those of allowing 
useless words to appear. 
3hblc I shows part of the chtssification of thc 
2,000 words. Words marked "p" arc proper nouns. 
The difference between propcr nouns in class a and 
proper nouns in other classes is that the former arc 
wcllknown. Most words classified as "d" are very 
common verbs (such as-,J-~(do) and {J~s-~(have)), 
adverbs, demonstrative pronouns, conjunctions, and 
numbers. It is thereti)rc impossible to define a 
stop-word list by only using parts-of-spccch bccausc 
ahnost all parts-of speech appear in class d words. 
4.1.2 Measures used in tile experiments 
To evaluate the effectiveness of several lneasures, 
we compared the ability of each measure to gather 
(avoid) representative (non-representative) terms. 
We randomly sorted thc 20,000 words and then 
compared the results with the restllts of sorting by 
other criteria: Rep(., LLR), Rep(., DIFFNUM), (f 
(tern~ liequency), and tfid.fi The comparison was 
done by nsing the accunmlated number of words 
marked by a specified class that appeared in the first 
N (1 _< N_< 2,000) words. The definition we used for 
tj- idf was 
Nlota\[ .t/- ira= 4771775 ?log N(r ' 
where T is a term, TF(7) is the term frequency of 7, 
Nt,,,<,l is the number of total documents, and N(7) is 
the number of documents that contain 7: 
4.1.3 Results 
Figure 4 compares, for all the sorting criteria, tile 
323 
accumulated number of words marked "a". The total 
number of class a words was 911. Rep( o, LLR) 
clearly outperformed the other measures. Although 
Rep(., DIFFNUM) outperformed .tfand tf-idf up to 
about the first 9,000 monograms, it otherwise 
under-performed them. If we use the threslaold value 
of Rep(., LLR), from the first word to the 1,511th 
word is considered representative. In this case, the 
recall and precision of the 1,511 words against all 
class a words were 85% and 50%, respectively. 
When using tf-idf the recall and precision of the 
first 1,511 words against all class a words were 79% 
and 47%, respectively (note that tJ'-idfdoes not have 
a clear threshold value, though). 
Although the degree of out-performance by
Rep(., LLR) is not seemingly large, this is a 
promising result because it has been pointed out that, 
in the related domains of term extraction, existing 
measures hardly outperform even the use of 
frequency (for example, Daille et al 1994, Caraballo 
et al 1999) when we use this type of comparison 
based on the accumulated numbers. 
Figure 5 compares, for all the sorting criteria, 
the accumulated number of words marked by d (454 
in total), in this case, fewer the number of words is 
better. The difference is far clearer in this case: 
Rep(., LLR) obviously outperformed the other 
measures. In contrast, tfidJ and frequency barely 
outperformed random sorting. Rep(., DIFFNUM) 
outperformed tfand (f-idfuntil about the first 3,000 
monograms, but under-performed otherwise. 
Figure 6 compares, for all the sorting criteria, 
the accumulated number of words marked ap 
(acceptable proper nouns, 216 in total ). Comparing 
this figure with Fig. 4, we see that the 
out-performance ofRep(., LLR) is more pronounced. 
Also, Rep(., DIFFNUM) globally outperformed tf
and tf-idf while the performance of( land tf-idfwcre 
nearly the same or even worse than with random 
sorting. 
IOOO 
900 
~00 
700 
600 
500 
400 
300 
10 
0 5000 10000 15000 20000 
Order 
? random ? Rep(., LLR) a Rep(., DIFFNUM) ~ t f id f  * tf 
Figure 4 
Sorting results on class a words 
350 
300 
Z 
250 
200 
.< 
150 
100 
~g / 
L 
0 5000 10000 15000 20000 
Order 
? random ~ Rep(., LLR) a Rcp(., DIFFNUM) ~ tt: idf ? tf  
Figure 5 
Sorting results on class d words 
p) 
a~ 150 
Z 
100 
.< 
o j~,,-- 
o 5(1{)0 I0000 15000 20000 
Order 
? random ~ Rep(., LLR) z~ Rep(., I)IFFNUM) ~ tl=id\[" ? tf 
Figure 6 
Sorting results on class ap words 
qhble 1 
Examples of the classified words 
chtss a class u class d 
~" 2 :L ~Y-2"~ 5/ 1..'<-- ~ O'/~s("g) (chilly) )kT'-I'i)J (83,000,000) 
(amusement park) ~'\['J?J2 (depressed) ~)<?2 (greatly) 
g)3~)~ (threlerfingletter) ;~'~'1 t (lshigami) p T-l'flJqM-/-: (1, t46) 
/ '7"4) 'OM- - JP  (fircwall) ~}5',;: (Shigeyuki) p ~J-~<~ (all) 
"\[~l'~t~', (antique) li~?;i,'2:t, '??(misdirected) ~" L L (not... in the least) 
7" \]- ~ ; / / /  (Atlanta) p ~}J(~A~ (agility) 
In the experiments, proper nouns generally 
have a high Rep-value, and some have particularly 
high scores. Proper nouns having particularly high 
scores are, for instance, the names ofsumo wrestlers 
or horses. This is because they appear in articles 
with special formats uch as sports reports. 
We attribute the difference of the performance 
between Rep(., LLR) and RED(., DIFFNUM) to the 
quantity of information used. Obviously information 
on the distribution of words in a document is more 
comprehensive than that on the number of different 
words. This encourages us to try other measures of 
document properties that incorporate ven more 
precise information. 
324 
4.2 Picking out fl'equeni non-representative 
monograms 
When we concentrate on the nlost fi-equent erms, 
Re/)(., DIFFNUM) outperfomlcd Rep(., LLR) in the 
following sense. We marked "clearly 
non-representative terms" in the 2,000 most frequent 
monograms, then counted the number of marked 
terms that were assigned Rt7)-values maller than 
the threshold value of a specified representativeness 
u lcasurc .  
The total number of checked terms was 563, 
and 409 of them are identified as non-representative 
by Rep(', LER). On the other hand, Rep( ?, 
DIFFNUM) identified 453 terms as 
non--representative. 
4.3 Rank correlation between measures 
We investigated the rank-correlation of the sorting 
results for the 20,000 terms used in the experiments 
described in subsection 4.1. Rank correlation was 
measured by Spearman's method and Kendall's 
method (see Appendix) using 2,000 terms randomly 
selected from the 20,000 terms. Table 2 shows the 
correlation between Rep(,, LLR) and other measures. 
It is interesting that the ranking by Rep(., LLR) and 
that by Rep(., DIFFNUM) had a very low 
correlation, even lower than with (f or (fidf This 
indicates that a combination of Rep(., LLR) and 
Rep(,, DIFFNUM) should provide a strong 
discriminative ability in term classification; this 
possibility deserves further investigation. 
Table 2 
Two types of Rank correlation between 
term-rankings byRep(., LLR) and other measures. 
Rep(., DIFFNUM) t/=ic(f tf 
Spearman -0.00792 0.202 0.198 
Kenda l l  -0 .0646 0.161 0.153 
4.4 Portability of baseline functions 
We examined the robustness of thc baseline 
fimctions; that is, whether a baseline function 
defined from a corpus can be used for normalization 
in a different corpus. This was investigated by using 
Re/)(., LLR) with seven different corpora. Seven 
baseline functions were defined from seven corpora, 
then were used for normalization for defining Rep(., 
LLR) in the corpus used in the experiments 
described in subesction 4.1. The per%rmance of the 
Re/)(,, LLR)s defined using the difl'erent baseline 
flmctions was compared in the same way as in the 
snbsection 4. l. The seven corpora used to construct 
baseline fhnctions were as follows: 
NK96-ORG: 15,8000 articles used in the experiments in 4.1 
NK96-50000:50,000 randomly selected articles from Ihe whole 
corpus N K96 (206,803 articles of Nikkei-shinhun 1996) 
N K96-100000: I 0(},000 randomly selected articles fn}m N K96 
NK96-200000: 2{}0,00(} randomly selcctcd articles fiom NK96 
NK98-1580{}0:158,0{}(} randomly selecled articles from articles in 
Nikkei-xhinhun 1998 
N('- 158000:158,{}00 randomly selected abstracts of academic papers 
I\]'Olll NACSIS corptl:.; (Kando ct al. 1999) 
NC-:\LI.: all abstracts (333,003 abstracts) in the NACSIS coq)us. 
Statistics on their content words are shown in Table 3. 
Table 3 
Corpora and statistics on their content words 
~ ~ .  NK96-OP, G NK96-soooo NKq6-1ooooo NK96-2ooooo 
fi o | ' Iota l  words  42,555,095 13,49S,244 26 ,934,068 53 .816,407 
;: ofdillbrent words 210,572 127,852 172.914 233,668 
~ ~  NK98-158000 NC-158000 NC-A I . I .  
# ,af total v,'ords 39,762, 127 30,770,682 64,806,627 
# of difliarent words 196,261 231,769 350.991 
Figure 7 compares, for all the baseline functions, the 
accumulated number of words marked "a" (see 
subsection 4.1). The pertbrmancc decreased only 
slightly when the baseline defned from NC-ALL 
was used. In other cases, the difl'erences was so 
small that they were almost invisible ill Fig. 7. The 
same results were obtained when using class d 
words and class ap words. 
tuoo 
9OO 
700 
-j 
5OO 
0 2000 40011 (dRRI XOOH I UO(lO 12tRR} 14000 160110 IROOt} 21111{11/ 
Order 
* random ~ NK96-OR( i  A NK96-5t}000 - NK96-100000 
c\] NK96-20{}000 * NK98-158{}(1{} + NC-158000 x NC-ALL  
Figure 7 
Sorting results on class a words 
We also examined the rank correlations 
between the ranking that resulted from each 
representativeness measure in the same way as 
described in subsection 4.2 (see Table 4). They were 
close to 100% except when combining the Kendall's 
method and NACSIS corpus baselines. 
Table 4 
Rank correlation between the measure defined by an 
NK96-ORG baseline and ones defined by other baselines 
(%) 
NK96-  NK96-  NK96-  NK9g-  
"~C- 1 5800C NC-A I . I .  500{}0 I.OOOO 2000{}0 158000 
Spcarmann 0.997 0.997 0.996 0.999 0.912 0.900 
Kendall 0.970 0.956 0.951 0.979 0.789 0.780 
These resnhs suggest hat a baseline function 
constructed from a corpus can be used to rank terms 
in considerably different corpora. This is particularly 
useful when we are dealing with a corpus silnilar to 
a known corpus  but  do  not  know the  precise word 
distributions in the corpus. The same tdnd of 
robustness was observed when we used Re/)(", 
325 
DIFFNUM). This baseline thnction robustness i  an 
important tbature of  measures defined using the 
baseline based. 
5. Conclusion and future works 
We have developed a better method -- the baseline 
method -- for defining the representativeness of  a 
term. A characteristic value of all docmnents 
containing a term T, D(T), is normalized by using a 
baseline function that estimates the characteristic 
value of  a randomly chosen doculnent set of  the 
same size as D(?). The normalized value is used to 
measure the representativeness of  the term T, and a 
measure defined by the baseline method offers 
several advantages compared to classical measures: 
(1) its definition is mathematically simple and clean 
(2) it can compare high-frequency terms with 
low-frequency terms, (3) the threshold value for 
being representative can be defined systcmatically, 
and (4) it can be applied to n-gram terms for any n. 
We developed two measures: one based on 
the normalized distance between two word 
distributions (Rep(., LLR)) and another based on 
the number of  different words in a document set 
(Rep( o, DIFFNUM)).  We compared these measures 
with two classical measures from various viewpoints, 
and confirmed that Rep(,, LLR) was superior. 
Experiments showed that the newly developed 
measures were particularly eflizctive for discarding 
frequent but uninformative terms. We can expect 
that these measures can be used for automated 
construction of  a stop-word list and improvement of  
similarity calculation of  documents. 
An important finding was that the baseline 
function is portable; that is, one defined on a corpus 
can be used for laormalization in a diflbrent corpus 
even if the two corpora have considerably diftbrent 
sizes or are in different domains. Wc can therefore 
apply the measures in a practical application when 
dealing with multiple similar corpora whose word 
distribution information is not fully known but we 
have the inforlnation on one particular corpus. 
We plan to apply Rep(., LLR) and Rep(., 
DIFFNUM) to several tasks in IR domain, such as 
the construction of  a stop-word list for indexing and 
term weighting in document-similarity calculation. 
It will also be interesting to theoretically 
estimate the baseline functions by using 
fundalnental parameters such as the total numbcr of  
words in a corpus or the total different number in the 
corpus. The natures of  the baseline functions 
deserve further study. 
Acknowledgements 
This project is supported in part by the Advanced 
Software Technology Project under the auspices of  
Information-technology Promotion Agency, Japan 
(IPA). 
References 
Caraballo, S. A. and Charniak, E. (1999). Determining 
the specificity of nouns fronl text. Prec. of EMNLP'99, 
pp. 63-70. 
Church, K. W. and Itanks, P. (1990). Word Association 
Norms, Mutual hlformation, and Lexicography, 
Conq)utational Linguistics 6( 1 ), pp.22-29. 
Daille, B. and Gaussiel; E., and Lange, J. (1994). Towards 
automatic extraction of monolingual nd bilingual 
terminology. Prec. of COL1NG'94, pp. 515-521. 
Dunning, T. (1993). Accurate Method for the Statistics of 
Surprise and Coincidence, Computational Linguistics 
19(1), pp.61-74. 
Firth, J. A synopsis ot' linguistic theory 1930- 1955. (t 957). 
Studies in Linguistic Analysix, Philological Society, Oxford. 
Frantzi, K. T., Ananiadou, S., and Tsujii, J. (1996). 
Extracting Terminological Expressions, IPSJ Technical 
Report of SIG NL, NLl12-12, pp.83-88. 
Hisamitsu, 'I:, Niwa, Y., and "l'sttiii, J. (1999). Measuring 
Representativeness of Terms, Prec. oflRAL'99, pp.83-90. 
Kageura, K. and Umino, B. (1996). Methods of automatic term 
recognition: A review. Termino logy  3(2), pp.259-289. 
Kando, N., I:,2uriyanaa, K. and Nozue, T. (1999). NACSIS test 
collection workshop (NTCIR-I), l'roc, of the 22nd Ammal 
hlternational A CM SIGIR Cot!\['. on Research and 
Development i  1R, pp.299-300. 
Kita, Y., Kate, Y., Otomo, 'E, and Yano, Y. (1994). 
Colnparativc Study of Automatic Extraction of Collocations 
fiOln Corpora: Mutual nlbrmation vs. Cost Criteria, Journal 
of Natural Language Processing, 1( 1 ), 21-33. 
Nagao, M., Mizutani, M., and lkeda, H. (1976). An Automated 
Method of the Extraction of hnportant Words fiom Japanese 
Scientific l)ocuments, Trans. oJIPSJ, 17(2), pp. 110-117. 
Nakagawa, H. and Mori, T. (1998). Nested Collocation and 
Compound Noun For Term Extraction, Prec. c( 
Computernt '98, pp.64-70 
Nishioka, S., Niwa, Y., lwayama, M., and Takano, A. (1997). 
DualNA VI: An intbrmation retrieval interface. Prec. o j
WISS'97, pp.43-48. (in Japanese) 
Niwa, Y., Nishioka, S., Iwayama, M., and Takano, A. (1997). 
'lbpic graph generalion lbr query navigation: UTse of 
fiequency classes lbr topic extraction. Prec. c?fNLPRS'97, 
pp.95-100. 
Norcault, q'., McGill, M., and Koll, M. B. (1977). A 
Pertbrmance Evaluation of Similarity Measure, Document 
Telill Weighting Schemes and Representation in a Boolean 
Environment. In Oddey, R. N. (ed.), Iq \ [brmal ion  Retrieval 
Resemz:h. London: Butterworths, pp.57-76. 
Salton, G. and Yang, C. S. (1973). On the Specification of Term 
Values in Automatic Indexing. Journal of Documentation 
29(4), pp.351-372. 
Sparck-Jones, K. (1973). Index Term Weighting. h(/brmation 
Storage and Retrieval 9(11), pp.616-633. 
Tcramoto, Y., Miyahara, Y., and Matsumoto, S.(1999). 
Word weight calculation for document retrieval by analyzing 
the distribution of co-occurrence words, Prec. of the 59th 
Ammal Meeting of lPS.l, 1P-06. (in Japanese) 
Appendix 
Asusume that items I1 ..... IN are ranked by measures A and B, 
and that the rank of item/: assigncd by A (B) is RiO" ) (R~(j)), 
where RA(i ) eRA( j )  (Rl4(i) ?Ri~(j)) if i ~j. Then, Spearman's rank 
correlation between the two rankings is given as 
t 6x~j(R4(i)-R"(i))2 
N(N ~ - 1) 
and Kendal l ' s  rank correlat ion between the two rank ings  is 
given as 
I ? ({# {(i, j) I  c~(&.,(i) - R A ( j ) )  = cr(Rz,(i ) - RB(j ) )}-  
N C2 
#{(i,./) l cr(R.4(i) - R.I(J)) = -cr(R~(i)  - Re(./))}) , 
where c~ (x)=l ifx > 0, clse ifx < 0, c~ (x) = -I. 
326 
Lex ica l i zed  H idden Markov  Mode ls  for  Par t -o f -Speech  Tagg ing  
Sang-Zoo  Lee  and Jun - i ch i  Tsu j i i  
Del)artInent of Infor lnation Science 
Graduate  School of Scien(:e 
University of Tokyo, Hongo 7-3-1 
Bunkyo-ku, Tokyo 113, Ja, l)3,iI 
{lee,tsujii} ((~is.s.u-tokyo.ac.jp 
Hae-Chang R im 
Det)artment of Computer  Science 
Korea Ulfiversity 
i 5-Ca Anam-Dong,  Seongbuk-C~u 
Seoul 136-701, Korea 
rim~)nll).korea.ac.kr 
Abst rac t  
Since most previous works tbr HMM-1)ased tag- 
ging consider only part-ofsl)eech intbrmation in 
contexts, their models (:minor utilize lexical in- 
forlnatiol~ which is crucial tbr resolving some 
morphological tmfl)iguity. In this paper we in- 
troduce mliformly lexicalized HMMs fin: i)art -
ofst)eech tagging in 1)oth English and \](ore, an. 
The lexicalized models use a simplified back-off 
smoothing technique to overcome data Sl)arse- 
hess. In experiment;s, lexi(:alized models a(:hieve 
higher accuracy than non-lexicifliz(~d models 
and the l)ack-off smoothing metho(l mitigates 
data sparseness 1)etter (;ban simple smoothing 
methods. 
1 I n t roduct ion  
1)arl;-Ofsl)e(:('h(POS) tagging is a l)ro(:ess ill 
which a l)rOl)('.r \])()S l;ag is assigned to ea(:h wor(l 
in raw tex(;s. Ev('n though morl)h()logi(:ally am- 
l)iguous words have more thnn one P()S tag, 
they l)elong to just one tag in a colll;ex(;. 'J~o 
resolve such ambiguity, taggers lmve to consult 
various som'ces of inibrmation such as lexica\] 
i)retbrences (e.g. without consulting context, 
table is more probably a n(mn than a. ver}) or 
an adje(:t;ive), tag n-gram context;s (e.g. after a 
non-1)ossessiv(: pronoun, table is more l)robal)ly 
a verb than a. nmm or an adjective., as in th, ey ta- 
ble an amendment), word n-grain conl;e.xl;s (e.g. 
betbre lamp, table is more probal)ly an adjective 
than ~ noun or ~ verb, as in I need a table lamp), 
and so on(Lee et al, 1.999). 
However, most previous HMM-1)ased tag- 
gers consider only POS intbrmation in con- 
texts, and so they C~I~IlII()t capture lexical infi)r- 
nmtion which is necessary for resolving some 
mort)hological alnbiguity. Some recent, works 
lmve rel)orted thai; tagging a('curacy could 
l)e iml)roved 1)y using lexicM intbrnml;ion in 
their models such as the transtbrmation-based 
patch rules(Brill, 1994), the ln~txinnun entropy 
model(lIatn~q)arkhi, 1996), the statistical ex- 
ical ruh:s(Lee et al, 1999), the IIMM consid- 
ering multi-words(Kim, 1996), the selectively 
lexicalized HMM(Kim et al, 1999), and so on. 
In the l)revious works(Kim, 1996)(Kim et al, 
1999), however, their ItMMs were lexicalized se- 
h:ctively and resl;rictively. 
\]n this l>al)er w('. prol)ose a method of uni- 
formly lcxicalizing the standard IIMM for part- 
o f  speech tagging in both English and Korean. 
Because the slmrse-da.ta problem is more seri- 
ous in lexicMized models ttl~ll ill the standard 
model, a simplified version of the well-known 
back-oil' smoothing nml;hod is used to overcome 
the. 1)rol)lem. For experiments, the Brown cor- 
pus(Francis, 1982) is used lbr English tagging 
and the KUNLP (:orlms(Lee ('t al., 1999) is 
used for Kore, an tagging. Tim eXl)criln(;nl;~t\] re- 
sults show that lexicalized models l)erform bet- 
ter than non-lexicalized models and the simpli- 
fied back-off smoothing technique can mitigate 
data sparseness betl;er than silnple smoothing 
techniques. 
2 Ti le "s tandard"  HMM 
We basically follow the not~ti(m of (Charniak 
et al, 1993) to describe Bayesian models. In 
this paper, we assume that {w I , 'w~,..., w ~0 } is 
a set of words, {t t , t '2 , . . . , t ;}  is a set of POS 
tags, a sequence of random variables l'lq,,~ = 
l~q lazy... I'E~ is a sentence of n words, and a 
sequence of random w~riables T1,,, = 7~T,2... TT~ 
is a sequence of n POS tags. Because each of 
random wtrbflfles W can take as its value any 
of the words in the vocabulary, we denote the 
value of l'l(i by wi mM a lmrticular sequence of 
wflues tbr H~,j (i < j) by wi, j. In a similar wl.ty, 
we denote the value of Ti by l,i and a particular 
481 
sequence of values for T/,j (i _< j) t)y ti,j. For 
generality, terms wi,j and ti,j (i > j) are defined 
as being empty. 
Tile purpose of Bayesian models for POS tag- 
ging is to find the most likely sequence of POS 
tags for a given sequence of' words, as follows: 
= arg lnaxPr (T , ,n  =- I W,,,, = w,, ,d 
tl,n 
Because l'efhrence to the random variables 
thelnselves can 1)e oulitted, the above equation 
b eco lnes :  
T('wl,n) = argmax Pr(tl,n \[ wl,,z) (1) 
~'l,~t 
Now, Eqn. 1 is transtbrnled into Eqn. 2 since 
Pr(wl,n) is constant for all tq,~, 
Pr (l.j ,n, wl,n) 
T(*/q,n) -- argmax 
t ,  .... Pr('wl,n) 
= arDnaxP,'(tj,,~,w,,,,) (2) 
tl ,n 
Then, tile prolmbility Pr(tL,z, wl,n ) is broken 
down into Eqn. 3 by using tile chain rule. 
fl(Pr(ti,t\],i-l,Wl,i-1) ) 
Pr(tl,n,~q,r,,) = x Pr(/~i \[tl,i,~Vl,i-l) (3) 
i= l  
Because it is difficult to compute Eqn. 3, the 
standard ItMM simplified it t)3; making a strict 
Markov assumption to get a more tract~d)le 
tbrm. 
Pr(tl,,,, Wl,n) ~ x Pr(wi I td (4) 
i= l  
I51 the standard HMM, the probability of the 
current tag ti depends oi5 only the previous K 
tags ti-K,i-1 and the t)robability of' the cur- 
rent word wi depends on only the current ag 1. 
Thereibre, this model cannot consider lexical in- 
formation in contexts. 
3 Lex ica l i zed  HMMs 
In English POS tagging, the tagging unit is a 
word. On the contrary, Korean POS tagging 
prefers a morpheme 2. 
1Usually, K is determined as1 (bigram as in (Char- 
niak et al, 1993)) or 2 (trigram as in (Merialdo, 1991)). 
2The main reason is that the mtmber of word-unit 
tags is not finite because I(orean words can be ti'eely 
and newly formed l)y agglutinating morphemes(Lee t 
al., 1999). 
, / ,  
Flies/NNS Flies/VBZ 
like/CS like/IN like/JJ like/VB 
a/A~ a/IN a/NN 
ttower/NN flower/VB 
. / .  
$/$ 
Figure 1: A word-unit lattice ot' "Flies like a 
\ [ l ower  ." 
Figure 1 shows a word-unit lattice of an Eil- 
glish sentence, "Flies like a flowc'r.", where each 
node has a word and its word-unit tag. Fig- 
ure 2 shows a morpheme-unit lattice of a Ko- 
rean sentence, "NcoNeun tIal Su issDa.", where 
each node has a morphenm and its morI)heme- 
unit tag. In case of Korean, transitions across 
a word boundary, which are depicted by a solid 
line, are distinguished fl'om transitions within a 
word, which are depicted by a dotted line. ill 
both cases, sequences connected by bold lines 
indicate the most likely sequences. 
3.1 Word-un i t  mode ls  
Lexicalized HMMs fbr word-unit agging are de- 
fined 1)y making a less strict Markov assmnp- 
tion, as tbllows: 
A(T(K,j), W( I ; j ) )~  Pr(tl,,~,wl,n) 
i=\] x Pr(wi I ti-L,i, wi-I , i -1) 
Ill models A(T(K,j), 14/(L j)) ,  the probability of 
the current tag ti depends on both tile previ- 
ous I f  tags t i -K, i - i  and the previous d words 
wi- j , i - i  and the probability of the current word 
'wi depends on the current ag and the previous 
L tags ti_L, i and the previous I words wi-l , i -~. 
So, they can consider lexieal inforination. In ex- 
periments, we set I f  as 1 or 2, J as 0 or K, L as 
1 or 2, and 1 as 0 or L. If J and I are zero, the 
above models are non-lexicalized models. Oth- 
erwise, they are lexicalized models. 
482 
$/, 
Neo/N NI" Ncol/VV 
?. 4 
No'an~ P X Ncun/EFD 
H~d/NNCC Hd/NNBU H~(VV \ ] Ia /VX 
S'a/NNCG Su/NNBG 
iss/\zJ iss/VX 
Da/EFF Da/EFC 
?"'OOoo,,,j~g_._.--"- 
./ss. 
$/$ 
Figure 2: A morl)heme-unit latti(:(; of "N,oN,'un 
llal S'u i.ssl)a." (= You (:an do it.) 
r l  f in a lexicalized model A(~/(9,2), lI ('J,2)), fin" ex- 
mnl)lc , the t)robal)ility of a node "a/AT" of tlm 
most likely sequen(:e in Figure 1 is calculate(t as 
tbllows: 
l'r(AT' I NM& vIL Fli(:,~, lit,:c) 
? tq  ? x Pr(a t :'1~, NNS,  VH, 1 l'~,c.s, lil,:c) 
3.2  Morphe lne-un i t  mode ls  
l);~yesian models for lnOrl)heme-unit tagging 
tin(t the most likely se(lueame of mor\])h(mms 
and corresponding tags fi)r ;~ given sequence of 
words, as follows: 
~'(11) ,1,,) = al'glll;XX Pr (c  l,v,, ?/~,,,u I '1,,,,~) (6) 
Cl~u flltl,,t 
, ra-ax Pr(c,,,,, m,,. ' ,,,,, ,,,) (7) 
Cl,~tllt~l,u 
In the above equations, u(_> 'n) denotes the 
llllIlll)cr of morph(mms in a Se(ltlell(;e ('orre- 
spending the given word sequ('ncc, c denotes 
a morl)heme-mfit tag, 'm. denotes a morl)heme , 
aim p denotes a type of transition froln the pre- 
v ious  tag to the current ag. p can have one of 
two values, "#" denoting a transition across a 
word bomldary and "+" denoting a transition 
within a word. Be(-ause it is difficult to calculate 
Eqn. 6, the word sequence term 'w~,,, is usually 
ignored as ill Eqn. 7. Instead, we introduce p in 
Eqn. 7 to consider word-spacing 3. 
Tile probability Pr(cj ,~L, P2,u, 'm,~ ,u) is also bro- 
ken down into Eqn. 8 t)3r using the chain rule. 
Pr(c~ ,,,, P2,,, , 'm, , ,,,,) 
f l  ( \])r(ci,Pi \[ cl,i-l,P2,i-l,'lnl,i-l) ) 
~- X P1"(1~'1,i \[('d,i,I,2,i,17tl,i_\]) (8) i=1 
\]3('caus(' Eqn. 8 is not easy to (;omlmte ~it is 
sinll)lified by making a Marker assmnt)tion to 
get; a more tractal)le forlll. 
In a similar way to the case of word-unit; tag- 
ging, lexicalize(t HMMs for morl)heme-mfit tag- 
ging are defined by making a less strict Markov 
assunq)tion, as tblh)ws: 
A(C\[,q(K,.\]), AJ\[sI(L,1)) 1= Pr(c\],,,,p2,,,, 'mq,~,) 
I'r(c \[,pd I ,,I,i-,Uc/--lC/-' (!)) 
~=~, x l ' r (mi l c i  l,,i\[,>-L+l,,i\],'mi-l,i--I) 
In models A(C\[.q(tc,,I),M\[q(L,Q), the 1)robal)il- 
ity of the (:urrent mori)heme tag ci depends 
on l)oth the 1)revious K |:ags Ci_K,i_ 1 (oi)tion- 
ally, th(' tyl)eS of their transition Pi-K~ 1,i-~) 
a.n(l the 1)revious ,\] morl)hemes H~,i_.l,i_ 1 all(1 
the probability of the current mort)heine 'm,i (t(> 
1)en(ls on the current, tag and I:he previous L 
tags % l,,i (optional\]y, the typ('~s of their tran- 
sition Pi -L-t-I,i) and the 1)revious I morl)hemes 
?lti--l,i-1. ~()~ t\]l(ly ('&ll &lSO (-onsid(,r h;xi(-al in- 
formation. 
In a lexicalized model A(C,.(~#), M(~,2)) whea:e 
word-spa(:ing is considered only in the tag prob- 
al)ilities, for example, the 1)rol)al)ility of a nod(; 
"S'u/NNBG" of the most likely sequence in Fig- 
urc 2 is calculated as follows: 
Pr(NNBG, # \[ Vl4 EFD, +, Ha, l) 
x Pr(gu \[ VV, EFD, NNBG,  Ha, l) 
3.3  Parameter  es t imat ion  
In supervised lcarning~ the simpliest parameter 
estimation is the maximum likelihood(ML) cs- 
t imation(Duda et al, 1973) which lnaximizes 
the i)robal)ility ot! a training set. The ML esti- 
mate of tag (K+l ) -gram i)robal)ility, PrML (f;i \[ 
t,i-K,i-i), is calculated as follows: 
P Pr(ti l ti_ir,i_j) __ \]: q ( t i - i ( , i )  (10) 
ML Fq(ti-lGi-l) 
aMost 1)rcvious HMM-bascd Korean taggcrs except 
(Kim et al, 1998) did not consider word-spacing. 
483 
where the flmction Fq(x) returns the fl:equency 
of x in the training set. When using the max- 
imum likelihood estimation, data sparseness i
more serious in lexicalized models than in non- 
lexicalized models because the former has even 
more parameters than the latter. 
In (Chen, 1996), where various smoothing 
techniques was tested for a language model 
by using the perplexity measure, a back-off 
smoothing(Katz, 1987) is said to perform bet- 
ter on a small traning set than other methods. 
In the back-off smoothing, the smoothed prob- 
ability of tag (K+l ) -gram PrsBo(ti \[ ti-l~,i-l) 
is calculated as tbllows: 
Pr (ti \[ ti-I(,i-~) = 
,5'1~20 
drPrML(ti \[ti-I(,i-1) " if r>0 (11) 
c~(ti-K,i-1) Prsso(ti \[ ti-K+l,i-l)if r = 0 
where r = Fq(ti_t(,i), r* = ( r+ 1)'nr+l 
7~, r
r* (r+l.) x~%.+l 
dr  ~ F l t l  
1-  (r+l)xm.+l 
n l  
n,. denotes the nmnber of (K+l ) -gram whose 
frequency is r, and the coefficient dr is called 
the discount ratio, which reflects the Good- 
~lhtring estimate(Good, 1953) 4. Eqn. 11 means 
that Prxgo(ti \[ ti-K,i-l) is under-etimated by 
dr than its maximum likelihood estimate, if 
r > 0, or is backed off by its smoothing term 
Prsuo(ti \[ ti-K+j,i-l) in proportion to the 
value of the flmction (~(ti-K,i-t) of its condi- 
tional term ti-K,i-1, if r = 0. 
However, because Eqn. 11 requires compli- 
cated computation in ~(ti-l(,i-1), we simI)lify 
it to get a flmction of the frequency of a condi- 
tional term, as tbllows: 
ct(Fq(ti-K,i-1) = f) = 
E\[Fq(ti-I(,i-1) = f\] Ax E7-o E\[Fq(ti-K,i-1) -= f\] 
where A = 1 - ~ Pr (tglti-/c,i-,), 
SBO ti--K,i~r>O 
E\[Fq(ti-g,i-1) = f\] = 
SP\]to ( ti \[ti-K + l,i-1) 
t i -  K + L i,r=O,F q( t i -  K, i -1)= f ' 
(12) 
In Eqn. 12, the range of .f is bucketed into 7 
4Katz  said that  d,. = i if r > 5. 
regions such as f = 0, 1, 2, 3, 4, 5 and f > 6 since 
it is also difficult to compute this equation tbr 
all possible values of f .  
Using the formalism of our simplified back-off 
smoothing, each of probabilities whose ML es- 
timate is zero is backed off by its corresponding 
smoothing term. In experiments, the smooth- 
ing terms of Prsl~o(ti \[ ti-K,i-l,~t)i-,l,i-l) are  
determined as follows: 
PI'sBo(ti\[ ti-Ii+l,i-h )if K> 1,d> 1 
wi_j+~,i_~ 
Prsuo(ti i fK  >_ 1, d = 1 
Prs13o(ti \[ ti-K+Li-l) if K > 1, J = 0 
PrAD(ti) if K = 0, J = 0 
Also, the snloothing terms of' Pl's\]~o(wi 
ti_L,i, Wi_l,i_ 1 ) are determined as follows: 
\[ Prst~o(wi 
Prsuo  (wi 
Prs,o (wi 
PrsBO(Wi) 
PrA.O i) 
ti-L+~,i, ) if L _> 1, I>  1 
i l ) i - I+ l  , i - I  
ti-L,i) if L _> 1, I = 1 
ti-L+Li) if L >_ 1, I = 0 
i f L  = 0, I --= 0 
i l L  = -1 ,  I = 0 
In Eqn. 13 and 14, the smoothing term of a 
unigram probability is calculated by using an 
additive smoothing with 5 = 10 .2 which is cho- 
sen through experiments. The equation for the 
additive smoothing(Chen, 1996) is as tbllows: 
Fq(ti-t(,i) + 5 
AD ~tl (Fq(ti-lf,i) + 5) 
In a similar way, the smoothing terms of param- 
eters in Eqn. 9 ~re determined. 
3.4 Model  decoding 
h'om the viewpoint of the lattice structure, the 
t)roblem of POS tagging can be regarded as the 
problem of finding the most likely path ti'om the 
start node ($/$) to the end node ($/$). The 
Viterbi search algorithm(Forney, 1973), which 
has been used for HMM decoding, can be effec- 
tively applied to this task just with slight mod- 
ification 5. 
4 Exper iments  
4.1 Environment 
In experiments, the Brown corpus is used tbr 
English POS tagging and the KUNLP corpus 
'%uch modification is explained in detail in (Lee, 
1999). 
(13) 
(14) 
484 
NW 1,113,189 
NS 
NT  
DA 
RUA 
Brown KUNLP 
167,115 
53,885 15,211 
82 65 
1.64 3.4:1 
61.54% 26.72% 
NW Number of words. NS Number of sen- 
tcnccs. NT Numl){'.r of tags (nlorpheme-unit 
tag for KUNLP). DA Degree of mnbiguity 
(i.e. the number of tags per word). RUA 
1\].atio f mlanlbiguous words. 
Table 1: Intbrmat ion al)out the Brown eortms 
and the KUNLP tort}us 
Inside-test ()utside-|;(;st 
ML 95.57 94 .97  
= 1) 
-AD(a - \](}- \]) 
AD(~ = 1{}2T)  - 
ADO; - -  =a) 
A\]) ( ( ;  = 
AD(5  = 
AD(5 = 
AD(5 = \]\]}-~7)- 
AD(5 = 
93.92 93.02 
95.02 94.79 
95.42 95.08 
95.55 95.05 
95.57 94.98 
95.57 94.94 :  
95.57 94.91 
95.57 94.89 
95.57 94.87 
SBO 95.55 95.25 
ML Maximum likelihood estimate (with sim- 
ple smoothing). A\]) Additiv(~ smoothing. 
SBO Sinll}liticd 1)ack-off smootlfing. 
lal)l(, 2: lagging accura(:y (}f A(C(\]:o), M0}:0 )) 
for Kore~m POS tagging. Table 1 shows some 
intbrmation M)out 1}oth (:ori)ora {~. Each of them 
was segmented into two parts, the training set 
of 90% and the test; set of 10%, ill. the way that  
each sentence in the test set was extra{'tc, d \]i'()ln 
every 1(} senl;ellce. A(:cording to Tabl(! 1, Ko- 
reml is said to 1)e lllOre (litli(:ult to disambiguat(; 
tl\]ml English. 
We assmne "closed" wmabulary for English 
and "open" vocabulary for Korean since we do 
not h~ve any Engl ish morphological  mmlyzer 
consistent with the Brown corlms. Therefore, 
for morphological mmlysis of English, we just 
aNote that some sentcnc('.s, which have coml}osite 
tags(such as "HV+TO" in "hafta") ,  "ILLEGAL" tag, 
or "NIL" tag~ were remov(M fronl the Brown corl)us and 
tags with "*" (not) such as "BEZ*" were r(',l)la(:(~(t 1)y (:of 
r{~st}o\]ttling ta s without "*" such as "BEZ". 
2M 
1.5M 
IM 
(}.5M 
I I I I I 
- ML  
AD .x .  - 
SBO 
1,02 ,01 ,02 ,01 ,02 ,0  
{},(} 0 ,01  ,(} 1 ,02 , (}  2 ,0  
\ ] '  - - I  I \[ I I 
.99 
.98 
.97 _ I~L~ ~_? 
1,02 ,01 ,023} 13} 2,{1 
(},0 {},{} 1 ,01 ,1}  2 ,02 ,0  
.98 
.97 
2)6 
.(,):, ?vii, -r J--  
AD '?- - 
.:)4 SBO -~--- 
1,02,01,02,(11,02,0 
o,00,01,01,02,02,0 
1. 
.99 
.98 
\[ 
( 
.97 
.96 
I I I I I i t ~  
I I I I I t t } I I I I I 
1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,01,01,12,0 1,1 1,1 0,01,0 1,12,0 2,2 2,2 2,2 2,2 
(a) # of paraln{;ters 
M\], -D- 
AD -?- - - 
SB( )  
I I I I I I I I I I I I I 
1 , 1 1, l 1,0 1,1 2,01,1 2,22,22,22,2 1 ,(} 2,01,12,2 
0,01 0 1,12,01,11,10,01,01 l 2,02,22,22,22,2 
(1)} Inside-test 
1,11,I 1,01,12,01,I 2,22,22,22,21,02,01,12,2 
0 01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(c) Ouiside-test 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,0 1,02,02,0 0,01,01,l  2,01,11,10,(11,{11,I 2,02,22,22,22,2 
(d) inside vs. outside-test in SBO 
Figure 3: Results of English tagging 
485 
looked up the dictionary tailored to the Brown 
corpus. In case of Korean, we have used a Ko- 
rean morphological analyzer(Lee, 1999) which 
is consistent with the KUNLP corpus. 
4.2  Resu l t s  and  eva luat ion  
Table 2 shows the tagging accuracy of the sim- 
plest HMM, A(C(l:0),M(0:0)), for Korean tag- 
ging, according to various smoothing meth- 
ods 7. Note that ML denotes a simple smooth- 
ing method where ML estimates with prob- 
ability less than 10 -9  a re  smoothed and re- 
placed by 10-9? Because, in the outside-test, 
AD(d = 10 -2) performs better than ML and 
kD(a ? 10-2), we use 5 = 10 -2 in our ad- 
ditive smoothing. According to Table 2, SBO 
I)ertbrms well even in the simplest HMM. 
Figure 3 illustrates 4 graphs'about the results 
of English tagging: (a) the number of param- 
eters in each model, (b) the accuracy of each 
model tbr the training set, (c) the accuracy of 
each model for the test set, and (d) the accuracy 
of each model with SBO tbr both training and 
test set. Here, labels in x-axis sI)ecify models 
K,  ,1 in the way that ~ denotes A(T(\];,j) , W(Lj)). 
Therefore, the first 6 models are non-lexicalized 
models and tile others are lexicalized models. 
Actually, SBO uses more parameters than 
others. The three smoothing methods, ML, 
AD, SBO, perform well for the training set; 
since the inside-tests usually have little data 
sparseness. On the other hand, tbr the un- 
seen test set, the simple methods, ML and 
AD, cannot mitigate the data sparseness prob- 
lem, especially in sophisticated models. How- 
ever, our method SBO can overcome the prob- 
lem, as shown in Figure 3(c). Also, we can 
see in Figure 3(d) that some lexicalized mod- 
els achieve higher accuracy than non-lexicalized 
models. We can say that the best lexicalized 
model, A(T(1,~),W(1,1)) using SBO, improved 
the simple bigram model, A(T(L0),W(0,0)) us- 
? ~ 0 mg SBO, from 97.19>/o to 97.87~ (the error re- 
duction ratio of 24.20%). Interestingly, some 
lexicalized models (such as A(T(1,1), W-(0,0)) and 
A(T(1,1), W(1,o))), which have a relatively small 
number of paranmters, perform better than 
non-lexicalized models in the case of outside- 
tests using SBO. Untbrtunately, we cannot ex- 
r Ins ide - tes t  means  an  exper iment  on  the  t ra in ing  set  
i t se l f  and  outs ide - tes t  an  exper iment  on  the  tes t  se t .  
.96 
.94 ~ ?  . .~  uu . X ? "" " ~ '  " .~1%~ ~ 
.92 
.90 
ML ~ k 
.88 AD .x. - 
SBO 
.86 I I I I I I I I I I f I I I I I I I 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02~0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(a) Outside-test 
? 97 I I I I I I ~ I I I d~ I I I t I I -~  
C,M + ? 
.966 C~,/l~/ + 
.9(;2 ~.~, -~I~ X \[\] 
+ 
1,02,01,02,01102,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02,0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(b) Considering word-spacing 
+ 
x 
? 
x 
\ [ \ ]  ? 
? 
I l l l l l l l  
Figure 4: Results of Korean tagging 
pect the result of outside-tests from that of 
inside-tests because there is no direct relation 
t)etween them 
Figm:e 4 includes 2 graphs about the re- 
sults of Korean tagging: (a) the outside ac- 
curacy of each model A(C(K,j),MiL,I)) and 
(b) the outside accnracy of each model 
A(C\[s\](~-g),M\[s\](L,0) with/without considering 
word-spacing when using SBO. Here, labels in 
K,J de-  x-axis specify models in the way that ,7,, 
notes A(C\[s\](K,j),i~/I\[.~\](Lj)) and, tbr example, 
C , ,M in (b) denotes k(C~(,r,j), M(L,r)). 
As shown in Figure 4, the simple meth- 
ods, ML and AD, cannot mitigate that sparse- 
data problem, t)ut our method SBO can over- 
come it. Also, some lexicalized models per- 
tbrm better than non-lexicalized models. On 
the other hand, considering word-spacing ives 
good clues to the models sometimes, but yet 
we cannot sw what is the best ww. From 
the experimental results, we can say that the 
best model, A(C(9,2),M(2,2)) using SBO, im- 
proved the previous models, A(C(1,0), M(o,o)) us- 
486 
ing ML(Lee, 1995), and A(G(,,0), M(0,0))using 
ML(Kim et al, 1998), t'ronl 94.97% and 95.05% 
to 96.98% (the error reduction ratio of 39.95% 
mid 38.99%) respectively. 
5 Conc lus ion  
We have 1)resented unitbrmly lexicalized HMMs 
for POS tagging of English and Korean. In 
the models, data sparseness was etlix:tively mit- 
igated by using our simplified ba(-k-ofl" smooth- 
ing. From the ext)eriments, we have ol)served 
that lexical intbrmation is usefifl fi)r POS tag- 
ging in HMMs, as is in other models, and 
ore" lexicalized models improved non-lexicalized 
models by the error reduction ratio of 24.20% 
(in English tagging) and 39.95% (in Korean tag- 
ging). 
G('.nerally, the mfiform extension of models 
requires ral)id increase of parameters, and hence 
suffers fl'om large storage a.nd sparse data. l~.e- 
cently in many areas where HMMs are used, 
many eflbrts to extend models non-mfitbrmly 
have been made, sometimes resulting in notice- 
able improvement. For this reason~ we are try- 
ing to transfbnn our uniform models into non- 
mliform models, which may 1)e more effective 
in terms of both st)ace (:omt)h'~xity and relial)le 
estimation of I)areme|;ers, without loss of accu- 
racy. 
Re ferences  
12. Brill. 1994. Some Advances in 
~l?ansformation-B ased Part of St)eech 
~Dtgging. In P~ve. of the 12th, Nat'l Cm?. on 
Art'tficial hdelligencc(AAAI-.9~), 722-727. 
E. Charniak, C. Hendrickson, N. Jacobson, and 
M. Perkowitz. 1993. l~3quations for Part- 
o f  Speech %~gging. In Proc, of the 11th, 
Nat'l CoT~:f. on Artificial Intclligence(AAAL 
93), 784-789. 
S. F. Chen. 1996. Building Probabilistic Models 
for Natural Language. Doctoral Dissert~tion, 
Harvard University, USA. 
R. O. Duda and R. E. Hart. 1973. Pattern CIas- 
s'~fication and Scene Analysis. John Wiley. 
G. D. Forney. 1973. The Viterbi Algorithm. Ill 
Proc. of the IEEE, 61:268-278. 
W. N. Francis and H. Ku~era. 1982. Fre- 
quency Analysis of English Usage: Lczicon 
and GTnmmar. Houghton Mitltin Coral)any , 
Boston, Massachusetts. 
I. J. Good. 1953. "The Population Frequen- 
cies of Species and the Estimation of Pop- 
ulation Parameters," Ill Biometrika, 40(3- 
4):237-264. 
S. M. Katz. 1987. Estimation of Probabilities 
fronl Sparse Data for the Language Model 
Component of a Speech Recognizer. In IEEE 
Transactions on Acoustics, Speech, and Signal 
i'rocessing(ASSl'), 35(3):400-401. 
J.-\]). Kim, S.-Z. Lee, and H.-C. Rim. 1998. 
A Morpheme-Unit POS Tagging Model Con- 
sidering Word-Spacing. Ill Pwc. of th.e I0 th 
National CoT~:fercnce on Korean h~:formation 
PTveessing, 3-8. 
J.-D. Kim, S.-Z. Lee, and H.-C. Rim. 1999. 
HMM Specialization with Selective Lexi- 
calization. In Pwe. of the joint SIGDAT 
Co~l:h':rence on Empirical Methods in Nat- 
'aral Language Processing and Very La'qtc 
Co'rpora(EMNLP- VL C-99), ld4-148. 
J.-H. Kim. 1996. Lcxieal Disambig'aation with 
Error-Driven Learning. Doctoral Disserta- 
tion, Korea Advanced Institute of Science and 
Te.clmology(KAIST), Korea. 
S.-H. Lee. 1995. Korean POS Tagging System 
Considering Unknown Words. Master The- 
sis, Korea Advanced Institute of Science and 
Teclmology(KAIST), Korea. 
S.-Z. Lee, .I.-D. Kim, W.-H. Ryu, and H.- 
C. Rim. 1999. A Part-of Speech Tagging 
Model Using Lexical l/.ules Based on Corlms 
Statistics. In Pwc. of the International Con- 
ference on Computer \])'lvcessin 9 of Oriental 
Languages(lCCPOL-99), 385-390. 
S.-Z. Lee. 1999. New Statistical Models for Au- 
tomatic POS Tagging. Doctoral Dissertation, 
l(orea University, Korea. 
B. Merialdo. 1991. Tagging Text with a Prol)- 
abilisl;ic Model. In P~vc. of the International 
Conference on Acoustic, Spccch and Signal 
Processing(ICASSP-91), 809-812. 
A. Ratnap~rkhi. 1996. A Maximum Entrol)y 
Model tbr Part-of-Speech Tagging. In Proe. 
of the Empirical Methods in Natural Lan- 
guage P~vcessi'ng Co'a:fercnce(EMNLP-9b'), 
133-142. 
487 
A Methodology for Terminology-based 
Knowledge Acquisition and Integration 
 
Hideki Mima1
?
, Sophia Ananiadou2, Goran Nenadic2 and Junichi Tsujii1 
 
1Dept. of Information Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{mima, tsujii}@is.s.u-tokyo.ac.jp 
2Computer Science, University of Salford 
Newton Building, Salford M5 4WT, UK 
{S.Ananiadou, G.Nenadic}@salford.ac.uk 
 
                                                
? Current affiliation: Dept. of Engineering, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113- 8656, Japan 
Abstract  
In this paper we propose an integrated knowledge 
management system in which terminology-based 
knowledge acquisition, knowledge integration, 
and XML-based knowledge retrieval are 
combined using tag information and ontology 
management tools. The main objective of the 
system is to facilitate knowledge acquisition 
through query answering against XML-based 
documents in the domain of molecular biology. 
Our system integrates automatic term recognition, 
term variation management, context-based 
automatic term clustering, ontology-based 
inference, and intelligent tag information retrieval. 
Tag-based retrieval is implemented through 
interval operations, which prove to be a powerful 
means for textual mining and knowledge 
acquisition. The aim is to provide efficient access 
to heterogeneous biological textual data and 
databases, enabling users to integrate a wide 
range of textual and non-textual resources 
effortlessly. 
Introduction 
With the recent increasing importance of 
electronic communication and data sharing over 
the Internet, there exist an increasingly growing 
number of publicly accessible knowledge sources, 
both in the form of documents and factual 
databases. These knowledge sources (KSs) are 
intrinsically heterogeneous and dynamic. They 
are heterogeneous since they are autonomously 
developed and maintained by independent 
organizations for different purposes. They are 
dynamic since constantly new information is 
being revised, added and removed. Such an 
heterogeneous and dynamic nature of KSs 
imposes challenges on systems that help users to 
locate and integrate knowledge relevant to their 
needs. 
   Knowledge, encoded in textual documents, is 
organised around sets of specialised (technical) 
terms (e.g. names of proteins, genes, acids). 
Therefore, knowledge acquisition relies heavily 
on the recognition of terms. However, the main 
problems that make term recognition difficult are 
the lack of clear naming conventions and 
terminology variation (cf. Jacquemin and 
Tzoukermann (1999)), especially in the domain 
of molecular biology. Therefore, we need a 
scheme to integrate terminology management as 
a key prerequisite for knowledge acquisition and 
integration. 
   However, automatic term extraction is not the 
ultimate goal itself, since the large number of 
new terms calls for a systematic way to access 
and retrieve the knowledge represented through 
them. Therefore, the extracted terms need to be 
placed in an appropriate framework by 
discovering relations between them, and by 
establishing the links between the terms and 
different factual databases. 
   In order to solve the problem, several 
approaches have been proposed. MeSH Term in 
MEDLINE (2002) and Gene Ontology (2002) 
provide a top-down controlled ontology 
framework, which aims to describe and constrain 
the terminology in the domain of molecular 
biology. On the other hand, automatic term 
acquisition approaches have been developed in 
order to address a dynamic and corpus-driven 
knowledge acquisition methodology (Mima et al, 
1999; 2001a).  
   Different approaches to linking relevant 
resources have also been suggested. The 
Semantic Web framework (Berners-Lee (1998)) 
aims to link relevant Web resources in bottom-up 
manner using the Resource Description 
Framework (RDF) (Bricklet and Guha, 2000) and 
an ontology. However, although the Semantic 
Web framework is powerful to express content of 
resources to be semantically retrieved, some 
manual description is expected using the 
RDF/ontology. Since no solution to the 
well-known difficulties in manual ontology 
development, such as the ontology 
conflictions/mismatches (Visser et al, 1997) is 
provided, an automated ontology management is 
required for the efficient and consistent 
knowledge acquisition and integration. TAMBIS 
(Baker et al, 1998) tried to provide a filter from 
biological information services by building a 
homogenising layer on top of the different 
sources using the classical mediator/wrapper 
architecture. It intended to provide source 
transparency using a mapping from terms placed 
in a conceptual knowledge base of molecular 
biology onto terms in external sources.  
   In this paper we introduce TIMS, an integrated 
knowledge management system in the domain of 
molecular biology, where terminology-based 
knowledge acquisition (KA), knowledge 
integration (KI), and XML-based knowledge 
retrieval are combined using tag information and 
ontology management tools. The management of 
knowledge resources, similarly to the Semantic 
Web, is based on XML, RDF, and 
ontology-based inference. However, our aim is to 
facilitate the KA and KI tasks not only by using 
manually defined resource descriptions, but also 
by exploit ing NLP techniques such as automatic 
term recognition (ATR) and automatic term 
clustering (ATC), which are used for automatic 
and systematic ontology population.  
    
   The paper is organised as follows: in section 1 
we present the overall TIMS architecture and 
briefly describe the components incorporated in 
the system, while section 2 gives the details of the 
proposed method for KA and KI. In the last 
section we present results, evaluation and 
discussion. 
1 TIMS ? system architecture 
XML-based Tag Information Management 
System (TIMS) is a core machinery for managing 
XML tag information obtained from sub 
functional components. Its main aim is to 
facilitate an efficient mechanism for KA and KI 
through a query answering system for 
XML-based documents in the domain of 
molecular biology, by using a tag information 
database.  
   Figure 1 shows the system architecture of 
TIMS. It integrates the following modules via  
XML-based data exchange: JTAG ? an 
annotation tool, ATRACT ? an automatic term 
recognition and clustering workbench, and the 
LiLFeS abstract machine, which we briefly 
describe in this section. ATRACT and LiLFeS 
play a central role in the knowledge acquisition 
process, which includes term recognition, 
ontology population, and ontology-based 
inference. In addition to these modules, TIMS 
implements an XML-data manager and a TIQL 
query processor (see Section 2).  
1.1 JTAG 
JTAG is an XML-based manual annotation and 
resource description aid tool. Its purpose is to 
support manual annotation (e.g. semantic 
tagging), adjusting term recognition results, 
developing RDF logic, etc. In addition, ontology 
information described in XML can also be 
developed and modified using the tool. All the 
annotations can be managed via a GUI.  
1.2 ATRACT 
In the domain of molecular biology, there is an 
increasing amount of new terms that represent 
newly created concepts. Since existing term 
Figure 1: System architecture of TIMS 
 
XML Data 
Retrieval 
TIMS
Tag Information Database  
XML Data 
Management 
ATRACT 
Automatic Term 
Recognition 
and Term 
Clustering 
XML data
XML data
XML data
Document/
Database
Retriever
L iLFeS 
Syntactic and 
Semantic Parser / 
RDF and Ontology 
Manager 
JTAG 
Manual Resource 
Description 
Aid Interface XML data
dictionaries cannot cover the needs of specialists, 
automatic term extraction tools are important for 
consistent term discovery. ATRACT (Mima et al, 
2001a) is a terminology management workbench 
that integrates ATR and ATC. Its main aim is to 
help biologists to gather and manage terminology 
in the domain. The module retrieves and 
classifies terms on the fly and sends the results as 
XML tag information to TIMS.  
   The ATR method is based on the C/NC-value 
method (Frantzi et al, 2000). The original 
method has been augmented with acronym 
acquisition and term variation management 
(Nenadic et al 2002), in order to link different 
terms that denote the same concept. Term 
variation management is based on term 
normalisation as an integral part of the ATR 
process. All orthographic, morphological and 
syntactic term variations and acronym variants (if 
any) are conflated prior to the statistical analysis, 
so that term candidates comprise all variants that 
appear in a corpus. 
   Besides term recognition, term clustering is an 
indispensable component in a knowledge 
management process (see figure 2). Since 
terminological opacity and polysemy are very 
common in molecular biology, term clustering is 
essential for the semantic integration of terms, 
the construction of domain ontology and for 
choosing the appropriate semantic information.  
   The ATC method is based on Ushioda?s AMI 
(Average Mutual Information)-hierarchical 
clustering method (Ushioda, 1996). Our 
implementation uses parallel symmetric 
processing for high speed clustering and is built 
on the C/NC-value results. As input, we use 
co-occurrences of automatically recognised 
terms and their contexts, and the output is a 
dendrogram of hierarchical term clusters (like a 
thesaurus). The calculated term cluster 
information is stored in LiLFeS (see below) and 
combined with a predefined ontology according 
to the term classes automatically assigned. 
1.3 LiLFeS 
LiLFeS (Miyao et al, 2000) is a Prolog-like 
programming language and language processor 
used for defining definite clause programs with 
typed feature structures. Since typed feature 
structures can be used like first order terms in 
Prolog, the LiLFeS language can describe 
various kinds of applications based on feature 
structures. Examples include HPSG parsers, 
HPSG-based grammars and compilers from 
HPSG to CFG. Furthermore, other NLP modules 
can be easily developed because feature structure 
processing can be directly written in the LiLFeS 
language. Within TIMS, LiLFeS is used to: 1) 
infer similarity between terms using hierarchical 
matching, and 2) parse sentences using 
HPSG-based parsers and convert the results into 
an XML-based formalism. 
 
2 Knowledge Integration and Management  
 
Knowledge integration and management in 
TIMS is organised by integrating XML-data 
management (section 2.1) and tag- and 
ontology-based information extraction (section 
2.2). Figure 3 illustrates a model of the 
knowledge management based on the knowledge 
integration and question-answering process 
within TIMS. In this scenario, a user formulates a 
query, which is processed by a query manager. 
The tag data manager retrieves the relevant data 
from the collection of documents via a tag 
database and ontology-based inference (such as 
POS Tagger 
Acronym Recognition 
C-value ATR 
Orthographic Variants 
Morphological Variants 
Syntactic Variants 
NC-value Analyzer 
Term Clustering  
(Semantic Analyzer) 
XML Documents Including 
Term Tags and Term 
Variation/Class Information 
Input Documents 
Figure 2. Term Ontology Development 
Recognition of Term 
Variations (synonyms) 
Recognition of Term 
Classes (Similar Terms) 
hierarchical matching of term classes).  
2.1 XML-tag data management 
Communication within TIMS is based on 
XML-data exchange.  TIMS initially parses the 
XML documents (which contain relevant 
terminology information generated automatically 
by ATRACT) and ?de-tags? them. Then, like in 
the TIPSTER architecture (Grishman, 1995), 
every tag information is stored separately from 
the original documents and managed by an 
external database software. This facility allows, 
as shown in figure 4, different types of tags (POS, 
syntactic, semantic, etc.) for the same document 
to be supported. 
2.2 Tag- and ontology-based IE 
The key feature of KA and KI within TIMS is a 
facility to logically retrieve data that is 
represented by different tags. This feature is 
implemented via interval operations. The main 
assumption is that the XML tags specify certain 
intervals within documents. Interval operations 
are XML specific text/data retrieval operations, 
which operate on such textual intervals. Each 
interval operation takes two sets of intervals as 
input and returns a set of intervals according to 
the specified logical operations. Currently, we 
define four types of logical operations: 
? Intersection ??? returns intersected intervals 
of all the intervals given. 
? Union ??? returns merged intervals of all the 
intersected intervals. 
? Subtraction ?y? returns differences in 
intervals of all the intersected intervals. 
? Concatenation ?+? returns concatenated 
intervals of all the continuous intervals. 
 
For example, the interval operation 1 
<VP>?(<V>?<term>) describes all verb 
(<V>)-term (<term>) pairs within a verb phrase 
(<VP>). Similarly, suppose X denotes a set of 
intervals of manually annotated tags for a 
document and Y denotes a set of intervals of 
automatically annotated tags for the same 
document. The interval operation ((X?Y) 
?{X?Y}) results in the differences between 
human and machine annotations (see figure 5). 
Interval operations are powerful means for 
textual mining from different sources using tag 
information.  In addition, LiLFeS enables tag 
(interval) retrieval to process not only regular 
                                                
1 ??? denotes a merged set of all the elements. 
 
Figure 3: Question-answering process in TIMS 
Database 
A
  
A
  
A
  
A
  
A
  
A
  
XML / HTML 
Knowledge 
Sources 
Tag Data 
Language 
Analyzer 
 
 
Tag Data 
Manager 
TIQL 
Processor
NLP Components 
TIMS  
Query to TIQL
Translator 
Query 
ATRACT 
Ontology 
Data 
A
  
A
  
A
  
A
  
A
  
A
  
 
LiLFeS 
Knowledge Acquisition 
Knowledge Integration 
 ? 
 
26 15 VERB 
? 
 
110 100 ADJ 
? 150 140 DNA 
? ? ? ?.. 
? 209 203 RNA 
? 10 5 NOUN
. . . end start Tag 
? 
 
35 15 VP 
? 
 
100 35 PP 
? 
 
150 100 VP 
? ? ? ?.. 
? 209 203 NP 
? 
 
10 5 NP 
. . . end start Tag 
Figure 4: Tag data management 
Part-of-speech tags 
? 
 
80 40 PROTEIN 
? 
 
180 160 DNA 
? 
 
220 200 DNA 
? ? ? ?.. 
? 
 
260 240 RNA 
? 
 
18 5 DNA 
. . . end start Tag 
Semantic tags 
Syntactic tags 
 
X = {                                                           }
Y = {                                                            }
 
X?Y  = {                                                              } 
?{X?Y}={                                                        
} 
 
(X?Y) ?{X?Y}={                                             
                                                                                               }
Figure 5. (X?Y) ?{X ?Y} 
pattern/string matching using tag information, 
but also the ontological hierarchy matching to 
subordinate classes using either predefined or 
automatically derived term ontology. Thus, 
semantically-based tag information retrieval can 
be achieved. For example, the interval operation2 
<VP>?<nucleic_acid*> will retrieve all 
subordinate terms/classes of nucleic acid, which 
are contained within a VP. 
   The interval operations can be performed over 
the specified documents and/or tag sets (e.g. 
syntactic, semantic tags, etc.) simultaneously or 
in batch mode, by selecting the documents/tag 
sets from a list. This accelerates the process of 
KA, as users are able to retrieve information from 
multiple KSs simultaneously. 
2.3 TIQL - Tag Information Query Language  
In order to integrate and expand the above 
components, we have developed a tag 
information query language (TIQL). Using this 
language, a user can specify the interval 
operations to be performed on selected 
documents (including the ontology inference to 
expand queries). The basic expression in TIQL 
has the following form: 
 
SELECT [n-tuple variables]  
FROM [XML document(s)] 
WHERE [interval operation] 
      FROM [XML document(s)] 
WHERE [interval operation] 
                     ?? 
where, [n-tuple variables] specifies the 
table output format, [XML document(s)] 
denotes the document(s) to be processed, and 
[interval operation] denotes an interval 
operation to be performed over the corresponding 
document with variables of each interval to be 
bound. 
For example, the following expression: 
 
SELECT   x1, x2  
 FROM   ?paper-1.xml? 
    WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
  FROM   ?paper-2.xml? 
 WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
 
                                                
2 ?*? denotes hierarchical matching. 
extracts all the hierarchically subordinate classes 
matched to (<EVENT>, <nucleic_acid>) pair 
within a VP from the specified XML-documents,  
and then automatically builds a table to display 
the results (see figure 6).  
   Since formulating an appropriate TIQL 
expression using interval operations might be 
cumbersome, in particular for novice users, 
TIMS was augmented with a capability of 
?recycling? predefined queries and macros. 
3 Evaluation and discussion 
We have conducted preliminary experiments 
using the proposed framework. In this paper we 
briefly present the quality of automatic term 
recognition and similarity measure calculation 
via automatically clustered terms. After that, we 
discuss the practical performance of tag 
manipulation in TIMS compared to string-based 
XML tag manipulation to show the advantage of 
the tag information management scheme.  
   The term recognition evaluation was performed 
on the NACSIS AI-domain corpus (Koyama et 
al., 1998), which includes 1800 abstracts and on a 
set of MEDLINE abstracts. Table 1 shows a 
sample of extracted terms and term variants. The 
ATR precisions of the top 100 intervals range 
from 93% to 98% (see figure 7; for detailed 
evaluation, see Mima et al (2001b) and Nenadic 
et al (2002)).  
 
 Title 
Background 
                                      
                  
........<DNA>androgen 
receptor gene</DNA>   
............... 
                       
         
                      
paper-2.xml 
Title 
Background 
                                      
                  
........<RNA>HB-EGF 
mRNA</RNA>   
............... 
                       
         
                            
paper-1.xml 
nucleic_acid 
 nucleic acid EVENT 
EVENT 
androgen receptor 
gene acid HB-EGF mRNA 
... 
activate 
bind 
... 
Figure 6. Ontology-based Tagged 
Information Retrieval 
 
   terms (and term variants) term-hood 
retinoic acid receptor                                              
     retinoic acid receptor 
     retinoic acid receptors 
     RAR, RARs 
6.33 
nuclear receptor  
     nuclear receptor 
     nuclear receptors 
     NR, NRs 
6.00 
all-trans retionic acid 
     all trans retionic acid 
     all-trans-retinoic acids 
     ATRA, at-RA, atRA 
4.75 
9-cis-retinoic acid 
     9-cis retinoic acid 
     9cRA, 9-c-RA 
4.25 
 
Table 1: Sample of recognised terms  
85
90
95
100
2.65-3.99 4.00-5.99 6.00-Top
C-value
pr
ec
is
io
n
 
Figure 7: ATR interval precision 
 
   For term clustering and tag manipulation 
performance we used the GENIA resources 
(GENIA corpus, 2002), which include 1,000 
MEDLINE abstracts (MEDLINE, 2002), with 
overall 40,000 (16,000 distinct) semantic tags 
annotated for terms in the domain of nuclear 
receptors. We used the similarity measure 
calculation as the central computing mechanism 
for inferring the relevance between the XML tags 
and tags specified in the TIQL/interval operation,  
determining the most relevant tags in the 
XML-based KS(s). As a gold standard, we used 
similarities between the terms that were 
calculated according to the hierarchy of the 
clustered terms according to the GENIA 
ontology. In this experiment, we have adopted a 
semantic similarity calculation method for 
measuring the similarity between terms described 
in (Oi et al, 1997). The three major sets of 
classes (namely, nucleic_acid, amino_acid, 
SOURCE) of manually classified terms from 
GENIA ontology (GENIA corpus, 2002) were 
used to calculate the average similarities (AS) of 
the elements. ASs of the elements within the 
same classes were greater than the ASs between 
elements from different classes, which proves 
that the terms were clustered reliably according 
to their semantic features. 
   In order to examine the tag manipulation 
performance of TIMS, we measured the 
processing times consumed for executing an 
interval operation in TIMS compared to the time 
needed by using string-based regular expression 
matching (REM). We focused on measuring the 
interval operation ??? with intervals (tags) 
<title> and <term> (i.e. extracting all terms 
within titles).   In the evaluation process, we used 
5 different samples to examine IE performances 
according to their size (namely the number of 
tags and file size in Kb).  
 
 Sample1 Sample2 Sample3 Sample4 Sample5 
TIMS 
(millisec.) 16 28 40 44 62 
REM 
(millisec.) 24 38 58 80 104 
# of tags 1146 2383 3730 4799 5876 
Size  
(K bytes) 92 191 298 382 470 
 
Table 2: TIMS - practical performance 
 
Table 2 and Figure 8 show the results: the 
processing times of TIMS were about 1.4-1.8 
times faster (depending on number of tags and 
corpus length) than those of REM. Therefore, we 
assume that the TIMS tag information 
management scheme can be considered as an 
efficient mechanism to facilitate knowledge 
acquisition and information extraction process. 
0
20
40
60
80
100
120
0 2000 4000 6000
# of tags
tim
e 
(m
ill
i s
ec
.)
TIMS
REM
Figure 8. IE performance (TIMS vs. REM) 
Conclusion 
In this paper, we presented a methodology for 
KA and KI over large KSs. We described TIMS, 
an XML-based integrated KA aid system, in 
which we have integrated automatic term 
recognition, term clustering, tagged data 
management and ontology-based knowledge  
retrieval. TIMS allows users to search and 
combine information from various sources. An 
important source of information in the system is 
derived from terminological knowledge, which is 
provided automatically in the XML format. 
Tag-based retrieval is implemented through 
interval operations, which ? in combination with 
hierarchical matching ? prove to be powerful 
means for textual mining and knowledge 
acquisition. 
   The system has been tested in the domain of 
molecular biology. The preliminary experiments 
show that the TIMS tag information management 
scheme is an efficient methodology to facilitate 
KA and IE in specialised fields. 
   Important areas of future research will involve 
expanding the scalability of the system to real 
WWW knowledge acquisition tasks and 
experiments with fine-grained term 
classification. 
References  
Baker P. G., Brass A., Bechhofer S., Goble C., Paton 
N. and Stevens R. (1998) TAMBIS: Transparent 
Access to Multiple Bioinformatics Information 
Sources. An Overview in Proc. of the Sixth 
International Conference on Intelligent Systems for 
Molecular Biology, ISMB98, Montreal. 
Berners-Lee, T. (1998) The Semantic Web as a 
longuage of logic, available at: http://www.w3.org/ 
DesignIssues/Logic.html 
Brickle, D. and Guha R. (2000) Resource Description 
Framework (RDF) Schema Specification 1.0, W3C 
Candidate Recommendation, available at 
http://www.w3.org/TR/rdf-schema 
Frantzi K. T., Ananiadou S. and Mima H. (2000) 
Automatic Recognition of Multi-Word Terms: the 
C-value/NC-value method, in International Journal 
on Digital Libraries, Vol. 3, No. 2, 115?130. 
Gene Ontology Consortium (2002) GO ontology. 
available at  http:// www.geneontology.org/ 
GENIA corpus (2002) GENIA project home page. 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 
Grishman R (1995) TIPSTER Phase II Architecture 
Design Document. New York University, available 
at http://www.tipster.org/arch.htm 
Jacquemin C. and Tzoukermann E. (1999) NLP for 
Term Variant Extraction: A Synergy of Morphology, 
Lexicon and Syntax. In T. Strzalkowski (editor), 
Natural Language Information Retrieval, Kluwer, 
Boston, pp. 25-74. 
Koyama T., Yoshioka M. and Kageura K. (1998) The 
Construction of a Lexically Motivated Corpus - The 
Problem with Defining Lexical Unit. In Proceedings 
of LREC 1998, Granada, Spain, pp. 1015?1019. 
MEDLINE (2002) National Library of Medicine, 
http://www.ncbi.nlm.nih.gov/PubMed/ 
Mima H., Ananiadou S. and Nenadic G. (2001a) 
ATRACT Workbench: An Automatic Term 
Recognition and Clustering of Te rms, in Text, 
Speech and Dialogue - TSD2001, Lecture Notes in 
AI 2166, Springer Verlag 
Mima H. and Ananiadou S. (2001b) An Application 
and Evaluation of the C/NC-value Approach for the 
Automatic term Recognition of Multi-Word units in 
Japanese, in International Journal on Terminology, 
Vol. 6(2), pp 175-194. 
Mima H., Ananiadou S. and Tsujii J. (1999) A 
Web-based integrated knowledge mining aid 
system using term-oriented natural language 
processing, in Proceedings of The 5th Natural 
Language Processing Pacific Rim Symposium, 
NLPRS'99, pp. 13?18. 
Miyao Y., Makino T., Torisawa K. and Tsujii J. 
(2000) The LiLFeS abstract machine and its 
evaluation with the LinGO grammar. Journal of 
Natural Language Engineering, Cambridge 
University Press, Vol. 6(1), pp.47-62. 
Nenadic G., Spasic I. and Ananiadou S. (2002) 
Automatic Acronym Acquisition and Term 
Variation Management within Domain Specific 
Texts, in Proc. of LREC 2002, Las Palmas, Spain, 
pp. 2155-2162. 
Oi K., Sumita E. and Iida H. (1997) Document 
Retrieval Method Using Semantic Similarity and 
Word Sense Disambiguation (in Japanese), in 
Journal of Natural Language Processing, Vol.4, 
No.3, pp.51-70. 
Visser P.R.S., Jones D.M., Bench-Capon T.J.M. and 
Shave M.J.R. (1997) An Analysis of Ontology 
Mismatches; Heterogeneity versus Interoperability. 
In AAAI 1997 Spring Symposium on Ontological 
Engineering, Stanford University, California, USA. 
Ushioda A. (1996) Hierarchical Clustering of Words. 
In Proc. of COLING ?96, Copenhagen 
Proceedings of NAACL HLT 2009: Short Papers, pages 121?124,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Bilingual Dictionary from Comparable Corpora with 
Dependency Heterogeneity 
 
Kun Yu Junichi Tsujii 
Graduate School of Information Science and Technology 
The University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{kunyu, tsujii}@is.s.u-tokyo.ac.jp 
 
Abstract 
This paper proposes an approach for bilingual 
dictionary extraction from comparable corpora. 
The proposed approach is based on the obser-
vation that a word and its translation share 
similar dependency relations. Experimental re-
sults using 250 randomly selected translation 
pairs prove that the proposed approach signifi-
cantly outperforms the traditional context-
based approach that uses bag-of-words around 
translation candidates. 
1 Introduction 
Bilingual dictionary plays an important role in many 
natural language processing tasks. For example, ma-
chine translation uses bilingual dictionary to reinforce 
word and phrase alignment (Och and Ney, 2003), cross-
language information retrieval uses bilingual dictionary 
for query translation (Grefenstette, 1998). The direct 
way of bilingual dictionary acquisition is aligning trans-
lation candidates using parallel corpora (Wu, 1994). But 
for some languages, collecting parallel corpora is not 
easy. Therefore, many researchers paid attention to bi-
lingual dictionary extraction from comparable corpora 
(Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and 
Morin, 2008; Robitaille et al, 2006; Morin et al, 2007; 
Otero, 2008), in which texts are not exact translation of 
each other but share common features. 
Context-based approach, which is based on the ob-
servation that a term and its translation appear in similar 
lexical contexts (Daille and Morin, 2008), is the most 
popular approach for extracting bilingual dictionary 
from comparable corpora and has shown its effective-
ness in terminology extraction (Fung, 2000; Chiao and 
Zweigenbaum, 2002; Robitaille et al, 2006; Morin et al, 
2007). But it only concerns about the lexical context 
around translation candidates in a restricted window. 
Besides, in comparable corpora, some words may appear 
in similar context even if they are not translation of each 
other. For example, using a Chinese-English comparable 
corpus from Wikipedia and following the definition in 
(Fung, 1995), we get context heterogeneity vector of 
three words (see Table 1). The Euclidean distance be-
tween the vector of  ????(economics)? and ?econom-
ics? is 0.084. But the Euclidean distance between the 
vector of  ????? and ?medicine? is 0.075. In such 
case, the incorrect dictionary entry ????/medicine? 
will be extracted by context-based approach. 
Table 1. Context heterogeneity vector of words. 
Word Context Heterogeneity Vector
???(economics) (0.185, 0.006) 
economics (0.101, 0.013) 
medicine (0.113,0.028) 
To solve this problem, we investigate a comparable 
corpora from Wikipedia and find the following phe-
nomenon: if we preprocessed the corpora with a de-
pendency syntactic analyzer, a word in source language 
shares similar head and modifiers with its translation in 
target language, no matter whether they occur in similar 
context or not. We call this phenomenon as dependency 
heterogeneity. Based on this observation, we propose an 
approach to extract bilingual dictionary from compara-
ble corpora. Not like only using bag-of-words around 
translation candidates in context-based approach, the 
proposed approach utilizes the syntactic analysis of 
comparable corpora to recognize the meaning of transla-
tion candidates. Besides, the lexical information used in 
the proposed approach does not restrict in a small win-
dow, but comes from the entire sentence. 
We did experiments with 250 randomly selected 
translation pairs. Results show that compared with the 
approach based on context heterogeneity, the proposed 
approach improves the accuracy of dictionary extraction 
significantly. 
2 Related Work  
In previous work about dictionary extraction from com-
parable corpora, using context similarity is the most 
popular one.  
At first, Fung (1995) utilized context heterogeneity 
for bilingual dictionary extraction. Our proposed ap-
proach borrows Fung?s idea but extends context hetero-
geneity to dependency heterogeneity, in order to utilize 
rich syntactic information other than bag-of-words.  
After that, researchers extended context heterogeneity 
vector to context vector with the aid of an existing bilin-
gual dictionary (Fung, 2000; Chiao and Zweigenbaum, 
2002; Robitaille et al, 2006; Morin et al, 2007; Daille 
and Morin, 2008). In these works, dictionary extraction 
121
is fulfilled by comparing the similarity between the con-
text vectors of words in target language and the context 
vectors of words in source language using an external 
dictionary. The main difference between these works 
and our approach is still our usage of syntactic depend-
ency other than bag-of-words. In addition, except for a 
morphological analyzer and a dependency parser, our 
approach does not need other external resources, such as 
the external dictionary. Because of the well-developed 
morphological and syntactic analysis research in recent 
years, the requirement of analyzers will not bring too 
much burden to the proposed approach. 
Besides of using window-based contexts, there were 
also some works utilizing syntactic information for bi-
lingual dictionary extraction. Otero (2007) extracted 
lexico-syntactic templates from parallel corpora first, 
and then used them as seeds to calculate similarity be-
tween translation candidates. Otero (2008) defined syn-
tactic rules to get lexico-syntactic contexts of words, and 
then used an external bilingual dictionary to fulfill simi-
larity calculation between the lexico-syntactic context 
vectors of translation candidates. Our approach differs 
from these works in two ways: (1) both the above works 
defined syntactic rules or templates by hand to get syn-
tactic information. Our approach uses data-driven syn-
tactic analyzers for acquiring dependency relations 
automatically. Therefore, it is easier to adapt our ap-
proach to other language pairs. (2) the types of depend-
encies used for similarity calculation in our approach are 
different from Otero?s work. Otero (2007; 2008) only 
considered about the modification dependency among 
nouns, prepositions and verbs, such as the adjective 
modifier of nouns and the object of verbs. But our ap-
proach not only uses modifiers of translation candidates, 
but also considers about their heads. 
3 Dependency Heterogeneity of Words in 
Comparable Corpora 
Dependency heterogeneity means a word and its trans-
lation share similar modifiers and head in comparable 
corpora. Namely, the modifiers and head of unrelated 
words are different even if they occur in similar context. 
Table 2. Frequently used modifiers (words are not ranked). 
???(economics) economics medicine 
??/micro keynesian physiology
??/macro new Chinese 
??/computation institutional traditional
?/new positive biology 
??/politics classical internal 
??/university labor science 
???/classicists development clinical 
??/development engineering veterinary 
 ??/theory finance western 
??/demonstration international agriculture
For example, Table 2 collects the most frequently 
used 10 modifiers of the words listed in Table 1. It 
shows there are 3 similar modifiers (italic words) be-
tween ????(economics)? and ?economics?. But there 
is no similar word between the modifiers of ????? 
and that of ?medicine?. Table 3 lists the most frequently 
used 10 heads (when a candidate word acts as subject) 
of the three words. If excluding copula, ????? and 
?economics? share one similar head (italic words). But 
????? and ?medicine? shares no similar head.  
Table 3. Frequently used heads  
(the predicate of subject, words are not ranked). 
???(economics) economics medicine 
?/is is is 
??/average has tends 
??/graduate was include 
??/admit emphasizes moved 
?/can non-rivaled means 
??/split became requires 
??/leave assume includes 
?/compare relies were 
??/become can has 
??/emphasize replaces may 
4 Bilingual Dictionary Extraction with De-
pendency Heterogeneity   
Based on the observation of dependency heterogeneity 
in comparable corpora, we propose an approach to ex-
tract bilingual dictionary using dependency heterogene-
ity similarity.  
4.1 Comparable Corpora Preprocessing 
Before calculating dependency heterogeneity similarity, 
we need to preprocess the comparable corpora. In this 
work, we focus on Chinese-English bilingual dictionary 
extraction for single-nouns. Therefore, we first use a 
Chinese morphological analyzer (Nakagawa and Uchi-
moto, 2007) and an English pos-tagger (Tsuruoka et al, 
2005) to analyze the raw corpora. Then we use Malt-
Parser (Nivre et al, 2007) to get syntactic dependency of 
both the Chinese corpus and the English corpus. The 
dependency labels produced by MaltParser (e.g. SUB) 
are used to decide the type of heads and modifiers.  
After that, the analyzed corpora are refined through 
following steps: (1) we use a stemmer1 to do stemming 
for the English corpus. Considering that only nouns are 
treated as translation candidates, we use stems for trans-
lation candidate but keep the original form of their heads 
and modifiers in order to avoid excessive stemming. (2) 
stop words are removed. For English, we use the stop 
word list from (Fung, 1995). For Chinese, we remove 
??(of)? as stop word. (3) we remove the dependencies 
including punctuations and remove the sentences with 
                                                          
1 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/  
122
more than k (set as 30 empirically) words from both 
English corpus and Chinese corpus, in order to reduce 
the effect of parsing error on dictionary extraction.  
4.2 Dependency Heterogeneity Vector Calculation 
Equation 1 shows the definition of dependency hetero-
geneity vector of a word W. It includes four elements. 
Each element represents the heterogeneity of a depend-
ency relation. ?NMOD? (noun modifier), ?SUB? (sub-
ject) and ?OBJ? (object) are the dependency labels 
produced by MaltParser.  
(HNMODHead ,HSUBHead ,HOBJHead ,HNMODMod )  (1)
HNMODHead (W ) = number of different heads of W with NMOD labeltotal number of heads of W with NMOD label
  
HSUBHead (W ) = number of different heads of W with SUB labeltotal number of heads of W with SUB label
 
 
HOBJHead (W ) = number of different heads of W with OBJ labeltotal number of heads of W with OBJ label
 
 
HNMODMod (W ) = number of different modifiers of W with NMOD labeltotal number of modifiers of W with NMOD label  
4.3 Bilingual Dictionary Extraction  
After calculating dependency heterogeneity vector of 
translation candidates, bilingual dictionary entries are 
extracted according to the distance between the vector of 
Ws in source language and the vector of Wt in target lan-
guage. We use Euclidean distance (see equation 2) for 
distance computation. The smaller distance between the 
dependency heterogeneity vectors of Ws and Wt, the 
more likely they are translations of each other. 
DH (Ws,Wt ) = DNMODHead 2 + DSUBHead 2 + DOBJHead 2 + DNMODMod 2 (2)
            DNMODHead = HNMODHead(Ws)?HNMODHead(Wt )  
            DSUBHead = HSUBHead (W s) ? HSUBHead (W t )   
            DOBJHead = HOBJHead (Ws)?HOBJHead (Wt )  
            DNMODMod = HNMODMod (Ws) ?HNMODMod (Wt )  
For example, following above definitions, we get de-
pendency heterogeneity vector of the words analyzed 
before (see Table 4). The distances between these vec-
tors are DH(???, economics) = 0.222,  DH(???, 
medicine) = 0.496. It is clear that the distance between 
the vector of ????(economics)? and ?economics? is 
much smaller than that between ????? and ?medi-
cine?. Thus, the pair ????/economics? is extracted 
successfully. 
Table 4. Dependency heterogeneity vector of words. 
Word Dependency Heterogeneity Vector
???(economics) (0.398, 0.677, 0.733, 0.471) 
economics (0.466, 0.500, 0.625, 0.432) 
medicine (0.748, 0.524, 0.542, 0.220) 
5 Results and Discussion  
5.1 Experimental Setting 
We collect Chinese and English pages from Wikipedia2 
with inter-language link and use them as comparable 
corpora. After corpora preprocessing, we get 1,132,492 
                                                          
2 http://download.wikimedia.org 
English sentences and 665,789 Chinese sentences for 
dependency heterogeneity vector learning. To evaluate 
the proposed approach, we randomly select 250 Chi-
nese/English single-noun pairs from the aligned titles of 
the collected pages as testing data, and divide them into 
5 folders. Accuracy (see equation 3) and MMR (Voor-
hees, 1999) (see equation 4) are used as evaluation met-
rics. The average scores of both accuracy and MMR 
among 5 folders are also calculated. 
Accuracy = ti
i=1
N? N  (3)
ti = 1, if there exists correct translation in top n ranking0, otherwise
? ? ? 
  
MMR = 1
N
1
rankii=1
N? ,     ranki = ri,  if ri < n0, otherwise
? ? ? 
 (4)
       n means top n evaluation,  
       ri means the rank of the correct translation in top n ranking 
      N means the total number of words for evaluation 
 
5.2 Results of Bilingual Dictionary Extraction 
Two approaches were evaluated in this experiment. One 
is the context heterogeneity approach proposed in (Fung, 
1995) (context for short). The other is our proposed ap-
proach (dependency for short). 
The average results of dictionary extraction are listed 
in Table 5. It shows both the average accuracy and aver-
age MMR of extracted dictionary entries were improved 
significantly (McNemar?s test, p<0.05) by the proposed 
approach. Besides, the increase of top5 evaluation was 
much higher than that of top10 evaluation, which means 
the proposed approach has more potential to extract pre-
cise bilingual dictionary entries.  
Table 5. Average results of dictionary extraction. 
context dependency  
ave.accu ave.MMR ave.accu ave.MMR 
Top5 0.132 0.064 0.208(?57.58%) 0.104(?62.50%)
Top10 0.296 0.086 0.380(?28.38%) 0.128(?48.84%)
5.3 Effect of Dependency Heterogeneity Vector 
Definition 
In the proposed approach, a dependency heterogeneity 
vector is defined as the combination of head and modi-
fier heterogeneities. To see the effects of different de-
pendency heterogeneity on dictionary extraction, we 
evaluated the proposed approach with different vector 
definitions, which are 
only-head: (HNMODHead ,HSUBHead ,HOBJHead )
only-mod: (HNMODMod ) 
only-NMOD: (HNMODHead ,HNMODMod )  
Table 6. Average results with different vector definitions. 
Top5 Top10  
ave.accu ave.MMR ave.accu ave.MMR
context 0.132 0.064 0.296 0.086 
dependency 0.208 0.104 0.380 0.128 
only-mod 0.156 0.080 0.336 0.103 
only-head 0.176 0.077 0.336 0.098 
only-NMODs 0.200 0.094 0.364 0.115 
123
The results are listed in Table 6. It shows with any 
types of vector definitions, the proposed approach out-
performed the context approach. Besides, if comparing 
the results of dependency, only-mod, and only-head, a 
conclusion can be drawn that head dependency hetero-
geneities and modifier dependency heterogeneities gave 
similar contribution to the proposed approach. At last, 
the difference between the results of dependency and 
only-NMOD shows the head and modifier with NMOD 
label contributed more to the proposed approach. 
5.4 Discussion 
To do detailed analysis, we collect the dictionary entries 
that are not extracted by context approach but extracted 
by the proposed approach (good for short), and the en-
tries that are extracted by context approach but not ex-
tracted by the proposed approach (bad for short) from 
top10 evaluation results with their occurrence time (see 
Table 7). If neglecting the entries ???/passports? and 
???/shanghai?, we found that the proposed approach 
tended to extract correct bilingual dictionary entries if 
both the two words occurred frequently in the compara-
ble corpora, but failed if one of them seldom appeared.   
Table 7. Good and bad dictionary entries. 
Good Bad 
Chinese English Chinese English 
???/262 jew/122 ???/53 crucifixion/19 
??/568 velocity/175 ???/6 aquarium/31 
??/2298 history/2376 ???/47 mixture/179 
??/1775 organizations/2194 ?/17 brick/66 
??/1534 movement/1541 ??/23 quantification/31
??/76 passports/80 ??/843 shanghai/1247 
But there are two exceptions: (1) although ???
(shanghai)? and ?shanghai? appeared frequently, the pro-
posed approach did not extract them correctly; (2) both 
???(passport)? and ?passports? occurred less than 100 
times, but they were recognized successfully by the pro-
posed approach. Analysis shows the cleanliness of the 
comparable corpora is the most possible reason. In the 
English corpus we used for evaluation, many words are 
incorrectly combined with ?shanghai? by ?br? (i.e. line 
break), such as ?airportbrshanghai?. These errors af-
fected the correctness of dependency heterogeneity vec-
tor of ?shanghai? greatly. Compared with the dirty 
resource of ?shanghai?, only base form and plural form 
of ?passport? occur in the English corpus. Therefore, the 
dependency heterogeneity vectors of ???? and ?pass-
ports? were precise and result in the successful extrac-
tion of this dictionary entry. We will clean the corpora to 
solve this problem in our future work. 
6 Conclusion and Future Work  
This paper proposes an approach, which not uses the 
similarity of bag-of-words around translation candidates 
but considers about the similarity of syntactic dependen-
cies, to extract bilingual dictionary from comparable 
corpora. Experimental results show that the proposed 
approach outperformed the context-based approach sig-
nificantly. It not only validates the feasibility of the pro-
posed approach, but also shows the effectiveness of 
applying syntactic analysis in real application.  
There are several future works under consideration 
including corpora cleaning, extending the proposed ap-
proach from single-noun dictionary extraction to multi-
words, and adapting the proposed approach to other lan-
guage pairs. Besides, because the proposed approach is 
based on the syntactic analysis of sentences with no 
more than k words (see Section 4.1), the parsing accu-
racy and the setting of threshold k will affect the cor-
rectness of dependency heterogeneity vector learning. 
We will try other thresholds and syntactic parsers to see 
their effects on dictionary extraction in the future. 
Acknowledgments 
This research is sponsored by Microsoft Research Asia 
Web-scale Natural Language Processing Theme. 
References  
Y.Chiao and P.Zweigenbaum. 2002. Looking for Candidate Transla-
tional Equivalents in Specialized, Comparable Corpora. Proceed-
ings of LREC 2002. 
B.Daille and E.Morin. 2008. An Effective Compositional Model for 
Lexical Alignment. Proceedings of IJCNLP-08. 
P.Fung. 1995. Compiling Bilingual Lexicon Entries from a Non-
parallel English-Chinese Corpus. Proceedings of the 3rd Annual 
Workshop on Very Large Corpora. pp. 173-183.  
P.Fung. 2000. A Statistical View on Bilingual Lexicon Extraction 
from Parallel Corpora to Non-parallel Corpora. Parallel Text Proc-
essing: Alignment and Use of Translation Corpora. Kluwer Aca-
demic Publishers. 
G.Grefenstette. 1998. The Problem of Cross-language Information 
Retrieval. Cross-language Information Retrieval. Kluwer Aca-
demic Publishers. 
E.Morin et al. 2007. Bilingual Terminology Mining ? Using Brain, 
not Brawn Comparable Corpora. Proceedings of ACL 2007. 
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Approach to Word 
Segmentation and POS Tagging. Proceedings of ACL 2007. 
J.Nivre et al. 2007. MaltParser: A Language-independent System for 
Data-driven Dependency Parsing. Natural Language Engineering. 
13(2): 95-135. 
F.Och and H.Ney. 2003. A Systematic Comparison of Various Statis-
tical Alignment Models. Computational Linguistics, 29(1): 19-51. 
P.Otero. 2007. Learning Bilingual Lexicons from Comparable English 
and Spanish Corpora. Proceedings of MT Summit XI. pp. 191-198.  
P.Otero. 2008. Evaluating Two Different Methods for the Task of 
Extracting Bilingual Lexicons from Comparable Corpora. Proceed-
ings of LREC 2008 Workshop on Comparable Corpora. pp. 19-26. 
X.Robitaille et al. 2006. Compiling French Japanese Terminologies 
from the Web. Proceedings of EACL 2006. 
Y.Tsuruoka et al. 2005. Developing a Robust Part-of-speech Tagger 
for Biomedical Text. Advances in Informatics ? 10th Panhellenic 
Conference on Informationcs. LNCS 3746. pp. 382-392. 
E.M.Voorhees. 1999. The TREC-8 Question Answering Track Report. 
Proceedings of the 8th Text Retrieval Conference.  
D.Wu. 1994. Learning an English-Chinese Lexicon from a Parallel 
Corpus. Proceedings of the 1st Conference of the Association for 
Machine Translation in the Americas.
124
 
	







 
	 	 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
Han-Cheol Cho
?
, Do-Gil Lee
?
, Jung-Tae Lee
?
, Pontus Stenetorp
?
, Jun?ichi Tsujii
?
and Hae-Chang Rim
?
?
Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
?
Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
Abstract
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
1 Introduction
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer?s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al, 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al, 2007) significantly, when the
29
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
2 The Proposed Method
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
2.1 Baseline Word Segmentation Model
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al, 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).
1
Let o
1,n
be a sequence of n-character user input
without WBMs, x
t
be the best word spacing state
for o
t
where 1 ? t ? n. Assume that x
t
is either 1
(space after o
t
) or 0 (no space after o
t
). Then each
best word spacing state x?
t
for all t can be found by
using Equation 1.
x?
t
= argmax
i?(0,1)
P (x
t
= i|o
1,n
) (1)
= argmax
i?(0,1)
P (o
1,n
, x
t
= i) (2)
= argmax
i?(0,1)
?
x
t?2
,x
t?1
P (x
t
= i|x
t?2
, o
t?1
, x
t?1
, o
t
)
?
?
x
t?1
P (o
t+1
|o
t?1
, x
t?1
, o
t
, x
t
= i)
?
?
x
t+1
P (o
t+2
|o
t
, x
t
= i, o
t+1
, x
t+1
) (3)
Equation 2 is derived by applying the Bayes?
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
1
In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of x
t?2
, x
t?1
, x
t+1
and x
t+2
values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al, 2007).
2.2 Confidence and Threshold Estimation
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
T = f(C) (4)
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, x
WS
1,n
, with the correct word
spacing states, x
correct
1,n
, as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.
2
C =
# of x
input
t
same to x
correct
t
# of x
input
t
(5)
?
# of x
input
t
same to x
WS
t
# of x
input
t
(6)
?
n
?
?
?
?
n
?
k=1
P (x
input
k
|o
1,n
) (7)
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.
3
2
In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3
The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
30
20%30%
40%50%
60%70%
80%90%
100%
100% 96% 92% 88% 84% 80%
Estim
ated C
onfid
ence
True Confidence
Figure 1: The relationship between estimated con-
fidence and true confidence
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f .
T ? f(C) = C (8)
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
2.3 Output Optimization
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
3 Experimental Results
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input?s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.
4
In the second experiment, we attempt
4
See Footnote 3.
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
For the first experiment, we use the Sejong corpus
5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al,
2007), the baseline WS model, and the proposed
method.
Although Lee?s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model?s
output is better than the user input. In Figure 2,
Lee?s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee?s model, al-
though the baseline WS model itself does not out-
performs Lee?s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
3.2 Performance Comparison with Web Text
having Usual Error Rate
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
5
Details available at: http://www.sejong.or.kr/eindex.php
31
84%
86%
88%
90%
92%
94%
96%
98%
100%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
w
or
d-u
nit
 
 
pr
ec
isio
n
word spacing error rate of user input (in character-unit precision)
Test corpus Lee's model Baseline WS model Proposed method
Figure 2: Performance improvement according to the word spacing error rate of user input
Method Web Text
Test Corpus 70.89%
Lee?s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
Table 1: Performance comparison with Web text
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee?s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
4 Conclusion
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al, 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
5 Acknowledgment
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28?35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ?04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
32
Comparison between Tagged Corpora for the Named Entity 
Task 
Chi~sh i  NOBATA N ige l  COLL IER  and Jun ' i ch i  TSUJ I I  
Kansa i  Advanced Research Center  Depar tment  of  In format ion  Science 
Communicat ions  Research Laboratory  Graduate  School of  Science 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku University of  Tokyo, Hongo 7-3-1 
Kobe,  Hyogo, 65\].-2492 JAPAN Bunkyo-ku,  Tokyo,  113-0033 JAPAN 
nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp 
Abst rac t  
We present two measures for compar- 
ing corpora based on infbrmation the- 
ory statistics uch as gain ratio as well 
as simple term-class ~equency counts. 
We tested the predictions made by these 
measures about corpus difficulty in two 
domains - -  news and molecular biol- 
ogy - -  using the result of two well-used 
paradigms for NE, decision trees and 
HMMs and found that gain ratio was the 
more reliable predictor. 
made by these measures against actual system 
performance. 
Recently IE systems based on supervised learn- 
ing paradigms uch as hidden Markov models 
(Bikel et al, 1997), maximum entropy (Borth- 
wick et al, 1998) and decision trees (Sekine et 
al., 1998) have emerged that should be easier to 
adapt to new domains than the dictionary-based 
systems of the past. Much of this work has taken 
advantage of smoothing techniques to overcome 
problems associated with data sparseness (Chen 
and Goodman, 1996). 
The two corpora we use in our NE experiments 
represent the following domains: 
1 In t roduct ion  
With the advent of the information society and 
increasing availability of large mounts  of infor- 
mation in electronic form, new technologies such 
as information extraction are emerging to meet 
user's information access needs. Recent evalu- 
ation conferences such as TREC (Voorhees and 
Harman, 2000) showed the feasibility of this task 
and highlighted the need to combine information 
ret r ied  (m) and extraction (IE) to go beyond 
simply offering the user a long ranked list of in- 
teresting documents to providing facts for user's 
questions. 
The problem of domain dependence r mains a 
serious one and in fact there has been very little 
work so far to compare the difllculty of IE tasks for 
different domaln~ and their corpora. Such knowl- 
edge is useful for developing IE systems that are 
portable between domains. This paper begins to 
address this issue, in particular the lowest level of 
IE task, defined in the TIPSTER sponsored MUC- 
6 conference (MUC, 1995) as named entity (NE). 
This is emerging as a key technology in several 
other IF-related tasks such as question answer- 
ing. We seek here to show theoretically motivated 
measures for comparing the ditficulty of corpora 
for the NE task in two domains, newswire and 
molecular-biology. We then test the predictions 
? Newswire: acquisition of names of people, or- 
ganizations and monetary units etc., from the 
MUC-6 data set. 
? Molecular-biology: acquisition of proteins, 
DNAs, RNAs etc. from a subset of the MED- 
LINE database (MEDLINE, 1999). 
Information extraction in the molecular-biology 
domain (Seldmlzu et al, 1998) (Craven and Kum- 
lien, 1999) (Rindflesch et al, 2000) has recently 
become a topic of interest o the NLP community. 
This is a result of the need to formalise the huge 
number of research results that appear in free-text 
form in online collections of journal abstracts and 
papers such as MEDLINE for databases such as 
Swissprot (Ban:och and Apwefler, 1997) and also 
to search such collections for facts in an intelligent 
way. 
The purpose of our study is not to show a high 
level of absolute system performance. In fact since 
we use only the MUC-6 executive succession data 
set of 60 articles and a new MEDLINE data set 
of 100 articles we cannot hope to achieve perfor- 
mance limits. What we aim to do is to compare 
model performance against he predictions of cor- 
pus difficulty made by two different methods. In 
the rest of this paper we firstly introduce the NE 
models used for evaluation, the two corpora we 
20 
examined and then the difficulty comparison met- 
rics. Predictive scores from the metrics are ex- 
amined against he actual performance of the NE 
models. 
2 Mode ls  
Recent studies into the use of supervised learning- 
based modeels for the NE task in the molecular- 
biology domain have shown that models based on 
hidden Markov models (HMMs) (Collier et al, 
2000) and decision trees (Nobata et al, 1999) are 
not only adaptable to this highly technical do- 
main, but are also much more generalizable to new 
classes of words than systems based on traditional 
hand-built heuristic rules such as (Fukuda et al, 
1998). We now describe two models used in our 
experiments based on the decision trees package 
C4.5 (Quiuian, 1993) and HMMs (Rabiner and 
Juang, 1986). 
2.1 Decision tree named entity 
recogniser:NE-DT 
A decision tree is a type of classifier which 
has "leaf nodes" indicating classes and "decision 
nodes" that specify some test to be carried out, 
with one branch or subtree for each possible out- 
come of the test. A decision tree can be used 
to classify an object by starting at the root of 
the tree and moving through it until a leaf is en- 
countered. When we can define suitable features 
for the decision tree, the system can achieve good 
performance with only a small amount of training 
data. 
The system we used is based on one that was 
originally created for Japanese documents (Seine 
et al, 1998). It has two phases, one for creating 
the decision tree from training data and the other 
for generating the class-tagged text based on the 
decision tree. When generating decision trees, tri- 
grams of words were used. For this system, words 
are considered to be quadruple features. The fol- 
lowing features are used to generate conditions in 
the decision tree: 
Par t -o f -speech in format ion:  There are 45 
part-of-speech categories, whose definitions 
are based on Pennsylvania Treebank's cat- 
egories. We use a tagger based on Adwait 
Ratnaparkhi's method (Ratnaparkhi, 1996). 
Character type in format ion:  Orthographic 
information is considered such as upper case, 
lower case, capitalization, numerical expres- 
sions, symbols. These character features 
are the same as those used by NEHMM 
described in the next section and shown in 
Table 1. 
Word  l ists specif ic to  the  domain :  Word 
lists are made from the training corpus. 
Only the 200 highest fxequency words are 
used. 
2.2 H idden Markov  mode l  named ent i ty  
reco~. i ser :  NEHMM 
HMMs are a widely u~d class of learning algo- 
rithms and can be considered to be stochastic fi- 
nite state machines. In the following model, sum- 
marized here from the full description given in 
(Collier et al, 2000), we consider words to be or- 
dered pairs consisting of a surface word, W, and 
a word feature, F ,  given as < W, F >. The word 
features themselves are discussed below. As is 
common practice, we need to calculate the prob- 
abilities for a word sequence for the first word's 
name class and every other word differently since 
we have no initial name-class to make a transition 
from. Accordingly we use the following equation 
to calculate the initial name class probability, 
Pr(NC~\[ < Wf~,t , Flli,,~ >)= 
aof(NC$,,s,\[ < Wf,,,,,Ffi,,t >)+ 
o~f(gcs~,,,I < -,Ff~,,, >) + 
a~f(NCfi,.,,) (i) 
and for all other words and their name classes 
as follows: 
Fr(NCT~ I < Wt,Ft >,< W~-,,Ft-, >,NC~-i) = 
Aof(NGtl < W~,F~ >,< Wt-,,Ft-1 >,NG~-,) + 
Alf(NCtI < .,F~ >,< W~-I,F~-i >,NC~- i )+ 
A2f(NC~I < W,,F~ >, < .. F,-, >,NCt-x) + 
AsI(NG, I < .,Ft >,< _, F~-, >,NG,- ,)+ 
A4f(NC, INC,-,) + 
Asf(NC,) (2) 
where f(I) is calculated with maximum- 
likelihood estimates from counts on training data. 
In our current system we set the constants Ai 
and al by hand and let ~ ai = 1.0, ~ Ai = 1.0, 
ao _> al  > ~,  ~o >_ A , . . .  >_ As. The cur- 
rent name-class NCt is conditioned on the cur- 
rent word and feature, the previous name-class, 
NCt-1, and previous word and feature. 
Equations 1 and 2 implement a linear- 
interpolating HMM that incorporates a number of 
sub-models designed to reduce the effects of data 
sparseness. 
Table 1: Word features v~ith examples 
Word Feature Example 
TwoDig i tN~ 25 
FourDigitNumber 2000 
DigitNumber 15012 
SingleCap M 
GreekLetter alpha 
CapsAndDigits 12 
TwoCaps RalGDS 
LettersAnd.Digits p52 
In i tCap Interleukin 
LowCaps kappaB 
Lowercase kinases 
Hyphon 
Backslash / 
Feature Ex. 
CloseSquare \] 
Colon 
SemiColon ; 
Percent % 
OpenParen ( 
CloseParen ) 
Comma 
FullStop . 
Determiner the 
Conjunction and 
Other *+~ 
Once the state transition probabilities have 
been calculated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of possible name class as- 
signments in linear time to find the highest prob- 
ability path, i.e. to maximise Pr(W, NC). The fi- 
nal stage of our algorithm that is used after narae- 
class tagging is complete is to use a clean-up mod- 
ule called Unity. This creates a frequency list 
of words and name-classes and then re-tags the 
text using the most frequently used name class 
assigned by the HMM. We have generally found 
that this improves F-score performance by be- 
tween 2 and 4%, both for re-tagging spuriously 
tagged words and for finding untagged words in 
unknown contexts that had been correctly tagged 
elsewhere in the text. 
Table 1 shows the char~ter  features that we 
used in both NEHMM and NE-DT. Our intuition 
is that such features will help the model to find 
similarities between known words that were found 
in the training set and unknown words and so 
overcome the unknown word problem. 
3 Corpora  
We used two corpora in our experiments repre- 
senting two popular domains in IE, molecular- 
biology (from MEDLINE) and newswire texts 
(from MUC-6). These are now described. 
3.1 MUC-6  
The corpus for MUC-6 (MUC, 1995) contains 60 
articles, from the test corpus for the dry and for- 
malruns. An example canbe seenin Figure 1. We 
can see several interesting features of the domain 
such as the focus of NF.,s on people and organiza- 
tion profiles. Moreover we see that there are many 
pre-name clue words such as "Ms." or "Rep." indi- 
cating that a Republican politician's name should 
follow. 
3.2 Biology 
In our tests in the domain of molecular-biology 
we are using abstracts available from PubMed's 
MEDLIhrE. The MEDLINE database is an online 
collection of abstracts for published journal arti- 
cles in biology and medicine and contains more 
than nine million articles. Currently we have ex- 
tracted a subset of MEDLINE based on a search 
using the keywords human AND blood cell AND 
transcription .factor yielding about 3650 abstracts. 
Of these 100 docmnents were NE tagged for our 
experiments using a human domain expert. An 
example of the annotated abstracts is shown in 
Figure 2. In contrast o MUC-6 each article is 
quite short and there are few pre-class clue words 
making the task much more like terminology iden- 
tification and classification than pure name find- 
ing. 
4 A f i r s t  a t tempt  a t  corpus  
compar i son  based  on  s imple  
token  f requency  
A simple and intuitive approach to NE task dif- 
ficulty comparison used in some previous tudies 
such as (palmer and Day, 1997) who studied cor- 
pora in six different languages, compares class to 
term-token ratios on the assumption that rarer 
classes are more difficult to acquire. The relative 
frequency counts from these ratios also give an in- 
direct measure of the granularity of a class, i.e. 
how wide it is. While this is appealing, we show 
that this approach does not necessarily give the 
best metric for comparison. 
Tables 2 and 3 show the ratio of the number of 
different words used in NEs to the total number 
of words in the NE  class vocabulary. The num- 
ber of different tokens is influenced by the corpus 
size and is not a suitable index that can uniformly 
show the difficulty for different NE tasks, there- 
fore it should be normalized. Here we use words 
as tokens. A value close to zero indicates little 
variation within the class and should imply that 
the class is easier to acquire. We see that the NEs 
in the biology domain seem overall to be easier 
to acquire than those in the MUC-6 domain given 
hxical variation. 
The figures in the second columns of Tables 2 
and 3 are normalized so that all numerals are re- 
placed by a single token. It still seems though 
that MUC-6 is a considerably more eheJlenging 
domain than biology. This is despite the fact that 
the ratios for ENAMEX expressions such as Date, 
22 
A graduate of <ENAMEX TYPE=" ORGANIZATION" >Harvard Law SChooI</ENAMEX>, Ms. 
<ENAMEX TYPE="PERSON'>Washington</ENAMEX> worked as a laywer for the corporate fi- 
nance division of the <ENAMEX TYPE='ORGANIZATION~>SEC</ENAMEX> in the late <TIMEX 
TYPE='DATE">1970s</TIMEX>. She has been a congressional staffer since <TIMEX TYPE= 
"DATE'>1979</TIMEX>. Separately, <ENAMEX TYPE='PERSON'>Clintou</ENAMEX> transi- 
tion officials said that <ENAMEX TYPE='PERSON">Frank Newman</ENAMEX>, 50, vice chairman 
and chief financial officer of <ENAMEX TYPE=" ORGANIZATION" >BankAmerica Corp.</ENAMEX>, 
is expected to be nominated as assistant <ENAMEX TYPE="ORGANIZATION~>Treasury</ENAMEX> 
secretary for domestic finance. 
Figure 1: Example sentences taken from the annotated MUC-6 NE text 
<PROTEIN>SOX-4</PROTEIN>, an <PROTEIN>Sty-like HMG box protein</PROTEIN>, is 
a transcriptional activator in <SOLrRCE.cell-type>lymphocytes</SOUl:tCE>. Previous studies in 
<SOURCE.cell-type>lymphocytes</SOUB.CE> have described two DNA-binding <PROTEIN>HMG 
bax proteins</PROTEIN>, <PROTEIN>TCF-I</PROTEIN> and <PROTEIN>LEF-I</PROTEIN>, 
with affinity for the <DNA>A/TA/TCAAAG motif</DNA> found in several <SOURCE.cell-type>T 
cell</SOUl~CE>-specific enhancers. Evaluation of cotransfection experiments in <SOURCE.cell-type>non- 
T cells</SOURCE> and the observed inactivity of an <DNA>AACAAAG concatamer</DNA> in the 
<PROTEIN>TCF-1 </PROTEIN> / <PROTEIN>LEF-1 </PROTEIN>-expressing <SOURCE.cell-line>T 
cell line BW5147</SOURCE>, led us to conclude that these two proteins did not mediate the observed 
enhancer effect. 
Figure 2: Example sentences taken from the annotated biology text 
Table 2: Frequency values for words in the MUC-6 
test corpus 
Class 
Org. 
Person 
Loc. 
Date 
Time 
Money 
Percent 
Al l  
Original 
0.28(=507 / 1783) 
0.45(=381 / 838) 
0.38(=148 / 390) 
0.23(=123 / 542) 
1.00(= 3 / 3) 
0.33(=138 / 423) 
0.39(= 42 / 108) 
0.33(=1342/4087) 
Table 3: Frequency values for words in the biology 
corpus 
Norm. numerals Class Original 
0.28(=507 / 1783) DNA 0.21(=245 / 1140) 
0.45(=381 / 838) Protein 0.15(=631 / 4125) 
0.38(=148 / 390) RNA 0.43(= 30 / 70) 
0.11(= 60 / 542) Source 0.16(=248 / 1533) 
1.00(= 3 / 3) All 0.17(=1'154/6868) 
0.05(= 20 / 423) 
0.03(= 3 / 108) 
0.27(=1122/4087) 
Money and Percent all fall significantly. Expres- 
sions in the Time class are so rare however that it 
is di~cult o make any sort of meaningftfl compar- 
ison. In the biology corpus, the ratios are not sig- 
nificantly changed and the NE classes defined for 
biology documents eem to have the same chuj-- 
acteristics as non-numeric ENAMEX classes in 
MUCC-6 documents. 
Comparing between the biology documents and 
the MUC-6 documents, we may say that identify- 
ing entities in biology docmnents is easier than 
identifying ENAMEX entities in MUC-6 docu- 
ments. 
5 Exper iments  
We evaluated the performance ofour two systems 
using a cross validation method. For the MUC- 
6 corpus, 6-fold cross validation was performed 
on the 60 texts and 5-fold cross validation was 
performed for the 100 texts in the biology corpus. 
Norm. numerals 
0.20(=228 / 1140) 
0.13(=540 / 4125) 
0.43(= 30 / 70) 
0.16(=242 / 1833). 
0.15(=I040/6868) 
We use "F-scores ~for evaluation of our experi- 
ments (Van Rijsbergen, 1979). "F-score" is a mea- 
surement combining "Recall" and "Predsion" and 
defined in Equation 3. "Recall" is the percent- 
age of answers proposed by the system that corre- 
spond to those in the human-made key set. "Pre- 
cision" is the percentage of correct answers among 
the answers proposed by the system. The F-scores 
presented here are automatically calculated using 
a scoring program (Chinchor, 1995). 
2 x Precision x Recall 
F-score = Precision + Recall (3) 
In Table 4 we show the actual performance 
of our term recognition systems, NE-DT and 
NEHMM. We can see that corpus comparisons 
based only on class-token ratios are inadequate o 
explain why both systems' performance was about 
the same in both domains or why NEHMM did 
better in both test corpora than NE-DT. The dif- 
ference in performance is despite there being more 
training examples in biology (3301 NEs) than in 
MUC-6 (2182 NEs). Part of the reason for this is 
97 
Table 4: Performance of the NE systems 
NEHMM with Unity 7&4 75.0 
NEHMM w/o Unity 74.2: 73.1 
NE-DT 68:~-" 69.4 
that the class-token ratios ignore individual sys- 
tem knowledge, i.e. the types of features that 
can be captured and useful in the corpus domain. 
Among other considerations they also fail to con- 
sider the overlap of words and features between 
classes in the same corpus domain. 
6 Corpus  compar i son  based  on  
in fo rmat ion  theoret i ca l  measures  
In this section we attempt o present measures 
that overcome some of the limitations of the class- 
token method. We evaluate tbe contribution from 
each feature used in our NE recognition systems 
by calculating its entropy. There are thee  types of 
feature information used by our two systems: lexo 
ical information, character type information, and 
part-of-speech information. 
The entropy for NE classes H(C) is defined by 
= - E p(c) log 2 p(c) H(C) 
cEC 
where: 
n(O 
p(c) = "N 
n(c): the number of words in class c 
N: the total number of words in text 
We can calculate the entropy for features in the 
same way. 
When a feature F is given, the conditional en- 
tropy for NE classes H(CIF) is defined by 
- ~ ~ p(~, f) logs p(cll) H(C\]F) 
cEC fEF  
where: 
p(c, I) = .(c, I) 
N 
n(c, I) p(cll) = n(l) 
n(c, f):  the number of words in class c 
with the feature value f 
n(/): the number of words 
with the feature value f 
Using these entropies, we can calculate infor- 
mation gain (Breiman et al, 1984) and gain ra- 
tio (Quinlan, 1990). Information gain for NE 
classes and a feature I(C; F) is given as follows: 
I(C; F) = H(C) - H(CIF ) 
The information gain I(C; F) shows how the fea- 
ture F is related with NE classes C. When F is 
completely independent ofC, the value of I(C; F) 
becomes the minimum value O. The maximum 
value of I(C;_F) is equivalent to that of H(C), 
when the feature F gives sufficient information to 
recognize named entities. Information gain can 
also be calculated by: 
I(C; F) = H(C) + H(F) - H(C, F) 
We show the values of the above three entropies 
in Table 5,6, and 7. In these tables, F is replaced 
with single letters which represent each of the 
model's features, i.e. character types (T), part- 
of-speech (P), and hxical information (W). 
Gain ratio is the normalized value of in.forma- 
tion gain. The gain ratio GR(C; F) is defined by 
GR(C; F) = I(C; F) 
H(C) 
The range of the gain ratio GR(C; F) is 0 < 
GR(C; F) _~ 1 even when the class entropy is 
different in various corpora, so we can compare 
the values directly in the different NE recognition 
tasks. 
6.1 Character types 
Character type features are used to identify 
named entities in the MUCC-6 and biology corpus. 
However, the distribution of the character types 
are quite different between these two types of doc- 
uments as we can see in Table 5. We see through 
the gain-ratio score that character type informa- 
tion has a greater predictive power for classes in 
MUC~ than biology due to the higher entropy 
of character type and class sequences in the bi- 
ology corpus, i.e. the greater disorder of this in- 
formation. The result partially shows why iden- 
tification and classification is harder in biological 
documents than in newspaper articles such as the 
MUC-6  corpus. 
6.2  Part-of -speech 
Table 6 shows the entropy scores for part-of- 
speech (POS) sequences in the two corpora. We 
see through the gain ratio scores that POS infor- 
mation is not so powerful for acquiring NEs in the 
biology domain compared to the MUC-6 domain. 
24 
Table 5: Values of Entropy for character type 
Entropy MUC-6 Biology 
H(T) \[\[ 1.880 2.013 
H(C) II 0.890 1.264 
H(C,T) II 2.345 2.974 
I(C;T) \[I .0.425 0.302 
GR(C;T) H 0.478 0.239 
Table 6: Values of Entropy for POSs 
Entropy MUC-6 Biology 
"H(P) 4.287 4.037 
H(C) 0.890 1.264 
H(C,P) 4.750 5.029 
I(C;P) 0.426 0.272 
GR(C;P) 0.479 0.216 
In fact POS information for biology is far less use- 
ful than character information when we compare 
the results in Tables 5 and 6, whereas POS has 
about the same predictive power as character in- 
formation in the MUC-6 domain. One likely ex- 
planation for this is that the POS tagger we use in 
NE-DT is trained on a corpus based on newspaper 
articles, therefore the assigned POS tags are often 
incorrect in biology documents. 
6.3 Lexical information 
Table 7 shows the entropy statistics for the two 
domains. Although entropy for words in biology 
is lower than MUC-6, the entropy for classes is 
higher leading to a lower gain ratio in biology. We 
also note that, as we would expect, in comparison 
to the other two types of knowledge, surface word 
forms are by far the most useful type of knowledge 
with a gain ratio in MUC-6 of 0.897 compared to 
0.479 for POS and 0.478 for character types in the 
same domain. However, such knowledge is also 
the least generalizable and runs the risk of data- 
sparseness. It therefore has to be complemented 
by more generalizable knowledge such as character 
features and POS. 
Table 7: Values of Entropy for words 
--Entropy MUC-6 Biology 
H(W) 9.570 8.89O 
H(C) 0.890 1.264 
H(C,W) 9.662 9.232 
I(C;W) 0.798 0.921 
~R(C;W) 0.897 0.729 
Table 8: Values of Entropy for NEHMM features 
in the MUC-6 corpus 
GR 
0.994 
0.898 
0.967 
0.798 
0.340 
0.806 
0.461 
0.558 
0.221 
0.806 
0.563 
0.971 
0.633 
Cross Entropy 
5.38(4.08-9.68) 
7.69(6.97-9.32) 
7.73(7.07-9.30) 
4.38(4.12-.-4.82) 
1.62(1.32-1.90) 
7.65(7.11-8.65) 
2.64(2.41-2.97) 
7.91(7.25--8.99) 
2.94(2.70-3.25) 
7.65(7.11-6.65) 
7.92(7.26-9.03) 
5.42(4.10-9.70) 
4.18(3.91-4.60) 
Coverage 
o.44(o.34-o.78) 
O. 77(0.72-0.90) 
0.79(0.73-0.90) 
0.99(0.98-1.00) 
L00(1.00-L00) 
0.65(0.81-0.93) 
1.00(0.99-1.00) 
0.83(0.79-0.92) 
1.00(1.00-1.00) 
0.85(0.81,-0.93) 
0.83(0.79-0.92) 
0.44 (0.34-O.75) 
0.99(0.99--1.00) 
Features. 
for A0 
for Al 
for A2 
for As 
Ct-1 
Wt 
Ft 
Wt- I  
F~-x 
Wt Fz 
W~-l F=-i 
Wt-l,~ 
F~-Lt 
Table 
in the biology corpus 
GR Cross Entropy 
0.977 5.83(5.66-6.14) 
0.793 7.93(7.77-8.08) 
0.929 7.79(7.65-7.85) 
0.643 5.07(4.95-5.21) 
0.315 2.26(2.24--2.28) 
0.694 7.64(7.52-7.78) 
0.257 3.12(3.06--3.19) 
0.423 7.99(7.62-8.08) 
0.093 3.33(3.27-3,43) 
0.694 7.64(7.52-7.78) 
0.424 7.98(7.82-8.04) 
0.904 5.96(5.78-6.24) 
0.339 4.66(4.53-4,78) 
9: Values of Entropy for NEHMM features 
Coverage 
0.49(0.48--0.52) 
o.6o(o.79-o.61) 
o.so(o.70-o.81) 
0.98(0.98-0.98) 
1.00(1.00-I.00) 
0.89(0.87-0.89) 
1.oo(1.OO-l.OO) 
0.87(0.86-0.88) 
1.00(1.00-1.00) 
0.89(0.87-0,89) 
o.87(0.85-0.86) 
0.50(0.49-0.52) 
0.99(0.98-0.99) 
Features 
for ~to 
for A1 
for ~t2 
for As 
Ct- I 
W= 
Fe 
Wt  Ft 
Wt-1 F,-z 
Wz-l,t 
F~-l,t 
6.4 Compar i son  between the 
comblnutlon of features 
In this section we show a comparison of gain ra- 
tio for the features used by both systems in each 
corpus. Values of gain ratio for each feature set 
are shown on the 'GR' column in Tables 8, 9, 10 
and 111. The values of GR show that surface 
words have the best contribution in both corpora 
for both systems. We can see that gain ratio for 
all features in NE-DT is actually lower than the 
top level model for NEHMM in biology, reflecting 
the actual system performance that we observed. 
We also see that in the biology corpus, the com- 
bination of all features in NE-DT has a lower con- 
tribution than in the MUC-6  corpus. This indi- 
cates the limitation of the current feature set for 
the biology corpus and shows that we need to uti- 
lize other types of features in this domain. 
Values for cross entropy between training and 
test sets are shown in Tables 8, 9, 10 and 11 to- 
IOn the 'Features' col, mn~ "(Features) for A#" 
means the features used in each HMM sub- 
model which corresponds with the A# in Eclua- 
tion 2. And also, 'ALL' in Tables 10 and 11 
means all the features used in decision tree, i.e. 
{P~-l,~,,+l,F~-l,t,t+l,W,-1,~,~+l). 
Table 10: Values of Entropy for NE-DT features 
in the MUC-6 corpus 
0.G91~8 ! Cross Entropy 
1.59(1.38-1.77) 
0.402 5.22(5.09..-5.32) 
0.4681 2.66(2.51-2.87) 
0.844 7.36(7.19-7.57) 
0.670 7.89(7.81-7.97) 
0.6691 3.87(3.67-4.07) 
0.977 4.42(4.10-4.88) 
0.822 9.25(9.10-9.40) 
0.807 4.92(4.72-5.08) 
0.998 1.89(1.67-2.16) 
Coverage 
0.12(0.10-0.13) 
1.00(0.99-:t.00) 
L00(0.99-1.00) 
o.81(o.8o~.83) 
0.98(0.96--0.98) 
0.99(0.98-1.00) 
0.36(0.34--0.40) 
0.89(0.87~0.91) 
0.96(0.95--0.96) 
0.15(0.13-9.17) 
Features 
ALL 
Pt 
Ft 
Wt 
Pt-l,$ 
Ft- l . t  
Wt--l,t 
Pt-l ,t,t+l 
F?-1.:.~+1 
W~-l.t.t+l 
Table 11: Values of Entropy for NE-DT features 
in the biology corpus 
GR Cross Entropy 
0.937 2.31(2.00-2.50) 
0.23"/ 5.31(5.21-5.38) 
0.262 3.27(3.14-3.41) 
0.416 7.63(7.50-7.79) 
0.370 7.78(7.69.-7.86) 
0.363 4.57(4.38-4.67) 
0.586 5.71(5.37-5.93) 
0.541 8.92(8.82-9.02) 
0.502 5.46(5.26-5.64) 
0.764 2.56(2.25-2.76) 
Coverage Features 
0.18(0.15-0.19) ALL 
1.00(0.99-1.00) P, 
1.00(1.00-1.00) Ft 
0.87(0.85--0.68) wt 
0.97(0.96-0.97) P~-a.= 
0.98(0.98-.0.99) F~-I,~ 
0.48(0.45--0.50) Wt-  s,~ 
0.88(0.87--0.89) Pt-x.~t.t +a 
0.96(0.94--0,96) Ft-l.t.~+a 
0.20(0.17--0.21) Wt_L?,t+t 
gether with error bounds in parentheses. These 
values are calculated for pairs of an NE class and 
features, and averaged for the n-fold experiments. 
In the MUC-6 corpus, 60 texts are separated into 
6 subsets, and one of them is used as the test set 
and the others are put together to form a train- 
ing set. Similarly, 100 texts are separated into 5 
subsets in the biology corpus. We also show the 
coverage of the pairs on the 'Coverage' col,,mn. 
Coverage means that how many pairs which ap- 
peared in a test set alo appear in a trainlug set. 
In these columns, the greater the cross entropy 
between features and a class, the more different 
their occurrences between tr~iuing and test sets. 
On the other hand, as the coverage for class- 
features pairs increases, so does the part of the 
test set that is covered with the given feature set. 
The results in both corpora for both systems 
show a drawback of surface words, since their cov- 
erage for a test set is lower than that of features 
like POSs and character types in both corpora 
Also, the coverage of surface words in the biol- 
ogy corpus is higher than in the MUC6 corpus 
as opposed to other features. The result matches 
our intuition that vocabulary inthe biology corpus 
is relatively restricted but has a variety of types 
other than normal English words. 
7 Conc lus ion  
The need for soundly-motivated metrics to com- 
pare the usefulness of corpora for specific tasks 
and systems is dearly necessary for the develop- 
ment of robust and portable information extrac- 
tion systems. 
In this paper we have shown that measures for 
comparing corpora based just on class-token ratios 
have difficulty predicting system performance and 
cannot adequately explain the difficulty of the NE 
task either generally or for specific systems. 
While we should be cautious in ma~ng sweep- 
ing conclusions due to the small size of corpora in 
our study, our results from gain ratio and cross 
entropy indicate that counts from the features of 
both systems will be more useful in the MUC6 cor- 
pus than in the biology corpus. We can also see 
that while the coverage is limited, surface words 
play a leading role for both systems. Gain ra- 
tio statistics for surface words in the two domains 
were far closer than for any other type of feature, 
and given that this is also the dominant knowl- 
edge type this seems to be one likely reason that 
the performance of systems is about the same in 
both domains. 
We have presented the results of applying two 
supervised learning based models to the named 
entity task in two widely different domains and 
explained the performance through class-token ra- 
tios, entropy and gain ratio. Measures such as 
entropy and gain ratio have been found to have 
the best predictive power, although the features 
used to calculate gain ratio are not sufficient o 
describe all the information that is necessary for 
the named entity task. In future work we intend 
to extend our study to new and larger NE corpora 
in various domains and to try to reduce the error 
factor in our calculations that is a result of corpus 
size. 
Re ferences  
A. Bairoch and 1t. Apweiler. 1997. The SWISS- 
PROT protein sequence data bank and its new 
supplement TrEMBL. Nucleic Acids Research, 
25:31-36. 
D. Bikel, S. Miller, R. Schwartz, and 
11. Wesichedel. 1997. Nymble: a high- 
performance learning name-finder. In Pro- 
ceedings of the Fifth Con/ererenee on Applied 
Natural Language Processing, pages 194--201. 
A. Borthwiek, J. Sterling, E. Agichtein, and 
11. Grishman. 1998. Exploiting diverse knowl- 
edge sources via maximum entropy in named 
entity recognition. In Proceedings of the Work- 
shop on Very Large Corpora (WYLC'98). 
L. Breiman, It. Friedman, A. Olshen, and 
C. Stone. 1984. Classification and regressiwa 
26 
trees. Belmont CA: Wadsworth International 
Group. 
S. Chen and J. Goodman. 1996. An empiri- 
cal study of smoothing techniques for language 
modeling. 3gst Annual Meeting of the Associ- 
ation of Computational Linguistics, California, 
USA, 24-27 3tree. 
N. Chinchor. 1995. MUC-5 evaluation metrics. 
In In Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
Maryland, USA., pages 69-78. 
N. Collier, C. Nobata, and J. Tsujii. 2000. Ex- 
tracting the names of genes and gene products 
with a hidden Markov model. In Proceedings 
of the 18th International Conference on Com- 
putational Linguistics (COLING'2000), Saar- 
bruchen, Germany, July 31st-August 4th. 
M. Craven and J. Kumlien. 1999. Constructing 
biological knowledge bases by extracting infor- 
mation from text sources. In Proceedings ofthe 
7th International Conference on Intelligent Sys- 
temps for Molecular Biology (ISMB-99), Hei- 
delburg, Germany, August 6--10. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Tak- 
ag i. 1998. Toward information extraction: 
identifying protein names from biological pa- 
pers. In Proceedings of the Pacific Symposium 
on Biocomputin9'98 (PSB'98), January. 
MEDLINE. 1999. The PubMed 
database can be found at:. 
http://www.ncbi.nlm.nih.gov/PubMed/. 
DARPA. 1995. Proceedings ofthe Sixth Message 
Understanding Conference(MUC-6), Columbia, 
MD, USA, November. Morgan Kaufmann. 
C. Nobata, N. Collier, and I. Tsujii. 1999. Au- 
tomatic term identification and classification 
in biology texts. In Proceedings of the Nat- 
ural Language Pacific Rim Symposium (NL- 
PRS'gO00), November. 
D. Palmer and D. Day. 1997. A statistical 
profile of the named entity task. In Proceed- 
ings of the Fifth Conference on Applied Natural 
Language Processing (ANLP'97), Washington 
D.C., USA., 31 March - 3 April. 
J.R. Quinlan. 1990. Introduction to Decision 
Trees. In J.W. Shavlik and T.G. Dietterich, ed- 
itors, Readings in Machine Learning. Morgan 
Kauf:marm Publishers, Inc., San Mateo, Cali- 
fornia. 
J.R. Quinlan. 1993. cJ.5 Programs for Machine 
Learning. Morgan Kaufmann Publishers, Inc., 
San Mateo, California. 
L. Rabiner and B. Juang. 1986. An introduction 
to bidden Markov models. 1EEE ASSP Maga- 
zine, pages 4-16, January. 
A. Ratnaparkhi. 1996. A maximum entropy 
model for part-of-speech tagging. In Uon\]er- 
ence on Empirical Methods in Natural Language 
Processing, pages 133-142, University of Penn- 
sylvania, May. 
T. Rindflesch, L. Tanabe, N. Weinstein, and L.. 
Hunter. 2000. EDGAR: Extraction of drugs, 
genes and relations from the biomedical litera- 
ture. In Pacific Symposium on Bio-inforraaties 
(PSB '2000), Hawai 'i, USA, January. 
T. Sekimizu, H. Park, and J. Tsujii. 1998. Iden- 
tifying the interaction between genes and gene 
products based on frequently seen verbs in reed- 
line abstracts. In Genome Informatics. Univer- 
sal Academy Press, Inc. 
Satosbi Sekine, Ralph Grishman, and Hiroyuki 
Sbinnou. 1998. A Decision Tree Method for 
Finding and Classifying Names in Japanese 
Texts. In Proceedings o\] the Sixth Workshop 
on Very Large Corpora, Montreal, Canada, Au- 
gust. 
C. Van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
A. J. Viterbi. 1967. Error bounds for convolutions 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, IT-13(2):260-269. 
E.M. Voorhees and D.K. Harman, editors. 
2000. The Eighth Text REtrieval Confer- 
ence (TREC-8), Electronic version available at 
http://trec.nist.gov/pubs.html. 
Proceedings of the Workshop on BioNLP, pages 162?170,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Bridging the Gap between Domain-Oriented and
Linguistically-Oriented Semantics
Sumire Uematsu Jin-Dong Kim Jun?ich Tsujii
Department of Computer Science
Graduate School of Information Science and Technology
University of Tokyo
7-3-1 Hongo Bunkyo-ku Tokyo 113-0033 Japan
{uematsu,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper compares domain-oriented and
linguistically-oriented semantics, based on the
GENIA event corpus and FrameNet. While
the domain-oriented semantic structures are
direct targets of Text Mining (TM), their ex-
traction from text is not straghtforward due
to the diversity of linguistic expressions. The
extraction of linguistically-oriented semactics
is more straghtforward, and has been studied
independentely of specific domains. In or-
der to find a use of the domain-independent
research achievements for TM, we aim at
linking classes of the two types of seman-
tics. The classes were connected by analyz-
ing linguistically-oriented semantics of the ex-
pressions that mention one biological class.
With the obtained relationship between the
classes, we discuss a link between TM and
linguistically-oriented semantics.
1 Introduction
This paper compares the linguistically-oriented and
domain-oriented semantics of the GENIA event cor-
pus, and suggests a factor for utilizing NLP tech-
niques for Text Mining (TM) in the bio-medical do-
main.
The increasing number of scientific articles in the
bio-medical domain has contributed in drawing con-
siderable attention to NLP-based TM. An impor-
tant step in NLP-based TM is obtaining the domain-
oriented semantics of sentences, as shown at the bot-
tom of figure 1. The BioInfer (Pyysalo et al, 2007)
and the GENIA event corpus (Kim et al, 2008) pro-
vide annotations of such semantic structures on col-
lections of bio-medical articles. Domain-oriented
semantic structures are valuable assets because their
representation suits information needs in the do-
main; however, the extraction of such structures is
difficult due to the large gap between the text and
these structures.
On the other hand, the extraction of linguistically-
oriented semantics from text has long been studied
in computational linguistics, and has recently been
formalized as Semantic Role Labeling (Gildea and
Jurafsky, 2002), and semantic structure extraction
(Baker et al, 2007)(Surdeanu et al, 2008). Seman-
tic structures in such tasks are exemplified in the
middle of figure 1. The linguistically-oriented se-
mantic structures are easier to extract, although the
information is not practical to the domain.
We aim at relating linguistically-oriented frames
of semantics with domain-oriented classes, thus
making a step forward in utilizing the computa-
tional linguistic resources for the bio-medical TM.
Of all the differences in the two type of seman-
tics, we focused on the fact that the former frames
are more sensitive to the perspective imposed by
the sentence writer. In the right hand-side exam-
ple of figure 1, the linguistically-oriented structure
treats PBMC, a cell entity, as an agent; however the
bio-medical structure reflects the scientific view that
there are no agents, objects acting with intention, in
bio-molecular phenomena.
As a preliminary investigation, we selected
four representative classes of bio-molecular phe-
nomena; Localization, Binding, Cell adhesion,
and Gene expression, and investigated domain-
oriented annotations for the classes in the GENIA
162
?, whereas in many other cell types, NF-kappa B TRANSLOCATES from cytosol to nucleus as a result of ?
?, both C3a and C3a(desArg) were found to enhance IL-6 RELEASE by PBMC in a dose-dependent manner.
Natural?language
FrameNet?expression?(Linguis?ally?oriented?seman?s) 
Class:?????Mo?n?Theme:?NF?kappa?B?Source:?from?cytosol?Goal:?????to?nucleus?
Class:????Releasing?Theme:?IL?6?Agent:???PBMC?
GENIA?expression?(Biologically?oriented?seman?s) 
Class:???????Localiza?n?Theme:????NF?kappa?B?FromLoc:?cytosol?ToLoc:??????nucleus?
Theme:????IL?6?FromLoc:?(inside?of)?PMBC?ToLoc:??????(outside?of)?PMBC?
Figure 1: A comparison of the linguistically-oriented and biologically-
oriented structure of semantics
event corpus. Expressions mentioning the four
classes were examined and manually classified into
linguistically-oriented frames, represented by those
defined in FrameNet (Baker et al, 1998). FN frames
associated to a bio-molecular event class constitute a
list of possible perspectives in mentioning phenom-
ena of the class.
The rest of this paper is structured in the fol-
lowing way: Section 2 reviews the existing work
on semantic structures and expression varieties in
the bio-medical domain, and provides a compari-
son to our work. In section 3, we describe the GE-
NIA event corpus, and the FrameNet frames used as
linguistically-oriented classes in our investigation.
Sections 4 and 5 explain the methods and results of
the corpus investigation; in particular the sections in-
vestigate how the linguistic frames were associated
to the domain-oriented classes of semantics. Finally,
we provide discussion and conclusion in section 6
and 7.
2 Related Work
Existing work on semantics approached domain-
oriented semantic structures from linguistically-
oriented semantics. In contrast, our approach uses
domain-oriented semantics to find the linguistic se-
mantics that represent them. We believe that the two
different approaches could complement each other.
The PASbio(Wattarujeekrit et al, 2004) pro-
poses Predicate Argument Structures (PASs), a type
of linguistically-oriented semantic structures, for
domain-specific lexical items, based on PASs de-
fined in PropBank(Wattarujeekrit et al, 2004) and
NomBank(Meyers et al, 2004). The PASs are de-
fined per lexical item, and is therefore distinct from a
biologically-oriented representation of events. (Co-
hen et al, 2008) investigated syntactic alternations
of verbs and their nominalized forms which oc-
curred in the PennBioIE corpus(Kulick et al, 2004),
whilst keeping PASs of the PASBio in their minds.
The BioFrameNet(Dolbey et al, 2006) is an at-
tempt to extend the FrameNet with specific frames
to the bio-medical domain, and to apply the frames
to corpus annotation. Our attempts were similar, in
that both were: 1) utilizing the FN frames or their
extensions to classify mentions of biological events,
and 2) relating the frames and the FEs (roles of par-
ticipants) with classes in domain ontologies; e.g. the
Gene Ontology(Ashburner et al, 2000).
As far as the authors know, it is the first at-
tempt to explicitly address the problem of linking
linguistically-oriented and domain-oriented frames
of semantics. However, it has been indirectly stud-
ied through works on TM or Relation Extraction
using linguistically-oriented semantic structures as
features, such as in the case with (Harabagiu et al,
2005).
3 Corpora
?We used domain-oriented annotations of the GE-
NIA event corpus and linguistically-oriented frames
defined in FrameNet (FN), to link domain-oriented
and linguistically-oriented frames of semantics. We
briefly describe these resources next.
163
Mo?n 
Releasing
Ge?g 
A?ching 
Being_located
Becoming
Event StateGENIA?event
Biological_process
Viral_life_cycle
Cellular_process
Physiological_process
Cell_adhesion
Cell_communica?n 
Localiza?n 
Binding
Metabolism
DNA_metabolism
Gene_expression
Crea?g 
Being_a?ched 
Figure 2: The resulting relationship between linguistically-oriented and
biologically-oriented frames.
The GENIA event corpus consists of 1,000 Med-
line abstracts; that is, 9,372 sentences annotated
with domain-oriented semantic structures. The an-
notation was completed for all mentions of biolog-
ical events, and resulted in 6,114 identified events.
Examples of annotated event structures are shown at
the bottom of figure 1. Each structure has attributes
type and themes, which respectively show the bio-
logical class of the mentioned event and phrases ex-
pressing the event participants. The event classes are
defined based on the terms in the Gene Ontology.
For example, the Localization class in the GENIA
event corpus is defined as an equivalent of the GO
term Localization (GO0051179). The event classi-
fications used in the corpus are depicted in the left
hand-side of figure 2. Arrows in the figure depict
the inheritance relations defined in the GENIA event
ontology. For instance, the Localization class is de-
fined as a type of Physiological process. Each of
the annotated structures has additional attributes that
point phrases that the annotator of the structure used
as a clue. Among the attributes, the clueType at-
tribute shows a clue phrase to the event class. In our
investigation, the attribute was treated as a predicate,
or an equivalent of the lexical unit in the FN.
FN is a network of frames that are are
linguistically-oriented classifications of semantics.
A FN frame is defined as ?a script-like conceptual
structure that describes a particular type of situation,
object, or event and the participants and proposi-
tions involved in it,? and is associated with words,
or lexical units, evoking the frame. For instance, the
verbs move, go and fly are lexical units of the Mo-
tion frame, and they share the same semantic struc-
ture. Each FN frame has annotation examples form-
ing an attestation of semantic overlap between the
lexical units. Additionally, FN defines several types
of frame-frame relations; e.g. inheritance, prece-
dence, subframe, etc. The right hand-side of figure
2 shows some FN frames and inheritance relation-
ships between them. The FN provides linguistically-
oriented classifications of event mentions based on
surface expressions, and also shows abstract rela-
tions between the frames.
4 Additional Annotation
Our aim is to link linguistically-oriented and
domain-oriented frames of the bio-medical text?s se-
mantics. A major problem in this task was that there
were no annotated corpora with both types of se-
mantic structures. Therefore, we decided to concen-
trate on the mentions of a few classes of biological
phenomena, and to annotate samples of the mentions
with linguistically-oriented structures conforming to
164
Freq. Keyword Frame
693 binding Attaching
247 bind Attaching
125 interaction Attaching, Being attached
120 complex ?
99 bound Attaching, Being attached
91 interact Attaching, Being attached
61 form Becoming
52 crosslink Attaching
46 formation Becoming
Table 1: The most frequent keywords of the Binding class,
mentioned 2,006 times in total.
Freq. Keyword Frame
131 translocation Motion
81 secretion Releasing
75 release Releasing
32 secrete Releasing
25 mobilization Motion
23 localization Being located
20 uptake Getting
18 translocate Motion
15 expression Creating
9 present Being located
Table 2: The most frequent keywords of the Localization
class, mentioned 582 times in total.
the FrameNet annotations.
The following provides the annotation proce-
dures. First, we collected linguistic expressions that
mention each of the selected GENIA event classes
from the GENIA event corpus. We then sampled
and annotated them with their linguistically-oriented
semantics which conformed to the FrameNet.
4.1 Target Classes and Keywords
We concentrated mainly on the mentions of four GE-
NIA classes; Localization, Binding, Cell adhesion,
and Gene expression. Gene expression, Binding,
and Localization are three of the most frequent four
classes in the GENIA event corpus.1 Binding and
Localization are the two most primitive molecular
events. The Cell adhesion class was included as a
comparison for the Binding class.
Counting keywords for mentioning events was
close to automatic. We extracted phrases pointed
by a clueType attribute from each event structure.
We then tokenized the phrases, performed a simple
stemming on the tokens, and counted the resulting
words. The stemming process simply replaced each
inflected word to its stem by consulting a small list
of inflected words with their stems. Manual work
was only used in making the small list.
4.2 FN Annotation
A major challenge encountered in annotating a sam-
pled expression with a semantic structure conform-
ing to FN, was in the assignment of a FN frame to
1Except correlation and regulation classes which express re-
lational information rather than events.
the mention. Our decision was based on the follow-
ing four points: 1) keywords used in the mention, 2)
description of FN frames, 3) syntactic positions of
the event participants, and 4) frame-frame relations.
The first indicates that a FN frame became a can-
didate frame for the mention, if the keyword in the
mention is a lexical unit of the FN frame. FN frames
and their lexical units could be easily checked by
consulting the FN dictionary. If there were no en-
tries for the keyword in the dictionary, synonyms or
words in the keyword?s definition were used. For ex-
ample, the verb translocate has no entries in the FN
dictionary, and the frames for verbs such as move
were used instead.
For the second point, we discarded FN frames that
are either evoked by a completely different sense of
the keyword, or too specific of a non-biological sit-
uations.
Before we assigned a FN frame to each mention,
we manually examined the syntactic positions of all
event participants present in the sampled GENIA
mentions. Combinations of the syntactic position
and event participants observed for a keyword were
compared with sample annotations of the candidate
FN frames.
We checked frame-frame relations between the
candidate frames, because they can be regarded
as evidence that shows that the conception of the
frames is related. For our aim, it was sufficient to
choose a set of frames that best describes the differ-
ent perspectives for mentioning one type of molecu-
lar phenomena. Even when some keywords seemed
to be dissimilar in the three points mentioned above,
165
Freq. Keyword Frame
98 adhesion Being attached
19 adherence Being attached
16 interaction Being attached, Attaching
15 binding Attaching
8 adherent Being attached
Table 3: The most frequent keywords of the Cell adhesion
class, mentioned 193 times in total.
Freq. Keyword Frame
1513 expression Creating
357 express Creating
239 production Creating
71 overexpression Creating
69 produce Creating
62 synthesis Creating
Table 4: The most frequent keywords of the
Gene expression class, mentioned 2,769 times in
total.
a single frame could be assigned to them if it was
quite clear that they shared a similar perspective.
The frame-frame relations provided in the FN were
treated as clues to the similarity.
Keywords frequently used in each event class are
listed in tables 1, 2, 3, and 4, with the final assign-
ment of FN frames to each keyword.
5 Analysis
After the linguistic annotation was performed, we
compared the GENIA event structure and the frame
structure of each sampled expression, and obtained
relations of the GENIA class-FN frame and GE-
NIA slot-FN participant. The resulting relationships
between FN frames and the four GENIA classes
demonstrate a gap between linguistically-oriented
and domain-oriented classification of events, as
shown in figure 2.
The relations can be explained by decomposing it
into two cases: 1) 1-to-n mappings, and 2) n-to-1
mappings. The n-to-n mapping from GENIA to FN
can then be regarded as a mix of the two cases. In
the following sections, the two cases are described
in detail. Further, we show conversion examples of
a FN structure to a GENIA event structure, which
were supported by the obtained GENIA participant-
FN participant relations.
5.1 1-to-N Mapping: Different Perspectives on
the Same Phenomena
A 1-to-n mapping from GENIA to FN can be ex-
plained as the case where the same molecular phe-
nomena are expressed from different perspectives.
5.1.1 Binding Expressed in Multiple frames
The Binding class in GENIA is defined as
?the selective, often stoichiometric interaction of a
molecule with one or more specific sites on an-
other molecule.? We associated the class with three
frames, and two frames of the three, Attaching and
Becoming frames, represent different perspectives
for mentioning the class. The Being attached frame
shares the same conception as Attaching, but ex-
presses states instead of events. See table 1 for key-
words of the class, and the frames assigned to the
words.
Attaching: In the perspective represented by this
frame, a binding phenomenon was recognized as a
event in which protein molecules were simply at-
tached to one another.
[The 3?-CAGGTG E-boxItem] could BIND
[USF proteinsGoal], ? ? ?
(PubMed ID 10037751, Event IDs E11, E12, E13)
Becoming: In the perspective represented by this
frame, a product of a binding event was treated, on
the surface, as a different entity from the original
parts.
When activated, [glucocorticoid recep-
torsEntity] FORM [a dimerFinal category] ? ? ?
(PubMed ID 10191934, Event ID E5)
This type of expression was possible because a prod-
uct of a binding often obtains a different function-
ality, and can be treated as a different type of en-
tity. Note that this frame was not associated with the
Cell adhesion class described in section 5.2.
166
A CB?
Figure 3: A schematic figure of translocation.
Being attached: Annotators recognized a protein
binding event from the sentence below, which basi-
cally mentions a state of the NF-kB.
In T cells and T cell lines, [NF-kBItem]
is BOUND [to a cytoplasmic proteic in-
hibitor, the IkBGoal].
(PubMed ID 1958222, Event ID E2, E102)
Although this type of expression shares a similar
point of view with the Attaching frame, we classi-
fied these expressions into the Being attached frame
in order to demonstrate cases in which a prerequisite
Binding event was inferred from a state.
5.1.2 Translocation Expressed in Multiple
Frames
The Localization class in the GENIA corpus is de-
fined as a class for ?any process by which a cell, a
substance, or a cellular entity, such as a protein com-
plex or organelle, is transported to, and/or main-
tained in a specific location.? Sampled expressions
of the class separated into mentions of a process, by
which an entity was transported to a specific loca-
tion, and those of the process in which an entity was
maintained in a specific location. We concentrate on
the former in this section, and describe the latter in
section 5.1.3.
We associated the frames: Motion, Releasing and
Getting with what we call translocation events, or
Localization events in which an entity was trans-
ported to a specific location. Figure 3 provides a
schematic representation of a translocation event.
Each of the three frames had a different perspective
in expressing the translocations. See table 2 for key-
words of the frames.
Motion: This group consists of expressions cen-
tered on the translocated entities of the translocation
- namely, B in the figure 3.
[NK cell NFATTheme] ? ? ? MIGRATES [to
the nucleusGoal] upon stimulation,? ? ?
(PubMed ID 7650486, Event ID E33)
Activation of T lymphocytes ? ? ? results
in TRANSLOCATION [of the transcrip-
tion factors NF-kappa B, AP-1, NFAT, and
STATTheme] [from the cytoplasmSource] [into
the nucleusGoal].
(PubMed ID 9834092, Event ID E67)
These expressions are similar to those of the Motion
frame in the FN.
[Her footTheme] MOVED [from the
brakeSource] [to the acceleratorGoal] and the
car glided forward.
Releasing: This group consists of expressions
centered on a starting point of the translocation -
namely, A in the figure 3.
In [unstimulated cells whichAgent] do not
SECRETE [IL-2Theme], only Sp1 binds to
this region, ? ? ?
(PubMed ID 7673240, Event ID E13)
Activation of NF-kappaB is thought to
be required for [cytokineTheme] RELEASE
[from LPS-responsive cellsAgent], ? ? ?
(PubMed ID 1007564, Event ID E14)
The verbal keywords occurred as a transitive in
most cases, and had subjects and objects that ex-
pressed starting points and entities in the transloca-
tions. This is a typical syntactic pattern of the Re-
leasing frame, if we regarded an Agent in the FN as
a starting point of the movement of a Theme.
[The policeAgent] RELEASED [the sus-
pectTheme].
Getting: This group consists of expressions cen-
tered on a goal point of the translocation - namely,
C in figure 3. We assumed that this group has an
opposite point of view from the Releasing frame.
The noun uptake was found to be a keyword in this
group.
The integral membrane ? ? ? appears to play
a physiological role in binding and UP-
TAKE [of Ox LDLTheme] [by monocyte-
macrophagesRecipient], ? ? ?
(PubMed ID 9285527, Event ID E10)
167
To summarize, we observed three groups of ex-
pressions that mention translocation events, and
each group represented different perspectives to
mention the events. Each of the groups and the as-
sociated frame seemed similar, in that they shared
similar keywords and possible syntactic positions to
express the event participant.
5.1.3 Localization excluding Translocation
Expressed in Multiple Frames
Localization events excluding translocations were
expressed in the Being located and Creating frames.
Being located: This group consists of expressions
that simply mention an entity in a specific location.
? ? ? [recombinant NFAT1Theme] LOCAL-
IZES [in the cytoplasm of transiently
transfected T cellsLocation] ? ? ?
(PubMed ID 8668213, Event ID E23)
Creating: A noun expression was observed to be
used by instances mentioning the presence of pro-
teins.
horbol esters are required to induce
[AIM/CD69Created entity] Cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
Expressions in these cases indicate an abbrevi-
ation for gene expression, which is a event of
Gene expression class. This type of overlap be-
tween the Localization and Gene expression is ex-
plained in section 5.2.2
5.2 N-to-1 Mapping: Same Conception for
Different Molecular Phenomenon
In contrast to the cases described in section 5.1, the
same conception could be applied to different bio-
logical phenomena.
5.2.1 Shared Conception for Binding and
Cell adhesion
Molecular events classified into Binding and
Cell adhesion shared the conception that two enti-
ties were attached to each other. However, types of
the entities involved are different. They are: the pro-
tein molecule in Binding, and cell in Cell adhesion.
CD36 is a cell surface glycoprotein
? ? ?, which INTERACTS with throm-
bospondin, ? ? ?, and erythrocytes para-
sitized with Plasmodium falciparum.
In the sentence above, an event involving a cell sur-
face glycoprotein and thrombospondin was recog-
nized as a Binding, whereas an event involving a cell
surface glycoprotein and erythrocytes was classified
as a Cell adhesion event.
5.2.2 Shared Expressions of Localization and
Gene expression
Both Localization and Gene expression classes
are connected with the Creating frame. Some
Localization events have a dependency on the
Gene expression event. Protein molecules are made
in events classified into the Gene expression class.
[Th1 cellsCreator] PRODUCE [IL-2 and
IFN-gammaCreated entity], ? ? ?
(PubMed ID 10226884, Event ID E11, E12)
The molecules are then translocated somewhere.
Consequently, localized protein molecules might in-
dicate a Gene expression event, and a phrase ?pro-
tein expression? was occasionally recognized as
mentioning a Localization.
horbol esters are required to induce
[AIM/CD69Created entity] cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
5.3 Conversion of FN Structures to GENIA
Events
During the investigation, we compared participant
slots of GENIA and FN structures, in addition to the
structures themselves. Figures 4 and 5 depict con-
version examples from a FN structure and its par-
ticipants to a GENIA structure, with the domain-
oriented type of each participant entity. The conver-
sions were supported by samples, and need quanti-
tative evaluation.
6 Discussion
By annotating sentences of the GENIA event corpus
with semantic structures conforming to FrameNet,
we explicitly compared linguistically-oriented and
168
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 4: FN-to-GENIA conversions for Binding
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 5: FN-to-GENIA conversions for Localization.
domain-oriented semantics of the bio-molecular ar-
ticles. Our preliminary result illustrates the gap be-
tween the two type of semantics, and a relationship
between them. We discuss development of a Text
Mining (TM) system, in association with the extrac-
tion of linguistically-oriented semantics, which has
been studied independently of TM.
First, our result would show that TM involves at
least two qualitatively different tasks. One task is
related to our results; that is, recognizing equiva-
lent events which are expressed from different per-
spectives, and hence expressed by using different
linguistic frames, and at the same time distinguish-
ing event mentions which share the same linguistic
frame but belong to different domain classes. Our
investigation indicates that this task is mainly depen-
dent on domain knowledge and how a phenomenon
can be conceptualized. Another task of TM is the ex-
traction of linguistically-oriented semantics, which
basically maps various syntactic realizations to the
shared structures. In order to develop a TM system,
we need to solve the two difficult tasks.
Second, TM could benefit from linguistically-
oriented frames by using them as an intermediat-
ing layer between text and domain-oriented infor-
mation. The domain-oriented semantic structures,
which is a target of TM, are inevitably dependent
on the domain. On the other hand, the extraction of
linguistically-oriented semantics from text is less de-
pendent. Therefore, using the linguistically-oriented
structure could be favorable to domain portability of
a TM system.
Our aim was explicitly linking linguistically-
oriented and domain-oriented semantics of the bio-
molecular articles, and the preliminary result show
the possibility of the extraction of linguistically-
oriented semantics contributing to TM. Further in-
v tigation of the relationship would be a important
step forward for TM in the bio-molecular domain.
Our investigation was preliminary. For exam-
ple, conversions from FN structures to GENIA event
structures, depicted in figures 4 and 5, were based
on manual investigation. Further, they were attested
by limited samples in the corpus. For our results to
contribute to a TM system, evaluation of the conver-
sions and automatic extraction of such conversions
must be considered.
7 Conclusion
This paper presents a relationship of domain-
oriented and linguistically-oriented frames of se-
mantics, obtained by an investigation of the GE-
NIA event corpus. In the investigation, we anno-
tated sample sentences from the GENIA event cor-
pus with linguistically-oriented semantic structures
as those of FrameNet, and compared them with
domain-oriented semantic annotations that the cor-
pus originally possesses. The resulting relations
between the domain-oriented and linguistically-
oriented frames suggest that mentions of a bio-
logical phenomenon could be realized in a num-
ber of linguistically-oriented frames, and that
the linguistically-oriented frames represent possible
perspectives in mentioning the phenomenon. The
resulting relations would illustrate a challenge in
developing a Text Mining system, and would indi-
cate importance of linguistically-oriented frames as
an intermediating layer between text and domain-
oriented information. Our future plan includes
evaluation of our conversions from a linguistically-
oriented to a domain-oriented structure, and auto-
matic extraction of such conversions.
169
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein,
H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. The Gene Ontology Consortium. Nat Genet,
25(1):25?29, May.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
Semeval-2007 task 19: Frame semantic structure ex-
traction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158, 09.
Andrew Dolbey, Michael Ellsworth, and Jan Scheffczyk.
2006. Bioframenet: A domain-specific framenet
extension with links to biomedical ontologies. In
Proceedings of the Second International Workshop
on Formal Biomedical Knowledge Representation:
?Biomedical Ontology in Action? (KR-MED 2006),
volume 222 of CEUR Workshop Proceedings. CEUR-
WS.org, Nov.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda M. Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-05, Proceedings of the Nine-
teenth International Joint Conference on Artificial In-
telligence, pages 1061?1066.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1):10.
Seth Kulick, Ann Bies, Mark Liberman, Mark Man-
del, Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Lynette Hirschman and James Puste-
jovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontolo-
gies and Databases, pages 61?68, Boston, Mas-
sachusetts, USA, May 6. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1):50.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
Tuangthong Wattarujeekrit, Parantu Shah, and Nigel Col-
lier. 2004. Pasbio: predicate-argument structures for
event extraction in molecular biology. BMC Bioinfor-
matics, 5(1):155.
170
Proceedings of the Workshop on BioNLP: Shared Task, pages 103?106,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
From Protein-Protein Interaction to Molecular Event Extraction
Rune S?tre?, Makoto Miwa?, Kazuhiro Yoshida? and Jun?ichi Tsujii?
{rune.saetre,mmiwa,kyoshida,tsujii}@is.s.u-tokyo.ac.jp
?Department of Computer Science
?Information Technology Center
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
Abstract
This document describes the methods and re-
sults for our participation in the BioNLP?09
Shared Task #1 on Event Extraction. It also
contains some error analysis and a brief dis-
cussion of the results. Previous shared tasks in
the BioNLP community have focused on ex-
tracting gene and protein names, and on find-
ing (direct) protein-protein interactions (PPI).
This year?s task was slightly different, since
the protein names were already manually an-
notated in the text. The new challenge was
to extract biological events involving these
given gene and gene products. We modi-
fied a publicly available system (AkanePPI)
to apply it to this new, but similar, protein
interaction task. AkanePPI has previously
achieved state-of-the-art performance on all
existing public PPI corpora, and only small
changes were needed to achieve competitive
results on this event extraction task. Our of-
ficial result was an F-score of 36.9%, which
was ranked as number six among submissions
from 24 different groups. We later balanced
the recall/precision by including more predic-
tions than just the most confident one in am-
biguous cases, and this raised the F-score on
the test-set to 42.6%. The new Akane program
can be used freely for academic purposes.
1 Introduction
With the increasing number of publications report-
ing on protein interactions, there is also a steadily
increasing interest in extracting information from
Biomedical articles by using Natural Language Pro-
cessing (BioNLP). There has been several shared
tasks arranged by the BioNLP community to com-
pare different ways of doing such Information Ex-
traction (IE), as reviewed in Krallinger et al(2008).
Earlier shared tasks have dealt with Protein-
Protein Interaction (PPI) in general, but this
task focuses on more specific molecular events,
such as Gene expression, Transcription, Pro-
tein catabolism, Localization and Binding, plus
(Positive or Negative) Regulation of proteins or
other events. Most of these events are related to PPI,
so our hypothesis was that one of the best perform-
ing PPI systems would perform well also on this
new event extraction task. We decided to modify a
publicly available system with flexible configuration
scripting (Miwa et al, 2008). Some adjustments had
to be made to the existing system, like adding new
types of Named Entities (NE) to represent the events
mentioned above. The modified AkaneRE (for Re-
lation Extraction) can be freely used in academia1.
2 Material and Methods
The event extraction system is implemented in a
pipeline fashion (Fig. 1).
2.1 Tokenization and Sentence Boundary
Detection
The text was split into single sentences by a sim-
ple sentence detection program, and then each sen-
tence was split into words (tokens). The tokeniza-
tion was done by using white-space as the token-
separator, but since all protein names are known dur-
ing both training and testing, some extra tokeniza-
tion rules were applied. For example, the protein
1http://www-tsujii.is.s.u-tokyo.ac.jp/?satre/akane/
103
Recursive Template  
Output
POS tagging
Parsing
(Enju & GDep)
Event Clueword 
Recognition
Event Template 
Extraction
Machine 
Learning (ML)
Training Data
ML Filtering
POS tagging
Event Clueword 
Recognition
Event Template 
Filling
Test Data
Models with 
Templates
Parsing
(Enju & GDep)
Tokenization Tokenization
Figure 1: System Overview
name ?T cell factor 1? is treated as a single token,
?T cell factor 1?, and composite tokens including a
protein name, like ?(T cell factor 1)?, are split into
several tokens, like ?(?, ?T cell factor 1? and ?)?, by
adding space around all given protein names. Also,
punctuation (commas, periods etc.) were treated as
separate tokens.
2.2 POS-tagging and Parsing
We used Enju2 and GDep3 to parse the text. These
parsers have their own built-in Part-of-Speech (POS)
taggers, and Enju also provides a normalized lemma
form for each token.
2.3 Event Clue-word tagging
Event clue-word detection was performed by a Ma-
chine Learning (ML) sequence labeling program.
This named-entity tagger program is based on a first
order Maximum Entropy Markov Model (MEMM)
and is described in Yoshida and Tsujii (2007). The
clue-word annotation of the shared-task training set
was converted into BIO format, and used to train the
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
3http://www.cs.cmu.edu/?sagae/parser/gdep/
MEMM model. The features used in the MEMM
model was extracted from surface strings and POS
information of the words corresponding to (or ad-
jacent to) the target BIO tags. The clue-word tag-
ger was applied to the development and test sets to
obtain the marginal probability that each word is a
clue-word of a certain category. The probabilities
were obtained by marginalizing the n-best output of
the MEMM tagger. We later also created clue-word
probability annotation of the training set, to enable
the template extraction program to access clue-word
probability information in the training phase.
2.4 Event Template Extraction
The training data was used to determine which
events to extract. As input to the system, a list of
Named Entity (NE) types and the Roles they can
play were provided. The roles can be thought of as
slots for arguments in event-frames, and in this task
the roles were Event (clue), Theme and Cause. In
the original AkanePPI (based on the AIMed corpus),
the only NE type was Protein, and the only role was
Theme (p1 and p2). All the (PPI) events were pair-
wise interactions, and there was no explicit event-
clue role. This means that all the events could be
represented with the single template shown first in
Table 1.
The BioNLP shared task used eight other NE
types, in addition to manually annotated Proteins,
namely Binding, Gene expression, Localization,
Protein catabolism, Transcription, Regulation, Pos-
itive Regulation and Negative Regulation. The first
five events have only Theme slots, which can only
be filled by Proteins, while the last three regulation
events are very diverse. They also have one Theme
slot, but they can have a Cause slot as well, and each
role/slot can be filled with either Proteins, or other
Events. See the first half of Table 1.
148 templates were extracted and clustered into
nine homogeneous groups which were classified
as nine separate sub-problems. The grouping was
based on whether the templates had an Event or a
Protein in the same role-positions. This way of orga-
nizing the groups was motivated by the fact that the
Proteins are 100% certain, while the accuracy of the
clue-word recognizer is only around 50% (estimated
on the training data). The bottom of Table 1 shows
the resulting nine general interaction templates.
104
2.5 Machine Learning with Maximum Entropy
Models
We integrated Maximum Entropy (ME) modeling,
also known as Logistic Regression, into AkaneRE.
This was done by using LIBLINEAR4, which han-
dles multi-class learning and prediction. Gold tem-
plates were extracted during training, and each tem-
plate was matched with all legal combinations of
Named Entities (including gold proteins/clue-words
and other recognized clue-word candidates) in each
sentence. The positive training examples were la-
beled as gold members of the template, and all other
combinations matching a given template were la-
beled as negative examples within that specific tem-
plate class. The templates were grouped into the
nine general templates shown in the bottom of Ta-
ble 1. Using one-vs-rest logistic regression, we
trained one multi-class classifier for each of the nine
groups individually. The ML features are shown in
Table 2.
In the test-phase, we extracted and labeled all re-
lation candidates matching all the templates from the
training-phase. The ML component was automati-
cally run independently for each of the nine groups
listed in the bottom of Table 1. Each time, all the
candidate template-instances in the current group
were assigned a confidence score by the classifier for
that group. This score is the probability that a can-
didate is a true relation, and a value above a certain
threshold means that the extracted relation will be
predicted as a true member of its specific template.
LIBLINEAR?s C-value parameter and the prediction
threshold were selected by hand to produce a good
F-score (according to the strict matching criterion)
on the development-test set.
2.6 Filtering and recursive output of the most
confident template instances
After machine learning, all the template instances
were filtered based on their confidence score. Af-
ter tuning the threshold to the development test-set,
we ended up using 1 as our C-value, and 3.5% as
our confidence threshold. Because the prediction
of Regulation Events were done independent from
the sub-events (or proteins) affected by that event,
some sub-events had to be included for complete-
4http://www.csie.ntu.edu.tw/?cjlin/liblinear/
ness, even if their confidence score was below the
threshold.
3 Results and Discussion
Our final official result was an F-score of 36.9%,
which was ranked as number six among the sub-
missions from 24 different groups. This means that
the AkanePPI system can achieve good results when
used on other PPI-related relation-extraction tasks,
such as this first BioNLP event recognition shared
task. The most common error was in predicting reg-
ulation events with other events as Theme or Cause.
The problem is that these events involve more than
one occurrence of event-trigger words, so the perfor-
mance is more negatively affected by our imperfect
clue-word detection system.
Since the recall was much lower on the test-set
than on the development test-set, we later allowed
the system to predict multiple confident alternatives
for a single event-word, and this raised our score on
the test-set from 36.9% to 42.6%. In hindsight, this
is obvious since there are many such examples in
the training data: E.g. ?over-express? is both posi-
tive regulation and Gene expression. The new sys-
tem, named AkaneRE (for Relation Extraction), can
be used freely for academic purposes.
As future work, we believe a closer integration
between the clue-word recognition and the template
prediction modules can lead to better performance.
Acknowledgments
?Grant-in-Aid for Specially Promoted Research?
and ?Genome Network Project?, MEXT, Japan.
References
Martin Krallinger et al 2008. Evaluation of text-mining
systems for biology: overview of the second biocre-
ative community challenge. Genome Biology, 9(S2).
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining multi-
ple layers of syntactic information for protein-protein
interaction extraction. In Proceedings of SMBM 2008,
pages 101?108, Turku, Finland, September.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of the Workshop on BioNLP 2007, June. Prague,
Czech Republic.
105
Freq Event Theme1 Theme2 Theme3 Theme4 Cause
- PPI Protein Protein
613 Binding Protein
213 Binding Protein Protein
3 Binding Protein Protein Protein
2 Binding Protein Protein Protein Protein
217 Regulation Protein Protein
12 Regulation Binding Protein
48 +Regulation Transcription Protein
4 +Regulation Phosphorylation Binding
5 -Regulation +Regulation Protein
... ... ... ...
Total 148 Templates
Count General Templates Theme1 Theme2 Theme3 Theme4 Cause
9 event templates Protein
1 event template Protein Protein
1 event template Protein Protein Protein
1 event template Protein Protein Protein Protein
3 event templates Protein Protein
12 event templates Protein Event
27 event templates Event
26 event templates Event Protein
68 event templates Event Event
Table 1: Interaction Templates from the training-set. Classic PPI at the top, compared to Binding and Regulation
events in the middle. 148 different templates were automatically extracted from the training data by AkaneRE. At
the bottom, the Generalized Interaction Templates are shown, with proteins distinguished from other Named Entities
(Events)
Feature Example
Text The binding of the most prominent factor, named TCF-1 ( T cell factor 1 ),
is correlated with the proto-enhancer activity of TCEd.
BOW B The
BOW M0 -comma- -lparen- factor most named of prominent PROTEIN the
BOW A -comma- -rparen- activity correlated is of proto-enhancer the TCEd with
Enju PATH (ENTITY1) (<prep arg12arg1) (of) (prep arg12arg2>) (factor)
(<verb arg123arg2) (name) (verb arg123arg3>) (ENTITY2)
pairs (ENTITY1 <prep arg12arg1) (<prep arg12arg1 of) (of prep arg12arg2>) ...
triples (ENTITY1 <prep arg12arg1 of) (<prep arg12arg1 of prep arg12arg2>) ...
GDep PATH (ENTITY1) (<NMOD) (name) (<VMOD) (ENTITY2)
pairs/triples (ENTITY1 <NMOD) (<NMOD name) ... (ENTITY1 <NMOD name) ...
Vector BOW B BOW M0...BOW M4 BOW A Enju PATH GDep PATH
Table 2: Bag-Of-Words (BOW) and shortest-path features for the machine learning. Several BOW feature groups were
created for each template, based on the position of the words in the sentence, relative to the position of the template?s
Named Entities (NE). Specifically, BOW B was made by the words from the beginning of the sentence to the first NE,
BOW A by the words between the last NE and the end of the sentence, and BOW M0 to BOW M4 was made by the
words between the main event clue-word and the NE in slot 0 through 4 respectively. The path features are made from
one, two or three neighbor nodes. We also included certain specific words, like ?binding?, as features.
106
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 85?88,
Paris, October 2009. c?2009 Association for Computational Linguistics
Evaluating Contribution of Deep Syntactic Information
to Shallow Semantic Analysis
Sumire Uematsu Jun?ichi Tsujii
Graduate School of Information Science and Technology
The University of Tokyo
{uematsu,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents shallow semantic pars-
ing based only on HPSG parses. An
HPSG-FrameNet map was constructed
from a semantically annotated corpus, and
semantic parsing was performed by map-
ping HPSG dependencies to FrameNet re-
lations. The semantic parsing was evalu-
ated in a Senseval-3 task; the results sug-
gested that there is a high contribution of
syntactic information to semantic analysis.
1 Introduction
This paper presents semantic parsing based only
on HPSG parses, and examines the contribution of
the syntactic information to semantic analysis.
In computational linguistics, many researchers
have studied the relationship between syntax and
semantics. Its quantitative analysis was formal-
ized as semantic parsing, or semantic role label-
ing, and has attracted the attention of researchers.
Recently, an improvement in the accuracy and
robustness of ?deep parsers? has enabled us to di-
rectly map deep syntactic dependencies to seman-
tic relations. Deep parsers are based on linguisti-
cally expressive grammars; e.g. HPSG, LFG, etc,
and less affected by syntactic alternations such as
passivization. Their results are therefore expected
to closely relate to semantic annotations. For ex-
ample, the sentences in figure 1 share the same
set of semantic roles, and the roles have one-to-
one relations to deep syntactic dependencies in the
sentences. However, the results of the deep parsers
are represented in complex structures, shown in
figure 3, and cannot be straightforwardly com-
pared to semantic annotations.
In order to directly map the deep dependencies
to semantic relations, we adapted the corpus anal-
ysis method of (Frank and Semecky?, 2004) for
the semantic parsing using HPSG parses. We per-
formed the semantic parsing by mapping paths in
HPSG parses to semantic predicate-argument re-
lations. The analysis of the HPSG paths for the
predicate-argument pairs, and the preliminary re-
sult of the semantic parsing indicate the contribu-
tion of syntactic analysis to semantic parsing.
2 Related Work
Besides (Frank and Semecky?, 2004)?s work, as
mentioned above, there have been several studies
on the relationship between deep syntax and se-
mantic parsing. Although the studies did not focus
on direct mappings between deep syntax and shal-
low semantics, they suggested a strong relation-
ship between the two. (Miyao and Tsujii, 2004)
evaluated the accuracy of an HPSG parser against
PropBank semantic annotations, and showed that
the HPSG dependants correlated with semantic ar-
guments of the PropBank, particularly with ?core?
arguments. In (Gildea and Hockenmaier, 2003)
and (Zhang et al, 2008), features from deep parses
were used for semantic parsing, together with fea-
tures from CFG or dependency parses. The deep
features were reported to contribute to a perfor-
mance gain.
3 Syntactic and Semantic Parsing
Some semantic relations are easily identified by
using syntactic parsing while others are more diffi-
cult. This section presents easy and difficult cases
in syntax-semantics map construction.
Trivial when using syntactic analysis: Syn-
tactic parsing, including CFG analysis, detects
semantic similarity of sentences sharing similar
phrase structures. For the example sentences a)
and b) in figure 1, the parsing provides similar
phrase structures, and therefore gives the same
syntactic dependency to occurrences of each role.
Trivial when using deep analysis: Deep pars-
ing reveals the semantic similarity of sentences
85
a) ?, ICommunicator praise themEvaluee for being 99 percent perfectReason. 
b) ?, but heCommunicator praised the Irish premierEvaluee for making a ``sensible?? speechReason.
?, HeEvaluee has been particularly praised as an exponent of ?,
d) ?, SheCommunicator was supposed, therefore, to praise himEvaluee and then ? 
c) The childEvaluee is praised for having a dry bedReason and ?
e) ItEvaluee received high praise, ?
f) AliceWearer ?s dress
g) Versace?s dress
Figure 1: Sentences with a set of semantic roles for the predicate praise.
a) ?, ICommunicator praise themEvaluee for being 99 percent perfectReason. 
b) ?, but heCommunicator praised the Irish premierEvaluee for making a ``sensible?? speechReason.
?, HeEvaluee has been particularly praised as an exponent of ?,
d) ?, SheCommunicator was supposed, therefore, to praise himEvaluee and then ? 
c) The childEvaluee is praised for having a dry bedReason and ?
e) ItEvaluee received high praise, ?
f) AliceWearer ?s dress
g) Versace?s dress
Figur 2: Example phr ses
for section 3.
Mary
Head?Complement?schema
Head?Subject?schema
likes
SYNSEM|LOCAL CATCONT|HOOK
HEADVAL 3?
6?
VFORM:??finverbAUX:???????noneSUBJ:???<??????>?1COMP:<???????>2?verb_arg12PRED:???like?ARG1: 4?ARG2: 5?The
SYNSEM: LOCAL CATCONT|HOOKHEAD:??detVAL|?SPEC:?<???????>8det_arg1PRED:???the?ARG1: 4?7? LOCAL2?
noun_arg0PRED:???Mary?
CAT
CONT|HOOK
HEADVAL CASE:??acc
nounAGR:???3sg
5?SUBJ:???<??????>?COMP:<???????>
SYNSEM:?girl noun_arg0PRED:???girl?SPR:?????<???????>?
LOCALCAT
CONT|HOOK
HEADVAL CASE:??nom
nounAGR:???3sg
4?
SUBJ:???<??????>?COMP:<???????>7SYNSEM:8?
Head?Specifier?schema
1?SYNSEM:? LOCALCAT
CONT|HOOK:?
HEADVAL CASE:??nom
nounAGR:???3sg
4?SUBJ:???<??????>?COMP:<???????> SYNSEM|LOCAL CATCONT|HOOK:?
HEAD:VAL 3?
6?
SUBJ:???<??????>?1COMP:<???????>
SYNSEM|LOCAL CATCONT|HOOK:?
HEAD:VAL 3?
6?
SUBJ:???<??????>?COMP:<???????>
Figure 3: An HPSG parse for The girl likes Mary.
containing complex syntactic phenomena, which
is not easily detected by CFG analysis. The sen-
tences c) and d) in figure 1 contain passivization
and object raising, while deep parsing provides
one dependency for each role in the figure.
Not trivial even when using deep analysis:
Some semantic arguments are not direct syntactic
dependants of their predicates - especially of noun
predicates. In sentence e) in figure 2, the Evaluee
phrase depends on the predicate praise, through
the support verb receive. The deep analysis would
be advantageous in capturing such dependencies,
because it provides receive with direct links to the
phrases of the role and the predicate.
Problematic when using only syntactic analy-
sis: Sometimes, the semantic role of a phrase is
strongly dependent on the type of the mentioned
entity, rather than on the syntactic dependency. In
phrases f) and g) in figure 2, the phrases Alice and
Versace, have the same syntactic relation to the
predicate dress. However, the Wearer role is given
only to the former phrase.
4 A Wide-Coverage HPSG Parser
We employed a wide-coverage HPSG parser for
semantic parsing, and used deep syntactic depen-
dencies encoded in a Predicate Argument Struc-
ture (PAS) in each parse node.
In our experiments, the parser results were con-
sidered as graphs, as illustrated by figures 3 and 4,
to extract HPSG dependencies conveniently. The
The               girl                likes            Mary.verb_arg12 noun_arg0noun_arg0det_arg1
ARG1
ARG2ARG1
Figure 4: A simplified representation of figure 3.
graph is obtained by ignoring most of the linguis-
tic information in the original parse nodes, and
by adding edges directing to the PAS dependants.
The PAS information is represented in the graph,
by the terminal nodes? PAS types, e.g. verb arg12,
etc., and by the added edges. Note that the inter-
pretation of the edge labels depends on the PAS
type. If the PAS type is verb arg12, the ARG2 de-
pendant is the object of the transitive verb or its
equivalence (the subject of the passive, etc.). If
the PAS type is prep arg12, then the dependant is
the NP governed by the preposition node.
5 Semantic Parsing Based on FrameNet
We employed FrameNet (FN) as a semantic cor-
pus. Furthermore, we evaluated our semantic pars-
ing on the SRL task data of Senseval-3 (Litkowski,
2004), which consists of FN annotations.
In FN, semantic frames are defined, and each
frame is associated with predicates that evoke the
frame. For instance, the verb and noun praise are
predicates of the Judgment communication frame,
and they share the same set of semantic roles.
The Senseval-3 data is a standard for evaluation
of semantic parsing. The task is defined as identi-
fying phrases and their semantic roles for a given
sentence, predicate, and frame. The data includes
null instantiations of roles1, which are ?conceptu-
ally salient?, but do not appear in the text.
6 Methods
The semantic parsing using an HPSG-FN map
consisted of the processes shown in figure 5.
1An example of a null instantiation is the Communicator
role in the sentence, ?All in all the conference was acclaimed
as a considerable success.?
86
Map?construc?n?
HPSG?parsingRaw?sentences?
Seman??annota?ns?
Training?data?
HPSG?parses?
Phrase?projec?n? 
HPSG?parses?with?seman?ally?marked?nodes?
HPSG?dependency?extrac?n? 
HPSG?dependency?between??predicate1?and?role1?
Map?instances?
HPSG?dependency?between??predicate1?and?role2?
HPSG?parsingRaw?sentences?
Predicate?annota?ns?
Test?data?
HPSG?parses?
Phrase?projec?n? 
HPSG?parses?with?nodes?marked?as?predicates?
Role?node?predic?n 
Feature?filter?
HPSG?parses?with?seman?ally?marked?nodes?
Role?predic?n?rules?
Seman??parsing?(Map?evalua?n)?
Figure 5: Processes in the map construction and evaluation.
It           recieved       high         praise,  ?adj_arg1verb_arg12noun_arg0
ARG2 ARG1ARG1
Evaluee?role
noun_arg0
Figure 6: an HPSG path for a
semantic relation.
Predicate base: The base form of the semantic
predicate word. (praise in the case of figure 6).
Predicate type: The PAS type of the HPSG
terminal node for the predicate - see section 4.
(noun arg0 in figure 6).
Intermediate word base: The base form of the
intermediate word, corresponding to a terminal
passed by the path, and satisfying pre-defined
conditions. The word may be a support verb.
- see figure 6. (receive in figure 6).
Intermediate word type: The PAS type of the
intermediate word. (verb arg12 in figure 6).
Dependency label sequence: The labels of
the path?s edges. We omitted labels presenting
head-child relations, for identifying a phrase with
another phrase sharing the same head word.
(Reverse of ARG2, ARG1 in figure 6).
Table 1: Features used to represent a HPSG path.
Filter Pred. Inter. Dep.
base type base type label
Same ? ? ? ? ?
AllInter ? ? ? ?
AllPred ? ? ? ?
AllPred-AllInter ? ? ?
Table 2: Syntactic features for role prediction.
Phrase projection: Because we used FN anno-
tations, which are independent of any syntactic
framework, role phrases needed to be projected
to appropriate HPSG nodes. We projected the
phrases based on maximal projection, which was
generally employed, with heads defined in the
HPSG.
HPSG dependency extraction: As an HPSG
dependency for a predicate-argument pair, we
used the shortest path between the predicate node
and the argument node in the HPSG parse. The
path was then represented by pre-defined fea-
tures, listed in table 1. The search for the short-
est path was done in the simplified graph of the
HPSG parse (see figure 4), with the edges denot-
ing deep dependencies, and head-child relations.
An instance of the HPSG-FN map consisted of the
path?s features, the FN frame, and the role label.
Role node prediction: The role prediction was
based on simple rules with scores. The rules were
obtained by filtering features of the map instances.
Table 2 shows the feature filters. The score of a
rule was the number of map instances matching
the rule?s features. In the test, for each node of a
HPSG parse, the role label with the highest score
was selected as the result, where the score of a la-
bel was that of the rule providing the label.
7 Experiments
For the experiments, we employed a wide cover-
age HPSG parser, Enju version 2.3.12, and the data
for the Semantic Role Labeling task of Senseval-3.
7.1 Analysis of Map Instances
We extracted 41,193 HPSG-FN map instances
from the training set, the training data apart from
the development set. The instances amounted to
97.7 % (41,193 / 42,163) of all the non-null in-
stantiated roles in the set, and HPSG paths were
short for many instances. Paths to syntactic ar-
guments were almost directly mapped to semantic
roles, while roles for other phrases were more am-
biguous.
The length distribution of HPSG paths: 64 %
(26410 / 41193) of the obtained HPSG paths were
length-one, and 8 % (3390 / 41193) were length-
two, due to the effect of direct links provided by
HPSG parsing. The length of a path was defined
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
87
Pred. Freq. Feature representation Interpretation
Verb 3792 verb arg12/?/?/ARG2 The object of the transitive predicate
3191 verb arg12/?/?/ARG1 The subject of the transitive predicate
Noun 7468 noun arg0/?/?/? NP headed by the predicate
1161 noun arg0/of/prep arg12/Rev-ARG1 The PP headed by ?of?, attaching to the predicate
Adj 1595 adj arg1/?/?/ARG1 The modifiee of the predicate
274 verb arg12/?/?/ARG2 The modifiee of the predicate treated as a verb
Table 3: Most frequent syntactic paths extracted for predicates of each POS.
as the number of the labels in the Dep. label seq.
of the path. Most of the one-length paths were
paths directing to syntactic arguments, and to PPs
attaching to the predicates. The two-length paths
included paths using support verbs (see figure 6).
Most frequent HPSG dependencies: The most
frequent paths are shown in table 3; syntactic de-
pendencies are presented and counted as taples of
Pred. type, Inter. base, Inter. type, and Dep.
label seq. The interpretation column describes
the syntactic dependencies for the taples. Note
that the column denotes normalized dependencies,
in which object indicates objects of active voice
verbs, subjects of passive-voiced verbs, etc.
7.2 Performance of Semantic Parsing
Finally, semantic parsing was evaluated on the test
data. Table 4 shows the overall performance. The
scores were measured by the Senseval-3 official
script, in the restrictive setting, and can be directly
compared to other systems? scores. Since our pre-
liminary system of semantic parsing ignored null
instantiations of roles, it lost around 0.10 point
of the recalls. We believe that such instantia-
tions may be separately treated. Although the sys-
tem was based on only the syntactic information,
and was very na??ve, the system?s performance was
promising, and showed the high contribution of
syntactic dependencies for semantic parsing.
8 Conclusion
This paper presents semantic parsing based on
only HPSG parses, and investigates the contribu-
tion of syntactic information to semantic parsing.
We constructed an HPSG-FN map by finding
the HPSG paths that corresponded to semantic re-
lations, and used it as role prediction rules in se-
mantic parsing. The semantic parsing was evalu-
ated on the SRL task data of Senseval-3. Although
the preliminary system used only the syntactic in-
formation, the performance was promising, and
Rule set Prec. Overlap Recall
Same 0.799 0.783 0.518
AllInter 0.599 0.586 0.589
AllPred 0.472 0.462 0.709
AllPred-AllInter 0.344 0.335 0.712
Senseval-3 best 0.899 0.882 0.772
Senseval-3 4th best 0.802 0.784 0.654
Table 4: Semantic parsing result on the test data.
indicated that syntactic dependencies may make
significant contribution to semantic analysis.
This paper also suggests a limit of the seman-
tic analysis based purely on syntax. A next step
for accurate HPSG-FN mapping could be analy-
sis of the interaction between the HPSG-FN map
and other information, such as named entity types
which were shown to be effective in many studies.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
Anette Frank and Jir??? Semecky?. 2004. Corpus-based
induction of an LFG syntax-semantics interface for
frame semantic processing. In Proc. of International
Workshop on Linguistically Interpreted Corpora.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial
grammar. In Proc. of EMNLP.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Proc. of Senseval-3.
Yusuke Miyao and Jun?ichi Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proc. of Coling.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid learning of dependency structures from hetero-
geneous linguistic resources. In Proc. of CoNLL.
88
Lenient Default Unification for Robust Processing
within Unification Based Grammar Formalisms
Takashi NINOMIYA,?? Yusuke MIYAO,? and Jun?ichi TSUJII??
? Department of Computer Science, University of Tokyo
? CREST, Japan Science and Technology Corporation
e-mail: {ninomi, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes new default unification, lenient
default unification. It works efficiently, and gives
more informative results because it maximizes the
amount of information in the result, while other de-
fault unification maximizes it in the default. We also
describe robust processing within the framework of
HPSG. We extract grammar rules from the results of
robust parsing using lenient default unification. The
results of a series of experiments show that parsing
with the extracted rules works robustly, and the cov-
erage of a manually-developed HPSG grammar for
Penn Treebank was greatly increased with a little
overgeneration.
1 Introduction
Parsing has often been considered to be crucial
for natural language processing, thus, efficient and
wide coverage parsing has been extensively pur-
sued in natural language literature. This study aims
at robust processing within the Head-driven Phrase
Structure Grammar (HPSG) to extend the cover-
age of manually-developed HPSG grammars. The
meaning of ?robust processing? is not limited to ro-
bust processing for ill-formed sentences found in
a spoken language, but includes robust processing
for sentences which are well-formed but beyond the
grammar writer?s expectation.
Studies of robust parsing within unification-based
grammars have been explored by many researchers
(Douglas and Dale, 1992; Imaichi and Matsumoto,
1995). They classified the errors found in analyzing
ill-formed sentences into several categories to make
them tractable, e.g., constraint violation, missing or
extra elements, etc. In this paper, we focus on re-
covery from the constraint violation errors, which is
a violation of feature values. All errors in agreement
fall into this category. Since many of the grammat-
ical components in HPSG are written as constraints
represented by feature structures, many of the er-
rors are expected to be recovered by the recovery of
constraint violation errors.
This paper proposes two new types of default
unification and describes their application to robust
processing. Default unification was originally stud-
ied to develop a system of lexical semantics to deal
with the default inheritance in a lexicon, but it is
also desirable for the recovery of such constraint vi-
olation errors due to the following merits: i) default
unification is always well-defined, and ii) a feature
structure is relaxed such that the amount of infor-
mation is maximized. From the viewpoint of robust
processing, an amount of lost information can be re-
garded as a cost (i.e., penalty) of robust processing.
In other words, default unification tries to minimize
the cost. Given a strict feature structure F and a
default feature structure G, default unification is de-
fined as unification that satisfies the following (writ-
ten as F <unionsq G): 1) It is always defined. 2) All strict
information is preserved. That is, F v (F <unionsq G). 3) It
reduces to standard unification in the case of F and
G being consistent. That is, (F <unionsq G) = (F unionsqG) if
F unionsqG is defined. With these definitions, Douglas?
relaxation technique can be regarded as a sort of de-
fault unification. They classify constraints into nec-
essary constraints and optional constraints, which
can be regarded as strict information and default in-
formation in the definition of default unification.
Carpenter (1993) gave concise and comprehen-
sive definitions of default unification. However, the
problem in Carpenter?s default unification is that it
tries to maximize the amount of information in a de-
fault feature structure, not the result of default uni-
fication. Consider the case where a grammar rule
is the default feature structure and the daughters are
the strict feature structure. The head feature prin-
ciple can be described as the structure-sharing be-
tween the values of the head feature in a mother
and in a head daughter. The set of constraints that
represent the head feature principle consists of only
one element. When we lose just one element in the
head feature principle, a large amount of informa-
tion in the daughter?s substructure is not propagated
to its mother. As Copestake (1993) mentioned, an-
other problem in Carpenter?s default unification is
that the time complexity for finding the optimal an-
swer of default unification is exponential because
we have to verify the unifiability of the power set of
constraints in a default feature structure.
Here, we propose ideal lenient default unifica-
tion, which tries to maximize the amount of infor-
mation of a result, not the amount of default infor-
mation. Thus, the problem of losing a large amount
of information in structure-sharing never arises. We
also propose lenient default unification whose algo-
rithm is much more efficient than the ideal one. Its
time complexity is linear to the size of the strict fea-
ture structure and the default feature structure. In-
stead, the amount of information of a result derived
by lenient default unification is equal to or less than
that of the ideal one.
We apply lenient default unification to robust pro-
cessing. Given an HPSG grammar, our approach
takes two steps; i) extraction of grammar rules from
the results of robust parsing using lenient default
unification for applying the HPSG grammar rules
(offline parsing), and ii) runtime parsing using the
HPSG grammar with the extracted rules. The ex-
tracted rules work robustly since they reflect the ef-
fects of recovery rules applied during offline robust
parsing and the conditions in which they are ap-
plied.
Sections 3 and 4 describe our default unification.
Our robust parsing is explained in Section 5. Sec-
tion 6 shows a series of experiments of robust pars-
ing with default unification.
2 Background
Default unification has been investigated by many
researchers (Bouma, 1990; Russell et al, 1991;
Copestake, 1993; Carpenter, 1993; Lascarides and
Copestake, 1999) in the context of developing lexi-
cal semantics. Here, we first explain the definition
given by Carpenter (1993) because his definition is
both concise and comprehensive.
2.1 Carpenter?s Default Unification
Carpenter proposed two types of default unification,
credulous default unification and skeptical default
unification.
(Credulous Default Unification)
F <unionsqc G =
{
F unionsqG?
??? G? v G is maximal such thatF unionsqG? is defined
}
(Skeptical Default Unification)
F <unionsqs G = ?(F <unionsqc G)
F is called a strict feature structure, whose in-
formation must not be lost, and G is called a de-
fault feature structure, whose information might be
lost but as little as possible so that F and G can be
unified. A credulous default unification operation
is greedy in that it tries to maximize the amount of
information it retains from the default feature struc-
ture. This definition returns a set of feature struc-
tures rather than a unique feature structure.
Skeptical default unification simply generalizes
the set of feature structures which results from cred-
ulous default unification. The definition of skeptical
default unification leads to a unique result. The de-
fault information which can be found in every result
of credulous default unification remains. Following
is an example of skeptical default unification.
[F: a] <unionsqs
[
F: 1 b
G: 1
H: c
]
= u
{ [F: a
G: b
H: c
]
,
[
F: 1 a
G: 1
H: c
]}
=
[F: a
G: ?
H: c
]
2.2 Forced Unification
Forced unification is another way to unify incon-
sistent feature structures. Forced unification always
succeeds by supposing the existence of the top type
(the most specific type) in a type hierarchy. Unifi-
cation of any pair of types is defined in the type hi-
erarchy, and therefore unification of any pair of fea-
ture structures is defined. One example is described
by Imaichi and Matsumoto (1995) (they call it cost-
based unification). Their unification always suc-
ceeds by supposing the top type, and it also keeps
the information about inconsistent types. Forced
unification can be regarded as one of the toughest
robust processing because it always succeeds and
never loses the information embedded in feature
structures. The drawback of forced unification is
the postprocessing of parsing, i.e., feature structures
with top types are not tractable. We write Funionsq f G for
the forced unification of F and G.
3 Ideal Lenient Default Unification
In this section, we explain our default unification,
ideal lenient default unification. Ideal lenient de-
fault unification tries to maximize the amount of
information of the result, subsuming the result of
forced unification. In other words, ideal lenient de-
fault unification tries to generate a result as similar
as possible to the result of forced unification such
that the result is defined in the type hierarchy with-
out the top type. Formally, we have:
Definition 3.1 Ideal Lenient Default Unification
F <unionsqi G = ?
{
F unionsqG?
?????
G? v f (Funionsq f G) is maximal
such that F unionsqG? is defined
without the top type
}
where v f is a subsumption relation where the top
type is defined.
From the definition of skeptical default unifica-
tion, ideal lenient default unification is equivalent
to F <unionsqs (Funionsq f G) assuming that skeptical default uni-
fication does not add the default information that in-
cludes the top type to the strict information.
Consider the following feature structures.
F =
?
???
F:
[F:a
G:b
H:c
]
G:
[F:a
G:a
H:c
]
?
???,G =
[
F: 1
G: 1
]
In the case of Carpenter?s default unification, the
results of skeptical and credulous default unification
become as follows: F <unionsqs G = F,F <unionsqc G = {F}. This
is because G is generalized to the bottom feature
structure, and hence the result is equivalent to the
strict feature structure.
With ideal lenient default unification, the result
becomes as follows.
F <unionsqi G =
?
?????
F:
[
F: 1 a
G:b
H: 2 c
]
G:
[
F: 1
G:a
H: 2
]
?
?????
v f
?
?F: 1
[F:a
G:>
H:c
]
G: 1
?
?
Note that the result of ideal lenient default unifica-
tion subsumes the result of forced unification.
As we can see in the example, ideal lenient de-
fault unification tries to keep as much information
of the structure-sharing as possible (ideal lenient
default unification succeeds in preserving the struc-
ture-sharing tagged as 1 and 2 though skeptical and
credulous default unification fail to capture it).
4 Lenient Default Unification
The optimal answer for ideal lenient default unifica-
tion can be found by calculating F <unionsqs (Funionsq f G). As
Copestake (1993) mentioned, the time complexity
of skeptical default unification is exponential, and
therefore the time complexity of ideal lenient de-
fault unification is also exponential.
As other researchers pursued efficient default uni-
fication (Bouma, 1990; Russell et al, 1991; Copes-
take, 1993), we also propose another definition of
default unification, which we call lenient default
unification. An algorithm derived for it finds its an-
swer efficiently.
Given a strict feature structure F and a default
feature structure G, let H be the result of forced uni-
fication, i.e., H = Funionsq f G. We define topnode(H)
as a function that returns the fail points (the nodes
that are assigned the top type in H), f pnode(H)
as a function that returns the fail path nodes (the
nodes from which a fail point can be reached), and
f pchild(H) as a a function that returns all the nodes
that are not fail path nodes but the immediate chil-
dren of fail path nodes.
Consider the following feature structures.
F =
?
????
F:F:
[F:F:a
G:G:b
H:H:c
]
G:
[
G:
[F:F:a
G:G:a
H:H:c
]
H:H:a
]
?
????,G =
[
F:F: 1?
G:G: 1
]
Figure 1 shows F , G and H in the graph notation.
This figure also shows the nodes that correspond to
topnode(H), f pnode(H) and f pchild(H).
 


 

 




F =

 
G =
	





H =












	




	


)(Htopnode? )(Hfpnode?
An Indexing Scheme for Typed Feature Structures
Takashi NINOMIYA,?? Takaki MAKINO,#? and Jun?ichi TSUJII??
?Department of Computer Science, University of Tokyo
?CREST, Japan Science and Technology Corporation
#Department of Complexity Science and Engineering, University of Tokyo?
?BSI, RIKEN
e-mail: {ninomi, mak, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an indexing substrate for typed
feature structures (ISTFS), which is an efficient re-
trieval engine for typed feature structures. Given a
set of typed feature structures, the ISTFS efficiently
retrieves its subset whose elements are unifiable or
in a subsumption relation with a query feature struc-
ture. The efficiency of the ISTFS is achieved by
calculating a unifiability checking table prior to re-
trieval and finding the best index paths dynami-
cally.
1 Introduction
This paper describes an indexing substrate for typed
feature structures (ISTFS), which is an efficient re-
trieval engine for typed feature structures (TFSs)
(Carpenter, 1992). Given a set of TFSs, the ISTFS
can efficiently retrieve its subset whose elements are
unifiable or in a subsumption relation with a query
TFS.
The ultimate purpose of the substrate is aimed at
the construction of large-scale intelligent NLP sys-
tems such as IR or QA systems based on unification-
based grammar formalisms (Emele, 1994). Recent
studies on QA systems (Harabagiu et al, 2001) have
shown that systems using a wide-coverage noun tax-
onomy, quasi-logical form, and abductive inference
outperform other bag-of-words techniques in accu-
racy. Our ISTFS is an indexing substrate that en-
ables such knowledge-based systems to keep and
retrieve TFSs, which can represent symbolic struc-
tures such as quasi-logical forms or a taxonomy and
the output of parsing of unification-based grammars
for a very large set of documents.
The algorithm for our ISTFS is concise and effi-
cient. The basic idea used in our algorithm uses a
necessary condition for unification.
(Necessary condition for unification) Let PathF
be the set of all feature paths defined in
? This research is partially funded by JSPS Research Fellow-
ship for Young Scientists.
TFS F , and FollowedType(pi,F) be the
type assigned to the node reached by fol-
lowing path pi .1 If two TFSs F and G
are unifiable, then FollowedType(pi,F) and
FollowedType(pi,G) are defined and unifiable
for all pi ? (PathF ?PathG).
The Quick Check algorithm described in (Torisawa
and Tsujii, 1995; Malouf et al, 2000) also uses
this condition for the efficient checking of unifia-
bility between two TFSs. Given two TFSs and stat-
ically determined paths, the Quick Check algorithm
can efficiently determine whether these two TFSs
are non-unifiable or there is some uncertainty about
their unifiability by checking the path values. It is
worth noting that this algorithm is used in many
modern unification grammar-based systems, e.g.,
the LKB system (Copestake, 1999) and the PAGE
system (Kiefer et al, 1999).
Unlike the Quick Check algorithm, which checks
unifiability between two TFSs, our ISTFS checks
unifiability between one TFS and n TFSs. The
ISTFS checks unifiability by using dynamically de-
termined paths, not statically determined paths. In
our case, using only statically determined paths
might extremely degrades the system performance.
Suppose that any statically determined paths are not
defined in the query TFS. Because there is no path
to be used for checking unifiability, it is required to
unify a query with every element of the data set. It
should also be noted that using all paths defined in
a query TFS severely degrades the system perfor-
mance because a TFS is a huge data structure com-
prised of hundreds of nodes and paths, i.e., most of
the retrieval time will be consumed in filtering. The
1More precisely, FollowedType(pi,F) returns the type as-
signed to the node reached by following pi from the root node
of FSPAT H(pi,F), which is defined as follows.
FSPAT H(pi,F) = F unionsqPV (pi)
PV (pi) =
{
the least feature structure where
path pi is defined
That is, FollowedType(pi,F) might be defined even if pi does
not exist in F .
ISTFS dynamically finds the index paths in order of
highest filtering rate. In the experiments, most ?non-
unifiable? TFSs were filtered out by using only a few
index paths found by our optimization algorithm.
2 Algorithm
Briefly, the algorithm for the ISTFS proceeds ac-
cording to the following steps.
1. When a set of data TFSs is given, the ISTFS
prepares a path value table and a unifiability
checking table in advance.
2. When a query TFS is given, the ISTFS re-
trieves TFSs which are unifiable with the query
from the set of data TFSs by performing the
following steps.
(a) The ISTFS finds the index paths by using
the unifiability checking table. The index
paths are the most restrictive paths in the
query in the sense that the set of the data
TFSs can be limited to the smallest one.
(b) The ISTFS filters out TFSs that are non-
unifiable by referring to the values of the
index paths in the path value table.
(c) The ISTFS finds exactly unifiable TFSs
by unifying the query and the remains of
filtering one-by-one, in succession.
This algorithm can also find the TFSs that are
in the subsumption relation, i.e., more-specific or
more-general, by preparing subsumption checking
tables in the same way it prepared a unifiability
checking table.
2.1 Preparing Path Value Table and
Unifiability Checking Table
Let D(= {F1,F2, . . . ,Fn}) be the set of data TFSs.
When D is given, the ISTFS prepares two tables, a
path value table Dpi,? and a unifiability checking ta-
ble Upi,? , for all pi ? PathD and ? ? Type. 2 A
TFS might have a cycle in its graph structure. In
that case, a set of paths becomes infinite. Fortu-
nately, our algorithm works correctly even if the set
of paths is a subset of all existing paths. Therefore,
paths which might cause an infinite set can be re-
moved from the path set. We define the path value
table and the unifiability checking table as follows:
Dpi,? ? {F |F ?D ? FollowedType(pi,F) = ?}
Upi,? ? ?
?
(??Type ? ?unionsq? is defined)
|Dpi,? |
2Type is a finite set of types.
Assuming that ? is the type of the node reached by
following pi in a query TFS, we can limit D to a
smaller set by filtering out ?non-unifiable? TFSs. We
have the smaller set:
U ?pi,? ?
?
?
(??Type ? ?unionsq? is defined)
Dpi,?
Upi,? corresponds to the size of U ?pi,? . Note that the
ISTFS does not prepare a table of U ?pi,? statically, but
just prepares a table of Upi,? whose elements are in-
tegers. This is because the system?s memory would
easily be exhausted if we actually made a table of
U ?pi,? . Instead, the ISTFS finds the best paths by re-
ferring to Upi,? and calculates only U ?pi,? where pi is
the best index path.
Suppose the type hierarchy and D depicted in
Figure 1 are given. The tables in Figure 2 show Dpi,?
and Upi,? calculated from Figure 1.
2.2 Retrieval
In what follows, we suppose that D was given, and
we have already calculated Dpi,? and Upi,? .
Finding Index Paths
The best index path is the most restrictive path in the
query in the sense thatD can be limited to the small-
est set by referring to the type of the node reached
by following the index path in the query.
Suppose a query TFS X and a constant k, which is
the maximum number of index paths, are given. The
best index path in PathX is path pi such that Upi,? is
minimum where ? is the type of the node reached
by following pi from the root node of X . We can
also find the second best index path by finding the
path pi s.t. Upi,? is the second smallest. In the same
way, we can find the i-th best index path s.t. i ? k.
Filtering
Suppose k best index paths have already been cal-
culated. Given an index path pi , let ? be the type of
the node reached by following pi in the query. An
element of D that is unifiable with the query must
have a node that can be reached by following pi and
whose type is unifiable with ? . Such TFSs (=U ?pi,? )
can be collected by taking the union of Dpi,? , where
? is unifiable with ? . For each index path, U ?pi,?
can be calculated, and the D can be limited to the
smaller one by taking their intersection. After filter-
ing, the ISTFS can find exactly unifiable TFSs by
unifying the query with the remains of filtering one
by one.
Suppose the type hierarchy and D in Figure 1 are
?

  	

  

   ?????? ?
  
  
:CDR
:CAR
F1 =
?
????
cons
CAR: 1
CDR:
?
?
cons
CAR: 2
CDR:
[ cons
CAR: 3
CDR: nil
]
?
?
?
????
F2 =
[ cons
CAR: 4
CDR: nil
]
,
F3 =
?
?
cons
CAR: 5
CDR:
[ cons
CAR: 6
CDR: nil
]
?
?
D = {F1,F2,F3}
Figure 1: An example of a type hierarchy and TFSs
Dpi,?
?
pi ? integer 1 2 3 4 5 6 list cons nil
? ? ? ? ? ? ? ? ? ? {F1,F2 ,F3} ?
CAR: ? ? {F1} ? ? {F2} {F3} ? ? ?
CDR: ? ? ? ? ? ? ? ? ? {F1 ,F3} {F2}
CDR:CAR: ? ? ? {F1} ? ? ? {F3} ? ? ?
CDR:CDR: ? ? ? ? ? ? ? ? ? {F1} {F3}
CDR:CDR:CAR: ? ? ? ? {F1} ? ? ? ? ? ?
CDR:CDR:CDR: ? ? ? ? ? ? ? ? ? ? {F1}? is an empty set.
Upi,?
?
pi ? integer 1 2 3 4 5 6 list cons nil
? 3 0 0 0 0 0 0 0 3 *3 0
CAR: 3 *3 1 0 0 1 1 0 0 0 0
CDR: 3 0 0 0 0 0 0 0 3 *2 1
CDR:CAR: 2 2 0 1 0 0 0 *1 0 0 0
CDR:CDR: 2 0 0 0 0 0 0 0 *2 1 1
CDR:CDR:CAR: 1 1 0 0 1 0 0 0 0 0 0
CDR:CDR:CDR: 1 0 0 0 0 0 0 0 1 0 1
Figure 2: An example of Dpi,? and Upi,?
QuerySetA QuerySetB
# of the data TFSs 249,994 249,994
Avg. # of unifiables 68,331.58 1,310.70
Avg. # of more specifics 66,301.37 0.00
Avg. # of more generals 0.00 0.00
Table 1: The average number of data TFSs and an-
swers for QuerySetA and QuerySetB
given, and the following query X is given:
X =
?
?
cons
CAR: integer
CDR:
[ cons
CAR: 6
CDR: list
]
?
?
In Figure 2, Upi,? where the pi and ? pair exists in
the query is indicated with an asterisk. The best in-
dex paths are determined in ascending order of Upi,?
indicated with an asterisk in the figure. In this ex-
ample, the best index path is CDR:CAR: and its corre-
sponding type in the query is 6. Therefore the unifi-
able TFS can be found by referring to DCDR:CAR:,6,
and this is {F3}.
3 Performance Evaluation
We measured the performance of the ISTFS on a
IBM xSeries 330 with a 1.26-GHz PentiumIII pro-
cessor and a 4-GB memory. The data set consist-
ing of 249,994 TFSs was generated by parsing the
 
  
  
  
          	 
     
      	 
        
        
 

	


Figure 3: The size of Dpi,? for the size of the data
set
800 bracketed sentences in the Wall Street Journal
corpus (the first 800 sentences in Wall Street Jour-
nal 00) in the Penn Treebank (Marcus et al, 1993)
with the XHPSG grammar (Tateisi et al, 1998). The
size of the data set was 151 MB. We also generated
two sets of query TFSs by parsing five randomly
selected sentences in the Wall Street Journal cor-
pus (QuerySetA and QuerySetB). Each set had 100
query TFSs. Each element of QuerySetA was the
daughter part of the grammar rules. Each element of
QuerySetB was the right daughter part of the gram-
mar rules whose left daughter part is instantiated.
Table 1 shows the number of data TFSs and the av-
erage number of unifiable, more-specific and more-
general TFSs for QuerySetA and QuerySetB. The
total time for generating the index tables (i.e., a set
of paths, the path value table (Dpi,? ), the unifiabil-
ity checking table (Upi,? ), and the two subsumption
checking tables) was 102.59 seconds. The size of
the path value table was 972 MByte, and the size of
the unifiability checking table and the two subsump-
tion checking tables was 13 MByte. The size of the
unifiability and subsumption checking tables is neg-
ligible in comparison with that of the path value ta-
ble. Figure 3 shows the growth of the size of the
path value table for the size of the data set. As seen
in the figure, it grows proportionally.
Figures 4, 5 and 6 show the results of retrieval
time for finding unifiable TFSs, more-specific TFSs
and more-general TFSs respectively. In the figures,
the X-axis shows the number of index paths that
are used for limiting the data set. The ideal time
means the unification time when the filtering rate is
100%, i.e., our algorithm cannot achieve higher ef-
ficiency than this optimum. The overall time is the
sum of the filtering time and the unification time.
As illustrated in the figures, using one to ten index
paths achieves the best performance. The ISTFS
achieved 2.84 times speed-ups in finding unifiables
for QuerySetA, and 37.90 times speed-ups in find-
ing unifiables for QuerySetB.
Figure 7 plots the filtering rate. In finding unifi-
able TFSs in QuerySetA, more than 95% of non-
unifiable TFSs are filtered out by using only three
index paths. In the case of QuerySetB, more than
98% of non-unifiable TFSs are filtered out by using
only one index path.
4 Discussion
Our approach is said to be a variation of path in-
dexing. Path indexing has been extensively studied
in the field of automated reasoning, declarative pro-
gramming and deductive databases for term index-
ing (Sekar et al, 2001), and was also studied in the
field of XML databases (Yoshikawa et al, 2001). In
path indexing, all existing paths in the database are
first enumerated, and then an index for each path is
prepared. Other existing algorithms differed from
ours in i) data structures and ii) query optimization.
In terms of data structures, our algorithm deals with
typed feature structures while their algorithms deal
with PROLOG terms, i.e., variables and instanti-
ated terms. Since a type matches not only the same
type or variables but unifiable types, our problem is
much more complicated. Yet, in our system, hierar-
chical relations like a taxonomy can easily be repre-
sented by types. In terms of query optimization, our
algorithm dynamically selects index paths to mini-
mize the searching cost. Basically, their algorithms
take an intersection of candidates for all paths in a
query, or just limiting the length of paths (McCune,
2001). Because such a set of paths often contains
many paths ineffective for limiting answers, our ap-
proach should be more efficient than theirs.
5 Conclusion and Future Work
We developed an efficient retrieval engine for TFSs,
ISTFS. The efficiency of ISTFS is achieved by cal-
culating a unifiability checking table prior to re-
trieval and finding the best index paths dynamically.
In future work, we are going to 1) minimize the
size of the index tables, 2) develop a feature struc-
ture DBMS on a second storage, and 3) incorporate
structure-sharing information into the index tables.
References
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge University Press, Cambridge, U.K.
A. Copestake. 1999. The (new) LKB system. Technical
report, CSLI, Stanford University.
M. C. Emele. 1994. TFS ? the typed feature struc-
ture representation formalism. In Proc. of the Interna-
tional Workshop on Sharable Natural Language Re-
sources (SNLR-1994).
S. Harabagiu, D. Moldovan, M. Pas?ca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. G??rju, V. Rus, and
Mora?rescu. 2001. Falcon: Boosting knowledge for
answer engines. In Proc. of TREC 9.
B. Kiefer, H.-U. Krieger, J. Carroll, and R. Malouf.
1999. A bag of useful techniques for efficient and ro-
bust parsing. In Proc. of ACL-1999, pages 473?480,
June.
R. Malouf, J. Carroll, and A. Copestake. 2000. Effi-
cient feature structure operations without compilation.
Journal of Natural Language Engineering, 6(1):29?
46.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):313?330.
W. McCune. 2001. Experiments with discrimination-
tree indexing and path indexing for term retrieval. Au-
tomated Reasoning, 18(2):147?167.
R. Sekar, I. V. Ramakrishnan, and A. Voronkov. 2001.
Term indexing. In Handbook of Automated Reason-
ing, pages 1853?1964. Elsevier Science Publishers.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii. 1998.
Translating the XTAG English grammar to HPSG. In
Proc. of TAG+4, pages 172?175.
K. Torisawa and J. Tsujii. 1995. Compiling HPSG-
style grammar to object-oriented language. In Proc.
of NLPRS-1995, pages 568?573.
M. Yoshikawa, T. Amagasa, T. Shimura, and S. Uemura.
2001. XRel: A path-based approach to storage and re-
trieval of XML documents using relational databases.
ACM Transactions on Internet Technology, 1(1):110?
141.
 
   
   
   
 
               	 
     	  
   
    	  
     

	


     	     
    	     Deep Linguistic Analysis for the Accurate Identification of
Predicate-Argument Relations
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper evaluates the accuracy of HPSG
parsing in terms of the identification of
predicate-argument relations. We could directly
compare the output of HPSG parsing with Prop-
Bank annotations, by assuming a unique map-
ping from HPSG semantic representation into
PropBank annotation. Even though PropBank
was not used for the training of a disambigua-
tion model, an HPSG parser achieved the ac-
curacy competitive with existing studies on the
task of identifying PropBank annotations.
1 Introduction
Recently, deep linguistic analysis has successfully
been applied to real-world texts. Several parsers
have been implemented in various grammar for-
malisms and empirical evaluation has been re-
ported: LFG (Riezler et al, 2002; Cahill et al,
2002; Burke et al, 2004), LTAG (Chiang, 2000),
CCG (Hockenmaier and Steedman, 2002b; Clark et
al., 2002; Hockenmaier, 2003), and HPSG (Miyao
et al, 2003; Malouf and van Noord, 2004). How-
ever, their accuracy was still below the state-of-the-
art PCFG parsers (Collins, 1999; Charniak, 2000) in
terms of the PARSEVAL score. Since deep parsers
can output deeper representation of the structure of
a sentence, such as predicate argument structures,
several studies reported the accuracy of predicate-
argument relations using a treebank developed for
each formalism. However, resources used for the
evaluation were not available for other formalisms,
and the results cannot be compared with each other.
In this paper, we employ PropBank (Kingsbury
and Palmer, 2002) for the evaluation of the accu-
racy of HPSG parsing. In the PropBank, semantic
arguments of a predicate and their semantic roles
are manually annotated. Since the PropBank has
been developed independently of any grammar for-
malisms, the results are comparable with other pub-
lished results using the same test data.
Interestingly, several studies suggested that the
identification of PropBank annotations would re-
quire linguistically-motivated features that can be
obtained by deep linguistic analysis (Gildea and
Hockenmaier, 2003; Chen and Rambow, 2003).
They employed a CCG (Steedman, 2000) or LTAG
(Schabes et al, 1988) parser to acquire syntac-
tic/semantic structures, which would be passed to
statistical classifier as features. That is, they used
deep analysis as a preprocessor to obtain useful fea-
tures for training a probabilistic model or statistical
classifier of a semantic argument identifier. These
results imply the superiority of deep linguistic anal-
ysis for this task.
Although the statistical approach seems a reason-
able way for developing an accurate identifier of
PropBank annotations, this study aims at establish-
ing a method of directly comparing the outputs of
HPSG parsing with the PropBank annotation in or-
der to explicitly demonstrate the availability of deep
parsers. That is, we do not apply statistical model
nor machine learning to the post-processing of the
output of HPSG parsing. By eliminating the effect
of post-processing, we can directly evaluate the ac-
curacy of deep linguistic analysis.
Section 2 introduces recent advances in deep lin-
guistic analysis and the development of semanti-
cally annotated corpora. Section 3 describes the de-
tails of the implementation of an HPSG parser eval-
uated in this study. Section 4 discusses a problem in
adopting PropBank for the performance evaluation
of deep linguistic parsers and proposes its solution.
Section 5 reports empirical evaluation of the accu-
racy of the HPSG parser.
2 Deep linguistic analysis and
semantically annotated corpora
Riezler et al (2002) reported the successful applica-
tion of a hand-crafted LFG (Bresnan, 1982) gram-
mar to the parsing of the Penn Treebank (Marcus
et al, 1994) by exploiting various techniques for
robust parsing. The study was impressive because
most researchers had believed that deep linguistic
analysis of real-world text was impossible. Their
success owed much to a consistent effort to main-
tain a wide-coverage LFG grammar, as well as var-
SVP
have
to
choose
this particular moment
S
NP VP
VP
NP
they
NP-1
did n?t
*-1
VP
VP
ARG0-choose
ARG1-chooseARG0-choose
REL-choose
Figure 1: Annotation of the PropBank
ious techniques for robust parsing.
However, the manual development of wide-
coverage linguistic grammars is still a difficult task.
Recent progress in deep linguistic analysis has
mainly depended on the acquisition of lexicalized
grammars from annotated corpora (Xia, 1999; Chen
and Vijay-Shanker, 2000; Chiang, 2000; Hocken-
maier and Steedman, 2002a; Cahill et al, 2002;
Frank et al, 2003; Miyao et al, 2004). This ap-
proach not only allows for the low-cost develop-
ment of wide-coverage grammars, but also provides
the training data for statistical modeling as a by-
product. Thus, we now have a basis for integrating
statistical language modeling with deep linguistic
analysis. To date, accurate parsers have been devel-
oped for LTAG (Chiang, 2000), CCG (Hockenmaier
and Steedman, 2002b; Clark et al, 2002; Hocken-
maier, 2003), and LFG (Cahill et al, 2002; Burke et
al., 2004). Those studies have opened up the appli-
cation of deep linguistic analysis to practical use.
However, the accuracy of those parsers was still
below PCFG parsers (Collins, 1999; Charniak,
2000) in terms of the PARSEVAL score, i.e., labeled
bracketing accuracy of CFG-style parse trees. Since
one advantage of deep parsers is that they can out-
put a sort of semantic representation, e.g. predicate-
argument structures, several studies have reported
the accuracy of predicate-argument relations (Hock-
enmaier and Steedman, 2002b; Clark et al, 2002;
Hockenmaier, 2003; Miyao et al, 2003). However,
their evaluation employed a treebank developed for
a specific grammar formalism. Hence, those results
cannot be compared fairly with parsers based on
other formalisms including PCFG parsers.
At the same time, following the great success
of machine learning approaches in NLP, many re-
search efforts are being devoted to developing vari-
ous annotated corpora. Notably, several projects are
underway to annotate large corpora with semantic
information such as semantic relations of words and
coreferences.
PropBank (Kingsbury and Palmer, 2002) and
FrameNet (Baker et al, 1998) are large English cor-
pora annotated with the semantic relations of words
in a sentence. Figure 1 shows an example of the
annotation of the PropBank. As the target text of
the PropBank is the same as the Penn Treebank, a
syntactic structure is given by the Penn Treebank.
The PropBank includes additional annotations rep-
resenting a predicate and its semantic arguments in
a syntactic tree. For example, in Figure 1, REL de-
notes a predicate, ?choose?, and ARG   represents
its semantic arguments: ?they? for the 0th argument
(i.e., subject) and ?this particular moment? for the
1st argument (i.e., object).
Existing studies applied statistical classifiers to
the identification of the PropBank or FrameNet an-
notations. Similar to many methods of applying ma-
chine learning to NLP tasks, they first formulated
the task as identifying in a sentence each argument
of a given predicate. Then, parameters of the iden-
tifier were learned from the annotated corpus. Fea-
tures of a statistical model were defined as a pat-
tern on a partial structure of the syntactic tree output
by an automatic parser (Gildea and Palmer, 2002;
Gildea and Jurafsky, 2002).
Several studies proposed the use of deep linguis-
tic features, such as predicate-argument relations
output by a CCG parser (Gildea and Hockenmaier,
2003) and derivation trees output by an LTAG parser
(Chen and Rambow, 2003). Both studies reported
that the identification accuracy improved by in-
troducing such deep linguistic features. Although
deep analysis has not outperformed PCFG parsers in
terms of the accuracy of surface structure, these re-
sults are implicitly supporting the necessity of deep
linguistic analysis for the recognition of semantic
relations.
However, these results do not directly reflect the
performance of deep parsers. Since these corpora
provide deeper structure of a sentence than surface
parse trees, they would be suitable for the evalua-
tion of deep parsers. In Section 4, we explore the
possibility of using the PropBank for the evaluation
of an HPSG parser.
3 Implementation of an HPSG parser
This study evaluates the accuracy of a general-
purpose HPSG parser that outputs predicate argu-
ment structures. While details have been explained
in other papers (Miyao et al, 2003; Miyao et al,
2004), in the remainder of this section, we briefly
review the grammar and the disambiguation model
of our HPSG parser.
SVP
have
to
choose
this particular moment
S
NP VP
VP
NP
they
NP-1
did n?t
*-1
VP
VParg
head
head
head head
head
head
head
arg
arg
arg
arg
mod
 
have
to
choose
this particular moment
they
did n?t
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >2
HEAD  verb
SUBJ  < _ >
HEAD  verb
SUBJ  <    >2
HEAD  verb
SUBJ  <    >1
HEAD  verb
SUBJ  <    >1
HEAD  noun
SUBJ  < >
COMPS  < >
head-comp
head-comp
head-comp
head-comp
subject-head
1
 
have to
they
did n?t
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS <    >
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  <    >
COMPS  < >
1
1
2
2
HEAD  verb
SUBJ  < >
COMPS <    >
1
3
HEAD  verb
SUBJ  <    >
COMPS  < >
13
1
choose this particular moment
HEAD  noun
SUBJ  < >
COMPS  < >
4
HEAD  verb
SUBJ  < >
COMPS <    >
1
4
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank-style parse tree
3.1 Grammar
The grammar used in this paper follows the theory
of HPSG (Pollard and Sag, 1994), and is extracted
from the Penn Treebank (Miyao et al, 2004). In
this approach, a treebank is annotated with partially
specified HPSG derivations using heuristic rules.
By inversely applying schemata to the derivations,
partially specified constraints are percolated and in-
tegrated into lexical entries, and a large HPSG-style
lexicon is extracted from the treebank.
Figure 2 shows an example of extracting HPSG
lexical entries from a Penn Treebank-style parse
tree. Firstly, given a parse tree (the top of the fig-
ure), we annotate partial specifications on an HPSG
derivation (the middle). Then, HPSG schemata are
applied to each branching in the derivation. Finally,
COMPS <                         >
SUBJ <                         >
PHON  ?choose?
HEAD  verb
REL  choose
ARG0
ARG1
HEAD  noun
SEM 1
HEAD  noun
SEM 2
SEM 1
2
Figure 3: Mapping from syntactic arguments to se-
mantic arguments
we get lexical entries for all of the words in the tree
(the bottom).
As shown in the figure, we can also obtain com-
plete HPSG derivation trees, i.e., an HPSG tree-
bank. It is available for the machine learning of dis-
ambiguation models, and can also be used for the
evaluation of HPSG parsing.
In an HPSG grammar, syntax-to-semantics map-
pings are implemented in lexical entries. For exam-
ple, when we have a lexical entries for ?choose?
as shown in Figure 3, the lexical entry includes
mappings from syntactic arguments (SUBJ and
COMPS features) into a predicate-argument struc-
ture (ARG0 and ARG1 features). Argument labels
in a predicate-argument structure are basically de-
fined in a left-to-right order of syntactic realizations,
while if we had a cue for a movement in the Penn
Treebank, arguments are put in its canonical posi-
tion in a predicate-argument structure.
3.2 Disambiguation model
By grammar extraction, we are able to obtain a large
lexicon together with complete derivation trees of
HPSG, i.e, an HPSG treebank. The HPSG treebank
can then be used as training data for the machine
learning of the disambiguation model.
Following recent research about disambiguation
models on linguistic grammars (Abney, 1997; John-
son et al, 1999; Riezler et al, 2002; Clark and Cur-
ran, 2003; Miyao et al, 2003; Malouf and van No-
ord, 2004), we apply a log-linear model or maxi-
mum entropy model (Berger et al, 1996) on HPSG
derivations. We represent an HPSG sign as a tu-
ple 	
 , where  is a lexical sign of the
head word, 
 is a part-of-speech, and  is a sym-
bol representing the structure of the sign (mostly
corresponding to nonterminal symbols of the Penn
Treebank). Given an HPSG schema  and the dis-
tance  between the head words of the head/non-
head daughter constituents, each (binary) branch-
ing of an HPSG derivation is represented as a tuple

 
127
128
129
130
A Robust Retrieval Engine for Proximal and Structural Search
Katsuya Masuda? Takashi Ninomiya?? Yusuke Miyao? Tomoko Ohta?? Jun?ichi Tsujii??
? Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
{kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
In the text retrieval area including XML and Region Al-
gebra, many researchers pursued models for specifying
what kinds of information should appear in specified
structural positions and linear positions (Chinenyanga
and Kushmerick, 2001; Wolff et al, 1999; Theobald and
Weilkum, 2000; Clarke et al, 1995). The models at-
tracted many researchers because they are considered to
be basic frameworks for retrieving or extracting complex
information like events. However, unlike IR by keyword-
based search, their models are not robust, that is, they
support only exact matching of queries, while we would
like to know to what degree the contents in specified
structural positions are relevant to those in the query even
when the structure does not exactly match the query.
This paper describes a new ranked retrieval model
that enables proximal and structural search for structured
texts. We extend the model proposed in Region Alge-
bra to be robust by i) incorporating the idea of ranked-
ness in keyword-based search, and ii) expanding queries.
While in ordinary ranked retrieval models relevance mea-
sures are computed in terms of words, our model assumes
that they are defined in more general structural fragments,
i.e., extents (continuous fragments in a text) proposed in
Region Algebra. We decompose queries into subqueries
to allow the system not only to retrieve exactly matched
extents but also to retrieve partially matched ones. Our
model is robust like keyword-based search, and also en-
ables us to specify the structural and linear positions in
texts as done by Region Algebra.
The significance of this work is not in the development
of a new relevance measure nor in showing superiority
of structure-based search over keyword-based search, but
in the proposal of a framework for integrating proximal
and structural ranking models. Since the model treats all
types of structures in texts, not only ordinary text struc-
tures like ?title,? ?abstract,? ?authors,? etc., but also se-
mantic tags corresponding to recognized named entities
or events can also be used for indexing text fragments
and contribute to the relevance measure. Since extents
are treated similarly to keywords in traditional models,
our model will be integrated with any ranking and scala-
bility techniques used by keyword-based models.
We have implemented the ranking model in our re-
trieval engine, and had preliminary experiments to eval-
uate our model. Unfortunately, we used a rather small
corpus for the experiments. This is mainly because
there is no test collection of the structured query and
tag-annotated text. Instead, we used the GENIA cor-
pus (Ohta et al, 2002) as structured texts, which was
an XML document annotated with semantics tags in the
filed of biomedical science. The experiments show that
our model succeeded in retrieving the relevant answers
that an exact-matching model fails to retrieve because of
lack of robustness, and the relevant answers that a non-
structured model fails because of lack of structural spec-
ification.
2 A Ranking Model for Structured
Queries and Texts
This section describes the definition of the relevance be-
tween a document and a structured query represented by
the region algebra. The key idea is that a structured query
is decomposed into subqueries, and the relevance of the
whole query is represented as a vector of relevance mea-
sures of subqueries.
The region algebra (Clarke et al, 1995) is a set of op-
erators, which represent the relation between the extents
(i.e. regions in texts). In this paper, we suppose the re-
gion algebra has seven operators; four containment oper-
ators (?, ?, 6?, 6?) representing the containment relation
between the extents, two combination operators (4, 5)
corresponding to ?and? and ?or? operator of the boolean
model, and ordering operator (3) representing the order
of words or structures in the texts. For convenience of
explanation, we represent a query as a tree structure as
  
 
	
		
 
Self-Organizing Markov Models and
Their Application to Part-of-Speech Tagging
Jin-Dong Kim
Dept. of Computer Science
University of Tokyo
jdkim@is.s.u-tokyo.ac.jp
Hae-Chang Rim
Dept. of Computer Science
Korea University
rim@nlp.korea.ac.kr
Jun?ich Tsujii
Dept. of Computer Science
University of Tokyo, and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a method to de-
velop a class of variable memory Markov
models that have higher memory capac-
ity than traditional (uniform memory)
Markov models. The structure of the vari-
able memory models is induced from a
manually annotated corpus through a de-
cision tree learning algorithm. A series of
comparative experiments show the result-
ing models outperform uniform memory
Markov models in a part-of-speech tag-
ging task.
1 Introduction
Many major NLP tasks can be regarded as prob-
lems of finding an optimal valuation for random
processes. For example, for a given word se-
quence, part-of-speech (POS) tagging involves find-
ing an optimal sequence of syntactic classes, and NP
chunking involves finding IOB tag sequences (each
of which represents the inside, outside and begin-
ning of noun phrases respectively).
Many machine learning techniques have been de-
veloped to tackle such random process tasks, which
include Hidden Markov Models (HMMs) (Rabiner,
1989), Maximum Entropy Models (MEs) (Rat-
naparkhi, 1996), Support Vector Machines
(SVMs) (Vapnik, 1998), etc. Among them,
SVMs have high memory capacity and show high
performance, especially when the target classifica-
tion requires the consideration of various features.
On the other hand, HMMs have low memory
capacity but they work very well, especially when
the target task involves a series of classifications that
are tightly related to each other and requires global
optimization of them. As for POS tagging, recent
comparisons (Brants, 2000; Schro?der, 2001) show
that HMMs work better than other models when
they are combined with good smoothing techniques
and with handling of unknown words.
While global optimization is the strong point of
HMMs, developers often complain that it is difficult
to make HMMs incorporate various features and to
improve them beyond given performances. For ex-
ample, we often find that in some cases a certain
lexical context can improve the performance of an
HMM-based POS tagger, but incorporating such ad-
ditional features is not easy and it may even degrade
the overall performance. Because Markov models
have the structure of tightly coupled states, an ar-
bitrary change without elaborate consideration can
spoil the overall structure.
This paper presents a way of utilizing statistical
decision trees to systematically raise the memory
capacity of Markov models and effectively to make
Markov models be able to accommodate various fea-
tures.
2 Underlying Model
The tagging model is probabilistically defined as
finding the most probable tag sequence when a word
sequence is given (equation (1)).
T (w1,k) = arg maxt1,k P (t1,k|w1,k) (1)
= arg max
t1,k
P (t1,k)P (w1,k|t1,k) (2)
? arg max
t1,k
k
?
i=1
P (ti|ti?1)P (wi|ti) (3)
By applying Bayes? formula and eliminating a re-
dundant term not affecting the argument maximiza-
tion, we can obtain equation (2) which is a combi-
nation of two separate models: the tag language
model, P (t1,k) and the tag-to-word translation
model, P (w1,k|t1,k). Because the number of word
sequences, w1,k and tag sequences, t1,k is infinite,
the model of equation (2) is not computationally
tractable. Introduction of Markov assumption re-
duces the complexity of the tag language model and
independent assumption between words makes the
tag-to-word translation model simple, which result
in equation (3) representing the well-known Hidden
Markov Model.
3 Effect of Context Classification
Let?s focus on the Markov assumption which is
made to reduce the complexity of the original tag-
ging problem and to make the tagging problem
tractable. We can imagine the following process
through which the Markov assumption can be intro-
duced in terms of context classification:
P (T = t1,k) =
k
?
i=1
P (ti|t1,i?1) (4)
?
k
?
i=1
P (ti|?(t1,i?1)) (5)
?
k
?
i=1
P (ti|ti?1) (6)
In equation (5), a classification function ?(t1,i?1) is
introduced, which is a mapping of infinite contextual
patterns into a set of finite equivalence classes. By
defining the function as follows we can get equation
(6) which represents a widely-used bi-gram model:
?(t1,i?1) ? ti?1 (7)
Equation (7) classifies all the contextual patterns
ending in same tags into the same classes, and is
equivalent to the Markov assumption.
The assumption or the definition of the above
classification function is based on human intuition.
( )conjP |?
( )conjfwP ,|?
( )conjvbP ,|?
( )conjvbpP ,|?
vbvb
vbpvbp
Figure 1: Effect of 1?st and 2?nd order context
atat
prepprep
nnnn( )prepP |?
( )in'',| prepP ?
( )with'',| prepP ?
( )out'',| prepP ?
Figure 2: Effect of context with and without lexical
information
Although this simple definition works well mostly,
because it is not based on any intensive analysis of
real data, there is room for improvement. Figure 1
and 2 illustrate the effect of context classification on
the compiled distribution of syntactic classes, which
we believe provides the clue to the improvement.
Among the four distributions showed in Figure 1,
the top one illustrates the distribution of syntactic
classes in the Brown corpus that appear after all the
conjunctions. In this case, we can say that we are
considering the first order context (the immediately
preceding words in terms of part-of-speech). The
following three ones illustrates the distributions col-
lected after taking the second order context into con-
sideration. In these cases, we can say that we have
extended the context into second order or we have
classified the first order context classes again into
second order context classes. It shows that distri-
butions like P (?|vb, conj) and P (?|vbp, conj) are
very different from the first order ones, while distri-
butions like P (?|fw, conj) are not.
Figure 2 shows another way of context extension,
so called lexicalization. Here, the initial first order
context class (the top one) is classified again by re-
ferring the lexical information (the following three
ones). We see that the distribution after the prepo-
sition, out is quite different from distribution after
other prepositions.
From the above observations, we can see that by
applying Markov assumptions we may miss much
useful contextual information, or by getting a better
context classification we can build a better context
model.
4 Related Works
One of the straightforward ways of context exten-
sion is extending context uniformly. Tri-gram tag-
ging models can be thought of as a result of the
uniform extension of context from bi-gram tagging
models. TnT (Brants, 2000) based on a second or-
der HMM, is an example of this class of models and
is accepted as one of the best part-of-speech taggers
used around.
The uniform extension can be achieved (rela-
tively) easily, but due to the exponential growth of
the model size, it can only be performed in restric-
tive a way.
Another way of context extension is the selective
extension of context. In the case of context exten-
sion from lower context to higher like the examples
in figure 1, the extension involves taking more infor-
mation about the same type of contextual features.
We call this kind of extension homogeneous con-
text extension. (Brants, 1998) presents this type of
context extension method through model merging
and splitting, and also prediction suffix tree learn-
ing (Schu?tze and Singer, 1994; D. Ron et. al, 1996)
is another well-known method that can perform ho-
mogeneous context extension.
On the other hand, figure 2 illustrates heteroge-
neous context extension, in other words, this type
of extension involves taking more information about
other types of contextual features. (Kim et. al, 1999)
and (Pla and Molina, 2001) present this type of con-
text extension method, so called selective lexicaliza-
tion.
The selective extension can be a good alternative
to the uniform extension, because the growth rate
of the model size is much smaller, and thus various
contextual features can be exploited. In the follow-
VP
N C
$
$ C N P V
P-1-
$ C N P V
Figure 3: a Markov model and its equivalent deci-
sion tree
ing sections, we describe a novel method of selective
extension of context which performs both homoge-
neous and heterogeneous extension simultaneously.
5 Self-Organizing Markov Models
Our approach to the selective context extension is
making use of the statistical decision tree frame-
work. The states of Markov models are represented
in statistical decision trees, and by growing the trees
the context can be extended (or the states can be
split).
We have named the resulting models Self-
Organizing Markov Models to reflect their ability to
automatically organize the structure.
5.1 Statistical Decision Tree Representation of
Markov Models
The decision tree is a well known structure that is
widely used for classification tasks. When there are
several contextual features relating to the classifi-
cation of a target feature, a decision tree organizes
the features as the internal nodes in a manner where
more informative features will take higher levels, so
the most informative feature will be the root node.
Each path from the root node to a leaf node repre-
sents a context class and the classification informa-
tion for the target feature in the context class will be
contained in the leaf node1 .
In the case of part-of-speech tagging, a classifi-
cation will be made at each position (or time) of a
word sequence, where the target feature is the syn-
tactic class of the word at current position (or time)
and the contextual features may include the syntactic
1While ordinary decision trees store deterministic classifi-
cation information in their leaves, statistical decision trees store
probabilistic distribution of possible decisions.
V
P,*,
N C
$
$ C N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
Figure 4: a selectively lexicalized Markov model
and its equivalent decision tree
V
P,*,
N
(N)C( )$
$ P-2- N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
(V)C( )
(*)C( )
(*)C( )(N)C( ) (V)C( )
Figure 5: a selectively extended Markov model and
its equivalent decision tree
classes or the lexical form of preceding words. Fig-
ure 3 shows an example of Markov model for a sim-
ple language having nouns (N), conjunctions (C),
prepositions (P) and verbs (V). The dollar sign ($)
represents sentence initialization. On the left hand
side is the graph representation of the Markov model
and on the right hand side is the decision tree repre-
sentation, where the test for the immediately preced-
ing syntactic class (represented by P-1) is placed on
the root, each branch represents a result of the test
(which is labeled on the arc), and the correspond-
ing leaf node contains the probabilistic distribution
of the syntactic classes for the current position2 .
The example shown in figure 4 involves a further
classification of context. On the left hand side, it is
represented in terms of state splitting, while on the
right hand side in terms of context extension (lexi-
calization), where a context class representing con-
textual patterns ending in P (a preposition) is ex-
tended by referring the lexical form and is classi-
fied again into the preposition, out and other prepo-
sitions.
Figure 5 shows another further classification of
2The distribution doesn?t appear in the figure explicitly. Just
imagine each leaf node has the distribution for the target feature
in the corresponding context.
context. It involves a homogeneous extension of
context while the previous one involves a hetero-
geneous extension. Unlike prediction suffix trees
which grow along an implicitly fixed order, decision
trees don?t presume any implicit order between con-
textual features and thus naturally can accommodate
various features having no underlying order.
In order for a statistical decision tree to be a
Markov model, it must meet the following restric-
tions:
? There must exist at least one contextual feature
that is homogeneous with the target feature.
? When the target feature at a certain time is clas-
sified, all the requiring context features must be
visible
The first restriction states that in order to be a
Markov model, there must be inter-relations be-
tween the target features at different time. The sec-
ond restriction explicitly states that in order for the
decision tree to be able to classify contextual pat-
terns, all the context features must be visible, and
implicitly states that homogeneous context features
that appear later than the current target feature can-
not be contextual features. Due to the second re-
striction, the Viterbi algorithm can be used with the
self-organizing Markov models to find an optimal
sequence of tags for a given word sequence.
5.2 Learning Self-Organizing Markov Models
Self-organizing Markov models can be induced
from manually annotated corpora through the SDTL
algorithm (algorithm 1) we have designed. It is a
variation of ID3 algorithm (Quinlan, 1986). SDTL
is a greedy algorithm where at each time of the node
making phase the most informative feature is se-
lected (line 2), and it is a recursive algorithm in the
sense that the algorithm is called recursively to make
child nodes (line 3),
Though theoretically any statistical decision tree
growing algorithms can be used to train self-
organizing Markov models, there are practical prob-
lems we face when we try to apply the algorithms to
language learning problems. One of the main obsta-
cles is the fact that features used for language learn-
ing often have huge sets of values, which cause in-
tensive fragmentation of the training corpus along
with the growing process and eventually raise the
sparse data problem.
To deal with this problem, the algorithm incor-
porates a value selection mechanism (line 1) where
only meaningful values are selected into a reduced
value set. The meaningful values are statistically
defined as follows: if the distribution of the target
feature varies significantly by referring to the value
v, v is accepted as a meaningful value. We adopted
the ?2-test to determine the difference between the
distributions of the target feature before and after re-
ferring to the value v. The use of ?2-test enables
us to make a principled decision about the threshold
based on a certain confidence level3.
To evaluate the contribution of contextual features
to the target classification (line 2), we adopted Lopez
distance (Lo?pez, 1991). While other measures in-
cluding Information Gain or Gain Ratio (Quinlan,
1986) also can be used for this purpose, the Lopez
distance has been reported to yield slightly better re-
sults (Lo?pez, 1998).
The probabilistic distribution of the target fea-
ture estimated on a node making phase (line 4) is
smoothed by using Jelinek and Mercer?s interpola-
tion method (Jelinek and Mercer, 1980) along the
ancestor nodes. The interpolation parameters are
estimated by deleted interpolation algorithm intro-
duced in (Brants, 2000).
6 Experiments
We performed a series of experiments to compare
the performance of self-organizing Markov models
with traditional Markov models. Wall Street Jour-
nal as contained in Penn Treebank II is used as the
reference material. As the experimental task is part-
of-speech tagging, all other annotations like syntac-
tic bracketing have been removed from the corpus.
Every figure (digit) in the corpus has been changed
into a special symbol.
From the whole corpus, every 10?th sentence from
the first is selected into the test corpus, and the re-
maining ones constitute the training corpus. Table 6
shows some basic statistics of the corpora.
We implemented several tagging models based on
equation (3). For the tag language model, we used
3We used 95% of confidence level to extend context. In
other words, only when there are enough evidences for improve-
ment at 95% of confidence level, a context is extended.
Algorithm 1: SDTL(E, t, F )
Data : E: set of examples,
t: target feature,
F : set of contextual features
Result : Statistical Decision Tree predicting t
initialize a null node;
for each element f in the set F do
1 sort meaningful value set V for f ;
if |V | > 1 then
2 measure the contribution of f to t;
if f contributes the most then
select f as the best feature b;
end
end
end
if there is b selected then
set the current node to an internal node;
set b as the test feature of the current node;
3 for each v in |V | for b do
make SDTL(Eb=v, t, F ? {b}) as the
subtree for the branch corresponding to
v;
end
end
else
set the current node to a leaf node;
4 store the probability distribution of t over
E ;
end
return current node;
1,289,20168,590Total
129,1006,859Test
1,160,10161,731Training
 	 
  	 
   	 
  	 
  	 
  	 
  	 
  	 
       	 
  	 
  	 
  	 
  	 
  	 
  	 
  	 
     
Figure 6: Basic statistics of corpora
the following 6 approximations:
P (t1,k) ?
k
?
i=1
P (ti|ti?1) (8)
?
k
?
i=1
P (ti|ti?2,i?1) (9)
?
k
?
i=1
P (ti|?(ti?2,i?1)) (10)
?
k
?
i=1
P (ti|?(ti?1, wi?1)) (11)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?1)) (12)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?2,i?1))(13)
Equation (8) and (9) represent first- and second-
order Markov models respectively. Equation (10)
? (13) represent self-organizing Markov models at
various settings where the classification functions
?(?) are intended to be induced from the training
corpus.
For the estimation of the tag-to-word translation
model we used the following model:
P (wi|ti)
= ki ? P (ki|ti) ? P? (wi|ti)
+(1 ? ki) ? P (?ki|ti) ? P? (ei|ti) (14)
Equation (14) uses two different models to estimate
the translation model. If the word, wi is a known
word, ki is set to 1 so the second model is ig-
nored. P? means the maximum likelihood probabil-
ity. P (ki|ti) is the probability of knownness gener-
ated from ti and is estimated by using Good-Turing
estimation (Gale and Samson, 1995). If the word, wi
is an unknown word, ki is set to 0 and the first term
is ignored. ei represents suffix of wi and we used the
last two letters for it.
With the 6 tag language models and the 1 tag-to-
word translation model, we construct 6 HMM mod-
els, among them 2 are traditional first- and second-
hidden Markov models, and 4 are self-organizing
hidden Markov models. Additionally, we used T3,
a tri-gram-based POS tagger in ICOPOST release
1.8.3 for comparison.
The overall performances of the resulting models
estimated from the test corpus are listed in figure 7.
From the leftmost column, it shows the model name,
the contextual features, the target features, the per-
formance and the model size of our 6 implementa-
tions of Markov models and additionally the perfor-
mance of T3 is shown.
Our implementation of the second-order hid-
den Markov model (HMM-P2) achieved a slightly
worse performance than T3, which, we are in-
terpreting, is due to the relatively simple imple-
mentation of our unknown word guessing module4.
While HMM-P2 is a uniformly extended model
from HMM-P1, SOHMM-P2 has been selectively
extended using the same contextual feature. It is
encouraging that the self-organizing model suppress
the increase of the model size in half (2,099Kbyte vs
5,630Kbyte) without loss of performance (96.5%).
In a sense, the results of incorporating word
features (SOHMM-P1W1, SOHMM-P2W1 and
SOHMM-P2W2) are disappointing. The improve-
ments of performances are very small compared to
the increase of the model size. Our interpretation
for the results is that because the distribution of
words is huge, no matter how many words the mod-
els incorporate into context modeling, only a few of
them may actually contribute during test phase. We
are planning to use more general features like word
class, suffix, etc.
Another positive observation is that a homo-
geneous context extension (SOHMM-P2) and a
heterogeneous context extension (SOHMM-P1W1)
yielded significant improvements respectively, and
the combination (SOHMM-P2W1) yielded even
more improvement. This is a strong point of using
decision trees rather than prediction suffix trees.
7 Conclusion
Through this paper, we have presented a framework
of self-organizing Markov model learning. The
experimental results showed some encouraging as-
pects of the framework and at the same time showed
the direction towards further improvements. Be-
cause all the Markov models are represented as de-
cision trees in the framework, the models are hu-
4T3 uses a suffix trie for unknown word guessing, while our
implementations use just last two letters.
?96.6??T3
96.9
96.8
96.3
96.5
96.5
95.6
 	
                 	

24,628KT0P-2, W-1, P-1SOHMM-P2W1
W-2, P-2, W-1, P-1
W-1, P-1
P-2, P-1
P-2, P-1
P-1
T0
T0
T0
T0
T0
14,247KSOHMM-P1W1
35,494K
2,099K
5,630K
123K
SOHMM-P2
SOHMM-P2W2 
HMM-P2
HMM-P1
                
Figure 7: Estimated Performance of Various Models
man readable and we are planning to develop editing
tools for self-organizing Markov models that help
experts to put human knowledge about language into
the models. By adopting ?2-test as the criterion for
potential improvement, we can control the degree of
context extension based on the confidence level.
Acknowledgement
The research is partially supported by Information
Mobility Project (CREST, JST, Japan) and Genome
Information Science Project (MEXT, Japan).
References
L. Rabiner. 1989. A tutorial on Hidden Markov Mod-
els and selected applications in speech recognition. in
Proceedings of the IEEE, 77(2):257?285
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, UK.
I. Schr o?der. 2001. ICOPOST - Ingo?s Collection
Of POS Taggers. In http://nats-www.informatik.uni-
hamburg.de/?ingo/icopost/.
T. Brants. 1998 Estimating HMM Topologies. In The
Tbilisi Symposium on Logic, Language and Computa-
tion: Selected Papers.
T. Brants. 2000 TnT - A Statistical Part-of-Speech Tag-
ger. In 6?th Applied Natural Language Processing.
H. Sch u?tze and Y. Singer. 1994. Part-of-speech tagging
using a variable memory Markov model. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
D. Ron, Y. Singer and N. Tishby. 1996 The Power of
Amnesia: Learning Probabilistic Automata with Vari-
able Memory Length. In Machine Learning, 25(2-
3):117?149.
J.-D. Kim, S.-Z. Lee and H.-C. Rim. 1999 HMM
Specialization with Selective Lexicalization. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large Cor-
pora(EMNLP/VLC99).
F. Pla and A. Molina. 2001 Part-of-Speech Tagging
with Lexicalized HMM. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing(RANLP2001).
R. Quinlan. 1986 Induction of decision trees. In Ma-
chine Learning, 1(1):81?106.
R. L o?pez de M a?ntaras. 1991. A Distance-Based At-
tribute Selection Measure for Decision Tree Induction.
In Machine Learning, 6(1):81?92.
R. L o?pez de M a?ntaras, J. Cerquides and P. Garcia. 1998.
Comparing Information-theoretic Attribute Selection
Measures: A statistical approach. In Artificial Intel-
ligence Communications, 11(2):91?100.
F. Jelinek and R. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pro-
ceedings of the Workshop on Pattern Recognition in
Practice.
W. Gale and G. Sampson. 1995. Good-Turing frequency
estimatin without tears. In Jounal of Quantitative Lin-
guistics, 2:217?237
A Debug Tool for Practical Grammar Development
Akane Yakushiji? Yuka Tateisi?? Yusuke Miyao?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,yoshinag,tsujii}@is.s.u-tokyo.ac.jp
Naoki Yoshinaga? Jun?ichi Tsujii??
Abstract
We have developed willex, a tool that
helps grammar developers to work effi-
ciently by using annotated corpora and
recording parsing errors. Willex has two
major new functions. First, it decreases
ambiguity of the parsing results by com-
paring them to an annotated corpus and
removing wrong partial results both au-
tomatically and manually. Second, willex
accumulates parsing errors as data for the
developers to clarify the defects of the
grammar statistically. We applied willex
to a large-scale HPSG-style grammar as
an example.
1 Introduction
There is an increasing need for syntactical parsers
for practical usages, such as information extrac-
tion. For example, Yakushiji et al (2001) extracted
argument structures from biomedical papers using
a parser based on XHPSG (Tateisi et al, 1998),
which is a large-scale HPSG. Although large-scale
and general-purpose grammars have been devel-
oped, they have a problem of limited coverage.
The limits are derived from deficiencies of gram-
mars themselves. For example, XHPSG cannot treat
coordinations of verbs (ex. ?Molybdate slowed but
did not prevent the conversion.?) nor reduced rel-
atives (ex. ?Rb mutants derived from patients with
retinoblastoma.?). Finding these grammar defects
and modifying them require tremendous human ef-
fort.
Hence, we have developed willex that helps to im-
prove the general-purpose grammars. Willex has two
major functions. First, it reduces a human workload
to improve the general-purpose grammar through
using language intuition encoded in syntactically
tagged corpora in XML format. Second, it records
data of grammar defects to allow developers to have
a whole picture of parsing errors found in the target
corpora to save debugging time and effort by priori-
tizing them.
2 What Is the Ideal Grammar Debugging?
There are already other grammar developing tools,
such as a grammar writer of XTAG (Paroubek et al,
1992), ALEP (Schmidt et al, 1996), ConTroll (Go?tz
and Meurers, 1997), a tool by Nara Institute of Sci-
ence and Technology (Miyata et al, 1999), and [incr
tsdb()] (Oepen et al, 2002). But these tools have
following problems; they largely depend on human
debuggers? language intuition, they do not help users
to handle large amount of parsing results effectively,
and they let human debuggers correct the bugs one
after another manually and locally.
To cope with these shortcomings, willex proposes
an alternative method for more efficient debugging
process.
The workflow of the conventional grammar devel-
oping tools and willex are different in the following
ways. With the conventional tools, human debug-
gers must check each sentence to find out grammar
defects and modify them one by one. On the other
hand, with willex human debuggers check sentences
that are tagged with syntactical structure, one by
one, find grammar defects, and record them, while
willex collects the whole grammar defect records.
Then human debuggers modify the found grammar
defects. This process allows human debuggers to
make priority over defects that appear more fre-
quently in the corpora, or defects that are more crit-
ical for purposes of syntactical parsing. Indeed, it
is possible for human debuggers using the conven-
tional tools to collect and modify the defects but
willex saves the trouble of human debuggers to col-
lect defects to modify them more efficiently.
3 Functions of willex
To create the new debugging tool, we have extended
will (Imai et al, 1998). Will is a browser of parsing
results of grammars based on feature structures. Will
and willex are implemented in JAVA.
3.1 Using XML Tagged Corpora
Willex uses sentence boundaries, word chunking,
and POSs/labels encoded in XML tagged corpora.
First, with the information of sentence boundaries
and word chunking, ambiguity of sentences is re-
duced, and ambiguity at parsing phase is also re-
duced. A parser connected to willex is assumed to
produce only results consistent with the information.
An example is shown in Figure 1 (<su> is a senten-
tial tag and <np> is a tag for noun phrases).
I  saw  a girl  with a telescope
I  saw  a girl  with a telescope


<su> I saw <np> a girl with a telescope </np></su>
Figure 1: An example of pa sing results along with
word chunking
Next, willex compares POSs/labels encoded in
XML tags and parsing results, and deletes improper
parsing trees. Therefore, it reduces numbers of par-
tial parsing trees, which appear in the way of parsing
and should be checked by human debuggers. In ad-
dition, human debuggers can delete partial parsing
trees manually later. Figure 2 shows a concrete ex-
ample. (NP and S are labels for noun and sentential
phrases respectively.)
POS/label from Tagged Corpus
POSs/labels from Partial Results
<NP> A cat </NP> knows everything
A      cat
D      N N       V
A      cat
NP S  
Figure 2: An example of deletion by using
POSs/labels
3.2 Output of Grammar Defects
Willex has a function to output information of gram-
mar defects into a file in order to collect the de-
fects data and treat them statistically. In addition,
we can save a log of debugging experiences which
show what grammar defects are found.
An example of an output file is shown in Table
1. It includes sentence numbers, word ranges in
which parsing failed, and comments input by a hu-
man debugger. For example, the first row of the ta-
ble means that the sentence #0 has coordinations of
verb phrases at position #3?#12, which cannot be
parsed. ?OK? in the second row means the sen-
tence is parsed correctly (i.e., no grammar defects
are found in the sentence). The third row means that
the word #4 of the sentence #2 has no proper lexical
entry.
The word ranges are specified by human debug-
gers using a GUI, which shows parsing results in
CKY tables and parse trees. The comments are input
by human debuggers in a natural language or chosen
from the list of previous comments. A postprocess-
ing module of willex sorts the error data by the com-
ments to help statistical analysis.
Table 1: An example of file output
Sentence # Word # comment
0 3?12 V-V coordination
1 ? OK
2 4 no lexical entry
4 Experiments and Discussion
We have applied willex to rental-XTAG, an HPSG-
style grammar converted from the XTAG English
grammar (The XTAG Research Group, 2001) by a
grammar conversion (Yoshinaga and Miyao, 2001).1
The corpus used is MEDLINE abstracts with tags
based on a slightly modified version of GDA-
DTD2 (Hasida, 2003). The corpus is ?partially
parsed?; the attachments of prepositional phrases are
annotated manually.
The tags do not always specify the correct struc-
tures based on rental-XTAG (i.e., the grammar as-
sumed by tags is different from rental-XTAG), so we
prepared a POS/label conversion table. We can use
tagged corpora based on various grammars different
from the grammar that the parser is assuming by us-
ing POS/label conversion tables.
We investigated 208 sentences (average 24.2
words) from 26 abstracts. 73 sentences were parsed
successfully and got correct results. Thus the cover-
age was 35.1%.
4.1 Qualitative Evaluation
Willex received three major positive feedbacks from
a user; first, the function of restricting partial results
was helpful, as it allows human debuggers to check
fewer results, second, the function to delete incorrect
partial results manually was useful, because there
are some cases that tags do not specify POSs/labels,
and third, human debuggers could use the record-
ing function to make notes to analyze them carefully
later.
However, willex also received some negative eval-
uations; the process of locating the cause of pars-
ing failure in a sentence was found to be a bit trou-
blesome. Also, willex loses its accuracy if the hu-
man debuggers themselves have trouble understand-
ing the correct syntactical structure of a sentence.3
1Since XTAG and rental-XTAG generate equivalent parse
results for the same input, debugging rental-XTAG means de-
bugging XTAG itself.
2GDA has no tags which specify prepositional phrases, so
we add <prep> and <prepp>.
3Thus, we divided the process of identifying grammar de-
fects to two steps. First, a non-expert roughly classifies pars-
ing errors and records temporary memorandums. Then, the
non-expert shows typical examples of sentences in each class
to experts and identifies grammar defects based on experts? in-
ference. Here, we can make use of the recording function of
We found from these evaluations that the func-
tions of willex can be used effectively, though more
automation is needed.
4.2 Quantitative Evaluation
Figure 3 shows the decrease in partial parsing trees
caused by using the tagged corpus. (Data of 10 sen-
tences among the 208 sentences are shown.) The
graph shows that human workload was reduced by
using the tagged corpus.
0
5000
10000
15000
20000
25000
30000
35000
10 15 20 25 30 35 40
n
u
m
b
e
r
 
o
f
 
p
a
r
t
i
a
l
 
r
e
s
u
l
t
s
length of a sentence (number of words)
without any info.with chunk info.with chunk and POS/label info.
Figure 3: Examples of numbers of partial results
4.3 Defects of rental-XTAG
Table 2 shows the defects of rental-XTAG which are
found by using willex.
Table 2: The defects of rental-XTAG
the defects of rental-XTAG #
no lexical entry 62
cannot handle reduced relative 35
cannot handle V-V coordination 22
Adjective does not post-modify NP 9
cannot parse ?, but not? 4
cannot handle objective to-infinitive 3
?, which ...? does not post-modify NP 3
cannot handle reduced as-relative clause 2
cannot parse ?greater than?(?>?) 2
misc. 17
From this table, it is inferred that (1) lack of lexi-
cal entries, (2) inability to parse reduced relative and
willex.
(3) inability to parse coordinations of verbs are seri-
ous problems of rental-XTAG.
4.4 Conflicts Between the Modified GDA and
rental-XTAG
Conflicts between rental-XTAG and the grammar on
which the modified GDA based cause parsing fail-
ures. Statistics of the conflicts is shown in Table 3.
Table 3: Conflicts between the modified GDA and
rental-XTAG
modified GDA rental-XTAG #
adjectival phrase verbal phrase 36
bracketing except ?,? 10
bracketing of ?,? 8
treatment of omitted words 2
misc. 5
These conflicts cannot be resolved by a simple
POS/label conversion table. One resolution is insert-
ing a preprocess module that deletes and moves tags
which cause conflicts.
We do not consider these conflicts as grammar de-
fects but the difference of grammars to be absorbed
in the conversion phase.
5 Conclusion and Future Work
We developed a debug tool, willex, which uses XML
tagged corpora and outputs information of grammar
defects. By using tagged corpora, willex succeeded
to reduce human workload. And by recording gram-
mar defects, it provides debugging environment with
a bigger perspective. But there remains a prob-
lem that a simple POS/label conversion table is not
enough to resolve conflicts of a debugged grammar
and a grammar assumed by tags. The tool should
support to handle the complicated conflicts.
In the future, we will try to modify willex to infer
causes of parsing errors (semi-)automatically. It is
difficult to find a point of parsing failure automati-
cally, because subsentences that have no correspon-
dent partial results are not always the failed point.
Hence, we will expand willex to find the longest
subsentences that are parsed successfully. Words,
POS/labels and features of the subsentences can be
clues to infer the causes of parsing errors.
References
Thilo Go?tz and Walt Detmar Meurers. 1997. The Con-
Troll system as large grammar development platform.
In Proc. of Workshop on Computational Environments
for Grammar Development and Linguistic Engineer-
ing, pages 38?45.
Koiti Hasida. 2003. Global docu-
ment annotation (GDA). available in
http://www.i-content.org/GDA/.
Hisao Imai, Yusuke Miyao, and Jun?ichi Tsujii. 1998.
GUI for an HPSG parser. In Information Processing
Society of Japan SIG Notes NL-127, pages 173?178,
September. In Japanese.
Takashi Miyata, Kazuma Takaoka, and Yuji Mat-
sumoto. 1999. Implementation of GUI debugger for
unification-based grammar. In Information Process-
ing Society of Japan SIG Notes NL-129, pages 87?94,
January. In Japanese.
Stephan Oepen, Emily M. Bender, Uli Callmeier, Dan
Flickinger, and Melanie Siegel. 2002. Parallel dis-
tributed grammar engineering for practical applica-
tions. In Proc. of the Workshop on Grammar Engi-
neering and Evaluation, pages 15?21.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. XTAG ? a graphical workbench for developing
Tree-Adjoining grammars. In Proc. of the 3rd Confer-
ence on Applied Natural Language Processing, pages
216?223.
Paul Schmidt, Axel Theofilidis, Sibylle Rieder, and
Thierry Declerck. 1996. Lean formalisms, linguis-
tic theory, and applications. Grammar development in
ALEP. In Proc. of COLING ?96, volume 1, pages
286?291.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG english
grammar to HPSG. In Proc. of TAG+4 workshop,
pages 172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
Technical Report IRCS Research Report 01-03,
IRCS, University of Pennsylvania. available in
http://www.cis.upenn.edu/?xtag/.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomedi-
cal papers using a full parser. In Pacific Symposium on
Biocomputing 2001, pages 408?419, January.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from LTAG to HPSG. In Proc. of the sixth
ESSLLI Student Session, pages 309?324.
Comparison between CFG filtering techniques for LTAG and HPSG
Naoki Yoshinaga?
? University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-0033, Japan
yoshinag@is.s.u-tokyo.ac.jp
Kentaro Torisawa?
? Japan Advanced Institute
of Science and Technology
1-1 Asahidai, Tatsunokuchi,
Ishikawa, 923-1292, Japan
torisawa@jaist.ac.jp
Jun?ichi Tsujii??
? CREST, JST (Japan Science
and Technology Corporation)
Hon-cho 4-1-8, Kawaguchi-shi,
Saitama, 332-0012, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
An empirical comparison of CFG filtering
techniques for LTAG and HPSG is pre-
sented. We demonstrate that an approx-
imation of HPSG produces a more effec-
tive CFG filter than that of LTAG. We also
investigate the reason for that difference.
1 Introduction
Various parsing techniques have been developed
for lexicalized grammars such as Lexicalized
Tree Adjoining Grammar (LTAG) (Schabes et al,
1988), and Head-Driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994). Along with
the independent development of parsing techniques
for individual grammar formalisms, some of them
have been adapted to other formalisms (Schabes et
al., 1988; van Noord, 1994; Yoshida et al, 1999;
Torisawa et al, 2000). However, these realiza-
tions sometimes exhibit quite different performance
in each grammar formalism (Yoshida et al, 1999;
Yoshinaga et al, 2001). If we could identify an al-
gorithmic difference that causes performance differ-
ence, it would reveal advantages and disadvantages
of the different realizations. This should also allow
us to integrate the advantages of the realizations into
one generic parsing technique, which yields the fur-
ther advancement of the whole parsing community.
In this paper, we compare CFG filtering tech-
niques for LTAG (Harbusch, 1990; Poller and
Becker, 1998) and HPSG (Torisawa et al, 2000;
Kiefer and Krieger, 2000), following an approach to
parsing comparison among different grammar for-
malisms (Yoshinaga et al, 2001). The key idea
of the approach is to use strongly equivalent gram-
mars, which generate equivalent parse results for the
same input, obtained by a grammar conversion as
demonstrated by Yoshinaga and Miyao (2001). The
parsers with CFG filtering predict possible parse
trees by a CFG approximated from a given grammar.
Comparison of those parsers are interesting because
effective CFG filters allow us to bring the empirical
time complexity of the parsers close to that of CFG
parsing. Investigating the difference between the
ways of context-free (CF) approximation of LTAG
and HPSG will thereby enlighten a way of further
optimization for both techniques.
We performed a comparison between the exist-
ing CFG filtering techniques for LTAG (Poller and
Becker, 1998) and HPSG (Torisawa et al, 2000),
using strongly equivalent grammars obtained by
converting LTAGs extracted from the Penn Tree-
bank (Marcus et al, 1993) into HPSG-style. We
compared the parsers with respect to the size of the
approximated CFG and its effectiveness as a filter.
2 Background
In this section, we introduce a grammar conver-
sion (Yoshinaga and Miyao, 2001) and CFG filter-
ing (Harbusch, 1990; Poller and Becker, 1998; Tori-
sawa et al, 2000; Kiefer and Krieger, 2000).
2.1 Grammar conversion
The grammar conversion consists of a conversion
of LTAG elementary trees to HPSG lexical entries
and an emulation of substitution and adjunction by
S
NP VP
V NP
S
NP VP
V S5.1
5.?
5.2
5.2.1 5.2.2
9.1
9.?
9.2
9.2.1 9.2.2
Tree 5: Tree 9: SCFG rulesNP VP
VP V NPVP V S
5.? 5.1 5.29.? 9.1 9.2
5.2 5.2.1 5.2.2
9.2 9.2.1 9.2.2
Figure 1: Extraction of CFG from LTAG
pre-determined grammar rules. An LTAG elemen-
tary tree is first converted into canonical elementary
trees which have only one anchor and whose sub-
trees of depth n (?1) contain at least one anchor. A
canonical elementary tree is then converted into an
HPSG lexical entry by regarding the leaf nodes as
arguments and by storing them in a stack.
We can perform a comparison between LTAG and
HPSG parsers using strongly equivalent grammars
obtained by the above conversion. This is because
strongly equivalent grammars can be a substitute for
the same grammar in different grammar formalisms.
2.2 CFG filtering techniques
An initial offline step of CFG filtering is performed
to approximate a given grammar with a CFG. The
obtained CFG is used as an efficient device to com-
pute the necessary conditions for parse trees.
The CFG filtering generally consists of two steps.
In phase 1, the parser first predicts possible parse
trees using the approximated CFG, and then filters
out irrelevant edges by a top-down traversal starting
from roots of successful context-free derivations. In
phase 2, it then eliminates invalid parse trees by us-
ing constraints in the given grammar. We call the
remaining edges that are used for the phase 2 pars-
ing essential edges.
The parsers with CFG filtering used in our ex-
periments follow the above parsing strategy, but are
different in the way the CF approximation and the
elimination of impossible parse trees in phase 2 are
performed. In the following sections, we briefly de-
scribe the CF approximation and the elimination of
impossible parse trees in each realization.
2.2.1 CF approximation of LTAG
In CFG filtering techniques for LTAG (Harbusch,
1990; Poller and Becker, 1998), every branching of
elementary trees in a given grammar is extracted as
a CFG rule as shown in Figure 1.
Grammar rule
lexicalSYNSEM  ?
signSYNSEM  ?signSYNSEM  ?
phrasalSYNSEM  ?
Grammar rule
phrasalSYNSEM  ?
signSYNSEM  ?signSYNSEM  ?
phrasalSYNSEM  ?
phrasalSYNSEM  ?
A
B
C
X
Y
B X AC Y B
signSYNSEM  ?
signSYNSEM  ?
CFG rules
Figure 2: Extraction of CFG from HPSG
Because the obtained CFG can reflect only local
constraints given in each local structure of the el-
ementary trees, it generates invalid parse trees that
connect local trees in different elementary trees. In
order to eliminate such parse trees, a link between
branchings is preserved as a node number which
records a unique node address (a subscript attached
to each node in Figure 1). We can eliminate these
parse trees by traversing essential edges in a bottom-
up manner and recursively propagating ok-flag from
a node number x to a node number y when a connec-
tion between x and y is allowed in the LTAG gram-
mar. We call this propagation ok-prop.
2.2.2 CF approximation of HPSG
In CFG filtering techniques for HPSG (Torisawa
et al, 2000; Kiefer and Krieger, 2000), the extrac-
tion process of a CFG from a given HPSG gram-
mar starts by recursively instantiating daughters of a
grammar rule with lexical entries and generated fea-
ture structures until new feature structures are not
generated as shown in Figure 2. We must impose
restrictions on values of some features (i.e., ignor-
ing them) and/or the number of rule applications in
order to guarantee the termination of the rule appli-
cation. A CFG is obtained by regarding each initial
and generated feature structures as nonterminals and
transition relations between them as CFG rules.
Although the obtained CFG can reflect local and
global constraints given in the whole structure of
lexical entries, it generates invalid parse trees be-
cause they do not reflect upon constraints given by
the values of features that are ignored in phase 1.
These parse trees are eliminated in phase 2 by apply-
ing a grammar rule that corresponds to the applied
CFG rule. We call this rule application rule-app.
Table 1: The size of extracted LTAGs (tree tem-
plates) and approximated CFGs (above: the number
of nonterminals; below: the number of rules)
Grammar G2 G2-4 G2-6 G2-8 G2-10 G2-21
LTAG 1,488 2,412 3,139 3,536 3,999 6,085
CFGPB 65 66 66 66 67 67
716 954 1,090 1,158 1,229 1,552
CFGTNT 1,989 3,118 4,009 4,468 5,034 7,454
18,323 35,541 50,115 58,356 68,239 118,464
Table 2: Parsing performance (sec.) with the
strongly equivalent grammars for Section 2 of WSJ
Parser G2 G2-4 G2-6 G2-8 G2-10 G2-21
PB 1.4 9.1 17.4 24.0 34.2 124.3
TNT 0.044 0.097 0.144 0.182 0.224 0.542
3 Comparison with CFG filtering
In this section, we compare a pair of CFG filter-
ing techniques for LTAG (Poller and Becker, 1998)
and HPSG (Torisawa et al, 2000) described in Sec-
tion 2.2.1 and 2.2.2. We hereafter refer to PB and
TNT for the C++ implementations of the former and
a valiant1 of the latter, respectively.2
We first acquired LTAGs by a method pro-
posed in Miyao et al (2003) from Sections 2-21 of
the Wall Street Journal (WSJ) in the Penn Tree-
bank (Marcus et al, 1993) and its subsets.3 We then
converted them into strongly equivalent HPSG-style
grammars using the grammar conversion described
in Section 2.1. Table 1 shows the size of CFG ap-
proximated from the strongly equivalent grammars.
Gx, CFGPB, and CFGTNT henceforth refer to the
LTAG extracted from Section x of WSJ and CFGs
approximated from Gx by PB and TNT, respectively.
The size of CFGTNT is much larger than that of
CFGPB. By investigating parsing performance using
these CFGs, we show that the larger size of CFGTNT
resulted in better parsing performance.
Table 2 shows the parse time with 254 sentences
of length n (?10) from Section 2 of WSJ (the av-
erage length is 6.72 words).4 This result shows not
only that TNT achieved a drastic speed-up against
1All daughters of rules are instantiated in the approximation.
2In phase 1, PB performs Earley (Earley, 1970) parsing
while TNT performs CKY (Younger, 1967) parsing.
3The elementary trees in the LTAGs are binarized.
4We used a subset of the training corpus to avoid the com-
plication of using default lexical entries for unknown words.
Table 3: The numbers of essential edges with the
strongly equivalent grammars for Section 02 of WSJ
Parser G2 G2-4 G2-6 G2-8 G2-10 G2-21
PB 791 1,435 1,924 2,192 2,566 3,976
TNT 63 121 174 218 265 536
Table 4: The success rate (%) of phase 2 operations
Operations G2 G2-4 G2-6 G2-8 G2-10 G2-21
ok-prop (PB) 38.5 34.3 33.1 32.3 31.7 31.0
rule-app (TNT) 100 100 100 100 100 100
PB, but also that performance difference between
them increases with the larger size of the grammars.
In order to estimate the degree of CF approxima-
tion, we measured the number of essential (inactive)
edges of phase 1. Table 3 shows the number of the
essential edges. The number of essential edges pro-
duced by PB is much larger than that produced by
TNT . We then investigated the effect on phase 2
as caused by the different number of the essential
edges. Table 4 shows the success rate of ok-prop
and rule-app. The success rate of rule-app is 100%,5
whereas that of ok-prop is quite low.6 These results
indicate that CFGTNT is superior to CFGPB with re-
spect to the degree of the CF approximation.
We can explain the reason for this difference by
investigating how TNT approximates HPSG-style
grammars converted from LTAGs. As described
in Section 2.1, the grammar conversion preserves
the whole structure of each elementary tree (pre-
cisely, a canonical elementary tree) in a stack, and
grammar rules manipulate a head element of the
stack. A generated feature structure in the approxi-
mation process thus corresponds to the whole unpro-
cessed portion of a canonical elementary tree. This
implies that successful context-free derivations ob-
tained by CFGTNT basically involve elementary trees
in which all substitution and adjunction have suc-
ceeded. However, CFGPB (also a CFG produced
by the other work (Harbusch, 1990)) cannot avoid
generating invalid parse trees that connect two lo-
5This means that the extracted LTAGs should be compatible
with CFG and were completely converted to CFGs by TNT .
6Similar results were obtained in preliminary experiments
using the XTAG English grammar (The XTAG Research Group,
2001) without features (parse time (sec.)/success rate (%) for
PB and TNT were 15.3/30.6 and 0.606/71.2 with the same sen-
tences), though space limitations preclude complete results.
cal structures where adjunction takes place between
them. We measured with G2-21 the proportion of the
number of ok-prop between two node numbers of
nodes that take adjunction and its success rate. It
occupied 87% of the total number of ok-prop and
its success rate was only 22%. These results sug-
gest that the global contexts in a given grammar is
essential to obtain an effective CFG filter.
It should be noted that the above investigation also
tells us another way of CF approximation of LTAG.
We first define a unique way of tree traversal such as
head-corner traversal (van Noord, 1994) on which
we can perform a sequential application of substitu-
tion and adjunction. We then recursively apply sub-
stitution and adjunction on that traversal to an ele-
mentary tree and a generated tree structure. Because
the processed portions of generated tree structures
are no longer used later, we regard the unprocessed
portions of the tree structures as nonterminals of
CFG. We can thereby construct another CFG filter-
ing for LTAG by combining this CFG filter with an
existing LTAG parsing algorithm (van Noord, 1994).
4 Conclusion and future direction
We presented an empirical comparison of LTAG and
HPSG parsers with CFG filtering. We compared the
parsers with strongly equivalent grammars obtained
by converting LTAGs extracted from the Penn Tree-
bank into HPSG-style. Experimental results showed
that the existing CF approximation of HPSG (Tori-
sawa et al, 2000) produced a more effective filter
than that of LTAG (Poller and Becker, 1998). By in-
vestigating the different ways of CF approximation,
we concluded that the global constraints in a given
grammar is essential to obtain an effective filter.
We are going to integrate the advantage of the CF
approximation of HPSG into that of LTAG in order
to establish another CFG filtering for LTAG. We will
also conduct experiments on trade-offs between the
degree of CF approximation and the size of approx-
imated CFGs as in Maxwell III and Kaplan (1993).
Acknowledgment
We thank Yousuke Sakao for his help in profiling
TNT parser and anonymous reviewers for their help-
ful comments. This work was partially supported by
JSPS Research Fellowships for Young Scientists.
References
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
K. Harbusch. 1990. An efficient parsing algorithm for
Tree Adjoining Grammars. In Proc. of ACL, pages
284?291.
B. Kiefer and H.-U. Krieger. 2000. A Context-Free ap-
proximation of Head-Driven Phrase Structure Gram-
mar. In Proc. of IWPT, pages 135?146.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):313?330.
J. T. Maxwell III and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?590.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Lexicalized
grammar acquisition. In Proc. of EACL companion
volume, pages 127?130.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
P. Poller and T. Becker. 1998. Two-step TAG parsing
revisited. In Proc. of TAG+4, pages 143?146.
Y. Schabes, A. Abeille?, and A. K. Joshi. 1988. Pars-
ing strategies with ?lexicalized? grammars: Applica-
tion to Tree Adjoining Grammars. In Proc. of COL-
ING, pages 578?583.
The XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
K. Torisawa, K. Nishida, Y. Miyao, and J. Tsujii. 2000.
An HPSG parser with CFG filtering. Natural Lan-
guage Engineering, 6(1):63?80.
G. van Noord. 1994. Head corner parsing for TAG.
Computational Intelligence, 10(4):525?534.
M. Yoshida, T. Ninomiya, K. Torisawa, T. Makino, and
J. Tsujii. 1999. Efficient FB-LTAG parser and its par-
allelization. In Proc. of PACLING, pages 90?103.
N. Yoshinaga and Y. Miyao. 2001. Grammar conver-
sion from LTAG to HPSG. In Proc. of ESSLLI Student
Session, pages 309?324.
N. Yoshinaga, Y. Miyao, K. Torisawa, and J. Tsujii.
2001. Efficient LTAG parsing using HPSG parsers. In
Proc. of PACLING, pages 342?351.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 2(10):189?208, February.
Finding Anchor Verbs for Biomedical IE
Using Predicate-Argument Structures
Akane YAKUSHIJI? Yuka TATEISI?? Yusuke MIYAO?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi TSUJII??
Abstract
For biomedical information extraction, most sys-
tems use syntactic patterns on verbs (anchor verbs)
and their arguments. Anchor verbs can be se-
lected by focusing on their arguments. We propose
to use predicate-argument structures (PASs), which
are outputs of a full parser, to obtain verbs and their
arguments. In this paper, we evaluated PAS method
by comparing it to a method using part of speech
(POSs) pattern matching. POS patterns produced
larger results with incorrect arguments, and the re-
sults will cause adverse effects on a phase selecting
appropriate verbs.
1 Introduction
Research in molecular-biology field is discovering
enormous amount of new facts, and thus there is
an increasing need for information extraction (IE)
technology to support database building and to find
novel knowledge in online journals.
To implement IE systems, we need to construct
extraction rules, i.e., rules to extract desired infor-
mation from processed resource. One subtask of the
construction is defining a set of anchor verbs, which
express realization of desired information in natural
language text.
In this paper, we propose a novel method of
finding anchor verbs: extracting anchor verbs from
predicate-argument structures (PASs) obtained by
full parsing. We here discuss only finding anchor
verbs, although our final purpose is construction
of extraction rules. Most anchor verbs take topi-
cal nouns, i.e., nouns describing target entities for
IE, as their arguments. Thus verbs which take top-
ical nouns can be candidates for anchor verbs. Our
method collects anchor verb candidates by choosing
PASs whose arguments are topical nouns. Then, se-
mantically inappropriate verbs are filtered out. We
leave this filtering phase as a future work, and dis-
cuss the acquisition of candidates. We have also in-
vestigated difference in verbs and their arguments
extracted by naive POS patterns and PAS method.
When anchor verbs are found based on whether
their arguments are topical nouns, like in (Hatzivas-
siloglou and Weng, 2002), it is important to obtain
correct arguments. Thus, in this paper, we set our
goal to obtain anchor verb candidates and their cor-
rect arguments.
2 Background
There are some works on acquiring extraction rules
automatically. Sudo et al (2003) acquired subtrees
derived from dependency trees as extraction rules
for IE in general domains. One problem of their sys-
tem is that dependency trees cannot treat non-local
dependencies, and thus rules acquired from the con-
structions are partial. Hatzivassiloglou and Weng
(2002) used frequency of collocation of verbs and
topical nouns and verb occurrence rates in several
domains to obtain anchor verbs for biological inter-
action. They used only POSs and word positions
to detect relations between verbs and topical nouns.
Their performance was 87.5% precision and 82.4%
recall. One of the reasons of errors they reported is
failures to detect verb-noun relations.
To avoid these problems, we decided to use PASs
obtained by full parsing to get precise relations be-
tween verbs and their arguments. The obtained pre-
cise relations will improve precision. In addition,
PASs obtained by full parsing can treat non-local
dependencies, thus recall will also be improved.
The sentence below is an example which sup-
ports advantage of full parsing. A gerund ?activat-
ing? takes a non-local semantic subject ?IL-4?. In
full parsing based on Head-Driven Phrase Structure
Grammar (HPSG) (Sag and Wasow, 1999), the sub-
ject of the whole sentence and the semantic subject
of ?activating? are shared, and thus we can extract
the subject of ?activating?.
IL-4 may mediate its biological effects by activat-
ing a tyrosine-phosphorylated DNA binding pro-
tein.
interacts
ARG1 it
1 with
MODIFY
ARG1      regions
2
1
of
MODIFY
ARG1 molecules
2
,
,
(a) (b) (c)
It interacts with non-polymorphic regions of major his-
tocompatibility complex class II molecules.
Figure 1: PAS examples
with
MODIFY
interacts
ARG1 it
ARG1 regions
Core verb
serves
ARG1      IL-5
1
ARG2
to
ARG1
ARG2
stimulate
ARG1
ARG2 binding
1
Core verb
1
Figure 2: Core verbs of PASs
3 Anchor Verb Finding by PASs
By using PASs, we extract candidates for anchor
verbs from a sentence in the following steps:
1. Obtain all PASs of a sentence by a full
parser. The PASs correspond not only to verbal
phrases but also other phrases such as preposi-
tional phrases.
2. Select PASs which take one or more topical
nouns as arguments.
3. From the selected PASs in Step 2, select PASs
which include one or more verbs.
4. Extract a core verb, which is the innermost ver-
bal predicate, from each of the chosen PASs.
In Step 1, we use a probabilistic HPSG parser
developed by Miyao et al (2003), (2004). PASs
obtained by the parser are illustrated in Figure 1.1
Bold words are predicates. Arguments of the predi-
cates are described in ARGn (n = 1, 2, . . .). MOD-
IFY denotes the modified PAS. Numbers in squares
denote shared structures. Examples of core verbs
are illustrated in Figure 2. We regard all arguments
in a PAS are arguments of the core verb.
Extraction of candidates for anchor verbs from
the sentence in Figure 1 is as follows. Here, ?re-
gions? and ?molecules? are topical nouns.
In Step 1, we obtain all the PASs, (a), (b) and (c),
in Figure 1.
1Here, named entities are regarded as chunked, and thus
internal structures of noun phrases are not illustrated.
Next, in Step 2, we check each argument of (a),
(b) and (c). (a) is discarded because it does not have
a topical noun argument.2 (b) is selected because
ARG1 ?regions? is a topical noun. Similarly, (c) is
selected because of ARG1 ?molecules?.
And then, in Step 3, we check each POS of a
predicate included in (b) and (c). (b) is selected be-
cause it has the verb ?interacts? in 1 which shares
the structure with (a). (c) is discarded because it
includes no verbs.
Finally, in Step 4, we extract a core verb from (b).
(b) includes 1 asMODIFY, and the predicate of 1
is the verb, ?interacts?. So we extract it.
4 Experiments
We investigated the verbs and their arguments ex-
tracted by PAS method and POS pattern matching,
which is less expressive in analyzing sentence struc-
tures but would be more robust.
For topical nouns and POSs, we used the GENIA
corpus (Kim et al, 2003), a corpus of annotated ab-
stracts taken from National Library of Medicine?s
MEDLINE database. We defined topical nouns as
the names tagged as protein, peptide, amino acid,
DNA, RNA, or nucleic acid. We chose PASs which
take one or more topical nouns as an argument or
arguments, and substrings matched by POS patterns
which include topical nouns. All names tagged in
the corpus were replaced by their head nouns in
order to reduce complexity of sentences and thus
reduce the task of the parser and the POS pattern
matcher.
4.1 Implementation of PAS method
We implemented PAS method on LiLFeS, a
unification-based programming system for typed
feature structures (Makino et al, 1998; Miyao et al,
2000).
The selection in Step 2 described in Section 3
is realized by matching PASs with nine PAS tem-
plates. Four of the templates are illustrated in Fig-
ure 3.
4.2 POS Pattern Method
We constructed a POS pattern matcher with a par-
tial verb chunking function according to (Hatzivas-
siloglou and Weng, 2002). Because the original
matcher has problems in recall (its verb group de-
tector has low coverage) and precision (it does not
consider other words to detect relations between
verb groups and topical nouns), we implemented
2(a) may be selected if the anaphora (?it?) is resolved. But
we regard anaphora resolving is too hard task as a subprocess
of finding anchor verbs.
*any*
ARG1 N1
N1 = topical noun
*any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
? ?
*any*
MODIFY *any*
ARG1 N1
N1 = topical noun
*any*
MODIFY *any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
Figure 3: PAS templates
N ? V G ? N
N ? V G
V G ? N
N : is a topical noun
V G: is a verb group which is accepted by a finite state
machine described in (Hatzivassiloglou andWeng, 2002)
or one of {VB, VBD, VBG, VBN, VBP, VBZ}
?: is 0?4 tokens which do not include {FW, NN, NNS,
NNP, NNPS, PRP, VBG, WP, *}
(Parts in Bold letters are added to the patterns of Hatzi-
vassiloglou and Weng (2002).)
Figure 4: POS patterns
our POS pattern matcher as a modified version of
one in (Hatzivassiloglou and Weng, 2002).
Figure 4 shows patterns in our experiment. The
last verb of V G is extracted if all of Ns are topical
nouns. Non-topical nouns are disregarded. Adding
candidates for verb groups raises recall of obtained
relations of verbs and their arguments. Restriction
on intervening tokens to non-nouns raises the preci-
sion, although it decreases the recall.
4.3 Experiment 1
We extracted last verbs of POS patterns and core
verbs of PASs with their arguments from 100 ab-
stracts (976 sentences) of the GENIA corpus. We
took up not the verbs only but tuples of the verbs
and their arguments (VAs), in order to estimate ef-
fect of the arguments on semantical filtering.
Results
The numbers of VAs extracted from the 100 ab-
stracts using POS patterns and PASs are shown in
Table 1. (Total ? VAs of verbs not extracted by the
other method) are not the same, because more than
one VA can be extracted on a verb in a sentence.
POS patterns method extracted more VAs, although
POS patterns PASs
Total 1127 766
VAs of verbs
not extracted 478 105
by the other
Table 1: Numbers of VAs extracted from the 100
abstracts
Appropriate Inappropriate Total
Correct 43 12 55
Incorrect 20 23 43
Total 63 35 98
Table 2: Numbers of VAs extracted by POS patterns
(in detail)
their correctness is not considered.
4.4 Experiment 2
For the first 10 abstracts (92 sentences), we man-
ually investigated whether extracted VAs are syn-
tactically or semantically correct. The investigation
was based on two criteria: ?appropriateness? based
on whether the extracted verb can be used for an an-
chor verb and ?correctness? based on whether the
syntactical analysis is correct, i.e., whether the ar-
guments were extracted correctly.
Based on human judgment, the verbs that rep-
resent interactions, events, and properties were se-
lected as semantically appropriate for anchor verbs,
and the others were treated as inappropriate. For ex-
ample, ?identified? in ?We identified ZEBRA pro-
tein.? is not appropriate and discarded.
We did not consider non-topical noun arguments
for POS pattern method, whereas we considered
them for PAS method. Thus decision on correctness
is stricter for PAS method.
Results
The manual investigation results on extracted
VAs from the 10 abstracts using POS patterns and
PASs are shown in Table 2 and 3 respectively.
POS patterns extracted more (98) VAs than PASs
(75), but many of the increment were from incor-
rect POS pattern matching. By POS patterns, 43
VAs (44%) were extracted based on incorrect anal-
ysis. On the other hand, by PASs, 20 VAs (27%)
were extracted incorrectly. Thus the ratio of VAs
extracted by syntactically correct analysis is larger
on PAS method.
POS pattern method extracted 38 VAs of verbs
not extracted by PAS method and 7 of them are cor-
rect. For PAS method, correspondent numbers are
Appropriate Inappropriate Total
Correct 44 11 55
Incorrect 14 6 20
Total 58 17 75
Table 3: Numbers of VAs extracted by PASs (in de-
tail)
11 and 4 respectively. Thus the increments tend to
be caused by incorrect analysis, and the tendency is
greater in POS pattern method.
Since not all of verbs that take topical nouns are
appropriate for anchor verbs, automatic filtering is
required. In the filtering phase that we leave as a
future work, we can use semantical classes and fre-
quencies of arguments of the verbs. The results with
syntactically incorrect arguments will cause adverse
effect on filtering because they express incorrect re-
lationship between verbs and arguments. Since the
numbers of extracted VAs after excluding the ones
with incorrect arguments are the same (55) between
PAS and POS pattern methods, it can be concluded
that the precision of PAS method is higher. Al-
though there are few (7) correct VAs which were
extracted by POS pattern method but not by PAS
method, we expect the number of such verbs can be
reduced using a larger corpus.
Examples of appropriate VAs extracted by only
one method are as follows: (A) is correct and (B)
incorrect, extracted by only POS pattern method,
and (C) is correct and (D) incorrect, extracted by
only PAS method. Bold words are extracted verbs
or predicates and italic words their extracted argu-
ments.
(A) This delay is associated with down-regulation
of many erythroid cell-specific genes, including
alpha- and beta-globin, band 3, band 4.1, and . . . .
(B) . . . show that several elements in the . . . region of
the IL-2R alpha gene contribute to IL-1 respon-
siveness, . . . .
(C) The CD4 coreceptor interacts with non-
polymorphic regions of . . . molecules on
non-polymorphic cells and contributes to T cell
activation.
(D) Whereas activation of the HIV-1 enhancer follow-
ing T-cell stimulation is mediated largely through
binding of the . . . factor NF-kappa B to two adja-
cent kappa B sites in . . . .
5 Conclusions
We have proposed a method of extracting anchor
verbs as elements of extraction rules for IE by us-
ing PASs obtained by full parsing. To compare
our method with more naive and robust methods,
we have extracted verbs and their arguments using
POS patterns and PASs. POS pattern method could
obtain more candidate verbs for anchor verbs, but
many of them were extracted with incorrect argu-
ments by incorrect matching. A later filtering pro-
cess benefits by precise relations between verbs and
their arguments which PASs obtained. The short-
coming of PAS method is expected to be reduced by
using a larger corpus, because verbs to extract will
appear many times in many forms. One of the future
works is to extend PAS method to handle events in
nominalized forms.
Acknowledgements
This work was partially supported by Grant-in-
Aid for Scientific Research on Priority Areas (C)
?Genome Information Science? from the Ministry
of Education, Culture, Sports, Science and Technol-
ogy of Japan.
References
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction
patterns from published text articles. Interna-
tional Journal of Medical Informatics, 67:19?32.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a se-
mantically annotated corpus for bio-textmining.
Bioinformatics, 19(suppl. 1):i180?i182.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Jun-ichi Tsujii. 1998. LiLFeS ? to-
wards a practical HPSG parser. In Proceedings
of COLING-ACL?98.
Yusuke Miyao, Takaki Makino, Kentaro Torisawa,
and Jun-ichi Tsujii. 2000. The LiLFeS abstract
machine and its evaluation with the LinGO gram-
mar. Natural Language Engineering, 6(1):47 ?
61.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2003. Probabilistic modeling of argument
structures including non-local dependencies. In
Proceedings of RANLP 2003, pages 285?291.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2004. Corpus-oriented grammar develop-
ment for acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank. In Pro-
ceedings of IJCNLP-04.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic
Theory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003, pages 224?231.
Proceedings of the 43rd Annual Meeting of the ACL, pages 75?82,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic CFG with latent annotations
Takuya Matsuzaki
 
Yusuke Miyao
 
Jun?ichi Tsujii  
 
Graduate School of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033

CREST, JST(Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012

matuzaki, yusuke, tsujii  @is.s.u-tokyo.ac.jp
Abstract
This paper defines a generative probabilis-
tic model of parse trees, which we call
PCFG-LA. This model is an extension of
PCFG in which non-terminal symbols are
augmented with latent variables. Fine-
grained CFG rules are automatically in-
duced from a parsed corpus by training a
PCFG-LA model using an EM-algorithm.
Because exact parsing with a PCFG-LA is
NP-hard, several approximations are de-
scribed and empirically compared. In ex-
periments using the Penn WSJ corpus, our
automatically trained model gave a per-
formance of 86.6% (F  , sentences  40
words), which is comparable to that of an
unlexicalized PCFG parser created using
extensive manual feature selection.
1 Introduction
Variants of PCFGs form the basis of several broad-
coverage and high-precision parsers (Collins, 1999;
Charniak, 1999; Klein and Manning, 2003). In those
parsers, the strong conditional independence as-
sumption made in vanilla treebank PCFGs is weak-
ened by annotating non-terminal symbols with many
?features? (Goodman, 1997; Johnson, 1998). Exam-
ples of such features are head words of constituents,
labels of ancestor and sibling nodes, and subcatego-
rization frames of lexical heads. Effective features
and their good combinations are normally explored
using trial-and-error.
This paper defines a generative model of parse
trees that we call PCFG with latent annotations
(PCFG-LA). This model is an extension of PCFG
models in which non-terminal symbols are anno-
tated with latent variables. The latent variables work
just like the features attached to non-terminal sym-
bols. A fine-grained PCFG is automatically induced
from parsed corpora by training a PCFG-LA model
using an EM-algorithm, which replaces the manual
feature selection used in previous research.
The main focus of this paper is to examine the
effectiveness of the automatically trained models in
parsing. Because exact inference with a PCFG-LA,
i.e., selection of the most probable parse, is NP-hard,
we are forced to use some approximation of it. We
empirically compared three different approximation
methods. One of the three methods gives a perfor-
mance of 86.6% (F  , sentences  40 words) on the
standard test set of the Penn WSJ corpus.
Utsuro et al (1996) proposed a method that auto-
matically selects a proper level of generalization of
non-terminal symbols of a PCFG, but they did not
report the results of parsing with the obtained PCFG.
Henderson?s parsing model (Henderson, 2003) has a
similar motivation as ours in that a derivation history
of a parse tree is compactly represented by induced
hidden variables (hidden layer activation of a neu-
ral network), although the details of his approach is
quite different from ours.
2 Probabilistic model
PCFG-LA is a generative probabilistic model of
parse trees. In this model, an observed parse tree
is considered as an incomplete data, and the corre-
75
	 

:

:

 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 83?90,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic disambiguation models for wide-coverage HPSG parsing
Yusuke Miyao
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper reports the development of log-
linear models for the disambiguation in
wide-coverage HPSG parsing. The esti-
mation of log-linear models requires high
computational cost, especially with wide-
coverage grammars. Using techniques to
reduce the estimation cost, we trained the
models using 20 sections of Penn Tree-
bank. A series of experiments empiri-
cally evaluated the estimation techniques,
and also examined the performance of the
disambiguation models on the parsing of
real-world sentences.
1 Introduction
Head-Driven Phrase Structure Grammar (HPSG)
(Pollard and Sag, 1994) has been studied extensively
from both linguistic and computational points of
view. However, despite research on HPSG process-
ing efficiency (Oepen et al, 2002a), the application
of HPSG parsing is still limited to specific domains
and short sentences (Oepen et al, 2002b; Toutanova
and Manning, 2002). Scaling up HPSG parsing to
assess real-world texts is an emerging research field
with both theoretical and practical applications.
Recently, a wide-coverage grammar and a large
treebank have become available for English HPSG
(Miyao et al, 2004). A large treebank can be used as
training and test data for statistical models. There-
fore, we now have the basis for the development and
the evaluation of statistical disambiguation models
for wide-coverage HPSG parsing.
The aim of this paper is to report the development
of log-linear models for the disambiguation in wide-
coverage HPSG parsing, and their empirical evalua-
tion through the parsing of the Wall Street Journal of
Penn Treebank II (Marcus et al, 1994). This is chal-
lenging because the estimation of log-linear models
is computationally expensive, and we require solu-
tions to make the model estimation tractable. We
apply two techniques for reducing the training cost.
One is the estimation on a packed representation of
HPSG parse trees (Section 3). The other is the filter-
ing of parse candidates according to a preliminary
probability distribution (Section 4).
To our knowledge, this work provides the first re-
sults of extensive experiments of parsing Penn Tree-
bank with a probabilistic HPSG. The results from
the Wall Street Journal are significant because the
complexity of the sentences is different from that of
short sentences. Experiments of the parsing of real-
world sentences can properly evaluate the effective-
ness and possibility of parsing models for HPSG.
2 Disambiguation models for HPSG
Discriminative log-linear models are now becom-
ing a de facto standard for probabilistic disambigua-
tion models for deep parsing (Johnson et al, 1999;
Riezler et al, 2002; Geman and Johnson, 2002;
Miyao and Tsujii, 2002; Clark and Curran, 2004b;
Kaplan et al, 2004). Previous studies on prob-
abilistic models for HPSG (Toutanova and Man-
ning, 2002; Baldridge and Osborne, 2003; Malouf
and van Noord, 2004) also adopted log-linear mod-
els. HPSG exploits feature structures to represent
linguistic constraints. Such constraints are known
83
to introduce inconsistencies in probabilistic models
estimated using simple relative frequency (Abney,
1997). Log-linear models are required for credible
probabilistic models and are also beneficial for in-
corporating various overlapping features.
This study follows previous studies on the proba-
bilistic models for HPSG. The probability,    , of
producing the parse result  from a given sentence 
is defined as
    


 
 
 
    
 



  

  

 

 

 
   
 
 
 

   
 



 

 

 

 
where  
 
   is a reference distribution (usually as-
sumed to be a uniform distribution), and    is a set
of parse candidates assigned to . The feature func-
tion 

   represents the characteristics of  and ,
while the corresponding model parameter 

   is
its weight. Model parameters that maximize the log-
likelihood of the training data are computed using a
numerical optimization method (Malouf, 2002).
Estimation of the above model requires a set of
pairs 
 
   , where 
 
is the correct parse for sen-
tence . While 
 
is provided by a treebank,    is
computed by parsing each  in the treebank. Pre-
vious studies assumed    could be enumerated;
however, the assumption is impractical because the
size of    is exponentially related to the length
of . The problem of exponential explosion is in-
evitable in the wide-coverage parsing of real-world
texts because many parse candidates are produced to
support various constructions in long sentences.
3 Packed representation of HPSG parse
trees
To avoid exponential explosion, we represent   
in a packed form of HPSG parse trees. A parse tree
of HPSG is represented as a set of tuples  	 
,
where 	 and 
 are the signs of mother, left daugh-
ter, and right daughter, respectively1. In chart pars-
ing, partial parse candidates are stored in a chart, in
which phrasal signs are identified and packed into an
equivalence class if they are determined to be equiv-
alent and dominate the same word sequence. A set
1For simplicity, only binary trees are considered. Extension
to unary and  -ary (    ) trees is trivial.
Figure 1: Chart for parsing ?he saw a girl with a
telescope?
of parse trees is then represented as a set of relations
among equivalence classes.
Figure 1 shows a chart for parsing ?he saw a
girl with a telescope?, where the modifiee (?saw?
or ?girl?) of ?with? is ambiguous. Each feature
structure expresses an equivalence class, and the ar-
rows represent immediate-dominance relations. The
phrase, ?saw a girl with a telescope?, has two trees
(A in the figure). Since the signs of the top-most
nodes are equivalent, they are packed into an equiv-
alence class. The ambiguity is represented as two
pairs of arrows that come out of the node.
Formally, a set of HPSG parse trees is represented
in a chart as a tuple 

 , where  is a set
of equivalence classes, 

  is a set of root
nodes, and      is a function to repre-
sent immediate-dominance relations.
Our representation of the chart can be interpreted
as an instance of a feature forest (Miyao and Tsujii,
2002; Geman and Johnson, 2002). A feature for-
est is an ?and/or? graph to represent exponentially-
many tree structures in a packed form. If    is
represented in a feature forest,       can be esti-
mated using dynamic programming without unpack-
ing the chart. A feature forest is formally defined as
a tuple,   ?, where  is a set of conjunc-
tive nodes,  is a set of disjunctive nodes,   
is a set of root nodes2,      is a conjunctive
daughter function, and ?     is a disjunctive
2For the ease of explanation, the definition of root node is
slightly different from the original.
84
Figure 2: Packed representation of HPSG parse trees
in Figure 1
daughter function. The feature functions 

   are
assigned to conjunctive nodes.
The simplest way to map a chart of HPSG parse
trees into a feature forest is to map each equivalence
class    to a conjunctive node    . How-
ever, in HPSG parsing, important features for dis-
ambiguation are combinations of a mother and its
daughters, i.e.,  	 
. Hence, we map the tuple


 
	
 

, which corresponds to  	 
, into a
conjunctive node.
Figure 2 shows (a part of) the HPSG parse trees
in Figure 1 represented as a feature forest. Square
boxes are conjunctive nodes, dotted lines express a
disjunctive daughter function, and solid arrows rep-
resent a conjunctive daughter function.
The mapping is formally defined as follows.
   

 
	
 

 

   
	
 

 

	
 

   

	,
   ,
   

 
	
 

 

 

 

 
	
 

 
	,
   

  

 

    

 


 
	
 

 
	
 

 
	
 

   

		,
and
 ?  

 
	
 

 
	
 

	 

 
	
 

  	.
Figure 3: Filtering of lexical entries for ?saw?
4 Filtering by preliminary distribution
The above method allows for the tractable estima-
tion of log-linear models on exponentially-many
HPSG parse trees. However, despite the develop-
ment of methods to improve HPSG parsing effi-
ciency (Oepen et al, 2002a), the exhaustive parsing
of all sentences in a treebank is still expensive.
Our idea is that we can omit the computation
of parse trees with low probabilities in the estima-
tion stage because    can be approximated with
parse trees with high probabilities. To achieve this,
we first prepared a preliminary probabilistic model
whose estimation did not require the parsing of a
treebank. The preliminary model was used to reduce
the search space for parsing a training treebank.
The preliminary model in this study is a unigram
model, 	    


  
  	  where    is a
word in the sentence , and 	 is a lexical entry as-
signed to . This model can be estimated without
parsing a treebank.
Given this model, we restrict the number of lexi-
cal entries used to parse a treebank. With a thresh-
old  for the number of lexical entries and a thresh-
old  for the probability, lexical entries are assigned
to a word in descending order of probability, until
the number of assigned entries exceeds , or the ac-
cumulated probability exceeds . If the lexical en-
try necessary to produce the correct parse is not as-
signed, it is additionally assigned to the word.
Figure 3 shows an example of filtering lexical en-
tries assigned to ?saw?. With   
, four lexical
entries are assigned. Although the lexicon includes
other lexical entries, such as a verbal entry taking a
sentential complement (   

 in the figure), they
are filtered out. This method reduces the time for
85
RULE the name of the applied schema
DIST the distance between the head words of the
daughters
COMMA whether a comma exists between daughters
and/or inside of daughter phrases
SPAN the number of words dominated by the phrase
SYM the symbol of the phrasal category (e.g. NP, VP)
WORD the surface form of the head word
POS the part-of-speech of the head word
LE the lexical entry assigned to the head word
Table 1: Templates of atomic features
parsing a treebank, while this approximation causes
bias in the training data and results in lower accu-
racy. The trade-off between the parsing cost and the
accuracy will be examined experimentally.
We have several ways to integrate 	  with the esti-
mated model      . In the experiments, we will
empirically compare the following methods in terms
of accuracy and estimation time.
Filtering only The unigram probability 	  is used
only for filtering.
Product The probability is defined as the product of
	  and the estimated model  .
Reference distribution 	  is used as a reference dis-
tribution of  .
Feature function  	  is used as a feature function
of  . This method was shown to be a gener-
alization of the reference distribution method
(Johnson and Riezler, 2000).
5 Features
Feature functions in the log-linear models are de-
signed to capture the characteristics of 

 
	
 

.
In this paper, we investigate combinations of the
atomic features listed in Table 1. The following
combinations are used for representing the charac-
teristics of the binary/unary schema applications.
binary 
 RULE,DIST,COMMA
SPAN
	
 SYM
	
WORD
	
 POS
	
 LE
	

SPAN

 SYM

WORD

 POS

 LE


unary  RULE,SYM,WORD,POS,LE
In addition, the following is for expressing the con-
dition of the root node of the parse tree.
root  SYM,WORD,POS,LE
Figure 4: Example features
Figure 4 shows examples: root is for the root
node, in which the phrase symbol is S and the
surface form, part-of-speech, and lexical entry of
the lexical head are ?saw?, VBD, and a transitive
verb, respectively. binary is for the binary rule ap-
plication to ?saw a girl? and ?with a telescope?,
in which the applied schema is the Head-Modifier
Schema, the left daughter is VP headed by ?saw?,
and the right daughter is PP headed by ?with?,
whose part-of-speech is IN and the lexical entry is
a VP-modifying preposition.
In an actual implementation, some of the atomic
features are abstracted (i.e., ignored) for smoothing.
Table 2 shows a full set of templates of combined
features used in the experiments. Each row rep-
resents a template of a feature function. A check
means the atomic feature is incorporated while a hy-
phen means the feature is ignored.
Restricting the domain of feature functions to


 
	
 

 seems to limit the flexibility of feature
design. Although it is true to some extent, this does
not necessarily mean the impossibility of incorpo-
rating features on nonlocal dependencies into the
model. This is because a feature forest model does
not assume probabilistic independence of conjunc-
tive nodes. This means that we can unpack a part of
the forest without changing the model. Actually, in
our previous study (Miyao et al, 2003), we success-
fully developed a probabilistic model including fea-
tures on nonlocal predicate-argument dependencies.
However, since we could not observe significant im-
provements by incorporating nonlocal features, this
paper investigates only the features described above.
86
RULE DIST COMMA SPAN SYM WORD POS LE
  
? ?
  
  
? ?
 
?
  
? ?

?

  
?
 
? ?

?
 
?
  

?
 
?
 
?

?
 
?

?


?
   
? ?
  
? ? ?
 
  
? ? ?

?
  
? ? ? ?

  
?

? ? ?

?
 
? ?
 

?
 
? ?

?

?
 
? ? ?


?
  
? ? ?
RULE SYM WORD POS LE

?
  

?
 
?

?

?

  
? ?

? ?
 

? ?

?

? ? ?

 
? ? ?
SYM WORD POS LE
?
  
?
 
?
?

?

 
? ?
? ?
 
? ?

?
? ? ?


? ? ?
Table 2: Feature templates for binary schema (left), unary schema (center), and root condition (right)
Avg. length LP LR UP UR F-score
Section 22 ( 40 words) 20.69 87.18 86.23 90.67 89.68 86.70
Section 22 ( 100 words) 22.43 86.99 84.32 90.45 87.67 85.63
Section 23 ( 40 words) 20.52 87.12 85.45 90.65 88.91 86.27
Section 23 ( 100 words) 22.23 86.81 84.64 90.29 88.03 85.71
Table 3: Accuracy for development/test sets
6 Experiments
We used an HPSG grammar derived from Penn
Treebank (Marcus et al, 1994) Section 02-21
(39,832 sentences) by our method of grammar de-
velopment (Miyao et al, 2004). The training data
was the HPSG treebank derived from the same por-
tion of the Penn Treebank3. For the training, we
eliminated sentences with no less than 40 words and
for which the parser could not produce the correct
parse. The resulting training set consisted of 33,574
sentences. The treebanks derived from Sections 22
and 23 were used as the development (1,644 sen-
tences) and final test sets (2,299 sentences). We
measured the accuracy of predicate-argument de-
pendencies output by the parser. A dependency is
defined as a tuple 

  

, where  is the
predicate type (e.g., adjective, intransitive verb), 

is the head word of the predicate,  is the argument
label (MODARG, ARG1, ..., ARG4), and 

is the
head word of the argument. Labeled precision/recall
(LP/LR) is the ratio of tuples correctly identified by
the parser, while unlabeled precision/recall (UP/UR)
is the ratio of 

and 

correctly identified re-
gardless of  and . The F-score is the harmonic
mean of LP and LR. The accuracy was measured by
parsing test sentences with part-of-speech tags pro-
3The programs to make the grammar and the tree-
bank from Penn Treebank are available at http://www-
tsujii.is.s.u-tokyo.ac.jp/enju/.
vided by the treebank. The Gaussian prior was used
for smoothing (Chen and Rosenfeld, 1999), and its
hyper-parameter was tuned for each model to max-
imize the F-score for the development set. The op-
timization algorithm was the limited-memory BFGS
method (Nocedal and Wright, 1999). All the follow-
ing experiments were conducted on AMD Opteron
servers with a 2.0-GHz CPU and 12-GB memory.
Table 3 shows the accuracy for the develop-
ment/test sets. Features occurring more than twice
were included in the model (598,326 features). Fil-
tering was done by the reference distribution method
with   
 and   
. The unigram model
for filtering was a log-linear model with two feature
templates, WORD POS LE and POS LE (24,847
features). Our results cannot be strictly compared
with other grammar formalisms because each for-
malism represents predicate-argument dependencies
differently; for reference, our results are competi-
tive with the corresponding measures reported for
Combinatory Categorial Grammar (CCG) (LP/LR
= 86.6/86.3) (Clark and Curran, 2004b). Different
from the results of CCG and PCFG (Collins, 1999;
Charniak, 2000), the recall was clearly lower than
precision. This results from the HPSG grammar
having stricter feature constraints and the parser not
being able to produce parse results for around one
percent of the sentences. To improve recall, we need
techniques of robust processing with HPSG.
87
LP LR Estimationtime (sec.)
Filtering only 34.90 23.34 702
Product 86.71 85.55 1,758
Reference dist. 87.12 85.45 655
Feature function 84.89 83.06 1,203
Table 4: Estimation method vs. accuracy and esti-
mation time
   F-score Estimationtime (sec.)
Parsing
time
(sec.)
Memory
usage
(MB)
5, 0.80 84.31 161 7,827 2,377
5, 0.90 84.69 207 9,412 2,992
5, 0.95 84.70 240 12,027 3,648
5, 0.98 84.81 340 15,168 4,590
10, 0.80 84.79 164 8,858 2,658
10, 0.90 85.77 298 13,996 4,062
10, 0.95 86.27 654 25,308 6,324
10, 0.98 86.56 1,778 55,691 11,700
15, 0.80 84.68 180 9,337 2,676
15, 0.90 85.85 308 14,915 4,220
15, 0.95 86.68 854 32,757 7,766
Table 5: Filtering threshold vs. accuracy and esti-
mation time
Table 4 compares the estimation methods intro-
duced in Section 4. In all of the following exper-
iments, we show the accuracy for the test set (
40 words) only. Table 4 revealed that our simple
method of filtering caused a fatal bias in training
data when a preliminary distribution was used only
for filtering. However, the model combined with a
preliminary model achieved sufficient accuracy. The
reference distribution method achieved higher accu-
racy and lower cost. The feature function method
achieved lower accuracy in our experiments. A pos-
sible reason is that a hyper-parameter of the prior
was set to the same value for all the features includ-
ing the feature of the preliminary distribution.
Table 5 shows the results of changing the filter-
ing threshold. We can determine the correlation be-
tween the estimation/parsing cost and accuracy. In
our experiment,  
 
 and  
 

 seem neces-
sary to preserve the F-score over 
.
Figure 5 shows the accuracy for each sentence
length. It is apparent from this figure that the ac-
curacy was significantly higher for shorter sentences
( 10 words). This implies that experiments with
only short sentences overestimate the performance
of parsers. Sentences with at least 10 words are nec-
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
0 5 10 15 20 25 30 35 40 45
pr
ec
isi
on
/re
ca
ll
sentence length
precision
recall
Figure 5: Sentence length vs. accuracy
 70
 75
 80
 85
 90
 95
 100
 0  5000  10000  15000  20000  25000  30000  35000  40000
pr
ec
isi
on
/re
ca
ll
training sentences
precision
recall
Figure 6: Corpus size vs. accuracy
essary to properly evaluate the performance of pars-
ing real-world texts.
Figure 6 shows the learning curve. A feature set
was fixed, while the parameter of the prior was op-
timized for each model. High accuracy was attained
even with small data, and the accuracy seemed to
be saturated. This indicates that we cannot further
improve the accuracy simply by increasing training
data. The exploration of new types of features is
necessary for higher accuracy.
Table 6 shows the accuracy with difference fea-
ture sets. The accuracy was measured by removing
some of the atomic features from the final model.
The last row denotes the accuracy attained by the
preliminary model. The numbers in bold type rep-
resent that the difference from the final model was
significant according to stratified shuffling tests (Co-
hen, 1995) with p-value  

. The results indicate
that DIST, COMMA, SPAN, WORD, and POS features
contributed to the final accuracy, although the dif-
88
Features LP LR # features
All 87.12 85.45 623,173
?RULE 86.98 85.37 620,511
?DIST 86.74 85.09 603,748
?COMMA 86.55 84.77 608,117
?SPAN 86.53 84.98 583,638
?SYM 86.90 85.47 614,975
?WORD 86.67 84.98 116,044
?POS 86.36 84.71 430,876
?LE 87.03 85.37 412,290
?DIST,SPAN 85.54 84.02 294,971
?DIST,SPAN,
COMMA 83.94 82.44 286,489
?RULE,DIST,
SPAN,COMMA 83.61 81.98 283,897
?WORD,LE 86.48 84.91 50,258
?WORD,POS 85.56 83.94 64,915
?WORD,POS,LE 84.89 83.43 33,740
?SYM,WORD,
POS,LE 82.81 81.48 26,761
None 78.22 76.46 24,847
Table 6: Accuracy with different feature sets
ferences were slight. In contrast, RULE, SYM, and
LE features did not affect the accuracy. However,
if each of them was removed together with another
feature, the accuracy decreased drastically. This im-
plies that such features had overlapping information.
Table 7 shows the manual classification of the
causes of errors in 100 sentences randomly chosen
from the development set. In our evaluation, one
error source may cause multiple errors of dependen-
cies. For example, if a wrong lexical entry was as-
signed to a verb, all the argument dependencies of
the verb are counted as errors. The numbers in the
table include such double-counting. Major causes
were classified into three types: argument/modifier
distinction, attachment ambiguity, and lexical am-
biguity. While attachment/lexical ambiguities are
well-known causes, the other is peculiar to deep
parsing. Most of the errors cannot be resolved by
features we investigated in this study, and the design
of other features is crucial for further improvements.
7 Discussion and related work
Experiments on deep parsing of Penn Treebank have
been reported for Combinatory Categorial Grammar
(CCG) (Clark and Curran, 2004b) and Lexical Func-
tional Grammar (LFG) (Kaplan et al, 2004). They
developed log-linear models on a packed represen-
tation of parse forests, which is similar to our rep-
resentation. Although HPSG exploits further com-
plicated feature constraints and requires high com-
Error cause # of errors
Argument/modifier distinction 58
temporal noun 21
to-infinitive 15
others 22
Attachment 53
prepositional phrase 18
to-infinitive 10
relative clause 8
others 17
Lexical ambiguity 42
participle/adjective 15
preposition/modifier 14
others 13
Comma 19
Coordination 14
Noun phrase identification 13
Zero-pronoun resolution 9
Others 17
Table 7: Error analysis
putational cost, our work has proved that log-linear
models can be applied to HPSG parsing and attain
accurate and wide-coverage parsing.
Clark and Curran (2004a) described a method of
reducing the cost of parsing a training treebank in
the context of CCG parsing. They first assigned to
each word a small number of supertags, which cor-
respond to lexical entries in our case, and parsed su-
pertagged sentences. Since they did not mention the
probabilities of supertags, their method corresponds
to our ?filtering only? method. However, they also
applied the same supertagger in a parsing stage, and
this seemed to be crucial for high accuracy. This
means that they estimated the probability of produc-
ing a parse tree from a supertagged sentence.
Another approach to estimating log-linear mod-
els for HPSG is to extract a small informative sam-
ple from the original set    (Osborne, 2000).
Malouf and van Noord (2004) successfully applied
this method to German HPSG. The problem with
this method was in the approximation of exponen-
tially many parse trees by a polynomial-size sample.
However, their method has the advantage that any
features on a parse tree can be incorporated into the
model. The trade-off between approximation and lo-
cality of features is an outstanding problem.
Other discriminative classifiers were applied to
the disambiguation in HPSG parsing (Baldridge and
Osborne, 2003; Toutanova et al, 2004). The prob-
lem of exponential explosion is also inevitable for
89
their methods. An approach similar to ours may be
applied to them, following the study on the learning
of a discriminative classifier for a packed represen-
tation (Taskar et al, 2004).
As discussed in Section 6, exploration of other
features is indispensable to further improvements.
A possible direction is to encode larger contexts of
parse trees, which were shown to improve the accu-
racy (Toutanova and Manning, 2002; Toutanova et
al., 2004). Future work includes the investigation of
such features, as well as the abstraction of lexical
dependencies like semantic classes.
References
S. P. Abney. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23(4).
J. Baldridge and M. Osborne. 2003. Active learning for
HPSG parse selection. In CoNLL-03.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. NAACL-2000, pages 132?139.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. Technical Re-
port CMUCS-99-108, Carnegie Mellon University.
S. Clark and J. R. Curran. 2004a. The importance of su-
pertagging for wide-coverage CCG parsing. In Proc.
COLING-04.
S. Clark and J. R. Curran. 2004b. Parsing the WSJ using
CCG and log-linear models. In Proc. 42th ACL.
P. R. Cohen. 1995. Empirical Methods for Artificial In-
telligence. MIT Press.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
S. Geman and M. Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. 40th ACL.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars.
In Proc. 1st NAACL.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic ?unification-based?
grammars. In Proc. ACL?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc.
HLT/NAACL?04.
R. Malouf and G. van Noord. 2004. Wide coverage pars-
ing with stochastic attribute value grammars. In Proc.
IJCNLP-04 Workshop ?Beyond Shallow Analyses?.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-
2002.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating predicate argu-
ment structure. In ARPA Human Language Technol-
ogy Workshop.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. HLT 2002.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Probabilistic
modeling of argument structures including non-local
dependencies. In Proc. RANLP 2003, pages 285?291.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a Head-
driven Phrase Structure Grammar from the Penn Tree-
bank. In Proc. IJCNLP-04.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
S. Oepen, D. Flickinger, J. Tsujii, and H. Uszkoreit, ed-
itors. 2002a. Collaborative Language Engineering:
A Case Study in Efficient Grammar-Based Processing.
CSLI Publications.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002b. The LinGO,
Redwoods treebank. motivation and preliminary appli-
cations. In Proc. COLING 2002.
M. Osborne. 2000. Estimation of stochastic attribute-
value grammar using an informative sample. In Proc.
COLING 2000.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In
Proc. 40th ACL.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In EMNLP 2004.
K. Toutanova and C. D. Manning. 2002. Feature selec-
tion for a rich HPSG grammar using decision trees. In
Proc. CoNLL-2002.
K. Toutanova, P. Markova, and C. Manning. 2004. The
leaf projection path view of parse trees: Exploring
string kernels for HPSG parse selection. In EMNLP
2004.
90
Resource sharing among HPSG and LTAG communities
by a method of grammar conversion from FB-LTAG to HPSG
Naoki Yoshinaga Yusuke Miyao
Department of Information Science, Graduate school of Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
fyoshinag, yusukeg@is.s.u-tokyo.ac.jp
Kentaro Torisawa
School of Information Science, Japan Advanced Institute of Science and Technology
Asahidai 1-1, Tatsunokuchi-cho, Noumi-gun, Ishikawa, 923-1292, Japan
Information and Human Behavior, PRESTO, Japan Science and Technology Corporation
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
torisawa@jaist.ac.jp
Jun?ichi Tsujii
Department of Computer Science, Graduate school of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
CREST, JST (Japan Science and Technology Corporation)
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper describes the RenTAL sys-
tem, which enables sharing resources
in LTAG and HPSG formalisms by a
method of grammar conversion from
an FB-LTAG grammar to a strongly
equivalent HPSG-style grammar. The
system is applied to the latest version
of the XTAG English grammar. Ex-
perimental results show that the ob-
tained HPSG-style grammar success-
fully worked with an HPSG parser, and
achieved a drastic speed-up against an
LTAG parser. This system enables to
share not only grammars and lexicons
but also parsing techniques.
1 Introduction
This paper describes an approach for shar-
ing resources in various grammar formalisms
such as Feature-Based Lexicalized Tree Adjoin-
ing Grammar (FB-LTAG1) (Vijay-Shanker, 1987;
Vijay-Shanker and Joshi, 1988) and Head-Driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) by a method of grammar conver-
sion. The RenTAL system automatically converts
an FB-LTAG grammar into a strongly equiva-
lent HPSG-style grammar (Yoshinaga and Miyao,
2001). Strong equivalence means that both gram-
mars generate exactly equivalent parse results,
and that we can share the LTAG grammars and
lexicons in HPSG applications. Our system can
reduce considerable workload to develop a huge
resource (grammars and lexicons) from scratch.
Our concern is, however, not limited to the
sharing of grammars and lexicons. Strongly
equivalent grammars enable the sharing of
ideas developed in each formalism. There
have been many studies on parsing tech-
niques (Poller and Becker, 1998; Flickinger et
al., 2000), ones on disambiguation models (Chi-
ang, 2000; Kanayama et al, 2000), and ones
on programming/grammar-development environ-
1In this paper, we use the term LTAG to refer to FB-
LTAG, if not confusing.
LTAG Resources
Grammar: 
Elementary tree 
templates
Lexicon
Type hierarchy 
extractor
Tree 
converter
Lexicon 
converter
RenTAL System
HPSG Resources
Grammar: 
Lexical entry 
templates
Lexicon
LTAG parsers HPSG parsers
Derivation trees Parse trees
Derivation 
translator
LTAG-based application
HPSG-based application
Figure 1: The RenTAL System: Overview
ment (Sarkar and Wintner, 1999; Doran et al,
2000; Makino et al, 1998). These works are re-
stricted to each closed community, and the rela-
tion between them is not well discussed. Investi-
gating the relation will be apparently valuable for
both communities.
In this paper, we show that the strongly equiv-
alent grammars enable the sharing of ?parsing
techniques?, which are dependent on each com-
putational framework and have never been shared
among HPSG and LTAG communities. We ap-
ply our system to the latest version of the XTAG
English grammar (The XTAG Research Group,
2001), which is a large-scale FB-LTAG gram-
mar. A parsing experiment shows that an efficient
HPSG parser with the obtained grammar achieved
a significant speed-up against an existing LTAG
parser (Yoshinaga et al, 2001). This result im-
plies that parsing techniques for HPSG are also
beneficial for LTAG parsing. We can say that the
grammar conversion enables us to share HPSG
parsing techniques in LTAG parsing.
Figure 1 depicts a brief sketch of the RenTAL
system. The system consists of the following four
modules: Tree converter, Type hierarchy extrac-
tor, Lexicon converter and Derivation translator.
The tree converter module is a core module of the
system, which is an implementation of the gram-
mar conversion algorithm given in Section 3. The
type hierarchy extractor module extracts the sym-
bols of the node, features, and feature values from
the LTAG elementary tree templates and lexicon,
and construct the type hierarchy from them. The
lexicon converter module converts LTAG elemen-
tary tree templates into HPSG lexical entries. The
derivation translator module takes HPSG parse
S
NP VP
V
run
VP
VP
V
can
*
NP
N
We
?1
?2
?1
anchor
foot node
*
substitution node
Initial tree
Auxiliary tree
Figure 2: Elementary trees
trees, and map them to LTAG derivation trees. All
modules other than the last one are related to the
conversion process from LTAG into HPSG, and
the last one enables to obtain LTAG analysis from
the obtained HPSG analysis.
Tateisi et al also translated LTAG into
HPSG (Tateisi et al, 1998). However, their
method depended on translator?s intuitive analy-
sis of the original grammar. Thus the transla-
tion was manual and grammar dependent. The
manual translation demanded considerable efforts
from the translator, and obscures the equiva-
lence between the original and obtained gram-
mars. Other works (Kasper et al, 1995; Becker
and Lopez, 2000) convert HPSG grammars into
LTAG grammars. However, given the greater ex-
pressive power of HPSG, it is impossible to con-
vert an arbitrary HPSG grammar into an LTAG
grammar. Therefore, a conversion from HPSG
into LTAG often requires some restrictions on the
HPSG grammar to suppress its generative capac-
ity. Thus, the conversion loses the equivalence of
the grammars, and we cannot gain the above ad-
vantages.
Section 2 reviews the source and the tar-
get grammar formalisms of the conversion algo-
rithm. Section 3 describes the conversion algo-
rithm which the core module in the RenTAL sys-
tem uses. Section 4 presents the evaluation of
the RenTAL system through experiments with the
XTAG English grammar. Section 5 concludes this
study and addresses future works.
2 Background
2.1 Feature-Based Lexicalized Tree
Adjoining Grammar (FB-LTAG)
LTAG (Schabes et al, 1988) is a grammar formal-
ism that provides syntactic analyses for a sentence
by composing elementary trees with two opera-
Arg :
we
can run
ID grammar rule
unify
Sym : NP
Arg : 
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :
Sym :
Arg :
2
3
2
unify
3
unify
ID grammar rule
we
can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Sym : 
Arg :
Arg :
1
1
|
2
Arg :
2
unify
we can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Arg :
Figure 6: Parsing with an HPSG grammar
S
NP
VP
V
run
NP
N
We
substitution
?1
?2
S
NP VP
V
run
N
We
Figure 3: Substitution
VP
VP
V
can
*
adjunction
?1
S
NP VP
V
run
N
We
S
NP
VP
VP
V
can
N
We
V
run
Figure 4: Adjunction
tions called substitution and adjunction. Elemen-
tary trees are classified into two types, initial trees
and auxiliary trees (Figure 2). An elementary tree
has at least one leaf node labeled with a terminal
symbol called an anchor (marked with ). In an
auxiliary tree, one leaf node is labeled with the
same symbol as the root node and is specially
marked as a foot node (marked with ). In an el-
ementary tree, leaf nodes with the exception of
anchors and the foot node are called substitution
nodes (marked with #).
Substitution replaces a substitution node with
another initial tree (Figure 3). Adjunction grafts
an auxiliary tree with the root node and foot
node labeled x onto an internal node of another
tree with the same symbol x (Figure 4). FB-
LTAG (Vijay-Shanker, 1987; Vijay-Shanker and
Joshi, 1988) is an extension of the LTAG formal-
ism. In FB-LTAG, each node in the elementary
trees has a feature structure, containing grammat-
ical constraints on the node. Figure 5 shows a
result of LTAG analysis, which is described not
derived tree
?2
?1?1
derivation tree
S
NP VP
VP
V
can
N
We
V
run
Figure 5: Derived trees and derivation trees
only by derived trees (i.e., parse trees) but also by
derivation trees. A derivation tree is a structural
description in LTAG and represents the history of
combinations of elementary trees.
There are several grammars developed in the
FB-LTAG formalism, including the XTAG En-
glish grammar, a large-scale grammar for En-
glish (The XTAG Research Group, 2001). The
XTAG group (Doran et al, 2000) at the Univer-
sity of Pennsylvania is also developing Korean,
Chinese, and Hindi grammars. Development of
a large-scale French grammar (Abeille? and Can-
dito, 2000) has also started at the University of
Pennsylvania and is expanded at University of
Paris 7.
2.2 Head-Driven Phrase Structure
Grammar (HPSG)
An HPSG grammar consists of lexical entries and
ID grammar rules, each of which is described
with typed feature structures (Carpenter, 1992). A
lexical entry for each word expresses the charac-
teristics of the word, such as the subcategorization
frame and the grammatical category. An ID gram-
mar rule represents a relation between a mother
and its daughters, and is independent of lexical
characteristics. Figure 6 illustrates an example of
bottom-up parsing with an HPSG grammar. First,
lexical entries for ?can? and ?run? are unified re-
spectively with the daughter feature structures of
Canonical elementary trees Non-canonical elementary trees
think
S
NP VP
V S
*
it
S
NP VP
N
V
VP
V
?
is
Non-anchored subtree
S
NP VP
V PP
P NP
for
look
PP S
P NP
a) Exception for Condition 1
b) Exception for Condition 2
Figure 7: A canonical elementary tree and exceptions
an ID grammar rule. The feature structure of the
mother node is determined as a result of these uni-
fications. The center of Figure 6 shows a rule ap-
plication to ?can run? and ?we?.
There are a variety of works on efficient pars-
ing with HPSG, which allow the use of HPSG-
based processing in practical application con-
texts (Flickinger et al, 2000). Stanford Univer-
sity is developing the English Resource Gram-
mar, an HPSG grammar for English, as a part
of the Linguistic Grammars Online (LinGO)
project (Flickinger, 2000). In practical con-
text, German, English, and Japanese HPSG-based
grammars are developed and used in the Verb-
mobil project (Kay et al, 1994). Our group
has developed a wide-coverage HPSG grammar
for Japanese (Mitsuishi et al, 1998), which is
used in a high-accuracy Japanese dependency an-
alyzer (Kanayama et al, 2000).
3 Grammar conversion
The grammar conversion from LTAG to
HPSG (Yoshinaga and Miyao, 2001) is the
core portion of the RenTAL system. The
conversion algorithm consists of:
1. Conversion of canonical elementary trees to
HPSG lexical entries.
2. Definition of ID grammar rules to emulate
substitution and adjunction.
3. Conversion of non-canonical elementary
trees to canonical ones.
The left-hand side of Figure 7 shows a canoni-
cal elementary tree, which satisfies the following
conditions:
Condition 1 A tree must have only one anchor.
Sym:
Arg:
Sym  :
Leaf :
Dir    :
right left
,
Foot?:
+
_
*
think
V S
VP
S
NP
V
think:
S
VP S
NP
foot node
anchor
trunk
*
substitution node
Sym  :
Leaf :
Dir    :
Foot?:
Figure 8: A conversion from a canonical elemen-
tary tree into an HPSG lexical entry
mother

Sym : 1
Arg : 2






h
Sym : 3
Arg : h i
i
substitution node
X
X
X
X
X
2
4
Arg :
*
2
4
Sym : 1
Leaf : 3
Dir : left
Foot? :  
3
5
j 2
+
3
5
trunk node
Figure 9: Left substitution rule
Condition 2 All branchings in a tree must con-
tain trunk nodes.
Trunk nodes are nodes on a trunk, which is a path
from an anchor to the root node (the thick lines in
Figure 7) (Kasper et al, 1995). Condition 1 guar-
antees that a canonical elementary tree has only
one trunk, and Condition 2 guarantees that each
branching consists of a trunk node, a leaf node,
and their mother (also a trunk node). The right-
hand side of Figure 7 shows elementary trees vi-
olating the conditions.
Canonical elementary trees can be directly con-
verted to HPSG lexical entries by regarding each
leaf node as a subcategorization element of the
anchor, and by encoding them into a list. Fig-
ure 8 shows an example of the conversion. By
following the trunk from the anchor ?think? to the
mother

Sym : 1
Arg : 2  3







Sym : 4
Arg : 3

foot node
P
P
P
P
P
2
4
Arg :
*
2
4
Sym : 1
Leaf : 4
Dir : left
Foot? : +
3
5
j 2
+
3
5
trunk node
 append
Figure 10: Left adjunction rule
root node labeled S, we store each branching in
a list. As shown in Figure 8, each branching is
specified by a leaf node and the mother node. A
feature Sym represents the non-terminal symbol
of the mother node. Features Leaf, Dir, Foot?
represent the leaf node; the non-terminal symbol,
the direction (on which side of the trunk node the
leaf node is), and the type (whether a foot node or
a substitution node), respectively.
Figures 9 and 10 show ID grammar rules to em-
ulate substitution and adjunction. These grammar
rules are independent of the original grammar be-
cause they don?t specify any characteristics spe-
cific to the original grammar.
In the substitution rule, the Sym feature of the
substitution node must have the value of the Leaf
feature 3 of the trunk node. The Arg feature of
the substitution node must be a null list, because
the substitution node must be unified only with
the node corresponding to the root node of the ini-
tial tree. The substitution rule percolates the tail
elements 2 of the Arg feature of a trunk node to
the mother in order to continue constructing the
tree.
In the adjunction rule, the Sym feature of a
foot node must have the same value as the Leaf
feature 4 . The value of the Arg feature of the
mother node is a concatenation list of both Arg
features 2 and 3 of its daughters because we
first construct the tree corresponding to the ad-
joining tree and next continue constructing the
tree corresponding to the adjoined tree. The value
?+? or ? ? of the Foot? feature explicitly de-
termines whether the next rule application is the
adjunction rule or the substitution rule.
Figure 11 shows an instance of rule applica-
tions. The thick line indicates the adjoined tree
(1) and the dashed line indicates the adjoining
Sym : NP
Arg : 
Sym : S
Arg : 
Sym : S
?1
2
1
5
3
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
Sym :        VP
Leaf :        S 
Dir :  right
Foot? :  +
Sym : NP
Arg : 
Sym : NP
Arg : 
Sym : V
Sym : S
Sym : VP
Sym : V
think:
loves:
you
? A
*
? B
4
4
7
7
8
6
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
5
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
3
6
8
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
3
6
Sym :        S
Leaf :        NP
Dir :  left
Foot? : 
,
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
,
4
9
9
?1
he
?2
?4
?3
Arg :
Arg :
Arg : Arg :
Arg :
what
? C
Figure 11: An example of rule applications
S
NP
VP
V PP
P NP
for
S
NP VP
V
P NP
for
look look
cut off
PP
look_for
PP
look_for
identifier
Figure 12: Division of a multi-anchored elemen-
tary tree into single-anchored trees
tree (2). The adjunction rule is applied to con-
struct the branching marked with ?, where ?think?
takes as an argument a node whose Sym feature?s
value is S. By applying the adjunction rule, the
Arg feature of the mother node (B) becomes a
concatenation list of both Arg features of 1 ( 8 )
and 1 ( 5 ). Note that when the construction of
1 is completed, the Arg feature of the trunk node
(C) will be its former state (A). We can continue
constructing 1 as if nothing had happened.
Multi-anchored elementary trees, which violate
Condition 1, are divided into multiple canonical
elementary trees. We call the cutting nodes in the
divided trees cut-off nodes (Figure 12). Note that
a cut-off node is marked by an identifier to pre-
serve a co-occurrence relation among the multiple
anchors. Figure 12 shows an example of the con-
version of a multi-anchored elementary tree for a
compound expression ?look for?. We first select
an anchor ?look? as the syntactic head, and tra-
verse the tree along the trunk from the root node
S to the anchor ?look?. We then cut off the multi-
PAd
P
P
substitution
all candidate initial trees 
for substitution
, ?
non-anchored subtree
multi-anchored trees without non-anchored subtrees
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
breaking points
on
tonext
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
, ?
Ad
P
on
tonext
Figure 13: Combination of a non-anchored subtree into anchored trees
anchored elementary tree at the node PP, and cut-
off nodes PP in resulting single-anchored trees are
marked by an identifier look for.
Non-canonical elementary trees violating Con-
dition 2 have a non-anchored subtree which is
a subtree of depth 1 or above with no anchor.
A non-anchored subtree is converted into multi-
anchored trees by substituting the deepest node
(Figure 13). Substituted nodes are marked as
breaking points to remember that the nodes orig-
inate from the substitution nodes. In the resulting
trees, all subtrees are anchored so that we can ap-
ply the above conversion algorithms. Figure 13
shows a conversion of a non-canonical elemen-
tary tree for it-cleft. A substitution node P in the
non-anchored subtree is selected, and is substi-
tuted by each initial tree. The substituted node
P in resulting multi-anchored trees are marked as
breaking points.
The above algorithm gives the conversion of
LTAG, and it can be easily extended to handle an
FB-LTAG grammar by merely storing a feature
structure of each node into the Sym feature and
Leaf feature together with the non-terminal sym-
bol. Feature structure unification is executed by
ID grammar rules.
The strong equivalence is assured because only
substitution/adjunction operations performed in
LTAG are performed with the obtained HPSG-
style grammar. This is because each element
in the Arg feature selects only feature structures
corresponding to trees which can substitute/be
adjoined by each leaf node of an elementary
tree. By following a history of rule applications,
each combination of elementary trees in LTAG
derivation trees can be readily recovered. The
strong equivalence holds also for conversion of
non-canonical elementary trees. For trees violat-
ing Condition 1, we can distinguish the cut-off
Table 1: The classification of elementary tree
templates in the XTAG English grammar (LTAG)
and converted lexical entry templates correspond-
ing to them (HPSG): A: canonical elementary
trees, B: elementary trees violating only Condi-
tion 1, C: elementary trees violating only Condi-
tion 2, D: elementary trees violating both condi-
tions
Grammar A B C D Total
LTAG 326 764 54 50 1,194
HPSG 326 1,992 1,083 2,474 5,875
nodes from the substitution nodes owing to iden-
tifiers, which recover the co-occurrence relation
in the original elementary trees between the di-
vided trees. For trees violating Condition 2, we
can identify substitution nodes in a combined tree
because they are marked as breaking points, and
we can consider the combined tree as two trees in
the LTAG derivation.
4 Experiments
The RenTAL system is implemented in LiL-
FeS (Makino et al, 1998)2. LiLFeS is one of
the fastest inference engines for processing fea-
ture structure logic, and efficient HPSG parsers
have already been built on this system (Nishida
et al, 1999; Torisawa et al, 2000). We ap-
plied our system to the XTAG English gram-
mar (The XTAG Research Group, 2001)3, which
is a large-scale FB-LTAG grammar for English.
2The RenTAL system is available at:
http://www-tsujii.is.s.u-tokyo.ac.jp/rental/
3We used the grammar attached to the latest distribution
of an LTAG parser which we used for the parsing experi-
ment. The parser is available at:
ftp://ftp.cis.upenn.edu/pub/xtag/lem/lem-0.13.0.i686.tgz
Table 2: Parsing performance with the XTAG En-
glish grammar for the ATIS corpus.
Parser Parse Time (sec.)
lem 19.64
TNT 0.77
The XTAG English grammar consists of 1,194 4
elementary tree templates and around 45,000 lex-
ical items5. We successfully converted all the
elementary tree templates in the XTAG English
grammar to HPSG lexical entry templates. Ta-
ble 1 shows the classifications of elementary tree
templates of the XTAG English grammar, ac-
cording to the conditions we introduced in Sec-
tion 3, and also shows the number of correspond-
ing HPSG lexical entry templates. Conversion
took about 25 minutes CPU time on a 700 Mhz
Pentium III Xeon with four gigabytes main mem-
ory.
The original and the obtained grammar gener-
ated exactly the same number of derivation trees
in the parsing experiment with 457 sentences
from the ATIS corpus (Marcus et al, 1994)6 (the
average length is 6.32 words). This result empir-
ically attested the strong equivalence of our algo-
rithm.
Table 2 shows the average parsing time with
the LTAG and HPSG parsers. In Table 2, lem
refers to the LTAG parser (Sarkar et al, 2000),
ANSI C implementation of the two-phase pars-
ing algorithm that performs the head corner pars-
ing (van Noord, 1994) without features (phase
1), and then executes feature unification (phase
2). TNT refers to the HPSG parser (Torisawa et
al., 2000), C++ implementation of the two-phase
parsing algorithm that performs filtering with a
compiled CFG (phase 1) and then executes fea-
ture unification (phase 2). Table 2 clearly shows
that the HPSG parser is significantly faster than
the LTAG parser. This result implies that parsing
techniques for HPSG are also beneficial for LTAG
4We eliminated 32 elementary trees because the LTAG
parser cannot produce correct derivation trees with them.
5These lexical items are a subset of the original XTAG
English grammar distribution.
6We eliminated 59 sentences because of a time-out of
the parsers, and 61 sentences because the LTAG parser does
not produce correct derivation trees because of bugs in its
preprocessor.
parsing. We can say that the grammar conversion
enables us to share HPSG parsing techniques in
LTAG parsing. Another paper (Yoshinaga et al,
2001) describes the detailed analysis on the factor
of the difference of parsing performance.
5 Conclusion
We described the RenTAL system, a grammar
converter from FB-LTAG to HPSG. The grammar
conversion guarantees the strong equivalence, and
hence we can obtain an HPSG-style grammar
equivalent to existing LTAG grammars. Experi-
mental result showed that the system enabled to
share not only LTAG grammars, but also HPSG
parsing techniques. This system will enable a
variety of resource sharing such as the sharing
of the programming/grammar-development envi-
ronment (Makino et al, 1998; Sarkar and Wint-
ner, 1999) and grammar extraction methods from
bracketed corpora (Xia, 1999; Chen and Vijay-
Shanker, 2000; Neumann, 1998). Although our
system connects only FB-LTAG and HPSG, we
believe that our approach can be extended to other
formalisms such as Lexical-Functional Gram-
mar (Kaplan and Bresnan, 1982).
Acknowledgment The authors are indebted
to Mr. Anoop Sarkar for his help in using his
parser in our experiment. The authors would like
to thank anonymous reviewers for their valuable
comments and criticisms on this paper.
References
Anne Abeille? and Marie-He?le`ne Candito. 2000.
FTAG: A Lexicalized Tree Adjoining Grammar for
French. In Anne Abeille? and Owen Rambow, edi-
tors, Tree Adjoining Grammars: Formal, Computa-
tional and Linguistic Aspects, pages 305?329. CSLI
publications.
Tilman Becker and Patrice Lopez. 2000. Adapting
HPSG-to-TAG compilation to wide-coverage gram-
mars. In Proc. of TAG+5, pages 47?54.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proc. of IWPT 2000.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar.
In Proc. of ACL 2000, pages 456?463.
Christy Doran, Beth Ann Hockey, Anoop Sarkar,
B. Srinivas, and Fei Xia. 2000. Evolution of the
XTAG system. In Anne Abeille? and Owen Ram-
bow, editors, Tree Adjoining Grammars: Formal,
Computational and Linguistic Aspects, pages 371?
403. CSLI publications.
Dan Flickinger, Stephen Oepen, Jun?ichi Tsujii, and
Hans Uszkoreit, editors. 2000. Natural Language
Engineering ? Special Issue on Efficient Processing
with HPSG: Methods, Systems, Evaluation. Cam-
bridge University Press.
Dan Flickinger. 2000. On building a more effi-
cient grammar by exploiting types. Natural Lan-
guage Engineering ? Special Issue on Efficient Pro-
cessing with HPSG: Methods, Systems, Evaluation,
6(1):15?28.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
isi, and Jun?ichi Tsujii. 2000. Hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proc. of COLING 2000, pages 411?417.
Ronald Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. The MIT Press.
Robert Kasper, Bernd Kiefer, Klaus Netter, and
K. Vijay-Shanker. 1995. Compilation of HPSG to
TAG. In Proc. of ACL ?94, pages 92?99.
M. Kay, J. Gawron, and P. Norvig. 1994. Verbmo-
bil: A Translation System for Face-to-Face Dialog.
CSLI Publications.
Takaki Makino, Minoru Yoshida, Kentaro Torisawa,
and Jun?ichi Tsujii. 1998. LiLFeS ? towards a
practical HPSG parsers. In Proc. of COLING?ACL
?98, pages 807?811.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yutaka Mitsuishi, Kentaro Torisawa, and Jun?ichi Tsu-
jii. 1998. HPSG-style underspecified Japanese
grammar with wide coverage. In Proc. of
COLING?ACL ?98, pages 876?880.
Gu?ter Neumann. 1998. Automatic extraction of
stochastic lexcalized tree grammars from treebanks.
In Proc. of TAG+4, pages 120?123.
Kenji Nishida, Kentaro Torisawa, and Jun?ichi Tsujii.
1999. An efficient HPSG parsing algorithm with ar-
ray unification. In Proc. of NLPRS ?99, pages 144?
149.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Peter Poller and Tilman Becker. 1998. Two-step TAG
parsing revisited. In Proc. of TAG+4, pages 143?
146.
Anoop Sarkar and Shuly Wintner. 1999. Typing as a
means for validating feature structures. In Proc.of
CLIN ?99, pages 159?167.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proc. of COL-
ING 2000, pages 37?42.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? gram-
mars: Application to Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 578?583.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG En-
glish grammar to HPSG. In Proc. of TAG+4, pages
172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
http://www.cis.upenn.edu/?xtag/.
Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and
Jun?ichi Tsujii. 2000. An HPSG parser with CFG
filtering. Natural Language Engineering ? Special
Issue on Efficient Processing with HPSG: Methods,
Systems, Evaluation, 6(1):63?80.
Gertjan van Noord. 1994. Head corner parsing for
TAG. Computational Intelligence, 10(4):525?534.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Fea-
ture structures based Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 714?719.
K. Vijay-Shanker. 1987. A Study of Tree Adjoining
Grammars. Ph.D. thesis, Department of Computer
& Information Science, University of Pennsylvania.
Fei Xia. 1999. Extracting Tree Adjoining Grammars
from bracketed corpora. In Proc. of NLPRS ?99,
pages 398?403.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from FB-LTAG to HPSG. In Proc. of
ESSLLI 2001 Student Session. To appear.
Naoki Yoshinaga, Yusuke Miyao, Kentaro Torisawa,
and Jun?ichi Tsujii. 2001. Efficient LTAG parsing
using HPSG parsers. In Proc. of PACLING 2001.
To appear.
Tuning Support Vector Machines for Biomedical Named Entity Recognition
Jun?ichi Kazama? Takaki Makino? Yoshihiro Ohta? Jun?ichi Tsujii? ?
? Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
? Department of Complexity Science and Engineering, Graduate School of Frontier Sciences,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
? Central Research Laboratory, Hitachi, Ltd., Kokubunji, Tokyo 185-8601, Japan
? CREST, JST (Japan Science and Technology Corporation)
Abstract
We explore the use of Support Vector Ma-
chines (SVMs) for biomedical named en-
tity recognition. To make the SVM train-
ing with the available largest corpus ? the
GENIA corpus ? tractable, we propose to
split the non-entity class into sub-classes,
using part-of-speech information. In ad-
dition, we explore new features such as
word cache and the states of an HMM
trained by unsupervised learning. Experi-
ments on the GENIA corpus show that our
class splitting technique not only enables
the training with the GENIA corpus but
also improves the accuracy. The proposed
new features also contribute to improve
the accuracy. We compare our SVM-
based recognition system with a system
using Maximum Entropy tagging method.
1 Introduction
Application of natural language processing (NLP) is
now a key research topic in bioinformatics. Since
it is practically impossible for a researcher to grasp
all of the huge amount of knowledge provided in
the form of natural language, e.g., journal papers,
there is a strong demand for biomedical information
extraction (IE), which extracts knowledge automati-
cally from biomedical papers using NLP techniques
(Ohta et al, 1997; Proux et al, 2000; Yakushiji et
al., 2001).
The process called named entity recognition,
which finds entities that fill the information slots,
e.g., proteins, DNAs, RNAs, cells etc., in the
biomedical context, is an important building block in
such biomedical IE systems. Conceptually, named
entity recognition consists of two tasks: identifica-
tion, which finds the region of a named entity in
a text, and classification, which determines the se-
mantic class of that named entity. The following il-
lustrates biomedical named entity recognition.
?Thus, CIITAPROTEIN not only acti-
vates the expression of class II genesDNA
but recruits another B cell-specific
coactivator to increase transcriptional
activity of class II promotersDNA in
B cellsCELLTYPE.?
Machine learning approach has been applied to
biomedical named entity recognition (Nobata et al,
1999; Collier et al, 2000; Yamada et al, 2000;
Shimpuku, 2002). However, no work has achieved
sufficient recognition accuracy. One reason is the
lack of annotated corpora for training as is often
the case of a new domain. Nobata et al (1999) and
Collier et al (2000) trained their model with only
100 annotated paper abstracts from the MEDLINE
database (National Library of Medicine, 1999), and
Yamada et al (2000) used only 77 annotated paper
abstracts. In addition, it is difficult to compare the
techniques used in each study because they used a
closed and different corpus.
To overcome such a situation, the GENIA cor-
pus (Ohta et al, 2002) has been developed, and at
this time it is the largest biomedical annotated cor-
pus available to public, containing 670 annotated ab-
stracts of the MEDLINE database.
Another reason for low accuracies is that biomed-
ical named entities are essentially hard to recognize
using standard feature sets compared with the named
entities in newswire articles (Nobata et al, 2000).
Thus, we need to employ powerful machine learning
techniques which can incorporate various and com-
plex features in a consistent way.
Support Vector Machines (SVMs) (Vapnik, 1995)
and Maximum Entropy (ME) method (Berger et al,
1996) are powerful learning methods that satisfy
such requirements, and are applied successfully to
other NLP tasks (Kudo and Matsumoto, 2000; Nak-
agawa et al, 2001; Ratnaparkhi, 1996). In this pa-
per, we apply Support Vector Machines to biomed-
ical named entity recognition and train them with
                                            Association for Computational Linguistics.
                              the Biomedical Domain, Philadelphia, July 2002, pp. 1-8.
                         Proceedings of the Workshop on Natural Language Processing in
the GENIA corpus. We formulate the named entity
recognition as the classification of each word with
context to one of the classes that represent region
and named entity?s semantic class. Although there
is a previous work that applied SVMs to biomedi-
cal named entity task in this formulation (Yamada et
al., 2000), their method to construct a classifier us-
ing SVMs, one-vs-rest, fails to train a classifier with
entire GENIA corpus, since the cost of SVM train-
ing is super-linear to the size of training samples.
Even with a more feasible method, pairwise (Kre?el,
1998), which is employed in (Kudo and Matsumoto,
2000), we cannot train a classifier in a reasonable
time, because we have a large number of samples
that belong to the non-entity class in this formula-
tion. To solve this problem, we propose to split the
non-entity class to several sub-classes, using part-of-
speech information. We show that this technique not
only enables the training feasible but also improves
the accuracy.
In addition, we explore new features such as word
cache and the states of an unsupervised HMM for
named entity recognition using SVMs. In the exper-
iments, we show the effect of using these features
and compare the overall performance of our SVM-
based recognition system with a system using the
Maximum Entropy method, which is an alternative
to the SVM method.
2 The GENIA Corpus
The GENIA corpus is an annotated corpus of pa-
per abstracts taken from the MEDLINE database.
Currently, 670 abstracts are annotated with named
entity tags by biomedical experts and made avail-
able to public (Ver. 1.1).1 These 670 abstracts are a
subset of more than 5,000 abstracts obtained by the
query ?human AND blood cell AND transcription
factor? to the MEDLINE database. Table 1 shows
basic statistics of the GENIA corpus. Since the GE-
NIA corpus is intended to be extensive, there exist
24 distinct named entity classes in the corpus.2 Our
task is to find a named entity region in a paper ab-
stract and correctly select its class out of these 24
classes. This number of classes is relatively large
compared with other corpora used in previous stud-
ies, and compared with the named entity task for
newswire articles. This indicates that the task with
the GENIA corpus is hard, apart from the difficulty
of the biomedical domain itself.
1Available via http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
2The GENIA corpus also has annotations for conjunc-
tive/disjunctive named entity expressions such as ?human B- or
T-cell lines? (Kim et al, 2001). In this paper we ignore such
expressions and consider that constituents in such expressions
are annotated as a dummy class ?temp?.
Table 1: Basic statistics of the GENIA corpus
# of sentences 5,109
# of words 152,216
# of named entities 23,793
# of words in NEs 50,229
# of words not in NEs 101,987
Av. length of NEs (?) 2.11 (1.40)
3 Named Entity Recognition Using SVMs
3.1 Named Entity Recognition as Classification
We formulate the named entity task as the classi-
fication of each word with context to one of the
classes that represent region information and named
entity?s semantic class. Several representations to
encode region information are proposed and exam-
ined (Ramshaw and Marcus, 1995; Uchimoto et al,
2000; Kudo and Matsumoto, 2001). In this paper,
we employ the simplest BIO representation, which
is also used in (Yamada et al, 2000). We modify
this representation in Section 5.1 in order to acceler-
ate the SVM training.
In the BIO representation, the region information
is represented as the class prefixes ?B-? and ?I-?, and
a class ?O?. B- means that the current word is at the
beginning of a named entity, I- means that the cur-
rent word is in a named entity (but not at the be-
ginning), and O means the word is not in a named
entity. For each named entity class C, class B-C and
I-C are produced. Therefore, if we have N named
entity classes, the BIO representation yields 2N + 1
classes, which will be the targets of a classifier. For
instance, the following corresponds to the annota-
tion ?Number of glucocorticoid receptorsPROTEIN in
lymphocytesCELLTYPE and ...?.
Number of glucocorticoid receptors
O O B-PROTEIN I-PROTEIN
in lymphocytes and ...
O B-CELLTYPE O ...
3.2 Support Vector Machines
Support Vector Machines (SVMs) (Cortes and Vap-
nik, 1995) are powerful methods for learning a clas-
sifier, which have been applied successfully to many
NLP tasks such as base phrase chunking (Kudo and
Matsumoto, 2000) and part-of-speech tagging (Nak-
agawa et al, 2001).
The SVM constructs a binary classifier that out-
puts +1 or ?1 given a sample vector x ? Rn. The de-
cision is based on the separating hyperplane as fol-
lows.
c(x) =
?????
+1 if w ? x + b > 0, w ? Rn, b ? R,
?1 otherwise
The class for an input x, c(x), is determined by see-
ing which side of the space separated by the hyper-
plane, w ? x + b = 0, the input lies on.
Given a set of labeled training samples
{(y1, x1), ? ? ? , (yL, xL)}, xi ? Rn, yi ? {+1,?1},
the SVM training tries to find the optimal hy-
perplane, i.e., the hyperplane with the maximum
margin. Margin is defined as the distance between
the hyperplane and the training samples nearest
to the hyperplane. Maximizing the margin insists
that these nearest samples (support vectors) exist
on both sides of the separating hyperplane and the
hyperplane lies exactly at the midpoint of these
support vectors. This margin maximization tightly
relates to the fine generalization power of SVMs.
Assuming that |w?xi+b| = 1 at the support vectors
without loss of generality, the SVM training can be
formulated as the following optimization problem.3
minimize 1
2
||w||2
subject to yi(w ? xi + b) ? 1, i = 1, ? ? ? , L.
The solution of this problem is known to be written
as follows, using only support vectors and weights
for them.
f (x) = w ? x + b=
?
i?S V s
yi?ix ? xi + b (1)
In the SVM learning, we can use a function k(xi, x j)
called a kernel function instead of the inner prod-
uct in the above equation. Introducing a kernel
function means mapping an original input x using
?(x), s.t. ?(xi) ??(x j) = k(xi, x j) to another, usually
a higher dimensional, feature space. We construct
the optimal hyperplane in that space. By using ker-
nel functions, we can construct a non-linear separat-
ing surface in the original feature space. Fortunately,
such non-linear training does not increase the com-
putational cost if the calculation of the kernel func-
tion is as cheap as the inner product. A polynomial
function defined as (sxi ? x j + r)d is popular in ap-
plications of SVMs to NLPs (Kudo and Matsumoto,
2000; Yamada et al, 2000; Kudo and Matsumoto,
2001), because it has an intuitively sound interpre-
tation that each dimension of the mapped space is a
3For many real-world problems where the samples may be
inseparable, we allow the constraints are broken with some
penalty. In the experiments, we use so-called 1-norm soft mar-
gin formulation described as:
minimize 1
2
||w||2 + C
L?
i=1
?i
subject to yi(w ? xi + b) ? 1 ? ?i, i = 1, ? ? ? , L,
?i ? 0, i = 1, ? ? ? , L.
(weighted) conjunction of d features in the original
sample.
3.3 Multi-Class SVMs
As described above, the standard SVM learning con-
structs a binary classifier. To make a named entity
recognition system based on the BIO representation,
we require a multi-class classifier. Among several
methods for constructing a multi-class SVM (Hsu
and Lin, 2002), we use a pairwise method proposed
by Kre?el (1998) instead of the one-vs-rest method
used in (Yamada et al, 2000), and extend the BIO
representation to enable the training with the entire
GENIA corpus. Here we describe the one-vs-rest
method and the pairwise method to show the neces-
sity of our extension.
Both one-vs-rest and pairwise methods construct
a multi-class classifier by combining many binary
SVMs. In the following explanation, K denotes the
number of the target classes.
one-vs-rest Construct K binary SVMs, each of
which determines whether the sample should
be classified as class i or as the other classes.
The output is the class with the maximum f (x)
in Equation 1.
pairwise Construct K(K ? 1)/2 binary SVMs, each
of which determines whether the sample should
be classified as class i or as class j. Each binary
SVM has one vote, and the output is the class
with the maximum votes.
Because the SVM training is a quadratic optimiza-
tion program, its cost is super-linear to the size of the
training samples even with the tailored techniques
such as SMO (Platt, 1998) and kernel evaluation
caching (Joachims, 1998). Let L be the number of
the training samples, then the one-vs-rest method
takes time in K ? OS V M(L). The BIO formula-
tion produces one training sample per word, and
the training with the GENIA corpus involves over
100,000 training samples as can be seen from Ta-
ble 1. Therefore, it is apparent that the one-vs-
rest method is impractical with the GENIA corpus.
On the other hand, if target classes are equally dis-
tributed, the pairwise method will take time in K(K?
1)/2?OS V M(2L/K). This method is worthwhile be-
cause each training is much faster, though it requires
the training of (K ? 1)/2 times more classifiers. It
is also reported that the pairwise method achieves
higher accuracy than other methods in some bench-
marks (Kre?el, 1998; Hsu and Lin, 2002).
3.4 Input Features
An input x to an SVM classifier is a feature repre-
sentation of the word to be classified and its context.
We use a bit-vector representation, each dimension
of which indicates whether the input matches with
a certain feature. The following illustrates the well-
used features for the named entity recognition task.
wk,i =
???????????
1 if a word at k,Wk, is the ith word
in the vocabularyV
0 otherwise (word feature)
posk,i =
???????????
1 if Wk is assigned the ith POS tag
in the POS tag list POS
0 otherwise (part-of-speech feature)
prek,i =
???????????
1 if Wk starts with the ith prefix
in the prefix list P
0 otherwise (prefix feature)
suf k,i =
???????????
1 if Wk starts with the ith suffix
in the suffix list S
0 otherwise (suffix feature)
subk,i =
???????????
1 if Wk contains the ith substring
in the substring list SB
0 otherwise (substring feature)
pck,i =
?????
1 if Wk(k < 0) was assigned ith class
0 otherwise (preceding class feature)
In the above definitions, k is a relative word position
from the word to be classified. A negative value rep-
resents a preceding word?s position, and a positive
value represents a following word?s position. Note
that we assume that the classification proceeds left
to right as can be seen in the definition of the pre-
ceding class feature. For the SVM classification, we
does not use a dynamic argmax-type classification
such as the Viterbi algorithm, since it is difficult to
define a good comparable value for the confidence of
a prediction such as probability. The consequences
of this limitation will be discussed with the experi-
mental results.
Features usually form a group with some vari-
ables such as the position unspecified. In this paper,
we instantiate all features, i.e., instantiate for all i,
for a group and a position. Then, it is convenient to
denote a set of features for a group g and a position
k as gk (e.g., wk and posk). Using this notation, we
write a feature set as {w?1,w0, pre?1, pre0, pc?1}.4
This feature description derives the following input
vector.5
x = {w?1,1,w?1,2, ? ? ? ,w?1,|V|,w0,1, ? ? ? ,w0,|V|,
pre?1,1, ? ? ? , pre0,|P|, pc?1,1, ? ? ? , pc?1,K}
4We will further compress this as {?w, pre?[?1,0], pc?1}.
5Although a huge number of features are instantiated, only
a few features have value one for a given g and k pair.
4 Named Entity Recognition Using ME
Model
The Maximum Entropy method, with which we
compare our SVM-based method, defines the prob-
ability that the class is c given an input vector x as
follows.
P(c|x) = 1
Z(x)
?
i
? fi(c,x)i ,
where Z(x) is a normalization constant, and fi(c, x)
is a feature function. A feature function is defined
in the same way as the features in the SVM learn-
ing, except that it includes c in it like f (c, x) =
(c is the jth class) ? wi,k(x). If x contains pre-
viously assigned classes, then the most probable
class sequence, c?T1 = argmaxc1,??? ,cT
?T
t=1 P(ct|xt) is
searched by using the Viterbi-type algorithm. We
use the maximum entropy tagging method described
in (Kazama et al, 2001) for the experiments, which
is a variant of (Ratnaparkhi, 1996) modified to use
HMM state features.
5 Tuning of SVMs for Biomedical NE Task
5.1 Class Splitting Technique
In Section 3.3, we described that if target classes are
equally distributed, the pairwise method will reduce
the training cost. In our case, however, we have a
very unbalanced class distribution with a large num-
ber of samples belonging to the class ?O? (see Table
1). This leads to the same situation with the one-vs-
rest method, i.e., if LO is the number of the samples
belonging to the class ?O?, then the most dominant
part of the training takes time in K ? OS V M(LO).
One solution to this unbalanced class distribution
problem is to split the class ?O? into several sub-
classes effectively. This will reduce the training cost
for the same reason that the pairwise method works.
In this paper, we propose to split the non-entity
class according to part-of-speech (POS) informa-
tion of the word. That is, given a part-of-speech
tag set POS, we produce new |POS| classes, ?O-
p? p ? POS. Since we use a POS tagger that out-
puts 45 Penn Treebank?s POS tags in this paper, we
have new 45 sub-classes which correspond to non-
entity regions such as ?O-NNS? (plural nouns), ?O-
JJ? (adjectives), and ?O-DT? (determiners).
Splitting by POS information seems useful for im-
proving the system accuracy as well, because in the
named entity recognition we must discriminate be-
tween nouns in named entities and nouns in ordi-
nal noun phrases. In the experiments, we show this
class splitting technique not only enables the feasi-
ble training but also improves the accuracy.
5.2 Word Cache and HMM Features
In addition to the standard features, we explore word
cache feature and HMM state feature, mainly to
solve the data sparseness problem.
Although the GENIA corpus is the largest anno-
tated corpus for the biomedical domain, it is still
small compared with other linguistic annotated cor-
pora such as the Penn Treebank. Thus, the data
sparseness problem is severe, and must be treated
carefully. Usually, the data sparseness is prevented
by using more general features that apply to a
broader set of instances (e.g., disjunctions). While
polynomial kernels in the SVM learning can effec-
tively generate feature conjunctions, kernel func-
tions that can effectively generate feature disjunc-
tions are not known. Thus, we should explicitly add
dimensions for such general features.
The word cache feature is defined as the disjunc-
tion of several word features as:
wck{k1,??? ,kn},i ? ?k?kwk,i
We intend that the word cache feature captures the
similarities of the patterns with a common key word
such as follows.
(a) ?human W?2 W?1 W0? and ?human W?1 W0?
(b) ?W0 gene? and ?W0 W1 gene?
We use a left word cache defined as lwck,i ?
wc{?k,??? ,0},i, and a right word cache defined as
rwck,i ? wc{1,??? ,k},i for patterns like (a) and (b) in
the above example respectively.
Kazama et al (2001) proposed to use as features
the Viterbi state sequence of a hidden Markov model
(HMM) to prevent the data sparseness problem in
the maximum entropy tagging model. An HMM is
trained with a large number of unannotated texts by
using an unsupervised learning method. Because
the number of states of the HMM is usually made
smaller than |V|, the Viterbi states give smoothed
but maximally informative representations of word
patterns tuned for the domain, from which the raw
texts are taken.
The HMM feature is defined in the same way as
the word feature as follows.
hmmk,i =
???????????
1 if the Viterbi state for Wk is
the ith state in the HMM?s statesH
0 otherwise (HMM feature)
In the experiments, we train an HMM using raw
MEDLINE abstracts in the GENIA corpus, and
show that the HMM state feature can improve the
accuracy.
5.3 Implementation Issues
Towards practical named entity recognition using
SVMs, we have tackled the following implementa-
tion issues. It would be impossible to carry out the
experiments in a reasonable time without such ef-
forts.
Parallel Training: The training of pairwise SVMs
has trivial parallelism, i.e., each SVM can be trained
separately. Since computers with two or more CPUs
are not expensive these days, parallelization is very
practical solution to accelerate the training of pair-
wise SVMs.
Fast Winner Finding: Although the pairwise
method reduces the cost of training, it greatly in-
creases the number of classifications needed to de-
termine the class of one sample. For example, for
our experiments using the GENIA corpus, the BIO
representation with class splitting yields more than
4,000 classification pairs. Fortunately, we can stop
classifications when a class gets K ? 1 votes and this
stopping greatly saves classification time (Kre?el,
1998). Moreover, we can stop classifications when
the current votes of a class is greater than the others?
possible votes.
Support Vector Caching: In the pairwise method,
though we have a large number of classifiers, each
classifier shares some support vectors with other
classifiers. By storing the bodies of all support vec-
tors together and letting each classifier have only the
weights, we can greatly reduce the size of the clas-
sifier. The sharing of support vectors also can be
exploited to accelerate the classification by caching
the value of the kernel function between a support
vector and a classifiee sample.
6 Experiments
To conduct experiments, we divided 670 abstracts
of the GENIA corpus (Ver. 1.1) into the train-
ing part (590 abstracts; 4,487 sentences; 133,915
words) and the test part (80 abstracts; 622 sen-
tences; 18,211 words).6 Texts are tokenized by us-
ing Penn Treebank?s tokenizer. An HMM for the
HMM state features was trained with raw abstracts
of the GENIA corpus (39,116 sentences).7 The
number of states is 160. The vocabulary for the
word feature is constructed by taking the most fre-
quent 10,000 words from the above raw abstracts,
the prefix/suffix/prefix list by taking the most fre-
quent 10,000 prefixes/suffixes/substrings.8
The performance is measured by precision, recall,
and F-score, which are the standard measures for the
6Randomly selected set used in (Shimpuku, 2002). We do
not use paper titles, while he used.
7These do not include the sentences in the test part.
8These are constructed using the training part to make the
comparison with the ME method fair.
Table 2: Training time and accuracy with/without
the class splitting technique. The number of training
samples includes SOS and EOS (special words for
the start/end of a sentence).
no splitting splitting
training time acc. time acc.
samples (sec.) (F-score) (sec.) (F-
score)
16,000 2,809 37.04 5,581 36.82
32,000 13,614 40.65 9,175 41.36
48,000 21,174 42.44 9,709 42.49
64,000 40,869 42.52 12,502 44.34
96,000 - - 21,922 44.93
128,000 - - 36,846 45.99
named entity recognition. Systems based on the BIO
representation may produce an inconsistent class se-
quence such as ?O B-DNA I-RNA O?. We interpret
such outputs as follows: once a named entity starts
with ?B-C? then we interpret that the named entity
with class ?C? ends only when we see another ?B-?
or ?O-? tag.
We have implemented SMO algorithm (Platt,
1998) and techniques described in (Joachims, 1998)
for soft margin SVMs in C++ programming lan-
guage, and implemented support codes for pairwise
classification and parallel training in Java program-
ming language. To obtain POS information required
for features and class splitting, we used an English
POS tagger described in (Kazama et al, 2001).
6.1 Class Splitting Technique
First, we show the effect of the class splitting
described in Section 5.1. Varying the size of
training data, we compared the change in the
training time and the accuracy with and with-
out the class splitting. We used a feature set
{?w, pre, suf , sub, pos?[?2,??? ,2], pc[?2,?1]} and the in-
ner product kernel.9 The training time was mea-
sured on a machine with four 700MHz PentiumIIIs
and 16GB RAM. Table 2 shows the results of the
experiments. Figure 1 shows the results graphi-
cally. We can see that without splitting we soon suf-
fer from super-linearity of the SVM training, while
with splitting we can handle the training with over
100,000 samples in a reasonable time. It is very im-
portant that the splitting technique does not sacrifice
the accuracy for speed, rather improves the accuracy.
6.2 Word Cache and HMM State Features
In this experiment, we see the effect of the word
cache feature and the HMM state feature described
in Section 3.4. The effect is assessed by the
accuracy gain observed by adding each feature
set to a base feature set and the accuracy degra-
dation observed by subtracting it from a (com-
9Soft margin constant C is 1.0 throughout the experiments.
Table 3: Effect of each feature set assessed by
adding/subtracting (F-score). Changes in bold face
means positive effect.
feature set (A) adding (B) sub. (k=2) (C) sub. (k=3)
Base 42.86 47.82 49.27
Left cache 43.25 (+0.39) 47.77 (-0.05) 49.02 (-0.25)
Right cache 42.34 (-0.52) 47.81 (-0.01) 49.07 (-0.20)
HMM state 44.70 (+1.84) 47.25 (-0.57) 48.03 (-1.24)
POS 44.82 (+1.96) 48.29 (+0.47) 48.75 (-0.52)
Prec. class 44.58 (+1.72) 43.32 (-4.50) 43.84 (-5.43)
Prefix 42.77 (-0.09) 48.11 (+0.29) 48.73 (-0.54)
Suffix 45.88 (+3.02) 47.07 (-0.75) 48.48 (-0.79)
Substring 42.16 (-0.70) 48.38 (+0.56) 50.23 (+0.96)
plete) base set. The first column (A) in Ta-
ble 3 shows an adding case where the base fea-
ture set is {w[?2,??? ,2]}. The columns (B) and
(C) show subtracting cases where the base feature
set is {?w, pre, suf , sub, pos, hmm?[?k,??? ,k], lwck, rwck,
pc[?2,?1]} with k = 2 and k = 3 respectively. The
kernel function is the inner product. We can see that
word cache and HMM state features surely improve
the recognition accuracy. In the table, we also in-
cluded the accuracy change for other standard fea-
tures. Preceeding classes and suffixes are definitely
helpful. On the other hand, the substring feature is
not effective in our setting. Although the effects of
part-of-speech tags and prefixes are not so definite,
it can be said that they are practically effective since
they show positive effects in the case of the maxi-
mum performance.
6.3 Comparison with the ME Method
In this set of experiments, we compare our
SVM-based system with a named entity recog-
nition system based on the Maximum Entropy
method. For the SVM system, we used the fea-
ture set {?w, pre, suf , pos, hmm?[?3,??? ,3], lwc3, rwc3,
pc[?2,?1]}, which is shown to be the best in the pre-
vious experiment. The compared system is a max-
imum entropy tagging model described in (Kazama
et al, 2001). Though it supports several character
type features such as number and hyphen and some
conjunctive features such as word n-gram, we do not
use these features to compare the performance un-
der as close a condition as possible. The feature set
used in the maximum entropy system is expressed
as {?w, pre, suf , pos, hmm?[?2,??? ,2], pc[?2,?1]}.10 Both
systems use the BIO representation with splitting.
Table 4 shows the accuracies of both systems. For
the SVM system, we show the results with the inner
product kernel and several polynomial kernels. The
row ?All (id)? shows the accuracy from the view-
10When the width becomes [?3, ? ? ? , 3], the accuracy de-
grades (53.72 to 51.73 in F-score).
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 0  20000  40000  60000  80000  100000  120000  140000
Tra
inin
g T
ime
 (se
cond
s)
Number of training samples
No splitSplit
(a) Training size vs. time
 0.36
 0.37
 0.38
 0.39
 0.4
 0.41
 0.42
 0.43
 0.44
 0.45
 0.46
 0  5000  10000  15000  20000  25000  30000  35000  40000  45000
Ter
m A
ccu
rac
y (F
-Sco
re)
Training Time (seconds)
No splitSplit
(b) Training time vs. accuracy
Figure 1: Effect of the class splitting technique.
point of the identification task, which only finds the
named entity regions. The accuracies for several ma-
jor entity classes are also shown. The SVM system
with the 2-dimensional polynomial kernel achieves
the highest accuracy. This comparison may be un-
fair since a polynomial kernel has the effect of us-
ing conjunctive features, while the ME system does
not use such conjunctive features. Nevertheless, the
facts: we can introduce the polynomial kernel very
easily; there are very few parameters to be tuned;11
we could achieve the higher accuracy; show an ad-
vantage of the SVM system.
It will be interesting to discuss why the SVM sys-
tems with the inner product kernel (and the polyno-
mial kernel with d = 1) are outperformed by the ME
system. We here discuss two possible reasons. The
first is that the SVM system does not use a dynamic
decision such as the Viterbi algorithm, while the ME
system uses it. To see this, we degrade the ME sys-
tem so that it predicts the classes deterministically
without using the Viterbi algorithm. We found that
this system only marks 51.54 in F-score. Thus, it can
be said that a dynamic decision is important for this
named entity task. However, although a method to
convert the outputs of a binary SVM to probabilistic
values is proposed (Platt, 1999), the way to obtain
meaningful probabilistic values needed in Viterbi-
type algorithms from the outputs of a multi-class
SVM is unknown. Solving this problem is certainly
a part of the future work. The second possible rea-
son is that the SVM system in this paper does not
use any cut-off or feature truncation method to re-
move data noise, while the ME system uses a sim-
ple feature cut-off method.12 We observed that the
ME system without the cut-off only marks 49.11 in
11C, s, r, and d
12Features that occur less than 10 times are removed.
F-score. Thus, such a noise reduction method is
also important. However, the cut-off method for the
ME method cannot be applied without modification
since, as described in Section 3.4, the definition of
the features are different in the two approaches. It
can be said the features in the ME method is ?finer?
than those in SVMs. In this sense, the ME method
allows us more flexible feature selection. This is an
advantage of the ME method.
The accuracies achieved by both systems can be
said high compared with those of the previous meth-
ods if we consider that we have 24 named entity
classes. However, the accuracies are not sufficient
for a practical use. Though higher accuracy will be
achieved with a larger annotated corpus, we should
also explore more effective features and find effec-
tive feature combination methods to exploit such a
large corpus maximally.
7 Conclusion
We have described the use of Support Vector Ma-
chines for the biomedical named entity recognition
task. To make the training of SVMs with the GE-
NIA corpus practical, we proposed to split the non-
entity class by using POS information. In addition,
we explored the new types of features, word cache
and HMM states, to avoid the data sparseness prob-
lem. In the experiments, we have shown that the
class splitting technique not only makes training fea-
sible but also improves the accuracy. We have also
shown that the proposed new features also improve
the accuracy and the SVM system with the polyno-
mial kernel function outperforms the ME-based sys-
tem.
Acknowledgements
We would like to thank Dr. Jin-Dong Kim for pro-
viding us easy-to-use preprocessed training data.
Table 4: Comparison: The SVM-based system and the ME-based system. (precision/recall/F-score)
SVM ME
inner product polynomial (s = 0.01, r = 1.0))
type # d = 1 d = 2 d = 3
All (2,782) 50.7 /49.8 /50.2 54.6 /48.8 /51.5 56.2 /52.8 /54.4 55.1 /51.5 /53.2 53.4 /53.0 /53.2
All(id) 71.8 /70.4 /71.1 75.0 /67.1 /70.8 75.9 /71.4 /73.6 75.3 /70.3 /72.7 73.5 /72.9 /73.2
protein (709) 47.2 /55.2 /50.8 45.7 /64.9 /53.6 49.2 /66.4 /56.5 48.7 /64.7 /55.6 49.1 /62.1 /54.8
DNA (460) 39.9 /37.6 /38.7 48.2 /31.5 /38.1 49.6 /37.0 /42.3 47.9 /37.4 /42.0 47.3 /39.6 /43.1
cell line (121) 54.8 /47.1 /50.7 61.2 /43.0 /50.5 60.2 /46.3 /52.3 62.2 /46.3 /53.1 58.0 /53.7 /55.8
cell type (199) 67.6 /74.4 /70.8 67.4 /74.9 /71.0 70.0 /75.4 /72.6 68.6 /72.4 /70.4 69.9 /72.4 /71.1
lipid (109) 77.0 /61.5 /68.4 83.3 /50.5 /62.9 82.7 /61.5 /70.5 79.2 /56.0 /65.6 68.9 /65.1 /67.0
other names (590) 52.5 /53.9 /53.2 60.2 /55.9 /58.0 59.3 /58.0 /58.6 58.9 /57.8 /58.3 59.0 /61.7 /60.3
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names
of genes and gene products with a hidden Markov model. In
Proc. of COLING 2000, pages 201?207.
C. Cortes and V. Vapnik. 1995. Support vector networks. Ma-
chine Learning, 20:273?297.
C. Hsu and C. Lin. 2002. A comparison of methods for multi-
class Support Vector Machines. In IEEE Transactions on
Neural Networks. to appear.
T. Joachims. 1998. Making large-scale support vector machine
learning practical. In Advances in Kernel Methods, pages
169?184. The MIT Press.
J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maximum entropy
tagger with unsupervised hidden markov models. In Proc. of
the 6th NLPRS, pages 333?340.
J. Kim, T. Ohta, Y. Tateisi, H. Mima, and J. Tsujii. 2001. XML-
based linguistic annotation of corpus. In Proc. of the First
NLP and XML Workshop.
U. Kre?el. 1998. Pairwise classification and support vector
machines. In Advances in Kernel Methods, pages 255?268.
The MIT Press.
T. Kudo and Y. Matsumoto. 2000. Use of support vector learn-
ing for chunk identification. In Proc. of CoNLL-2000 and
LLL-2000.
T. Kudo and Y. Matsumoto. 2001. Chunking with Support
Vector Machines. In Proc. of NAACL 2001, pages 192?199.
T. Nakagawa, T. Kudoh, and Y. Matsumoto. 2001. Unknown
word guessing and part-of-speech tagging using support vec-
tor machines. In Proc. of the 6th NLPRS, pages 325?331.
National Library of Medicine. 1999. MEDLINE. available at
http://www.ncbi.nlm.nih.gov/.
C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic term
identification and classification in biology texts. In Proc. of
the 5th NLPRS, pages 369?374.
C. Nobata, N. Collier, and J. Tsujii. 2000. Comparison between
tagged corpora for the named entity task. In Proc. of the
Workshop on Comparing Corpora (at ACL?2000), pages 20?
27.
Y. Ohta, Y. Yamamoto, T. Okazaki, I. Uchiyama, and T. Tak-
agi. 1997. Automatic construction of knowledge base from
biological papers. In Proc. of the 5th ISMB, pages 218?225.
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and Tsujii J. 2002. The
GENIA corpus: An annotated research abstract corpus in
molecular biology domain. In Proc. of HLT 2002.
J. C. Platt. 1998. Fast training of support vector machines us-
ing sequential minimal optimization. In Advances in Kernel
Methods, pages 185?208. The MIT Press.
J. C. Platt. 1999. Probabilistic outputs for support vector ma-
chines and comparisons to regularized likelihood methods.
Advances in Large Margin Classifiers.
D. Proux, F. Prechenmann, and L. Julliard. 2000. A pragmatic
information extraction strategy for gathering data on genetic
interactions. In Proc. of the 8th ISMB, pages 279?285.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proc. of the 3rd ACL
Workshop on Very Large Corpora.
A. Ratnaparkhi. 1996. A maximum entropy model for part-
of-speech tagging. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 133?142.
S. Shimpuku. 2002. A medical/biological term recognizer with
a term hidden Markov model incorporating multiple infor-
mation sources. A master thesis. University of Tokyo.
K. Uchimoto, M. Murata, Q. Ma, H. Ozaku, and H. Isahara.
2000. Named entity extraction based on a maximum entropy
model and transformation rules. In Proc. of the 38th ACL,
pages 326?335.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001. Event
extraction from biomedical papers using a full parser. In
Proc. of PSB 2001, pages 408?419.
H. Yamada, T. Kudo, and Y. Matsumoto. 2000. Using sub-
strings for technical term extraction and classification. IPSJ
SIGNotes, (NL-140):77?84. (in Japanese).
A model of syntactic disambiguation based on lexicalized grammars
Yusuke Miyao
Department of Computer Science,
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science,
University of Tokyo
CREST, JST
(Japan Science and Technology Corporation)
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a new approach to syntac-
tic disambiguation based on lexicalized gram-
mars. While existing disambiguation mod-
els decompose the probability of parsing re-
sults into that of primitive dependencies of two
words, our model selects the most probable
parsing result from a set of candidates allowed
by a lexicalized grammar. Since parsing re-
sults given by the lexicalized grammar cannot
be decomposed into independent sub-events,
we apply a maximum entropy model for fea-
ture forests, which allows probabilistic model-
ing without the independence assumption. Our
approach provides a general method of produc-
ing a consistent probabilistic model of parsing
results given by lexicalized grammars.
1 Introduction
Recent studies on the automatic extraction of lexicalized
grammars (Xia, 1999; Chen and Vijay-Shanker, 2000;
Hockenmaier and Steedman, 2002a) allow the modeling
of syntactic disambiguation based on linguistically moti-
vated grammar theories including LTAG (Chiang, 2000)
and CCG (Clark et al, 2002; Hockenmaier and Steed-
man, 2002b). However, existing models of disambigua-
tion with lexicalized grammars are a mere extension of
lexicalized probabilistic context-free grammars (LPCFG)
(Collins, 1996; Collins, 1997; Charniak, 1997), which
are based on the decomposition of parsing results into the
syntactic/semantic dependencies of two words in a sen-
tence under the assumption of independence of the de-
pendencies. While LPCFG models have proved that the
incorporation of lexical associations (i.e., dependencies
of words) significantly improves the accuracy of parsing,
this idea has been naively inherited in the recent studies
on disambiguation models of lexicalized grammars.
However, the disambiguation models of lexicalized
grammars should be totally different from that of LPCFG,
because the grammars define the relation of syntax and
semantics, and can restrict the possible structure of pars-
ing results. Parsing results cannot simply be decomposed
into primitive dependencies, because the complete struc-
ture is determined by solving the syntactic constraints
of a complete sentence. For example, when we apply
a unification-based grammar, LPCFG-like modeling re-
sults in an inconsistent probability model because the
model assigns probabilities to parsing results not allowed
by the grammar (Abney, 1997). We have only two ways
of adhering to LPCFG models: preserve the consistency
of probability models by abandoning improvements to
the lexicalized grammars using complex constraints (Chi-
ang, 2000), or ignore the inconsistency in probability
models (Clark et al, 2002).
This paper provides a new model of syntactic disam-
biguation in which lexicalized grammars can restrict the
possible structures of parsing results. Our modeling aims
at providing grounds for i) producing a consistent proba-
bilistic model of lexicalized grammars, as well as ii) eval-
uating the contributions of syntactic and semantic prefer-
ences to syntactic disambiguation. The model is com-
posed of the syntax and semantics probabilities, which
represent syntactic and semantic preferences respectively.
The syntax probability is responsible for determining the
syntactic categories chosen by words in a sentence, and
the semantics probability selects the most plausible de-
pendencies of words from candidates allowed by the syn-
tactic categories yielded by the syntax probability. Since
the sequence of syntactic categories restricts the possi-
ble structure of parsing results, the semantics probabil-
ity is a conditional probability without decomposition
into the primitive dependencies of words. Recently used
machine learning methods including maximum entropy
models (Berger et al, 1996) and support vector machines
(Vapnik, 1995) provide grounds for this type of model-
ing, because it allows various dependent features to be
incorporated into the model without the independence as-
sumption.
The above approach, however, has a serious deficiency:
a lexicalized grammar assigns exponentially many pars-
ing results because of local ambiguities in a sentence,
which is problematic in estimating the parameters of a
probability model. To cope with this, we adopted an
algorithm of maximum entropy estimation for feature
forests (Miyao and Tsujii, 2002; Geman and Johnson,
2002), which allows parameters to be efficiently esti-
mated. The algorithm enables probabilistic modeling
of complete structures, such as transition sequences in
Markov models and parse trees, without dividing them
into independent sub-events. The algorithm avoids expo-
nential explosion by representing a probabilistic event by
a packed representation of a feature space. If a complete
structure is represented with a feature forest of a tractable
size, the parameters can be efficiently estimated by dy-
namic programming.
A series of studies on parsing with wide-coverage LFG
(Johnson et al, 1999; Riezler et al, 2000; Riezler et al,
2002) have had a similar motivation to ours. Their mod-
els have also been based on a discriminative model to
select a parsing result from all candidates given by the
grammar. A significant difference is that we apply max-
imum entropy estimation for feature forests to avoid the
inherent problem with estimation: the exponential explo-
sion of parsing results given by the grammar. They as-
sumed that parsing results would be suppressed to a rea-
sonable number through using heuristic rules, or by care-
fully implementing a fully restrictive and wide-coverage
grammar, which requires a considerable amount of effort
to develop. Our contention is that this problem can be
solved in a more sophisticated way as is discussed in this
paper. Another difference is that our model is separated
into syntax and semantics probabilities, which will ben-
efit computational/linguistic investigations into the rela-
tion between syntax and semantics, and allow separate
improvements to both models.
Overall, the approach taken in this paper is different
from existing models in the following respects.
? Since it does not require the assumption of inde-
pendence, the probability model is consistent with
lexicalized grammars with complex constraints in-
cluding unification-based grammar formalism. Our
model can assign consistent probabilities to parsing
results of lexicalized grammars, while the traditional
models assign probabilities to parsing results not al-
lowed by the grammar.
? Since the syntax and semantics probabilities are sep-
arate, we can improve them individually. For exam-
ple, the syntax model can be improved by smooth-
ing using the syntactic classes of words, while the
semantics model should be able to be improved by
using semantic classes. In addition, the model can
be a starting point that allows the theory of syntax
and semantics to be evaluated through consulting an
extensive corpus.
We evaluated the validity of our model through experi-
ments on a disambiguation task of parsing the Penn Tree-
bank (Marcus et al, 1994) with an automatically acquired
LTAG grammar. To assess the contribution of the syntax
and semantics probabilities to the accuracy of parsing and
to evaluate the validity of applying maximum entropy es-
timation for feature forests, we compared three models
trained with the same training set and the same set of fea-
tures. Following the experimental results, we concluded
that i) a parser with the syntax probability only achieved
high accuracy with the lexicalized grammar, ii) the in-
corporation of preferences for lexical association through
the semantics probability resulted in significant improve-
ments, and iii) our model recorded an accuracy that was
quite close to the traditional model, which indicated the
validity of applying maximum entropy estimation for fea-
ture forests.
In what follows, we first describe the existing models
for syntactic disambiguation, and discuss problems with
them in Section 2. We then define the general form for
parsing results of lexicalized grammars, and introduce
our model in Section 3. We prove the validity of our ap-
proach through a series of experiments in Section 4.
2 Traditional models for syntactic
disambiguation
This section reviews the existing models for syntactic dis-
ambiguation from the viewpoint of representing parsing
results of lexicalized grammars. In particular, we dis-
cuss how the models incorporate syntactic/semantic pref-
erences for syntactic disambiguation. The existing stud-
ies are based on the decomposition of parsing results into
primitive lexical dependencies where syntactic/semantic
preferences are combined. This traditional scheme of
syntactic disambiguation can be problematic with lexi-
calized grammars. Throughout the discussion, we refer
to the example sentence ?What does your student want to
write??, whose parse tree is in Figure 1.
2.1 Lexicalized parse trees
The first successful work on syntactic disambiguation
was based on lexicalized probabilistic context-free gram-
mar (LPCFG) (Collins, 1997; Charniak, 1997). Although
LPCFG is not exactly classified into lexicalized grammar
formalism, we should mention these studies since they
demonstrated that lexical dependencies were essential to
improving the accuracy of parsing.
what
does
your want
to write
S
S
S
VP
VP
NP
student
Figure 1: A parse tree for ?What does your student want
to write??
what
does
your want
to write
S
S
S
VP
VP
NP
student write
want
want
want
want
student
Figure 2: A lexicalized parse tree
A lexicalized parse tree is an extension of a parse tree
that is achieved by augmenting each non-terminal with its
lexical head. There is an example of a lexicalized parse
tree in Figure 2, which is a lexicalized version of the one
in Figure 1. A lexicalized parse tree is represented by
a set of branchings in the tree1: T = {?w
h
i
, w
n
i
, r
i
?},
where w
h
i
is a head word, w
n
i
the head word of a
non-head, and r
i
a grammar rule corresponding to each
branching. LPCFG models yield a probability of the
complete parse tree T = {?w
h
i
, w
n
i
, r
i
?} by the prod-
uct of probabilities of branchings in it.
p(T ) =
?
i
p(w
h
i
, w
n
i
, r
i
|?),
where ? is a condition of the probability, which is usually
the nonterminal symbol of the mother node. Since each
branching is augmented with the lexical heads of non-
terminals in the rule, the model can capture lexical de-
pendencies, which increase the accuracy. This is because
lexical dependencies approximately represent the seman-
tic preference of a sentence. As is well known, a syntactic
structure is not accurately disambiguated only with syn-
tactic preferences, and the incorporation of approximate
1For simplicity, we have assumed parse trees are only com-
posed of binary branchings.
semantic preferences was the key to improving the accu-
racy of syntactic disambiguation.
We should note that this model has the following three
disadvantages.
1. The model fails to represent some linguistic depen-
dencies, including long-distance dependencies and
argument/modifier distinctions. Since an existing
study incorporates these relations ad hoc (Collins,
1997), they are apparently crucial in accurate dis-
ambiguation. This is also problematic for providing
a sufficient representation of semantics.
2. The model assumes the statistical independence of
branchings, which is apparently not preserved. For
example, the ambiguity of PP-attachments should be
resolved by considering three words: the modifiee of
the PP, its preposition, and the object of the PP.
3. The preferences of syntax and semantics are com-
bined in the lexical dependencies of two words,
i.e., features for syntactic preference and those for
semantic preference are not distinguished in the
model. Lexicalized grammars formalize the con-
straints of the relations between syntax and seman-
tics, but the model does not assume the existence
of such constraints. The model prevents further im-
provements to the syntax/semantics models; in addi-
tion to the linguistic analysis of the relation between
syntax and semantics.
2.2 Derivation trees
Recent work on the automatic extraction of LTAG (Xia,
1999; Chen and Vijay-Shanker, 2000) and disambigua-
tion models (Chiang, 2000) has been the first on the sta-
tistical model for syntactic disambiguation based on lexi-
calized grammars. However, the models are based on the
lexical dependencies of elementary trees, which is a sim-
ple extension of the LPCFG. That is, the models are still
based on decomposition into primitive lexical dependen-
cies.
Derivation trees, the structural description in LTAG
(Schabes et al, 1988), represent the association of lex-
ical items i.e., elementary trees. In LTAG, all syntactic
constraints of words are described in an elementary tree,
and the dependencies of elementary trees, i.e., a deriva-
tion tree, describe the semantic relations of words more
directly than lexicalized parse trees. For example, Fig-
ure 3 has a derivation tree corresponding to the parse
tree in Figure 12. The dotted lines represent substitu-
tion while the solid lines represent adjunction. We should
note that the relations captured by ad-hoc augmentation
2The nodes in a derivation tree are denoted with the names
of the elementary trees, while we have omitted details.
what does student want to
write
your
Figure 3: A derivation tree
of lexicalized parse trees, such as the distinction of argu-
ments/modifiers and unbounded dependencies (Collins,
1997), are elegantly represented in derivation trees. For-
mally, a derivation tree is represented as a set of depen-
dencies: D = {??
i
, ?
?
j
, r
i
?}, where ?
i
is an elemen-
tary tree, ?
?
i
represents a node in ?
j
where substitu-
tion/adjunction has occurred, and r
i
is a label of the ap-
plied rule, i.e., adjunction or substitution.
A probability of derivation tree D = {??
i
, ?
?
j
, r
i
?} is
generally defined as follows (Schabes et al, 1988; Chi-
ang, 2000).
p(D) =
?
i
p(?
i
|?
?
j
, r
i
)
Note that each probability on the right represents the syn-
tactic/semantic preference of a dependency of two lexical
items. We can readily see that the model is very similar
to LPCFG models.
The first problem with LPCFG is partially solved
by this model, since the dependencies not represented
in LPCFG (e.g., long-distance dependencies and ar-
gument/modifier distinctions) are elegantly represented,
while some relations (e.g., the control relation between
?want? and ?student?) are not yet represented. However,
the other two problems remain unsolved in this model.
In particular, when we apply Feature-Based LTAG (FB-
LTAG), the above probability is no longer consistent be-
cause of the non-local constraints caused by feature uni-
fication (Abney, 1997).
2.3 Dependency structures
A disambiguation model for wide-coverage CCG (Clark
et al, 2002) aims at representing deep linguistic depen-
dencies including long-distance dependencies and con-
trol relations. This model can represent all the syntac-
tic/semantic dependencies of words in a sentence. How-
ever, the statistical model is still a mere extension of
LPCFG, i.e., it is based on decomposition into primitive
lexical dependencies.
In this model, a lexicalized grammar defines the map-
ping from a sentence into dependency structures, which
represent all the necessary dependencies of words in a
sentence, including long-distance dependencies and con-
trol relations. There is an example in Figure 4, which
what doesstudent want to
write
your
ARG1
ARG2
ARG1
MODIFY
MODIFY
Figure 4: A dependency structure
corresponds to the parse tree in Figure 1. Note that this
representation includes a dependency not represented in
the derivation tree (the control relation between ?want?
and ?student?). A dependency structure is formally de-
fined as a set of dependencies: S = {?w
h
i
, w
n
i
, ?
i
?},
where w
h
i
and w
n
i
are a head and argument word of the
dependency, and ?
i
is an argument position of the head
word filled by the argument word.
An existing model assigns a probability value to de-
pendency structure S = {?w
h
i
, w
n
i
, ?
i
?} as follows.
p =
?
i
p(w
n
i
|w
h
i
, ?
i
)
Primitive probability is approximated by the relative fre-
quency of lexical dependencies of two words in a training
corpus.
Since dependency structures include all necessary de-
pendency relations, the first problem with LPCFG is now
completely solved. However, the third problem still re-
mains unsolved. The probability of a complete parse tree
is defined as the product of probabilities of primitive de-
pendencies of two words. In addition, the second prob-
lem is getting worse; the independence assumption is ap-
parently violated in this model, since the possible depen-
dency structures are restricted by the grammar. The prob-
ability model is no longer consistent.
3 Probability Model based on Lexicalized
Grammars
This section introduces our model of syntactic disam-
biguation, which is based on the decomposition of the
parsing model into the syntax and semantics models. The
concept behind it is that the plausibility of a parsing re-
sult is determined by i) the plausibility of syntax, and ii)
selecting the most probable semantics from the structures
allowed by the given syntax. This section formalizes the
general form of statistical models for disambiguation of
parsing including lexicalized parse trees, derivation trees,
and dependency structures. Problems with the existing
models are then discussed, and our model is introduced.
Suppose that a set W of words and a set C of syn-
tactic categories (e.g., nonterminal symbols of CFG, ele-
mentary trees of LTAG, feature structures of HPSG (Sag
and Wasow, 1999)) are given. A lexicalized grammar is
Lexicalized parse tree
?write, what, S? write S?,
?write, does, S? does S?,
?write, student, S? NP VP?,
?student, your, NP? your student?,
?write, want, VP? want VP?,
?write, to, VP? to write?
Derivation tree
?write, what, SUBST?,
?write, does, ADJ?,
?write, student, SUBST?,
?student, your, ADJ?,
?write, want, ADJ?,
?write, to, ADJ?
Dependency structure
?write, what, ARG2?,
?write, does, MODIFY?,
?write, student, ARG1?,
?student, your, MODIFY?,
?write, want, MODIFY?,
?want, student, ARG1?,
?write, to, MODIFY?
Figure 5: Parsing results of lexicalized grammars
then defined as a tuple G = ?L, R?, where L = {l =
?w, c?|w ? W , c ? C} is a lexicon and R is a set of
grammar rules. A parsing result of lexicalized gram-
mars is defined as a labeled graph structure A = {a|a =
?l
h
, l
n
, d?}, where a is an edge representing the depen-
dency of head l
h
and argument l
n
labeled with d. For
example, the lexicalized parse tree in Figure 2 is repre-
sented in this form as in Figure 5, as well as the derivation
tree and the dependency structure.
Given the above definition, the existing models dis-
cussed in Section 2 yield a probability P (A|w) for given
sentence w as in the following general form.
P (A|w) =
?
a?A
p(a|?),
In short, the probability of the complete structure is de-
fined as the product of probabilities of lexical depen-
dencies. For example, p(a|?) corresponds to the prob-
ability of branchings in LPCFG models, that of substi-
tution/adjunction in derivation tree models, and that of
primitive dependencies in dependency structure models.
The models, however, have a crucial weakness with
lexicalized grammar formalism; probability values are
assigned to parsing results not allowed by the grammar,
i.e., the model is no longer consistent. Hence, the disam-
biguation model of lexicalized grammars should not be
decomposed into primitive lexical dependencies.
A possible solution to this problem is to directly es-
timate p(A|w) by applying a maximum entropy model
(Berger et al, 1996). However, such modeling will lead
us to extensive tweaking of features that is theoretically
unjustifiable, and will not contribute to the theoretical
investigation of the relations of syntax and semantics.
Since lexicalized grammars express all syntactic con-
straints by syntactic categories of words, we have as-
sumed that we first determine which syntactic category c
should be chosen, and then determine which argument re-
lations are likely to appear under the constraints imposed
by the syntactic categories. Formally,
p(A|w) = p(c|w)p(A|c).
The first probability in the above formula is the prob-
ability of syntactic categories, i.e., the probability of se-
lecting a sequence of syntactic categories in a sentence.
Since syntactic categories in lexicalized grammars deter-
mine the syntactic constraints of words, this expresses the
syntactic preference of each word in a sentence. Note that
our objective is not only to improve parsing accuracy but
also to investigate the relation between syntax and seman-
tics. We have not adopted the local contexts of words as
in the supertaggers in LTAG (Joshi and Srinivas, 1994)
because they partially include the semantic preferences
of a sentence. The probability is purely unigram to se-
lect the probable syntactic category for each word. The
probability is then given by the product of probabilities
to select a syntactic category for each word from a set of
candidate categories allowed by the lexicon.
p(c|w) =
?
i
p(c
i
|w
i
)
The second describes the probability of semantics,
which expresses the semantic preferences of relating the
words in a sentence. Note that the semantics probabil-
ity is dependent on the syntactic categories determined
by the syntax probability, because in lexicalized grammar
formalism, a series of syntactic categories determines the
possible structures of parsing results. Parsing results are
obtained by solving the constraints given by the grammar.
Hence, we cannot simply decompose semantics probabil-
ity into the dependency probabilities of two words. We
define semantics probability as a discriminative model
that selects the most probable parsing result from a set
of candidates given by parsing.
Since semantics probability cannot be decomposed
into independent sub-events, we applied a maximum en-
tropy model, which allowed probabilistic modeling with-
out the independence assumption. Using this model, we
can assign consistent probabilities to parsing results with
complex structures, such as ones represented with feature
structures (Abney, 1997; Johnson et al, 1999). Given
parsing result A, semantics probability is defined as fol-
lows:
p(A|c) = 1
Z
c
exp
?
?
?
s?S(A)
?(s)
?
?
Z
c
=
?
A
?
?A(c)
exp
?
?
?
s
?
?S(A
?
)
?(s?)
?
? ,
where S(A) is a set of connected subgraphs of A, ?(s)
is a weight of subgraph s, and A(c) is a set of parsing
results allowed by the sequence of syntactic categories c.
Since we aim at separating syntactic and semantic pref-
erences, feature functions for semantic probability distin-
guish only words, not syntactic categories. We should
note that subgraphs should not be limited to an edge, i.e.,
the lexical dependency of two words. By taking more
than one edge as a subgraph, we can represent the depen-
dency of more than two words, although existing mod-
els do not adopt such dependencies. Various ambigui-
ties should be resolved by considering the dependency
of more than two words; e.g. PP-attachment ambiguity
should be resolved by the dependency of three words.
Consequently, the probability model takes the follow-
ing form.
p(A|w) =
{
?
i
p(c
i
|w
i
)
}
?
?
?
1
Z
c
exp
?
?
?
s?S(A)
?(s)
?
?
?
?
?
However, this model has a crucial flaw: the maxi-
mum likelihood estimation of semantics probability is
intractable. This is because the estimation requires Z
c
to be computed, which requires summation over A(c),
exponentially many parsing results. To cope with this
problem, we applied an efficient algorithm of maximum
entropy estimation for feature forests (Miyao and Tsu-
jii, 2002; Geman and Johnson, 2002). This enabled
the tractable estimation of the above probability, when
a set of candidates are represented in a feature forest of a
tractable size.
Here, we should mention that the disadvantages of the
traditional models discussed in Section 2 have been com-
pletely solved by this model. It can be applied to any
parsing results given by a lexicalized grammar, does not
require the independence assumption, and is defined as a
combination of syntax and semantics probabilities, where
the semantics probability is a discriminative model that
selects a parsing result from the set of candidates given
by the syntax probability.
4 Experiments
The model proposed in Section 3 is generally applica-
ble to any lexicalized grammars, and this section reports
the evaluation of our model with a wide-coverage LTAG
grammar, which is automatically acquired from the Penn
Treebank (Marcus et al, 1994) Sections 02?21. The
grammar was acquired by an algorithm similar to (Xia,
1999), and consisted of 2,105 elementary trees, where
1,010 were initial trees and 1,095 were auxiliary ones.
The coverage of the grammar against Section 22 (1,700
sentences) was 92.6% (1,575 sentences) in a weak sense
(i.e., the grammar could output a structure consistent with
the bracketing in the test corpus), and 68.0% (1,156 sen-
tences) in a strong sense (i.e., the grammar could output
exactly the correct derivation).
Since the grammar acquisition algorithm could output
derivation trees for the sentences in the training corpus
(Section 02?21), we used them as a training set of the
probability model. The model of syntax probability was
estimated with syntactic categories appearing in the train-
ing set. For estimating the semantics probability, a parser
produced all possible derivation trees for each sequence
of syntactic categories (corresponding to each sentence)
in the training set, and the obtained derivation trees, i.e.,
A(c), are passed to a maximum entropy estimator. By ap-
plying the grammar acquisition algorithm to Section 22,
we obtained the derivation trees of the sentences in this
section, and from this set we prepared a test set by elim-
inating non-sententials, long sentences (including more
than 40 words), sentences not covered by the grammar,
and sentences that caused time-outs in parsing. The re-
sulting set consisted of 917 derivation trees.
The following three disambiguation models were pre-
pared using the training set.
syntax Only composed of the syntax probability, i.e.,
p(c|w)
traditional Similar to our model, but semantics proba-
bility p(A|c) was decomposed into the probabilities
of the primitive dependencies of two words as in the
traditional modeling, i.e., this model is an inconsis-
tent probability model
our model The model by maximum entropy estimation
for feature forests
The syntax probability was a unigram model, and con-
texts around the word such as previous words/categories
were not used. Hence, it includes only syntactic prefer-
ences of words. The semantics parts of traditional and
our model were maximum entropy models, where ex-
actly the same set of features were used, i.e., the differ-
ence between the two models was only in an event repre-
sentation: derivation trees were decomposed into primi-
tive dependencies in traditional, while in our model they
were represented by a feature forest without decompo-
sition. Hence, we can evaluate the effects of applying
maximum entropy estimation for feature forests by com-
paring our model with traditional. While our model al-
lowed features to be incorporated that were not limited
to the dependencies of two words (Section 3), the models
used throughout the experiments only included features
of the dependencies of two words. The semantics proba-
bilities were developed with two sets of features includ-
exact partial
syntax 73.4 77.3
traditional 79.2 83.4
our model 79.6 83.6
Table 1: Accuracy of dependencies (1)
exact partial
syntax 73.4 77.3
traditional 79.6 83.6
our model 78.9 82.8
Table 2: Accuracy of dependencies (2)
ing surface forms/POSs of words, the labels of dependen-
cies (substitution/adjunction), and the distance between
two words. The first feature set had 283,755 features
and the other had 150,156 features excluding fine-grained
features of the first set. There were 701,819 events for
traditional, and 32,371 for our model. The difference in
the number of events was caused by the difference in the
units of events, i.e., an event corresponded to a depen-
dency in traditional, while it corresponded to a sentence
in our model.
The parameters of the models were estimated by the
limited-memory BFGS algorithm (Nocedal, 1980) with
a Gaussian distribution as the prior probability distri-
bution for smoothing (Chen and Rosenfeld, 1999) im-
plemented in a maximum entropy estimator for feature
forests (Miyao, 2002). The estimation for traditional was
converged in 67 iterations in 127 seconds, and our model
in 29 iterations in 111 seconds on a Pentium III 1.26-GHz
CPU with 4 GB of memory. These results reveal that the
estimation with our model is comparatively efficient with
traditional. The parsing algorithm was CKY-style pars-
ing with beam thresholding, which was similar to ones
used in (Collins, 1996; Clark et al, 2002). Although
we needed to compute normalizing factor Z
c
to obtain
probability values, we used unnormalized products as the
preference score for beam thresholding, following (Clark
et al, 2002). We did not use any preprocessing such as
supertagging (Joshi and Srinivas, 1994) and the parser
searched for the most plausible derivation tree from the
derivation forest in terms of the probability given by the
combination of syntax and semantics probabilities.
Tables 1 and 2 list the accuracy of dependencies, i.e.,
edges in derivation trees, for each model with two sets
of features for the semantics model3. Since in derivation
trees each word in a sentence depends on one and only
one word (see Figure 3), the accuracy is the number of
3Since the features of the syntax part were not changed, the
results for syntax are exactly the same.
correct edges divided by the number of all edges in the
tree. The exact column indicates the ratio of dependen-
cies where the syntactic category, the argument position,
and the dependee head word of the argument word are
correctly output. The partial column shows the ratio of
dependencies where the words are related regardless of
the label. We should note that the exact measure is a very
stringent because the model must select the correct syn-
tactic category from 2,105 categories.
First, we can see that syntax achieved a high level of
accuracy although it was not quite sufficient yet. We
think this was because the grammar could adequately re-
strict the possible structure of parsing results, and the dis-
ambiguation model tried to search for the most probable
structure from the candidates allowed by the grammar.
Second, traditional and our model recorded significantly
higher accuracy than syntax. The accuracy of our model
was almost matched traditional, which proved the valid-
ity of probabilistic modeling with maximum entropy es-
timation for feature forests. The differences between tra-
ditional and our model were insignificant and the results
proved that a consistent probability model of parsing can
be built without the independence assumption, and attains
performance that rivals the traditional models in terms of
parsing accuracy.
We should note that accuracy can further be improved
with our model because it allows other features to be in-
corporated that were not used in these experiments be-
cause the model is not rely on the decomposition into
the dependencies of two words. Another possibility to
increase the accuracy is to refine the LTAG grammar. Al-
though we assumed that all syntactic constraints were
expressed with syntactic categories (Section 3), i.e., el-
ementary trees, the grammar used in the experiments
were not augmented with feature structures and not suffi-
ciently restrictive to eliminate syntactically invalid struc-
tures. Since our model did not include the preferences of
syntactic relations of words, we expect the refinement of
the grammar will greatly improve the accuracy.
5 Conclusion
This paper described a novel model for syntactic dis-
ambiguation based on lexicalized grammars, where the
model selects the most probable parsing result from the
candidates allowed by a lexicalized grammar. Since lex-
icalized grammars can restrict the possible structure of
parsing results, the probabilistic model cannot simply
be decomposed into independent events as in the ex-
isting disambiguation models for parsing. By apply-
ing a maximum entropy model for feature forests, we
achieved probabilistic modeling without decomposition.
Through experiments, we proved the syntax-only model
could record with high level of accuracy with a lexical-
ized grammar, and maximum entropy estimation for fea-
ture forests could attain competitive accuracy compared
to the traditional model. We see this work as the first step
in the application of linguistically motivated grammars to
the parsing of real-world texts as well as the evaluation of
linguistic theories by consulting extensive corpora.
Future work should include the application of our
model to other lexicalized grammars including HPSG.
The development of sophisticated parsing strategies is
also required to improve the accuracy and efficiency of
parsing. Since parsing results of lexicalized grammars
such as HPSG and CCG can include non-local dependen-
cies, we cannot simply apply well-known parsing strate-
gies, such as beam thresholding, which assume the local
computation of preference scores. Further investigations
must be left for future research.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4).
Adam L. Berger, Stephen A. Della Pietra, and Vincent.
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of 14th National Conference on Artificial Intelli-
gence, pages 598?603.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMUCS-99-108, Carnegie Mellon Uni-
versity.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGs from the Penn Treebank. In Proceed-
ings of 6th IWPT.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL 2000, pages 456?463.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of 40th
ACL.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of 34th
ACL, pages 184?191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of 35th
ACL.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of 40th
ACL, pages 279?286.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of 3rd LREC.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of 40th ACL,
pages 335?342.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of 37th
ACL, pages 535?541.
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags): Almost pars-
ing. In Proceedings of 17th COLING, pages 161?165.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings of
HLT 2002.
Yusuke Miyao. 2002. Amis ? a maximum entropy es-
timator for feature forests. Available via http://www-
tsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.
Jorge Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35:773?783.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of 38th ACL.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of 40th ACL.
Ivan A. Sag and ThomasWasow. 1999. Syntactic Theory
? A Formal Introduction. CSLI Lecture Notes no. 92.
CSLI Publications.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized grammars?:
Application to tree adjoining grammars. In Proceed-
ings of 12th COLING, pages 578?583.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of 5th NLPRS.
An Efficient Clustering Algorithm for Class-based Language Models
Takuya Matsuzaki Yusuke Miyao
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
matuzaki,yusuke,tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Abstract
This paper defines a general form for class-
based probabilistic language models and pro-
poses an efficient algorithm for clustering
based on this. Our evaluation experiments re-
vealed that our method decreased computation
time drastically, while retaining accuracy.
1 Introduction
Clustering algorithms have been extensively studied in
the research area of natural language processing because
many researchers have proved that ?classes? obtained by
clustering can improve the performance of various NLP
tasks. Examples have been class-based -gram models
(Brown et al, 1992; Kneser and Ney, 1993), smooth-
ing techniques for structural disambiguation (Li and Abe,
1998) and word sense disambiguation (Shu?tze, 1998).
In this paper, we define a general form for class-based
probabilistic language models, and propose an efficient
and model-theoretic algorithm for clustering based on
this. The algorithm involves three operations, CLAS-
SIFY, MERGE, and SPLIT, all of which decreases the
optimization function based on the MDL principle (Ris-
sanen, 1984), and can efficiently find a point near the lo-
cal optimum. The algorithm is applicable to more general
tasks than existing studies (Li and Abe, 1998; Berkhin
and Becher, 2002), and computational costs are signifi-
cantly small, which allows its application to very large
corpora.
Clustering algorithms may be classified into three
types. The first is a type that uses various heuristic mea-
sure of similarity between the elements to be clustered
and has no interpretation as a probabilitymodel (Widdow,
2002). The resulting clusters from this type of method
are not guaranteed to work effectively as a component
of a statistical language model, because the similarity
used in clustering is not derived from the criterion in the
learning process of the statistical model, e.g. likelihood.
The second type has clear interpretation as a probability
model, but no criteria to determine the number of clusters
(Brown et al, 1992; Kneser and Ney, 1993). The perfor-
mance of methods of this type depend on the number of
clusters that must be specified before the clustering pro-
cess. It may prove rather troublesome to determine the
proper number of clusters in this type of method. The
third has interpretation as a probability model and uses
some statistically motivated model selection criteria to
determine the proper number of clusters. This type has
a clear advantage compared to the second. AutoClass
(Cheeseman and Stutz, 1996), the Bayesian model merg-
ing method (Stolcke and Omohundro, 1996) and Li?s
method (Li, 2002) are examples of this type. AutoClass
and the Bayesian model merging are based on soft clus-
tering models and Li?s method is based on a hard clus-
tering model. In general, computational costs for hard
clustering models are lower than that for soft clustering
models. However, the time complexity of Li?s method is
of cubic order in the size of the vocabulary. Therefore, it
is not practical to apply it to large corpora.
Our model and clustering algorithm provide a solution
to these problems with existing clustering algorithms.
Since the model has clear interpretation as a probability
model, the clustering algorithm uses MDL as clustering
criteria and using a combination of top-down clustering,
bottom-up clustering, and a K-means style exchange al-
gorithm, the method we propose can perform the cluster-
ing efficiently.
We evaluated the algorithm through experiments on
a disambiguation task of Japanese dependency analysis.
In the experiments, we observed that the proposed algo-
rithm?s computation time is roughly linear to the size of
the vocabulary, and it performed slightly better than the
existing method. Our main intention in the experiments
was to see improvements in terms of computational cost,
not in performance in the test task. We will show, in Sec-
tions 2 and 3, that the proposed method can be applied
to a broader range of tasks than the test task we evalu-
ate in the experiments in Section 4. We need further ex-
periments to determine the performance of the proposed
method with more general tasks.
2 Probability model
2.1 Class-based language modeling
Our probability model is a class-based model and it is an
extension of the model proposed by Li and Abe (1998).
We extend their two-dimensional class model to a multi-
dimensional class model, i.e., we incorporate an arbitrary
number of random variables in our model.
Although our probabilitymodel and learning algorithm
are general and not restricted to particular domains, we
mainly intend to use them in natural language process-
ing tasks where large amounts of lexical knowledge are
required. When we incorporate lexical information into
a model, we inevitably face the data-sparseness problem.
The idea of ?word class? (Brown et al, 1992) gives a gen-
eral solution to this problem. A word class is a group
of words which performs similarly in some linguistic
phenomena. Part-of-speech are well-known examples of
such classes. Incorporating word classes into linguistic
models yields good smoothing or, hopefully, meaningful
generalization from given samples.
2.2 Model definition
Let us introduce some notations to define our model. In
our model, we have considered  kinds of discrete ran-
dom variables 

 

     

and their joint distribu-
tion. 

denotes a set of possible values for the -th vari-
able 

. Our probability model assumes disjunctive par-
titions of each 

, which are denoted by 

?s. A disjunc-
tive partition   

 

     

 of  is a subset of


, and satisfies 

 

 	 
   and   



.
We call elements in a partition 

classes of elements in


. 


, or 

for short, denotes a class in 

which
contains an element   

.
With these notations, our probability model is ex-
pressed as:
 

 

 

 

     

 


  


 


     





 

 


 (1)
In this paper, we have considered a hard clusteringmodel,
i.e.,     for any   . Li & Abe?s model
(1998) is an instance of this joint probability model,
where   . Using more than 2 variables the model can
represent the probability for the co-occurrence of triplets,
such as subject, verb, object.
2.3 Clustering criterion
To determine the proper number of classes in each par-
tition 

     

, we need criteria other than the maxi-
mum likelihood criterion, because likelihood always be-
come greater when we use smaller classes. We can see
this class number decision problem as a model selection
problem and apply some statistically motivated model
selection criteria. As mentioned previously (following
Li and Abe (1998)) we used the MDL principle as our
clustering criterion.
Assume that we have  samples of co-occurrence
data:
  

 

 

     

  
        
The objective function in both clustering and parame-
ter estimations in our method is the description length,
, which is defined as follows:
   	

 
  (2)
where  denotes the model and 

 is the likelihood
of samples  under model  :


 
	


 

 

     

 (3)
The first term in Eq.2,  	

, is called the data
description length. The second term, , is called the
model description length, and when sample size  is
large, it can be approximated as
 


	
where  is the number of free parameters in model  .
We used this approximated form throughout this paper.
Given the number of classes, 

 

 for each  
     , we have





  free parameters for joint
probabilities    . Also, for each class , we
have   free parameters for conditional probabilities
 , where   . Thus, we have
 







   





 






 

 





 
Our learning algorithm tries to minimize  by
adjusting the parameters in the model, selecting partition


of each 

, and choosing the numbers of classes, 

in each partition 

.
3 Clustering algorithm
Our clustering algorithm is a combination of three ba-
sic operations: CLASSIFY, SPLIT and MERGE. We it-
eratively invoke these until a terminate condition is met.
Briefly, these three work as follows. The CLASSIFY
takes a partition  in  as input and improves the par-
tition by moving the elements in  from one class to an-
other. This operation is similar to one iteration in the K-
means algorithm. The MERGE takes a partition  as in-
put and successively chooses two classes 

and 

from
 and replaces themwith their union,



. The SPLIT
takes a class, , and tries to find the best division of 
into two new classes, which will decrease the description
length the most.
All of these three basic operations decrease the de-
scription length. Consequently, our overall algorithm
also decreases the description length monotonically and
stops when all three operations cause no decrease in de-
scription length. Strictly, this termination does not guar-
antee the resulting partitions to be even locally opti-
mal, because SPLIT operations do not perform exhaus-
tive searches in all possible divisions of a class. Doing
such an exhaustive search is almost impossible for a class
of modest size, because the time complexity of such an
exhaustive search is of exponential order to the size of the
class. However, by properly selecting the number of tri-
als in SPLIT, we can expect the results to approach some
local optimum.
It is clear that the way the three operations are com-
bined affects the performance of the resulting class-based
model and the computation time required in learning. In
this paper, we basically take a top-down, divisive strat-
egy, but at each stage of division we do CLASSIFY op-
erations on the set of classes at each stage. When we
cannot divide any classes and CLASSIFY cannot move
any elements, we invoke MERGE to merge classes that
are too finely divided. This top-down strategy can drasti-
cally decrease the amount of computation time compared
to the bottom-up approaches used by Brown et al (1992)
and Li and Abe (1998).
The following is the precise algorithm for our main
procedure:
Algorithm 1 MAIN PROCEDURE()
INPUT
 : an integer specifying the number of trials in a
SPLIT operation
OUTPUT
Partitions 

  

and estimated parameters in the
model
PROCEDURE
Step 0 

  

 	 INITIALIZE

 

 
Step 1 Do Step 2 through Step 3 until no change is made
through one iteration
Step 2 For     , do Step 2.1 through Step 2.2
Step 2.1 Do Step 2.1.1 until no change occurs through it
Step 2.1.1 For      , 

	 CLASSIFY


Step 2.2 For each   

,  	 SPLIT 
Step 3 For     , 

	 MERGE


Step 4 Return the resulting partitions with the parame-
ters in the model
In the Step 0 of the algorithm, INITIALIZE creates
the initial partitions of 

     

. It first divides each


     

into two classes and then applies CLASSIFY
to each partition 

     

one by one, while any ele-
ments can move.
The following subsections explain the algorithm for
the three basic operations in detail and show that they
decrease  monotonically.
3.1 Iterative classification
In this subsection, we explain a way of finding a local
optimum in the possible classification of elements in 

,
given the numbers of classes in partitions 

.
Given the number of classes, optimization in terms of
the description length (Eq.2) is just the same as optimiz-
ing the likelihood (Eq.3). We used a greedy algorithm
which monotonically increases the likelihood while
updating classification. Our method is a generalized
version of the previously reported K-means/EM-
algorithm-style, iterative-classification methods in
Kneser and Ney (1993), Berkhin and Becher (2002) and
Dhillon et al (2002). We demonstrate that the method is
applicable to more generic situations than those previ-
ously reported, where the number of random variables is
arbitrary.
To explain the algorithmmore fully, we define ?counter
functions? Training a Naive Bayes Classifier via the EM Algorithm with a Class
Distribution Constraint
Yoshimasa Tsuruoka?? and Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Combining a naive Bayes classifier with the
EM algorithm is one of the promising ap-
proaches for making use of unlabeled data for
disambiguation tasks when using local con-
text features including word sense disambigua-
tion and spelling correction. However, the use
of unlabeled data via the basic EM algorithm
often causes disastrous performance degrada-
tion instead of improving classification perfor-
mance, resulting in poor classification perfor-
mance on average. In this study, we introduce
a class distribution constraint into the iteration
process of the EM algorithm. This constraint
keeps the class distribution of unlabeled data
consistent with the class distribution estimated
from labeled data, preventing the EM algorithm
from converging into an undesirable state. Ex-
perimental results from using 26 confusion sets
and a large amount of unlabeled data show
that our proposed method for using unlabeled
data considerably improves classification per-
formance when the amount of labeled data is
small.
1 Introduction
Many of the tasks in natural language processing can
be addressed as classification problems. State-of-the-
art machine learning techniques including Support Vec-
tor Machines (Vapnik, 1995), AdaBoost (Schapire and
Singer, 2000) and Maximum Entropy Models (Ratna-
parkhi, 1998; Berger et al, 1996) provide high perfor-
mance classifiers if one has abundant correctly labeled
examples.
However, annotating a large set of examples generally
requires a huge amount of human labor and time. This
annotation cost is one of the major obstacles to applying
machine learning techniques to real-world NLP applica-
tions.
Recently, learning algorithms called minimally super-
vised learning or unsupervised learning that can make use
of unlabeled data have received much attention. Since
collecting unlabeled data is generally much easier than
annotating data, such techniques have potential for solv-
ing the problem of annotation cost. Those approaches in-
clude a naive Bayes classifier combined with the EM al-
gorithm (Dempster et al, 1977; Nigam et al, 2000; Ped-
ersen and Bruce, 1998), Co-training (Blum and Mitchell,
1998; Collins and Singer, 1999; Nigam and Ghani, 2000),
and Transductive Support Vector Machines (Joachims,
1999). These algorithms have been applied to some
tasks including text classification and word sense disam-
biguation and their effectiveness has been demonstrated
to some extent.
Combining a naive Bayes classifier with the EM algo-
rithm is one of the promising minimally supervised ap-
proaches because its computational cost is low (linear to
the size of unlabeled data), and it does not require the
features to be split into two independent sets unlike co-
training.
However, the use of unlabeled data via the basic EM
algorithm does not always improve classification perfor-
mance. In fact, this often causes disastrous performance
degradation resulting in poor classification performance
on average. To alleviate this problem, we introduce a
class distribution constraint into the iteration process of
the EM algorithm. This constraint keeps the class dis-
tribution of unlabeled data consistent with the class dis-
tribution estimated from labeled data, preventing the EM
algorithm from converging into an undesirable state.
In order to assess the effectiveness of the proposed
method, we applied it to the problem of semantic disam-
biguation using local context features. Experiments were
conducted with 26 confusion sets and a large number of
unlabeled examples collected from a corpus of one hun-
dred million words.
This paper is organized as follows. Section 2 briefly
reviews the naive Bayes classifier and the EM algorithm
as means of using unlabeled data. Section 3 presents the
idea of using a class distribution constraint and how to
impose this constraint on the learning process. Section
4 describes the problem of confusion set disambiguation
and the features used in the experiments. Experimental
results are presented in Section 5. Related work is dis-
cussed in Section 6. Section 7 offers some concluding
remarks.
2 Naive Bayes Classifier
The naive Bayes classifier is a simple but effective classi-
fier which has been used in numerous applications of in-
formation processing such as image recognition, natural
language processing, information retrieval, etc. (Escud-
ero et al, 2000; Lewis, 1998; Nigam and Ghani, 2000;
Pedersen, 2000).
In this section, we briefly review the naive Bayes clas-
sifier and the EM algorithm that is used for making use
of unlabeled data.
2.1 Naive Bayes Model
Let x be a vector we want to classify, and ck be a possible
class. What we want to know is the probability that the
vector x belongs to the class ck. We first transform the
probability P (ck|x) using Bayes? rule,
P (ck|x) = P (ck)?
P (x|ck)
P (x)
. (1)
Class probability P (ck) can be estimated from training
data. However, direct estimation of P (ck|x) is impossi-
ble in most cases because of the sparseness of training
data.
By assuming the conditional independence of the ele-
ments of a vector, P (x|ck) is decomposed as follows,
P (x|ck) =
d
?
j=1
P (xj |ck), (2)
where xj is the jth element of vector x. Then Equation 1
becomes
P (ck|x) = P (ck)?
?d
j=1 P (xj |ck)
P (x)
. (3)
With this equation, we can calculate P (ck|x) and classify
x into the class with the highest P (ck|x).
Note that the naive Bayes classifier assumes the con-
ditional independence of features. This assumption how-
ever does not hold in most cases. For example, word oc-
currence is a commonly used feature for text classifica-
tion. However, obvious strong dependencies exist among
word occurrences. Despite this apparent violation of the
assumption, the naive Bayes classifier exhibits good per-
formance for various natural language processing tasks.
There are some implementation variants of the naive
Bayes classifier depending on their event models (Mc-
Callum and Nigam, 1998). In this paper, we adopt the
multi-variate Bernoulli event model. Smoothing was
done by replacing zero-probability with a very small con-
stant (1.0? 10?4).
2.2 EM Algorithm
The Expectation Maximization (EM) algorithm (Demp-
ster et al, 1977) is a general framework for estimating
the parameters of a probability model when the data has
missing values. This algorithm can be applied to min-
imally supervised learning, in which the missing values
correspond to missing labels of the examples.
The EM algorithm consists of the E-step in which the
expected values of the missing sufficient statistics given
the observed data and the current parameter estimates are
computed, and the M-step in which the expected values
of the sufficient statistics computed in the E-step are used
to compute complete data maximum likelihood estimates
of the parameters (Dempster et al, 1977).
In our implementation of the EM algorithm with the
naive Bayes classifier, the learning process using unla-
beled data proceeds as follows:
1. Train the classifier using only labeled data.
2. Classify unlabeled examples, assigning probabilistic
labels to them.
3. Update the parameters of the model. Each proba-
bilistically labeled example is counted as its proba-
bility instead of one.
4. Go back to (2) until convergence.
3 Class Distribution Constraint
3.1 Motivation
As described in the previous section, the naive Bayes
classifier can be easily extended to exploit unlabeled data
by using the EM algorithm. However, the use of unla-
beled data for actual tasks exhibits mixed results. The
performance is improved for some cases, but not in all
cases. In our preliminary experiments, using unlabeled
data by means of the EM algorithm often caused signifi-
cant deterioration of classification performance.
To investigate the cause of this, we observed the
change of class distribution of unlabeled data occuring in
the process of the EM algorithm. What we found is that
sometimes the class distribution of unlabeled data greatly
diverges from that of the labeled data. For example, when
the proportion of class A examples in labeled data was
about 0.9, the EM algorithm would sometimes converge
into states where the proportion of class A is about 0.7.
This divergence of class distribution clearly indicated the
EM algorithm converged into an undesirable state.
One of the possible remedies for this phenomenon is
that of forcing class distribution of unlabeled data not to
diverge from the class distribution estimated from labeled
data. In this work, we introduce a class distribution con-
straint (CDC) into the training process of the EM algo-
rithm. This constraint keeps the class distribution of un-
labeled data consistent with that of labeled data.
3.2 Calibrating Probabilistic Labels
We implement class distribution constraints by calibrat-
ing probabilistic labels assigned to unlabeled data in the
process of the EM algorithm. In this work, we consider
only binary classification: classes A and B.
Let pi be the probabilistic label of the ith example
representing the probability that this example belongs to
class A.
Let ? be the proportion of class A examples in the la-
beled data L. If the proportion of the class A examples
(the proportion of the examples whose p i is greater than
0.5) in unlabeled data U is different from ?, we consider
that the values of the probabilistic labels should be cali-
brated.
The basic idea of the calibration is to shift all the prob-
ability values of unlabeled data to the extent that the class
distribution of unlabeled data becomes identical to that of
labeled data. In order for the shifting of the probability
values not to cause the values to go outside of the range
from 0 to 1, we transform the probability values by an
inverse sigmoid function in advance. After the shifting,
the values are returned to probability values by a sigmoid
function.
The whole calibration process is given below:
1. Transform the probabilistic labels p
1
, ...pn by the in-
verse function of the sigmoid function,
f(x) =
1
1 + e?x
. (4)
into real value ranging from ?? to ?. Let the
transformed values be q
1
, ...qn.
2. Sort q
1
, ...qn in descending order. Then, pick up the
value qborder that is located at the position of pro-
portion ? in these n values.
3. Since qborder is located at the border between the
examples of label A and those of label B, the value
should be close to zero (= probability is 0.5). Thus
we calibrate all qi by subtracting qborder.
4. Transform q
1
, ...qn by a sigmoid function back into
probability values.
This calibration process is conducted between the E-
step and the M-step in the EM algorithm.
4 Confusion Set Disambiguation
We applied the naive Bayes classifier with the EM algo-
rithm to confusion set disambiguation. Confusion set dis-
ambiguation is defined as the problem of choosing the
correct word from a set of words that are commonly
confused. For example, quite may easily be mistyped
as quiet. An automatic proofreading system would
need to judge which is the correct use given the con-
text surrounding the target. Example confusion sets in-
clude: {principle, principal}, {then, than}, and {weather,
whether}.
Until now, many methods have been proposed for this
problem including winnow-based algorithms (Golding
and Roth, 1999), differential grammars (Powers, 1998),
transformation based learning (Mangu and Brill, 1997),
decision lists (Yarowsky, 1994).
Confusion set disambiguation has very similar char-
acteristics to a word sense disambiguation problem in
which the system has to identify the meaning of a pol-
ysemous word given the surrounding context. The merit
of using confusion set disambiguation as a test-bed for a
learning algorithm is that since one does not need to an-
notate the examples to make labeled data, one can con-
duct experiments using an arbitrary amount of labeled
data.
4.1 Features
As the input of the classifier, the context of the target must
be represented in the form of a vector. We use a binary
feature vector which contains only the values of 0 or 1 for
each element.
In this work, we use the local context surrounding the
target as the feature of an example. The features of a
target are the two preceding words and the two following
words. For example, if the disambiguation target is quiet
and the system is given the following sentence
?...between busy and quiet periods and it...?
the contexts of this example are represented as follows:
busy
?2
, and
?1
, periods
+1
, and
+2
In the input vector, only the elements corresponding to
these features are set to 1, while all the other elements are
set to 0.
Table 1: Confusion Sets used in the Experiments
Confusion Set Baseline #Unlabeled
I, me 86.4 474726
accept, except 53.2 14876
affect, effect 79.1 20653
among, between 80.1 101621
amount, number 76.1 50310
begin, being 93.0 82448
cite, sight 95.1 3498
country, county 80.8 17810
fewer, less 91.6 35413
its, it?s 83.7 177488
lead, led 53.5 25195
maybe, may be 92.4 36519
passed, past 66.8 24450
peace, piece 57.0 11219
principal, principle 61.7 8670
quiet, quite 88.8 29618
raise, rise 60.8 13392
sight, site 61.1 9618
site, cite 96.0 5594
than, then 63.8 216286
their, there 63.8 372471
there, they?re 96.4 146462
they?re, their 96.9 237443
weather, whether 87.5 29730
your, you?re 88.6 108185
AVERAGE 78.2 90147
5 Experiment
To conduct large scale experiments, we used the British
National Corpus 1 that is currently one of the largest cor-
pora available. The corpus contains roughly one hundred
million words collected from various sources.
The confusion sets used in our experiments are the
same as in Golding?s experiment (1999). Since our al-
gorithm requires the classification to be binary, we de-
composed three-class confusion sets into pairwise binary
classifications. Table 1 shows the resulting confusion sets
used in the following experiments. The baseline perfor-
mances, achieved by simply selecting the majority class,
are shown in the second column. The number of unla-
beled data are shown in the rightmost column.
The 1,000 test sets were randomly selected from the
corpus for each confusion set. They do not overlap the
labeled data or the unlabeled data used in the learning
process.
1Data cited herein has been extracted from the British Na-
tional Corpus Online service, managed by Oxford University
Computing Services on behalf of the BNC Consortium. All
rights in the texts cited are reserved.
Table 2: Results of Confusion Sets Disambiguation with
32 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 87.4 96.3 96.0
accept, except 77.2 89.0 81.1
affect, effect 86.4 91.6 93.6
among, between 80.1 64.4 79.5
amount, number 69.6 61.6 68.8
begin, being 95.1 86.6 95.1
cite, sight 95.1 95.1 95.1
country, county 77.5 70.4 76.0
fewer, less 89.0 77.4 85.4
its, it?s 85.3 92.3 94.2
lead, led 65.3 64.2 63.7
maybe, may be 91.1 77.6 92.9
passed, past 77.9 70.2 82.0
peace, piece 78.4 81.5 82.1
principal, principle 72.8 88.7 79.4
quiet, quite 85.3 75.9 83.5
raise, rise 83.7 86.1 81.0
sight, site 67.7 68.7 67.9
site, cite 96.2 93.3 92.8
than, then 74.7 84.0 85.3
their, there 88.4 91.4 90.2
there, they?re 96.4 96.4 89.1
they?re, their 96.9 96.9 96.9
weather, whether 90.6 92.3 93.7
your, you?re 87.8 81.8 90.3
AVERAGE 83.8 82.9 85.4
The results are shown in Table 2 through Table 5.
These four tables correspond to the cases in which the
number of labeled examples is 32, 64, 128 and 256 as
indicated by the table captions. The first column shows
the confusion sets. The second column shows the clas-
sification performance of the naive Bayes classifier with
which only labeled data was used for training. The third
column shows the performance of the naive Bayes classi-
fier with which unlabeled data was used via the basic EM
algorithm. The rightmost column shows the performance
of the EM algorithm that was extended with our proposed
calibration process.
Notice that the effect of unlabeled data were very dif-
ferent for each confusion set. As shown in Table 2, the
precision was significantly improved for some confusion
sets including {I, me}, {accept, except} and {affect, ef-
fect} . However, disastrous performance deterioration
can be observed, especially that of the basic EM algo-
rithm, in some confusion sets including {among, be-
tween}, {country, county}, and {site, cite}.
On average, precision was degraded by the use of un-
Table 3: Results of Confusion Sets Disambiguation with
64 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 89.4 96.8 95.7
accept, except 82.9 89.3 87.5
affect, effect 89.4 92.4 93.6
among, between 79.9 76.3 80.5
amount, number 71.5 68.7 69.1
begin, being 95.8 92.1 95.7
cite, sight 95.1 95.8 96.4
country, county 78.7 73.4 74.5
fewer, less 87.6 74.3 87.3
its, it?s 85.8 94.0 92.5
lead, led 76.2 66.8 72.8
maybe, may be 92.6 84.0 96.2
passed, past 79.7 72.5 88.4
peace, piece 81.1 81.2 82.4
principal, principle 75.2 90.2 89.8
quiet, quite 86.5 84.0 89.2
raise, rise 85.7 85.6 86.9
sight, site 71.9 69.0 69.0
site, cite 96.3 95.8 95.5
than, then 79.7 83.8 83.2
their, there 90.5 91.9 92.1
there, they?re 96.2 85.2 91.4
they?re, their 96.9 96.9 95.8
weather, whether 90.6 91.4 93.3
your, you?re 88.0 83.3 94.2
AVERAGE 85.7 84.6 87.7
labeled data via the basic EM algorithm (from 83.3% to
82.9%). On the other hand, the EM algorithm with the
class distribution constraint improved average classifica-
tion performance (from 83.3% to 85.4%). This improved
precision nearly reached the performance achieved by
twice the size of labeled data without unlabeled data (see
the average precision of NB in Table 3). This perfor-
mance gain indicates that the use of unlabeled data ef-
fectively doubles the labeled training data.
In Table 3, the tendency of performance improvement
(or degradation) in the use of unlabeled data is almost the
same as in Table 2. The basic EM algorithm degraded the
performance on average, while our method improved av-
erage performance (from 85.7% to 87.7%). This perfor-
mance gain effectively doubled the size of labeled data.
The results with 128 labeled examples are shown in Ta-
ble 4. Although the use of unlabeled examples by means
of our proposed method still improved average perfor-
mance (from 87.6% to 88.6%), the gain is smaller than
that for a smaller amount of labeled data.
With 256 labeled examples (Table 5), the average per-
Table 4: Results of Confusion Sets Disambiguation with
128 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 90.7 96.9 96.4
accept, except 85.7 90.7 89.4
affect, effect 91.9 93.1 93.3
among, between 80.0 76.3 80.1
amount, number 78.2 68.9 69.3
begin, being 94.4 88.1 95.0
cite, sight 96.9 96.9 98.1
country, county 81.3 75.1 75.7
fewer, less 89.9 74.9 89.4
its, it?s 88.6 93.2 95.2
lead, led 80.5 82.5 82.2
maybe, may be 94.5 80.9 94.4
passed, past 81.8 74.1 85.5
peace, piece 84.1 81.3 82.5
principal, principle 79.8 89.8 89.5
quiet, quite 86.5 82.7 90.1
raise, rise 85.2 86.4 87.7
sight, site 75.6 70.3 70.5
site, cite 96.1 95.8 97.0
than, then 81.7 84.2 84.5
their, there 91.8 91.5 91.2
there, they?re 95.9 83.4 91.3
they?re, their 96.9 96.9 96.7
weather, whether 92.0 92.6 95.1
your, you?re 88.9 84.1 94.5
AVERAGE 87.6 85.2 88.6
formance gain was negligible (from 89.2% to 89.3%).
Figure 1 summarizes the average precisions for differ-
ent number of labeled examples. Average peformance
was improved by the use of unlabeled data with our pro-
posed method when the amount of labeled data was small
(from 32 to 256) as shown in Table 2 through Table
5. However, when the number of labeled examples was
large (more than 512), the use of unlabeled data degraded
average performance.
5.1 Effect of the amount of unlabeled data
When the use of unlabeled data improves classification
performance, the question of how much unlabeled data
are needed becomes very important. Although unlabeled
data are generally much more obtainable than labeled
data, acquiring more than several-thousand unlabeled ex-
amples is not always an easy task. As for confusion set
disambiguation, Table 1 indicates that it is sometimes im-
possible to collect tens of thousands examples even in a
very large corpus.
In order to investigate the effect of the amount of un-
Table 5: Results of Confusion Sets Disambiguation with
256 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 93.4 96.6 96.4
accept, except 89.7 90.3 91.2
affect, effect 93.4 93.5 93.9
among, between 79.6 75.1 80.4
amount, number 81.4 68.9 69.2
begin, being 94.6 89.9 96.6
cite, sight 97.6 97.9 98.4
country, county 84.2 76.5 77.5
fewer, less 90.8 83.0 89.2
its, it?s 90.2 93.3 94.5
lead, led 82.9 79.8 82.6
maybe, may be 96.0 87.1 94.7
passed, past 83.5 74.6 86.3
peace, piece 84.6 81.4 85.7
principal, principle 83.4 90.5 90.5
quiet, quite 88.6 86.8 91.2
raise, rise 88.0 87.1 88.4
sight, site 79.2 71.7 73.2
site, cite 97.3 97.6 97.4
than, then 82.3 85.5 85.9
their, there 93.6 92.1 92.0
there, they?re 96.5 83.0 91.1
they?re, their 96.8 90.8 97.3
weather, whether 93.8 91.9 94.7
your, you?re 89.7 83.8 94.6
AVERAGE 89.2 85.9 89.3
labeled data, we conducted experiments by varying the
amount of unlabeled data for some confusion sets that ex-
hibited significant performance gain by using unlabeled
data.
Figure 2 shows the relationship between the classifica-
tion performance and the amount of unlabeled data for
three confusion sets: {I, me}, {principal, principle}, and
{passed, past}. The number of labeled examples in all
cases was 64.
Note that performance continued to improve even
when the number of unlabeled data reached more than
ten thousands. This suggests that we can further improve
the performance for some confusion sets by using a very
large corpus containing more than one hundred million
words.
Figure 2 also indicates that the use of unlabeled data
was not effective when the amount of unlabeled data was
smaller than one thousand. It is often the case with mi-
nor words that the number of occurrences does not reach
one thousand even in a one-hundred-million word corpus.
Thus, constructing a very very large corpus (containing
75
80
85
90
95
100
100 1000
Pr
ec
is
io
n 
(%
)
Number of Labeled Examples
NB
NB+EM
NB+EM+CDC
Figure 1: Relationship between Average Precision and
the Amount of Labeled Data
60
65
70
75
80
85
90
95
100
1000 10000 100000
Pr
ec
is
io
n 
(%
)
Number of Unlabeled Examples
I, me
principal, principle
passed, past
Figure 2: Relationship between Precision and the
Amount of Unlabeled Data
more than billions of words) appears to be beneficial for
infrequent words.
6 Related Work
Nigam et al(2000) reported that the accuracy of text clas-
sification can be improved by a large pool of unlabeled
documents using a naive Bayes classifier and the EM al-
gorithm. They presented two extensions to the basic EM
algorithm. One is a weighting factor to modulate the con-
tribution of the unlabeled data. The other is the use of
multiple mixture components per class. With these exten-
sions, they reported that the use of unlabeled data reduces
classification error by up to 30%.
Pedersen et al(1998) employed the EM algorithm and
Gibbs Sampling for word sense disambiguation by using
a naive Bayes classifier. Although Gibbs Sampling re-
sults in a small improvement over the EM algorithm, the
results for verbs and adjectives did not reach baseline per-
formance on average. The amount of unlabeled data used
in their experiments was relatively small (from several
hundreds to a few thousands).
Yarowsky (1995) presented an approach that signif-
icantly reduces the amount of labeled data needed for
word sense disambiguation. Yarowsky achieved accura-
cies of more than 90% for two-sense polysemous words.
This success was likely due to the use of ?one sense per
discourse? characteristic of polysemous words.
Yarowsky?s approach can be viewed in the context of
co-training (Blum and Mitchell, 1998) in which the fea-
tures can be split into two independent sets. For word
sense disambiguation, the sets correspond to the local
contexts of the target word and the ?one sense per dis-
course? characteristic. Confusion sets however do not
have the latter characteristic.
The effect of a huge amount of unlabeled data for
confusion set disambiguation is discussed in (Banko and
Brill, 2001). Bank and Brill conducted experiments of
committee-based unsupervised learning for two confu-
sion sets. Their results showed that they gained a slight
improvement by using a certain amount of unlabeled
data. However, test set accuracy began to decline as ad-
ditional data were harvested.
As for the performance of confusion set disambigua-
tion, Golding (1999) achieved over 96% by a winnow-
based approach. Although our results are not directly
comparable with their results since the data sets are
different, our results does not reach the state-of-the-
art performance. Because the performance of a naive
Bayes classifier is significantly affected by the smoothing
method used for paramter estimation, there is a chance to
improve our performance by using a more sophisticated
smoothing technique.
7 Conclusion
The naive Bayes classifier can be combined with the well-
established EM algorithm to exploit the unlabeled data
. However, the use of unlabeled data sometimes causes
disastrous degradation of classification performance.
In this paper, we introduce a class distribution con-
straint into the iteration process of the EM algorithm.
This constraint keeps the class distribution of unlabeled
data consistent with the true class distribution estimated
from labeled data, preventing the EM algorithm from
converging into an undesirable state.
Experimental results using 26 confusion sets and a
large amount of unlabeled data showed that combining
the EM algorithm with our proposed constraint consis-
tently reduced the average classification error rates when
the amount of labeled data is small. The results also
showed that use of unlabeled data is especially advan-
tageous when the amount of labeled data is small (up to
about one hundred).
7.1 Future Work
In this paper, we empirically demonstrated that a class
distribution constraint reduced the chance of undesirable
convergence of the EM algorithm. However, the theoret-
ical justification of this constraint should be clarified in
future work.
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Workshop on Computa-
tional Learning Theory, Morgan Kaufmann Publish-
ers.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Royal Statstical Society B 39, pages 1?38.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. 16th International Conf. on Machine Learning,
pages 200?209. Morgan Kaufmann, San Francisco,
CA.
David D. Lewis. 1998. Naive Bayes at forty: The in-
dependence assumption in information retrieval. In
Claire Ne?dellec and Ce?line Rouveirol, editors, Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4?15, Chem-
nitz, DE. Springer Verlag, Heidelberg, DE.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In Proc. 14th Interna-
tional Conference on Machine Learning, pages 187?
194. Morgan Kaufmann.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifica-
tion. In AAAI-98 Workshop on Learning for Text Cat-
egorization.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86?93.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39(2/3):103?134.
Ted Pedersen and Rebecca Bruce. 1998. Knowledge
lean word-sense disambiguation. In AAAI/IAAI, pages
800?805.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 63?69,
Seattle, WA, May.
David M. W. Powers. 1998. Learning and application
of differential grammars. In T. Mark Ellison, editor,
CoNLL97: Computational Natural Language Learn-
ing, pages 88?96. Association for Computational Lin-
guistics, Somerset, New Jersey.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, the University of Pennsylvania.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. New York.
David Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in
spanish and french. In Meeting of the Association for
Computational Linguistics, pages 88?95.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. Proc. of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 189?196.
Evaluation and Extension of Maximum Entropy Models
with Inequality Constraints
Jun?ichi Kazama?
kazama@is.s.u-tokyo.ac.jp
?Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033, Japan
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?CREST, JST
(Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi,
Saitama 332-0012, Japan
Abstract
A maximum entropy (ME) model is usu-
ally estimated so that it conforms to equal-
ity constraints on feature expectations.
However, the equality constraint is inap-
propriate for sparse and therefore unre-
liable features. This study explores an
ME model with box-type inequality con-
straints, where the equality can be vio-
lated to reflect this unreliability. We eval-
uate the inequality ME model using text
categorization datasets. We also propose
an extension of the inequality ME model,
which results in a natural integration with
the Gaussian MAP estimation. Experi-
mental results demonstrate the advantage
of the inequality models and the proposed
extension.
1 Introduction
The maximum entropy model (Berger et al, 1996;
Pietra et al, 1997) has attained great popularity in
the NLP field due to its power, robustness, and suc-
cessful performance in various NLP tasks (Ratna-
parkhi, 1996; Nigam et al, 1999; Borthwick, 1999).
In the ME estimation, an event is decomposed
into features, which indicate the strength of certain
aspects in the event, and the most uniform model
among the models that satisfy:
Ep?[fi] = Ep[fi], (1)
for each feature. Ep?[fi] represents the expectation
of feature fi in the training data (empirical expec-
tation), and Ep[fi] is the expectation with respect
to the model being estimated. A powerful and ro-
bust estimation is possible since the features can be
as specific or general as required and does not need
to be independent of each other, and since the most
uniform model avoids overfitting the training data.
In spite of these advantages, the ME model still
suffers from a lack of data as long as it imposes the
equality constraint (1), since the empirical expecta-
tion calculated from the training data of limited size
is inevitably unreliable. A careful treatment is re-
quired especially in NLP applications since the fea-
tures are usually very sparse. In this study, text cat-
egorization is used as an example of such tasks with
sparse features.
Previous work on NLP proposed several solutions
for this unreliability such as the cut-off, which sim-
ply omits rare features, the MAP estimation with
the Gaussian prior (Chen and Rosenfeld, 2000), the
fuzzy maximum entropy model (Lau, 1994), and fat
constraints (Khudanpur, 1995; Newman, 1977).
Currently, the Gaussian MAP estimation (com-
bined with the cut-off) seems to be the most promis-
ing method from the empirical results. It succeeded
in language modeling (Chen and Rosenfeld, 2000)
and text categorization (Nigam et al, 1999). As
described later, it relaxes constraints like Ep?[fi] ?
Ep[fi] =
?
i
?2
, where ?i is the model?s parameter.
This study follows this line, but explores the fol-
lowing box-type inequality constraints:
Ai ? Ep?[fi] ? Ep[fi] ? ?Bi, Ai, Bi > 0. (2)
Here, the equality can be violated by the widths Ai
and Bi. We refer to the ME model with the above
inequality constraints as the inequality ME model.
This inequality constraint falls into a type of fat con-
straints, ai ? Ep[fi] ? bi, as suggested by (Khudan-
pur, 1995). However, as noted in (Chen and Rosen-
feld, 2000), this type of constraint has not yet been
applied nor evaluated for NLPs.
The inequality ME model differs from the Gaus-
sian MAP estimation in that its solution becomes
sparse (i.e., many parameters become zero) as a re-
sult of optimization with inequality constraints. The
features with a zero parameter can be removed from
the model without changing its prediction behavior.
Therefore, we can consider that the inequality ME
model embeds feature selection in its estimation.
Recently, the sparseness of the solution has been rec-
ognized as an important concept in constructing ro-
bust classifiers such as SVMs (Vapnik, 1995). We
believe that the sparse solution improves the robust-
ness of the ME model as well.
We also extend the inequality ME model so that
the constraint widths can move using slack vari-
ables. If we penalize the slack variables by their 2-
norm, we obtain a natural integration of the inequal-
ity ME model and the Gaussian MAP estimation.
While it incorporates the quadratic stabilization of
the parameters as in the Gaussian MAP estimation,
the sparseness of the solution is preserved.
We evaluate the inequality ME models empiri-
cally, using two text categorization datasets. The
results show that the inequality ME models outper-
form the cut-off and the Gaussian MAP estimation.
Such high accuracies are achieved with a fairly small
number of active features, indicating that the sparse
solution can effectively enhance the performance. In
addition, the 2-norm extended model is shown to be
more robust in several situations.
2 The Maximum Entropy Model
The ME estimation of a conditional model p(y|x)
from the training examples {(xi, yi)} is formulated
as the following optimization problem.1
maximize
p
H(p) =
?
x
p?(x)
?
y
p(y|x) log p(y|x)
subject to Ep?[fi]? Ep[fi] = 0 1 ? i ? F. (3)
1To be precise, we have also the constraints
P
y
p(y|x) ?
1 = 0 x ? X . Note that although we explain using a condi-
tional model throughout the paper, the discussion can be applied
easily to a joint model by considering the condition x is fixed.
The empirical expectations and model expectations
in the equality constraints are defined as follows.
Ep?[fi] =
?
x p?(x)
?
y p?(y|x)fi(x, y), (4)
Ep[fi] =
?
x p?(x)
?
y p(y|x)fi(x, y), (5)
p?(x) = c(x)/L, p?(y|x) = c(x, y)/c(x), (6)
where c(?) indicates the number of times ? occurred
in the training data, and L is the number of training
examples.
By the Lagrange method, p(y|x) is found to have
the following parametric form:
p?(y|x) =
1
Z(x)
exp(
?
i
?ifi(x, y)), (7)
where Z(x) =
?
y exp(
?
i ?ifi(x, y)). The dual
objective function becomes:
L(?) =
?
x p?(x)
?
y p?(y|x)
?
i ?ifi(x, y) (8)
?
?
x p?(x) log
?
y exp(
?
i ?ifi(x, y)).
The ME estimation becomes the maximization of
L(?). And it is equivalent to the maximization of the
log-likelihood: LL(?) = log
?
x,y p?(y|x)
p?(x,y)
.
This optimization can be solved using algo-
rithms such as the GIS algorithm (Darroch and Rat-
cliff, 1972) and the IIS algorithm (Pietra et al,
1997). In addition, gradient-based algorithms can
be applied since the objective function is concave.
Malouf (2002) compares several algorithms for the
ME estimation including GIS, IIS, and the limited-
memory variable metric (LMVM) method, which is
a gradient-based method, and shows that the LMVM
method requires much less time to converge for real
NLP datasets. We also observed that the LMVM
method converges very quickly for the text catego-
rization datasets with an improvement in accuracy.
Therefore, we use the LMVM method (and its vari-
ant for the inequality models) throughout the exper-
iments. Thus, we only show the gradient when men-
tioning the training. The gradient of the objective
function (8) is computed as:
?L(?)
??
i
= Ep?[fi]? Ep[fi]. (9)
3 The Inequality ME Model
The maximum entropy model with the box-type in-
equality constraints (2) can be formulated as the fol-
lowing optimization problem:
maximize
p
?
x
p?(x)
?
y
p(y|x) log p(y|x),
subject to Ep?[fi]? Ep[fi]? Ai ? 0, (10)
Ep[fi]? Ep?[fi]? Bi ? 0. (11)
By using the Lagrange method for optimization
problems with inequality constraints, the following
parametric form is derived.
p?,?(y|x) =
1
Z(x)
exp(
?
i
(?i ? ?i)fi(x, y)),
?i ? 0, ?i ? 0, (12)
where parameters ?i and ?i are the Lagrange mul-
tipliers corresponding to constraints (10) and (11).
The Karush-Kuhn-Tucker conditions state that, at
the optimal point,
?i(Ep?[fi]? Ep[fi]? Ai) = 0,
?i(Ep[fi]? Ep?[fi]? Bi) = 0.
These conditions mean that the equality constraint is
maximally violated when the parameter is non-zero,
and if the violation is strictly within the widths, the
parameter becomes zero. We call a feature upper
active when ?i > 0, and lower active when ?i > 0.
When ?i??i = 0, we call that feature active.2 Inac-
tive features can be removed from the model without
changing its behavior. Since Ai >0 and Bi >0, any
feature should not be upper active and lower active
at the same time.3
The inequality constraints together with the con-
straints
?
y p(y|x)? 1 = 0 define the feasible re-
gion in the original probability space, on which the
entropy varies and can be maximized. The larger
the widths, the more the feasible region is enlarged.
Therefore, it can be implied that the possibility of a
feature becoming inactive (the global maximal point
is strictly within the feasible region with respect
to that feature?s constraints) increases if the corre-
sponding widths become large.
2The term ?active? may be confusing since in the ME re-
search, a feature is called active when f
i
(x, y) > 0 for an
event. However, we follow the terminology in the constrained
optimization.
3This is only achieved with some tolerance in practice.
The solution for the inequality ME model would
become sparse if the optimization determines many
features as inactive with given widths. The relation
between the widths and the sparseness of the solu-
tion is shown in the experiment.
The dual objective function becomes:
L(?, ?) =
?
x p?(x)
?
y p?(y|x)
?
i(?i ? ?i)fi(x, y)
?
?
x p?(x) log
?
y exp(
?
i(?i ? ?i)fi(x, y))
?
?
i ?iAi ?
?
i ?iBi. (13)
Thus, the estimation is formulated as:
maximize
?
i
?0,?
i
?0
L(?, ?).
Unlike the optimization in the standard maximum
entropy estimation, we now have bound constraints
on parameters which state that parameters must be
non-negative. In addition, maximizing L(?, ?) is no
longer equivalent to maximizing the log-likelihood
LL(?, ?). Instead, we maximize:
LL(?, ?) ?
?
i ?iAi ?
?
i ?iBi. (14)
Although we can use many optimization algorithms
to solve this dual problem since the objective func-
tion is still concave, a method that supports bounded
parameters must be used. In this study, we use the
BLMVM algorithm (Benson and More?, ), a variant
of the limited-memory variable metric (LMVM) al-
gorithm, which supports bound constraints.4
The gradient of the objective function is:
?L(?,?)
??
i
= Ep?[fi] ? Ep[fi] ? Ai,
?L(?,?)
??
i
= Ep[fi] ? Ep?[fi] ? Bi. (15)
4 Soft Width Extension
In this section, we present an extension of the in-
equality ME model, which we call soft width. The
soft width allows the widths to move as Ai + ?i
and ?Bi ? ?i using slack variables, but with some
penalties in the objective function. This soft width
extension is analogous to the soft margin extension
of the SVMs, and in fact, the mathematical discus-
sion is similar. If we penalize the slack variables
4Although we consider only the gradient-based method here
as noted earlier, an extension of GIS or IIS to support bounded
parameters would also be possible.
by their 2-norm, we obtain a natural combination of
the inequality ME model and the Gaussian MAP es-
timation. We refer to this extension using 2-norm
penalty as the 2-norm inequality ME model. As the
Gaussian MAP estimation has been shown to be suc-
cessful in several tasks, it should be interesting em-
pirically, as well as theoretically, to incorporate the
Gaussian MAP estimation into the inequality model.
We first review the Gaussian MAP estimation in the
following, and then we describe our extension.
4.1 The Gaussian MAP estimation
In the Gaussian MAP ME estimation (Chen and
Rosenfeld, 2000), the objective function is:
LL(?) ?
?
i(
1
2?2
i
)?2i , (16)
which is derived as a consequence of maximizing
the log-likelihood of the posterior probability, using
a Gaussian distribution centered around zero with
the variance ?2i as a prior on parameters. The gra-
dient becomes:
?L(?)
??
i
= Ep?[fi]? Ep[fi]?
?
i
?2
i
. (17)
At the optimal point, Ep?[fi] ? Ep[fi] ? ?i?2
i
= 0.
Therefore, the Gaussian MAP estimation can also be
considered as relaxing the equality constraints. The
significant difference between the inequality ME
model and the Gaussian MAP estimation is that the
parameters are stabilized quadratically in the Gaus-
sian MAP estimation (16), while they are stabilized
linearly in the inequality ME model (14).
4.2 2-norm penalty extension
Our 2-norm extension to the inequality ME model is
as follows.5
maximize
p,?,?
H(p)? C
1
?
i ?i
2
? C
2
?
i ?
2
i ,
subject to Ep?[fi] ? Ep[fi] ? Ai ? ?i, (18)
Ep[fi] ? Ep?[fi] ? Bi ? ?i, (19)
5It is also possible to impose 1-norm penalties in the objec-
tive function. It yields an optimization problem which is iden-
tical to the inequality ME model except that the parameters are
upper-bounded as 0 ? ?
i
? C
1
and 0 ? ?
i
? C
2
. We will not
investigate this 1-norm extension in this paper and leave it for
future research.
where C
1
and C
2
is the penalty constants. The para-
metric form is identical to the inequality ME model
(12). However, the dual objective function becomes:
LL(?, ?) ?
?
i
(
?iAi +
?2
i
4C
1
)
?
?
i
(
?iBi +
?2
i
4C
2
)
.
Accordingly, the gradient becomes:
?L(?,?)
??
i
= Ep?[fi] ? Ep[fi] ?
(
Ai +
?
i
2C
1
)
,
?L(?,?)
??
i
= Ep[fi]? Ep?[fi]?
(
Bi +
?
i
2C
2
)
. (20)
It can be seen that this model is a natural combina-
tion of the inequality ME model and the Gaussian
MAP estimation. It is important to note that the so-
lution sparseness is preserved in the above model.
5 Calculation of the Constraint Width
The widths, Ai and Bi, in the inequality constraints
are desirably widened according to the unreliability
of the feature (i.e., the unreliability of the calculated
empirical expectation). In this paper, we examine
two methods to determine the widths.
The first is to use a common width for all features
fixed by the following formula.
Ai = Bi = W ?
1
L
, (21)
where W is a constant, width factor, to control the
widths. This method can only capture the global re-
liability of all the features. That is, only the reli-
ability of the training examples as a whole can be
captured. We call this method single.
The second, which we call bayes, is a method that
determines the widths based on the Bayesian frame-
work to differentiate between the features depending
on their reliabilities.
For many NLP applications including text catego-
rization, we use the following type of features.
fj,i(x, y) = hi(x) if y = yj, 0 otherwise. (22)
In this case, if we assume the approximation,
p?(y|x) ? p?(y|hi(x) > 0), the empirical expectation
can be interpreted as follows.6
Ep?[fj,i]=
?
x: h
i
(x)>0
p?(x)p?(y = yj|hi(x)>0)hi(x).
6This is only for estimating the unreliability, and is not used
to calculate the actual empirical expectations in the constraints.
Here, a source of unreliability is p?(y|hi(x)>0). We
consider p?(y|hi(x) > 0) as the parameter ? of the
Bernoulli trials. That is, p(y|hi(x) > 0) = ? and
p(y?|hi(x)>0) = 1 ? ?. Then, we estimate the pos-
terior distribution of ? from the training examples
by Bayesian estimation and utilize the variance of
the distribution. With the uniform distribution as the
prior, k times out of n trials give the posterior distri-
bution: p(?) = Be(1+k, 1+n?k), where Be(?, ?)
is the beta distribution. The variance is calculated as
follows.
V [?] =
(1+k)(1+n?k)
(2+n)2(n+3)
. (23)
Letting k = c(fj,i(x, y)>0) and n = c(hi(x)>0),
we obtain fine-grained variances narrowed accord-
ing to c(hi(x) > 0) instead of a single value, which
just captures the global reliability. Assuming the in-
dependence of training examples, the variance of the
empirical expectation becomes:
V
[
Ep?[fj,i]
]
=
[
?
x: h
i
(x)>0 {p?(x)hi(x)}
2
]
V [?j,i].
Then, we calculate the widths as follows:
Ai = Bi = W ?
?
V
[
Ep?[fj,i]
]
. (24)
6 Experiments
For the evaluation, we use the ?Reuters-21578, Dis-
tribution 1.0? dataset and the ?OHSUMED? dataset.
The Reuters dataset developed by David D. Lewis
is a collection of labeled newswire articles.7 We
adopted ?ModApte? split to split the collection,
and we obtained 7, 048 documents for training, and
2, 991 documents for testing. We used 112 ?TOP-
ICS? that actually occurred in the training set as the
target categories.
The OHSUMED dataset (Hersh et al, 1994) is a
collection of clinical paper abstracts from the MED-
LINE database. Each abstract is manually assigned
MeSH terms. We simplified a MeSH term, like
?A/B/C ? A?, and used the most frequent 100
simplified terms as the target categories. We ex-
tracted 9, 947 abstracts for training, and 9, 948 ab-
stracts for testing from the file ?ohsumed.91.?
A documents is converted to a bag-of-words vec-
tor representation with TFIDF values, after the stop
7Available from http://www.daviddlewis.com/resources/
words are removed and all the words are downcased.
Since the text categorization task requires that mul-
tiple categories are assigned if appropriate, we con-
structed a binary categorizer, pc(y ? {+1,?1}|d),
for each category c. If the probability pc(+1|d) is
greater than 0.5, the category is assigned. To con-
struct a conditional maximum entropy model, we
used the feature function of the form (22), where
hi(d) returns the TFIDF value of the i-th word of
the document vector.
We implemented the estimation algorithms as an
extension of an ME estimation tool, Amis,8 using
the Toolkit for Advanced Optimization (TAO) (Ben-
son et al, 2002), which provides the LMVM and the
BLMVM optimization modules. For the inequal-
ity ME estimation, we added a hook that checks the
KKT conditions after the normal convergence test.9
We compared the following models:
? ME models only with cut-off (cut-off ),
? ME models with cut-off and the Gaussian MAP
estimation (gaussian),
? Inequality ME models (ineq),
? Inequality ME models with 2-norm extension
described in Section 4 (2-norm),10
For the inequality ME models, we compared the two
methods to determine the widths, single and bayes,
as described in Section 5. Although the Gaussian
MAP estimation can use different ?i for each fea-
ture, we used a common variance ? for gaussian.
Thus, gaussian roughly corresponds to single in the
way of dealing with the unreliability of features.
Note that, for inequality models, we started with
all possible features and rely on their ability to re-
move unnecessary features automatically by solu-
tion sparseness. The average maximum number of
features in a categorizer is 63, 150.0 for the Reuters
dataset and 116, 452.0 for the OHSUMED dataset.
8Developed by Yusuke Miyao so as to support various
ME estimations such as the efficient estimation with compli-
cated event structures (Miyao and Tsujii, 2002). Available at
http://www-tsujii.is.s.u-tokyo.ac.jp/
?yusuke/amis
9The tolerance for the normal convergence test (relative im-
provement) and the KKT check is 10?4. We stop the training if
the KKT check has been failed many times and the ratio of the
bad (upper and lower active) features among the active features
is lower than 0.01.
10Here, we fix the penalty constants C
1
= C
2
= 10
16
.
 0.8
 0.805
 0.81
 0.815
 0.82
 0.825
 0.83
 0.835
 0.84
 0.845
 0.85
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
Ac
cu
ra
cy
 (F
-sc
ore
)
Width Factor
A
B
CD
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
(a) Reuters
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
Ac
cu
ra
cy
 (F
-sc
ore
)
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
(b) OHSUMED
Figure 1: Accuracies as a function of the width factor W for the development sets.
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
# 
of
 A
ct
ive
 F
ea
tu
re
s
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(a) Reuters
 0
 20000
 40000
 60000
 80000
 100000
 120000
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
# 
of
 A
ct
ive
 F
ea
tu
re
s
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(b) OHSUMED
Figure 2: The average number of active features as a function of width factor W .
6.1 Results
We first found the best values for the control param-
eters of each model, W , ?, and the cut-off threshold,
by using the development set. We show that the in-
equality models outperform the other methods in the
development set. We then show that these values are
valid for the evaluation set. We used the first half of
the test set as the development set, and the second
half as the evaluation set.
Figure 1 shows the accuracies of the inequality
ME models for various width factors. The accura-
cies are presented by the ?micro averaged? F-score.
The horizontal lines show the highest accuracies of
cut-off and gaussian models found by exhaustive
search. For cut-off, we varied the cut-off thresh-
old and found the best threshold. For gaussian, we
varied ? with each cut-off threshold, and found the
best ? and cut-off combination. We can see that
the inequality models outperform the cut-off method
and the Gaussian MAP estimation with an appro-
priate value for W in both datasets. Although the
OHSUMED dataset seems harder than the Reuters
dataset, the improvement in the OHSUMED dataset
is greater than that in the Reuters dataset. This may
be because the OHSUMED dataset is more sparse
than the Reuters dataset. The 2-norm extension
boosts the accuracies, especially for bayes, at the
moderate W s (i.e., with the moderate numbers of
active features). However, we can not observe the
apparent advantage of the 2-norm extension in terms
of the highest accuracy here.
Figure 2 shows the average number of active fea-
tures of each inequality ME model for various width
factors. We can see that active features increase
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 100  1000  10000
Ac
cu
ra
cy
 (F
-sc
ore
)
# of Active Features
B
D
F
E
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
(a) Reuters
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 1000  10000  100000
Ac
cu
ra
cy
 (F
-sc
ore
)
# of Active Features
B
D
F E
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
(b) OHSUMED
Figure 3: Accuracies as a function of the average number of active features for the development sets. For
gaussian, the accuracy with the best ? found by exhaustive search is shown for each cut-off threshold.
when the widths become small as expected.
Figure 3 shows the accuracy of each model as a
function of the number of active features. We can
see that the inequality ME models achieve the high-
est accuracy with a fairly small number of active fea-
tures, removing unnecessary features on their own.
Besides, they consistently achieve much higher ac-
curacies than the cut-off and the Gaussian MAP es-
timation with a small number of features.
Table 1 summarizes the above results including
the best control parameters for the development set,
and shows how well each method performs for the
evaluation set with these parameters. We can see that
the best parameters are valid for the evaluation sets,
and the inequality ME models outperform the other
methods in the evaluation set as well. This means
that the inequality ME model is generally superior
to the cut-off method and the Gaussian MAP estima-
tion. At this point, the 2-norm extension shows the
advantage of being robust, especially for the Reuters
dataset. That is, the 2-norm models outperform the
normal inequality models in the evaluation set. To
see the reason for this, we show the average cross
entropy of each inequality model as a function of
the width factor in Figure 4. The average cross en-
tropy was calculated as ? 1
C
?
c
1
L
?
i log pc(yi|di),
where C is the number of categories. The cross en-
tropy of the 2-norm model is consistently more sta-
ble than that of the normal inequality model. Al-
though there is no simple relation between the abso-
lute accuracy and the cross entropy, this consistent
difference can be one explanation for the advantage
of the 2-norm extension. Besides, it is possible that
the effect of 2-norm extension appears more clearly
in the Reuters dataset because the robustness is more
important in the Reuters dataset since the develop-
ment set is rather small and easy to overfit.
Lastly, we could not observe the advantage of
bayes method in these experiments. However, since
our method is still in development, it is premature
to conclude that the idea of using different widths
according to its unreliability is not successful. It is
possible that the uncertainty of p?(x), which were not
concerned about, is needed to be modeled, or the
Bernoulli trial assumption is inappropriate. Further
investigation on these points must be done.
7 Conclusion and Future Work
We have shown that the inequality ME models
outperform the cut-off method and the Gaussian
MAP estimation, using the two text categoriza-
tion datasets. Besides, the inequality ME models
achieved high accuracies with a small number of
features due to the sparseness of the solution. How-
ever, it is an open question how the inequality ME
model differs from other sophisticated methods of
feature selection based on other criteria.
Future work will investigate the details of the in-
equality model including the effect of the penalty
constants of the 2-norm extension. Evaluations on
other NLP tasks are also planned. In addition, we
need to analyze the inequality ME model further to
Table 1: The summary of the experiments.
Reuters OHSUMED
best setting # active feats acc (dev) acc (eval) best setting # active feats acc (dev) acc (eval)
cut-off cthr=2 16, 961.9 83.24 86.38 cthr=0 116, 452.0 58.83 58.35
gaussian cthr=3, ?=4.22E3 12, 326.6 84.01 87.04 cthr=8, ?=2.55E3 10, 154.7 59.53 59.08
ineq+single W =1.78E?11 9, 479.9 84.47 87.41 W =4.22E?2 1, 375.5 61.23 61.10
2-norm+single W =5.62E?11 6, 611.1 84.35 87.59 W =4.50E?2 1, 316.5 61.26 61.23
ineq+bayes W =3.16E?15 63, 150.0 84.21 87.37 W =9.46 1, 136.6 60.65 60.31
2-norm+bayes W =3.16E?9 10, 022.3 84.01 87.57 W =9.46 1, 154.5 60.67 60.32
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
Av
g.
 E
nt
ro
py
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(a) Reuters
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
Av
g.
 E
nt
ro
py
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(b) OHSUMED
Figure 4: W vs. the average cross entropy for the development sets.
clarify the reasons for its success.
Acknowledgments We would like to thank
Yusuke Miyao, Yoshimasa Tsuruoka, and the
anonymous reviewers for many helpful comments.
References
S. J. Benson and J. J. More?. A limited memory variable metric
method for bound constraint minimization. Technical Re-
port ANL/MCS-P909-0901, Argonne National Laboratory.
S. Benson, L. C. McInnes, J. J. More?, and J. Sarich. 2002.
TAO users manual. Technical Report ANL/MCS-TM-242-
Revision 1.4, Argonne National Laboratory.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
A. Borthwick. 1999. A maximum entropy approach to named
entity recognition. Ph.D. Thesis. New York University.
S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and Au-
dio Processing, 8(1):37?50.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathematical
Statistics, 43:1470?1480.
W. Hersh, C. Buckley, T.J. Leone, and D. Hickam. 1994.
OHSUMED: An interactive retrieval evaluation and new
large test collection for research. In Proc. of the 17th An-
nual ACM SIGIR Conference, pages 192?201.
S. Khudanpur. 1995. A method of ME estimation with re-
laxed constraints. In Johns Hopkins Univ. Language Model-
ing Workshop, pages 1?17.
R. Lau. 1994. Adaptive statistical language modeling. A Mas-
ter?s Thesis. MIT.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. of the sixth CoNLL.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for
feature forests. In Proc. of HLT 2002.
W. Newman. 1977. Extension to the ME method. In IEEE
Trans. on Information Theory, volume IT-23, pages 89?93.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maxi-
mum entropy for text classification. In IJCAI-99 Workshop
on Machine Learning for Information Filtering, pages 61?
67.
S. Pietra, V. Pietra, and J. Lafferty. 1997. Inducing features of
random fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380?393.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proc. of the EMNLP, pages 133?142.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
Boosting Precision and Recall of Dictionary-Based Protein Name
Recognition
Yoshimasa Tsuruoka?? and Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Dictionary-based protein name recogni-
tion is the first step for practical infor-
mation extraction from biomedical doc-
uments because it provides ID informa-
tion of recognized terms unlike machine
learning based approaches. However, dic-
tionary based approaches have two se-
rious problems: (1) a large number of
false recognitions mainly caused by short
names. (2) low recall due to spelling vari-
ation. In this paper, we tackle the for-
mer problem by using a machine learning
method to filter out false positives. We
also present an approximate string search-
ing method to alleviate the latter prob-
lem. Experimental results using the GE-
NIA corpus show that the filtering using
a naive Bayes classifier greatly improves
precision with slight loss of recall, result-
ing in a much better F-score.
1 Introduction
The rapid increase of machine readable biomedical
texts (e.g. MEDLINE) makes automatic information
extraction from those texts much more attractive.
Especially extracting information of protein-protein
interactions from MEDLINE abstracts is regarded as
one of the most important tasks today (Marcotte et
al., 2001; Thomas et al, 2000; Ono et al, 2001).
To extract information of proteins, one has to first
recognize protein names in a text. This kind of prob-
lem has been studied in the field of natural language
processing as named entity recognition tasks. Ohta
et al (2002) provided the GENIA corpus, an an-
notated corpus of MEDLINE abstracts, which can
be used as a gold-standard for evaluating and train-
ing named entity recognition algorithms. There
are some research efforts using machine learning
techniques to recognize biological entities in texts
(Takeuchi and Collier, 2002; Kim and Tsujii, 2002;
Kazama et al, 2002).
One drawback of these machine learning based
approaches is that they do not provide identification
information of recognized terms. For the purpose
of information extraction of protein-protein interac-
tion, the ID information of recognized proteins, such
as GenBank 1 ID or SwissProt 2 ID, is indispensable
to integrate the extracted information with the data
in other information sources.
Dictionary-based approaches, on the other hand,
intrinsically provide ID information because they
recognize a term by searching the most similar
(or identical) one in the dictionary to the target
term. This advantage currently makes dictionary-
based approaches particularly useful as the first step
for practical information extraction from biomedical
documents (Ono et al, 2001).
However, dictionary-based approaches have two
serious problems. One is a large number of false
positives mainly caused by short names, which sig-
nificantly degrade overall precision. Although this
problem can be avoided by excluding short names
from the dictionary, such a solution makes it impos-
sible to recognize short protein names. We tackle
1GenBank is one of the largest genetic sequence databases.
2The Swiss-Prot is an annotated protein sequence database.
this problem by using a machine learning technique.
Each recognized candidate is checked if it is really
protein name or not by a classifier trained on an an-
notated corpus.
The other problem of dictionary based approaches
is spelling variation. For example, the protein name
?NF-Kappa B? has many spelling variants such as
?NF Kappa B,? ?NF kappa B,? ?NF kappaB,? and
?NFkappaB.? Exact matching techniques, however,
regard these terms as completely different terms.
We alleviate this problem by using an approximate
string matching method in which surface-level sim-
ilarities between terms are considered.
This paper is organized as follows. Section 2
describes the overview of our method. Section 3
presents the approximate string searching algorithm
for candidate recognition. Section 3 describes how
to filter out false recognitions by a machine learning
method. Section 5 presents the experimental results
using the GENIA corpus. Some related work is de-
scribed in Section 6. Finally, Section 7 offers some
concluding remarks.
2 Method Overview
Our protein name recognition method consists of
two phases. In the first phase, we scan the text for
protein name candidates using a dictionary. In the
second phase, we check each candidate whether it is
really protein name or not using a machine learning
method. We call these two phases recognition phase
and filtering phase respectively. The overview of the
method is given below.
? Recognition phase
Protein name candidates are identified using a
protein name dictionary. To alleviate the prob-
lem of spelling variation, we use an approxi-
mate string matching technique.
? Filtering phase
Every protein name candidates is classified into
?accepted? or ?rejected? by a classifier. The
classifier uses the context of the term and the
term itself as the features for the classification.
Only ?accepted? candidates are recognized as
protein names.
In the following sections, we describe the details
of each phase.
21234-
223451
32123R
43212G
43211E
43210
2-RG
134
3451
313R
431G
4311E
4310
-RG
Figure 1: Dynamic Programming Matrix
3 Candidate Recognition
The most straightforward way to exploit a dictio-
nary for candidate recognition is the exact (longest)
match algorithm. For exact match, many fast match-
ing algorithms (e.g. Boyer-Moore algorithm (1977))
have been proposed. However, the existence of
many spelling variations for the same protein name
makes the exact matching less attractive. For exam-
ple, even a short protein name ?EGR-1? has at least
the six following variations:
EGR-1, EGR 1, Egr-1, Egr 1, egr-1, egr 1.
Since longer protein names have a huge number
of possible variations, it is impossible to enrich the
dictionary by expanding each protein name as de-
scribed above.
3.1 Approximate String Searching
To deal with the problem of spelling variation, we
need a kind of ?elastic? matching algorithm, by
which a recognition system scan a text to find a sim-
ilar term to (if any) a protein name in the dictio-
nary. We need a similarity measure to do such a task.
The most popular measure of similarity between
two strings is edit distance, which is the minimum
number of operations on individual characters (e.g.
substitutions, insertions, and deletions) required to
transform one string of symbols into another. For ex-
ample, the edit distance between ?EGR-1? and ?GR-
2? is two, because one substitution (1 for 2) and one
deletion (E) are required.
To calculate the edit distance between two strings,
we can use a dynamic programming technique. Fig-
ure 1 illustrates an example. For clarity of presen-
tation, all costs are assumed to be 1. The matrix
C0..|x|,0..|y| is filled, where Ci,j represents the mini-
mum number of operations needed to match x1..i to
y1..j . This is computed as follows (Navarro, 1998)
Ci,0 = i (1)
C0,j = j (2)
Ci,j = if (xi = yj) then Ci?1,j?1 (3)
else 1 + min(Ci?1,j , Ci,j?1, Ci?1,j?1)
The calculation can be done by either a row-
wise left-to-right traversal or a column-wise top-to-
bottom traversal.
There are many fast algorithms other than the dy-
namic programming for uniform-cost edit distance,
where the weight of each edit operation is constant
within the same type (Navarro, 2001). However,
what we expect is that the distance between ?EGR-
1? and ?EGR 1? will be smaller than that between
?EGR-1? and ?FGR-1?, while the uniform-cost edit
distances of them are equal.
The dynamic programming based method is flex-
ible enough to allow us to define arbitrary costs for
individual operations depending on a letter being op-
erated. For example, we can make the cost of the
substitution between a space and a hyphen much
lower than that of the substitution between ?E? and
?F.? Therefore, we use the dynamic programming
based method for our task.
Table 1 shows the cost function used in our ex-
periments. Both insertion and deletion costs are 100
except for spaces and hyphens. Substitution costs
for similar letters are 10. Substitution costs for the
other different letters are 50.
3.2 String Searching
We have described a method for calculating the
similarity between two strings in the previous sec-
tion. However, what we need is approximate string
searching in which the recognizer scans a text to
find a similar term to (if any) a term in the dictio-
nary. The dynamic programming based method can
be easily extended for approximate string searching.
The method is illustrated in Figure 2. The pro-
tein name to be matched is ?EGR-1? and the text
to be scanned is ?encoded by EGR include.? String
searching can be done by just setting the elements
corresponding separators (e.g. space) in the first row
Table 1: Cost Function
Operation Letter Cost
Insertion space or hyphen 10
Other letters 100
Deletion space or hyphen 10
Other letters 100
Substitution A letter for the same letter 0
A numeral for a numeral 10
space for hyphen 10
hyphen for space 10
A capital letter for the
corresponding small letter 10
A small letter for the
corresponding capital letter 10
Other letters 50
to zero. After filling the whole matrix, one can find
that ?EGR-1? can be matched to this text at the place
of ?EGR 1? with cost 1 by searching for the lowest
value in the bottom row.
To take into account the length of a term, we adopt
a normalized cost, which is calculated by dividing
the cost by the length of the term:
(nomalized cost) = (cost) + ?(length of the term) (4)
where ? is a constant value 3. When the costs of two
terms are the same, the longer one is preferred due
to this constant.
To recognize a protein name in a given text, we
perform the above calculation for every term con-
tained in the dictionary and select the term that has
the lowest normalized cost.
If the normalized cost is lower than the predefined
threshold. The corresponding range in the text is
recognized as a protein name candidate.
3.3 Implementation Issues for String Searching
A naive way for string searching using a dictionary
is to conduct the procedure described in the previ-
ous section one by one for every term in the dictio-
nary. However, since the size of the dictionary is
very large, this naive method takes too much time to
perform a large scale experiment.
3? was set to 0.4 in our experiments.
7654444321123444476544444-
76555432122345555765555551
6
6
6
6
d
7
7
7
7
e
54333321012333376543333R
54322222101222276543222G
54321111210121176543211E
54321010321021076543210
ulcni1RGEybdedocne
65444432112344446544444
655543212234555565555551
6
6
6
6
d e
5433332101233336543333R
5432222210122226543222G
5432111121012116543211E
5432101032102106543210
ulcni1RGEybdedocne
Figure 2: Example of String Searching using Dynamic Programming Matrix
Navarro (2001) have presented a way to reduce
redundant calculations by constructing a trie of the
dictionary. The trie is used as a device to avoid
repeating the computation of the cost against same
prefix of many patterns. Suppose that we have just
calculated the cost of the term ?EGR-1? and next we
have to calculate the cost of the term ?EGR-2,? it is
clear that we do not have to re-calculated the first
four rows in the matrix (see Figure 2). They also
pointed out that it is possible to determine, prior to
reaching the bottom of the matrix, that the current
term cannot produce any relevant match: if all the
values of the current row are larger than the thresh-
old, then a match cannot occur since we can only
increase the cost or at best keep it the same.
4 Filtering Candidates by a Naive Bayes
Classifier
One of the serious problems of dictionary-based
recognition is a large number of false recognitions
mainly caused by short entries in the dictionary. For
example, the dictionary constructed from GenBank
contains an entry ?NK.? However, the word ?NK?
is frequently used as a part of the term ?NK cells.?
In this case, ?NK? is an abbreviation of ?natural
killer? and is not a protein name. Therefore this en-
try makes a large number of false recognitions lead-
ing to low precision performance.
In the filtering phase, we use a classifier trained on
an annotated corpus to suppress such kind of false
recognition. The objective of this phase is to im-
prove precision without the loss of recall.
We conduct binary classification (?accept? or ?re-
ject?) on each candidate. The candidates that are
classified into ?rejected? are filtered out. In other
words, only the candidates that are classified into
?accepted? are recognized as protein names.
In this paper, we use a naive Bayes classifier for
this classification task.
4.1 Naive Bayes classifier
The naive Bayes classifier is a simple but effective
classifier which has been used in numerous applica-
tions of information processing such as image recog-
nition, natural language processing and information
retrieval (Lewis, 1998; Escudero et al, 2000; Peder-
sen, 2000; Nigam and Ghani, 2000).
Here we briefly review the naive Bayes model.
Let ~x be a vector we want to classify, and ck be a
possible class. What we want to know is the prob-
ability that the vector ~x belongs to the class ck. We
first transform the probability P (ck|~x) using Bayes?
rule,
P (ck|~x) = P (ck) ?
P (~x|ck)
P (~x) (5)
Class probability P (ck) can be estimated from train-
ing data. However, direct estimation of P (ck|~x) is
impossible in most cases because of the sparseness
of training data.
By assuming the conditional independence
among the elements of a vector, P (~x|ck) is
decomposed as follows,
P (~x|ck) =
d
?
j=1
P (xj|ck), (6)
where xj is the jth element of vector ~x. Then Equa-
tion 5 becomes
P (ck|~x) = P (ck) ?
?d
j=1 P (xj |ck)
P (~x) (7)
By this equation, we can calculate P (ck|~x) and clas-
sify ~x into the class with the highest P (ck|~x).
There are some implementation variants of the
naive Bayes classifier depending on their event mod-
els (McCallum and Nigam, 1998). In this paper, we
adopt the multi-variate Bernoulli event model.
4.2 Features
As the input of the classifier, the features of the tar-
get must be represented in the form of a vector. We
use a binary feature vector which contains only the
values of 0 or 1 for each element.
In this paper, we use the local context surround-
ing a candidate term and the words contained in the
term as the features. We call the former contextual
features and the latter term features.
The features used in our experiments are given be-
low.
? Contextual Features
W?1 : the preceding word.
W+1 : the following word.
? Term Features
Wbegin : the first word of the term.
Wend : the last word of the term.
Wmiddle : the other words of the term without
positional information (bag-of-words).
Suppose the candidate term is ?putative zinc fin-
ger protein, ? and the sentence is:
... encoding a putative zinc finger protein was
found to derepress beta- galactosidase ...
We obtain the following active features for this
example.
{W?1 a}, {W+1 was}, {Wbegin putative}, {Wend
protein}, {Wmiddle zinc}, {Wmiddle finger}.
4.3 Training
The training of the classifier is done with an anno-
tated corpus. We first scan the corpus for protein
name candidates by dictionary matching. If a recog-
nized candidate is annotated as a protein name, this
candidate and its context are used as a positive (?ac-
cepted?) example for training. Otherwise, it is used
as a negative (?rejected?) example.
5 Experiment
5.1 Corpus and Dictionary
We conducted experiments of protein name recogni-
tion using the GENIA corpus version 3.01 (Ohta et
al., 2002). The GENIA corpus is an annotated cor-
pus, which contains 2000 abstracts extracted from
MEDLINE database. These abstracts are selected
from the search results with MeSH terms Human,
Blood Cells, and Transcription Factors.
The biological entities in the corpus are annotated
according to the GENIA ontology. Although the
corpus has many categories such as protein, DNA,
RNA, cell line and tissue, we used only the protein
category. When a term was recursively annotated,
only the innermost (shortest) annotation was consid-
ered.
The test data was created by randomly selecting
200 abstracts from the corpus. The remaining 1800
abstracts were used as the training data. The protein
name dictionary was constructed from the training
data by gathering all the terms that were annotated
as proteins.
Each recognition was counted as correct if the
both boundaries of the recognized term exactly
matched the boundaries of an annotation in the cor-
pus.
5.2 Improving Precision by Filtering
We first conducted experiments to evaluate how
much precision is improved by the filtering process.
In the recognition phase, the longest matching algo-
rithm was used for candidate recognition.
The results are shown in Table 2. F-measure is de-
fined as the harmonic mean for precision and recall
as follows:
F = 2 ? precision ? recallprecision + recall (8)
Table 2: Precision Improvement by Filtering
Precision Recall F-measure
w/o filtering 48.6 70.7 57.6
with filtering 74.3 65.3 69.5
Table 3: Recall Improvement by Approximate
String Search
Threshold Precision Recall F-measure
1.0 72.6 39.5 51.2
2.0 73.7 63.7 68.3
3.0 74.0 66.5 70.1
4.0 73.9 66.8 70.2
5.0 73.4 67.1 70.1
6.0 73.6 67.1 70.2
7.0 73.5 67.2 70.2
8.0 73.1 67.4 70.2
9.0 72.9 67.8 70.2
10.0 72.6 67.7 70.0
The first row shows the performances achieved
without filtering. In this case, all the candidates
identified in the recognition phase are regarded as
protein names. The second row shows the perfor-
mance achieved with filtering by the naive Bayes
classifier. In this case, only the candidates that are
classified into ?accepted? are regarded as protein
names. Notice that the filtering significantly im-
proved the precision (from 48.6% to 74.3%) with
slight loss of the recall. The F-measure was also
greatly improved (from 57.6% to 69.5%).
5.3 Improving Recall by Approximate String
Search
We also conducted experiments to evaluate how
much we can further improve the recognition per-
formance by using the approximate string search-
ing method described in Section 3. Table 3 shows
the results. The leftmost columns show the thresh-
olds of the normalized costs for approximate string
searching. As the threshold increased, the preci-
sion degraded while the recall improved. The best
F-measure was 70.2%, which is better than that of
exact matching by 0.7% (see Table 2).
Table 4: Performance using Different Feature Set
Feature Set Precision Recall F-measure
Contextual 61.0 62.6 61.8
features
Term 71.3 67.9 69.5
features
All features 73.5 67.2 70.2
5.4 Efficacy of Contextual Features
The advantage of using a machine learning tech-
nique is that we can exploit the context of a candi-
date for deciding whether it is really protein name or
not. In order to evaluate the efficacy of contexts, we
conducted experiments using different feature sets.
The threshold of normalized cost was set to 7.0.
Table 4 shows the results. The first row shows the
performances achieved by using only contextual fea-
tures. The second row shows those achieved by us-
ing only term features. The performances achieved
by using both feature sets are shown in the third row.
The results indicate that candidate terms them-
selves are strong cues for classification. However,
the fact that the best performance was achieved
when both feature sets were used suggests that the
context of a candidate conveys useful information
about the semantic class of the candidate.
6 Related Work
Kazama et al (2002) reported an F-measure of
56.5% on the GENIA corpus (Version 1.1) using
Support Vector Machines. Collier et al (2001)
reported an F-measure of 75.9% evaluated on 100
MEDLINE abstracts using a Hidden Markov Model.
These research efforts are machine learning based
and do not provide ID information of recognized
terms.
Krauthammer et al (2000) proposed a dictionary-
based gene/protein name recognition method. They
used BLAST for approximate string matching by
mapping sequences of text characters into sequences
of nucleotides that can be processed by BLAST.
They achieved a recall of 78.8% and a precision of
71.1% by a partial match criterion, which is less
strict than our exact match criterion.
7 Conclusion
In this paper we propose a two-phase protein name
recognition method. In the first phase, we scan texts
for protein name candidates using a protein name
dictionary and an approximate string searching tech-
nique. In the second phase, we filter the candidates
using a machine learning technique.
Since our method is dictionary-based, it can pro-
vide ID information of recognized terms unlike ma-
chine learning based approaches. False recognition,
which is a common problem of dictionary-based ap-
proaches, is suppressed by a classifier trained on an
annotated corpus.
Experimental results using the GENIA corpus
show that the filtering using a naive Bayes classi-
fier greatly improves precision with slight loss of re-
call. We achieved an F-measure of 70.2% for protein
name recognition on the GENIA corpus.
The future direction of this research involves:
? Use of state-of-the-art classifiers
We have used a naive Bayes classifier in our
experiments because it requires a small com-
putational resource and exhibits good perfor-
mance. There is a chance, however, to improve
performance by using state-of-the-art machine
learning techniques including maximum en-
tropy models and support vector machines.
? Use of other elastic matching algorithms
We have restricted the computation of similar-
ity to edit distance. However, it is not uncom-
mon that the order of the words in a protein
name is altered, for example,
?beta-1 integrin?
?integrin beta-1?
The character-level edit distance cannot capture
this -kind of similarities.
References
Robert S. Boyer and J. Strother Moore. 1977. A fast
string searching algorithm. Communications of the
ACM, 20(10):762?772.
Nigel Collier, Chikashi Nobata, and Junichi Tsujii. 2001.
Automatic acquisition and classification of molecular
biology terminology using a tagged corpus. Journal of
Terminology, 7(2):239?258.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proceed-
ings of the ACL-02 Workshop on Natural Language
Processing in the Biomedical Domain.
Jin Dong Kim and Jun?ichi Tsujii. 2002. Corpus-based
approach to biological entity recognition. In Text Data
Mining SIG (ISMB2002).
Michael Krauthammer, Andrey Rzhetsky, Pavel Moro-
zov, and Carol Friedman. 2000. Using BLAST for
identifying gene and protein names in journal articles.
Gene, 259:245?252.
David D. Lewis. 1998. Naive Bayes at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4?15.
Edward M. Marcotte, Ioannis Xenarios, and David Eisen-
berg. 2001. Mining literature for protein-protein inter-
actions. BIOINFORMATICS, 17(4):359?363.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In AAAI-98 Workshop on Learning for Text
Categorization.
G. Navarro, R. Baeza-Yates, and J.M. Arcoverde. 2001.
Matchsimile: A flexible approximate matching tool for
personal names searching. In Proceedings of the XVI
Brazilian Symposium on Databases (SBBD?2001),
pages 228?242.
Gonzalo Navarro. 1998. Approximate Text Searching.
Ph.D. thesis, Dept. of Computer Science, Univ. of
Chile.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33(1):31?
88.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86?93.
Tomoko Ohta, Yuka Tateishi, Hideki Mima, and Jun?ichi
Tsujii. 2002. Genia corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated extraction
of information on protein-protein interactions from the
biological literature. BIOINFORMATICS, 17(2):155?
161.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 63?69.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning 2002 (CoNLL-2002), pages 119?125.
James Thomas, David Milward, Christos Ouzounis,
Stephen Pulman, and Mark Carroll. 2000. Automatic
extraction of protein interactions from scientific ab-
stracts. In Proceedings of the Pacific Symposium on
Biocomputing (PSB2000), volume 5, pages 502?513.
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 25?31, Detroit, June 2005. c?2005 Association for Computational Linguistics
A Machine Learning Approach to Acronym Generation
Yoshimasa Tsuruoka
 
 
CREST
Japan Science and Technology Agency
Japan
Sophia Ananiadou
School of Computing
Salford University
United Kingdom
tsuruoka@is.s.u-tokyo.ac.jp
S.Ananiadou@salford.ac.uk
tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii  

Department of Computer Science
The University of Tokyo
Japan
Abstract
This paper presents a machine learning
approach to acronym generation. We for-
malize the generation process as a se-
quence labeling problem on the letters in
the definition (expanded form) so that a
variety of Markov modeling approaches
can be applied to this task. To con-
struct the data for training and testing, we
extracted acronym-definition pairs from
MEDLINE abstracts and manually anno-
tated each pair with positional informa-
tion about the letters in the acronym. We
have built an MEMM-based tagger using
this training data set and evaluated the
performance of acronym generation. Ex-
perimental results show that our machine
learning method gives significantly bet-
ter performance than that achieved by the
standard heuristic rule for acronym gen-
eration and enables us to obtain multi-
ple candidate acronyms together with their
likelihoods represented in probability val-
ues.
1 Introduction
Technical terms and named-entities play important
roles in knowledge integration and information re-
trieval in the biomedical domain. However, spelling
variations make it difficult to identify the terms con-
veying the same concept because they are written
in different manners. Acronyms constitute a major
part of spelling variations (Nenadic et al, 2002), so
proper management of acronyms leads to improved
performance of the information systems in this do-
main.
As for the methods for recognizing acronym-
definition pairs from running text, there are many
studies reporting high performance (e.g. over 96%
accuracy and 82% recall) (Yoshida et al, 2000; Ne-
nadic et al, 2002; Schwartz and Hearst, 2003; Za-
hariev, 2003; Adar, 2004). However, another aspect
that we have to consider for efficient acronym man-
agement is to generate acronyms from the given def-
inition (expanded form).
One obvious application of acronym generation
is to expand the keywords in information retrieval.
As reported in (Wren et al, 2005), for example,
you can retrieve only 25% of the documents con-
cerning the concept of ?JNK? by using the key-
word ?c-jun N-terminal kinase?. In more than 33%
of the documents the concept is written with its
acronym ?JNK?. To alleviate this problem, some
research efforts have been devoted to constructing
a database containing a large number of acronym-
definition pairs from running text of biomedical doc-
uments (Adar, 2004).
However, the major problem of this database-
building approach is that building the database offer-
ing complete coverage is nearly impossible because
not all the biomedical documents are publicly avail-
able. Although most of the abstracts of biomedical
papers are publicly available on MEDLINE, there
is still a large number of full-papers which are not
available.
In this paper, we propose an alternative approach
25
to providing acronyms from their definitions so
that we can obtain acronyms without consulting
acronym-definition databases.
One of the simplest way to generate acronyms
from definitions would be to choose the letters at the
beginning of each word and capitalize them. How-
ever, there are a lot of exceptions in the acronyms
appearing in biomedical documents. The followings
are some real examples of the definition-acronym
pairs that cannot be created with the simple heuristic
method.
RNA polymerase (RNAP)
antithrombin (AT)
melanoma cell adhesion molecule (Mel-CAM)
the xenoestrogen 4-tert-octylphenol (t-OP)
In this paper we present a machine learning ap-
proach to automatic generation of acronyms in order
to capture a variety of mechanisms of acronym gen-
eration. We formalize this problem as a sequence
labeling task such as part-of-speech tagging, chunk-
ing and other natural language tagging tasks so that
common Markov modeling approaches can be ap-
plied to this task.
2 Acronym Generation as a Sequence
Labeling Problem
Given the definition (expanded form), the mecha-
nism of acronym generation can be regarded as the
task of selecting the appropriate action on each letter
in the definition.
Figure 1 illustrates an example, where the defini-
tion is ?Duck interferon gamma? and the generated
acronym is ?DuIFN-gamma?. The generation pro-
ceeds as follows:
The acronym generator outputs the first
two letters unchanged and skips the fol-
lowing three letters. Then the generator
capitalizes ?i? and skip the following four
letters...
By assuming that an acronym is made up of alpha-
numeric letters, spaces and hyphens, the actions be-
ing taken by the generator are classified into the fol-
lowing five classes.
  SKIP
The generator skips the letter.
  UPPER
If the target letter is uppercase, the generator
outputs the same letter. If the target letter is
lowercase, the generator coverts the letter into
the corresponding upper letter.
  LOWER
If the target letter is lowercase, the generator
outputs the same letter. If the target letter is
uppercase, the generator coverts the letter into
the corresponding lowercase letter.
  SPACE
The generator convert the letter into a space.
  HYPHEN
The generator convert the letter into a hyphen.
From the probabilistic modeling point of view,
this task is to find the sequence of actions 
that maximizes the following probability given the
observation 	
	  	 


	
 (1)
Observations are the letters in the definition and
various types of features derived from them. We de-
compose the probability in a left-to-right manner.






	



ffCombining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 545?552
Manchester, August 2008
Comparative Parser Performance Analysis across Grammar Frameworks
through Automatic Tree Conversion using Synchronous Grammars
Takuya Matsuzaki 1 Jun?ichi Tsujii 1,2,3
1. Department of Computer Science, University of Tokyo, Japan
2. School of Computer Science, University of Manchester, UK
3. National Center for Text Mining, UK
{matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a methodology for the
comparative performance analysis of the
parsers developed for different grammar
frameworks. For such a comparison, we
need a common representation format of
the parsing results since the representation
of the parsing results depends on the gram-
mar frameworks; hence they are not di-
rectly comparable to each other. We first
convert the parsing result to a shallow CFG
analysis by using an automatic tree con-
verter based on synchronous grammars.
The use of such a shallow representation as
a common format has the advantage of re-
duced noise introduced by the conversion
in comparison with the noise produced by
the conversion to deeper representations.
We compared an HPSG parser with sev-
eral CFG parsers in our experiment and
found that meaningful differences among
the parsers? performance can still be ob-
served by such a shallow representation.
1 Introduction
Recently, there have been advancement made in
the parsing techniques for large-scale lexicalized
grammars (Clark and Curran, 2004; Ninomiya et
al., 2005; Ninomiya et al, 2007), and it have
presumably been accelerated by the development
of the semi-automatic acquisition techniques of
large-scale lexicalized grammars from parsed cor-
pora (Hockenmaier and Steedman, 2007; Miyao
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
et al, 2005). In many of the studies on lexical-
ized grammar parsing, the accuracy of the pars-
ing results is evaluated in terms of the accuracy of
the semantic representations output by the parsers.
Since the formalisms for the semantic representa-
tion are different across the grammar frameworks,
it has been difficult to directly compare the perfor-
mance of the parsers developed for different gram-
mar frameworks.
Several researchers in the field of lexicalized
grammar parsing have recently started to seek a
common representation of parsing results across
different grammar frameworks (Clark and Curran,
2007; Miyao et al, 2007). For example, Clark
and Curran (2007) developed a set of mapping
rules from the output of a Combinatorial Catego-
rial grammar parser to the Grammatical Relations
(GR) (Carroll et al, 1998). They found that the
manual development of such mapping rules is not a
trivial task; their mapping rules covered only 85%
of the GRs in a GR-annotated corpus; i.e., 15% of
the GRs in the corpus could not be covered by the
mapping from the gold-standard CCG analyses of
those sentences.
We propose another method for the cross-
framework performance analysis of the parsers
wherein the output of parsers are first converted
to a CFG tree. Specifically, we use CFG trees of
the style used in the Penn Treebank (PTB) (Mar-
cus et al, 1994), in which the non-terminal labels
are simple phrasal categories (i.e., we do not use
function-tags, empty nodes, and co-indexing). We
hereafter name such CFG trees, ?PTB-CFG trees.?
We use an automatic tree converter based on a
stochastic synchronous grammar in order to make
the PTB-CFG trees from the analyses based on a
lexicalized grammar.
In such a shallow representation, some infor-
545
mation given by the lexicalized parsers is lost.
For instance, long-distance dependency and con-
trol/raising distinction cannot be directly repre-
sented in the PTB-CFG tree. From the viewpoint
of NLP-application developer, the parser evalua-
tion based on such a shallow representation may
be not very informative because performance met-
rics based on the shallow representation, e.g., la-
beled bracketing accuracy, do not serve as a direct
indicator of the usefulness of the parser in their
applications. Nevertheless, we consider the parser
performance analysis based on the shallow repre-
sentation is still very useful from the viewpoint
of parser developers because the accuracy of the
structure of the CFG-trees is, though not an ideal
one, a good indicator of the parsers? structural dis-
ambiguation performance.
In addition, there are at least two advantages in
using the CFG-trees as the common representation
for the evaluation. The first advantage is that the
conversion from the parser?s output to the CFG-
trees can be achieved with much higher accuracy
than to deeper representations like GRs; we ob-
tained a conversion accuracy of around 98% in our
experiments using an HPSG grammar. The accu-
racy of the conversion is critical in the quantita-
tive comparison of parsers that have similar per-
formances because the difference in the parsers?
ability would soon be masked by the errors intro-
duced in the conversion process. The second ad-
vantage is that we can compare the converted out-
put directly against the outputs of the well-studied
CFG-parsers derived from PTB.
In the experiments, we applied the conversion to
an HPSG parser, and compared the results against
several CFG parsers. We found that the parsing
accuracy of the HPSG parser is a few points lower
than state-of-the-art CFG parsers in terms of the
labeled bracketing accuracy. By further investi-
gating the parsing results, we have identified a
portion of the reason for the discrepancy, which
comes from the difference in the architecture of the
parsers.
2 Background
In this section, we first give a brief overview of
the semi-automatic acquisition framework of lex-
icalized grammars. Although our methodology is
also applicable to manually developed grammars,
in this paper, we concentrate on the evaluation of
the parsers developed for lexicalized grammars de-
rived from a CFG treebank. Next, we introduce
a specific instance of the treebank-derived lexical-
ized grammars used in our experiment: the Enju
English HPSG grammar. Using the Enju grammar
as a concrete example, we present the motivations
for our tree conversion method based on a stochas-
tic synchronous grammar. We also provide a sum-
mary of the basic concepts and terminologies of
the stochastic synchronous grammar.
2.1 Semi-automatic Acquisition of
Lexicalized Grammars
A lexicalized grammar generally has two compo-
nents: a small set of grammar rules and a large
set of lexical items. The grammar rules represent
generic grammatical constraints while the lexical
items represent word-specific characteristics. An
analysis of a sentence is created by iteratively com-
bining lexical items assigned to to the words in the
sentence by applying the grammar rules.
Several researchers have suggested to extract the
lexicon; i.e., the set of lexical items, from a tree-
bank such as PTB. Most of the lexicon acquisition
methods proceed as follows:
1. Fix the the grammar rules and the basic de-
sign of the lexical items.
2. Re-analyse the sentences in terms of the tar-
get grammar framework, exploiting the anal-
ysis given in the source treebank. A re-
analysis is generally represented as a deriva-
tion of the sentence; i.e., a history of rule ap-
plications.
3. Find a lexical item for each word in the sen-
tences so that it matches the re-analysis of the
sentence, and extract it.
We used the pairs of the original trees and the re-
analyses of the same sentence as a parallel tree-
bank, from which we extract a synchronous gram-
mar.
2.2 The Enju HPSG Grammar
We used the Enju English HPSG grammar (Miyao
et al, 2005) 1 in the experiments. The design of
the grammar basically follows the definition in the
text by Pollard and Sag (1994). A program called
Mayz is distributed with the grammar, which was
1Version 2.2., publicly available from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju
546
used to make the HPSG treebank (i.e., a set of re-
analyses based on the HPSG grammar) from PTB;
the lexicon was extracted from the HPSG treebank.
We reproduced the HPSG treebank using the pro-
gram.
An analysis of a sentence in the HPSG for-
malism is represented by a phrasal tree, in which
each node is assigned a data structure called
typed feature structure (TFS). The TFS represents
syntactic/semantic structures of the corresponding
phrase. To convert an HPSG analysis to a corre-
sponding PTB-CFG trees, we first map the TFSs to
atomic symbols like PP, NP, NX, etc. (33 symbols
in total). We hereafter name such HPSG trees af-
ter the TFS-to-symbol mapping, ?simplified HPSG
trees.? Similarly to the PTB-CFG trees, the simpli-
fied HPSG trees do not include empty categories,
co-indexing, and function-tags. However, we can-
not attain a PTB-CFG tree by simply mapping
those atomic symbols to the corresponding PTB
non-terminal symbols, because the analyses by the
PTB-CFG and the HPSG yield different tree struc-
tures for the same sentence.
The conversion of the tree structure from HPSG
trees to PTB-CFG trees can be regarded as the
inverse-mapping of the transformation from PTB
trees to HPSG trees implemented in the Mayz pro-
gram. A most notable transformation is the bina-
rization of the PTB trees; all the branches in the
HPSG treebank are unary or binary. The binariza-
tion scheme used in Mayz is similar to the head-
centered binarization, which is often used for the
extraction of ?Markovised? PCFGs from the tree-
bank. Mayz identifies the head daughters by using
a modified version of Collins? head finding rules
(Collins, 1999). It is also notable that the PTB-
to-HPSG transformation by Mayz often makes a
bracketing in the HPSG analyses that crosses with
the original bracketing in the PTB. Such a trans-
formation is used, for instance, to change the at-
tachment level of an article to a noun phrase with
a post-modifier (Figure 1).
The tree transformation by Mayz is achieved
by sequentially applying many tree transformation
rules to an input PTB tree. Although each of the
rules operates on a relatively small region of the
tree, the net result can be a very complex transfor-
mation. It is thus very difficult, if not impossible,
to invert the transformation programmatically.
NP
the NX
cat PP
on NP
the wall
NP
NP
the cat
PP
on NP
the wall
Figure 1: Different attachment level of the arti-
cles: HPSG analysis (left) and PTB-CFG analysis
(right).
 
  



 
   


 

 
	

	


   	

	
 

 

 


   

  









 



 



Figure 2: An example of synchronous CFG
2.3 Stochastic Synchronous Tree-Substitution
Grammar for Tree Conversion
For the purpose of the inverted transformation
of simplified HPSG trees to PTB-CFG trees, we
use a statistical approach based on the stochastic
synchronous grammars. Stochastic synchronous
grammars are a family of probabilistic models that
generate a pair of trees by recursively applying
synchronous productions, starting with a pair of
initial symbols. See e.g., Eisner (2003) for a more
formal definition. Figure 2 shows an example of
synchronous CFG, which generates the pairs of
strings of the form (abmc, cbma). Each non-
terminal symbol on the yields of the synchronous
production is linked to a non-terminal symbol on
the other rule?s yield. In the figure, the links are
represented by subscripts. A linked pair of the non-
terminal symbols is simultaneously expanded by
another synchronous production.
The probability of a derivation D of a tree pair
?S, T ? is defined as the product of the probability
of the pair of initial symbols (i.e., the root nodes of
S and T ), and the probabilities of the synchronous
productions used in the derivation:
P (D) = P
(?
R
1
, R
2
?)
?
?t1
i
,t
2
i
??D
P
(?
t
1
i
, t
2
i
?)
,
where ?R1, R2? is the pair of the symbols of the
root nodes of S and T , and ?t1
i
, t
2
i
? is a syn-
chronous production.
547

Coling 2008: Companion volume ? Posters and Demonstrations, pages 43?46
Manchester, August 2008
Word Sense Disambiguation for All Words using Tree-Structured
Conditional Random Fields
Jun Hatori
?
Yusuke Miyao
?
Jun?ichi Tsujii
???
?
Graduate School of Interdisciplinary Information Studies, University of Tokyo
?
Graduate School of Information Science and Technology, University of Tokyo
?
National Centre for Text Mining / 131 Princess Street, Manchester, M1 7DN, UK
?
School of Computer Science, University of Manchester
{hatori,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We propose a supervised word sense
disambiguation (WSD) method using
tree-structured conditional random fields
(TCRFs). By applying TCRFs to a
sentence described as a dependency tree
structure, we conduct WSD as a labeling
problem on tree structures. To incorpo-
rate dependencies between word senses,
we introduce a set of features on tree
edges, in combination with coarse-grained
tagsets, and show that these contribute
to an improvement in WSD accuracy.
We also show that the tree-structured
model outperforms the linear-chain model.
Experiments on the SENSEVAL-3 data
set show that our TCRF model performs
comparably with state-of-the-art WSD
systems.
1 Introduction
Word sense disambiguation (WSD) is one of the
fundamental underlying problems in computa-
tional linguistics. The task of WSD is to determine
the appropriate sense for each polysemous word
within a given text.
Traditionally, there are two task settings for
WSD: the lexical sample task, in which only one
targeted word is disambiguated given its context,
and the all-words task, in which all content words
within a text are disambiguated. Whilst most of
the WSD research so far has been toward the lex-
ical sample task, the all-words task has received
c
? Jun Hatori, Yusuke Miyao, and Jun?ichi Tsu-
jii, 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
relatively less attention, suffering from a serious
knowledge bottleneck problem. Since it is con-
sidered to be a necessary step toward practical ap-
plications, there is an urgent need to improve the
performance of WSD systems that can handle the
all-words task.
In this paper, we propose a novel approach for
the all-words task based on tree-structured condi-
tional random fields (TCRFs). Our TCRF model
incorporates the inter-word sense dependencies, in
combination with WORDNET hierarchical infor-
mation and a coarse-grained tagset, namely super-
senses, by which we can alleviate the data sparse-
ness problem.
2 Background
2.1 Inter-word sense dependencies
Since the all-words task requires us to disam-
biguate all content words, it seems reasonable to
assume that we could perform better WSD by con-
sidering the sense dependencies among words, and
optimizing word senses over the whole sentence.
Specifically, we base our model on the assumption
that there are strong sense dependencies between a
head word and its dependents in a dependency tree;
therefore, we employ the dependency tree struc-
tures for modeling the sense dependencies.
There have been a few WSD systems that incor-
porate the inter-word sense dependencies (e.g. Mi-
halcea and Faruque (2004)). However, to the ex-
tent of our knowledge, their effectiveness has not
explicitly examined thus far for supervised WSD.
2.2 WORDNET information
Supersense A supersense corresponds to the
lexicographers? file ID in WORDNET, with which
each noun or verb synset is associated. Since
43
they are originally introduced for ease of lexicog-
raphers? work, their classification is fairly gen-
eral, but not too abstract, and is hence expected
to act as good coarse-grained semantic categories.
The numbers of the supersenses are 26 and 15
for nouns and verbs. The effectiveness of the
use of supersenses and other coarse-grained tagsets
for WSD has been recently shown by several re-
searchers (e.g. Kohomban and Lee (2005), Cia-
ramita and Altun (2006), and Mihalcea et al
(2007)).
Sense number A sense number is the number of
a sense of a word in WORDNET. Since senses of a
word are ordered according to frequency, the sense
number can act as a powerful feature for WSD,
which offers a preference for frequent senses, and
especially as a back-off feature, which enables our
model to output the first sense when no other fea-
ture is available for that word.
2.3 Tree-structured CRFs
Conditional Random Fields (CRFs) are graph-
based probabilistic discriminative models pro-
posed by Lafferty et al (2001).
Tree-structured CRFs (TCRFs) are different
from widely used linear-chain CRFs, in that the
probabilistic variables are organized in a tree struc-
ture rather than in a linear sequence. Therefore, we
can consider them more appropriate for modeling
the semantics of sentences, which cannot be repre-
sented by linear structures.
Although TCRFs have not yet been applied to
WSD, they have already been applied to some NLP
tasks, such as semantic annotation (Tang et al,
2006), proving to be useful in modeling the seman-
tic structure of a text.
Formulation In CRFs, the conditional probabil-
ity of a label set y for an observation sequence x
is calculated by
p(y|x) =
1
Z(x)
exp
[
?
e?E,j
?
j
f
j
(e,x,y)
+
?
v?V,k
?
k
g
k
(v,x,y)
]
(1)
where E and V are the sets of edges and vertices,
f
j
and g
k
are the feature vectors for an edge and a
vertex, ?
j
and ?
k
are the weight vectors for them,
and Z(x) is the normalization function. For a de-
tailed description of TCRFs, see Tang et al (2006).
ROOT
destroy
confidenceman
the in
bank
ROOT
destroy
confidenceman
bank
<NMOD>
<ROOT> <ROOT>
<NMOD>
<NMOD> : in<PMOD>
<SBJ> <OBJ>
<SBJ> <OBJ>
Figure 1: An example sentence described as a de-
pendency tree structure.
3 WSD Model using Tree-structured
CRFs
3.1 Overview
Let us consider the following sentence.
(i) The man destroys confidence in banks.
In the beginning, we parse a given sentence by
using a dependency parser. The left-hand side of
Figure 1 shows the dependency tree for Sentence
(i) in the CoNLL-X dependency format.
Next, we convert the outputted tree into a tree of
content words, as illustrated in the right-hand side
of Figure 1, since our WSD task does not focus on
the disambiguation of function words.
Finally, we conduct WSD as a labeling task on
tree structures, by maximizing the probability of
a tree of word senses, given scores for vertex and
edge features.
3.2 Sense Labels
Using the information in WORDNET, we define
four sense labels for a word: a sense s
1
(v), a synset
s
2
(v), a topmost synset s
3
(v), and a supersense
s
4
(v). A topmost synset s
3
(v) is the superordi-
nate synset at the topmost level in the WORDNET
hierarchy, and note that a supersense s
4
(v) is only
available for nouns and verbs. We incorporate all
these labels together into the vertex and edge fea-
tures described in the following sections.
3.3 Vertex features
Most of the vertex features we use are those used
by Lee and Ng (2002). All these features are com-
bined with each of the four sense labels s
n
(v), and
incorporated as g
k
in Equation (1).
? Word form, lemma, and part of speech.
? Word forms, lemmas, and parts of speech of
the head and dependents in a dependency tree.
44
#sentences #words
Development 470 5,178
Brown-1 10,712 100,804
Brown-2 8,956 85,481
SENSEVAL-3 300 2,081
Table 1: Statistics of the corpora.
? Bag-of-words within 60-words window.
? Parts-of-speech of neighboring six words.
? Local n-gram within neighboring six words.
Additionally, we include as a vertex feature the
sense number, introduced in Section 2.2.
3.4 Edge features
For each edge, all possible sense bigrams
(i.e. s
1
(v)-s
1
(v
?
),s
1
(v)-s
2
(v
?
),? ? ? ,s
4
(v)-s
4
(v
?
)),
and the combination of sense bigrams with de-
pendency relation labels (e.g. ?SUB,? ?NMOD?)
and/or removed function words in between (e.g.
?of,? ?in?) are defined as edge features, which cor-
respond to f
j
in Equation (1).
4 Experiment
4.1 Experimental settings
In the experiment, we use as our main evalua-
tion data set the Brown-1 and Brown-2 sections of
SEMCOR. The last files in the five largest cate-
gories in Brown-1 are used for development, and
the rest of Brown-1 and all files in Brown-2 are al-
ternately used for training and testing. We also use
the SENSEVAL-3 English all-words data (Snyder
and Palmer, 2004) for testing, in order to compare
the performance of our model with other systems.
The statistics of the data sets are shown in Table 1.
All sentences are parsed by the Sagae?s depen-
dency parser (Sagae and Tsujii, 2007), and the
TCRF model is trained using Amis (Miyao and
Tsujii, 2002). During the development phase, we
tune the parameter of L
2
regularization for CRFs.
Note that, in all experiments, we try all content
words annotated with WORDNET synsets; there-
fore, the recalls are always equal to the precisions.
4.2 Results
First, we trained and evaluated our models on
SEMCOR. Table 2 shows the overall performance
of our models. BASELINE model is the first sense
baseline. NO-EDGE model uses only the ver-
tex features, while each of the Sn-EDGE models
makes use of the edge features associated with
System Recall
PNNL (Tratz et al, 2007) 67.0%
Simil-Prime (Kohomban and Lee, 2005) 66.1%
ALL-EDGE 65.5%
GAMBL (Decadt et al, 2004) 65.2%
SENSELEARNER (Mihalcea et al,2004) 64.6%
BASELINE 62.2%
Table 3: The comparison of the performance of
WSD systems evaluated on the SENSEVAL-3 En-
glish all-words test set.
a sense label s
n
, where n ? {1, 2, 3, 4}. The
ALL-EDGE model incorporates all possible com-
binations of sense labels. The only difference
in the ALL-EDGE? model is that it omits fea-
tures associated with dependency relation labels,
so that we can compare the performance with the
ALL-EDGE?(Linear) model, which is based on the
linear-chain model.
In the experiment, all models with one or more
edge features outperformed both the NO-EDGE
and BASELINE model. The ALL-EDGE model
achieved 75.78% and 77.49% recalls for the two
data sets, with 0.41% and 0.43% improvements
over the NO-EDGE model. By the stratified shuf-
fling test (Cohen, 1995), these differences are
shown to be statistically significant
1
, with the
exception of S3-EDGE model. Also, the tree-
structured model ALL-EDGE? is shown to outper-
form the linear-chain model ALL-EDGE?(Linear)
by 0.13% for both data sets (p = 0.013, 0.006).
Finally, we trained our models on the Brown-1
and Brown-2 sections, and evaluated them on the
SENSEVAL-3 English all-words task data. Table 3
shows the comparison of our model with the state-
of-the-art WSD systems. Considering the differ-
ence in the amount of training data, we can con-
clude that the performance of our TCRF model
is comparable to state-of-the-art WSD systems,
for all systems in Table 3 other than Simil-Prime
(Kohomban and Lee, 2005)
2
utilizes other sense-
annotated data, such as the SENSEVAL data sets
and example sentences in WORDNET.
1
Although some of the improvements seem marginal, they
are still statistically significant. This is probably because
sense bigram features are rarely active, given the size of the
training corpus, and most of the system outputs are the first
senses. Indeed, 91.3% of the outputs of ALL-EDGE model
are the first senses, for example.
2
Kohomban and Lee (2005) used almost the same train-
ing data as our system, but they utilize the instance weighting
technique and the combination of several classifiers, which
our system does not.
45
Training set Brown-1 Brown-2
Testing set Brown-2 Brown-1
Model Recall Offset #correct Recall Offset #correct
ALL-EDGE? 75.77% 0.40%  64766/85481 77.45% 0.39%  78077/100804
ALL-EDGE? (Linear) 75.64% 0.27%  64662/85481 77.32% 0.26%  77944/100804
ALL-EDGE 75.78% 0.41%  64779/85481 77.49% 0.43%  78114/100804
S4-EDGE 75.46% 0.09%  64507/85481 77.15% 0.09%  77769/100804
S3-EDGE 75.40% 0.03% ? 64452/85481 77.13% 0.07%  77750/100804
S2-EDGE 75.45% 0.08%  64494/85481 77.12% 0.06%  77738/100804
S1-EDGE 75.44% 0.07%  64491/85481 77.10% 0.04% > 77724/100804
NO-EDGE 75.37% 0.00% 64427/85481 77.06% 0.00% 77677/100804
BASELINE 74.36% 63567/85481 75.91% 76524/100804
Table 2: The performance of our system trained and evaluated on SEMCOR. The statistical significance
of the improvement over NO-EDGE model is shown in the ?Offset? fields, where ?,? ?>,? and ??? denote
p < 0.01, p < 0.05, and p ? 0.05, respectively.
5 Conclusion
In this paper, we proposed a novel approach for the
all-words WSD based on TCRFs. Our proposals
are twofold: one is to apply tree-structured CRFs
to dependency trees, and the other is to use bigrams
of fine- and coarse-grained senses as edge features.
In our experiment, the sense dependency fea-
tures are shown to improve the WSD accuracy.
Since the combination with coarse-grained tagsets
are also proved to be effective, they can be used to
alleviate the data sparseness problem. Moreover,
we explicitly proved that the tree-structured model
outperforms the linear-chain model, indicating that
dependency trees are more appropriate for repre-
senting semantic dependencies.
Although our model is based on a simple frame-
work, its performance is comparable to state-of-
the-art WSD systems. Since we can use addition-
ally other sense-annotated resources and sophisti-
cated machine learning techniques, our model still
has a great potential for improvement.
References
Ciaramita, M. and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proc. of the
Conf. on Empirical Methods in Natural Language
Processing (EMNLP).
Cohen, P. R. 1995. Empirical methods for artificial
intelligence. MIT Press.
Decadt, B., V. Hoste, W. Daelemans, and A. V. den
Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In Senseval-3: Third
Int?l Workshop on the Evaluation of Systems for the
Semantic Analysis of Text.
Kohomban, U. S. and W. S. Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
Proc. of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL).
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of 18th
Int?l Conf. on Machine Learning (ICML).
Lee, Y. K. and H. T. Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms
for word sense disambiguation. In Proc. of the Conf.
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Mihalcea, R. and E. Faruque. 2004. SenseLearner:
Minimally supervised word sense disambiguation
for all words in open text. In Proc. of ACL/SIGLEX
Senseval-3, Barcelona, Spain, July.
Mihalcea, R., A. Csomai, and M. Ciaramita. 2007.
UNT-Yahoo: SuperSenseLearner: Combining
SenseLearner with SuperSense and other coarse se-
mantic features. In Proc. of the 4th Int?l Workshop
on the Semantic Evaluations (SemEval-2007).
Miyao, Y. and J. Tsujii. 2002. Maximum entropy esti-
mation for feature forests. In Proc. of Human Lan-
guage Technology Conf. (HLT 2002).
Sagae, K. and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007.
Snyder, B. and M. Palmer. 2004. The english all-words
task. In Senseval-3: Third Int?l Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text.
Tang, J., M. Hong, J. Li, and B. Liang. 2006. Tree-
structured conditional random fields for semantic an-
notation. In Proc. of the 5th Int?l Semantic Web Conf.
Tratz, S., A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A super-
vised maximum entropy approach to word sense dis-
ambiguation. In Proc. of the 4th Int?l Workshop on
Semantic Evaluations (SemEval-2007).
46
Coling 2008: Companion volume ? Posters and Demonstrations, pages 63?66
Manchester, August 2008
Exact Inference for Multi-label Classification using Sparse Graphical
Models
Yusuke Miyao? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes a parameter estima-
tion method for multi-label classification
that does not rely on approximate infer-
ence. It is known that multi-label clas-
sification involving label correlation fea-
tures is intractable, because the graphi-
cal model for this problem is a complete
graph. Our solution is to exploit the spar-
sity of features, and express a model struc-
ture for each object by using a sparse
graph. We can thereby apply the junc-
tion tree algorithm, allowing for efficient
exact inference on sparse graphs. Exper-
iments on three data sets for text catego-
rization demonstrated that our method in-
creases the accuracy for text categorization
with a reasonable cost.
1 Introduction
This paper describes an exact inference method
for multi-label classification (Schapire and Singer,
2000; Ghamrawi and McCallum, 2005), into
which label correlation features are incorporated.
In general, directly solving this problem is compu-
tationally intractable, because the graphical model
for this problem is a complete graph. Neverthe-
less, an important characteristic of this problem,
in particular for text categorization, is that only a
limited number of features are active; i.e., non-
zero, for a given object x. This sparsity of fea-
tures is a desirable characteristic, because we can
remove the edges of the graphical model when no
corresponding features are active. We can there-
fore expect that a graphical model for each object
is a sparse graph. When a graph is sparse, we
can apply the junction tree algorithm (Cowell et
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
al., 1999), allowing for efficient exact inference on
sparse graphs.
Our method is evaluated on three data sets for
text categorization; one is from clinical texts, and
the others are from newswire articles. We ob-
serve the trade-off between accuracy and training
cost, while changing the number of label correla-
tion features to be included.
2 Multi-label Classification
Given a set of labels, L = {l
1
, . . . , l
|L|
}, multi-
label classification is the task of assigning a sub-
set y ? L to a document x. In the framework of
statistical machine learning, this problem can be
formulated as a problem of maximizing a scoring
function ?:
y? = argmax
y
?(x, y) = argmax
y
?(f(x, y)). (1)
As is usually the case in statistical machine
learning, we represent a probabilistic event,
?x, y?, with a feature vector, f(x, y) =
?f
1
(x, y), . . . , f
|f |
(x, y)?. In text categorization,
most effective features represent a frequency of a
word w in a document; i.e.,
f
l,w
(x, y) =
{
c
x
(w) if l ? y,
0 otherwise,
where c
x
(w) is a frequency of w in x.
The most popular method for multi-label classi-
fication is to create |L| binary classifiers, each of
which determines whether or not to assign a single
label (Yang and Pedersen, 1997). However, since
the decision for each label is independent of the de-
cision for other labels, this method cannot be sen-
sitive to label correlations, or the tendency of label
cooccurrences.
A recent research effort has been devoted to
the modeling of label correlations. While a num-
ber of approaches have been proposed for deal-
ing with label correlations (see Tsoumakas and
63
Katakis (2007) for the comprehensive survey), the
intuitively-appealing method is to incorporate fea-
tures on two labels into the model (Ghamrawi and
McCallum, 2005). The following label correlation
feature indicates a cooccurrence of two labels and
a word:
f
l,l
?
,w
(x, y) =
{
c
x
(w) if l, l? ? y,
0 otherwise.
3 A Method for Exact Inference
A critical difficulty encountered in the model with
label correlation features is the computational cost
for training and decoding. When features on every
pair of labels are included in the model, its graph-
ical model becomes a complete graph, which in-
dicates that the exact inference for this model is
NP-hard. However, not all edges are necessary
in actual inference, because of the sparsity of fea-
tures. That is, we can remove edges between l and
l
? when no corresponding features are active; i.e.,
f
l,l
?
,w
(x, y) = 0 for all w. In text categorization,
when feature selection is performed, many edges
can be removed because of this characteristic.
Therefore, our idea is to enjoy this sparsity of
features. We construct a graphical model for each
document, and put edges only when one or more
features are active on the corresponding label pair.
When a graph is sparse, we can apply a method
for exact inference, such as the junction tree al-
gorithm (Cowell et al, 1999). The junction tree
algorithm is a generic algorithm for exact infer-
ence on any graphical model, and it allows for ef-
ficient inference on sparse graphs. The method
converts a graph into a junction tree, which is a
tree of cliques in the original graph. When we
have a junction tree for each document, we can
efficiently perform belief propagation in order to
compute argmax in Equation (1), or the marginal
probabilities of cliques and labels, necessary for
the parameter estimation of machine learning clas-
sifiers, including perceptrons (Collins, 2002), and
maximum entropy models (Berger et al, 1996).
The computational complexity of the inference on
junction trees is proportional to the exponential of
the tree width, which is the maximum number of
labels in a clique, minus one.
An essential idea of this method is that a graph-
ical model is constructed for each document. Even
when features are defined on all pairs of labels,
active features for a specific document are lim-
ited. When combined with feature selection, this
# train # test # labels card.
cmc2007 978 976 45 1.23
reuters10 6,490 2,545 10 1.10
reuters90 7,770 3,019 90 1.24
Table 1: Statistics of evaluation data sets
? ? c
cmc2007 1,000 10 0
reuters10 5,000 20 5
reuters90 5,000 80 5
Table 2: Parameters for evaluation data sets
method greatly increases the sparsity of the result-
ing graphs, which is key to efficiency.
A weakness of this method comes from the as-
sumption of feature sparseness. We are forced to
apply feature selection, which is considered effec-
tive in text categorization, but not necessarily for
other tasks. The design of features is also restricted
in order to ensure the sparsity of features.
4 Experiments
4.1 Experimental Settings
We evaluate our method for multi-label classifica-
tion using three data sets for text categorization.
Table 1 shows the statistics of these data. In this
table, ?card.? denotes the average number of la-
bels assigned to a document.
cmc2007 is a data set used in the Computa-
tional Medicine Center (CMC) Challenge 2007
(Pestian et al, 2007)1. This challenge aimed at
the assignment of ICD-9-CM codes, such as cough
and pneumonia, to clinical free texts. It should be
noted that this data is controlled, so that both train-
ing and test sets include the exact same label com-
binations, and the number of combinations is 90.
This indicates that this task can be solved as a clas-
sification of 90 classes. However, since this is an
unrealistic situation for actual applications, we do
not rely on this characteristic in this work.
reuters10 and reuters90 are taken from
the Reuters-21578 collection,2 which is a popu-
lar benchmark for text categorization. This text
collection consists of newswire articles, and each
document is assigned topic categories, such as
grain and ship. We split the data into training and
test sets, according to the so-called ModApte split.
1Available at http://www.computationalmedicine.org
2Available at http://www.daviddlewis.com/resources/
testcollections/reuters21578/
64
cmc2007
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 82.79 69.88 83.09 69.06
100 83.49 70.70 83.68 70.39
200 82.95 69.67 83.67 70.18
400 83.03 69.98 83.49 70.49
800 83.51 71.41 83.58 70.70
1600 83.10 70.49 83.56 71.00
3200 80.74 66.70 82.02 69.57
reuters10
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 94.23 89.71 93.71 88.76
500 94.22 89.98 93.80 89.19
1000 94.43 90.37 94.07 89.55
2000 94.46 90.61 94.04 89.94
4000 94.12 90.26 94.12 89.98
8000 94.14 90.61 94.50 90.81
16000 93.92 90.29 94.30 90.88
reuters90
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 84.07 77.91 86.83 79.50
500 84.96 78.27 86.89 79.66
1000 85.38 78.70 86.94 79.99
2000 85.73 79.79 86.55 79.93
4000 85.72 79.73 86.54 80.23
8000 85.90 80.19 86.77 80.39
16000 86.17 80.52 ? ?
Table 3: Accuracy for cmc2007, reuters10,
and reuters90
From this data, we create two data sets. The first
set, reuters10, is a subset of the ModApte split,
to which the 10 largest categories are assigned.
The other, reuters90, consists of documents
that are labeled by 90 categories, having at least
one document in each of the training and test sets.
In the following experiments, we run two ma-
chine learning classifiers: Bayes Point Machines
(BPM) (Herbrich et al, 2001), and the maximum
entropy model (ME) (Berger et al, 1996). For
BPM, we run 100 averaged perceptrons (Collins,
2002) with 10 iterations for each. For ME, the
orthant-wise quasi-Newton method (Andrew and
Gao, 2007) is applied, with the hyper parameter
for l
1
regularization fixed to 1.0.
We use word unigram features that represent the
frequency of a particular word in a target docu-
ment. We also use features that indicate the non-
existence of a word, which we found effective in
preliminary experiments; feature f
l,w?
(x, y) is 1 if
l ? y and w is not included in the document x.
Words are stemmed and number expressions are
normalized to a unique symbol. Words are not
used if they are included in the stopword list (322
cmc2007
? max. width avg. width time (sec.)
0 0 0.00 90
100 2 1.17 132
200 3 1.51 145
400 3 1.71 165
800 4 2.11 200
1600 5 2.93 427
3200 4 3.99 2280
reuters10
? max. width avg. width time (sec.)
0 0 0.00 787
500 2 1.72 1378
1000 3 2.00 1752
2000 4 2.16 2594
4000 6 2.90 7183
8000 6 4.22 21555
16000 6 5.67 116535
reuters90
? max. width avg. width time (sec.)
0 0 0.00 26172
500 5 1.74 28067
1000 6 2.24 38510
2000 6 3.22 42479
4000 8 3.68 60029
8000 14 4.56 153268
16000 17 6.39 ?
Table 4: Tree width and training time for
cmc2007, reuters10, and reuters90
words), or they occur fewer than a threshold, c, in
training data. We set c = 5 for reuters10 and
reuters90, following previous works (Gham-
rawi and McCallum, 2005), while c = 0 for
cmc2007, because the data is small.
These features are selected according to av-
eraged mutual information (information gain),
which is the most popular method in previous
works (Yang and Pedersen, 1997; Ghamrawi and
McCallum, 2005). For each label, features are
sorted according to this score, and top-ranked fea-
tures are included in the model. By preliminary
experiments, we fixed parameters, ? for word uni-
gram features and ? for non-existence features, for
each data set, as shown in Table 2.
The same method is applied to the selection of
label correlation features. In the following experi-
ments, we observe the accuracy and training time
by changing the threshold parameter ? for the se-
lection of label correlation features.
4.2 Results
Table 33 shows microaveraged F-scores (micro-
F1) and subset accuracies (sub. acc.) (Ghamrawi
and McCallum, 2005) while varying ?, the num-
3The experiment with ? = 16000 for ME was not per-
formed due to its cost (estimated time is approx. two weeks).
65
ber of label correlation features. In all data sets
and with all classifiers, the accuracy is increased
by incorporating label correlation features. The re-
sults also demonstrate that the accuracy saturates,
or even decreases, with large ?. This indicates that
the feature selection is necessary not only for ob-
taining efficiency, but also for higher accuracy.
Table 4 shows tree widths, and the time for the
training of the ME models. As shown, the graph-
ical model is represented effectively with sparse
graphs, even when the number of label correlation
features is increased. With these results, we can
conclude that our method can model label correla-
tions with a tractable cost.
The accuracy for cmc2007 is significantly bet-
ter than the results reported in Patrick et al (2007)
(micro-F1=81.1) in a similar setting, in which only
word unigram features are used. Our best result is
approaching the results of Crammer et al (2007)
(micro-F1=84.6), which exploits various linguisti-
cally motivated features. Numerous results have
been reported for reuters10, and most of them
report the microaveraged F-score around 91 to 94,
while our best result is comparable to the state-of-
the-art accuracy. For reuters90, Ghamrawi and
McCallum (2005) achieved an improvement in the
microaveraged F-score from 86.34 to 87.01, which
is comparable to our result.
5 Conclusion
This paper described a method for the exact infer-
ence for multi-label classification with label corre-
lation features. Experimental results on text cate-
gorization with the CMC challenge data and the
Reuters-21578 text collection demonstrated that
our method improves the accuracy for text cate-
gorization with a tractable cost. The availability
of exact inference enables us to apply various ma-
chine learning methods not yet investigated in this
paper, including support vector machines.
From the perspective of machine learning re-
search, feature selection methods should be recon-
sidered. While we used a feature selection method
that is widely accepted in text categorization re-
search, it has no direct connection with machine
learning models. Since feature selection methods
motivated by the optimization criteria of machine
learning models have been proposed (Riezler and
Vasserman, 2004), we expect that the integration
of our proposal with those methods will open up a
new framework for multi-label classification.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Grant-in-Aid for Young Scientists (MEXT,
Japan).
References
Andrew, G. and J. Gao. 2007. Scalable training of
l
1
-regularized log-linear models. In 24th Annual In-
ternational Conference on Machine Learning.
Berger, A. L., S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing. Computational Linguistics,
22(1):39?71.
Collins, M. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In 2002 Conference on
Empirical Methods in Natural Language Processing.
Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. 1999. Probabilistic Networks and Ex-
pert Systems. Springer-Verlag, New York.
Crammer, K., M. Dredze, K. Ganchev, and P. P. Taluk-
dar. 2007. Automatic code assignment to medical
text. In BioNLP 2007, pages 129?136.
Ghamrawi, N. and A. McCallum. 2005. Collective
multi-label classification. In ACM 14th Conference
on Information and Knowledge Management.
Herbrich, R., T. Graepel, and C. Campbell. 2001.
Bayes point machines. Journal of Machine Learn-
ing Research, 1:245?279.
Patrick, J., Y. Zhang, and Y. Wang. 2007. Evaluat-
ing feature types for encoding clinical notes. In 10th
Conference of the Pacific Association for Computa-
tional Linguistics, pages 218?225.
Pestian, J. P., C. Brew, P. Matykiewicz, DJ Hovermale,
N. Johnson, K. B. Cohen, and W. Duch. 2007.
A shared task involving multi-label classification of
clinical free text. In BioNLP 2007, pages 97?104.
Riezler, S. and A. Vasserman. 2004. Gradient fea-
ture testing and l
1
regularization for maximum en-
tropy parsing. In 42nd Meeting of the Association
for Computational Linguistics.
Schapire, R. E. and Y. Singer. 2000. Boostexter: a
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
Tsoumakas, G. and I. Katakis. 2007. Multi-label clas-
sification: an overview. Journal of Data Warehous-
ing and Mining, 3(3):1?13.
Yang, Y. and J. O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
14th International Conference on Machine Learn-
ing, pages 412?420.
66
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1044?1050,
Prague, June 2007. c?2007 Association for Computational Linguistics
Dependency Parsing and Domain Adaptation with LR Models and 
Parser Ensembles 
Kenji Sagae1 and Jun?ichi Tsujii1,2,3 
1Department of Computer Science 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining 
{sagae,tsujii}@is.s.u-tokyo.ac.jp 
 
 
Abstract 
We present a data-driven variant of the LR 
algorithm for dependency parsing, and ex-
tend it with a best-first search for probabil-
istic generalized LR dependency parsing.  
Parser actions are determined by a classifi-
er, based on features that represent the cur-
rent state of the parser.  We apply this pars-
ing framework to both tracks of the CoNLL 
2007 shared task, in each case taking ad-
vantage of multiple models trained with 
different learners.  In the multilingual track, 
we train three LR models for each of the 
ten languages, and combine the analyses 
obtained with each individual model with a 
maximum spanning tree voting scheme.  In 
the domain adaptation track, we use two 
models to parse unlabeled data in the target 
domain to supplement the labeled out-of-
domain training set, in a scheme similar to 
one iteration of co-training. 
1 Introduction 
There are now several approaches for multilingual 
dependency parsing, as demonstrated in the 
CoNLL 2006 shared task (Buchholz and Marsi, 
2006).  The dependency parsing approach pre-
sented here extends the existing body of work 
mainly in four ways:  
1. Although stepwise 1  dependency parsing has 
commonly been performed using parsing algo-
                                               
1
 Stepwise parsing considers each step in a parsing algo-
rithm separately, while all-pairs parsing considers entire 
rithms designed specifically for this task, such 
as those described by Nivre (2003) and Yamada 
and Matsumoto (2003), we show that this can 
also be done using the well known LR parsing 
algorithm (Knuth, 1965), providing a connec-
tion between current research on shift-reduce 
dependency parsing and previous parsing work 
using LR and GLR models;  
2. We generalize the standard deterministic step-
wise framework to probabilistic parsing, with 
the use of a best-first search strategy similar to 
the one employed in constituent parsing by Rat-
naparkhi (1997) and later by Sagae and Lavie 
(2006);  
3. We provide additional evidence that the parser 
ensemble approach proposed by Sagae and La-
vie (2006a) can be used to improve parsing ac-
curacy, even when only a single parsing algo-
rithm is used, as long as variation can be ob-
tained, for example, by using different learning 
techniques or changing parsing direction from 
forward to backward (of course, even greater 
gains may be achieved when different algo-
rithms are used, although this is not pursued 
here); and, finally, 
4. We present a straightforward way to perform 
parser domain adaptation using unlabeled data 
in the target domain. 
 
We entered a system based on the approach de-
scribed in this paper in the CoNLL 2007 shared 
                                                                          
trees.  For a more complete definition, see the CoNLL-
X shared task description paper (Buchholz and Marsi, 
2006). 
1044
task (Nivre et al, 2007), which differed from the 
2006 edition by featuring two separate tracks, one 
in multilingual parsing, and a new track on domain 
adaptation for dependency parsers.  In the multi-
lingual parsing track, participants train dependency 
parsers using treebanks provided for ten languages: 
Arabic (Hajic et al, 2004), Basque (Aduriz et al 
2003), Catalan (Mart? et al, 2007), Chinese (Chen 
et al, 2003), Czech (B?hmova et al, 2003), Eng-
lish (Marcus et al, 1993; Johansson and Nugues, 
2007), Greek (Prokopidis et al, 2005), Hungarian 
(Czendes et al, 2005), Italian (Montemagni et al, 
2003), and Turkish (Oflazer et al,  2003).  In the 
domain adaptation track, participants were pro-
vided with English training data from the Wall 
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) converted to dependencies (Jo-
hansson and Nugues, 2007) to train parsers to be 
evaluated on material in the biological (develop-
ment set) and chemical (test set) domains (Kulick 
et al, 2004), and optionally on text from the 
CHILDES database (MacWhinney, 2000; Brown, 
1973). 
 Our system?s accuracy was the highest in the 
domain adaptation track (with labeled attachment 
score of 81.06%), and only 0.43% below the top 
scoring system in the multilingual parsing track 
(our average labeled attachment score over the ten 
languages was 79.89%).  We first describe our ap-
proach to multilingual dependency parsing, fol-
lowed by our approach for domain adaptation.  We 
then provide an analysis of the results obtained 
with our system, and discuss possible improve-
ments. 
2 A Probabilistic LR Approach for De-
pendency Parsing 
Our overall parsing approach uses a best-first 
probabilistic shift-reduce algorithm based on the 
LR algorithm (Knuth, 1965).  As such, it follows a 
bottom-up strategy, or bottom-up-trees, as defined 
in Buchholz and Marsi (2006), in contrast to the 
shift-reduce dependency parsing algorithm de-
scribed by Nivre (2003), which is a bottom-up/top-
down hybrid, or bottom-up-spans.  It is unclear 
whether the use of a bottom-up-trees algorithm has 
any advantage over the use of a bottom-up-spans 
algorithm (or vice-versa) in practice, but the avail-
ability of different algorithms that perform the 
same parsing task could be advantageous in parser 
ensembles.  The main difference between our pars-
er and a traditional LR parser is that we do not use 
an LR table derived from an explicit grammar to 
determine shift/reduce actions.  Instead, we use a 
classifier with features derived from much of the 
same information contained in an LR table: the top 
few items on the stack, and the next few items of 
lookahead in the remaining input string.  Addition-
ally, following Sagae and Lavie (2006), we extend 
the basic deterministic LR algorithm with a best-
first search, which results in a parsing strategy sim-
ilar to generalized LR parsing (Tomita, 1987; 
1990), except that we do not perform Tomita?s 
stack-merging operations.   
The resulting algorithm is projective, and non-
projectivity is handled by pseudo-projective trans-
formations as described in (Nivre and Nilsson, 
2005).  We use Nivre and Nilsson?s PATH 
scheme2. 
For clarity, we first describe the basic variant of 
the LR algorithm for dependency parsing, which is 
a deterministic stepwise algorithm.  We then show 
how we extend the deterministic parser into a best-
first probabilistic parser. 
2.1 Dependency Parsing with a Data-Driven 
Variant of the LR Algorithm 
The two main data structures in the algorithm are a 
stack S and a queue Q.  S holds subtrees of the fi-
nal dependency tree for an input sentence, and Q 
holds the words in an input sentence.  S is initia-
lized to be empty, and Q is initialized to hold every 
word in the input in order, so that the first word in 
the input is in the front of the queue.3 
The parser performs two main types of actions: 
shift and reduce.  When a shift action is taken, a 
word is shifted from the front of Q, and placed on 
the top of S (as a tree containing only one node, the 
word itself).  When a reduce action is taken, the 
                                               
2
 The PATH scheme was chosen (even though Nivre and 
Nilsson report slightly better results with the HEAD 
scheme) because it does not result in a potentially qua-
dratic increase in the number of dependency label types, 
as observed with the HEAD and HEAD+PATH 
schemes.  Unfortunately, experiments comparing the 
use of the different pseudo-projectivity schemes were 
not performed due to time constraints. 
3
 We append a ?virtual root? word to the beginning of 
every sentence, which is used as the head of every word 
in the dependency structure that does not have a head in 
the sentence. 
1045
two top items in S (s1 and s2) are popped, and a 
new item is pushed onto S.  This new item is a tree 
formed by making the root s1 of a dependent of the 
root of s2, or the root of s2 a dependent of the root 
of s1.  Depending on which of these two cases oc-
cur, we call the action reduce-left or reduce-right, 
according to whether the head of the new tree is to 
the left or to the right its new dependent.  In addi-
tion to deciding the direction of a reduce action, 
the label of the newly formed dependency arc must 
also be decided.  
Parsing terminates successfully when Q is emp-
ty (all words in the input have been processed) and 
S contains only a single tree (the final dependency 
tree for the input sentence).  If Q is empty, S con-
tains two or more items, and no further reduce ac-
tions can be taken, parsing terminates and the input 
is rejected.  In such cases, the remaining items in S 
contain partial analyses for contiguous segments of 
the input. 
2.2 A Probabilistic LR Model for Dependen-
cy Parsing 
In the traditional LR algorithm, parser states are 
placed onto the stack, and an LR table is consulted 
to determine the next parser action.  In our case, 
the parser state is encoded as a set of features de-
rived from the contents of the stack S and queue Q, 
and the next parser action is determined according 
to that set of features.  In the deterministic case 
described above, the procedure used for determin-
ing parser actions (a classifier, in our case) returns 
a single action.  If, instead, this procedure returns a 
list of several possible actions with corresponding 
probabilities, we can then parse with a model simi-
lar to the probabilistic LR models described by 
Briscoe and Carroll (1993), where the probability 
of a parse tree is the product of the probabilities of 
each of the actions taken in its derivation. 
To find the most probable parse tree according 
to the probabilistic LR model, we use a best-first 
strategy.  This involves an extension of the deter-
ministic shift-reduce into a best-first shift-reduce 
algorithm.  To describe this extension, we first in-
troduce a new data structure Ti that represents a 
parser state, which includes a stack Si, a queue Qi, 
and a probability Pi.  The deterministic algorithm 
is a special case of the probabilistic algorithm 
where we have a single parser state T0 that contains 
S0 and Q0, and the probability of the parser state is 
1.  The best-first algorithm, on the other hand, 
keeps a heap H containing multiple parser states 
T0... Tm.  These states are ordered in the heap ac-
cording to their probabilities, which are determined 
by multiplying the probabilities of each of the 
parser actions that resulted in that parser state.  The 
heap H is initialized to contain a single parser state 
T0, which contains a stack S0, a queue Q0 and prob-
ability P0 = 1.0.  S0 and Q0 are initialized in the 
same way as S and Q in the deterministic algo-
rithm.  The best-first algorithm then loops while H 
is non-empty.  At each iteration, first a state Tcurrent 
is popped from the top of H.  If Tcurrent corresponds 
to a final state (Qcurrent is empty and Scurrent contains 
a single item), we return the single item in Scurrent 
as the dependency structure corresponding to the 
input sentence.  Otherwise, we get a list of parser 
actions act0...actn (with associated probabilities 
Pact0...Pactn) corresponding to state Tcurrent.  For 
each of these parser actions actj, we create a new 
parser state Tnew by applying actj to Tcurrent, and set 
the probability Tnew to be Pnew = Pcurrnet * Pactj.  
Then, Tnew is inserted into the heap H.  Once new 
states have been inserted onto H for each of the n 
parser actions, we move on to the next iteration of 
the algorithm. 
3 Multilingual Parsing Experiments 
For each of the ten languages for which training 
data was provided in the multilingual track of the 
CoNLL 2007 shared task, we trained three LR 
models as follows.  The first LR model for each 
language uses maximum entropy classification 
(Berger et al, 1996) to determine possible parser 
actions and their probabilities4.  To control overfit-
ting in the MaxEnt models, we used box-type in-
equality constraints (Kazama and Tsujii, 2003). 
The second LR model for each language also uses 
MaxEnt classification, but parsing is performed 
backwards, which is accomplished simply by re-
versing the input string before parsing starts.  Sa-
gae and Lavie (2006a) and Zeman and ?abokrtsk? 
(2005) have observed that reversing the direction 
of stepwise parsers can be beneficial in parser 
combinations. The third model uses support vector 
machines 5  (Vapnik, 1995) using the polynomial 
                                               
4
 Implementation by Yoshimasa Tsuruoka, available at 
http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 
5
 Implementation by Taku Kudo, available at 
http://chasen.org/~taku/software/TinySVM/ and all vs. 
all was used for multi-class classification. 
1046
kernel with degree 2. Probabilities were estimated 
for SVM outputs using the method described in 
(Platt, 1999), but accuracy improvements were not 
observed during development when these esti-
mated probabilities were used instead of simply the 
single best action given by the classifier (with 
probability 1.0), so in practice the SVM parsing 
models we used were deterministic. 
At test time, each input sentence is parsed using 
each of the three LR models, and the three result-
ing dependency structures are combined according 
to the maximum-spanning-tree parser combination 
scheme6 (Sagae and Lavie, 2006a) where each de-
pendency proposed by each of the models has the 
same weight (it is possible that one of the more 
sophisticated weighting schemes proposed by Sa-
gae and Lavie may be more effective, but these 
were not attempted).  The combined dependency 
tree is the final analysis for the input sentence. 
Although it is clear that fine-tuning could pro-
vide accuracy improvements for each of the mod-
els in each language, the same set of meta-
parameters and features were used for all of the ten 
languages, due to time constraints during system 
development.  The features used were7:  
 
? For the subtrees in S(1) and S(2) 
? the number of children of the root word of 
the subtrees; 
? the number of children of the root word of 
the subtree to the right of the root word; 
? the number of children of the root word of 
the subtree to the left of the root word; 
? the POS tag and DEPREL of the rightmost 
and leftmost children;  
? The POS tag of the word immediately to the 
right of the root word of S(2); 
? The POS tag of the word immediately to the 
left of S(1); 
                                               
6
 Each dependency tree is deprojectivized before the 
combination occurs. 
7
 S(n) denotes the nth item from the top of the stack 
(where S(1) is the item on top of the stack), and Q(n) 
denotes the nth item in the queue.  For a description of 
the features names in capital letters, see the shared task 
description (Nivre et al, 2007). 
? The previous parser action; 
? The features listed for the root words of the 
subtrees in table 1.   
In addition, the MaxEnt models also used selected 
combinations of these features.  The classes used 
to represent parser actions were designed to encode 
all aspects of an action (shift vs. reduce, right vs. 
left, and dependency label) simultaneously. 
Results for each of the ten languages are shown 
in table 2 as labeled and unlabeled attachment 
scores, along with the average labeled attachment 
score and highest labeled attachment score for all 
participants in the shared task.  Our results shown 
in boldface were among the top three scores for 
those particular languages (five out of the ten lan-
guages). 
 
 
 S(1) S(2) S(3) Q(0) Q(1) Q(3) 
WORD x x x x x  
LEMMA x x  x   
POS x x x x x x 
CPOS x x  x   
FEATS x x  x   
Table 1: Additional features. 
 
 
 
Language LAS UAS Avg 
LAS 
Top 
LAS 
Arabic 74.71 84.04 68.34 76.52 
Basque 74.64 81.19 68.06 76.94 
Catalan 88.16 93.34 79.85 88.70 
Chinese 84.69 88.94 76.59 84.69 
Czech 74.83 81.27 70.12 80.19 
English 89.01 89.87 80.95 89.61 
Greek 73.58 80.37 70.22 76.31 
Hungarian 79.53 83.51 71.49 80.27 
Italian 83.91 87.68 78.06 84.40 
Turkish 75.91 82.72 70.06 79.81 
ALL 79.90 85.29 65.50 80.32 
Table 2: Multilingual results. 
 
 
4 Domain Adaptation Experiments 
In a similar way as we used multiple LR models in 
the multilingual track, in the domain adaptation 
track we first trained two LR models on the out-of-
1047
domain labeled training data.  The first was a for-
ward MaxEnt model, and the second was a back-
ward SVM model.  We used these two models to 
perform a procedure similar to a single iteration of 
co-training, except that selection of the newly (au-
tomatically) produced training instances was done 
by selecting sentences for which the two models 
produced identical analyses.  On the development 
data we verified that sentences for which there was 
perfect agreement between the two models had 
labeled attachment score just above 90 on average, 
even though each of the models had accuracy be-
tween 78 and 79 over the entire development set. 
Our approach was as follows:  
 
1. We trained the forward MaxEnt and backward 
SVM models using the out-of-domain labeled 
training data;  
2. We then used each of the models to parse the 
first two of the three sets of domain-specific 
unlabeled data that were provided (we did not 
use the larger third set) 
3. We compared the output for the two models, 
and selected only identical analyses that were 
produced by each of the two separate models;  
4. We added those analyses (about 200k words in 
the test domain) to the original (out-of-
domain) labeled training set;  
5. We retrained the forward MaxEnt model with 
the new larger training set; and finally  
6. We used this model to parse the test data. 
Following this procedure we obtained a labeled 
attachment score of 81.06, and unlabeled attach-
ment score of 83.42, both the highest scores for 
this track.  This was done without the use of any 
additional resources (closed track), but these re-
sults are also higher than the top score for the open 
track, where the use of certain additional resources 
was allowed.  See (Nivre et al, 2007). 
5 Analysis and Discussion 
One of the main assumptions in our use of differ-
ent models based on the same algorithm is that 
while the output generated by those models may 
often differ, agreement between the models is an 
indication of correctness.  In our domain adapta-
tion approach, this was clearly true.  In fact, the 
approach would not have worked if this assump-
tion was false.  Experiments on the development 
set were encouraging.  As stated before, when the 
parsers agreed, labeled attachment score was over 
90, even though the score of each model alone was 
lower than 79.  The domain-adapted parser had a 
score of 82.1, a significant improvement.  Interes-
tingly, the ensemble used in the multilingual track 
also produced good results on the development set 
for the domain adaptation data, without the use of 
the unlabeled data at all, with a score of 81.9 (al-
though the ensemble is more expensive to run). 
The different models used in each track were 
distinct in a few ways: (1) direction (forward or 
backward); (2) learner (MaxEnt or SVM); and (3) 
search strategy (best-first or deterministic).  Of 
those differences, the first one is particularly inter-
esting in single-stack shift-reduce models, as ours.  
In these models, the context to each side of a (po-
tential) dependency differs in a fundamental way.  
To one side, we have tokens that have already been 
processed and are already in subtrees, and to the 
other side we simply have a look-ahead of the re-
maining input sentence.  This way, the context of 
the same dependency in a forward parser may dif-
fer significantly from the context of the same de-
pendency in a backward parser.  Interestingly, the 
accuracy scores of the MaxEnt backward models 
were found to be generally just below the accuracy 
of their corresponding forward models when tested 
on development data, with two exceptions: Hunga-
rian and Turkish.  In Hungarian, the accuracy 
scores produced by the forward and backward 
MaxEnt LR models were not significantly differ-
ent, with both labeled attachment scores at about 
77.3 (the SVM model score was 76.1, and the final 
combination score on development data was 79.3).  
In Turkish, however, the backward score was sig-
nificantly higher than the forward score, 75.0 and 
72.3, respectively. The forward SVM score was 
73.1, and the combined score was 75.8.   In expe-
riments performed after the official submission of 
results, we evaluated a backward SVM model 
(which was trained after submission) on the same 
development set, and found it to be significantly 
more accurate than the forward model, with a score 
of 75.7.  Adding that score to the combination 
raised the combination score to 77.9 (a large im-
provement from 75.8).  The likely reason for this 
difference is that over 80% of the dependencies in 
the Turkish data set have the head to the right of 
1048
the dependent, while only less than 4% have the 
head to the left.  This means that the backward 
model builds much more partial structure in the 
stack as it consumes input tokens, while the for-
ward model must consume most tokens before it 
starts making attachments.  In other words, context 
in general in the backward model has more struc-
ture, and attachments are made while there are still 
look-ahead tokens, while the opposite is generally 
true in the forward model. 
6 Conclusion  
Our results demonstrate the effectiveness of even 
small ensembles of parsers that are relatively 
similar (using the same features and the same 
algorithm).  There are several possible extensions 
and improvements to the approach we have 
described.  For example, in section 3 we mention 
the use of different weighting schemes in 
dependency voting.  We list additional ideas that 
were not attempted due to time constraints, but that 
are likely to produce improved results. 
One of the simplest improvements to our ap-
proach is simply to train more models with no oth-
er changes to our set-up.  As mentioned in section 
5, the addition of a backward SVM model did im-
prove accuracy on the Turkish set significantly, 
and it is likely that improvements would also be 
obtained in other languages.  In addition, other 
learning approaches, such as memory-based lan-
guage processing (Daelemans and Van den Bosch, 
2005), could be used.  A drawback of adding more 
models that became obvious in our experiments 
was the increased cost of both training (for exam-
ple, the SVM parsers we used required significant-
ly longer to train than the MaxEnt parsers) and 
run-time (parsing with MBL models can be several 
times slower than with MaxEnt, or even SVM).  A 
similar idea that may be more effective, but re-
quires more effort, is to add parsers based on dif-
ferent approaches.  For example, using MSTParser 
(McDonald and Pereira, 2005), a large-margin all-
pairs parser, in our domain adaptation procedure 
results in significantly improved accuracy (83.2 
LAS).  Of course, the use of different approaches 
used by different groups in the CoNLL 2006 and 
2007 shared tasks represents great opportunity for 
parser ensembles. 
Acknowledgements 
We thank the shared task organizers and treebank 
providers.  We also thank the reviewers for their 
comments and suggestions, and Yusuke Miyao for 
insightful discussions.  This work was supported in 
part by Grant-in-Aid for Specially Promoted Re-
search 18002007.      
References 
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer.  
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to 
naturallanguage processing. Computational 
Linguistics, 22(1):39?71. 
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
E. Briscoe and J. Carroll. 1993. Generalized Probabilis-
tic LR Parsing of Natural Language (Corpora) with 
Unification-Based Grammars. In Computational Lin-
guistics, 19(1), pages 25-59. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task 
on Multilingual Dependency Parsing. In Proc. of the 
Tenth Conference on Computational Natural 
Language Learning (CoNLL-X). New York, NY. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
W. Daelemans and A. Van den Bosch.  2005.  Memory-
based language processing.  Cambridge University 
Press.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
1049
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
J. Kazama, and J. Tsujii. 2003. Evaluation and 
extension of maximum entropy models with ine-
quality constraints.  In Proceedings of EMNLP 2003. 
D. Knuth. 1965. On the translation of languages from 
left to right, Information and Control 8, 607-639. 
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
R. McDonald, K.Crammer, and F. Pereira. 2005.  On-
line large-margin training of dependency parsers. In 
Proc. of the 43rd Annual Meeting of the Association 
for Computational Linguistics, 2005 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210. 
J. Nivre. 2003. An efficient algorithm for dependency 
parsing.  In Proc. of the Eighth International 
Workshop on Parsing Technologies (IWPT?03). 
Nancy, France. 
J. Nivre, and J. Nilsson. 2005. Pseudo-Projective 
Dependency Parsing. In Proceedings of the 43rd 
Annual Meeting of the Association for Computational 
Linguistics (ACL), 99-106. Ann Arbor, MI. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the CoNLL 
2007 Shared Task. Joint Conf. on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, pages 261?277.  
J. Platt. 1999. Probabilistic Outputs for Support Vector 
Machines and Comparisons to Regularized 
Likelihood Methods. In Advances in Large Margin 
Classiers, MIT Press. 
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
A. Ratnaparkhi. 1997. A linear observed time statistical 
parser based on maximum entropy models. In 
Proceedings of the Second Conference on Empirical 
Methods in Natural Language Processing. Prov-
idence, RI 
K. Sagae, and A. Lavie. 2006. A best-first probabilistic 
shift-reduce parser. Proceedings of the 43rd Meeting 
of the Association for Computational Linguistics - 
posters (ACL'06). Sydney, Australia. 
K. Sagae, and A. Lavie. 2006a. Parser combination by 
reparsing. Proceedings of the 2006 Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational 
Linguistics - short papers (HLT-NAACL'06). New 
York, NY. 
M. Tomita. 1987. An efficient augmented context-free 
parsing algorithm. Computational Linguistics, 13:31?
46. 
M. Tomita. 1990. The generalized LR parser/compiler - 
version 8.4. In Proceedings of the International 
Conference on Computational Linguistics 
(COLING?90), pages 59?63. Helsinki, Finland. 
V. N. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer-Verlag. 
H. Yamada, and Y. Matsumoto. 2003.  Statistical 
dependency analysis with support vector machines. 
In Proceedings of the Eighth International Workshop 
on Parsing Technologies (IWPT?03). Nancy, France. 
D. Zeman, Z. ?abokrtsk?. 2005. Improving Parsing Ac-
curacy by Combining Diverse Dependency Parsers. 
In Proceedings of the International Workshop on 
Parsing Technologies (IWPT 2005). Vancouver, Brit-
ish Columbia. 
 
1050
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 121?130,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Rich Feature Vector for Protein-Protein Interaction Extraction from
Multiple Corpora
Makoto Miwa1 Rune S?tre1 Yusuke Miyao1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,rune.saetre,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Because of the importance of protein-
protein interaction (PPI) extraction from
text, many corpora have been proposed
with slightly differing definitions of pro-
teins and PPI. Since no single corpus is
large enough to saturate a machine learn-
ing system, it is necessary to learn from
multiple different corpora. In this paper,
we propose a solution to this challenge.
We designed a rich feature vector, and we
applied a support vector machine modi-
fied for corpus weighting (SVM-CW) to
complete the task of multiple corpora PPI
extraction. The rich feature vector, made
from multiple useful kernels, is used to
express the important information for PPI
extraction, and the system with our fea-
ture vector was shown to be both faster
and more accurate than the original kernel-
based system, even when using just a sin-
gle corpus. SVM-CW learns from one cor-
pus, while using other corpora for support.
SVM-CW is simple, but it is more effec-
tive than other methods that have been suc-
cessfully applied to other NLP tasks ear-
lier. With the feature vector and SVM-
CW, our system achieved the best perfor-
mance among all state-of-the-art PPI ex-
traction systems reported so far.
1 Introduction
The performance of an information extraction pro-
gram is highly dependent on various factors, in-
cluding text types (abstracts, complete articles, re-
ports, etc.), exact definitions of the information to
be extracted, shared sub-topics of the text collec-
tions from which information is to be extracted.
Even if two corpora are annotated in terms of the
same type of information by two groups, the per-
formance of a program trained by one corpus is
unlikely to be reproduced in the other corpus. On
the other hand, from a practical point of view, it is
worth while to effectively use multiple existing an-
notated corpora together, because it is very costly
to make new annotations.
One problem with several different corpora is
protein-protein interaction (PPI) extraction from
text. While PPIs play a critical role in un-
derstanding the working of cells in diverse bio-
logical contexts, the manual construction of PPI
databases such as BIND, DIP, HPRD, IntAct, and
MINT (Mathivanan et al, 2006) is known to be
very time-consuming and labor-intensive. The au-
tomatic extraction of PPI from published papers
has therefore been a major research topic in Natu-
ral Language Processing for Biology (BioNLP).
Among several PPI extraction task settings, the
most common is sentence-based, pair-wise PPI ex-
traction. At least four annotated corpora have been
provided for this setting: AIMed (Bunescu et al,
2005), HPRD50 (Fundel et al, 2006), IEPA (Ding
et al, 2002), and LLL (Ne?dellec, 2005). Each of
these corpora have been used as the standard cor-
pus for training and testing PPI programs. More-
over, several corpora are annotated for more types
of events than just for PPI. Such examples include
BioInfer (Pyysalo et al, 2007), and GENIA (Kim
et al, 2008a), and they can be reorganized into PPI
corpora. Even though all of these corpora were
made for PPI extraction, they were constructed
based on different definitions of proteins and PPI,
which reflect different biological research inter-
ests (Pyysalo et al, 2008).
Research on PPI extraction so far has revealed
that the performance on each of the corpora could
121
benefit from additional examples (Airola et al,
2008). Learning from multiple annotated cor-
pora could lead to better PPI extraction perfor-
mance. Various research paradigms such as induc-
tive transfer learning (ITL) and domain adaptation
(DA) have mainly focused on how to effectively
use corpora annotated by other groups, by reduc-
ing the incompatibilities (Pan and Yang, 2008).
In this paper, we propose the extraction of PPIs
from multiple different corpora. We design a rich
feature vector, and as an ITL method, we ap-
ply a support vector machine (SVM) modified for
corpus weighting (SVM-CW) (Schweikert et al,
2008), in order to evaluate the use of multiple cor-
pora for the PPI extraction task. Our rich feature
vector is made from multiple useful kernels, each
of which is based on multiple parser inputs, pro-
posed by Miwa et al (2008). The system with our
feature vector was better than or at least compa-
rable to the state-of-the-art PPI extraction systems
on every corpus. The system is a good starting
point to use the multiple corpora. Using one of the
corpora as the target corpus, SVM-CW weights
the remaining corpora (we call them the source
corpora) with ?goodness? for training on the tar-
get corpus. While SVM-CW is simple, we show
that SVM-CW can improve the performance of the
system more effectively and more efficiently than
other methods proven to be successful in other
NLP tasks earlier. As a result, SVM-CW with our
feature vector is comprised of a PPI system with
five different models, of which each model is su-
perior to the best model in the original PPI extrac-
tion task, which used only the single corpus.
2 Related Works
While sentence-based, pair-wise PPI extraction
was initially tackled by using simple methods
based on co-occurrences, lately, more sophisti-
cated machine learning systems augmented by
NLP techniques have been applied (Bunescu et al,
2005). The task has been tackled as a classifica-
tion problem. To pull out useful information from
NLP tools including taggers and parsers, several
kernels have been applied to calculate the similar-
ity between PPI pairs. Miwa et al (2008) recently
proposed the use of multiple kernels using multi-
ple parsers. This outperformed other systems on
the AIMed, which is the most frequently used cor-
pus for the PPI extraction task, by a wide margin.
To improve the performance using external
Classification
Result
Training
Data
Feature 
vector
Raw Texts
Parsers
Classifier
Test 
Data
Raw Texts
Model
Pair Information
Pair Information
Label
Figure 1: Overview of our PPI extraction system
training data, many ITL and DA methods have
been proposed. Most of ITL methods assume that
the feature space is same, and that the labels may
be different in only some examples, while most of
DA methods assume that the labels are the same,
and that the feature space is different. Among the
methods, we use adaptive SVM (aSVM) (Yang et
al., 2007), singular value decomposition (SVD)
based alternating structure optimization (SVD-
ASO) (Ando et al, 2005), and transfer AdaBoost
(TrAdaBoost) (Dai et al, 2007) to compare with
SVM-CW. We do not use semi-supervised learn-
ing (SSL) methods, because it would be consid-
erably costly to generate enough clean unlabeled
data needed for SSL (Erkan et al, 2007). aSVM
is seen as a promising DA method among sev-
eral modifications of SVM including SVM-CW.
aSVM tries to find a model that is close to the one
made from other classification problems. SVD-
ASO is one of the most successful SSL, DA, or
multi-task learning methods in NLP. The method
tries to find an additional useful feature space by
solving auxiliary problems that are close to the tar-
get problem. With well-designed auxiliary prob-
lems, the method has been applied to text clas-
sification, text chunking, and word sense disam-
biguation (Ando, 2006). The method was reported
to perform better than or comparable to the best
state-of-the-art systems in all of these tasks. TrAd-
aBoost was proposed as an ITL method. In train-
ing, the method reduces the effect of incompatible
examples by decreasing their weights, and thereby
tries to use useful examples from source corpora.
The method has been applied to text classifica-
tion, and the reported performance was better than
SVM and transductive SVM (Dai et al, 2007).
3 PPI Extraction System
The target task of our system is a sentence-based,
pair-wise PPI extraction. It is formulated as a clas-
sification problem that judges whether a given pair
122
XPGp1 protein interacts with multiple subunits of
TFIIHprot and with CSBp2 protein.
Figure 2: A sentence including an interacting pro-
tein pair (p1, p2). (AIMed PMID 8652557, 9th
sentence, 3rd pair)
BOW
v-walks
e-walks
Graph BOW
v-walks
e-walks
Graph
Normalization
Parsers
KSDEPEnju
a sentence including a pair
feature vector
BOW Graph BOW
v-walks
e-walks
Graph
v-walks
e-walks
Figure 3: Extraction of a feature vector from the
target sentence
of proteins in a sentence is interacting or not. Fig-
ure 2 shows an example of a sentence in which the
given pair (p1 and p2) actually interacts.
Figure 1 shows the overview of the proposed
PPI extraction system. As a classifier using a sin-
gle corpus, we use the 2-norm soft-margin lin-
ear SVM (L2-SVM) classifier, with the dual co-
ordinate decent (DCD) method, by Hsieh et al
(2008). In this section, we explain the two main
features: the feature vector, and the corpus weight-
ing method for multiple corpora.
3.1 Feature Vector
We propose a feature vector with three types of
features, corresponding to the three different ker-
nels, which were each combined with the two
parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao
et al, 2008); this feature vector is used because the
kernels with these parsers were shown to be effec-
tive for PPI extraction by Miwa et al (2008), and
because it is important to start from a good per-
formance single corpus system. Both parsers were
retrained using the GENIA Treebank corpus pro-
vided by Kim et al (2003). By using our linear
feature vector, we can perform calculations faster
by using fast linear classifiers like L2-SVM, and
we also obtain a more accurate extraction, than by
using the original kernel method.
Figure 3 summarizes the way in which the fea-
ture vector is constructed. The system extracts
Bag-of-Words (BOW), shortest path (SP), and
graph features from the output of two parsers. The
PROT M:1, and M:1, interact M:1, multiple M:1,
of M:1, protein M:1, subunit M:1, with M:2, pro-
tein A:1
Figure 4: Bag-of-Words features of the pair in Fig-
ure 2 with their positions (B:Before, M:in the Mid-
dle of, A:After) and frequencies.
NMOD SBJ
rNMOD
ENTITY1 protein interact ENTITY2protein protein
ENTITY1 protein interacts with multiple and with ENTITY2 protein .
NMOD SBJ
COOD
COORD
NMOD
PMOD
NMOD SBJ
rNMOD
protein interact protein
SBJ rCOOD
rPMOD
V-walks  
E-walks
???
???
???
Figure 5: Vertex walks, edge walks in the upper
shortest path between the proteins in the parse tree
by KSDEP. The walks and their subsets are used
as the shortest path features of the pair in Figure 2.
output is grouped according to the feature-type
and parser, and each group of features is separately
normalized by the L2-norm1. Finally, all values
are put into a single feature vector, and the whole
feature vector is then also normalized by the L2-
norm. The features are constructed by using pred-
icate argument structures (PAS) from Enju, and by
using the dependency trees from KSDEP.
3.1.1 Bag-of-Words (BOW) Features
The BOW feature includes the lemma form of a
word, its relative position to the target pair of pro-
teins (Before, Middle, After), and its frequency in
the target sentence. BOW features form the BOW
kernel in the original kernel method. BOW fea-
tures for the pair in Figure 2 are shown in Figure 4.
3.1.2 Shortest Path (SP) Features
SP features include vertex walks (v-walks), edge
walks (e-walks), and their subsets (Kim et al,
2008b) on the target pair in a parse structure, and
represent the connection between the pair. The
features are the subsets of the tree kernels on the
shortest path (S?tre et al, 2007). Figure 5 illus-
trates the shortest path between the pair in Fig-
ure 2, and its v-walks and e-walks extracted from
the shortest path in the parse tree by KSDEP. A
v-walk includes two lemmas and their link, while
1The vector normalized by the L2-norm is also called a
unit vector.
123
an e-walk includes a lemma and its two links. The
links indicates the predicate argument relations for
PAS, and the dependencies for dependency trees.
3.1.3 Graph Features
Graph features are made from the all-paths graph
kernel proposed by Airola et al (2008). The ker-
nel represents the target pair using graph matrices
based on two subgraphs, and the graph features are
all the non-zero elements in the graph matrices.
The two subgraphs are a parse structure sub-
graph (PSS) and a linear order subgraph (LOS).
Figure 6 describes the subgraphs of the sentence
parsed by KSDEP in Figure 2. PSS represents the
parse structure of a sentence. PSS has word ver-
tices or link vertices. A word vertex contains its
lemma and its part-of-speech (POS), while a link
vertex contains its link. Additionally, both types
of vertices contain their positions relative to the
shortest path. The ?IP?s in the vertices on the
shortest path represent the positions, and the ver-
tices are differentiated from the other vertices like
?P?, ?CC?, and ?and:CC? in Figure 6. LOS repre-
sents the word sequence in the sentence. LOS has
word vertices, each of which contains its lemma,
its relative position to the target pair, and its POS.
Each subgraph is represented by a graph matrix
G as follows:
G = L
T
?
?
n=1
A
n
L, (1)
where L is a N?L label matrix, A is an N?N
edge matrix, N represents the number of vertices,
and L represents the number of labels. The la-
bel of a vertex includes all information described
above (e.g. ?ENTITY1:NN:IP? in Figure 6). If
two vertices have exactly same information, the
labels will be same. G can be calculated effi-
ciently by using the Neumann Series (Airola et al,
2008). The label matrix represents the correspon-
dence between labels and vertices. L
ij
is 1 if the
i-th vertex corresponds to the j-th label, and 0 oth-
erwise. The edge matrix represents the connection
between the pairs of vertices. A
ij
is a weight w
ij
(0.9 or 0.3 in Figure 6 (Airola et al, 2008)) if the
i-th vertex is connected to the j-th vertex, and 0
otherwise. By this calculation, G
ij
represent the
sum of the weights of all paths between the i-th
label and the j-th label.
A B H I L
positive 1,000 2,534 163 335 164
all 5,834 9,653 433 817 330
Table 1: The sizes of used PPI corpora. A:AIMed,
B:BioInfer, H:HPRD50, I:IEPA, and L:LLL.
50
60
70
80
90
100
0 20 40 60 80 100
% examples
AImed (F)
BioInfer (F)
AImed (AUC)
BioInfer (AUC)
Figure 7: Learning curves on two large corpora.
The x-axis is related to the percentage of the ex-
amples in a corpus. The curves are obtained by a
10-fold CV with a random split.
3.2 Corpus Weighting for Mixing Corpora
Table 1 shows the sizes of the PPI corpora that we
used. Their widely-ranged differences including
the sizes were manually analyzed by Pyysalo et
al. (2008). While AIMed, HPRD50, IEPA, and
LLL were all annotated as PPI corpora, BioInfer in
its original form contains much more fine-grained
information than does just the PPI. BioInfer was
transformed into a PPI corpus by a program, so
making it the largest of the five. Among them,
AIMed alone was created by annotating whole ab-
stracts, while the other corpora were made by an-
notating single sentences selected from abstracts.
Figure 7 shows the learning curves on two large
corpora: AIMed and BioInfer. The curves are
obtained by performing a 10-fold cross valida-
tion (CV) on each corpus, with random splits, us-
ing our system. The curves show that the perfor-
mances can benefit from the additional examples.
To get a better PPI extraction system for a chosen
target, we need to draw useful shared information
from external source corpora. We refer to exam-
ples in the source corpora as ?source examples?,
and examples in a target corpus as ?target exam-
ples?. Among the corpora, we assume that the la-
bels in some examples are incompatible, and that
their distributions are also different, but that the
feature space is shared.
In order to draw useful information from the
source corpora to get a better model for the target
124
ENTITY1
NN
IP
protein
NN
IP
interact
VBZ
IP
with
IN
IP
multiple
JJ
subunit
NNS
of
IN
PROT
NN
and
CC
with
IN
IP
ENTITY2
NN
IP
protein
NN
IP
.
.
NMOD
IP
SBJ
IP
COOD
IP
PMOD
NMOD NMOD
PMOD
CC
COORD
IP
NMOD
IP
PMOD
IP
P
ENTITY1
NN
protein
NN
M
interact
VBZ
M
with
IN
M
multiple
JJ
M
subunit
NNS
M
of
IN
M
PROT
NN
M
and
CC
M
with
IN
M
ENTITY2
NN
protein
NN
A
.
.
0.9,            0.3
IP: In shortest Path, B:Before, M:in the Middle of, A:After
Figure 6: Parse structure subgraph and linear order subgraph to extract graph features of the pair in
Figure 2. The parse structure subgraph is from the parse tree by KSDEP.
corpus, we use SVM-CW, which has been used
as a DA method. Given a set of instance-label
pairs (xi, yi), i = 1, . . ., ls + lt, xi?Rn, and
y
i
?{?1,+1}, we solve the following problem:
min
w
1
2
w
T
w + C
s
ls
?
i=1
`
i
+ C
t
ls+lt
?
i=ls+1
`
i
, (2)
where w is a weight vector, ` is a loss function,
and ls and lt are the numbers of source and target
examples respectively. C
s
? 0 and C
t
? 0 are
penalty parameters. We use a squared hinge loss
`
i
= max(0, 1? y
i
w
T
xi)2. Here, the source cor-
pora are treated as one corpus. The problem, ex-
cluding the second term, is equal to L2-SVM. The
problem can be solved using the DCD method.
As an ITL method, SVM-CW weights each cor-
pus, and tries to benefit from the source corpora,
by adjusting the effect of their compatibility and
incompatibility. For the adjustment, these penalty
parameters should be set properly. Since we are
unaware of the widely ranged differences among
the corpora, we empirically estimated them by
performing 10-fold CV on the training data.
4 Evaluation
4.1 Evaluation Settings
We used five corpora for evaluation: AIMed,
BioInfer, HPRD50, IEPA, and LLL. For the com-
parison with other methods, we report the F-
score (%), and the area under the receiver op-
erating characteristic (ROC) curve (AUC) (%)
using (abstract-wise) a 10-fold CV and a one-
answer-per-occurrence criterion. These measures
are commonly used for the PPI extraction tasks.
The F-score is a harmonic mean of Precision and
Recall. The ROC curve is a plot of a true posi-
tive rate (TPR) vs a false positive rate (FPR) for
different thresholds. We tuned the regularization
parameters of all classifiers by performing a 10-
fold CV on the training data using a random split.
The other parameters were fixed, and we report the
highest of the macro-averaged F-scores as our fi-
nal F-score. For 10-fold CV, we split the corpora
as recommended by Airola et al (2008).
4.2 PPI Extraction on a Single Corpus
In this section, we evaluate our system on a single
corpus, in order to evaluate our feature vector and
to justify the use of the following modules: nor-
malization methods and classification methods.
First, we compare our preprocessing method
with other preprocessing methods to confirm how
our preprocessing method improves the perfor-
mance. Our method produced 64.2% in F-score
using L2-SVM on AIMed. Scaling all features in-
dividually to have a maximal absolute value of 1,
produced only 44.2% in the F-score, while nor-
malizing the feature vector by L2-norm produced
61.5% in the F-score. Both methods were inferior
to our method, because the values of features in
the same group should be treated together, and be-
cause the values of features in the different groups
should not have a big discrepancy. Weighting each
125
L2 L1 LR AP CW
F 64.2 64.0 64.2 62.7 63.0
AUC 89.1 88.8 89.0 88.5 87.8
Table 2: Classification performance on AIMed us-
ing five different linear classifiers. The F-score (F)
and Area Under the ROC curve (AUC) are shown.
L2 is L2-SVM, L1 is L1-SVM, LR is logistic re-
gression, AP is averaged perceptron, and CW is
confidence weighted linear classification.
group with different values can produce better re-
sults, as will be explored in our future work.
Next, using our feature vector, we applied
five different linear classifiers to extract PPI
from AIMed: L2-SVM, 1-norm soft-margin
SVM (L1-SVM), logistic regression (LR) (Fan
et al, 2008), averaged perceptron (AP) (Collins,
2002), and confidence weighted linear classifica-
tion (CW) (Dredze et al, 2008). Table 2 indicates
the performance of these classifiers on AIMed.
We employed better settings for the task than did
the original methods for AP and CW. We used a
Widrow-Hoff learning rule (Bishop, 1995) for AP,
and we performed one iteration for CW. L2-SVM
is as good as, if not better, than other classifiers (F-
score and AUC). In the least, L2-SVM is as fast as
these classifiers. AP and CW are worse than the
other three methods, because they require a large
number of examples, and are un-suitable for the
current task. This result indicates that all linear
classifiers, with the exception of AP and CW, per-
form almost equally, when using our feature vec-
tor.
Finally, we implemented the kernel method by
Miwa et al (2008). For a 10-fold CV on AIMed,
the running time was 9,507 seconds, and the per-
formance was 61.5% F-score and 87.1% AUC.
Our system used 4,702 seconds, and the perfor-
mance was 64.2% F-score and 89.1% AUC. This
result displayed that our system, with L2-SVM,
and our new feature vector, is better, and faster,
than the kernel-based system.
4.3 Evaluation of Corpus Weighting
In this section, we first apply each model from a
source corpus to a target corpus, to show how dif-
ferent the corpora are. We then evaluate SVM-CW
by comparing it with three other methods (see Sec-
tion 2) with limited features, and apply it to every
corpus.
0
10
20
30
40
50
60
70
80
90
AIMed BioInfer HPRD50 IEPA LLL
F
Target corpus
AIMed
BioInfer
HPRD50
IEPA
LLL
co-occ
Model
Figure 8: F-score on a target corpus using a model
on a source corpus. For the comparison, we show
the 10-fold CV result on each target corpus and
co-occurrences. The regularization parameter was
fixed to 1.
First, we apply the model from a source corpus
to a target corpus. Figure 8 shows how the model
from a source corpus performs on the target cor-
pus. Interestingly, the model from IEPA performs
better on LLL than the model from LLL itself. All
the results showed that using different corpora (ex-
cept IEPA) is worse than just using the same cor-
pora. However, the cross-corpora scores are still
better than the co-occurrences base-line, which in-
dicates that the corpora share some information,
even though they are not fully compatible.
Next, we compare SVM-CW with three other
methods: aSVM, SVD-ASO, and TrAdaBoost.
For this comparison, we used our feature vec-
tor without including the graph features, because
SVD-ASO and TrAdaBoost require large compu-
tational resources. We applied SVD-ASO and
TrAdaBoost in the following way. As for SVD-
ASO, we made 400 auxiliary problems from the
labels of each corpus by splitting features ran-
domly, and extracted 50 additional features each
for 4 feature groups. In total, we made new 200
additional features from 2,000 auxiliary problems.
As recommended by Ando et al (2005), we re-
moved negative weights, performed SVD to each
feature group, and iterated ASO once. Since Ad-
aBoost easily overfitted with our rich feature vec-
tor, we applied soft margins (Ratsch et al, 2001)
to TrAdaBoost. The update parameter for source
examples was calculated using the update param-
eter on the training data in AdaBoost and the orig-
inal parameter in TrAdaBoost. This ensures that
the parameter would be the same as the original
parameter, when the C value in the soft margin ap-
proaches infinity.
126
aSVM SVD-ASO TrAdaBoost SVM-CW L2-SVM
F AUC F AUC F AUC F AUC F AUC
AIMed 63.6 88.4 62.9 88.3 63.4 88.4 64.0 88.6 63.2 88.4
BioInfer 66.5 85.2 65.7 85.1 66.1 85.2 66.7 85.4 66.2 85.1
HPRD50 71.2 84.3 68.7 80.8 72.6 85.3 72.7 86.4 67.2 80.7
IEPA 73.8 85.4 72.3 83.8 74.3 86.3 75.2 85.9 73.0 84.7
LLL 85.9 89.2 79.3 85.5 86.5 88.8 86.9 90.3 80.3 86.3
Table 3: Comparison of methods on multiple corpora. Our feature vector without graph features is used.
The source corpora with the best F-scores are reported for aSVM, TrAdaBoost, and SVM-CW.
F-score AUC
A B H I L all A B H I L all
A (64.2) 64.0 64.7 65.2 63.7 64.2 (89.1) 89.5 89.2 89.3 89.0 89.4
B 67.9 (67.6) 67.9 67.9 67.7 68.3 86.2 (86.1) 86.2 86.3 86.2 86.4
H 71.3 71.2 (69.7) 74.1 70.8 74.9 84.7 85.0 (82.8) 85.0 83.4 87.9
I 74.4 75.6 73.7 (74.4) 74.4 76.6 86.7 87.1 85.4 (85.6) 86.9 87.8
L 83.2 85.9 82.0 86.7 (80.5) 84.1 86.3 87.1 87.4 90.8 (86.0) 86.2
Table 4: F-score and AUC by SVM-CW. Rows correspond to a target corpus, and columns a source
corpus. A:AIMed, B:BioInfer, H:HPRD50, I:IEPA, and L:LLL corpora. ?all? signifies that all source
corpora are used as one source corpus, ignoring the differences among the corpora. For the comparison,
we show the 10-fold CV result on each target corpus.
In Table 3, we demonstrate the results of the
comparison. SVM-CW improved the classifica-
tion performance at least as much as all the other
methods. The improvement is mainly attributed to
the aggressive use of source examples while learn-
ing the model. Some source examples can be used
as training data, as indicated in Figure 8. SVM-
CW does not set the restriction between C
s
and
C
t
in Equation (2), so it can use source exam-
ples aggressively while learning the model. Since
aSVM transfers a model, and SVD-ASO transfers
an additional feature space, aSVM and SVD-ASO
do not use the source examples while learning the
model. In addition to the difference in the data us-
age, the settings of aSVM and SVD-ASO do not
match the current task. As for aSVM, the DA as-
sumption (that the labels are the same) does not
match the task. In SVD-ASO, the numbers of both
source examples and auxiliary problems are much
smaller than those reported by Ando et al (2005).
TrAdaBoost uses the source examples while learn-
ing the model, but never increases the weight of
the examples, and it attempts to reduce their ef-
fects.
Finally, we apply SVM-CW to all corpora using
all features. Table 4 summarizes the F-score and
AUC by SVM-CW with all features. SVM-CW
is especially effective for small corpora, show-
ing that SVM-CW can adapt source corpora to a
small annotated target corpus. The improvement
on AIMed is small compared to the improvement
on BioInfer, even though these corpora are sim-
ilar in size. One of the reasons for this is that
whole abstracts are annotated in AIMed, therefore
making the examples biased. The difference be-
tween L2-SVM and SVM-CW + IEPA on AIMed
is small, but statistically, it is significant (McNe-
mar test (McNemar, 1947), P = 0.0081). In the
cases of HPRD50 + IEPA, LLL + IEPA, and two
folds in BioInfer + IEPA, C
s
is larger than C
t
in
Equation (2). This is worth noting, because the
source corpus is more weighted than the target cor-
pus, and the prediction performance on the tar-
get corpus is improved. Most methods put more
trust in the target corpus than in the source cor-
pus, and our results show that this setting is not al-
ways effective for mixing corpora. The results also
indicate that IEPA contains more useful informa-
tion for extracting PPI than other corpora, and that
using source examples aggressively is important
for these combinations. We compared the results
of L2-SVM and SVM-CW + IEPA on AIMed,
and found that 38 pairs were described as ?inter-
action? or ?binding? in the sentences among 61
127
SVM-CW L2-SVM Airola et al
F AUC F AUC F AUC
A 65.2 89.3 64.2 89.1 56.4 84.8
B 68.3 86.4 67.6 86.1 61.3 81.9
H 74.9 87.9 69.7 82.8 63.4 79.7
I 76.6 87.8 74.4 85.6 75.1 85.1
L 86.7 90.8 80.5 86.0 76.8 83.4
Table 6: Comparison with the results by Airola
et al (2008). A:AIMed, B:BioInfer, H:HPRD50,
I:IEPA, and L:LLL corpora. The results with the
highest F-score from Table 4 are reported as the
results for SVM-CW.
newly found pairs. This analysis is evidence that
IEPA contains instances to help find such inter-
actions, and that SVM-CW helps to collect gold
pairs that lack enough supporting instances in a
single corpus, by adding instances from other cor-
pora. SVM-CW missed coreferential relations that
were also missed by L2-SVM. This can be at-
tributed to the fact that the coreferential informa-
tion is not stored in our current feature vector; so
we need an even more expressive feature space.
This is left as future work.
SVM-CW is effective on most corpus combi-
nations, and all the models from single corpora
can be improved by adding other source corpora.
This result is impressive, because the baselines by
L2-SVM on just single corpora are already better
than or at least comparable to other state-of-the-art
PPI extraction systems, and also because the vari-
ety of the differences among different corpora is
quite wide depending on various factors including
annotation policies of the corpora (Pyysalo et al,
2008). The results suggest that SVM-CW is useful
as an ITL method.
4.4 Comparison with Other PPI Systems
We compare our system with other previously
published PPI extraction systems. Tables 5 and
6 summarize the comparison. Table 5 summa-
rizes the comparison of several PPI extraction sys-
tems evaluated on the AIMed corpus. As indi-
cated, the performance of the heavy kernel method
is lower than our fast rich feature-vector method.
Our system is, to the extent of our knowledge, the
best performing PPI extraction system evaluated
on the AIMed corpus, both in terms of AUC and
F-scores. Airola et al (2008) first reported results
using all five corpora. We cannot directly com-
pare our result with the F-score results, because
they tuned the threshold, but our system still out-
performs the system by Airola et al (2008) on ev-
ery corpus in AUC values. The results also indi-
cate that our system outperforms other systems on
all PPI corpora, and that both the rich feature vec-
tor and the corpus weighting are effective for the
PPI extraction task.
5 Conclusion
In this paper, we proposed a PPI extraction system
with a rich feature vector, using a corpus weight-
ing method (SVM-CW) for combining the mul-
tiple PPI corpora. The feature vector extracts as
much information as possible from the main train-
ing corpus, and SVM-CW incorporate other exter-
nal source corpora in order to improve the perfor-
mance of the classifier on the main target corpus.
To the extent of our knowledge, this is the first ap-
plication of ITL and DA methods to PPI extrac-
tion. As a result, the system, with SVM-CW and
the feature vector, outperformed all other PPI ex-
traction systems on all of the corpora. The PPI
corpora share some information, and it is shown
to be effective to add other source corpora when
working with a specific target corpus.
The main contributions of this paper are: 1)
conducting experiments in extracting PPI using
multiple corpora, 2) suggesting a rich feature
vector using several previously proposed features
and normalization methods, 3) the combination of
SVM with corpus weighting and the new feature
vector improved results on this task compared with
prior work.
There are many differences among the corpora
that we used, and some of the differences are still
unresolved. For further improvement, it would be
necessary to investigate what is shared and what
is different among the corpora. The SVM-CW
method, and the PPI extraction system, can be ap-
plied generally to other classification tasks, and
to other binary relation extraction tasks, without
the need for modification. There are several other
tasks in which many different corpora, which at
first glance seem compatible, exist. By apply-
ing SVM-CW to such corpora, we will analyze
which differences can be resolved by SVM-CW,
and what differences require a manual resolution.
For the PPI extraction system, we found many
false negatives that need to be resolved. For fur-
ther improvement, we need to analyze the cause
128
positive all P R F AUC
SVM-CW 1,000 5,834 60.0 71.9 65.2 89.3
L2-SVM 1,000 5,834 62.7 66.6 64.2 89.1
(Miwa et al, 2008) 1,005 5,648 60.4 69.3 64.2 (61.5) 87.9 (87.1)
(Miyao et al, 2008) 1,059 5,648 54.9 65.5 59.5
(Airola et al, 2008) 1,000 5,834 52.9 61.8 56.4 84.8
(S?tre et al, 2007) 1,068 5,631 64.3 44.1 52.0
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0
(Bunescu and Mooney, 2005) 65.0 46.4 54.2
Table 5: Comparison with previous PPI extraction results on the AIMed corpus. The numbers of positive
and all examples, precision (P), recall (R), F-score (F), and AUC are shown. The result with the highest
F-score from Table 4 is reported as the result for SVM-CW. The scores in the parentheses of Miwa et al
(2008) indicate the result using the same 10-fold splits as our result, as indicated in Section 4.2.
of these false negatives more deeply, and design a
more discriminative feature space. This is left as a
future direction of our work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Scientific Research (C) (General) (MEXT, Japan).
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross corpus learn-
ing. BMC Bioinformatics.
Rie Kubota Ando, Tong Zhang, and Peter Bartlett.
2005. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. Jour-
nal of Machine Learning Research, 6:1817?1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL-X),
pages 77?84, June.
C. M. Bishop. 1995. Neural Networks for Pattern
Recognition. Oxford University Press.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS 2005.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP 2002,
pages 1?8.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong
Yu. 2007. Boosting for transfer learning. In ICML
2007, pages 193?200.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML 2008, pages 264?271.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extract-
ing protein interaction sentences using dependency
parsing. In EMNLP 2007.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear SVM. In ICML 2008, pages 408?415.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19:i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008a. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
129
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Suresh Mathivanan, Balamurugan Periaswamy, TKB
Gandhi, Kumaran Kandasamy, Shubha Suresh, Riaz
Mohmood, YL Ramachandra, and Akhilesh Pandey.
2006. An evaluation of human protein-protein inter-
action data in the public domain. BMC Bioinformat-
ics, 7 Suppl 5:S19.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157, June.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining mul-
tiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining
in Biomedicine (SMBM 2008), pages 101?108.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya
Matsuzaki, and Jun?ichi Tsujii. 2008. Task-
oriented evaluation of syntactic parsers and their
representations. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguistics
(ACL?08:HLT).
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Sinno Jialin Pan and Qiang Yang. 2008. A survey on
transfer learning. Technical Report HKUST-CS08-
08, Department of Computer Science and Engineer-
ing, Hong Kong University of Science and Technol-
ogy, Hong Kong, China, November.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Gunnar Ratsch, Takashi Onoda, and Klaus-Robert
Muller. 2001. Soft margins for adaboost. Machine
Learning, 42(3):287?320.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Syntactic features for protein-protein interaction ex-
traction. In LBM 2007 short papers.
Gabriele Schweikert, Christian Widmer, Bernhard
Scho?lkopf, and Gunnar Ra?tsch. 2008. An empir-
ical analysis of domain adaptation algorithms for
genomic sequence analysis. In NIPS, pages 1433?
1440.
Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007. Cross-domain video concept detection using
adaptive SVMs. In MULTIMEDIA ?07: Proceed-
ings of the 15th international conference on Multi-
media, pages 188?197.
130
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162?1171,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Descriptive and Empirical Approaches to Capturing Underlying
Dependencies among Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we provide descriptive and
empirical approaches to effectively ex-
tracting underlying dependencies among
parsing errors. In the descriptive ap-
proach, we define some combinations of
error patterns and extract them from given
errors. In the empirical approach, on the
other hand, we re-parse a sentence with
a target error corrected and observe er-
rors corrected together. Experiments on
an HPSG parser show that each of these
approaches can clarify the dependencies
among individual errors from each point
of view. Moreover, the comparison be-
tween the results of the two approaches
shows that combining these approaches
can achieve a more detailed error analysis.
1 Introduction
For any kind of technology, analyzing causes of
errors given by a system is a very helpful process
for improving its performance. In recent sophisti-
cated parsing technologies, the process has taken
on more and more important roles since critical
ideas for parsing performance have already been
introduced and the researches are now focusing on
exploring the rest of the pieces for making addi-
tional improvements.
In most cases for parsers? error analysis, re-
searchers associate output errors with failures in
handling certain linguistic phenomena and attempt
to avoid them by adding or modifying correspond-
ing settings of their parsers. However, such an
analysis cannot been done so smoothly since pars-
ing errors sometimes depend on each other and the
underlying dependencies behind superficial phe-
nomena cannot be captured easily.
In this paper, we propose descriptive and em-
pirical approaches to effective extraction of de-
pendencies among parsing errors and engage in a
deeper error analysis with them. In our descriptive
approach, we define various combinations of error
patterns as organized error phenomena on the ba-
sis of linguistic knowledge, and then extract such
combinations from given errors. In our empirical
approach, on the other and, we re-parse a sentence
under the condition where a target error is cor-
rected, and errors which are additionally corrected
are regarded as dependent errors. By capturing de-
pendencies among parsing errors through system-
atic approaches, we can effectively collect errors
which are related to the same linguistic properties.
In the experiments, we applied both of our ap-
proaches to an HPSG parser Enju (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), and then evalu-
ated the obtained error classes. After examining
the individual approaches, we explored the com-
bination of them.
2 Parser and its evaluation
A parser is a system which interprets structures
of given sentences from some grammatical or in
some cases semantical viewpoints, and interpreted
structures are utilized as essential information for
various natural language tasks such as informa-
tion extraction, machine translation, and so on.
In most cases, an output structure of a parser is
based on a certain grammatical framework such as
CFG, CCG (Steedman, 2000), LFG (Kaplan and
Bresnan, 1995) or HPSG (Pollard and Sag, 1994).
Since such a framework can usually produce more
than one probable structure for a sentence, a parser
1162
John aux_arg12
ARG1 ARG2
verb_arg1
ARG1
has : come :
Figure 1: Predicate argument relations
Abbr. Full Abbr. Full
aux auxiliary lgs logical subject
verb verb coord coordination
prep prepositional conj conjunction
det determiner argN
1
... take argument(s)
adj adjunction (N
1
th, ...)
app apposition mod modify a word
relative relative
Table 1: Descriptions for predicate types
often utilizes some kind of disambiguation model
for choosing the best one.
While various parsers take different manners
in capturing linguistic phenomena based on their
frameworks, they are at least required to obtain
some kinds of relations between the words in sen-
tences. On the basis of the requirements, a parser
is usually evaluated on how correctly it gives in-
tended linguistic relations. ?Predicate argument
relation? is one of the most common evaluation
measurements for a parser since it is a very fun-
damental linguistic behavior and is less dependent
on parser systems. This measure divides linguis-
tic structural phenomena in a sentence into min-
imal predicative events. In one predicate argu-
ment relation, a word which represents an event
(predicate) takes some words as participants (argu-
ments). Although no fixed formulation exists for
the relations, there are to a large extent common
conceptions for them based on linguistic knowl-
edge among researchers.
Figure 1 shows an example of predicate argu-
ment relations given by Enju. In the sentence
?John has come.?, ?has? is a predicate of type
?aux arg12? and takes ?John? and ?come? as the
first and second arguments. ?come? is also a pred-
icate of the type ?verb arg1? and takes ?John? as
the first and the only argument. In this formalism,
each predicate type is represented as a combina-
tion of ?the grammatical nature of a word? and
?the arguments which it takes,? which are repre-
sented by the descriptions in Table 1. ?aux arg12?
in Figure 1 indicates that it is an auxiliary word
and takes two arguments ?ARG1? and ?ARG2.?
In order to improve the performance of a parser,
analyzing parsing errors is very much worth the
I watched the girl on TV Correct answer:
ARG1 ARG2
ARG1 ARG2
I watched the girl on TV Parser output:
ARG1 ARG2
ARG1 ARG2
Obtain inconsistent outputs as errors
Error: I watched the girl on TV 
ARG1
ARG1 Error
Figure 2: An example of parsing errors
Error: The book on which read the shelf  I yesterdayARG1
ARG2
ARG2ARG1Figure 3: Co-occurring parsing errors
effort. Since the errors are output according to
a given evaluation measurement such as ?predi-
cate argument relation,? we researchers carefully
explore them and infer the linguistic phenom-
ena which cause the erroneous outputs. Figure 2
shows an example of parsing errors for sentence ?I
watched the girl on TV.? Note that the errors are
based on predicate argument relations as shown
above and that the predicate types are abbreviated
in this figure. When we focus on the error output,
we can observe that ?ARG1? of predicate ?on?
was mistaken by the parser. In this case, ?ARG1?
represents a modifiee of the preposition, and we
then conclude that the ill attachment of a prepo-
sitional phrase caused this error. By continuing
such error analysis, weak points of the parser are
revealed and can be useful clues for further im-
provements.
However, in most researches on parsing tech-
nologies, error analysis has been limited to narrow
and shallow explorations since there are various
dependencies behind erroneous outputs. In Fig-
ure 3, for example, two errors were given: wrong
outputs for ?ARG1? of ?which? and ?ARG2? of
?read.? Both of these two errors originated from
the fact that the relative clause took a wrong an-
tecedent ?the shelf.? In this sentence, the former
1163
Error:
ARG1ARG1
They completed the sale of for 
ARG1
ARG1
it to him $1,000 
Confliction
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 2: (Impossible)
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 1: (Possible)
Can each error occur independently?
ARG1
ARG1
ARG1 ARG1
Figure 4: Sketch of error propagation
?ARG1? directly corresponds to the antecedent
while the latter ?ARG2? indirectly referred to the
same antecedent as the object of the verb ?read.?
The two predicate argument relations thus took the
same word as their common arguments, and there-
fore the two errors co-occurred.
On the other hand, one-way inductive relations
also exist among errors. In Figure 4, ?ARG1? of
?for? and ?to? were mistaken by a parser. We can
know that each of the errors was caused by an ill
attachment of a prepositional phrase with the same
analysis as shown in Figure 2. What is important
in this example is the manner in their occurrences.
The former error can appear by itself (Analysis 1)
while the latter cannot because of the structural
conflict with the former error (Analysis 2). The
appearance of the latter error thus induces that of
the former error. In error analysis, we have to cor-
rectly capture such various relations, which leads
us to a costly and less rewarding analysis.
In order to make advancements on this prob-
lem, we propose two types of approaches to real-
izing a deeper error analysis on parsing. In the ex-
periments, we examine our approaches for actual
errors which are given by the HPSG parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006).
Enju was developed for capturing detailed syntac-
tic or semantic properties and relations for a sen-
tence with an HPSG framework (Pollard and Sag,
1994). In this research, we focus on error analysis
based on predicate argument relations, and in the
experiments with Enju, utilize the relations which
Erroneous phenomena Matched patterns
[Argument selection]
Prepositional attachment ARG1 of prep arg
Adjunction attachment ARG1 of adj arg
Conjunction attachment ARG1 of conj arg
Head selection for ARG1 of det arg
noun phrase
Coordination ARG1/2 of coord arg
[Predicate type selection]
Preposition/Adjunction prep arg / adj arg
Gerund acts as modifier/not verb mod arg / verb arg
Coordination/conjunction coord arg / conj arg
# of arguments prep argX / prep argY
for preposition (X 6= Y )
Adjunction/adjunctive noun adj arg / noun arg
[More structural errors]
To-infinitive for see Figure 7
modifier/argument of verb
Subject for passive sentence see Figure 8
or not
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Patterns defined for descriptive approach
are represented in parsed tree structures.
3 Two approaches for error analysis
In this section, we propose two approaches for er-
ror analysis which enable us to capture underlying
dependencies among parsing errors. Our descrip-
tive approach matches the patterns of error com-
binations with given parsing errors and collects
matched erroneous participants. Our empirical ap-
proach, on the other hand, detects co-occurring
errors by re-parsing a sentence under a situation
where each of the errors is forcibly corrected.
3.1 Descriptive approach
Our descriptive approach for capturing dependen-
cies among parsing errors is to extract certain rep-
resentative structures of errors and collect the er-
rors which involve them. Parsing errors have a ten-
dency to occur with certain patterns of structures
representing linguistic phenomena. We first define
such patterns through observations with a part of
error outputs, and then match them with the rest.
Table 2 summarizes the patterns for erroneous
phenomena which we defined for matching in
the experiments. In the table, the patterns for
14 phenomena are given and classified into four
types according to their matching manners. Each
of the patterns for ?Argument selection? examine
whether a focused argument for a certain predi-
cate type is erroneous or not. Figure 5 shows the
pattern for ?Prepositional attachment,? which col-
1164
prep_arg
ARG1 Error
Parser output: ?
They completed the sale of for :
ARG1ARG1
it to : him $1,000 
Pattern:
prep_arg12 prep_arg12Correct output:
ARG1ARG1
They completed the sale of for :it to : him $1,000 prep_arg12 prep_arg12Parser output:
Example:
Figure 5: Pattern for ?Prepositional attachment?
gerund:     verb_argParser output: gerund: verb_mod_argCorrect answer:
(Patterns of correct answer and parser output can be interchanged)Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_arg123 you to havein ARG1
MOD ARG2
ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting:     verb_arg123 you to haveinNot exist 
ARG2
ARG3
ARG1
(MOD)
Figure 6: Pattern for ?Gerund acts as modifier or
not?
lects wrong ARG1 for predicate type ?prep arg?.
From the sentence in the figure, we can obtain
two errors for ?Prepositional attachment? around
prepositions ?to? and ?for.? On the other hand,
each ?Predicate type selection? pattern collects er-
rors around a word whose predicate type is erro-
neous. Figure 6 shows the pattern for ?Gerund
acts as modifier or not,? which collects errors
around gerunds whose predicate types are erro-
neous. From the example sentence in the figure,
we can obtain an erroneous predicate type for ?ex-
pecting? and collect errors around it for ?Gerund
acts as modifier or not.?
We can implement more structural errors than
simple argument or predicate type selections. Fig-
ures 7 and 8 show the patterns for ?To-infinitive
for modifier/argument of verb? and ?Subject for
passive sentence or not? respectively. The pat-
tern for the latter phenomenon collects errors on
recognitions of prepositional phrases which be-
have as subjects for passive expressions. The pat-
tern collects errors not only around prepositions
but also around the verbs which take the preposi-
Parser output: aux_arg12to :verb1 ?ARG3 verb2
Correct output: aux_mod_arg12
MOD
to :
ARG2
Unknown subject ARG1 ARG1
verb1 ? verb2
The  figures  ? were  adjusted to : remove ...aux_arg12
Example:
Parser output:
Correct answer:
ARG3
The  figures  ? were  adjusted to : remove ...aux_mod_arg12 
MOD ARG2
Unknown subject ARG1 ARG1
Pattern: (Patterns of correct answer and parser output can be interchanged)
Figure 7: Pattern for ?To-infinitive for modi-
fier/argument of verb?
Example:
Pattern:
Parser output: prep_arg12Unknown subject verb1 ?ARG1ARG1 ?
Correct output: lgs_arg2 ARG2verb1 ? ?ARG1
A  50-state  study  released in  September  by : Friends  ?
Unknown subject ARG1ARG1 prep_arg12Parser output:
Correct answer: A  50-state  study  released in  September  by : Friends  ?ARG1ARG2 lgs_arg12ARG2
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
tional phrases as a subject.
Since these patterns are based on linguistic
knowledge given by a human, the process could
provide a relatively precise analysis with a lower
cost than a totally manual analysis.
3.2 Empirical approach
Our empirical approach, on the other hand, briefly
traces the parsing process which results in each of
the target errors. We collect co-occurring errors
as strongly relevant ones, and then extract depen-
dencies among the obtained groups. Parsing errors
could originate from wrong processing at certain
stages in the parsing, and errors with a common
origin would by necessity appear together. We re-
parse a target sentence under the condition where a
certain error is forcibly corrected and then collect
errors which are corrected together as the ?rela-
tive? ones. An error group where all errors are
relative to each other can be regarded as a ?co-
occurring error group.? Errors in the same co-
1165
Example:
Pattern:
relative_arg1
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The book on relative_arg1 read ARG2the shelf  I yesterdayARG1
ARG2
ARG1
which :
The book on relative_arg1 read the shelf  I yesterdaywhich :
Figure 9: Pattern for ?Relative clause attachment?
our work force
Error 1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
Error 1
Error 2
Error 3
Correct Error 2
Error 1
Error 1
Extract co-occurring error groups and inductive relations 
Error 4 Error 1
Error 4Error 3
Error 3Correct
Correct
Correct
corrected together
corrected together
corrected together
corrected together
,
,
,
Error 1 Error 2 Error 3 Error 4
today
ARG1
Correct answer:
It    has    no    bearing on
our work force todayonParser output: ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1
It    has    no    bearing
Error 2 Error 3 Error 4 Error 5
Error 5 Error 4Correct corrected togetherError 1 Error 3Error 2, , ,
Error 4,
Error 2 Error 4,,
Error 2 Error 3,,
Error 5Induce
Co-occurring error group Co-occurring error group
Figure 10: An image of our empirical approach
occurring error group are expected to participate
in the same phenomenon. Dependencies among
errors are then expected to be summarized with in-
ductions among co-occurring error groups.
Figure 10 shows an image of this approach. In
this example, ?today? should modify noun phrase
?our work force? while the parser decided that ?to-
day? was also in the noun phrase. As a result, there
are five errors: three wrong outputs for ?ARG2?
of ?on? (Error 1) and ?ARG1? of ?our? (Error 2)
and ?work? (Error 3), excess relation ?ARG1? of
?force? (Error 4), and missing relation ?ARG1? for
?today? (Error 5). By correcting each of the errors
1, 2, 3 and 4, all of these errors are corrected to-
gether, and therefore classified into the same co-
occurring error group. Although error 5 cannot
participate in the group, correcting error 5 can cor-
rect all of the errors in the group, and therefore an
# ofError types Errors Patterns
? Analyzed 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier or not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence 8 3
or not
[Others]
Comma 444 372
Relative clause attachment 102 38
? Unanalyzed 2,631 ?
Total 4,709 ?
Table 3: Errors extracted with descriptive analysis
inductive relation is given from error 5 to the co-
occurring error group. We can then finally obtain
the inductive relations as shown at the bottom of
Figure 10. This approach can trace the actual be-
havior of the parser precisely, and can therefore
capture underlying dependencies which cannot be
found only by observing error outputs.
4 Experiments
We applied our approaches to parsing errors given
by the HPSG parser Enju, which was trained on
the Penn Treebank (Marcus et al, 1994) section
2-21. We first examined each approach, and then
explored the combination of the approaches.
4.1 Evaluation of descriptive approach
We examined our descriptive approach. We first
parsed sentences in the Penn Treebank section 22
with Enju, and then observed the errors. Based on
the observation, we next described the patterns as
shown in Section 3. After that, we parsed section
0 and then applied the patterns to the errors.
Table 3 summarizes the extracted errors. As the
table shows, with the 14 error patterns, we suc-
cessfully matched 1,671 locations in error outputs
and covered 2,078 of 4,709 errors, which com-
prised of more than 40% of the total errors. This
was the first step of the application of our ap-
proach, and in the future work we would like to
1166
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Co-occurring errors 1,978
Extracted inductive relations 501
F-score (LP/LR) 90.69 (90.78/93.59)
Table 4: Summary of our empirical approach




       	 
 










Figure 11: Frequency of each size of co-occurring
error group
add more patterns for capturing more phenomena.
When we focused on individual patterns, we
could observe that the simple error phenomena
such as the attachments were dominant. The first
reason for this would be that such phenomena
were among minimal linguistic events. This would
make the phenomena components of other more
complex ones. The second reason for the dom-
inance would be that the patterns for these error
phenomena were easy to implement only with ar-
gument inconsistencies, and only one or a few pat-
terns could cover every probable error. Among
these dominant error types, the number of prepo-
sitional attachments was outstanding. The er-
ror types which required matching with predicate
types were fewer than the attachment errors since
the limited patterns on the predicate types would
narrow the possible linguistic behavior of the can-
didate words. When we focus on more structural
errors, the table shows that the rates of the partici-
pant errors to matched locations were much larger
than those for simpler pattern errors. Once our pat-
terns matches, they could collect many errors at
the same time.
4.2 Evaluation of empirical approach
Next, we applied our empirical approach in the
same settings as in the previous section. We first
parsed sentences in section 0 and then applied our
approach to the obtained errors. In the experi-
ments, some errors could not be forcibly corrected
by our approach. The parser ?cut off? less proba-
ble parse substructures before giving the predicate
Sentence: The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once  it  enters the    
lungs  ,  with  even  brief  exposures  to  it  causing  symptoms  that  show  up  decades  later
,  researchers  said
(a)(b)
(c) (d)
(a) fiber      , : crocidoliteapp_arg12
fiber      , : crocidolitecoord_arg12
Correct answer:
Parser output:
is     usually     resilient     ? the     lungs        ,        with(b)
symptoms    that     show : up    decades    later(c)
Parser output:
Correct answer: verb_arg1
symptoms    that     show : up    decades    laterverb_arg12
(d)
ARG1 ARG2
ARG1 ARG2
ARG1 ARG1
ARG1 ARG2
ARG1 ARG1
Correct answer:
Parser output: is     usually     resilient     ? the     lungs        ,        withARG1 ARG1
Correct answer:
Parser output:
It    causing    symptoms    that    show    up    decades    laterARG1
It    causing    symptoms    that    show    up    decades    laterARG1
Figure 12: Obtained co-occurring error groups
argument relation for reducing the cost of parsing.
In this research, we ignored the errors which were
subject to such ?cut off? as ?uncorrectable? ones,
and focused only on the remaining ?correctable?
errors. In our future work, we would like to con-
sider the ?uncorrectable? errors.
Table 4 shows the summary of the analysis with
our approach. Enju gave 4,709 errors for section
0. Among these errors, the correctable errors were
3,085, and from these errors, we successfully ob-
tained 1,978 co-occurring error groups and 501 in-
ductive relations. Figure 11 shows the frequency
for each size of co-occurring groups. About a half
of the groups contains only single errors, which
would indicate that the errors could have only one-
way inductive relations with other errors. The rest
of this section explores examples of the obtained
co-occurring error groups and inductive relations.
Figure 12 shows an example of the extracted co-
occurring error groups. For the sentence shown at
the top of the figure, Enju gave seven errors. By
introducing our empirical approach, these errors
were definitely classified into four co-occurring er-
ror groups (a) to (d), and there were no inductive
relations detected among them. Group (a) contains
two errors on the comma?s local behavior as ap-
position or coordination. Group (b) contains the
errors on the words which gave almost the same
attachment behaviors. Group (c) contains the er-
rors on whether the verb ?show? took ?decades?
1167
Error types # of correctable errors # of independent errors Correction effect (errors)
[Argument selection]
Prepositional attachment 531 397 766
Adjunction attachment 196 111 352
Conjunction attachment 33 12 79
Head selection for noun phrase 22 0 84
Coordination 146 62 323
[Predicate type selection]
Preposition/Adjunction 72 30 114
Gerund acts as modifier or not 39 18 62
Coordination/conjunction 36 16 61
# of arguments for preposition 24 23 26
Adjunction/adjunctive noun 8 6 10
[More structural errors]
To-infinitive for 75 27 87
modifier/argument of verb
Subject for passive sentence or not 8 3 9
[Others]
Comma 372 147 723
Relative clause attachment 84 27 119
Total 1,646 979 ?
Table 5: Induction relations between errors for each linguistic phenomenon and other errors
Sentence: She  says  she  offered  Mrs.  Yeargin a  quiet  resignation
and  thought  she  could  help  save  her  teaching  certificate(a) (b)
Correcting (a) induced correcting (b)
(b) Correct answer:
Parser output:
? thought  she  could  help   save : her  teaching  certificateverb_arg123
? thought  she  could  help   save : her  teaching  certificateverb_arg12
ARG1 ARG2
ARG1
ARG1 ARG2 ARG3
(a) Correct answer:
Parser output:
? thought   she   could     help : save   her   teaching   certificateverb_arg12
? thought   she   could     help : save   her   teaching   certificateaux_arg12
ARG1 ARG2
ARG2 ARG2
ARG1 ARG2
ARG2ARG2
Figure 13: Inductive relation between obtained co-
occurring error groups
as its object or not. Group (d) contains an error on
the attachment of the adverb ?later?. Regardless
of the overlap of the regions in the sentence for
(c) and (d), our approach successfully classified
the errors into the two independent groups. With
our approach, it would be empirically shown that
the errors in each group actually co-occurred and
the group was independent. This would enable us
to concentrate on each of the co-occurring error
groups without paying attention to the influences
from the errors in other groups.
Figure 13 shows another example of the anal-
ysis with our empirical approach. In this case, 8
errors for a sentence were classified into two co-
occurring error groups (a) and (b), and our ap-
proach showed that correction in group (a) re-
sulted in correcting group (b) together. The errors
in group (a) were on whether ?help? behaved as an
auxiliary or pure verbal role. The errors in group
(b) were on whether ?save? took only one object
?her teaching certificate,? or two objects ?her? and
?teaching certificate.? Between group (a) and (b),
no ?structural? conflict could be found when cor-
recting only each of the groups. We could then
guess that the inductive relation between these two
groups was implicitly given by the disambigua-
tion model of the parser. By dividing the errors
into minimum units and clarifying the effects of
correcting a target error, error analysis with our
empirical approach could suggest some policy for
parser improvements.
4.3 Combination of two approaches
On the basis of the experiments shown in the pre-
vious sections, we would like to explore possibili-
ties for obtaining a more detailed analysis by com-
bining the two approaches.
4.3.1 Interactions between a target linguistic
phenomenon and other errors
Our descriptive approach could classify the pars-
ing errors according to the linguistic phenomena
they participated in. We then attempt to reveal how
such classified errors interacted with other errors
from the viewpoints of our empirical approach. In
order to enable the analysis by our empirical ap-
proach, we focused only on the correctable errors.
1168
Sentence: It  invests  heavily  in  dollar-denominated  securities  overseas  and  is
currently  waiving  management  fees  ,  which  boosts  its  yield (a)(b)(a)
It  invests  heavily  in  dollar-denominated  securities    overseas :adj_arg1
?Adjunction attachment?
ARG1
ARG1
Pattern matched: 
is  currently  waiving  management  fees              ,         which           boosts   its  yield
(b)
?Comma? , ?Relative clause attachment?Pattern matched: 
ARG1ARG1ARG1
ARG1ARG1ARG1
Error:
Error:
Figure 14: Combination of results given by de-
scriptive and empirical approaches (1)
Table 5 reports the degree to which the classi-
fied errors were related to other individual errors.
The leftmost numbers show the numbers of cor-
rectable errors, which were the focused errors in
the experiments. The central numbers show the
numbers of ?independent? errors, that is, the errors
which could be corrected only by correcting them-
selves. The rightmost numbers show ?correction
effects,? that is, the number of errors which would
consequently be corrected if all of the errors for
the focused phenomena were forcibly corrected.
?Independent? errors are obtained by collecting
error phenomena groups which consist of unions
of co-occurring error groups and each error in
which is not induced by other errors. Figure 14
shows an example of ?independent? errors. For
the sentence at the top of the figure, the parser had
four errors on ARG1 of ?overseas,? the comma,
?which? and ?boosts.? Our empirical approach
then classified these errors into two co-occurring
error groups (a) and (b), and there was no induc-
tive relation between the groups. Our descrip-
tive approach, on the other hand, matched all of
the errors with the patterns for ?Adjunction at-
tachment,? ?Comma? and ?Relative clause attach-
ment.? Since the error for the ?Adjunction attach-
ment? equals to a co-occurring group (a) and is not
induced by other errors, the error is ?independent.?
Table 5 shows that, for ?Prepositional attach-
ment?, ?Adjunction attachments,? ?# of argu-
ments for preposition? and ?Adjunction/adjunctive
noun,? more than half of the errors for the focused
phenomena are ?independent.? Containing many
?independent? errors would mean that the parser
should handle these phenomena further more in-
tensively as an independent event.
Sentence: Clark  J.  Vitulli was  named  senior  vice  president  and  general  manager 
of  this  U.S.  sales  and  marketing  arm  of  Japanese  auto  Maker  Mazda  Motor  Corp(b) (a)
(b)
(a)
senior  vice  president  and  general  manager  of  this  U.S.  sales   and :coord_arg12
?Coordination? (fragment)
ARG1
ARG1
Pattern matched: 
Correcting (a) induced correcting (b)
manager   of     this : U.S.   sales    and : marketing  arm  of
?Coordination? (fragment),
?Head selection of noun phrase?Pattern matched: 
det_arg1 coord_arg12
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Error:
Error:
Figure 15: Combination of results given by de-
scriptive and empirical approaches (2)
The ?correction effect? for a focused linguistic
phenomenon can be obtained by counting errors in
the union of the correctable error set for the phe-
nomenon and the error sets which were induced by
the individual errors in the set. We would show an
example of correction effect in Figure 15. In the
figure, the parser had six errors for the sentence
at the top: three false outputs for ARG1 of ?and,?
?this? and ?U.S.,? two false outputs for ARG2 of
?of? and ?and,? and missing output for ARG1 of
?sales.? Our empirical approach classified these
errors into two co-occurring error groups (a) and
(b), and extracted an inductive relation from (a) to
(b). Our descriptive approach, on the other hand,
matched two errors on ?and? with pattern ?Coor-
dination? and one error on ?this? with ?Head se-
lection for noun phrase.? When we focus on the
error for ?Head selection of noun phrase? in co-
occurring group (a), the correction of the error in-
duced the rest of the errors in (a), and further in-
duced the error in (b) according to the inductive
relation from (a) to (b). Therefore, a ?correction
effect? for the error results in six errors.
Table 5 shows that, for ?Conjunction attach-
ment,? ?Head selection for noun phrase? and ?Co-
ordination,? each ?correction effect? results in
more than twice the forcibly corrected errors. Im-
proving the parser so that it can resolve such high-
correction-effect erroneous phenomena may ad-
ditionally improve the parsing performances to a
great extent. On the other hand, ?Head selection
for noun phrase? contains no ?independent? error,
and therefore could not be handled independently
of other erroneous phenomena at all. Consider-
1169
ing the effects from outer events might make the
treatment of ?Head selection for noun phrase? a
more complicated process than other phenomena,
regardless of its high ?correction effect.?
Table 5 would thus suggest which phenomenon
we should resolve preferentially from the three
points of view: the number of errors, the number
of ?independent? errors and its ?correction effect.?
Considering these points, ?Prepositional attach-
ment? seems most preferable for handling first.
4.3.2 Possibilities for further analysis
Since the errors for the phenomenon were system-
atically collected with our descriptive approach,
we can work on further focused error analyses
which would answer such questions as ?Which
preposition causes most errors in attachments??,
?Which pair of a correct answer and an erroneous
output for predicate argument relations can occur
most frequently??, and so on. Our descriptive ap-
proach would enable us to thoroughly obtain such
analyses with more closely-defined patterns. In
addition, our empirical approach would clarify the
influences of the obtained error properties on the
parser?s behaviors. The results of the focused anal-
yses might reasonably lead us to the features that
can be captured as parameters for model training,
or policies for re-ranking the parse candidates.
The combination of our approaches would give
us interesting clues for planning effective strate-
gies for improving the parser. Our challenges for
combining the two approaches are now in the pre-
liminary stage and there would be many possibili-
ties for further detailed analysis.
5 Related work
Although there have been many researches which
analyzed errors on their own systems in the part of
the experiments, there have been few researches
which focused mainly on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They observed
the accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded for one step in this point, and attempted
to reveal the way of the propagations. In exam-
ining the combination of the two types of pars-
ing, McDonald and Nivre (2007) utilized similar
approaches to our empirical analysis. They al-
lowed a parser to give only structures given by
the parsers. They implemented the ideas for eval-
uating the parser?s potentials whereas we imple-
mented the ideas for observing error propagations.
Dredze et al (2007) showed the possibility
that many parsing errors in the domain adaptation
tasks came from inconsistencies between annota-
tion manners of training resources. Such findings
would further suggest that, comparing given errors
without considering the inconsistencies could lead
to the misunderstanding of what occurs in domain
transitions. The summarized error dependencies
given by our approaches would be useful clues for
extracting such domain-dependent error phenom-
ena.
Gime?nez and Ma`rquez (2008) proposed an au-
tomatic error analysis approach in machine trans-
lation (MT) technologies. They were developing
a metric set which could capture features in MT
outputs at different linguistic levels with different
levels of granularity. As we considered the parsing
systems, they explored the way to resolve costly
and non-rewarding error analysis in the MT field.
One of their objectives was to enable researchers
to easily access detailed linguistic reports on their
systems and to concentrate only on analyses for
the system improvements. From this point of view,
our research might provide an introduction into
such rewarding analysis in parsing.
6 Conclusions
We proposed empirical and descriptive approaches
to extracting dependencies among parsing errors.
In the experiments, with each of our approaches,
we successfully obtained relevant errors. More-
over, the possibility was shown that the combina-
tion of our approaches would give a more detailed
error analysis which would bring us useful clues
for parser improvements.
In our future work, we will improve the per-
formance of our approaches by adding more pat-
terns for the descriptive approach and by handling
uncorrectable errors for the empirical approach.
With the obtained robust information, we will ex-
plore rewarding ways for parser improvements.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
1170
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards
heterogeneous automatic MT error analysis. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
pages 1894?1901.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
1171
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328?1337,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes
Yusuke Miyao
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
University of Tokyo
University of Manchester
National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
The work presented in this paper explores
a supervised method for learning a prob-
abilistic model of a lexicon of VerbNet
classes. We intend for the probabilis-
tic model to provide a probability dis-
tribution of verb-class associations, over
known and unknown verbs, including pol-
ysemous words. In our approach, train-
ing instances are obtained from an ex-
isting lexicon and/or from an annotated
corpus, while the features, which repre-
sent syntactic frames, semantic similarity,
and selectional preferences, are extracted
from unannotated corpora. Our model
is evaluated in type-level verb classifica-
tion tasks: we measure the prediction ac-
curacy of VerbNet classes for unknown
verbs, and also measure the dissimilarity
between the learned and observed proba-
bility distributions. We empirically com-
pare several settings for model learning,
while we vary the use of features, source
corpora for feature extraction, and disam-
biguated corpora. In the task of verb clas-
sification into all VerbNet classes, our best
model achieved a 10.69% error reduction
in the classification accuracy, over the pre-
viously proposed model.
1 Introduction
Lexicons are invaluable resources for semantic
processing. In many cases, lexicons are neces-
sary to restrict a set of semantic classes to be as-
signed to a word. In fact, a considerable number of
works on semantic processing implicitly or explic-
itly presupposes the availability of a lexicon, such
as in word sense disambiguation (WSD) (Mc-
Carthy et al, 2004), and in token-level verb class
disambiguation (Lapata and Brew, 2004; Girju et
al., 2005; Li and Brew, 2007; Abend et al, 2008).
In other words, those methods are heavily de-
pendent on the availability of a semantic lexicon.
Therefore, recent research efforts have invested in
developing semantic resources, such as WordNet
(Fellbaum, 1998), FrameNet (Baker et al, 1998),
and VerbNet (Kipper et al, 2000; Kipper-Schuler,
2005), which greatly advanced research in seman-
tic processing. However, the construction of such
resources is expensive, and it is unrealistic to pre-
suppose the availability of full-coverage lexicons;
this is the case because unknown words always ap-
pear in real texts, and word-semantics associations
may vary (Abend et al, 2008).
This paper explores a method for the supervised
learning of a probabilistic model for the VerbNet
lexicon. We target the automatic classification of
arbitrary verbs, including polysemous verbs, into
all VerbNet classes; further, we target the esti-
mation of a probabilistic model, which represents
the saliences of verb-class associations for polyse-
mous verbs. In our approach, an existing lexicon
and/or an annotated corpus are used as the training
data. Since VerbNet classes are designed to rep-
resent the distinctions in the syntactic frames that
verbs can take, features, representing the statistics
of syntactic frames, are extracted from the unan-
notated corpora. Additionally, as the classes rep-
resent semantic commonalities, semantically in-
spired features, like distributionally similar words,
are used. These features can be considered as a
generalized representation of verbs, and we ex-
pect that the obtained probabilistic model predicts
VerbNet classes of the unknown words.
Our model is evaluated in two tasks of type-
level verb classification: one is the classification
of monosemous verbs into a small subset of the
classes, which was studied in some previous works
(Joanis and Stevenson, 2003; Joanis et al, 2008).
The other task is the classification of all verbs into
the full set of VerbNet classes, which has not yet
1328
been attempted. In the experiments, training in-
stances are obtained from VerbNet and/or Sem-
Link (Loper et al, 2007), while features are ex-
tracted from the British National Corpus or from
Wall Street Journal. We empirically compare sev-
eral settings for model learning by varying the
set of features, the source domain and the size
of a corpus for feature extraction, and the use of
the token-level statistics obtained from a manually
disambiguated corpus. We also provide the anal-
ysis of the remaining errors, which will lead us to
further improve the supervised learning of a prob-
abilistic semantic lexicon.
Supervised methods for automatic verb classifi-
cation have been extensively investigated (Steven-
son et al, 1999; Stevenson and Merlo, 1999;
Merlo and Stevenson, 2001; Stevenson and Joa-
nis, 2003; Joanis and Stevenson, 2003; Joanis et
al., 2008). However, their focus has been lim-
ited to a small subset of verb classes, and a lim-
ited number of monosemous verbs. The main con-
tributions of the present work are: i) to provide
empirical results for the automatic classification
of all verbs, including polysemous ones, into all
VerbNet classes, and ii) to empirically explore the
effective settings for the supervised learning of a
probabilistic lexicon of verb semantic classes.
2 Background
2.1 Verb lexicon
Levin?s (1993) work on verb classification has
broadened the field of computational research that
concerns the relationships between the syntactic
and semantic structures of verbs. The principal
idea behind the work is that the meanings of verbs
can be identified by observing possible syntactic
frames that the verbs can take. In other words,
with the knowledge of syntactic frames, verbs can
be semantically classified. This idea provided the
computational linguistics community with crite-
ria for the definition and the classification of verb
semantics; it has subsequently resulted in the re-
search of the induction of verb classes (Korhonen
and Briscoe, 2004), and the construction of a verb
lexicon based on Levin?s criteria.
VerbNet (Kipper et al, 2000; Kipper-Schuler,
2005) is a lexicon of verbs organized into classes
that share the same syntactic behaviors and seman-
tics. The design of classes originates from Levin
(1993), though the design has been considerably
reorganized and extends beyond the original clas-
43 Emission
43.1 Light Emission
beam, glow, sparkle, . . .
43.2 Sound Emission
blare, chime, jangle, . . .
. . .
44 Destroy
annihilate, destroy, ravage, . . .
45 Change of State
. . .
47 Existence
47.1 Exist
exist, persist, remain, . . .
47.2 Entity-Specific Modes Being
bloom, breathe, foam, . . .
47.3 Modes of Being with Motion
jiggle, sway, waft, . . .
. . .
Figure 1: VerbNet classes
43.2 Sound Emission
Theme V
Theme V P:loc Location
P:loc Location V Theme
there V Theme P:loc Location
Agent V Theme
Theme V Oblique
Location V with Theme
47.3 Modes of Being with Motion
Theme V
Theme V P:loc Location
P:loc Location V Theme
there V Theme
Agent V Theme
Figure 2: Syntactic frames for VerbNet classes
sification. The classes therefore cover more En-
glish verbs, and the classification should be more
consistent (Korhonen and Briscoe, 2004; Kipper
et al, 2006).
The current version of VerbNet includes 270
classes.1 Figure 1 shows a part of the classes of
VerbNet. The top-level categories, e.g. Emis-
sion and Destroy, represent a coarse classifica-
tion of verb semantics. They are further classi-
fied into verb classes, each of which expresses
a group of verbs sharing syntactic frames. Fig-
ure 2 shows an excerpt from VerbNet, which rep-
resents the possible syntactic frames for the Sound
Emission class, including ?chime? and ?jangle,?
and the Modes of Being with Motion class, in-
cluding ?jiggle? and ?waft.? In this figure, each
line represents a syntactic frame, where Agent,
1Throughout this paper, we refer to VerbNet 2.3. Sub-
classes are ignored in this work, following the setting of
Abend et al (2008).
1329
. . . the walls still shook;VN=47.3 and an evacuation
alarm blared;VN=43.2 outside.
Suddenly the woman begins;VN=55.1 swaying
;VN=47.3 and then . . .
Figure 3: An excerpt from SemLink
Theme, and Location indicate the thematic
roles, V denotes a verb, and P specifies a prepo-
sition. P:loc defines locative prepositions such
as: ?in? and ?at.? For example, the second syn-
tactic frame of Sound Emission, i.e., Theme V
P:loc Location, corresponds to the follow-
ing sentence:
1. The coins jangled in my pocket.
Theme corresponds to ?the coins,? V to ?jangled,?
P:loc to ?in,? and Location to ?my pocket.?
While VerbNet provides associations between
verbs and semantic classes, SemLink (Loper et
al., 2007) additionally provides mappings among
VerbNet, FrameNet (Baker et al, 1998), PropBank
(Palmer et al, 2005), and WordNet (Fellbaum,
1998). Since FrameNet and PropBank include an-
notated instances of sentences, SemLink can be
considered as a corpus annotated with VerbNet
classes. Figure 3 presents some annotated sen-
tences obtained from SemLink. For example, the
annotation ?blared;VN=43.2? indicates that the
occurrence of ?blare? in this context is classified
as Sound Emission.
2.2 Related work
There has been much research effort invested in
the automatic classification of verbs into lexical
semantic classes, in a supervised or unsupervised
way. The present work inherits the spirit of the su-
pervised approaches to verb classification (Steven-
son et al, 1999; Stevenson and Merlo, 1999;
Merlo and Stevenson, 2001; Stevenson and Joanis,
2003; Joanis and Stevenson, 2003; Joanis et al,
2008). Our learning framework basically follows
the above listed works: features are obtained from
an unannotated (automatically parsed) corpus, and
gold verb-class associations are used as training
instances for machine learning classifiers, such as
decision trees and support vector machines. How-
ever, those works targeted a small subset of Levin
classes, and a limited number of monosemous
verbs; for example, Merlo and Stevenson (2001)
studied three classes and 59 verbs, and Joanis et al
(2008) focused on 14 classes and 835 verbs. Al-
though these works provided a theoretical frame-
work for supervised verb classification, their re-
sults were not readily available for practical ap-
plications, because of the limitation in the cover-
age of the targeted classes/verbs on real texts. On
the contrary, we target the classification of arbi-
trary verbs, including polysemous verbs, into all
VerbNet classes (270 in total). In this realistic sit-
uation, we will empirically compare settings for
model learning, in order to explore effective con-
ditions to obtain better models.
Another difference from the aforementioned
works is that we aim at obtaining a probabilis-
tic model, which represents saliences of classes
of polysemous verbs. Lapata and Brew (2004)
and Li and Brew (2007) focused on this issue,
and described methods for inducing probabilities
of verb-class associations. The obtained proba-
bilistic model was intended to be incorporated into
a token-level disambiguation model. Their meth-
ods claimed to be unsupervised, meaning that the
induction of a probabilistic lexicon did not re-
quire any hand-annotated corpora. In fact, how-
ever, their methods relied on the existence of a
full-coverage lexicon, both in training and running
time. In their methods, a lexicon was necessary
for restricting possible classes to which each word
belongs. Since most verbs are associated with
only a couple of classes, such a restriction signif-
icantly reduces the search space, and the problem
becomes much easier to solve. This presupposi-
tion is implicitly or explicitly used in other seman-
tic disambiguation tasks (McCarthy et al, 2004),
but it is unrealistic for practical applications.
Clustering methods have also been extensively
researched for verb classification (Stevenson and
Merlo, 1999; Schulte im Walde, 2000; McCarthy,
2001; Korhonen, 2002; Korhonen et al, 2003;
Schulte im Walde, 2003). The extensive research
is in large part due to the intuition that the set of
classes could not be fixed beforehand. In partic-
ular, it is often problematic to define a static set
of semantic classes. However, it is reasonable to
assume that the set of VerbNet classes is fixed, be-
cause Levin-type classes are more static than on-
tological classes, like in WordNet synsets. There-
fore, we can apply supervised classification meth-
ods to our task. It is true that the current VerbNet
classes are imperfect and require revisions, but in
this work we adopt them as they are, because as
1330
time advances, more stable classifications will be-
come available.
The problem focused in this work has a close re-
lationship with automatic thesaurus/ontology ex-
pansion. In fact, we evaluate our method in the
task of automatic verb classification, which can
be considered as lexicon expansion. The most
prominent difference of the present work from the-
saurus/ontology expansion is that the number of
classes is much smaller in our problem, and the set
of verb classes can be assumed to be fixed. These
characteristics indicate that our problem is easier
and more well-defined than is the case for auto-
matic thesaurus/ontology expansion.
Supervised approaches to token-level verb class
disambiguation have recently been addressed
(Girju et al, 2005; Abend et al, 2008), largely ow-
ing to SemLink. Their approaches fundamentally
follow traditional supervised WSD methods: ex-
tracting features representing the context in which
the target word appears, and training a classifica-
tion model with an annotated corpus. While those
works achieved an impressive accuracy (more than
95%), the results may not necessarily indicate the
method?s effectiveness; rather, it may imply the
importance of a lexicon. In fact, these works re-
strict their target to verb tokens, in which the cor-
rect class exists in a given lexicon, and they only
consider candidate classes that are registered in the
lexicon. This setting reduces the ambiguity signif-
icantly, and the problem becomes much easier to
handle; for example, approximately half of verb
tokens are monosemous in their setting. Thus, a
simple baseline achieves very high accuracy fig-
ures. However, in our preliminary experiment
on token-level verb classification with unknown
verbs, we found that the accuracy for unknown
verbs (i.e., lemmas not included in the VerbNet
lexicon) is catastrophically low. This indicates
that VerbNet and SemLink are insufficient for un-
known verbs, and that we cannot expect the avail-
ability of a full-coverage lexicon in the real world.
Instead of a static lexicon, our probabilistic model
is intended to be used as a prior distribution for the
token-level disambiguation, as in Lapata and Brew
(2004)?s model.
3 A probabilistic model for verb
semantic classes
In this work, supervised learning is applied to the
probabilistic modeling of a lexicon of verb seman-
tic classes. We do not presuppose the existence of
a full-coverage lexicon; instead, we use an existing
lexicon for the training data. Combined with fea-
tures extracted from unannotated corpora, a proba-
bilistic model is learned from the existing lexicon.
Like other supervised learning applications, our
probabilistic lexicon can predict classes for words
that are not included in the original lexicon.
Our model is defined in the following way. We
assume that the set, C, of verb classes is fixed,
while a set of verbs is unfixed. With this assump-
tion, probabilistic modeling can be reduced to a
classification problem. Specifically, the goal is to
obtain a probability distribution, p(c|v), of verb
class c ? C for a given verb (lemma) v. We
can therefore apply well-known supervised learn-
ing methods to estimate p(c|v).
This probability is modeled in the form of a log-
linear model.
p(c|v) =
1
Z
exp
(
?
i
?
i
f
i
(c, v)
)
,
where f
i
(c, v) are features that represent charac-
teristics of c and v, and ?
i
are model parameters
that express weights of the corresponding features.
Model parameters can be estimated when train-
ing instances, i.e., pairs ?c, v?, and features,
f
i
(c, v), for each instance are given. Therefore,
what we have to do is to prepare the training in-
stances ?c, v?, and effective features f
i
(c, v) that
contribute to the better estimation of probabili-
ties. In token tagging tasks, both training instances
and features are extracted from annotated corpora.
However, since our goal is the probabilistic mod-
eling of a lexicon, we have to determine how to
derive the training instances and features for lexi-
con entries, to be discussed in the next section.
For the parameter estimation of log-linear mod-
els, we applied the stochastic gradient descent
method. A hyperparameter for l
2
-regularization
was tuned to minimize the KL-divergence (see
Section 4.4) for the development set.
4 Experiment design
In this work, we empirically compare several set-
tings for the learning of the above probabilistic
model, in the two tasks of automatic verb classi-
fication. In what follows, we explain the train-
ing/test data, corpora for extracting features, and
the design of the features and evaluation tasks.
The measures for evaluation are also introduced.
1331
1 sound_emission-43.2 chime
0.5 sound_emission-43.2 blare
0.5 manner_speaking-37.3 blare
0.5 modes_of_being_with_motion-47.3 sway
0.5 urge-58.1 sway
1 sound_emission-43.2 chime
0.7 sound_emission-43.2 blare
0.3 manner_speaking-37.3 blare
0.6 modes_of_being_with_motion-47.3 sway
0.4 urge-58.1 sway
Figure 4: Training instances obtained from Verb-
Net (upper) and VerbNet+SemLink (lower)
4.1 Data
As our goal is the supervised learning of a lexicon
of verb semantic classes, VerbNet is used as the
training/test data. In addition, since we aim at rep-
resenting the saliences of verb-class associations
with probabilities, the gold probabilities are nec-
essary. For this purpose, we count the occurrences
of each verb-class association in the VerbNet-
PropBank token mappings in the subset of the
SemLink corresponding to sections 2 through 21
of Penn Treebank (Marcus et al, 1994). Fre-
quency counts are normalized for each lemma,
with the Laplace smoothing (the parameter is 0.5).
In this work, we compare the two settings for
creating training instances. By comparing the re-
sults of these settings, we evaluate the necessity
of an annotated corpus for learning a probabilistic
lexicon of verb semantic classes.
VerbNet We collect all ?c, v? pairs registered in
VerbNet. For each v, all of the associated
classes are assigned equal weights (see the
upper part of Figure 4).
VerbNet+SemLink Each pair ?c, v? in VerbNet
is weighted by the normalized frequency ob-
tained from SemLink (see the lower part of
Figure 4).
Because VerbNet classes represent groups of
syntactic frames, and it is impossible to guess the
verb class by referring to only one occurrence in
a text, it is necessary to have statistics over a suf-
ficient amount of a corpus. Hence, features are
extracted from a large unannotated corpus. In this
paper, we use the following two corpora:
WSJ Wall Street Journal newspaper articles
(around 40 million words).
BNC British National Corpus, which is a bal-
anced corpus of around 100 million words.
In addition to the variance of the corpus domains,
we vary the size of the corpus to observe the ef-
fect of increasing the corpus size. These corpora
are automatically parsed by Enju 2.3.1 (Miyao and
Tsujii, 2008), and the features are extracted from
the parsing results.
4.2 Features
Levin-like classes, including VerbNet, are de-
signed to represent distinctions in syntactic frames
and alternations. Hence, if we were given the per-
fect knowledge of the possible syntactic frames,
verbs can be classified into the correct classes al-
most perfectly (Dorr and Jones, 1996). Previ-
ous works thus proposed features that express the
corpus statistics of syntactic frames. However,
class boundaries are subtle in some cases; several
classes share syntactic frames with each other to a
large extent.
For example, the classes shown in Figure 2 have
very similar syntactic frames. The difference is in-
dicated in the last two frames of Sound Emission,
although they appear much less frequently in real
texts. Therefore, it is difficult to accurately capture
the distinctions between these classes, if we are
only provided with the statistics of the syntactic
frames that appear in real texts. In this case, how-
ever, it is easy to observe that the verbs of these
classes have different selectional preferences; that
is, the Theme of Sound Emission verbs would
be objects that make sounds, while the Theme of
Modes of Being with Motion is likely to be ob-
jects that move.2 Although Levin?s classification
initially focused on syntactic alternations, the re-
sulting classes represent some semantic common-
alities. Hence, it would be reasonable to design
features that capture such semantic characteristics.
In this work, we re-implemented the following
features proposed by Joanis et al (2008) as the
starting point.
Syntactic slot Features to count the occurrences
of each syntactic slot, such as subject, ob-
ject, and prepositional phrases. For the sub-
ject slot, we also count its transitive and in-
transitive usages separately. Additionally, we
count the appearances of reflexive pronouns
and semantically empty constituents (it and
2Syntactic frames in VerbNet include specifications of se-
lectional preferences, such as animate and place, although
we do not explicitly use them, because it is not apparent to
determine the members of these semantic classes.
1332
Syntactic slot subj:0.885
intrans-subj:0.578
Slot overlap overlap-subj-obj:0.299
overlap-obj-in:0.074
Tense, voice, aspect pos-VBG:0.307
pos-VBD:0.290
Animacy anim-subj:0.244
anim-obj:0.057
Slot POS subj-PRP:0.270
subj-NN:0.270
Syntactic frame NP_V:0.326
NP_V_NP:0.307
Similar word sim-rock:0.090
sim-swing:0.083
Slot class subj-C82:0.219
obj-C12:0.081
Figure 5: Example of features for ?sway?
there). Differently from Joanis et al (2008),
we consider non-nominal arguments, such as
sentential and adjectival complements.
Slot overlap Features to measure the overlap in
words (lemmas) between two syntactic slots
of the verb. They are intended to approxi-
mate argument alternations, such as the erga-
tive alternation. For example, for the alter-
nation ?The sky cleared?/?The clouds cleared
from the sky,? a feature to indicate the overlap
between the subject slot and the from slot is
added (Joanis et al, 2008). The value of this
feature is computed by the method of Merlo
and Stevenson (2001).
Tense, voice, aspect Features to approximate the
tendency of the tense, voice, and aspect of
the target verb. The Penn Treebank POS tags
for verbs (VB, VBP, VBZ, VBG, VBD, and
VBN) are counted. In addition, included are
the frequency of the co-occurrences with an
adverb or an auxiliary verb, and the count of
usages as a noun or an adjective.
Animacy Features to measure the frequency of
animate arguments for each syntactic slot.
Personal pronouns except it are counted as
animate, following Joanis et al (2008), while
named entity recognition was not used.
Examples of these features are shown in Figure 5.
For details, refer to Joanis et al (2008).
The above features mainly represent syntactic
behaviors of target verbs. Since our target classes
are broader than in the previous works, we further
enhance the syntactic features. Additionally, as
discussed above, semantically motivated features
may present strong clues to distinguish among
syntactically similar classes. We therefore include
the following four types of feature; the first two
are syntactic, while the other two are intended to
capture semantic characteristics:
Slot POS In addition to the syntactic slot fea-
tures, we add features that represent a com-
bination of a syntactic slot and the POS of
its head word. Since VerbNet includes ex-
tended classes that take verbal and adjecti-
val arguments, the POSs of arguments would
provide a strong clue to discriminate among
these syntactic frames.
Syntactic frame The number of arguments and
their syntactic categories. This feature was
mentioned as a baseline in Joanis et al
(2008), but we include it in our model.
Similar word Similar words (lemmas) to the tar-
get verb. Similar words are automatically
obtained from a corpus (the same corpus as
used for feature extraction) by Lin (1998)?s
method. This feature is motivated by the
hypothesis that distributionally similar words
tend to be classified into the same class. Be-
cause Lin?s method is based on the similar-
ity of words in syntactic slots, the obtained
similar words are expected to represent a verb
class that share selectional preferences.
Slot class Semantic classes of the head words of
the arguments. This feature is also intended
to approximate selectional preferences. The
semantic classes are obtained by clustering
nouns, verbs, and adjectives into 200, 100,
and 50 classes respectively, by using the k-
medoid method with Lin (1998)?s similarity.
Figure 5 shows an example of the features for
?sway,? extracted from the BNC corpus.3 Feature
values are defined as relative frequencies for each
lemma; while, for similar word features, feature
values are weighted by Lin?s similarity measure.
4.3 Tasks
We evaluate our model in the tasks of auto-
matic verb classification (a.k.a. lexicon expan-
sion): given gold verb-class associations for some
set of verbs, we predict the classes for unknown
3
?C82? and ?C12? are automatically assigned cluster
names.
1333
Verb class Levin class number
Recipient 13.1, 13.3
Admire 31.2
Amuse 31.1
Run 51.3.2
Sound Emission 43.2
Light and Substance Emission 43.1, 43.4
Cheat 10.6
Steal and Remove 10.5, 10.1
Wipe 10.4.1, 10.4.2
Spray/Load 9.7
Fill 9.8
Other Verbs of Putting 9.1?6
Change of State 45.1?4
Object Drop 26.1, 26.3, 26.7
Table 1: 14 classes used in Joanis et al (2008) and
their corresponding Levin class numbers
verbs. While our main target is the full set of Verb-
Net classes, we also show results for the task stud-
ied in the previous work.
14-class task The task to classify (almost)
monosemous verbs into 14 classes. Refer to
Table 1 for the definition of the 14 classes.
Following Joanis et al (2008)?s task def-
inition, we removed verbs that belong to
multiple classes in these 14 classes, and also
removed overly polysemous verbs (in our
experiment, verb-class associations that have
the relative frequency that is less than 0.5
in SemLink are removed). For each class,
member verbs are randomly split into 50%
(training), 25% (development), and 25%
(final test) sets.
All-class task The task to classify all target verbs
into 268 classes.4 Any verbs that did not
occur at least 100 times in the BNC cor-
pus were removed.5 The remaining verbs
(2517 words) are randomly split into 80%
(training), 10% (development), and 10% (fi-
nal test) sets, under the constraint that at least
one instance for each class is included in the
training set.6
4.4 Evaluation measures
For the 14-class task, we simply measure the clas-
sification accuracy. However, the evaluation in the
4Two classes (Being Dressed and Debone) are not used in
the experiments because no lemmas belonged to these classes
after filtering by the frequency in BNC.
5This is the same preprocessing as Joanis et al (2008),
although we use VerbNet, while Joanis et al (2008) used the
original Levin classifications.
6Because polysemous verbs belong to multiple classes,
the class-wise data split was not adopted for the all-class task.
all-class task is not trivial, because verbs may be
assigned multiple classes.
Since our purpose is to obtain a probabilistic
model rather than to classify monosemous verbs,
the evaluation criterion should be sensitive to the
probabilistic distribution on the test data. In this
paper, we adopt two evaluation measures. One
is the top-N weighted accuracy; we count the
number of correct pairs ?c, v? in the N -best out-
puts from the model (where N is the number of
gold classes for each lemma), where each count is
weighted by the relative frequency (i.e., the counts
in SemLink) of the pair in the test set. For exam-
ple, in the case for ?blare? in Figure 4, if the model
states that Sound Emission has the largest prob-
ability, we get 0.7 points. If Manner Speaking
has the largest probability, we instead obtain 0.3
points. Intuitively, the score is higher when the
model presents larger probabilities to classes with
higher relative frequencies. This measure is simi-
lar to the top-N precision in information retrieval;
it evaluates the ranked output by the model. It
is intuitively interpretable, but is insufficient for
evaluating the quality of probability distributions.
The other measure is KL-divergence, which is
popularly used for measuring the dissimilarity be-
tween two probability distributions. This is de-
fined as follows:
KL(p||q) =
?
x
p(x) log(p(x))? p(x) log(q(x)).
In the experiments, this measure is applied, with
the assumption that p is the relative frequency
of ?c, v? in the test set, and that q is the esti-
mated probability distribution. Although the KL-
divergence is not a true distance metric, it is suf-
ficient for measuring the fitting of the estimated
model to the true distribution. We report the
KL-divergence averaged over all verbs in the test
set. Since this measure indicates a dissimilarity, a
smaller value is better. When p and q are equiva-
lent, KL(p||q) = 0.
5 Experimental results
Table 2 shows the accuracy obtained for the 14-
class task. The first column denotes the incorpo-
rated features (?Joanis et al?s features? or ?All fea-
tures?), and the sources of the features (?WSJ? or
?BNC?). The two baseline results are also given:
?Baseline (random)? indicates that classes are ran-
domly output, and ?Baseline (majority)? indicates
1334
Accuracy
Baseline (random) 7.14
Baseline (majority) 26.47
Joanis et al?s features/WSJ 56.86
Joanis et al?s features/BNC 64.22
All features/WSJ 60.29
All features/BNC 68.14
Table 2: Accuracy for the 14-class task
Accuracy KL
Baseline (random) 0.37 ?
Baseline (majority) 8.69 ?
Joanis et al?s features/WSJ 30.26 3.65
Joanis et al?s features/BNC 35.66 3.32
All features/WSJ 34.07 3.37
All features/BNC 42.54 2.99
Table 3: Accuracy and KL-divergence for the all-
class task (the VerbNet+SemLink setting)
that the majority class (i.e., the class that has the
largest number of member verbs) is output to every
lemma. While these figures cannot be compared
directly to the previous works due to the difference
in the preprocessing, Joanis et al (2008) achieved
58.4% accuracy for the 14-class task. Table 3 and
4 present the results for the all-class task. Table 3
gives the accuracy and KL-divergence achieved
by the model trained with the VerbNet+SemLink
training instances, while Table 4 presents the same
measures by the training instances created from
VerbNet only.
Our models performed substantially better on
both tasks than the baseline models. The results
also proved that the features we proposed in this
paper contributed to the further improvement of
the model from Joanis et al (2008). In the all-class
task with the VerbNet+SemLink setting, our fea-
tures achieved 10.69% error reduction in the accu-
racy over Joanis et al (2008)?s features. Another
interesting fact is that the model with BNC con-
sistently outperformed the model with WSJ. This
outcome is somewhat surprising, provided that the
relative frequencies in the training/test sets are cre-
ated from the WSJ portion of SemLink. The rea-
son for this is independent of the corpus size, as
will be shown below. When comparing Table 3
and 4, we can see that using SemLink statistics
resulted in a slightly better model. This result
is predictable, because the evaluation measures
are sensitive to the relative frequencies estimated
from SemLink. However, the difference remained
small. In both of the tasks and the evaluation mea-
sures, the best model was achieved when we use
Accuracy KL
Baseline (random) 0.37 ?
Baseline (majority) 8.69 ?
Joanis et al?s features/WSJ 29.65 3.67
Joanis et al?s features/BNC 35.78 3.34
All features/WSJ 34.53 3.40
All features/BNC 42.38 3.02
Table 4: Accuracy and KL-divergence for the all-
class task (the VerbNet only setting)
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Acc
ura
cy
Corpus size (M words)
Accuracy (Joanis et al?s features, WSJ)Accuracy (Joanis et al?s features, BNC)Accuracy (all features, WSJ)Accuracy (all features, BNC)
Figure 6: Corpus size vs. accuracy
all the features extracted from BNC, and create
training instances from VerbNet+SemLink.
Figure 6 and 7 plot the accuracy and KL-
divergence against the size of the unannotated cor-
pus used for feature extraction. The result clearly
indicates that the learning curve still grows at the
corpus size with 100 million words (especially for
the all features + BNC setting), which indicates
that better models are obtained by increasing the
size of the unannotated corpora.
Therefore, we can claim that the differences be-
tween the domains and the size of the unannotated
corpora are more influential than the availability of
the annotated corpora. This indicates that learning
only from a lexicon would be a viable solution,
when a token-disambiguated corpus like SemLink
is unavailable.
Table 5 shows the contribution of each feature
group. BNC is used for feature extraction, and
VerbNet+SemLink is used for the creation of train-
ing instances. The results demonstrated the effec-
tiveness of the slot POS features, and in particular,
for the all-class task, most likely because Verb-
Net covers verbs that take non-nominal arguments.
Additionally, the similar word features contributed
equally or more in both of the tasks. This result
suggests that we were reasonable in hypothesizing
that distributionally similar words tend to be clas-
1335
 2.5
 3
 3.5
 4
 4.5
 0  20  40  60  80  100
KL-
dive
rge
nce
Corpus size (M words)
KL (Joanis et al?s features, WSJ)KL (Joanis et al?s features, BNC)KL (all features, WSJ)KL (all features, BNC)
Figure 7: Corpus size vs. KL-divergence
14-classes All classes
Accuracy Accuracy KL
Baseline (random) 7.14 0.37 ?
Baseline (majority) 26.47 8.69 ?
Joanis et al?s features 64.22 35.66 3.32
+ Slot POS 66.67 38.77 3.18
+ Syntactic frame 64.71 35.99 3.29
+ Similar word 68.14 37.88 3.10
+ Slot class 64.71 36.51 3.26
All features 68.14 42.54 2.99
Table 5: Contribution of features
sified into the same class. Slot classes also con-
tributed to a slight improvement, indicating that
selectional preferences are effective clues for pre-
dicting VerbNet classes. The result of the ?All fea-
tures? model for the all-class task attests that these
features worked collaboratively, and using them
all resulted in a considerably better model.
From the analysis of the confusion matrix for
the outputs by our best model, we identified sev-
eral reasons for the remaining misclassification er-
rors. A major portion of the errors were caused by
confusing the classes that take the same preposi-
tions. Examples of these errors include:
? Other Change of State verbs were misclas-
sified into the Butter class: ?embalm,? ?lam-
inate.? (they take ?with? phrases)
? Judgement verbs were misclassified into the
Characterize class: ?acclaim,? ?hail.? (they
take ?as? phrases)
Since prepositions are strong features for auto-
matic verb classification (Joanis et al, 2008), the
classes that take the same prepositions remained
confusing. The discovery of the features to dis-
criminate among these classes would be crucial for
further improvement.
Another major error is in classifying verbs into
Other Change of State. Examples include:
? Amuse verbs: ?impair,? ?recharge.?
? Herd verbs: ?aggregate,? ?mass.?
Because Other Change of State is one of the
biggest classes, supervised learning tends to place
a high probability to this class. Therefore, when
strong clues do not exist, verbs tend to be mis-
classified into this class. In addition, this class is
not syntactically/semantically homogeneous, and
is likely to introduce noise in the machine learn-
ing classifier. A possible solution to this problem
would be to exclude this class from the classifica-
tion, and to process the class separately.
6 Conclusions
We presented a method for the supervised learn-
ing of a probabilistic model for a lexicon of Verb-
Net classes. By combining verb-class associa-
tions from VerbNet and SemLink, and features ex-
tracted from a large unannotated corpus, we could
successfully train a log-linear model in a super-
vised way. The experimental results attested to
our success that features proposed in this paper
worked effectively in obtaining a better probabil-
ity distribution. Not only syntactic features, but
also semantic features were shown to be effective.
While each of these features could increase the ac-
curacy, they collaboratively contributed to a large
improvement. In the all-class task, we obtained
10.69% error reduction in the classification accu-
racy over Joanis et al (2008)?s model. We also ob-
served the trend that a larger corpus for feature ex-
traction led to a better model, indicating that a bet-
ter model will be obtained by increasing the size of
an unannotated corpus.
We could identify the effective features and set-
tings for this problem, but the classification into
all VerbNet classes remained challenging. One
possible direction for this research topic would be
to use our model for the semi-automatic construc-
tion of verb lexicons, with the help of human cura-
tion. However, there is also a demand for explor-
ing other types of features that can discriminate
among confusing classes.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research and Grant-
in-Aid for Young Scientists (MEXT, Japan).
1336
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2008.
A supervised algorithm for verb disambiguation into
VerbNet classes. In Proceedings of COLING 2008,
pages 9?16.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL 1998.
Bonnie J. Dorr and Doug Jones. 1996. Role of word
sense disambiguation in lexical acquisition: Predict-
ing semantics from syntactic cues. In Proceedings
of COLING-96, pages 322?327.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Roxana Girju, Dan Roth, and Mark Sammons. 2005.
Token-level disambiguation of VerbNet classes. In
The Interdisciplinary Workshop on Verb Features
and Verb Classes.
Eric Joanis and Suzanne Stevenson. 2003. A general
feature space for automatic verb classification. In
Proceedings of EACL 2003, pages 163?170.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of 17th National Conference on Ar-
tificial Intelligence.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending VerbNet with
novel verb classes. In Proceedings of LREC 2006.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. the-
sis, Computer and Information Science Department,
University of Pennsylvania.
Anna Korhonen and Ted Briscoe. 2004. Extended
lexical-semantic classification of English verbs. In
Proceedings of the HLT/NAACL Workshop on Com-
putational Lexical Semantics.
Anna Korhonen, Yuval Krymolowski, and Zvika
Marx. 2003. Clustering polysemic subcategoriza-
tion frame distributions semantically. In Proceed-
ings of ACL 2003.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the Workshop on Unsupervised Lexical Acquisition,
pages 51?58.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Juanguo Li and Chris Brew. 2007. Disambiguating
Levin verbs using untagged data. In Proceedings of
RANLP 2007.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL
1998.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics, Tilburg, the Netherlands.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of ACL 2004.
Diana McCarthy. 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb-classification based on statistical distri-
bution of argument structure. Computational Lin-
guistics, 27(3):373?408.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31(1).
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behavior.
In Proceedings of COLING 2000, pages 747?753.
Sabine Schulte im Walde. 2003. Experiments on the
choice of features for learning verb classes. In Pro-
ceedings of EACL 2003, pages 315?322.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of CoNLL 2003, pages 71?78.
Suzanne Stevenson and Paola Merlo. 1999. Automatic
verb classification using grammatical features. In
Proceedings of EACL 1999, pages 45?52.
Suzanne Stevenson, Paola Merlo, Natalia Kariaeva,
and Kamin Whitehouse. 1999. Supervised learning
of lexical semantic verb classes using frequency dis-
tributions. In Proceedings of SigLex99: Standardiz-
ing Lexical Resources, pages 15?22.
1337
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513?1522,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Classifying Relations for Biomedical Named Entity Disambiguation
Xinglong Wang
??
Jun?ichi Tsujii
???
Sophia Ananiadou
??
?
School of Computer Science, University of Manchester, UK
?
National Centre for Text Mining, UK
?
Department of Computer Science, University of Tokyo, Japan
{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Named entity disambiguation concerns
linking a potentially ambiguous mention
of named entity in text to an unambigu-
ous identifier in a standard database. One
approach to this task is supervised classifi-
cation. However, the availability of train-
ing data is often limited, and the avail-
able data sets tend to be imbalanced and,
in some cases, heterogeneous. We pro-
pose a new method that distinguishes a
named entity by finding the informative
keywords in its surrounding context, and
then trains a model to predict whether each
keyword indicates the semantic class of
the entity. While maintaining a compara-
ble performance to supervised classifica-
tion, this method avoids using expensive
manually annotated data for each new do-
main, and thus achieves better portability.
1 Introduction
While technology on named entity recognition
(NER) matures, many researchers in the field of
information extraction (IE) gradually shifted their
focus to more complex tasks such as named en-
tity disambiguation and relation extraction. Both
tasks are particularly important for biomedical text
mining, which concerns automatically extracting
facts from the exponentially growing biomedical
literature (Hunter and Cohen, 2006). One type of
facts is relations between biomedical named en-
tities, such as disease-drug relation, gene-disease
relation, protein-protein interaction (PPI), etc. To
automatically extract these facts, advanced natu-
ral language processing techniques such as parsing
have been adopted to analyse the syntactic and se-
mantic structure of text. The idea is that linguistic
structures between the interacting biological enti-
ties may have common characteristics that can be
exploited by similarity measures or machine learn-
ing algorithms. For example, Erkan et al (2007)
used the shortest path between two genes accord-
ing to edit distance in a dependency tree to de-
fine a kernel function for extracting gene interac-
tions. Miwa et al (2008) comparably evaluated a
number of kernels for incorporating syntactic fea-
tures, including the bag-of-word kernel, the subset
tree kernel (Moschitti, 2006) and the graph ker-
nel (Airola et al, 2008), and they concluded that
combining all kernels achieved better results than
using any individual one. Miyao et al (2008)
used syntactic paths as one of the features to train
a support vector machines (SVM) model for PPIs
and also discussed how different parsers and out-
put representations affected the end results.
Another crucial IE task is named entity disam-
biguation, which concerns grounding mentions of
named entities in text to unambiguous concepts as
defined in some standard dictionary or database.
For instance, given a search term Python, users
may like to see the results grouped into the fol-
lowing categories: a type of snake, a programming
language, or a film (Bunescu and Pas?ca, 2006).
One approach to such lexical disambiguation tasks
is supervised classification. However, such tech-
niques suffer from the knowledge acquisition bot-
tleneck, meaning that manually annotating train-
ing data is costly and can never satisfy the need by
the machine learning algorithms. In addition, su-
pervised techniques may not yield reliable results
when the distributions of the semantic classes are
different in the training and test datasets (Agirre
and Martinez, 2004; Koeling et al, 2005). For ex-
ample, on the task of word sense disambiguation,
a model trained on a dataset where the predom-
inant sense of the word star is ?heavenly body?,
may not work well on text mainly composed of
entertainment news. Such problems are also ma-
jor concerns when developing a system to disam-
biguate biomedical named entities (e.g., protein,
1513
gene, and disease), for which some researchers
rely on hand-crafted rules in addition to a small
amount of training data (Morgan and Hirschman,
2007; Hakenberg et al, 2008).
This paper proposes a new disambiguation
method that, instead of classifying each individual
occurrence of an entity, it classifies pair-wise re-
lations between the entity mention in question and
the ?cue words? in its adjacent context, where each
cue word is assumed to bear a semantic class. We
then select the cue word that has a positive rela-
tion with the entity, and pass its semantic tag to it.
While an individual entity mention may belong to
a large number of semantic classes, a relation can
only take one of two values: positive or negative,
hence transforming a complex multi-classification
problem into a less complicated binary classifica-
tion task. The remainder of the paper is organised
as follows: Section 2 proposes the disambigua-
tion method and Section 3 introduces the task of
disambiguating the model organisms of biomedi-
cal named entities. Section 4 describes in detail
our proposed method and also a number of base-
line systems for comparison purposes. Section 5
shows the evaluation results and discusses the ad-
vantages and drawback of our system, and we fi-
nally conclude in Section 6.
2 Disambiguation as Relation
Classification
The named entity disambiguation task is defined
as follows: given a mention of a named entity in
text, we automatically assign a semantic tag d to
it, where d ? D, and D is a pre-compiled dic-
tionary with |D| entries. When |D| is small, the
problem can be approached by supervised classi-
fication. For example, to determine whether an
occurrence of an entity is a protein, a gene or an
RNA, Hatzivassiloglou et al (2001) compared
performance of 3 supervised classification meth-
ods and reported results near the human agree-
ment rate. Nevertheless, when |D| is large (e.g.,
> 100), the performance of classification may de-
crease, especially when the distribution of d in
training dataset differs from that in the test set. In
other words, when |D| is large, named entity dis-
ambiguation becomes a multi-class classification
task on heterogeneous and imbalanced datasets,
which is challenging for a machine learning model
to learn to discriminate enough between the se-
mantic classes (Japkowicz, 2000).
We propose an alternative method for named
entity disambiguation. Intuitively, in the surround-
ing context of an ambiguous entity, one can of-
ten find ?cue words? that are informative indica-
tors of the entity?s semantic category. These cue
words are provided by authors to remind readers
the semantic identity of a named entity. For ex-
ample, in an article about protein p53, phrase ?hu-
man protein p53? may be mentioned, where both
human and protein contain semantic information
regarding p53: human indicates the model organ-
ism of p53, and protein suggests the type of this
entity. Such cue words may occur infrequently in
the training data, making it difficult for machine
learning classifiers to capture.
Our method exploits this observation. Given a
sentence, let E be the set of ?target? entities (e.g.,
p53) and W of the ?cue? words (e.g., human) that
co-occur in a sentence, we define a relation as a
pair r = ?e, w?, where e ? E and w ? W , and
r is a positive relation if e belongs to the semantic
class indicated by w, and is a negative one if not.
Then we can disambiguate e by accomplishing the
following steps: 1) identify W and build a set
of relations R = {?e, w
i
?|w
i
?W, i = 1, 2, .., n},
where n is the size of W ; and 2) classify every
r ? R and assign the semantic tag of w
j
to e such
that r
j
= ?e, w
j
? is positive. The first task can be
tackled by a dictionary lookup, or by an NER sys-
tem, if manually annotated data is available. The
second is essentially a binary relation classifica-
tion task, and in this work, we use an SVM model
exploiting bag-of-word and syntactic features.
3 Species Disambiguation
We show the performance of the proposed method
on a task of resolving one major source of am-
biguity in protein and gene entities: model or-
ganisms. Model organisms are species studied to
understand particular biological phenomena. Bi-
ological experiments are often conducted on one
species, with the expectation that the discover-
ies will provide insight into the workings of oth-
ers, including humans, which are more difficult
to study directly. From viruses, prokaryotes, to
plants and animals, there are dozens of organ-
isms commonly used in biological studies, such
as E. coli, Drosophila, Homo sapiens, and hun-
dreds more are frequently mentioned in biologi-
cal research papers. In biomedical articles, entities
of different species are commonly referred to us-
1514
ing the same name, causing great ambiguity. For
example, searching a protein sequence database,
RefSeq
1
with query ?tumor protein p53? resulted
in over 100 proteins, as the name is shared by
many organisms.
The importance of distinguishing model organ-
isms has been recognised by the community of
biomedical text mining. Chen et al (2005) col-
lected gene names from various source databases
and calculated intra- and inter-species ambigui-
ties. Overall, only 25 (0.02%) official symbols
were ambiguous within the organisms. However,
when official symbols from 21 organisms were
combined, the ambiguity increased substantially
to 21, 279 (14.2%) symbols. Hakenberg et al
(2008) showed that species disambiguation is one
of the most important steps for term normalisa-
tion and identification, which concerns automat-
ically associating mentions of biomedical enti-
ties in text to unique database identifiers (Mor-
gan et al, 2008). Also, the task of extracting
PPIs in the recent BioCreative Challenge II work-
shop (Hirschman et al, 2007) requires protein
pairs to be recognised and normalised, which in-
evitably involves species disambiguation.
More specifically, given a text, in which men-
tions of biomedical named entities are annotated,
a species disambiguation system automatically as-
signs a species identifier, as in a standard database
of model organisms, to every entity mention. The
types of biomedical named entities concerned in
this study are protein, gene, protein complex and
mRNA/cDNA, and we used identifiers from the
NCBI Taxonomy of model organisms.
2
The work
focuses on species disambiguation and assumes
that the entities are already identified. In practice,
an automated named entity recogniser (e.g., AB-
NER (Settles, 2005)) should be used before apply-
ing the systems.
4 Approaches
This section describes a number of approaches to
species disambiguation, highlighting the relation
classification method proposed in Section 2.
4.1 Heuristics Baselines
The cue words for species are words denoting
names of model organisms (e.g., mouse as in
1
http://www.ncbi.nlm.nih.gov/RefSeq
2
http://www.ncbi.nlm.nih.gov/sites/
entrez?db=taxonomy
phrase ?mouse p53?). Another clue is the pres-
ence of the species-indicating prefixes in gene and
protein names. For instance, prefix ?h? in en-
tity ?hSos-1? suggests that it is a human protein.
Throughout this paper, we refer to such cue words
(e.g., mouse, hSos-1) as ?species words?. Note
that a species ?word? may contain multiple tokens
(e.g., E. Coli).
We encoded this knowledge in a rule-based
species tagging system (Wang and Grover, 2008).
The system takes a 2-step approach. First, it marks
up species words in the document using a species-
word detection program,
3
which searches every
word in a dictionary of model organisms and as-
signs a species ID to the word if a match is found.
The dictionary was built using the NCBI taxon-
omy
4
and the UniProt controlled vocabulary of
species,
5
and in total it contains 420,224 species
words for 324,157 species IDs. When species
words are identified, we disambiguate an entity
mention using one of the following rules:
1. previous species word: If the word preceding an entity
is a species word, assign the species ID indicated by
that word to the entity.
2. species word in the same sentence: If a species word
and an entity appear in the same sentence, assign its
species ID to the entity. When more than one species
word co-occurs in the sentence, priority is given to the
species word to the entity?s left with the smallest dis-
tance. If all species words occur to the right of the en-
tity, take the nearest one.
3. majority vote: assign the most frequently occurring
species ID in the document to all entity mentions.
It is expected that the first rule would produce
good precision. However, it can only disam-
biguate the fraction of entities that happen to have
a species word to their immediate left. The second
rule relaxes the first by allowing an entity to take
the species indicated by its nearest species word
in the same sentence, which should increase recall
but decrease precision. Statistics from our dataset
(see Section 5.1) show that only 5.68% entities can
potentially be resolved by rule 1 and 22.16% by
rule 2, while the majority rule can tackle every en-
tity mention in the dataset.
3
The species word detector identifies the cue words and
was used in all the systems studied in this paper. We could
not properly evaluate the detector due to the lack of man-
ually annotated data. Its performance, however, would not
affect the comparative evaluation results, and improvement
to species word detection should increase the performance of
these disambiguation systems.
4
ftp://ftp.ncbi.nih.gov/pub/taxonomy/
5
http://www.expasy.ch/cgi-bin/speclist
1515
4.2 Supervised Classification
The disambiguation problem can be approached as
a classification task. Given an entity mention and
its surrounding context, a machine learning model
classifies the entity into one of the classes, where
each class corresponds to a species ID. We car-
ried out experiments with two classification meth-
ods: multi-class classification and one-class clas-
sification, where a maximum entropy model
6
was
used for the former and SVM-light
7
for the lat-
ter. In one-class classification, we trained a se-
ries of binary SVM classifiers, each constructing
a separating hyperplane that maximises the mar-
gin between the instances of one specific species
(i.e., the target class) and a set of randomly se-
lected instances of other species (i.e., the outlier
class). We used equal numbers of instances for
both classes in training. The following types of
features were used in both multi-class and one-
class experiments, where the values of n were
set empirically by cross-validation on the training
data:
? leftContext The n word lemmas to the left of the entity
(n = 200).
? rightContext The n word lemmas to the right of the
entity (n = 200).
? leftSpeciesIDs The n species IDs to the left of the entity
(with order, n = 5).
? rightSpeciesIDs The n species IDs to the right of the
entity (with order, n = 5).
? leftNouns The n nouns to the left of the entity (with
order, n = 2).
? leftAdjs The n adjectives to the left of the entity (with
order, n = 2).
? leftSpeciesWords The n species word forms to the left
of the entity (n = 5).
? rightSpeciesWords The n species word forms to the
right of the entity (n = 5).
? firstLetter The first character of the entity itself (e.g.,
?h? in hP53).
? documentSpeciesIDs All species IDs that occur in the
document in question.
? useStopWords filter out function words.
? useStopPattern filter out words consisting only of digits
and punctuation characters.
Feature selection was also carried out for the
one-class classification experiments. We com-
pared two feature selection methods that report-
edly work well on the task of text classification:
information gain (IG) (Yang and Pedersen, 1997)
6
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
7
http://svmlight.joachims.org/
The ARG1 ARG1 ARG2
ARG1 ARG2ARG1
Drosophila Kip3 isorthologue of Klp67A.
Figure 1: Predicate argument structure (PAS).
and Bi-Normal separation (BNS) (Forman, 2003).
IG measures the decrease in entropy when the
feature is given vs. absent, and is defined as:
IG(Y |X) = H(Y ) ? H(Y |X) where H(Y ) is
the uncertainty about the value of Y (i.e., Y ?s en-
tropy), and H(Y |X) is Y ?s conditional entropy
given X . The BNS is defined as: |F
?1
(x) ?
F
?1
(y)|, where F
?1
is the standard Normal distri-
bution?s inverse cumulative probability function,
namely, z-score; x is the ratio between the number
of positive cases containing the feature in ques-
tion, and the total number of positive cases; and y
is the ratio between the number of negative cases
containing the feature, and the total number of
negative cases.
We computed a weight for each feature and then
ranked the features according to their weight, with
respect to each feature selection method. The top
10% features were used in training. Given a test
instance, the one-class classification method first
counts the species words in the document that the
instance appears in, and then applies in sequence
the binary models of each occurring species, start-
ing from the most frequent one. For example, if
a document contains 5 occurrences of human and
3 mouse, we first apply the human species model
to judge whether an entity mention is of human
species, and only if not, the mouse model was ap-
plied. The most-frequent species in the document
was used as backup when none of the binary mod-
els gives positive answers.
4.3 Relation Classification
4.3.1 Overview
As for the proposed relation classification method,
in the training phase, we first selected the sen-
tences in which an entity mention and a species
word co-occur, and constructed pair-wise entity-
species relations. We then assigned each relation a
binary label: a relation is positive if the species ID
inferred from the species word matches the gold-
standard species annotation on the entity, and is
negative otherwise. For example, for the sentence
shown in Figure 1, where Drosophila is a species
word, and Kip3 and Klp67A are proteins, relation
?Kip3, Drosophila? is a negative instance and the
1516
pair ?Klp67A, Drosophila? is a positive one.
8
For each relation, a vector of features were ex-
tracted. We followed the PPI extraction method
described in (Miyao et al, 2008), where two types
of features were used for a SVM classifier. The
first was bag-of-word features, i.e., the words be-
fore, between and after the pair of entities, where
the words were lemmatised. We added an ad-
ditional feature of the distance between the en-
tity and the cue word. The other type was syn-
tactic features obtained from parsers. For bag-
of-word features, a linear kernel was used, and
for syntactic ones, a subset tree kernel (Mos-
chitti, 2006) was adopted. The syntactic features
were represented in a flat tree format. Figure 2
shows such a feature for the negative instance
?Kip3, Drosophila? from Figure 1. Note that all
species words (e.g., Drosophila) were normalised
to ?SPECIESWORD?, and entities (e.g., Kip3) to
?ENTITY?, which not only reduces the noise in
the feature set, but also makes the model more
species-generic. From the training dataset (see
Section 5.1), 25, 413 relations were extracted, of
which 63.3% were positive.
(ENJU(noun arg1(SPECIESWORD orthologue))
(prep arg12(of orthologue))
(prep arg12(of ENTITY)))
Figure 2: A syntactic feature obtained from the ENJU
parser.
To identify the species of an entity in unseen
text, we first parsed the sentence, and then listed
all pairs of species words and entities as relations.
Having extracted the bag-of-word and syntactic
features from the instance, the trained model was
applied to judge whether each species-entity rela-
tion was positive. The entity mention in a positive
relation would be tagged with the ID indicated by
the species word, while the mentions in negative
relations would be left untagged. The next section
describes in detail how we extracted the syntactic
features from text.
4.3.2 Syntactic Features
Given a sentence, a natural language parser au-
tomatically recognises its syntactic structure and
outputs a parse tree, in which nodes represent
words or syntactic constituents. A path between
8
Orthologues are genes/proteins in different species but
have similar sequences. In this example it implies that
Klp67A is a Drosophila protein but Kip3 is not.
Parser Input Output
C&C POS-tagged GR
ENJU POS-tagged PAS
ENJU-Genia POS-tagged PAS
Minipar Sentence-detected Minipar
RASP Tokenised GR
Stanford POS-tagged SD
Stanford-Genia POS-tagged SD
Table 1: Parsers and their input and output format
a pair of nodes can be interpreted as a syntactic re-
lation between sentence units, which was proved
useful to infer biological relations (e.g., Airola et
al., 2008; Miwa et al, 2008).
We experimented with the following parsers
(summarised in Table 1):
? Dependency parsers identify one word as the head
of a sentence and all other words are either a depen-
dent of that word, or else dependent on some other
word that connects to the headword through a sequence
of dependencies. We used Minipar (Lin, 1998) and
RASP (Briscoe et al, 2006) for the experiments;
? Constituent-structured parsers split a sentence into
syntactic constituents such as noun phrases or verb
phrases. We used the Stanford parser (Klein and Man-
ning, 2003), and also a variant of the Stanford parser
(i.e., Stanford-Genia), which was trained on the GE-
NIA treebank (Tateisi et al, 2005) for biomedical text;
? Deep parsers aim to compute in-depth syntactic and
semantic structures based on syntactic theories such as
HPSG (Pollard and Sag, 1994) and CCG (Steedman,
2000). We used the C&C parser (Clark and Curran,
2007), ENJU (Miyao and Tsujii, 2008), and a variant
of ENJU (Hara et al, 2007) adapted for the biomedical
domain (i.e., ENJU-Genia);
There were a number of practical issues to con-
sider when using parsers for this task. Firstly, be-
fore parsing, the text needs to be linguistically pre-
processed, and the quality of this process has a sig-
nificant impact on parsers? performance. The pre-
processing steps include sentence boundary detec-
tion, tokenisation and part-of-speech (POS) tag-
ging, all of which can be tricky especially when
applied to biomedical text (Grover et al, 2003).
To avoid the noise that can be introduced in the
pre-processing steps and to concentrate on evalu-
ating the performance of the parsers, we used the
same pre-processing tools (Alex et al, 2008a)
9
whenever possible. The middle column in Ta-
ble 1 shows how the input text was linguisti-
cally pre-processed with respect to each parser.
A POS-tagged text implies that it was also sen-
tence boundary detected and tokenised Except for
9
These particular tools were chosen because they were
adopted to pre-process the ITI-TXM dataset, which we used
in our study.
1517
RASP and Minipar, all parsers took POS-tagged
text as input. RASP requires POS tags and punctu-
ation labels that were derived from the CLAWS-7
tagset,
10
whereas our dataset uses POS labels from
the Penn Treebank tagset (Marcus et al, 1994).
As RASP does not recognise the Penn tagset, we
used its build-in POS tagger. Minipar, on the other
hand, does not support input of tokenised or POS-
tagged text, and therefore took split sentences as
input.
Secondly, the output representations of the
parsers are different and we preferred a format
that depicts relations between words instead of
syntactic constituents. In total, 4 representations
were used: grammatical relation (GR) (Briscoe et
al., 2006), Stanford typed dependency (SD) (de
Marneffe et al, 2006), Minipar?s own representa-
tion (Lin, 1998), and ENJU?s predicate-argument
structure (PAS). All the above representations de-
fine relations of words in triples, where a depen-
dency triple (i.e., GR, SD and Minipar) consists
of head, dependent and relation, and a PAS triple
contains predicate, argument, and relation. Fig-
ure 1 shows a sentence parsed by ENJU in PAS
representation. The right-most column in Table 1
lists the output representation of each parser. A
syntactic path between an entity and a species
word was represented by a sequence of triples,
each following the order of head-dependent or
predicate-argument. These paths were used as
syntactic features for the SVM classifier.
4.4 Spreading Strategies
Except for the majority vote rule, the approaches
described in Sections 4.1 and 4.3 were expected
to yield low recall, because they can only detect
intra-sentential relations, and therefore only be ap-
plied to the entities having at least one species
word appearing in the same sentence.
Since our aim is to disambiguate as many entity
mentions as possible, we would like to ?spread?
the decisions from the disambiguated mentions to
their ?relatives? in the same document. We define
an entity mention e? as another mention e?s rela-
tive under either of the following conditions: a)
if e? has the same surface form with e; or, b) if
e? is an abbreviation or an antecedent of e, where
abbreviation/antecedent pairs were detected using
the algorithm described in (Schwartz and Hearst,
10
http://ucrel.lancs.ac.uk/claws7tags.
html
2003). Given the set of disambiguated mentions,
we then ?spread? their species IDs to their rela-
tives in the same document. After this process, the
mentions that do not have any disambiguated rela-
tives would still be missed by the system. In such
cases, we used a ?default? species, as determined
by the rule of majority vote (see Section 4.1).
5 Evaluation
5.1 Data and Ontology
The species disambiguation experiments were
conducted using the ITI-TXM corpus (Alex et al,
2008b), a collection of full-length biomedical re-
search articles manually annotated with linguistic
and biomedical information for developing auto-
matic information extraction systems. The cor-
pus contains two datasets covering slightly dif-
ferent domains: enriched protein-protein interac-
tion (EPPI) and tissue expression (TE). When-
ever possible, protein, protein complex, gene, and
mRNA/cDNA entities were tagged with NCBI
Taxonomy IDs, denoting their species, and it was
the species annotation that this study used.
The EPPI and TE datasets have different distri-
butions of species. The entities in EPPI belong to
118 species with human being the most frequent at
51.98%. In TE, the entities are across 67 species
and mouse is the most frequent at 44.67%.
11
The
inter-annotator agreement of species annotation on
EPPI and TE are 86.45% and 95.11%, respectively.
The species disambiguation systems were de-
veloped on the training portions of the EPPI and
TE corpora, each containing 221 articles, and eval-
uated on a dataset combining the development
test (DEVTEST) datasets of EPPI and TE, contain-
ing 58 and 48 articles, respectively. The com-
bined training dataset contains 96, 992 entity men-
tions belonging to 138 model organisms, while the
DEVTEST dataset contains 23, 118 entities of 54
species. The diversity of model organisms in this
corpus highlights the fact that a primary consid-
eration when developing a species disambiguation
system is its ability to distinguish a wide range of
species with minimal additional manual effort.
5.2 Results
5.2.1 Evaluation Metrics
The evaluation was carried out on the DEVTEST
dataset, and the systems are compared using av-
11
These figures were obtained from the training split of the
datasets.
1518
micro-avg. macro-avg.
Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01
SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09
SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34
Table 2: Evaluation results of the classification systems on
DEVTEST (precision/recall/F1-score, in %)
eraged precision, recall and F1 scores over all
species. In more detail, for each model organism
that appears in the DEVTEST dataset, we collect
two lists of entity mentions of that species: one
from the gold-standard DEVTEST dataset, and the
other from the output of a disambiguation system.
Then the list of system output is compared against
the gold-standard list to obtain precision, recall
and F1 score. For each system, the scores ob-
tained from all species are averaged using micro-
average and macro-average. The micro-average is
the mean of the summation of contingency metrics
for all model organisms, so that scores of the more
frequent species influence the mean more than
those of less frequent ones. The macro-average is
the mean of precision, recall, or F1 over all labels,
thus attributing equal weights to each species, and
measuring a system?s adaptability across different
model organisms.
5.2.2 Evaluation Results
First of all, Table 2 shows the results of the clas-
sification methods described in Section 4.2. The
multi-classification system using a maximum en-
tropy model (Maxent) yielded the highest overall
micro-averaged F1. Among the SVM-based sys-
tems, the one using IG feature selection achieved
better performance. In particular, it outperformed
the Maxent model in term of macro-averages. The
performance of the SVM model with BNS feature
selection is disappointing, perhaps because the oc-
currences of a feature in each instance are not nor-
mally distributed. As the Maxent system obtained
better results, it was used to compare with other
disambiguation systems.
Table 3 shows the results of a number of meth-
ods described in the previous sections. The meth-
ods are categorised into 4 groups: rule-based
baseline systems, a Maxent classification model,
relation-classification methods, and a hybrid sys-
tem. The difference between the relation classifi-
cation systems is the features adopted. Rel-Context
was trained on only bag-of-word and distance fea-
tures, whereas each other system also used syn-
tactic features provided by a specific parser. For
example, the Rel-RASP system identifies an entity?s
species by finding positive relations between the
entity and its neighbouring species words, using
features including bag-of-word, distance, and de-
pendency paths generated by RASP. The hybrid
system (Hbrd) ran the Rel-ENJU-Genia system on top
of the outcome of Maxent. When a conflict oc-
curs, the species ID is chosen by Rel-ENJU-Genia.
The idea is that the relation classification system
is more accurate than Maxent when it is applica-
ble, and hence would improve precision on dis-
ambiguating the species with few or no training
instances.
Without spreading (shown in the ?NO SPRD?
columns of Table 3), most of the rule-based and re-
lation classification systems only work on a subset
of DEVTEST, resulting in low recall: Rule-Sp works
on the small proportion of entities (5.68%) with a
preceding species word, while the other systems
only work on the collection of sentences contain-
ing at least one species word and one entity, which
covers 4.60% sentences and 22.16% entity men-
tions. Rule-Majority, Maxent, and Hbrd, on the other
hand, apply to all entity mentions, and therefore
they are only compared against the others when
spreading was applied.
The results shown in the ?NO SPRD? columns
can be viewed as a comparative evaluation of
the usefulness of the syntactic features supplied
by the parsers on this particular task. The rule-
based systems set high baselines: Rule-Sp pro-
duced good precision and Rule-SpSent achieved the
highest micro-averaged F1, thanks to its high
coverage, which is also an upperbound of recall
for the relation classification systems. Neverthe-
less, it is encouraging that the relation classifica-
tion systems obtained higher precision than Rule-
SpSent, which is important, considering the de-
cisions will be transfered to the untagged entity
mentions across the document. Indeed, as shown
in the SPRD columns in Table 3, most relation
classification systems outperformed the Rule-SpSent
baseline when spreading was used. The scores
of the systems using different parser outputs only
vary slightly. Rel-Context, on the other hand, sur-
passed others in terms of micro-averaged preci-
sion, while sacrificing micro-averaged recall and
macro-averaged scores.
Next, the SPRD columns in Table 3 show the re-
sults when the spreading rules were applied, which
1519
METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)
Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08
Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00
Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Rel-C&C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14
Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67
Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21
Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44
Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80
Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45
Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64
Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80
Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)
effectively improved recall (see Section 5.2.3
for discussion on statistical significance tests on
the results). The Maxent system achieved very
good micro-averaged precision, but low macro-
averaged scores. In fact, as shown in Table 4, Max-
ent can only disambiguate 7 species (out of a total
of 54) that have relatively large amount of train-
ing instances,
12
and failed completely on other
species. This suggests that Maxent may not be able
to generate good micro-averaged scores when ap-
plied to a dataset where the dominant species are
different from those in the training set. On the
other hand, the relation-classification approaches
have a clear advantage over Maxent as measured
by macro-averaged scores. As shown in Table 4,
Rel-ENJU-Genia worked well on most of the species,
displaying its good adaptability, while achieving
comparable micro-averaged F1 to Maxent. Over-
all, Hbrd, which combines the strengths of relation
classification and the Maxent classification model,
obtained the highest points as measured by every
metric.
5.2.3 Statistical Significance
To see whether our methods significantly im-
proved the baseline systems, we performed ran-
domisation tests (Noreen, 1989; Yeh, 2000) on
some of the results shown in Table 3. The in-
tuition of randomisation test is as follows: when
comparing two systems (e.g., A and B), we erase
the labels ?output of A? or ?output of B? from all
observations. The null hypothesis is that there is
no difference between A and B, and thus any re-
sponse produced by one of the systems could have
as likely come from the other. We shuffle these re-
12
The following 7 species occur most frequently in the
training set: H. sapiens (43.25%), M. musculus (27.05%),
R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis
(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).
Species Name Pct Mxt Rel Hbrd
H. sapiens 50.13% 76.25 65.33 79.51
M. musculus 13.99% 66.41 58.29 68.27
X. tropicalis 7.35% 64.80 77.72 71.39
D. melanogaster 6.34% 93.17 78.46 95.15
S. cerevisiae 4.79% 90.12 83.32 87.68
R. norvegicus 2.97% 44.04 38.69 51.77
T. aestivum 2.62% 0.00 89.68 23.35
P. americana 2.27% 0.00 98.50 7.76
C. elegans 2.08% 96.83 95.88 97.50
H. herpesvirus 5 1.58% 0.00 54.46 4.27
R. virus 1.45% 0.00 28.54 6.45
H. spumaretrovirus 1.17% 0.00 99.37 2.49
... ... ... ... ...
Macro-average 9.85 30.21 31.80
Micro-average 70.48 67.13 73.70
Table 4: The micro-averaged F1 scores (%) of Maxent
(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd with
respect to each of the most frequent 12 species in DEVTEST.
sponses R times, reassign each response to A or
B and see how likely such a shuffle produces a
difference in the metric of interest that is at least
as large as the difference observed when using A
and B on the test data. Let r denote the number
of times that such a difference occurred, then as
R ? ?,
r+1
R+1
approaches the significance level.
In our case, the metrics tested were micro- and
macro-averaged precision, recall and F1.
Following this procedure, we tested whether the
improvements made by a relation classification
based system (i.e., Rel-ENJU-Genia with SPRD) and
the hybrid system (i.e., Hbrd) over the baseline sys-
tems were statistically significant. We carried out
approximate randomisation with 10,000 shuffles
and the test results are shown in Table 5. The nu-
merical figures in the cells are differences in pre-
cision, recall and F1 between a pair of systems.
The significance levels (i.e., p-values) are indi-
cated by superscript marks, whose correspond-
ing values are displayed in Table 6. For exam-
1520
Rule-Majority Rule-Sp Rule-SpSent Maxent
Rel
micro-avg 2.77
?
/3.46
?
/3.13
?
1.95
?
/2.04
?
/2.00
?
1.57
?
/2.22
?
/1.92
?
-1.57
?
/ -5.02
?
/ -3.35
?
macro-avg 15.24
?
/13.12
?
/12.13
?
3.75
a
/4.21
a
/3.20
a
9.35
?
/8.44
?
/7.10
?
21.92
?
/24.87
?
/20.35
?
Hbrd
micro-avg 8.01
?
/11.27
?
/9.70
?
7.19
?
/9.85
?
/8.57
?
6.81
?
/10.04
?
/8.49
?
3.67
?
/2.78
?
/2.82
b
macro-avg 27.22
?
/15.72
c
/13.72
d
15.73
?
/6.82
e
/4.80
f
21.33
?
/11.05
g
/ 8.70
h
33.91
i
/27.47
?
/21.95
?
Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-
proved the baseline systems. The numerical figures in the cells show the differences between the two systems as measured by
precision/recall/F1 in percentage. The superscript marks indicate the significance levels and are explained in Table 6.
ple, the difference in micro-averaged precision be-
tween Rel-ENJU-Genia and Rule-Majority on the test
data was 2.77%, and in 10,000 approximate ran-
domisation trials, there was zero times
13
that Rel-
ENJU-Genia?s micro-averaged precision is greater
than Rule-Majority?s by at least 2.77% (p < 0.0001).
MARK VALUE MARK VALUE
* p < 0.0001 a p < 0.06
b p < 0.002 c p < 0.0003
d p < 0.0002 e p < 0.03
f p < 0.05 g p < 0.003
h p < 0.005 i p < 0.07
Table 6: p-values.
The test results confirmed that, the improve-
ments made by Hbrd are statistically significant
with at least 95% confidence as measured by all
metrics except for macro-averaged precision. The
relation classification approach achieved signifi-
cantly lower performance than Maxent in terms of
micro-averaged scores (hence the ?-? sign in the
corresponding cell in Table 5), but in all other
cases it can reject the null hypothesis with very
high confidence (i.e., p < 0.0001).
6 Conclusions and Future Work
This paper proposes a method that tackles a com-
plex disambiguation problem by breaking it into
two cascaded simpler tasks of cue word discov-
ery and binary relation classification. We evalu-
ated the method on the task of disambiguating the
model organisms of biomedical named entities,
along with a number of other approaches. As mea-
sured by micro-averaged F1 score, a supervised
classification approach (Maxent) yielded the second
best result. However, it can only disambiguate
a small number of species that have abundant
training instances. With spreading rules, a rela-
tion classification system (Rel-ENJU-Genia) trained
on word and syntactic features from ENJU-Genia
also obtained good micro-averaged F1, while sur-
13
The numbers of times are not shown in Table5 for
brevity.
passing Maxent significantly in terms of macro-
averaged scores. Combining these two systems
achieved the best overall performance. Neverthe-
less, we combined the two methods in a rather
crude way, leaving ample room for exploring bet-
ter strategies in the future.
One drawback of the relation classification sys-
tems is that they can not cover all entity mentions
but only the ones with informative keywords co-
occurring in the same sentence. We overcame the
drawback by using spreading rules. For some ap-
plications, however, it may be sufficient to make
predictions exclusively for cases where the sys-
tems are applicable. Also, the predictions with
high confidence can be used as seed training ma-
terial for automatically harvesting more training
data.
Acknowledgments
The work reported in this paper is funded by Pfizer
Ltd.. The UK National Centre for Text Mining is
funded by JISC. The ITI-TXM corpus used in the
experiments was developed at School of Informat-
ics, University of Edinburgh, in the TXM project,
which was funded by ITI Life Sciences, Scotland.
References
E. Agirre and D. Martinez. 2004. Unsupervised WSD based
on automatically retrieved examples: The importance of
bias. In Proceedings of EMNLP.
A. Airola, S. Pyysalo, J. Bj?orne, T. Pahikkala, F. Ginter, and
T. Salakoski. 2008. A graph kernel for protein-protein
interaction extraction. In Proceedings of BioNLP.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a.
Assisted curation: does text mining really help? In Pro-
ceedings of the Pacific Symposium on Biocomputing.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b.
The ITI TXM corpus: Tissue expression and protein-
protein interactions. In Proceedings of the Workshop on
Building and Evaluating Resources for Biomedical Text
Mining at LREC.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL Interactive Presentation Sessions.
1521
R. Bunescu and M. Pas?ca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name
ambiguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248?256.
S. Clark and J. R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models. Com-
putational Linguistics, 33(4).
M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006.
Generating typed dependency parses from phrase struc-
ture. In Proceedings of LREC.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interaction
sentences using dependency parsing. In Proceedings of
the Joint Conference of EMNLP and CoNLL.
G. Forman. 2003. An extensive empirical study of feature se-
lection metrics for text classification. Journal of Machine
Learning Research, 3:1289?1305.
C. Grover, M. Lapata, and A. Ascarides. 2003. A compar-
ison of parsing technologies for the biomedical domain.
Natural Language Engineering, 1(1):1?38.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of gene
mentions with GNAT. Bioinformatics, 24(16).
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact
of re-training a lexical disambiguation model on domain
adaptation of an HPSG parser. In Proceedings of the 10th
International Conference on Parsing Technology.
V. Hatzivassiloglou, PA Dubou?e, and A. Rzhetsky. 2001.
Disambiguating proteins, genes, and RNA in text: a ma-
chine learning approach. Bioinformatics, 17(Suppl 1).
L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-
itors. 2007. The BioCreative II - Critical Assessment
for Information Extraction in Biology Challenge, volume
9(Suppl 2). Genome Biology.
L. Hunter and K. B. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed. Molecular Cell,
21(5):589?594.
N. Japkowicz. 2000. Learning from imbalanced data sets: a
comparison of various strategies. In Proceedings of AAAI
Workshop on Learning from Imbalanced Data Sets.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of ACL.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of HLT/EMNLP.
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008.
Combining multiple layers of syntactic information for
protein-protein interaction extraction. In Proceedings of
SMBM.
Y. Miyao and J. Tsujii. 2008. Feature forest models for prob-
abilistic HPSG parsing. Computational Linguistics, 34(1).
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsujii.
2008. Task-oriented evaluation of syntactic parsers and
their representations. In Proceedings of ACL-08: HLT.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreAtIvE II gene normalisation. In Proceedings of the
BioCreAtIvE II Workshop, Madrid.
A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.
Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and
L. Hirschman. 2008. Overview of BioCreAtIvE II gene
normalization. Genome Biology, 9(Suppl 2).
A. Moschitti. 2006. Making tree kernels practical for natural
language learning. In Proceedings of EACL.
E. W. Noreen. 1989. Computer Intensive Methods for Test-
ing Hypothesis. John Wiley & Sons.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
A. S. Schwartz and M. A. Hearst. 2003. Identifying abbrevi-
ation definitions in biomedical text. In Proceedings of the
Pacific Symposium on Biocomputing.
B. Settles. 2005. ABNER: An open source tool for automat-
ically tagging genes, proteins, and other entity names in
text. Bioinformatics, 21(14):3191?3192.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syn-
tax annotation for the GENIA corpus. In Proceedings of
IJCNLP.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings of LREC.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings of
ICML.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of COLING.
1522
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 772?780,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Sequential Labeling with Latent Variables:
An Exact Inference Algorithm and Its Efficient Approximation
Xu Sun? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, Manchester, UK
{sunxu, tsujii}@is.s.u-tokyo.ac.jp
Abstract
Latent conditional models have become
popular recently in both natural language
processing and vision processing commu-
nities. However, establishing an effective
and efficient inference method on latent
conditional models remains a question. In
this paper, we describe the latent-dynamic
inference (LDI), which is able to produce
the optimal label sequence on latent con-
ditional models by using efficient search
strategy and dynamic programming. Fur-
thermore, we describe a straightforward
solution on approximating the LDI, and
show that the approximated LDI performs
as well as the exact LDI, while the speed is
much faster. Our experiments demonstrate
that the proposed inference algorithm out-
performs existing inference methods on
a variety of natural language processing
tasks.
1 Introduction
When data have distinct sub-structures, mod-
els exploiting latent variables are advantageous
in learning (Matsuzaki et al, 2005; Petrov and
Klein, 2007; Blunsom et al, 2008). Actu-
ally, discriminative probabilistic latent variable
models (DPLVMs) have recently become popu-
lar choices for performing a variety of tasks with
sub-structures, e.g., vision recognition (Morency
et al, 2007), syntactic parsing (Petrov and Klein,
2008), and syntactic chunking (Sun et al, 2008).
Morency et al (2007) demonstrated that DPLVM
models could efficiently learn sub-structures of
natural problems, and outperform several widely-
used conventional models, e.g., support vector ma-
chines (SVMs), conditional random fields (CRFs)
and hidden Markov models (HMMs). Petrov and
Klein (2008) reported on a syntactic parsing task
that DPLVM models can learn more compact and
accurate grammars than the conventional tech-
niques without latent variables. The effectiveness
of DPLVMs was also shown on a syntactic chunk-
ing task by Sun et al (2008).
DPLVMs outperform conventional learning
models, as described in the aforementioned pub-
lications. However, inferences on the latent condi-
tional models are remaining problems. In conven-
tional models such as CRFs, the optimal label path
can be efficiently obtained by the dynamic pro-
gramming. However, for latent conditional mod-
els such as DPLVMs, the inference is not straight-
forward because of the inclusion of latent vari-
ables.
In this paper, we propose a new inference al-
gorithm, latent dynamic inference (LDI), by sys-
tematically combining an efficient search strategy
with the dynamic programming. The LDI is an
exact inference method producing the most prob-
able label sequence. In addition, we also propose
an approximated LDI algorithm for faster speed.
We show that the approximated LDI performs as
well as the exact one. We will also discuss a
post-processing method for the LDI algorithm: the
minimum bayesian risk reranking.
The subsequent section describes an overview
of DPLVM models. We discuss the probability
distribution of DPLVM models, and present the
LDI inference in Section 3. Finally, we report
experimental results and begin our discussions in
Section 4 and Section 5.
772
y1 y2 ym
xmx2x1
h1 h2 hm
xmx2x1
ymy2y1
CRF DPLVM
Figure 1: Comparison between CRF models and
DPLVM models on the training stage. x represents
the observation sequence, y represents labels and
h represents the latent variables assigned to the la-
bels. Note that only the white circles are observed
variables. Also, only the links with the current ob-
servations are shown, but for both models, long
range dependencies are possible.
2 Discriminative Probabilistic Latent
Variable Models
Given the training data, the task is to learn a map-
ping between a sequence of observations x =
x1, x2, . . . , xm and a sequence of labels y =
y1, y2, . . . , ym. Each yj is a class label for the j?th
token of a word sequence, and is a member of a
set Y of possible class labels. For each sequence,
the model also assumes a sequence of latent vari-
ables h = h1, h2, . . . , hm, which is unobservable
in training examples.
The DPLVM model is defined as follows
(Morency et al, 2007):
P (y|x,?) =
?
h
P (y|h,x,?)P (h|x,?), (1)
where ? represents the parameter vector of the
model. DPLVM models can be seen as a natural
extension of CRF models, and CRF models can
be seen as a special case of DPLVMs that employ
only one latent variable for each label.
To make the training and inference efficient, the
model is restricted to have disjointed sets of latent
variables associated with each class label. Each
hj is a member in a set Hyj of possible latent vari-
ables for the class label yj . H is defined as the set
of all possible latent variables, i.e., the union of all
Hyj sets. Since sequences which have any hj /?
Hyj will by definition have P (y|hj ,x,?) = 0,
the model can be further defined as:
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual condi-
tional random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a train-
ing set consisting of n labeled sequences, (xi,yi),
for i = 1 . . . n, parameter estimation is performed
by optimizing the objective function,
L(?) =
n?
i=1
logP (yi|xi,?)?R(?). (4)
The first term of this equation represents a condi-
tional log-likelihood of a training data. The sec-
ond term is a regularizer that is used for reducing
overfitting in parameter estimation.
3 Latent-Dynamic Inference
On latent conditional models, marginalizing la-
tent paths exactly for producing the optimal la-
bel path is a computationally expensive prob-
lem. Nevertheless, we had an interesting observa-
tion on DPLVM models that they normally had a
highly concentrated probability mass, i.e., the ma-
jor probability are distributed on top-n ranked la-
tent paths.
Figure 2 shows the probability distribution of
a DPLVM model using a L2 regularizer with the
variance ?2 = 1.0. As can be seen, the probabil-
ity distribution is highly concentrated, e.g., 90%
of the probability is distributed on top-800 latent
paths.
Based on this observation, we propose an infer-
ence algorithm for DPLVMs by efficiently com-
bining search and dynamic programming.
3.1 LDI Inference
In the inference stage, given a test sequence x, we
want to find the most probable label sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the
y? cannot directly be produced by the Viterbi
algorithm because of the incorporation of latent
variables.
In this section, we describe an exact inference
algorithm, the latent-dynamic inference (LDI),
for producing the optimal label sequence y? on
DPLVMs (see Figure 3). In short, the algorithm
773
 0
 20
 40
 60
 80
 100
0.4K 0.8K 1.2K 1.6K 2K
To
p-
n 
Pr
ob
ab
ilit
y 
M
as
s 
(%
)
n
Figure 2: The probability mass distribution of la-
tent conditional models on a NP-chunking task.
The horizontal line represents the n of top-n latent
paths. The vertical line represents the probability
mass of the top-n latent paths.
generates the best latent paths in the order of their
probabilities. Then it maps each of these to its as-
sociated label paths and uses a method to compute
their exact probabilities. It can continue to gener-
ate the next best latent path and the associated la-
bel path until there is not enough probability mass
left to beat the best label path.
In detail, an A? search algorithm1 (Hart et al,
1968) with a Viterbi heuristic function is adopted
to produce top-n latent paths, h1,h2, . . .hn. In
addition, a forward-backward-style algorithm is
used to compute the exact probabilities of their
corresponding label paths, y1,y2, . . .yn. The
model then tries to determine the optimal label
path based on the top-n statistics, without enumer-
ating the remaining low-probability paths, which
could be exponentially enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?)?(1?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence
in current stage. It is straightforward to prove
that y? = y1, and further search is unnecessary.
This is because the remaining probability mass,
1??yk?LPn P (yk|x,?), cannot beat the currentoptimal label path in this case.
1A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
Definition:
Proj(h) = y ?? hj ? Hyj for j = 1 . . .m;
P (h) = P (h|x,?);
P (y) = P (y|x,?).
Input:
weight vector ?, and feature vector F (h,x).
Initialization:
Gap = ?1; n = 0; P (y?) = 0; LP0 = ?.
Algorithm:
while Gap < 0 do
n = n+ 1
hn = HeapPop[?, F (h,x)]
yn = Proj(hn)
if yn /? LPn?1 then
P (yn) = DynamicProg
?
h:Proj(h)=yn P (h)
LPn = LPn?1 ? {yn}
if P (yn) > P (y?) then
y? = yn
Gap = P (y?)?(1??yk?LPn P (yk))else
LPn = LPn?1
Output:
the most probable label sequence y?.
Figure 3: The exact LDI inference for latent condi-
tional models. In the algorithm, HeapPop means
popping the next hypothesis from the A? heap; By
the definition of the A? search, this hypothesis (on
the top of the heap) should be the latent path with
maximum probability in current stage.
3.2 Implementation Issues
We have presented the framework of the LDI in-
ference. Here, we describe the details on imple-
menting its two important components: designing
the heuristic function, and an efficient method to
compute the probabilities of label path.
As described, the A? search can produce top-n
results one-by-one using a heuristic function (the
backward term). In the implementation, we use
the Viterbi algorithm (Viterbi, 1967) to compute
the admissible heuristic function for the forward-
style A? search:
Heui(hj) = max
h?i=hj?h??HP
|h|
i
P ?(h? |x,??), (7)
where h?i = hj represents a partial latent path
started from the latent variable hj . HP|h|i rep-
resents all possible partial latent paths from the
774
position i to the ending position, |h|. As de-
scribed in the Viterbi algorithm, the backward
term, Heui(hj), can be efficiently computed by
using dynamic programming to reuse the terms
(e.g., Heui+1(hj)) in previous steps. Because this
Viterbi heuristic is quite good in practice, this way
we can produce the exact top-n latent paths effi-
ciently (see efficiency comparisons in Section 5),
even though the original problem is NP-hard.
The probability of a label path, P (yn) in Fig-
ure 3, can be efficiently computed by a forward-
backward algorithm with a restriction on the target
label path:
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?). (8)
3.3 An Approximated Version of the LDI
By simply setting a threshold value on the search
step, n, we can approximate the LDI, i.e., LDI-
Approximation (LDI-A). This is a quite straight-
forward method for approximating the LDI. In
fact, we have also tried other methods for approx-
imation. Intuitively, one alternative method is to
design an approximated ?exact condition? by us-
ing a factor, ?, to estimate the distribution of the
remaining probability:
P (y1|x,?)??(1?
?
yk?LPn
P (yk|x,?)) ? 0. (9)
For example, if we believe that at most 50% of the
unknown probability, 1 ??yk?LPn P (yk|x,?),can be distributed on a single label path, we can
set ? = 0.5 to make a loose condition to stop the
inference. At first glance, this seems to be quite
natural. However, when we compared this alter-
native method with the aforementioned approxi-
mation on search steps, we found that it worked
worse than the latter, in terms of performance and
speed. Therefore, we focus on the approximation
on search steps in this paper.
3.4 Comparison with Existing Inference
Methods
In Matsuzaki et al (2005), the Best Hidden Path
inference (BHP) was used:
yBHP = argmax
y
P (hy|x,??), (10)
where hy ? Hy1 ? . . .?Hym . In other words,
the Best Hidden Path is the label sequence
which is directly projected from the optimal la-
tent path h?. The BHP inference can be seen
as a special case of the LDI, which replaces the
marginalization-operation over latent paths with
the max-operation.
In Morency et al (2007), y? is estimated by the
Best Point-wise Marginal Path (BMP) inference.
To estimate the label yj of token j, the marginal
probabilities P (hj = a|x,?) are computed for
all possible latent variables a ? H. Then the
marginal probabilities are summed up according
to the disjoint sets of latent variables Hyj and the
optimal label is estimated by the marginal proba-
bilities at each position i:
yBMP (i) = argmax
yi?Y
P (yi|x,??), (11)
where
P (yi = a|x,?) =
?
h?Ha P (h|x,?)?
h P (h|x,?)
. (12)
Although the motivation is similar, the exact
LDI (LDI-E) inference described in this paper is a
different algorithm compared to the BLP inference
(Sun et al, 2008). For example, during the search,
the LDI-E is able to compute the exact probability
of a label path by using a restricted version of the
forward-backward algorithm, also, the exact con-
dition is different accordingly. Moreover, in this
paper, we more focus on how to approximate the
LDI inference with high performance.
The LDI-E produces y? while the LDI-A, the
BHP and the BMP perform estimation on y?. We
will compare them via experiments in Section 4.
4 Experiments
In this section, we choose Bio-NER and NP-
chunking tasks for experiments. First, we describe
the implementations and settings.
We implemented DPLVMs by extending the
HCRF library developed by Morency et al (2007).
We added a Limited-Memory BFGS optimizer
(L-BFGS) (Nocedal and Wright, 1999), and re-
implemented the code on training and inference
for higher efficiency. To reduce overfitting, we
employed a Gaussian prior (Chen and Rosenfeld,
1999). We varied the the variance of the Gaussian
prior (with values 10k, k from -3 to 3), and we
found that ?2 = 1.0 is optimal for DPLVMs on
the development data, and used it throughout the
experiments in this section.
775
The training stage was kept the same as
Morency et al (2007). In other words, there
is no need to change the conventional parameter
estimation method on DPLVM models for adapt-
ing the various inference algorithms in this paper.
For more information on training DPLVMs, refer
to Morency et al (2007) and Petrov and Klein
(2008).
Since the CRF model is one of the most success-
ful models in sequential labeling tasks (Lafferty et
al., 2001; Sha and Pereira, 2003), in this paper, we
choosed CRFs as a baseline model for the compar-
ison. Note that the feature sets were kept the same
in DPLVMs and CRFs. Also, the optimizer and
fine tuning strategy were kept the same.
4.1 BioNLP/NLPBA-2004 Shared Task
(Bio-NER)
Our first experiment used the data from the
BioNLP/NLPBA-2004 shared task. It is a biomed-
ical named-entity recognition task on the GENIA
corpus (Kim et al, 2004). Named entity recogni-
tion aims to identify and classify technical terms
in a given domain (here, molecular biology) that
refer to concepts of interest to domain experts.
The training set consists of 2,000 abstracts from
MEDLINE; and the evaluation set consists of 404
abstracts from MEDLINE. We divided the origi-
nal training set into 1,800 abstracts for the training
data and 200 abstracts for the development data.
The task adopts the BIO encoding scheme, i.e.,
B-x for words beginning an entity x, I-x for
words continuing an entity x, and O for words be-
ing outside of all entities. The Bio-NER task con-
tains 5 different named entities with 11 BIO en-
coding labels.
The standard evaluation metrics for this task are
precision p (the fraction of output entities match-
ing the reference entities), recall r (the fraction
of reference entities returned), and the F-measure
given by F = 2pr/(p+ r).
Following Okanohara et al (2006), we used
word features, POS features and orthography fea-
tures (prefix, postfix, uppercase/lowercase, etc.),
as listed in Table 1. However, their globally depen-
dent features, like preceding-entity features, were
not used in our system. Also, to speed up the
training, features that appeared rarely in the train-
ing data were removed. For DPLVM models, we
tuned the number of latent variables per label from
2 to 5 on preliminary experiments, and used the
Word Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,
wiwi+1}
?{hi, hi?1hi}
POS Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti,
titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1,
titi+1ti+2}
?{hi, hi?1hi}
Orth. Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi,
oioi+1, oi+1oi+2}
?{hi, hi?1hi}
Table 1: Feature templates used in the Bio-NER
experiments. wi is the current word, ti is the cur-
rent POS tag, oi is the orthography mode of the
current word, and hi is the current latent variable
(for the case of latent models) or the current label
(for the case of conventional models). No globally
dependent features were used; also, no external re-
sources were used.
Word Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,
wiwi+1}
?{hi, hi?1hi}
Table 2: Feature templates used in the NP-
chunking experiments. wi and hi are defined fol-
lowing Table 1.
number 4.
Two sets of experiments were performed. First,
on the development data, the value of n (the search
step, see Figure 3 for its definition) was varied in
the LDI inference; the corresponding F-measure,
exactitude (the fraction of sentences that achieved
the exact condition, Eq. 6), #latent-path (num-
ber of latent paths that have been searched), and
inference-time were measured. Second, the n
tuned on the development data was employed for
the LDI on the test data, and experimental com-
parisons with the existing inference methods, the
BHP and the BMP, were made.
4.2 NP-Chunking Task
On the Bio-NER task, we have studied the LDI
on a relatively rich feature-set, including word
features, POS features and orthographic features.
However, in practice, there are many tasks with
776
Models S.A. Pre. Rec. F1 Time
LDI-A 40.64 68.34 66.50 67.41 0.4K s
LDI-E 40.76 68.36 66.45 67.39 4K s
BMP 39.10 65.85 66.49 66.16 0.3K s
BHP 39.93 67.60 65.46 66.51 0.1K s
CRF 37.44 63.69 64.66 64.17 0.1K s
Table 3: On the test data of the Bio-NER task, ex-
perimental comparisons among various inference
algorithms on DPLVMs, and the performance of
CRFs. S.A. signifies sentence accuracy. As can
be seen, at a much lower cost, the LDI-A (A signi-
fies approximation) performed slightly better than
the LDI-E (E signifies exact).
only poor features available. For example, in POS-
tagging task and Chinese/Japanese word segmen-
tation task, there are only word features available.
For this reason, it is necessary to check the perfor-
mance of the LDI on poor feature-set. We chose
another popular task, the NP-chunking, for this
study. Here, we used only poor feature-set, i.e.,
feature templates that depend only on words (see
Table 2 for details), taking into account 200K fea-
tures. No external resources were used.
The NP-chunking data was extracted from the
training/test data of the CoNLL-2000 shallow-
parsing shared task (Sang and Buchholz, 2000). In
this task, the non-recursive cores of noun phrases
called base NPs are identified. The training set
consists of 8,936 sentences, and the test set con-
sists of 2,012 sentences. Our preliminary exper-
iments in this task suggested the use of 5 latent
variables for each label on latent models.
5 Results and Discussions
5.1 Bio-NER
Figure 4 shows the F-measure, exactitude, #latent-
path and inference inference time of the DPLVM-
LDI model, against the parameter n (the search
step, see Table 3), on the development dataset. As
can be seen, there was a dramatic climbing curve
on the F-measure, from 68.78% to 69.73%, when
we increased the number of the search step from
1 to 30. When n = 30, the F-measure has al-
ready reached its plateau, with the exactitude of
83.0%, and the inference time of 80 seconds. In
other words, the F-measure approached its plateau
when n went to 30, with a high exactitude and a
low inference time.
68
69
70
0K 2K 4K 6K 8K 10K
F-
m
ea
su
re
(%
)
65
70
75
80
85
90
95
0K 2K 4K 6K 8K 10K
Ex
ac
tit
ud
e(%
)
0
100
200
300
400
500
600
700
0K 2K 4K 6K 8K 10K
#l
at
en
t-p
at
h
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0K 2K 4K 6K 8K 10K
Ti
m
e(K
s)
n
68
69
70
0 50 100 150 200 250
65
70
75
80
85
90
95
0 50 100 150 200 250
0
100
200
300
400
500
600
0 50 100 150 200 250
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0 50 100 150 200 250
n
Figure 4: (Left) F-measure, exactitude, #latent-
path (averaged number of latent paths being
searched), and inference time of the DPLVM-LDI
model, against the parameter n, on the develop-
ment dataset of the Bio-NER task. (Right) En-
largement of the beginning portion of the left fig-
ures. As can be seen, the curve of the F-measure
approached its plateau when n went to 30, with a
high exactitude and a low inference time.
Our significance test based on McNemar?s test
(Gillick and Cox, 1989) shows that the LDI with
n = 30 was significantly more accurate (P <
0.01) than the BHP inference, while the inference
time was at a comparable level. Further growth
of n after the beginning point of the plateau in-
creases the inference time linearly (roughly), but
achieved only very marginal improvement on F-
measure. This suggests that the LDI inference can
be approximated aggressively by stopping the in-
ference within a small number of search steps, n.
This can achieve high efficiency, without an obvi-
ous degradation on the performance.
Table 3 shows the experimental comparisons
among the LDI-Approximation, the LDI-Exact
(here, exact means the n is big enough, e.g., n =
10K), the BMP, and the BHP on DPLVM mod-
777
Models S.A. Pre. Rec. F1 Time
LDI-A 60.98 91.76 90.59 91.17 42 s
LDI-E 60.88 91.72 90.61 91.16 1K s
BHP 59.34 91.54 90.30 90.91 25 s
CRF 58.37 90.92 90.33 90.63 18 s
Table 4: Experimental comparisons among differ-
ent inference algorithms on DPLVMs, and the per-
formance of CRFs using the same feature set on
the word features.
els. The baseline was the CRF model with the
same feature set. On the LDI-A, the parameter n
tuned on the development data was employed, i.e.,
n = 30.
To our surprise, the LDI-A performed slightly
better than the LDI-E even though the perfor-
mance difference was marginal. We expected that
LDI-A would perform worse than the LDI-E be-
cause LDI-A uses the aggressive approximation
for faster speed. We have not found the exact
cause of this interesting phenomenon, but remov-
ing latent paths with low probabilities may resem-
ble the strategy of pruning features with low fre-
quency in the training phase. Further analysis is
required in the future.
The LDI-A significantly outperformed the BHP
and the BMP, with a comparable inference time.
Also, all models of DPLVMs significantly outper-
formed CRFs.
5.2 NP-Chunking
As can be seen in Figure 5, compared to Figure 4
of the Bio-NER task, very similar curves were ob-
served in the NP-chunking task. It is interesting
because the tasks are different, and their feature
sets are very different.
The F-measure reached its plateau when n was
around 30, with a fast inference speed. This
echoes the experimental results on the Bio-NER
task. Moreover, as can be seen in Table 4, at a
much lower cost on inference time, the LDI-A per-
formed as well as the LDI-E. The LDI-A outper-
forms the BHP inference. All the DPLVM mod-
els outperformed CRFs. The experimental results
demonstrate that the LDI also works well on poor
feature-set.
89
89.2
89.4
89.6
89.8
0K 2K 4K 6K 8K 10K
F-
m
ea
su
re
(%
)
65
70
75
80
85
90
95
0K 2K 4K 6K 8K 10K
Ex
ac
tit
ud
e(%
)
0
200
400
600
800
0K 2K 4K 6K 8K 10K
#l
at
en
t-p
at
h
0
0.2
0.4
0.6
0.8
0K 2K 4K 6K 8K 10K
Ti
m
e(K
s)
n
89
89.2
89.4
89.6
89.8
0 50 100 150 200 250
65
70
75
80
85
90
95
0 50 100 150 200 250
0
200
400
600
800
0 50 100 150 200 250
0
0.2
0.4
0.6
0.8
0 50 100 150 200 250
n
Figure 5: (Left) F-measure, exactitude, #latent-
path, and inference time of the DPLVM-LDI
model against the parameter n on the NP-
chunking development dataset. (Right) Enlarge-
ment of the beginning portion of the left figures.
The curves echo the results on the Bio-NER task.
5.3 Post-Processing of the LDI: Minimum
Bayesian Risk Reranking
Although the label sequence produced by the LDI
inference is indeed the optimal label sequence by
means of probability, in practice, it may be benefi-
cial to use some post-processing methods to adapt
the LDI towards factual evaluation metrics. For
example, in practice, many natural language pro-
cessing tasks are evaluated by F-measures based
on chunks (e.g., named entities).
We further describe in this section the MBR
reranking method for the LDI. Here MBR rerank-
ing can be seen as a natural extension of the LDI
for adapting it to various evaluation criterions,
EVAL:
yMBR=argmax
y
?
y??LPn
P (y?)fEVAL(y|y?). (13)
The intuition behind our MBR reranking is the
778
Models Pre. Rec. F1 Time
LDI-A 91.76 90.59 91.17 42 s
LDI-A + MBR 92.22 90.40 91.30 61 s
Table 5: The effect of MBR reranking on the NP-
chunking task. As can be seen, MBR-reranking
improved the performance of the LDI.
?voting? by those results (label paths) produced by
the LDI inference. Each label path is a voter, and
it gives another one a ?score? (the score depend-
ing on the reference y? and the evaluation met-
ric EVAL, i.e., fEVAL(y|y?)) with a ?confidence?
(the probability of this voter, i.e., P (y?)). Finally,
the label path with the highest value, combining
scores and confidences, will be the optimal result.
For more details of the MBR technique, refer to
Goel & Byrne (2000) and Kumar & Byrne (2002).
An advantage of the LDI over the BHP and the
BMP is that the LDI can efficiently produce the
probabilities of the label sequences in LPn. Such
probabilities can be used directly for performing
the MBR reranking. We will show that it is easy
to employ the MBR reranking for the LDI, be-
cause the necessary statistics (e.g., the probabili-
ties of the label paths, y1,y2, . . .yn) are already
produced. In other words, by using LDI infer-
ence, a set of possible label sequences has been
collected with associated probabilities. Although
the cardinality of the set may be small, it accounts
for most of the probability mass by the definition
of the LDI. Eq.13 can be directly applied on this
set to perform reranking.
In contrast, the BHP and the BMP inference are
unable to provide such information for the rerank-
ing. For this reason, we can only report the results
of the reranking for the LDI.
As can be seen in Table 5, MBR-reranking im-
proved the performance of the LDI on the NP-
chunking task with a poor feature set. The pre-
sented MBR reranking algorithm is a general so-
lution for various evaluation criterions. We can
see that the different evaluation criterion, EVAL,
shares the common framework in Eq. 13. In prac-
tice, it is only necessary to re-implement the com-
ponent of fEVAL(y,y?) for a different evaluation
criterion. In this paper, the evaluation criterion is
the F-measure.
6 Conclusions and Future Work
In this paper, we propose an inference method, the
LDI, which is able to decode the optimal label se-
quence on latent conditional models. We study
the properties of the LDI, and showed that it can
be approximated aggressively for high efficiency,
with no loss in the performance. On the two NLP
tasks, the LDI-A outperformed the existing infer-
ence methods on latent conditional models, and its
inference time was comparable to that of the exist-
ing inference methods.
We also briefly present a post-processing
method, i.e., MBR reranking, upon the LDI
algorithm for various evaluation purposes. It
demonstrates encouraging improvement on the
NP-chunking tasks. In the future, we plan to per-
form further experiments to make a more detailed
study on combining the LDI inference and the
MBR reranking.
The LDI inference algorithm is not necessarily
limited in linear-chain structure. It could be ex-
tended to other latent conditional models with tree
structure (e.g., syntactic parsing with latent vari-
ables), as long as it allows efficient combination
of search and dynamic-programming. This could
also be a future work.
Acknowledgments
We thank Xia Zhou, Yusuke Miyao, Takuya Mat-
suzaki, Naoaki Okazaki and Galen Andrew for en-
lightening discussions, as well as the anonymous
reviewers who gave very helpful comments. The
first author was partially supported by University
of Tokyo Fellowship (UT-Fellowship). This work
was partially supported by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan).
References
Phillip Blunsom, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for sta-
tistical machine translation. Proceedings of ACL?08.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
L. Gillick and S. Cox. 1989. Some statistical issues
in the comparison of speech recognition algorithms.
International Conference on Acoustics Speech and
Signal Processing, v1:532?535.
V. Goel and W. Byrne. 2000. Minimum bayes-risk au-
tomatic speech recognition. Computer Speech and
Language, 14(2):115?135.
779
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost path. IEEE Trans. On System Science and
Cybernetics, SSC-4(2):100?107.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
and Yuka Tateisi. 2004. Introduction to the bio-
entity recognition task at JNLPBA. Proceedings of
JNLPBA?04, pages 70?75.
S. Kumar and W. Byrne. 2002. Minimum bayes-
risk alignment of bilingual texts. Proceedings of
EMNLP?02.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of ICML?01, pages 282?
289.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic CFG with latent annotations.
Proceedings of ACL?05.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?chi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. Proceedings of ACL?06.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT-NAACL?07), pages 404?
411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems
20 (NIPS), pages 1153?1160, Cambridge, MA. MIT
Press.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunk-
ing. Proceedings of CoNLL?00, pages 127?132.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL?03.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING?08), pages 841?848.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260?269.
780
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790?798,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Fast Full Parsing by Linear-Chain Conditional Random Fields
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper presents a chunking-based dis-
criminative approach to full parsing. We
convert the task of full parsing into a series
of chunking tasks and apply a conditional
random field (CRF) model to each level
of chunking. The probability of an en-
tire parse tree is computed as the product
of the probabilities of individual chunk-
ing results. The parsing is performed in a
bottom-up manner and the best derivation
is efficiently obtained by using a depth-
first search algorithm. Experimental re-
sults demonstrate that this simple parsing
framework produces a fast and reasonably
accurate parser.
1 Introduction
Full parsing analyzes the phrase structure of a sen-
tence and provides useful input for many kinds
of high-level natural language processing such as
summarization (Knight and Marcu, 2000), pro-
noun resolution (Yang et al, 2006), and infor-
mation extraction (Miyao et al, 2008). One of
the major obstacles that discourage the use of full
parsing in large-scale natural language process-
ing applications is its computational cost. For ex-
ample, the MEDLINE corpus, a collection of ab-
stracts of biomedical papers, consists of 70 million
sentences and would require more than two years
of processing time if the parser needs one second
to process a sentence.
Generative models based on lexicalized PCFGs
enjoyed great success as the machine learning
framework for full parsing (Collins, 1999; Char-
niak, 2000), but recently discriminative models
attract more attention due to their superior accu-
racy (Charniak and Johnson, 2005; Huang, 2008)
and adaptability to new grammars and languages
(Buchholz and Marsi, 2006).
A traditional approach to discriminative full
parsing is to convert a full parsing task into a series
of classification problems. Ratnaparkhi (1997)
performs full parsing in a bottom-up and left-to-
right manner and uses a maximum entropy clas-
sifier to make decisions to construct individual
phrases. Sagae and Lavie (2006) use the shift-
reduce parsing framework and a maximum en-
tropy model for local classification to decide pars-
ing actions. These approaches are often called
history-based approaches.
A more recent approach to discriminative full
parsing is to treat the task as a single structured
prediction problem. Finkel et al (2008) incor-
porated rich local features into a tree CRF model
and built a competitive parser. Huang (2008) pro-
posed to use a parse forest to incorporate non-local
features. They used a perceptron algorithm to op-
timize the weights of the features and achieved
state-of-the-art accuracy. Petrov and Klein (2008)
introduced latent variables in tree CRFs and pro-
posed a caching mechanism to speed up the com-
putation.
In general, the latter whole-sentence ap-
proaches give better accuracy than history-based
approaches because they can better trade off deci-
sions made in different parts in a parse tree. How-
ever, the whole-sentence approaches tend to re-
quire a large computational cost both in training
and parsing. In contrast, history-based approaches
are less computationally intensive and usually pro-
duce fast parsers.
In this paper, we present a history-based parser
using CRFs, by treating the task of full parsing as
a series of chunking problems where it recognizes
chunks in a flat input sequence. We use the linear-
790
Estimated  volume  was   a   light  2.4  million  ounces  .
VBN         NN    VBD DT  JJ    CD     CD NNS   .
QPNP
Figure 1: Chunking, the first (base) level.
volume          was   a   light    million       ounces .
NP             VBD DT  JJ          QP            NNS   .
NP
Figure 2: Chunking, the 2nd level.
chain CRF model to perform chunking.
Although our parsing model falls into the cat-
egory of history-based approaches, it is one step
closer to the whole-sentence approaches because
the parser uses a whole-sequence model (i.e.
CRFs) for individual chunking tasks. In other
words, our parser could be located somewhere
between traditional history-based approaches and
whole-sentence approaches. One of our motiva-
tions for this work was that our parsing model
may achieve a better balance between accuracy
and speed than existing parsers.
It is also worth mentioning that our approach is
similar in spirit to supertagging for parsing with
lexicalized grammar formalisms such as CCG and
HPSG (Clark and Curran, 2004; Ninomiya et al,
2006), in which significant speed-ups for parsing
time are achieved.
In this paper, we show that our approach is in-
deed appealing in that the parser runs very fast
and gives competitive accuracy. We evaluate our
parser on the standard data set for parsing exper-
iments (i.e. the Penn Treebank) and compare it
with existing approaches to full parsing.
This paper is organized as follows. Section 2
presents the overall chunk parsing strategy. Sec-
tion 3 describes the CRF model used to perform
individual chunking steps. Section 4 describes the
depth-first algorithm for finding the best derivation
of a parse tree. The part-of-speech tagger used in
the parser is described in section 5. Experimen-
tal results on the Penn Treebank corpus are pro-
vided in Section 6. Section 7 discusses possible
improvements and extensions of our work. Sec-
tion 8 offers some concluding remarks.
volume          was                    ounces          .
NP             VBD                    NP           .
VP
Figure 3: Chunking, the 3rd level.
volume                           was                   .
NP                               VP                .
S
Figure 4: Chunking, the 4th level.
2 Full Parsing by Chunking
This section describes the parsing framework em-
ployed in this work.
The parsing process is conceptually very sim-
ple. The parser first performs chunking by iden-
tifying base phrases, and converts the identified
phrases to non-terminal symbols. It then performs
chunking for the updated sequence and converts
the newly recognized phrases into non-terminal
symbols. The parser repeats this process until the
whole sequence is chunked as a sentence
Figures 1 to 4 show an example of a parsing pro-
cess by this framework. In the first (base) level,
the chunker identifies two base phrases, (NP Es-
timated volume) and (QP 2.4 million), and re-
places each phrase with its non-terminal symbol
and head1. In the second level, the chunker iden-
tifies a noun phrase, (NP a light million ounces),
and converts it into NP. This process is repeated
until the whole sentence is chunked at the fourth
level. The full parse tree is recovered from the
chunking history in a straightforward way.
This idea of converting full parsing into a se-
ries of chunking tasks is not new by any means?
the history of this kind of approach dates back to
1950s (Joshi and Hopely, 1996). More recently,
Brants (1999) used a cascaded Markov model to
parse German text. Tjong Kim Sang (2001) used
the IOB tagging method to represent chunks and
memory-based learning, and achieved an f-score
of 80.49 on the WSJ corpus. Tsuruoka and Tsu-
jii (2005) improved upon their approach by using
1The head word is identified by using the head-
percolation table (Magerman, 1995).
791
 0
 1000
 2000
 3000
 4000
 5000
 0  5  10  15  20  25  30
# 
se
nt
en
ce
s
Height
Figure 5: Distribution of tree height in WSJ sec-
tions 2-21.
a maximum entropy classifier and achieved an f-
score of 85.9. However, there is still a large gap
between the accuracy of chunking-based parsers
and that of widely-used practical parsers such as
Collins parser and Charniak parser (Collins, 1999;
Charniak, 2000).
2.1 Heights of Trees
A natural question about this parsing framework is
how many levels of chunking are usually needed to
parse a sentence. We examined the distribution of
the heights of the trees in sections 2-21 of the Wall
Street Journal (WSJ) corpus. The result is shown
in Figure 5. Most of the sentences have less than
20 levels. The average was 10.0, which means we
need to perform, on average, 10 chunking tasks to
obtain a full parse tree for a sentence if the parsing
is performed in a deterministic manner.
3 Chunking with CRFs
The accuracy of chunk parsing is highly depen-
dent on the accuracy of each level of chunking.
This section describes our approach to the chunk-
ing task.
A common approach to the chunking problem
is to convert the problem into a sequence tagging
task by using the ?BIO? (B for beginning, I for
inside, and O for outside) representation. For ex-
ample, the chunking process given in Figure 1 is
expressed as the following BIO sequences.
B-NP I-NP O O O B-QP I-QP O O
This representation enables us to use the linear-
chain CRF model to perform chunking, since the
task is simply assigning appropriate labels to a se-
quence.
3.1 Linear Chain CRFs
A linear chain CRF defines a single log-linear
probabilistic distribution over all possible tag se-
quences y for the input sequence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x),
where fk(t, yt, yt?1,x) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x).
This model allows us to define features on states
and edges combined with surface observations.
The weights of the features are determined in
such a way that they maximize the conditional log-
likelihood of the training data:
L? =
N
?
i=1
log p(y(i)|x(i)) + R(?),
where R(?) is introduced for the purpose of regu-
larization which prevents the model from overfit-
ting the training data. The L1 or L2 norm is com-
monly used in statistical natural language process-
ing (Gao et al, 2007). We used L1-regularization,
which is defined as
R(?) = 1C
K
?
k=1
|?k|,
where C is the meta-parameter that controls the
degree of regularization. We used the OWL-QN
algorithm (Andrew and Gao, 2007) to obtain the
parameters that maximize the L1-regularized con-
ditional log-likelihood.
3.2 Features
Table 1 shows the features used in chunking for
the base level. Since the task is basically identical
to shallow parsing by CRFs, we follow the feature
sets used in the previous work by Sha and Pereira
(2003). We use unigrams, bigrams, and trigrams
of part-of-speech (POS) tags and words.
The difference between our CRF chunker and
that in (Sha and Pereira, 2003) is that we could
not use second-order CRF models, hence we could
not use trigram features on the BIO states. We
792
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Word Unigrams h?2, h?1, h0, h+1, h+2
Word Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Word Trigrams h?1h0h+1
Table 1: Feature templates used in the base level chunking. s represents a terminal symbol (i.e. POS tag)
and the subscript represents a relative position. h represents a word.
found that using second order CRFs in our task
was very difficult because of the computational
cost. Recall that the computational cost for CRFs
is quadratic to the number of possible states. In
our task, we need to consider the states for all non-
terminal symbols, whereas their work is only con-
cerned with noun phrases.
Table 2 shows feature templates used in the non-
base levels of chunking. In the non-base levels of
chunking, we can use a richer set of features than
the base-level chunking because the chunker has
access to the information about the partial trees
that have been already created. In addition to the
features listed in Table 1, the chunker looks into
the daughters of the current non-terminal sym-
bol and use them as features. It also uses the
words and POS tags around the edges of the re-
gion covered by the current non-terminal symbol.
We also added a special feature to better capture
PP-attachment. The chunker looks at the head of
the second daughter of the prepositional phrase to
incorporate the semantic head of the phrase.
4 Searching for the Best Parse
The probability for an entire parse tree is com-
puted as the product of the probabilities output by
the individual CRF chunkers:
score =
h
?
i=0
p(yi|xi), (1)
where i is the level of chunking and h is the height
of the tree. The task of full parsing is then to
choose the series of chunking results that maxi-
mizes this probability.
It should be noted that there are cases where
different derivations (chunking histories) lead to
the same parse tree (i.e. phrase structure). Strictly
speaking, therefore, what we describe here as the
probability of a parse tree is actually the proba-
bility of a single derivation. The probabilities of
the derivations should then be marginalized over
to produce the probability of a parse tree, but in
this paper we ignore this effect and simply focus
only on the best derivation.
We use a depth-first search algorithm to find the
highest probability derivation. Figure 6 shows the
algorithm in pseudo-code. The parsing process is
implemented with a recursive function. In each
level of chunking, the recursive function first in-
vokes a CRF chunker to obtain chunking hypothe-
ses for the given sequence. For each hypothesis
whose probability is high enough to have possibil-
ity of constituting the best derivation, the function
calls itself with the sequence updated by the hy-
pothesis. The parsing process is performed in a
bottom up manner and this recursive process ter-
minates if the whole sequence is chunked as a sen-
tence.
To extract multiple chunking hypotheses from
the CRF chunker, we use a branch-and-bound
algorithm rather than the A* search algorithm,
which is perhaps more commonly used in previous
studies. We do not give pseudo code, but the ba-
sic idea is as follows. It first performs the forward
Viterbi algorithm to obtain the best sequence, stor-
ing the upper bounds that are used for pruning in
branch-and-bound. It then performs a branch-and-
bound algorithm in a backward manner to retrieve
possible candidate sequences whose probabilities
are greater than the given threshold. Unlike A*
search, this method is memory efficient because it
is performed in a depth-first manner and does not
require priority queues for keeping uncompleted
hypotheses.
It is straightforward to introduce beam search in
this search algorithm?we simply limit the num-
ber of hypotheses generated by the CRF chunker.
We examine how the width of the beam affects the
parsing performance in the experiments.
793
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Head Unigrams h?2, h?1, h0, h+1, h+2
Head Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Head Trigrams h?1h0h+1
Symbol & Daughters s0d01, ... s0d0m
Symbol & Word/POS context s0wj?1, s0pj?1, s0wk+1 , s0pk+1
Symbol & Words on the edges s0wj , s0wk
Freshness whether s0 has been created in the level just below
PP-attachment h?1h0m02 (only when s0 = PP)
Table 2: Feature templates used in the upper level chunking. s represents a non-terminal symbol. h
represents a head percolated from the bottom for each symbol. d0i is the ith daughter of s0. wj is the
first word in the range covered by s0. wj?1 is the word preceding wj . wk is the last word in the range
covered by s0. wk+1 is the word following wk. p represents POS tags. m02 represents the head of the
second daughter of s0.
Word Unigram w?2, w?1, w0, w+1, wi+2
Word Bigram w?1w0, w0w+1, w?1w+1
Prefix, Suffix prefixes of w0
suffixes of w0
(up to length 10)
Character features w0 has a hyphen
w0 has a number
w0 has a capital letter
w0 is all capital
Normalized word N(w0)
Table 3: Feature templates used in the POS tagger.
w represents a word and the subscript represents a
relative position.
5 Part-of-Speech Tagging
We use the CRF model also for POS tagging.
The CRF-based POS tagger is incorporated in the
parser in exactly the same way as the other lay-
ers of chunking. In other words, the POS tagging
process is treated like the bottom layer of chunk-
ing, so the parser considers multiple probabilistic
hypotheses output by the tagger in the search al-
gorithm described in the previous section.
5.1 Features
Table 3 shows the feature templates used in the
POS tagger. Most of them are standard features
commonly used in POS tagging for English. We
used unigrams and bigrams of neighboring words,
prefixes and suffixes of the current word, and some
characteristics of the word. We also normalized
the current word by lowering capital letters and
converting all the numerals into ?#?, and used the
normalized word as a feature.
6 Experiments
We ran parsing experiments using the Wall Street
Journal corpus. Sections 2-21 were used as the
training data. Section 22 was used as the devel-
opment data, with which we tuned the feature set
and parameters for learning and parsing. Section
23 was reserved for the final accuracy report.
The training data for the CRF chunkers were
created by converting each parse tree in the train-
ing data into a list of chunking sequences like
the ones presented in Figures 1 to 4. We trained
three CRF models, i.e., the POS tagging model,
the base chunking model, and the non-base chunk-
ing model. The training took about two days on a
single CPU.
We used the evalb script provided by Sekine and
Collins for evaluating the labeled recall/precision
of the parser outputs2. All experiments were car-
ried out on a server with 2.2 GHz AMD Opteron
processors and 16GB memory.
6.1 Chunking Performance
First, we describe the accuracy of individual
chunking processes. Table 4 shows the results
for the ten most frequently occurring symbols on
the development data. Noun phrases (NP) are the
2The script is available at http://nlp.cs.nyu.edu/evalb/. We
used the parameter file ?COLLINS.prm?.
794
1: procedure PARSESENTENCE(x)
2: PARSE(x, 1, 0)
3:
4: function PARSE(x, p, q)
5: if x is chunked as a complete sentence
6: return p
7: H ? PERFORMCHUNKING(x, q/p)
8: for h ? H in descending order of their
probabilities do
9: r ? p? h.probability
10: if r > q then
11: x? ? UPDATESEQUENCE(x, h)
12: s? PARSE(x?, r, q)
13: if s > q then
14: q ? s
15: return q
16:
17: function PERFORMCHUNKING(x, t)
18: perform chunking with a CRF chunker and
19: return a set of chunking hypotheses whose
20: probabilities are greater than t.
21:
22: function UPDATESEQUENCE(x, h)
23: update sequence x according to chunking
24: hypothesis h and return the updated
25: sequence.
Figure 6: Searching for the best parse with a
depth-first search algorithm. This pseudo-code il-
lustrates how to find the highest probability parse,
but in the real implementation, the function needs
to keep track of chunking histories as well as prob-
abilities.
most common symbol and consist of 55% of all
phrases. The accuracy of noun phrases recognition
was relatively high, but it may be useful to design
special features for this particular type of phrase,
considering the dominance of noun phrases in the
corpus. Although not directly comparable, Sha
and Pereira (2003) report almost the same level
of accuracy (94.38%) on noun phrase recognition,
using a much smaller training set. We attribute
their superior performance mainly to the use of
second-order features on state transitions. Table 4
also suggests that adverb phrases (ADVP) and ad-
jective phrases (ADJP) are more difficult to recog-
nize than other types of phrases, which coincides
with the result reported in (Collins, 1999).
It should be noted that the performance reported
in this table was evaluated using the gold standard
sequences as the input to the CRF chunkers. In the
Symbol # Samples Recall Prec. F-score
NP 317,597 94.79 94.16 94.47
VP 76,281 91.46 91.98 91.72
PP 66,979 92.84 92.61 92.72
S 33,739 91.48 90.64 91.06
ADVP 21,686 84.25 85.86 85.05
ADJP 14,422 77.27 78.46 77.86
QP 14,308 89.43 91.16 90.28
SBAR 11,603 96.42 96.97 96.69
WHNP 8,827 95.54 97.50 96.51
PRT 3,391 95.72 90.52 93.05
: : : : :
all 579,253 92.63 92.62 92.63
Table 4: Chunking performance (section 22, all
sentences).
Beam Recall Prec. F-score Time (sec)
1 86.72 87.83 87.27 16
2 88.50 88.85 88.67 41
3 88.69 89.08 88.88 61
4 88.72 89.13 88.92 92
5 88.73 89.14 88.93 119
10 88.68 89.19 88.93 179
Table 5: Beam width and parsing performance
(section 22, all sentences).
real parsing process, the chunkers have to use the
output from the previous (one level below) chun-
ker, so the quality of the input is not as good as
that used in this evaluation.
6.2 Parsing Performance
Next, we present the actual parsing performance.
The first set of experiments concerns the relation-
ship between the width of beam and the parsing
performance. Table 5 shows the results obtained
on the development data. We varied the width of
the beam from 1 to 10. The beam width of 1 cor-
responds to deterministic parsing. Somewhat un-
expectedly, the parsing accuracy did not drop sig-
nificantly even when we reduced the beam width
to a very small number such as 2 or 3.
One of the interesting findings was that re-
call scores were consistently lower than precision
scores throughout all experiments. A possible rea-
son is that, since the score of a parse is defined
as the product of all chunking probabilities, the
parser could prefer a parse tree that consists of
a small number of chunk layers. This may stem
795
from the history-based model?s inability of prop-
erly trading off decisions made by different chun-
kers.
Overall, the parsing speed was very high. The
deterministic version (beam width = 1) parsed
1700 sentences in 16 seconds, which means that
the parser needed only 10 msec to parse one sen-
tence. The parsing speed decreases as we increase
the beam width.
The parser was also memory efficient. Thanks
to L1 regularization, the training process did not
result in many non-zero feature weights. The num-
bers of non-zero weight features were 58,505 (for
the base chunker), 263,889 (for the non-base chun-
ker), and 42,201 (for the POS tagger). The parser
required only 14MB of memory to run.
There was little accuracy difference between the
beam width of 4 and 5, so we adopted the beam
width of 4 for the final accuracy report on the test
data.
6.3 Comparison with Previous Work
Table 6 shows the performance of our parser on
the test data and summarizes the results of previ-
ous work. Our parser achieved an f-score of 88.4
on the test data, which is comparable to the accu-
racy achieved by recent discriminative approaches
such as Finkel et al (2008) and Petrov & Klein
(2008), but is not as high as the state-of-the-art
accuracy achieved by the parsers that can incor-
porate global features such as Huang (2008) and
Charniak & Johnson (2005). Our parser was more
accurate than traditional history-based approaches
such as Sagae & Lavie (2006) and Ratnaparkhi
(1997), and was significantly better than previous
cascaded chunking approaches such as Tsuruoka
& Tsujii (2005) and Tjong Kim Sang (2001).
Although the comparison presented in the table
is not perfectly fair because of the differences in
hardware platforms, the results show that our pars-
ing model is a promising addition to the parsing
frameworks for building a fast and accurate parser.
7 Discussion
One of the obvious ways to improve the accuracy
of our parser is to improve the accuracy of in-
dividual CRF models. As mentioned earlier, we
were not able to use second-order features on state
transitions, which would have been very useful,
due to the problem of computational cost. Incre-
mental feature selection methods such as grafting
(Perkins et al, 2003) may help us to incorporate
such higher-order features, but the problem of de-
creased efficiency of dynamic programming in the
CRF would probably need to be addressed.
In this work, we treated the chunking problem
as a sequence labeling problem by using the BIO
representation for the chunks. However, semi-
Markov conditional random fields (semi-CRFs)
can directly handle the chunking problem by
considering all possible combinations of subse-
quences of arbitrary length (Sarawagi and Cohen,
2004). Semi-CRFs allow one to use a richer set
of features than CRFs, so the use of semi-CRFs
in our parsing framework should lead to improved
accuracy. Moreover, semi-CRFs would allow us to
incorporate some useful restrictions in producing
chunking hypotheses. For example, we could nat-
urally incorporate the restriction that every chunk
has to contain at least one symbol that has just
been created in the previous level3. It is hard for
the normal CRF model to incorporate such restric-
tions.
Introducing latent variables into the CRF model
may be another promising approach. This is the
main idea of Petrov and Klein (2008), which sig-
nificantly improved parsing accuracy.
A totally different approach to improving the
accuracy of our parser is to use the idea of ?self-
training? described in (McClosky et al, 2006).
The basic idea is to create a larger set of training
data by applying an accurate parser (e.g. rerank-
ing parser) to a large amount of raw text. We can
then use the automatically created treebank as the
additional training data for our parser. This ap-
proach suggests that accurate (but slow) parsers
and fast (but not-so-accurate) parsers can actually
help each other.
Also, since it is not difficult to extend our parser
to produce N-best parsing hypotheses, one could
build a fast reranking parser by using the parser as
the base (hypotheses generating) parser.
8 Conclusion
Although the idea of treating full parsing as a se-
ries of chunking problems has a long history, there
has not been a competitive parser based on this
parsing framework. In this paper, we have demon-
strated that the framework actually enables us to
3For example, the sequence VBD DT JJ in Figure 2 can-
not be a chunk in the current level because it would have been
already chunked in the previous level if it were.
796
Recall Precision F-score Time (min)
This work (deterministic) 86.3 87.5 86.9 0.5
This work (search, beam width = 4) 88.2 88.7 88.4 1.7
Huang (2008) 91.7 Unk
Finkel et al (2008) 87.8 88.2 88.0 >250*
Petrov & Klein (2008) 88.3 3*
Sagae & Lavie (2006) 87.8 88.1 87.9 17
Charniak & Johnson (2005) 90.6 91.3 91.0 Unk
Tsuruoka & Tsujii (2005) 85.0 86.8 85.9 2
Collins (1999) 88.1 88.3 88.2 39**
Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk
Charniak (2000) 89.6 89.5 89.5 23**
Ratnaparkhi (1997) 86.3 87.5 86.9 Unk
Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the
training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the
parsers.
build a competitive parser if we use CRF mod-
els for each level of chunking and a depth-first
search algorithm to search for the highest proba-
bility parse.
Like other discriminative learning approaches,
one of the advantages of our parser is its general-
ity. The design of our parser is very generic, and
the features used in our parser are not particularly
specific to the Penn Treebank. We expect it to be
straightforward to adapt the parser to other projec-
tive grammars and languages.
This parsing framework should be useful when
one needs to process a large amount of text or
when real time processing is required, in which
the parsing speed is of top priority. In the deter-
ministic setting, our parser only needed about 10
msec to parse a sentence.
Acknowledgments
This work described in this paper has been
funded by the Biotechnology and Biological Sci-
ences Research Council (BBSRC; BB/E004431/1)
and the European BOOTStrep project (FP6 -
028099). The research team is hosted by the
JISC/BBSRC/EPSRC sponsored National Centre
for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
Thorsten Brants. 1999. Cascaded markov models. In
Proceedings of EACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL 2000,
pages 132?139.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING 2004, pages 282?
288.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08:HLT, pages 586?594.
Aravind K. Joshi and Phil Hopely. 1996. A parser
from antiquity. Natural Language Engineering,
2(4):291?294.
797
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI/IAAI, pages 703?710.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL.
Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08:HLT, pages 46?54.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP 2006,
pages 155?163.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems 20
(NIPS), pages 1153?1160.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP 1997, pages 1?10.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 691?698.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proceedings of NIPS.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL.
Erik Tjong Kim Sang. 2001. Transforming a chunker
to a parser. In J. Veenstra W. Daelemans, K. Sima?an
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000, pages 177?188. Rodopi.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of IWPT, pages
133?140.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic features. In Proceedings of COLING/ACL,
pages 41?48.
798
Bilingual Synonym Identification with Spelling Variations
Takashi Tsunakawa? Jun?ichi Tsujii???
?Department of Computer Science,
Graduate School of Information Science and Technology, University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan
?School of Computer Science, University of Manchester
Oxford Road, Manchester, M13 9PL, UK
?National Centre for Text Mining 131 Princess Street, Manchester, M1 7DN, UK
{tuna, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper proposes a method for identify-
ing synonymous relations in a bilingual lex-
icon, which is a set of translation-equivalent
term pairs. We train a classifier for identi-
fying those synonymous relations by using
spelling variations as main clues. We com-
pared two approaches: the direct identifi-
cation of bilingual synonym pairs, and the
merger of two monolingual synonyms. We
showed that our approach achieves a high
pair-wise precision and recall, and outper-
forms the baseline method.
1 Introduction
Automatically collecting synonyms from language
resources is an ongoing task for natural language
processing (NLP). Most NLP systems have diffi-
culties in dealing with synonyms, which are differ-
ent representations that have the same meaning in
a language. Information retrieval (IR) could lever-
age synonyms to improve the coverage of search re-
sults (Qiu and Frei, 1993). For example, when we
input the query ?transportation in India? into an IR
system, the system can expand the query to its syn-
onyms; e.g. ?transport? and ?railway?, to find more
documents.
This paper proposes a method for the automatic
identification of bilingual synonyms in a bilingual
lexicon, with spelling variation clues. A bilingual
synonym set is a set of translation-equivalent term
pairs sharing the same meaning. Although a number
of studies have aimed at identifying synonyms, this
is the first study that simultaneously finds synonyms
in two languages, to our best knowledge.
Let us consider the case where a user enters the
Japanese query ?ko?jo?? (??, industrial plant) into a
cross-lingual IR system to find English documents.
After translating the query into the English trans-
lation equivalent, ?plant,? the cross-lingual IR sys-
tem may expand the query to its English synonyms,
e.g. ?factory,? and ?workshop,? and retrieve docu-
ments that include the expanded terms. However,
the term ?plant? is ambiguous; the system may also
expand the query to ?vegetable,? and the system is
prevented by the term which is different from our
intention. In contrast, the system can easily reject
the latter expansion, ?vegetable,? if we are aware of
bilingual synonyms, which indicate synonymous re-
lations over bilingual lexicons: (ko?jo?, plant)? (ko?jo?,
factory) and (shokubutsu1, plant) ? (shokubutsu,
vegetable)2 (See Figure 1). The expression of the
translation equivalent, (ko?jo?, plant), helps a cross-
lingual IR system to retrieve documents that include
the term ?plant,? used in the meaning for ko?jo?, or in-
dustrial plants.
We present a supervised machine learning ap-
proach for identifying bilingual synonyms. Design-
ing features for bilingual synonyms such as spelling
variations and bilingual associations, we train a clas-
sifier with a manually annotated bilingual lexicon
with synonymous information. In order to evaluate
the performance of our method, we carried out ex-
periments to identify bilingual synonyms by two ap-
proaches: the direct identification of bilingual syn-
onym pairs, and bilingual synonym pairs merged
from two monolingual synonym lists. Experimental
results show that our approach achieves the F-scores
1Shokubutsu (??) means botanical plant.
2??? represents the synonymous relation.
457
Figure 1: An example of an ambiguous term ?plant?,
and the synonyms and translation equivalents (TE)
89.3% in the former approach and 91.4% in the lat-
ter, thus outperforming the baseline method that em-
ploys only bilingual relations as its clues.
The remainder of this paper is organized as fol-
lows. The next section describes related work on
synonym extraction and spelling variations. Section
3 describes the overview and definition of bilingual
synonyms, the proposed method and employed fea-
tures. In Section 4 we evaluate our method and con-
clude this paper.
2 Related work
There have been many approaches for detecting syn-
onyms and constructing thesauri. Two main re-
sources for synonym extraction are large text cor-
pora and dictionaries.
Many studies extract synonyms from large mono-
lingual corpora by using context information around
target terms (Croach and Yang, 1992; Park and Choi,
1996; Waterman, 1996; Curran, 2004). Some re-
searchers (Hindle, 1990; Grefenstette, 1994; Lin,
1998) classify terms by similarities based on their
distributional syntactic patterns. These methods of-
ten extract not only synonyms, but also semantically
related terms, such as antonyms, hyponyms and co-
ordinate terms such as ?cat? and ?dog.?
Some studies make use of bilingual corpora or
dictionaries to find synonyms in a target language
(Barzilay and McKeown, 2001; Shimohata and
Sumita, 2002; Wu and Zhou, 2003; Lin et al, 2003).
Lin et al (2003) chose a set of synonym candidates
for a term by using a bilingual dictionary and com-
puting distributional similarities in the candidate set
to extract synonyms. They adopt the bilingual in-
formation to exclude non-synonyms (e.g., antonyms
and hyponyms) that may be used in the similar con-
texts. Although they make use of bilingual dictio-
naries, this study aims at finding bilingual synonyms
directly.
In the approaches based on monolingual dictio-
naries, the similarities of definitions of lexical items
are important clues for identifying synonyms (Blon-
del et al, 2004; Muller et al, 2006). For instance,
Blondel et al (2004) constructed an associated dic-
tionary graph whose vertices are the terms, and
whose edges from v1 to v2 represent occurrence of
v2 in the definition for v1. They choose synonyms
from the graph by collecting terms pointed to and
from the same terms.
Another strategy for finding synonyms is to con-
sider the terms themselves. We divide it into two
approaches: rule-based and distance-based.
Rule-based approaches implement rules with
language-specific patterns and detect variations by
applying rules to terms. Stemming (Lovins, 1968;
Porter, 1980) is one of the rule-based approaches,
which cuts morphological suffix inflections, and ob-
tains the stems of words. There are other types of
variations for phrases; for example, insertion, dele-
tion or substitution of words, and permutation of
words such as ?view point? and ?point of view? are
such variations (Daille et al, 1996).
Distance-based approaches model the similarity
or dissimilarity measure between two terms to find
similar terms. The edit distance (Levenshtein, 1966)
is the most widely-used measure, based on the mini-
mum number of operations of insertion, deletion, or
substitution of characters for transforming one term
into another. It can be efficiently calculated by using
458
Term pairs Concept
p1 = (sho?mei (??), light) c1
p2 = (sho?mei, lights) c1
p3 = (karui (??), light) c2
p4 = (raito (???), light) c1, c2
p5 = (raito, lights) c1
p6 = (raito, right) c3
p7 = (migi (?), right) c3
p8 = (raito, right fielder) c4
p9 = (kenri (??), right) c5
p10 = (kenri, rights) c5
Table 1: An Example of a bilingual lexicon and syn-
onym sets (concepts)
J terms E terms Description
c1 sho?mei, raito light, lights illumination
c2 karui, raito light lightweight
c3 migi, raito right right-side
c4 raito right fielder (baseball)
c5 kenri right, rights privilege
Table 2: The concepts in Table 1
a dynamic programming algorithm, and we can set
the costs/weights for each character type.
3 Bilingual Synonyms and Translation
Equivalents
This section describes the notion of bilingual syn-
onyms and our method for identifying the synony-
mous pairs of translation equivalents. We consider a
bilingual synonym as a set of translation-equivalent
term pairs referring to the same concept.
Tables 1 and 2 are an example of bilingual
synonym sets. There are ten Japanese-English
translation-equivalent term pairs and five bilingual
synonym sets in this example. A Japanese term
?raito? is the phonetic transcription of both ?light?
and ?right,? and it covers four concepts described by
the three English terms. Figure 2 illustrates the re-
lationship among these terms. The synonymous re-
lation and the translation equivalence are considered
to be similar in that two terms share the meanings.
Following synonymous relation between terms in
one language, we deal with the synonymous relation
between bilingual translation-equivalent term pairs
Figure 2: Relations among terms in Table 2
Solid lines show that two terms are translation
equivalents, while dotted lines show that two terms
are (monolingual) synonyms.
as bilingual synonyms. The advantage of manag-
ing the lexicon in the format of bilingual synonyms
is that we can facilitate to tie the concepts and the
terms.
3.1 Definitions
Let E and F be monolingual lexicons. We first as-
sume that a term e ? E (or f ? F ) refers to one
or more concepts, and define that a term e is a syn-
onym3 of e?(? E) if and only if e and e? share an
identical concept4. Let ??? represent the synony-
mous relation, and this relation is not transitive be-
cause a term often has several concepts:
e ? e? ? e? ? e?? 6=? e ? e??. (1)
We define a synonym set (synset) Ec as a set whose
elements share an identical concept c: Ec = {e ?
E|?e refers to c}. For a term set Ec(? E),
Ec is a synonym set (synset)
=? ?e, e? ? Ec e ? e? (2)
is true, but the converse is not necessarily true, be-
cause of the ambiguity of terms. Note that one term
can belong to multiple synonym sets from the defi-
nition.
Let D(? F ? E) be a bilingual lexicon defined
as a set of term pairs (f, e) (f ? F, e ? E) satis-
fying that f and e refer to an identical concept. We
3For distinguishing from bilingual synonyms, we often call
the synonym a monolingual synonym.
4The definition of concepts, that is, the criteria of deciding
whether two terms are synonymous or not, is beyond the fo-
cus of this paper. We do not assume that related terms such as
hypernyms, hyponyms and coordinates are kinds of synonyms.
In our experiments the criteria depend on manual annotation of
synonym IDs in the training data.
459
call these pairs translation equivalents, which refer
to concepts that both f and e refer to. We define
that two bilingual lexical items p and p?(? D) are
bilingual synonyms if and only if p and p? refer to an
identical concept in common with the definition of
(monolingual) synonyms. This relation is not tran-
sitive again, and if e ? e? and f ? f ?, it is not
necessarily true that p ? p?:
e ? e? ? f ? f ? 6=? p ? p? (3)
because of the ambiguity of terms. Similarly, we
can define a bilingual synonym set (synset) Dc as
a set whose elements share an identical meaning c:
Dc = {p ? D|?p refers to c}. For a set of transla-
tion eqiuvalents Dc,
Dc is a bilingual synonym set (synset)
=? ?p, p? ? Dc p ? p? (4)
is true, but the converse is not necessarily true.
3.2 Identifying bilingual synonym pairs
In this section, we describe an algorithm to identify
bilingual synonym pairs by using spelling variation
clues. After identifying the pairs, we can construct
bilingual synonym sets by assuming that the con-
verse of the condition (4) is true, and finding sets
of bilingual lexical items in which all paired items
are bilingual synonyms. We can see this method
as the complete-linkage clustering of translation-
equivalent term pairs. We can adopt another option
to construct them by assuming also that the bilingual
synonymous relation has transitivity: p ? p? ? p? ?
p?? =? p ? p??, and this can be seen as simple-
linkage clustering. This simplified method ignores
the ambiguity of terms, and it may construct a bilin-
gual synonym sets which includes many senses. In
spite of the risk, it is effective to find large synonym
sets in case the bilingual synonym pairs are not suf-
ficiently detected. In this paper we focus only on
identifying bilingual synonym pairs and evaluating
the performance of the identification.
We employ a supervised machine learning tech-
nique with features related to spelling variations
and so on. Figure 3 shows the framework for this
method. At first we prepare a bilingual lexicon with
synonymous information as training data, and gen-
erate a list consisting of all bilingual lexical item
Figure 3: Overview of our framework
pairs in the bilingual lexicon. The presence or ab-
sence of bilingual synonymous relations is attached
to each element of the list. Then, we build a classi-
fier learned by training data, using a maximum en-
tropy model (Berger et al, 1996) and the features
related to spelling variations in Table 3.
We apply some preprocessings for extracting
some features. For English, we transform all terms
into lower-case, and do not apply any other trans-
formations such as tokenization by symbols. For
Japanese, we apply a morphological analyzer JU-
MAN (Kurohashi et al, 1994) and obtain hiragana
representations5 as much as possible6. We may re-
quire other language-specific preprocessings for ap-
plying this method to other languages.
We employed binary or real-valued features de-
scribed in Table 3. Moreover, we introduce
the following combinatorial features: h1F ? h1E ,?
h2F ? h2E ,
?
h3F ? h3E , h5E ? h5F , h6 ? h2F and
h7 ? h2E .
3.2.1 Two approaches for identifying bilingual
synonym pairs
There are two approaches for identifying bilin-
gual synonym pairs: one is directly identifying
whether two bilingual lexical items are bilingual
synonyms (?bilingual? method), and another is first
5Hiragana is one of normalized representations of Japanese
terms, which denotes how to pronounce the term. Japanese vo-
cabulary has many of homonyms, which are semantically differ-
ent but have the same pronunciation. Despite the risk of classi-
fying homonyms into synonyms, we do not use original forms
of Japanese terms because they are typically too short to extract
character similarities.
6We keep unknown terms of JUMAN unchanged.
460
h1F , h1E : Agreement of the
first characters
Whether the first characters match or not
h2F , h2E : Normalized edit
distance
1? ED(w,w
?)
max(|w|,|w?|) , where ED(w,w
?) is a non-weighted edit distance between w and w? and
|w| is the number of characters in w
h3F , h3E : Bigram similarity |bigram(w)?bigram(w
?)|
max(|w|,|w?|)?1 , where bigram(w) is a multiset of character-based bigrams in w
h4F , h4E : Agreement or
known synonymous relation
of word sub-sequences
The count that sub-sequences of the target terms match as known terms or are in known
synonymous relation
h5F , h5E : Existence of cross-
ing bilingual lexical items
For bilingual lexical items (f1, e1) and (f2, e2), whether (f1, e2) (for h5F ) or (f2, e1) (for
h5E) is in the bilingual lexicon of the training set
h6: Acronyms Whether one English term is an acronym for another (Schwartz and Hearst, 2003)
h7: Katakana variants Whether one Japanese term is a katakana variant for another (Masuyama et al, 2004)
Table 3: Features used for identifying bilingual synonym pairs
hiF is the feature value when the terms w and w?(? F ) are compared in the i-th feature and so as hiE . h6 is
only for English and h7 is only for Japanese.
identifying monolingual synonyms in each language
and then merging them according to the bilingual
items (?monolingual? method). We implement these
two approaches and compare the results. For identi-
fying monolingual synonyms, we use features with
bilingual items as follows: For a term pair e1 and
e2, we obtain all the translation candidates F1 =
{f |(f, e1) ? D} and F2 = {f ?|(f ?, e2) ? D},
and calculate feature values related to F1 and/or F2
by obtaining the maximum feature value using F1
and/or F2. After that, if all the following four con-
ditions (p1 = (f1, e1) ? D, p2 = (f2, e2) ? D,
f1 ? e1 and f2 ? e2) are satisfied, we assume that
p1 and p2 are bilingual synonym pairs7.
4 Experiment
4.1 Experimental settings
We performed experiments to identify bilingual syn-
onym pairs by using the Japanese-English lexicon
with synonymous information8. The lexicon con-
sists of translation-equivalent term pairs extracted
from titles and abstracts of scientific papers pub-
lished in Japan. It contains many spelling variations
and synonyms for constructing and maintaining the
7Actually, these conditions are not sufficient to derive the
bilingual synonym pairs described in Section 3.1. We assume
this approximation because there seems to be few counter ex-
amples in actual lexicons.
8This data was edited and provided by Japan Science and
Technology Agency (JST).
Total train dev. test
|D| 210647 168837 20853 20957
|J | 136128 108325 13937 13866
|E| 115002 91057 11862 12803
Synsets 50710 40568 5071 5071
Pairs 814524 651727 77706 85091
Table 5: Statistics of the bilingual lexicon for our
experiment
|D|, |J |, and |E| are the number of bilingual lexi-
cal items, the number of Japanese vocabularies, and
the number of English vocabularies, respectively.
?Synsets? and ?Pairs? are the numbers of synonym
sets and synonym pairs, respectively.
thesaurus of scientific terms and improving the cov-
erage. Table 4 illustrates this lexicon.
Table 5 shows the statistics of the dictionary. We
used information only synonym IDs and Japanese
and English representations. We extract pairs of
bilingual lexical items, and treat them as events for
training of the maximum entropy method. The pa-
rameters were adjusted so that the performance is
the best for the development set. For a monolin-
gual method, we used Tb = 0.8, and for a bilingual
method, we used Tb = 0.7.
4.2 Evaluation
We evaluated the performance of identifying bilin-
gual synonym pairs by the pair-wise precision P ,
461
Synset ID J term E term
130213 ???? (shintai-bui) Body Regions
130213 ???? (shintai-bui) body part
130213 ???? (shintai-bui) body region
130213 ???? (shintai-bubun) body part
130217 Douglas? (Douglas-ka) Douglas? Pouch
130217 Douglas? (Douglas-ka) Douglas? Pouch
130217 ????? (Dagurasu-ka) pouch of Douglas
130217 ????? (Dagurasu-ka) pouch of Douglas
130217 ????? (chokucho?-shikyu?-ka) rectouterine pouch
130217 ????? (chokucho?-shikyu?-ka) rectouterine pouch
Table 4: A part of the lexicon used
Each bilingual synonym set consists of items that have the same synset ID. ?? (bubun) is a synonym of
?? (bui). ? (ka) is a hiragana representation of? (ka). ???? (Dagurasu) is a Japanese transcription
of ?Douglas?.
recall R and F-score F defined as follows:
P = C
T
,R = C
N
,F = 2PR
P +R
, (5)
where C, T and N are the number of correctly pre-
dicted pairs as synonyms, predicted pairs to become
synonyms, and synonym pairs in the lexicon9, re-
spectively.
We compared the results with the baseline and the
upper bound. The baseline assumes that each bilin-
gual lexical item is a bilingual synonym if either the
Japanese or English terms are identical. The upper
bound assumes that all the monolingual synonyms
are known and each bilingual item is a bilingual syn-
onym if the Japanese terms and the English terms
are synonymous. The baseline represents the per-
formance when we do not consider spelling varia-
tions, and the upper bound shows the limitation of
the monolingual approach.
4.3 Result
Table 6 shows the evaluation scores of our experi-
ments. The ?monolingual? and ?bilingual? methods
are described in Section 3.2.1. We obtained high
precision and recall scores, although we used fea-
tures primarily with spelling variations. Both meth-
ods significantly outperform the baseline, and show
the importance of considering spelling variations.
9N includes the number of synonym pairs filtered out from
training set by the bigram similarity threshold Tb.
Set Method Precision Recall F-score
dev. baseline 0.977
(31845/32581)
0.410
(31845/77706)
0.577
monolingual 0.911
(74263/81501)
0.956
(74263/77706)
0.932
bilingual 0.879
(72782/82796)
0.937
(72782/77706)
0.907
upper bound 0.984
(77706/78948)
1 0.992
test baseline 0.972
(33382/34347)
0.392
(33382/85091)
0.559
monolingual 0.900
(79099/87901)
0.930
(79099/85091)
0.914
bilingual 0.875
(77640/88714)
0.912
(77640/85091)
0.893
upper bound 0.979
(85091/86937)
1 0.989
Table 6: Evaluation scores
The ?monolingual? method achieved higher preci-
sion and recall than the ?bilingual? method. It in-
dicates that monolingual synonym identification is
effective in finding bilingual synonyms. The up-
per bound shows that there are still a few errors by
the assumption used by the ?monolingual? method.
However, the high precision of the upper bound rep-
resents the well-formedness of the lexicon we used.
We need more experiments on other bilingual lex-
icons to conclude that our method is available for
462
Features Precision Recall F-score
All 0.911 0.956 0.932
?h1F , h1E 0.911 0.974 0.941
?h2F , h2E 0.906 0.947 0.926
?h3F , h3E 0.939 0.930 0.934
?h4F , h4E 0.919 0.734 0.816
?h5F , h5E 0.869 0.804 0.831
?h6, h7 0.940 0.934 0.937
?combs. 0.936 0.929 0.932
Table 7: Evaluation scores of the bilingual method
with removing features on the development set
?h represents removing the feature h and combina-
torial features using h. ?combs. represents remov-
ing all the combinatorial features.
many kinds of lexicons.
To investigate the effectiveness of each feature,
we compared the scores when we remove several
features. Table 7 shows these results. Contrary to
our intuition, we found that features of agreement
of the first characters (h1) remarkably degraded the
recall without gains in precision. One of the rea-
sons for such results is that there are many cases
of non-synonyms that have the same first character.
We need to investigate more effective combinations
of features or to apply other machine learning tech-
niques for improving the performance. From these
results, we consider that the features of h4 are effec-
tive for improving the recall, and that the features of
h2 and h5 contribute improvement of both the pre-
cision and the recall. h3, h6, h7, and combinatorial
features seem to improve the recall at the expense
of precision. Which measure is important depends
on the importance of our target for using this tech-
nique. It depends on the requirements that we em-
phasize, but in general the recall is more important
for finding more bilingual synonyms.
5 Conclusion and future work
This paper proposed a method for identifying bilin-
gual synonyms in a bilingual lexicon by using clues
of spelling variations. We described the notion of
bilingual synonyms, and presented two approaches
for identifying them: one is to directly predict the
relation, and another is to merge monolingual syn-
onyms identified, according to the bilingual lexicon.
Our experiments showed that the proposed method
significantly outperformed the method that did not
use features primarily with spelling variations; the
proposed method extracted bilingual synonyms with
high precision and recall. In addition, we found that
merging monolingual synonyms by the dictionary is
effective for finding bilingual synonyms; there oc-
cur few errors through the assumption described in
Section 3.2.1.
Our future work contains implementing more fea-
tures for identifying synonymous relations, con-
structing bilingual synonym sets, and evaluating our
method for specific tasks such as thesaurus construc-
tion or cross-lingual information retrieval.
Currently, the features used do not include other
clues with spelling variations, such as the weighted
edit distance, transformation patterns, stemming and
so on. Another important clue is distributional infor-
mation, such as the context. We can use both mono-
lingual and bilingual corpora for extracting distribu-
tions of terms, and bilingual corpora are expected to
be especially effective for our goal.
We did not perform an experiment to construct
bilingual synonym sets from synonym pairs in this
paper. Described in Section 3.1, bilingual syn-
onym sets can be constructed from bilingual syn-
onym pairs by assuming some approximations. The
approximation that permits transitivity of bilingual
synonymous relations increases identified bilingual
synonyms, and thus causes an increase in recall and
decrease in precision. It is an open problem to find
appropriate strategies for constructing bilingual syn-
onym sets.
Finally, we plan to evaluate our method for spe-
cific tasks. For data-driven machine translation, it is
expected that data sparseness problem is alleviated
by merging the occurrences of low-frequency terms.
Another application is cross-lingual information re-
trieval, which can be improved by using candidate
expanded queries from bilingual synonym sets.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
463
Japan Science and Technology Agency (JST) for
providing a useful bilingual lexicon with synony-
mous information. We acknowledge the anonymous
reviewers for helpful comments and suggestions.
References
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proc. of the
39th Annual Meeting of the Association for Computa-
tional Linguistics, pages 50?57.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
Vincent D. Blondel, Anah?? Gajardo, Maureen Heymans,
Pierre Senellart, and Paul Van Dooren. 2004. A mea-
sure of similarity between graph vertices: Applications
to synonym extraction and web searching. SIAM Re-
view, 46(4):647?666.
Carolyn J. Croach and Bokyung Yang. 1992. Experi-
ments in automatic statistical thesaurus construction.
In Proc. of the 15th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 77?88. ACM Press.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Be?atrice Daille, Beno??t Habert, Christian Jacquemin, and
Jean Royaute?. 1996. Empirical observation of term
variations and principles for their description. Termi-
nology, 3(2):197?258.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. of the 28th
Annual Meeting of the Association for Computational
Linguistics, pages 268?275.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyser JUMAN. In Proc. of Interna-
tional Workshop on Sharable Natural Language Re-
sources, pages 22?28.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proc. of the 2003 International Joint
Conference on Artificial Intelligence, pages 1492?
1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of the 17th International
Conference on Computational Linguistics, volume 2,
pages 768?774.
Julie B. Lovins. 1968. Development of a stemming al-
gorithm. Mechanical Translation and Computational
Linguistics, 11:22?31.
Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-
agawa. 2004. Automatic construction of Japanese
KATAKANA variant list from large corpus. In
Proc. of the 20th International Conference on Com-
putational Linguistics, volume 2, pages 1214?1219.
Philippe Muller, Nabil Hathout, and Bruno Gaume.
2006. Synonym extraction using a semantic distance
on a dictionary. In Proc. of TextGraphs: the 2nd Work-
shop on Graph Based Methods for Natural Language
Processing, pages 65?72.
Young C. Park and Key-Sun Choi. 1996. Automatic the-
saurus construction using Bayesian networks. Infor-
mation Processing and Management, 32(5):543?553.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Yonggang Qiu and Hans-Peter Frei. 1993. Concept-
based query expansion. In Proc. of SIGIR-93, 16th
ACM International Conference on Research and De-
velopment in Information Retrieval, pages 160?169.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proc. of the 8th Pacific Sympo-
sium on Biocomputing, pages 451?462.
Mitsuo Shimohata and Eiichiro Sumita. 2002. Auto-
matic paraphrasing based on parallel corpus for nor-
malization. In Proc. of the 3rd International Con-
ference on Language Resources and Evaluation, vol-
ume 2, pages 453?457.
Scott A. Waterman. 1996. Distinguished usage. In
Corpus Processing for Lexical Acquisition, pages 143?
172. MIT Press.
Hua Wu and Ming Zhou. 2003. Optimizing synonym
extraction using monolingual and bilingual resources.
In Proc. of the 2nd International Workshop on Para-
phrasing.
464
TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
A Discriminative Approach to Japanese Abbreviation Extraction
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper addresses the difficulties in rec-
ognizing Japanese abbreviations through the
use of previous approaches, examining ac-
tual usages of parenthetical expressions in
newspaper articles. In order to bridge the
gap between Japanese abbreviations and
their full forms, we present a discrimina-
tive approach to abbreviation recognition.
More specifically, we formalize the abbrevi-
ation recognition task as a binary classifica-
tion problem in which a classifier determines
a positive (abbreviation) or negative (non-
abbreviation) class, given a candidate of ab-
breviation definition. The proposed method
achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus
containing 7,887 (1,430 abbreviations and
6,457 non-abbreviation) instances of paren-
thetical expressions.
1 Introduction
Human languages are rich enough to be able to
express the same meaning through different dic-
tion; we may produce different sentences to convey
the same information by choosing alternative words
or syntactic structures. Lexical resources such as
WordNet (Miller et al, 1990) enhance various NLP
applications by recognizing a set of expressions re-
ferring to the same entity/concept. For example, text
retrieval systems can associate a query with alterna-
tive words to find documents where the query is not
obviously stated.
Abbreviations are among a highly productive type
of term variants, which substitutes fully expanded
terms with shortened term-forms. Most previous
studies aimed at establishing associations between
abbreviations and their full forms in English (Park
and Byrd, 2001; Pakhomov, 2002; Schwartz and
Hearst, 2003; Adar, 2004; Nadeau and Turney,
2005; Chang and Schu?tze, 2006; Okazaki and Ana-
niadou, 2006). Although researchers have proposed
various approaches to solving abbreviation recog-
nition through methods such as deterministic algo-
rithm, scoring function, and machine learning, these
studies rely on the phenomenon specific to English
abbreviations: all letters in an abbreviation appear in
its full form.
However, abbreviation phenomena are heavily de-
pendent on languages. For example, the term one-
segment broadcasting is usually abbreviated as one-
seg in Japanese; English speakers may find this pe-
culiar as the term is likely to be abbreviated as 1SB
or OSB in English. We show that letters do not pro-
vide useful clues for recognizing Japanese abbrevia-
tions in Section 2. Elaborating on the complexity of
the generative processes for Japanese abbreviations,
Section 3 presents a supervised learning approach to
Japanese abbreviations. We then evaluate the pro-
posed method on a test corpus from newspaper arti-
cles in Section 4 and conclude this paper.
2 Japanese Abbreviation Survey
Researchers have proposed several approaches to
abbreviation recognition for non-alphabetical lan-
guages. Hisamitsu and Niwa (2001) compared dif-
ferent statistical measures (e.g., ?2 test, log like-
889
Table 1: Parenthetical expressions used in Japanese newspaper articles
lihood ratio) to assess the co-occurrence strength
between the inner and outer phrases of parenthet-
ical expressions X (Y). Yamamoto (2002) utilized
the similarity of local contexts to measure the para-
phrase likelihood of two expressions based on the
distributional hypothesis (Harris, 1954). Chang and
Teng (2006) formalized the generative processes of
Chinese abbreviations with a noisy channel model.
Sasano et al (2007) designed rules about letter types
and occurrence frequency to collect lexical para-
phrases used for coreference resolution.
How are these approaches effective in recogniz-
ing Japanese abbreviation definitions? As a prelimi-
nary study, we examined abbreviations described in
parenthetical expressions in Japanese newspaper ar-
ticles. We used the 7,887 parenthetical expressions
that occurred more than eight times in Japanese ar-
ticles published by the Mainichi Newspapers and
Yomiuri Shimbun in 1998?1999. Table 1 summa-
rizes the usages of parenthetical expressions in four
groups. The field ?para? indicates whether the inner
and outer elements of parenthetical expressions are
interchangeable.
The first group acronym (I) reduces a full form to
a shorter form by removing letters. In general, the
process of acronym generation is easily interpreted:
the left example in Table 1 consists of two Kanji let-
ters taken from the heads of the two words, while
the right example consists of the letters at the end of
the 1st, 2nd, and 4th words in the full form. Since
all letters in an acronym appear in its full form, pre-
vious approaches to English abbreviations are also
applicable to Japanese acronyms. Unfortunately, in
this survey the number of such ?authentic? acronyms
amount to as few as 90 (1.2%).
The second group acronym with translation (II) is
characteristic of non-English languages. Full forms
are imported from foreign terms (usually in En-
glish), but inherit the foreign abbreviations. The
third group alias (III) presents generic paraphrases
that cannot be interpreted as abbreviations. For ex-
ample, Democratic People?s Republic of Korea is
known as its alias North Korea. Even though the
formal name does not refer to the ?northern? part, the
alias consists of Korea, and the locational modifier
North. Although the second and third groups retain
their interchangeability, computers cannot recognize
abbreviations with their full forms based on letters.
The last group (IV) does not introduce inter-
changeable expressions, but presents additional in-
formation for outer phrases. For example, a location
usage of a parenthetical expression X (Y) describes
an entity X, followed by its location Y. Inner and
outer elements of parenthetical expressions are not
interchangeable. We regret to find that as many as
81.9% of parenthetical expressions were described
for this usage. Thus, this study regards acronyms
(with and without translation) and alias as Japanese
890
Table 2: Top 10 frequent parenthetical expressions
used in Japanese newspapers from 1998?1999
abbreviations in a broad sense, based on their in-
terchangeabilities. In other words, the goal of this
study is to classify parenthetical expressions X (Y)
into true abbreviations (groups I, II, III) and other
usages of parentheses (group IV).
How much potential do statistical approaches
have to identify Japanese abbreviations? Table 2
shows the top 10 most frequently appearing paren-
thetical expressions in this survey. The ?class? field
represents the category1: T: acronym with transla-
tion, A: alias, and O: non-abbreviation. The most
frequently occurring parenthetical expression was
Democratic People?s Republic of Korea (North Ko-
rea) (4,160 occurrences). 7 instances in the table
were acronyms with translation (#2?5, #7?8), and
an alias (#1), but 3 non-abbreviation instances (#6,
#9, and #10) expressed nationalities of information
sources. Even if we designed a simple method
to choose the top 10 parenthetical expressions, the
recognition performance would be no greater than
70% precision.
3 A discriminative approach to
abbreviation recognition
In order to bridge the gap between Japanese abbre-
viations and their full forms, we present a discrim-
inative approach to abbreviation recognition. More
specifically, we formalize the abbreviation recogni-
tion task as a binary classification problem in which
1No acronym was included in the top 10 list.
Figure 1: Paraphrase occurrence with parentheses
a classifier determines a positive (abbreviation) or
negative (non-abbreviation) class, given a parenthet-
ical expression X (Y). We model the classifier by
using Support Vector Machines (SVMs) (Vapnik,
1998). The classifier combines features that char-
acterize various aspects of abbreviation definitions.
Table 3 shows the features and their values for the
abbreviation EU, and its full form: O-shu Rengo
(European Union). A string feature is converted into
a set of boolean features, each of which indicates
?true? or ?false? of the value. Due to the space limita-
tion, the rest of this section elaborates on paraphrase
ratio and SKEW features.
Paraphrase ratio Let us consider the situation in
which an author describes an abbreviation definition
X (Y) to state a paraphrase X ? Y in a document.
The effect of the statement is to define the meaning
of the abbreviation Y as X in case the reader may
be unaware/uncertain of the abbreviation Y. For ex-
ample, if an author wrote a parenthetical expression,
Multi-Document Summarization (MDS), in a docu-
ment, readers would recognize the meaning of the
expression MDS. Even if they were aware of the def-
inition, MDS alone would be ambiguous; it could
stand for Multi Dimensional Scaling, Missile De-
fense System, etc. Therefore, an author rarely uses
the expression Y before describing its definition.
At the same time, the author would use the expres-
sion Y more than X after describing the definition, if
it were to declare the abbreviation Y for X. Figure 1
illustrates this situation with two documents. Doc-
ument (a) introduces the abbreviation EU for Euro-
pean Union because the expression EU occurs more
frequently than European Union after the parentheti-
cal expression. In contrast, the parenthetical expres-
891
Feature Type Description Example
PR(X,Y ) numeric Paraphrase ratio 0.426
SKEW(X,Y ) numeric Similarity of local contexts measured by the skew divergence 1.35
freq(X) numeric Frequency of occurrence of X 2,638
freq(Y ) numeric Frequency of occurrence of Y 8,326
freq(X,Y ) numeric Frequency of co-occurrence of X and Y 3,121
?2(X,Y ) numeric Co-occurrence strength measured by the ?2 test 2,484,521
LLR(X,Y ) numeric Co-occurrence strength measured by the log-likelihood ratio 6.8
match(X,Y ) boolean Predicate to test whether X contains all letters in Y 0
Letter types string Pair of letter types of X and Y Kanji/Alpha
First letter string The first letter in the abbreviation Y E
Last letter string The last letter in the abbreviation Y U
POS tags string Pair of POS tags for X and Y NNP/NNP
POS categories string Pair of POS categories for X and Y NN/NN
NE tags string Pair of NE tags for X and Y ORG/ORG
Table 3: Features for the SVM classifier and their values for the abbreviation EU.
sion in document (b) describes the property (nation-
ality) of a person Beckham.
Suppose that we have a document that has a par-
enthetical expression with expressionsX and Y . We
regard a document introducing an abbreviation Y for
X if the document satisfies both of these conditions:
1. The expression Y appears more frequently than
the expression X does after the definition pat-
tern.
2. The expression Y does not appear before the
definition pattern.
Formula 1 assesses the paraphrase ratio of the ex-
pressions X and Y,
PR(X,Y ) =
dpara(X,Y )
d(X,Y )
. (1)
In this formula, dpara(X,Y ) denotes the number
of documents satisfying the above conditions, and
d(X,Y ) presents the number of documents having
the parenthetical expression X(Y ). The function
PR(X, Y) ranges from 0 (no abbreviation instance)
to 1 (all parenthetical expressions introduce the ab-
breviation).
Similarity of local contexts We regard words that
have dependency relations from/to the target expres-
sion as the local contexts of the expression, apply-
ing all sentences to a dependency parser (Kudo and
Matsumoto, 2002). Collecting the local context of
the target expressions, we compute the skew diver-
gence (Lee, 2001), which is a weighted version of
Kullback-Leibler (KL) divergence, to measure the
resemblance of probability distributions P and Q:
SKEW?(P ||Q) = KL(P ||?Q+ (1? ?)P ), (2)
KL(P ||Q) =
?
i
P (i) log
P (i)
Q(i)
. (3)
In these formulas, P is the probability distribution
function of the words in the local context for the ex-
pression X , Q is for Y , and ? is a skew parameter
set to 0.99. The function SKEW?(P ||Q) becomes
close to zero if the probability distributions of local
contexts for the expressions X and Y are similar.
Other features In addition, we designed twelve
features for abbreviation recognition: five fea-
tures, freq(X), freq(Y ), freq(X,Y ), ?2(X,Y ), and
LLR(X,Y ) to measure the co-occurrence strength
of the expressions X and Y (Hisamitsu and Niwa,
2001), match(X,Y ) feature to test whether or not
all letters in an abbreviation appear in its full form,
three features letter type, first letter, and last let-
ter corresponding to rules about letter types in ab-
breviation definitions, and three features POS tags,
POS categories, and NE tags to utilize information
from a morphological analyzer and named-entity
tagger (Kudo and Matsumoto, 2002).
4 Evaluation
4.1 Results
We built a system for Japanese abbreviation recogni-
tion by using the LIBSVM implementation2 with a
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
892
Group Recall
Acronym 94.4%
Acronym with translation 97.4%
Alias 81.4%
Total 87.6%
Table 4: Recall for each role of parentheses
linear kernel, which obtained the best result through
experiments. The performance was measured under
a ten-fold cross-validation on the corpus built in the
survey, which contains 1,430 abbreviation instances
and 6,457 non-abbreviation instances.
The proposed method achieved 95.7% accuracy,
90.0% precision, and 87.6% recall for recognizing
Japanese abbreviations. We cannot compare this
performance directly with the previous work be-
cause of the differences in the task design and cor-
pus. For reference, Yamamoto (2002) reported 66%
precision (he did not provide the recall value) for
a similar task: the acquisition of lexical paraphrase
from Japanese newspaper articles.
Table 4 reports the recall value for each group
of abbreviations. This analysis shows the distribu-
tion of abbreviations unrecognized by the proposed
method. Japanese acronyms, acronyms with transla-
tion, and aliases were recognized at 94.4%, 97.4%,
and 81.4% recall respectively. It is interesting to see
that the proposed method could extract acronyms
with translation and aliases even though we did not
use any bilingual dictionaries.
4.2 Analyses for individual features
The numerical and boolean features are monotone
increasing functions (decreasing for the SKEW fea-
ture) as two expressions X and Y are more likely
to present an abbreviation definition. For example,
the more authors introduce a paraphrase X ? Y,
the higher the value that PR(X,Y ) feature yields.
Thus, we emulate a simple classifier for each feature
that labels a candidate of abbreviation definition as a
positive instance only if the feature value is higher
than a given threshold ?, e.g., PR(X,Y ) > 0.9.
Figure 2 shows the precision?recall curve for each
feature with variable thresholds.
The paraphrase ratio (PR) feature outperformed
other features with a wide margin: the precision and
recall values for the best F1 score were 66.2% and
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
Co-occurrence frequency
Log likelihood ratio
Skew divergence
Letter match
Paraphrase rate
Chi square
Figure 2: Precision?recall curve of each feature
Feature Accuracy Reduction
All 95.7% ?
- PR(X,Y ) 95.2% 0.5%
- SKEW(X,Y ) 95.4% 0.3%
- freq(X,Y ) 95.6% 0.1%
- ?2(X,Y ) 95.6% 0.1%
- LLR(X,Y ) 95.3% 0.4%
- match(X,Y ) 95.5% 0.2%
- Letter type 94.5% 1.2%
- POS tags 95.6% 0.1%
- NE tags 95.7% 0.0%
Table 5: Contribution of the features
48.1% respectively. Although the performance of
this feature alone was far inferior to the proposed
method, to some extent Formula 1 estimated actual
occurrences of abbreviation definitions.
The performance of the match (letter inclusion)
feature was as low as 58.2% precision and 6.9% re-
call3. It is not surprising that the match feature had
quite a low recall, because of the ratio of ?authentic?
acronyms (about 6%) in the corpus. However, the
match feature did not gain a good precision either.
Examining false cases, we found that this feature
could not discriminate cases where an outer element
contains its inner element accidentally; e.g., Tokyo
Daigaku (Tokyo), which describes a university name
followed by its location (prefecture) name.
Finally, we examined the contribution of each fea-
ture by eliminating a feature one by one. If a feature
was important for recognizing abbreviations, the ab-
sence of the feature would drop the accuracy. Each
row in Table 5 presents an eliminated feature, the
accuracy without the feature, and the reduction of
3This feature drew the precision?recall locus in a stepping
shape because of its discrete values (0 or 1).
893
the accuracy. Unfortunately, the accuracy reductions
were so few that we could not discuss contributions
of features with statistical significance. The letter
type feature had the largest influence (1.2%) on the
recognition task, followed by the paraphrase ratio
(0.5%) and log likelihood ratio (0.4%).
5 Conclusion
In this paper we addressed the difficulties in rec-
ognizing Japanese abbreviations by examining ac-
tual usages of parenthetical expressions in news-
paper articles. We also presented the discrimina-
tive approach to Japanese abbreviation recognition,
which achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus. A future
direction of this study would be to apply the pro-
posed method to other non-alphabetical languages,
which may have similar difficulties in modeling the
generative process of abbreviations. We also plan to
extend this approach to the Web documents.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Science
and Technology (JST, Japan). We used Mainichi
Shinbun and Yomiuri Shinbun newspaper articles for
the evaluation corpus.
References
Eytan Adar. 2004. SaRAD: A simple and robust abbre-
viation dictionary. Bioinformatics, 20(4):527?533.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99?119. Artech House, Inc.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
atomic chinese abbreviation pairs: A probabilistic
model for single character word recovery. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 17?24, Sydney, Australia,
July. Association for Computational Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10:146?162.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extracting
useful terms from parenthetical expression by combin-
ing simple rules and statistical measures: A compara-
tive evaluation of bigram statistics. In Didier Bouri-
gault, Christian Jacquemin, and Marie-C L?Homme,
editors, Recent Advances in Computational Terminol-
ogy, pages 209?224. John Benjamins.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the CoNLL 2002 (COLING 2002 Post-
Conference Workshops), pages 63?69.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to wordnet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identification.
In 8th Canadian Conference on Artificial Intelligence
(AI?2005) (LNAI 3501), pages 319?329.
Naoaki Okazaki and Sophia Ananiadou. 2006. A term
recognition approach to acronym recognition. In Pro-
ceedings of the COLING-ACL 2006 Main Conference
Poster Sessions, pages 643?650, Sydney, Australia.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of 40th
annual meeting of ACL, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the EMNLP 2001, pages 126?133.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2007. Improving coreference resolution us-
ing bridging reference resolution and automatically
acquired synonyms. In Anaphora: Analysis, Alo-
gorithms and Applications, 6th Discourse Anaphora
and Anaphor Resolution Colloquium, DAARC2007,
pages 125?136.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451?462.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Kazuhide Yamamoto. 2002. Acquisition of lexical para-
phrases from texts. In 2nd International Workshop
on Computational Terminology (Computerm 2002, in
conjunction with COLING 2002), pages 1?7, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
894
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Discriminative Latent Variable Chinese Segmenter
with Hybrid Word/Character Information
Xu Sun
Department of Computer Science
University of Tokyo
sunxu@is.s.u-tokyo.ac.jp
Yaozhong Zhang
Department of Computer Science
University of Tokyo
yaozhong.zhang@is.s.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
School of Computer Science
University of Manchester
yoshimasa.tsuruoka@manchester.ac.uk
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo, Japan
School of Computer Science, University of Manchester, UK
National Centre for Text Mining, UK
tsujii@is.s.u-tokyo.ac.jp
Abstract
Conventional approaches to Chinese word
segmentation treat the problem as a character-
based tagging task. Recently, semi-Markov
models have been applied to the problem, in-
corporating features based on complete words.
In this paper, we propose an alternative, a
latent variable model, which uses hybrid in-
formation based on both word sequences and
character sequences. We argue that the use of
latent variables can help capture long range
dependencies and improve the recall on seg-
menting long words, e.g., named-entities. Ex-
perimental results show that this is indeed the
case. With this improvement, evaluations on
the data of the second SIGHAN CWS bakeoff
show that our system is competitive with the
best ones in the literature.
1 Introduction
For most natural language processing tasks, words
are the basic units to process. Since Chinese sen-
tences are written as continuous sequences of char-
acters, segmenting a character sequence into a word
sequence is the first step for most Chinese process-
ing applications. In this paper, we study the prob-
lem of Chinese word segmentation (CWS), which
aims to find these basic units (words1) for a given
sentence in Chinese.
Chinese character sequences are normally am-
biguous, and out-of-vocabulary (OOV) words are a
major source of the ambiguity. Typical examples
of OOV words include named entities (e.g., orga-
nization names, person names, and location names).
Those named entities may be very long, and a dif-
ficult case occurs when a long word W (|W | ? 4)
consists of some words which can be separate words
on their own; in such cases an automatic segmenter
may split the OOV word into individual words. For
example,
(Computer Committee of International Federation of
Automatic Control) is one of the organization names
in the Microsoft Research corpus. Its length is 13
and it contains more than 6 individual words, but it
should be treated as a single word. Proper recogni-
tion of long OOV words are meaningful not only for
word segmentation, but also for a variety of other
purposes, e.g., full-text indexing. However, as is il-
lustrated, recognizing long words (without sacrific-
ing the performance on short words) is challenging.
Conventional approaches to Chinese word seg-
mentation treat the problem as a character-based la-
1Following previous work, in this paper, words can also refer
to multi-word expressions, including proper names, long named
entities, idioms, etc.
56
beling task (Xue, 2003). Labels are assigned to each
character in the sentence, indicating whether the
character xi is the start (Labeli = B), middle or end
of a multi-character word (Labeli = C). A popu-
lar discriminative model that have been used for this
task is the conditional random fields (CRFs) (Laf-
ferty et al, 2001), starting with the model of Peng
et al (2004). In the Second International Chinese
Word Segmentation Bakeoff (the second SIGHAN
CWS bakeoff) (Emerson, 2005), two of the highest
scoring systems in the closed track competition were
based on a CRF model (Tseng et al, 2005; Asahara
et al, 2005).
While the CRF model is quite effective compared
with other models designed for CWS, it may be lim-
ited by its restrictive independence assumptions on
non-adjacent labels. Although the window can in
principle be widened by increasing the Markov or-
der, this may not be a practical solution, because
the complexity of training and decoding a linear-
chain CRF grows exponentially with the Markov or-
der (Andrew, 2006).
To address this difficulty, a choice is to relax the
Markov assumption by using the semi-Markov con-
ditional random field model (semi-CRF) (Sarawagi
and Cohen, 2004). Despite the theoretical advan-
tage of semi-CRFs over CRFs, however, some pre-
vious studies (Andrew, 2006; Liang, 2005) explor-
ing the use of a semi-CRF for Chinese word seg-
mentation did not find significant gains over the
CRF ones. As discussed in Andrew (2006), the rea-
son may be that despite the greater representational
power of the semi-CRF, there are some valuable fea-
tures that could be more naturally expressed in a
character-based labeling model. For example, on
a CRF model, one might use the feature ?the cur-
rent character xi is X and the current label Labeli
is C?. This feature may be helpful in CWS for gen-
eralizing to new words. For example, it may rule
out certain word boundaries if X were a character
that normally occurs only as a suffix but that com-
bines freely with some other basic forms to create
new words. This type of features is slightly less nat-
ural in a semi-CRF, since in that case local features
?(yi, yi+1, x) are defined on pairs of adjacent words.
That is to say, information about which characters
are not on boundaries is only implicit. Notably, ex-
cept the hybrid Markov/semi-Markov system in An-
drew (2006)2, no other studies using the semi-CRF
(Sarawagi and Cohen, 2004; Liang, 2005; Daume?
III and Marcu, 2005) experimented with features of
segmenting non-boundaries.
In this paper, instead of using semi-Markov mod-
els, we describe an alternative, a latent variable
model, to learn long range dependencies in Chi-
nese word segmentation. We use the discrimina-
tive probabilistic latent variable models (DPLVMs)
(Morency et al, 2007; Petrov and Klein, 2008),
which use latent variables to carry additional infor-
mation that may not be expressed by those original
labels, and therefore try to build more complicated
or longer dependencies. This is especially meaning-
ful in CWS, because the used labels are quite coarse:
Label(y) ? {B,C}, where B signifies beginning a
word and C signifies the continuation of a word.3
For example, by using DPLVM, the aforementioned
feature may turn to ?the current character xi is X ,
Labeli = C, and LatentV ariablei = LV ?. The
current latent variable LV may strongly depend on
the previous one or many latent variables, and there-
fore we can model the long range dependencies
which may not be captured by those very coarse la-
bels. Also, since character and word information
have their different advantages in CWS, in our latent
variable model, we use hybrid information based on
both character and word sequences.
2 A Latent Variable Segmenter
2.1 Discriminative Probabilistic Latent
Variable Model
Given data with latent structures, the task is to
learn a mapping between a sequence of observa-
tions x = x1, x2, . . . , xm and a sequence of labels
y = y1, y2, . . . , ym. Each yj is a class label for the
j?th character of an input sequence, and is a mem-
ber of a set Y of possible class labels. For each se-
quence, the model also assumes a sequence of latent
variables h = h1, h2, . . . , hm, which is unobserv-
able in training examples.
The DPLVM is defined as follows (Morency et al,
2The system was also used in Gao et al (2007), with an
improved performance in CWS.
3In practice, one may add a few extra labels based on lin-
guistic intuitions (Xue, 2003).
57
2007):
P (y|x,?) =?
h
P (y|h,x,?)P (h|x,?), (1)
where ? are the parameters of the model. DPLVMs
can be seen as a natural extension of CRF models,
and CRF models can be seen as a special case of
DPLVMs that have only one latent variable for each
label.
To make the training and inference efficient, the
model is restricted to have disjoint sets of latent vari-
ables associated with each class label. Each hj is a
member in a set Hyj of possible latent variables for
the class label yj . H is defined as the set of all pos-
sible latent variables, i.e., the union of all Hyj sets.
Since sequences which have any hj /? Hyj will by
definition have P (y|x,?) = 0, the model can be
further defined4 as:
P (y|x,?) = ?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual conditional
random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a training
set consisting of n labeled sequences, (xi,yi), for
i = 1 . . . n, parameter estimation is performed by
optimizing the objective function,
L(?) =
n?
i=1
log P (yi|xi,?) ? R(?). (4)
The first term of this equation is the conditional log-
likelihood of the training data. The second term is
a regularizer that is used for reducing overfitting in
parameter estimation.
For decoding in the test stage, given a test se-
quence x, we want to find the most probable label
sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the best
label path y? cannot directly be produced by the
4It means that Eq. 2 is from Eq. 1 with additional definition.
Viterbi algorithm because of the incorporation of
hidden states. In this paper, we use a technique
based on A? search and dynamic programming de-
scribed in Sun and Tsujii (2009), for producing the
most probable label sequence y? on DPLVM.
In detail, an A? search algorithm5 (Hart et al,
1968) with a Viterbi heuristic function is adopted to
produce top-n latent paths, h1,h2, . . .hn. In addi-
tion, a forward-backward-style algorithm is used to
compute the exact probabilities of their correspond-
ing label paths, y1,y2, . . .yn. The model then tries
to determine the optimal label path based on the
top-n statistics, without enumerating the remaining
low-probability paths, which could be exponentially
enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?) ? (1 ?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence in
current stage. It is straightforward to prove that
y? = y1, and further search is unnecessary. This
is because the remaining probability mass, 1 ??
yk?LPn P (yk|x,?), cannot beat the current op-timal label path in this case. For more details of the
inference, refer to Sun and Tsujii (2009).
2.2 Hybrid Word/Character Information
We divide our main features into two types:
character-based features and word-based features.
The character-based features are indicator functions
that fire when the latent variable label takes some
value and some predicate of the input (at a certain
position) corresponding to the label is satisfied. For
each latent variable label hi (the latent variable la-
bel at position i), we use the predicate templates as
follows:
? Input characters/numbers/letters locating at po-
sitions i ? 2, i ? 1, i, i + 1 and i + 2
? The character/number/letter bigrams locating
at positions i ? 2, i ? 1, i and i + 1
5A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
58
? Whether xj and xj+1 are identical, for j = (i?
2) . . . (i + 1)
? Whether xj and xj+2 are identical, for j = (i?
3) . . . (i + 1)
The latter two feature templates are designed to de-
tect character or word reduplication, a morphologi-
cal phenomenon that can influence word segmenta-
tion in Chinese.
The word-based features are indicator functions
that fire when the local character sequence matches
a word or a word bigram. A dictionary containing
word and bigram information was collected from the
training data. For each latent variable label unigram
hi, we use the set of predicate template checking for
word-based features:
? The identity of the string xj . . . xi, if it matches
a word A from the word-dictionary of training
data, with the constraint i?6 < j < i; multiple
features will be generated if there are multiple
strings satisfying the condition.
? The identity of the string xi . . . xk, if it matches
a word A from the word-dictionary of training
data, with the constraint i < k < i+6; multiple
features could be generated.
? The identity of the word bigram (xj . . . xi?1,
xi . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
? The identity of the word bigram (xj . . . xi,
xi+1 . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that using low-frequency features that occur
only a few times in the training set improves perfor-
mance on the development set. We hence do not do
any thresholding of the DPLVM features: we simply
use all those generated features.
The aforementioned word based features can in-
corporate word information naturally. In addition,
following Wang et al (2006), we found using a
very simple heuristic can further improve the seg-
mentation quality slightly. More specifically, two
operations, merge and split, are performed on the
DPLVM/CRF outputs: if a bigram A B was not ob-
served in the training data, but the merged one AB
was, then A B will be simply merged into AB; on
the other hand, if AB was not observed but A B ap-
peared, then it will be split into A B. We found this
simple heuristic on word information slightly im-
proved the performance (e.g., for the PKU corpus,
+0.2% on the F-score).
3 Experiments
We used the data provided by the second Inter-
national Chinese Word Segmentation Bakeoff to
test our approaches described in the previous sec-
tions. The data contains three corpora from different
sources: Microsoft Research Asia (MSR), City Uni-
versity of Hong Kong (CU), and Peking University
(PKU).
Since the purpose of this work is to evaluate the
proposed latent variable model, we did not use ex-
tra resources such as common surnames, lexicons,
parts-of-speech, and semantics. For the generation
of word-based features, we extracted a word list
from the training data as the vocabulary.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the de-
coder), precision (P , the percentage of words in the
decoder output that are segmented correctly), bal-
anced F-score (F ) defined by 2PR/(P + R), recall
of OOV words (R-oov). For more detailed informa-
tion on the corpora and these metrics, refer to Emer-
son (2005).
3.1 Training the DPLVM Segmenter
We implemented DPLVMs in C++ and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions. We
employ the feature templates defined in Section 2.2,
taking into account those 3,069,861 features for the
MSR data, 2,634,384 features for the CU data, and
1,989,561 features for the PKU data.
As for numerical optimization, we performed
gradient decent with the Limited-Memory BFGS
59
(L-BFGS)6 optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order Quasi-
Newton method that numerically estimates the cur-
vature from previous gradients and updates. With
no requirement on specialized Hessian approxima-
tion, L-BFGS can handle large-scale problems in an
efficient manner.
Since the objective function of the DPLVM model
is non-convex, we randomly initialized parameters
for the training.7 To reduce overfitting, we employed
an L2 Gaussian weight prior8 (Chen and Rosen-
feld, 1999). During training, we varied the L2-
regularization term (with values 10k, k from -3 to
3), and finally set the value to 1. We use 4 hidden
variables per label for this task, compromising be-
tween accuracy and efficiency.
3.2 Comparison on Convergence Speed
First, we show a comparison of the convergence
speed between the objective function of DPLVMs
and CRFs. We apply the L-BFGS optimization algo-
rithm to optimize the objective function of DPLVM
and CRF models, making a comparison between
them. We find that the number of iterations required
for the convergence of DPLVMs are fewer than for
CRFs. Figure 1 illustrates the convergence-speed
comparison on the MSR data. The DPLVM model
arrives at the plateau of convergence in around 300
iterations, with the penalized loss of 95K when
#passes = 300; while CRFs require 900 iterations,
with the penalized loss of 98K when #passes =
900.
However, we should note that the time cost of the
DPLVM model in each iteration is around four times
higher than the CRF model, because of the incorpo-
ration of hidden variables. In order to speed up the
6For numerical optimization on latent variable models, we
also experimented the conjugate-gradient (CG) optimization al-
gorithm and stochastic gradient decent algorithm (SGD). We
found the L-BFGS with L2 Gaussian regularization performs
slightly better than the CG and the SGD. Therefore, we adopt
the L-BFGS optimizer in this study.
7For a non-convex objective function, different parame-
ter initializations normally bring different optimization results.
Therefore, to approach closer to the global optimal point, it
is recommended to perform multiple experiments on DPLVMs
with random initialization and then select a good start point.
8We also tested the L-BFGS with L1 regularization, and we
found the L-BFGS with L2 regularization performs better in
this task.
0
300K
600K
900K
1200K
1500K
1800K
 100  200  300  400  500  600  700  800  900
O
bj.
 Fu
nc
. V
alu
e
Forward-Backward Passes
DPLVM
CRF
Figure 1: The value of the penalized loss based on the
number of iterations: DPLVMs vs. CRFs on the MSR
data.
Style #W.T. #Word #C.T. #Char
MSR S.C. 88K 2,368K 5K 4,050K
CU T.C. 69K 1,455K 5K 2,403K
PKU S.C. 55K 1,109K 5K 1,826K
Table 1: Details of the corpora. W.T. represents word
types; C.T. represents character types; S.C. represents
simplified Chinese; T.C. represents traditional Chinese.
training speed of the DPLVM model in the future,
one solution is to use the stochastic learning tech-
nique9. Another solution is to use a distributed ver-
sion of L-BFGS to parallelize the batch training.
4 Results and Discussion
Since the CRF model is one of the most successful
models in Chinese word segmentation, we compared
DPLVMs with CRFs. We tried to make experimen-
tal results comparable between DPLVMs and CRF
models, and have therefore employed the same fea-
ture set, optimizer and fine-tuning strategy between
the two. We also compared DPLVMs with semi-
CRFs and other successful systems reported in pre-
vious work.
4.1 Evaluation Results
Three training and test corpora were used in the test,
including the MSR Corpus, the CU Corpus, and the
9We have tried stochastic gradient decent, as described pre-
viously. It is possible to try other stochastic learning methods,
e.g., stochastic meta decent (Vishwanathan et al, 2006).
60
MSR data P R F R-oov
DPLVM (*) 97.3 97.3 97.3 72.2
CRF (*) 97.1 96.8 97.0 72.0
semi-CRF (A06) N/A N/A 96.8 N/A
semi-CRF (G07) N/A N/A 97.2 N/A
CRF (Z06-a) 96.5 96.3 96.4 71.4
Z06-b 97.2 96.9 97.1 71.2
ZC07 N/A N/A 97.2 N/A
Best05 (T05) 96.2 96.6 96.4 71.7
CU data P R F R-oov
DPLVM (*) 94.7 94.4 94.6 68.8
CRF (*) 94.3 93.9 94.1 65.8
CRF (Z06-a) 95.0 94.2 94.6 73.6
Z06-b 95.2 94.9 95.1 74.1
ZC07 N/A N/A 95.1 N/A
Best05 (T05) 94.1 94.6 94.3 69.8
PKU data P R F R-oov
DPLVM (*) 95.6 94.8 95.2 77.8
CRF (*) 95.2 94.2 94.7 76.8
CRF (Z06-a) 94.3 94.6 94.5 75.4
Z06-b 94.7 95.5 95.1 74.8
ZC07 N/A N/A 94.5 N/A
Best05 (C05) 95.3 94.6 95.0 63.6
Table 2: Results from DPLVMs, CRFs, semi-CRFs, and
other systems.
PKU Corpus (see Table 1 for details). The results
are shown in Table 2. The results are grouped into
three sub-tables according to different corpora. Each
row represents a CWS model. For each group, the
rows marked by ? represent our models with hy-
brid word/character information. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; A06 represents the semi-CRF model in An-
drew (2006)10, which was also used in Gao et al
(2007) (denoted as G07) with an improved perfor-
mance; Z06-a and Z06-b represents the pure sub-
word CRF model and the confidence-based com-
bination of CRF and rule-based models, respec-
tively (Zhang et al, 2006); ZC07 represents the
word-based perceptron model in Zhang and Clark
(2007); T05 represents the CRF model in Tseng et
al. (2005); C05 represents the system in Chen et al
10It is a hybrid Markov/semi-Markov CRF model which
outperforms conventional semi-CRF models (Andrew, 2006).
However, in general, as discussed in Andrew (2006), it is essen-
tially still a semi-CRF model.
(2005). The best F-score and recall of OOV words
of each group is shown in bold.
As is shown in the table, we achieved the best
F-score in two out of the three corpora. We also
achieved the best recall rate of OOV words on those
two corpora. Both of the MSR and PKU Corpus use
simplified Chinese, while the CU Corpus uses the
traditional Chinese.
On the MSR Corpus, the DPLVM model reduced
more than 10% error rate over the CRF model us-
ing exactly the same feature set. We also compared
our DPLVM model with the semi-CRF models in
Andrew (2006) and Gao et al (2007), and demon-
strate that the DPLVM model achieved slightly bet-
ter performance than the semi-CRF models. Andrew
(2006) and Gao et al (2007) only reported the re-
sults on the MSR Corpus.
In summary, tests for the Second International
Chinese Word Segmentation Bakeoff showed com-
petitive results for our method compared with the
best results in the literature. Our discriminative la-
tent variable models achieved the best F-scores on
the MSR Corpus (97.3%) and PKU Corpus (95.2%);
the latent variable models also achieved the best re-
calls of OOV words over those two corpora. We will
analyze the results by varying the word-length in the
following subsection.
4.2 Effect on Long Words
One motivation of using a latent variable model for
CWS is to use latent variables to more adequately
learn long range dependencies, as we argued in Sec-
tion 1. In the test data of the MSR Corpus, 19% of
the words are longer than 3 characters; there are also
8% in the CU Corpus and 11% in the PKU Corpus,
respectively. In the MSR Corpus, there are some ex-
tremely long words (Length > 10), while the CU
and PKU corpus do not contain such extreme cases.
Figure 2 shows the recall rate on different groups
of words categorized by their lengths (the number
of characters). As we expected, the DPLVM model
performs much better on long words (Length ? 4)
than the CRF model, which used exactly the same
feature set. Compared with the CRF model, the
DPLVM model exhibited almost the same level of
performance on short words. Both models have
the best performance on segmenting the words with
the length of two. The performance of the CRF
61
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-M
SR
 (%
)
Length of Word (MSR)
DPLVM
CRF
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-C
U 
(%
)
Length of Word (CU)
DPLVM
CRF
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-P
KU
 (%
)
Length of Word (PKU)
DPLVM
CRF
Figure 2: The recall rate on words grouped by the length.
model deteriorates rapidly as the word length in-
creases, which demonstrated the difficulty on mod-
eling long range dependencies in CWS. Compared
with the CRF model, the DPLVM model performed
quite well in dealing with long words, without sacri-
ficing the performance on short words. All in all, we
conclude that the improvement of using the DPLVM
model came from the improvement on modeling
long range dependencies in CWS.
4.3 Error Analysis
Table 3 lists the major errors collected from the la-
tent variable segmenter. We examined the collected
errors and found that many of them can be grouped
into four types: over-generalization (the top row),
errors on named entities (the following three rows),
errors on idioms (the following three rows) and er-
rors from inconsistency (the two rows at the bottom).
Our system performed reasonably well on very
complex OOV words, such as
(Agricultural Bank of China,
Gold Segmentation Segmenter Output
//
Co-allocated org. names
(Chen Yao) //
(Chen Fei) //
(Vasillis) //
//
//
// //
Idioms
// (propagandist)
(desertification) //
Table 3: Error analysis on the latent variable seg-
menter. The errors are grouped into four types: over-
generalization, errors on named entities, errors on idioms
and errors from data-inconsistency.
Shijiazhuang-city Branch, the second sales depart-
ment) and (Science
and Technology Commission of China, National In-
stitution on Scientific Information Analysis). How-
ever, it sometimes over-generalized to long words.
For example, as shown in the top row,
(National Department of Environmental Protection)
and (The Central Propaganda Department)
are two organization names, but they are incorrectly
merged into a single word.
As for the following three rows, (Chen Yao)
and (Chen Fei) are person names. They are
wrongly segmented because we lack the features to
capture the information of person names (such use-
ful knowledge, e.g., common surname list, are cur-
rently not used in our system). In the future, such
errors may be solved by integrating open resources
into our system. (Vasillis) is a transliter-
ated foreign location name and is also wrongly seg-
mented.
For the corpora that considered 4 character idioms
as a word, our system successfully combined most
of new idioms together. This differs greatly from the
results of CRFs. However, there are still a number
of new idioms that failed to be correctly segmented,
as listed from the fifth row to the seventh row.
Finally, some errors are due to inconsistencies in
the gold segmentation. For example, // (pro-
pagandist) is two words, but a word with similar
62
structure, (theorist), is one word.
(desertification) is one word, but its synonym,
// (desertification), is two words in the gold seg-
mentation.
5 Conclusion and Future Work
We presented a latent variable model for Chinese
word segmentation, which used hybrid information
based on both word and character sequences. We
discussed that word and character information have
different advantages, and could be complementary
to each other. Our model is an alternative to the ex-
isting word based models and character based mod-
els.
We argued that using latent variables can better
capture long range dependencies. We performed
experiments and demonstrated that our model can
indeed improve the segmentation accuracy on long
words. With this improvement, tests on the data
of the Second International Chinese Word Segmen-
tation Bakeoff show that our system is competitive
with the best in the literature.
Since the latent variable model allows a wide
range of features, so the future work will consider
how to integrate open resources into our system. The
latent variable model handles latent-dependencies
naturally, and can be easily extended to other label-
ing tasks.
Acknowledgments
We thank Kun Yu, Galen Andrew and Xiaojun Lin
for the enlightening discussions. We also thank the
anonymous reviewers who gave very helpful com-
ments. This work was partially supported by Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word segmen-
tation. Proceedings of the fourth SIGHAN workshop,
pages 134?137.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word seg-
mentation. Proceedings of the fourth SIGHAN work-
shop.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: approximate large mar-
gin methods for structured prediction. Proceedings of
ICML?05.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of the
fourth SIGHAN workshop, pages 123?133.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL?07), pages 824?831.
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
path. IEEE Trans. On System Science and Cybernet-
ics, SSC-4(2):100?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. Proceed-
ings of ICML?01, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings of
CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
F. Peng and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random
fields. Proceedings of COLING?04.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings of
NIPS?08.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Proceedings of ICML?04.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm and
its efficient approximation. Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?09).
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bakeoff
63
2005. Proceedings of the fourth SIGHAN workshop,
pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. Proceedings of ICML?06, pages 969?
976.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop, pages
138?141, July.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Computa-
tional Linguistics and Chinese Language Processing,
8(1).
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. Pro-
ceedings of ACL?07.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. Proceedings of
HLT/NAACL?06 companion volume short papers.
64
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424?432,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Lexicon Mining from Parenthetical Expressions
in Monolingual Web Pages
Xianchao Wu? Naoaki Okazaki? Jun?ichi Tsujii??
?Computer Science, Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
?School of Computer Science, University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a semi-supervised learn-
ing framework for mining Chinese-English
lexicons from large amount of Chinese Web
pages. The issue is motivated by the ob-
servation that many Chinese neologisms are
accompanied by their English translations in
the form of parenthesis. We classify par-
enthetical translations into bilingual abbrevi-
ations, transliterations, and translations. A
frequency-based term recognition approach is
applied for extracting bilingual abbreviations.
A self-training algorithm is proposed for min-
ing transliteration and translation lexicons. In
which, we employ available lexicons in terms
of morpheme levels, i.e., phoneme correspon-
dences in transliteration and grapheme (e.g.,
suffix, stem, and prefix) correspondences in
translation. The experimental results verified
the effectiveness of our approaches.
1 Introduction
Bilingual lexicons, as lexical or phrasal parallel
corpora, are widely used in applications of multi-
lingual language processing, such as statistical ma-
chine translation (SMT) and cross-lingual informa-
tion retrieval. However, it is a time-consuming task
for constructing large-scale bilingual lexicons by
hand. There are many facts cumber the manual de-
velopment of bilingual lexicons, such as the contin-
uous emergence of neologisms (e.g., new technical
terms, personal names, abbreviations, etc.), the dif-
ficulty of keeping up with the neologisms for lexi-
cographers, etc. In order to turn the facts to a better
way, one of the simplest strategies is to automati-
cally mine large-scale lexicons from corpora such as
the daily updated Web.
Generally, there are two kinds of corpora used
for automatic lexicon mining. One is the purely
monolingual corpora, wherein frequency-based
expectation-maximization (EM, refer to (Dempster
et al, 1977)) algorithms and cognate clues play a
central role (Koehn and Knight, 2002). Haghighi
et al (2008) presented a generative model based
on canonical correlation analysis, in which monolin-
gual features such as the context and orthographic
substrings of words were taken into account. The
other is multilingual parallel and comparable cor-
pora (e.g., Wikipedia1), wherein features such as co-
occurrence frequency and context are popularly em-
ployed (Cheng et al, 2004; Shao and Ng, 2004; Cao
et al, 2007; Lin et al, 2008).
In this paper, we focus on a special type of com-
parable corpus, parenthetical translations. The issue
is motivated by the observation that Web pages and
technical papers written in Asian languages (e.g.,
Chinese, Japanese) sometimes annotate named enti-
ties or technical terms with their translations in En-
glish inside a pair of parentheses. This is considered
to be a traditional way to annotate new terms, per-
sonal names or other named entities with their En-
glish translations expressed in brackets. Formally,
a parenthetical translation can be expressed by the
following pattern,
f1 f2 ... fJ (e1 e2 ... eI). (1)
Here, f1 f2 ... fJ (fJ1 ), the pre-parenthesis text, de-
notes the word sequence of some language other
than English; and e1 e2 ... eI (eI1), the in-parenthesis
text, denotes the word sequence of English. We sep-
arate parenthetical translations into three categories:
1http://en.wikipedia.org/wiki/Main Page
424
Type Examples with translations in italic
?? ?? ?? ?? ?? (GCOS)to Global Climate Observing System (GCOS)
?? ? ?? ???- ???(Shipton-Tilman)brand will be among Shipton-Tilman (Shipton-Tilman)
?????? ???(Cancelbots)time bomb, Cancelbots (Cancelbots)
? ?? ?? ? ??? ???? ??(Bradford University)
the English Bradford University (Bradford University)
that holds lessons in Hongkong
Abbreviation
Transliteration
Translation
Mixture
Table 1: Parenthetical translation categories and exam-
ples extracted from Chinese Web pages. Mixture stands
for the mixture of translation (University) and translitera-
tion (Bradford). ??? denotes the left boundary of fJ1 .
bilingual abbreviation, transliteration, and transla-
tion. Table 1 illustrates examples of these categories.
We address several characteristics of parenthetical
translations that differ from traditional comparable
corpora. The first is that they only appear in mono-
lingual Web pages or documents, and the context
information of eI1 is unknown. Second, frequency
and word number of eI1 are frequently small. This
is because parenthetical translations are only used
when the authors thought that fJ1 contained some
neologism(s) which deserved further explanation in
another popular language (e.g., English). Thus, tra-
ditional context based approaches are not applicable
and frequency based approaches may yield low re-
call while with high precision. Furthermore, cog-
nate clues such as orthographic features are not ap-
plicable between language pairs such as English and
Chinese.
Parenthetical translation mining faces the follow-
ing issues. First, we need to distinguish paren-
thetical translations from parenthetical expressions,
since parenthesis has many functions (e.g., defining
abbreviations, elaborations, ellipsis, citations, anno-
tations, etc.) other than translation. Second, the
left boundary (denoted as ? in Table 1) of the pre-
parenthesis text need to be determined to get rid of
the unrelated words. Third, we need further distin-
guish different translation types, such as bilingual
abbreviation, the mixture of translation and translit-
eration, as shown in Table 1.
In order to deal with these problems, supervised
(Cao et al, 2007) and unsupervised (Li et al, 2008)
methods have been proposed. However, supervised
approaches are restricted by the quality and quantity
of manually constructed training data, and unsuper-
vised approaches are totally frequency-based with-
out using any semantic clues. In contrast, we pro-
pose a semi-supervised framework for mining par-
enthetical translations. We apply a monolingual ab-
breviation extraction approach to bilingual abbrevia-
tion extraction. We construct an English-syllable to
Chinese-pinyin transliteration model which is self-
trained using phonemic similarity measurements.
We further employ our cascaded translation model
(Wu et al, 2008) which is self-trained based on
morpheme-level translation similarity.
This paper is organized as follows. We briefly
review the related work in the next section. Our
system framework and self-training algorithm is de-
scribed in Section 3. Bilingual abbreviation ex-
traction, self-trained transliteration models and cas-
caded translation models are described in Section 4,
5, and 6, respectively. In Section 7, we evaluate our
mined lexicons by Wikipedia. We conclude in Sec-
tion 8 finally.
2 Related Work
Numerous researchers have proposed a variety of
automatic approaches to mine lexicons from the
Web pages or other large-scale corpora. Shao and
Ng (2004) presented a method to mine new transla-
tions from Chinese and English news documents of
the same period from different news agencies, com-
bining both transliteration and context information.
Kuo et al (2006) used active learning and unsu-
pervised learning for mining transliteration lexicon
from the Web pages, in which an EM process was
used for estimating the phonetic similarities between
English syllables and Chinese characters.
Cao et al (2007) split parenthetical translation
mining task into two parts, transliteration detection
and translation detection. They employed a translit-
eration lexicon for constructing a grapheme-based
transliteration model and annotated boundaries man-
ually to train a classifier. Lin et al (2008) applied
a frequency-based word alignment approach, Com-
petitive Link (Melanmed, 2000), to determine the
outer boundary (Section 7).
On the other hand, there have been many semi-
supervised approaches in numerous applications
425
Parenthetical expression extraction{C(E)} 
Chinese word segmentation{c?(e?)} S-MSRSeg 
Heuristic filtering{c?(e?)} 
Chinese Web pages 
Bilingual abbreviation mining 
Section 4 
Transliteration lexicon mining 
Section 5 
Translation lexicon mining 
Section 6 
(Lin et al, 2008) 
Figure 1: The system framework of mining lexicons from
Chinese Web pages.
(Zhu, 2007), such as self-training in word sense
disambiguation (Yarowsky, 2005) and parsing (Mc-
Closky et al, 2008). In this paper, we apply self-
training to a new topic, lexicon mining.
3 System Framework and Self-Training
Algorithm
Figure 1 illustrates our system framework for min-
ing lexicons from Chinese Web pages. First, par-
enthetical expressions matching Pattern 1 are ex-
tracted. Then, pre-parenthetical Chinese sequences
are segmented into word sequences by S-MSRSeg2
(Gao et al, 2006). The initial parenthetical transla-
tion corpus is constructed by applying the heuristic
rules defined in (Lin et al, 2008)3. Based on this
corpus, we mine three lexicons step by step, a bilin-
gual abbreviation lexicon, a transliteration lexicon,
and a translation lexicon. The abbreviation candi-
dates are extracted firstly by using a heuristic rule
(Section 4.1). Then, the transliteration candidates
are selected by employing a transliteration model
(Section 5.1). Specially, fJ1 (eI1) is taken as a translit-
eration candidate only if a word ei in eI1 can be
transliterated. In addition, a transliteration candidate
will also be considered as a translation candidate if
not all ei can be transliterated (refer to the mixture
example in Table1). Finally, after abbreviation filter-
ing and transliteration filtering, the remaining candi-
2http://research.microsoft.com/research/downloads/details/
7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx
3e.g., fJ1 is predominantly in Chinese and eI1 is predomi-
nantly in English
Algorithm 1 self-training algorithm
Require: L, U = {fJ1 (eI1)}, T , M ?L, (labeled) train-
ing set; U , (unlabeled) candidate set; T , test set; M, the
transliteration or translation model.
1: Lexicon = {} ? new mined lexicon
2: repeat
3: N = {} ? new mined lexicon during one iteration
4: train M on L
5: evaluate M on T
6: for fJ1 (eI1) ? U do
7: topN = {C?|decode eI1 by M}
8: N = N ? {(c, eI1)|c ? fJ1 ?
?C? ? topN s.t. similarity{c, C?} ? ?}
9: end for
10: U = U ?N
11: L = unified(L ?N)
12: Lexicon = unified(Lexicon ?N)
13: until |N | ? ?
14: return Lexicon ? the output
dates are used for translation lexicon mining.
Algorithm 1 addresses the self-training algorithm
for lexicon mining. The main part is a loop from
Line 2 to Line 13. A given seed lexicon is taken
as labeled data and is split into training and testing
sets (L and T ). U={fJ1 (eI1)}, stands for the (unla-
beled) parenthetical expression set. Initially, a trans-
lation/transliteration model (M) is trained on L and
evaluated on T (Line 4 and 5). Then, the English
phrase eI1 of each unlabeled entry is decoded by M,
and the top-N outputs are stored in set topN (Line
7?8). A similarity function on c (a word substring
of fJ1 ) and a top-N output C ? is employed to make
the decision of classification: the pair (c, eI1) will be
selected as a new entry if the similarity between c
and C ? is no smaller than a threshold value ? (Line
8). After processing each entry in U , the new mined
lexicon N is deleted from U and unified with the
current training set L as the new training set (Line
10 and 11). Also, N is added to the final lexicon
(Line 12). When |N | is lower than a threshold, the
loop stops. Finally, the algorithm returns the mined
lexicon.
One of the open problems in Algorithm 1 is how
to append new mined entries into the existing seed
lexicon, considering they have different distribu-
tions. One way is to design and estimate a weight
function on the frequency of new mined entries. For
simplicity, we use a deficient strategy that takes the
weights of all new mined entries to be one.
426
4 Bilingual Abbreviation Extraction
4.1 Methodology
The method that we use for extracting a bilingual
abbreviation lexicon from parenthetical expressions
is inspired by (Okzaki and Ananiadou, 2006). They
used a term recognition approach to build a monolin-
gual abbreviation dictionary from the Medical Liter-
ature Analysis and Retrieval System Online (MED-
LINE) abstracts, wherein acronym definitions (e.g.,
ADM is short for adriamycin, adrenomedullin, etc.)
are abundant. They reported 99% precision and 82-
95% recall. Through locating a textual fragment
with an acronym and its expanded form in pattern
long form (short form), (2)
they defined a heuristic formula to compute the long-
form likelihood LH(c) for a candidate c:
LH(c) = freq(c)? ?
t?Tc
freq(t)? freq(t)?
t?Tc freq(t)
.
(3)
Here, c is a long-form candidate; freq(c) denotes the
frequency of co-occurrence of c with a short-form;
and Tc is a set of nested long-form candidates, each
of which consists of a preceding word followed by
the candidate c. Obviously, for t ? Tc, Equation 3
can be explained as:
LH(c) = freq(c)? E[freq(t)]. (4)
In this paper, we apply their method on the task
of bilingual abbreviation lexicon extraction. Now,
the long-form is a Chinese word sequence and the
short-form is an English acronym. We filter the par-
enthetical expressions in the Web pages with several
heuristic rules to meet the form of pattern 2 and to
save the computing time:
? the short-form (eI1) should contain only one En-
glish word (I = 1), and all letters in which
should be capital;
? similar with (Lin et al, 2008), the pre-
parenthesis text is trimmed with: |c| ? 10 ?
|eI1|+ 6 when |eI1| ? 6, and |c| ? 2? |eI1|+ 6,
otherwise. |c| and |eI1| are measured in bytes.
We further trim the remaining pre-parenthesis
text by punctuations other than hyphens and
dots, i.e., the right most punctuation and its left
subsequence are discarded.
o. Chinese long-form candidates LH T/F
1 ?? ?? ?? 172.5 T
Tumor-Associated Antigen
2 ? ? ?? ? 79.9 T
thioacetamide
3 ? 33.8 F
amine
4 ?? 24.5 F
antigen
5 ?? ?? 21.2 F
associated antigen
6 ? ?? ?? ?? 16.5 F
's Tumor-Associated Antigen
7 ? ??? 16.2 T
total amino acid
Table 2: Top-7 Chinese long-form candidates for the En-
glish acronym TAA, according to the LH score.
4.2 Experiment
We used SogouT Internet Corpus Version 2.04,
which contains about 13 billion original Web pages
(mainly Chinese) in the form of 252 gigabyte .txt
files. In addition, we used 55 gigabyte (.txt for-
mat) Peking University Chinese Paper Corpus. We
constructed a partially parallel corpus in the form
of Pattern 1 from the union of the two corpora us-
ing the heuristic rules defined in (Lin et al, 2008).
We gained a partially parallel corpus which contains
12,444,264 entries.
We extracted 107,856 distinct English acronyms.
Limiting LH score ? 1.0 in Equation 3, we gained
2,020,012 Chinese long-form candidates for the
107,856 English acronyms. Table 2 illustrates the
top-7 Chinese long-form candidates of the English
acronym TAA. Three candidates are correct (T) long-
forms while the other 4 are wrong (F). Wrong can-
didates from No. 3 to 5 are all subsequences of the
correct candidate No. 1. No. 6 includes No. 1 while
with a Chinese functional word de in the left most
side. These error types can be easily tackled with
some filtering patterns, such as ?remove the left most
functional word in the long-form candidates?, ?only
keep the relatively longer candidates with larger LH
score?, etc.
Since there does not yet exists a common eval-
uation data set for the bilingual abbreviation lexi-
con, we manually evaluated a small sample of it.
4http://www.sogou.com/labs/dl/t.html
427
Of the 107,856 English acronyms, we randomly se-
lected 200 English acronyms and their top-1 Chi-
nese long-form candidates for manually evaluating.
We found, 92 candidates were correct including 3
transliteration examples. Of the 108 wrong candi-
dates, 96 candidates included the correct long-form
with some redundant words on the left side (i.e., c =
(word)+ correct long-form), the other 12 candidates
missed some words of the correct long-form or had
some redundant words right before the left paren-
thesis (i.e., c = (word)? correct long-form (word)+
or c = (word)? subsequence of correct long-form
word)?). We classified the redundant word right be-
fore the correct long-form of each of the 96 candi-
dates, de occupied 32, noun occupied 7, verb occu-
pied 18, prepositions and conjunctions occupied the
remaining ones.
In total, the abbreviation translation accuracy is
44.5%. We improved the accuracy to 60.5% with
an additional de filtering pattern. According to for-
mer mentioned error analysis, the accuracy may fur-
ther be improved if a Chinese part-of-speech tagger
is employed and the non-nominal words in the long-
form are removed beforehand.
5 Self-Training for Transliteration Models
In this section, we first describe and compare three
transliteration models. Then, we select and train the
best model following Algorithm 1 for lexicon min-
ing. We investigate two things, the scalability of the
self-trained model given different amount of initial
training data, and the performance of several strate-
gies for selecting new training samples.
5.1 Model description
We construct and compare three forward translit-
eration models, a phoneme-based model (English
phonemes to Chinese pinyins), a grapheme-based
model (English syllables to Chinese characters)
and a hybrid model (English syllables to Chinese
pinyins). Similar models have been compared in
(Oh et al, 2006) for English-to-Korean and English-
to-Japanese transliteration. All the three models are
phrase-based, i.e., adjacent phonemes or graphemes
are allowable to form phrase-level transliteration
units. Building the correspondences on phrase
level can effectively tackle the missing or redundant
phoneme/grapheme problem during transliteration.
For example, when Aamodt is transliterated into a
mo? te`5, a and d are missing. The problem can be
easily solved when taking Aa and dt as single units
for transliterating.
Making use of Moses (Koehn et al, 2007), a
phrase-based SMT system, Matthews (2007) has
shown that the performance was comparable to re-
cent state-of-the-art work (Jiang et al, 2007) in
English-to-Chinese personal name transliteration.
Matthews (2007) took transliteration as translation
at the surface level. Inspired by his idea, we also
implemented our transliteration models employing
Moses. The main difference is that, while Matthews
(2007) tokenized the English names into individual
letters before training in Moses, we split them into
syllables using the heuristic rules described in (Jiang
et al, 2007), such that one syllable only contains one
vowel letter or a combination of a consonant and a
vowel letter.
English syllable sequences are used in the
grapheme-based and hybrid models. In the
phoneme-based model, we transfer English names
into phonemes and Chinese characters into Pinyins
in virtue of the CMU pronunciation dictionary6 and
the LDC Chinese character-to-pinyin list7.
In the mass, the grapheme-based model is the
most robust model, since no additional resources are
needed. However, it suffers from the Chinese homo-
phonic character problem. For instance, pinyin ai
corresponds to numerous Chinese characters which
are applicable to personal names. The phoneme-
based model is the most suitable model that reflects
the essence of transliteration, while restricted by ad-
ditional grapheme to phoneme dictionaries. In or-
der to eliminate the confusion of Chinese homo-
phonic characters and alleviate the dependency on
additional resources, we implement a hybrid model
that accepts English syllables and Chinese pinyins
as formats of the training data. This model is called
hybrid, since English syllables are graphemes and
Chinese pinyins are phonemes.
5The tones of Chinese pinyins are ignored in our translitera-
tion models for simplicity.
6http://www.speech.cs.cmu.edu/cgi-bin/cmudict
7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt
428
 grapheme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 phoneme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 Comparison on EMatch
0.0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8max_phrase_length
grapheme phoneme hybrid
 hybrid-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
Figure 2: The performances of the transliteration models
and their comparison on EMatch.
5.2 Experimental model selection
Similar to (Jiang et al, 2007), the transliteration
models were trained and tested on the LDC Chinese-
English Named Entity Lists Version 1.08. The origi-
nal list contains 572,213 English people names with
Chinese transliterations. We extracted 74,725 en-
tries in which the English names also appeared in
the CMU pronunciation dictionary. We randomly
selected 3,736 entries as an open testing set and the
remaining entries as a training set9. The results were
evaluated using the character/pinyin-based 4-gram
BLEU score (Papineni et al, 2002), word error rate
(WER), position independent word error rate (PER),
and exact match (EMatch).
Figure 2 reports the performances of the three
models and the comparison based on EMatch. From
the results, we can easily draw the conclusion that
the hybrid model performs the best under the maxi-
mal phrase length (mpl, the maximal phrase length
allowed in Moses) from 1 to 8. The performances
of the models converge at or right after mpl =
4. The pinyin-based WER of the hybrid model is
39.13%, comparable to the pinyin error rate 39.6%,
reported in (Jiang et al, 2007)10. Thus, our further
8Linguistic Data Consortium catalog number:
LDC2005T34 (former catalog number: LDC2003E01)
9Jiang et al (2007) selected 25,718 personal name pairs
from LDC2003E01 as the experiment data: 200 as development
set, 200 as test set, and the remaining entries as training set.
10It should be notified that we achieved this result by using
larger training set (70,989 vs. 25,718) and larger test set (3,736
vs. 200) comparing with (Jiang et al, 2007), and we did not use
% 0t 1t 2t 3t 4t 5t Strategy
5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em
.3911 .3979 .3954 .3974 .3965 top1 am
.4062 .4182 .4208 .4218 .4201 top5 em
.3987 .4177 .4190 .4192 .4189 top5 am
10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em
.4121 .4190 .4180 .4174 .4200 top1 am
.4305 .4386 .4399 .4438 .4403 top5 em
.4289 .4263 .4292 .4291 .4288 top5 am
20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em
.4532 .4578 .4544 .4545 .4541 top1 am
.4624 .4762 .4754 .4748 .4746 top5 em
.4605 .4677 .4677 .4674 .4679 top5 am
40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em
.4774 .4794 .4779 .4789 .4784 top1 am
.4808 .4811 .4791 .4795 .4790 top5 em
.4775 .4778 .4781 .4785 .4779 top5 am
60 .5032 .4939 .5004 .5012 .5012 .5016 top1 em
.4919 .4988 .4990 .4994 .4990 top1 am
.5013 .5063 .5059 .5066 .5065 top5 em
.4919 .4960 .4970 .4977 .4962 top5 am
80 .5038 .4984 .4984 .5004 .5006 .4995 top1 em
.4916 .4916 .4914 .4915 .4916 top1 am
.5039 .5037 .5053 .5054 .5042 top5 em
.4950 .5028 .5027 .5032 .5032 top5 am
100 .5045 .5077 .5053 .5067 .5063 .5066 top1 em
.5045 .5054 .5046 .5050 .5055 top1 am
.5108 .5102 .5111 .5108 .5115 top5 em
.5105 .5106 .5100 .5094 .5109 top5 am
Table 3: The BLEU score of self-trained h4 translitera-
tion models under four selection strategies. nt (n=1..5)
stands for the n-th iteration.
self-training experiments are pursued on the hybrid
model taking mpl to be 4 (short for h4, hereafter).
5.3 Experiments on the self-trained hybrid
model
As former mentioned, we investigate the scalability
of the self-trained h4 model by respectively using 5,
10, 20, 40, 60, 80, and 100 percent of initial training
data, and the performances of using exact matching
(em) or approximate matching (am, line 8 in Algo-
rithm 1) on the top-1 and top-5 outputs (line 7 in Al-
gorithm 1) for selecting new training samples. We
used edit distance (ed) to measure the em and am
similarities:
ed(c, C ?) = 0 or < syllable number(C ?)/2. (5)
When applying Algorithm 1 for transliteration lexi-
con mining, we decode each word in eI1 respectively.
The algorithm terminated in five iterations when we
set the terminal threshold ? (Line 13 in Algorithm 1)
to be 100.
For simplicity, Table 3 only illustrates the BLEU
score of h4 models under four selection strategies.
From this table, we can draw the following conclu-
sions. First, with fewer initial training data, the im-
provement is better. The best relative improvements
additional Web resources as Jiang et al (2007) did.
429
are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%,
and 1.39%, respectively. Second, using top-5 and
em for new training data selection performs the best
among the four strategies. Compared under each it-
eration, using top-5 is better than using top-1; em
is better than am; and top-5 with am is a little bet-
ter than top-1 with em. We mined 39,424, 42,466,
46,116, 47,057, 49,551, 49,622, and 50,313 distinct
entries under the six types of initial data with top-5
plus em strategy. The 50,313 entries are taken as the
final transliteration lexicon for further comparison.
6 Self-Training for a Cascaded Translation
Model
We classify the parenthetical translation candidates
by employing a translation model. In contrast to
(Lin et al, 2008), wherein the lengthes of prefixes
and suffixes of English words were assumed to be
three bytes, we segment words into morphemes (se-
quences of prefixes, stems, and suffixes) by Morfes-
sor 0.9.211, an unsupervised language-independent
morphological analyzer (Creutz and Lagus, 2007).
We use the morpheme-level translation similarity
explicitly in our cascaded translation model (Wu et
al., 2008), which makes use of morpheme, word,
and phrase level translation units. We train Moses
to gain a phrase-level translation table. To gain a
morpheme-level translation table, we run GIZA++
(Och and Ney, 2003) on both directions between En-
glish morphemes and Chinese characters, and take
the intersection of Viterbi alignments. The English-
to-Chinese translation probabilities computed by
GIZA++ are attached to each morpheme-character
element in the intersection set.
6.1 Experiment
The Wanfang Chinese-English technical term dictio-
nary12, which contains 525,259 entries in total, was
used for training and testing. 10,000 entries were
randomly selected as the test set and the remaining
as the training set. Again, we investigated the scala-
bility of the self-trained cascaded translation model
by respectively using 20, 40, 60, 80, and 100 per-
cent of initial training data. An aggressive similar-
11http://www.cis.hut.fi/projects/morpho/
12http://www.wanfangdata.com.cn/Search/ResourceBrowse
.aspx
% 0t 1t 2t 3t 4t 5t
20 .1406 .1196 .1243 .1239 .1176 .1179
40 .1091 .1224 .1386 .1345 .1479 .1466
60 .1630 .1624 .1429 .1714 .1309 .1398
80 .1944 .1783 .1886 .1870 .1884 .1873
100 .1810 .1814 .1539 .1981 .1542 .1944
Table 4: The BLEU score of self-trained cascaded trans-
lation model under five initial training sets.
ity measurement was used for selecting new training
samples:
first char(c) = first char(C ?) ? min{ed(c, C ?)}.
(6)
Here, we judge if the first characters of c and C ?
are similar or not. c was gained by deleting zero
or more characters from the left side of fJ1 . When
more than one c satisfied this condition, the c that
had the smallest edit distance with C ? was selected.
When applying Algorithm 1 for translation lexicon
mining, we took eI1 as one input for decoding instead
of decoding each word respectively. Only the top-1
output (C ?) was used for comparing. The algorithm
stopped in five iterations when we set the terminal
threshold ? to be 2000.
For simplicity, Table 4 only illustrates the BLEU
score of the cascaded translation model under five
initial training sets. For the reason that there are fi-
nite phonemes in English and Chinese while the se-
mantic correspondences between the two languages
tend to be infinite, Table 4 is harder to be analyzed
than Table 3. When initially using 40%, 60%, and
100% training data for self-training, the results tend
to be better at some iterations. We gain 35.6%,
5.2%, and 9.4% relative improvements, respectively.
However, the results tend to be worse when 20% and
80% training data were used initially, with 11.6%
and 3.0% minimal relative loss. The best BLEU
scores tend to be better when more initial training
data are available. We mined 1,038,617, 1,025,606,
1,048,761, 1,056,311, and 1,060,936 distinct entries
under the five types of initial training data. The
1,060,936 entries are taken as the final translation
lexicon for further comparison.
7 Wikipedia Evaluation
We have mined three kinds of lexicons till now,
an abbreviation lexicon containing 107,856 dis-
430
En. to Ch. Ch. to En.
Cov EMatch Cov EMatch
Our Lexicon 22.8% 5.2% 23.2% 5.5%
Unsupervised 23.5% 5.4% 24.0% 5.4%
Table 5: The results of our lexicon and an unsupervised-
mined lexicon (Lin et al, 2008) evaluated under
Wikipedia title dictionary. Cov is short for coverage.
similar English acronyms with 2,020,012 Chinese
long-form candidates; a transliteration lexicon with
50,313 distinct entries; and a translation lexicon
with 1,060,936 distinct entries. The three lexicons
are combined together as our final lexicon.
Similar with (Lin et al, 2008), we compare our
final mined lexicon with a dictionary extracted from
Wikipedia, the biggest multilingual free-content en-
cyclopedia on the Web. We extracted the titles of
Chinese and English Wikipedia articles13 that are
linked to each other. Since most titles contain less
than five words, we take a linked title pair as a trans-
lation entry without considering the word alignment
relation between the words inside the titles. The re-
sult lexicon contains 105,320 translation pairs be-
tween 103,823 Chinese titles and 103,227 English
titles. Obviously, only a small percentage of titles
have more than one translation. Whenever there is
more than one translation, we take the candidate en-
try as correct if and only if it matches one of the
translations.
Moreover, we compare our semi-supervised ap-
proach with an unsupervised approach (Lin et al,
2008). Lin et al (2008) took ?2(fj , ei) score
14(Gale and Church, 1991) with threshold 0.001 as
the word alignment probability in a word alignment
algorithm, Competitive Link. Competitive Link tries
to align an unlinked ei with an unlinked fj by the
condition that ?2(fj , ei) is the biggest. Lin et al
(2008) relaxed the unlinked constraints to allow con-
secutive sequence of words on one side to be linked
to the same word on the other side15. The left
13English and Chinese Wikipedia pages due to 2008.09.23
are used here.
14?2(fj , ei) = (ad?bc)
2
(a+b)(a+c)(b+d)(c+d) , where a is the number
of fJ1 (eI1) containing both ei and fj ; (a + b) is the number of
fJ1 (eI1) containing ei; (a+ c) is the number of fJ1 (eI1) contain-
ing fj ; and d is the number of fJ1 (eI1) containing neither ei nor
fj .
15Instead of requiring both ei and fj to have no previous link-
boundary inside fJ1 is determined when each ei in
eI1 is aligned. After applying the modified Compet-
itive Link on the partially parallel corpus which in-
cludes 12,444,264 entries (Section 4.2), we obtained
2,628,366 distinct pairs.
Table 5 shows the results of the two lexicons eval-
uated under Wikipedia title dictionary. The coverage
is measured by the percentage of titles which ap-
pears in the mined lexicon. We then check whether
the translation in the mined lexicon is an exact match
of one of the translations in the Wikipedia lexicon.
Through comparing the results, our mined lexicon is
comparable with the lexicon mined in an unsuper-
vised way. Since the selection is based on phone-
mic and semantic clues instead of frequency, a par-
enthetical translation candidate will not be selected
if the in-parenthetical English text is failed to be
transliterated or translated. This is one reason that
explains why we earned a little lower coverage. An-
other reason comes from the low coverage rate of
seed lexicons used for self-training, only 8.65% En-
glish words in the partially parallel corpus are cov-
ered by the Wanfang dictionary.
8 Conclusion
We have proposed a semi-supervised learning
framework for mining bilingual lexicons from par-
enthetical expressions in monolingual Web pages.
We classified the parenthesis expressions into three
categories: abbreviation, transliteration, and transla-
tion. A set of heuristic rules, a self-trained hybrid
transliteration model, and a self-trained cascaded
translation model were proposed for each category,
respectively.
We investigated the scalability of the self-trained
transliteration and translation models by training
them with different amount of data. The results shew
the stability (transliteration) and feasibility (transla-
tion) of our proposals. Through employing the par-
allel Wikipedia article titles as a gold standard lex-
icon, we gained the comparable results comparing
our semi-supervised framework with our implemen-
tation of Lin et al (2008)?s unsupervised mining
approach.
ages, they only require that at least one of them be unlinked and
that (suppose ei is unlinked and fj is linked to ek) none of the
words between ei and ek be linked to any word other than fj .
431
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
the anonymous reviewers for their constructive com-
ments.
References
Cao, Guihong, Jianfeng Gao, and Jian-Yun Nie. 2007.
A system to Mine Large-Scale Bilingual Dictionar-
ies from Monolingual Web Pages. In MT Summit XI.
pages 57?64, Copenhagen, Denmark.
Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and Lee-
Feng Chien. 2004. Creating Multilingual Translation
Lexicons with Regional Variations Using Web Cor-
pora. In ACL 2004, pages 534?541, Barcelona,
Spain.
Creutz, Mathias and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):Article 3.
Dempster, A. P., N. M. Laird and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Gale, W. and K. Church. 1991. Identifying word corre-
spondence in parallel text. In DARPA NLP Workshop.
Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.
2006. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, 31(4):531?574.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In ACL-08:HLT. pages
771?779, Columbus, Ohio.
Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named Entity Translation with Web Min-
ing and Transliteration. In IJCAI 2007. pages 1629?
1634, Hyderabad, India.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007 Poster Session, pages 177?180.
Koehn, Philipp and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
SIGLEX 2002, pages 9?16.
Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web. In
COLING-ACL 2006. pages 1129?1136.
Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and
Marius Pas?ca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In ACL-
08:HLT, pages 994?1002, Columbus, Ohio.
Matthews, David. 2007. Machine Transliteration of
Proper Names. A Thesis of Master. University of Ed-
inburgh.
McClosky, David, Eugene Charniak, and Mark Johnson
2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561?
568, manchester, UK.
Melamed, I. Dan. 2000. Models of Translational Equiv-
alence among Words. Computational Linguistics,
26(2):221?249.
Och, Franz Josef and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara.
2006. A Comparison of Different Machine Translit-
eration Models. Journal of Artifical Intelligence Re-
search, 27:119?151.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Building
an Abbreviation Dictionary Using a Term Recognition
Approach. Bioinformatics, 22(22):3089?3095.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL). pages 311?318, Philadel-
phia.
Shao, Li and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING), pages 618?624,
Geneva, Switzerland.
Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and
Jun?ichi Tsujii. 2008. Improving English-to-Chinese
Translation for Technical Terms Using Morphological
Information. In Proceedings of the 8th Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA), pages 202?211, Waikiki, Hawai?i.
Yarowsky, David. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, pages 189?196, Cam-
bridge, Massachusetts.
Zhu, Xiaojin. 2007. Semi-Supervised Learning Litera-
ture Survery. University of Wisconsin - Madison.
432
Proceedings of NAACL HLT 2009: Short Papers, pages 97?100,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Combination Features with L1 Regularization
Daisuke Okanohara? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
?NaCTeM (National Center for Text Mining)
{hillbig,tsujii}@is.s.u-tokyo.ac.jp
Abstract
When linear classifiers cannot successfully
classify data, we often add combination fea-
tures, which are products of several original
features. The searching for effective combi-
nation features, namely feature engineering,
requires domain-specific knowledge and hard
work. We present herein an efficient algorithm
for learning an L1 regularized logistic regres-
sion model with combination features. We
propose to use the grafting algorithm with ef-
ficient computation of gradients. This enables
us to find optimal weights efficiently without
enumerating all combination features. By us-
ing L1 regularization, the result we obtain is
very compact and achieves very efficient in-
ference. In experiments with NLP tasks, we
show that the proposed method can extract ef-
fective combination features, and achieve high
performance with very few features.
1 Introduction
A linear classifier is a fundamental tool for many
NLP applications, including logistic regression
models (LR), in that its score is based on a lin-
ear combination of features and their weights,. Al-
though a linear classifier is very simple, it can
achieve high performance on many NLP tasks,
partly because many problems are described with
very high-dimensional data, and high dimensional
weight vectors are effective in discriminating among
examples.
However, when an original problem cannot be
handled linearly, combination features are often
added to the feature set, where combination features
are products of several original features. Examples
of combination features are, word pairs in docu-
ment classification, or part-of-speech pairs of head
and modifier words in a dependency analysis task.
However, the task of determining effective combina-
tion features, namely feature engineering, requires
domain-specific knowledge and hard work.
Such a non-linear phenomenon can be implic-
itly captured by using the kernel trick. However,
its computational cost is very high, not only during
training but also at inference time. Moreover, the
model is not interpretable, in that effective features
are not represented explicitly. Many kernels meth-
ods assume an L2 regularizer, in that many features
are equally relevant to the tasks (Ng, 2004).
There have been several studies to find efficient
ways to obtain (combination) features. In the con-
text of boosting, Kudo (2004) have proposed a
method to extract complex features that is similar
to the item set mining algorithm. In the context of
L1 regularization. Dud??k (2007), Gao (2006), and
Tsuda (2007) have also proposed methods by which
effective features are extracted from huge sets of fea-
ture candidates. However, their methods are still
very computationally expensive, and we cannot di-
rectly apply this kind of method to a large-scale NLP
problem.
In the present paper, we propose a novel algorithm
for learning of an L1 regularized LR with combina-
tion features. In our algorithm, we can exclusively
extract effective combination features without enu-
merating all of the candidate features. Our method
relies on a grafting algorithm (Perkins and Theeiler,
2003), which incrementally adds features like boost-
ing, but it can converge to the global optimum.
We use L1 regularization because we can obtain
a sparse parameter vector, for which many of the
parameter values are exactly zero. In other words,
learning with L1 regularization naturally has an in-
trinsic effect of feature selection, which results in an
97
efficient and interpretable inference with almost the
same performance as L2 regularization (Gao et al,
2007).
The heart of our algorithm is a way to find a
feature that has the largest gradient value of likeli-
hood from among the huge set of candidates. To
solve this problem, we propose an example-wise al-
gorithm with filtering. This algorithm is very simple
and easy to implement, but effective in practice.
We applied the proposed methods to NLP tasks,
and found that our methods can achieve the same
high performance as kernel methods, whereas the
number of active combination features is relatively
small, such as several thousands.
2 Preliminaries
2.1 Logistic Regression Model
In this paper, we consider a multi-class logistic re-
gression model (LR). For an input x, and an output
label y ? Y , we define a feature vector ?(x, y) ?
Rm.
Then in LR, the probability for a label y, given an
input x, is defined as follows:
p(y|x;w) = 1Z(x,w) exp
(
wT?(x, y)) , (1)
where w ? Rm is a weight vector1 correspond-
ing to each input dimension, and Z(x,w) =?
y exp(wT?(x, y)) is the partition function.We estimate the parameter w by a maximum like-
lihood estimation (MLE) with L1 regularization us-
ing training examples {(x1, y1), . . . , (xn, yn)}:
w? = argmin
w
? L(w) + C?
i
|wi| (2)
L(w) = ?
i=1...n
log p(yi|xi;w)
where C > 0 is the trade-off parameter between the
likelihood term and the regularization term. This es-
timation is a convex optimization problem.
2.2 Grafting
To maximize the effect of L1 regularization, we use
the grafting algorithm (Perkins and Theeiler, 2003);
namely, we begin with the empty feature set, and
incrementally add effective features to the current
problem. Note that although this is similar to the
1A bias term b is often considered by adding an additional
dimension to ?(x, y)
boosting algorithm for learning, the obtained result
is always optimal. We explain the grafting algorithm
here again for the sake of clarity.
The grafting algorithm is summarized in Algo-
rithm 1.
In this algorithm we retain two variables; w stores
the current weight vector, and H stores the set of
features with a non-zero weight. Initially, we set
w = 0, and H = {}. At each iteration, the fea-
ture is selected that has the largest absolute value of
the gradient of the likelihood. Let vk = ?L(w)?wk bethe gradient value of the likelihood of a feature k.
By following the definition, the value vk can be cal-
culated as follows,
vk =
?
i,y
?i,y?k(xi, y), (3)
where ?i,y = I(yi = y)? p(yi|xi;w) and I(a) is 1
if a is true and 0 otherwise.
Then, we add k? = argmaxk |vk| to H and opti-
mize (2) with regard to H only. The solution w that
is obtained is used in the next search. The iteration
is continued until |v?k| < C.We briefly explain why we can find the optimal
weight by this algorithm. Suppose that we optimize
(2) with all features, and initialize the weights us-
ing the results obtained from the grafting algorithm.
Since all gradients of likelihoods satisfy |vk| ? C,
and the regularization term pushes the weight toward
0 by C, any changes of the weight vector cannot in-
crease the objective value in (2). Since (2) is the
convex optimization problem, the local optimum is
always the global optimum, and therefore this is the
global optimum for (2)
The point is that, given an efficient method to esti-
mate v?k without the enumeration of all features, wecan solve the optimization in time proportional to the
active feature, regardless of the number of candidate
features. We will discuss this in the next section.
3 Extraction of Combination Features
This section presents an algorithm to compute, for
combination features, the feature v?k that has thelargest absolute value of the gradient.
We propose an element-wise extraction method,
where we make use of the sparseness of the training
data.
In this paper, we assume that the values of the
combination features are less than or equal to the
original ones. This assumption is typical; for exam-
ple, it is made in the case where we use binary values
for original and combination features.
98
Algorithm 1 Grafting
Input: training data (xi, yi) (i = 1, ? ? ? , n) and
parameter C
H = {},w = 0
loop
v = ?L(w)?w (L(w) is the log likelihood term)
k? = argmax
k
|vk| (The result of Algorithm 2)
if |vk? | < C then break
H = H ? k?
Optimize w with regards to H
end loop
Output w and H
First, we sort the examples in the order of their?
y |?i,y| values. Then, we look at the examples oneby one. Let us assume that r examples have been
examined so far. Let us define
t = ?
i?r,y
?i,y?(xi, y) (4)
t? = ?
i>r,y
??i,y?(xi, y) t+ =
?
i>r,y
?+i,y?(xi, y)
where ??i,y = min(?i,y, 0) and ?+i,y = max(?i,y, 0).Then, simple calculus shows that the gradient
value for a combination feature k, vk, for which
the original features are k1 and k2, is bounded be-
low/above thus;
tk + t?k < vk < tk + t+k (5)
tk + max(t?k1, t?k2) < vk < tk + min(t+k1, t+k2).
Intuitively, the upper bound of (5) is the case where
the combination feature fires only for the examples
with ?i,y ? 0, and the lower bound of (5is the case
where the combination feature fires only for the ex-
amples with ?i,y ? 0. The second inequality arises
from the fact that the value of a combination feature
is equal to or less than the values of its original fea-
tures. Therefore, we examine (5) and check whether
or not |vk| will be larger than C. If not, we can re-
move the feature safely.
Since the examples are sorted in the order of their?
y |?i,y|, the bound will become tighter quickly.Therefore, many combination features are filtered
out in the early steps. In experiments, the weights
for the original features are optimized first, and then
the weights for combination features are optimized.
This significantly reduces the number of candidates
for combination features.
Algorithm 2 Algorithm to return the feature that has
the largest gradient value.
Input: training data (xi, yi) and its ?i,y value
(i = 1, . . . , n, y = 1, . . . , |Y |), and the param-
eter C. Examples are sorted with respect to their?
y |?i,y| values.
t+ =?ni=1
?
y max(?i,y, 0)?(x, y)
t? =?ni=1
?
y min(?i,y, 0)?(x, y)
t = 0, H = {} // Active Combination Feature
for i = 1 to n and y ? Y do
for all combination features k in xi do
if |vk| > C (Check by using Eq.(5) ) then
vk := vk + ?i,y?k(xi, y)
H = H ? k
end if
end for
t+ := t+ ?max(?i,y, 0)?(xi, y)
t? := t? ?min(?i,y, 0)?(xi, y)
end for
Output: argmaxk?H vk
Algorithm 2 presents the details of the overall al-
gorithm for the extraction of effective combination
features. Note that many candidate features will be
removed just before adding.
4 Experiments
To measure the effectiveness of the proposed
method (called L1-Comb), we conducted experi-
ments on the dependency analysis task, and the doc-
ument classification task. In all experiments, the pa-
rameterC was tuned using the development data set.
In the first experiment, we performed Japanese
dependency analysis. We used the Kyoto Text Cor-
pus (Version 3.0), Jan. 1, 3-8 as the training data,
Jan. 10 as the development data, and Jan. 9 as the
test data so that the result could be compared to those
from previous studies (Sassano, 2004)2. We used the
shift-reduce dependency algorithm (Sassano, 2004).
The number of training events was 11, 3332, each of
which consisted of two word positions as inputs, and
y = {0, 1} as an output indicating the dependency
relation. For the training data, the number of orig-
inal features was 78570, and the number of combi-
nation features of degrees 2 and 3 was 5787361, and
169430335, respectively. Note that we need not see
all of them using our algorithm.
2The data set is different from that in the CoNLL shared
task. This data set is more difficult.
99
Table 1: The performance of the Japanese dependency
task on the Test set. The active features column shows
the number of nonzero weight features.
DEP. TRAIN ACTIVE
ACC. (%) TIME (S) FEAT.
L1-COMB 89.03 605 78002
L1-ORIG 88.50 35 29166
SVM 3-POLY 88.72 35720 (KERNEL)
L2-COMB3 89.52 22197 91477782
AVE. PERCE. 87.23 5 45089
In all experiments, combination features of de-
grees 2 and 3 (the products of two or three original
features) were used.
We compared our methods using LR with L1
regularization using original features (L1-Original),
SVM with a 3rd-polynomial Kernel, LR with L2
regularization using combination features with up to
3 combinations (L2-Comb3), and an averaged per-
ceptron with original features (Ave. Perceptron).
Table 1 shows the result of the Japanese depen-
dency task. The accuracy result indicates that the
accuracy was improved with automatically extracted
combination features. In the column of active fea-
tures, the number of active features is listed. This
indicates thatL1 regularization automatically selects
very few effective features. Note that, in training,
L1-Comb used around 100 MB, while L2-Comb3
used more than 30 GB. The most time consuming
part for L1-Comb was the optimization of the L1-
LR problem.
Examples of extracted combination features in-
clude POS pairs of head and modifiers, such as
Head/Noun-Modifier/Noun, and combinations of
distance features with the POS of head.
For the second experiment, we performed the
document classification task using the Tech-TC-300
data set (Davidov et al, 2004)3. We used the tf-idf
scores as feature values. We did not filter out any
words beforehand. The Tech-TC-300 data set con-
sists of 295 binary classification tasks. We divided
each document set into a training and a test set. The
ratio of the test set to the training set was 1 : 4. The
average number of features for tasks was 25, 389.
Table 2 shows the results for L1-LR with combi-
nation features and SVM with linear kernel4. The
results indicate that the combination features are ef-
fective.
3http://techtc.cs.technion.ac.il/techtc300/techtc300.html
4SVM with polynomial kernel did not achieve significant
improvement
Table 2: Document classification results for the Tech-TC-
300 data set. The column F2 shows the average of F2
scores for each method of classification.
F2
L1-COMB 0.949
L1-ORIG 0.917
SVM (LINEAR KERNEL) 0.896
5 Conclusion
We have presented a method to extract effective
combination features for the L1 regularized logis-
tic regression model. We have shown that a simple
filtering technique is effective for enumerating effec-
tive combination features in the grafting algorithm,
even for large-scale problems. Experimental results
show that a L1 regularized logistic regression model
with combination features can achieve comparable
or better results than those from other methods, and
its result is very compact and easy to interpret. We
plan to extend our method to include more complex
features, and apply it to structured output learning.
References
Davidov, D., E. Gabrilovich, and S. Markovitch. 2004.
Parameterized generation of labeled datasets for text
categorization based on a hierarchical directory. In
Proc. of SIGIR.
Dud??k, Miroslav, Steven J. Phillips, and Robert E.
Schapire. 2007. Maximum entropy density estima-
tion with generalized regularization and an application
to species distribution modeling. JMLR, 8:1217?1260.
Gao, J., H. Suzuki, and B. Yu. 2006. Approximation
lasso methods for language modeling. In Proc. of
ACL/COLING.
Gao, J., G. Andrew, M. Johnson, and K. Toutanova.
2007. A comparative study of parameter estimation
methods for statistical natural language processing. In
Proc. of ACL, pages 824?831.
Kudo, T. and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proc. of
EMNLP.
Ng, A. 2004. Feature selection, l1 vs. l2 regularization,
and rotational invariance. In NIPS.
Perkins, S. and J. Theeiler. 2003. Online feature selec-
tion using grafting. ICML.
Saigo, H., T. Uno, and K. Tsuda. 2007. Mining com-
plex genotypic features for predicting HIV-1 drug re-
sistance. Bioinformatics, 23:2455?2462.
Sassano, Manabu. 2004. Linear-time dependency analy-
sis for japanese. In Proc. of COLING.
100
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73?80,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Discriminative Language Model with Pseudo-Negative Samples
Daisuke Okanohara  Jun?ichi Tsujii 
 Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
hillbig,tsujii@is.s.u-tokyo.ac.jp
Abstract
In this paper, we propose a novel discrim-
inative language model, which can be ap-
plied quite generally. Compared to the
well known N-gram language models, dis-
criminative language models can achieve
more accurate discrimination because they
can employ overlapping features and non-
local information. However, discriminative
language models have been used only for
re-ranking in specific applications because
negative examples are not available. We
propose sampling pseudo-negative examples
taken from probabilistic language models.
However, this approach requires prohibitive
computational cost if we are dealing with
quite a few features and training samples.
We tackle the problem by estimating the la-
tent information in sentences using a semi-
Markov class model, and then extracting
features from them. We also use an on-
line margin-based algorithm with efficient
kernel computation. Experimental results
show that pseudo-negative examples can be
treated as real negative examples and our
model can classify these sentences correctly.
1 Introduction
Language models (LMs) are fundamental tools for
many applications, such as speech recognition, ma-
chine translation and spelling correction. The goal
of LMs is to determine whether a sentence is correct
or incorrect in terms of grammars and pragmatics.
The most widely used LM is a probabilistic lan-
guage model (PLM), which assigns a probability to
a sentence or a word sequence. In particular, N-
grams with maximum likelihood estimation (NLMs)
are often used. Although NLMs are simple, they are
effective for many applications.
However, NLMs cannot determine correctness
of a sentence independently because the probabil-
ity depends on the length of the sentence and the
global frequencies of each word in it. For exam-
ple,   
 
    

, where    is the probability
of a sentence  given by an NLM, does not always
mean that 

is more correct, but instead could occur
when 

is shorter than 
 
, or if 

has more com-
mon words than 
 
. Another problem is that NLMs
cannot handle overlapping information or non-local
information easily, which is important for more ac-
curate sentence classification. For example, a NLM
could assign a high probability to a sentence even if
it does not have a verb.
Discriminative language models (DLMs) have
been proposed to classify sentences directly as cor-
rect or incorrect (Gao et al, 2005; Roark et al,
2007), and these models can handle both non-local
and overlapping information. However DLMs in
previous studies have been restricted to specific ap-
plications. Therefore the model cannot be used for
other applications. If we had negative examples
available, the models could be trained directly by
discriminating between correct and incorrect sen-
tences.
In this paper, we propose a generic DLM, which
can be used not only for specific applications, but
also more generally, similar to PLMs. To achieve
73
this goal, we need to solve two problems. The first
is that since we cannot obtain negative examples (in-
correct sentences), we need to generate them. The
second is the prohibitive computational cost because
the number of features and examples is very large. In
previous studies this problem did not arise because
the amount of training data was limited and they did
not use a combination of features, and thus the com-
putational cost was negligible.
To solve the first problem, we propose sampling
incorrect sentences taken from a PLM and then
training a model to discriminate between correct and
incorrect sentences. We call these examples Pseudo-
Negative because they are not actually negative sen-
tences. We call this method DLM-PN (DLM with
Pseudo-Negative samples).
To deal with the second problem, we employ an
online margin-based learning algorithm with fast
kernel computation. This enables us to employ com-
binations of features, which are important for dis-
crimination between correct and incorrect sentences.
We also estimate the latent information in sentences
by using a semi-Markov class model to extract fea-
tures. Although there are substantially fewer la-
tent features than explicit features such as words or
phrases, latent features contain essential information
for sentence classification.
Experimental results show that these pseudo-
negative samples can be treated as incorrect exam-
ples, and that DLM-PN can learn to correctly dis-
criminate between correct and incorrect sentences
and can therefore classify these sentences correctly.
2 Previous work
Probabilistic language models (PLMs) estimate the
probability of word strings or sentences. Among
these models, N-gram language models (NLMs) are
widely used. NLMs approximate the probability by
conditioning only on the preceding     words.
For example, let  denote a sentence of  words,
  
 
 

     
 
. Then, by the chain rule of
probability and the approximation, we have
      
 
 

     
 


 
  
  


  
     
  
 (1)
The parameters can be estimated using the maxi-
mum likelihood method.
Since the number of parameters in NLM is still
large, several smoothing methods are used (Chen
and Goodman, 1998) to produce more accurate
probabilities, and to assign nonzero probabilities to
any word string.
However, since the probabilities in NLMs depend
on the length of the sentence, two sentences of dif-
ferent length cannot be compared directly.
Recently, Whole Sentence Maximum Entropy
Models (Rosenfeld et al, 2001) (WSMEs) have
been introduced. They assign a probability to
each sentence using a maximum entropy model.
Although WSMEs can encode all features of a
sentence including non-local ones, they are only
slightly superior to NLMs, in that they have the dis-
advantage of being computationally expensive, and
not all relevant features can be included.
A discriminative language model (DLM) assigns
a score 	  to a sentence , measuring the correct-
ness of a sentence in terms of grammar and prag-
matics, so that 	  
  implies  is correct and
	    implies  is incorrect. A PLM can be
considered as a special case of a DLM by defining
	 using   . For example, we can take 	  
     , where  is some threshold, and 
is the length of .
Given a sentence , we extract a feature vector
( ) from it using a pre-defined set of feature
functions 



 
. The form of the function 	 we
use is
	        (2)
where   is a feature weighting vector.
Since there is no restriction in designing  ,
DLMs can make use of both over-lapping and non-
local information in . We estimate  using training
samples  

 

 for   , where 

  if 

is correct and 

   if 

is incorrect.
However, it is hard to obtain incorrect sentences
because only correct sentences are available from
the corpus. This problem was not an issue for previ-
ous studies because they were concerned with spe-
cific applications and therefore were able to obtain
real negative examples easily. For example, Roark
(2007) proposed a discriminative language model, in
which a model is trained so that a correct sentence
should have higher score than others. The differ-
ence between their approach and ours is that we do
not assume just one application. Moreover, they had
74
For i=1,2,...
Choose a word 

at random
according to the distribution
  


  
     
  

If 

 "end of a sentence"
Break
End End
Figure 1: Sample procedure for pseudo-negative ex-
amples taken from N-gram language models.
training sets consisting of one correct sentence and
many incorrect sentences, which were very similar
because they were generated by the same input. Our
framework does not assume any such training sets,
and we treat correct or incorrect examples indepen-
dently in training.
3 Discriminative Language Model with
Pseudo-Negative samples
We propose a novel discriminative language model;
a Discriminative Language Model with Pseudo-
Negative samples (DLM-PN). In this model,
pseudo-negative examples, which are all assumed to
be incorrect, are sampled from PLMs.
First a PLM is built using training data and then
examples, which are almost all negative, are sam-
pled independently from PLMs. DLMs are trained
using correct sentences from a corpus and negative
examples from a Pseudo-Negative generator.
An advantage of sampling is that as many nega-
tive examples can be collected as correct ones, and
a distinction can be clearly made between truly cor-
rect sentences and incorrect sentences, even though
the latter might be correct in a local sense.
For sampling, any PLMs can be used as long
as the model supports a sentence sampling proce-
dure. In this research we used NLMs with interpo-
lated smoothing because such models support effi-
cient sentence sampling. Figure 1 describes the sam-
pling procedure and figure 2 shows an example of a
pseudo-negative sentence.
Since the focus is on discriminating between cor-
rect sentences from a corpus and incorrect sentences
sampled from the NLM, DLM-PN may not able to
classify incorrect sentences that are not generated
from the NLM. However, this does not result in a se-
We know of no program, and animated
discussions about prospects for trade
barriers or regulations on the rules
of the game as a whole, and elements
of decoration of this peanut-shaped
to priorities tasks across both target
countries
Figure 2: Example of a sentence sampled by PLMs
(Trigram).
Corpus
Build a probabilistic language model
Sample sentences
Positive (Pseudo-) Negative
Binary Classifier
test sentences
Return positive/negative label or score (margin)
Input training examples
Probabilistic LM
(e.g. N-gram LM)
Figure 3: Framework of our classification process.
rious problem, because these sentences, if they exist,
can be filtered out by NLMs.
4 Online margin-based learning with fast
kernel computation
The DLM-PN can be trained by using any binary
classification learning methods. However, since the
number of training examples is very large, batch
training has suffered from prohibitively large com-
putational cost in terms of time and memory. There-
fore we make use of an online learning algorithm
proposed by (Crammer et al, 2006), which has a
much smaller computational cost. We follow the
definition in (Crammer et al, 2006).
The initiation vector  
 
is initialized to  and for
each round the algorithm observes a training exam-
ple 

  

 and predicts its label 

to be either
 or  . After the prediction is made, the true la-
bel 

is revealed and the algorithm suffers an instan-
taneous hinge-loss     

 

     

  

 


which reflects the degree to which its prediction was
wrong. If the prediction was wrong, the parameter
75
  is updated as
 
 
 
 


    



  (3)
subject to     

 

   and    (4)
where  is a slack term and  is a positive parameter
which controls the influence of the slack term on the
objective function. A large value of will result in a
more aggressive update step. This has a closed form
solution as
 
 
  

 





(5)
where 

 	


 

 

 
. As in SVMs, a fi-
nal weight vector can be represented as a kernel-
dependent combination of the stored training exam-
ples.
    








  (6)
Using this formulation the inner product can be re-
placed with a general Mercer kernel  

 such
as a polynomial kernel or a Gaussian kernel.
The combination of features, which can capture
correlation information, is important in DLMs. If
the kernel-trick (Taylor and Cristianini, 2004) is ap-
plied to online margin-based learning, a subset of
the observed examples, called the active set, needs
to be stored. However in contrast to the support set
in SVMs, an example is added to the active set every
time the online algorithm makes a prediction mis-
take or when its confidence in a prediction is inad-
equately low. Therefore the active set can increase
in size significantly and thus the total computational
cost becomes proportional to the square of the num-
ber of training examples. Since the number of train-
ing examples is very large, the computational cost is
prohibitive even if we apply the kernel trick.
The calculation of the inner product between two
examples can be done by intersection of the acti-
vated features in each example. This is similar to
a merge sort and can be executed in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624?631,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
HPSG Parsing with Shallow Dependency Constraints
Kenji Sagae1 and Yusuke Miyao1 and Jun?ichi Tsujii1,2,3
1Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
2School of Computer Science, University of Manchester
3National Center for Text Mining
{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We present a novel framework that com-
bines strengths from surface syntactic pars-
ing and deep syntactic parsing to increase
deep parsing accuracy, specifically by com-
bining dependency and HPSG parsing. We
show that by using surface dependencies to
constrain the application of wide-coverage
HPSG rules, we can benefit from a num-
ber of parsing techniques designed for high-
accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. Our
framework results in a 1.4% absolute im-
provement over a state-of-the-art approach
for wide coverage HPSG parsing.
1 Introduction
Several efficient, accurate and robust approaches to
data-driven dependency parsing have been proposed
recently (Nivre and Scholz, 2004; McDonald et al,
2005; Buchholz and Marsi, 2006) for syntactic anal-
ysis of natural language using bilexical dependency
relations (Eisner, 1996). Much of the appeal of these
approaches is tied to the use of a simple formalism,
which allows for the use of efficient parsing algo-
rithms, as well as straightforward ways to train dis-
criminative models to perform disambiguation. At
the same time, there is growing interest in pars-
ing with more sophisticated lexicalized grammar
formalisms, such as Lexical Functional Grammar
(LFG) (Bresnan, 1982), Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Schabes et al, 1988), Head-
driven Phrase Structure Grammar (HPSG) (Pollard
and Sag, 1994) and Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000), which represent deep
syntactic structures that cannot be expressed in a
shallower formalism designed to represent only as-
pects of surface syntax, such as the dependency
formalism used in current mainstream dependency
parsing.
We present a novel framework that combines
strengths from surface syntactic parsing and deep
syntactic parsing, specifically by combining depen-
dency and HPSG parsing. We show that, by us-
ing surface dependencies to constrain the applica-
tion of wide-coverage HPSG rules, we can bene-
fit from a number of parsing techniques designed
for high-accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. From the
point of view of HPSG parsing, accuracy can be im-
proved significantly through the use of highly ac-
curate discriminative dependency models, without
the difficulties involved in adapting these models
to a more complex and linguistically sophisticated
formalism. In addition, improvements in depen-
dency parsing accuracy are converted directly into
improvements in HPSG parsing accuracy. From the
point of view of dependency parsing, the applica-
tion of HPSG rules to structures generated by a sur-
face dependency model provides a principled and
linguistically motivated way to identify deep syntac-
tic phenomena, such as long-distance dependencies,
raising and control.
We begin by describing our dependency and
HPSG parsing approaches in section 2. In section
3, we present our framework for HPSG parsing with
shallow dependency constraints, and in section 4 we
624
Figure 1: HPSG parsing
evaluate this framework empirically. Sections 5 and
6 discuss related work and conclusions.
2 Fast dependency parsing and
wide-coverage HPSG parsing
2.1 Data-driven dependency parsing
Because we use dependency parsing as a step in
deep parsing, it is important that we choose a pars-
ing approach that is not only accurate, but also effi-
cient. The deterministic shift/reduce classifier-based
dependency parsing approach (Nivre and Scholz,
2004) has been shown to offer state-of-the-art accu-
racy (Nivre et al, 2006) with high efficiency due to
a greedy search strategy. Our approach is based on
Nivre and Scholz?s approach, using support vector
machines for classification of shift/reduce actions.
2.2 Wide-coverage HPSG parsing
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemas explain general
construction rules, and a large number of lexical en-
tries express word-specific syntactic/semantic con-
straints. Figure 1 shows an example of the process
of HPSG parsing. First, lexical entries are assigned
to each word in a sentence. In Figure 1, lexical
entries express subcategorization frames and pred-
icate argument structures. Parsing proceeds by ap-
plying schemas to lexical entries. In this example,
the Head-Complement Schema is applied to the lex-
ical entries of ?tried? and ?running?. We then obtain
a phrasal structure for ?tried running?. By repeat-
edly applying schemas to lexical/phrasal structures,
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank
we finally obtain an HPSG parse tree that covers the
entire sentence.
In this paper, we use an HPSG parser developed
by Miyao and Tsujii (2005). This parser has a wide-
coverage HPSG lexicon which is extracted from the
Penn Treebank. Figure 2 illustrates their method
for extraction of HPSG lexical entries. First, given
a parse tree from the Penn Treebank (top), HPSG-
style constraints are added and an HPSG-style parse
tree is obtained (middle). Lexical entries are then ex-
tracted from the terminal nodes of the HPSG parse
tree (bottom). This way, in addition to a wide-
coverage lexicon, we also obtain an HPSG treebank,
which can be used as training data for disambigua-
tion models.
The disambiguation model of this parser is based
on a maximum entropy model (Berger et al, 1996).
The probability p(T |W ) of an HPSG parse tree T
for the sentence W = ?w1, . . . , wn? is given as:
p(T |W ) = p(T |L,W )p(L|W )
=
1
Z
exp
(
?
i
?ifi(T )
)
?
j
p(lj |W ),
where L = ?l1, . . . , ln? are lexical entries and
625
p(li|W ) is the supertagging probability, i.e., the
probability of assignining the lexical entry li to wi
(Ninomiya et al, 2006). The probability p(T |L,W )
is a maximum entropy model on HPSG parse trees,
where Z is a normalization factor, and feature func-
tions fi(T ) represent syntactic characteristics, such
as head words, lengths of phrases, and applied
schemas. Given the HPSG treebank as training data,
the model parameters ?i are estimated so as to maxi-
mize the log-likelihood of the training data (Malouf,
2002).
3 HPSG parsing with dependency
constraints
While a number of fairly straightforward models can
be applied successfully to dependency parsing, de-
signing and training HPSG parsing models has been
regarded as a significantly more complex task. Al-
though it seems intuitive that a more sophisticated
linguistic formalism should be more difficult to pa-
rameterize properly, we argue that the difference in
complexity between HPSG and dependency struc-
tures can be seen as incremental, and that the use
of accurate and efficient techniques to determine the
surface dependency structure of a sentence provides
valuable information that aids HPSG disambigua-
tion. This is largely because HPSG is based on a lex-
icalized grammar formalism, and as such its syntac-
tic structures have an underlying dependency back-
bone. However, HPSG syntactic structures includes
long-distance dependencies, and the underlying de-
pendency structure described by and HPSG structure
is a directed acyclic graph, not a dependency tree (as
used by mainstream approaches to data-driven de-
pendency parsing). This difference manifests itself
in words that have multiple heads. For example, in
the sentence I tried to run, the pronoun I is a depen-
dent of tried and of run. This makes it possible to
represent that I is the subject of both verbs, precisely
the kind of information that cannot be represented in
dependency parsing. If we ignore long-distance de-
pendencies, however, HPSG structures can be seen
as lexicalized trees that can be easily converted into
dependency trees.
Given that for an HPSG representation of the syn-
tactic structure of a sentence we can determine a
dependency tree by removing long-distance depen-
dencies, we can use dependency parsing techniques
(such as the deterministic dependency parsing ap-
proach mentioned in section 2.1) to determine the
underlying dependency trees in HPSG structures.
This is the basis for the parsing framework presented
here. In this approach, deep dependency analysis
is done in two stages. First, a dependency parser
determines the shallow dependency tree for the in-
put sentence. This shallow dependency tree corre-
sponds to the underlying dependency graph of the
HPSG structure for the input sentence, without de-
pendencies that roughly correspond to deep syntax.
The second step is to perform HPSG parsing, as
described in section 2.2, but using the shallow de-
pendency tree to constrain the application of HPSG
rules. We now discuss these two steps in more detail.
3.1 Determining shallow dependencies in
HPSG structures using dependency parsing
In order to apply a data-driven dependency ap-
proach to the task of identifying the shallow de-
pendency tree in HPSG structures, we first need a
corpus of such dependency trees to serve as train-
ing data. We created a dependency training corpus
based on the Penn Treebank (Marcus et al, 1993),
or more specifically on the HPSG Treebank gener-
ated from the Penn Treebank (see section 2.2). For
each HPSG structure in the HPSG Treebank, a de-
pendency tree is extracted in two steps. First, the
HPSG tree is converted into a CFG-style tree, sim-
ply by removing long-distance dependency links be-
tween nodes. A dependency tree is then extracted
from the resulting lexicalized CFG-style tree, as is
commonly done for converting constituent trees into
dependency trees after the application of a head-
percolation table (Collins, 1999).
Once a dependency training corpus is available,
it is used to train a dependency parser as described
in section 2.1. This is done by training a classifier
to determine parser actions based on local features
that represent the current state of the parser (Nivre
and Scholz, 2004; Sagae and Lavie, 2005). Train-
ing data for the classifier is obtained by applying the
parsing algorithm over the training sentences (for
which the correct dependency structures are known)
and recording the appropriate parser actions that re-
sult in the formation of the correct dependency trees,
coupled with the features that represent the state of
626
the parser mentioned in section 2.1. An evaluation
of the resulting dependency parser and its efficacy in
aiding HPSG parsing is presented in section 4.
3.2 Parsing with dependency constraints
Given a set of dependencies, the bottom-up process
of HPSG parsing can be constrained so that it does
not violate the given dependencies. This can be
achieved by a simple extension of the parsing algo-
rithm, as follows. During parsing, we store the lex-
ical head of each partial parse tree. In each schema
application, we can determine which child is the
head; for example, the left child is the head when
we apply the Head-Complement Schema. Given this
information and lexical heads, the parser can iden-
tify the dependency produced by this schema appli-
cation, and can therefore judge whether the schema
application violates the dependency constraints.
This method forces the HPSG parser to produce
parse trees that strictly conform to the output of
the dependency parser. However, this means that
the HPSG parser outputs no successful parse results
when it cannot find the parse tree that is completely
consistent with the given dependencies. This situ-
ation may occur when the dependency parser pro-
duces structures that are not covered in the HPSG
grammar. This is especially likely with a fully data-
driven dependency parser that uses local classifica-
tion, since its output may not be globally consistent
grammatically. In addition, the HPSG grammar is
extracted from the HPSG Treebank using a corpus-
based procedure, and it does not necessarily cover
all possible grammatical phenomena in unseen text
(Miyao and Tsujii, 2005).
We therefore propose an extension of this ap-
proach that uses predetermined dependencies as soft
constraints. Violations of schema applications are
detected in the same way as before, but instead of
strictly prohibiting schema applications, we penal-
ize the log-likelihood of partial parse trees created
by schema applications that violate the dependen-
cies constraints. Given a negative value ?, we add
? to the log-probability of a partial parse tree when
the schema application violates the dependency con-
straints. That is, when a parse tree violates n depen-
dencies, the log-probability of the parse tree is low-
ered by n?. The meta parameter ? is determined so
as to maximize the accuracy on the development set.
Soft dependency constraints can be implemented
as explained above as a straightforward extension of
the parsing algorithm. In addition, it is easily inte-
grated with beam thresholding methods of parsing.
Because beam thresholding discards partial parse
trees that have low log-probabilities, we can ex-
pect that the parser would discard partial parse trees
based on violation of the dependency constraints.
4 Experiments
We evaluate the accuracy of HPSG parsing with de-
pendency constraints on the HPSG Treebank (Miyao
et al, 2003), which is extracted from the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993)1. Sections 02-21 were used for training
(for HPSG and dependency parsers), section 22 was
used as development data, and final testing was per-
formed on section 23. Following previous work on
wide-coverage parsing with lexicalized grammars
using the Penn Treebank, we evaluate the parser by
measuring the accuracy of predicate-argument rela-
tions in the parser?s output. A predicate-argument
relation is defined as a tuple ??,wh, a, wa?, where
? is the predicate type (e.g. adjective, intransitive
verb), wh is the head word of the predicate, a is the
argument label (MODARG, ARG1, ... , ARG4), and
wa is the head word of the argument. Labeled pre-
cision (LP)/labeled recall (LR) is the ratio of tuples
correctly identified by the parser. These predicate-
argument relations cover the full range of syntactic
dependencies produced by the HPSG parser (includ-
ing, long-distance dependencies, raising and control,
in addition to surface dependencies).
In the experiments presented in this section, in-
put sentences were automatically tagged with parts-
of-speech with about 97% accuracy, using a max-
imum entropy POS tagger. We also report results
on parsing text with gold standard POS tags, where
explicitly noted. This provides an upper-bound on
what can be expected if a more sophisticated multi-
tagging scheme (James R. Curran and Vadas, 2006)
is used, instead of hard assignment of single tags in
a preprocessing step as done here.
1The extraction software can be obtained from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju.
627
4.1 Baseline
HPSG parsing results using the same HPSG gram-
mar and treebank have recently been reported by
Miyao and Tsujii (2005) and Ninomia et al (2006).
By running the HPSG parser described in section 2.2
on the development data without dependency con-
straints, we obtain similar values of LP (86.8%) and
LR (85.6%) as those reported by Miyao and Tsu-
jii (Miyao and Tsujii, 2005). Using the extremely
lexicalized framework of (Ninomiya et al, 2006) by
performing supertagging before parsing, we obtain
similar accuracy as Ninomiya et al (87.1% LP and
85.9% LR).
4.2 Dependency constraints and the penalty
parameter
Parsing the development data with hard dependency
constraints confirmed the intuition that these con-
straints often describe dependency structures that do
not conform to HPSG schema used in parsing, re-
sulting in parse failures. To determine the upper-
bound on HPSG parsing with hard dependency con-
straints, we set the HPSG parser to disallow the ap-
plication of any rules that result in the creation of
dependencies that violate gold standard dependen-
cies. This results in high precision (96.7%), but re-
call is low (82.3%) due to parse failures caused by
lack of grammatical coverage 2. Using dependen-
cies produced by the shift-reduce SVM parser, we
obtain 91.5% LP and 65.7% LR. This represents a
large gain in precision over the baseline, but an even
greater loss in recall, which limits the usefulness of
the parser, and severely hurts the appeal of hard con-
straints.
We focus the rest of our experiments on parsing
with soft dependency constraints. As explained in
section 3, this involves setting the penalty parame-
ter ?. During parsing, we subtract ? from the log-
probability of applying any schema that violates the
dependency constraints given to the HPSG parser.
Figure 3 illustrates the effect of ? when gold stan-
dard dependencies (and gold standard POS tags) are
used. We note that setting ? = 0 causes the parser
2Although the HPSG grammar does not have perfect cov-
erage of unseen text, it supports complete and mostly correct
analyses for all sentences in the development set. However,
when we require completely correct analyses by using hard con-
straints, lack of coverage may cause parse failures.
8990919293949596 0
5
10
15
20
25
30
35
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 3: The effect of ? on HPSG parsing con-
strained by gold standard dependencies.
to ignore dependency constraints, providing base-
line performance. Conversely, setting a high enough
value (? = 30 is sufficient, in practice) causes any
substructures that violate the dependency constraints
to be used only when they are absolutely neces-
sary to produce a valid parse for the input sentence.
In figure 3, this corresponds to an upper-bound on
the accuracy of parsing with soft dependency con-
straints (94.7% f-score), since gold standard depen-
dencies are used.
We set ? empirically with simple hill climbing on
the development set. Because it is expected that the
optimal value of ? depends on the accuracy of the
surface dependency parser, we set separate values
for parsing with a POS tagger or with gold standard
POS tags. Figure 4 shows the accuracy of HPSG
predicate-argument relations obtained with depen-
dency constraints determined by dependency pars-
ing with gold standard POS tags. With both au-
tomatically assigned and gold standard POS tags,
we observe an improvement of about 0.6% in pre-
cision, recall and f-score, when the optimal ? value
is used in each case. While this corresponds to a rel-
ative error reduction of over 6% (or 12%, if we con-
sider the upper-bound dictated by imperfect gram-
matical coverage), a more interesting aspect of this
framework is that it allows techniques designed for
improving dependency accuracy to improve HPSG
parsing accuracy directly, as we illustrate next.
628
89.489.689.89090.290.490.690.891 0
0.5
1
1.5
2
2.5
3
3.5
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 4: The effect of ? on HPSG parsing con-
strained by the output of a dependency parser using
gold standard POS tags.
4.3 Determining constraints with dependency
parser combination
Parser combination has been shown to be a power-
ful way to obtain very high accuracy in dependency
parsing (Sagae and Lavie, 2006). Using dependency
constraints allows us to improve HPSG parsing ac-
curacy simply by using an existing parser combina-
tion approach. As a first step, we train two addi-
tional parsers with the dependencies extracted from
the HPSG Treebank. The first uses the same shift-
reduce framework described in section 2.1, but it
process the input from right to left (RL). This has
been found to work well in previous work on depen-
dency parser combination (Zeman and Z?abokrtsky?,
2005; Sagae and Lavie, 2006). The second parser
is MSTParser, the large-margin maximum spanning
tree parser described in (McDonald et al, 2005)3.
We examine the use of two combination schemes:
one using two parsers, and one using three parsers.
The first combination approach is to keep only de-
pendencies for which there is agreement between the
two parsers. In other words, dependencies that are
proposed by one parser but not the other are simply
discarded. Using the left-to-right shift-reduce parser
and MSTParser, we find that this results in very high
precision of surface dependencies on the develop-
ment data. In the second approach, combination of
3Downloaded from http://sourceforge.net/projects/mstparser
the three dependency parsers is done according to
the maximum spanning tree combination scheme of
Sagae and Lavie (2006), which results in high accu-
racy of surface dependencies. For each of the com-
bination approaches, we use the resulting dependen-
cies as constraints for HPSG parsing, determining
the optimal value of ? on the development set in
the same way as done for a single parser. Table 1
summarizes our experiments on development data
using parser combinations to produce dependency
constraints 4. The two combination approaches are
denoted as C1 and C2.
Parser Dep ? HPSG Diff
none (baseline) ? ? 86.5 ?
LR shift-reduce 91.2 1.5 87.1 0.6
RL shift-reduce 90.1 ? ?
MSTParser 91.0 ? ?
C1 (agreement) 96.8* 2.5 87.4 0.9
C2 (MST) 92.4 2.5 87.4 0.9
Table 1: Summary of results on development data.
* The shallow accuracy of combination C1 corre-
sponds to the dependency precision (no dependen-
cies were reported for 8% of all words in the devel-
opment set).
4.4 Results
Having determined ? values on development data
for the shift-reduce dependency parser, the two-
parser agreement combination, and the three-parser
maximum spanning tree combination, we parse the
test data (section 23) using these three different
sources of dependency constraints for HPSG pars-
ing. Our final results are shown in table 2, where
we also include the results published in (Ninomiya
et al, 2006) for comparison purposes, and the result
of using dependency constraints obtained with gold
standard POS tags.
By using two unlabeled dependency parsers to
provide soft dependency constraints, we obtain a
1% absolute improvement in precision and recall of
predicate-argument identification in HPSG parsing
over a strong baseline. Our baseline approach out-
performed previously published results on this test
4The accuracy figures for the dependency parsers is ex-
pressed as unlabeled accuracy of the surface dependencies only,
and are not comparable to the HPSG parsing accuracy figures
629
Parser LP LR F-score
HPSG Baseline 87.4 87.0 87.2
Shift-Reduce + HPSG 88.2 87.7 87.9
C1 + HPSG 88.5 88.0 88.2
C2 + HPSG 88.4 87.9 88.1
Baseline(gold) 89.8 89.4 89.6
Shift-Reduce(gold) 90.62 90.23 90.42
C1+HPSG(gold) 90.9 90.4 90.6
C2+HPSG(gold) 90.8 90.4 90.6
Miyao and Tsujii, 2005 85.0 84.3 84.6
Ninomiya et al, 2006 87.4 86.3 86.8
Table 2: Final results on test set. The first set of
results show our HPSG baseline and HPSG with soft
dependency constraints using three different sources
of dependency constraints. The second set of results
show the accuracy of the same parsers when gold
part-of-speech tags are used. The third set of results
is from existing published models on the same data.
set, and our best performing combination scheme
obtains an absolute improvement of 1.4% over the
best previously published results using the HPSG
Treebank. It is interesting to note that the results ob-
tained with dependency parser combinations C1 and
C2 were very similar, even though in C1 only two
parsers were used, and constraints were provided for
about 92% of shallow dependencies (with accuracy
higher than 96%). Clearly, precision is crucial in de-
pendency constraints.
Finally, although it is necessary to perform de-
pendency parsing to pre-compute dependency con-
straints, the total time required to perform the en-
tire process of HPSG parsing with dependency con-
straints is close to that of the baseline HPSG ap-
proach. This is due to two reasons: (1) the de-
pendency parsing approaches used to pre-compute
constraints are several times faster than the baseline
HPSG approach, and (2) the HPSG portion of the
process is significantly faster when dependency con-
straints are used, since the constraints help sharpen
the search space, making search more efficient. Us-
ing the baseline HPSG approach, it takes approx-
imately 25 minutes to parse the test set. The to-
tal time required to parse the test set using HPSG
with dependency constraints generated by the shift-
reduce parser is 27 minutes. With combination C1,
parsing time increases to 30 minutes, since two de-
pendency parsers are used sequentially.
5 Related work
There are other approaches that combine shallow
processing with deep parsing (Crysmann et al,
2002; Frank et al, 2003; Daum et al, 2003) to im-
prove parsing efficiency. Typically, shallow parsing
is used to create robust minimal recursion seman-
tics, which are used as constraints to limit ambigu-
ity during parsing. Our approach, in contrast, uses
syntactic dependencies to achieve a significant im-
provement in the accuracy of wide-coverage HPSG
parsing. Additionally, our approach is in many
ways similar to supertagging (Bangalore and Joshi,
1999), which uses sequence labeling techniques as
an efficient way to pre-compute parsing constraints
(specifically, the assignment of lexical entries to in-
put words).
6 Conclusion
We have presented a novel framework for taking ad-
vantage of the strengths of a shallow parsing ap-
proach and a deep parsing approach. We have
shown that by constraining the application of rules
in HPSG parsing according to results from a depen-
dency parser, we can significantly improve the ac-
curacy of deep parsing by using shallow syntactic
analyses.
To illustrate how this framework allows for im-
provements in the accuracy of dependency parsing
to be used directly to improve the accuracy of HPSG
parsing, we showed that by combining the results of
different dependency parsers using the search-based
parsing ensemble approach of (Sagae and Lavie,
2006), we obtain improved HPSG parsing accuracy
as a result of the improved dependency accuracy.
Although we have focused on the use of HPSG
and dependency parsing, the general framework pre-
sented here can be applied to other lexicalized gram-
mar formalisms, such as LTAG, CCG and LFG.
Acknowledgements
This research was partially supported by Grant-in-
Aid for Specially Promoted Research 18002007.
630
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Joan Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Natural Language
Learning. New York, NY.
M. Collins. 1999. Head-Driven Models for Natural Lan-
guage Parsing. Phd thesis, University of Pennsylva-
nia.
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan
Mueller, Guenter Neumann, Jakub Piskorski, Ulrich
Schaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu,
Markus Becker, and Hans-Ulrich Krieger. 2002. An
integrated architecture for shallow and deep process-
ing. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Michael Daum, Kilian A. Foth, and Wolfgang Menzel.
2003. Constraint-based integration of deep and shal-
low parsing techniques. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2003).
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING?96). Copenhagen, Denmark.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2003), pages
104?111.
Stephen Clark James R. Curran and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In
Proceedings of COLING/ACL 2006. Sydney, Aus-
tralia.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 2002 Conference on Natural Language
Learning.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Ryan McDonald, Fernando Pereira, K. Ribarov, and
J. Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Process-
ing (HLT-EMNLP). Vancouver, Canada.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 42nd Meeting of the Associ-
ation for Computational Linguistics. Ann Arbor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Corpus oriented grammar development for
aquiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of the Tenth Con-
ference on Natural Language Learning.
T. Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, and
J. Tsujii. 2006. Extremely lexicalized models for ac-
curate and fast hpsg parsing. In Proceedings of the
2006 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2006).
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 64?70. Geneva, Switzerland.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Natural Language Learning.
New York, NY.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the 2006 Meeting of
the North American ACL. New York, NY.
Yves Schabes, Anne Abeille, and Aravind Joshi. 1988.
Parsing strategies with lexicalized grammars: Appli-
cation to tree adjoining grammars. In Proceedings of
12th COLING.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Zeman and Zdenek Z?abokrtsky?. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In Proceedings of the International Workshop
on Parsing Technologies. Vancouver, Canada.
631
Proceedings of ACL-08: HLT, pages 46?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Task-oriented Evaluation of Syntactic Parsers and Their Representations
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a comparative evalua-
tion of several state-of-the-art English parsers
based on different frameworks. Our approach
is to measure the impact of each parser when it
is used as a component of an information ex-
traction system that performs protein-protein
interaction (PPI) identification in biomedical
papers. We evaluate eight parsers (based on
dependency parsing, phrase structure parsing,
or deep parsing) using five different parse rep-
resentations. We run a PPI system with several
combinations of parser and parse representa-
tion, and examine their impact on PPI identi-
fication accuracy. Our experiments show that
the levels of accuracy obtained with these dif-
ferent parsers are similar, but that accuracy
improvements vary when the parsers are re-
trained with domain-specific data.
1 Introduction
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame-
works (Charniak, 2000; Klein and Manning, 2003;
Charniak and Johnson, 2005; Petrov and Klein,
2007), but also include dependency parsers (Mc-
Donald and Pereira, 2006; Nivre and Nilsson, 2005;
Sagae and Tsujii, 2007) and deep parsers (Kaplan
et al, 2004; Clark and Curran, 2004; Miyao and
Tsujii, 2008). However, efforts to perform extensive
comparisons of syntactic parsers based on different
frameworks have been limited. The most popular
method for parser comparison involves the direct
measurement of the parser output accuracy in terms
of metrics such as bracketing precision and recall, or
dependency accuracy. This assumes the existence of
a gold-standard test corpus, such as the Penn Tree-
bank (Marcus et al, 1994). It is difficult to apply
this method to compare parsers based on different
frameworks, because parse representations are often
framework-specific and differ from parser to parser
(Ringger et al, 2004). The lack of such comparisons
is a serious obstacle for NLP researchers in choosing
an appropriate parser for their purposes.
In this paper, we present a comparative evalua-
tion of syntactic parsers and their output represen-
tations based on different frameworks: dependency
parsing, phrase structure parsing, and deep pars-
ing. Our approach to parser evaluation is to mea-
sure accuracy improvement in the task of identify-
ing protein-protein interaction (PPI) information in
biomedical papers, by incorporating the output of
different parsers as statistical features in a machine
learning classifier (Yakushiji et al, 2005; Katrenko
and Adriaans, 2006; Erkan et al, 2007; S?tre et al,
2007). PPI identification is a reasonable task for
parser evaluation, because it is a typical information
extraction (IE) application, and because recent stud-
ies have shown the effectiveness of syntactic parsing
in this task. Since our evaluation method is applica-
ble to any parser output, and is grounded in a real
application, it allows for a fair comparison of syn-
tactic parsers based on different frameworks.
Parser evaluation in PPI extraction also illu-
minates domain portability. Most state-of-the-art
parsers for English were trained with the Wall Street
Journal (WSJ) portion of the Penn Treebank, and
high accuracy has been reported for WSJ text; how-
ever, these parsers rely on lexical information to at-
tain high accuracy, and it has been criticized that
these parsers may overfit to WSJ text (Gildea, 2001;
46
Klein and Manning, 2003). Another issue for dis-
cussion is the portability of training methods. When
training data in the target domain is available, as
is the case with the GENIA Treebank (Kim et al,
2003) for biomedical papers, a parser can be re-
trained to adapt to the target domain, and larger ac-
curacy improvements are expected, if the training
method is sufficiently general. We will examine
these two aspects of domain portability by compar-
ing the original parsers with the retrained parsers.
2 Syntactic Parsers and Their
Representations
This paper focuses on eight representative parsers
that are classified into three parsing frameworks:
dependency parsing, phrase structure parsing, and
deep parsing. In general, our evaluation methodol-
ogy can be applied to English parsers based on any
framework; however, in this paper, we chose parsers
that were originally developed and trained with the
Penn Treebank or its variants, since such parsers can
be re-trained with GENIA, thus allowing for us to
investigate the effect of domain adaptation.
2.1 Dependency parsing
Because the shared tasks of CoNLL-2006 and
CoNLL-2007 focused on data-driven dependency
parsing, it has recently been extensively studied in
parsing research. The aim of dependency pars-
ing is to compute a tree structure of a sentence
where nodes are words, and edges represent the re-
lations among words. Figure 1 shows a dependency
tree for the sentence ?IL-8 recognizes and activates
CXCR1.? An advantage of dependency parsing is
that dependency trees are a reasonable approxima-
tion of the semantics of sentences, and are readily
usable in NLP applications. Furthermore, the effi-
ciency of popular approaches to dependency pars-
ing compare favorable with those of phrase struc-
ture parsing or deep parsing. While a number of ap-
proaches have been proposed for dependency pars-
ing, this paper focuses on two typical methods.
MST McDonald and Pereira (2006)?s dependency
parser,1 based on the Eisner algorithm for projective
dependency parsing (Eisner, 1996) with the second-
order factorization.
1http://sourceforge.net/projects/mstparser
Figure 1: CoNLL-X dependency tree
Figure 2: Penn Treebank-style phrase structure tree
KSDEP Sagae and Tsujii (2007)?s dependency
parser,2 based on a probabilistic shift-reduce al-
gorithm extended by the pseudo-projective parsing
technique (Nivre and Nilsson, 2005).
2.2 Phrase structure parsing
Owing largely to the Penn Treebank, the mainstream
of data-driven parsing research has been dedicated
to the phrase structure parsing. These parsers output
Penn Treebank-style phrase structure trees, although
function tags and empty categories are stripped off
(Figure 2). While most of the state-of-the-art parsers
are based on probabilistic CFGs, the parameteriza-
tion of the probabilistic model of each parser varies.
In this work, we chose the following four parsers.
NO-RERANK Charniak (2000)?s parser, based on a
lexicalized PCFG model of phrase structure trees.3
The probabilities of CFG rules are parameterized on
carefully hand-tuned extensive information such as
lexical heads and symbols of ancestor/sibling nodes.
RERANK Charniak and Johnson (2005)?s rerank-
ing parser. The reranker of this parser receives n-
best4 parse results from NO-RERANK, and selects
the most likely result by using a maximum entropy
model with manually engineered features.
BERKELEY Berkeley?s parser (Petrov and Klein,
2007).5 The parameterization of this parser is op-
2http://www.cs.cmu.edu/?sagae/parser/
3http://bllip.cs.brown.edu/resources.shtml
4We set n = 50 in this paper.
5http://nlp.cs.berkeley.edu/Main.html#Parsing
47
Figure 3: Predicate argument structure
timized automatically by assigning latent variables
to each nonterminal node and estimating the param-
eters of the latent variables by the EM algorithm
(Matsuzaki et al, 2005).
STANFORD Stanford?s unlexicalized parser (Klein
and Manning, 2003).6 Unlike NO-RERANK, proba-
bilities are not parameterized on lexical heads.
2.3 Deep parsing
Recent research developments have allowed for ef-
ficient and robust deep parsing of real-world texts
(Kaplan et al, 2004; Clark and Curran, 2004; Miyao
and Tsujii, 2008). While deep parsers compute
theory-specific syntactic/semantic structures, pred-
icate argument structures (PAS) are often used in
parser evaluation and applications. PAS is a graph
structure that represents syntactic/semantic relations
among words (Figure 3). The concept is therefore
similar to CoNLL dependencies, though PAS ex-
presses deeper relations, and may include reentrant
structures. In this work, we chose the two versions
of the Enju parser (Miyao and Tsujii, 2008).
ENJU The HPSG parser that consists of an HPSG
grammar extracted from the Penn Treebank, and
a maximum entropy model trained with an HPSG
treebank derived from the Penn Treebank.7
ENJU-GENIA The HPSG parser adapted to
biomedical texts, by the method of Hara et al
(2007). Because this parser is trained with both
WSJ and GENIA, we compare it parsers that are
retrained with GENIA (see section 3.3).
3 Evaluation Methodology
In our approach to parser evaluation, we measure
the accuracy of a PPI extraction system, in which
6http://nlp.stanford.edu/software/lex-parser.
shtml
7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
This study demonstrates that IL-8 recognizes and
activates CXCR1, CXCR2, and the Duffy antigen
by distinct mechanisms.
The molar ratio of serum retinol-binding protein
(RBP) to transthyretin (TTR) is not useful to as-
sess vitamin A status during infection in hospi-
talised children.
Figure 4: Sentences including protein names
ENTITY1(IL-8) SBJ?? recognizes OBJ?? ENTITY2(CXCR1)
Figure 5: Dependency path
the parser output is embedded as statistical features
of a machine learning classifier. We run a classi-
fier with features of every possible combination of a
parser and a parse representation, by applying con-
versions between representations when necessary.
We also measure the accuracy improvements ob-
tained by parser retraining with GENIA, to examine
the domain portability, and to evaluate the effective-
ness of domain adaptation.
3.1 PPI extraction
PPI extraction is an NLP task to identify protein
pairs that are mentioned as interacting in biomedical
papers. Because the number of biomedical papers is
growing rapidly, it is impossible for biomedical re-
searchers to read all papers relevant to their research;
thus, there is an emerging need for reliable IE tech-
nologies, such as PPI identification.
Figure 4 shows two sentences that include pro-
tein names: the former sentence mentions a protein
interaction, while the latter does not. Given a pro-
tein pair, PPI extraction is a task of binary classi-
fication; for example, ?IL-8, CXCR1? is a positive
example, and ?RBP, TTR? is a negative example.
Recent studies on PPI extraction demonstrated that
dependency relations between target proteins are ef-
fective features for machine learning classifiers (Ka-
trenko and Adriaans, 2006; Erkan et al, 2007; S?tre
et al, 2007). For the protein pair IL-8 and CXCR1
in Figure 4, a dependency parser outputs a depen-
dency tree shown in Figure 1. From this dependency
tree, we can extract a dependency path shown in Fig-
ure 5, which appears to be a strong clue in knowing
that these proteins are mentioned as interacting.
48
(dep_path (SBJ (ENTITY1 recognizes))
(rOBJ (recognizes ENTITY2)))
Figure 6: Tree representation of a dependency path
We follow the PPI extraction method of S?tre et
al. (2007), which is based on SVMs with SubSet
Tree Kernels (Collins and Duffy, 2002; Moschitti,
2006), while using different parsers and parse rep-
resentations. Two types of features are incorporated
in the classifier. The first is bag-of-words features,
which are regarded as a strong baseline for IE sys-
tems. Lemmas of words before, between and after
the pair of target proteins are included, and the linear
kernel is used for these features. These features are
commonly included in all of the models. Filtering
by a stop-word list is not applied because this setting
made the scores higher than S?tre et al (2007)?s set-
ting. The other type of feature is syntactic features.
For dependency-based parse representations, a de-
pendency path is encoded as a flat tree as depicted in
Figure 6 (prefix ?r? denotes reverse relations). Be-
cause a tree kernel measures the similarity of trees
by counting common subtrees, it is expected that the
system finds effective subsequences of dependency
paths. For the PTB representation, we directly en-
code phrase structure trees.
3.2 Conversion of parse representations
It is widely believed that the choice of representa-
tion format for parser output may greatly affect the
performance of applications, although this has not
been extensively investigated. We should therefore
evaluate the parser performance in multiple parse
representations. In this paper, we create multiple
parse representations by converting each parser?s de-
fault output into other representations when possi-
ble. This experiment can also be considered to be
a comparative evaluation of parse representations,
thus providing an indication for selecting an appro-
priate parse representation for similar IE tasks.
Figure 7 shows our scheme for representation
conversion. This paper focuses on five representa-
tions as described below.
CoNLL The dependency tree format used in the
2006 and 2007 CoNLL shared tasks on dependency
parsing. This is a representation format supported by
several data-driven dependency parsers. This repre-
Figure 7: Conversion of parse representations
Figure 8: Head dependencies
sentation is also obtained from Penn Treebank-style
trees by applying constituent-to-dependency conver-
sion8 (Johansson and Nugues, 2007). It should be
noted, however, that this conversion cannot work
perfectly with automatic parsing, because the con-
version program relies on function tags and empty
categories of the original Penn Treebank.
PTB Penn Treebank-style phrase structure trees
without function tags and empty nodes. This is the
default output format for phrase structure parsers.
We also create this representation by converting
ENJU?s output by tree structure matching, although
this conversion is not perfect because forms of PTB
and ENJU?s output are not necessarily compatible.
HD Dependency trees of syntactic heads (Fig-
ure 8). This representation is obtained by convert-
ing PTB trees. We first determine lexical heads of
nonterminal nodes by using Bikel?s implementation
of Collins? head detection algorithm9 (Bikel, 2004;
Collins, 1997). We then convert lexicalized trees
into dependencies between lexical heads.
SD The Stanford dependency format (Figure 9).
This format was originally proposed for extracting
dependency relations useful for practical applica-
tions (de Marneffe et al, 2006). A program to con-
vert PTB is attached to the Stanford parser. Although
the concept looks similar to CoNLL, this representa-
8http://nlp.cs.lth.se/pennconverter/
9http://www.cis.upenn.edu/?dbikel/software.
html
49
Figure 9: Stanford dependencies
tion does not necessarily form a tree structure, and is
designed to express more fine-grained relations such
as apposition. Research groups for biomedical NLP
recently adopted this representation for corpus anno-
tation (Pyysalo et al, 2007a) and parser evaluation
(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).
PAS Predicate-argument structures. This is the de-
fault output format for ENJU and ENJU-GENIA.
Although only CoNLL is available for depen-
dency parsers, we can create four representations for
the phrase structure parsers, and five for the deep
parsers. Dotted arrows in Figure 7 indicate imper-
fect conversion, in which the conversion inherently
introduces errors, and may decrease the accuracy.
We should therefore take caution when comparing
the results obtained by imperfect conversion. We
also measure the accuracy obtained by the ensem-
ble of two parsers/representations. This experiment
indicates the differences and overlaps of information
conveyed by a parser or a parse representation.
3.3 Domain portability and parser retraining
Since the domain of our target text is different from
WSJ, our experiments also highlight the domain
portability of parsers. We run two versions of each
parser in order to investigate the two types of domain
portability. First, we run the original parsers trained
with WSJ10 (39832 sentences). The results in this
setting indicate the domain portability of the original
parsers. Next, we run parsers re-trained with GE-
NIA11 (8127 sentences), which is a Penn Treebank-
style treebank of biomedical paper abstracts. Accu-
racy improvements in this setting indicate the pos-
sibility of domain adaptation, and the portability of
the training methods of the parsers. Since the parsers
listed in Section 2 have programs for the training
10Some of the parser packages include parsing models
trained with extended data, but we used the models trained with
WSJ section 2-21 of the Penn Treebank.
11The domains of GENIA and AImed are not exactly the
same, because they are collected independently.
with a Penn Treebank-style treebank, we use those
programs as-is. Default parameter settings are used
for this parser re-training.
In preliminary experiments, we found that de-
pendency parsers attain higher dependency accuracy
when trained only with GENIA. We therefore only
input GENIA as the training data for the retraining
of dependency parsers. For the other parsers, we in-
put the concatenation of WSJ and GENIA for the
retraining, while the reranker of RERANK was not re-
trained due to its cost. Since the parsers other than
NO-RERANK and RERANK require an external POS
tagger, a WSJ-trained POS tagger is used with WSJ-
trained parsers, and geniatagger (Tsuruoka et al,
2005) is used with GENIA-retrained parsers.
4 Experiments
4.1 Experiment settings
In the following experiments, we used AImed
(Bunescu and Mooney, 2004), which is a popular
corpus for the evaluation of PPI extraction systems.
The corpus consists of 225 biomedical paper ab-
stracts (1970 sentences), which are sentence-split,
tokenized, and annotated with proteins and PPIs.
We use gold protein annotations given in the cor-
pus. Multi-word protein names are concatenated
and treated as single words. The accuracy is mea-
sured by abstract-wise 10-fold cross validation and
the one-answer-per-occurrence criterion (Giuliano
et al, 2006). A threshold for SVMs is moved to
adjust the balance of precision and recall, and the
maximum f-scores are reported for each setting.
4.2 Comparison of accuracy improvements
Tables 1 and 2 show the accuracy obtained by using
the output of each parser in each parse representa-
tion. The row ?baseline? indicates the accuracy ob-
tained with bag-of-words features. Table 3 shows
the time for parsing the entire AImed corpus, and
Table 4 shows the time required for 10-fold cross
validation with GENIA-retrained parsers.
When using the original WSJ-trained parsers (Ta-
ble 1), all parsers achieved almost the same level
of accuracy ? a significantly better result than the
baseline. To the extent of our knowledge, this is
the first result that proves that dependency parsing,
phrase structure parsing, and deep parsing perform
50
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 53.2/56.5/54.6 N/A N/A N/A N/A
KSDEP 49.3/63.0/55.2 N/A N/A N/A N/A
NO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/A
RERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/A
BERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/A
STANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/A
ENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3
Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 49.1/65.6/55.9 N/A N/A N/A N/A
KSDEP 51.6/67.5/58.3 N/A N/A N/A N/A
NO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/A
RERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/A
BERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/A
STANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/A
ENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2
ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4
Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)
WSJ-trained GENIA-retrained
MST 613 425
KSDEP 136 111
NO-RERANK 2049 1372
RERANK 2806 2125
BERKELEY 1118 1198
STANFORD 1411 1645
ENJU 1447 727
ENJU-GENIA 821
Table 3: Parsing time (sec.)
equally well in a real application. Among these
parsers, RERANK performed slightly better than the
other parsers, although the difference in the f-score
is small, while it requires much higher parsing cost.
When the parsers are retrained with GENIA (Ta-
ble 2), the accuracy increases significantly, demon-
strating that the WSJ-trained parsers are not suffi-
ciently domain-independent, and that domain adap-
tation is effective. It is an important observation that
the improvements by domain adaptation are larger
than the differences among the parsers in the pre-
vious experiment. Nevertheless, not all parsers had
their performance improved upon retraining. Parser
CoNLL PTB HD SD PAS
baseline 424
MST 809 N/A N/A N/A N/A
KSDEP 864 N/A N/A N/A N/A
NO-RERANK 851 4772 882 795 N/A
RERANK 849 4676 881 778 N/A
BERKELEY 869 4665 895 804 N/A
STANFORD 847 4614 886 799 N/A
ENJU 832 4611 884 789 1005
ENJU-GENIA 874 4624 895 783 1020
Table 4: Evaluation time (sec.)
retraining yielded only slight improvements for
RERANK, BERKELEY, and STANFORD, while larger
improvements were observed for MST, KSDEP, NO-
RERANK, and ENJU. Such results indicate the dif-
ferences in the portability of training methods. A
large improvement from ENJU to ENJU-GENIA shows
the effectiveness of the specifically designed do-
main adaptation method, suggesting that the other
parsers might also benefit from more sophisticated
approaches for domain adaptation.
While the accuracy level of PPI extraction is
the similar for the different parsers, parsing speed
51
RERANK ENJU
CoNLL HD SD CoNLL HD SD PAS
KSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)
RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)
HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)
SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)
ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)
HD 57.1 (+0.5) 58.1 (+0.9)
SD 58.3 (+1.1)
Table 5: Results of parser/representation ensemble (f-score)
differs significantly. The dependency parsers are
much faster than the other parsers, while the phrase
structure parsers are relatively slower, and the deep
parsers are in between. It is noteworthy that the
dependency parsers achieved comparable accuracy
with the other parsers, while they are more efficient.
The experimental results also demonstrate that
PTB is significantly worse than the other represen-
tations with respect to cost for training/testing and
contributions to accuracy improvements. The con-
version from PTB to dependency-based representa-
tions is therefore desirable for this task, although it
is possible that better results might be obtained with
PTB if a different feature extraction mechanism is
used. Dependency-based representations are com-
petitive, while CoNLL seems superior to HD and SD
in spite of the imperfect conversion from PTB to
CoNLL. This might be a reason for the high per-
formances of the dependency parsers that directly
compute CoNLL dependencies. The results for ENJU-
CoNLL and ENJU-PAS show that PAS contributes to a
larger accuracy improvement, although this does not
necessarily mean the superiority of PAS, because two
imperfect conversions, i.e., PAS-to-PTB and PTB-to-
CoNLL, are applied for creating CoNLL.
4.3 Parser ensemble results
Table 5 shows the accuracy obtained with ensembles
of two parsers/representations (except the PTB for-
mat). Bracketed figures denote improvements from
the accuracy with a single parser/representation.
The results show that the task accuracy significantly
improves by parser/representation ensemble. Inter-
estingly, the accuracy improvements are observed
even for ensembles of different representations from
the same parser. This indicates that a single parse
representation is insufficient for expressing the true
Bag-of-words features 48.2/54.9/51.1
Yakushiji et al (2005) 33.7/33.1/33.4
Mitsumori et al (2006) 54.2/42.6/47.7
Giuliano et al (2006) 60.9/57.2/59.0
S?tre et al (2007) 64.3/44.1/52.0
This paper 54.9/65.5/59.5
Table 6: Comparison with previous results on PPI extrac-
tion (precision/recall/f-score)
potential of a parser. Effectiveness of the parser en-
semble is also attested by the fact that it resulted in
larger improvements. Further investigation of the
sources of these improvements will illustrate the ad-
vantages and disadvantages of these parsers and rep-
resentations, leading us to better parsing models and
a better design for parse representations.
4.4 Comparison with previous results on PPI
extraction
PPI extraction experiments on AImed have been re-
ported repeatedly, although the figures cannot be
compared directly because of the differences in data
preprocessing and the number of target protein pairs
(S?tre et al, 2007). Table 6 compares our best re-
sult with previously reported accuracy figures. Giu-
liano et al (2006) and Mitsumori et al (2006) do
not rely on syntactic parsing, while the former ap-
plied SVMs with kernels on surface strings and the
latter is similar to our baseline method. Bunescu and
Mooney (2005) applied SVMs with subsequence
kernels to the same task, although they provided
only a precision-recall graph, and its f-score is
around 50. Since we did not run experiments on
protein-pair-wise cross validation, our system can-
not be compared directly to the results reported
by Erkan et al (2007) and Katrenko and Adriaans
52
(2006), while S?tre et al (2007) presented better re-
sults than theirs in the same evaluation criterion.
5 Related Work
Though the evaluation of syntactic parsers has been
a major concern in the parsing community, and a
couple of works have recently presented the com-
parison of parsers based on different frameworks,
their methods were based on the comparison of the
parsing accuracy in terms of a certain intermediate
parse representation (Ringger et al, 2004; Kaplan
et al, 2004; Briscoe and Carroll, 2006; Clark and
Curran, 2007; Miyao et al, 2007; Clegg and Shep-
herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,
2007a; Sagae et al, 2008). Such evaluation requires
gold standard data in an intermediate representation.
However, it has been argued that the conversion of
parsing results into an intermediate representation is
difficult and far from perfect.
The relationship between parsing accuracy and
task accuracy has been obscure for many years.
Quirk and Corston-Oliver (2006) investigated the
impact of parsing accuracy on statistical MT. How-
ever, this work was only concerned with a single de-
pendency parser, and did not focus on parsers based
on different frameworks.
6 Conclusion and Future Work
We have presented our attempts to evaluate syntac-
tic parsers and their representations that are based on
different frameworks; dependency parsing, phrase
structure parsing, or deep parsing. The basic idea
is to measure the accuracy improvements of the
PPI extraction task by incorporating the parser out-
put as statistical features of a machine learning
classifier. Experiments showed that state-of-the-
art parsers attain accuracy levels that are on par
with each other, while parsing speed differs sig-
nificantly. We also found that accuracy improve-
ments vary when parsers are retrained with domain-
specific data, indicating the importance of domain
adaptation and the differences in the portability of
parser training methods.
Although we restricted ourselves to parsers
trainable with Penn Treebank-style treebanks, our
methodology can be applied to any English parsers.
Candidates include RASP (Briscoe and Carroll,
2006), the C&C parser (Clark and Curran, 2004),
the XLE parser (Kaplan et al, 2004), MINIPAR
(Lin, 1998), and Link Parser (Sleator and Temperley,
1993; Pyysalo et al, 2006), but the domain adapta-
tion of these parsers is not straightforward. It is also
possible to evaluate unsupervised parsers, which is
attractive since evaluation of such parsers with gold-
standard data is extremely problematic.
A major drawback of our methodology is that
the evaluation is indirect and the results depend
on a selected task and its settings. This indicates
that different results might be obtained with other
tasks. Hence, we cannot conclude the superiority of
parsers/representations only with our results. In or-
der to obtain general ideas on parser performance,
experiments on other tasks are indispensable.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Grant-in-Aid for Young Scientists (MEXT, Japan).
References
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
T. Briscoe and J. Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In COLING/ACL 2006 Poster Session.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004, pages 439?446.
R. C. Bunescu and R. J. Mooney. 2005. Subsequence
kernels for relation extraction. In NIPS 2005.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
ACL 2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000, pages 132?139.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In 42nd ACL.
S. Clark and J. R. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In ACL
2007.
A. B. Clegg and A. J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC Bioinformatics,
8:24.
53
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th ACL.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interac-
tion sentences using dependency parsing. In EMNLP
2007.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP 2001, pages 167?202.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In EACL 2006.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating im-
pact of re-training a lexical disambiguation model on
domain adaptation of an HPSG parser. In IWPT 2007.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, and
A. Vasserman. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In HLT/NAACL?04.
S. Katrenko and P. Adriaans. 2006. Learning relations
from biomedical corpora using dependency trees. In
KDECB, pages 61?80.
J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19:i180?182.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL 2003.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In LREC Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL 2005.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with SVM. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Y. Miyao, K. Sagae, and J. Tsujii. 2007. Towards
framework-independent evaluation of deep linguistic
parsers. In Grammar Engineering across Frameworks
2007, pages 238?258.
A. Moschitti. 2006. Making tree kernels practical for
natural language processing. In EACL 2006.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In ACL 2005.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL 2007.
S. Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.
2006. Lexical adaptation of link grammar to the
biomedical sublanguage: a comparative evaluation of
three approaches. BMC Bioinformatics, 7(Suppl. 3).
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007a. BioInfer: a cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-
monen, and T. Salakoski. 2007b. On the unification of
syntactic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA. In
BioNLP 2007, pages 25?32.
C. Quirk and S. Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical ma-
chine translation. In EMNLP 2006.
E. K. Ringger, R. C. Moore, E. Charniak, L. Vander-
wende, and H. Suzuki. 2004. Using the Penn Tree-
bank to evaluate non-treebank parsers. In LREC 2004.
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic
features for protein-protein interaction extraction. In
LBM 2007 short papers.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In EMNLP-CoNLL 2007.
K. Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a Link Grammar. In 3rd IWPT.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Develop-
ing a robust part-of-speech tagger for biomedical text.
In 10th Panhellenic Conference on Informatics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In First International
Symposium on Semantic Mining in Biomedicine.
54
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19?27,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Comparative Study on Generalization of Semantic Roles in FrameNet
Yuichiroh Matsubayashi? Naoaki Okazaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, UK
{y-matsu,okazaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
A number of studies have presented
machine-learning approaches to semantic
role labeling with availability of corpora
such as FrameNet and PropBank. These
corpora define the semantic roles of predi-
cates for each frame independently. Thus,
it is crucial for the machine-learning ap-
proach to generalize semantic roles across
different frames, and to increase the size
of training instances. This paper ex-
plores several criteria for generalizing se-
mantic roles in FrameNet: role hierar-
chy, human-understandable descriptors of
roles, semantic types of filler phrases, and
mappings from FrameNet roles to the-
matic roles of VerbNet. We also pro-
pose feature functions that naturally com-
bine and weight these criteria, based on
the training data. The experimental result
of the role classification shows 19.16%
and 7.42% improvements in error reduc-
tion rate and macro-averaged F1 score, re-
spectively. We also provide in-depth anal-
yses of the proposed criteria.
1 Introduction
Semantic Role Labeling (SRL) is a task of analyz-
ing predicate-argument structures in texts. More
specifically, SRL identifies predicates and their
arguments with appropriate semantic roles. Re-
solving surface divergence of texts (e.g., voice
of verbs and nominalizations) into unified seman-
tic representations, SRL has attracted much at-
tention from researchers into various NLP appli-
cations including question answering (Narayanan
and Harabagiu, 2004; Shen and Lapata, 2007;
buy.v PropBank FrameNet
Frame buy.01 Commerce buy
Roles ARG0: buyer Buyer
ARG1: thing bought Goods
ARG2: seller Seller
ARG3: paid Money
ARG4: benefactive Recipient
... ...
Figure 1: A comparison of frames for buy.v de-
fined in PropBank and FrameNet
Moschitti et al, 2007), and information extrac-
tion (Surdeanu et al, 2003).
In recent years, with the wide availability of cor-
pora such as PropBank (Palmer et al, 2005) and
FrameNet (Baker et al, 1998), a number of stud-
ies have presented statistical approaches to SRL
(Ma`rquez et al, 2008). Figure 1 shows an exam-
ple of the frame definitions for a verb buy in Prop-
Bank and FrameNet. These corpora define a large
number of frames and define the semantic roles for
each frame independently. This fact is problem-
atic in terms of the performance of the machine-
learning approach, because these definitions pro-
duce many roles that have few training instances.
PropBank defines a frame for each sense of
predicates (e.g., buy.01), and semantic roles are
defined in a frame-specific manner (e.g., buyer and
seller for buy.01). In addition, these roles are asso-
ciated with tags such as ARG0-5 and AM-*, which
are commonly used in different frames. Most
SRL studies on PropBank have used these tags
in order to gather a sufficient amount of training
data, and to generalize semantic-role classifiers
across different frames. However, Yi et al (2007)
reported that tags ARG2?ARG5 were inconsis-
tent and not that suitable as training instances.
Some recent studies have addressed alternative ap-
proaches to generalizing semantic roles across dif-
ferent frames (Gordon and Swanson, 2007; Zapi-
19
Transfer::RecipientGiving::Recipient Commerce_buy::BuyerCommerce_sell::Buyer Commerce_buy::SellerCommerce_sell::SellerGiving::Donor
Transfer::Donor
Buyer SellerAgent role-to-role relationhierarchical classthematic rolerole descriptor
Recipient Donor
Figure 2: An example of role groupings using different criteria.
rain et al, 2008).
FrameNet designs semantic roles as frame spe-
cific, but also defines hierarchical relations of se-
mantic roles among frames. Figure 2 illustrates
an excerpt of the role hierarchy in FrameNet; this
figure indicates that the Buyer role for the Com-
merce buy frame (Commerce buy::Buyer here-
after) and the Commerce sell::Buyer role are in-
herited from the Transfer::Recipient role. Al-
though the role hierarchy was expected to gener-
alize semantic roles, no positive results for role
classification have been reported (Baldewein et al,
2004). Therefore, the generalization of semantic
roles across different frames has been brought up
as a critical issue for FrameNet (Gildea and Juraf-
sky, 2002; Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006)
In this paper, we explore several criteria for gen-
eralizing semantic roles in FrameNet. In addi-
tion to the FrameNet hierarchy, we use various
pieces of information: human-understandable de-
scriptors of roles, semantic types of filler phrases,
and mappings from FrameNet roles to the thematic
roles of VerbNet. We also propose feature func-
tions that naturally combines these criteria in a
machine-learning framework. Using the proposed
method, the experimental result of the role classi-
fication shows 19.16% and 7.42% improvements
in error reduction rate and macro-averaged F1, re-
spectively. We provide in-depth analyses with re-
spect to these criteria, and state our conclusions.
2 Related Work
Moschitti et al (2005) first classified roles by us-
ing four coarse-grained classes (Core Roles, Ad-
juncts, Continuation Arguments and Co-referring
Arguments), and built a classifier for each coarse-
grained class to tag PropBank ARG tags. Even
though the initial classifiers could perform rough
estimations of semantic roles, this step was not
able to solve the ambiguity problem in PropBank
ARG2-5. When training a classifier for a seman-
tic role, Baldewein et al (2004) re-used the train-
ing instances of other roles that were similar to the
target role. As similarity measures, they used the
FrameNet hierarchy, peripheral roles of FrameNet,
and clusters constructed by a EM-based method.
Gordon and Swanson (2007) proposed a general-
ization method for the PropBank roles based on
syntactic similarity in frames.
Many previous studies assumed that thematic
roles bridged semantic roles in different frames.
Gildea and Jurafsky (2002) showed that classifica-
tion accuracy was improved by manually replac-
ing FrameNet roles into 18 thematic roles. Shi
and Mihalcea (2005) and Giuglea and Moschitti
(2006) employed VerbNet thematic roles as the
target of mappings from the roles defined by the
different semantic corpora. Using the thematic
roles as alternatives of ARG tags, Loper et al
(2007) and Yi et al (2007) demonstrated that the
classification accuracy of PropBank roles was im-
proved for ARG2 roles, but that it was diminished
for ARG1. Yi et al (2007) also described that
ARG2?5 were mapped to a variety of thematic
roles. Zapirain et al (2008) evaluated PropBank
ARG tags and VerbNet thematic roles in a state-of-
the-art SRL system, and concluded that PropBank
ARG tags achieved a more robust generalization of
the roles than did VerbNet thematic roles.
3 Role Classification
SRL is a complex task wherein several problems
are intertwined: frame-evoking word identifica-
tion, frame disambiguation (selecting a correct
frame from candidates for the evoking word), role-
phrase identification (identifying phrases that fill
semantic roles), and role classification (assigning
correct roles to the phrases). In this paper, we fo-
cus on role classification, in which the role gen-
eralization is particularly critical to the machine
learning approach.
In the role classification task, we are given a
sentence, a frame evoking word, a frame, and
20
member roles 
Commerce_pay::Buyer
Intentionall_act::Agent
Giving::Donor
Getting::Recipient
Giving::Recipient
Sending::Recipient
Giving::Time
Placing::Time
Event::Time
Commerce_pay::Buyer
Commerce_buy::Buyer
Commerce_sell::Buyer
Buyer
Recipient Time
C_pay::Buyer
GIVING::Donor
Intentionally_ACT::Agent
Avoiding::Agent
Evading::Evader
Evading::Evader
Avoiding::Agent
Getting::Recipient
Evading::Evader
St::Sentient St::Physical_Obj
Giving::Theme
Placing::Theme
St::State_of_affairs
Giving::Reason   Evading::Reason
Giving::Means    Evading::Purpose
Theme::Agent
Theme::Theme
Commerce_buy::Goods
Getting::Theme
Evading:: Pursuer
Commerce_buy::Buyer
Commerce_sell::Seller
Evading::Evader
Role-descriptor groupsHierarchical-relation groups Semantic-type groupsThematic-role groups
Group name
legend
Figure 4: Examples for each type of role group.
INPUT:frame = Commerce_sell
candidate   roles ={Seller, Buyer, Goods, Reason, Time, ... , Place}
sentence = Can't [you] [sell Commerce_sell] [the factory] [to some other company]? 
OUTPUT:  
sentence = Can't [you Seller] [sell Commerce_sell] [the factory Goods][to some other company Buyer] ?
Figure 3: An example of input and output of role
classification.
phrases that take semantic roles. We are inter-
ested in choosing the correct role from the can-
didate roles for each phrase in the frame. Figure 3
shows a concrete example of input and output; the
semantic roles for the phrases are chosen from the
candidate roles: Seller, Buyer, Goods, Reason,
... , and Place.
4 Design of Role Groups
We formalize the generalization of semantic roles
as the act of grouping several roles into a
class. We define a role group as a set of
role labels grouped by a criterion. Figure 4
shows examples of role groups; a group Giv-
ing::Donor (in the hierarchical-relation groups)
contains the roles Giving::Donor and Com-
merce pay::Buyer. The remainder of this section
describes the grouping criteria in detail.
4.1 Hierarchical relations among roles
FrameNet defines hierarchical relations among
frames (frame-to-frame relations). Each relation
is assigned one of the seven types of directional
relationships (Inheritance, Using, Perspective on,
Causative of, Inchoative of, Subframe, and Pre-
cedes). Some roles in two related frames are also
connected with role-to-role relations. We assume
that this hierarchy is a promising resource for gen-
eralizing the semantic roles; the idea is that the
role at a node in the hierarchy inherits the char-
acteristics of the roles of its ancestor nodes. For
example, Commerce sell::Seller in Figure 2 in-
herits the property of Giving::Donor.
For Inheritance, Using, Perspective on, and
Subframe relations, we assume that descendant
roles in these relations have the same or special-
ized properties of their ancestors. Hence, for each
role yi, we define the following two role groups,
Hchildyi = {y|y = yi ? y is a child of yi},
Hdescyi = {y|y = yi ? y is a descendant of yi}.
The hierarchical-relation groups in Figure 4 are
the illustrations of Hdescyi .
For the relation types Inchoative of and
Causative of, we define role groups in the oppo-
site direction of the hierarchy,
Hparentyi = {y|y = yi ? y is a parent of yi},
Hanceyi = {y|y = yi ? y is an ancestor of yi}.
This is because lower roles of Inchoative of
and Causative of relations represent more neu-
tral stances or consequential states; for example,
Killing::Victim is a parent of Death::Protagonist
in the Causative of relation.
Finally, the Precedes relation describes the se-
quence of states and events, but does not spec-
ify the direction of semantic inclusion relations.
Therefore, we simply try Hchildyi , H
desc
yi , H
parent
yi ,
and Hanceyi for this relation type.
4.2 Human-understandable role descriptor
FrameNet defines each role as frame-specific; in
other words, the same identifier does not appear
in different frames. However, in FrameNet,
human experts assign a human-understandable
name to each role in a rather systematic man-
ner. Some names are shared by the roles in
different frames, whose identifiers are dif-
ferent. Therefore, we examine the semantic
21
commonality of these names; we construct an
equivalence class of the roles sharing the same
name. We call these human-understandable
names role descriptors. In Figure 4, the role-
descriptor group Buyer collects the roles Com-
merce pay::Buyer, Commerce buy::Buyer,
and Commerce sell::Buyer.
This criterion may be effective in collecting
similar roles since the descriptors have been anno-
tated by intuition of human experts. As illustrated
in Figure 2, the role descriptors group the seman-
tic roles which are similar to the roles that the
FrameNet hierarchy connects as sister or parent-
child relations. However, role-descriptor groups
cannot express the relations between the roles
as inclusions since they are equivalence classes.
For example, the roles Commerce sell::Buyer
and Commerce buy::Buyer are included in the
role descriptor group Buyer in Figure 2; how-
ever, it is difficult to merge Giving::Recipient
and Commerce sell::Buyer because the Com-
merce sell::Buyer has the extra property that one
gives something of value in exchange and a hu-
man assigns different descriptors to them. We ex-
pect that the most effective weighting of these two
criteria will be determined from the training data.
4.3 Semantic type of phrases
We consider that the selectional restriction is help-
ful in detecting the semantic roles. FrameNet pro-
vides information concerning the semantic types
of role phrases (fillers); phrases that play spe-
cific roles in a sentence should fulfill the se-
mantic constraint from this information. For
instance, FrameNet specifies the constraint that
Self motion::Area should be filled by phrases
whose semantic type is Location. Since these
types suggest a coarse-grained categorization of
semantic roles, we construct role groups that con-
tain roles whose semantic types are identical.
4.4 Thematic roles of VerbNet
VerbNet thematic roles are 23 frame-independent
semantic categories for arguments of verbs,
such as Agent, Patient, Theme and Source.
These categories have been used as consis-
tent labels across verbs. We use a partial
mapping between FrameNet roles and Verb-
Net thematic roles provided by SemLink. 1
Each group is constructed as a set Tti =
1http://verbs.colorado.edu/semlink/
{y|SemLink maps y into the thematic role ti}.
SemLink currently maps 1,726 FrameNet roles
into VerbNet thematic roles, which are 37.61% of
roles appearing at least once in the FrameNet cor-
pus. This may diminish the effect of thematic-role
groups than its potential.
5 Role classification method
5.1 Traditional approach
We are given a frame-evoking word e, a frame f
and a role phrase x detected by a human or some
automatic process in a sentence s. Let Yf be the
set of semantic roles that FrameNet defines as be-
ing possible role assignments for the frame f , and
let x = {x1, . . . , xn} be observed features for x
from s, e and f . The task of semantic role classifi-
cation can be formalized as the problem of choos-
ing the most suitable role y? from Yf . Suppose we
have a model P (y|f,x) which yields the condi-
tional probability of the semantic role y for given
f and x. Then we can choose y? as follows:
y? = argmax
y?Yf
P (y|f,x). (1)
A traditional way to incorporate role groups
into this formalization is to overwrite each role
y in the training and test data with its role
group m(y) according to the memberships of
the group. For example, semantic roles Com-
merce sell::Seller and Giving::Donor can be re-
placed by their thematic-role group Theme::Agent
in this approach. We determine the most suitable
role group c? as follows:
c? = argmax
c?{m(y)|y?Yf}
Pm(c|f,x). (2)
Here, Pm(c|f,x) presents the probability of the
role group c for f and x. The role y? is determined
uniquely iff a single role y ? Yf is associated
with c?. Some previous studies have employed this
idea to remedy the data sparseness problem in the
training data (Gildea and Jurafsky, 2002). How-
ever, we cannot apply this approach when multi-
ple roles in Yf are contained in the same class. For
example, we can construct a semantic-type group
St::State of affairs in which Giving::Reason and
Giving::Means are included, as illustrated in Fig-
ure 4. If c? = St::State of affairs, we cannot dis-
ambiguate which original role is correct. In ad-
dition, it may be more effective to use various
22
groupings of roles together in the model. For in-
stance, the model could predict the correct role
Commerce sell::Seller for the phrase ?you? in
Figure 3 more confidently, if it could infer its
thematic-role group as Theme::Agent and its par-
ent group Giving::Donor correctly. Although the
ensemble of various groupings seems promising,
we need an additional procedure to prioritize the
groupings for the case where the models for mul-
tiple role groupings disagree; for example, it is un-
satisfactory if two models assign the groups Giv-
ing::Theme and Theme::Agent to the same phrase.
5.2 Role groups as feature functions
We thus propose another approach that incorpo-
rates group information as feature functions. We
model the conditional probability P (y|f,x) by us-
ing the maximum entropy framework,
p(y|f,x) = exp(
?
i ?igi(x, y))
?
y?Yf exp(
?
i ?igi(x, y))
. (3)
Here, G = {gi} denotes a set of n feature func-
tions, and ? = {?i} denotes a weight vector for
the feature functions.
In general, feature functions for the maximum
entropy model are designed as indicator functions
for possible pairs of xj and y. For example, the
event where the head word of x is ?you? (x1 = 1)
and x plays the role Commerce sell::Seller in a
sentence is expressed by the indicator function,
grole1 (x, y) =
?
?
?
?
?
1 (x1 = 1 ?
y = Commerce sell::Seller)
0 (otherwise)
.
(4)
We call this kind of feature function an x-role.
In order to incorporate role groups into the
model, we also include all feature functions for
possible pairs of xj and role groups. Equation 5
is an example of a feature function for instances
where the head word of x is ?you? and y is in the
role group Theme::Agent,
gtheme2 (x, y) =
?
?
?
?
?
1 (x1 = 1 ?
y ? Theme::Agent)
0 (otherwise)
. (5)
Thus, this feature function fires for the roles wher-
ever the head word ?you? plays Agent (e.g., Com-
merce sell::Seller, Commerce buy::Buyer and
Giving::Donor). We call this kind of feature func-
tion an x-group function.
In this way, we obtain x-group functions for
all grouping methods, e.g., gthemek , g
hierarchy
k .
The role-group features will receive more training
instances by collecting instances for fine-grained
roles. Thus, semantic roles with few training in-
stances are expected to receive additional clues
from other training instances via role-group fea-
tures. Another advantage of this approach is that
the usefulness of the different role groups is de-
termined by the training processes in terms of
weights of feature functions. Thus, we do not need
to assume that we have found the best criterion for
grouping roles; we can allow a training process to
choose the criterion. We will discuss the contribu-
tions of different groupings in the experiments.
5.3 Comparison with related work
Baldewein et al (2004) suggested an approach
that uses role descriptors and hierarchical rela-
tions as criteria for generalizing semantic roles
in FrameNet. They created a classifier for each
frame, additionally using training instances for the
role A to train the classifier for the role B, if the
roles A and B were judged as similar by a crite-
rion. This approach performs similarly to the over-
writing approach, and it may obscure the differ-
ences among roles. Therefore, they only re-used
the descriptors as a similarity measure for the roles
whose coreness was peripheral. 2
In contrast, we use all kinds of role descriptors
to construct groups. Since we use the feature func-
tions for both the original roles and their groups,
appropriate units for classification are determined
automatically in the training process.
6 Experiment and Discussion
We used the training set of the Semeval-2007
Shared task (Baker et al, 2007) in order to ascer-
tain the contributions of role groups. This dataset
consists of the corpus of FrameNet release 1.3
(containing roughly 150,000 annotations), and an
additional full-text annotation dataset. We ran-
domly extracted 10% of the dataset for testing, and
used the remainder (90%) for training.
Performance was measured by micro- and
macro-averaged F1 (Chang and Zheng, 2008) with
respect to a variety of roles. The micro average bi-
ases each F1 score by the frequencies of the roles,
2In FrameNet, each role is assigned one of four different
types of coreness (core, core-unexpressed, peripheral, extra-
thematic) It represents the conceptual necessity of the roles
in the frame to which it belongs.
23
and the average is equal to the classification accu-
racy when we calculate it with all of the roles in
the test set. In contrast, the macro average does
not bias the scores, thus the roles having a small
number of instances affect the average more than
the micro average.
6.1 Experimental settings
We constructed a baseline classifier that uses
only the x-role features. The feature de-
sign is similar to that of the previous stud-
ies (Ma`rquez et al, 2008). The characteristics
of x are: frame, frame evoking word, head
word, content word (Surdeanu et al, 2003),
first/last word, head word of left/right sister,
phrase type, position, voice, syntactic path (di-
rected/undirected/partial), governing category
(Gildea and Jurafsky, 2002), WordNet super-
sense in the phrase, combination features of
frame evoking word & headword, combination
features of frame evoking word & phrase type,
and combination features of voice & phrase type.
We also used PoS tags and stem forms as extra
features of any word-features.
We employed Charniak and Johnson?s rerank-
ing parser (Charniak and Johnson, 2005) to an-
alyze syntactic trees. As an alternative for the
traditional named-entity features, we used Word-
Net supersenses: 41 coarse-grained semantic cate-
gories of words such as person, plant, state, event,
time, location. We used Ciaramita and Altun?s Su-
per Sense Tagger (Ciaramita and Altun, 2006) to
tag the supersenses. The baseline system achieved
89.00% with respect to the micro-averaged F1.
The x-group features were instantiated similarly
to the x-role features; the x-group features com-
bined the characteristics of x with the role groups
presented in this paper. The total number of fea-
tures generated for all x-roles and x-groups was
74,873,602. The optimal weights ? of the fea-
tures were obtained by the maximum a poste-
rior (MAP) estimation. We maximized an L2-
regularized log-likelihood of the training set us-
ing the Limited-memory BFGS (L-BFGS) method
(Nocedal, 1980).
6.2 Effect of role groups
Table 1 shows the micro and macro averages of F1
scores. Each role group type improved the micro
average by 0.5 to 1.7 points. The best result was
obtained by using all types of groups together. The
result indicates that different kinds of group com-
Feature Micro Macro ?Err.
Baseline 89.00 68.50 0.00
role descriptor 90.78 76.58 16.17
role descriptor (replace) 90.23 76.19 11.23
hierarchical relation 90.25 72.41 11.40
semantic type 90.36 74.51 12.38
VN thematic role 89.50 69.21 4.52
All 91.10 75.92 19.16
Table 1: The accuracy and error reduction rate of
role classification for each type of role group.
Feature #instances Pre. Rec. Micro
baseline ? 10 63.89 38.00 47.66
? 20 69.01 51.26 58.83
? 50 75.84 65.85 70.50
+ all groups ? 10 72.57 55.85 63.12
? 20 76.30 65.41 70.43
? 50 80.86 74.59 77.60
Table 2: The effect of role groups on the roles with
few instances.
plement each other with respect to semantic role
generalization. Baldewein et al (2004) reported
that hierarchical relations did not perform well for
their method and experimental setting; however,
we found that significant improvements could also
be achieved with hierarchical relations. We also
tried a traditional label-replacing approach with
role descriptors (in the third row of Table 1). The
comparison between the second and third rows in-
dicates that mixing the original fine-grained roles
and the role groups does result in a more accurate
classification.
By using all types of groups together, the
model reduced 19.16 % of the classification errors
from the baseline. Moreover, the macro-averaged
F1 scores clearly showed improvements resulting
from using role groups. In order to determine
the reason for the improvements, we measured
the precision, recall, and F1-scores with respect
to roles for which the number of training instances
was at most 10, 20, and 50. In Table 2, we show
that the micro-averaged F1 score for roles hav-
ing 10 instances or less was improved (by 15.46
points) when all role groups were used. This result
suggests the reason for the effect of role groups; by
bridging similar semantic roles, they supply roles
having a small number of instances with the infor-
mation from other roles.
6.3 Analyses of role descriptors
In Table 1, the largest improvement was obtained
by the use of role descriptors. We analyze the ef-
fect of role descriptors in detail in Tables 3 and 4.
Table 3 shows the micro-averaged F1 scores of all
24
Coreness #roles #instances/#role #groups #instances/#group #roles/#group
Core 1902 122.06 655 354.4 2.9
Peripheral 1924 25.24 250 194.3 7.7
Extra-thematic 763 13.90 171 62.02 4.5
Table 4: The analysis of the numbers of roles, instances, and role-descriptor groups, for each type of
coreness.
Coreness Micro
Baseline 89.00
Core 89.51
Peripheral 90.12
Extra-thematic 89.09
All 90.77
Table 3: The effect of employing role-descriptor
groups of each type of coreness.
semantic roles when we use role-descriptor groups
constructed from each type of coreness (core3, pe-
ripheral, and extra-thematic) individually. The pe-
ripheral type generated the largest improvements.
Table 4 shows the number of roles associated
with each type of coreness (#roles), the number of
instances for the original roles (#instances/#role),
the number of groups for each type of coreness
(#groups), the number of instances for each group
(#instances/#group), and the number of roles per
each group (#roles/#group). In the peripheral
type, the role descriptors subdivided 1,924 distinct
roles into 250 groups, each of which contained 7.7
roles on average. The peripheral type included
semantic roles such as place, time, reason, dura-
tion. These semantic roles appear in many frames,
because they have general meanings that can be
shared by different frames. Moreover, the seman-
tic roles of peripheral type originally occurred in
only a small number (25.24) of training instances
on average. Thus, we infer that the peripheral
type generated the largest improvement because
semantic roles in this type acquired the greatest
benefit from the generalization.
6.4 Hierarchical relations and relation types
We analyzed the contributions of the FrameNet hi-
erarchy for each type of role-to-role relations and
for different depths of grouping. Table 5 shows
the micro-averaged F1 scores obtained from var-
ious relation types and depths. The Inheritance
and Using relations resulted in a slightly better ac-
curacy than the other types. We did not observe
any real differences among the remaining five re-
lation types, possibly because there were few se-
3We include Core-unexpressed in core, because it has a
property of core inside one frame.
No. Relation Type Micro
- baseline 89.00
1 + Inheritance (children) 89.52
2 + Inheritance (descendants) 89.70
3 + Using (children) 89.35
4 + Using (descendants) 89.37
5 + Perspective on (children) 89.01
6 + Perspective on (descendants) 89.01
7 + Subframe (children) 89.04
8 + Subframe (descendants) 89.05
9 + Causative of (parents) 89.03
10 + Causative of (ancestors) 89.03
11 + Inchoative of (parents) 89.02
12 + Inchoative of (ancestors) 89.02
13 + Precedes (children) 89.01
14 + Precedes (descendants) 89.03
15 + Precedes (parents) 89.00
16 + Precedes (ancestors) 89.00
18 + all relations (2,4,6,8,10,12,14) 90.25
Table 5: Comparison of the accuracy with differ-
ent types of hierarchical relations.
mantic roles associated with these types. We ob-
tained better results by using not only groups for
parent roles, but also groups for all ancestors. The
best result was obtained by using all relations in
the hierarchy.
6.5 Analyses of different grouping criteria
Table 6 reports the precision, recall, and micro-
averaged F1 scores of semantic roles with respect
to each coreness type.4 In general, semantic roles
of the core coreness were easily identified by all
of the grouping criteria; even the baseline system
obtained an F1 score of 91.93. For identifying se-
mantic roles of the peripheral and extra-thematic
types of coreness, the simplest solution, the de-
scriptor criterion, outperformed other criteria.
In Table 7, we categorize feature functions
whose weights are in the top 1000 in terms of
greatest absolute value. The behaviors of the role
groups can be distinguished by the following two
characteristics. Groups of role descriptors and se-
mantic types have large weight values for the first
word and supersense features, which capture the
characteristics of adjunctive phrases. The original
roles and hierarchical-relation groups have strong
4The figures of role descriptors in Tables 4 and 6 differ.
In Table 4, we measured the performance when we used one
or all types of coreness for training. In contrast, in Table 6,
we used all types of coreness for training, but computed the
performance of semantic roles for each coreness separately.
25
Feature Type Pre. Rec. Micro
baseline c 91.07 92.83 91.93
p 81.05 76.03 78.46
e 78.17 66.51 71.87
+ descriptor group c 92.50 93.41 92.95
p 84.32 82.72 83.51
e 80.91 69.59 74.82
+ hierarchical c 92.10 93.28 92.68
relation p 82.23 79.84 81.01
class e 77.94 65.58 71.23
+ semantic c 92.23 93.31 92.77
type group p 83.66 81.76 82.70
e 80.29 67.26 73.20
+ VN thematic c 91.57 93.06 92.31
role group p 80.66 76.95 78.76
e 78.12 66.60 71.90
+ all group c 92.66 93.61 93.13
p 84.13 82.51 83.31
e 80.77 68.56 74.17
Table 6: The precision and recall of each type of
coreness with role groups. Type represents the
type of coreness; c denotes core, p denotes periph-
eral, and e denotes extra-thematic.
associations with lexical and structural character-
istics such as the syntactic path, content word, and
head word. Table 7 suggests that role-descriptor
groups and semantic-type groups are effective for
peripheral or adjunctive roles, and hierarchical re-
lation groups are effective for core roles.
7 Conclusion
We have described different criteria for general-
izing semantic roles in FrameNet. They were:
role hierarchy, human-understandable descriptors
of roles, semantic types of filler phrases, and
mappings from FrameNet roles to thematic roles
of VerbNet. We also proposed a feature design
that combines and weights these criteria using the
training data. The experimental result of the role
classification task showed a 19.16% of the error
reduction and a 7.42% improvement in the macro-
averaged F1 score. In particular, the method we
have presented was able to classify roles having
few instances. We confirmed that modeling the
role generalization at feature level was better than
the conventional approach that replaces semantic
role labels.
Each criterion presented in this paper improved
the accuracy of classification. The most success-
ful criterion was the use of human-understandable
role descriptors. Unfortunately, the FrameNet hi-
erarchy did not outperform the role descriptors,
contrary to our expectations. A future direction
of this study would be to analyze the weakness of
the FrameNet hierarchy in order to discuss possi-
ble improvement of the usage and annotations of
features of x class type
or hr rl st vn
frame 0 4 0 1 0
evoking word 3 4 7 3 0
ew & hw stem 9 34 20 8 0
ew & phrase type 11 7 11 3 1
head word 13 19 8 3 1
hw stem 11 17 8 8 1
content word 7 19 12 3 0
cw stem 11 26 13 5 0
cw PoS 4 5 14 15 2
directed path 19 27 24 6 7
undirected path 21 35 17 2 6
partial path 15 18 16 13 5
last word 15 18 12 3 2
first word 11 23 53 26 10
supersense 7 7 35 25 4
position 4 6 30 9 5
others 27 29 33 19 6
total 188 298 313 152 50
Table 7: The analysis of the top 1000 feature func-
tions. Each number denotes the number of feature
functions categorized in the corresponding cell.
Notations for the columns are as follows. ?or?:
original role, ?hr?: hierarchical relation, ?rd?: role
descriptor, ?st?: semantic type, and ?vn?: VerbNet
thematic role.
the hierarchy.
Since we used the latest release of FrameNet
in order to use a greater number of hierarchical
role-to-role relations, we could not make a direct
comparison of performance with that of existing
systems; however we may say that the 89.00% F1
micro-average of our baseline system is roughly
comparable to the 88.93% value of Bejan and
Hathaway (2007) for SemEval-2007 (Baker et al,
2007). 5 In addition, the methodology presented in
this paper applies generally to any SRL resources;
we are planning to determine several grouping cri-
teria from existing linguistic resources and to ap-
ply the methodology to the PropBank corpus.
Acknowledgments
The authors thank Sebastian Riedel for his useful
comments on our work. This work was partially
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of Coling-ACL 1998, pages 86?90.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
5There were two participants that performed whole SRL
in SemEval-2007. Bejan and Hathaway (2007) evaluated role
classification accuracy separately for the training data.
26
ture extraction. In Proceedings of SemEval-2007,
pages 99?104.
Ulrike Baldewein, Katrin Erk, Sebastian Pado?, and
Detlef Prescher. 2004. Semantic role labeling
with similarity based generalization using EM-based
clustering. In Proceedings of Senseval-3, pages 64?
68.
Cosmin Adrian Bejan and Chris Hathaway. 2007.
UTD-SRL: A Pipeline Architecture for Extract-
ing Frame Semantic Structures. In Proceedings
of SemEval-2007, pages 460?463. Association for
Computational Linguistics.
X. Chang and Q. Zheng. 2008. Knowledge Ele-
ment Extraction for Knowledge-Based Learning Re-
sources Organization. Lecture Notes in Computer
Science, 4823:102?113.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proceedings of EMNLP-2006, pages 594?602.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the ACL, pages 929?936.
Andrew Gordon and Reid Swanson. 2007. General-
izing semantic role annotations across syntactically
similar verbs. In Proceedings of ACL-2007, pages
192?199.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
propbank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Semantics,
pages 118?128.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational linguistics, 34(2):145?159.
Alessandro Moschitti, Ana-Maria Giuglea, Bonaven-
tura Coppola, and Roberto Basili. 2005. Hierar-
chical semantic role labeling. In Proceedings of
CoNLL-2005, pages 201?204.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL-07,
pages 776?783.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of Coling-2004, pages 693?701.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of EMNLP-CoNLL 2007, pages 12?21.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing-2005, pages 100?111.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of ACL-2003, pages 8?15.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of HLT-NAACL 2007, pages 548?555.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet. In Proceedings of ACL-08:
HLT, pages 550?558.
27
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477?485,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Stochastic Gradient Descent Training for
L1-regularized Log-linear Models with Cumulative Penalty
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Stochastic gradient descent (SGD) uses
approximate gradients estimated from
subsets of the training data and updates
the parameters in an online fashion. This
learning framework is attractive because
it often requires much less training time
in practice than batch training algorithms.
However, L1-regularization, which is be-
coming popular in natural language pro-
cessing because of its ability to pro-
duce compact models, cannot be effi-
ciently applied in SGD training, due to
the large dimensions of feature vectors
and the fluctuations of approximate gra-
dients. We present a simple method to
solve these problems by penalizing the
weights according to cumulative values for
L1 penalty. We evaluate the effectiveness
of our method in three applications: text
chunking, named entity recognition, and
part-of-speech tagging. Experimental re-
sults demonstrate that our method can pro-
duce compact and accurate models much
more quickly than a state-of-the-art quasi-
Newton method for L1-regularized log-
linear models.
1 Introduction
Log-linear models (a.k.a maximum entropy mod-
els) are one of the most widely-used probabilistic
models in the field of natural language process-
ing (NLP). The applications range from simple
classification tasks such as text classification and
history-based tagging (Ratnaparkhi, 1996) to more
complex structured prediction tasks such as part-
of-speech (POS) tagging (Lafferty et al, 2001),
syntactic parsing (Clark and Curran, 2004) and se-
mantic role labeling (Toutanova et al, 2005). Log-
linear models have a major advantage over other
discriminative machine learning models such as
support vector machines?their probabilistic out-
put allows the information on the confidence of
the decision to be used by other components in the
text processing pipeline.
The training of log-liner models is typically per-
formed based on the maximum likelihood crite-
rion, which aims to obtain the weights of the fea-
tures that maximize the conditional likelihood of
the training data. In maximum likelihood training,
regularization is normally needed to prevent the
model from overfitting the training data,
The two most common regularization methods
are called L1 and L2 regularization. L1 regular-
ization penalizes the weight vector for its L1-norm
(i.e. the sum of the absolute values of the weights),
whereas L2 regularization uses its L2-norm. There
is usually not a considerable difference between
the two methods in terms of the accuracy of the
resulting model (Gao et al, 2007), but L1 regu-
larization has a significant advantage in practice.
Because many of the weights of the features be-
come zero as a result of L1-regularized training,
the size of the model can be much smaller than that
produced by L2-regularization. Compact models
require less space on memory and storage, and en-
able the application to start up quickly. These mer-
its can be of vital importance when the application
is deployed in resource-tight environments such as
cell-phones.
A common way to train a large-scale L1-
regularized model is to use a quasi-Newton
method. Kazama and Tsujii (2003) describe a
method for training a L1-regularized log-linear
model with a bound constrained version of the
BFGS algorithm (Nocedal, 1980). Andrew and
Gao (2007) present an algorithm called Orthant-
Wise Limited-memory Quasi-Newton (OWL-
QN), which can work on the BFGS algorithm
without bound constraints and achieve faster con-
vergence.
477
An alternative approach to training a log-linear
model is to use stochastic gradient descent (SGD)
methods. SGD uses approximate gradients esti-
mated from subsets of the training data and up-
dates the weights of the features in an online
fashion?the weights are updated much more fre-
quently than batch training algorithms. This learn-
ing framework is attracting attention because it of-
ten requires much less training time in practice
than batch training algorithms, especially when
the training data is large and redundant. SGD was
recently used for NLP tasks including machine
translation (Tillmann and Zhang, 2006) and syn-
tactic parsing (Smith and Eisner, 2008; Finkel et
al., 2008). Also, SGD is very easy to implement
because it does not need to use the Hessian infor-
mation on the objective function. The implemen-
tation could be as simple as the perceptron algo-
rithm.
Although SGD is a very attractive learning
framework, the direct application of L1 regular-
ization in this learning framework does not result
in efficient training. The first problem is the inef-
ficiency of applying the L1 penalty to the weights
of all features. In NLP applications, the dimen-
sion of the feature space tends to be very large?it
can easily become several millions, so the appli-
cation of L1 penalty to all features significantly
slows down the weight updating process. The sec-
ond problem is that the naive application of L1
penalty in SGD does not always lead to compact
models, because the approximate gradient used at
each update is very noisy, so the weights of the
features can be easily moved away from zero by
those fluctuations.
In this paper, we present a simple method for
solving these two problems in SGD learning. The
main idea is to keep track of the total penalty and
the penalty that has been applied to each weight,
so that the L1 penalty is applied based on the dif-
ference between those cumulative values. That
way, the application of L1 penalty is needed only
for the features that are used in the current sample,
and also the effect of noisy gradient is smoothed
away.
We evaluate the effectiveness of our method
by using linear-chain conditional random fields
(CRFs) and three traditional NLP tasks, namely,
text chunking (shallow parsing), named entity
recognition, and POS tagging. We show that our
enhanced SGD learning method can produce com-
pact and accurate models much more quickly than
the OWL-QN algorithm.
This paper is organized as follows. Section 2
provides a general description of log-linear mod-
els used in NLP. Section 3 describes our stochastic
gradient descent method for L1-regularized log-
linear models. Experimental results are presented
in Section 4. Some related work is discussed in
Section 5. Section 6 gives some concluding re-
marks.
2 Log-Linear Models
In this section, we briefly describe log-linear mod-
els used in NLP tasks and L1 regularization.
A log-linear model defines the following prob-
abilistic distribution over possible structure y for
input x:
p(y|x) = 1Z(x) exp
?
i
wifi(y,x),
where fi(y,x) is a function indicating the occur-
rence of feature i, wi is the weight of the feature,
and Z(x) is a partition (normalization) function:
Z(x) =
?
y
exp
?
i
wifi(y,x).
If the structure is a sequence, the model is called
a linear-chain CRF model, and the marginal prob-
abilities of the features and the partition function
can be efficiently computed by using the forward-
backward algorithm. The model is used for a va-
riety of sequence labeling tasks such as POS tag-
ging, chunking, and named entity recognition.
If the structure is a tree, the model is called a
tree CRF model, and the marginal probabilities
can be computed by using the inside-outside algo-
rithm. The model can be used for tasks like syn-
tactic parsing (Finkel et al, 2008) and semantic
role labeling (Cohn and Blunsom, 2005).
2.1 Training
The weights of the features in a log-linear model
are optimized in such a way that they maximize
the regularized conditional log-likelihood of the
training data:
Lw =
N
?
j=1
log p(yj |xj ;w)?R(w), (1)
where N is the number of training samples, yj is
the correct output for input xj , and R(w) is the
478
regularization term which prevents the model from
overfitting the training data. In the case of L1 reg-
ularization, the term is defined as:
R(w) = C
?
i
|wi|,
where C is the meta-parameter that controls the
degree of regularization, which is usually tuned by
cross-validation or using the heldout data.
In what follows, we denote by L(j,w)
the conditional log-likelihood of each sample
log p(yj |xj ;w). Equation 1 is rewritten as:
Lw =
N
?
j=1
L(j,w)? C
?
i
|wi|. (2)
3 Stochastic Gradient Descent
SGD uses a small randomly-selected subset of the
training samples to approximate the gradient of
the objective function given by Equation 2. The
number of training samples used for this approx-
imation is called the batch size. When the batch
size is N , the SGD training simply translates into
gradient descent (hence is very slow to converge).
By using a small batch size, one can update the
parameters more frequently than gradient descent
and speed up the convergence. The extreme case
is a batch size of 1, and it gives the maximum
frequency of updates and leads to a very simple
perceptron-like algorithm, which we adopt in this
work.1
Apart from using a single training sample to
approximate the gradient, the optimization proce-
dure is the same as simple gradient descent,2 so
the weights of the features are updated at training
sample j as follows:
wk+1 = wk + ?k
?
?w (L(j,w)?
C
N
?
i
|wi|),
where k is the iteration counter and ?k is the learn-
ing rate, which is normally designed to decrease
as the iteration proceeds. The actual learning rate
scheduling methods used in our experiments are
described later in Section 3.3.
1In the actual implementation, we randomly shuffled the
training samples at the beginning of each pass, and then
picked them up sequentially.
2What we actually do here is gradient ascent, but we stick
to the term ?gradient descent?.
3.1 L1 regularization
The update equation for the weight of each feature
i is as follows:
wik+1 = wik + ?k
?
?wi
(L(j,w)? CN |wi|).
The difficulty with L1 regularization is that the
last term on the right-hand side of the above equa-
tion is not differentiable when the weight is zero.
One straightforward solution to this problem is to
consider a subgradient at zero and use the follow-
ing update equation:
wik+1 = wik + ?k
?L(j,w)
?wi
? CN ?ksign(w
k
i ),
where sign(x) = 1 if x > 0, sign(x) = ?1 if x <
0, and sign(x) = 0 if x = 0. In this paper, we call
this weight updating method ?SGD-L1 (Naive)?.
This naive method has two serious problems.
The first problem is that, at each update, we need
to perform the application of L1 penalty to all fea-
tures, including the features that are not used in
the current training sample. Since the dimension
of the feature space can be very large, it can sig-
nificantly slow down the weight update process.
The second problem is that it does not produce
a compact model, i.e. most of the weights of the
features do not become zero as a result of train-
ing. Note that the weight of a feature does not be-
come zero unless it happens to fall on zero exactly,
which rarely happens in practice.
Carpenter (2008) describes an alternative ap-
proach. The weight updating process is divided
into two steps. First, the weight is updated with-
out considering the L1 penalty term. Then, the
L1 penalty is applied to the weight to the extent
that it does not change its sign. In other words,
the weight is clipped when it crosses zero. Their
weight update procedure is as follows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ?
C
N ?k),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i +
C
N ?k).
In this paper, we call this update method ?SGD-
L1 (Clipping)?. It should be noted that this method
479
-0.1
-0.05
 0
 0.05
 0.1
 0  1000  2000  3000  4000  5000  6000
W
ei
gh
t
Updates
Figure 1: An example of weight updates.
is actually a special case of the FOLOS algorithm
(Duchi and Singer, 2008) and the truncated gradi-
ent method (Langford et al, 2009).
The obvious advantage of using this method is
that we can expect many of the weights of the
features to become zero during training. Another
merit is that it allows us to perform the applica-
tion of L1 penalty in a lazy fashion, so that we
do not need to update the weights of the features
that are not used in the current sample, which leads
to much faster training when the dimension of the
feature space is large. See the aforementioned pa-
pers for the details. In this paper, we call this effi-
cient implementation ?SGD-L1 (Clipping + Lazy-
Update)?.
3.2 L1 regularization with cumulative
penalty
Unfortunately, the clipping-at-zero approach does
not solve all problems. Still, we often end up with
many features whose weights are not zero. Re-
call that the gradient used in SGD is a crude ap-
proximation to the true gradient and is very noisy.
The weight of a feature is, therefore, easily moved
away from zero when the feature is used in the
current sample.
Figure 1 gives an illustrative example in which
the weight of a feature fails to become zero. The
figure shows how the weight of a feature changes
during training. The weight goes up sharply when
it is used in the sample and then is pulled back
toward zero gradually by the L1 penalty. There-
fore, the weight fails to become zero if the feature
is used toward the end of training, which is the
case in this example. Note that the weight would
become zero if the true (fluctuationless) gradient
were used?at each update the weight would go
up a little and be pulled back to zero straightaway.
Here, we present a different strategy for apply-
ing the L1 penalty to the weights of the features.
The key idea is to smooth out the effect of fluctu-
ating gradients by considering the cumulative ef-
fects from L1 penalty.
Let uk be the absolute value of the total L1-
penalty that each weight could have received up
to the point. Since the absolute value of the L1
penalty does not depend on the weight and we are
using the same regularization constant C for all
weights, it is simply accumulated as:
uk =
C
N
k
?
t=1
?t. (3)
At each training sample, we update the weights
of the features that are used in the sample as fol-
lows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ? (uk + qk?1i )),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i + (uk ? qk?1i )),
where qki is the total L1-penalty that wi has actu-
ally received up to the point:
qki =
k
?
t=1
(wt+1i ? w
t+ 12
i ). (4)
This weight updating method penalizes the
weight according to the difference between uk and
qk?1i . In effect, it forces the weight to receive the
total L1 penalty that would have been applied if
the weight had been updated by the true gradients,
assuming that the current weight vector resides in
the same orthant as the true weight vector.
It should be noted that this method is basi-
cally equivalent to a ?SGD-L1 (Clipping + Lazy-
Update)? method if we were able to use the true
gradients instead of the stochastic gradients.
In this paper, we call this weight updating
method ?SGD-L1 (Cumulative)?. The implemen-
tation of this method is very simple. Figure 2
shows the whole SGD training algorithm with this
strategy in pseudo-code.
480
1: procedure TRAIN(C)
2: u? 0
3: Initialize wi and qi with zero for all i
4: for k = 0 to MaxIterations
5: ? ? LEARNINGRATE(k)
6: u? u + ?C/N
7: Select sample j randomly
8: UPDATEWEIGHTS(j)
9:
10: procedure UPDATEWEIGHTS(j)
11: for i ? features used in sample j
12: wi ? wi + ? ?L(j,w)?wi
13: APPLYPENALTY(i)
14:
15: procedure APPLYPENALTY(i)
16: z ? wi
17: if wi > 0 then
18: wi ? max(0, wi ? (u + qi))
19: else if wi < 0 then
20: wi ? min(0, wi + (u? qi))
21: qi ? qi + (wi ? z)
22:
Figure 2: Stochastic gradient descent training with
cumulative L1 penalty. z is a temporary variable.
3.3 Learning Rate
The scheduling of learning rates often has a major
impact on the convergence speed in SGD training.
A typical choice of learning rate scheduling can
be found in (Collins et al, 2008):
?k =
?0
1 + k/N , (5)
where ?0 is a constant. Although this scheduling
guarantees ultimate convergence, the actual speed
of convergence can be poor in practice (Darken
and Moody, 1990).
In this work, we also tested simple exponential
decay:
?k = ?0??k/N , (6)
where ? is a constant. In our experiments, we
found this scheduling more practical than that
given in Equation 5. This is mainly because ex-
ponential decay sweeps the range of learning rates
more smoothly?the learning rate given in Equa-
tion 5 drops too fast at the beginning and too
slowly at the end.
It should be noted that exponential decay is not
a good choice from a theoretical point of view, be-
cause it does not satisfy one of the necessary con-
ditions for convergence?the sum of the learning
rates must diverge to infinity (Spall, 2005). How-
ever, this is probably not a big issue for practition-
ers because normally the training has to be termi-
nated at a certain number of iterations in practice.3
4 Experiments
We evaluate the effectiveness our training algo-
rithm using linear-chain CRF models and three
NLP tasks: text chunking, named entity recogni-
tion, and POS tagging.
To compare our algorithm with the state-of-the-
art, we present the performance of the OWL-QN
algorithm on the same data. We used the publicly
available OWL-QN optimizer developed by An-
drew and Gao.4 The meta-parameters for learning
were left unchanged from the default settings of
the software: the convergence tolerance was 1e-4;
and the L-BFGS memory parameter was 10.
4.1 Text Chunking
The first set of experiments used the text chunk-
ing data set provided for the CoNLL 2000 shared
task.5 The training data consists of 8,936 sen-
tences in which each token is annotated with the
?IOB? tags representing text chunks such as noun
and verb phrases. We separated 1,000 sentences
from the training data and used them as the held-
out data. The test data provided by the shared task
was used only for the final accuracy report.
The features used in this experiment were uni-
grams and bigrams of neighboring words, and un-
igrams, bigrams and trigrams of neighboring POS
tags.
To avoid giving any advantage to our SGD al-
gorithms over the OWL-QN algorithm in terms of
the accuracy of the resulting model, the OWL-QN
algorithm was used when tuning the regularization
parameter C. The tuning was performed in such a
way that it maximized the likelihood of the heldout
data. The learning rate parameters for SGD were
then tuned in such a way that they maximized the
value of the objective function in 30 passes. We
first determined ?0 by testing 1.0, 0.5, 0.2, and 0.1.
We then determined ? by testing 0.9, 0.85, and 0.8
with the fixed ?0.
3This issue could also be sidestepped by, for example,
adding a small O(1/k) term to the learning rate.
4Available from the original developers? websites:
http://research.microsoft.com/en-us/people/galena/ or
http://research.microsoft.com/en-us/um/people/jfgao/
5http://www.cnts.ua.ac.be/conll2000/chunking/
481
Passes Lw/N # Features Time (sec) F-score
OWL-QN 160 -1.583 18,109 598 93.62
SGD-L1 (Naive) 30 -1.671 455,651 1,117 93.64
SGD-L1 (Clipping + Lazy-Update) 30 -1.671 87,792 144 93.65
SGD-L1 (Cumulative) 30 -1.653 28,189 149 93.68
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.622 23,584 148 93.66
Table 1: CoNLL-2000 Chunking task. Training time and accuracy of the trained model on the test data.
-2.4
-2.2
-2
-1.8
-1.6
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 3: CoNLL 2000 chunking task: Objective
 0
 50000
 100000
 150000
 200000
 0  10  20  30  40  50
# 
Ac
tiv
e 
fe
at
ur
es
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 4: CoNLL 2000 chunking task: Number of
active features.
Figures 3 and 4 show the training process of
the model. Each figure contains four curves repre-
senting the results of the OWL-QN algorithm and
three SGD-based algorithms. ?SGD-L1 (Cumu-
lative + ED)? represents the results of our cumu-
lative penalty-based method that uses exponential
decay (ED) for learning rate scheduling.
Figure 3 shows how the value of the objec-
tive function changed as the training proceeded.
SGD-based algorithms show much faster conver-
gence than the OWL-QN algorithm. Notice also
that ?SGD-L1 (Cumulative)? improves the objec-
tive slightly faster than ?SGD-L1 (Clipping)?. The
result of ?SGD-L1 (Naive)? is not shown in this
figure, but the curve was almost identical to that
of ?SGD-L1 (Clipping)?.
Figure 4 shows the numbers of active features
(the features whose weight are not zero). It is
clearly seen that the clipping-at-zero approach
fails to reduce the number of active features, while
our algorithms succeeded in reducing the number
of active features to the same level as OWL-QN.
We then trained the models using the whole
training data (including the heldout data) and eval-
uated the accuracy of the chunker on the test data.
The number of passes performed over the train-
ing data in SGD was set to 30. The results are
shown in Table 1. The second column shows the
number of passes performed in the training. The
third column shows the final value of the objective
function per sample. The fourth column shows
the number of resulting active features. The fifth
column show the training time. The last column
shows the f-score (harmonic mean of recall and
precision) of the chunking results. There was no
significant difference between the models in terms
of accuracy. The naive SGD training took much
longer than OWL-QN because of the overhead of
applying L1 penalty to all dimensions.
Our SGD algorithms finished training in 150
seconds on Xeon 2.13GHz processors. The
CRF++ version 0.50, a popular CRF library de-
veloped by Taku Kudo,6 is reported to take 4,021
seconds on Xeon 3.0GHz processors to train the
model using a richer feature set.7 CRFsuite ver-
sion 0.4, a much faster library for CRFs, is re-
ported to take 382 seconds on Xeon 3.0GHz, using
the same feature set as ours.8 Their library uses the
OWL-QN algorithm for optimization. Although
direct comparison of training times is not impor-
6http://crfpp.sourceforge.net/
7http://www.chokkan.org/software/crfsuite/benchmark.html
8ditto
482
tant due to the differences in implementation and
hardware platforms, these results demonstrate that
our algorithm can actually result in a very fast im-
plementation of a CRF trainer.
4.2 Named Entity Recognition
The second set of experiments used the named
entity recognition data set provided for the
BioNLP/NLPBA 2004 shared task (Kim et al,
2004).9 The training data consist of 18,546 sen-
tences in which each token is annotated with the
?IOB? tags representing biomedical named enti-
ties such as the names of proteins and RNAs.
The training and test data were preprocessed
by the GENIA tagger,10 which provided POS tags
and chunk tags. We did not use any information on
the named entity tags output by the GENIA tagger.
For the features, we used unigrams of neighboring
chunk tags, substrings (shorter than 10 characters)
of the current word, and the shape of the word (e.g.
?IL-2? is converted into ?AA-#?), on top of the
features used in the text chunking experiments.
The results are shown in Figure 5 and Table
2. The trend in the results is the same as that of
the text chunking task: our SGD algorithms show
much faster convergence than the OWL-QN algo-
rithm and produce compact models.
Okanohara et al (2006) report an f-score of
71.48 on the same data, using semi-Markov CRFs.
4.3 Part-Of-Speech Tagging
The third set of experiments used the POS tag-
ging data in the Penn Treebank (Marcus et al,
1994). Following (Collins, 2002), we used sec-
tions 0-18 of the Wall Street Journal (WSJ) corpus
for training, sections 19-21 for development, and
sections 22-24 for final evaluation. The POS tags
were extracted from the parse trees in the corpus.
All experiments for this work, including the tun-
ing of features and parameters for regularization,
were carried out using the training and develop-
ment sets. The test set was used only for the final
accuracy report.
It should be noted that training a CRF-based
POS tagger using the whole WSJ corpus is not a
trivial task and was once even deemed impractical
in previous studies. For example, Wellner and Vi-
lain (2006) abandoned maximum likelihood train-
9The data is available for download at http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/ERtask/report.html
10http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
-3.8
-3.6
-3.4
-3.2
-3
-2.8
-2.6
-2.4
-2.2
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 5: NLPBA 2004 named entity recognition
task: Objective.
-2.8
-2.7
-2.6
-2.5
-2.4
-2.3
-2.2
-2.1
-2
-1.9
-1.8
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 6: POS tagging task: Objective.
ing because it was ?prohibitive? (7-8 days for sec-
tions 0-18 of the WSJ corpus).
For the features, we used unigrams and bigrams
of neighboring words, prefixes and suffixes of
the current word, and some characteristics of the
word. We also normalized the current word by
lowering capital letters and converting all the nu-
merals into ?#?, and used the normalized word as a
feature.
The results are shown in Figure 6 and Table 3.
Again, the trend is the same. Our algorithms fin-
ished training in about 30 minutes, producing ac-
curate models that are as compact as that produced
by OWL-QN.
Shen et al, (2007) report an accuracy of 97.33%
on the same data set using a perceptron-based bidi-
rectional tagging model.
5 Discussion
An alternative approach to producing compact
models for log-linear models is to reformulate the
483
Passes Lw/N # Features Time (sec) F-score
OWL-QN 161 -2.448 30,710 2,253 71.76
SGD-L1 (Naive) 30 -2.537 1,032,962 4,528 71.20
SGD-L1 (Clipping + Lazy-Update) 30 -2.538 279,886 585 71.20
SGD-L1 (Cumulative) 30 -2.479 31,986 631 71.40
SGD-L1 (Cumulative + Exponential-Decay) 30 -2.443 25,965 631 71.63
Table 2: NLPBA 2004 Named entity recognition task. Training time and accuracy of the trained model
on the test data.
Passes Lw/N # Features Time (sec) Accuracy
OWL-QN 124 -1.941 50,870 5,623 97.16%
SGD-L1 (Naive) 30 -2.013 2,142,130 18,471 97.18%
SGD-L1 (Clipping + Lazy-Update) 30 -2.013 323,199 1,680 97.18%
SGD-L1 (Cumulative) 30 -1.987 62,043 1,777 97.19%
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.954 51,857 1,774 97.17%
Table 3: POS tagging on the WSJ corpus. Training time and accuracy of the trained model on the test
data.
problem as a L1-constrained problem (Lee et al,
2006), where the conditional log-likelihood of the
training data is maximized under a fixed constraint
of the L1-norm of the weight vector. Duchi et
al. (2008) describe efficient algorithms for pro-
jecting a weight vector onto the L1-ball. Although
L1-regularized and L1-constrained learning algo-
rithms are not directly comparable because the ob-
jective functions are different, it would be inter-
esting to compare the two approaches in terms
of practicality. It should be noted, however, that
the efficient algorithm presented in (Duchi et al,
2008) needs to employ a red-black tree and is
rather complex.
In SGD learning, the need for tuning the meta-
parameters for learning rate scheduling can be an-
noying. In the case of exponential decay, the set-
ting of ? = 0.85 turned out to be a good rule
of thumb in our experiments?it always produced
near best results in 30 passes, but the other param-
eter ?0 needed to be tuned. It would be very useful
if those meta-parameters could be tuned in a fully
automatic way.
There are some sophisticated algorithms for
adaptive learning rate scheduling in SGD learning
(Vishwanathan et al, 2006; Huang et al, 2007).
However, those algorithms use second-order infor-
mation (i.e. Hessian information) and thus need
access to the weights of the features that are not
used in the current sample, which should slow
down the weight updating process for the same
reason discussed earlier. It would be interesting
to investigate whether those sophisticated learning
scheduling algorithms can actually result in fast
training in large-scale NLP tasks.
6 Conclusion
We have presented a new variant of SGD that can
efficiently train L1-regularized log-linear models.
The algorithm is simple and extremely easy to im-
plement.
We have conducted experiments using CRFs
and three NLP tasks, and demonstrated empiri-
cally that our training algorithm can produce com-
pact and accurate models much more quickly than
a state-of-the-art quasi-Newton method for L1-
regularization.
Acknowledgments
We thank N. Okazaki, N. Yoshinaga, D.
Okanohara and the anonymous reviewers for their
useful comments and suggestions. The work de-
scribed in this paper has been funded by the
Biotechnology and Biological Sciences Research
Council (BBSRC; BB/E004431/1). The research
team is hosted by the JISC/BBSRC/EPSRC spon-
sored National Centre for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
484
Bob Carpenter. 2008. Lazy sparse stochastic gradient
descent for regularized multinomial logistic regres-
sion. Technical report, Alias-i.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of COLING 2004, pages 103?110.
Trevor Cohn and Philip Blunsom. 2005. Semantic role
labeling with tree conditional random fields. In Pro-
ceedings of CoNLL, pages 169?172.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. The Jour-
nal of Machine Learning Research (JMLR), 9:1775?
1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Christian Darken and John Moody. 1990. Note on
learning rate schedules for stochastic optimization.
In Proceedings of NIPS, pages 832?838.
Juhn Duchi and Yoram Singer. 2008. Online and
batch learning using forward-looking subgradients.
In NIPS Workshop: OPT 2008 Optimization for Ma-
chine Learning.
Juhn Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto
the l1-ball for learning in high dimensions. In Pro-
ceedings of ICML, pages 272?279.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Han-Shen Huang, Yu-Ming Chang, and Chun-Nan
Hsu. 2007. Training conditional random fields by
periodic step size adaptation for large-scale text min-
ing. In Proceedings of ICDM, pages 511?516.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of EMNLP
2003.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA), pages
70?75.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. The
Journal of Machine Learning Research (JMLR),
10:777?801.
Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y.
Ng. 2006. Efficient l1 regularized logistic regres-
sion. In Proceedings of AAAI-06, pages 401?408.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In Proceedings
of COLING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In Proceedings of ACL, pages 760?767.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP, pages 145?156.
James C. Spall. 2005. Introduction to Stochastic
Search and Optimization. Wiley-IEEE.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of COLING/ACL, pages 721?728.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL, pages 589?
596.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In Proceedings of ICML, pages
969?976.
Ben Wellner and Marc Vilain. 2006. Leveraging
machine readable dictionaries in discriminative se-
quence models. In Proceedings of LREC 2006.
485
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905?913,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Approach to Abbreviating Terms:
A Discriminative Latent Variable Model with Global Information
Xu Sun?, Naoaki Okazaki?, Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, UK
{sunxu, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
The present paper describes a robust ap-
proach for abbreviating terms. First, in
order to incorporate non-local informa-
tion into abbreviation generation tasks, we
present both implicit and explicit solu-
tions: the latent variable model, or alter-
natively, the label encoding approach with
global information. Although the two ap-
proaches compete with one another, we
demonstrate that these approaches are also
complementary. By combining these two
approaches, experiments revealed that the
proposed abbreviation generator achieved
the best results for both the Chinese and
English languages. Moreover, we directly
apply our generator to perform a very dif-
ferent task from tradition, the abbreviation
recognition. Experiments revealed that the
proposed model worked robustly, and out-
performed five out of six state-of-the-art
abbreviation recognizers.
1 Introduction
Abbreviations represent fully expanded forms
(e.g., hidden markov model) through the use of
shortened forms (e.g., HMM). At the same time,
abbreviations increase the ambiguity in a text.
For example, in computational linguistics, the
acronym HMM stands for hidden markov model,
whereas, in the field of biochemistry, HMM is gen-
erally an abbreviation for heavy meromyosin. As-
sociating abbreviations with their fully expanded
forms is of great importance in various NLP ap-
plications (Pakhomov, 2002; Yu et al, 2006;
HaCohen-Kerner et al, 2008).
The core technology for abbreviation disam-
biguation is to recognize the abbreviation defini-
tions in the actual text. Chang and Schu?tze (2006)
reported that 64,242 new abbreviations were intro-
duced into the biomedical literatures in 2004. As
such, it is important to maintain sense inventories
(lists of abbreviation definitions) that are updated
with the neologisms. In addition, based on the
one-sense-per-discourse assumption, the recogni-
tion of abbreviation definitions assumes senses of
abbreviations that are locally defined in a docu-
ment. Therefore, a number of studies have at-
tempted to model the generation processes of ab-
breviations: e.g., inferring the abbreviating mech-
anism of the hidden markov model into HMM.
An obvious approach is to manually design
rules for abbreviations. Early studies attempted
to determine the generic rules that humans use
to intuitively abbreviate given words (Barrett and
Grems, 1960; Bourne and Ford, 1961). Since
the late 1990s, researchers have presented var-
ious methods by which to extract abbreviation
definitions that appear in actual texts (Taghva
and Gilbreth, 1999; Park and Byrd, 2001; Wren
and Garner, 2002; Schwartz and Hearst, 2003;
Adar, 2004; Ao and Takagi, 2005). For example,
Schwartz and Hearst (2003) implemented a simple
algorithm that mapped all alpha-numerical letters
in an abbreviation to its expanded form, starting
from the end of both the abbreviation and its ex-
panded forms, and moving from right to left.
These studies performed highly, especially for
English abbreviations. However, a more extensive
investigation of abbreviations is needed in order to
further improve definition extraction. In addition,
we cannot simply transfer the knowledge of the
hand-crafted rules from one language to another.
For instance, in English, abbreviation characters
are preferably chosen from the initial and/or cap-
ital characters in their full forms, whereas some
905
p o l y g l y c o l i c a c i dP S S S P S S S S S S S S P S S S [PGA]
??? ? ???S P P S S S P [???]Institute of History and Philology at Academia Sinica(b): Chinese Abbreviation Generation
(a): English Abbreviation Generation    
Figure 1: English (a) and Chinese (b) abbreviation
generation as a sequential labeling problem.
other languages, including Chinese and Japanese,
do not have word boundaries or case sensitivity.
A number of recent studies have investigated
the use of machine learning techniques. Tsuruoka
et al (2005) formalized the processes of abbrevia-
tion generation as a sequence labeling problem. In
the present study, each character in the expanded
form is tagged with a label, y ? {P,S}1, where
the label P produces the current character and
the label S skips the current character. In Fig-
ure 1 (a), the abbreviation PGA is generated from
the full form polyglycolic acid because the under-
lined characters are tagged with P labels. In Fig-
ure 1 (b), the abbreviation is generated using the
2nd and 3rd characters, skipping the subsequent
three characters, and then using the 7th character.
In order to formalize this task as a sequential
labeling problem, we have assumed that the la-
bel of a character is determined by the local in-
formation of the character and its previous label.
However, this assumption is not ideal for model-
ing abbreviations. For example, the model can-
not make use of the number of words in a full
form to determine and generate a suitable num-
ber of letters for the abbreviation. In addition, the
model would be able to recognize the abbreviat-
ing process in Figure 1 (a) more reasonably if it
were able to segment the word polyglycolic into
smaller regions, e.g., poly-glycolic. Even though
humans may use global or non-local information
to abbreviate words, previous studies have not in-
corporated this information into a sequential label-
ing model.
In the present paper, we propose implicit and
explicit solutions for incorporating non-local in-
formation. The implicit solution is based on the
1Although the original paper of Tsuruoka et al (2005) at-
tached case sensitivity information to the P label, for simplic-
ity, we herein omit this information.
y1 y2 ym
xmx2x1
h1 h2 hm
xmx2x1
ymy2y1
CRF DPLVM
Figure 2: CRF vs. DPLVM. Variables x, y, and h
represent observation, label, and latent variables,
respectively.
discriminative probabilistic latent variable model
(DPLVM) in which non-local information is mod-
eled by latent variables. We manually encode non-
local information into the labels in order to provide
an explicit solution. We evaluate the models on the
task of abbreviation generation, in which a model
produces an abbreviation for a given full form. Ex-
perimental results indicate that the proposed mod-
els significantly outperform previous abbreviation
generation studies. In addition, we apply the pro-
posed models to the task of abbreviation recogni-
tion, in which a model extracts the abbreviation
definitions in a given text. To the extent of our
knowledge, this is the first model that can per-
form both abbreviation generation and recognition
at the state-of-the-art level, across different lan-
guages and with a simple feature set.
2 Abbreviator with Non-local
Information
2.1 A Latent Variable Abbreviator
To implicitly incorporate non-local information,
we propose discriminative probabilistic latent
variable models (DPLVMs) (Morency et al, 2007;
Petrov and Klein, 2008) for abbreviating terms.
The DPLVM is a natural extension of the CRF
model (see Figure 2), which is a special case of the
DPLVM, with only one latent variable assigned for
each label. The DPLVM uses latent variables to
capture additional information that may not be ex-
pressed by the observable labels. For example, us-
ing the DPLVM, a possible feature could be ?the
current character xi = X, the label yi = P, and
the latent variable hi = LV.? The non-local infor-
mation can be effectively modeled in the DPLVM,
and the additional information at the previous po-
sition or many of the other positions in the past
could be transferred via the latent variables (see
Figure 2).
906
Using the label set Y = {P,S}, abbreviation
generation is formalized as the task of assigning
a sequence of labels y = y1, y2, . . . , ym for a
given sequence of characters x = x1, x2, . . . , xm
in an expanded form. Each label, yj , is a mem-
ber of the possible labels Y . For each sequence,
we also assume a sequence of latent variables
h = h1, h2, . . . , hm, which are unobservable in
training examples.
We model the conditional probability of the la-
bel sequence P (y|x) using the DPLVM,
P (y|x,?) =
?
h
P (y|h,x,?)P (h|x,?). (1)
Here, ? represents the parameters of the model.
To ensure that the training and inference are ef-
ficient, the model is often restricted to have dis-
jointed sets of latent variables associated with each
label (Morency et al, 2007). Each hj is a member
in a set Hyj of possible latent variables for the la-
bel yj . Here, H is defined as the set of all possi-
ble latent variables, i.e., H is the union of all Hyj
sets. Since the sequences having hj /? Hyj will,
by definition, yield P (y|x,?) = 0, the model is
rewritten as follows (Morency et al, 2007; Petrov
and Klein, 2008):
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?). (2)
Here, P (h|x,?) is defined by the usual formula-
tion of the conditional random field,
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
where f(h,x) represents a feature vector.
Given a training set consisting of n instances,
(xi,yi) (for i = 1 . . . n), we estimate the pa-
rameters ? by maximizing the regularized log-
likelihood,
L(?) =
n?
i=1
logP (yi|xi,?)?R(?). (4)
The first term expresses the conditional log-
likelihood of the training data, and the second term
represents a regularizer that reduces the overfitting
problem in parameter estimation.
2.2 Label Encoding with Global Information
Alternatively, we can design the labels such that
they explicitly incorporate non-local information.
? ? ? ? ? ? ? ? ? ? ? ? ? ?S S P S S S S S S P S P S SS0 S0 P1 S1 S1 S1 S1 S1 S1 P2 S2 P3 S3 S3
Management office of the imports and exports of endangered speciesOrig.GI
Figure 3: Comparison of the proposed label en-
coding method with global information (GI) and
the conventional label encoding method.
In this approach, the label yi at position i at-
taches the information of the abbreviation length
generated by its previous labels, y1, y2, . . . , yi?1.
Figure 3 shows an example of a Chinese abbre-
viation. In this encoding, a label not only con-
tains the produce or skip information, but also the
abbreviation-length information, i.e., the label in-
cludes the number of all P labels preceding the
current position. We refer to this method as label
encoding with global information (hereinafter GI).
The concept of using label encoding to incorporate
non-local information was originally proposed by
Peshkin and Pfeffer (2003).
Note that the model-complexity is increased
only by the increase in the number of labels. Since
the length of the abbreviations is usually quite
short (less than five for Chinese abbreviations and
less than 10 for English abbreviations), the model
is still tractable even when using the GI encoding.
The implicit (DPLVM) and explicit (GI) solu-
tions address the same issue concerning the in-
corporation of non-local information, and there
are advantages to combining these two solutions.
Therefore, we will combine the implicit and ex-
plicit solutions by employing the GI encoding in
the DPLVM (DPLVM+GI). The effects of this
combination will be demonstrated through experi-
ments.
2.3 Feature Design
Next, we design two types of features: language-
independent features and language-specific fea-
tures. Language-independent features can be used
for abbreviating terms in English and Chinese. We
use the features from #1 to #3 listed in Table 1.
Feature templates #4 to #7 in Table 1 are used
for Chinese abbreviations. Templates #4 and #5
express the Pinyin reading of the characters, which
represents a Romanization of the sound. Tem-
plates #6 and #7 are designed to detect character
duplication, because identical characters will nor-
mally be skipped in the abbreviation process. On
907
#1 The input char. xi?1 and xi
#2 Whether xj is a numeral, for j = (i? 3) . . . i
#3 The char. bigrams starting at (i? 2) . . . i
#4 The Pinyin of char. xi?1 and xi
#5 The Pinyin bigrams starting at (i? 2) . . . i
#6 Whether xj = xj+1, for j = (i? 2) . . . i
#7 Whether xj = xj+2, for j = (i? 3) . . . i
#8 Whether xj is uppercase, for j = (i? 3) . . . i
#9 Whether xj is lowercase, for j = (i? 3) . . . i
#10 The char. 3-grams starting at (i? 3) . . . i
#11 The char. 4-grams starting at (i? 4) . . . i
Table 1: Language-independent features (#1 to
#3), Chinese-specific features (#4 through #7), and
English-specific features (#8 through #11).
the other hand, such duplication detection features
are not so useful for English abbreviations.
Feature templates #8?#11 are designed for En-
glish abbreviations. Features #8 and #9 encode the
orthographic information of expanded forms. Fea-
tures #10 and #11 represent a contextual n-gram
with a large window size. Since the number of
letters in Chinese (more than 10K characters) is
much larger than the number of letters in English
(26 letters), in order to avoid a possible overfitting
problem, we did not apply these feature templates
to Chinese abbreviations.
Feature templates are instantiated with values
that occur in positive training examples. We used
all of the instantiated features because we found
that the low-frequency features also improved the
performance.
3 Experiments
For Chinese abbreviation generation, we used the
corpus of Sun et al (2008), which contains 2,914
abbreviation definitions for training, and 729 pairs
for testing. This corpus consists primarily of noun
phrases (38%), organization names (32%), and
verb phrases (21%). For English abbreviation gen-
eration, we evaluated the corpus of Tsuruoka et
al. (2005). This corpus contains 1,200 aligned
pairs extracted from MEDLINE biomedical ab-
stracts (published in 2001). For both tasks, we
converted the aligned pairs of the corpora into la-
beled full forms and used the labeled full forms as
the training/evaluation data.
The evaluation metrics used in the abbreviation
generation are exact-match accuracy (hereinafter
accuracy), including top-1 accuracy, top-2 accu-
racy, and top-3 accuracy. The top-N accuracy rep-
resents the percentage of correct abbreviations that
are covered, if we take the top N candidates from
the ranked labelings of an abbreviation generator.
We implemented the DPLVM in C++ and op-
timized the system to cope with large-scale prob-
lems. We employ the feature templates defined in
Section 2.3, taking into account these 81,827 fea-
tures for the Chinese abbreviation generation task,
and the 50,149 features for the English abbrevia-
tion generation task.
For numerical optimization, we performed a
gradient descent with the Limited-Memory BFGS
(L-BFGS) optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order
Quasi-Newton method that numerically estimates
the curvature from previous gradients and up-
dates. With no requirement on specialized Hes-
sian approximation, L-BFGS can handle large-
scale problems efficiently. Since the objective
function of the DPLVM model is non-convex,
different parameter initializations normally bring
different optimization results. Therefore, to ap-
proach closer to the global optimal point, it is
recommended to perform multiple experiments on
DPLVMs with random initialization and then se-
lect a good start point. To reduce overfitting,
we employed a L2 Gaussian weight prior (Chen
and Rosenfeld, 1999), with the objective function:
L(?) = ?ni=1 logP (yi|xi,?)?||?||2/?2. Dur-
ing training and validation, we set ? = 1 for the
DPLVM generators. We also set four latent vari-
ables for each label, in order to make a compro-
mise between accuracy and efficiency.
Note that, for the label encoding with
global information, many label transitions (e.g.,
P2S3) are actually impossible: the label tran-
sitions are strictly constrained, i.e., yiyi+1 ?
{PjSj,PjPj+1,SjPj+1,SjSj}. These con-
straints on the model topology (forward-backward
lattice) are enforced by giving appropriate features
a weight of ??, thereby forcing all forbidden la-
belings to have zero probability. Sha and Pereira
(2003) originally proposed this concept of imple-
menting transition restrictions.
4 Results and Discussion
4.1 Chinese Abbreviation Generation
First, we present the results of the Chinese abbre-
viation generation task, as listed in Table 2. To
evaluate the impact of using latent variables, we
chose the baseline system as the DPLVM, in which
each label has only one latent variable. Since this
908
Model T1A T2A T3A Time
Heu (S08) 41.6 N/A N/A N/A
HMM (S08) 46.1 N/A N/A N/A
SVM (S08) 62.7 80.4 87.7 1.3 h
CRF 64.5 81.1 88.7 0.2 h
CRF+GI 66.8 82.5 90.0 0.5 h
DPLVM 67.6 83.8 91.3 0.4 h
DPLVM+GI (*) 72.3 87.6 94.9 1.1 h
Table 2: Results of Chinese abbreviation gener-
ation. T1A, T2A, and T3A represent top-1, top-
2, and top-3 accuracy, respectively. The system
marked with the * symbol is the recommended
system.
special case of the DPLVM is exactly the CRF
(see Section 2.1), this case is hereinafter denoted
as the CRF. We compared the performance of the
DPLVM with the CRFs and other baseline sys-
tems, including the heuristic system (Heu), the
HMM model, and the SVM model described in
S08, i.e., Sun et al (2008). The heuristic method
is a simple rule that produces the initial character
of each word to generate the corresponding abbre-
viation. The SVM method described by Sun et al
(2008) is formalized as a regression problem, in
which the abbreviation candidates are scored and
ranked.
The results revealed that the latent variable
model significantly improved the performance
over the CRF model. All of its top-1, top-2,
and top-3 accuracies were consistently better than
those of the CRF model. Therefore, this demon-
strated the effectiveness of using the latent vari-
ables in Chinese abbreviation generation.
As the case for the two alternative approaches
for incorporating non-local information, the la-
tent variable method and the label encoding
method competed with one another (see DPLVM
vs. CRF+GI). The results showed that the la-
tent variable method outperformed the GI encod-
ing method by +0.8% on the top-1 accuracy. The
reason for this could be that the label encoding ap-
proach is a solution without the adaptivity on dif-
ferent instances. We will present a detailed discus-
sion comparing DPLVM and CRF+GI for the En-
glish abbreviation generation task in the next sub-
section, where the difference is more significant.
In contrast, to a larger extent, the results demon-
strate that these two alternative approaches are
complementary. Using the GI encoding further
improved the performance of the DPLVM (with
+4.7% on top-1 accuracy). We found that major
? ? ? ? ? ? ?P S P S P S PP1 S1 P2 S2 S2 S2 P3
State Tobacco Monopoly Administration DPLVM DPLVM+GI ???? [Wrong]??? [Correct]
Figure 4: An example of the results.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1  2  3  4  5  6
P
er
ce
nt
ag
e 
(%
)
Length of Produced Abbr.
Gold Train
Gold Test
DPLVM
DPLVM+GI
Figure 5: Percentage distribution of Chinese
abbreviations/Viterbi-labelings grouped by length.
improvements were achieved through the more ex-
act control of the output length. An example is
shown in Figure 4. The DPLVM made correct de-
cisions at three positions, but failed to control the
abbreviation length.2 The DPLVM+GI succeeded
on this example. To perform a detailed analysis,
we collected the statistics of the length distribution
(see Figure 5) and determined that the GI encod-
ing improved the abbreviation length distribution
of the DPLVM.
In general, the results indicate that all of the se-
quential labeling models outperformed the SVM
regression model with less training time.3 In the
SVM regression approach, a large number of neg-
ative examples are explicitly generated for the
training, which slowed the process.
The proposed method, the latent variable model
with GI encoding, is 9.6% better with respect to
the top-1 accuracy compared to the best system on
this corpus, namely, the SVM regression method.
Furthermore, the top-3 accuracy of the latent vari-
able model with GI encoding is as high as 94.9%,
which is quite encouraging for practical usage.
4.2 English Abbreviation Generation
In the English abbreviation generation task, we
randomly selected 1,481 instances from the gen-
2The Chinese abbreviation with length = 4 should have
a very low probability, e.g., only 0.6% of abbreviations with
length = 4 in this corpus.
3On Intel Dual-Core Xeon 5160/3 GHz CPU, excluding
the time for feature generation and data input/output.
909
Model T1A T2A T3A Time
CRF 55.8 65.1 70.8 0.3 h
CRF+GI 52.7 63.2 68.7 1.3 h
CRF+GIB 56.8 66.1 71.7 1.3 h
DPLVM 57.6 67.4 73.4 0.6 h
DPLVM+GI 53.6 63.2 69.2 2.5 h
DPLVM+GIB (*) 58.3 N/A N/A 3.0 h
Table 3: Results of English abbreviation genera-
tion.
somatosensory evoked potentials
(a) P1P2 P3 P4 P5 SMEPS
(b) P P P P SEPS
(a): CRF+GI with p=0.001 [Wrong]
(b): DPLVM with p=0.191 [Correct]
Figure 6: A result of ?CRF+GI vs. DPLVM?. For
simplicity, the S labels are masked.
eration corpus for training, and 370 instances for
testing. Table 3 shows the experimental results.
We compared the performance of the DPLVM
with the performance of the CRFs. Whereas the
use of the latent variables still significantly im-
proves the generation performance, using the GI
encoding undermined the performance in this task.
In comparing the implicit and explicit solutions
for incorporating non-local information, we can
see that the implicit approach (the DPLVM) per-
forms much better than the explicit approach (the
GI encoding). An example is shown in Figure 6.
The CRF+GI produced a Viterbi labeling with a
low probability, which is an incorrect abbrevia-
tion. The DPLVM produced the correct labeling.
To perform a systematic analysis of the
superior-performance of DPLVM compare to
CRF+GI, we collected the probability distribu-
tions (see Figure 7) of the Viterbi labelings from
these models (?DPLVM vs. CRF+GI? is high-
lighted). The curves suggest that the data sparse-
ness problem could be the reason for the differ-
ences in performance. A large percentage (37.9%)
of the Viterbi labelings from the CRF+GI (ENG)
have very small probability values (p < 0.1).
For the DPLVM (ENG), there were only a few
(0.5%) Viterbi labelings with small probabilities.
Since English abbreviations are often longer than
Chinese abbreviations (length < 10 in English,
whereas length < 5 in Chinese4), using the GI
encoding resulted in a larger label set in English.
4See the curve DPLVM+GI (CHN) in Figure 7, which
could explain the good results of GI encoding for the Chi-
nese task.
 0
 10
 20
 30
 40
 50
 0  0.2  0.4  0.6  0.8  1
P
er
ce
nt
ag
e 
(%
)
Probability of Viterbi labeling
CRF (ENG)
CRF+GI (ENG)
DPLVM (ENG)
DPLVM+GI (ENG)
DPLVM+GI (CHN)
Figure 7: For various models, the probability dis-
tributions of the produced abbreviations on the test
data of the English abbreviation generation task.
mitomycin C
DPLVM P P MC [Wrong]
DPLVM+GI P1 P2 P3 MMC [Correct]
Figure 8: Example of abbreviations composed
of non-initials generated by the DPLVM and the
DPLVM+GI.
Hence, the features become more sparse than in
the Chinese case.5 Therefore, a significant number
of features could have been inadequately trained,
resulting in Viterbi labelings with low probabili-
ties. For the latent variable approach, its curve
demonstrates that it did not cause a severe data
sparseness problem.
The aforementioned analysis also explains the
poor performance of the DPLVM+GI. However,
the DPLVM+GI can actually produce correct ab-
breviations with ?believable? probabilities (high
probabilities) in some ?difficult? instances. In
Figure 8, the DPLVM produced an incorrect la-
beling for the difficult long form, whereas the
DPLVM+GI produced the correct labeling con-
taining non-initials.
Hence, we present a simple voting method to
better combine the latent variable approach with
the GI encoding method. We refer to this new
combination as GI encoding with ?back-off? (here-
inafter GIB): when the abbreviation generated by
the DPLVM+GI has a ?believable? probability
(p > 0.3 in the present case), the DPLVM+GI
then outputs it. Otherwise, the system ?backs-off?
5In addition, the training data of the English task is much
smaller than for the Chinese task, which could make the mod-
els more sensitive to data sparseness.
910
Model T1A Time
CRF+GIB 67.2 0.6 h
DPLVM+GIB (*) 72.5 1.4 h
Table 4: Re-evaluating Chinese abbreviation gen-
eration with GIB.
Model T1A
Heu (T05) 47.3
MEMM (T05) 55.2
DPLVM (*) 57.5
Table 5: Results of English abbreviation genera-
tion with five-fold cross validation.
to the parameters trained without the GI encoding
(i.e., the DPLVM).
The results in Table 3 demonstrate that the
DPVLM+GIB model significantly outperformed
the other models because the DPLVM+GI model
improved the performance in some ?difficult? in-
stances. The DPVLM+GIB model was robust
even when the data sparseness problem was se-
vere.
By re-evaluating the DPLVM+GIB model for
the previous Chinese abbreviation generation task,
we demonstrate that the back-off method also im-
proved the performance of the Chinese abbrevia-
tion generators (+0.2% from DPLVM+GI; see Ta-
ble 4).
Furthermore, for interests, like Tsuruoka et al
(2005), we performed a five-fold cross-validation
on the corpus. Concerning the training time in
the cross validation, we simply chose the DPLVM
for comparison. Table 5 shows the results of the
DPLVM, the heuristic system (Heu), and the max-
imum entropy Markov model (MEMM) described
by Tsuruoka et al (2005).
5 Recognition as a Generation Task
We directly migrate this model to the abbrevia-
tion recognition task. We simplify the abbrevia-
tion recognition to a restricted generation problem
(see Figure 9). When a context expression (CE)
with a parenthetical expression (PE) is met, the
recognizer generates the Viterbi labeling for the
CE, which leads to the PE or NULL. Then, if the
Viterbi labeling leads to the PE, we can, at the
same time, use the labeling to decide the full form
within the CE. Otherwise, NULL indicates that the
PE is not an abbreviation.
For example, in Figure 9, the recognition is re-
stricted to a generation task with five possible la-
... cannulate for arterial pressure (AP)...
(1) P P AP
(2) P P AP
(3) P P AP
(4) P P AP
(5) SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS NULL
Figure 9: Abbreviation recognition as a restricted
generation problem. In some labelings, the S la-
bels are masked for simplicity.
Model P R F
Schwartz & Hearst (SH) 97.8 94.0 95.9
SaRAD 89.1 91.9 90.5
ALICE 96.1 92.0 94.0
Chang & Schu?tze (CS) 94.2 90.0 92.1
Nadeau & Turney (NT) 95.4 87.1 91.0
Okazaki et al (OZ) 97.3 96.9 97.1
CRF 89.8 94.8 92.1
CRF+GI 93.9 97.8 95.9
DPLVM 92.5 97.7 95.1
DPLVM+GI (*) 94.2 98.1 96.1
Table 6: Results of English abbreviation recogni-
tion.
belings. Other labelings are impossible, because
they will generate an abbreviation that is not AP.
If the first or second labeling is generated, AP is
selected as an abbreviation of arterial pressure. If
the third or fourth labeling is generated, then AP
is selected as an abbreviation of cannulate for ar-
terial pressure. Finally, the fifth labeling (NULL)
indicates that AP is not an abbreviation.
To evaluate the recognizer, we use the corpus6
of Okazaki et al (2008), which contains 864 ab-
breviation definitions collected from 1,000 MED-
LINE scientific abstracts. In implementing the
recognizer, we simply use the model from the ab-
breviation generator, with the same feature tem-
plates (31,868 features) and training method; the
major difference is in the restriction (according to
the PE) of the decoding stage and penalizing the
probability values of the NULL labelings7.
For the evaluation metrics, following Okazaki
et al (2008), we use precision (P = k/m), re-
call (R = k/n), and the F-score defined by
6The previous abbreviation generation corpus is improper
for evaluating recognizers, and there is no related research on
this corpus. In addition, there has been no report of Chinese
abbreviation recognition because there is no data available.
The previous generation corpus (Sun et al, 2008) is improper
because it lacks local contexts.
7Due to the data imbalance of the training corpus, we
found the probability values of the NULL labelings are ab-
normally high. To deal with this imbalance problem, we sim-
ply penalize all NULL labelings by using p = p? 0.7.
911
Model P R F
CRF+GIB 94.0 98.9 96.4
DPLVM+GIB 94.5 99.1 96.7
Table 7: English abbreviation recognition with
back-off.
2PR/(P + R), where k represents #instances in
which the system extracts correct full forms, m
represents #instances in which the system extracts
the full forms regardless of correctness, and n rep-
resents #instances that have annotated full forms.
Following Okazaki et al (2008), we perform 10-
fold cross validation.
We prepared six state-of-the-art abbreviation
recognizers as baselines: Schwartz and Hearst?s
method (SH) (2003), SaRAD (Adar, 2004), AL-
ICE (Ao and Takagi, 2005), Chang and Schu?tze?s
method (CS) (Chang and Schu?tze, 2006), Nadeau
and Turney?s method (NT) (Nadeau and Turney,
2005), and Okazaki et al?s method (OZ) (Okazaki
et al, 2008). Some methods use implementations
on the web, including SH8, CS9, and ALICE10.
The results of other methods, such as SaRAD, NT,
and OZ, are reproduced for this corpus based on
their papers (Okazaki et al, 2008).
As can be seen in Table 6, using the latent vari-
ables significantly improved the performance (see
DPLVM vs. CRF), and using the GI encoding
improved the performance of both the DPLVM
and the CRF. With the F-score of 96.1%, the
DPLVM+GI model outperformed five of six state-
of-the-art abbreviation recognizers. Note that all
of the six systems were specifically designed and
optimized for this recognition task, whereas the
proposed model is directly transported from the
generation task. Compared with the generation
task, we find that the F-measure of the abbrevia-
tion recognition task is much higher. The major
reason for this is that there are far fewer classifi-
cation candidates of the abbreviation recognition
problem, as compared to the generation problem.
For interests, we also tested the effect of the
GIB approach. Table 7 shows that the back-off
method further improved the performance of both
the DPLVM and the CRF model.
8http://biotext.berkeley.edu/software.html
9http://abbreviation.stanford.edu/
10http://uvdb3.hgc.jp/ALICE/ALICE index.html
6 Conclusions and Future Research
We have presented the DPLVM and GI encod-
ing by which to incorporate non-local information
in abbreviating terms. They were competing and
generally the performance of the DPLVM was su-
perior. On the other hand, we showed that the two
approaches were complementary. By combining
these approaches, we were able to achieve state-
of-the-art performance in abbreviation generation
and recognition in the same model, across differ-
ent languages, and with a simple feature set. As
discussed earlier herein, the training data is rela-
tively small. Since there are numerous unlabeled
full forms on the web, it is possible to use a semi-
supervised approach in order to make use of such
raw data. This is an area for future research.
Acknowledgments
We thank Yoshimasa Tsuruoka for providing the
English abbreviation generation corpus. We also
thank the anonymous reviewers who gave help-
ful comments. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan).
References
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527?
533.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
June A. Barrett and Mandalay Grems. 1960. Abbrevi-
ating words systematically. Communications of the
ACM, 3(5):323?324.
Charles P. Bourne and Donald F. Ford. 1961. A study
of methods for systematically abbreviating english
words and names. Journal of the ACM, 8(4):538?
552.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In Sophia Ananiadou
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. In Proceedings of ACL?08: HLT, Short
Papers, pages 61?64, June.
912
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR?07, pages 1?8.
David Nadeau and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intelli-
gence (AI?2005) (LNAI 3501), page 10 pages.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2008. A discriminative alignment model for
abbreviation recognition. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08), pages 657?664, Manch-
ester, UK.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
ACL?02, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP?01, pages 126?133.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian in-
formation extraction network. In Proceedings of IJ-
CAI?03, pages 421?426.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings
of NIPS?08.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB?03), pages 451?462.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL?03.
Xu Sun, Houfeng Wang, and Bo Wang. 2008. Pre-
dicting chinese abbreviations from definitions: An
empirical learning approach using support vector re-
gression. Journal of Computer Science and Tech-
nology, 23(4):602?611.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191?198.
Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun?ichi
Tsujii. 2005. A machine learning approach to
acronym generation. In Proceedings of the ACL-
ISMB Workshop, pages 25?31.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426?434.
Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
913
BioNLP 2007: Biological, translational, and clinical language processing, pages 209?216,
Prague, June 2007. c?2007 Association for Computational Linguistics
Reranking for Biomedical Named-Entity Recognition
Kazuhiro Yoshida? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{kyoshida, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper investigates improvement of au-
tomatic biomedical named-entity recogni-
tion by applying a reranking method to the
COLING 2004 JNLPBA shared task of bio-
entity recognition. Our system has a com-
mon reranking architecture that consists of a
pipeline of two statistical classifiers which
are based on log-linear models. The ar-
chitecture enables the reranker to take ad-
vantage of features which are globally de-
pendent on the label sequences, and fea-
tures from the labels of other sentences than
the target sentence. The experimental re-
sults show that our system achieves the la-
beling accuracies that are comparable to the
best performance reported for the same task,
thanks to the 1.55 points of F-score improve-
ment by the reranker.
1 Introduction
Difficulty and potential application of biomedical
named-entity recognition has attracted many re-
searchers of both natural language processing and
bioinformatics. The difficulty of the task largely
stems from a wide variety of named entity expres-
sions used in the domain. It is common for practi-
cal protein or gene databases to contain hundreds of
thousands of items. Such a large variety of vocab-
ulary naturally leads to long names with productive
use of general words, making the task difficult to be
solved by systems with naive Markov assumption of
label sequences, because such systems must perform
their prediction without seeing the entire string of
the entities.
Importance of the treatment of long names might
be implicitly indicated in the performance com-
parison of the participants of JNLPBA shared
task (Kim et al, 2004), where the best perform-
ing system (Zhou and Su, 2004) attains their scores
by extensive post-processing, which enabled the
system to make use of global information of the
entity labels. After the shared task, many re-
searchers tackled the task by using conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), which
seemed to promise improvement over locally opti-
mized models like maximum entropy Markov mod-
els (MEMMs) (McCallum et al, 2000). However,
many of the CRF systems developed after the shared
task failed to reach the best performance achieved
by Zhou et al One of the reasons may be the defi-
ciency of the dynamic programming-based systems,
that the global information of sequences cannot be
incorporated as features of the models. Another rea-
son may be that the computational complexity of
the models prevented the developers to invent ef-
fective features for the task. We had to wait until
Tsai et al (2006), who combine pattern-based post-
processing with CRFs, for CRF-based systems to
achieve the same level of performance as Zhou et al
As such, a key to further improvement of the perfor-
mance of bio-entity recognition has been to employ
global features, which are effective to capture the
features of long names appearing in the bio domain.
In this paper, we use reranking architecture,
which was successfully applied to the task of nat-
ural language parsing (Collins, 2000; Charniak and
209
Johnson, 2005), to address the problem. Reranking
enables us to incorporate truly global features to the
model of named entity tagging, and we aim to real-
ize the state-of-the-art performance without depend-
ing on rule-based post-processes.
Use of global features in named-entity recogni-
tion systems is widely studied for sequence labeling
including general named-entity tasks like CoNLL
2003 shared task. Such systems may be classified
into two kinds, one of them uses a single classifier
which is optimized incorporating non-local features,
and the other consists of pipeline of more than one
classifiers. The former includes Relational Markov
Networks by Bunescu et al (2004) and skip-edge
CRFs by Sutton et al (2004). A major drawback
of this kind of systems may be heavy computational
cost of inference both for training and running the
systems, because non-local dependency forces such
models to use expensive approximate inference in-
stead of dynamic-programming-based exact infer-
ence. The latter, pipelined systems include a re-
cent study by Krishnan et al (2006), as well as
our reranking system. Their method is a two stage
model of CRFs, where the second CRF uses the
global information of the output of the first CRF.
Though their method is effective in capturing var-
ious non-local dependencies of named entities like
consistency of labels, we may be allowed to claim
that reranking is likely to be more effective in bio-
entity tagging, where the treatment of long entity
names is also a problem.
This paper is organized as follows. First, we
briefly overview the JNLPBA shared task of bio-
entity recognition and its related work. Then we ex-
plain the components of our system, one of which is
an MEMM n-best tagger, and the other is a reranker
based on log-linear models. Then we show the ex-
periments to tune the performance of the system us-
ing the development set. Finally, we compare our
results with the existing systems, and conclude the
paper with the discussion for further improvement
of the system.
2 JNLPBA shared task and related work
This section overviews the task of biomedical named
entity recognition as presented in JNLPBA shared
task held at COLING 2004, and the systems that
were successfully applied to the task. The train-
ing data provided by the shared task consisted of
2000 abstracts of biomedical articles taken from the
GENIA corpus version 3 (Ohta et al, 2002), which
consists of the MEDLINE abstracts with publication
years from 1990 to 1999. The articles are annotated
with named-entity BIO tags as an example shown in
Table 1. As usual, ?B? and ?I? tags are for beginning
and internal words of named entities, and ?O? tags
are for general English words that are not named en-
tities. ?B? and ?I? tags are split into 5 sub-labels,
each of which are used to represent proteins, genes,
cell lines, DNAs, cell types, and RNAs. The test
set of the shared task consists of 404 MEDLINE ab-
stracts whose publication years range from 1978 to
2001. The difference of publication years between
the training and test sets reflects the organizer?s in-
tention to see the entity recognizers? portability with
regard to the differences of the articles? publication
years.
Kim et al (Kim et al, 2004) compare the 8 sys-
tems participated in the shared task. The systems
use various classification models including CRFs,
hidden Markov models (HMMs), support vector ma-
chines (SVMs), and MEMMs, with various features
and external resources. Though it is impossible to
observe clear correlation between the performance
and classification models or resources used, an im-
portant characteristic of the best system by Zhou et
al. (2004) seems to be extensive use of rule-based
post processing they apply to the output of their clas-
sifier.
After the shared task, several researchers tack-
led the problem using the CRFs and their ex-
tensions. Okanohara et al (2006) applied semi-
CRFs (Sarawagi and Cohen, 2004), which can treat
multiple words as corresponding to a single state.
Friedrich et al (2006) used CRFs with features from
the external gazetteer. Current state-of-the-art for
the shared-task is achieved by Tsai et al (2006),
whose improvement depends on careful design of
features including the normalization of numeric ex-
pressions, and use of post-processing by automati-
cally extracted patterns.
210
IL-2 gene expression requires reactive oxygen production by 5-lipoxygenase .
B-DNA I-DNA O O O O O O B-protein O
Figure 1: Example sentence from the training data.
State name Possible next state
BOS B-* or O
B-protein I-protein, B-* or O
B-cell type I-cell type, B-* or O
B-DNA I-DNA, B-* or O
B-cell line I-cell line, B-* or O
B-RNA I-RNA, B-* or O
I-protein I-protein, B-* or O
I-cell type I-cell type, B-* or O
I-DNA I-DNA, B-* or O
I-cell line I-cell line, B-* or O
I-RNA I-RNA, B-* or O
O B-* or O
Table 1: State transition of MEMM.
3 N-best MEMM tagger
As our n-best tagger, we use a first order MEMM
model (McCallum et al, 2000). Though CRFs (Laf-
ferty et al, 2001) can be regarded as improved ver-
sion of MEMMs, we have chosen MEMMs because
MEMMs are usually much faster to train compared
to CRFs, which enables extensive feature selection.
Training a CRF tagger with features selected us-
ing an MEMM may result in yet another perfor-
mance boost, but in this paper we concentrate on the
MEMM as our n-best tagger, and consider CRFs as
one of our future extensions.
Table 1 shows the state transition table of our
MEMM model. Though existing studies suggest
that changing the tag set of the original corpus, such
as splitting of O tags, can contribute to the perfor-
mances of named entity recognizers (Peshkin and
Pfefer, 2003), our system uses the original tagset
of the training data, except that the ?BOS? label is
added to represent the state before the beginning of
sentences.
Probability of state transition to the i-th label of a
sentence is calculated by the following formula:
P (li|li?1, S) =
exp(?j ?jfj(li, li?1, S))
?
l exp(
?
j ?jfj(l, li?1, S))
. (1)
Features used Forward tagging Backward tagging
unigrams, bi-
grams and pre-
vious labels
(62.43/71.77/66.78) (66.02/74.73/70.10)
unigrams and
bigrams (61.64/71.73/66.30) (65.38/74.87/69.80)
unigrams and
previous labels (62.17/71.67/66.58) (65.59/74.77/69.88)
unigrams (61.31/71.81/66.15) (65.61/75.25/70.10)
Table 2: (Recall/Precision/F-score) of forward and
backward tagging.
where li is the next BIO tag, li?1 is the previous
BIO tag, S is the target sentence, and fj and lj
are feature functions and parameters of a log-linear
model (Berger et al, 1996). As a first order MEMM,
the probability of a label li is dependent on the pre-
vious label li?1, and when we calculate the normal-
ization constant in the right hand side (i.e. the de-
nominator of the fraction), we limit the range of l to
the possible successors of the previous label. This
probability is multiplied to obtain the probability of
a label sequence for a sentence:
P (l1...n|S) =
?
i
P (li|li?1). (2)
The probability in Eq. 1. is estimated as a single
log-linear model, regardless to the types of the target
labels.
N-best tag sequences of input sentences are ob-
tained by well-known combination of the Viterbi al-
gorithm and A* algorithm. We implemented two
methods for thresholding the best sequences: N -
best takes the sequences whose ranks are higher than
N , and ?-best takes the sequences that have proba-
bility higher than that of the best sequences with a
factor ?, where ? is a real value between 0 and 1. The
?-best method is used in combination with N -best to
limit the maximum number of selected sequences.
3.1 Backward tagging
There remains one significant choice when we de-
velop an MEMM tagger, that is, the direction of tag-
ging. The results of the preliminary experiment with
211
forward and backward MEMMs with word unigram
and bigram features are shown in Table 2. (The eval-
uation is done using the same training and develop-
ment set as used in Section 5.) As can be seen, the
backward tagging outperformed forward tagging by
a margin larger than 3 points, in all the cases.
One of the reasons of these striking differences
may be long names which appear in biomedical
texts. In order to recognize long entity names, for-
ward tagging is preferable if we have strong clues of
entities which appear around their left boundaries,
and backward tagging is preferable if clues appear
at right boundaries. A common example of this ef-
fect is a gene expression like ?XXX YYY gene.? The
right boundary of this expression is easy to detect
because of the word ?gene.? For a backward tagger,
the remaining decision is only ?where to stop? the
entity. But a forward tagger must decide not only
?where to start,? but also ?whether to start? the en-
tity, before the tagger encounter the word ?gene.? In
biomedical named-entity tagging, right boundaries
are usually easier to detect, and it may be the reason
of the superiority of the backward tagging.
We could have partially alleviated this effect by
employing head-word triggers as done in Zhou et
al. (2004), but we decided to use backward tag-
ging because the results of a number of preliminary
experiments, including the ones shown in Table 2
above, seemed to be showing that the backward tag-
ging is preferable in this task setting.
3.2 Feature set
In our system, features of log-linear models are gen-
erated by concatenating (or combining) the ?atomic?
features, which belong to their corresponding atomic
feature classes. Feature selection is done by de-
ciding whether to include combination of feature
classes into the model. We ensure that features in the
same atomic feature class do not co-occur, so that a
single feature-class combination generates only one
feature for each event. The following is a list of
atomic feature classes implemented in our system.
Label features The target and previous labels. We
also include the coarse-grained label distinction to
distinguish five ?I? labels of each entity classes from
the other labels, expecting smoothing effect.
Word-based features Surface strings, base forms,
parts-of-speech (POSs), word shapes1, suffixes and
prefixes of words in input sentence. These features
are extracted from five words around the word to be
tagged, and also from the words around NP-chunk
boundaries as explained bellow.
Chunk-based features Features dependent on the
output of shallow parser. Word-based features of
the beginning and end of noun phrases, and the dis-
tances of the target word from the beginning and end
of noun phrases are used.
4 Reranker
Our reranker is based on a log-linear classifier.
Given n-best tag sequences Li(1 ? i ? n), a log-
linear model is used to estimate the probability
P (Li|S) =
exp(?j ?jfj(Li, S))
?
k exp(
?
j ?jfj(Lk, S))
. (3)
From the n-best sequences, reranker selects a se-
quence which maximize this probability.
The features used by the reranker are explained in
the following sections. Though most of the features
are binary-valued (i.e. the value of fj in Eq. 3. is
exclusively 1 or 0), the logarithm of the probability
of the sequence output by the n-best tagger is also
used as a real-valued feature, to ensure the reranker?s
improvement over the n-best tagger.
4.1 Basic features
Basic features of the reranker are straightforward ex-
tension of the features used in the MEMM tagger.
The difference is that we do not have to care the lo-
cality of the features with regard to the labels.
Characteristics of words that are listed as word-
based features in the previous section is also used
for the reranker. Such features are chiefly extracted
from around the left and right boundaries of entities.
In our experiments, we used five words around the
leftmost and rightmost words of the entities. We also
use the entire string, affixes, word shape, concatena-
tion of POSs, and length of entities. Some of our
1The shape of a word is defined as a sequence of character
types contained in the word. Character types include uppercase
letters, lowercase letters, numerics, space characters, and the
other symbols.
212
features depend on two adjacent entities. Such fea-
tures include the word-based features of the words
between the entities, and the verbs between the en-
tities. Most of the features are used in combination
with entity types.
4.2 N-best distribution features
N-best tags of sentences other than the target sen-
tence is available to the rerankers. This information
is sometimes useful for recognizing the names in
the target sentence. For example, proteins are often
written as ?XXX protein? where XXX is a protein
name, especially when they are first introduced in an
article, and thereafter referred to simply as ?XXX.?
In such cases, the first appearance is easily identified
as proteins only by local features, but the subsequent
ones might not, and the information of the first ap-
pearance can be effectively used to identify the other
appearances.
Our system uses the distribution of the tags of
the 20 neighboring sentences of the target sentence
to help the tagging of the target sentence. Tag
distributions are obtained by marginalizing the n-
best tag sequences. Example of an effective feature
is a binary-valued feature which becomes 1 when
the candidate entity names in the target sentence is
contained in the marginal distribution of the neigh-
boring sentences with a probability which is above
some threshold.
We also use the information of overlapping
named-entity candidates which appear in the target
sentence. When there is an overlap between the en-
tities in the target sequence and any of the named-
entity candidates in the marginal distribution of the
target sentence, the corresponding features are used
to indicate the existence of the overlapping entity
and its entity type.
5 Experiments
We evaluated the performance of the system on the
data set provided by the COLING 2004 JNLPBA
shared-task. which consists of 2000 abstracts from
the MEDLINE articles. GENIA tagger 2, a biomed-
ical text processing tool which automatically anno-
2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/. The
tagger is trained on the GENIA corpus, so it is likely to show
very good performance on both training and development sets,
but not on the test set.
Features used (Recall/Precision/F-score)
full set (73.90/77.58/75.69)
w/o shallow parser (72.63/76.35/74.44)
w/o previous labels (72.06/75.38/73.68)
Table 3: Performance of MEMM tagger.
tates POS tags, shallow parses and named-entity tags
is used to preprocess the corpus, and POS and shal-
low parse information is used in our experiments.
We divided the data into 20 contiguous and
equally-sized sections, and used the first 18 sec-
tions for training, and the last 2 sections for testing
while development (henceforth the training and de-
velopment sets, respectively). The training data of
the reranker is created by the n-best tagger, and ev-
ery set of 17 sections from the training set is used
to train the n-best tagger for the remaining section
(The same technique is used by previous studies
to avoid the n-best tagger?s ?unrealistically good?
performance on the training set (Collins, 2000)).
Among the n-best sequences output by the MEMM
tagger, the sequence with the highest F-score is used
as the ?correct? sequence for training the reranker.
The two log-linear models for the MEMM tagger
and reranker are estimated using a limited-memory
BFGS algorithm implemented in an open-source
software Amis3. In both models, Gaussian prior dis-
tributions are used to avoid overfitting (Chen and
Rosenfeld, 1999), and the standard deviations of the
Gaussian distributions are optimized to maximize
the performance on the development set. We also
used a thresholding technique which discards fea-
tures with low frequency. This is also optimized us-
ing the development set, and the best threshold was
4 for the MEMM tagger, and 50 for the reranker 4.
For both of the MEMM tagger and reranker, com-
binations of feature classes are manually selected to
improve the accuracies on the development set. Our
final models include 49 and 148 feature class combi-
nations for the MEMM tagger and reranker, respec-
tively.
Table 3 shows the performance of the MEMM
tagger on the development set. As reported in many
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/.
4We treated feature occurrences both in positive and nega-
tive examples as one occurrence.
213
Features used (Recall/Precision/F-score)
oracle (94.62/96.07/95.34)
full set (75.46/78.85/77.12)
w/o features that
depend on two
entities
(74.67/77.99/76.29)
w/o n-best distribu-
tion features
(74.99/78.38/76.65)
baseline (73.90/77.58/75.69)
Table 4: Performance of the reranker.
of the previous studies (Kim et al, 2004; Okanohara
et al, 2006; Tzong-Han Tsai et al, 2006), features of
shallow parsers had a large contribution to the per-
formance. The information of the previous labels
was also quite effective, which indicates that label
unigram models (i.e. 0th order Markov models, so
to speak) would have been insufficient for good per-
formance.
Then we developed the reranker, using the results
of 50-best taggers as training data. Table 4 shows the
performance of the reranker pipelined with the 50-
best MEMM tagger, where the ?oracle? row shows
the upper bound of reranker performance. Here, we
can observe that the reranker successfully improved
the performance by 1.43 points from the baseline
(i.e. the one-best of the MEMM tagger). It is also
shown that the global features that depend on two
adjacent entities, and the n-best distribution features
from the outside of the target sentences, are both
contributing to this performance improvement.
We also conducted experimental comparison of
two thresholding methods which are described in
Section 3. Since we can train and test the reranker
with MEMM taggers that use different thresholding
methods, we could make a table of the performance
of the reranker, changing the MEMM tagger used
for both training and evaluation5.
Tables 5 and 6 show the F-scores obtained by
various MEMM taggers, where the ?oracle? column
again shows the performance upper bound. (All
of the ?-best methods are combined with 200-best
thresholding.) Though we can roughly state that the
reranker can work better with n-best taggers which
5These results might not be a fair comparison, because the
feature selection and hyper-parameter tuning are done using a
reranker which is trained and tested with a 50-best tagger.
are more ambiguous than those used for their train-
ing, the differences are so slight to see clear ten-
dencies (For example, the columns for the reranker
trained using the 10-best MEMM tagger seems to be
a counter example against the statement).
We may also be able to say that the ?-best meth-
ods are generally performing slightly better, and it
could be explained by the fact that we have bet-
ter oracle performance with less ambiguity in ?-best
methods.
However, the scores in the column corresponding
to the 50-best training seems to be as high as any of
the scores of the ?-best methods, and the best score
is also achieved in that column. The reason may be
because our performance tuning is done exclusively
using the 50-best-trained reranker. Though we could
have achieved better performance by doing feature
selection and hyper-parameter tuning again using ?-
best MEMMs, we use the reranker trained on 50-
best tags run with 70-best MEMM tagger as the best
performing system in the following.
5.1 Comparison with existing systems
Table 7 shows the performance of our n-best tag-
ger and reranker on the official test set, and the best
reported results on the same task. As naturally ex-
pected, our system outperformed the systems that
cannot accommodate truly global features (Note that
one point of F-score improvement is valuable in this
task, because inter-annotator agreement rate of hu-
man experts in bio-entity recognition is likely to be
about 80%. For example, Krauthammer et al (2004)
report the inter-annotater agreement rate of 77.6%
for the three way bio-entity classification task.) and
the performance can be said to be at the same level as
the best systems. However, in spite of our effort, our
system could not outperform the best result achieved
by Tsai et al What makes Tsai et al?s system per-
form better than ours might be the careful treatment
of numeric expressions.
It is also notable that our MEMM tagger scored
71.10, which is comparable to the results of the sys-
tems that use CRFs. Considering the fact that the
tagger?s architecture is a simple first-order MEMM
which is far from state-of-the-art, and it uses only
POS taggers and shallow parsers as external re-
sources, we can say that simple machine-learning-
based method with carefully selected features could
214
Thresholding method for training
Thresholding
method for
testing
oracle avg. # of an-
swers
10-best 20-best 30-best 40-best 50-best 70-best 100-best
10-best 91.00 10 76.51 76.53 76.85 76.73 77.01 76.68 76.86
20-best 93.31 20 76.40 76.55 76.83 76.62 76.95 76.68 76.85
30-best 94.40 30 76.34 76.52 76.91 76.63 77.06 76.75 76.90
40-best 94.94 40 76.39 76.58 76.91 76.71 77.14 76.75 76.92
50-best 95.34 50 76.37 76.58 76.90 76.65 77.12 76.78 76.92
70-best 95.87 60 76.38 76.57 76.91 76.71 77.16 76.81 76.97
100-best 96.26 70 76.38 76.59 76.95 76.74 77.10 76.82 76.98
Table 5: Comparison of the F-scores of rerankers trained and evaluated with various N -best taggers.
Thresholding method for training
Thresholding
method for
testing
oracle
avg. #
of an-
swers
0.05-best 0.02-best 0.008-best 0.004-best 0.002-best 0.0005-best 0.0002-best
0.05-best 91.65 10.7 76.70 76.80 76.93 76.64 77.02 76.78 76.52
0.02-best 93.45 17.7 76.79 76.91 77.07 76.79 77.09 76.89 76.70
0.008-best 94.81 27.7 76.79 77.01 77.05 76.80 77.14 76.88 76.73
0.004-best 95.55 37.5 76.79 76.98 76.97 76.74 77.12 76.86 76.71
0.002-best 96.09 49.3 76.79 76.98 76.96 76.73 77.13 76.85 76.72
0.0005-best 96.82 77.7 76.79 76.98 76.96 76.73 77.13 76.85 76.70
0.0002-best 97.04 99.2 76.83 77.01 76.96 76.71 77.13 76.88 76.70
Table 6: Comparison of the F-scores of rerankers trained and evaluated with various ?-best taggers.
F-score Method
71.10 MEMMThis paper
72.65 reranking
Tsai et al (2006) 72.98 CRF, post-processing
Zhou et al (2004) 72.55
HMM,
SVM, post-
processing,
gazetteer
Friedrich et al (2006) 71.5 CRF,gazetteer
Okanohara et al (2006) 71.48 semi-CRF
Table 7: Performance comparison on the test set.
be sufficient practical solutions for this kind of tasks.
6 Conclusion
This paper showed that the named-entity recogni-
tion, which have usually been solved by dynamic-
programming-based sequence-labeling techniques
with local features, can have innegligible perfor-
mance improvement from reranking methods. Our
system showed clear improvement over many of the
machine-learning-based systems reported to date,
and also proved comparable to the existing state-of-
the-art systems that use rule-based post-processing.
Our future plans include further sophistication of
features, such as the use of external gazetteers which
is reported to improve the F-score by 1.0 and 2.7
points in (Zhou and Su, 2004) and (Friedrich et
al., 2006), respectively. We expect that reranking
architecture can readily accommodate dictionary-
based features, because we can apply elaborated
string-matching algorithms to the qualified candi-
date strings available at reranking phase.
We also plan to apply self-training of n-best tag-
ger which successfully boosted the performance
of one of the best existing English syntactic
parser (McClosky et al, 2006). Since the test data of
the shared-task consists of articles that represent the
different publication years, the effects of the publi-
cation years of the texts used for self-training would
be interesting to study.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
215
to Natural Language Processing. Computational Lin-
guistics, 22(1).
R. Bunescu and R. Mooney. 2004. Relational markov
networks for collective information extraction. In Pro-
ceedings of ICML 2004.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL 2005.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. In Technical
Report CMUCS.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of 17th In-
ternational Conference on Machine Learning, pages
175?182. Morgan Kaufmann, San Francisco, CA.
Christoph M. Friedrich, Thomas Revillion, Martin Hof-
mann, and Juliane Fluck. 2006. Biomedical and
Chemical Named Entity Recognition with Conditional
Random Fields: The Advantage of Dictionary Fea-
tures. In Proceedings of the Second International Sym-
posium on Semantic Mining in Biomedicine.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the Bio-Entity Recognition Task at JNLPBA. In
Proceedings of the International Workshop on Natu-
ral Language Processing in Biomedicine and its Appli-
cations (JNLPBA-04), pages 70?75, Geneva, Switzer-
land.
Michael Krauthammer and Goran Nenadic. 2004. Term
identification in the biomedical literature. Journal of
Biomedical Informatics, 37(6).
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In Pro-
ceedings of ACL 2006.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of 18th International Conference on Ma-
chine Learning, pages 282?289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Models for
Information Extraction and Segmentation. In ICML
2000.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
NAACL 2006.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA Corpus: an Annotated Research
Abstract Corpus in Molecular Biology Domain. In
Proceedings of the Human Language Technology Con-
ference (HLT 2002), March.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the Scala-
bility of Semi-Markov Conditional Random Fields for
Named Entity Recognition. In Proceedings of ACL
2006, Sydney, Australia, July.
Leonid Peshkin and Avi Pfefer. 2003. Bayesian Infor-
mation Extraction Network. In Proceedings of the
Eighteenth International Joint Conf. on Artificial In-
telligence.
S. Sarawagi and W. Cohen. 2004. Semimarkov con-
ditional random fields for information extraction. In
Proceedings of ICML 2004.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive Segmentation and Labeling of Distant Entities in
Information Extraction. Technical report, University
of Massachusetts. Presented at ICML Workshop on
Statistical Relational Learning and Its Connections to
Other Fields.
Richard Tzong-Han Tsai, Cheng-Lung Sung, Hong-Jie
Dai, Hsieh-Chuan Hung, Ting-Yi Sung, and Wen-Lian
Hsu. 2006. NERBio: using selected word conjunc-
tions, term normalization, and global patterns to im-
prove biomedical named entity recognition. In BMC
Bioinformatics 2006, 7(Suppl 5):S11.
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recognition.
In Proceedings of the International Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-04), pages 96?99.
216
Proceedings of the 10th Conference on Parsing Technologies, pages 11?22,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Evaluating Impact of Re-training a Lexical Disambiguation Model
on Domain Adaptation of an HPSG Parser
Tadayoshi Hara1 Yusuke Miyao1 Jun?ichi Tsujii1;2;3
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
2School of Computer Science, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
3NaCTeM(National Center for Text Mining)
Manchester Interdisciplinary Biocentre, University of Manchester
131 Princess St, MANCHESTER M1 7DN, UK
E-mail: fharasan, yusuke, tsujiig@is.s.u-tokyo.ac.jp
Abstract
This paper describes an effective approach
to adapting an HPSG parser trained on the
Penn Treebank to a biomedical domain. In
this approach, we train probabilities of lex-
ical entry assignments to words in a tar-
get domain and then incorporate them into
the original parser. Experimental results
show that this method can obtain higher
parsing accuracy than previous work on do-
main adaptation for parsing the same data.
Moreover, the results show that the combi-
nation of the proposed method and the exist-
ing method achieves parsing accuracy that is
as high as that of an HPSG parser retrained
from scratch, but with much lower training
cost. We also evaluated our method in the
Brown corpus to show the portability of our
approach in another domain.
1 Introduction
Domain portability is an important aspect of the ap-
plicability of NLP tools to practical tasks. There-
fore, domain adaptation methods have recently been
proposed in several NLP areas, e.g., word sense dis-
ambiguation (Chan and Ng, 2006), statistical pars-
ing (Lease and Charniak, 2005; McClosky et al,
2006), and lexicalized-grammar parsing (Johnson
and Riezler, 2000; Hara et al, 2005). Their aim was
to re-train a probabilistic model for a new domain at
low cost, and more or less successfully improved the
accuracy for the domain.
In this paper, we propose a method for adapting
an HPSG parser (Miyao and Tsujii, 2002; Ninomiya
et al, 2006) trained on the WSJ section of the Penn
Treebank (Marcus et al, 1994) to a biomedical do-
main. Our method re-trains a probabilistic model of
lexical entry assignments to words in a target do-
main, and incorporates it into the original parser.
The model of lexical entry assignments is a log-
linear model re-trained with machine learning fea-
tures only of word n-grams. Hence, the cost for the
re-training is much lower than the cost of training
the entire disambiguation model from scratch.
In the experiments, we used an HPSG parser orig-
inally trained with the Penn Treebank, and evaluated
a disambiguation model re-trained with the GENIA
treebank (Kim et al, 2003), which consists of ab-
stracts of biomedical papers. We varied the size of
a training corpus, and measured the transition of the
parsing accuracy and the cost required for parameter
estimation. For comparison, we also examined other
possible approaches to adapting the same parser. In
addition, we applied our approach to the Brown cor-
pus (Kucera and Francis, 1967) in order to examine
portability of our approach.
The experimental results revealed that by sim-
ply re-training the probabilistic model of lexical en-
try assignments we achieve higher parsing accuracy
than with a previously proposed adaptation method.
In addition, combined with the existing adaptation
method, our approach achieves accuracy as high as
that obtained by re-training the original parser from
scratch, but with much lower training cost. In this
paper, we report these experimental results in detail,
and discuss how disambiguation models of lexical
entry assignments contribute to domain adaptation.
In recent years, it has been shown that lexical in-
11
formation plays a very important role for high accu-
racy of lexicalized grammar parsing. Bangalore and
Joshi (1999) indicated that, correct disambiguation
with supertagging, i.e., assignment of lexical entries
before parsing, enabled effective LTAG (Lexical-
ized Tree-Adjoining Grammar) parsing. Clark and
Curran (2004a) showed that supertagging reduced
cost for training and execution of a CCG (Combina-
tory Categorial Grammar) parser while keeping ac-
curacy. Clark and Curran (2006) showed that a CCG
parser trained on data derived from lexical category
sequences alone was only slightly less accurate than
one trained on complete dependency structures. Ni-
nomiya et al (2006) also succeeded in significantly
improving speed and accuracy of HPSG parsing by
using supertagging probabilities. These results indi-
cate that the probability of lexical entry assignments
is essential for parse disambiguation.
Such usefulness of lexical information has also
been shown for domain adaptation methods. Lease
and Charniak (2005) showed how existing domain-
specific lexical resources on a target domain may be
leveraged to augment PTB-training: part-of-speech
tags, dictionary collocations, and named-entities.
Our findings basically follow the above results. The
contribution of this paper is to provide empirical re-
sults of the relationships among domain variation,
probability of lexical entry assignment, training data
size, and training cost. In particular, this paper em-
pirically shows how much in-domain corpus is re-
quired for satisfiable performance.
In Section 2, we introduce an HPSG parser and
describe an existing method for domain adaptation.
In Section 3, we show our methods of re-training
a lexical disambiguation model and incorporating
it into the original model. In Section 4, we exam-
ine our method through experiments on the GENIA
treebank. In Section 5, we examine the portability
of our method through experiments on the Brown
corpus. In Section 6, we showed several recent re-
searches related to domain adaptation.
2 An HPSG Parser
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of grammar rules describe
general construction rules, and a large number of
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Grammar Rule
3
1
Unification
HEAD 
SUBCAT < >
1
2
HEAD 
SUBCAT < >
3
2
HEAD 
SUBCAT < >
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Lexical Entries
John has come
John has come
Figure 1: Parsing a sentence ?John has come.?
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <>
Figure 2: An HPSG parse tree for a sentence ?John
has come.?
lexical entries express word-specific characteristics.
The structures of sentences are explained using com-
binations of grammar rules and lexical entries.
Figure 1 shows an example of HPSG parsing of
the sentence ?John has come.? First, as shown at the
top of the figure, an HPSG parser assigns a lexical
entry to each word in this sentence. Next, a gram-
mar rule is assigned and applied to lexical entries. At
the middle of this figure, the grammar rule is applied
to the lexical entries for ?has? and ?come.? We then
obtain the structure represented at the bottom of the
figure. After that, the application of grammar rules
is done iteratively, and then we can finally obtain the
parse tree as is shown in Figure 2. In practice, since
two or more parse candidates can be given for one
sentence, a disambiguation model gives probabili-
ties to these candidates, and a candidate given the
highest probability is then chosen as a correct parse.
12
The HPSG parser used in this study is Ninomiya
et al (2006), which is based on Enju (Miyao and
Tsujii, 2005). Lexical entries of Enju were extracted
from the Penn Treebank (Marcus et al, 1994), which
consists of sentences collected from The Wall Street
Journal (Miyao et al, 2004). The disambiguation
model of Enju was trained on the same treebank.
The disambiguation model of Enju is based on
a feature forest model (Miyao and Tsujii, 2002),
which is a log-linear model (Berger et al, 1996) on
packed forest structure. The probability, p
E
(tjw),
of producing the parse result t for a given sentence
w = hw
1
; :::; w
u
i is defined as
p
E
(tjw) =
1
Z
s
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl);
Z
s
=
X
t2T (w)
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl)
where l = hl
1
; :::; l
u
i is a list of lexical entries as-
signed to w, p
lex
(l
i
jw; i) is a probabilistic model
giving the probability that lexical entry l
i
is assigned
to word w
i
, q
syn
(tjl) is an unnormalized log-linear
model of tree construction and gives the possibil-
ity that parse candidate t is produced from lexical
entries l, and T (w) is a set of parse candidates as-
signed to w. With a treebank of a target domain as
training data, model parameters of p
lex
and q
syn
are
estimated so as to maximize the log-likelihood of the
training data.
Probabilistic model p
lex
is defined as a log-linear
model as follows.
p
lex
(l
i
jw; i) =
1
Z
w
i
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
Z
w
i
=
X
l
i
2L(w
i
)
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
where L(w
i
) is a set of lexical entries which can
be assigned to word w
i
. Before training this model,
L(w
i
) for all w
i
are extracted from the training tree-
bank. The feature function f
j
(l
i
;w; i) represents the
characteristics of l
i
, w and w
i
, while corresponding

j
is its weight. For the feature functions, instead of
using unigram features adopted in Miyao and Tsujii
(2005), Ninomiya et al (2006) used ?word trigram?
and ?POS 5-gram? features which are listed in Ta-
ble 1. With the revised Enju model, they achieved
Table 1: Features for the probabilities of lexical en-
try selection
surrounding words w
 1
w
0
w
1
(word trigram)
surrounding POS tags p
 2
p
 1
p
0
p
1
p
2
(POS 5-gram)
combinations w
 1
w
0
; w
0
w
1
; p
 1
w
0
; p
0
w
0
;
p
1
w
0
; p
0
p
1
p
2
p
3
; p
 2
p
 1
p
0
;
p
 1
p
0
p
1
; p
0
p
1
p
2
; p
 2
p
 1
;
p
 1
p
0
; p
0
p
1
; p
1
p
2
parsing accuracy as high as Miyao and Tsujii (2005),
with around four times faster parsing speed.
Johnson and Riezler (2000) suggested the pos-
sibility of the method for adapting a stochastic
unification-based grammar including HPSG to an-
other domain. They incorporated auxiliary distribu-
tions as additional features for an original log-linear
model, and then attempted to assign proper weights
to the new features. With this approach, they suc-
ceeded in decreasing to a degree indistinguishable
sentences for a target grammar.
Our previous work proposed a method for adapt-
ing an HPSG parser trained on the Penn Treebank
to a biomedical domain (Hara et al, 2005). We
re-trained a disambiguation model of tree construc-
tion, i.e., q
syn
, for the target domain. In this ap-
proach, q
syn
of the original parser was used as a
reference distribution (Jelinek, 1998) of another log-
linear model, and the new model was trained using a
target treebank. Since re-training used only a small
treebank of the target domain, the cost was small and
parsing accuracy was successfully improved.
3 Re-training of a Disambiguation Model
of Lexical Entry Assignments
Our idea of domain adaptation is to train a disam-
biguation model of lexical entry assignments for the
target domain and then incorporate it into the origi-
nal parser. Since Enju includes the disambiguation
model of lexical entry assignments as p
lex
, we can
implement our method in Enju by training another
disambiguation model p0
lex
(l
i
jw; i) of lexical entry
assignments for the biomedical domain, and then re-
placing the original p
lex
with the newly trained p0
lex
.
In this paper, for p0
lex
, we train a disambigua-
tion model p
lex mix
(l
i
jw; i) of lexical entry assign-
ments. p
lex mix
is a maximum entropy model and
the feature functions for it is the same as p
lex
as
13
given in Table 1. With these feature functions, we
train p
lex mix
on the treebanks both of the original
and biomedical domains.
In the experiments, we examine the contribution
of our method to parsing accuracy. In addition, we
implement several other possible methods for com-
parison of the performances.
baseline: use the original model of Enju
GENIA only: execute the same method of training
the disambiguation model of Enju, using only
the GENIA treebank
Mixture: execute the same method of training the
disambiguation model of Enju, using both of
the Penn Treebank and the GENIA treebank (a
kind of smoothing method)
HMT05: execute the method proposed in our pre-
vious work (Hara et al, 2005)
Our method: replace p
lex
in the original model
with p
lex mix
, while leaving q
syn
as it is
Our method (GENIA): replace p
lex
in the original
model with p
lex genia
, which is a probabilistic
model of lexical entry assignments trained only
with the GENIA treebank, while leaving q
syn
as it is
Our method + GENIA: replace p
lex
in the original
model with p
lex mix
and q
syn
with q
syn genia
,
which is a disambiguation model of tree con-
struction trained with the GENIA treebank
Our method + HMT05: replace p
lex
in the orig-
inal model with p
lex mix
and q
syn
with the
model re-trained with our previous method
(Hara et al, 2005) (the combination of our
method and the ?HMT05? method)
baseline (lex): use only p
lex
as a disambiguation
model
GENIA only (lex): use only p
lex genia
as a disam-
biguation model, which is a probabilistic model
of lexical entry assignments trained only with
the GENIA treebank
Mixture (lex): use only p
lex mix
as a disambigua-
tion model
The ?baseline? method does no adaptation to the
biomedical domain, and therefore gives lower pars-
ing accuracy for the domain than for the original do-
main. This method is regarded as the baseline of
the experiments. The ?GENIA only? method relies
solely on the treebank for the biomedical domain,
and therefore it cannot work well with the small tree-
bank. The ?Mixture? method is a kind of smoothing
method using all available training data at the same
time, and therefore the method can give the highest
accuracy of the three, which would be regarded as
the ideal accuracy with the naive methods. However,
training this model is expected to be very costly.
The ?baseline (lex),? ?GENIA only (lex),? and
?Mixture (lex)? approaches rely solely on models of
lexical entry assignments, and show lower accuracy
than those that contain both of models of lexical en-
try assignments and tree constructions. These ap-
proaches can be utilized as indicators of importance
of combining the two types of models.
Our previous work (Hara et al, 2005) showed that
the model trained with the ?HMT05? method can
give higher accuracy than the ?baseline? method,
even with the small amount of the treebanks in the
biomedical domain. The model also takes much less
cost to train than with the ?Mixture? method. How-
ever, they reported that the method could not give as
high accuracy as the ?Mixture? method.
4 Experiments with the GENIA Corpus
4.1 Experimental Settings
We implemented the models shown in Section 3,
and then evaluated the performance of them. The
original parser, Enju, was developed on Section 02-
21 of the Penn Treebank (39,832 sentences) (Miyao
and Tsujii, 2005; Ninomiya et al, 2006). For
training those models, we used the GENIA tree-
bank (Kim et al, 2003), which consisted of 1,200
abstracts (10,848 sentences) extracted from MED-
LINE. We divided it into three sets of 900, 150, and
150 abstracts (8,127, 1,361, and 1,360 sentences),
and these sets were used respectively as training, de-
velopment, and final evaluation data. The method
of Gaussian MAP estimation (Chen and Rosenfeld,
1999) was used for smoothing. The meta parameter
 of the Gaussian distribution was determined so as
to maximize the accuracy on the development set.
14
  
  
  
  
  
   
  
 
                 
	 
                   
 Proceedings of the 10th Conference on Parsing Technologies, pages 60?68,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A log-linear model with an n-gram reference distribution for accurate HPSG
parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
ninomi@r.dl.itc.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
tsujii@is.s.u-tokyo.ac.jp
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
Abstract
This paper describes a log-linear model with
an n-gram reference distribution for accurate
probabilistic HPSG parsing. In the model,
the n-gram reference distribution is simply
defined as the product of the probabilities
of selecting lexical entries, which are pro-
vided by the discriminative method with ma-
chine learning features of word and POS
n-gram as defined in the CCG/HPSG/CDG
supertagging. Recently, supertagging be-
comes well known to drastically improve
the parsing accuracy and speed, but su-
pertagging techniques were heuristically in-
troduced, and hence the probabilistic mod-
els for parse trees were not well defined.
We introduce the supertagging probabilities
as a reference distribution for the log-linear
model of the probabilistic HPSG. This is the
first model which properly incorporates the
supertagging probabilities into parse tree?s
probabilistic model.
1 Introduction
For the last decade, fast, accurate and wide-coverage
parsing for real-world text has been pursued in
sophisticated grammar formalisms, such as head-
driven phrase structure grammar (HPSG) (Pollard
and Sag, 1994), combinatory categorial grammar
(CCG) (Steedman, 2000) and lexical function gram-
mar (LFG) (Bresnan, 1982). They are preferred
because they give precise and in-depth analyses
for explaining linguistic phenomena, such as pas-
sivization, control verbs and relative clauses. The
main difficulty of developing parsers in these for-
malisms was how to model a well-defined proba-
bilistic model for graph structures such as feature
structures. This was overcome by a probabilistic
model which provides probabilities of discriminat-
ing a correct parse tree among candidates of parse
trees in a log-linear model or maximum entropy
model (Berger et al, 1996) with many features for
parse trees (Abney, 1997; Johnson et al, 1999; Rie-
zler et al, 2000; Malouf and van Noord, 2004; Ka-
plan et al, 2004; Miyao and Tsujii, 2005). Follow-
ing this discriminative approach, techniques for effi-
ciency were investigated for estimation (Geman and
Johnson, 2002; Miyao and Tsujii, 2002; Malouf and
van Noord, 2004) and parsing (Clark and Curran,
2004b; Clark and Curran, 2004a; Ninomiya et al,
2005).
An interesting approach to the problem of parsing
efficiency was using supertagging (Clark and Cur-
60
ran, 2004b; Clark and Curran, 2004a; Wang, 2003;
Wang and Harper, 2004; Nasr and Rambow, 2004;
Ninomiya et al, 2006; Foth et al, 2006; Foth and
Menzel, 2006), which was originally developed for
lexicalized tree adjoining grammars (LTAG) (Ban-
galore and Joshi, 1999). Supertagging is a process
where words in an input sentence are tagged with
?supertags,? which are lexical entries in lexicalized
grammars, e.g., elementary trees in LTAG, lexical
categories in CCG, and lexical entries in HPSG. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated in
the case of a CCG parser (Clark and Curran, 2004a)
with the result of a drastic improvement in the pars-
ing speed. Wang and Harper (2004) also demon-
strated the effects of supertagging with a statisti-
cal constraint dependency grammar (CDG) parser
by showing accuracy as high as the state-of-the-art
parsers, and Foth et al (2006) and Foth and Menzel
(2006) reported that accuracy was significantly im-
proved by incorporating the supertagging probabili-
ties into manually tuned Weighted CDG. Ninomiya
et al (2006) showed the parsing model using only
supertagging probabilities could achieve accuracy as
high as the probabilistic model for phrase structures.
This means that syntactic structures are almost de-
termined by supertags as is claimed by Bangalore
and Joshi (1999). However, supertaggers themselves
were heuristically used as an external tagger. They
filter out unlikely lexical entries just to help parsing
(Clark and Curran, 2004a), or the probabilistic mod-
els for phrase structures were trained independently
of the supertagger?s probabilistic models (Wang and
Harper, 2004; Ninomiya et al, 2006). In the case of
supertagging of Weighted CDG (Foth et al, 2006),
parameters for Weighted CDG are manually tuned,
i.e., their model is not a well-defined probabilistic
model.
We propose a log-linear model for probabilistic
HPSG parsing in which the supertagging probabil-
ities are introduced as a reference distribution for
the probabilistic HPSG. The reference distribution is
simply defined as the product of the probabilities of
selecting lexical entries, which are provided by the
discriminative method with machine learning fea-
tures of word and part-of-speech (POS) n-gram as
defined in the CCG/HPSG/CDG supertagging. This
is the first model which properly incorporates the su-
pertagging probabilities into parse tree?s probabilis-
tic model. We compared our model with the proba-
bilistic model for phrase structures (Miyao and Tsu-
jii, 2005). This model uses word and POS unigram
for its reference distribution, i.e., the probabilities of
unigram supertagging. Our model can be regarded
as an extension of a unigram reference distribution
to an n-gram reference distribution with features that
are used in supertagging. We also compared with a
probabilistic model in (Ninomiya et al, 2006). The
probabilities of their model are defined as the prod-
uct of probabilities of supertagging and probabilities
of the probabilistic model for phrase structures, but
their model was trained independently of supertag-
ging probabilities, i.e., the supertagging probabili-
ties are not used for reference distributions.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical entries
express word-specific characteristics. The structures
of sentences are explained using combinations of
schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly applying
schemata to lexical/phrasal signs. Finally, the parse
result is output as a phrasal sign that dominates the
sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
61
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries deter-
mine the dominant syntactic structures.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability that a parse result T is assigned to a
given sentence w = ?w1, . . . , wn? is
(Probabilistic HPSG)
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature func-
tion that represents a characteristic of parse tree T ,
and Zw is the sum over the set of all possible parse
trees for the sentence. Intuitively, the probability
is defined as the normalized product of the weights
exp(?u) when a characteristic corresponding to fu
appears in parse result T . The model parameters, ?u,
are estimated using numerical optimization methods
(Malouf, 2002) to maximize the log-likelihood of
the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the compu-
tation of p(T |w) for all parse candidates assigned
to sentence w. Because the number of parse can-
didates is exponentially related to the length of the
sentence, the estimation is intractable for long sen-
tences. To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). Miyao and Tsujii (2005) also introduced a
preliminary probabilistic model p0(T |w) whose es-
timation does not require the parsing of a treebank.
This model is introduced as a reference distribution
(Jelinek, 1998; Johnson and Riezler, 2000) of the
probabilistic HPSG model; i.e., the computation of
parse trees given low probabilities by the model is
omitted in the estimation stage (Miyao and Tsujii,
2005), or a probabilistic model can be augmented
by several distributions estimated from the larger
and simpler corpus (Johnson and Riezler, 2000). In
(Miyao and Tsujii, 2005), p0(T |w) is defined as the
product of probabilities of selecting lexical entries
with word and POS unigram features:
(Miyao and Tsujii (2005)?s model)
puniref (T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in
T and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model with
other two types of probabilistic models using a su-
pertagger (Ninomiya et al, 2006). The first one is
the simplest probabilistic model, which is defined
with only the probabilities of lexical entry selec-
tion. It is defined simply as the product of the prob-
abilities of selecting all lexical entries in the sen-
tence; i.e., the model does not use the probabilities
of phrase structures like the probabilistic models ex-
plained above. Given a set of lexical entries, L, a
sentence, w = ?w1, . . . , wn?, and the probabilistic
model of lexical entry selection, p(li ? L|w, i), the
first model is formally defined as follows:
62
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(Ninomiya et al (2006)?s model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi in T
and p(li|w, i) is the probability of selecting lexical
entry li for wi.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic model of lexical entry selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical entries
for the word wi.
The second model is a hybrid model of supertag-
ging and the probabilistic HPSG. The probabilities
are given as the product of Ninomiya et al (2006)?s
model 1 and the probabilistic HPSG.
(Ninomiya et al (2006)?s model 3)
pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)
In the experiments, we compared our model with
Miyao and Tsujii (2005)?s model and Ninomiya et
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
fsptag =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Feature templates.
al. (2006)?s model 1 and 3. The features used in our
model and their model are combinations of the fea-
ture templates listed in Table 1 and Table 2. The
feature templates fbinary and funary are defined for
constituents at binary and unary branches, froot is a
feature template set for the root nodes of parse trees.
flex is a feature template set for calculating the uni-
gram reference distribution and is used in Miyao and
Tsujii (2005)?s model. fsptag is a feature template
set for calculating the probabilities of selecting lex-
ical entries in Ninomiya et al (2006)?s model 1 and
3. The feature templates in fsptag are word trigrams
and POS 5-grams. An example of features applied
to the parse tree for the sentence ?Spring has come?
is shown in Figure 2.
3 Probabilistic HPSG with an n-gram
reference distribution
In this section, we propose a probabilistic model
with an n-gram reference distribution for probabilis-
tic HPSG parsing. This is an extension of Miyao
and Tsujii (2005)?s model by replacing the unigram
reference distribution with an n-gram reference dis-
tribution. Our model is formally defined as follows:
63
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
combinations of feature templates for fsptag
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)
pnref (T |w) =
1
Znref pmodel1(T |w) exp
(?
u
?ufu(T )
)
Znref =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In our model, Ninomiya et al (2006)?s model 1
is used as a reference distribution. The probabilis-
tic model of lexical entry selection and its feature
templates are the same as defined in Ninomiya et al
(2006)?s model 1.
The formula of our model is the same as Ni-
nomiya et al (2006)?s model 3. But, their model
is not a probabilistic model with a reference distri-
bution. Both our model and their model consist of
the probabilities for lexical entries (= pmodel1(T |w))
and the probabilities for phrase structures (= the rest
of each formula). The only difference between our
model and their model is the way of how to train
model parameters for phrase structures. In both our
model and their model, the parameters for lexical en-
tries (= the parameters of pmodel1(T |w)) are first es-
timated from the word and POS sequences indepen-
dently of the parameters for phrase structures. That
is, the estimated parameters for lexical entries are
the same in both models, and hence the probabilities
of pmodel1(T |w) of both models are the same. Note
that the parameters for lexical entries will never be
updated after this estimation stage; i.e., the parame-
ters for lexical entries are not estimated in the same
time with the parameters for phrase structures. The
difference of our model and their model is the esti-
mation of parameters for phrase structures. In our
model, given the probabilities for lexical entries, the
parameters for phrase structures are estimated so as
to maximize the entire probabilistic model (= the
product of the probabilities for lexical entries and
the probabilities for phrase structures) in the train-
ing corpus. In their model, the parameters for phrase
structures are trained without using the probabili-
ties for lexical entries, i.e., the parameters for phrase
structures are estimated so as to maximize the prob-
abilities for phrase structures only. That is, the pa-
rameters for lexical entries and the parameters for
phrase structures are trained independently in their
model.
Miyao and Tsujii (2005)?s model also uses a ref-
erence distribution, but with word and POS unigram
features, as is explained in the previous section. The
only difference between our model and Miyao and
Tsujii (2005)?s model is that our model uses se-
quences of word and POS tags as n-gram features
for selecting lexical entries in the same way as su-
pertagging does.
4 Experiments
We evaluated the speed and accuracy of parsing
by using Enju 2.1, the HPSG grammar for English
(Miyao et al, 2005; Miyao and Tsujii, 2005). The
lexicon of the grammar was extracted from Sec-
tions 02-21 of the Penn Treebank (Marcus et al,
1994) (39,832 sentences). The grammar consisted
of 3,797 lexical entries for 10,536 words1. The prob-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by apply-
ing lexical rules to observed lexical entries in the HPSG tree-
bank (Nakanishi et al, 2004). The lexicon, however, included
many lexical entries that do not appear in the HPSG treebank.
64
No. of tested sentences Total No. of sentences Avg. length of tested sentences
Section 23 2,299 (100.00%) 2,299 22.2
Section 24 1,245 (99.84%) 1,247 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (Gold POSs)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 87.26 86.50 86.88 90.73 89.93 90.33 604
Ninomiya et al (2006)?s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129
Ninomiya et al (2006)?s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152
our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234
our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379
Section 23 (POS tagger)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 84.96 84.25 84.60 89.55 88.80 89.17 674
Ninomiya et al (2006)?s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154
Ninomiya et al (2006)?s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183
Matsuzaki et al (2007)?s model 86.93 86.47 86.70 - - - 30
our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260
our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821
Table 4: Experimental results for Section 23.
abilistic models were trained using the same portion
of the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and quick check
(Malouf et al, 2000).
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tuple
??,wh, a, wa?, where ? is the predicate type (e.g.,
adjective, intransitive verb), wh is the head word of
the predicate, a is the argument label (MODARG,
ARG1, ..., ARG4), and wa is the head word of
the argument. Labeled precision (LP)/labeled re-
call (LR) is the ratio of tuples correctly identified
by the parser2. Unlabeled precision (UP)/unlabeled
recall (UR) is the ratio of tuples without the pred-
icate type and the argument label. This evaluation
scheme was the same as used in previous evaluations
of lexicalized grammars (Hockenmaier, 2003; Clark
The HPSG treebank is used for training the probabilistic model
for lexical entry selection, and hence, those lexical entries that
do not appear in the treebank are rarely selected by the proba-
bilistic model. The ?effective? tag set size, therefore, is around
1,361, the number of lexical entries without those never-seen
lexical entries.
2When parsing fails, precision and recall are evaluated, al-
though nothing is output by the parser; i.e., recall decreases
greatly.
and Curran, 2004b; Miyao and Tsujii, 2005). The
experiments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the Tree-
bank was used as the development set, and the per-
formance was evaluated using sentences of ? 100
words in Section 23. The performance of each
model was analyzed using the sentences in Section
24 of ? 100 words. Table 3 details the numbers
and average lengths of the tested sentences of ? 100
words in Sections 23 and 24, and the total numbers
of sentences in Sections 23 and 24.
The parsing performance for Section 23 is shown
in Table 4. The upper half of the table shows the per-
formance using the correct POSs in the Penn Tree-
bank, and the lower half shows the performance us-
ing the POSs given by a POS tagger (Tsuruoka and
Tsujii, 2005). LF and UF in the figure are labeled
F-score and unlabeled F-score. F-score is the har-
monic mean of precision and recall. We evaluated
our model in two settings. One is implemented with
a narrow beam width (?our model 1? in the figure),
and the other is implemented with a wider beam
width (?our model 2? in the figure)3. ?our model
3The beam thresholding parameters for ?our model 1? are
?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? = 2.5, ?last =15.0, ?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? =2.5, ?last = 15.0, ?0 = 6.0,?? = 3.5, and ?last = 20.0.
65
83.00%
83.50%
84.00%
84.50%
85.00%
85.50%
86.00%
86.50%
87.00%
87.50%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F-sc
ore
Miyao and Tsujii(2005)'s modelNinomiya et al(2006)'s model 1Ninomiya et al(2006)'s model 3
our model
Figure 3: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
1? was introduced to measure the performance with
balanced F-score and speed, which we think appro-
priate for practical use. ?our model 2? was intro-
duced to measure how high the precision and re-
call could reach by sacrificing speed. Our mod-
els increased the parsing accuracy. ?our model 1?
was around 2.6 times faster and had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model. ?our model 2? was around 2.3 times slower
but had around 2.9 points higher F-score than Miyao
and Tsujii (2005)?s model. We must admit that the
difference between our models and Ninomiya et al
(2006)?s model 3 was not as great as the differ-
ence from Miyao and Tsujii (2005)?s model, but ?our
model 1? achieved 0.56 points higher F-score, and
?our model 2? achieved 0.8 points higher F-score.
When the automatic POS tagger was introduced, F-
score dropped by around 2.4 points for all models.
We also compared our model with Matsuzaki et
al. (2007)?s model. Matsuzaki et al (2007) pro-
The terms ? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs in the chart
cell. The terms ? and ? are the thresholds of the number and
the beam width of lexical entries, and ? is the beam width for
global thresholding (Goodman, 1997). The terms with suffixes
0 are the initial values. The parser iterates parsing until it suc-
ceeds to generate a parse tree. The parameters increase for each
iteration by the terms prefixed by ?, and parsing finishes when
the parameters reach the terms with suffixes last. Details of the
parameters are written in (Ninomiya et al, 2005). The beam
thresholding parameters for ?our model 2? are ?0 = 18,?? =
6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0, ?0 =18,?? = 6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0.
In ?our model 2?, the global thresholding was not used.
posed a technique for efficient HPSG parsing with
supertagging and CFG filtering. Their results with
the same grammar and servers are also listed in the
lower half of Table 4. They achieved drastic im-
provement in efficiency. Their parser ran around 6
times faster than Ninomiya et al (2006)?s model 3,
9 times faster than ?our model 1? and 60 times faster
than ?our model 2.? Instead, our models achieved
better accuracy. ?our model 1? had around 0.5 higher
F-score, and ?our model 2? had around 0.8 points
higher F-score. Their efficiency is mainly due to
elimination of ungrammatical lexical entries by the
CFG filtering. They first parse a sentence with a
CFG grammar compiled from an HPSG grammar,
and then eliminate lexical entries that are not in the
parsed CFG trees. Obviously, this technique can
also be applied to the HPSG parsing of our mod-
els. We think that efficiency of HPSG parsing with
our models will be drastically improved by applying
this technique.
The average parsing time and labeled F-score
curves of each probabilistic model for the sentences
in Section 24 of ? 100 words are graphed in Fig-
ure 3. The graph clearly shows the difference of
our model and other models. As seen in the graph,
our model achieved higher F-score than other model
when beam threshold was widen. This implies that
other models were probably difficult to reach the F-
score of ?our model 1? and ?our model 2? for Section
23 even if we changed the beam thresholding param-
eters. However, F-score of our model dropped eas-
66
ily when we narrow down the beam threshold, com-
pared to other models. We think that this is mainly
due to its bad implementation of parser interface.
The n-gram reference distribution is incorporated
into the kernel of the parser, but the n-gram fea-
tures and a maximum entropy estimator are defined
in other modules; n-gram features are defined in a
grammar module, and a maximum entropy estimator
for the n-gram reference distribution is implemented
with a general-purpose maximum entropy estimator
module. Consequently, strings that represent the n-
gram information are very frequently changed into
feature structures and vice versa when they go in and
out of the kernel of the parser. On the other hand, Ni-
nomiya et al (2006)?s model 3 uses the supertagger
as an external module. Once the parser acquires the
supertagger?s outputs, the n-gram information never
goes in and out of the kernel. This advantage of Ni-
nomiya et al (2006)?s model can apparently be im-
plemented in our model, but this requires many parts
of rewriting of the implemented parser. We estimate
that the overhead of the interface is around from 50
to 80 ms/sentence. We think that re-implementation
of the parser will improve the parsing speed as esti-
mated. In Figure 3, the line of our model crosses the
line of Ninomiya et al (2006)?s model. If the esti-
mation is correct, our model will be faster and more
accurate so that the lines in the figure do not cross.
Speed-up in our model is left as a future work.
5 Conclusion
We proposed a probabilistic model in which su-
pertagging is consistently integrated into the prob-
abilistic model for HPSG. In the model, the n-gram
reference distribution is simply defined as the prod-
uct of the probabilities of selecting lexical entries
with machine learning features of word and POS n-
gram as defined in the CCG/HPSG/CDG supertag-
ging. We conducted experiments on the Penn Tree-
bank with a wide-coverage HPSG parser. In the ex-
periments, we compared our model with the prob-
abilistic HPSG with a unigram reference distribu-
tion (Miyao and Tsujii, 2005) and the probabilistic
HPSG with supertagging (Ninomiya et al, 2006).
Though our model was not as fast as Ninomiya
et al (2006)?s models, it achieved the highest ac-
curacy among them. Our model had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.56 points higher F-score than
the Ninomiya et al (2006)?s model 3. When we sac-
rifice parsing speed, our model achieved around 2.9
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.8 points higher F-score than Ni-
nomiya et al (2006)?s model 3. Our model achieved
higher F-score because parameters for phrase struc-
tures in our model are trained with the supertagging
probabilities, which are not in other models.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge, MA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proc. of
ACL?04, pages 104?111.
Killian Foth and Wolfgang Menzel. 2006. Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proc. of COLING-ACL 2006.
Killian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. of COLING-ACL 2006.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. The MIT Press.
67
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proc. of NAACL-2000, pages 154?161.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proc. of ACL ?99,
pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proc. of IJCNLP-04 Workshop ?Beyond
Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proc. of IJCAI 2007, pages 1671?
1676.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proc. of HLT
2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii, 2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Acquir-
ing a Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lexi-
cal rules on parsing with a treebank grammar. In Proc.
of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun?ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic HPSG parsing. In Proc. of IWPT 2005, pages
103?114.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proc. of EMNLP 2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proc. of ACL?00, pages 480?487.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proc. of HLT/EMNLP 2005,
pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc.
of ACL?04 Incremental Parsing workshop: Bringing
Engineering and Cognition Together, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
68
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating the Effects of Treebank Size in a Practical Application for 
Parsing 
Kenji Sagae1, Yusuke Miyao1, Rune S?tre1 and Jun'ichi Tsujii1,2,3 
1Department of Computer Science, Univerisity of Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining, Manchester, UK 
{sagae,yusuke,rune.saetre,tsujii@is.s.u-tokyo.ac.jp} 
 
 
 
 
 
Abstract 
Natural language processing modules such as 
part-of-speech taggers, named-entity recog-
nizers and syntactic parsers are commonly 
evaluated in isolation, under the assumption 
that artificial evaluation metrics for individual 
parts are predictive of practical performance 
of more complex language technology sys-
tems that perform practical tasks. Although 
this is an important issue in the design and en-
gineering of systems that use natural language 
input, it is often unclear how the accuracy of 
an end-user application is affected by parame-
ters that affect individual NLP modules.  We 
explore this issue in the context of a specific 
task by examining the relationship between 
the accuracy of a syntactic parser and the 
overall performance of an information extrac-
tion system for biomedical text that includes 
the parser as one of its components.  We 
present an empirical investigation of the rela-
tionship between factors that affect the accu-
racy of syntactic analysis, and how the 
difference in parse accuracy affects the overall 
system.   
1 Introduction 
Software systems that perform practical tasks with 
natural language input often include, in addition to 
task-specific components, a pipeline of basic natu-
ral language processing modules, such as part-of-
speech taggers, named-entity recognizers, syntactic 
parsers and semantic-role labelers.  Although such 
building blocks of larger language technology so-
lutions are usually carefully evaluated in isolation 
using standard test sets, the impact of improve-
ments in each individual module on the overall 
performance of end-to-end systems is less well 
understood.  While the effects of the amount of 
training data, search beam widths and various ma-
chine learning frameworks have been explored in 
detail with respect to speed and accuracy in basic 
natural language processing tasks, how these trade-
offs in individual modules affect the performance 
of the larger systems they compose is an issue that 
has received relatively little attention.  This issue, 
however, is of great practical importance in the 
effective design and engineering of complex soft-
ware systems that deal with natural language.   
In this paper we explore some of these issues 
empirically in an information extraction task in the 
biomedical domain, the identification of protein- 
protein interactions (PPI) mentioned in papers ab-
stracts from MEDLINE, a large database of bio-
medical papers.  Due in large part to the creation of 
biomedical treebanks (Kulick et al, 2004; Tateisi 
et al, 2005) and rapid progress of data-driven 
parsers (Lease and Charniak, 2005; Nivre et al, 
2007), there are now fast, robust and accurate syn-
tactic parsers for text in the biomedical domain.  
Recent research shows that parsing accuracy of 
biomedical corpora is now between 80% and 90% 
(Clegg and Shepherd, 2007; Pyysalo et al, 2007; 
Sagae et al, 2008).  Intuitively, syntactic relation-
ships between words should be valuable in deter-
mining possible interactions between entities 
present in text.  Recent PPI extraction systems 
have confirmed this intuition (Erkan et al, 2007; 
S?tre et al, 2007; Katrenko and Adriaans, 2006).     
While it is now relatively clear that syntactic 
parsing is useful in practical tasks that use natural 
language corpora in bioinformatics, several ques-
14
tions remain as to research issues that affect the 
design and testing of end-user applications, includ-
ing how syntactic analyses should be used in a 
practical setting, whether further improvements in 
parsing technologies will result in further im-
provements in practical systems, whether it is im-
portant to continue the development of treebanks 
and parser adaptation techniques for the biomedi-
cal domain, and how much effort should be spent 
on comparing and benchmarking parsers for bio-
medical data.  We attempt to shed some light on 
these matters by presenting experiments that show 
the relationship of the accuracy of a dependency 
parser and the accuracy of the larger PPI system 
that includes the parser.  We investigate the effects 
of domain-specific treebank size (the amount of 
available manually annotated training data for syn-
tactic parsers) and final system performance, and 
obtain results that should be informative to re-
searchers in bioinformatics who rely on existing 
NLP resources to design information extraction 
systems, as well as to members of the parsing 
community who are interested in the practical im-
pact of parsing research. 
In section 2 we discuss our motivation and re-
lated efforts.  Section 3 describes the system for 
identification of protein-protein interactions used 
in our experiments, and in section 4 describes the 
syntactic parser that provides the analyses for the 
PPI system, and the data used to train the parser.  
We describe our experiments, results and analysis 
in section 5, and conclude in section 6.  
2 Motivation and related work 
While recent work has addressed questions relating 
to the use of different parsers or different types of 
syntactic representations in the PPI extraction task 
(S?tre et al, 2007, Miyao et al, 2008), little con-
crete evidence has been provided for potential ben-
efits of improved parsers or additional resources 
for training syntactic parsers.  In fact, although 
there is increasing interest in parser evaluation in 
the biomedical domain in terms of precision/recall 
of brackets and dependency accuracy (Clegg and 
Shepherd, 2007; Pyysalo et al, 2007; Sagae et al, 
2008), the relationship between these evaluation 
metrics and the performance of practical informa-
tion extraction systems remains unclear.  In the 
parsing community, relatively small accuracy gains 
are often reported as success stories, but again, the 
precise impact of such improvements on practical 
tasks in bioinformatics has not been established. 
One aspect of this issue is the question of do-
main portability and domain adaptation for parsers 
and other NLP modules.  Clegg and Shepherd 
(2007) mention that available statistical parsers 
appear to overfit to the newswire domain, because 
of their extensive use of the Wall Street Journal 
portion of the Penn Treebank (Marcus et al, 1994) 
during development and training.  While this claim 
is supported by convincing evaluations that show 
that parsers trained on the WSJ Penn Treebank 
alone perform poorly on biomedical text in terms 
of accuracy of dependencies or bracketing of 
phrase structure, the benefits of using domain-
specific data in terms of practical system perfor-
mance have not been quantified.  These expected 
benefits drive the development of domain-specific 
resources, such as the GENIA treebank (Tateisi et 
al., 2005), and parser domain adaption (Hara et al, 
2007), which are of clear importance in parsing 
research, but of largely unconfirmed impact on 
practical systems. 
Quirk and Corston-Oliver (2006) examine a 
similar issue, the relationship between parser accu-
racy and overall system accuracy in syntax-
informed machine translation.  Their research is 
similar to the work presented here, but they fo-
cused on the use of varying amounts of out-of-
domain training data for the parser, measuring how 
a translation system for technical text performed 
when its syntactic parser was trained with varying 
amounts of Wall Street Journal text.  Our work, in 
contrast, investigates the use of domain-specific 
training material in parsers for biomedical text, a 
domain where significant amounts of effort are 
allocated for development of domain-specific NLP 
resources in hope that such resources will result in 
better overall performance in practical systems.  
3 A PPI extraction system based on syn-
tactic parsing 
PPI extraction is an NLP task to identify protein 
pairs that are mentioned as interacting in biomedi-
cal papers.  Figure 2 shows two sentences that in-
clude protein names: the former sentence mentions 
a protein interaction, while the latter does not.  
Given a protein pair, PPI extraction is a task of 
binary classification; for example, <IL-8, CXCR1> 
15
is a positive example, and <RBP, TTR> is a ne-
gative example. 
Following recent work on using dependency 
parsing in systems that identify protein interactions 
in biomedical text (Erkan et al, 2007; S?tre et al, 
2007; Katrenko and Adriaans, 2006), we have built 
a system for PPI extraction that uses dependency 
relations as features. As exemplified, for the pro-
tein pair IL-8 and CXCR1 in the first sentence of 
Figure 2, a dependency parser outputs a dependen-
cy tree shown in Figure 1.  From this dependency 
tree, we can extract a dependency path between 
IL-8 and CXCR1 (Figure 3), which appears to be 
a strong clue in knowing that these proteins are 
mentioned as interacting. 
The system we use in this paper is similar to the 
one described in S?tre et al (2007), except that it 
uses syntactic dependency paths obtained with a 
dependency parser, but not predicate-argument 
paths based on deep-parsing.  This method is based 
on SVM with SubSet Tree Kernels (Collins, 2002; 
Moschitti, 2006).  A dependency path is encoded 
as a flat tree as depicted in Figure 4. Because a tree 
kernel measures the similarity of trees by counting 
common subtrees, it is expected that the system 
finds effective subsequences of dependency paths.   
In addition to syntactic dependency features, we 
incorporate bag-of-words features, which are re-
garded as a strong baseline for IE systems.  We use 
lemmas of words before, between and after the pair 
of target proteins. 
In this paper, we use Aimed (Bunescu and 
Mooney, 2004), which is a popular benchmark for 
the evaluation of PPI extraction systems.  The 
Aimed corpus consists of 225 biomedical paper 
abstracts (1970 sentences), which are sentence-
split, tokenized, and annotated with proteins and 
PPIs.  
4 A data-driven dependency parser for 
biomedical text 
The parser we used as component of our PPI ex-
traction system was a shift-reduce dependency 
parser that uses maximum entropy models to de-
termine the parser?s actions.  Our overall parsing 
approach uses a best-first probabilistic shift-reduce 
algorithm, working left-to right to find labeled de-
pendencies one at a time. The algorithm is essen-
tially a dependency version of the constituent 
parsing algorithm for probabilistic parsing with 
LR-like data-driven models described by Sagae 
and Lavie (2006).  This dependency parser has 
been shown to have state-of-the-art accuracy in the 
CoNLL shared tasks on dependency parsing 
(Buchholz and Marsi, 2006; Nivre, 2007). Sagae 
and Tsujii (2007) present a detailed description of 
the parsing approach used in our work, including 
the parsing algorithm and the features used to clas-
sify parser actions.  In summary, the parser uses an 
algorithm similar to the LR parsing algorithm 
(Knuth, 1965), keeping a stack of partially built 
syntactic structures, and a queue of remaining in-
put tokens.  At each step in the parsing process, the 
parser can apply a shift action (remove a token 
from the front of the queue and place it on top of 
the stack), or a reduce action (pop the two topmost 
This study demonstrates that IL-8 recognizes 
and activates CXCR1, CXCR2, and the Duf-
fy antigen by distinct mechanisms. 
 
The molar ratio of serum retinol-binding pro-
tein (RBP) to transthyretin (TTR) is not 
useful to assess vitamin A status during infec-
tion in hospitalized children. 
Figure 2: Example sentences with protein names 
Figure 1: A dependency tree 
ROOT  IL-8  recognizes  and  activates  CXCR1 
ROOT 
SBJ 
OBJ 
COORD 
CC 
ENTITY1(IL-8)    recognizes   ENTITY2(CXCR1) 
Figure 3: A dependency path between protein names 
SBJ OBJ 
16
stack items, and push a new item composed of the 
two popped items combined in a single structure). 
This parsing approach is very similar to the one 
used successfully by Nivre et al (2006), but we 
use a maximum entropy classifier (Berger et al, 
1996) to determine parser actions, which makes 
parsing considerably faster. In addition, our pars-
ing approach performs a search over the space of 
possible parser actions, while Nivre et al?s ap-
proach is deterministic. 
The parser was trained using 8,000 sentences 
from the GENIA Treebank (Tateisi et al, 2005), 
which contains abstracts of papers taken from 
MEDLINE, annotated with syntactic structures.  
To determine the effects of training set size on the 
parser, and consequently on the PPI extraction sys-
tem, we trained several parsing models with differ-
ent amounts of GENIA Treebank data.  We started 
with 100 sentences, and increased the training set 
by 100 sentence increments, up to 1,000 sentences.  
From that point, we increased the training set by 
1,000 sentence increments.  Figure 5 shows the 
labeled dependency accuracy for the varying sizes 
of training sets.  The accuracy was measured on a 
portion of the GENIA Treebank reserved as devel-
opment data.  The result clearly demonstrates that 
the increase in the size of the training set contri-
butes to increasing parse accuracy.  Training the 
parser with only 100 sentences results in parse ac-
curacy of about 72.5%.  Accuracy rises sharply 
with additional training data until the size of the 
training set reaches about 1,000 sentences (about 
82.5% accuracy).  From there, accuracy climbs 
consistently, but slowly, until 85.6% accuracy is 
reached with 8,000 sentences of training data. 
It should be noted that parser accuracy on the 
Aimed data used in our PPI extraction experiments 
may be slightly lower, since the domain of the 
GENIA Treebank is not exactly the same as the 
Aimed corpus.  Both of them were extracted from 
MEDLINE, but the criteria for data selection were 
not the same in the two corpora, creating possible 
differences in sub-domains.  We also note that the 
accuracy of a parser trained with more than 40,000 
sentences from the Wall Street Journal portion of 
the Penn Treebank is under 79%, a level equivalent 
to that obtained by training the parser with only 
500 sentences of GENIA data. 
 
 
Figure 5: Data size vs. parse accuracy 
 
5 Experiments and Results 
In this section we present our PPI extraction expe-
riments applying the dependency parsers trained 
with the different amounts of the GENIA Treebank 
in our PPI system.  As we mentioned, the GENIA 
Treebank is used for training the parser, while the 
Aimed is used for training and evaluation of PPI 
extraction.  A part-of-speech tagger trained with 
GENIA and PennBioIE was used.  We do not ap-
ply automatic protein name detection, and instead 
use the gold-standard protein annotations in the 
Aimed corpus.  Before running a parser, multiword 
protein names are concatenated and treated as sin-
gle words. As described in Section 3, bag-of-words 
and syntactic dependency paths are fed as features 
to the PPI classifier. The accuracy of PPI extrac-
tion is measured by the abstract-wise 10-fold cross 
validation (S?tre et al 2007). 
When we use the part-of-speech tagger and the 
dependency parser trained with WSJ, the accuracy 
(F-score) of PPI extraction on this data set is 55.2.  
The accuracy increases to 56.9 when we train the 
part-of-speech tagger with GENIA and Penn BioIE, 
while using the WSJ-trained parser.  This confirms 
the claims by Lease and Charniak (2005) that sub-
sentential lexical analysis alone is helpful in adapt-
ing WSJ parsers to the biomedical domain.  While 
Lease and Charniak looked only at parse accuracy, 
70
75
80
85
90
0 2000 4000 6000 8000
Figure 4: A tree kernel representation of the dependency 
path 
(dep_path (SBJ (ENTITY1 ecognizes)) 
(rOBJ (recognizes ENTITY2))) 
17
our result shows that the increase in parse accuracy 
is, as expected, beneficial in practice. 
Figure 6 shows the relationship between the 
amount of parser training data and the F-score for 
the PPI extraction.  The result shows that the accu-
racy of PPI extraction increases with the use of 
more sentences to train the parser.    The best accu-
racy was obtained when using 4,000 sentences, 
where parsing accuracy is around 84.3.  Although 
it may appear that further increasing the training 
data for the parser may not improve the PPI extrac-
tion accuracy (since only small and inconsistent 
variations in F-score are observed in Figure 6), 
when we plot the curves shown in Figures 5 and 6 
in a single graph (Figure 7), we see that the two 
curves match each other to a large extent.  This is 
supported by the strong correlation between parse 
accuracy and PPI accuracy observed in Figure 8.  
While this suggests that training the parser with a 
larger treebank may result in improved accuracy in 
PPI extraction, we observe that a 1% absolute im-
provement in parser accuracy corresponds roughly 
to a 0.25 improvement in PPI extraction F-score.  
Figure 5 indicates that to obtain even a 1% im-
provement in parser accuracy by using more train-
ing data, the size of the treebank would have to 
increase significantly. 
Although the results presented so far seem to 
suggest the need for a large data annotation effort 
to achieve a meaningful improvement in PPI ex-
traction accuracy, there are other ways to improve 
the overall accuracy of the system without an im-
provement in parser accuracy.  One obvious alter-
native is to increase the size of the PPI-annotated 
corpus (which is distinct from the treebank used to 
train the parser).  As mentioned in section 3, our 
system is trained using the Aimed corpus, which 
contains 225 abstracts from biomedical papers with 
manual annotations indicating interactions between 
proteins.  Pairs of proteins with no interaction de-
scribed in the text are used as negative examples, 
and pairs of proteins described as interacting are 
used as positive examples.  The corpus contains a 
total of roughly 9,000 examples.  Figure 9 shows 
how the overall system accuracy varies when dif-
ferent amounts of training data (varying amounts 
of training examples) are used to train the PPI sys-
tem (keeping the parse accuracy constant, using all 
of the available training data in the GENIA tree-
bank to train the parser).  While Figure 5 indicates 
that a significant improvement in parse accuracy 
requires a large increase in the treebank used to 
train the parser, and Figure 7 shows that improve-
ments in PPI extraction accuracy may require a 
sizable improvement in parse accuracy, Figure 9 
suggests that even a relatively small increase in the 
PPI corpus may lead to a significant improvement 
in PPI extraction accuracy. 
 
Figure 6: Parser training data size vs. PPI extraction 
accuracy 
 
 
 
Figure 7: Parser training data size vs. parser accuracy 
and PPI extraction accuracy 
 
 
 
Figure 8: Parse accuracy vs. PPI extraction accuracy 
 
53
54
55
56
57
58
0 2000 4000 6000 8000
53
54
55
56
57
58
68
72
76
80
84
88
0 5000 10000
Parser
PPI F-score
53
54
55
56
57
58
70 75 80 85 90
18
 
 
Figure 9: Number of PPI training examples vs. PPI ex-
traction accuracy 
 
While some of the conclusions that can be 
drawn from these results may be somewhat sur-
prising, most are entirely expected.  However, even 
in these straightforward cases, our experiments 
provide some empirical evidence and concrete 
quantitative analysis to complement intuition.  We 
see that using domain-specific training data for the 
parsing component for the PPI extraction system 
produces superior results, compared to using train-
ing data from the WSJ Penn Treebank.  When the 
parser trained on WSJ sentences is used, PPI ex-
traction accuracy is about 55, compared to over 57 
when sentences from biomedical papers are used.  
This corresponds fairly closely to the differences in 
parser accuracy: the accuracy of the parser trained 
on 500 sentences from GENIA is about the same 
as the accuracy of the parser trained on the entire 
WSJ Penn Treebank, and when these parsers are 
used in the PPI extraction system, they result in 
similar overall task accuracy.  However, the results 
obtained when a domain-specific POS tagger is 
combined with a parser trained with out-of-domain 
data, overall PPI results are nearly at the same lev-
el as those obtained with domain-specific training 
data (just below 57 with a domain-specific POS 
tagger and out-of-domain parser, and just above 57 
for domain-specific POS tagger and parser).  At 
the same time, the argument against annotating 
domain-specific data for parsers in new domains is 
not a strong one, since higher accuracy levels (for 
both the parser and the overall system) can be ob-
tained with a relatively small amount of domain-
specific data. 
Figures 5, 6 and 7 also suggest that additional 
efforts in improving parser accuracy (through the 
use of feature engineering, other machine learning 
techniques, or an increase in the size of its training 
set) could improve PPI extraction accuracy, but a 
large improvement in parser accuracy may be re-
quired.  When we combine these results with the 
findings obtained by Miyao et al (2008), they sug-
gest that a better way to improve the overall sys-
tem is to spend more effort in designing a specific 
syntactic representation that addresses the needs of 
the system, instead of using a generic representa-
tion designed for measuring parser accuracy.  
Another potentially fruitful course of action is to 
design more sophisticated and effective ways for 
information extraction systems to use NLP tools, 
rather than simply extracting features that corres-
pond to small fragments of syntactic trees.  Of 
course, making proper use of natural language 
analysis is a considerable challenge, but one that 
should be kept in mind through the design of prac-
tical systems that use NLP components. 
6 Conclusion 
This paper presented empirical results on the rela-
tionship between the amount of training data used 
to create a dependency parser, and the accuracy of 
a system that performs identification of protein-
protein interactions using the dependency parser.  
We trained a dependency parser with different 
amounts of data from the GENIA Treebank to es-
tablish how the improvement in parse accuracy 
corresponds to improvement in practical task per-
formance in this information extraction task.  
While parsing accuracy clearly increased with 
larger amounts of data, and is likely to continue 
increasing with additional annotation of data for 
the GENIA Treebank, the trend in the accuracy of 
PPI extraction indicates that a sizable improvement 
in parse accuracy may be necessary for improved 
detection of protein interactions. 
When combined with recent findings by Miyao 
et al (2008), our results indicate that further work 
in designing PPI extraction systems that use syn-
tactic dependency features would benefit from 
more adequate syntactic representations or more 
sophisticated use of NLP than simple extraction of 
syntactic subtrees.  Furthermore, to improve accu-
racy in this task, efforts on data annotation should 
focus on task-specific data (manual annotation of 
40
45
50
55
60
65
0 5000 10000 15000
19
protein interactions in biomedical papers), rather 
than on additional training data for syntactic pars-
ers.  While annotation of parser training data might 
seems like a cost-effective choice, since improved 
parser results might be beneficial in a number of 
systems where the parser can be used, our results 
show that, in this particular task, efforts should be 
focused elsewhere, such as the annotation of addi-
tion PPI data.  
Acknowledgements 
We thank the anonymous reviewers for their in-
sightful comments.  This work was partially sup-
ported by Grant-in-Aid for Specially Promoted 
Research (MEXT, Japan), Genome Network 
Project (MEXT, Japan), and Grant-in-Aid for 
Young Scientists (MEXT, Japan). 
References 
 
Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 
22(1):39?71. 
Clegg, A. and Shepherd, A. 2007. Benchmarking natu-
ral-language parsers for biological applications using 
dependency graphs. BMC Bioinformatics, 8:24. 
Erkan, G., A. Ozgur, and D. R. Radev. 2007. Semisu-
pervised classification for extracting protein interac-
tion sentences using dependency parsing. In 
Proceedings of CoNLL-EMNLP 2007. 
Hara, T., Miyao, Y and Tsujii, J. 2007. Evaluating Im-
pact of Re-training a Lexical Disambiguation Model 
on Domain Adaptation of an HPSG Parser. In Pro-
ceedings of the International Conference on Parsing 
Technologies (IWPT). 
Katrenko, S. and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency 
trees. In Proceedings of the first workshop on Know-
ledge Discovery and Emergent Complexity in BioIn-
formatics (KDECB), pages 61?80. 
Kulick, S., A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer, A. Schein and L. Ungar. 2004. 
Integrated Annotation for Biomedical Information 
Extraction. In Proceedings of Biolink 2004: Linking 
Biological Literature, Ontologies and Databases 
(HLT-NAACL workshop). 
Lease, M. and Charniak, E. 2005. Parsing Biomedical 
Literature. In R. Dale, K.-F. Wong, J. Su, and O. 
Kwong, editors, Proceedings of the 2nd International 
Joint Conference on Natural Language Processing 
(IJCNLP'05), volume 3651 of Lecture Notes in 
Computer Science, pages 58 ? 69. 
Miyao, Y., S?tre, R., Sagae, K., Matsuzaki, T. and Tsu-
jii, J. 2008. Task-Oriented Evaluation of Syntactic 
Parsers and Their Representations.  In Proceedings of 
the 46th Annual Meeting of the Association for Com-
putational Linguistics. 
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., 
Riedel, S. and Yuret, D. 2007. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
the CoNLL 2007 Shared Task in EMNLP-CoNLL. 
Nivre, Joakim, Johan Hall, Jens Nilsson, Gulsen Eryi-
git,and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector 
machines. In Proceedings of the Tenth Conference on 
Computational Natural Language Learning, shared 
task session. 
Pyysalo S., Ginter F., Haverinen K., Heimonen J., Sala-
koski T. and Laippala V. 2007. On the unification of 
syntactic annotations under the Stanford dependency 
scheme: A case study on BioInfer and GENIA.  In 
Proceedings of BioNLP 2007: Biological, Transla-
tional and Clinical Language Processing. 
Quirk, C. and Corston-Oliver S. 2006. The impact of 
parse quality on syntactically-informed statistical 
machine translation. In Proceedings of EMNLP 2007. 
S?tre, R., Sagae, K., and Tsujii, J. 2007. Syntactic fea-
tures for protein-protein interaction extraction.  In 
Proceedings of the International Symposium on Lan-
guages in Biology and Medicine (LBM short oral 
presentations). 
Sagae, K. and Lavie, A. 2006. A best-first probabilistic 
shift-reduce parser. In Proceedings of the 
COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 691?698, Sydney, Australia, July. Asso-
ciation for Computational Linguistics. 
Sagae, K., Miyao, Y. and Tsujii, J. 2008. Challenges in 
Mapping of Syntactic Representations for Frame-
work-Independent Parser Evaluation. In Proceedings 
of the Workshop on Automated Syntatic Annotations 
for Interoperable Language Resources at the First 
International Conference on Global Interoperability 
for Language Resources (ICGL'08). 
Tateisi, Y., Yakushiji, A., Ohta, T., and Tsujii, J. 2005. 
Syntax annotation for the GENIA corpus. In Pro-
ceedings Second International Joint Conference on 
Natural Language Processing: Companion Volume 
including Posters/Demos and tutorial abstracts. 
20
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 30?37,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Accelerating the Annotation of Sparse Named Entities
by Dynamic Sentence Selection
Yoshimasa Tsuruoka1, Jun?ichi Tsujii1,2,3 and Sophia Ananiadou1,3
1 School of Computer Science, The University of Manchester, UK
2 Department of Computer Science, The University of Tokyo, Japan
3 National Centre for Text Mining (NaCTeM), Manchester, UK
yoshimasa.tsuruoka@manchester.ac.uk
tsujii@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
Abstract
This paper presents an active learning-like
framework for reducing the human effort for
making named entity annotations in a corpus.
In this framework, the annotation work is per-
formed as an iterative and interactive process
between the human annotator and a proba-
bilistic named entity tagger. At each itera-
tion, sentences that are most likely to con-
tain named entities of the target category are
selected by the probabilistic tagger and pre-
sented to the annotator. This iterative anno-
tation process is repeated until the estimated
coverage reaches the desired level. Unlike ac-
tive learning approaches, our framework pro-
duces a named entity corpus that is free from
the sampling bias introduced by the active
strategy. We evaluated our framework by
simulating the annotation process using two
named entity corpora and show that our ap-
proach could drastically reduce the number
of sentences to be annotated when applied to
sparse named entities.
1 Introduction
Named entities play a central role in conveying im-
portant domain specific information in text, and
good named entity recognizers are often required
in building practical information extraction systems.
Previous studies have shown that automatic named
entity recognition can be performed with a reason-
able level of accuracy by using various machine
learning models such as support vector machines
(SVMs) or conditional random fields (CRFs) (Tjong
Kim Sang and De Meulder, 2003; Settles, 2004;
Okanohara et al, 2006).
However, the lack of annotated corpora, which are
indispensable for training machine learning models,
makes it difficult to broaden the scope of text mining
applications. In the biomedical domain, for exam-
ple, several annotated corpora such as GENIA (Kim
et al, 2003), PennBioIE (Kulick et al, 2004), and
GENETAG (Tanabe et al, 2005) have been created
and made publicly available, but the named entity
categories annotated in these corpora are tailored to
their specific needs and not always sufficient or suit-
able for text mining tasks that other researchers need
to address.
Active learning is a framework which can be used
for reducing the amount of human effort required to
create a training corpus (Dagan and Engelson, 1995;
Engelson and Dagan, 1996; Thompson et al, 1999;
Shen et al, 2004). In active learning, samples that
need to be annotated by the human annotator are
picked up by a machine learning model in an iter-
ative and interactive manner, considering the infor-
mativeness of the samples. Active learning has been
shown to be effective in several natural language
processing tasks including named entity recognition.
The problem with active learning is, however, that
the resulting annotated data is highly dependent on
the machine learning algorithm and the sampling
strategy employed, because active learning anno-
tates only a subset of the given corpus. This sam-
pling bias is not a serious problem if one is to use the
annotated corpus only for their own machine learn-
ing purpose and with the same machine learning al-
gorithm. However, the existence of bias is not desir-
able if one also wants the corpus to be used by other
applications or researchers. For the same reason, ac-
30
tive learning approaches cannot be used to enrich an
existing linguistic corpus with a new named entity
category.
In this paper, we present a framework that enables
one to make named entity annotations for a given
corpus with a reduced cost. Unlike active learn-
ing approaches, our framework aims to annotate all
named entities of the target category contained in
the corpus. Obviously, if we were to ensure 100%
coverage of annotation, there is no way of reducing
the annotation cost, i.e. the human annotator has to
go through every sentence in the corpus. However,
we show in this paper that it is possible to reduce
the cost by slightly relaxing the requirement for the
coverage, and the reduction can be drastic when the
target named entities are sparse.
We should note here that the purpose of this pa-
per is not to claim that our approach is superior to
existing active learning approaches. The goals are
different?while active learning aims at optimizing
the performance of the resulting machine learning-
based tagger, our framework aims to help develop
an unbiased named entity-annotated corpus.
This paper is organized as follows. Section 2 de-
scribes the overall annotation flow in our framework.
Section 3 presents how to select sentences using the
output of a probabilistic tagger. Section 4 describes
how to estimate the coverage during the course of
annotation. Experimental results using two named
entity corpora are presented in section 5. Section 6
describes related work and discussions. Concluding
remarks are given in section 7.
2 Annotating Named Entities by Dynamic
Sentence Selection
Figure 1 shows the overall flow of our annotation
framework. The framework is an iterative process
between the human annotator and a named entity
tagger based on CRFs. In each iteration, the CRF
tagger is trained using all annotated sentences avail-
able and is applied to the unannotated sentences to
select sentences that are likely to contain named
entities of the target category. The selected sen-
tences are then annotated by the human annotator
and moved to the pool of annotated sentences.
This overall flow of annotation framework is very
similar to that of active learning. In fact, the only
1. Select the first n sentences from the corpus and
annotate the named entities of the target cate-
gory.
2. Train a CRF tagger using all annotated sen-
tences.
3. Apply the CRF tagger to the unannotated sen-
tences in the corpus and select the top n sen-
tences that are most likely to contain target
named entities.
4. Annotate the selected sentences.
5. Go back to 2 (repeat until the estimated cover-
age reaches a satisfactory level).
Figure 1: Annotating named entities by dynamic sentence
selection.
differences are the criterion of sentence selection
and the fact that our framework uses the estimated
coverage as the stopping condition. In active learn-
ing, sentences are selected according to their infor-
mativeness to the machine learning algorithm. Our
approach, in contrast, selects sentences that are most
likely to contain named entities of the target cate-
gory. Section 3 elaborates on how to select sentences
using the output of the CRF-based tagger.
The other key in this annotation framework is
when to stop the annotation work. If we repeat the
process until all sentences are annotated, then obvi-
ously there is not merit of using this approach. We
show in section 4 that we can quite accurately esti-
mate how much of the entities in the corpus are al-
ready annotated and use this estimated coverage as
the stopping condition.
3 Selecting Sentences using the CRF
tagger
Our annotation framework takes advantage of the
ability of CRFs to output multiple probabilistic hy-
potheses. This section describes how we obtain
named entity candidates and their probabilities from
CRFs in order to compute the expected number of
named entities contained in a sentence 1.
1We could use other machine learning algorithms for this
purpose as long as they can produce probabilistic output. For
31
3.1 The CRF tagger
CRFs (Lafferty et al, 2001) can be used for named
entity recognition by representing the spans of
named entities using the ?BIO? tagging scheme, in
which ?B? represents the beginning of a named en-
tity, ?I? the inside, and ?O? the outside (See Table 2
for example). This representation converts the task
of named entity recognition into a sequence tagging
task.
A linear chain CRF defines a single log-linear
probabilistic distribution over the possible tag se-
quences y for a sentence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt),
where fk(t, yt, yt?1,xt) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt).
This modeling allows us to define features on states
(?BIO? tags) and edges (pairs of adjacent ?BIO?
tags) combined with observations (e.g. words and
part-of-speech (POS) tags).
The weights of the features are determined
in such a way that they maximize the condi-
tional log-likelihood of the training data2 L(?) =
?N
i=1 log p?(y(i)|x(i)). We use the L-BFGS algo-
rithm (Nocedal, 1980) to compute those parameters.
Table 1 lists the feature templates used in the CRF
tagger. We used unigrams of words/POS tags, and
prefixes and suffixes of the current word. The cur-
rent word is also normalized by lowering capital let-
ters and converting all numerals into ?#?, and used
as a feature. We created a word shape feature from
the current word by converting consecutive capital
letters into ?A?, small letters ?a?, and numerals ?#?.
example, maximum entropy Markov models are a possible al-
ternative. We chose the CRF model because it has been proved
to deliver state-of-the-art performance for named entity recog-
nition tasks by previous studies.
2In the actual implementation, we used L2 norm penalty for
regularization.
Word Unigram wi, wi?1, wi+1 & yi
POS Unigram pi, pi?1, pi+1 & yi
Prefix, Suffix prefixes of wi & yi
suffixes of wi & yi
(up to length 3)
Normalized Word N(wi) & yi
Word Shape S(wi) & yi
Tag Bi-gram true & yi?1yi
Table 1: Feature templates used in the CRF tagger.
3.2 Computing the expected number of named
entities
To select sentences that are most likely to contain
named entities of the target category, we need to
obtain the expected number of named entities con-
tained in each sentence. CRFs are well-suited for
this task as the output is fully probabilistic.
Suppose, for example, that the sentence is ?Tran-
scription factor GATA-1 and the estrogen receptor?.
Table 2 shows an example of the 5-best sequences
output by the CRF tagger. The sequences are rep-
resented by the aforementioned ?BIO? representa-
tion. For example, the first sequence indicates that
there is one named entity ?Transcription factor? in
the sequence. By summing up these probabilistic se-
quences, we can compute the probabilities for pos-
sible named entities in a sentence. From the five se-
quences in Table 2, we obtain the following three
named entities and their corresponding probabilities.
?Transcription factor? (0.677 + 0.242 = 0.916)
?estrogen receptor? (0.242 + 0.009 = 0.251)
?Transcription factor GATA-1? (0.012 + 0.009 =
0.021)
The expected number of named entities in this
sentence can then be calculated as 0.916 + 0.251 +
0.021 = 1.188.
In this example, we used 5-best sequences as an
approximation of all possible sequences output by
the tagger, which are needed to compute the exact
expected number of entities. One possible way to
achieve a good approximation is to use a large N for
N -best sequences, but there is a simpler and more
efficient way 3, which directly produces the exact
3We thank an anonymous reviewer for pointing this out.
32
Probability Transcription factor GATA-1 and the estrogen receptor
0.677 B I O O O O O
0.242 B I O O O B I
0.035 O O O O O O O
0.012 B I I O O O O
0.009 B I I O O B I
: : : : : : : :
Table 2: N-best sequences output by the CRF tagger.
expected number of entities. Recall that named enti-
ties are represented with the ?BIO? tags. Since one
entity always contains one ?B? tag, we can compute
the number of expected entities by simply summing
up the marginal probabilities for the ?B? tag on each
token in the sentence4.
Once we compute the expected number of enti-
ties for every unannotated sentence in the corpus,
we sort the sentences in descending order of the ex-
pected number of entities and choose the top n sen-
tences to be presented to the human annotator.
4 Coverage Estimation
To ensure the quality of the resulting annotated cor-
pus, it is crucial to be able to know the current cov-
erage of annotation at each iteration in the annota-
tion process. To compute the coverage, however,
one needs to know the total number of target named
entities in the corpus. The problem is that it is not
known until all sentences are annotated.
In this paper, we solve this dilemma by using
an estimated value for the total number of entities.
Then, the estimated coverage can be computed as
follows:
(estimated coverage) = mm + ?i?U Ei
(1)
where m is the number of entities actually annotated
so far and Ei is the expected number of entities in
sentence i, and U is the set of unannotated sentences
in the corpus. At any iteration, m is always known
and Ei is obtained from the output of the CRF tagger
as explained in the previous section.
4The marginal probabilities on each token can be computed
by the forward-backward algorithm, which is much more effi-
cient than computing N -best sequences for a large N .
# Entities Sentences (%)
CoNLL: LOC 7,140 5,127 (36.5%)
CoNLL: MISC 3,438 2,698 (19.2%)
CoNLL: ORG 6,321 4,587 (32.7%)
CoNLL: PER 6,600 4,373 (31.1%)
GENIA: DNA 2,017 5,251 (28.3%)
GENIA: RNA 225 810 ( 4.4%)
GENIA: cell line 835 2,880 (15.5%)
GENIA: cell type 1,104 5,212 (28.1%)
GENIA: protein 5,272 13,040 (70.3%)
Table 3: Statistics of named entities.
5 Experiments
We carried out experiments to see how our method
can improve the efficiency of annotation process
for sparse named entities. We evaluate our method
by simulating the annotation process using existing
named entity corpora. In other words, we use the
gold-standard annotations in the corpus as the anno-
tations that would be made by the human annotator
during the annotation process.
5.1 Corpus
We used two named entity corpora for the exper-
iments. One is the training data provided for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), which consists of 14,041 sen-
tences and includes four named entity categories
(LOC, MISC, ORG, and PER) for the general do-
main. The other is the training data provided for
the NLPBA shared task (Kim et al, 2004), which
consists of 18,546 sentences and five named entity
categories (DNA, RNA, cell line, cell type, and pro-
tein) for the biomedical domain. This corpus is cre-
ated from the GENIA corpus (Kim et al, 2003) by
merging the original fine-grained named entity cate-
gories.
33
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 2: Annotation of LOC in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 3: Annotation of MISC in the CoNLL corpus.
Table 3 shows statistics of the named entities in-
cluded in the corpora. The first column shows the
number of named entities for each category. The
second column shows the number of the sentences
that contain the named entities of each category. We
can see that some of the named entity categories are
very sparse. For example, named entities of ?RNA?
appear only in 4.4% of the sentences in the corpus.
In contrast, named entities of ?protein? appear in
more than 70% of the sentences in the corpus.
In the experiments reported in the following sec-
tions, we do not use the ?protein? category because
there is no merit of using our framework when most
sentences are relevant to the target category.
5.2 Results
We carried out eight sets of experiments, each of
which corresponds to one of those named entity cat-
egories shown in Table 3 (excluding the ?protein?
category). The number of sentences selected in each
iteration (the value of n in Figure 1) was set to 100
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 4: Annotation of ORG in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 5: Annotation of PER in the CoNLL corpus.
throughout all experiments.
Figures 2 to 5 show the results obtained on the
CoNLL data. The figures show how the coverage
increases as the annotation process proceeds. The
x-axis shows the number of annotated sentences.
Each figure contains three lines. The normal line
represents the coverage actually achieved, which is
computed as follows:
(coverage) = entities annotatedtotal number of entities . (2)
The dashed line represents the coverage estimated
by using equation 1. For the purpose of comparison,
the dotted line shows the coverage achieved by the
baseline annotation strategy in which sentences are
selected sequentially from the beginning to the end
in the corpus.
The figures clearly show that our method can
drastically accelerate the annotation process in com-
parison to the baseline annotation strategy. The im-
provement is most evident in Figure 3, in which
34
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 6: Annotation of DNA in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 7: Annotation of RNA in the GENIA corpus.
named entities of the category ?MISC? are anno-
tated.
We should also note that coverage estimation was
surprisingly accurate. In all experiments, the differ-
ence between the estimated coverage and the real
coverage was very small. This means that we can
safely use the estimated coverage as the stopping
condition for the annotation work.
Figures 6 to 9 show the experimental results on
the GENIA data. The figures show the same char-
acteristics observed in the CoNLL data. The accel-
eration by our framework was most evident for the
?RNA? category.
Table 4 shows how much we can save the annota-
tion cost if we stop the annotation process when the
estimated coverage reaches 99%. The first column
shows the coverage actually achieved and the second
column shows the number and ratio of the sentences
annotated in the corpus. This table shows that, on
average, we can achieve a coverage of 99.0% by an-
notating 52.4% of the sentences in the corpus. In
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 8: Annotation of cell line in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 9: Annotation of cell type in the GENIA corpus.
other words, we could roughly halve the annotation
cost by accepting the missing rate of 1.0%.
As expected, the cost reduction was most drastic
when ?RNA?, which is the most sparse named entity
category (see Table 3), was targeted. The cost reduc-
tion was more than seven-fold. These experimental
results confirm that our annotation framework is par-
ticularly useful when applied to sparse named enti-
ties.
Table 4 also shows the timing information on the
experiments 5. One of the potential problems with
this kind of active learning-like framework is the
computation time required to retrain the tagger at
each iteration. Since the human annotator has to
wait while the tagger is being retrained, the compu-
tation time required for retraining the tagger should
not be very long. In our experiments, the worst
case (i.e. DNA) required 443 seconds for retrain-
ing the tagger at the last iteration, but in most cases
5We used AMD Opteron 2.2GHz servers for the experiments
and our CRF tagger is implemented in C++.
35
Coverage Sentences Annotated (%) Cumulative Time (second) Last Interval (second)
CoNLL: LOC 99.1% 7,600 (54.1%) 3,362 92
CoNLL: MISC 96.9% 5,400 (38.5%) 1,818 61
CoNLL: ORG 99.7% 8,900 (63.4%) 5,201 104
CoNLL: PER 98.0% 6,200 (44.2%) 2,300 75
GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443
GENIA: RNA 99.2% 2,500 (13.5%) 822 56
GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284
GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295
Average 99.0% - (52.4%) - -
Table 4: Coverage achieved when the estimated coverage reached 99%.
the training time for each iteration was kept under
several minutes.
In this work, we used the BFGS algorithm for
training the CRF model, but it is probably possible to
further reduce the training time by using more recent
parameter estimation algorithms such as exponenti-
ated gradient algorithms (Globerson et al, 2007).
6 Discussion and Related Work
Our annotation framework is, by definition, not
something that can ensure a coverage of 100%. The
seriousness of a missing rate of, for example, 1% is
not entirely clear?it depends on the application and
the purpose of annotation. In general, however, it
is hard to achieve a coverage of 100% in real an-
notation work even if the human annotator scans
through all sentences, because there is often ambi-
guity in deciding whether a particular named entity
should be annotated or not. Previous studies report
that inter-annotator agreement rates with regards to
gene/protein name annotation are f-scores around
90% (Morgan et al, 2004; Vlachos and Gasperin,
2006). We believe that the missing rate of 1% can be
an acceptable level of sacrifice, given the cost reduc-
tion achieved and the unavoidable discrepancy made
by the human annotator.
At the same time, we should also note that our
framework could be used in conjunction with ex-
isting methods for semi-supervised learning to im-
prove the performance of the CRF tagger, which
in turn will improve the coverage. It is also pos-
sible to improve the performance of the tagger by
using external dictionaries or using more sophis-
ticated probabilistic models such as semi-Markov
CRFs (Sarawagi and Cohen, 2004). These enhance-
ments should further improve the coverage, keeping
the same degree of cost reduction.
The idea of improving the efficiency of annota-
tion work by using automatic taggers is certainly not
new. Tanabe et al (2005) applied a gene/protein
name tagger to the target sentences and modified
the results manually. Culotta and McCallum (2005)
proposed to have the human annotator select the
correct annotation from multiple choices produced
by a CRF tagger for each sentence. Tomanek et
al. (2007) discuss the reusability of named entity-
annotated corpora created by an active learning ap-
proach and show that it is possible to build a cor-
pus that is useful to different machine learning algo-
rithms to a certain degree.
The limitation of our framework is that it is use-
ful only when the target named entities are sparse
because the upper bound of cost saving is limited
by the proportion of the relevant sentences in the
corpus. Our framework may therefore not be suit-
able for a situation where one wants to make an-
notations for named entities of many categories si-
multaneously (e.g. creating a corpus like GENIA
from scratch). In contrast, our framework should be
useful in a situation where one needs to modify or
enrich named entity annotations in an existing cor-
pus, because the target named entities are almost al-
ways sparse in such cases. We should also note that
named entities in full papers, which recently started
to attract much attention, tend to be more sparse than
those in abstracts.
7 Conclusion
We have presented a simple but powerful framework
for reducing the human effort for making name en-
tity annotations in a corpus. The proposed frame-
work allows us to annotate almost all named entities
36
of the target category in the given corpus without
having to scan through all the sentences. The frame-
work also allows us to know when to stop the anno-
tation process by consulting the estimated coverage
of annotation.
Experimental results demonstrated that the frame-
work can reduce the number of sentences to be anno-
tated almost by half, achieving a coverage of 99.0%.
Our framework was particularly effective when the
target named entities were very sparse.
Unlike active learning, this work enables us to
create a named entity corpus that is free from the
sampling bias introduced by the active learning strat-
egy. This work will therefore be especially useful
when one needs to enrich an existing linguistic cor-
pus (e.g. WSJ, GENIA, or PennBioIE) with named
entity annotations for a new semantic category.
Acknowledgment
This work is partially supported by BBSRC grant
BB/E004431/1. The UK National Centre for Text
Mining is sponsored by the JISC/BBSRC/EPSRC.
References
Aron Culotta and Andrew McCallum. 2005. Reducing
labeling effort for structured prediction tasks. In Pro-
ceedings of AAAI-05, pages 746?751.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL, pages 319?326.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proceedings of ICML, pages 305?
312.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (Suppl. 1):180?182.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein, and
Lyle Ungar. 2004. Integrated annotation for biomed-
ical information extraction. In Proceedings of HLT-
NAACL 2004 Workshop: Biolink 2004, pages 61?68.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37:396?410.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In COLING 2004 International Joint workshop
on Natural Language Processing in Biomedicine and
its Applications (NLPBA/BioNLP) 2004, pages 107?
110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL,
pages 589?596, Barcelona, Spain.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl 1):S3.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of ICML, pages 406?414.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of EMNLP-CoNLL,
pages 486?495.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
37
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 90?91,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Prediction of Protein Sub-cellular Localization
using Information from Texts and Sequences
Hong-Woo Chun1,2,3 Chisato Yamasaki2,3 Naomi Saichi2,3 Masayuki Tanaka2,3
chun@dbcls.rois.ac.jp, {chisato-yamasaki, nao-saichi, masa-tanaka}@aist.go.jp
Teruyoshi Hishiki3 Tadashi Imanishi3,5 Takashi Gojobori3,6
{t-hishiki, t.imanishi, t-gojobori}@aist.go.jp
Jin-Dong Kim4 Jun?ichi Tsujii4,7,8 Toshihisa Takagi1,9
{jdkim, tsujii}@is.s.u-tokyo.ac.jp, takagi@dbcls.rois.ac.jp
1 Database Center for Life Science, Research Organization of Information and System,
Engineering 12th Bldg., University of Tokyo, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, 113-0032, Japan
2 Japan Biological Information Research Center, Japan Biological Informatics Consortium
3 Biological Information Research Center,
National Institute of Advanced Industrial Science and Technology, Japan
4 Department of Computer Science, University of Tokyo, Japan
5 Graduate School of Information Science and Technology, Hokkaido University, Japan
6 Center for Information Biology and DNA Data Bank of Japan, National Institute of Genetics
7 School of Informatics, University of Manchester, UK
8 National Centre for Text Mining, UK
9 Department of Computational Biology, University of Tokyo, Japan
Abstract
This paper presents a novel prediction ap-
proach for protein sub-cellular localization. We
have incorporated text and sequence-based ap-
proaches.
1 Introduction
Natural Language Processing (NLP) has tackled and
solved a lot of prediction problems in Biology. One
practical research issue is Protein Sub-Cellular Lo-
calization (PSL) Prediction. Many previous ap-
proaches have combined information from both texts
and sequences by a machine learning (ML) technique
(Shatkay et al, 2007). All of them have not used tra-
ditional NLP techniques such as parsing. Our aim
is to develop a novel PSL prediction system using
information from texts and sequences. At the same
time, we demonstrated the effectiveness of the tra-
ditional NLP and the sequence-based features in the
viewpoint of the text-based approach.
2 Methodology
A Maximum Entropy-based ML technique has been
used to combine information from both texts and se-
quences. To develop a supervised ML-based predic-
tion system, an annotated corpus is needed to train
the system. However, there is no publicly available
corpus that contains the PSL. Therefore, we have
constructed a corpus using GENIA corpus as an ini-
tial data, because the annotation of Protein and Cel-
lular component in GENIA corpus is already done
by human experts. The new types of annotation con-
tain two tasks. The first annotation is to classify
1,117 cellular components in GENIA corpus into 11
locations, and the second annotation is to catego-
rize a relation between a protein and a location into
positive, negative, and neutral. Biologists selected
11 locations based on Gene Ontology: Cytoplasm,
Cytoskeleton, Endoplasmic reticulum, Extracellular,
Golgi apparatus, Granule, Lysosome, Mitochondria,
Nucleus, Peroxisome, and Plasma membrane. The
number of co-occurrences in GENIA corpus is 864.
1 Three human experts annotated with 79.49% of
inter-annotator agreement. For calculating the inter-
annotator agreement, all annotators annotated 117
1The co-occurrence in the proposed approach is a sentence
that contains at least one pair of protein and cellular component
names.
90
# Relevant Performance : F-score (Precision, Recall)
Location relations Baseline Text Sequence Text + Sequence
Nucleus 173 0.282 (0.164, 1.0) 0.764 (0.736, 0.794) 0.725 (0.569, 1.000) 0.778 (0.758, 0.798)
Cytoplasm 94 0.163 (0.089, 1.0) 0.828 (0.804, 0.852) 0.788 (0.657, 0.984) 0.828 (0.804, 0.852)
Plasma membrane 23 0.043 (0.022, 1.0) 0.875 (0.814, 0.946) 0.857 (0.766, 0.973) 0.885 (0.841, 0.932)
Table 1: Performance of protein sub-cellular localization prediction for each location.
co-occurrences. From the texts, we used eight fea-
tures: (1) protein and cellular component names an-
notated by human experts, (2) adjacent one and two
words of names, (3) bag of words, (4) order of names,
(5) distance between names, (6) syntactic category
of names, (7) predicates of names, and (8) part-of-
speech of predicates. To analyze the syntactic struc-
ture, we used the ENJU full parser whose output is
predicate-argument structures of a sentence.
To combine the information from sequences, we
attempted to predict PSL for all proteins in GE-
NIA corpus by two existing sequence-based meth-
ods: WoLF PSORT (Horton et al, 2006) and SOSUI
(Hirokawa et al, 1998). Approximately 14% of pro-
tein names in GENIA corpus obtained results. From
the sequences, we used two features: (1) existence
of the sequence-based results, and (2) the number of
sequence-based results.
3 Experimental results and Conclusion
The proposed approach has integrated text and
sequence-based approaches. To evaluate the system,
we performed 10-fold cross validation using 864 co-
occurrences including positive, negative, and neutral
relations. We measured the precision, recall, and
F-score of the system for all experiments. Among
864 co-occurrences in GENIA corpus, 301 positive
or negative co-occurrences have been considered as
relevant relations, and the remaining 563 neutral re-
lations have been considered as irrelevant relations.
Four approaches have been compared based on
three locations in Table 1. The four approaches are
baseline, text-based approach, sequence-based ap-
proach, and integration of the text and sequence-
based approaches. Baseline experiment used an as-
sumption: there is a relevant relation if a protein and
a cellular component names occur together in a co-
occurrence. The three locations selected when there
are the sequence-based results and the number of rel-
evant relations is more than one. All experiments
showed that the integration of text and sequence-
based approaches is the best, even though the exper-
iments for Cytoplasm showed the best performance
at both the text-based approach and the integration
approach.
A new prediction method has been developed for
protein sub-cellular localization, and it has integrated
text and sequence-based approach using an ML tech-
nique. The traditional NLP techniques contributed
to improve performance of the text-based approach,
and the text and sequence-based approaches recipro-
cally contributed to obtain a improved PSL predic-
tion method. The newly constructed corpus will be
included in the next version of GENIA corpus. There
are weak points in the proposed approach. The cur-
rent evaluation method has been focusing on eval-
uating the text-based approach, and the results of
the sequence-based approach were obtained for only
14% of proteins in GENIA corpus, so these situations
might be the reason that the sequence-based approach
did contribute a little. Thus, we need to evaluate the
proposed approach with a more reasonable method.
Acknowledgments
We acknowledge Fusano Todokoro for her technical
assistance.
References
Paul Horton, Keun-Joon Park, Takeshi Obayashi and
Kenta Nakai. 2006. Protein Subcellular Localization
Prediction with WoLF PSORT. Asia Pacific Bioinfor-
matics Conference (APBC), pp. 39?48.
Takatsugu Hirokawa, Seah Boon-Chieng and Shigeki Mi-
taku. 1998. SOSUI: classification and secondary
structure prediction system for membrane proteins.
Bioinformatics, 14(4): pp. 378?379.
Hagit Shatkay, Annette Ho?glund, Scott Brady, Torsten
Blum, Pierre Do?nnes and Oliver Kohlbacher. 2007.
SherLoc: high-accuracy prediction of protein subcellu-
lar localization by integrating text and protein sequence
data. Bioinformatics., 23(11): pp. 1410?1417
91
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 118?119,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Raising the Compatibility of Heterogeneous Annotations:
A Case Study on Protein Mention Recognition
Yue Wang? Kazuhiro Yoshida? Jin-Dong Kim? Rune S?tre? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{wangyue, kyoshida, jdkim, rune.saetre, tsujii}@is.s.u-tokyo.ac.jp
Abstract
While there are several corpora which claim
to have annotations for protein references,
the heterogeneity between the annotations is
recognized as an obstacle to develop expen-
sive resources in a synergistic way. Here we
present a series of experimental results which
show the differences of protein mention an-
notations made to two corpora, GENIA and
AImed.
1 Introduction
There are several well-known corpora with protein
mention annotations. It is a natural request to bene-
fit from the existing annotations, but the heterogene-
ity of the annotations remains an obstacle. The het-
erogeneity is caused by different definitions of ?pro-
tein?, annotation conventions, and so on.
It is clear that by raising the compatibility of an-
notations, we can reduce the performance degrada-
tion caused by the heterogeneity of annotations.
In this work, we design several experiments to
observe the effect of removing or relaxing the het-
erogeneity between the annotations in two corpora.
The experimental results show that if we understand
where the difference is, we can raise the compati-
bility of the heterogeneous annotations by removing
the difference.
2 Corpora and protein mention recognizer
We used two corpora: the GENIA corpus (Kim
et al, 2003), and the AImed corpus (Bunescu and
Mooney, 2006). There are 2,000 MEDLINE ab-
stracts and 93,293 entities in the GENIA corpus.
?
??
??
??
??
??
??
??
??
??
?? ?? ?? ?? ??? ??? ??? ???????
???????????????????
???
????
Proceedings of the Workshop on BioNLP, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Static Relations: a Piece in the Biomedical Information Extraction Puzzle
Sampo Pyysalo? Tomoko Ohta? Jin-Dong Kim? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{smp,okap,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We propose a static relation extraction task to
complement biomedical information extrac-
tion approaches. We argue that static re-
lations such as part-whole are implicitly in-
volved in many common extraction settings,
define a task setting making them explicit, and
discuss their integration into previously pro-
posed tasks and extraction methods. We fur-
ther identify a specific static relation extrac-
tion task motivated by the BioNLP?09 shared
task on event extraction, introduce an anno-
tated corpus for the task, and demonstrate the
feasibility of the task by experiments showing
that the defined relations can be reliably ex-
tracted. The task setting and corpus can serve
to support several forms of domain informa-
tion extraction.
1 Introduction
Relation Extraction (RE) is a key task in biomedi-
cal Information Extraction (IE). The automatic de-
tection of relevant types of relations ? for various
definitions of relevant ? between entities has been
one of the primary focus points for significant do-
main research efforts over the past decade, and a
substantial number of biomedical RE methods and
annotated corpora have been published (Zweigen-
baum et al, 2007). Motivated by the needs of biolo-
gists and e.g. database curation efforts, most domain
RE efforts target relations involving biologically rel-
evant changes in the involved entities, commonly to
the complete exclusion of static relations. However,
static relations such as entity membership in a fam-
ily and one entity being a part of another are not only
relevant IE targets in themselves but can also play an
important supporting role in IE systems not primar-
ily targeting them.
In this paper, we investigate the role of static re-
lations in causal RE and event extraction. Here,
we use relation extraction in the MUC and ACE
(Sundheim, 1995; Doddington et al, 2004) sense to
refer to the task of extracting binary relations, or-
dered pairs of entities, where both participating enti-
ties must be specified and their roles (agent, patient,
etc.) are fixed by the relation. By contrast, event ex-
traction is understood to involve events (things that
happen) and representations where the number and
roles of participants may vary more freely. We re-
fer to relations where one one entity causes another
to change as causal relations; typical domain exam-
ples are phosphorylation and activation. Static rela-
tions, by contrast, hold between two entities without
implication of change or causality: examples from
the ACE IE task include Physical.Located and Part-
Whole.Artifact.
2 Task definition
In the following, we argue that static relations are
relevant to much of current biomedical IE work,
present a task setting making these relations explicit,
and discuss applications of static relation annotation
and extraction methods.
2.1 Named entity-driven IE and static relations
Named entities (NEs) provide a simple anchor con-
necting text to entities in the real world and thus a
natural starting point for IE. Named entity recog-
nition (NER) is well studied and several biomed-
1
ical NER systems are available (see e.g. (Wilbur
et al, 2007; Leaman and Gonzalez, 2008)), and
most domain IE approaches are NE-driven: a typi-
cal way to cast the RE task is as deciding for each
pair of co-occurring NEs whether a relevant rela-
tion is stated for them in context. Like the previ-
ous LLL and BioCreative2-PPI relation extraction
tasks (Ne?dellec, 2005; Krallinger et al, 2007), the
BioNLP?09 shared task on event extraction (Kim et
al., 2009) similarly proceeds from NEs, requiring
participants to detect events and determine the roles
given NEs play in them.
Any domain IE approach targeting nontrivial
causal NE relations or events necessarily involves
decisions relating to static relations. Consider, for
example, the decision whether to extract a relation
between NE1 and NE2 in the following cases (affects
should here be understood as a placeholder for any
relevant statement of causal relation):
1) NE1 affects NE2 gene
2) NE1 affects NE2 promoter
3) NE1 affects NE2 mutant
4) NE1 affects NE2 antibody
5) NE1 affects NE2 activator
The decision here depends on the interpretation of
the noun compounds (NCs) NE2 gene, NE2 pro-
moter, etc. Depending on the IE setting, one might,
for example, judge that statements (1)?(3) justify the
extraction of an (NE1, NE2) relation, while (4) and
(5) do not. This question is rarely formalized as
a separate (sub)task in domain studies, and meth-
ods targeting e.g. the LLL, BioCreative2-PPI and
BioNLP?09 shared task relations and events must
learn to resolve this question together with the sep-
arate issue of which words and syntactic structures
express relevant causal relations.
2.2 Task setting
The relation extraction problems represented by ex-
amples (1)?(5) above are closely related to the well-
studied issue of NC semantics. However, the prob-
lem extends past simple binary NCs to include judg-
ments on the relations of arbitrary base NPs (nouns
with premodifiers) to contained NEs,
NE1 affects truncated NE2
NE1 affects NE2/NE3 complexes
NE1 affects NE2-dependent phosphatase
and further to relations of NPs with NEs that are syn-
tactically less immediately attached:
NE1 affects first exon of NE2
NE1 affects an element in the NE2 promoter
NE1 affects members of the immediate-early acti-
vation genes family such as NE2
The problem thus encompasses also more general
relations between nominals.
While these different cases could also be studied
as separate tasks, in the current IE context they can
be seen as presenting a continuum of different syn-
tactic realizations of similar relations that also carry
the same implications for further processing. We
propose to treat them together, formulating the spe-
cific task studied in this paper as follows:
Given: named entity NE and another entity E
with their context in text,
Determine: whether there is a relevant static re-
lation R(NE, E) and its type.
Here, relevant relations are defined as those that jus-
tify an inference of some role for the NE in causal re-
lations/events involving E. Additionally, the level of
granularity chosen for typing is chosen according to
the need to determine the role of the NE in the rela-
tions/events. These choices are intentionally depen-
dent on the IE context: we do not expect to be able
to formulate a universally accepted set of relevance
criteria or relations. Our choice of relation scope
and types here follows the perspective of a currently
highly relevant IE problem, the BioNLP?09 shared
task on event extraction. We aim to recognize a set
of relations sufficient to capture the relevant rela-
tionships of the NEs provided as given information
in the shared task (all of protein/gene/RNA type)
and the terms annotated in the GENIA Event corpus
(Kim et al, 2008) as participants in events.
We note that this task setting excludes the recog-
nition of candidate NEs and other entities. The as-
sumption that they are given is analogous to the
common NE-NE causal relation extraction setting.
Further, requiring their recognition would, in our
view, unnecessarily complicate the task with aspects
of NER and NP chunking, well-studied separate
tasks.
We next sketch a formulation of an causal rela-
tion/event extraction task incorporating static rela-
tions and briefly present one possible way in which
2
static relation extraction could be applied in IE set-
tings not explicitly targeting such relations.
2.3 Applications of static relations
In the following, we assume that NEs are detected in
a prior processing step. Consider, then, the task of
extracting relevant information from the following
sentence:
NE1 is a subunit of the complex that inhibits the
expression of mutant forms of NE2
An example causal relation extraction target here
could be
Inhibit(NE1,NE2)
while an event extraction task might aim to recog-
nize the events
E1:Expression(NE2)
E2:Inhibit(NE1, E1)
An IE system directly targeting either representa-
tion will need to simultaneously address issues re-
lating to the causal statements and static relations.
Static relation annotation makes this explicit (square
brackets are used to mark non-NE entities):
Part-Whole.Component-Object(NE1, [complex])
Variant(NE2, [mutant forms])
This type of static relation detection as prior step to
causal relation or event extraction could be applied
in at least two different ways: primarily augment-
ing the extracted information, or alternatively assist-
ing in the extraction of the information considered
above. Assuming the successful extraction of the
above static relations, the input can be reformulated
as
NE1 is a subunit of the [complex] that inhibits the
expression of [mutant forms] of NE2
Then, under the augmented extraction model, the
causal relation and event extraction targets would be,
respectively,
Inhibit([complex],[mutant forms])
and
E1:Expression([mutant forms])
E2:Inhibit([complex], E1)
Taken together with the static relations, this provides
a more detailed representation of the information
stated in the example sentence. Further, simple rules
would suffice to derive the simplified representations
involving only the NEs, and such rules would have
the further benefit of making explicit which inter-
vening static relations are taken to support the infer-
ence that an NE is involved in a stated causal relation
or event.
Alternatively, under the assisted extraction model,
with the assumption that the static relations are taken
to allow the inference that any relation or event hold-
ing of the other entities holds for the NEs, the input
to the causal relation or event extraction system can
be recast as
NE1 is a subunit of the NE?1 that inhibits the ex-
pression of NE?2 of NE2
where NE?1 and NE?2 should be understood as
aliases for NE1 and NE2, respectively. Now, un-
der the causal relation extraction model, each of
the (NE1,NE2), (NE?1, NE2), (NE1,NE?2), (NE?1,NE?2)
pairs can serve as an example of the desired rela-
tion, both for the purposes of training and actual
extraction (the event extraction case can be treated
analogously). By increasing the number of positive
cases, this application of information on static rela-
tions would be expected to have a positive effect on
the performance of the primary causal relation/event
extraction method.
While these two alternatives are only rough
sketches of possible uses of static relation annota-
tion, we expect either could be developed into a
practical implementation. Further, these examples
by no means exhaust the possibilities of this class
of annotation. As static relation extraction can thus
be seen to have multiple potential benefits for both
causal relation and event extraction, we believe the
efforts to pursue static relations as a separate task
and to develop resources specific to this task are jus-
tified.
3 Relations
Based on an analysis of the shared task data (see
Section 4.1), we recognize the static relations illus-
trated in Table 1. In the following, we briefly discuss
the types and their selection.
3
Name Examples
Variant Bcl-6 gene, IL-1 mRNA, wild-type SHP1, TRADD mutant, human IL-1beta,
[cell-surface isoforms] of CD43, phosphorylated CREB protein
PW.Object-Component IL-6 promoter, GR N-terminal transactivation domain, SAA promoter sequence,
proximal IL-2 promoter-enhancer, [transcriptional enhancers] including IFNB
PW.Component-Object NF-kappa B1/RelA heterodimer, p65 homodimer, p50-p65 complex,
STAT1-containing [DNA-binding complex], [heterodimer] of p50 and p65
PW.Member-Collection CREB/ATF family, p21ras small GTP binding proteins,
[non-heat shock genes] such as IL1B, [cellular genes] including GM-CSF
PW.Place-Area beta-globin locus
Table 1: Relations. In examples, NEs are underlined and square brackets are used to mark the extent of non-NE entities
that do not span the entire example text.
3.1 Selection criteria
Relations could be recognized and split into differ-
ent types at a number of different granularities. Mo-
tivated by practical IE applications, we aimed to de-
fine a static relation extraction subtask that fits natu-
rally into existing IE frameworks and to create an-
notation that supplements existing annotation and
avoids overlap in annotated information. The practi-
cal goals also motivate our aim to recognize a min-
imal set of different relation types that can satisfy
other goals, fewer distinctions implying an easier
task and more reliable extraction.
To decide whether to use a single relation type or
introduce several subtypes to annotate a given set of
cases, we aimed to introduce coherent relation types,
each implying consistent further processing. More
specifically, we required that each relation R(NE,
entity) must uniquely and consistently define the re-
lation and roles of the participants, and that in the
relevant IE context the relation alone is sufficient to
decide how to interpret the role of the NE in other
relations/events. Specific examples are given in the
introduction of the chosen relation types below.
In the following, we follow in part the relation
taxonomy and relation definitions of (Winston et al,
1987). However, we recognize that there is no clear
agreement on how to subdivide these relations and
do not suggest this to be the only appropriate choice.
3.2 Part-whole relations
Part-whole, or meronymic, relations are, not surpris-
ingly, the most common class of static relations in
our data: a single generic Part-Whole relation could
capture more than half of the relevant relations in
the corpus. However, although the relations be-
tween the NE and entity in, for example, [complex]
containing NE and [site] in NE are both types of
Part-Whole (below PW) relations, the roles of par-
ticipants are not consistently defined: in PW(NE,
[site]) the entity is a component of the NE, while
in PW(NE, [complex]) the roles are reversed. We
thus recognize separate PW.Object-Component and
PW.Component-Object relations. By contrast, while
the relation between a NE representing a gene and a
site on that gene is is arguably different from the re-
lation between a protein NE and a site on the protein,
we do not distinguish these relations as the annota-
tion would duplicate information available in as part
of the entity typing in the corpus and would further
imply a static relation extraction task that incorpo-
rates aspects of NE recognition.
Also frequent in the data are relations such as
that between a protein and a protein family it be-
longs to. While many cases are clearly identifiable
as PW.Member-Collection relations, others could al-
ternatively be analysed as Class-Member. As in our
context the relations in e.g. P, a member of the [type
F protein family] and P, a [type F protein] imply
the same processing, we will apply the PW.Member-
Collection label to both, as well as to ad hoc col-
lections such as [cellular genes] such as NE, even
if this requires a somewhat relaxed interpretation of
the relation label. Finally, there are a few cases in
our data (e.g. NE locus) that we view as instances of
the PW.Place-Area relation.
3.3 Variant relations
To avoid unnecessary division of relations that im-
ply in our context similar interpretation and process-
ing, we define a task-specific Variant relation that
4
encompasses a set of possible relation types holding
between an NE and its variants along multiple dif-
ferent axes. One significant class of cases annotated
as Variant includes expressions such as NE gene and
NE protein, under the interpretation that NE refers
to the abstract information that is ?realized? as ei-
ther DNA, RNA or protein form, and the entity to
one of these realizations (for alternative interpreta-
tions, see e.g. (Rosario and Hearst, 2001; Heimonen
et al, 2008)).
The Variant relation is also used to annotate NE-
entity relations where the entity expresses a different
state of the NE, such as a phosphorylated or mutated
state. While each possible post-translational modifi-
cation, for example, could alternatively be assigned
a specific relation type, in the present IE context
these would only increase the difficulty of the task
without increasing the applicability of the resulting
annotation.
3.4 Other/Out annotation
We apply a catch-all category, Other/Out, for anno-
tating candidate (NE, entity) pairs between which
there is no relevant static relation. This label is thus
applied to a number of quite different cases: causal
relations, both implied (e.g. NE receptors, NE re-
sponse element) and explicitly stated (NE binds the
[site]), relations where the entity is considered too
far removed from the NE to support reliable infer-
ence of a role for the NE in causal relations/events
involving the entity (e.g. [antibodies] for NE), and
cases where no relation is stated (e.g. NE and other
[proteins]). The diversity of this generic category
of irrelevant cases is a necessary consequence of the
aim to avoid annotation involving decisions directly
relating to other tasks by creating distinctions be-
tween e.g. causal and no relation.
3.5 Sufficiency of the setting and relation types
We have cast the static relation extraction task as al-
ways involving an NE, which in the present context
is further always of a protein, gene or RNA type.
This restriction considerably simplifies the task con-
ceptually and reduces annotation effort as well as ex-
pected extraction difficulty, as the type of only one
of the entities involved in the relation can vary sig-
nificantly. However, it is not obvious that the restric-
tion allows coherent relations types to be defined. If
the corpus contained frequent cases where the stated
relationship of the NE to the entity involved different
types of relevant relations (e.g. collections of parts
of an NE), it would be necessary to either recog-
nized ?mixed? or combined relations or extend the
task to include general entity-entity relations.
Interestingly, during annotation we encountered
only two cases (less than 0.1% of those annotated)
involving two of the recognized relation types at
once: mutant NE promoter and 5? truncation mu-
tants of the NE promoter1. While this result is likely
affected by a number of complex factors (annota-
tion criteria, NE and entity types, granularity of re-
lations, etc.), we find the outcome ? which was nei-
ther planned for nor forced on the data ? a very en-
couraging sign of the sufficiency of the task setting
for this and related domain IE tasks.
4 Data
We created the data set by building on the annota-
tion of the GENIA Event corpus (Kim et al, 2008),
making use of the rich set of annotations already
contained in the corpus: term annotation for NEs
and other entities (Ohta et al, 2002), annotation of
events between these terms, and treebank structure
closely following the Penn Treebank scheme (Tateisi
et al, 2005).
4.1 Annotation
The existing GENIA annotations served as the basis
of the new annotation. We initially selected as can-
didates entities annotated as participating in events
considered in the BioNLP?09 shared task.
As the term annotation includes nesting of en-
tities, NEs contained within these relevant entities
were used as the starting point for the annotation.
We first performed a preliminary study of the rele-
vant static relations occurring between the entities
and NEs occurring within them to determine the
set of relations to annotate. Next, all unique cases
where a selected entity contained an NE were anno-
tated with the appropriate relation based on the con-
tained text of the entity, with the text of the contained
NE normalized away. For the present study, we ex-
cluded from consideration cases where the annota-
1To resolve these cases, we simply ignored the implied Vari-
ant relation.
5
tion indicated simple aliasing (e.g. [CREB/ATF]), a
relation irrelevant to our purpose and found in the
selected data only due to the annotation specifying
one entity but two NEs in these cases. In this step,
830 unique cases representing a total of 1601 entities
containing NEs were annotated.
The nesting structure of the term annotation does
not, however, capture all relevant static relations:
the term annotation scheme disallows discontinuous
terms and annotation of terms with structure more
complex than base NPs. Thus, the possible relations
of NEs to entities to which they were connected e.g.
by a prepositional phrase cannot be directly derived
from the existing annotation. As an example, the
nesting in [NE region] directly suggest the existence
of a relation, while no such connection appears in
[region] of NE. To annotate relations for entities for
which the term annotation does not identify a can-
didate related NE, it is necessary to form (NE, en-
tity) pairs with co-occurring NEs. Even when the
candidate NEs were restricted to those occurring in
the same sentence, the number of such pairs in the
corpus was over 17,000, beyond the scope of what
could be annotated as part of this effort. Further, as
the great majority of co-occurring (NE, entity) pairs
will have no relevant static relation, we used heuris-
tics to increase the proportion of relevant and near-
miss cases in the annotated data.
We first converted the gold standard annotation of
the GENIA treebank (Tateisi et al, 2005) into a de-
pendency representation using the Stanford parser
tools (de Marneffe et al, 2006) and then deter-
mined the shortest paths in the dependency analy-
ses connecting each relevant entity with each NE.
The (NE, entity) pairs were then ordered according
to the length of these paths, on the assumption that
syntactically more closely related entities are more
likely to have a relevant static relation. Annotation
then proceeded on the ordered list of pairs. Dur-
ing the annotation, we further developed more or-
dering heuristics, such as giving higher ranking to
candidate pairs connected by a path that contains
a subpath known to connect pairs with relevant re-
lations. Such known paths were first derived from
the BioInfer static relation annotation (Pyysalo et al,
2007) and later extracted from previously annotated
cases. In this annotation process, judgments were
performed with reference to the full sentence con-
Annotated instances
Relation cont. nonc. total
PW.Object-Component 394 133 527
PW.Component-Object 299 44 343
Variant 253 20 273
PW.Member-Collection 25 124 149
PW.Place-Area 4 1 5
Other/Out 626 778 1404
total 1601 1100 2701
Table 2: Statistics for annotated data. Number of in-
stances given separately for relations annotated between
entities with contained (cont.) and non-contained (nonc.)
NEs.
text. In total, 1100 cases were annotated in this way.
All stages of the annotation process involved only
lists formatted as simple text files for markup and
custom-written software for processing.
Table 2 contains statistics for the annotated data,
showing separately the number of annotated re-
lations of entities to contained and non-contained
NEs. There are interesting differences in the rela-
tion type distribution between these two categories,
reflecting the different ways in which relations are
typically stated. This difference in distribution sug-
gests that it may be beneficial to give the two cases
different treatment in extraction.
4.2 Representation
For simplicitly of use, we provide the annotated data
in two equivalent representations: a simple inline
XML format and a standoff format. The XML for-
mat closely resembles the representation used for the
SemEval-2007 Semantic Relations between Nomi-
nals task (Girju et al, 2007). Here, each NE-Entity
pair is given its own entry with its sentence con-
text in which only the pair is marked. In the alter-
nate standoff representation, all entities appearing in
each sentence are tagged, and the annotated relations
given separately. These representations are easily
processed and should be usable with little modifica-
tion with many existing relation extraction methods.
We further split the data into training,
development-test and test sets according to the
same division applied in the BioNLP?09 shared
task on event extraction. This division allows the
dataset to be easily integrated into settings using the
shared task data, combining static relation and event
extraction approaches.
6
5 Experiments
The selected task setting and representation form a
natural basis for two alternative classification prob-
lems: a binary classification problem for detecting
the presence of any relevant relation, and a multi-
class classification problem where the correct rela-
tion type must also be determined. In the following,
we describe experiments using the dataset in these
two settings. While we apply a state-of-the-art ma-
chine learning method and a fairly expressive repre-
sentation, the aim of the experiments is only to de-
termine the relative difficulty of the relation extrac-
tion task and to establish a moderately competitive
baseline result for the newly created dataset.
We use a linear Support Vector Machine (SVM)
classifier (Chang and Lin, 2001) with N-gram fea-
tures defined over token sequences delimited by the
beginning and end of the entity and the position of
the NE. The NE is treated as a single token and
its text content blinded from the classifier to avoid
overfitting on specific names. Features are gener-
ated from two sequences of tokens: those inside
the entity and, when the NE is not contained in the
entity, those between the entity and the NE (inclu-
sive of the entity and NE at the sequence bound-
aries). In preliminary experiments on the develop-
ment test set we found no clear benefit from includ-
ing N-gram features extracted from a broader con-
text, supporting an assumption that the problem can
be mostly addressed on the basis of local features.
By contrast, preliminary experiments supported the
use of the simple Porter algorithm (Porter, 1980) for
stemming, the inclusion of uni-, bi- and trigram fea-
tures, and normalization of the feature vectors to unit
length; these were adopted for the final experiment.
The SVM regularization parameter was optimized
using a sparse search with evaluation on the devel-
opment test set.
We first reduced the annotated data into a binary
classification problem with the Other/Out class rep-
resenting negative (irrelevant) and the other rela-
tions positive (relevant) cases. The results for this
experiment were very encouraging, giving both a
high classification accuracy of 86.8% and an F-score
of 84.1%. The test set contains 179 positive and
269 negative cases, giving a majority baseline ac-
curacy of 60.0% and an all-true baseline F-score of
P R F
Relevant 81.2 87.2 84.1
PW.Object-Component 94.2 75.4 83.8
PW.Component-Object 60.0 71.2 65.1
Variant 88.0 57.9 69.8
PW.Member-Collection 54.5 37.5 44.4
Table 3: Classification results with (P)recision, (R)ecall
and (F)-score for the binary Relevant/Irrelevant exper-
iment and classwise results for the relevant classes
(PW.Place-Area excluded for lack of data).
57.1%. The classifier notably and statistically sig-
nificantly (McNemar?s test, p < 0.01) outperforms
these simple baselines. We then performed a sep-
arate multiclass classification experiment, predict-
ing the specific type of the relation, also including
the Other/Out type. In this experiment, accuracy re-
mained relatively high at 81.9%, while per-class pre-
cision and recall results (considering each class in
turn positive and all others negative, see Table 3) in-
dicate some remaining challenges. The results vary
somewhat predictably with the number of exam-
ples per relation type (Table 2): while PW.Object-
Component relations can be predicted at high pre-
cision and fair recall, performance for PW.Member-
Collection relations falls behind expectations for a
local relation extraction problem.
To briefly relate these results to domain causal RE
results, we note that the recently proposed state-of-
the-art method of (Airola et al, 2008) was reported
to achieve F-scores ranging between 56.4?76.8% on
five different causal RE corpora in a binary classi-
fication setting. As our relatively simple method
achieves a notably higher 84.1% F-score at the bi-
nary static RE task, we can conclude that this static
RE task is not as difficult as the causal RE tasks.
This is encouraging for the prospects of static RE in
support of domain causal RE and event extraction.
6 Related work
Relations of types that we have here termed static
have figured prominently in the MUC and ACE se-
ries of events that have largely defined the ?gen-
eral domain? IE research program (Sundheim, 1995;
Doddington et al, 2004). In this line of research,
event-type annotation is used (as the name implies)
to capture events, defined as ?[...] something that
happens [...] [that] can frequently be described as a
7
change of state? (LDC, 2005) and relation-type an-
notation is applied for relevant non-causal relation-
ships. General static relations have been studied ex-
tensively also in broader, non-IE contexts (see e.g.
(Girju et al, 2007)).
In the biomedical domain, static relations have re-
ceived relatively little attention. Domain noun com-
pound semantics, including static relations, have
been considered in studies by (Rosario and Hearst,
2001) and (Nakov et al, 2005), but in IE settings
static relations tend to appear only implicitly, as in
the RelEx causal RE system of (Fundel et al, 2007),
or through the causal relations they imply: for ex-
ample, in the AIMed corpus (Bunescu et al, 2005)
statements such as NE1/NE2 complex are annotated
as a binding relation between the two NEs, not Part-
Whole relations with the broader entity. By contrast,
there has been considerable focus on the extraction
of ?things that happen,? dominantly making use of
relation-type corpus annotation and extraction ap-
proaches: a study of five corpora containing primar-
ily causal relation annotation is found in (Pyysalo et
al., 2008); more complete lists of domain corpora
are maintained by Kevin Cohen2 and Jo?rg Haken-
berg3. For a thorough review of recent work in do-
main RE, we refer to (Zweigenbaum et al, 2007).
BioInfer (Pyysalo et al, 2007), to the best of our
knowledge the first domain corpus to include event-
type annotation, also includes annotation for a set
of static relation types. The design of the BioIn-
fer corpus and relationship type ontology as well as
work applying the corpus in jointly targeting event
extraction and static relation extraction (Heimonen
et al, 2008; Bjo?rne et al, 2008) have considerably
influenced the present study. A key difference in fo-
cus is that BioInfer primarily targets NE-NE rela-
tions, while our concern here has been the relations
of NEs with other, non-NE entities, specifically fo-
cusing on the requirements of the BioNLP?09 shared
task. A class of static relations, connecting Mu-
tants and Fragments with their parent proteins, is
annotated in the recently introduced ITI TXM cor-
pora (Alex et al, 2008). While somewhat limited
in the scope of static relations, this annotation cov-
ers an extensive number of instances, over 20,000,
2http://compbio.uchsc.edu/ccp/corpora/obtaining.shtml
3http://www2.informatik.hu-
berlin.de/?hakenber/links/benchmarks.html
and could likely support the development of high-
reliability methods for the extraction extraction of
these specific static relations. As discussed in detail
in Section 4.1, previously published versions of the
GENIA corpus (Kim et al, 2008) contain NE, term
and event annotation, but no static relations have
been annotated in GENIA prior to this effort.
While previously introduced corpora thus cover
aspects of the annotation required to address the
static relation extraction task considered in this pa-
per, we are not aware of previously published re-
sources that would address this task specifically or
contain annotation supporting the entire task as en-
visioned here.
7 Conclusions and future work
In this paper, we have argued for a position for static
relations in biomedical domain IE, specifically
advancing the subtask of extracting static relations
between named entities and other entities appearing
in their context. We explored this subtask in the
specific IE context of the BioNLP?09 shared task on
event extraction, identifying possible instances of
static relations relevant to the task setting. We then
studied these instances of detail, defining a minimal
set of basic static relations argued to be sufficient
to support the type of IE envisioned in the shared
task. We annotated 2701 instances of candidate
static relations, creating the first domain corpus
of static relations explicitly designed to support
IE, and performed experiments demonstrating that
the static relation extraction task can be performed
accurately, yet retains challenges for future work.
The newly annotated corpus is publicly available at
www-tsujii.is.s.u-tokyo.ac.jp/GENIA
to encourage further research on this task.
Acknowledgments
Discussions with members of the BioInfer group
were central for developing many of the ideas pre-
sented here. We are grateful for the efforts of Maki
Niihori in producing supporting annotation applied
in this work. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(Ministry of Education, Culture, Sports, Science and
Technology (MEXT), Japan), and Genome Network
Project (MEXT, Japan).
8
References
Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9(Suppl 11):S2.
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Stuart Roebuck,
Richard Tobin, and Xinglong Wang. 2008. The ITI
TXM corpora: Tissue expressions and protein-protein
interactions. In Proceedings of LREC?08.
Jari Bjo?rne, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. How complex are complex protein-
protein interactions? In Proceedings SMBM?08.
Razvan C Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Kumar
Ramani, and Yuk Wah Wong. 2005. Comparative ex-
periments on learning information extractors for pro-
teins and their interactions. Artificial Intelligence in
Medicine, 33(2):139?155.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of LREC?04, pages 837?840.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx?Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of Se-
mEval?07, pages 13?18.
Juho Heimonen, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. Complex-to-pairwise mapping of
biological relationships using a semantic network rep-
resentation. In Proceedings of SMBM?08.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Proceed-
ings of BioNLP?09.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the second BioCreative PPI
task: Automatic extraction of protein-protein interac-
tions. In Proceedings of BioCreative II, pages 41?54.
LDC. 2005. ACE (automatic content extraction) en-
glish annotation guidelines for events. Technical re-
port, Linguistic Data Consortium.
R. Leaman and G. Gonzalez. 2008. Banner: An exe-
cutable survey of advances in biomedical named entity
recognition. In Proceedings of PSB?08, pages 652?
663.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC?06, pages 449?454.
Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti
Hearst. 2005. Scaling up bionlp: Application of a text
annotation architecture to noun compound bracketing.
In Proceedings of BioLINK?05.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceedings
of LLL?05.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: An annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence (HLT?02), pages 73?77.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(2):130?137.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Antti Airola, Juho Heimonen, and Jari
Bjo?rne. 2008. Comparative analysis of five protein-
protein interaction corpora. BMC Bioinformatics,
9(Suppl. 3):S6.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
EMLNP?01, pages 82?90.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of MUC-6, pages
13?31.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the GE-
NIA corpus. In Proceedings of IJCNLP?05, pages
222?227.
John Wilbur, Larry Smith, and Lorrie Tanabe. 2007.
Biocreative 2 gene mention task. In Proceedings of
BioCreative 2, pages 7?16.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. 2007. Frontiers of biomedical
text mining: Current progress. Briefings in Bioinfor-
matics.
9
Proceedings of the Workshop on BioNLP, pages 106?107,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incorporating GENETAG-style annotation to GENIA corpus
Tomoko Ohta? and Jin-Dong Kim? and Sampo Pyysalo? and Yue Wang? and Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,jdkim,smp,wangyue,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
Proteins and genes are the most important entities in
molecular biology, and their automated recognition
in text is the most widely studied task in biomed-
ical information extraction (IE). Several corpora
containing annotation for these entities have been
introduced, GENIA (Kim et al, 2003; Kim et al,
2008) and GENETAG (Tanabe et al, 2005) being
the most prominent and widely applied. While both
aim to address protein/gene annotation, their an-
notation principles differ notably. One key differ-
ence is that GENETAG annotates the conceptual en-
tity, gene, which is often associated with a function,
while GENIA concentrates on the physical forms of
gene, i.e. protein, DNA and RNA. The difference
has caused serious problems relating to the compat-
ibility and comparability of the annotations. In this
work, we present an extension of GENIA annotation
which integrates GENETAG-style gene annotation.
The new version of the GENIA corpus is the first to
bring together these two types of entity annotation.
2 GGP Annotation
Gene is the basic unit of heredity, which is encoded
in the coding region of DNA. Its physical manifes-
tations as RNA and Protein are often called its prod-
ucts. In our view of these four entity types, gene is
taken as an abstract entity whereas protein, DNA and
RNA are physical entities. While the three physical
entity types are disjoint, the abstract concept, gene,
is defined from a different perspective and is realized
in, not disjoint from, the physical entity types.
The latest public version of GENIA corpus (here-
after ?old corpus?) contains annotations for gene-
Protein DNA RNA GGP
Old Annotation 21,489 8,653 876 N/A
New Annotation 15,452 7,872 863 12,272
Table 1: Statistics on annotation for gene-related entities
related entities, but they are classified into only
physical entity types: Protein, DNA and RNA. The
corpus revisions described in this work are two-fold.
First, annotation for the abstract entity, gene, were
added (Table 1, GGP). To emphasize the character-
istics of the new entity type, which does not dis-
tinguish a gene and its products, we call it GGP
(gene or gene product). Second, the addition of GGP
annotation triggered large-scale removal of Protein,
DNA and RNA annotation instances for cases where
the physical form of the gene was not referred to
(Due to space limitations, we omit RNA from now
on). The time cost involved with this revision was
approximately 500 person-hours.
3 Quality Assessment
To measure the effect of revision, we performed
NER experiments with old and new annotation (Ta-
bles 2 and 3). We split the corpus into disjoint 90%
and 10% parts for use in training and test, respec-
tively. We used the BANNER (Leaman and Gonza-
lez, 2008) NE tagger and created a separate single-
class NER problem for each entity type.
In the old annotation, consistency is moderate
for protein (77.70%), while DNA is problematic
(58.03%). The new GGP annotation has been
achieved in a fairly consistent way (81.44%). How-
ever, the removal of annotation for entities previ-
ously marked as protein or DNA had opposite effects
on the two: better performance for DNA (64.06%),
106
Precision Recall F-score
Protein 80.78 74.84 77.70
DNA 64.90 52.48 58.03
Table 2: NER performance before GGP annotation
Precision Recall F-score
Protein 71.20 56.61 63.08
DNA 69.59 59.35 64.06
GGP 86.86 76.65 81.44
Protein+ 83.22 78.20 80.63
Table 3: NER performance after GGP annotation
Phosphorylation Gene expression
GGP in protein 70% GGP abstract 34%
Protein 25% Protein 24%
GGP abstract 3% GGP in Protein 17%
Peptide 1% GGP in DNA 9%
Table 4: Distribution of theme entity types in GENIA
implying annotation consistency improved with the
removals, but worse for Protein (63.08%).
We find the primary explanation for this effect in
the statistics in Table 1: in the revision, a large num-
ber of protein annotations (6,037) but only a small
number of DNA annotations (780) were replaced
with GGP. To distinguish such GGPs from those em-
bedded in Protein or DNA annotations, we call them
?abstract? GGPs, as they appear in text without in-
formation on their physical form. Nevertheless, in
the old annotation, they had to be annotated as either
protein or DNA, which might have caused inconsis-
tent annotation. However, the statistics show a clear
preference for choosing Protein over DNA. The rad-
ical drop of performance in protein recognition can
then be explained in part as a result of removing this
systematic preference.
Aside from the discussion on whether the pref-
erence is general or specific, we interpret the pref-
erence as a need for ?potential? proteins to be re-
trieved together with ?real? proteins, which was an-
swered by the old protein annotation. To reproduce
this class in the new annotation, we added abstract
GGPs to the Protein annotation and performed an
NER experiment. The result (Table 3, Protein+)
shows a clear improvement over the comparable re-
sult for the old protein annotation.
In conclusion, we argue, the revision of the GE-
NIA annotation, in addition to introducing a new en-
tity class, has led to a significant improvement of
overall consistency.
4 Discussion
Although there are already corpora such as GENE-
TAG with annotation similar to GGPs, we expect
this newly introduced class of annotation to support
existing annotations of GENIA, such as event and
co-reference annotation, opening up new possibili-
ties for application. The quality of entity annota-
tion should be closely related to that of other seman-
tic annotation, e.g. events. For example, the event
type Phosphorylation is about a change on physi-
cal entities, e.g. proteins and peptides, and as such,
it is expected that themes of these events would be
physical entities. On the other hand, the event type
Gene expression is about the manifestation of an ab-
stract entity (gene) as a physical entity (protein) and
would thus be expected to involve both abstract and
physical entities. Statistics from GENIA (Table 4)
show that the theme selection made in event anno-
tation well reflects these characteristics of the two
event types. The observation suggests that there is a
good likelihood that improvement of the entity an-
notation can be further transferred to other semantic
annotation, which is open for future work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
References
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl. 1):i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
R. Leaman and G. Gonzalez. 2008. Banner: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomputing,
pages 652?663.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and W John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
107
Proceedings of the Workshop on BioNLP: Shared Task, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Overview of BioNLP?09 Shared Task on Event Extraction
Jin-Dong Kim? Tomoko Ohta? Sampo Pyysalo? Yoshinobu Kano? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{jdkim,okap,smp,kano,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The paper presents the design and implemen-
tation of the BioNLP?09 Shared Task, and
reports the final results with analysis. The
shared task consists of three sub-tasks, each of
which addresses bio-molecular event extrac-
tion at a different level of specificity. The data
was developed based on the GENIA event cor-
pus. The shared task was run over 12 weeks,
drawing initial interest from 42 teams. Of
these teams, 24 submitted final results. The
evaluation results are encouraging, indicating
that state-of-the-art performance is approach-
ing a practically applicable level and revealing
some remaining challenges.
1 Introduction
The history of text mining (TM) shows that shared
tasks based on carefully curated resources, such
as those organized in the MUC (Chinchor, 1998),
TREC (Voorhees, 2007) and ACE (Strassel et al,
2008) events, have significantly contributed to the
progress of their respective fields. This has also been
the case in bio-TM. Examples include the TREC Ge-
nomics track (Hersh et al, 2007), JNLPBA (Kim et
al., 2004), LLL (Ne?dellec, 2005), and BioCreative
(Hirschman et al, 2007). While the first two ad-
dressed bio-IR (information retrieval) and bio-NER
(named entity recognition), respectively, the last two
focused on bio-IE (information extraction), seeking
relations between bio-molecules. With the emer-
gence of NER systems with performance capable of
supporting practical applications, the recent interest
of the bio-TM community is shifting toward IE.
Similarly to LLL and BioCreative, the
BioNLP?09 Shared Task (the BioNLP task, here-
after) also addresses bio-IE, but takes a definitive
step further toward finer-grained IE. While LLL and
BioCreative focus on a rather simple representation
of relations of bio-molecules, i.e. protein-protein
interactions (PPI), the BioNLP task concerns the
detailed behavior of bio-molecules, characterized as
bio-molecular events (bio-events). The difference in
focus is motivated in part by different applications
envisioned as being supported by the IE methods.
For example, BioCreative aims to support curation
of PPI databases such as MINT (Chatr-aryamontri
et al, 2007), for a long time one of the primary tasks
of bioinformatics. The BioNLP task aims to support
the development of more detailed and structured
databases, e.g. pathway (Bader et al, 2006) or Gene
Ontology Annotation (GOA) (Camon et al, 2004)
databases, which are gaining increasing interest
in bioinformatics research in response to recent
advances in molecular biology.
As the first shared task of its type, the BioNLP
task aimed to define a bounded, well-defined bio-
event extraction task, considering both the actual
needs and the state of the art in bio-TM technology
and to pursue it as a community-wide effort. The
key challenge was in finding a good balance between
the utility and the feasibility of the task, which was
also limited by the resources available. Special con-
sideration was given to providing evaluation at di-
verse levels and aspects, so that the results can drive
continuous efforts in relevant directions. The pa-
per discusses the design and implementation of the
BioNLP task, and reports the results with analysis.
1
Type Primary Args. Second. Args.
Gene expression T(P)
Transcription T(P)
Protein catabolism T(P)
Phosphorylation T(P) Site
Localization T(P) AtLoc, ToLoc
Binding T(P)+ Site+
Regulation T(P/Ev), C(P/Ev) Site, CSite
Positive regulation T(P/Ev), C(P/Ev) Site, CSite
Negative regulation T(P/Ev), C(P/Ev) Site, CSite
Table 1: Event types and their arguments. The type of the
filler entity is specified in parenthesis. The filler entity
of the secondary arguments are all of Entity type which
represents any entity but proteins: T=Theme, C=Cause,
P=Protein, Ev=Event.
2 Task setting
To focus efforts on the novel aspects of the event
extraction task, is was assumed that named entity
recognition has already been performed and the task
was begun with a given set of gold protein anno-
tation. This is the only feature of the task setting
that notably detracts from its realism. However,
given that state-of-the-art protein annotation meth-
ods show a practically applicable level of perfor-
mance, i.e. 88% F-score (Wilbur et al, 2007), we
believe the choice is reasonable and has several ad-
vantages, including focus on event extraction and ef-
fective evaluation and analysis.
2.1 Target event types
Table 1 shows the event types addressed in the
BioNLP task. The event types were selected from
the GENIA ontology, with consideration given to
their importance and the number of annotated in-
stances in the GENIA corpus. The selected event
types all concern protein biology, implying that they
take proteins as their theme. The first three types
concern protein metabolism, i.e. protein production
and breakdown. Phosphorylation is a representa-
tive protein modification event, and Localization and
Binding are representative fundamental molecular
events. Regulation (including its sub-types, Posi-
tive and Negative regulation) represents regulatory
events and causal relations. The last five are uni-
versal but frequently occur on proteins. For the bio-
logical interpretation of the event types, readers are
referred to Gene Ontology (GO) and the GENIA on-
tology.
The failure of p65 translocation to the nucleus . . .
T3 (Protein, 40-46)
T2 (Localization, 19-32)
E1 (Type:T2, Theme:T3, ToLoc:T1)
T1 (Entity, 15-18)
M1 (Negation E1)
Figure 1: Example event annotation. The protein an-
notation T3 is given as a starting point. The extraction
of annotation in bold is required for Task 1, T1 and the
ToLoc:T1 argument for Task 2, and M1 for Task 3.
As shown in Table 1, the theme or themes of all
events are considered primary arguments, that is, ar-
guments that are critical to identifying the event. For
regulation events, the entity or event stated as the
cause of the regulation is also regarded as a primary
argument. For some event types, other arguments
detailing of the events are also defined (Secondary
Args. in Table 1).
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types require only unary arguments, and the
task can be cast as relation extraction between a
predicate (event trigger) and an argument (Protein).
The Binding type is more complex in requiring the
detection of an arbitrary number of arguments. Reg-
ulation events always take a Theme argument and,
when expressed, also a Cause argument. Note that a
Regulation event may take another event as its theme
or cause, a unique feature of the BioNLP task com-
pared to other event extraction tasks, e.g. ACE.
2.2 Representation
In the BioNLP task, events are expressed using three
different types of entities. Text-bound entities (t-
entities hereafter) are represented as text spans with
associated class information. The t-entities include
event triggers (Localization, Binding, etc), protein
references (Protein) and references to other entities
(Entity). A t-entity is represented by a pair, (entity-
type, text-span), and assigned an id with the pre-
fix ?T?, e.g. T1?T3 in Figure 1. An event is ex-
pressed as an n-tuple of typed t-entities, and has
a id with prefix ?E?, e.g. E1. An event modifi-
cation is expressed by a pair, (predicate-negation-
or-speculation, event-id), and has an id with prefix
?M?, e.g. M1.
2
Item Training Devel. Test
Abstract 800 150 260
Sentence 7,449 1,450 2,447
Word 176,146 33,937 57,367
Event 8,597 / 8,615 1,809 / 1,815 3,182 / 3,193
Table 2: Statistics of the data sets. For events,
Task1/Task2 shown separately as secondary arguments
may introduce additional differentiation of events.
2.3 Subtasks
The BioNLP task targets semantically rich event ex-
traction, involving the extraction of several different
classes of information. To facilitate evaluation on
different aspects of the overall task, the task is di-
vided to three sub-tasks addressing event extraction
at different levels of specificity.
Task 1. Core event detection detection of typed,
text-bound events and assignment of given pro-
teins as their primary arguments.
Task 2. Event enrichment recognition of sec-
ondary arguments that further specify the
events extracted in Task 1.
Task 3. Negation/Speculation detection detection
of negations and speculation statements
concerning extracted events.
Task 1 serves as the backbone of the shared task and
is mandatory for all participants. Task 2 involves the
recognition of Entity type t-entities and assignment
of those as secondary event arguments. Task 3 ad-
dresses the recognition of negated or speculatively
expressed events without specific binding to text. An
example is given in Fig. 1.
3 Data preparation
The BioNLP task data were prepared based on the
GENIA event corpus. The data for the training and
development sets were derived from the publicly
available event corpus (Kim et al, 2008), and the
data for the test set from an unpublished portion of
the corpus. Table 2 shows statistics of the data sets.
For data preparation, in addition to filtering out
irrelevant annotations from the original GENIA cor-
pus, some new types of annotation were added to
make the event annotation more appropriate for the
purposes of the shared task. The following sections
describe the key changes to the corpus.
3.1 Gene-or-gene-product annotation
The named entity (NE) annotation of the GENIA
corpus has been somewhat controversial due to dif-
ferences in annotation principles compared to other
biomedical NE corpora. For instance, the NE an-
notation in the widely applied GENETAG corpus
(Tanabe et al, 2005) does not differentiate proteins
from genes, while GENIA annotation does. Such
differences have caused significant inconsistency in
methods and resources following different annota-
tion schemes. To remove or reduce the inconsis-
tency, GENETAG-style NE annotation, which we
term gene-or-gene-product (GGP) annotation, has
been added to the GENIA corpus, with appropriate
revision of the original annotation. For details, we
refer to (Ohta et al, 2009). The NE annotation used
in the BioNLP task data is based on this annotation.
3.2 Argument revision
The GENIA event annotation was made based on
the GENIA event ontology, which uses a loose typ-
ing system for the arguments of each event class.
For example, in Figure 2(a), it is expressed that
the binding event involves two proteins, TRAF2
and CD40, and that, in the case of CD40, its cy-
toplasmic domain takes part in the binding. With-
out constraints on the type of theme arguments,
the following two annotations are both legitimate:
(Type:Binding, Theme:TRAF2, Theme:CD40)
(Type:Binding, Theme:TRAF2,
Theme:CD40 cytoplasmic domain)
The two can be seen as specifying the same event
at different levels of specificity1. Although both al-
ternatives are reasonable, the need to have consis-
tent training and evaluation data requires a consis-
tent choice to be made for the shared task.
Thus, we fix the types of all non-event
primary arguments to be proteins (specifically
GGPs). For GENIA event annotations involving
themes other than proteins, additional argument
types were introduced, for example, as follows:
1In the GENIA event annotation guidelines, annotators are
instructed to choose the more specific alternative, thus the sec-
ond alternative for the example case in Fig. 2(a).
3
(a)
TRAF2 is a ? which binds to the CD40 cytoplasmic domain
GGP GGP PDR
(b)
HMG-I binds to GATA motifs
GGP DDR
(c)
alpha B2 bound the PEBP2 site within the GM-CSF promoter
GGP GGPDDR DDR
Figure 2: Entity annotation to example sentences
from (a) PMID10080948, (b) PMID7575565, and (c)
PMID7605990 (simplified).
(a)
Ah receptor recognizes the B cell transcription factor, BSAP
(b)
Grf40 binds to linker for activation of T cells (LAT)
(c)
expression of p21(WAF1/CIP1) and p27(KIP1)
(d)
included both p50/p50 and p50/p65 dimers
(e)
IL-4 Stat, also known as Stat6
Figure 3: Equivalent entities in example sentences from
(a) PMID7541987 (simplified), (b) PMID10224278, (c)
PMID10090931, (d) PMID9243743, (e) PMID7635985.
(Type:Binding, Theme1:TRAF2, Theme2:CD40,
Site2:cytoplasmic domain)
Note that the protein, CD40, and its domain, cyto-
plasmic domain, are associated by argument num-
bering. To resolve issues related to the mapping
between proteins and related entities systematically,
we introduced partial static relation annotation for
relations such as Part-Whole, drawing in part on
similar annotation of the BioInfer corpus (Pyysalo
et al, 2007). For details of this part of the revision
process, we refer to (Pyysalo et al, 2009).
Figure 2 shows some challenging cases. In (b),
the site GATA motifs is not identified as an argument
of the binding event, because the protein containing
it is not stated. In (c), among the two sites (PEBP2
site and promoter) of the gene GM-CSF, only the
more specific one, PEBP2, is annotated.
3.3 Equivalent entity references
Alternative names for the same object are fre-
quently introduced in biomedical texts, typically
through apposition. This is illustrated in Figure 3(a),
where the two expressions B cell transcription fac-
tor and BSAP are in apposition and refer to the
same protein. Consequently, in this case the fol-
lowing two annotations represent the same event:
(Type:Binding, Theme:Ah receptor,
Theme:B cell transcription factor)
(Type:Binding, Theme:Ah receptor, Theme:BSAP)
In the GENIA event corpus only one of these is an-
notated, with preference given to shorter names over
longer descriptive ones. Thus of the above exam-
ple events, the latter would be annotated. How-
ever, as both express the same event, in the shared
task evaluation either alternative was accepted as
correct extraction of the event. In order to im-
plement this aspect of the evaluation, expressions
of equivalent entities were annotated as follows:
Eq (B cell transcription factor, BSAP)
The equivalent entity annotation in the revised GE-
NIA corpus covers also cases other than simple ap-
position, illustrated in Figure 3. A frequent case in
biomedical literature involves use of the slash sym-
bol (?/?) to state synonyms. The slash symbol is
ambiguous as it is used also to indicate dimerized
proteins. In the case of p50/p50, the two p50 are
annotated as equivalent because they represent the
same proteins at the same state. Note that although
rare, also explicitly introduced aliases are annotated,
as in Figure 3(e).
4 Evaluation
For the evaluation, the participants were given the
test data with gold annotation only for proteins. The
evaluation was then carried out by comparing the
annotation predicted by each participant to the gold
annotation. For the comparison, equality of anno-
tations is defined as described in Section 4.1. The
evaluation results are reported using the standard
recall/precision/f-score metrics, under different cri-
teria defined through the equalities.
4.1 Equalities and Strict matching
Equality of events is defined as follows:
Event Equality equality holds between any two
events when (1) the event types are the same,
(2) the event triggers are the same, and (3) the
arguments are fully matched.
4
A full matching of arguments between two events
means there is a perfect 1-to-1 mapping between the
two sets of arguments. Equality of individual argu-
ments is defined as follows:
Argument Equality equality holds between any
two arguments when (1) the role types are the
same, and (2-1) both are t-entities and equality
holds between them, or (2-2) both are events
and equality holds between them.
Due to the condition (2-2), event equality is defined
recursively for events referring to events. Equality
of t-entities is defined as follows:
T-entity Equality equality holds between any two
t-entities when (1) the entity types are the same,
and (2) the spans are the same.
Any two text spans (beg1, end1) and (beg2, end2),
are the same iff beg1 = beg2 and end1 = end2.
Note that the event triggers are also t-entities thus
their equality is defined by the t-entity equality.
4.2 Evaluation modes
Various evaluation modes can be defined by varying
equivalence criteria. In the following, we describe
three fundamental variants applied in the evaluation.
Strict matching The strict matching mode requires
exact equality, as defined in section 4.1. As some
of its requirements may be viewed as unnecessarily
precise, practically motivated relaxed variants, de-
scribed in the following, are also applied.
Approximate span matching The approximate
span matching mode is defined by relaxing the
requirement for text span matching for t-entities.
Specifically, a given span is equivalent to a gold
span if it is entirely contained within an extension
of the gold span by one word both to the left and
to the right, that is, beg1 ? ebeg2 and end1 ?
eend2, where (beg1, end1) is the given span and
(ebeg2, eend2) is the extended gold span.
Approximate recursive matching In strict match-
ing, for a regulation event to be correct, the events it
refers to as theme or cause must also be be strictly
correct. The approximate recursive matching mode
is defined by relaxing the requirement for recursive
event matching, so that an event can match even
if the events it refers to are only partially correct.
Event Release date
Announcement Dec 8
Sample data Dec 15
Training data Jan 19 ? 21, Feb 2 (rev1), Feb 10 (rev2)
Devel. data Feb 7
Test data Feb 22 ? Mar 2
Submission Mar 2 ? Mar 9
Table 3: Shared task schedule. The arrows indicate a
change of schedule.
Specifically, for partial matching, only Theme argu-
ments are considered: events can match even if re-
ferred events differ in non-Theme arguments.
5 Schedule
The BioNLP task was held for 12 weeks, from the
sample data release to the final submission. It in-
cluded 5 weeks of system design period with sam-
ple data, 6 weeks of system development period with
training and development data, and a 1 week test pe-
riod. The system development period was originally
planned for 5 weeks but extended by 1 week due to
the delay of the training data release and the revi-
sion. Table 3 shows key dates of the schedule.
6 Supporting Resources
To allow participants to focus development efforts
on novel aspects of event extraction, we prepared
publicly available BioNLP resources readily avail-
able for the shared task. Several fundamental
BioNLP tools were provided through U-Compare
(Kano et al, 2009)2, which included tools for to-
kenization, sentence segmentation, part-of-speech
tagging, chunking and syntactic parsing.
Participants were also provided with the syntactic
analyses created by a selection of parsers. We ap-
plied two mainstream Penn Treebank (PTB) phrase
structure parsers: the Bikel parser3, implementing
Collins? parsing model (Bikel, 2004) and trained
on PTB, and the reranking parser of (Charniak
and Johnson, 2005) with the self-trained biomed-
ical parsing model of (McClosky and Charniak,
2008)4. We also applied the GDep5, native de-
pendency parser trained on the GENIA Treebank
2http://u-compare.org/
3http://www.cis.upenn.edu/?dbikel/software.html
4http://www.cs.brown.edu/?dmcc/biomedical.html
5http://www.cs.cmu.edu/?sagae/parser/gdep/
5
NLP Task
Team Task Org Word Chunking Parsing Trigger Argument Ext. Resources
UTurku 1-- 3C+2BI Porter MC SVM SVM (SVMlight)
JULIELab 1-- 1C+2L+2B OpenNLP OpenNLP GDep Dict+Stat SVM(libSVM) UniProt, Mesh,
Porter ME(Mallet) GOA, UMLS
ConcordU 1-3 3C Stanford Stanford Dict+Stat Rules WordNet, VerbNet,
UMLS
UT+DBCLS 12- 2C Porter MC Dict MLN(thebeast)
CCG
VIBGhent 1-3 2C+1B Porter, Stanford Dict SVM(libSVM)
UTokyo 1-- 3C GTag GDep, Dict ME(liblinear) UIMA
Enju
UNSW 1-- 1C+1B GDep CRF Rules WordNet, MetaMap
UZurich 1-- 3C LingPipe, LTChunk Pro3Gres Dict Rules
Morpha
ASU+HU+BU 123 6C+2BI Porter BioLG, Dict Rules Lucene
Charniak Rules
Cam 1-- 3C Porter RASP Dict Rules
UAntwerp 12- 3C GTag GDep MBL MBL(TiMBL)
Rules
UNIMAN 1-- 4C+2BI Porter GDep Dict, CRF SVM MeSH, GO
GTag Rules
SCAI 1-- 1C Rules
UAveiro 1-- 1C+1L NooJ NooJ Rules BioLexicon
USzeged 1-3 3C+1B GTag Dict, VSM C4.5(WEKA) BioScope
Rules
NICTA 1-3 4C GTag ERG CRF(CRF++) Rules JULIE
CNBMadrid 12- 2C+1B Porter, GTag CBR
GTag Rules
CCP-BTMG 123 7C LingPipe LingPipe OpenDMAP LingPipe, CM Rules GO, SO, MIO,
UIMA
CIPS-ASU 1-- 3C MontyTagger Custom Stanford CRF(ABNER) Rules,
NB(WEKA)
UMich 1-- 2C Stanford MC Dict SVM(SVMlight)
PIKB 1-- 5C+2B MIRA MIRA
KoreaU 1-- 5C GTag GDep Rules, ME ME WSJ
Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-
CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,
CBR=Case-Based Reasoning, CM=ConceptMapper.
(Tateisi et al, 2005), and a version of the C&C CCG
deep parser6 adapted to biomedical text (Rimell and
Clark, 2008).
The text of all documents was segmented and to-
kenized using the GENIA Sentence Splitter and the
GENIA Tagger, provided by U-Compare. The same
segmentation was enforced for all parsers, which
were run using default settings. Both the native out-
put of each parser and a representation in the popular
Stanford Dependency (SD) format (de Marneffe et
al., 2006) were provided. The SD representation was
created using the Stanford tools7 to convert from the
PTB scheme, the custom conversion introduced by
(Rimell and Clark, 2008) for the C&C CCG parser,
and a simple format-only conversion for GDep.
7 Results and Discussion
7.1 Participation
In total, 42 teams showed interest in the shared task
and registered for participation, and 24 teams sub-
6http://svn.ask.it.usyd.edu.au/trac/candc/wiki
7http://nlp.stanford.edu/software/lex-parser.shtml
mitted final results. All 24 teams participated in the
obligatory Task 1, six in each of Tasks 2 and 3, and
two teams completed all the three tasks.
Table 4 shows a profile of the 22 final teams,
excepting two who wished to remain anonymous.
A brief examination on the team organization (the
Org column) shows a computer science background
(C) to be most frequent among participants, with
less frequent participation from bioinformaticians
(BI), biologists (B) and liguists (L). This may be
attributed in part to the fact that the event extrac-
tion task required complex computational modeling.
The role of computer scientists may be emphasized
in part due to the fact that the task was novel to most
participants, requiring particular efforts in frame-
work design and implementation and computational
resources. This also suggests there is room for im-
provement from more input from biologists.
7.2 Evaluation results
The final evaluation results of Task 1 are shown in
Table 5. The results on the five event types involv-
6
Team Simple Event Binding Regulation All
UTurku 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
JULIELab 59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66
ConcordU 49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62
UT+DBCLS 55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35
VIBGhent 54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54
UTokyo 45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88
UNSW 45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92
UZurich 44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78
ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09
Cam 39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80
UAntwerp 41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58
UNIMAN 50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35
SCAI 43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26
UAveiro 43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38
Team 24 41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10
USzeged 47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21
NICTA 31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29
CNBMadrid 50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15
CCP-BTMG 28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66
CIPS-ASU 39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74
UMich 52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28
PIKB 26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25
Team 09 27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04
KoreaU 20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31
Table 5: Evaluation results of Task 1 (recall / precision / f-score).
Team All Site for Phospho.(56) AtLoc & ToLoc (65) All Second Args.
UT+DBCLS 35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
UAntwerp 21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76
ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00
Team 24 22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66 21.54 / 66.67 / 32.56 30.10 / 76.62 / 43.22
CCP-BTMG 13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96
CNBMadrid 25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57 32.31 / 47.73 / 38.53 50.00 / 09.71 / 16.27
Table 6: Evaluation results for Task 2.
ing only a single primary theme argument are shown
in one merged class, ?Simple Event?. The broad per-
formance range (31% ? 70%) indicates even the ex-
traction of simple events is not a trivial task. How-
ever, the top-ranked systems show encouraging per-
formance, achieving or approaching 70% f-score.
The performance ranges for Binding (5% ? 44%)
and Regulation (1% ? 40%) events show their ex-
traction to be clearly more challenging. It is in-
teresting that while most systems show better per-
formance for binding over regulation events, the
systems [ConcordU] and [UT+DBCLS] are better
for regulation, showing somewhat reduced perfor-
mance for Binding events. This is in particular con-
trast to the following two systems, [ViBGhent] and
[UTokyo], which show far better performance for
Binding than Regulation events. As one possible
explanation, we find that the latter two differentiate
binding events by their number of themes, while the
former two give no specific treatment to multi-theme
binding events. Such observations and comparisons
are a clear benefit of a community-wide shared task.
Table 6 shows the evaluation results for the teams
who participated in Task 2. The ?All? column shows
the overall performance of the systems for Task 2,
while the ?All Second Args.? column shows the
performance of finding only the secondary argu-
ments. The evaluation results show considerable
differences between the criteria. For example, the
system [Team 24] shows performance comparable
to the top ranked system in finding secondary argu-
ments, although its overall performance for Task 2
is more limited. Table 6 also shows the three sys-
tems, [UT+DBCLS], [Team 24] and [CNBMadrid],
7
Team Negation Speculation
ConcordU 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27
VIBGhent 10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18
ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24
NICTA 05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30
USzeged 05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87
CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95
Table 7: Evaluation results for Task 3.
 0
 10
 20
 30
 40
 50
 60
02/18 02/21 02/24 02/27 03/02 03/05 03/08
daily average
Figure 4: Scatterplot of the evaluation results on the de-
velopment data during the system development period.
show performance at a practical level in particular in
finding specific sites of phosphorylation.
As shown in Table 7, the performance range for
Task 3 is very low although the representation of the
task is as simple as the simple events. We attribute
the reason to the fact that Task 3 is the only task of
which the annotation is not bound to textual clue,
thus no text-bound annotation was provided.
Figure 4 shows a scatter plot of the performance
of the participating systems during the system devel-
opment period. The performance evaluation comes
from the log of the online evaluation system on the
development data. It shows the best performance
and the average performance of the participating
systems were trending upwards up until the dead-
line of final submission, which indicates there is still
much potential for improvement.
7.3 Ensemble
Table 8 shows experimental results of a system en-
semble using the final submissions. For the ex-
periments, the top 3?10 systems were chosen, and
the output of each system treated as a weighted
vote8. Three weighting schemes were used; ?Equal?
weights each vote equally; ?Averaged? weights each
8We used the ?ensemble? function of U-Compare.
Ensemble Equal Averaged Event Type
Top 3 53.19 53.19 54.08
Top 4 54.34 54.34 55.21
Top 5 54.77 55.03 55.10
Top 6 55.13 55.77 55.96
Top 7 54.33 55.45 55.73
Top 10 52.79 54.63 55.18
Table 8: Experimental results of system ensemble.
vote by the overall f-score of the system; ?Event
Type? weights each vote by the f-score of the sys-
tem for the specific event type. The best score,
55.96%, was obtained by the ?Event Type? weight-
ing scheme, showing a 4% unit improvement over
the best individual system. While using the final
scores for weighting uses data that would not be
available in practice, similar weighting could likely
be obtained e.g. using performance on the devel-
opment data. The experiment demonstrates that an
f-score better than 55% can be achieved simply by
combining the strengths of the systems.
8 Conclusion
Meeting with the community-wide participation, the
BioNLP Shared Task was successful in introducing
fine-grained event extraction to the domain. The
evaluation results of the final submissions from the
participants are both promising and encouraging for
the future of this approach to IE. It has been revealed
that state-of-the-art performance in event extraction
is approaching a practically applicable level for sim-
ple events, and also that there are many remain-
ing challenges in the extraction of complex events.
A brief analysis suggests that the submitted data
together with the system descriptions are rich re-
sources for finding directions for improvements. Fi-
nally, the experience of the shared task participants
provides an invaluable basis for cooperation in fac-
ing further challenges.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
8
References
Gary D. Bader, Michael P. Cary, and Chris Sander. 2006.
Pathguide: a Pathway Resource List. Nucleic Acids
Research., 34(suppl 1):D504?506.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-
vian Lee, Emily Dimmer, John Maslen, David Binns,
Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.
2004. The Gene Ontology Annotation (GOA)
Database: sharing knowledge in Uniprot with Gene
Ontology. Nucl. Acids Res., 32(suppl 1):D262?266.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Andrew Chatr-aryamontri, Arnaud Ceol, Luisa Montec-
chi Palazzi, Giuliano Nardelli, Maria Victoria Schnei-
der, Luisa Castagnoli, and Gianni Cesareni. 2007.
MINT: the Molecular INTeraction database. Nucleic
Acids Research, 35(suppl 1):D572?574.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
William Hersh, Aaron Cohen, Ruslenm Lynn, , and
Phoebe Roberts. 2007. TREC 2007 Genomics track
overview. In Proceeding of the Sixteenth Text RE-
trieval Conference.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics. To appear.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA), pages 70?75.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In J. Cussens
and C. Ne?dellec, editors, Proceedings of the 4th Learn-
ing Language in Logic Workshop (LLL05), pages 31?
37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-style
annotation to GENIA corpus. In Proceedings of Nat-
ural Language Processing in Biomedicine (BioNLP)
NAACL 2009 Workshop. To appear.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Laura Rimell and Stephen Clark. 2008. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, To Appear.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Ellen Voorhees. 2007. Overview of TREC 2007. In
The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of Second BioCreative Challenge Evalu-
ation Workshop, pages 7?16.
9
Proceedings of the Workshop on BioNLP: Shared Task, pages 41?49,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Markov Logic Approach to Bio-Molecular Event Extraction
Sebastian Riedel
??
Hong-Woo Chun
??
Toshihisa Takagi
??
Jun'ichi Tsujii
???
?
Database Center for Life Science, Research Organization of Information and System, Japan
?
Department of Computer Science, University of Tokyo, Japan
?
Department of Computational Biology, University of Tokyo, Japan
?
School of Informatics, University of Manchester, UK
?
National Centre for Text Mining, UK
{sebastian,chun,takagi}@dbcls.rois.ac.jp
tsujii@is.s.u-tokyo.ac.jp
Abstract
In this paper we describe our entry to the
BioNLP 2009 Shared Task regarding bio-
molecular event extraction. Our work can
be described by three design decisions: (1)
instead of building a pipeline using local
classifier technology, we design and learn
a joint probabilistic model over events in
a sentence; (2) instead of developing spe-
cific inference and learning algorithms for
our joint model, we apply Markov Logic, a
general purpose Statistical Relation Learn-
ing language, for this task; (3) we represent
events as relational structures over the to-
kens of a sentence, as opposed to structures
that explicitly mention abstract event en-
tities. Our results are competitive: we
achieve the 4th best scores for task 1 (in
close range to the 3rd place) and the best
results for task 2 with a 13 percent point
margin.
1 Introduction
The continuing rapid development of the Inter-
net makes it very easy to quickly access large
amounts of data online. However, it is impossi-
ble for a single human to read and comprehend a
significant fraction of the available information.
Genomics is not an exception, with databases
such as MEDLINE storing a vast amount of
biomedical knowledge.
A possible way to overcome this is informa-
tion extraction (IE) based on natural language
processing (NLP) techniques. One specific IE
sub-task concerns the extraction of molecular
events that are mentioned in biomedical liter-
ature. In order to drive forward research in this
domain, the BioNLP Shared task 2009 (Kim
et al, 2009) concerned the extraction of such
events from text. In the course of the shared task
the organizers provided a training/development
set of abstracts for biomedical papers, annotated
with the mentioned events. Participants were
required to use this data in order to engineer
a event predictor which was then evaluated on
unseen test data.
The shared task covered three sub-tasks. The
first task concerned the extraction of events
along with their clue words and their main argu-
ments. Figure 1 shows a typical example. The
second task was an extension of the first one,
requiring participants to not only predict the
core arguments of each event, but also the cel-
lular locations the event is associated with in
the text. The events in this task were simi-
lar in nature to those in figure 1, but would
also contain arguments that are neither events
nor proteins but cellular location terms. In con-
trast to the protein terms, cellular location terms
were not given as input and had to be predicted,
too. Finally, for task 3 participants were asked
to extract negations and speculations regarding
events. However, in our work we only tackled
Task 1 and Task 2, and hence we omit further
details on Task 3 for brevity.
Our approach to biomedical event extraction
is inspired by recent work on Semantic Role La-
belling (Meza-Ruiz and Riedel, 2009; Riedel and
Meza-Ruiz, 2008) and can be characterized by
three decisions that we will illustrate in the fol-
lowing. First, we do not build a pipelined sys-
tem that first predicts event clues and cellular
locations, and then relations between these; in-
41
stead, we design and learn a joint discrimina-
tive model of the complete event structure for
a given sentence. This allows us to incorporate
global correlations between decisions in a prin-
cipled fashion. For example, we know that any
event that has arguments which itself are events
(such as the positive regulation event in figure
1) has to be a regulation event. This means that
when we make the decision about the type of
an event (e.g., in the first step of a classifica-
tion pipeline) independently from the decisions
about its arguments and their type, we run the
risk of violating this constraint. However, in a
joint model this can be easily avoided.
Our second design choice is the following: in-
stead of designing and implementing specific in-
ference and training methods for our structured
model, we useMarkov Logic, a Statistical Re-
lational Learning language, and define our global
model declaratively. This simplified the imple-
mentation of our system significantly, and al-
lowed us to construct a very competitive event
extractor in three person-months. For example,
the above observation is captured by the simple
formula:
eventType (e, t) ? role (e, a, r) ? event (a) ?
regType (t) (1)
Finally, we represent event structures as rela-
tional structures over tokens of a sentence,
as opposed to structures that explicitly mention
abstract event entities (compare figure 1 and 2).
The reason is as follows. Markov Logic, for now,
is tailored to link prediction problems where we
may make inferences about the existence of rela-
tions between given entities. However, when the
identity and number of objects of our domain is
unknown, things become more complicated. By
mapping to relational structure over grounded
text, we also show a direct connection to recent
formulations of Semantic Role Labelling which
may be helpful in the future.
The remainder of this paper is organized as
follows: we will first present the preprocessing
steps we perform (section 2), then the conversion
to a link prediction problem (section 3). Subse-
quently, we will describe Markov Logic (section
4) and our Markov Logic Network for event ex-
!"# !"$ !"%
&'()*
+,*-*+,*-*
+,*-*
1 2 3 4 5 6 7 8 9
Figure 1: Example gold annotation for task 1 of the
shared task.
1 2 3 4 5 6 7 8 9
Figure 2: Link Prediction version of the events in
figure 1.
traction (section 5). Finally, we present our re-
sults (in section 6) and conclude (section 7).
2 Preprocessing
The original data format provided by the shared
task organizers consists of (a) a collection
biomedical abstracts, and (b) standoff anno-
tation that describes the proteins, events and
sites mentioned in these abstracts. The organiz-
ers also provided a set of dependency and con-
stituent parses for the abstracts. Note that these
parses are based on a different tokenisation of the
text in the abstracts.
In our first preprocessing step we convert the
standoff annotation in the original data to stand-
off annotation for the tokenisation used in the
parses. This allows us to formulate our proba-
bilistic model in terms of one consistent tokeni-
sation (and be able to speak of token instead of
character offsets). Then we we retokenise the
input text (for the parses) according the protein
boundaries that were given in the shared task
data (in order to split strings such as p50/p55).
Finally, we use this tokenisation to once again
adapt the stand-off annotation (using the previ-
ously adapted version as input).
3 Link Prediction Representation
As we have mentioned earlier, before we learn
and apply our Statistical Relational Model, we
convert the task to link prediction over a se-
quence of tokens. In the following we will present
this transformation in detail.
42
To simplify our later presentation we will first
introduce a formal representation of the events,
proteins and locations mentioned in a sentence.
Let us simply identify both proteins and cellular
location entities with their token position in the
sentence. Furthermore, let us describe an event e
as a tuple (i, t, A) where i is the token position of
the clue word of e and t is the event type of e; A
is a set of labelled arguments (a, r) where each a
is either a protein, location or event, and r is the
role a plays with respect to e. We will identify
the set of all proteins, locations and events for a
sentence with P , L and E, respectively.
For example, in figure 1 we have P =
{4, 7} , L = ? and E = {e13, e14, e15} with
e15 = (5, gene_expr, {(4,Theme)})
e14 = (2, pos_reg, {(e15,Theme) , (7,Cause)})
e13 = (1, neg_reg, {(e14,Theme)})
3.1 Events to Links
As we mentioned in section 1, Markov Logic (or
its interpreters) are not yet able to deal with
cases where the number and identity of entities is
unknown, while relations/links between known
objects can be readily modelled. In the follow-
ing we will therefore present a mapping of an
event structure E to a labelled relation over to-
kens. Essentially, we project E to a pair (L,C)
where L is a set of labelled token-to-token links
(i, j, r), and C is a set of labelled event clues
(i, t). Note that this mapping has another ben-
efit: it creates a predicate-argument structure
very similar to most recent formulations of Se-
mantic Role Labelling (Surdeanu et al, 2008).
Hence it may be possible to re-use or adapt the
successful approaches in SRL in order to improve
bio-molecular event extraction. Since our ap-
proach is inspired by the Markov Logic role la-
beller in (Riedel and Meza-Ruiz, 2008), this work
can be seen as an attempt in this direction.
For a sentence with given P , L and E, algo-
rithm 1 presents our mapping from E to (L,C).
For brevity we omit a more detailed description
of the algorithm. Note that for our running ex-
ample eventsToLinks would return
C = {(1, neg_reg) , (2, pos_reg) , (5, gene_expr)}
(2)
Algorithm 1 Event to link conversion
/* returns all clues C and links L given
by the events in E */
1 function eventsToLinks (E):
2 C ? ?, L? ?
3 for each event (i, t, A) ? E do
4 C ? C?{(i, t)}
5 for each argument (a, r) ? A do
6 if a is an event (i?, t?, A?) do
7 L? L?{(i, i?, r)} with a = (i?, t?, A?)
8 else
9 L? L ? {(i, a, r)}
10 return (C,L)
and
L = {(1, 2,Theme) , (2, 5,Theme) ,
(2, 7,Cause) , (5, 4,Theme)} . (3)
3.2 Links to Events
The link-based representation allows us to sim-
plify the design of our Markov Logic Network.
However, after we applied the MLN to our data,
we still need to transform this representation
back to an event structure (in order to use or
evaluate it). This mapping is presented in al-
gorithm 2 and discussed in the following. Note
that we expect the relational structure L to be
cycle free. We again omit a detailed discussion of
this algorithm. However, one thing to notice is
the special treatment we give to binding events.
Roughly speaking, for the binding event clue c
we create an event with all arguments of c in
L. For a non-binding event clue c we first col-
lect all roles for c, and then create one event per
assignment of argument tokens to these roles.
If we would re-convert C and L from equation
2 and 3, respectively, we could return to our orig-
inal event structure in figure 1. However, con-
verting back and forth is not loss-free in general.
For example, if we have a non-binding event in
the original E set with two arguments A and B
with the same role Theme, the round-trip con-
version would generate two events: one with A
as Theme and one with B as Theme.
4 Markov Logic
Markov Logic (Richardson and Domingos, 2006)
is a Statistical Relational Learning language
43
Algorithm 2 link to event conversion. Assume:
no cycles; tokens can only be one of protein, site
or event; binding events have only protein argu-
ments.
/* returns all events E specified
by clues C and links L */
1 function linksToEvents (C,L)
2 return S(i,t)?C resolve (i, C, L)
/* returns all events for
the given token i */
1 function resolve (i, C, L)
2 if no t with (i, t) ? C return {i}
3 t? type (i, C)
4 if t = binding return {(i, t, A)} with
5 A = {(a, r) | (i, a, r) ? L}
6 Ri ? {r?|?a : (i, a, r) ? L}
7 for each role r ? Ri do
8 Ar ? {a| (i, a, r) ? L}
9 Br ?
S
a?Ar {(resolve (a) , r)}
10 return SA?expand(Br1 ,...,Brn ) {(i, t, A)}
/* returns all possible argument
sets for Br1 , . . . , Brn */
1 function expand (Br1 , . . . , Brn )
2 if n = 1 return Brn
3 return
S
a?Br1
S
A?expand(Br2 ,...,Brn ) {(a, r1)} ?A
based on First Order Logic and Markov Net-
works. It can be seen as a formalism that ex-
tends First Order Logic to allow formulae that
can be violated with some penalty. From an al-
ternative point of view, it is an expressive tem-
plate language that uses First Order Logic for-
mulae to instantiate Markov Networks of repet-
itive structure.
Let us introduce Markov Logic by considering
the event extraction task (as relational structure
over tokens as generated by algorithm 1). In
Markov Logic we can model this task by first
introducing a set of logical predicates such as
eventType(Token,Type), role(Token,Token,Role)
and word(Token,Word). Then we specify a set of
weighted first order formulae that define a distri-
bution over sets of ground atoms of these pred-
icates (or so-called possible worlds). Note that
we will refer predicates such as word as observed
because they are known in advance. In contrast,
role is hidden because we need to infer its ground
atoms at test time.
Ideally, the distribution we define with these
weighted formulae assigns high probability to
possible worlds where events are correctly iden-
tified and a low probability to worlds where this
is not the case. For example, in our running ex-
ample a suitable set of weighted formulae would
assign a higher probability to the world
{word (1, prevented) , eventType (1, neg_reg) ,
role(1, 2,Theme), event(2), . . .}
than to the world
{word (1, prevented) , eventType (1, binding) ,
role(1, 2,Theme), event(2), . . .}
In Markov Logic a set of weighted first order for-
mulae is called a Markov Logic Network (MLN).
Formally speaking, an MLN M is a set of pairs
(?,w) where ? is a first order formula and w a
real weigh t. M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
?
(4)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with
the constants of our domain. f?c is a feature
function that returns 1 if in the possible world y
the ground formula we get by replacing the free
variables in ? by the constants in the binding
c is true and 0 otherwise. Z is a normalisation
constant.
4.1 Inference and Learning
Assuming that we have an MLN, a set of weights
and a given sentence, we need to predict the
choice of event clues and roles with maximal
a posteriori probability (MAP). To this end
we apply a method that is both exact and ef-
ficient: Cutting Plane Inference Riedel (2008,
CPI) with Integer Linear Programming (ILP) as
base solver.
In order to learn the weights of the MLN
we use the 1-best MIRA Crammer and Singer
(2003) Online Learning method. As MAP infer-
ence method that is applied in the inner loop of
the online learner we apply CPI, again with ILP
as base solver. The loss function for MIRA is a
44
weighted sum FP +?FN where FP is the num-
ber of false positives, FN the number of false
negatives and ? = 0.01.
5 Markov Logic Network for Event
Extraction
We define four hidden predicates our task:
event(i) indicates that there is an event with
clue word i; eventType(i,t) denotes that at token
i there is an event with type t; site(i) denotes a
cellular location mentioned at token i; role(i,j,r)
indicates that token i has the argument j with
role r. In other words, the four hidden predicates
represent the set of sites L (via site), the set of
event clues C (via event and eventType) and the
set of links L (via role) presented in section 3.
There are numerous observed predicates we
use. Firstly, the provided information about
protein mentions is captured by the predicate
protein(i), indicating there is a protein mention
ending at token i. We also describe event types
and roles in more detail: regType( t) holds for
an event type t iff it is a regulation event type;
task1Role(r) and task2Role(r) hold for a role r
if is a role of task 1 (Theme, Cause) or task 2
(Site, CSite, etc.).
Furthermore, we use predicates that de-
scribe properties of tokens (such as the word
or stem of a token) and token pairs (such
as the dependency between two tokens); this
set is presented in table 1. Here the path
and pathNL predicates may need some fur-
ther explanation. When path(i,j,p,parser) is
true, there must be a labelled dependency
path p between i and j according to the
parser parser. For example, in figure 1 we
will observe path(1,5,dobj?prep_of?,mcclosky-
charniak). pathNL just omits the depen-
dency labels, leading to path(1,5,??,mcclosky-
charniak) for the same example.
We use two parses per sentence: the outputs
of a self-trained reranking parser Charniak and
Johnson (2005); McClosky and Charniak (2008)
and a CCG parser (Clark and Curran, 2007),
provided as part of the shared task dataset. As
dictionaries we use a collection of cellular lo-
cation terms taken from the Genia event cor-
pus (Kim et al, 2008), a small handpicked set of
event triggers and a list of English stop words.
Predicate Description
word(i,w) Token i has word w.
stem(i,s) i has (Porter) stem s.
pos(i,p) i has POS tag p.
hyphen(i,w) i has word w after last hyphen.
hyphenStem(i,s) i has stem s after last hyphen.
dict(i,d) i appears in dictionary d.
genia(i,p) i is event clue in the Genia
corpus with precision p.
dep(i,j,d,parser) i is head of token j with
dependency d according to
parser parser.
path(i,j,p,parser) Labelled Dependency path
according to parser parser
between tokens i and j is p.
pathNL(i,j,p,parser) Unlabelled dependency path
according to parser p between
tokens i and j is path.
Table 1: Observable predicates for token and token
pair properties.
5.1 Local Formulae
A formula is local if its groundings relate any
number of observed ground atoms to exactly one
hidden ground atom. For example, the ground-
ing
dep (1, 2, dobj, ccg) ? word (1, prevented) ?
eventType (2, pos_reg) (5)
of the local formula
dep(h, i, d, parser) ? word (h,+w) ?
eventType(i,+t) (6)
connects a single hidden eventType ground atom
with an observed word and dep atom. Note that
the + prefix for variables indicates that there is
a different weight for each possible pair of word
and event type (w, t).
5.1.1 Local Entity Formulae
The local formulae for the hidden event/1
predicate can be summarized as follows. First,
we add a event (i) formula that postulates the
existence of an event for each token. The weight
of this formulae serves as a general bias for or
against the existence of events.
45
Next, we add one formula
T (i,+t) ? event (i) (7)
for each simple token property predicate T in
table 1 (those in the first section of the table).
For example, when we plug in word for T we get
a formula that encourages or discourages the ex-
istence of an event token based on the word form
of the current token: word (i,+t) ? event (i).
We also add the formula
genia (i, p) ? event (i) (8)
and multiply the feature-weight product for each
of its groundings with the precision p. This is
corresponds to so-called real-valued feature func-
tions, and allows us to incorporate probabili-
ties and other numeric quantities in a principled
fashion.
Finally, we add a version of formula 6 where
we replace eventType(i,t) with event(i).
For the cellular location site predicate we
use exactly the same set of formulae but re-
place every occurrence of event(i) with site(i).
This demonstrates the ease with which we could
tackle task 2: apart from a small set of global
formulae we introduce later, we did not have to
do more than copy one file (the event model file)
and perform a search-and-replace. Likewise, in
the case of the eventType predicate we simply
replace event(i) with eventType(i,+t).
5.1.2 Local Link Formulae
The local formulae for the role/3 predicate
are different in nature because they assess two
tokens and their relation. However, the first for-
mula does look familiar: role (i, j,+r). This for-
mula captures a (role-dependent) bias for the ex-
istence of a role between any two tokens.
The next formula we add is
dict (i,+di) ? dict (j,+dj) ? role (i, j,+r) (9)
and assesses each combination of dictionaries
that the event and argument token are part of.
Furthermore, we add the formula
path (i, j,+p,+parser) ? role (i, j,+r) (10)
that relates the dependency path between two
tokens i and j with the role that j plays with
respect to i. We also add an unlabelled version
of this formula (using pathNL instead of path).
Finally, we add a formula
P (i, j,+p,+parser) ? T (i,+t) ?
role (i, j,+r) (11)
for each P in {path,pathNL} and T in
{word,stem,pos,dict,protein}. Note that for
T=protein we replace T (i,+t) with T (i).
5.2 Global Formulae
Global formulae relate two or more hidden
ground atoms. For example, the formula in
equation 1 is global. While local formulae can be
used in any conventional classifier (in the form
of feature functions conditioned only on the in-
put data) this does not hold for global ones.
We could enforce global constraints such as the
formula in equation 1 by building up structure
incrementally (e.g. start with one classifier for
events and sites, and then predict roles between
events and arguments with another). However,
this does not solve the typical chicken-and-egg
problem: evidence for possible arguments could
help us to predict the existence of event clues,
and evidence for events help us to predict argu-
ments. By contrast, global formulae can capture
this type of correlation very naturally.
Table 2 shows the global formulae we use. We
divide them into three parts. The first set of for-
mulae (CORE) ensures that event and eventType
atoms are consistent. In all our experiments we
will always include all CORE formulae; without
them we might return meaningless solutions that
have events with no event types, or types with-
out events.
The second set of formulae (VALID) consist
of CORE and formulae that ensure that the link
structure represents a valid set of events. For
example, this includes formula 12 that enforces
each event to have at least one theme.
Finally, FULL includes VALID and two con-
straints that are not strictly necessary to enforce
valid event structures. However, they do help us
to improve performance. Formula 14 forbids a
token to be argument of more than one event. In
fact, this formula does not hold all the time, but
46
# Formula Description
1 event (i)? ?t.eventType (i, t) If there is an event there should be an event type.
2 eventType (i, t)? event (i) If there is an event type there should be an event.
3 eventType (i, t) ? t 6= o? ?eventType (i, o) There cannot be more than one event type per token.
4 ?site (i) ? ?event (i) A token cannot be both be event and site.
5 role (i, j, r)? event (i) If j plays the role r for i then i has to be an event.
6 role (i, j, r1) ? r1 6= r2 ? ?role (i, j, r2) There cannot be more than one role per argument.
7 eventType (e, t) ? role (e, a, r) ? event (a)? regType (t) Only reg. type events can have event arguments.
9 role (i, j, r) ? taskOne (r)? event (j) ? protein (j) For task 1 roles arguments must be proteins or events
10 role (i, j, r) ? taskTwo (r)? site (j) Task 2 arguments must be cellular locations (site).
11 site (j)? ?i, r.role (i, j, r) ? taskTwo (r) Sites are always associated with an event.
12 event (i)? ?j.role (i, j,Theme) Every events need a theme.
13 eventType (i, t) ? ?allowed (t, r)? ?role (i, j, r) Certain events may not have certain roles.
14 role (i, j, r1) ? k 6= i? ?role (k, j, r2) A token cannot be argument of more than one event.
15 j < k ? i < j ? role (i, j, r1)? ?role (i, k, r2) No inside outside chains.
Table 2: All three sets of global formulae used: CORE (1-3), VALID (1-13), FULL (1-15).
by adding it we could improve performance. For-
mula 15 is our answer to a type of event chain
that earlier models would tend to produce.
Note that all formulae but formula 15 are de-
terministic. This amounts to giving them a very
high/infinite weight in advance (and not learn-
ing it during training).
6 Results
In table 3 we can see our results for task 1 and
2 of the shared task. The measures we present
here correspond to the approximate span, ap-
proximate recursive match criterion that counts
an event as correctly predicted if all arguments
are extracted and the event clue tokens approx-
imately match the gold clue tokens. For more
details on this metric we refer the reader to the
shared task overview paper.
To put our results into context: for task 1 we
reached the 4th place among 20 participants, are
in close range to place 2 and 3, and significantly
outperform the 5th best entry. Moreover, we
had highest scoring scores for task 2 with a 13%
margin to the runner-up. Using both training
and development set for training (as allowed by
the task organisers), our task 1 score rises to
45.1, slightly higher than the score of the current
third.
In terms of accuracy across different event
types our model performs worse for binding, reg-
ulation type and transcription events. Binding
events are inherently harder to correctly extract
because they often have multiple core arguments
while other non-regulation events have only one;
just missing one of the binding arguments will
lead to an event that is considered as error with
no partial credit given. If we would give credit
for binding with partially correct arguments our
F-score for binding events would rise to 49.8.
One reason why regulation events are difficult
to extract is the fact that they often have argu-
ments which themselves are events, too. In this
case our recall is bound by the recall for argu-
ment events because we can never find a regu-
lation event if we cannot predict the argument
event. Note that we are still unsure about tran-
scription events, in particular because we ob-
serve 49% F-score for such events in the devel-
opment set.
How does our model benefit from the global
formulae we describe in section 5 (and which
represent one of the core benefits of a Markov
Logic approach)? To evaluate this we compare
our FULL model with CORE and VALID from
table 2. Note that because the evaluation inter-
face rejects invalid event structures, we cannot
use the evaluation metrics of the shared task.
Instead we use table 4 to present an evaluation
in terms of ground atom F1-score for the hidden
predicates of our model. This amounts to a per-
47
Task 1 Task 2
R P F R P F
Loc 37.9 88.0 53.0 32.8 76.0 45.8
Bind 23.1 48.2 31.2 22.4 47.0 30.3
Expr 63.0 75.1 68.5 63.0 75.1 68.5
Trans 16.8 29.9 21.5 16.8 29.9 21.5
Cata 64.3 81.8 72.0 64.3 81.8 72.0
Phos 78.5 77.4 77.9 69.1 70.1 69.6
Total 48.3 68.9 56.8 46.8 67.0 55.1
Reg 23.7 40.8 30.0 22.3 38.5 28.2
Pos 26.8 42.8 32.9 26.7 42.3 32.7
Neg 27.2 40.2 32.4 26.1 38.6 31.2
Total 26.3 41.8 32.3 25.8 40.8 31.6
Total 36.9 55.6 44.4 35.9 54.1 43.1
Table 3: (R)ecall, (P)recision, and (F)-Score for task
1 and 2 in terms of event types.
role, per-site and per-event-clue evaluation. The
numbers here will not directly correspond to ac-
tual scores, but generally we can assume that if
we do better in our metrics, we will likely have
better scores.
In table 4 we notice that ensuring consistency
between all predicates has a significant impact
on the performance across the board (see the
VALID results). Furthermore, when adding ex-
tra formulae that are not strictly necessary for
consistency, but which encourage more likely
event structure, we again see significant improve-
ments (see FULL results). Interestingly, al-
though the extra formulae only directly consider
role atoms, they also have a significant impact
on event and particularly site extraction perfor-
mance. This reflects how in a joint model deci-
sions which would appear in the end of a tradi-
tional pipeline (e.g., extracting roles for events)
can help steps that would appear in the begin-
ning (extracting events and sites).
For the about 7500 sentences in the training
set we need about 3 hours on a MacBook Pro
with 2.8Ghz and 4Gb RAM to learn the weights
of our MLN. This allowed us to try different sets
of formulae in relatively short time.
7 Conclusion
Our approach the BioNLP Shared Task 2009 can
be characterized by three decisions: (a) jointly
CORE VALID FULL
eventType 52.8 63.2 64.3
role 44.0 53.5 55.7
site 42.0 46.0 51.5
Total 50.7 60.1 61.9
Table 4: Ground atom F-scores for global formulae.
modelling the complete event structure for a
given sentence; (b) using Markov Logic as gen-
eral purpose-framework in order to implement
our joint model; (c) framing the problem as a
link prediction problem between tokens of a sen-
tence.
Our results are competitive: we reach the 4th
place in task 1 and the 1st place for task 2 (with
a 13% margin). Furthermore, the declarative na-
ture of Markov Logic helped us to achieve these
results with a moderate amount of engineering.
In particular, we were able to tackle task 2 by
copying the local formulae for event prediction,
and adding three global formulae (4, 10 and 11
in table 2). Finally, our system was fast to train
(3 hours) . This greatly simplified the search for
good sets of formulae.
We have also shown that global formulae sig-
nificantly improve performance in terms of event
clue, site and argument prediction. While a sim-
ilar effect may be possible with reranking archi-
tectures, we believe that in terms of implemen-
tation efforts our approach is at least as simple.
In fact, our main effort lied in the conversion to
link prediction, not in learning or inference. In
future work we will therefore investigate means
to extend Markov Logic (interpreter) in order to
directly model event structure.
Acknowledgements
We thank Dr. Chisato Yamasaki and Dr.
Tadashi Imanishi, BIRC, AIST, for their help.
This work is supported by the Integrated
Database Project (MEXT, Japan), the Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan) and the Genome Network Project
(MEXT, Japan).
48
References
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics (ACL' 05). pages
173180.
Clark, Stephen and James R. Curran. 2007.
Wide-coverage efficient statistical parsing
with ccg and log-linear models. Comput. Lin-
guist. 33(4):493552.
Crammer, Koby and Yoram Singer. 2003. Ultra-
conservative online algorithms for multiclass
problems. Journal of Machine Learning Re-
search 3:951991.
Kim, Jin D., Tomoko Ohta, and Jun'ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformat-
ics 9(1).
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo,
Yoshinobu Kano, and Jun'ichi Tsujii. 2009.
Overview of bionlp'09 shared task on event ex-
traction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL
2009 Workshop. To appear.
McClosky, David and Eugene Charniak. 2008.
Self-training for biomedical parsing. In
Proceedings of the 46rd Annual Meeting of
the Association for Computational Linguistics
(ACL' 08).
Meza-Ruiz, Ivan and Sebastian Riedel. 2009.
Jointly identifying predicates, arguments and
senses using markov logic. In Joint Human
Language Technology Conference/Annual
Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL '09).
Richardson, Matt and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107136.
Riedel, Sebastian. 2008. Improving the accuracy
and efficiency of map inference for markov
logic. In Proceedings of the 24th Annual Con-
ference on Uncertainty in AI (UAI '08).
Riedel, Sebastian and Ivan Meza-Ruiz. 2008.
Collective semantic role labelling with markov
logic. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL' 08). pages 193197.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llu?s M?rquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependen-
cies. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL-2008).
49
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 22?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrated NLP Evaluation System for Pluggable Evaluation Metrics 
with Extensive Interoperable Toolkit 
 
 
Yoshinobu Kano1   Luke McCrohon1   Sophia Ananiadou2   Jun?ichi Tsujii1,2 
 
1 Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester and National Centre for 
Text Mining, 131 Princess St, M1 7DN, UK 
  
[kano,tsujii]@is.s.u-tokyo.ac.jp 
luke.mccrohon@gmail.com 
sophia.ananiadou@manchester.ac.uk 
 
  
 
Abstract 
To understand the key characteristics of NLP 
tools, evaluation and comparison against dif-
ferent tools is important. And as NLP applica-
tions tend to consist of multiple semi-
independent sub-components, it is not always 
enough to just evaluate complete systems, a 
fine grained evaluation of underlying compo-
nents is also often worthwhile. Standardization 
of NLP components and resources is not only 
significant for reusability, but also in that it al-
lows the comparison of individual components 
in terms of reliability and robustness in a wid-
er range of target domains.  But as many eval-
uation metrics exist in even a single domain, 
any system seeking to aid inter-domain eval-
uation needs not just predefined metrics, but 
must also support pluggable user-defined me-
trics. Such a system would of course need to 
be based on an open standard to allow a large 
number of components to be compared, and 
would ideally include visualization of the dif-
ferences between components. We have de-
veloped a pluggable evaluation system based 
on the UIMA framework, which provides vi-
sualization useful in error analysis. It is a sin-
gle integrated system which includes a large 
ready-to-use, fully interoperable library of 
NLP tools. 
1 Introduction 
When building NLP applications, the same sub-
tasks tend to appear frequently while construct-
ing different systems.  Due to this, the reusability 
of tools designed for such subtasks is a common 
design consideration; fine grained interoperabili-
ty between sub components, not just between 
complete systems. 
In addition to the benefits of reusability, inte-
roperability is also important in evaluation of 
components. Evaluations are normally done by 
comparing two sets of data, a gold standard data 
and test data showing the components perfor-
mance. Naturally this comparison requires the 
two data sets to be in the same data format with 
the same semantics. Comparing of "Apples to 
Apples" provides another reason why standardi-
zation of NLP tools is beneficial. Another advan-
tage of standardization is that the number of gold 
standard data sets that can be compared against is 
also increased, allowing tools to be tested in a 
wider range of domains. 
The ideal is that all components are standar-
dized to conform to an open, widely used intero-
perability framework. One possible such frame-
work is UIMA; Unstructured Information Man-
agement Architecture (Ferrucci et al, 2004), 
which is an open project of OASIS and Apache. 
We have been developing U-Compare (Kano et 
al., 2009)1, an integrated testing an evaluation 
platform based on this framework. 
                                                 
1 Features described in this paper are integrated as U-
Compare system, publicly available from:  
http://u-compare.org/ 
22
Although U-Compare already provided a wide 
range of tools and NLP resources, its inbuilt 
evaluation mechanisms were hard coded into the 
system and were not customizable by end users. 
Furthermore the evaluation metrics used were 
based only on simple strict matchings which se-
verely limited its domains of application. We 
have extended the evaluation mechanism to al-
low users to define their own metrics which can 
be integrated into the range of existing evalua-
tion tools.  
The U-Compare library of interoperable tools 
has also been extended; especially with regard to 
resources related to biomedical named entity ex-
traction. U-Compare is currently providing the 
world largest library of type system compatible 
UIMA components. 
In section 2 of this paper we first look at the 
underlying technologies, UIMA and 
U-Compare. Then we describe the new plugga-
ble evaluation mechanism in section 3 and our 
interoperable toolkit with our type system in sec-
tion 4 and 5. 
 
2 Background 
2.1 UIMA 
UIMA is an open framework specified by OA-
SIS2. Apache UIMA provides a reference im-
plementation as an open source project, with 
both a pure java API and a C++ development kit . 
UIMA itself is intended to be purely a frame-
work, i.e. it does not intend to provide specific 
tools or type system definitions. Users should 
develop such resources themselves. In the fol-
lowing subsections, we briefly describe the basic 
concepts of UIMA, and define keywords used to 
explain our system in later sections. 
2.1.1 CAS and Type System 
The UIMA framework uses the ?stand-off anno-
tation? style (Ferrucci et al, 2006). The underl-
ing raw text of a document is generally kept un-
changed during analysis, and the results of 
processing the text are added as new stand-off 
annotations with references to their positions in 
the raw text. A Common Analysis Structure 
(CAS) holds a set of such annotations. Each of 
which is of a given type as defined in a specified 
                                                 
                                                
2 http://www.oasis-open.org/committees/uima/ 
hierarchical type system. Annotation3 types may 
define features, which are themselves typed. 
Apache UIMA provides definitions of a range of 
built in primitive types, but a more complete type 
system should be specified by developers. The 
top level Apache UIMA type is referred to as 
TOP, other primitive types include. int, String, 
Annotation and FSArray (an array of any annota-
tions). 
2.1.2 Component and Capability 
UIMA components receive and update CAS one 
at a time. Each UIMA component has a capabili-
ty property, which describes what types of anno-
tations it takes as input and what types of anno-
tations it may produce as output.  
UIMA components can be deployed either lo-
cally, or remotely as SOAP web services. Re-
motely deployed web service components and 
locally deployed components can be freely com-
bined in UIMA workflows. 
2.1.3 Aggregate Component and Flow Con-
troller 
UIMA components can be either primitive or 
aggregate. Aggregate components include other 
components as subcomponents. Subcomponents 
may themselves be aggregate. In the case where 
an aggregate has multiple subcomponents these 
are by default processed in linear order. This or-
dering can be customized by implementing a 
custom flow controller. 
2.2 U-Compare 
U-Compare is a joint project of the University of 
Tokyo, the Center for Computational Pharma-
cology at the University of Colorado School of 
Medicine, and the UK National Centre for Text 
Mining. 
U-Compare provides an integrated platform 
for users to construct, edit and compare 
workflows compatible with any UIMA compo-
nent. It also provides a large, ready-to-use toolkit 
of interoperable NLP components for use with 
any UIMA based system. This toolkit is currently 
the world largest repository of type system com-
patible components. These all implement the U-
Compare type system described in section 3. 
 
3  In the UIMA framework, Annotation is a base 
type which has begin and end offset values. In this paper 
we call any objects (any subtype of TOP) as annotations. 
23
2.2.1 Related Works 
There also exist several other public UIMA 
component repositories: CMU UIMA component 
repository, BioNLP UIMA repository (Baum-
gartner et al, 2008), JCoRe (Hahn et al, 2008), 
Tsujii Lab Component Repository at the Univer-
sity of Tokyo (Kano et al, 2008a), etc. Each 
group uses their own type system, and so com-
ponents provided by each group are incompatible. 
Unlike U-Compare these repositories are basical-
ly only collections of UIMA components, U-
Compare goes further by providing a fully inte-
grated set of UIMA tools and utilities. 
2.2.2 Integrated Platform 
U-Compare provides a variety of features as part 
of an integrated platform. The system can be 
launched with a single click in a web browser; all 
required libraries are downloaded and updated 
automatically in background.  
The Workflow Manager GUI helps users to 
create workflows in an easy drag-and-drop fa-
shion. Similarly, import/export of workflows, 
running of workflows and saving results can all 
be handled via a graphical interface.  
U-Compare special parallel aggregate compo-
nents allow combinations of specified compo-
nents to be automatically combined and com-
pared based on their I/O capabilities (Kano et al, 
2008b). When workflows are run, U-Compare 
shows statistics and visualizations of results ap-
propriate to the type of workflow. For example 
when workflows including parallel aggregate 
components are run comparison statistics be-
tween all possible parallel component combina-
tions are given. 
3 Integrated System for Pluggable 
Evaluation Metrics  
While U-Compare already has a mechanism to 
automatically create possible combinations of 
components for comparison from a specified 
workflow, the comparison (evaluation) metric 
itself was hard coded into the system. Only com-
parison based on simple strict matching was 
possible. 
However, many different evaluation metrics 
exist, even for the same type of annotations. For 
example, named entity recognition results are 
often evaluated based on several different anno-
tation intersection criteria: exact match, left/right 
only match, overlap, etc. Evaluation metrics for 
nested components can be even more complex 
(e.g. biomedical relations, deep syntactic struc-
tures). Sometimes new metrics are also required 
for specific tasks. Thus, a mechanism for plugg-
able evaluation metrics in a standardized way is 
seen as desirable. 
3.1 Pluggable Evaluation Component 
Our design goal for the evaluation systems is to 
do as much of the required work as possible and 
to provide utilities to reduce developer?s labor. 
We also want our design to be generic and fix 
within existing UIMA standards. 
The essential process of evaluation can be ge-
neralized and decomposed as follows: 
 
(a) prepare a pair of annotation sets which 
will be used for comparison, 
(b) select annotations which should be in-
cluded in the final evaluation step, 
(c) compare selected annotations against 
each other and mark matched pairs. 
For example, in the case of the Penn Treebank 
style syntactic bracket matching, these steps cor-
respond to (a) prepare two sets of constituents 
and tokens, (b) select only the constituents (re-
moving null elements if required), (c) compare 
constituents between the sets and return any 
matches. 
In our new design, step (a) is performed by the 
system, (b) and (c) are performed by an evalua-
tion component. The evaluation component is 
just a normal UIMA component, pluggable based 
on the UIMA standard. This component is run on 
a CAS which was constructed by the system dur-
ing step (a). This CAS includes an instance of 
ComparisonSet type and its features GoldAnno-
tationGroup and TestAnnotationGroup. Corres-
ponding to step (b), based on this input the com-
parison component should make a selection of 
annotations and store them as FSArray for both 
GoldAnnotations and TestAnnotations. Finally 
for step (c), the component should perform a 
matching and store the results as MatchedPair 
instances in the MatchedAnnotations feature of 
the ComparisonSet. 
Precision, recall, and F1 scores are calculated 
by U-Compare based on the outputted Compari-
sonSet. These calculation can be overridden and 
customized if the developer so desires. 
Implementation of the compare() method of 
the evaluation component is recommended. It is 
used by the system when showing instance based 
evaluations of what feature values are used in 
24
matching, which features are matched, and which 
are not. 
3.2 Combinatorial Evaluation and Er-
ror Analysis 
By default, evaluation statistics are calculated by 
simply counting the numbers of gold, test, 
matched annotations in the returned Compari-
sonSet instance. Then precision, recall, and F1 
scores for each CAS and for the complete set of 
CASes are calculated. Users can specify which 
evaluation metrics are used for each type of an-
notations based on the input specifications they 
set for supplied evaluation components. 
Normally, precision, recall, and F1 scores are 
the only evaluation statistics used in the NLP 
community. It is often the case in many research 
reports that a new tool A performs better than 
another tool B, increasing the F1 score by 1%. In 
such cases it is important to analysis what pro-
portion of annotations are shared between A, B, 
and the gold standard. Is A a strict 1% increase 
over B? Or does it cover 2% of instances B 
doesn?t but miss a different 1%? Our system 
provides these statistics as well. 
Further, our standardized evaluation system 
makes more advanced evaluation available. 
Since the evaluation metrics themselves are more 
or less arbitrary, we should carefully observe the 
results of evaluations. When two or more metrics 
are available for the same type of annotations, 
we can compare the results of each to analyze 
and validate the individual evaluations. 
An immediate application of such comparison 
would be in a voting system, which takes the 
results of several tools as input and selects com-
mon overlapping annotations as output. 
U-Compare also provides visualizations of 
evaluation results allowing instance-based error 
analysis. 
4 U-Compare Type System 
U-Compare currently provides the world largest 
set of type system compatible UIMA compo-
nents. We will describe some of these in section 
5. In creating compatible components in UIMA a 
key task is their type system definitions. 
The U-Compare type system is designed in a 
hierarchical fashion with distinct types to achieve 
a high level of interoperability. It is intended to 
be a shared type system capable of mapping 
types originally defined as part of independent 
type systems (Kano et al, 2008c). In this section 
we describe the U-Compare type system in detail. 
4.1 Basic Types 
While most of the U-Compare types are inherit-
ing a UIMA built-in type, Annotation (Figure 1), 
there are also types directly extending the TOP 
type; let us call these types as metadata types.  
AnnotationMetadata holds a confidence value, 
which is common to all of the U-Compare anno-
tation types as a feature of BaseAnnotation type. 
BaseAnnotation extends DiscontinuousAnnota-
tion, in which fragmental annotations can be 
stored as a FSArray of Annotations, if any.  
ExternalReference is another common meta-
data type where namespace and ID are stored, 
referring to an external ontology entity outside 
UIMA/U-Compare. Because it is not realistic to 
represent everything like such a detailed ontolo-
gy hierarchy in a UIMA type system, this meta-
data is used to recover original information, 
which are not expressed as UIMA types. Refe-
renceAnnotation is another base annotation type, 
which holds an instance of this ExternalRefe-
rence.  
UniqueLabel is a special top level type for ex-
plicitly defined finite label sets, e.g. the Penn 
Treebank tagset. Each label in such a tagset is 
mapped to a single type where UniqueLabel as its 
BaseAnnotation 
<AnnotationMetadata> 
SyntacticAnnotation 
Token
POSToken
<POS> 
RichToken
<String>base
Sentence Dependency 
<DependencyLabel> 
Stanford 
Dependency 
TreeNode 
<TOP>parent 
<FSArray>children
AbstractConstituent
NullElement 
<NullElementLabel>
<Constituent> 
Constituent 
<ConstituentLabel>
FunctionTaggedConstituent 
<FunctionLabel> 
TemplateMappedConstituent 
<Constituent> 
TOP
Coordinations
<FSArray>
Figure 2. Syntactic Types in U-Compare. 
25
ancestor, putting middle level types if possible 
(e.g. Noun type for the Penn Treebank POS tag-
set). These types are omitted in the figure.  
4.2 Syntactic Types 
SyntacticAnnotation is the base type of all syn-
tactic types (Figure 2). POSToken holds a POS 
label, RichToken additionally holds a base form. 
Dependency is used by dependency parsers, 
while TreeNode is for syntactic tree nodes. Con-
stituent, NullElement, FunctionTaggedConsti-
tiuent, TemplateMappedConstituent are designed 
to fully represent all of the Penn Treebank style 
annotations. Coordination is a set of references to 
coordinating nodes (currently used by the Genia 
Treebank). We are planning on extending the set 
of syntactic types to cover the outputs of several 
deep parsers. 
4.3 Semantic Types  
SemanticAnnotation is the base type for semantic 
annotations; it extends ReferenceAnnotation by 
holding the original reference.  
SemanticClassAnnotation is a rather complex 
type designed to be somewhat general. In many 
cases, semantic annotations may reference other 
semantic annotations, e.g. references between 
biological events. Such references are often la-
beled with their roles which we express with the 
ExternalReference type. Such labeled references 
are expressed by LinkingAnnotationSet. As a role 
may refer to more than one annotation, Linkin-
gAnnotationSet has an FSArray of SemanticAn-
notation as a feature. 
There are several biomedical types included in 
Figure 3, e.g. DNA, RNA, Protein, Gene, Cel-
lLine, CellType, etc. It is however difficult to 
decide which ontological entities should be in-
cluded in such a type system. One reason for this 
is that such concepts are not always distinct; dif-
ferent ontologies may give overlapping defini-
tions of these concepts. Further, the number of 
possible substance level entities is infinite; caus-
ing difficult in their expression as individual 
types. The current set of biomedical types in the 
U-Compare type system includes types which are 
frequently used for evaluation in the BioNLP 
research. 
4.4 Document Types  
DocumentAnnotation is the base type for docu-
ment related annotations (Figure 4). It extends 
DocumentClassAnnotation 
<FSArray:DocumentAttribute> 
<FSArray:ReferenceAnnotation> 
DocumentAttribute 
<ExternalReference> 
DocumentAnnotation 
DocumentReferenceAttribute
<ReferenceAnnotation>
DocumentValueAttribute
<String>value 
ReferenceAnnotation TOP
Figure 4. Document types in the U-Compare type system. 
SemanticAnnotation
ReferenceAnnotation
SemanticClassAnnotation 
<FSArray:LinkedAnnotationSet>
NamedEntity EventAnnotation
CellType CellLine GeneOrGeneProductRNADNAProper 
Name
Title 
Place Protein GenePerson 
ProteinRegion
DNARegion
LinkingAnnotationSet 
<ExternalReference> 
<FSArray:SemanticAnnotation>
CoreferenceAnnotation DiscourseEntity Expression
Negation 
TOP 
Speculation
Figure 3. Semantic types in the U-Compare type system. 
26
ReferenceAnnotation to reference the full exter-
nal type in the same way as SemanticAnnotation.  
187 
The document length in bytes is 
output in the first line (end with 
new line),  
DocumentClassAnnotation together with Do-
cumentAttribute are intended to express XML 
style data. XML tags may have fields storing 
their values, and/or idref fields refering to other 
tags. DocumentValueAttiributerepresents simple 
value field, while DocumentReferenceAttribute 
represents idref type fields. A DocumentClas-
sAnnotation corresponds to the tag itself. 
then the raw text follows as is 
(attaching a new line in the end), 
finally annotations follow line by 
line. 
0 187 Document id="u1" 
0 3 POSToken id="u2" pos="DT" 
.... 
Although these types can represent most doc-
ument structures, we still plan to add several 
specific types such as Paragraph, Title, etc. 
Figure 5. An example of the U-Compare simple I/O 
format. 
 
5 Interoperable Components and Utili-
ties 
In this section, we describe our extensive toolkit 
of interoperable components and the set of utili-
ties integrated into the U-Compare system. All of 
the components in our toolkit are compatible 
with the U-Compare type system described in the 
previous section. 
5.1 Corpus Reader Components  
In the UIMA framework, a component which 
generates CASes is called a Collection Reader. 
We have developed several collection readers 
which read annotated corpora and generates an-
notations using the U-Compare type system.  
Because our primary target domain was bio-
medical field, there are corpus readers for the 
biomedical corpora; Aimed corpus (Bunescu et 
al., 2006) reader and BioNLP ?09 shared task 
format reader generate event annotations like 
protein-protein interaction annotations; Readers 
for BIO/IOB format, Bio1 corpus (Tateisi et al, 
2000), BioCreative (Hirschman et al, 2004) task 
1a format, BioIE corpus (Bies et al, 2005), 
NLPBA shared task dataset (Kim et al, 2004), 
Texas Corpus (Bunescu et al, 2005), Yapex 
Corpus (Kristofer Franzen et al, 2002), generate 
biomedical named entities, and Genia Treebank 
corpus (Tateisi et al, 2005) reader generates 
Penn Treebank (Marcus et al, 1993) style brack-
eting and part-of-speech annotations. Format 
readers require users to prepare annotated data, 
while others include corpora themselves, auto-
matically downloaded as an archive on users? 
demand. 
In addition, there is File System Collection 
Reader from Apache UIMA which reads files as 
plain text. We have developed an online interac-
tive text reader, named Input Text Reader. 
5.2 Analysis Engine Components  
There are many tools covering from basic syn-
tactic annotations to the biomedical annotations. 
Some of the tools are running as web services, 
but users can freely mix local services and web 
services. 
For syntactic annotations: sentence detectors 
from GENIA, LingPipe, NaCTeM, OpenNLP 
and Apache UIMA; tokenizers from GENIA tag-
ger (Tsuruoka et al, 2005), OpenNLP, Apache 
UIMA and Penn Bio Tokenizer; POS taggers 
from GENIA tagger, LingPipe, OpenNLP and 
Stepp Tagger; parsers from OpenNLP (CFG), 
Stanford Parser (dependency) (de Marneffe et al, 
2006), Enju (HPSG) (Miyao et al, 2008). 
For semantic annotations: ABNER (Settles, 
2005) for NLPBA/BioCreative trained models, 
GENIA Tagger, NeMine, MedT-NER, LingPipe 
and OpenNLP NER, for named entity recogni-
tions. Akane++ (S?tre et al, 2007) for protein-
protein interaction detections. 
5.3 Components for Developers  
Although Apache UIMA provides APIs in both 
Java and C++ to help users develop UIMA com-
ponents, a level of understanding of the UIMA 
framework is still required. Conversion of exist-
ing tools to the UIMA framework can also be 
difficult, particularly when they are written in 
other programming languages. 
We have designed a simple I/O format to 
make it easy for developers who just want to 
provide a UIMA wrapper for existing tools.  
Input of this format consists of two parts: raw 
text and annotations The first line of the raw text 
section is an integer of byte count of the length 
of the text. The raw text then follows with a new-
line character appended at the end. Annotations 
are then included; one annotation per line, some-
times referring another annotation by assigned 
ids (Figure 5). A line consists of begin position, 
27
end position, type name, unique id, and feature 
values if any. Double newlines indicates an end 
of a CAS. 
Output of the component is lines of annota-
tions if any created by the component. 
U-Compare provides a wrapper component 
which uses this I/O format, communicating with 
wrapped tools via standard I/O streams. 
5.4 Type System Converters 
As U-Compare is a joint project, the U-Compare 
toolkit includes UIMA components originally 
developed using several different type systems. 
In order to integrate these components into the 
U-Compare type system, we have developed 
type system converter components for each ex-
ternal type system. 
The CCP team at the University of Colorado 
made a converter between their CCP type system 
and our type system. We also developed conver-
ters for OpenNLP components and Apache UI-
MA components. These converters remove any 
original annotations not compatible with the U-
Compare type system. This prevents duplicated 
converters from translating external annotation 
multiple times in the same workflow. 
We are providing such non U-Compare com-
ponents by aggregating with type system conver-
ters, so users do not need to aware of the type 
system conversions. 
5.5 Utility Tools  
We have developed and integrated several utility 
tools, especially GUI tools for usability and error 
analysis. 
Figure 6 is showing our workflow manager 
GUI, which provides functions to create a user 
workflow by an easy drag-and-drop way. By 
clicking ?Run Workflow? button in that manager 
window, statistics will be shown (Figure 8).  
Figure 6. A s
There are also a couple of annotation visuali-
zation tools. Figure 7 is showing a viewer for 
tree structures and HPSG feature structures. Fig-
ure 9 is showing a general annotation viewer, 
when annotations have complex inter-
dependencies. 
6 Summary and Future Directions  
We have designed and developed a pluggable 
evaluation system based on the UIMA frame-
work. This evaluation system is integrated with 
the U-Compare combinatorial comparison me-
chanism which makes evaluation of many factors 
available automatically. 
creenshot of Workflow Manager 
GUI and Component Library. 
Since the system behavior is dependent on the 
type system used, we have carefully designed the 
U-Compare type system to cover a broad range 
of concepts used in NLP applications. Based di-
rectly on this type system, or using type system 
converters, we have developed a large toolkit of 
type system compatible interoperable UIMA 
component. All of these features are integrated 
into U-Compare. 
Figure 7. A screenshot of HPSG feature structure 
viewer, showing a skeleton CFG tree, feature values 
and head/semhead links. 
28
In future we are planning to increase the num-
ber of components available, e.g. more syntactic 
parsers, corpus readers, and resources for lan-
guages other than English. This will also re-
quired enhancements to the existing type system 
to support additional components. Finally we 
also hope to add integration with machine learn-
ing tools in the near future. 
 
Acknowledgments 
onal Centre for Text Mining is 
Figure 8. A screenshot of a comparison statistics showing number of instances (gold, test, and
matched), F1, precision, and recall scores of two evaluation metrics on the same data. 
 
We wish to thank Dr. Lawrence Hunter?s text 
mining group at Center for Computational Phar-
macology, University of Colorado School of 
Medicine, for helping build the type system and 
for making their tools available for this research. 
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT, 
Japan). The Nati
funded by JISC. 
W.
ning sys-
tems. J Biomed Discov Collab, 3(1), 1. 
An
ie in the Sky, ACL, Ann Arbor, 
Michigan, USA. 
Ra
tificial Intelligence in Medi-
cine, 33(2), 139-155. 
References  
 A. Baumgartner, Jr., K. B. Cohen, and L. Hunter. 
2008. An open-source framework for large-scale, 
flexible evaluation of biomedical text mi
n  Bies, Seth Kulick, and Mark Mandel. 2005. Pa-
rallel entity and treebank annotation. In Proceed-
ings of the the Workshop on Frontiers in Corpus 
Annotations II: P
zvan  Bunescu, Ruifang Ge, Rohit J. Kate, Edward 
M. Marcotte, Raymond J. Mooney, Arun Kumar 
Ramani, et al 2005. Comparative experiments on 
learning information extractors for proteins and 
their interactions. ArFigure 9. A screenshot of a visualization of com-
plex annotations. 
29
Razvan Bunescu, and Raymond Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. In Y. 
Weiss, B. Scholkopf and J. Platt (Eds.), Advances 
in Neural Information Processing Systems 18 (171-
-178). Cambridge, MA: MIT Press. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. 
In Proceedings of the the 5th International Confe-
rence on Language Resources and Evaluation 
(LREC 2006). 
David Ferrucci, and Adam Lally. 2004. Building an 
example application with the Unstructured Infor-
mation Management Architecture. Ibm Systems 
Journal, 43(3), 455-475. 
David Ferrucci, Adam Lally, Daniel Gruhl, and Ed-
ward Epstein. 2006. Towards an Interoperability 
Standard for Text and Multi-Modal Analytics. 
U. Hahn, E. Buyko, R. Landefeld, M. M?hlhausen, M.  
Poprat, K.  Tomanek, et al 2008, May. An Over-
view of JCoRe, the JULIE Lab UIMA Component 
Repository. In Proceedings of the LREC'08 Work-
shop, Towards Enhanced Interoperability for Large 
HLT Systems: UIMA for NLP, Marrakech, Moroc-
co. 
Lynette Hirschman, Alexander Yeh, Christian 
Blaschke, and Antonio Valencia. 2004. Overview 
of BioCreAtIvE: critical assessment of information 
extraction for biology. BMC Bionformatics, 
6(Suppl 1:S1). 
Yoshinobu Kano, William A Baumgartner, Luke 
McCrohon, Sophia Ananiadou, Kevin B Cohen, 
Lawrence Hunter, et al 2009. U-Compare: share 
and compare text mining tools with UIMA. Bioin-
formatics, accepted. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Keiichi-
ro Fukamachi, Kazuhiro Yoshida, Yusuke Miyao, 
et al 2008c, January. Sharable type system design 
for tool inter-operability and combinatorial com-
parison. In Proceedings of the the First Internation-
al Conference on Global Interoperability for Lan-
guage Resources (ICGL), Hong Kong. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Keiichiro Fukamachi, Yusuke Miyao, 
et al 2008b, January. Towards Data And Goal 
Oriented Analysis: Tool Inter-Operability And 
Combinatorial Comparison. In Proceedings of the 
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP), Hyderabad, India. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Yusuke Miyao, Yoshimasa Tsuruoka, 
et al 2008a, January. Filling the gaps between 
tools and users: a tool comparator, using protein-
protein interaction as an example. In Proceedings 
of the Pacific Symposium on Biocomputing (PSB), 
Hawaii, USA. 
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, 
Yuka Tateisi, and Nigel Collier. 2004. Introduction 
to the Bio-Entity Recognition Task at JNLPBA. In 
Proceedings of the International Workshop on Nat-
ural Language Processing in Biomedicine and its 
Applications (JNLPBA-04), Geneva, Switzerland. 
Kristofer Franzen, Gunnar Eriksson, Fredrik Olsson, 
Lars Asker, Per Liden, and Joakim Coster. 2002. 
Protein names and how to find them. International 
Journal of Medical Informatics, 67(1-3), 49-61. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the penn treebank. Com-
putational Linguistics, 19(2), 313-330. 
Yusuke Miyao, and Jun'ichi Tsujii. 2008. Feature 
Forest Models for Probabilistic HPSG Parsing. 
Computational Linguistics, 34(1), 35-80. 
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi, and To-
moko Ohta. 2007, April. AKANE System: Protein-
Protein Interaction Pairs in BioCreAtIvE2 Chal-
lenge, PPI-IPS subtask. In Proceedings of the 
Second BioCreative Challenge Evaluation Work-
shop. 
Burr Settles. 2005. ABNER: an open source tool for 
automatically tagging genes, proteins and other 
entity names in text. Bioinformatics, 21(14), 3191-
3192. 
Yuka Tateisi, Tomoko Ohta, Nigel Collier, Chikashi 
Nobata, and Jun'ichi Tsujii. 2000, August. Building 
an Annotated Corpus from Biology Research Pa-
pers. In Proceedings of the COLING 2000 Work-
shop on Semantic Annotation and Intelligent Con-
tent, Luxembourg. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun'ichi Tsujii. 2005, October. Syntax Annotation 
for the GENIA Corpus. In Proceedings of the the 
Second International Joint Conference on Natural 
Language Processing (IJCNLP '05), Companion 
volume, Jeju Island, Korea. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin Dong Kim, 
Tomoko Ohta, J. McNaught, Sophia Ananiadou, et 
al. 2005. Developing a robust part-of-speech tag-
ger for biomedical text. In Advances in Informatics, 
Proceedings (Vol. 3746, 382-392). Berlin: Sprin-
ger-Verlag Berlin. 
 
30
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180?191,
Paris, October 2009. c?2009 Association for Computational Linguistics
Effective Analysis of Causes and Inter-dependencies of Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we propose two methods for
analyzing errors in parsing. One is to clas-
sify errors into categories which grammar
developers can easily associate with de-
fects in grammar or a parsing model and
thus its improvement. The other is to
discover inter-dependencies among errors,
and thus grammar developers can focus on
errors which are crucial for improving the
performance of a parsing model.
The first method uses patterns of er-
rors to associate them with categories of
causes for those errors, such as errors in
scope determination of coordination, PP-
attachment, identification of antecedent of
relative clauses, etc. On the other hand,
the second method, which is based on re-
parsing with one of observed errors cor-
rected, assesses inter-dependencies among
errors by examining which other errors
were to be corrected as a result if a spe-
cific error was corrected.
Experiments show that these two meth-
ods are complementary and by being com-
bined, they can provide useful clues as to
how to improve a given grammar.
1 Introduction
In any kind of complex systems, analyzing causes
of errors is a crucial step for improving its perfor-
mance. In recent sophisticated parsing technolo-
gies, the step of error analysis has been becoming
more and more convoluted and time-consuming,
if not impossible. While common performance
evaluation measures such as F-values are useful to
compare the performance of systems or evaluate
improvement of a system, they hardly give useful
clues as to how to improve a system. Evaluation
measures usually assume uniform units such as the
number of correctly or incorrectly recognized con-
stituent boundaries and their labels, or in a similar
vein, dependency links among words and their la-
bels, and then compute single values such as the F-
value. These values do not give any insights as to
where the weaknesses exist in a parsing model. As
a result, the improvement process takes the form
of time consuming trial-error cycles.
Once grammar developers know the actual dis-
tribution of errors across different categories such
as PP-attachment, complement/adjunct distinc-
tion, gerund/participle distinction, etc., they can
think of focused and systematic improvement of
a parsing model.
Another problem of the F-value in terms of
uniform units is that it does not take inter-
dependencies among errors into consideration. In
particular, for parsers based on grammar for-
malisms such as LFG (Kaplan and Bresnan, 1995),
HPSG (Pollard and Sag, 1994), or CCG (Steed-
man, 2000), units (eg. single predicate-argument
links) are inter-related through hierarchical struc-
tures and structure sharing assumed by these for-
malisms. Single errors are inherently propagated
to other sets of errors. This is also the case, though
to a lesser extent, for parsing models in which
shallow parsing is followed by another component
for semantic label assignment.
In order to address these two issues, we propose
two methods in this paper. One is to recognize
cause categories of errors and the other is to cap-
ture inter-dependencies among errors. The former
method defines various patterns of errors to iden-
tify categories of error causes. The latter method
re-parses a sentence with a single target error cor-
rected, and regards the errors which are corrected
in re-parse as errors dependent on the target.
Although these two methods are implemented
for a specific parser using HPSG (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), the same ideas
can be applied to any type of parsing models.
180
Predicate 
Sentence: John    has    come
Predicative event 1:
Predicative event 2:
Word:  hasGrammatical nature:  auxiliary# of arguments:  2
Argument 1 John
Argument 2 come
Predicate 
Word:  comeGrammatical nature:  verb# of arguments:  1
Argument 1 John
Predicate-argument relations
Figure 1: Predicate-argument relations
John aux_2args
ARG1 ARG2
verb_1arg
ARG1
has : come :
Figure 2: Representation of predicate-argument
relations
In the following, Section 2 introduces a parser
and its evaluation metrics, Section 3 illustrates dif-
ficulties in analyzing parsing errors based on com-
mon evaluation measures, and Section 4 proposes
the two methods for effective error analysis. Sec-
tion 5 presents experimental results which show
how our methods work for analyzing actual pars-
ing errors. Section 6 and Section 7 illustrate fur-
ther application of these methods to related topics.
Section 8 summarizes this research and indicates
some of future directions.
2 A parser and its evaluation
A parser is a system which interprets given sen-
tences in terms of structures derived from syn-
tactic or in some cases semantic viewpoints, and
structures constructed as a result are used as es-
sential information for various tasks of natural lan-
guage processing such as information extraction,
machine translation, and so on.
In this paper, we address issues involved in im-
proving the performance of a parser which pro-
duces structural representations deeper than sur-
face constituent structures. Such a parser is called
a ?deep parser.? In many deep parsers, the output
structure is defined by a linguistics-based gram-
mar framework such as CFG, CCG (Steedman,
2000), LFG (Kaplan and Bresnan, 1995) or HPSG
Abbr. Full Abbr. Full
aux auxiliary conj conjunction
prep prepositional lgs logical subject
verb verb app apposition
coord coordination relative relative
det determiner Narg(s) takes N arguments
adj adjunction mod modifies a word
Table 1: Descriptions for predicate types
(Pollard and Sag, 1994). Alternatively, some deep
parsing models assume staged processing in which
a stage of shallow parsing is followed by a stage of
semantic role labeling, which assigns labels indi-
cating semantic relationships between predicates
and their arguments. In either case, we assume a
parser to produce a single ?deep? structural rep-
resentation for a given sentence, which is chosen
from a set of possible interpretations as the most
probable one by a disambiguation model.
For evaluation of the performance of a parser,
various metrics have been introduced according
to the structure captured by a given grammar
formalism or a system of semantic labels. In
most cases, instead of examining correctness for
a whole structure, a parser is evaluated in terms of
the F-value which shows how correctly it recog-
nizes relationships among words and assigns ?la-
bels? to the relationships in the structure. In this
paper, we assume a certain type of ?predicate-
argument relation.?
In this measurement, a structure given for a
sentence is decomposed into a set of predicative
words and their arguments. A predicate takes
other words as its arguments. In our representa-
tion, the arguments are labeled by semantically
neutral labels such as ARGn(n = 1...5) and
MOD. In this representation, a basic unit is a
triplet, such as
<Predicate:PredicateType,
ArgumentLabel,
Argument>,
where ?Predicate? and ?Argument? are surface
words. As shown in the examples in Section 4,
?PredicateType? bears extra information concern-
ing the syntactic construction in which the triplet
is embedded. ARG1-ARG5 express relations be-
tween a Head and its complement, while MOD ex-
presses a relation between an Adjunct and its mod-
ifiee. Since all dependency relations are expressed
by triplets, triplets contain not only semantic de-
181
I saw a girl with a telescope Correct answer:
ARG1 ARG2
ARG1 ARG2
Parser output:
ARG1 ARG2
ARG1 ARG2Compare
Error (25%): 
ARG1
ARG1
Correct (75%):
ARG1 ARG2
ARG2ARG1 ARG2
ARG2
I saw a girl with a telescope 
I saw a girl with a telescope 
I saw a girl with a telescope 
Figure 3: An example of parsing performance
evaluations
pendencies but also many dependencies which are
essentially syntactic in nature. Figure 1 shows an
example used in Miyao and Tsujii (2005) and Ni-
nomiya et al (2006).
This example shows predicate-argument rela-
tions for ?John has come.? There are two pred-
icates in this sentence, ?has? and ?come?. The
word ?has?, which is used as an auxiliary verb,
takes two words, ?John? and ?come?, as its ar-
guments, and therefore two triplets of predicate-
argument relation, <has ARG1 John> and <has
ARG2 come>. As for the predicative word
?come?, we have one triplet <come ARG1 John>.
Note that, in this HPSG analysis, the auxiliary
verb ?has? is analyzed in such a way that it takes
one NP as subject and one VP as complement,
and that the subject of the auxiliary verb is shared
by the verb (?come?) in VP as its subject (Fig-
ure 2). The fact that ?has? in this sentence is an
auxiliary verb is indicated by the ?PredicateType?,
aux 2args. A ?PredicateType? consists of a type
and the number of arguments it takes (Table 1).
3 Difficulties in analyzing parsing errors
Figure 3 shows an example of the evaluation of
the parser based on these predicate-argument rela-
tions. Note that the predicate types are abbreviated
in this figure. In the sentence ?I saw a girl with a
telescope?, there should be four triplets for the two
predicates, ?saw? and ?with,? each of which takes
Error:
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Conflict
Analysis 2: (Impossible)
They completed the sale of for ARG1ARG1
it to him $1,000 
Analysis 1: (Possible) ARG1ARG1
ARG1ARG1
Can each error occur independently?
They completed the sale of for ARG1 ARG1
it to him $1,000 
ARG1ARG1
Figure 4: Sketch of error propagation
The book on which read the shelf  I yesterdayARG1 ARG2
ARG2ARG1
Error:
Figure 5: Parsing errors around one relative clause
attachment
two arguments. Although the parser output does
indeed contain four triplets, the first argument of
?with? is not the correct one. Thus, this output is
erroneous, with the F-value of 75%.
While the F-value thus computed is fine for cap-
turing the performance of a parser, it does not offer
any help for improving its performance.
First, because it does not give any indica-
tion on what portion of erroneous triplets are in
PP-attachment, complement/adjunct distinction,
gerund/participle distinction, etc., one cannot de-
termine which part of a parsing model should be
improved. In order to identify error categories, we
have to manually compare a parsing output with
a correct parse and classify them. Consider again
the example in Figure 3. We can easily observe
that ?ARG1? of predicate ?with? was mistaken. In
this case, the word linked via ?ARG1? represents
a modifiee of the prepositional phrase, and thereby
we conclude that the error is in PP-attachment.
While the process looks straightforward for this
simple sentence and error, to perform such a man-
ual inspection for all sentences and more complex
types of errors is costly, and becomes inhibitive
when the size of a test set of sentences is realisti-
182
cally large.
Another problem with the F-value is that it ig-
nores inter-dependencies among errors. Since the
F-value does not consider inter-dependencies, one
cannot determine which errors are more crucial
than others in terms of the performance of the sys-
tem as a whole.
A simple example of inter-dependency is shown
in Figure 4. ?ARG1? of ?for? and ?to? were mis-
taken by a parser, both of which can be classified
as PP-attachments as in Figure 3. However, the
two errors are not independent. The former error
can occur by itself (Analysis 1) while the latter
cannot because of the structural conflict with the
former (Analysis 2). The occurrence of the latter
error thus forces the former.
Moreover, inter-dependency in a deep parser
based on linguistics-based formalisms can be
complicated. Error propagation is ingrained in
grammar itself. Consider Figure 5. In this exam-
ple, a wrong decision on the antecedent of a rela-
tive clause results in a wrong triplet of the predi-
cate in the embedded clause with the antecedent.
That is, the two erroneous triplets, one of the
?ARG1? of ?which? and the other of the ?ARG2?
of ?read,? were caused by a single wrong deci-
sion of the antecedent of a relative clause. Such
a propagation of errors can be even more compli-
cated, for example, when the predicate in the rela-
tive clause is a control verb.
In the following section we propose two meth-
ods for analyzing errors. Although both meth-
ods are implemented for the specific parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006),
the same ideas can be implemented for any parsing
model.
4 Methods for effective error analysis
4.1 Recognizing categories of error causes
While the Enju parser produces rich feature struc-
tures as output, the performance is evaluated by
the F-value in terms of basic units of predicate-
argment structure. As we illustrated in Section 2,
the basic unit is a triplet in the following form.
<Predicate:PredicateType,
ArgumentLabel,
Argument>
We illustrated in Section 2 how we can identify
errors in PP-attachment simply by examining a
The  car  was  designed to : use   it  for ...
Correct output:
aux_2argsto :[ verb1 ] ?ARG3 [ verb2 ]Parser output:
aux_mod_2args
MOD
to :
ARG2
Unknown subject ARG1 ARG1
[ verb1 ] ? [ verb2 ]
aux_2args
Example:
Parser output:
Correct answer:
ARG3
The  car  was  designed to : use   it  for ...aux_mod_2args
MOD ARG2
Unknown subject ARG1 ARG1
Pattern:
(Patterns of correct answer and parser output can be interchanged)
Figure 6: Pattern for ?To-infinitive for modi-
fier/argument of verb?
triplet produced by the parser with the correspond-
ing triplet in the gold standard parse.
However, in more complex cases, we have to
consider a set of mismatched triplets collectively
in order to map errors to meaningful error causes.
The following are typical examples of error causes
and pattern rules which identify them.
(1) Interpretation of Infinitival Clauses as Adjunct
or Complement
Two different types of interpretations of the in-
finitival clauses are explicitly indicated by ?Predi-
cateType.? Consider the following two sentences.
(a) [Infinitival clause as an adjunct of the main
clause]
The car was designed (by John) to use it for
business trips.
(b) [Infinitival clause as an argument of catena-
tive verb]
The car is designed to run fast.
In both sentences, ?to? is treated as a predicate to
represent the infinitival clauses in triplets. How-
ever, Enju marks the ?PredicateType? of (a) as
?aux-mod-2args,? while it marks the predicate
simply as ?aux-2args? in (b). Furthermore, the
linkage between the main clause and the infinitival
clause is treated differently. In (a), the infinitival
clause takes the main clause with relation MOD,
while in (b) the main clause takes the infinitival
183
[ gerund ]: verb_Narg(s)Parser output: [ gerund ]: verb_mod_Narg(s)Correct answer:(Patterns of correct answer and parser output can be interchanged)
Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_3args
you to have
in MOD
ARG1
ARG2 ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting: verb_3args
you to have
in
Not exist 
ARG2 ARG3
ARG1 (MOD)
? ?
? ?
Figure 7: Pattern for ?Gerund acts as modifier or
not?
clause as ARG3. Furthermore, in the catenative
verb interpretation of ?designed?, the deep object
(the surface subject in this example) fills ARG1
of the verb in the infinitival clause (complement),
while in the adjunct interpretation, the deep sub-
ject which is missing in this sentence occupies
the same role. Consequently, a single erroneous
choice between these two interpretations results in
a set of mismatched triplets.
We recognize such a set of mismatched triplets
by a pattern rule (Figure 6) and map them to this
type of error cause.
(2) Interpretation of Gerund-Participle interpreta-
tions
A treatment similar to (1) is taken for different
interpretations of Gerund. Interpretation as Ad-
junct of a main clause is signaled by the ?Predi-
cateType? verb-mod-*, while an interpretation as
a modifier of a noun is represented by the ?Predi-
cateType? verb (Figure 7).
(3) Interpretation of ?by?
A prepositional phrase with ?by? in a passive
clause can be interpreted as a deep subject, while
the same phrase can be interpreted as an ordinary
PP phrase that is used as an adjunct. The first in-
terpretation is marked by the ?PredicateType? lgs
(logical subject) which takes only one argument.
The relationship between the passivized verb and
the deep subject is captured by ARG1 which goes
Example:
Pattern:
Correct output:
Parser output: prep_2args
Unknown subject
[ verb1 ] ?ARG1ARG1 ?
lgs_1arg ARG1[ verb1 ] ? ?ARG1
A 50-state study released in September  by : Friends ?
Unknown subject
? ARG1ARG1
prep_2argsParser output:
Correct answer:
A 50-state study released in September  by : Friends ??
ARG1ARG1
lgs_1argARG1
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
Example:
Pattern:
relative_1arg
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
relative_1arg
relative_1arg
Error
? ?
Figure 9: Pattern for ?Relative clause attachment?
from the verb to the noun phrase. On the other
hand, in the interpretation as an ordinary PP, the
preposition as predicate links the main verb and
NP via ARG1 and ARG2, respectively (Figure 8).
Again, a set of mismatched triplets should be
mapped to a single cause of errors via a pattern
rule.
(4) Antecedent of a Relative Clause
This type of error is manifested by two mis-
matched triplets with different predicates. This is
because a wrong choice of antecedent for a rela-
tive clause results in a wrong link for the trace of
the relative clause.
Since a relative clause pronoun is treated as a
184
Cause categories Patterns
[Argument selection]
Prepositional attachment ARG1 of prep *
Adjunction attachment ARG1 of adj *
Conjunction attachment ARG1 of conj *
Head selection for noun phrase ARG1 of det *
Coordination ARG1/2 of coord *
[Predicate type selection]
Preposition/Adjunction prep * ? adj *
Gerund acts as modifier/not verb mod Narg(s)
? verb Narg(s)
Coordination/conjunction coord * ? conj *
# of arguments for preposition prep Marg(s)
? prep Narg(s)
Adjunction/adjunctive noun adj * ? noun *
[More structural errors]
To-infinitive for see Figure 6
modifier/argument of verb
Subject for passive sentence/not see Figure 8
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Defined patterns for cause categories
predicate which takes the antecedent as its single
argument, identification of error type can be done
simply by looking at ARG1. However, since the
errors usually propagate to the triplets that contain
their traces, we have to map them together to the
single error (Figure 9).
Table 2 shows the errors across different types
which our current version of pattern rules can
identify.
4.2 Capturing inter-dependencies among
errors
Some inter-dependencies among erroneous
triplets are ingrained in grammar, such as the case
of antecedent of a relative clause in (4) in Section
4.1. Some are caused by general constraints such
as the projection principle in dependency structure
(Figure 4 in Section 2).
Regardless of causes of dependencies, to recog-
nize inter-dependencies among errors is a crucial
step of effective error analysis.
Our method consists of the following four steps:
[Step 1] Re-parsing a target sentence: A given sen-
tence is re-parsed under the condition where an er-
ror is forcibly corrected.
[Step 2] Forming a network of inter-dependencies
of errors: By comparing the new parse result (a
set of triplets) with the initial parse result, this
step creates a directed graph of errors in the ini-
1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
1
2
3
Correct 2
1
1
Form inter-dependent error groups anderror propagation network
4 1
433CorrectCorrect
Correct
disappear
disappear
disappear
disappear
,
,
,
1 2 3 4
ARG1our work force todayon
Errors:
ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1It  has  no  bearing
2 3 4
5
5 4Correct disappear1 32, , ,
4,
2 4,,
2 3,,
5Propagation
Resultant network:
Inter-dependent error group Inter-dependent error group
(a)
(b)
(c)
Inter-dependency among errors:re-parse
re-parse
re-parse
re-parse
re-parse
Figure 10: Schema of capturing inter-
dependencies
tial parse. A directed link shows that correction of
the error in the starting node produces a new parse
result in which the error in the receiving node of
the link disappears.
[Step 3] Forming groups of inter-dependent errors:
This step recognizes a group of inter-dependent er-
rors which forms a directed circle in the network
created by [Step 2].
[Step 4] Forming a network of error propagation:
This step creates a new network by reducing each
of inter-dependent error groups of [Step 3] to a sin-
gle node.
Figure 10 illustrates how these steps work. In
this example, while ?today? should modify the
noun phrase ?our work force?, the initial parse
wrongly takes ?today? as the head noun of the
whole noun phrase. As a result, there are five er-
rors; three wrong outputs, ?ARG2? of ?on? (Er-
ror 1), ?ARG1? of ?our? (Error 2) and ?ARG1?
of ?work? (Error 3). There is an extra triplet for
?ARG1? of ?force? (Error 4), and a triplet for
?ARG1? of ?today? (Error 5) is missing (Figure
10 (a)).
Figure 10 (b) shows inter-dependencies among
the errors recognized by [Step 2], and Figure 10
185
# ofCause categories of errors Errors Locations
Classified 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier/not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence/not 8 3
[Others]
Comma 444 372
Relative clause attachment 102 38
Unclassified 2,631 ?
Total 4,709 ?
Table 3: Errors classified into cause categories
(c) shows what the resultant network looks like.
An inter-dependent error group of 1, 2, 3 and 4 is
recognized by [Step 3] and represented as a single
node. Error 5 is propagated to this node in the final
network.
5 Experiments
We applied our methods to the analyses of actual
errors produced by Enju. This version of Enju was
trained on the Penn Treebank (Marcus et al, 1994)
Section 2-21.
5.1 Observation of identified cause categories
We first parsed sentences in PTB Section 22, and
based on the observation of errors, we defined the
patterns in Section 4. We then parsed sentences in
Section 0. The errors in Section 0 were mapped to
error cause categories by the pattern rules created
for Section 22.
Table 3 summarizes the distribution across the
causes of errors. The left and right numbers in the
table show the number of erroneous triplets clas-
sified into the categories and the frequency of the
patterns matched, respectively. The table shows
that, with the 14 pattern rules, we successfully ob-
served 1,671 hits and 2,078 erroneous triplets are
dealt with by these hits. This amounts to more
than 40% erroneous triplets (2,078/4,709). Since
this was the first attempt, we expect the coverage
can be easily improved by adding new patterns.
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Inter-dependent error groups 1,978
Correction propagations 501
F-score (LP/LR) 90.69 (90.78/90.59)
Table 4: Summary of inter-dependencies




       	 
 










Figure 11: Frequency of each size of inter-
dependent error group
From the table, we can observe that a signif-
icant portion of errors is covered by simple types
of error causes such as PP-attachment and Adjunct
attachment. They are simple in the sense that the
number of erroneous triplets treated and the fre-
quency of the pattern application coincide. How-
ever, their conceived significance may be over-
rated. These simple types may constitute parts of
more complex error causes. Furthermore, since
pattern rules for these simple causes are easy to
prepare and have already been covered by the cur-
rent version, most of the remaining 60% of the er-
roneous triplets are likely to require patterns for
more complex causes.
On the other hand, patterns for complex causes
collect more erroneous triplets once they are fired.
This tendency is more noticeable in structural pat-
terns of errors. For example, in ?To-infinitive for
modifier/argument of verb,? there were only 22
hits for the pattern, while the number of erroneous
triplets is 120. This implies five triplets per hit.
This is because, in a deep parser, a wrong choice
between adjunct or complement interpretations of
a to-infinitival clause affects the interpretation of
implicit arguments in the clause through control.
Though expected, such detailed observations show
how differences between shallow and deep parsers
may affect evaluation methods and the methods of
analyzing errors.
5.2 Observation of inter-dependencies
In the inter-dependency experiments we per-
formed, some errors could not be forcibly cor-
rected by our method. This was because the parser
186
The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once
it  enters  the  lungs  ,  with  even  brief  exposures  to  it causing
symptoms  that  show  up  decades  later  ,  researchers  said  .
(a) (b)(c) (d)
? fiber      , : crocidolite ?app_2args
? fiber      , : crocidolite ?coord_2args
Correct answer:
Parser output:
? is   usually   resilient   ? the   lungs      ,       with   ?
? symptoms   that    show : up   decades   later  ?
Parser output:
Correct answer: verb_1arg
? symptoms   that    show : up   decades   later  ?verb_2args
? it   causing   symptoms   that   show   up   decades   later  ?
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Inter-dependent error group (c):
Inter-dependent error group (d):
ARG1 ARG2
ARG1 ARG2
ARG1ARG1
ARG1ARG1
ARG1
ARG1
ARG1
ARG2
ARG1
ARG1
Figure 12: Obtained inter-dependent error groups
we use prunes less probable parse substructures
during parsing. In some cases, even if we gave a
large positive value to a triplet which should be in-
cluded in the final parse, parsing paths which can
contain the triplet were pruned before. In this re-
search, we ignored such errors as ?uncorrectable?
ones, and focused on the remaining ?correctable?
errors.
Table 4 shows a summary of the analysis. As the
previous experiment, Enju produced 4,709 errors
for Section 0, of which 3,085 were correctable. By
applying the method illustrated in Section 4.2, we
obtained 1,978 inter-dependent error groups and
501 correction propagation relationships among
the groups.
Figure 11 shows the frequency of the size of
inter-dependent error groups. About half of the
groups contain only single errors which could
have only one-way correction propagations with
other errors or were completely independent of
other errors.
Figure 12 shows an example of the extracted
inter-dependent error groups. For the sentence
shown at the top, Enju gave seven errors. By ap-
plying the method in Section 4.2, these errors were
grouped into four inter-dependent error groups (a)
to (d), and no correction propagations were de-
She  says  she  offered  Mrs.  Yeargin a  quiet  resignation  and
thought  she  could  help  save  her  teaching  certificate .(a) (b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):Correct answer:
Parser output:
? she  could  help  save : her  teaching  certificate .verb_3args
? she  could  help  save : her  teaching  certificate .verb_2args
Correct answer:
Parser output:
? thought   she  could   help : save   ?verb_2args
? thought   she  could   help : save   ?aux_2args
Correction propagation from (a) to (b)
ARG2
ARG2
ARG2
ARG2
ARG1 ARG2
ARG1 ARG2
ARG3ARG1
ARG1
ARG2
ARG2
ARG1
Figure 13: Correction propagation between ob-
tained inter-dependent error groups
tected among them. Group (a) contains two errors
on the comma?s local behavior as apposition or co-
ordination. Group (b) contains the errors on the
words which give almost the same attachment be-
haviors. Group (c) contains the errors on whether
the verb ?show? took ?decades? as its object or
not. Group (d) contains an error on the attachment
of the adverb ?later?. Regardless of the overlap
of the regions in the sentence for (c) and (d), our
approach successfully grouped the errors into two
independent groups. The method shows that the
errors in each group are inter-dependent, but er-
rors in one group are independent of those in an-
other. This enables us to concentrate on each of
the co-occurring error groups separately, without
minding the errors in other groups.
Figure 13 shows another example. In this ex-
ample, eight errors for a sentence were classified
into two inter-dependent error groups (a) and (b).
Moreover, it shows that the correction of group (a)
results in correction of group (b).
The errors in group (a) were related to the
choice as to whether ?help? had an auxiliary or
a pure verbal role. The errors in group (b) were
related with the choice as to whether ?save? took
only one object (?her teaching certificate?) or two
objects (?her? and ?teaching certificate?). Be-
tween group (a) and (b), no ?structural? con-
187
It  invests  heavily  in  dollar-denominated  securities  overseas  and
is  currently  waiving  management  fees  ,  which  boosts  its yield .
(a)(b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Adjunction attachmentCause categories: 
Comma, Relative clause attachmentCause categories: 
It  invests  heavily  in  ? securities  overseas : ?adj_1argARG1 ARG1
? is  currently  waiving  management  fees   ,   which   boosts  ?
ARG1ARG1ARG1
ARG1ARG1ARG1
Figure 14: Combining our two methods (1)
flict could arise when correcting only each of the
groups. We could then hypothesize that the cor-
rection propagation between the two groups were
caused by the disambiguation model.
By dividing the errors into minimal units and
clarifying the effects of correcting a target error,
we can conclude that the inter-dependent group
(a) should be handled first for effective improve-
ment of the parser. In such a way, obtained inter-
dependencies among errors can suggest an effec-
tive strategy for parser improvement.
5.3 Combination of the two methods
By combining the two methods described in Sec-
tion 4.1 and 4.2, we can see how each error cause
affects the performance of a parser. The results
are summarized in Table 5. The leftmost column
in the table shows the numbers of errors in terms
of triplets, which are the same as the leftmost col-
umn in Table 3.
The ?independence rate? shows the ratio of er-
roneous triplets in the category which are not af-
fected by correction of other erroneous triplets. On
the other hand, the ?correction effect? shows how
many erroneous triplets would be corrected if one
of the erroneous triplets in the category was cor-
rected. These two columns are computed by using
the error propagation network constructed in Sec-
tion 4.2. That is, by using the network we obtain
the number of erroneous triplets to be corrected if
a given erroneous triplet in the category was cor-
rected, sum up these numbers and then calculate
the average number of expected side-effect correc-
Clark  J.  Vitulli was  named  senior  vice  president  and  general  
manager  of   this  U.S.  sales  and  marketing  arm of Japanese
auto Maker Mazda Motor Corp .
(b)(a)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Coordination (fragment)Head selection for noun phraseCause categories: 
? president  ? of  this  U.S.  sales   and : ?coord_2argsARG1ARG1
of   this : U.S. sales and : marketing armdet_1arg coord_2args
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Correction propagation from (a) to (b)
Coordination (fragment)Cause categories: 
Figure 15: Combining our two methods (2)
tion per erroneous triplet in the category.
Figure 14 shows an example of independent er-
rors. For the sentence at the top, the parser pro-
duced four errors. The method in Section 4.2
successfully discovered two inter-dependent error
groups (a) and (b). There was no error propaga-
tion relation between the two groups. On the other
hand, the method in Section 4.1 associated all of
these four errors with the categories of ?Adjunc-
tion attachment,? ?Comma? and ?Relative clause
attachment,? and the error for the ?Adjunction at-
tachment? corresponds to the inter-dependent er-
ror group (a). Because this group is not a receiving
node of any propagation in the network, the error
is regarded as an ?independent? one.
Independent errors mean that, if a new parsing
model could correct them, the correction would
not be destroyed by other errors which remain in
the new parsing model.
The correction effect shows the opposite and
desirable effect of the nature of the dependency
among errors which the propagation network rep-
resents. This means that, if one of erroneous
triplets in the category was corrected, the correc-
tion would be amplified through propagation, and
as a result other errors would also be corrected.
We show an example of the correction effect in
Figure 15. In the figure, the parser had six errors;
three false outputs for ARG1 of ?and,? ?this? and
188
Independence Correction Expected rangeCause categories of errors # of errors
rate (%) effect (%) of error correction
[Argument selection]
Prepositional attachment 579 74.8 144.3 625.0 - 835.5
Adjunction attachment 261 56.6 179.6 265.3 - 468.8
Conjunction attachment 43 36.4 239.4 37.5 - 102.9
Head selection for noun phrase 30 0.0 381.8 0.0 - 114.5
Coordination 202 42.5 221.2 189.9 - 446.8
[Predicate type selection]
Preposition/Adjunction 108 41.7 158.3 71.3 - 171.0
Gerund acts as modifier/not 84 46.2 159.0 61.7 - 133.0
Coordination/conjunction 54 44.4 169.4 40.6 - 91.5
# of arguments for preposition 51 95.8 108.3 52.9 - 55.2
Adjunction/adjunctive noun 13 75.0 125.0 12.2 - 16.3
[More structural errors]
To-infinitive for 120 36.0 116.0 50.1 - 139.2
modifier/argument of verb
Subject for passive sentence/not 8 37.5 112.5 3.4 - 9.0
[Others]
Comma 444 39.5 194.4 341.0 - 863.1
Relative clause attachment 102 32.1 141.7 46.4 - 144.5
Table 5: Correction propagations between errors for each cause category and the other errors
?U.S.,? two false outputs for ARG2 of ?of? and
?and,? and a missing output for ARG1 of ?sales.?
Our method for inter-dependencies classified these
errors into two inter-dependent error groups (a)
and (b), and extracted an correction propagation
from (a) to (b). Our method for cause categories,
on the other hand, associated two errors of ?and?
with the category ?Coordination? and one error of
?this? with the category ?Head selection for noun
phrase.? When we correct an error in the interde-
pendent error group (a), the correction leads to not
only correction of the other errors in (a) but also
correction of the error in (b) via correction prop-
agation from (a) to (b). Therefore, a correction
effect of an error in group (a) results in 6.0.
On the basis of the above considerations, we es-
timated the range of the effect which an error cor-
rection in each category has. The minimum of ex-
pected correction range in Table 5 is given by the
product of the number of erroneous triplets in the
category, the independence rate and the correction
effect. On the other hand, the maximum is given
by the product of the number of erroneous triplets
in the category and the correction effect. This as-
sumes that all corrections made in the category are
not cancelled by other errors, while the figure in
the minimum are based on the assumption that all
corrections made in the category, except for the in-
dependent ones, are cancelled by other errors.
Table 5 would thus suggest which categories
should be resolved with high priority, from three
points of view: the number of errors in the cat-
egory, the number of independent errors, and the
correction effect.
6 Further applications of our methods
In this section, as an example of the further ap-
plication of our methods, we attempt to analyze
parsing behaviors in domain adaptation from the
viewpoints of error cause categories.
In Hara et al (2007), we proposed a method for
adapting Enju to a target domain, and then suc-
ceeded in improving the parser performance for
the GENIA corpus (Kim et al, 2003), a biomed-
ical domain. Table 6 summarizes the parsing re-
sults for three types of settings respectively: pars-
ing PTB with Enju (?Enju for PTB?), parsing GE-
NIA with Enju (?Enju for GENIA?), and parsing
GENIA with the adapted model (?Adapted for GE-
NIA?). We then analyzed the performance transi-
tion among these settings from the viewpoint of
the cause categories given in Section 4.1 (Table 7).
In order to compare the error frequencies among
different settings, we took the percentage of target
errors in all of the evaluated triplets. The signed
values between the two settings show how much
the errors increased when moving from the left set-
tings to the right ones.
When we focus on the transition from ?Enju
for PTB? to ?Enju for GENIA,? we can observe
that the change in the domain resulted in a dif-
ferent distribution of error causes. The errors for
most categories increased, and in particular, the er-
rors for ?Prepositional attachment? and ?Coordi-
189
Enju for PTB Enju for GENIA Adapted for GENIA
Evaluated sentences 1,811 842 842
Evaluated triplets 44,934 22,230 22,230
Errors 4,709 3,120 2,229
F-score (LP/LR) 90.69 (90.78/90.59) 87.41 (87.60/87.22) 90.93 (91.10/90.76)
Table 6: Summary of parsing performances for domain and model variations
Rate of errors against total examined relations in test set (%)Cause categories of errors Enju for PTB ?? Enju for GENIA ?? Adapted for GENIA
Classified 4.62 +2.60? 7.22 ?1.80? 5.42
[Argument selection]
Prepositional attachment 1.29 +0.93? 2.22 ?0.64? 1.58
Adjunction attachment 0.58 +0.38? 0.96 ?0.20? 0.76
Conjunction attachment 0.10 ?0.04? 0.06 ?0.04? 0.02
Head selection for noun phrase 0.07 +0.17? 0.24 ?0.06? 0.18
Coordination 0.45 +0.59? 1.04 ?0.25? 0.79
[Predicate type selection]
Preposition/Adjunction 0.24 +0.08? 0.32 ?0.06? 0.26
Gerund acts as modifier/not 0.19 ?0.07? 0.12 +0.01? 0.13
Coordination/conjunction 0.12 ?0.00? 0.12 ?0.07? 0.05
# of arguments for preposition 0.11 ?0.02? 0.09 ?0.00? 0.09
Adjunction/adjunctive noun 0.03 +0.19? 0.22 ?0.08? 0.14
[More structural errors]
To-infinitive for 0.27 +0.02? 0.29 ?0.09? 0.20
modifier/argument of verb
Subject for passive sentence/not 0.02 +0.34? 0.36 +0.01? 0.37
[Others]
Comma 0.99 ?0.03? 0.96 ?0.31? 0.65
Relative clause attachment 0.23 +0.05? 0.28 ?0.03? 0.25
Unclassified 5.86 +0.96? 6.82 ?2.22? 4.60
Total (Classified + Unclassified) 10.48 +3.56? 14.04 ?4.01? 10.03
Table 7: Error distributions for domain and model variations
nation? increased remarkably. On the other hand,
the transition from ?Enju for GENIA? to ?Adapted
for GENIA? shows that their adaptation method
succeeded in reducing the errors for most cate-
gories to some extent. However, for ?Preposi-
tional attachment,? ?Coordination,? and ?Subject
for passive sentence or not,? there were still no-
ticeable gaps in error distribution between ?Enju
for PTB? and ?Adapted for GENIA.? This would
mean that the adapted model requires further per-
formance improvement if we expect the same level
of performances for those categories as the parser
originally obtained in PTB.
We could thus capture some biases of cause
categories which occur in domain transition or
in domain adaptation, which would not be clari-
fied by F-score evaluation methods. With inter-
dependencies given by the method described in
Section 4.2, the above analysis would be useful
for effectively exploring further adaptation.
7 Related works
Although there have been many researchers who
analyzed errors in their own systems in the experi-
ments, there has been little research which focused
on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They consid-
ered accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded one step further and attempted to re-
veal the nature of the propagations. In examin-
ing the combination of the two types of parsing,
they utilized approaches similar to our method for
capturing inter-dependencies of errors. They al-
lowed a parser to give only structures produced by
the parsers and utilized the ideas for evaluating the
parser?s potentials, whereas we utilized it for ob-
serving error propagations.
Dredze et al (2007) showed that many of the
parsing errors in domain adaptation tasks may
come from inconsistencies between the annota-
tions of training resources. This would sug-
gest that just error comparisons without consider-
ing the inconsistencies could lead to a misunder-
190
standing of what happens in domain transitions.
The summarized error cause categories and inter-
dependencies given by our methods would be use-
ful clues for extracting such domain-dependent er-
ror phenomena.
When we look into other research areas in nat-
ural language processing, Gime?nez and Ma`rquez
(2008) proposed an automatic error analysis ap-
proach in machine translation (MT) technologies.
They developed a metric set which could capture
features in MT outputs at different linguistic lev-
els with different levels of granularity. Like we
considered parsing systems, they explored ways to
resolve costly and rewardless error analysis in the
MT field. One of their objectives was to enable
researchers to easily obtain detailed linguistic re-
ports on the behavior of their systems, and to con-
centrate on analyses for the system improvements.
8 Conclusion
We proposed two methods for analyzing parsing
errors. One is to assign errors to cause categories,
and the other is to capture inter-dependencies
among errors. The first method defines error pat-
terns to identify cause categories and then asso-
ciates errors involved in the patterns with the cor-
responding categories. The second method re-
parses a sentence with a target error corrected, and
regards errors corrected together as dependent on
the target.
In our experiments with an HPSG parser, we
successfully associated more than 40% of the er-
rors with 14 cause categories, and captured 1,978
inter-dependent error groups. Moreover, the com-
bination of our methods gave a more detailed error
analysis for effective improvement of the parser.
In our future work, we would give more pat-
tern rules for classifying a large percentage of er-
rors into cause categories, and incorporate uncor-
rectable errors into inter-dependency analysis. Af-
ter improving the analytical facilities of our indi-
vidual methods, we would explore the possibil-
ity of combining the methods for obtaining more
powerful and detailed clues on how to improve
parsing performance.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards het-
erogeneous automatic mt error analysis. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), pages 1894?1901.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages
11?22.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180?i182.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
191
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210?213,
Paris, October 2009. c?2009 Association for Computational Linguistics
HPSG Supertagging: A Sequence Labeling View
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
Supertagging is a widely used speed-up
technique for deep parsing. In another
aspect, supertagging has been exploited
in other NLP tasks than parsing for
utilizing the rich syntactic information
given by the supertags. However, the
performance of supertagger is still a
bottleneck for such applications. In this
paper, we investigated the relationship
between supertagging and parsing, not
just to speed up the deep parser; We
started from a sequence labeling view
of HPSG supertagging, examining how
well a supertagger can do when separated
from parsing. Comparison of two types
of supertagging model, point-wise model
and sequential model, showed that the
former model works competitively well
despite its simplicity, which indicates
the true dependency among supertag
assignments is far more complex than the
crude first-order approximation made in
the sequential model. We then analyzed
the limitation of separated supertagging
by using a CFG-filter. The results showed
that big gains could be acquired by resort-
ing to a light-weight parser.
1 Introduction
Supertagging is an important part of lexicalized
grammar parsing. A high performance supertag-
ger greatly reduces the load of a parser and ac-
celerates its speed. A supertag represents a lin-
guistic word category, which encodes syntactic be-
havior of the word. The concept of supertagging
was first proposed for lexicalized tree adjoining
grammar (LTAG) (Bangalore and Joshi, 1999) and
then extended to other lexicalized grammars, such
as combinatory categorial grammar (CCG) (Clark,
2002) and Head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). Recently, syn-
tactic information in supertags has been exploited
for NLP tasks besides parsing, such as NP chunk-
ing (Shen and Joshi, 2003), semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Hassan et al, 2007). Supertagging serves
there as an implicit and convenient way to incor-
porate rich syntactic information in those tasks.
Improving the performance of supertagging can
thus benefit these two aspects: as a preproces-
sor for deep parsing and as an independent, al-
ternative technique for ?almost? parsing. How-
ever, supertags are derived from a grammar and
thus have a strong connection to parsing. To fur-
ther improve the supertagging accuracy, the rela-
tion between supertagging and parsing is crucial.
With this motivation, we investigate how well a se-
quence labeling model can do when it is separated
from a parser, and to what extent the ignorance of
long distance dependencies in the sequence label-
ing formulation affects the supertagging results.
Specifically, we evaluated two different types
of supertagging model, point-wise model and se-
quential model, for HPSG supertagging. CFG-
filter was then used to empirically evaluate the
effect of long distance dependencies in supertag-
ging. The point-wise model achieved competitive
result of 92.53% accuracy on WSJ-HPSG tree-
bank with fast training speed, while the sequen-
tial model augmented with supertag edge features
did not give much further improvement over the
point-wise model. Big gains acquired by using
CFG-filter indicates that further improvement may
be achieved by resorting to a light-weight parser.
2 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a kind of lexi-
calized grammar. In HPSG, many lexical entries
are used to express word-specific characteristics,
210
while only small amount of rule schemas are used
to describe general constructions. A supertag in
HPSG corresponds to a template of lexical entry.
For example, one possible supertag for ?big? is
?[<ADJP>]N lxm?, which indicates that the syn-
tactic category of ?big? is adjective and it modi-
fies a noun to its right. The number of supertags
is generally much larger than the number of labels
used in other sequence labeling tasks; Comparing
to 45 POS tags used in PennTreebank, the HPSG
grammar used in our experiments includes 2,308
supertags. Because of this, it is often very hard or
even impossible to apply computationary demand-
ing methods to HPSG supertagging.
3 Perceptron and Bayes Point Machine
Perceptron is an efficient online discriminative
training method. We used perceptron with weight-
averaging (Collins, 2002) as the basis of our su-
pertagging model. We also use perceptron-based
Bayes point machine (BPM) (Herbrich et al,
2001) in some of the experiments. In short, a BPM
is an average of a number of averaged perceptrons?
weights. We use average of 10 averaged percep-
trons, each of which is trained on a different ran-
dom permutation of the training data.
3.1 Formulation
Here we follow the definition of Collins? per-
ceptron to learn a mapping from the input space
(w, p) ? W ? P to the supertag space s ? S. We
use function GEN(w,p) to indicate all candidates
given input (w, p). Feature function f maps a train-
ing sample (w, p, s) ?W ?P ?S to a point in the
feature space Rd. To get feature weights ? ? Rd
of feature function, we used the averaged percep-
tron training method described in (Collins, 2002),
and the average of its 10 different runs (i.e., BPM).
For decoding, given an input (w, p) and a vector
of feature weights ?, we want to find an output s
which satisfies:
F (w, p) = argmax
s?GEN(w, p)
? ? f(w, p, s)
For the input (w, p), we treat it in two fash-
ions: one is (w, p) representing a single word
and a POS tag. Another is (w, p) representing
whole word and POS tags sequence. We call them
point-wise model and sequential model respec-
tively. Viterbi algorithm is used for decoding in
sequential model.
template type template
Word wi,wi?1,wi+1,
wi?1&wi, wi&wi+1
POS pi, pi?1, pi?2, pi+1,
pi+2, pi?1&pi, pi?2&pi?1,
pi?1&pi+1, pi&pi+1, pi+1&pi+2
Word-POS pi?1&wi, pi&wi, pi+1&wi
Supertag? si?1 , si?2&si?1
Substructure {ssi,1, ..., ssi,N}?Word
{ssi,1, ..., ssi,N}? POS
{ssi,1, ..., ssi,N}?Word-POS
{ssi?1,1, ..., ssi?1,N}?
{ssi,1, ..., ssi,N}?
Table 1: Feature templates for point-wise model
and sequential model. Templates with ? are only
used by sequential model. ssi,j represents j-th
substructure of supertag at i. For briefness, si is
omitted for each template. ??? means set-product.
e.g., {a,b}?{A,B}={a&A,a&B,b&A,b&B}
3.2 Features
Feature templates are listed in Table 1. To make
the results comparable with previous work, we
adopt the same feature templates as Matsuzaki et.
al. (2007). For sequential model, supertag con-
texts are added to the features. Because of the
large number of supertags, those supertag edge
features could be very sparse. To alleviate this
sparseness, we extracted sub-structures from the
lexical template of each supertag, and use them for
making generalized node/edge features as shown
in Table 1. The sub-structures we used include
subcategorization frames (e.g., subject=NP, ob-
ject=NP PP), direction and category of modifiee
phrase (e.g., mod left=VP), voice and tense of a
verb (e.g., passive past).
3.3 CFG-filter
Long distance dependencies are also encoded in
supertags. For example, when a transitive verb
gets assigned a supertag that specifies it has a PP-
object, in most cases a preposition to its right must
be assigned an argument (not adjunct) supertag,
and vice versa. Such kind of long distance context
information might be important for supertag dis-
ambiguation, but is not easy to incorporate into a
sequence labeling model separated from a parser.
To examine the limitation of supertagging sep-
arated from a parser, we used CFG-filter as an ap-
211
Model Name Acc%
PW-AP 92.29
SEQ-AP 92.53
PW-AP+CFG 93.57
SEQ-AP+CFG 93.68
Table 2: Averaged 10-cross validation of averaged
perceptron on Section 02-21.
proximation of an HPSG parser. We firstly cre-
ated a CFG that approximates the original HPSG
grammar, using the iterative method by Kiefer
and Krieger (2000). Given the supertags as pre-
terminals, the approximating CFG was then used
for finding a maximally scored sequence of su-
pertags which satisfies most of the grammatical
constraints in the original HPSG grammar (Mat-
suzaki et al, 2007). By comparing the supertag-
ging results before and after CFG-filtering, we can
quantify how many errors are caused by ignorance
of the long-range dependencies in supertagger.
4 Experiments and Analysis
We conducted experiments on WSJ-HPSG tree-
bank corpus (Miyao, 2006), which was semi-
automatically converted from the WSJ portion of
PennTreebank. The number of training iterations
was set to 5 for all models. Gold-standard POS
tags are used as input. The performance is evalu-
ated by accuracy1 and speed of supertagging on an
AMD Opteron 2.4GHz server.
Table 2 shows the averaged results of 10-
fold cross-validation of averaged perceptron (AP)
models2 on section 02-21. We can see the dif-
ference between point-wise AP model and se-
quential AP model is small (0.24%). It becomes
even smaller after CFG-filtering (0.11%). Table
3 shows the supertagging accuracy on section 22
based on BPM. Although not statistically signif-
icantly different from previous ME model (Mat-
suzaki et al, 2007), point-wise model (PW-BPM)
achieved competitive result 92.53% with faster
training. In addition, 0.27% and 0.29% gains were
brought by using BPM from PW-AP (92.26%) and
PW-SEQ (92.54%) with P-values less than 0.05.
The improvement by using sequential mod-
els (PW-AP?SEQ-AP: 0.24%, PW-BPM?SEQ-
BPM: 0.3%, statistically significantly different),
1?UNK? supertags are ignored in evaluation as previous.
2For time limitation, cross validation for BPM was not
conducted.
Model Name Acc% Training/
Testing Time ?
ME (Matsuzaki 07?) 92.45 ? 3h / 12s
PW-BPM 92.53 285s / 10s
SEQ-BPM 92.83 1721s / 13s
PW-BPM+SUB 92.68 1275s / 25s
SEQ-BPM+SUB 92.99 9468s / 107s
PW-BPM+CFG 93.60 285s / 78s
SEQ-BPM+CFG 93.70 1721s / 195s
PW-BPM+SUB+CFG 93.72 1275s / 170s
SEQ-BPM+SUB+CFG 93.88 9468s / 1011s
Table 3: Supertagging accuracy and training&
testing speed on section 22. (?) Test time was cal-
culated on totally 1648 sentences.
compared to point-wise models, were not so large,
but the training time was around 6 times longer.
We think the reason is twofold. First, as previous
research showed, POS sequence is very informa-
tive in supertagging (Clark, 2004). A large amount
of local syntactic information can be captured in
POS tags of surrounding words, although a few
long-range dependencies are of course not. Sec-
ond, the number of supertags is large and the su-
pertag edge features used in sequential model are
inevitably suffered from data sparseness. To alle-
viate this, we extracted sub-structure from lexical
templates (i.e., lexical items corresponding to su-
pertags) to augment the supertag edge features, but
only got 0.16% improvement (SEQ-BPM+SUB).
Furthermore, we also got 0.15% gains with P-
value less than 0.05 by incorporating the sub-
structure features into point-wise model (PW-
BPM+SUB). We hence conclude that the contri-
bution of the first-order edge features is not large
in sequence modeling for HPSG supertagging.
As we explained in Section 3.3, sequence label-
ing models have inherent limitation in the ability
to capture long distance dependencies between su-
pertags. This kind of ambiguity could be easier to
solve in a parser. To examine this, we added CFG-
filter which works as an approximation of a full
HPSG parser, after the sequence labeling model.
As expected, there came big gains of 1.26% (from
PW-AP to PW-AP+CFG) and 1.15% (from PW-
BPM to PW-BPM+CFG). Even for the sequen-
tial model we also got 1.15% (from SEQ-AP to
SEQ-AP+CFG) and 0.87% (from SEQ-BPM to
SEQ-BPM+CFG) respectively. All these models
were statistically significantly different from orig-
212
inal ones.
We also gave error analysis on test results.
Comparing SEQ-AP with SEQ-AP+CFG, one of
the most frequent types of ?correct supertag? by
the CFG-filter was for word ?and?, wherein a su-
pertag for NP-coordination (?NP and NP?) was
corrected to one for VP-coordination (?VP and
VP? or ?S and S?). It means the disambiguation
between the two coordination type is difficult for
supertaggers, presumably because they looks very
similar with a limited length of context since the
sequence of the NP-object of left conjunct, ?and?,
the NP subject of right conjunct looks very similar
to a NP coordination. The different assignments
by SEQ-AP+CFG from SEQ-AP include 725 right
corrections, while it changes 298 correct predic-
tions by SEQ-AP to wrong assignments. One pos-
sible reason for some of ?wrong correction? is re-
lated to the approximation of grammar. But this
gives clue that for supertagging task: just using
sequence labeling models is limited, and we can
resort to use some light-weight parser to handle
long distance dependencies.
Although some of the ambiguous supertags
could be left for deep parsing, like multi-tagging
technique (Clark, 2004), we also consider the
tasks where supertags can be used while conduct-
ing deep parsing is too computationally costly. Al-
ternatively, focusing on supertagging, we could
treat it as a sequence labeling task, while a conse-
quent light-weight parser is a disambiguator with
long distance constraint.
5 Conclusions
In this paper, through treating HPSG supertag-
ging in a sequence labeling way, we examined
the relationship between supertagging and parsing
from an angle. In experiment, even for sequential
models, CFG-filter gave much larger improvement
than one gained by switching from a point-wise
model to a sequential model. The accuracy im-
provement given by the CFG-filter suggests that
we could gain further improvement by combining
a supertagger with a light-weight parser.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. The first author was partially sup-
ported by University of Tokyo Fellowship (UT-
Fellowship). This work was partially supported
by Grant-in-Aid for Specially Promoted Research
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25:237?265.
John Chen and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41?48.
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19?24.
Stephen Clark. 2004. The importance of supertagging
for wide-coverage ccg parsing. In Proceedings of
COLING-04, pages 282?288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. pages 1?8.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine transla-
tion. In Proceedings of ACL 2007, pages 288?295.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, 1:245?279.
Bernd Kiefer and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135?146.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient hpsg parsing with supertagging
and cfg-filtering. In Proceedings of IJCAI-07, pages
1671?1676.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Mat-
suzaki, and Yusuke Miyao. 2006. Extremely lex-
icalized models for accurate and fast hpsg parsing.
In Proceedings of EMNLP-2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago /
CSLI.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
213
Encoding Biomedical Resources in TEI: the Case of the GENIA Corpus
Tomaz? Erjavec
Dept. of Intelligent Systems
Joz?ef Stefan Institute, Ljubljana
Yuka Tateisi
CREST
Japan Science and
Technology Corporation
Jin-Dong Kim
Dept. of Information Science
University of Tokyo
Tomoko Ohta
CREST
Japan Science and
Technology Corporation
Jun-ichi Tsujii
CREST JST &
Dept. of Information Science
University of Tokyo
Abstract
It is well known that standardising the
annotation of language resources signifi-
cantly raises their potential, as it enables
re-use and spurs the development of com-
mon technologies. Despite the fact that
increasingly complex linguistic informa-
tion is being added to biomedical texts,
no standard solutions have so far been
proposed for their encoding. This pa-
per describes a standardised XML tagset
(DTD) for annotated biomedical corpora
and other resources, which is based on
the Text Encoding Initiative Guidelines
P4, a general and parameterisable stan-
dard for encoding language resources. We
ground the discussion in the encoding of
the GENIA corpus, which currently con-
tains 2,000 abstracts taken from the MED-
LINE database, and has almost 100,000
hand-annotated terms marked for seman-
tic class from the accompanying ontol-
ogy. The paper introduces GENIA and
TEI and implements a TEI parametrisa-
tion and conversion for the GENIA cor-
pus. A number of aspects of biomedi-
cal language are discussed, such as com-
plex tokenisation, prevalence of contrac-
tions and complex terms, and the linkage
and encoding of ontologies.
1 Introduction
With the growing research on processing texts from
the biomedical domain, the number of resources,
esp. corpora, is increasing rapidly. Such corpora can
be heavily annotated, e.g., with meta-data, words
and part-of-speech tags, named entities, phrases,
terms, concepts, translation equivalents, etc. Cor-
pora are invaluable to the further development of
technologies for utilising the information in biomed-
ical texts, as they provide them with training and
testing data. Given the value of such resources, it
is important to ensure their reusability and increase
their interchange potential ? a step in this direc-
tion is developing common encodings for biomedi-
cal corpora.
Standardisation of resource encoding practices
has now, for some time, been in the forefront of at-
tention. Most of these advances are Web-driven, and
include XML and related recommendations, such as
XSLT, XML Schemas, XPointer, SAX, etc. The
higher level standards, of meta-data (RDF) and on-
tologies (OWL) have been especially influential in
encoding biomedical resources. However, there re-
mains the question how to best encode the structure
of the text themselves, how to mark-up added lin-
guistic analyses, and how to implement linkages be-
tween the text and and further resources, such as lex-
ica, thesauri and ontologies. As discussed in (Ide
and Brew, 2000), in order to qualify as a ?good?
annotated corpus, its encoding should provide for
reusabilty and extensibily.
In this paper we build on previous work (Erjavec
et al, 2003) and show how to develop a standard-
ised encoding for biomedical corpora. We base
our discussion on the case of the GENIA corpus
(Ohta et al, 2002), which is originaly encoded in
GPML, the GENIA Project Markup Language, an
XML DTD. We re-encode the corpus into a stan-
dardised annotation scheme, based on the Text En-
coding Initiative Guidelines P4 (Sperberg-McQueen
and Burnard, 2002), and specify a constructive map-
ping from the original DTD to the developed encod-
ing via a XSLT transformation.
One of the motivations for such an re-encoding
is that TEI is well-designed and widely accepted ar-
chitecture, which has been often used for annotating
language corpora, and by porting to it, GENIA, and
other projects, can gain new insights into possible
encoding practices and maybe make the corpus bet-
ter suited for interchange. As the transformation to
TEI is fully automatic, there is also no need to aban-
don the original markup format (in this case GPML),
which, as it has been crafted specially for the corpus,
provides a tighter encoding than can be possible with
the more general TEI.
The paper thus proposes the creation of a prac-
tical annotation scheme for linguistically annotated
(biomedical) corpora, the conversion to which is
automatic and supports consistency checking and
validation. The paper also serves as a guide to
parametrising TEI and draws attention to certain as-
pects of biomedical corpora which are likely to face
all that wish to process such texts.
The paper is structured as follows: Section 2 in-
troduces the GENIA corpus; Section 3 introduces
the TEI, gives some pros and cons of using it,
and the method of parametrising TEI for particular
projects; Section 4 discusses such a parametrisation
for biomedical corpora and explains the conversion
of the GENIA corpus to TEI; Section 5 discusses
some challenging properties of biomedical text an-
notations; finally, Section 6 offers some conclusions
and directions for further work.
2 The GENIA Corpus
The GENIA corpus (Ohta et al, 2002) is be-
ing developed in the scope of the GENIA project,
which seeks to develop information extraction tech-
niques for scientific texts using NLP technol-
ogy. The corpus consists of semantically anno-
tated published abstracts from the biomedical do-
main. The corpus is a collection of articles ex-
tracted from the on-line MEDLINE abstracts (U.S.
National Center for Biotechnology Information,
http://www.ncbi.nlm.nih.gov/, PubMed database).
Since the focus of the corpus is on biological re-
actions concerning transcription factors in human
blood cells, articles were selected that contain the
MeSH terms human, blood cell and transcription
factor.
As usual for the field, the articles are composed
largely of structurally very complex technical terms,
and are almost incomprehensible to a layperson. A
typical heading e.g., reads IL-2 gene expression and
NF-kappa B activation through CD28 requires reac-
tive oxygen production by 5-lipoxygenase.
The main value of the GENIA corpus comes from
its annotation: all the abstracts and their titles have
been marked-up by two domain experts for bio-
logically meaningful terms, and these terms have
been semantically annotated with descriptors from
the GENIA ontology.
The GENIA ontology is a taxonomy of, currently,
47 biologically relevant nominal categories, such as
body part, virus, or RNA domain or region; the tax-
onomy has 35 terminal categories.
The terms of the corpus are semantically de-
fined as those sentence constituents that can be cate-
gorised using the terminal categories from the ontol-
ogy. Syntactically such constituents are quite varied:
they include qualifiers and can be recursive.
The GENIA corpus is encoded in the Genia
Project Markup Language. The GPML is an XML
DTD (Kim et al, 2001) where each article con-
tains its MEDLINE ID, title and abstract. The texts
of the abstracts are segmented into sentences, and
these contain the constituents with their semantic
classification. The GENIA ontology is provided to-
gether with the GENIA corpus and is encoded in
DAML+OIL (http://www.daml.org/ ), the standard
XML-based ontology description language. This
structure and its annotation will be further discussed
below.
A suite of supporting tools has been developed or
tuned for the GENIA corpus and GPML: the term
annotation is performed with the XMLMind editor;
an XPath-based concordancer has been developed
for searching the corpus; and CSS stylesheets are
available for browsing it.
At the time of writing, the latest version of the
GENIA corpus is 3.01, which has been released
in April 2003. It consists of 2,000 abstracts with
over 400,000 words and more than 90,000 marked-
up terms. This version has not yet been marked-
up with tokens or PoS information, although an
earlier version (Genia-V3.0p) has been. The GE-
NIA corpus is available free of charge from the GE-
NIA project homepage, at http://www-tsujii.is.s.u-
tokyo.ac.jp/GENIA/.
3 The Text Encoding Initiative
The Text Encoding Initiative was established in
1987 as a systematised attempt to develop a fully
general text encoding model and set of encoding
conventions based upon it, suitable for processing
and analysis of any type of text, in any language,
and intended to serve the increasing range of ex-
isting (and potential) applications and uses. The
TEI Guidelines for Electronic Text Encoding and
Interchange were first published in April 1994 in
two substantial green volumes, known as TEI P3.
In May 1999, a revised edition of TEI P3 was
produced, correcting several typographic and other
errors. In December 2000 the TEI Consortium
(http://www.tei-c.org/ ) was set up to maintain and
develop the TEI standard. In 2002, the Consortium
announced the availability of a major revision of TEI
P3, the TEI P4 (Sperberg-McQueen and Burnard,
2002) the object of which is to provide equal sup-
port for XML and SGML applications using the TEI
scheme. The revisions needed to make TEI P4 have
been deliberately restricted to error correction only,
with a view to ensuring that documents conforming
to TEI P3 will not become illegal when processed
with TEI P4. For GENIA, we are using the XML-
compatible version of TEI P4.
In producing P4, many possibilities for other,
more fundamental changes have been identified.
With the establishment of the TEI Council, it be-
came possible to agree on a programme of work to
enhance and modify the Guidelines more fundamen-
tally over the coming years. TEI P5 will be the next
full revision of the Guidelines. The work on P5 has
started, and the date of its appearance will likely be
in 2004 and there are currently several TEI Working
Groups addressing various parts of the Guidelines
that need attention.
More than 80 projects spanning over 30 languages
have so far made use of the TEI guidelines, pro-
ducing diverse resources, e.g., text-critical editions
of classical works. TEI has also been influential
in corpus encoding, where the best known exam-
ple is probably the British National Corpus. How-
ever, while the TEI has been extensively used for
annotating PoS tagged corpora, it been less popu-
lar for encoding texts used by the the Information
Retrieval/Extraction community; here, a number of
other initiatives have taken the lead in encoding, say,
ontologies or inter-document linking.
3.1 Pros and cons of using TEI
Why, if a corpus is already encoded in XML using
a home-grown DTD, to re-encoded it in TEI at all?
One reasons is certainly the validation aspect of the
exercise: re-coding a corpus, or any other resource,
reveals hidden (and in practice incorrect) assump-
tions about its structure. Re-coding to a standard
recommendation also forces the corpus designers to
face issues which might have been overlooked in the
original design.
There are also other advantages of using TEI as
the interchange format: (1) it is a wide-coverage,
well-designed (modular and extensible), widely ac-
cepted and well-maintained architecture; (2) it pro-
vides extensive documentation, which comprises not
only the Guidelines but also papers and documen-
tation (best practices) of various projects; (3) it of-
fers community support via the tei-l public discus-
sion list; (4) various TEI-dedicated software already
exists, and more is likely to become available; and
(5) using it contributes to the adoption of open stan-
dards and recommendations.
However, using a very general recommendation
which tries to cater for any possible situation brings
with it also several disadvantages:
Tag abuse TEI might not have elements / attributes
with the exact meaning we require. This re-
sults in a tendency to misuse tags for purposes
they were not meant for; however, it is a case
of individual judgement to decide whether to
(slightly) abuse a tag, or to implement a lo-
cal extension to add the attribute or element re-
quired.
Tag bloat Being a general purpose recommenda-
tion, TEI can ? almost by definition ? never
be optimal for a specific application. Thus a
custom developed DTD will be leaner, have
less (redundant) tags and simpler content mod-
els.
TEI for humanities While the Guidelines cover a
vast range of text types and annotations, they
are maybe the least developed for ?high level?
NLP applications or have failed to keep abreast
of ?cutting-edge? initiatives. As will be seen,
critical areas are the encoding of ontologies, of
lexical databases and of feature structures.
3.2 Building the TEI DTD
The TEI Guidelines (Sperberg-McQueen and
Burnard, 2002) consist of the formal part, which
is a set of SGML/XML DTD fragments, and the
documentation, which explains the rationale behind
the elements available in these fragments, as well as
giving overall information about the structure of the
TEI.
The formal SGML/XML part of TEI comes as a
set of DTD fragments or tagsets. A TEI DTD for a
particular application is then constructed by select-
ing an appropriate combination of such tagsets. TEI
distinguishes the following types of tagsets:
Core tagset : standard components of the TEI main
DTD in all its forms; these are always included
without any special action by the encoder.
Base tagsets : basic building blocks for specific text
types; exactly one base must be selected by the
encoder, unless one of the combined bases is
used.
Additional tagsets : extra tags useful for particular
purposes. All additional tagsets are compatible
with all bases and with each other; an encoder
may therefore add them to the selected base in
any combination desired.
User defined tagsets : these extra tags give the pos-
sibility of extending and overriding the defi-
nitions provided in the TEI tagset. Further-
more, they give the option of explicitly includ-
<!DOCTYPE teiCorpus.2 SYSTEM
"http://www.tei-c.org/P4X/DTD/tei2.dtd"
[<!ENTITY % TEI.XML "INCLUDE">
<!ENTITY % TEI.prose "INCLUDE">
<!ENTITY % TEI.linking "INCLUDE">
<!ENTITY % TEI.analysis "INCLUDE">
<!ENTITY % TEI.corpus "INCLUDE">
<!ENTITY % TEI.extensions.ent SYSTEM
?geniaex.ent?>
<!ENTITY % TEI.extensions.dtd SYSTEM
?geniaex.dtd?>
]>
Figure 1: The XML TEI prolog for GENIA
ing or ignoring (disallowing) each particular el-
ement licensed by the chosen base and addi-
tional tagsets.
While a project-particular XML DTD can be con-
structed by including and ignoring the TEI DTD
fragments directly (as exemplified in Figure 1), it is
also possible to build ? for easier processing ? a
one-file DTD with the help of the on-line TEI Pizza
Chef service, available from the TEI web site.
4 Parametrising TEI for biomedical
corpora
In previous work (Erjavec et al, 2003) we have al-
ready proposed a TEI parametrisation of GENIA
which was quite broad in its scope. Because a num-
ber of tagsets could prove useful in the long term
this parametrisation collected not only those that we
considered necessary for the current version of GE-
NIA, but also some that might prove of service in the
future. Furthermore, we supported the encoding of
both version 2.1 and 3.0 of the corpus. The resulting
DTD was thus very generous in what kinds of data it
caters for. To focus the discussion we, in the current
paper, only address tagset that are immediately rele-
vant to annotating biomedical texts. In Figure 1 we
define the XML DTD that can be used for encoding
biomedical resources, and that we used for GENIA
V3.01. The XML prolog given in this Figure defines
that ?teiCorpus.2? is the root element of the corpus,
that the external DTD resides at the given URL be-
longing to the TEI Consortium, and that a number
of TEI modules, detailed below, are being used to
parametrise the TEI to arrive at our particular DTD.
4.1 TEI.XML
TEI P4 allows both standard SGML and XML en-
codings. Including the TEI.XML option indicates
that the target DTD is to be expressed in XML.
4.2 TEI.prose
The base tagset does not declare many elements but
rather inherits all of the TEI core, which includes the
TEI header, and text elements. A TEI document will
typically have as its root element ?TEI.2? which is
composed of the ?teiHeader?, followed by the ?text?;
c.f. right hand side of Figure 2, but note that the root
element from the TEI.corpus module is used for the
complete corpus.
The TEI header describes an encoded work so that
the text (corpus) itself, its source, its encoding, and
its revisions are all thoroughly documented.
TEI.prose also contains elements and attributes
for describing text structure, e.g. ?div? for text divi-
sion, ?p? for paragraph, ?head? for text header, etc.
The tagset is therefore useful for encoding the gross
structure of the corpus texts; for an illustration again
see Figure 2.
4.3 TEI.linking
This additional tagset provides mechanisms for link-
ing, segmentation, and alignment. The elements
provided here enable links to be made e.g., between
the articles and their source URLs, or between con-
cepts and their hypernyms.
It should be noted that while the TEI treatment
of external pointers had been very influential, it was
overtaken and made obsolete by newer recommen-
dations. However, the TEI does have a Working
Group on Stand-Off Markup, XLink and XPointer,
which should produce new TEI encoding recom-
mendations for this area in 2003.
4.4 TEI.analysis
This additional tagset is used for associating sim-
ple linguistic analyses and interpretations with text
elements. It can be used to annotate words, ?w?,
clauses, ?cl?, and sentences, ?s? with dedicated tags,
as well as arbitrary and possibly nested segments
with the ?seg?. Such elements can be, via at-
tributes, associated with their analyses. This tagset
has proved very popular for PoS-annotated corpora;
for an illustration see Figure 3.
4.5 TEI.corpus
This additional tagset introduces a new root element,
?teiCorpus.2?, which comprises a (corpus) header
and a series of ?TEI.2? elements. The TEI.corpus
tagset alo extends the certain header elements to
provide more detailed descriptions of the corpus ma-
terial.
4.6 TEI.extensions.ent
The file gives, for each element sanctioned by the
chosen modules, whether we include or ignore it in
our parametrisation. While this is not strictly neces-
sary (without any such specification, all the elements
would be included) we thought it wise to constrain
the content models somewhat, to reduce the bewil-
dering variety of choices that the TEI otherwise of-
fers. Also, such an entity extension file gives the
complete list of all the TEI elements that are allowed
(and disallowed) in GENIA, which might prove use-
ful for documentation purposes.
4.7 TEI.extensions.dtd
This file specifies the changes we have made to TEI
elements. We have e.g., added the url attribute to
?xptr? and ?xref ? and tagging attributes to word and
punctuation elements.
4.8 Conversion of GPML to TEI
Because the source format of GENIA will remain
the simpler GPML, it is imperative to have an au-
tomatic procedure for converting to the TEI inter-
change format. The translation process takes advan-
tage of the fact that both the input and output are
encoded in XML, which makes it possible to use the
XSL Transformation Language, XSLT that defines a
standard declarative specification of transformations
between XML documents. There also exist a num-
ber of free XSLT processors; we used Daniel Veil-
lard?s xsltproc.
The transformation is written as a XSLT
stylesheet, which makes reference to two docu-
ments: the GENIA ontology in TEI and the template
for the corpus header. The stylesheet then resolves
the GPML encoded corpus into TEI. The translation
of the corpus is thus fully automatic, except for the
taxonomy, which was translated by hand.
Figure 2 illustrates the top level structure of the
corpus, and how it differs between the GPML and
TEI encodings. The most noticeable difference is,
apart from the renaming of elements, the addition
of headers to the corpus and texts. In the GENIA
?teiHeader? we give e.g., the name, address, avail-
ability, sampling description, and, for each abstract?s
?sourceDesc?, two ?xptr?s: the first gives the URL of
the HTML article in the MEDLINE database, while
the second is the URL of the article in the origi-
nal XML. It should be noted that we use a locally
defined url attribute for specifying the value of the
pointer.
5 Characteristics of biomedical texts
In this section we review some challenges that
biomedical texts present to the processing and en-
coding of linguistic information, and the manner of
their encoding in our DTD.
5.1 Tokens
Tokenisation, i.e., the identification of words and
punctuation marks, is the lowest level of linguistic
analysis, yet is, in spite (or because) of this of con-
siderable importance. As all other levels of linguis-
tic markup make direct or direct reference to the to-
ken stream of the text, so if this is incorrect, errors
will propagate to all other annotations.
It is also interesting to note that current annota-
tion practice is more and more leaning toward stand-
off markup, i.e., annotations that are separated from
the primary data (text) and make reference to it only
via pointers. However, it is beneficial to have some
markup in the primary data to which it is possible to
refer, and this markup is, almost exclusivelly, that of
tokens; see e.g., (Freese et al, 2003).
Version V1.1 of GENIA has been also annotated
with LTG tools (Grover et al, 2002). In short, the
corpus is tokenised, and then part-of-speech tagged
with two taggers, each one using a different tagset,
and the nouns and verbs lemmatised. Additionally,
the deverbal nominalisations are assigned their ver-
bal stems.
The conversion to TEI is also able to handle this
additional markup, by using the TEI.analysis mod-
ule. The word and punctuation tokens are encoded
as ?w? and ?c? elements respectively, which are fur-
ther marked with type and lemma and the locally de-
fined c1, c2 and vstem. An example of such markup
<s>
<w c1="DT" c2="DB">All</w>
<c type="HYPH" c1=":" c2="-">-</c>
<w c1="VBZ" c2="JJ">trans</w>
<w c1="JJ" c2="JJ">retinoic</w>
<w lemma="acid" c1="NN" c2="NN1">acid</w>
<c type="BR" c1="(" c2="(">(</c>
<w lemma="Ra" c1="NN" c2="NP1">RA</w>
<c type="BR" c1=")" c2=")">)</c>
<w lemma="be" c1="VBZ" c2="VBZ">is</w>
<w c1="DT" c2="AT1">an</w>
<w c1="JJ" c2="JJ">important</w>
...
Figure 3: TEI encoding of annotated tokens
is given in Figure 3.
Given the high density of technical terms,
biomedical texts are rife with various types of con-
tractions, such as abbreviations, acronyms, prefixes,
etc. As seen already in Figure 3, one of the
more problematic apects of tokenisaton are paren-
theses. Almost all tokenisers (e.g., the LT one, or
the UPENN tokeniser) take these as separate tokens,
but many are in biomedical texts parts of terms. So,
out of almost 35,000 distinct terms that have been
marked up in the GENIA corpus, over 1,700 con-
tain parentheses. Some examples: (+)-pentazocine,
(3H)-E2 binding, (gamma(c))-like molecule.
Correct tokenisation of the biomedical texts is
thus a challenging tasks, and it is fair to say that,
from a linguistic processing perspective, complex
tokenisation is one of the defining characteristics of
such corpora.
5.2 Terms
Annotation of terms is a prerequisite for meaningful
processing of biomedical texts, yet it is often diffi-
cult to decide what constitutes a term in a text, and
how to abstract away from local variations. Biomed-
ical texts are largerly (one could almost say excu-
sivelly) composed of terms, and, as mentioned, this
brings with it complex abbreviatory mechanisms.
Even though TEI offers a ?term? element, we
chose, in line with the original GPML encoding, to
rather use the TEI.analysis clause (?cl?) element to
encode terms. In GENIA, the terms have been hand-
annotated, and marked up with concepts from the
GENIA ontology; this was also the defining factor
of term-hood, namely that the term could be linked
<!DOCTYPE set SYSTEM "gpml.dtd"> <!DOCTYPE teiCorpus.2 SYSTEM "genia-tei.dtd">
<set> <TEIcorpus.2>
<article> <teiHeader type="corpus">
<articleinfo><bibliomisc> *Corpus_header*</teiHeader>
*MEDLINE_ID* <TEI.2 id="*MEDLINE_ID*">
</bibliomisc></articleinfo> <teiHeader type="text">
<title> *Article_header*</teiHeader>
*Title_of_article* <text><body>
</title> <div type="abstract">
<abstract> <head>*Title_of_article*</head>
*Abstract_of_article* <p>*Abstract_of_article*</p>
</abstract> </div>
</article> </body></text></TEI.2>
*More_articles* *More_articles*
</set> </TEIcorpus.2>
Figure 2: The GPML and TEI structure of the corpus
to a terminal concept of the GENIA ontology.
In spite of the simple semantic definition, the syn-
tactic structure of the terms in the corpus varies
dramatically. Biomedical terms are in some ways
similar to named entities (names of people, orga-
nizations, etc.) but from the linguistic perspective,
they are different in that named entities are mostly
proper nouns, while terms mostly contain common
nouns, and the two differ in their syntactic proper-
ties. Terms in the corpus can also be nested, where
complex terms are composed out of simpler ones,
e.g., ?cl??cl?IL-2 gene?/cl? transcription?/cl?.
This nesting, and the reference to ontology con-
cepts is often far from simple, as (partial) terms can
appear in coordinated clauses involving ellipsis. For
example, ?CD2 and CD 25 receptors? refers to two
terms, CD2 receptors and CD25 receptors, but only
the latter actually appears in the text.
In such cases by parsing the coordination
all the terms can be identified and annotated;
the TEI encoding achieves this by specifyng
the propositional formula involving the par-
ticipating concepts in the function attribute;
for example, ?cl function=?(AND G.tissue
G.tissue)? ana=?G.tissue???cl?normal?/cl? and
?cl?hypopigmented?/cl? ?cl?skin samples?/cl??/cl?.
The ana attribute encodes the IDREF of the con-
cept; currently, only same valued concepts are either
conjoined or disjoined.
The number of ?cl? elements in the GENIA cor-
pus is 96,582, among which 89,682 are simple terms
and 1,583 are nested terms that are contain 3,431
terms. 5,137 terms do not yet have the ana attribute
for concept identification, so the total number of
ontology-linked terms is 93,293.
5.3 Ontologies
One of the more interesting questions in recoding
GENIA in TEI was how to encode the ontology. The
ontology is in GENIA GPML encoded in a separate
document, conforming to the OIL+DAML specifi-
cation. This, inter alia, means that that XML file
heavily relies on XML Namespaces and the RDF
recommendation. An illustrative fragment is given
on the left side of Figure 4.
Currently the GENIA ontology has a simple tree-
like structure, i.e., it corresponds to a taxonomy,
so we translated it to the TEI ?taxonomy? element,
which is contained in the ?classDecl? of the header
?encodingDesc?. The TEI defines this element
as ?[the classification declaration] contains one or
more taxonomies defining any classificatory codes
used elsewhere in the text?, i.e., is exactly suited for
our purposes.
There are quite substantial differences between
the two encodings: the DAML+OIL models class
inclusion with links, while the TEI does it as XML
element inclusion. This is certainly the simpler and
more robust solution, but requires that the ontol-
ogy is a taxonomy, i.e., tree structured. The sec-
ond difference is in the status of the identifiers: in
DAML+OIL they are general #CDATA links, which
need a separate (XLink/XPointer) mechanisms for
their resolution. In TEI they are XML ID attributes,
<daml:Class rdf:ID="source"></daml:Class> <taxonomy id="G.taxonomy">
<daml:Class rdf:ID="natural"> <category id="G.source">
<rdfs:subClassOf rdf:resource="#source"/> <catDesc>biological source</catDesc>
</daml:Class> <category id="G.natural">
<daml:Class rdf:ID="organism"> <catDesc>natural</catDesc>
<rdfs:subClassOf rdf:resource="#natural"/> <category id="G.organism">
</daml:Class> <catDesc>organism</catDesc>
<daml:Class rdf:ID="multi_cell"> <category id="G.multi_cell">
<rdfs:subClassOf rdf:resource="#organism"/> <catDesc>multi-cellular</catDesc>
</daml:Class> </category>
... ...
Figure 4: The GENIA DAML+OIL and TEI ontology
and can rely on the XML parser to resolve them.
While this is a simpler solution, it does support
document-internal reference only.
6 Conclusions
The paper proposed an XML paramterisation of TEI
P4 developed for linguistically annotated biomedi-
cal corpora, and applied it to the GENIA corpus.
The conversion from the Genia Project Markup Lan-
guage to this encoding has been implemented in
XSLT and both the TEI-conformant parametrisation
(TEI extension file and one-file DTD) and the XSLT
stylesheets are, together with a report documenting
them, available at http://nl.ijs.si/et/genia/, while the
GENIA corpus is freely available from http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/.
The paper gave a survey of the TEI modules that
can be useful for encoding a wide variety of linguis-
tically annotated corpora. This contribution, it is
hoped, can thus serve as a blueprint for parametris-
ing TEI for diverse corpus resources.
Further work involves the inclusion of other
knowledge sources into the corpus, say of Medi-
cal Subject Headings (MeSH), Unified Medical Lan-
guage System (UMLS), International Classification
of Disease (ICD), etc. The place of these annota-
tions in the corpus will have to be considered, and
their linking to the existing information determined.
References
Tomaz? Erjavec, Jin-Dong Kim, Tomoko Ohta, Yuka
Tateisi, and Jun ichi Tsujii. 2003. Stretching the TEI:
Converting the GENIA corpus. In Proceedings of the
EACL-03 Workshop on Linguistically Interpreted Cor-
pora (LINC-03), pages 117?124, Budapest. ACL.
Marion Freese, Ulrich Heid, and Martin Emele. 2003.
Enhancing XCES to XCOMFORT: An Extensible
Modular Architecture for Manipulation of Text Re-
sources. In Proceedings of the EACL-03 Workshop
on Language Technology and the Semantic Web: 3rd
Workshop on NLP and XML (NLPXML-2003), pages
33?40, Budapest. ACL.
Claire Grover, Ewan Klein, Alex Lascarides, and Maria
Lapata. 2002. XML-based NLP Tools for Analysing
and Annotating Medical Language. In 2nd Workshop
on NLP and XML (CoLing Workshop NLPXML-2002).
http://www.ltg.ed.ac.uk/software/ttt/.
Nancy Ide and Chris Brew. 2000. Requrements, Tools
and Architectures for Annotated Corpora. In Proceed-
ings of Data Architectures and Software Support for
Large Corpora, pages 1?5, Budapest. ELRA.
Jin-Dong Kim, Tomoko Ohta, and Jun-ichi Tsujii. 2001.
XML-based Linguistic Annotation of Corpus. In Pro-
ceedings of the first NLP and XML Workshop, pages
44?53.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA Corpus: an Annotated Research Abstract
Corpus in Molecular Biology Domain. In Proceedings
of the Human Language Technology Conference, page
To appear.
C. M. Sperberg-McQueen and Lou Burnard, editors.
2002. Guidelines for Electronic Text Encoding and
Interchange, The XML Version of the TEI Guidelines.
The TEI Consortium. http://www.tei-c.org/.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707?714,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Translating HPSG-style Outputs of a Robust Parser
into Typed Dynamic Logic
Manabu Sato? Daisuke Bekki? Yusuke Miyao? Jun?ichi Tsujii??
? Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? Center for Evolutionary Cognitive Sciences, University of Tokyo
Komaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan
?School of Informatics, University of Manchester
PO Box 88, Sackville St, Manchester M60 1QD, UK
?SORST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
? {sa-ma, yusuke, tsujii}@is.s.u-tokyo.ac.jp
? bekki@ecs.c.u-tokyo.ac.jp
Abstract
The present paper proposes a method
by which to translate outputs of a ro-
bust HPSG parser into semantic rep-
resentations of Typed Dynamic Logic
(TDL), a dynamic plural semantics de-
fined in typed lambda calculus. With
its higher-order representations of con-
texts, TDL analyzes and describes
the inherently inter-sentential nature of
quantification and anaphora in a strictly
lexicalized and compositional manner.
The present study shows that the pro-
posed translation method successfully
combines robustness and descriptive ad-
equacy of contemporary semantics. The
present implementation achieves high
coverage, approximately 90%, for the
real text of the Penn Treebank corpus.
1 Introduction
Robust parsing technology is one result of the
recent fusion between symbolic and statistical
approaches in natural language processing and
has been applied to tasks such as information
extraction, information retrieval and machine
translation (Hockenmaier and Steedman, 2002;
Miyao et al, 2005). However, reflecting the
field boundary and unestablished interfaces be-
tween syntax and semantics in formal theory
of grammar, this fusion has achieved less in
semantics than in syntax.
For example, a system that translates the
output of a robust CCG parser into seman-
tic representations has been developed (Bos et
al., 2004). While its corpus-oriented parser at-
tained high coverage with respect to real text,
the expressive power of the resulting semantic
representations is confined to first-order predi-
cate logic.
The more elaborate tasks tied to discourse
information and plurality, such as resolution
of anaphora antecedent, scope ambiguity, pre-
supposition, topic and focus, are required to
refer to ?deeper? semantic structures, such as
dynamic semantics (Groenendijk and Stokhof,
1991).
However, most dynamic semantic theories
are not equipped with large-scale syntax that
covers more than a small fragment of target
languages. One of a few exceptions is Min-
imal Recursion Semantics (MRS) (Copestake
et al, 1999), which is compatible with large-
scale HPSG syntax (Pollard and Sag, 1994)
and has affinities with UDRS (Reyle, 1993).
For real text, however, its implementation, as
in the case of the ERG parser (Copestake
and Flickinger, 2000), restricts its target to the
static fragment of MRS and yet has a lower
coverage than corpus-oriented parsers (Baldwin,
to appear).
The lack of transparency between syntax and
discourse semantics appears to have created a
tension between the robustness of syntax and
the descriptive adequacy of semantics.
In the present paper, we will introduce
a robust method to obtain dynamic seman-
tic representations based on Typed Dynamic
Logic (TDL) (Bekki, 2000) from real text
by translating the outputs of a robust HPSG
parser (Miyao et al, 2005). Typed Dy-
namic Logic is a dynamic plural seman-
tics that formalizes the structure underlying
the semantic interactions between quantifica-
tion, plurality, bound variable/E-type anaphora
707
re?????e7?t xi1 ? ? ?xin ? ?G(i7?e)7?t .? gi7?e.g ? G? r
?
gx1, . . . ,gxm
?
? ? prop ? ?G(i7?e)7?t .? gi7?e.g ? G???hi7?e.h ? ?G?
?
? prop
...? prop
?
? ? ?G(i7?e)7?t .(? ? ? ?(?G))
re f
?
xi
?
[? prop] [? prop] ? ?G(i7?e)7?t .
?
?
?
i f G
?
x = ?G
?
x
then ? gi 7?e.g ? ?G? G?x = ?G
?
x
otherwise unde f ined
?
?
?
?
??
where prop ? ((i 7? e) 7? t) 7? (i 7? e) 7? t
g? ? G? 7?t ? Gg
G(i7?e) 7?t
.
xi ? ? de.?gi7?e.g ? G?gx = d
?
??
Figure 1: Propositions of TDL (Bekki, 2005)
and presuppositions. All of this complex
discourse/plurality-related information is encap-
sulated within higher-order structures in TDL,
and the analysis remains strictly lexical and
compositional, which makes its interface with
syntax transparent and straightforward. This is
a significant advantage for achieving robustness
in natural language processing.
2 Background
2.1 Typed Dynamic Logic
Figure 1 shows a number of propositions de-
fined in (Bekki, 2005), including atomic pred-
icate, negation, conjunction, and anaphoric ex-
pression. Typed Dynamic Logic is described in
typed lambda calculus (G?del?s System T) with
four ground types: e(entity), i(index), n(natural
number), and t(truth). While assignment func-
tions in static logic are functions in meta-
language from type e variables (in the case of
first-order logic) to objects in the domain De,
assignment functions in TDL are functions in
object-language from indices to entities. Typed
Dynamic Logic defines the notion context as
a set of assignment functions (an object of
type (i 7? e) 7? t) and a proposition as a func-
tion from context to context (an object of type
((i 7? e) 7? t) 7? (i 7? e) 7? t). The conjunctions
of two propositions are then defined as com-
posite functions thereof. This setting conforms
to the view of ?propositions as information
flow?, which is widely accepted in dynamic
semantics.
Since all of these higher-order notions are
described in lambda terms, the path for compo-
sitional type-theoretic semantics based on func-
tional application, functional composition and
type raising is clarified. The derivations of
TDL semantic representations for the sentences
?A boy ran. He tumbled.? are exemplified in
Figure 2 and Figure 3. With some instantia-
tion of variables, the semantic representations
of these two sentences are simply conjoined
and yield a single representation, as shown in
(1).
?
????
boy0x1s1
run0e1s1
agent 0e1x1
re f (x2) [ ]
?
tumble0e2s2
agent 0e2x2
?
?
????(1)
The propositions boy0x1s1, run
0e1s1 and
agent 0e1x1 roughly mean ?the entity referred
to by x1 is a boy in the situation s1?, ?the
event referred to by e1 is a running event in
the situation s1?, and ?the agent of event e1
is x1?, respectively.
The former part of (1) that corresponds to
the first sentence, filtering and testing the input
context, returns the updated context schema-
tized in (2). The updated context is then
passed to the latter part, which corresponds to
the second sentence as its input.
? ? ? x1 s1 e1 ? ? ?
john situation1 running1
john situation2 running2
...
...
...
(2)
This mechanism makes anaphoric expressions,
such as ?He? in ?He tumbles?, accessible to its
preceding context; namely, the descriptions of
their presuppositions can refer to the preceding
context compositionally. Moreover, the refer-
ents of the anaphoric expressions are correctly
calculated as a result of previous filtering and
testing.
708
?a?
? ni7?i7?p7?p.?wi 7?i7?i7?p7?p.
? ei.? si.? ? p.nx1s
?
wx1es?
?
?boy?
? xi.? si.? ? p.
?
boy0xs?
?
?wi7?i7?i7?p7?p.? ei.? si.? ? p.
?
boy0x1s
wx1es?
?
?ran?
? sb j(i7?i 7?i7?p7?p)7?i 7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
run0es
agent 0ex?
#!
? ei.? si.? ? p.
?
??
boy0x1s1
run0es
agent 0ex1?
?
??
Figure 2: Derivation of a TDL semantic representation of ?A boy ran?.
?he?
?wi7?i7?i7?p7?p.
? ei.? si.? ? p.re f ?x2
?
[ ]
?
wx2es?
?
?tumbled?
? sb j(i7?i7?i7?p7?p)7?i7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
tumble0es
agent 0ex?
#!
? ei.? si.? ? p.re f ?x2
?
[ ]
?
tumble0e2s2
agent 0e2x2
?
Figure 3: Derivation of TDL semantic representation of ?He tumbled?.
Although the antecedent for x2 is not de-
termined in this structure, the possible candi-
dates can be enumerated: x1, s1 and e1, which
precede x2. Since TDL seamlessly represents
linguistic notions such as ?entity?, ?event? and
?situation?, by indices, the anaphoric expres-
sions, such as ?the event? and ?that case?, can
be treated in the same manner.
2.2 Head-driven Phrase Structure
Grammar
Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) is a kind of lexicalized gram-
mar that consists of lexical items and a small
number of composition rules called schema.
Schemata and lexical items are all described
in typed feature structures and the unification
operation defined thereon.
?
?????
PHON ?boy?
SYN
SEM
?
??????
HEAD
?
noun
MOD h i
?
VAL
"
SUBJ h i
COMPS h i
SPR hdeti
#
SLASH h i
?
??????
?
?????
(3)
Figure 4 is an example of a parse tree,
where the feature structures marked with the
same boxed numbers have a shared struc-
ture. In the first stage of the derivation of
this tree, lexical items are assigned to each
of the strings, ?John? and ?runs.? Next, the
mother node, which dominates the two items,
?
??
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
??
?
??
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
?? : 2
?
???
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
???
John runs
Figure 4: An HPSG parse tree
is generated by the application of Subject-Head
Schema. The recursive application of these op-
erations derives the entire tree.
3 Method
In this section, we present a method to de-
rive TDL semantic representations from HPSG
parse trees, adopting, in part, a previous
method (Bos et al, 2004). Basically, we first
assign TDL representations to lexical items that
are terminal nodes of a parse tree, and then
compose the TDL representation for the en-
tire tree according to the tree structure (Figure
5). One problematic aspect of this approach is
that the composition process of TDL semantic
representations and that of HPSG parse trees
are not identical. For example, in the HPSG
709
?
?
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
?
Subject-Head Schema
* ? e.? s.? ? .
re f (x1) [John0x1s1]
"
run0es
agent 0ex1?
#
?run
_empty_
+
Composition Rules
normal composition
word formation
nonlocal application
unary derivation
?
?
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
? : 2
?
??
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
??
Assignment Rules
? ?w.? e.? s.? ? .
re f (x1) [John0x1s1] [wx1es? ]
?John
_empty_
?* ? sb j.sb j?
? x.? e.? s.? ? .
"
run0es
agent 0ex?
#!
?run
_empty_
+
John runs John runs
Figure 5: Example of the application of the rules
parser, a compound noun is regarded as two
distinct words, whereas in TDL, a compound
noun is regarded as one word. Long-distance
dependency is also treated differently in the
two systems. Furthermore, TDL has an opera-
tion called unary derivation to deal with empty
categories, whereas the HPSG parser does not
have such an operation.
In order to overcome these differences and
realize a straightforward composition of TDL
representations according to the HPSG parse
tree, we defined two extended composition
rules, word formation rule and non-local
application rule, and redefined TDL unary
derivation rules for the use in the HPSG
parser. At each step of the composition, one
composition rule is chosen from the set of
rules, based on the information of the schemata
applied to the HPSG tree and TDL represen-
tations of the constituents. In addition, we de-
fined extended TDL semantic representations,
referred to as TDL Extended Structures (TD-
LESs), to be paired with the extended compo-
sition rules.
In summary, the proposed method is com-
prised of TDLESs, assignment rules, composi-
tion rules, and unary derivation rules, as will
be elucidated in subsequent sections.
3.1 Data Structure
A TDLES is a tuple hT, p,ni, where T is an
extended TDL term, which can be either a
TDL term or a special value ? . Here, ?
is a value used by the word formation rule,
which indicates that the word is a word modi-
fier (See Section 3.3). In addition, p and n are
the necessary information for extended compo-
sition rules, where p is a matrix predicate in T
and is used by the word formation rule, and
n is a nonlocal argument, which takes either
a variable occurring in T or an empty value.
This element corresponds to the SLASH fea-
ture in HPSG and is used by the nonlocal
application rule.
The TDLES of the common noun ?boy? is
given in (4). The contents of the structure
are T , p and n, beginning at the top. In
(4), T corresponds to the TDL term of ?boy?
in Figure 2, p is the predicate boy, which is
identical to a predicate in the TDL term (the
identity relation between the two is indicated
by ???). If either T or p is changed, the other
will be changed accordingly. This mechanism
is a part of the word formation rule, which
offers advantages in creating a new predicate
from multiple words. Finally, n is an empty
value.
* ? x.? s.? ? .
?
?boy0xs?
?
?boy
_empty_
+
(4)
3.2 Assignment Rules
We define assignment rules to associate HPSG
lexical items with corresponding TDLESs. For
closed class words, such as ?a?, ?the? or
?not?, assignment rules are given in the form
of a template for each word as exemplified
below.
"
PHON ?a?
HEAD det
SPEC hnouni
#
?
* ? x.? s.? ? .
? ? n.?w.? e.? s.? ? .
nx1s
?
wx1es?
?
?
_empty_
_empty_
+(5)
710
Shown in (5) is an assignment rule for the
indefinite determiner ?a?. The upper half of
(5) shows a template of an HPSG lexical item
that specifies its phonetic form as ?a?, where
POS is a determiner and specifies a noun. A
TDLES is shown in the lower half of the fig-
ure. The TDL term slot of this structure is
identical to that of ?a? in Figure 2, while slots
for the matrix predicate and nonlocal argument
are empty.
For open class words, such as nouns, verbs,
adjectives, adverbs and others, assignment rules
are defined for each syntactic category.
?
?????
PHON P
HEAD noun
MOD hi
SUBJ hi
COMPS hi
SPR hdeti
?
?????
?
* ? x.? s.? ? .
?
?P0xs?
?
?P
_empty_
+
(6)
The assignment rule (6) is for common nouns.
The HPSG lexical item in the upper half of (6)
specifies that the phonetic form of this item is
a variable, P, that takes no arguments, does
not modify other words and takes a specifier.
Here, POS is a noun. In the TDLES assigned
to this item, an actual input word will be sub-
stituted for the variable P, from which the ma-
trix predicate P0 is produced. Note that we can
obtain the TDLES (4) by applying the rule of
(6) to the HPSG lexical item of (3).
As for verbs, a base TDL semantic represen-
tation is first assigned to a verb root, and the
representation is then modified by lexical rules
to reflect an inflected form of the verb. This
process corresponds to HPSG lexical rules for
verbs. Details are not presented herein due to
space limitations.
3.3 Composition Rules
We define three composition rules: the func-
tion application rule, the word formation
rule, and the nonlocal application rule.
Hereinafter, let SL = hTL, pL,nLi and SR =
hTR, pR,nRi be TDLESs of the left and the
right daughter nodes, respectively. In addition,
let SM be TDLESs of the mother node.
Function application rule: The composition
of TDL terms in the TDLESs is performed by
function application, in the same manner as in
the original TDL, as explained in Section 2.1.
Definition 3.1 (function application rule). If
Type
?
TL
?
= ? and Type?TR
?
= ? 7? ? then
SM =
* TRTL
pR
union
?
nL,nR
?
+
Else if Type
?
TL
?
= ? 7? ? and Type?TR
?
= ? then
SM =
* TLTR
pL
union
?
nL,nR
?
+
In Definition 3.1, Type(T ) is a function
that returns the type of TDL term T , and
union(nL,nR) is defined as:
union
?
nL,nR
?
=?
??
??
empty i f nL = nR = _empty_
n i f nL = n, nR = _empty_
n i f nL = _empty_, nR = n
unde f ined i f nL 6= _empty_, nR 6= _empty_
This function corresponds to the behavior of
the union of SLASH in HPSG. The composi-
tion in the right-hand side of Figure 5 is an
example of the application of this rule.
Word formation rule: In natural language,
it is often the case that a new word is cre-
ated by combining multiple words, for exam-
ple, ?orange juice?. This phenomenon is called
word formation. Typed Dynamic Logic and
the HPSG parser handle this phenomenon in
different ways. Typed Dynamic Logic does
not have any rule for word formation and re-
gards ?orange juice? as a single word, whereas
most parsers treat ?orange juice? as the sepa-
rate words ?orange? and ?juice?. This requires
a special composition rule for word formation
to be defined. Among the constituent words of
a compound word, we consider those that are
not HPSG heads as word modifiers and define
their value for T as ? . In addition, we apply
the word formation rule defined below.
Definition 3.2 (word formation rule). If
Type
?
TL
?
= ? then
SM =
* TR
concat
?
pL, pR
?
nR
+
Else if Type
?
TR
?
= ? then
SM =
* TL
concat
?
pL, pR
?
nL
+
711
concat (pL, pR) in Definition 3.2 is a func-
tion that returns a concatenation of pL and pR.
For example, the composition of a word mod-
ifier ?orange? (7) and and a common noun
?juice? (8) will generate the TDLES (9).
? ?
orange
_empty_
?
(7)
* ? x.? s.? ? .
?
? juice0xs?
?
? juice
_empty_
+
(8)
* ? x.? s.? ? .
?
?orange_ juice0xs?
?
?orange_ juice
_empty_
+
(9)
Nonlocal application rule: Typed Dynamic
Logic and HPSG also handle the phenomenon
of wh-movement differently. In HPSG, a wh-
phrase is treated as a value of SLASH, and
the value is kept until the Filler-Head Schema
are applied. In TDL, however, wh-movement
is handled by the functional composition rule.
In order to resolve the difference between
these two approaches, we define the nonlocal
application rule, a special rule that introduces
a slot relating to HPSG SLASH to TDLESs.
This slot becomes the third element of TD-
LESs. This rule is applied when the Filler-
Head Schema are applied in HPSG parse trees.
Definition 3.3 (nonlocal application rule).
If Type
?
TL
?
= (? 7? ? ) 7? ? , Type?TR
?
= ? ,
Type
?
nR
?
= ? and the Filler-Head Schema are applied
in HPSG, then
SM =
*
TL
?? nR.TR
?
pL
_empty_
+
3.4 Unary Derivation Rules
In TDL, type-shifting of a word or a phrase is
performed by composition with an empty cat-
egory (a category that has no phonetic form,
but has syntactic/semantic functions). For ex-
ample, the phrase ?this year? is a noun phrase
at the first stage and can be changed into a
verb modifier when combined with an empty
category. Since many of the type-shifting rules
are not available in HPSG, we defined unary
derivation rules in order to provide an equiva-
lent function to the type-shifting rules of TDL.
These unary rules are applied independently
with HPSG parse trees. (10) and (11) illus-
trate the unary derivation of ?this year?. (11)
Table 1: Number of implemented rules
assignment rules
HPSG-TDL template 51
for closed words 16
for open words 35
verb lexical rules 27
composition rules
binary composition rules 3
function application rule
word formation rule
nonlocal application rule
unary derivation rules 12
is derived from (10) using a unary derivation
rule.
? ?w.? e.? s.? ? .re f ?x1
??
?year0x1s1
??
wx1es?
?
?year
_empty_
?(10)
* ? v.? e.? s.? ? .
re f
?
x1
??
?year0x1s1
??
ves
?
mod 0ex1?
??
?year
_empty_
+
(11)
4 Experiment
The number of rules we have implemented is
shown in Table 1. We used the Penn Treebank
(Marcus, 1994) Section 22 (1,527 sentences) to
develop and evaluate the proposed method and
Section 23 (2,144 sentences) as the final test
set.
We measured the coverage of the construc-
tion of TDL semantic representations, in the
manner described in a previous study (Bos
et al, 2004). Although the best method for
strictly evaluating the proposed method is to
measure the agreement between the obtained
semantic representations and the intuitions of
the speaker/writer of the texts, this type of
evaluation could not be performed because of
insufficient resources. Instead, we measured
the rate of successful derivations as an indica-
tor of the coverage of the proposed system.
The sentences in the test set were parsed by
a robust HPSG parser (Miyao et al, 2005),
and HPSG parse trees were successfully gen-
erated for 2,122 (98.9%) sentences. The pro-
posed method was then applied to these parse
trees. Table 2 shows that 88.3% of the un-
712
Table 2: Coverage with respect to the test set
covered sentences 88.3 %
uncovered sentences 11.7 %
assignment failures 6.2 %
composition failures 5.5 %
word coverage 99.6 %
Table 3: Error analysis: the development set
# assignment failures 103
# unimplemented words 61
# TDL unsupporting words 17
# nonlinguistic HPSG lexical items 25
# composition failures 72
# unsupported compositions 20
# invalid assignments 36
# nonlinguistic parse trees 16
seen sentences are assigned TDL semantic rep-
resentations. Although this number is slightly
less than 92.3%, as reported by Bos et al,
(2004), it seems reasonable to say that the pro-
posed method attained a relatively high cover-
age, given the expressive power of TDL.
The construction of TDL semantic represen-
tations failed for 11.7% of the sentences. We
classified the causes of the failure into two
types. One of which is application failure of
the assignment rules (assignment failure); that
is, no assignment rules are applied to a num-
ber of HPSG lexical items, and so no TD-
LESs are assigned to these items. The other
is application failure of the composition rules
(composition failure). In this case, a type mis-
match occurred in the composition, and so a
TDLES was not derived.
Table 3 shows further classification of the
causes categorized into the two classes. We
manually investigated all of the failures in the
development set.
Assignment failures are caused by three fac-
tors. Most assignment failures occurred due to
the limitation in the number of the assignment
rules (as indicated by ?unimplemented words?
in the table). In this experiment, we did not
implement rules for infrequent HPSG lexical
items. We believe that this type of failure
will be resolved by increasing the number of
ref($1)[]
[lecture($2,$3) &
past($3) &
agent($2,$1) &
content($2,$4) &
ref($5)[]
[every($6)[ball($6,$4)]
[see($7,$4) &
present($4) &
agent($7,$5) &
theme($7,$6) &
tremendously($7,$4) &
ref($8)[]
[ref($9)[groove($9,$10)]
[be($11,$4) &
present($4) &
agent($11,$8) &
in($11,$9) &
when($11,$7)]]]]]
Figure 6: Output for the sentence: ?When
you?re in the groove, you see every ball
tremendously,? he lectured.
assignment rules. The second factor in the
table, ?TDL unsupported words?, refers to ex-
pressions that are not covered by the current
theory of TDL. In order to resolve this type of
failure, the development of TDL is required.
The third factor, ?nonlinguistic HPSG lexical
items? includes a small number of cases in
which TDLESs are not assigned to the words
that are categorized as nonlinguistic syntactic
categories by the HPSG parser. This problem
is caused by ill-formed outputs of the parser.
The composition failures can be further clas-
sified into three classes according to their
causative factors. The first factor is the ex-
istence of HPSG schemata for which we have
not yet implemented composition rules. These
failures will be fixed by extending of the def-
inition of our composition rules. The sec-
ond factor is type mismatches due to the un-
intended assignments of TDLESs to lexical
items. We need to further elaborate the as-
signment rules in order to deal with this prob-
lem. The third factor is parse trees that are
linguistically invalid.
The error analysis given above indicates that
we can further increase the coverage through
the improvement of the assignment/composition
rules.
Figure 6 shows an example of the output
for a sentence in the development set. The
variables $1, . . . ,$11 are indices that
713
represent entities, events and situations. For
example, $3 represents a situation and $2
represents the lecturing event that exists
in $3. past($3) requires that the sit-
uation is past. agent($2,$1) requires
that the entity $1 is the agent of $2.
content($2,$4) requires that $4 (as a
set of possible worlds) is the content of
$2. be($11,$4) refers to $4. Finally,
every($6)[ball($6,$4)][see($7,$4)
...] represents a generalized quantifier
?every ball?. The index $6 serves as an
antecedent both for bound-variable anaphora
within its scope and for E-type anaphora out-
side its scope. The entities that correspond to
the two occurrences of ?you? are represented
by $8 and $5. Their unification is left as
an anaphora resolution task that can be easily
solved by existing statistical or rule-based
methods, given the structural information of
the TDL semantic representation.
5 Conclusion
The present paper proposed a method by which
to translate HPSG-style outputs of a robust
parser (Miyao et al, 2005) into dynamic se-
mantic representations of TDL (Bekki, 2000).
We showed that our implementation achieved
high coverage, approximately 90%, for real
text of the Penn Treebank corpus and that the
resulting representations have sufficient expres-
sive power of contemporary semantic theory
involving quantification, plurality, inter/intra-
sentential anaphora and presupposition.
In the present study, we investigated the
possibility of achieving robustness and descrip-
tive adequacy of semantics. Although previ-
ously thought to have a trade-off relationship,
the present study proved that robustness and
descriptive adequacy of semantics are not in-
trinsically incompatible, given the transparency
between syntax and discourse semantics.
If the notion of robustness serves as a cri-
terion not only for the practical usefulness of
natural language processing but also for the
validity of linguistic theories, then the compo-
sitional transparency that penetrates all levels
of syntax, sentential semantics, and discourse
semantics, beyond the superficial difference be-
tween the laws that govern each of the levels,
might be reconsidered as an essential principle
of linguistic theories.
References
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim and Stephan Oepen (to
appear) Beauty and the Beast: What running a
broad-coverage precision grammar over the BNC
taught us about the grammar ? and the cor-
pus, In Linguistic Evidence: Empirical, Theoreti-
cal, and Computational Perspectives, Mouton de
Gruyter.
Daisuke Bekki. 2000. Typed Dynamic Logic for
Compositional Grammar, Doctoral Dissertation,
University of Tokyo.
Daisuke Bekki. 2005. Typed Dynamic Logic and
Grammar: the Introduction, manuscript, Univer-
sity of Tokyo,
Johan Bos, Stephen Clark, Mark Steedman, James
R. Curran and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser, In Proc. COLING ?04, Geneva.
Ann Copestake, Dan Flickinger, Ivan A. Sag and
Carl Pollard. 1999. Minimal Recursion Seman-
tics: An introduction, manuscript.
Ann Copestake and Dan Flickinger. 2000.
An open-source grammar development environ-
ment and broad-coverage English grammar using
HPSG In Proc. LREC-2000, Athens.
Jeroen Groenendijk and Martin Stokhof. 1991. Dy-
namic Predicate Logic, In Linguistics and Philos-
ophy 14, pp.39-100.
Julia Hockenmaier and Mark Steedman. 2002. Ac-
quiring Compact Lexicalized Grammars from a
Cleaner Treebank, In Proc. LREC-2002, Las Pal-
mas.
Mitch Marcus. 1994. The Penn Treebank: A
revised corpus design for extracting predicate-
argument structure. In Proceedings of the ARPA
Human Language Technolog Workshop, Prince-
ton, NJ.
Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-
jii. 2005. Corpus-oriented Grammar Develop-
ment for Acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank, in IJC-
NLP 2004, LNAI3248, pp.684-693. Springer-
Verlag.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar, Studies in Contem-
porary Linguistics. University of Chicago Press,
Chicago, London.
Uwe Reyle. 1993. Dealing with Ambiguities by
Underspecification: Construction, Representation
and Deduction, In Journal of Semantics 10,
pp.123-179.
714
A Hybr id  Japanese  Parser wi th  Hand-craf ted Grammar  and 
Stat ist ics 
Hiroshi Kanayama 1, Kentaro Torisawa 1:*, 
Yutaka Mitsuishi* a,nd Jun' ichi  Tsujiit* 
\[ Tokyo Research Lal)oratory, IBM Jal)an, Ltd. 
1623-1 d Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Jal)an 
:!: Del)artment of Inlbrnmtion Science, Graduate School of Science, University of 2bkyo 
7-a-1 Hongo, Bunkyo-ku, Tokyo 113-0033, .\]al)an 
? hfformation and Human Behavior, PII.I~3S'\].'O, Jal)an Scie, tce and Technology Corporation 
Kawaguehi l ion-the 4-1-8, Kawaguchi-shi, Saitmna a32-0012, Japan 
, CCL, UMIST, U.K. 
{kanayama, torisawa, mitsuisi, tsujii}@is, s.u-tokyo, ac. jp 
Abstract  
This l)al)er (leseril)es a hybrid t)arsing method for 
Jal)anese which uses both a hand-crafted gram- 
mar and a statistical 1;e(:hniqlte. The key featm'e 
of our  syst (nn is that  in o rder  to es t imate  likeli- 
hood  for a parse tree, the sys te ln  Ilses informa- 
tion taken from Mternative 1)artial parse trees gen- 
erated by the grammar. This utilization of alter- 
native trees enables us to construct a new statis- 
tical model called 'l}'it)let/Quadrul)let Model. We 
show that this model can capture a certain ten- 
dency in .\]apalmse yntactic structures and this point 
COlltril)lltes to i lnl)rovetl lel l t  of l)al'sill~ acct l racy oil 
a shallow level. We rel)ort that, with an under- 
Sl)ecified HPSG-1)ased grammar and ~t maximum en- 
tropy estilnation, our parser achieved high a(:curacy: 
88.6% accuracy in del)endency analysis of the EDR 
annotated eorlms, and that it, outI)erformed oth(u' 
lmrely statistical l)arsing methods on the same cor- 
pus. This result suggests that 1)rol)er t reatlnent of 
hand-crafted gra, mnml'S ca,n contribute to l )arsi ltg ac- 
curacy  on a shallow level. 
1 Introduct ion 
There have been many attempts to combine hand- 
crafted high-level gramnmrs, such as FB-UfAG, 
HPSG and LFG, and statistical disambiguation 
techniques to ol)tain precise linguistic struc, tures 
(Schabes, 1992; Almey, 1996; Carroll el, al., 1998). 
One evident advantage of this apl)roaeh over lmrely 
statistical parsing techniques is that grammars can 
provide precise smnantie representations. However, 
considering that remarkable parsing accuracy in a 
shallow level has been achieved by purely statisti- 
cal techniques (e.g. Ratnal)arkhi (1997)), it may be 
thought more reasonable to use high-level gramnmrs 
just tin' 1)osti)rocessing which nmps results of shallow 
syntactical analyses onto dee 1) analyses. 
'l.'his work was conducted while the first ~mthor was a 
graduate student at Univ. of Tokyo. 
Figure 1: A tree M with a non-head aughter NH and 
a }mad aughter H. 
In this work we prol)ose that hand-crafl, ed high- 
level grammars (:all be useful in shallow-level analy- 
ses and statistical models. In our fl'amework, gram- 
mars are used to obtain precise features for probabil- 
ity estimation, which are difficult to obtain without a 
grannnar, and we show that such features contribute 
to high parsing accuracy on a shallow level. 
In this lmper, the most preferable parse trees are 
chosen with a statistical model. In our method, the 
likelihood value L(M) of a (partial) tree M in Fig- 
ure 1 is detined as in (1): 
L(M)  dor L(NH) x L(H)  x P('n ~ h) (1) 
where NH is M's non-head daughter (whose lexical 
head is n), H is the head-daughter (whose lexical 
head is h), and /)(n -~ h) is the probability of n 
t)eing related to h. For a. single lexical iteni W, L(W) 
is defined as 1.0. 
In most models already proposed, the probability 
P (n  ~ h) is calculated with the conditional proba- 
bility (2): 
- ,  h) d?d P(T I ?',,, %, (2) 
where T indicates that the dependency is true; (1)~ 
and q~h are attributes of 'n and h, respectively. And 
An,h, the distance between the two words, is widely 
used, because this attribute is believed to strongly 
affect whether those two words are going to be re- 
late& 
In contrast, in the statistical model proposed in 
this paper, P(n  -~ It) depends not only on the at- 
trilmtes of the tree M, but also on alternative trees 
411 
Input smltence : < . . . n . . . h l  . . . h i  . . . h t  . . . > 
Mi Mz 
M1 
n h ,1  " " " ?z  h i  ~,  h , l  
Figure 2: Pro'tiM trees whose non-head aughter's lexi- 
cal head is n. 
M 
l r 
Figure 3: ~h'anstbrmation fl'om a tree to a dependency. 
l' and r' denote the bunsets,~s l and r belong to, respec- 
tively. 
in the parse forest generated by the grmmnar. More 
precisely, when P(n  --+ h) is calculated, we consider 
partial trees whose non-head aughter's lexical head 
is n, as displayed in Figure 2. Here alternative pos- 
sible hk (k = 1, - . . ,  l) are taken into consideration, 
and ordered according to their distance to n. We 
call such set of hk modification candidates, and all 
modification candidates are placed together in the 
conditional part of the probability as in (3). Now 
assume h = hi. 
P(i I %,, %2, ,  %, , ,  %,) 
(3) 
where "i" indicates the ith candidate mnong the 
modification candidates. Equation (3) shows two 
important properties of our model. One point lies in 
the new distance metric. (3) is the probability that n 
chooses the ith candidate as the modifiee among the 
modification candidates which are ordered according 
to their distance to n. Thus, we no longer require 
the distance metric A~,h, instead we use the relative 
position among the modification candidates, which 
works as an attribute of the modification. The other 
point is the use of the attributes of the alternative 
parse trees, that is, attributes of the modifier and all 
its modification candidates are considered simulta- 
neously. We show that these techniques ophisticate 
our model, by providing linguistic examples in Sec- 
tion 3.2. 
In practice, however, treating all candidates i not 
feasible because of data-sparseness. We therefore 
apply a strategy of restricting the modification can- 
didates to at most three. The strategy and its justi- 
fication are discussed in Section 3.1. 
Applying the strategy to the equation (3), we ob- 
tain equations (4) and (5): 
P(It hi) de=f P(i I (i = 1, 2) (4) 
hi) der P(i I %, ,  %=, %,) (i = *, 2, t)(5) 
When there are only two candidates, equation (4) 
is used; otherwise, equation (5) is used. Our statis- 
tical model is called the ~Hplet/Quadrut)let Modal, 
which was named after the nmnbcr of constituents 
in the conditional parts of the equations. 
We report that our parsing framework achieved 
high accuracy (88.6%) in dependency analysis of 
Japanese with a combination of an underspecified 
HPSG-based Japanese grammar, SLUNG (Mitsu- 
ishi et al, 1998) and the maximum entropy method 
(Berger et al, 1996). Moreover, the resulting parse 
trees generated by our hybrid parser are legitimate 
trees in terms of given hand-crafted grammars, and 
we are expecting that we can enjoy advantages pro- 
vided by high-level gramnmr formalisms, such as 
construction of semantic structures. 
In the above explanation, we used the notion of 
lexical heads for the estimation of probabilities of 
trees for the sake of simplicity. But, in the present 
implementation, we use bunscts,Ls instead of lexical 
heads, and a relation on a tree is converted to a 
bunsetsu-dependency as shown in Figure 3. A bun- 
sctsu is a basic syntactic unit in Japanese. It consists 
of a content word and some flmctional morphemes 
such as a particle. 
In Section 2, we describe some existing statisti- 
cal parsers, and the Japanese grannnar which we 
adopted. Section 3 describes our statistical method 
and its adwmtages in detail. We report ext)erimental 
results in Section 4. 
2 Background 
In this section, we describe several models for 
Japanese dependency analysis and works on statisti- 
cal approaches with gramlnars. Next, we introduce 
SLUNG, the HPSG-based Japanese grammar which 
is used in our hybrid parser. 
2.1 P rev ious  Dependency Analysis Mode ls  
of Japanese 
Several statistical models for Japanese dependency 
analysis which do not utilize a lland-crafted granl- 
mar have been proposed. We evaluate the accuracy 
of bunsetsu-dependencies as they do, thus here we 
introduce thenl for comparison. All models intro- 
duced below are based on the likelihood value of the 
dependency between two bunsetsus. But they differ 
from each other in the attributes or outputs which 
are considered when a likelihood value is calculated. 
There are some models which calculate the likeli- 
hood values of a dependency between bunsetsu i and 
j as in (6), such as a decision tree model (Haruno et 
al., 1998), a maximum entropy model (Uchimoto et 
al., 1999), a model based on distance and lexical in- 
formation (Pujio and Matsumoto, 1998). Attributes 
(I)i and ~I,j consist of a part-of-speech (POS), a lexi- 
cal item, presence of a comma, and so on. And Ai,j 
412 
is the number of intervening bnnscts'us between i and 
j. 
p(i -~ j) d,j ~'Crl ,I,i, %, a~,j) ((0 
However, these lnodels Nil to reftect contextual 
information because attributes of the surrounding 
bunsets,tts are not considered. 
Uchimoto et al (2000) proposed a model us- 
ing posterior context;. The model utilizes not only 
attributes about bunscts~s i, j but also attributes 
about all bunsets~> (including j) wlfich tbllow bun- 
setsu i. That is, instead of learning two output val- 
ues "T(true)" or ':F(false)" for the del)endency be- 
tween two bunsets~zs, three output values are used 
*br leanfing: the b~m.setsu i is "bynd (dependent on 
a bunsctsu beyond j)" ,  "dpnd (del)endent on the 
b~tsets~t 3)" or "btwn (dependent on a b'unscts~t be- 
tween i and j)".  The 1)robability is calculated by 
multiplying probabilities for all bunscts,~ls which tbl- 
low b~trtsctsu i as in (7). 'l'hey report that this kind 
of contextual information improves accuracy. How- 
ever, the model has to assume, the independency of
all the random variables, which may cause some er- 
ro rs .  
P(i --, j) "?Z H ~'(by.d I ?'~, %, &,k) 
i<k<j  
xP(dpnd I (1)i, il)5, Aid) x Hl'(btw,, \[ (I,i, q?k, A<k)(7) 
k>j 
The difference between our model and these pre- 
vious models are discussed in Section 3. 
2.2 Stat is t ica l  Approaches  w i th  a grmnnmr  
There have been nlally l)rOl)osals tbr statistical 
t'rameworks particularly designed tbr 1)arsers with 
hand-crafted grmnmars (Schal)es, 1992; Briscoe and 
Carroll, 1993; Abney, 1996; Inui et al, 1!)97). The 
main issue in tiffs type of research is how to assign 
likelihoods to a single linguistic structure generated 
by a gramlnar. Some of tlmm (Briscoe and Carroll, 
1!)93; hmi et al, 1997) treat information on contexts, 
but the contextual intbrmation is de.rived only fl'om 
a structure to wlfich the parser is trying to assign 
a likelihood value. Then, tim major difference be.- 
tween their method and ours is that we consider the 
attributes of alternative linguistic structures gener- 
ated by the grammar in order to deternfine the like- 
lihood for linguistic structures. 
2.3 SLUNG : J apanese  Grammar  
The Japanese grammar which we adopted, SLUNG 
(Mitsuishi et al, 1998), is an HPSG-based under- 
specified grammar. It consists of 8 rule schemata, 
48 lexical templates for POSs and 105 lexical entries 
for functional words. As can be seen fl'om these fig- 
ures, the granmmr does not contain detailed lexk:al 
information that needs intensive labor for develop- 
ment. However, it is precise in the sense that it 
aclfieves 83.7% dependency accuracy with a silnple 
heuristics 2 for the El)I{ almotated corl)us , and it 
can produce at least one parse tree for 98.4% sen- 
tences in the EDR annotated corpus. We use the 
grammar for generating parse tree forests, and our 
'l~'iplet/Quadruplet Model is used tbr picking Ul) a 
single tree fl'om a forest. 
3 The  Hybr id  Pars ing  Method 
This section describes tim procedure of parsing with 
the ~l"riplet/Quadrul)let Model. Our hybrid 1)arsing 
method proceeds as tbllows: 
? At; the beginning, dependency structures are 
obtained from trees generated by SLUNG. For 
each bunsctsu, modification candidates are enu- 
merated, and if there are four or more candi- 
dates, tlmy are restricted to three. The lmuristic 
used in this process is described in Section 3.1. 
? Then, with the ~'il)let/Quadruplef; Mode.l and 
maxinnnn entropy estimation, prol)abilities of 
the del)endencies are calculated. Secti(m 3.2 
discusses the characteristics and advantages of 
the model. 
? Finally, the most preferable trees for the whole 
sentence are selected. 
3.1 Rest r i c t ion  o f  Modi f i ca t ion  Cand idates  
Kanayama et al (1999) report that when mod- 
ification candidates are emnnerated according to 
SLUNG, 98.6% of the correct modifie.es are in one of 
the following three 1)ositions among the candidates: 
1;11(; nearest one from the modifier, the second nearest 
one, and the. farthest one. 
As a consequence, we can siml)lil\[y I;11(; problem 
by considering only these three candidates and dis- 
carding tim other candidates, with only 1.4% poten- 
tial errors. We therefore assume that the. number of 
modification candidates ix always three or less. 
This idea is sinfilar to that of Sekine (2000)'s 
study, which restricts the candidates to five, i)ut in 
his case, without a granmmr. 
3.2 The  Tr ip le t /Quadrup le t  Mode l  
The 'Diplel,/Quadruplet Model calculates the like- 
lihood of the dependency between bunsetsu i and 
bunsctsu cn; P( i  --, cn) with the formulas (8) and 
(9), where c,~ denotes the nth candidate among b,m- 
sctsu i's candidates; (I,i denotes some attributes of 
i; and ~I~?,~ denotes attributes of c,~ (including at- 
tributes between i and cn). 
P(i -~ c,d dJ P(n I ?,~, %.,, %~) (.n = 1,2)(8)  
P(~ -~ c,~) ,,~r p(,~ I ?'i, %,, ,I,~, ,I%) (,n = 1, 2 , / ) (9 )  
2This heuristics is a Japanese version of a left-association 
rule: see (Mitsuishi et M., 1998) for detail. 
413 
As (8) and (9) suggest, the model considers at- 
tributes of the modifier bunsetsu and attributes of all 
modification candidates imultaneously in the condi- 
tional parts of the probabilities. Moreover, what is 
calculated is not tile probability of "whether the de- 
pendency is correct (T, see Formula(6))", but the 
probability of "which of tile given candidates i cho- 
sen as tile nlodifiee (n =1, 2, or 1)". These charac- 
teristics imply the fbllowing two advantages. 
Advantage  1 A new distance metric. The correct 
modifiee can be chosen by considering relative 
position among grannnatically licensed candi- 
dates, instead of the absolute distance between 
bunsets~as. 
Advantage  2 2)'eating alternative trees. The can- 
didates are taken into consideration simultane- 
ously. But because the nlodifica?ion candidates 
are restricted to at most three, we considerably 
avoid data-sparseness 1)rot)lems. 
Below we discuss these advantages in order. These 
advantages clarify the differences fl'om previous 
models described in Section 2.1, and are empMcally 
confirmed through the experiments in Section 4. 
3.2 .1  Advantage  1 : A new dis tance  metr i c  
As discussed in Section 2.1, the distance metric Ai,j 
used in previous statistical methods was obtained 
simply by counting intervening words or b'unscts,t~ 
l)etween i and j. On the other hand, we use the rel- 
ative position among the modification candidates as 
the distance metric. Tile following examples illus- 
trate a difference between those two types of melric. 
The correct modifiee of kare-ga is hashir'u-no-wo in 
both (10a) ~u~d (lOb). 
(10)a. kare-ga hashiru-no-wo mira koto 
he-SUBJ mm see fact 
(the fact that I saw him run) 
b. kare-ga yukkuri hashiru-no-wo mira koto 
he-SUB.} slowly run see fact 
(the fact that I saw him run slowly) 
In previous models, (10a) and (10b) would yield, 
P. ( kare-o,~--* t~ashir'u-no-wo)=P(T I kar,~-ga, h,~shiru-no-~vo,A1) 
\])b(kare-ga--+ hashi,.tt-?zo-wo)=\])(~l'll~a~ve-ga, hashi,'u-,zo-wo,A2) 
respectively, where A1 = 1 and A 2 = 2. Then, the 
two probabilities above do not have the same value 
in general. 
Our grammar does not allow the dependency 
"kare-.qa --~yukkurY tbr (10b). The modification 
candidates of karc-ga are hashiru-no-wo and mita, 
hence (8) gives the probabilities between kare-ga and 
hashiru-no-wo as follows, in both examples. 
\]~ (kare-ga -~ hashiru- no- wo ) 
= Pb(karc-,qa --~hashiru-no-wo) 
= P(llkare-ga, hashiru-no-wo, mita) 
Thus ,  P(kare-ga --+hashiru-no-wo) has the  same va lue 
for both examples. Our interl)retation of this difl'er- 
enee is sumnlarized as follows. The word yukk'uri s 
an adverb modifying the verb h, ash&'u. Our linguis- 
tic intuition tells us that the presence of such adverb 
should not affect the strength tbr the dependency 
between kare-ga and hashiru-no-wo. According to 
this intuition, the existence of the adverb should be 
considered as a noise. Our model allows us to ignore 
such a noise in learning from annotated corpus, while 
previous nlodels are atfected by such noisy elements. 
3.2 .2  Advantage  2 : Treat ing  a l te rnat ive  
t rees  or contextua l  in fo rmat ion  
Consider the following examples. 
(11) a. Ta~v-no kawaii musume 
NP Adj NP 
Taro-POgS 1)retty daughter 
(~\[h.ro's pretty daughter) 
b. Taro-no yuujin-no musumc 
NP NP NP 
Taro-POSS friend-POSS daughter 
(Taro's fl'iend's daughter) 
Contrary to tim previous examl)les, TaTv-no ill 
(11) ntodifies different nlodification candidates. In 
example (11a), "~hr'o-no --+musume" is the correct 
dependency while "Taro-no -~musume" is not cor- 
rect in (11l)). This difference is caused t)y the b'u'a- 
setsu between Taro-no and musume, kawaii (Adj) 
in ( l la)  and y,u~lfin-no (NP) in ( l lb) .  Actually, the 
grannnar allows Taro-no to depend on either of these 
types of words. Thus, in our model, 
/',('late-no --, musume) 
l't, ( ~1aro-7~o --. m~*sume) 
= P(21 
Then, P(varo-no-+musume) has different values 
for the two examples, hi the annotated corpus, 
l'(21~laro-no, kawaii, musume) tends to have a high 
value since kawaii is an adjective. However, since 
yuujin-no is an NP, P(2\[Taro-no, yuujin-no, musume) 
tends to have a low value. 
Now consider previous models. 
Pb(Taro-,~o--+ m**s~mz~) = P(TI Tin'o-no, musume, 2) 
Then, contrary to our model, P(Taro-no --~musumc) 
lms exactly the same wdue for both examples. The 
outconle is determined by 
= P(T I  Taro-no, kawaii, 1) 
In text corpora, P(TITaro-no , yu~,jin-no, 1) tends 
to be high, and consequently, P(T ITaro-no, musume, 
2) is very small. These values will make the correct 
prediction for (111)) as yuujin-no will be favored over 
musume. However, for (11a), these models are likely 
to incorrectly favor kawaii over musume. This is 
414 
because 1'('.171 Tin'o-no, mus'ume, 2), being very small, is 
likely to be snlaller than P(T\] :late-no, t~,,waii, 1). 
4 Exper iments  and  D iscuss ion  
\].'his section reports a series of parsing experiments 
with our mode, l and gives some discussion. 
4.1 Env i ron lnents  
We used the EDR ,lal)anese Corl)us (El)R, 1996) 
for training and evaluation of 1)arsing accuracy. The 
EI)R Corpus ix a ,Japanese treebank which consists 
of 208,1.57 sentences from newspapers and maga- 
zines. We. used 192,778 sentences for training, (1,744 
for pro-analysis (as reported in Section 3.1), and 
3,372 tbr testing 3. 
With tril)lets constitute(\] of a modifice and two 
modi f i ca t ion  eandida.te.s ext ractc ( l  t i 'onl the learn- 
ing corl)uS l;hc Triplet Model is ('.onstructed. \Vith 
the quadruplets constituted of a moditiee and three 
candidates, the Quadruplet Model is constructed. 
'?hese~ inodels arc estimated by the ChoiccMaker 
Maxinmm Entropy Estimator (Borthwick, 19!)9). 
The features fin' the estimation are listed in Ta- 
/)le 1. The values partially fo lk)w other researches 
e.g. Uchimoto el; al. (\]999), and JUMAN's outputs 
are used for POS classification. Mainly the head of 
the b'unsc.tsu (the rightmost morl)helnc in a b'unscts'u, 
except for whose major POS is "peculiar", "auxiliary 
verb", "particle", "suffix" or "copula") and type  of 
the b'ltnscts'u (the rightmost morphenm in a b'wnsel.s'lt 
except br whose major P()S is "l)eculiar") are used 
as thc at.tributes. \~;e show the meaning of some 
f( 'atures below. 
POS JUMAN's  minor  \]?()S (for both  "head" and 
':type"). 
part ic le,  adverb  Frequent words: 26 lmrticles and 
69 adverbs. 
head lex 2.(14 lexical forms regardless of their POS. 
type  lex 70 suffixes or auxiliary verbs. 
inf lection 6 types of inttcction : "normal", "a(lver- 
l)ial", "adnominal", "tc-fornf', "ta-tbrm", and 
"others". 
The cohmm %aria(ion" in Tal)h; 1 denotes the 
mnnbcr of possible values tbr the feature. "Valid 
features" indicates the nmnber of features which al)- 
peared three times or more in the training corlms. 
4.2 Resu l ts  
Wil;h our model and the features described above, 
the accuracy shown in Tal)lc 2 is achieved. We oval- 
uate the following two tyl)eS of accuracy: 
35,263 SOld,enccs were rOllloved 1)eCmlSe the order of the 
words in the annotal;ion ditl'ered front that in the original 
SOl l tO l lCeS .  
\[ I i \,,re,it, ........ <,~ 
I l l  Pea l ,  u re  type  Va l ' i a t ion  ' l ' r ip .  \] Q ,md,  
I I I oad  I 'OS  ?~f nmdi f ie r  24  ,12 6,t  
2 Type  P ( )S  o f  i l l od i l ' i e r  34  0(l !)D 
3 l ' a r t i ch ,  o f  l / l ed( t ie r  27  , t7  7:{ 
4 Adverb  ,fl' i n (~d i f io r  70  131 103 
5 Ty lm l(}X ( i f  i i l t ld i f io l"  71 110  225 
d In f lec t i (n l  ()|" i . lod i f i (n"  {; 12 18 
7 \Vh( ! th l ! r  lx l ( id i t i ( !r  has  a COlll l l l lt *2 4 ~- 
8 l Iead  I 'OS  , f f  l tH~dil ' ioo 24  7(I 158  
9 Type  I 'OS  o f  lnqMi | ' i ee  34  96  231 
10 lh ,ad  \]?~x ~,1 in i )d i f ieo  2D5 l lG , I  2597 
11 lh t r t ; i c lo  \[d" Inod i l ie (~ 27  92  20.1 
12 ' l ' ypo  Iox  o |  i l l od i | i /~o  71 210 454 
13 \]nlh~l:t . i tHI  o f  In ( .d i f le ( ,  6 2,1 5:1 
1.l ~ .Vhothor  Inodi l ' ioc~ has  it GI)Ill l IIII 2 8 18 
15 \Vhethor  n lod i f ioe  has  %rod' 2 8 18  
IG ~Vho l ,h~r  inod l f ioe  l i l ts " /o"  2 6 17 
17 :\]/ ( if  (~()1\[111~ii~ b(!tlg(~(!ll l;l';?l IHtlt.utlt.uos ,1 16 36  
18 # i) f  "i l ia" l)tl~,~v(!(*lI t~V() bltYl~t:~gllH 3 12 27  
19 2 X 8 816 1187 2727 
20  2 x 7 x 14 13G 38(} 8711 
21 3 X 10 7905 6,t(15 13,10:{ 
22  2 x 9 1156 1213 311)8 
23  3 X 11 729 618 1637 
2.1 2 X 11 i118 1025 2 .194 
'25 2 X 12 241, t  1483 351 '1  
26  2 x 3 x 7 x 8 132192 1331 3O58 
27  1 X 2 X 6 X 8 X 13 7( )5024 ( i( i05 1,t7{10 
I t 'r''':''~ I - I  '224:':~ I """~" J
Table 1: Used f'eaturcs : l,k,,atures from 8 to 27 are re- 
lated to the nm(litiee, thus they are considered for each 
candidate, li'eatures from 19 to 27 are combination fea- 
\ [ ; I l I 'CS .  
I I I -COVCI 'a~O 
S(~.l l tO l  ICCS 
\]htnset.su accuracy 88.ooX,(23078/2(i062) 
Sentence accu,'acy /16.{)0% (1560/3326) 
All lhm.set.su accuracy 88.33% (2335()/26436) 
sentences ,qentcncc~ accuracy 46.35% (1563/3372) 
Tal)h; 2: l{csulls of parsing with the Tril)let/Quadrul)let 
Modcl. 
Bunsetsu  accuracy  The percentage of bu.n,~cts'us 
whose rood(rice is correctly identified. The dc- 
nonfinator includes all b'unsets'us except for the 
last bun,~cts'u of a sentence. 
Sentence accuracy  The percentage of sentences 
whose detmndencies art'. perfectly correct. 
"h>coverage sentences" is the accuracy for the 
sentences flw which SLUNG could generate parse 
trees. We give the accuracy for "All sentences" too, 
by 1)art(ally 1)arsing sentences which SLUNG fail to 
parse. The coverage of SLUNG is al)out 99%, thus 
high accuracy is achieved even for "All sentences". 
Moreover, we conducted a series of experiments 
in order to evaluate the COld;ribution of each charac- 
teristic in our parsing model. The parsing schemes 
used are the four in Figure 3. Major differences 
among them are (I) whether a gralnlnar is used, 
(II) whether modification candidates are restricted 
to three, and (III) whether a previous pair model 
with Formula (6) or the 'lS'iplet/Quadrulflet Model 
with Formula (8),(9) was used. 
W/O Grammar  Mode l  This model does not use 
a grammar. Likelihood values for dcpenden- 
4I 5 
W/O Grammar 
W/O Restriction 
Pair 
~IMplet/Quadruplet 
I G R P 
P 
\]~Un8gts~L accuracy 
86.70%(22594/26062) 
+ - P 87.37%(22770/26062) 
+ + P 87.67%(22849/26062) 
+ + T 88.55%(23078/26062) 
Table 3: Bunsetsu accuracies for four models. Cohmm 
"G" indicates whether the grmmnar is used, "R" indi- 
cates whether the modification candidates are restricted 
to three, and "F" denotes the formula; "P" is the pair 
tbrmula (6), and "T" is the %'iplet/Quadruplet formula 
(s), (9). 
cies are calculated for all bunsctsiLs that follow 
a modifier bunsctsu. Formula (6) is used, and 
as a distance metric Ai,j, the mnnl)er of bun- 
scts~ls between the modifier and tile modifiee 4 
are combined with all features. In general lines, 
this model corresponds to models such as (Fu- 
lie and Matsumoto, 1998; Haruno et al, 1998; 
Uchimoto et al, 1999). 
W/O Rest r i c t ion  Mode l  Modification candi- 
dates are restricted by SLUNG. Tim remaining 
is the same as the W/O Grannnar Model. 
Pa i r  Mode l  Modification candidates are restricted 
to three, in the way described in Section 3.1. 
The remaining is the same as W/O Grannnar 
Model. 
T r ip le t /Quadrup le t  Mode l  Tiffs is the model 
proposed in the paper. Modification candidates 
are restricted to tln'ee, and Fornmla (8) or (9) 
are used. 
From the result shown in Table 3, we can say 
our method contributes to the improvement of our 
parser, because of the following reasons: 
? The %'iplet/Quadruplet Model outperforms the 
Pair Model by 0.9%. Both of them restricts 
modification candidates to three, l)nt tim accu- 
racy got higher when all candidates are consid- 
ered simultaneously. It is because of the two 
adwmtages described in Section a.2. 
? TILe Pair Model outperforms the W/O Restric- 
tion Model by 0.3%. Thus the restriction of 
modification candidates does slot reduce tile ac- 
curacy. 
? TILe W/O Restriction Model outperforms tile 
W/O Grammar Model by 0.7%. This means 
that the use of a grammar as a preprocessor 
works well to pick up possible modifice. 
We found that many structures imilar to the 
ones described iLL Section 3.2 appeared in the EDR 
4Three vahms: "1", "from 2 to 5", "6 or more" are distin- 
guished. 
In-coverage \]3unsct.su accuracy 87.08% (8299/9530) 
sentences Sentence accuracy 44.70% (493/\]103) 
Table 4: Accuracy tbr Kyoto University Corpus 
corpus. Our Tl'iplet/Quadruplet model could treat 
these structures precisely as we intended. Tlfis is the 
main factor that contributed to the improvement of
the overall parsing accuracy. 
Based on tim above experiments, we can say that 
our approach to use the grammar as a preprocessor 
before the calculating of the probability is appropri- 
ate for the improvement of parsing accuracy. 
4.3 Compar i son  to  o ther  mode ls  
4.3.1 Mode ls  us ing the  EDR corpus  
There are several works which use the EDR corpus 
for evaluation. The decision tree model (Haruno et 
al., 1998) achieves around 85%, the integrated model 
of lexical/syntactic information (Slfirai et al, 1998) 
achieves around 86%, and the lexicalized statistical 
model (Ft0io and Matsumoto, 1999) achieves 86.8% 
in bunsets'u accuracy. Our model outperforms all of 
them by 2 or 3%. 
4.3.2 Mode ls  us ing the  Kyoto  corpus  
Slfirai et al (1998) used the Kyoto University text 
corpus (Kurohashi and Nagao, 1997) for evaluation 
and achieved around 86%. Uclfimoto et al (2000) 
also used the Kyoto corlms , and their accuracy was 
87.9%. For comparison, we applied our method to 
the same 1,246 sentences that Uclfimoto et al (2000) 
used. The result is shown in Table 4. 
Our result is worse than theirs. The reason is 
thought o l)e as follows: 
? g~re use tim EDR corpus for training. Although 
we used around 24 times the amount of train- 
ing data that Uchimoto et al used, our training 
data lead to ca'ors in tile analysis of the Kyoto 
Corpus, because of differences in tile mmotation 
schenms adopted. 
? Uchimoto et al used the correct morphological 
analyses, but we used JUMAN. Solnetimes this 
may cause errors. 
? The grammar SLUNG was designed for tile 
EDR corpus, and some types of structures in 
the Kyoto Corpus are not allowed. 
Clearly, our parser should be improved to overcome 
these problems and compared with other works di- 
rectly. 
4.4 D iscuss ion  and I~lture Work  
TILe following are some observations about the speed 
of our parser. Existing statistical parsers are quite 
etficient compared to grammar-based systems. Par- 
ticularly, our system used an HPSG-1)ased grmmnar, 
416 
whose speed is said to be slow. However, recent ad- 
vances in HPSG 1)arsing (~Ibrisawa et al, 2000) en- 
abled us to obtain a unique parse tree with our sys- 
gem in 0.5 sec. in average tbr sentences in the EDR 
corpus. 
Future work shall extend SLUNG so that senmntie 
representatkms are produced. Carroll el; al. (1.998) 
discussed i;he 1)recisiol~ of argument si;ruetures. V~Te 
1)elieve that the focus of ore' study will shift; from a 
shallow level to such a deeper level for ()Ill' tinal aim, 
realization of intelligent natural anguage processing 
systems. 
5 Conclusion 
\?e 1)resenl;ed a hyl)rid 1)arsing scheme l;hat uses a 
hand-crafted grammar and a statist.teal technique. 
As other hybrid pa.rsing ntethods, l;he st.al;isi;ical 
technique is used for 1licking u 1) the most l)re, ferable, 
lmrs(; ire(; fl'om l;he parse fol"(;sI; gent'.rai;e,d I)y t;h(~ 
grammar. The difference fl'om other works is that 
the precise contexi;ual information needed to esti- 
mate |;he likelihood of a parse, 1;ree is obtained fl'ont 
adternative 1)arse trees generated 1)5' the grammar, 
and that such contextual information from alterna- 
tive I;rees enables Its to eonsl;ruel; our new statisti- 
cal model called the ' l?iplet/Quadruplet model. We 
have shown that these poinl;s contributed to sul)sl;an- 
tia l illlprovenlenl; of parsing acellra(:y ill ,lal)ane~se dc- 
1)en(lency analysis, through a serie, s of ext)(~riments 
using an I iPSG-based .lalmnese grammar SLUNC, 
and the, maxinmm entropy method. 
References 
St;even Abney. 1996. Sl;ochasti(: aH;ribut(',-vahm 
grannnars. The Computation and \]\]anguage E- 
Print Archive, October. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent. J. Della Pietra. \]996. A itiaxilnuln entropy 
approach to natural anguag('~ processing. Compu- 
tatio'n, al Li'n.gui.stics, 22(1.)::/9 71. 
Andrew Borthwiek. 199.(). Choieemaker maximmn 
entropy estimator. ChoieeMaker 'lbch., Inc. Email 
borthwic~cs .nyu. edu for information. 
~\[l'~d Briseoe and John Carroll. 1993. Generalized 
1)robabilistic LR parsing of natural anguage (col 
1)Ol'~t) with unifieation-I)ased gramnmrs. Compu- 
tational Linguistics, 19(1):25-50. 
Jolm Carroll, Guido Minnen, and ~lL'd Briscoe. 1998, 
Can subeategorisation probabilities help a statis- 
tical parser? In Proc. of th, e 5th, ACL/SIGDAT 
Workshop on Very Lawe Corpora, pages 118 126. 
EDR. 1996. EDR (Japan Electronic Dictionary Re- 
search Institute, Ltd.) dictionary version 1.5 tech- 
nical guide,. Second edition is awdlable via 
h t tp  ://www. i i j ne t ,  or.  j p/edr/E_TG .html. 
Masakazu Fujio and Yuji Matsumot;o. 1998. 
,htl)anes<', <lel)endeney structure analysis 1)ased on 
lexicalized statistics. In PTvc. of the 3rd Cm@r- 
ence on Empirical Methods in Natural Language 
Procc.ssin 9, pages 88 96. 
Masakazu FI0io and Ymtji Ma?sumoto. 1999. Sta- 
tistieal syntactic analysis based on co-occurrence 
probability of words. In P'roc. of 5th workshop 
of Nat'u~nl Language Processing, pages 71 78. (in 
Jal)altese ) .
Masahiko Haruno, Satoshi Shirai, and goshiflmfi 
Ooyama. 1998. Using decision trees to construct 
a. practical parser. In Prec. COLING ACL '98, 
pages 505 -511. 
Kentaro hmi, Virach Sornlertbunwmich, Hozumi 
Tanaka, and Takenobu 9bkmmga. 1997. A new 
l>robal)ilistic LR language lnodel t'(31' statistical 
parsing. 'l'echnical I{eport TR974)005, Dept. of 
Coml)uter Science, Tokyo Institute of 'lbehnology. 
l liroshi Kanayanm, Kentaro 'l.brisawa, Yutaka Mit- 
suishi, and Jun'i(:hi Tsujii. 1999. Statistical de- 
1)e, ndency analysis with an HPSG-1)ased Jal)anese 
grannnar. In P'roc. 5th NLPRb', pages 138-143. 
Sadao I(urohashi att(l Makoto Nagao. 1.997. Kyoto 
University text corpus in'ojecK In Prec. of 3rd 
Ann'ual M(~cti~ N of Nat,u, raI Language i)roccssi'ng, 
l)ages 115 118. (in Japanese). 
Yutaka Mii;suishi, Kentaro Torisawa, and Jun'ichi 
Tsujii. 1.(/98. HPSG-stsde undersl)e(:ified .Japanese 
grammar with wide coverage. In P'mc. COLING- 
ACL '98, 1)ages 876 880, Augusl;. 
Adwait llatnalmrkhi. 1997. A linear obse, rved tinl(; 
statistical lm.rser based Oll maximum entropy 
models. In P'mc. th.c Empirical Mt~thods in Nat'u- 
'ral \])a,'n,guag(: \])'roce, ssi'n,9 Co~@rence. 
Yves Sehabes. \]992. Stoclmsti(: lexicalize, d tree- 
adjohfing granmwms. In P'mc. 1dth COLINO, 
pages d26 432. 
S~toshi Sekim,. 2000. Japanese dependency analysis 
using a deternfinistic, tinite state, transdue(;r. In 
Prec. COLING 2000. (this proceedings). 
Kiyoaki Shirai, Kentaro huff, Takenolm Tokunaga, 
and Hozumi Tanaka. 1998. A framework of inl;e- 
grating ,syntactic and lexical statistics in si;atisti- 
cal 1)arsing..lo,urnal ofNat'ural Langua9c l)~vccss - 
int.\], 5(3). (in Japanese). 
Kentaro 'Ibrisawa, Kenji Nishida, Yusuke Miyao, 
and Jun'ichi Tsujii. 2000. An HPSG parser with 
CFG filtering. Jounal of Nat'mal Language E'n, gi- 
nccrin.q. (to al)pear ).
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi 1sa- 
hara. 19!19. Japanese dependency structure anal- 
ysis based on maximum entropy models. In P'roc. 
13th EACL, pages 196 -203. 
Kiyotaka Uchimoto, Masaki Mural;a, Satoshi Sekine, 
and Ititoshi isahara. 2000. \])el)endeney model us- 
ing posterior context. In Prec. of Sixth, intcrna- 
lionel Workshop on Parsing 7'cch, nolo9ics. 
41 7 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 657?664
Manchester, August 2008
A Discriminative Alignment Model for Abbreviation Recognition
Naoaki Okazaki
?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou
?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii
??
tsujii@is.s.u-tokyo.ac.jp
?
Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?
School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper presents a discriminative align-
ment model for extracting abbreviations
and their full forms appearing in actual
text. The task of abbreviation recognition
is formalized as a sequential alignment
problem, which finds the optimal align-
ment (origins of abbreviation letters) be-
tween two strings (abbreviation and full
form). We design a large amount of fine-
grained features that directly express the
events where letters produce or do not pro-
duce abbreviations. We obtain the optimal
combination of features on an aligned ab-
breviation corpus by using the maximum
entropy framework. The experimental re-
sults show the usefulness of the alignment
model and corpus for improving abbrevia-
tion recognition.
1 Introduction
Abbreviations present two major challenges in nat-
ural language processing: term variation and am-
biguity. Abbreviations substitute for expanded
terms (e.g., dynamic programming) through the
use of shortened term-forms (e.g., DP). At the
same time, the abbreviation DP appearing alone in
text is ambiguous, in that it may refer to different
concepts, e.g., data processing, dirichlet process,
differential probability. Associating abbreviations
and their full forms is useful for various applica-
tions including named entity recognition, informa-
tion retrieval, and question answering.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The task of abbreviation recognition, in which
abbreviations and their expanded forms appearing
in actual text are extracted, addresses the term vari-
ation problem caused by the increase in the num-
ber of abbreviations (Chang and Sch?utze, 2006).
Furthermore, abbreviation recognition is also cru-
cial for disambiguating abbreviations (Pakhomov,
2002; Gaudan et al, 2005; Yu et al, 2006), pro-
viding sense inventories (lists of abbreviation def-
initions), training corpora (context information of
full forms), and local definitions of abbreviations.
Hence, abbreviation recognition plays a key role in
abbreviation management.
Numerous researchers have proposed a variety
of heuristics for recognizing abbreviation defini-
tions, e.g., the use of initials, capitalizations, syl-
lable boundaries, stop words, lengths of abbrevia-
tions, and co-occurrence statistics (Park and Byrd,
2001; Wren and Garner, 2002; Liu and Fried-
man, 2003; Okazaki and Ananiadou, 2006; Zhou
et al, 2006; Jain et al, 2007). Schwartz and
Hearst (2003) implemented a simple algorithm that
finds the shortest expression containing all alpha-
numerical letters of an abbreviation. Adar (2004)
presented four scoring rules to choose the most
likely expanded form in multiple candidates. Ao
and Takagi (2005) designed more detailed condi-
tions for accepting or discarding candidates of ab-
breviation definitions.
However, these studies have limitations in dis-
covering an optimal combination of heuristic rules
from manual observations of a corpus. For exam-
ple, when expressions transcrip
:
tion factor 1 and
thyroid transcription factor 1 are full-form can-
didates for the abbreviation TTF-1
1
, an algorithm
should choose the latter expression over the shorter
1
In this paper, we use straight and
::::
wavy underlines to rep-
resent correct and incorrect origins of abbreviation letters.
657
expression (former). Previous studies hardly han-
dle abbreviation definitions where full forms (e.g.,
water activity) shuffle their abbreviation letters
(e.g., AW). It is also difficult to reject ?negative?
definitions in a text; for example, an algorithm
should not extract an abbreviation definition from
the text, ?the replicon encodes a large
:::
replic
:
ation
protein (RepA),? since RepA provides a descrip-
tion of the protein rather than an abbreviation.
In order to acquire the optimal rules from
the corpora, several researchers applied machine
learning methods. Chang and Sch?utze (2006) ap-
plied logistic regression to combine nine features.
Nadeau and Turney (2005) also designed seven-
teen features to classify candidates of abbrevia-
tion definitions into positive or negative instances
by using the Support Vector Machine (SVM).
Notwithstanding, contrary to our expectations, the
machine-learning approach could not report better
results than those with hand-crafted rules.
We identify the major problem in the previ-
ous machine-learning approach: these studies did
not model associations between abbreviation let-
ters and their origins, but focused only on indirect
features such as the number of abbreviation letters
that appear at the head of a full form. This was
probably because the training corpus did not in-
clude annotations on the exact origins of abbrevia-
tion letters but only pairs of abbreviations and full
forms. It was thus difficult to design effective fea-
tures for abbreviation recognition and to reuse the
knowledge obtained from the training processes.
In this paper, we formalize the task of abbrevi-
ation recognition as a sequential alignment prob-
lem, which finds the optimal alignment (origins of
abbreviation letters) between two strings (abbrevi-
ation and full form). We design a large amount
of features that directly express the events where
letters produce or do not produce abbreviations.
Preparing an aligned abbreviation corpus, we ob-
tain the optimal combination of the features by us-
ing the maximum entropy framework (Berger et
al., 1996). We report the remarkable improve-
ments and conclude this paper.
2 Proposed method
2.1 Abbreviation alignment model
We express a sentence x as a sequence of letters
(x
1
, ..., x
L
), and an abbreviation candidate y in the
sentence as a sequence of letters (y
1
, ..., y
M
). We
define a letter mapping a = (i, j) to indicate that
the abbreviation letter y
j
is produced by the letter
in the full form x
i
. A null mapping a = (i, 0) indi-
cates that the letter in the sentence x
i
is unused to
form the abbreviation. Similarly, a null mapping
a = (0, j) indicates that the abbreviation letter y
j
does not originate from any letter in x. We de-
fine a
(x)
and a
(y)
in order to represent the first and
second elements of the letter mapping a. In other
words, a
(x)
and a
(y)
are equal to i and j respec-
tively, when a = (i, j). Finally, an abbreviation
alignment a is defined as a sequence of letter map-
pings, a = (a
1
, ..., a
T
), where T represents the
number of mappings in the alignment.
Let us consider the following example sentence:
We investigate the effect of thyroid tran-
scription factor 1 (TTF-1).
This sentence contains an abbreviation candidate
TTF-1 in parentheses
2
. Figure 1 illustrates the
correct alignment a (bottom line) and its two-
dimensional representation for the example sen-
tence
3
; the abbreviation letters ?t,? ?t,? ?f,? ?-,? and
?1? originate from x
30
, x
38
, x
52
, nowhere (null
mapping), and x
59
respectively.
We directly model the conditional probability of
the alignment a, given x and y, using the maxi-
mum entropy framework (Berger et al, 1996),
P (a|x,y) =
exp {? ? F (a,x,y)}
?
a?C(x,y)
exp {? ? F (a,x,y)}
.
(1)
In Formula 1, F = {f
1
, ..., f
K
} is a global feature
vector whose elements present K feature func-
tions, ? = {?
1
, ..., ?
K
} denotes a weight vector
for the feature functions, and C(x,y) yields a set
of possible alignments for the given x and y. We
obtain the following decision rule to choose the
most probable alignment a? for given x and y,
a? = argmax
a?C(x,y)
P (a|x,y). (2)
Note that a set of possible alignments C(x,y)
always includes a negative alignment whose ele-
ments are filled with null-mappings (refer to Sec-
tion 2.3 for further detail). This allows the formula
to withdraw the abbreviation candidate y when any
expression in x is unlikely to be a definition.
2
Refer to Section 3.1 for text makers for abbreviations.
3
We ignore non-alphabetical letters in abbreviations.
658
    We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x:
a
            ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~ ~ 
8 words 4 lettersmin(|y|+5, 2|y|)
<NIL> T  T  F  -   1  <SF>
y:
0 0 0 0 0 0 0 0 0
1
2
3
5
91 13 16 2122 25 28 30 38 47 52 55 59 61
1
0
1
2
3
4
5
6
t =
i
 j
a =
a 
2 3 4 5 6 7 8 9 10 11 12 13 14
Null outsideOther positions Abbreviation Null inside Associate inside
a = ((9,0), (13,0), (16,0), (21,0), (22, 0), (25,0), (28,0), (30,1), (38,2), (47,0), (52,3), (55,0), (59,5), (61,6))
Figure 1: The correct alignment for the example sentence and its two dimensional representation.
2.2 Features
The main advantage of the discriminative align-
ment model is its ability to incorporate a wide
range of non-independent features. Inspired
by feature engineering for Conditional Random
Fields (CRFs) (Lafferty et al, 2001), we design
two kinds of features: unigram (state) features de-
fined on each letter mapping, and bigram (tran-
sition) features defined on each pair of adjacent
letter mappings. Given a triplet, a, x, and y, a
global feature function f
k
(a,x,y) ? F sums up
the boolean values (0 or 1) of the corresponding
local feature g
k
(a,x,y, t) at t ? {1, ..., T},
f
k
(a,x,y) =
T
?
t=1
g
k
(a,x,y, t). (3)
In other words, f
k
(a,x,y) counts the number of
times the local feature is fired in the alignment a.
A unigram feature corresponds to the observa-
tion at x
i
and y
j
associated by a mapping a
t
=
(i, j). A unigram feature encodes the condition
where the letter in the full form x
i
is chosen or
unchosen for producing the abbreviation letter y
j
.
For example, we may infer from the letter mapping
at a
8
= (30, 1) in Figure 1, that x
30
is mapped to
y
1
because: x
30
is at the head of the word, y
1
is a
capital letter, and both x
30
and y
1
are at the head
of the word and abbreviation.
Bigram features, combining two observations at
a
s
and a
t
(1 ? s < t ? T ), are useful in capturing
the common characteristics shared by an abbrevi-
ation definition. For instance, we may presume in
Figure 1 that the head letters in the full form might
be selectively used for producing the abbreviation,
based on the observations at a
8
= (30, 1) and
a
9
= (38, 2). In order to focus on the conditions
for consecutive non-null mappings, we choose the
previous position s for the given t.
s =
?
?
?
t? 1
(
a
t(y)
= 0 ? ?u : a
u(y)
= 0
)
max
1?u<t
{
u | a
u(y)
6= 0
}
(otherwise)
(4)
Formula 4 prefers the non-null mapping that is the
most adjacent to t over the previous mapping (t ?
1). In Figure 1, transitions a
9
?a
11
and a
11
?a
13
exist for this reason.
In this study, we express unigram and bi-
gram features with atomic functions (Table 1)
that encode observation events of x
a
t(x)
, y
a
t(y)
,
a
t
, x
a
s(x)
?x
a
t(x)
, and y
a
s(y)
?y
a
t(y)
. Atomic
functions x ctype, y ctype, x position, and
y position present common heuristics used by
previous studies. The function x word examines
the existence of stop words (e.g., the, of, in) to
prevent them from producing abbreviation letters.
We also include x pos (part-of-speech of the word)
since a number of full forms are noun phrases.
Functions x diff , x diff wd, and y diff are de-
signed specifically for bigram features, receiving
two positions s and t in their arguments. The
function x diff mainly deals with abbreviation def-
initions that include consecutive letters of their
full forms, e.g., amplifier (AMP). The function
659
Function Return value
x ctype
?
(a,x, t) x
a
t(x)
+?
is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter
x position
?
(a,x, t) x
a
t(x)
+?
is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word
x char
?
(a,x, t) The lower-cased letter of x
a
t(x)
+?
x word
?
(a,x, t) The lower-cased word (offset position ?) containing the letter x
a
t(x)
x pos
?
(a,x, t) The part-of-speech code of the word (offset position ?) containing the letter x
a
t(x)
y ctype(a,y, t) y
a
t(y)
is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter
y position(a,y, t) y
a
t(y)
is at the {N (NIL) H (head), T (tail), I (inner)} of the word
y char(a,y, t) The lower-cased letter of y
a
t(y)
a state(a,y, t) {SKIP (a
t(y)
= 0),MATCH (1 ? a
t(y)
? |y|),ABBR (a
t(y)
= |y|+ 1)}
x diff(a,x, s, t) (a
t(x)
? a
s(x)
) if letters x
a
t(x)
and x
a
s(x)
are in the same word, NONE otherwise
x diff wd(a,x, s, t) The number of words between x
a
t(x)
and x
a
s(x)
y diff(a,y, s, t) (a
t(y)
? a
s(y)
)
Table 1: Atomic functions to encode observation events in x and y
Combination Rules
unigram(t) xy unigram(t)? {a state(t)}
xy unigram(t) x unigram(t)? y unigram(t)?
(
x unigram(t)? y unigram(t)
)
x unigram(t) x state
0
(t)? x state
?1
(t)? x state
1
(t)
?
(
x state
?1
(t)? x state
0
(t)
)
?
(
x state
0
(t)? x state
1
(t)
)
y unigram(t)
{
y ctype(t), y position(t), y ctype(t)y position(t)
}
x state
?
(t)
{
x ctype
?
(t), x position
?
(t), x char
?
(t), x word
?
(t), x pos
?
(t), x ctype
?
(t)x position
?
(t),
x position
?
(t)x pos
?
(t), x pos
?
(t)x ctype
?
(t), x ctype
?
(t)x position
?
(t)x pos
?
(t)
}
bigram(s, t) xy bigram(s, t)? {a state(s)a state(t)}
xy bigram(s, t)
(
x state
0
(s)? x state
0
(t)? trans(s, t)
)
?
(
y unigram(s)? y unigram(t)? trans(s, t)
)
?
(
x state
0
(s)? y unigram(s)? x state
0
(t)? y unigram(t)? trans(s, t)
)
trans(s, t)
{
x diff(s, t), x diff wd(s, t), y diff(s, t)
}
Table 2: Generation rules for unigram and bigram features.
x diff wd measures the distance of two words.
The function y diff models the ordering of abbre-
viation letters; this function always returns non-
negative values if the abbreviation contains letters
in the same order as in its full form.
We express unigram and bigram features with
the atomic functions. For example, Formula 5 de-
fines a unigram feature for the event where the cap-
ital letter in a full-form word x
a
t(x)
produces the
identical abbreviation letter y
a
t(y)
.
g
k
(a,x,y, t) =
?
?
?
?
?
?
?
?
?
?
?
1 x ctype
0
(a,x, t) = U
? y ctype(a,y, t) = U
? a state(a,y, t) = MATCH
0 (otherwise)
(5)
For notation simplicity, we rewrite this boolean
function as (arguments a, x, and y are omitted),
1
{x ctype
0
(t)y ctype(t)a state(t)=U;U;MATCH}
. (6)
In this formula, 1
{v=v?}
is an indicator function that
equals 1 when v = v? and 0 otherwise. The term
v presents a generation rule for a feature, i.e., a
combination rule of atomic functions.
Table 2 displays the complete list of gener-
ation rules for unigram and bigram features
4
,
unigram(t) and bigram(s, t). For each genera-
tion rule in unigram(t) and bigram(s, t), we de-
fine boolean functions that test the possible values
yielded by the corresponding atomic function(s).
2.3 Alignment candidates
Formula 1 requires a sum over the possible align-
ments, which amounts to 2
LM
for a sentence (L
letters) with an abbreviation (M letters). It is
unrealistic to compute the partition factor of the
formula directly; therefore, the factor has been
computed by dynamic programing (McCallum et
al., 2005; Blunsom and Cohn, 2006; Shimbo and
Hara, 2007) or approximated by the n-best list of
highly probable alignments (Och and Ney, 2002;
Liu et al, 2005). Fortunately, we can prune align-
ments that are unlikely to present full forms, by in-
troducing the natural assumptions for abbreviation
definitions:
4
In Table 2, a set of curly brackets {} denotes a list (array)
rather than a mathematical set. Operators ? and ? present
concatenation and Cartesian product of lists. For instance,
when A = {a, b} and B = {c, d}, A?B = {a, b, c, d} and
A?B = {ac, ad, bc, bd}.
660
  investigate the effect of thyroid transcription factor 1
0   0  0    00  0  0 0       0        0    0  0   00   0  0    00  0  0 0       1        2    3  0   50   0  0    00  0  0 1       2        0    3  0   50   0  0    00  0  0 1       0        2    3  0   50   0  0    00  0  0 2       1        0    3  0   50   0  0    00  0  0 2       0        1    3  0   50   0  0    00  0  3 0       0        1    0  2   50   0  0    00  0  3 0       1        2    0  0   50   0  0    00  0  3 0       1        0    0  2   5
.   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   .
x: ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~
min(|y|+5, 2|y|) = 8 words, (|y| = 4; y = "TTF-1")
94 13 16 2122 25 28 30 38 47 52 55 59ia =
ShffleShffleShffleShffleShffle
#0
#1
#2
#3
#4
#5
#6
#7
#8
Figure 2: A part of the possible alignments for the
abbreviation TTF-1 in the example sentence.
1. A full form may appear min(m + 5, 2m)
words before its abbreviation in the same sen-
tence, where m is the number of alphanu-
meric letters in the abbreviation (Park and
Byrd, 2001).
2. Every alphanumeric letter in an abbreviation
must be associated with the identical (case-
insensitive) letter in its full form.
3. An abbreviation letter must not originate from
multiple letters in its full form; a full-form let-
ter must not produce multiple letters.
4. Words in a full form may be shuffled at most
d times, so that all alphanumeric letters in the
corresponding abbreviation appear in the re-
arranged full form in the same order. We de-
fine a shuffle operation as removing a series
of word(s) from a full form, and inserting the
removed word(s) to another position.
5. A full form does not necessarily exist in the
text span defined by assumption 1.
Due to the space limitation, we do not describe
the algorithm for obtaining possible alignments
that are compatible with these assumptions. Al-
ternatively, Figure 2 illustrates a part of possible
alignments C(x,y) for the example sentence. The
alignment #2 represents the correct definition for
the abbreviation TTF-1. We always include the
negative alignment (e.g., #0) where no abbrevia-
tion letters are associated with any letters in x.
The alignments #4?8 interpret the generation
process of the abbreviation by shuffling the words
in x. For example, the alignment #6 moves the
word ?of? to the position between ?factor? and
?1?. Shuffled alignments cover abbreviation defini-
tions such as receptor of estrogen (ER) and water
activity (AW). We call the parameter d, distortion
parameter, which controls the acceptable level of
reordering (distortion) for the abbreviation letters.
2.4 Parameter estimation
Parameter estimation for the abbreviation
alignment model is essentially the same as
for general maximum entropy models. Given
a training set that consists of N instances,
(
(a
(1)
,x
(1)
,y
(1)
), ..., (a
(N)
,x
(N)
,y
(N)
)
)
, we
maximize the log-likelihood of the conditional
probability distribution by using the maximum
a posterior (MAP) estimation. In order to avoid
overfitting, we regularize the log-likelihood with
either the L
1
or L
2
norm of the weight vector ?,
L
1
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
1
?
1
, (7)
L
2
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
2
2
2?
2
2
. (8)
In these formulas, ?
1
and ?
2
are regularization pa-
rameters for the L
1
and L
2
norms. Formulas 7
and 8 are maximized by the Orthant-Wise Limited-
memory Quasi-Newton (OW-LQN) method (An-
drew and Gao, 2007) and the Limited-memory
BFGS (L-BFGS) method (Nocedal, 1980)
5
.
3 Experiments
3.1 Aligned abbreviation corpus
The Medstract Gold Standard Corpus (Pustejovsky
et al, 2002) was widely used for evaluating abbre-
viation recognition methods (Schwartz and Hearst,
2003; Adar, 2004). However, we cannot use
this corpus for training the abbreviation alignment
model, since it lacks annotations on the origins of
abbreviation letters. In addition, the size of the
corpus is insufficient for a supervised machine-
learning method.
Therefore, we built our training corpus with
1,000 scientific abstracts that were randomly cho-
sen from the MEDLINE database. Although the
alignment model is independent of linguistic pat-
terns for abbreviation definitions, in the corpus we
found only three abbreviation definitions that were
described without parentheses. Hence, we em-
ployed parenthetical expressions, full-form ?(? ab-
breviation ?)?, to locate possible abbreviation def-
initions (Wren and Garner, 2002). In order to ex-
clude parentheses inserting clauses into passages,
5
We used Classias for parameter estimation:
http://www.chokkan.org/software/classias/
661
we consider the inner expression of parentheses as
an abbreviation candidate, only if the expression
consists of two words at most, the length of the ex-
pression is between two to ten characters, the ex-
pression contains at least an alphabetic letter, and
the first character is alphanumeric.
We asked a human annotator to assign refer-
ence abbreviation alignments for 1,420 parentheti-
cal expressions (instances) in the corpus. If a par-
enthetical expression did not introduce an abbre-
viation, e.g., ?... received treatment at 24 months
(RRMS),? the corresponding instance would have
a negative alignment (as #0 in Figure 2). Eventu-
ally, our aligned corpus consisted of 864 (60.8%)
abbreviation definitions (with positive alignments)
and 556 (39.2%) other usages of parentheses (with
negative alignments). Note that the log-likelihood
in Formula 7 or 8 increases only if the probabilistic
model predicts the reference alignments, regard-
less of whether they are positive or negative.
3.2 Baseline systems
We prepared five state-of-the-art systems of ab-
breviation recognition as baselines: Schwartz
and Hearst?s method (SH) (Schwartz and Hearst,
2003), SaRAD (Adar, 2004), ALICE (Ao and
Takagi, 2005), Chang and Sch?utze?s method
(CS) (Chang and Sch?utze, 2006), and Nadeau and
Turney?s method (NT) (Nadeau and Turney, 2005).
We utilized the implementations available on the
Web for SH
6
, CS
78
, and ALICE
9
, and we repro-
duced SaRAD and NT, based on their papers.
Our implementation of NT consists of a classi-
fier that discriminates between positive (true) and
negative (false) full forms, using all of the feature
functions presented in the original paper. Although
the original paper presented heuristics for gener-
ating full-form candidates, we replaced the candi-
date generator with the function C(x,y), so that
the classifier and our alignment model can receive
the same set of full-form candidates. The classi-
fier of the NT system was modeled by the LIB-
SVM implementation
10
with Radial Basis Func-
6
Abbreviation Definition Recognition Software:
http://biotext.berkeley.edu/software.html
7
Biomedical Abbreviation Server:
http://abbreviation.stanford.edu/
8
We applied a score cutoff of 0.14.
9
Abbreviation LIfter using Corpus-based Extraction:
http://uvdb3.hgc.jp/ALICE/ALICE index.html
10
LIBSVM ? A Library for Support Vector Machines:
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm/
System P R F1
Schwartz & Hearst (SH) .978 .940 .959
SaRAD .891 .919 .905
ALICE .961 .920 .940
Chang & Sch?utze (CS) .942 .900 .921
Nadeau & Turney (NT) .954 .871 .910
Proposed (d = 0; L
1
) .973 .969 .971
Proposed (d = 0; L
2
) .964 .968 .966
Proposed (d = 1; L
1
) .960 .981 .971
Proposed (d = 1; L
2
) .957 .976 .967
Table 3: Performance on our corpus.
tion (RBF) kernel
11
. If multiple full-form can-
didates for an abbreviation are classified as posi-
tives, we choose the candidate that yields the high-
est probability estimate.
3.3 Results
We trained and evaluated the proposed method on
our corpus by performing 10-fold cross valida-
tion
12
. Our corpus includes 13 out of 864 (1.5%)
abbreviation definitions in which the abbreviation
letters are shuffled. Thus, we have examined two
different distortion parameters, d = 0, 1 in this
experiment. The average numbers of candidates
produced by the candidate generator C(x,y) per
instance were 8.46 (d = 0) and 69.1 (d = 1), re-
spectively. The alignment model was trained in a
reasonable execution time
13
, ca. 5 minutes (d = 0)
and 1.5 hours (d = 1).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) on the basis of the number of cor-
rect abbreviation definitions recognized by each
system. The proposed method achieved the best
F1 score (0.971) of all systems. The inclusion of
distorted abbreviations (d = 1) gained the high-
est recall (0.981 with L
1
regularization). Base-
line systems with refined heuristics (SaRAD and
ALICE) could not outperform the simplest sys-
tem (SH). The previous approaches with machine
learning (CS and NT) were roughly comparable to
rule-based methods.
We also evaluated the alignment model on the
Medstract Gold Standard development corpus to
examine the adaptability of the alignment model
trained with our corpus (Table 4). Since the origi-
11
We tuned kernel parameters C = 128 and ? = 2.0 by
using the grid-search tool in the LIBSVM distribution.
12
We determined the regularization parameters as ?
1
= 3
and ?
2
= 3 after testing {0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10} for
the regularization parameters. The difference between the
highest and lowest F1 scores was 1.8%.
13
On Intel Dual-Core Xeon 5160/3GHz CPU, excluding
time for feature generation and data input/output.
662
System P R F1
Schwartz & Hearst (SH) .942 .891 .916
SaRAD .909 .859 .884
ALICE .960 .945 .953
Chang & Sch?utze (CS) .858 .852 .855
Nadeau & Turney (NT) .889 .875 .882
Proposed (d = 1; L
1
) .976 .945 .960
Table 4: Performance on Medstract corpus.
# Atomic function(s) F1
(1) x position + x ctype .905
(2) (1) + x char + y char .885
(3) (1) + x word + x pos .941
(4) (1) + x diff + x diff wd + y diff .959
(5) (1) + y position + y ctype .964
(6) All atomic functions .966
Table 5: Effect of atomic functions (d = 0; L
2
).
nal version of the Medstract corpus includes anno-
tation errors, we used the version revised by Ao
and Takagi (2005). For this reason, the perfor-
mance of ALICE might be over-estimated in this
evaluation; ALICE delivered much better results
than Schwartz & Hearst?s method on this corpus.
The abbreviation alignment model trained with
our corpus (d = 1; L
1
) outperformed the baseline
systems for all evaluation metrics. It is notable that
the model could recognize abbreviation definitions
with shuffled letters, e.g., transfer of single embryo
(SET) and inorganic phosphate (PI), without any
manual tuning for this corpus. In some false cases,
the alignment model yielded incorrect probability
estimates. For example, the probabilities of the
alignments prepubertal bipolarity, bi
:
polarity, and
non-definition (negative) for the abbreviation BP
were computed as 3.4%, 89.6%, and 6.7%, respec-
tively; but the first expression prepubertal bipolar-
ity is the correct definition for the abbreviation.
Table 5 shows F1 scores of the proposed method
trained with different sets of atomic functions. The
baseline setting (1), which built features only with
x position and x ctype functions, gained a 0.905
F1 score; further, adding more atomic functions
generally improves the score. However, the x char
and y char functions decreased the performance
since the alignment model was prone to overfit to
the training data, relying on the existence of spe-
cific letters in the training instances. Interestingly,
the model was flexible enough to achieve a high
performance with four atomic functions (5).
Table 6 demonstrates the ability for our ap-
proach to obtain effective features; the table shows
the top 10 (out of 850,009) features with high
# Feature ?
1 U: x position
0
=H;y ctype
0
=U;y position
0
=H/M 1.7370
2 B: y position
0
=I/y position
0
=I/x diff=1/M-M 1.3470
3 U: x ctype
?1
=L;x ctype
0
=L/S 0.96342
4 B: x ctype
0
=L/x ctype
0
=L/x diff wd=0/M-M 0.94009
5 U: x position
0
=I;x char
1
=?t?/S 0.91645
6 U: x position
0
=H;x pos
0
=NN;y ctype
0
=U/M 0.86786
7 U: x ctype
?1
=S;x
c
type
0
=L;M 0.86474
8 B: x char
0
=?o?/x ctype
0
=L/y diff=0/M-S 0.71262
9 U: x char
?1
=?o?;x ctype
0
=L/M 0.69764
10 B: x position
0
=H/x ctype
0
=U/y diff=1/M-M 0.66418
Table 6: Top ten features with high weights.
weights assigned by the MAP estimation with L
1
regularization. A unigram and bigram features
have prefixes ?U:? and ?B:? respectively; a feature
expresses conditions at s (bigram features only),
conditions at t, and mapping status (match or skip)
separated by ?/? symbols. For example, the #1 fea-
ture associates a letter at the head of a full-form
word with the uppercase letter at the head of its
abbreviation. The #4 feature is difficult to obtain
from manual observations, i.e., the bigram feature
suggests the production of two abbreviation letters
from two lowercase letters in the same word.
4 Conclusion
We have presented a novel approach for recogniz-
ing abbreviation definitions. The task of abbrevi-
ation recognition was successfully formalized as
a sequential alignment problem. We developed
an aligned abbreviation corpus, and obtained fine-
grained features that express the events wherein
a full forum produces an abbreviation letter. The
experimental results showed remarkable improve-
ments and usefulness of the alignment approach
for abbreviation recognition. We expect the use-
fullness of the discriminative model for building
an comprehensible abbreviation dictionary.
Future work would be to model cases in which
a full form yields non-identical letters (e.g., ?one?
? ?1? and ?deficient? ? ?-?), and to demonstrate
this approach with more generic linguistic patterns
(e.g., aka, abbreviated as, etc.). We also plan to
explore a method for training a model with an un-
aligned abbreviation corpus, estimating the align-
ments simultaneously from the corpus.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
663
References
Adar, Eytan. 2004. SaRAD: A simple and robust
abbreviation dictionary. Bioinformatics, 20(4):527?
533.
Andrew, Galen and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Ma-
chine Learning (ICML 2007), pages 33?40.
Ao, Hiroko and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (Coling-ACL 2006), pages 65?72.
Chang, Jeffrey T. and Hinrich Sch?utze. 2006. Abbre-
viations in biomedical text. In Ananiadou, Sophia
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Gaudan, Sylvain, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in Medline. Bioinformatics, 21(18):3658?
3664.
Jain, Alpa, Silviu Cucerzan, and Saliha Azzam. 2007.
Acronym-expansion recognition and ranking on the
web. In Proceedings of the IEEE International Con-
ference on Information Reuse and Integration (IRI
2007), pages 209?214.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning (ICML 2001), pages
282?289.
Liu, Hongfang and Carol Friedman. 2003. Mining ter-
minological knowledge in large biomedical corpora.
In the 8th Pacific Symposium on Biocomputing (PSB
2003), pages 415?426.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL 2005), pages 459?466.
McCallum, Andrew, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proceedings of the 21st Conference on Un-
certainty in Artificial Intelligence (UAI 2005), pages
388?395.
Nadeau, David and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intel-
ligence (AI?2005) (LNAI 3501), page 10 pages.
Nocedal, Jorge. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 295?302.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Build-
ing an abbreviation dictionary using a term recogni-
tion approach. Bioinformatics, 22(24):3089?3095.
Pakhomov, Serguei. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL 2002), pages 160?167.
Park, Youngja and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the 2001 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2001), pages 126?133.
Pustejovsky, James, Jos?e Casta?no, Roser Saur??, Anna
Rumshinsky, Jason Zhang, and Wei Luo. 2002.
Medstract: creating large-scale information servers
for biomedical libraries. In Proceedings of the ACL-
02 workshop on Natural language processing in the
biomedical domain, pages 85?92.
Schwartz, Ariel S. and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB 2003), pages 451?462.
Shimbo, Masashi and Kazuo Hara. 2007. A dis-
criminative learning model for coordinate conjunc-
tions. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 610?619.
Wren, Jonathan D. and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated construc-
tion of comprehensive acronym-definition dictionar-
ies. Methods of Information in Medicine, 41(5):426?
434.
Yu, Hong, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based
approach for automatically disambiguating biomedi-
cal abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
Zhou, Wei, Vetle I. Torvik, and Neil R. Smalheiser.
2006. ADAM: another database of abbreviations in
MEDLINE. Bioinformatics, 22(22):2813?2818.
664
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 753?760
Manchester, August 2008
Shift-Reduce Dependency DAG Parsing 
Kenji Sagae? 
Institute for Creative Technologies 
University of Southern California 
13274 Fiji Way 
Marina del Rey, CA 90292 
sagae@ict.usc.edu 
Jun?ichi Tsujii 
Department of Computer Science 
University of Tokyo 
School of Computer Science 
University of Manchester 
National Center for Text Mining 
tsujii@is.s.u-tokyo.ac.jp 
 
 
Abstract? 
Most data-driven dependency parsing 
approaches assume that sentence struc-
ture is represented as trees. Although 
trees have several desirable properties 
from both computational and linguistic 
perspectives, the structure of linguistic 
phenomena that goes beyond shallow 
syntax often cannot be fully captured by 
tree representations.  We present a pars-
ing approach that is nearly as simple as 
current data-driven transition-based de-
pendency parsing frameworks, but out-
puts directed acyclic graphs (DAGs). We 
demonstrate the benefits of DAG parsing 
in two experiments where its advantages 
over dependency tree parsing can be 
clearly observed: predicate-argument 
analysis of English and syntactic analysis 
of Danish with a representation that in-
cludes long-distance dependencies and 
anaphoric reference links.  
1 Introduction 
Natural language parsing with data-driven de-
pendency-based frameworks has received an in-
creasing amount of attention in recent years 
(McDonald et al, 2005; Buchholz and Marsi, 
2006; Nivre et al, 2006).  Dependency represen-
tations directly reflect word-to-word relation-
                                               
? ? This work was conducted while the author was at 
the Computer Science Department of the University 
of Tokyo. 
 
? 2008.  Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0).  Some rights reserved.  
ships in a dependency graph, where the words in 
a sentence are the nodes, and labeled edges cor-
respond to head-dependent syntactic relations.  In 
addition to being inherently lexicalized, depen-
dency analyses can be generated efficiently and 
have been show to be useful in a variety of prac-
tical tasks, such as question answering (Wang et 
al., 2007), information extraction in biomedical 
text (Erkan et al, 2007; Saetre et al 2007) and 
machine translation (Quirk and Corston-Oliver, 
2006).  
However, despite rapid progress in the devel-
opment of parsers for several languages (Nivre et 
al., 2007) and algorithms for more linguistically 
adequate non-projective structures (McDonald et 
al., 2005; Nivre and Nilsson, 2006), most of the 
current data-driven dependency parsing ap-
proaches are limited to producing only depen-
dency trees, where each word has exactly one 
head.  Although trees have desirable properties 
from both computational and linguistic perspec-
tives, the structure of linguistic phenomena that 
goes beyond shallow syntax often cannot be fully 
captured by tree representations.  Well-known 
linguistically-motivated dependency-based syn-
tactic frameworks, such as Hudson?s Word 
Grammar (Hudson, 1984), recognize that to 
represent phenomena such as relative clauses, 
control relations and other long-distance depen-
dencies, more general graphs are needed.  Hud-
son (2005) illustrates his syntactic framework 
with the analysis shown in figure 1.  In this ex-
ample, the arcs above the sentence correspond to 
a typical dependency tree commonly used in de-
pendency parsing.  It is clear, however, that the 
entire dependency structure is not a tree, but a 
directed acyclic graph (DAG), where words may 
have one or more heads. The arcs below the sen-
tence represent additional syntactic dependencies 
commonly ignored in current dependency pars-
ing approaches that are limited to producing tree 
753
structures.  There are several other linguistic 
phenomena that cannot be represented naturally 
with dependency trees, but can easily be 
represented with dependency DAGs, including 
anaphoric reference and semantically motivated 
predicate-argument relations.  Although there are 
parsing approaches (often referred to as deep 
parsing approaches) that compute DAG depen-
dency structures, this is usually done through 
more complex lexicalized grammar formalisms 
(such as HPSG, CCG and LFG) and unification 
operations with tree-based parsing algorithms. 
We introduce a new data-driven framework 
for dependency parsing that produces dependen-
cy DAGs directly from input strings, in a manner 
nearly as simple as other current transition-based 
dependency parsers (Nivre et al, 2007) produce 
dependency trees.  By moving from tree struc-
tures to DAGs, it is possible to use dependency 
parsing techniques to address a wider range of 
linguistic phenomena beyond surface syntax.  
We show that this framework is effective and 
efficient in analysis of predicate-argument de-
pendencies represented as DAGs, and in syntac-
tic parsing using DAGs that include long-
distance dependencies, gapping dependents and 
anaphoric reference information, in addition to 
surface syntactic dependents. 
Our parsing framework, based on shift-reduce 
dependency parsing, is presented in section 2.  
Experiments and results are presented and dis-
cussed in section 3.  We review related work in 
section 4, and conclude in section 5. 
2 A shift-reduce parsing framework for 
dependency DAGs 
One of the key assumptions in both graph-based 
(McDonald et al, 2005) and transition-based 
(Nivre, 2004; Nivre and Nilsson, 2006) ap-
proaches to data-driven dependency parsing is 
that the dependency structure produced by the 
parser is a tree, where each word has exactly one 
head (except for a single root word, which has no 
head in the sentence).  This assumption, of 
course, has to be abandoned in dependency DAG 
parsing.  McDonald et al (2006) point out that, 
while exact inference is intractable if the tree 
constraints are abandoned in their graph-based 
parsing framework, it is possible to compute 
more general graphs (such as DAGs) using ap-
proximate inference, finding a tree first, and add-
ing extra edges that increase the graph?s overall 
score.  Our approach, in contrast, extends shift-
reduce (transition-based) approaches, finding a 
DAG directly.  Because data-driven shift-reduce 
dependency parsing is based on local decisions 
(informed by rich a rich feature set), the addi-
tional computational cost of computing DAGs 
instead of trees is small in practice, as we will 
show. 
We first describe how the basic shift-reduce 
bottom-up dependency parsing algorithm de-
scribed by Nivre (2004) can be modified to allow 
multiple heads per word.  We then explore the 
same type of modification to Nivre?s arc-eager 
algorithm, which is a variant of the basic shift-
reduce algorithm where arcs can be created at the 
first opportunity.  Like their tree counterparts, 
our algorithms for dependency DAGs produce 
only projective structures, assuming that projec-
tivity for DAGs is defined in much the same way 
as for trees.  Informally, we define a projective 
DAG to be a DAG where all arcs can be drawn 
above the sentence (written sequentially in its 
original order) in a way such that no arcs cross 
and there are no covered roots (although a root is 
not a concept associated with DAGs, we borrow 
the term from trees to denote words with no 
heads in the sentence).  However, non-
projectivity is predictably more wide-spread in 
DAG representations, since there are at least as 
many arcs as in a tree representation, and often 
more, including arcs that represent non-local re-
lationships.  We then discuss the application of 
pseudo-projective transformations (Nivre and 
Nilsson, 2005) and an additional arc-reversing 
transform to dependency DAGs.  Using a shift-
reduce algorithm that allows multiple heads per 
word and pseudo-projective transformations to-
What do you   think we should wait for ? 
r 
x 
x 
x 
x 
x,c 
s 
s 
s s 
o 
r r 
Figure 1: Word Grammar dependency graph 
(Hudson, 2005). Key for edge types: com-
plement (c), object (o), sharer/xcomp (r), 
subject (s), and extractee (x). 
754
gether forms a complete dependency DAG pars-
ing framework. 
2.1 Basic shift-reduce parsing with multiple 
heads 
The basic bottom-up left-to-right dependency 
parsing algorithm described by Nivre (2004) 
keeps a list of tokens (initialized to contain the 
input string) and a stack (initialized to be empty), 
and allows three types of actions: (1) shift, which 
removes the next item from the input list and 
pushes it onto the top of the stack; (2) left-
reduce, which pops the top two items from the 
stack, creates a left-arc between the words they 
represent in the sentence, and push the top item 
(which is now the head of the item previously 
below it) back onto the stack; and (3) right-
reduce, which works in the same way as left-
reduce, but creates a right-arc instead, and push-
es back onto the stack the item that was below 
the top item on the stack (which is now the head 
of the item previously on top of the stack)1.  New 
dependency edges (or arcs) are only created by 
reduce actions, which are constrained so that 
they can only be applied to create a head-
dependent pair where the dependent has already 
found all of its own dependents (if any).  This is 
necessary because once a word is assigned a 
head it is popped from the stack and never visited 
again, since each word has only one head.  This 
constraint, responsible for the parser?s bottom-up 
behavior, should be kept in mind, as it is relevant 
in the design of the multiple-head parsing algo-
rithm below. 
To allow words to have multiple heads, we 
first need to create two new parser actions that 
create dependency arcs without removing the 
dependent from further consideration for being a 
dependent of additional heads.  The first new 
action is left-attach, which creates a left depen-
dency arc attaching the top two items on the 
stack, making the top item the head of the item 
immediately below, as long as a right arc be-
tween the two items does not already exist.  This 
action is similar to left-reduce, except that nei-
ther item is removed from the stack (no reduction 
occurs).  The second new action, right-attach, 
includes one additional final step: first, it creates 
a right dependency arc between the top two items 
on the stack (as long as a left arc between the two 
items does not already exist), making the top 
item a dependent of the item immediately below; 
                                               
1 Like Nivre (2004), we consider the direction of the 
dependency arc to be from the head to the dependent. 
X Y Z 
Initial state: 
(a) Desired output: 
X Y Z 
Input tokens Stack 
Action: SHIFT 
Y Z 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: LEFT-ATTACH 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: LEFT-REDUCE 
Z 
Input tokens Stack 
X 
Current arcs: X      Y     Z 
Action: LEFT-REDUCE 
Z 
Input tokens Stack 
Current arcs: X      Y     Z 
X Y Z 
Initial state: 
(b) Desired output: 
X Y Z 
Input tokens Stack 
Action: SHIFT 
Y Z 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: RIGHT-ATTACH 
Y 
Input tokens Stack 
X 
Current arcs: X      Y     Z 
Action: RIGHT-REDUCE 
X 
Input Stack 
Current arcs: X      Y     Z 
Z 
Z 
Action:  SHIFT 
X 
Input tokens Stack 
Current arcs: X      Y     Z 
Z 
Action: RIGHT-REDUCE 
Z 
Input tokens Stack 
Current arcs: X      Y     Z 
Figure 2: Example of how the basic algorithm 
builds dependencies with multiple heads. 
755
and, as a second step, it pops the top item on the 
stack (newly made a dependent), and places it 
back on the list of input words.  This second step 
is necessary because of the constraint that words 
can only be made dependents once all of its own 
dependents have been found.  The behavior of 
the algorithm is illustrated in figure 2, where (a) 
shows an application of left-attach, and (b) 
shows an application of right-attach.  In (b), we 
note that without placing the dependent in the 
right-attach action (Z) back on the input list, the 
dependency between X and Y could not be 
created.  If we abandon the algorithm?s bottom-
up behavior, it is possible to modify the parser 
actions so that it is not necessary to place items 
back in the input list.  This is discussed in section 
2.2. 
In summary, the algorithm has each of the 
three actions from the tree-based algorithm (shift, 
right-reduce, and left-reduce), and two additional 
actions that allow words to be dependents of 
more than one head (right-attach and left-attach).  
Although the algorithm as described so far builds 
unlabeled structures, the extension to labeled 
structures is straightforward: any action that re-
sults in a new arc being created must also choose 
a label for the arc.  Another way to accomplish 
the same goal is to have a copy of each arc-
producing action for each possible arc label.  
This is the same labeling extension as in the al-
gorithm for trees.  Finally, we note that the algo-
rithm does not explicitly prevent multiple arcs 
(with the same direction) from being created be-
tween the same two words.  In the unlabeled 
case, such a constraint can be easily placed on 
arc-producing actions.  In the labeled case, how-
ever, it is useful to allow arcs with different la-
bels to link the same two words2. 
2.2 Arc-eager shift-reduce parsing with 
multiple heads 
Nivre?s arc-eager algorithm was designed to 
build dependencies at the first opportunity, 
avoiding situations where items that form a chain 
of right arcs all have to be placed on the stack 
before any structure is built, as in figure 2(b) for 
example.  This is done by creating dependencies 
not between the top two items on the stack, but 
between the single top item on the stack and the 
next word on the input list, resulting in a hybrid 
                                               
2 This means that the structures produced by the algo-
rithm are technically not limited to projective DAGs, 
since they can also be projective labeled multi-
digraphs. 
bottom-up/top-down strategy.  A similar idea can 
result in an algorithm for dependencies that allow 
multiple heads per word, but in this case the re-
sulting algorithm is not as similar to the arc-
eager algorithm for trees as the algorithm in sec-
tion 2.1 is to its tree-based counterpart. 
The projective DAG arc-eager algorithm has 
four actions, each corresponding to one action of 
the tree-based algorithm, but only the shift action 
is the same as in the tree based algorithm.  The 
four actions in the new algorithm are: (1) shift, 
which removes the next token from the input 
string and pushes it onto the top of the stack; (2) 
reduce, which pops the stack, removing only its 
top item, as long as that item has at least one 
head (unlike in the tree-based algorithm, howev-
er, the algorithm may not reduce immediately 
when an item that has a head is on the top of the 
stack); (3) left-arc, which creates a left depen-
dency arc between the word on top of the stack 
and the next token in the input string, where the 
token in the input string is the head and the item 
on the stack is the dependent (the stack and input 
list are left untouched), as long as a right arc does 
not already exist between the two words; and (4) 
right-arc, which creates a right dependency arc 
between the word on top of the stack and the 
next token in the input list, where the item on the 
stack is the head and the token in the input list is 
the dependent (again, the stack and input list are 
left untouched), as long as a left arc does not al-
ready exist between the two words. 
Like the algorithm in section 2.1, this algo-
rithm can easily be extended to produce labeled 
structures, and it also allows multiple edges (with 
the same direction) between the same two words. 
2.3 Graph transformations for DAGs 
Although the algorithms presented in sections 2.1 
and 2.2 can produce dependency structures 
where a word may have more than one head, 
they are of limited interest on their own, since 
they can only produce projective structures, and 
many of the interesting linguistic phenomena that 
can be represented with DAGs cannot be 
represented with projective DAGs.  Fortunately, 
the pseudo-projective transformations (Nivre and 
Nilsson, 2006) used in tree-based dependency 
parsing can easily be applied to DAGs.  These 
transformations consist of identifying specific 
non-projective arcs, and moving their heads up 
towards the root, making them projective.  The 
process also involves creating markings on the 
labels of the edges involved, so that the trans-
formations are (mostly) reversible.  Because non-
756
projectivity is more common in linguistically 
interesting DAGs, however, the trans-
form/detransform process may be more lossy 
than it is when applied to trees.  This, of course, 
varies according to specific DAGs used for 
representing specific phenomena.  For pseudo-
transformations to work well, we must allow 
multiple differently labeled arcs between the 
same two words (which, as mentioned before, the 
algorithms do).  Combining the algorithm in sec-
tions 2.1 or 2.2 with pseudo-projective parsing, 
we can use DAG training data and produce DAG 
output in the overall parsing framework. 
An alternative to using pseudo-projective 
transformations is to develop an algorithm for 
DAG parsing based on the family of algorithms 
described by Covington (2001), in the same way 
the algorithms in sections 2.1 and 2.2 were de-
veloped based on the algorithms described by 
Nivre (2004).  Although this may be straightfor-
ward, a potential drawback of such an approach 
is that the number of parse actions taken in a Co-
vington-style algorithm is always quadratic on 
the length of the input sentence, resulting in 
parsers that are more costly to train and to run 
(Nivre, 2007).  The algorithms presented here, 
however, behave identically to their linear run-
time tree counterparts when they are trained with 
graphs that are limited to tree structures.  Addi-
tional actions are necessary only when words 
with more than one head are encountered.  For 
data sets where most words have only one head, 
the performance the algorithms described in sec-
tions 2.1 and 2.2 should be close to that of shift-
reduce projective parsing for dependency trees.  
In data sets where most words have multiple 
heads (resulting in higher arc density), the use of 
a Covington-style algorithm may be advanta-
geous, but this is left as an area of future investi-
gation. 
In addition to pseudo-projective transforma-
tions, an additional transformation that is useful 
in DAG parsing is arc reversal.  This consists of 
simply reversing the direction of an edge, adding 
a special mark to its label to indicate that its di-
rection has been reversed.  Detransformation is 
trivial and can be done with perfect accuracy, 
since it can be accomplished by simply reversing 
the arcs marked as reversed.  This transformation 
is useful in cases where structures are mostly in 
DAG form, but may sometimes contain cycles.  
Arc reversal can be used to change the direction 
of an arc in the cycle, making the previously cyc-
lic structure a DAG, which can be handled in the 
framework presented here. 
3 Experiments 
To investigate the efficacy of our DAG parsing 
framework on natural language data annotated 
with dependency DAGs, we conducted two expe-
riments.  The first uses predicate-argument de-
pendencies taken from the HPSG Treebank built 
by Miyao et al (2004) from the WSJ portion of 
the Penn Treebank.  These predicate-argument 
structures are, in general, dependency graphs that 
do contain cycles (although infrequently), and 
also contain a large number of words with mul-
tiple heads.  Since the predicate-argument de-
pendencies are annotated explicitly in the HPSG 
Treebank, extracting a corpus of gold-standard 
dependency graphs is trivial.  The second expe-
riment uses the Danish Dependency Treebank, 
developed by Kromann (2003).  This treebank 
follows a dependency scheme that includes, in 
addition to standard grammatical relations com-
monly used in dependency parsing, long-distance 
dependencies, gapping dependents, and anaphor-
ic reference links.  As with the HPSG predicate 
argument data, a few structures in the data con-
tain cycles, but most of the structures in the tree-
bank are DAGs.  In the experiments presented 
below, the algorithm described in section 2.1 was 
used.  We believe the use of the arc-eager algo-
rithm described in section 2.2 would produce 
similar results, but this is left as future work. 
3.1 Learning component 
The DAG parsing framework, as described so 
far, must decide when to apply each appropriate 
parser action.  As with other data-driven depen-
dency parsing approaches with shift-reduce algo-
rithms, we use a classifier to make these deci-
sions.  Following the work of Sagae and Tsujii 
(2007), we use maximum entropy models for 
classification.  During training, the DAGs are 
first projectivized with pseudo-projective trans-
formations.  They are then processed by the pars-
ing algorithm, which records each action neces-
sary to build the correct structure in the training 
data, along with their corresponding parser con-
figurations (stack and input list contents).  From 
each of these parser configurations, a set of fea-
tures is extracted and used with the correct pars-
ing action as a training example for the maxi-
mum entropy classifier.  The specific features we 
used in both experiments are the same features 
described by Sagae and Tsujii, with the follow-
ing two changes: (1) the addition of a feature that 
indicates whether an arc already exists between 
the top two items on the stack, or the top item on 
757
the stack and the next item on the input list, and 
if so, what type of arc (direction and label); and 
(2) we did not use lemmas, morphological in-
formation or coarse grained part-of-speech tags.  
For the complete list of features used, please see 
(Sagae and Tsujii, 2007). 
During run-time, the classifier is used to de-
termine the parser action according to the current 
parser configuration.  Like Sagae and Tsujii, we 
use a beam search instead of running the algo-
rithm in deterministic mode, although we also 
report deterministic parsing results. 
3.2 Predicate-argument analysis 
The predicate-argument dependencies extracted 
from the HPSG Treebank include information 
such as extraction, raising, control, and other 
long-distance dependencies.  Unlike in structures 
from PropBank, predicate-argument information 
is provided for nearly all words in the data.  Fol-
lowing previous experiments with Penn Tree-
bank WSJ data, or data derived from it, we used 
sections 02-21 as training material, section 22 for 
development, and section 23 for testing.  Only 
the predicate-argument dependencies were used, 
not the phrase structures or other information 
from the HPSG analyses.  Part-of-speech tagging 
was done separately using a maximum entropy 
tagger (Tsuruoka and Tsujii, 2005) with accuracy 
of 97.1%. 
Cycles were eliminated from the dependency 
structures using the arc reversal transform in the 
following way: for each cycle detected in the 
data, the shortest arc in the cycle was reversed 
until no cycles remained.  We applied pseudo-
projective transformation and detransformation 
to determine how much information is lost in this 
process.  By detransforming the projective 
graphs generated from gold-standard dependen-
cies, we obtain labeled precision of 98.1% and 
labeled recall of 97.7%, which is below the accu-
racy expected for detransformation of syntactic 
dependency trees, but still within a range we 
considered acceptable.  This represents an upper-
bound for the accuracy of the DAG parser (in-
cluding the arc-reversal and pseudo-projective 
transformations, and the algorithm described in 
section 2.1). 
Table 1 shows the results obtained with our 
DAG parsing framework in terms of labeled pre-
cision, recall and F-score (89.0, 88.5 and 88.7, 
respectively).  For comparison, we also show 
previously published results obtained by Miyao 
and Tsujii (2005), and Sagae et al (2007), which 
used the same data, but obtained the predicate-
argument analyses using an HPSG parser.  Our 
results are very competitive, at roughly the same 
level as the best previously published results on 
this data set, but obtained with significantly 
higher speed.  The parser took less than four mi-
nutes to process the test set, and pseudo-
projective and arc-reversal detransformation took 
less than one minute in standard hardware (a Li-
nux workstation with a Pentium 4 processor and 
4Gb of RAM).  Sagae et al (2007) reported that 
an HPSG parser took about 20 minutes to parse 
the same data.  Our results were obtained with a 
beam width of 150 parser states.  Running the 
parser with a beam width of 1 (a single parser 
state), emulating the deterministic search used by 
Nivre (2004), resulted in numerous parse failures 
(the end of the input string is reached, and no 
further dependency arcs are created) in the de-
velopment set, and therefore very low dependen-
cy recall (90.1 precision and 36.2 recall on de-
velopment data).  Finally, in table 1 we also 
show results obtained with standard bottom-up 
shift-reduce dependency parsing for trees, using 
the parser described in (Sagae and Tsujii, 2007).  
To train the dependency tree parser, we trans-
formed the DAG predicate-argument structures 
into trees by removing arcs.  Arcs were selected 
for removal as follows: for each word that had 
more than one head, only the arc between the 
word and its closest head (in linear distance in 
the sentence) was kept.  Although this strategy 
still produces dependency analyses with relative-
ly high F-score (87.0), recall is far lower than 
when DAG parsing is used, and the tree parser 
has no mechanism for capturing some of the 
structures captured by the DAG parser.  
 
Parser Precision Recall F-score 
DAG-beam 89.0 88.5 88.7 
Tree only 89.8 84.3 87.0 
Sagae et al 88.5 88.0 88.2 
Miyao & Tsujii 85.0 84.3 84.6 
 
Table 1: Results from experiments with HPSG 
predicate-argument dependencies (labeled preci-
sion, recall and F-score).  Our results are denoted 
by DAG-beam and tree only, and others are pre-
viously published results using the same data. 
3.3 Danish Dependency Treebank experi-
ments 
Our experiments with the Danish Dependency 
Treebank followed the same setup as described 
for the HPSG predicate-argument structures.  
The accuracy of pseudo-projective transforma-
758
tion and detransformation was higher, at 99.4% 
precision and 98.8% recall.  To divide the data 
into training, development and test sections, we 
followed the same procedure as McDonald et al 
(2006), who used the same data, so our results 
could be compared directly (a small number of 
graphs that contained cycles was discarded, as 
done by McDonald et al). 
Our results are shown in table 2 (unlabeled 
precision and recall are used, for comparison 
with previous work, in addition to labeled preci-
sion and recall), along with the results obtained 
by McDonald et al, who used an approximate 
inference strategy in a graph-based dependency 
parsing framework, where a dependency tree is 
computed first, and arcs that improve on the 
overall graph score are added one by one.  As in 
the previous section, we also include results ob-
tained with tree-only parsing.  Obtaining tree 
structures from the Danish Dependency Tree-
bank is straightforward, since anaphoric refer-
ence and long-distance dependency arcs are 
marked as such explicitly and can be easily re-
moved. 
In addition to overall results, we also meas-
ured the parser?s precision and recall on long-
distance dependencies and anaphoric reference.  
On long-distance dependencies the parser had 
83.2 precision and 82.0 recall.  On anaphoric 
reference links the parser has 84.9 precision and 
84.4 recall.  Although these are below the pars-
er?s overall accuracy figures, they are encourag-
ing results.  Finally, unlike with the HPSG predi-
cate-argument structures, using a beam width of 
1 reduces precision and recall by only about 1.5. 
 
Parser Precision Recall F-score 
DAG-beam 87.3 87.1 87.2 
Tree only 87.5 82.7 85.0 
McDonald et al 86.2 84.9 85.6 
DAG-labeled 82.7 82.2 82.4 
 
Table 2: Results from experiments with the 
Danish Dependency Treebank.  Precision, recall 
and F-score for the first three rows are for unla-
beled dependencies. The last row, DAG-labeled, 
shows our results in labeled precision, recall and 
F-score (not directly comparable to other rows). 
 
4 Related work 
The work presented here builds on the dependen-
cy parsing work of Nivre (2004), as discussed in 
section 2, on the work of Nivre and Nilsson 
(2006) on pseudo-projective transformations, and 
on the work of Sagae and Tsujii (2007) in using a 
beam search in shift-reduce dependency parsing 
using maximum entropy classifiers.  As men-
tioned before, McDonald et al (2006) presented 
an approach to DAG parsing (that could also eas-
ily be applied to cyclic structures) using approx-
imate inference in an edge-factored dependency 
model starting from dependency trees.  In their 
model, the addition of extra arcs to the tree was 
learned with the parameters to build the initial 
tree itself, which shows the power and flexibility 
of approximate inference in graph-based depen-
dency models. 
Other parsing approaches that produce depen-
dency graphs that are not limited to tree struc-
tures include those based on linguistically-
motivated lexicalized grammar formalisms, such 
as HPSG, CCG and LFG.  In particular, Clark et 
al. (2002) use a probabilistic model of dependen-
cy DAGs extracted from the CCGBank (Hock-
enmeier and Steedman, 2007) in a CCG parser 
that builds the CCG predicate-argument depen-
dency structures following the CCG derivation, 
not directly through DAG parsing.  Similarly, the 
HPSG parser of Miyao and Tsujii (2005) builds 
the HPSG predicate-argument dependency struc-
ture following unification operations during 
HPSG parsing.  Sagae et al (2007) use a depen-
dency parsing combined with an HPSG parser to 
produce predicate-argument dependencies.  
However, the dependency parser is used only to 
produce a dependency tree backbone, which the 
HPSG parser then uses to produce the more gen-
eral dependency graph.  A similar strategy is 
used in the RASP parser (Briscoe et al, 2006), 
which builds a dependency graph through unifi-
cation operations performed during a phrase 
structure tree parsing process. 
5 Conclusion 
We have presented a framework for dependency 
DAG parsing, using a novel algorithm for projec-
tive DAGs that extends existing shift-reduce al-
gorithms for parsing with dependency trees, and 
pseudo-projective transformations applied to 
DAG structures. 
We have demonstrated that the parsing ap-
proach is effective in analysis of predicate-
argument structure in English using data from the 
HPSG Treebank (Miyao et al, 2004), and in 
parsing of Danish using a rich dependency repre-
sentation (Kromann, 2003). 
759
Acknowledgements 
We thank Yusuke Miyao and Takuya Matsuzaki 
for insightful discussions.  This work was partial-
ly supported by Grant-in-Aid for Specially Pro-
moted Research (MEXT, Japan). 
References 
Briscoe, T., Carroll, J. and Watson, R. 2006. The 
second release of the RASP system. In Proceed-
ings of the COLING/ACL-06 Demo Session.  
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X 
Shared Task on Multilingual Dependency Parsing.  
In Proceedings of the 10th Conference on Compu-
tational Natural Language Learning (CoNLL-X) 
Shared Task session. 
Clark, Stephen, Julia Hockenmaier, and Mark Steed-
man. 2002. Building Deep Dependency Structures 
using a Wide-Coverage CCG Parser. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing.  In Proceedings of 
the Annual ACM Southeast Conference, 95-102. 
Erkan, Gunes, Arzucan Ozgur, and Dragomir R. Ra-
dev. 2007. Semisupervised classification for ex-
tracting protein interaction sentences using depen-
dency parsing. In Proceedings of  CoNLL-EMNLP. 
Hudson, Richard. 1984. Word Grammar. Oxford: 
Blackwell. 
Hudson, Richard. 2005. Word Grammar. In K. Brown 
(Ed.), Encyclopedia of Language and Linguistics 
(second ed., pp 633-642). Elsevier. 
Hockenmaier, Julia and Mark Steedman. 2007. 
CCGbank: a corpus of CCG derivations and de-
pendency structures extracted from the Penn Tree-
bank. In Computational Linguistics 33(3), pp 355-
396, MIT press. 
Kromann, Matthias T. 2003. The Danish dependency 
treebank and the underlying linguistic theory. In 
Proceedings of the Second Workshop on Treebanks 
and Linguistic Theories (TLT). 
McDonald, Ryan and Fernando Pereira. 2006. Online 
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of 
the European Chapter of the Association for Com-
putational Linguistics (EACL). 
McDonald, Ryan, Fernando Pereira, Kiril Ribarov and 
Jan Hajic. 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of the Human Language Technology Confe-
rence and Conference on Empirical Methods in 
Natural Language Processing (HLT-EMNLP). 
Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development 
for acquiring a Head-driven Phrase Structure 
Grammar from the Penn Treebank. In Proceedings 
of the International Joint Conference on Natural 
Language Processing (IJCNLP).  
Miyao Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics. 
Nivre, Joakim. 2004. Incrementality in Deterministic 
Dependency Parsing. In Incremental Parsing: 
Bringing Engineering and Cognition Together 
(Workshop at ACL-2004).  
Nivre, Joakim, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson, Sebastian Riedel, Deniz 
Yuret. 2007. In Proceedings of the CoNLL 2007 
Shared Task in the Joint Conference on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning.  
Nivre, Joakim. and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL), pp. 99-106. 
Nivre, Joakim. 2007. Incremental non-projective de-
pendency parsing.  In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics (NAACL-HLT?07). 
Saetre, R., Sagae, K., and Tsujii, J. 2007. Syntactic 
features for protein-protein interaction extraction.  
In Proceedings of the International Symposium on 
Languages in Biology and Medicine (LBM short 
oral presentations). 
Sagae, Kenji., Yusuke Miyao Jun?ichi and Tsujii. 
2007. HPSG Parsing with shallow dependency 
constraints. In Proceedings of the 44th Meeting of 
the Association for Computational Linguistics. 
Sagae, K., Tsujii, J. 2007. Dependency parsing and 
domain adaptation with LR models and parser en-
sembles. In Proceedings of the CoNLL 2007 
Shared Task. in EMNLP-CoNLL. 
Tsuruoka, Yoshimasa and Tsujii, Jun?ichi. 2005. Bidi-
rectional inference with the easiest-first strategy for 
tagging sequence data. In Proceedings of the Hu-
man Language Technology Conference and Confe-
rence on Empirical Methods in Natural Language 
Processing (HLT-EMNLP), pp. 523-530. 
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of 
the Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
760
Coling 2008: Companion volume ? Posters and Demonstrations, pages 127?130
Manchester, August 2008
Building a Bilingual Lexicon Using Phrase-based
Statistical Machine Translation via a Pivot Language
Takashi Tsunakawa? Naoaki Okazaki? Jun?ichi Tsujii??
?Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo 7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan
?School of Computer Science, University of Manchester / National Centre for Text Mining
131 Princess Street, Manchester, M1 7DN, UK
{tuna, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper proposes a novel method for
building a bilingual lexicon through a pivot
language by using phrase-based statisti-
cal machine translation (SMT). Given two
bilingual lexicons between language pairs
L
f
?L
p
and L
p
?L
e
, we assume these lexi-
cons as parallel corpora. Then, we merge
the extracted two phrase tables into one
phrase table between L
f
and L
e
. Fi-
nally, we construct a phrase-based SMT
system for translating the terms in the lex-
icon L
f
?L
p
into terms of L
e
and, ob-
tain a new lexicon L
f
?L
e
. In our experi-
ments with Chinese-English and Japanese-
English lexicons, our system could cover
72.8% of Chinese terms and drastically im-
prove the utilization ratio.
1 Introduction
The bilingual lexicon is a crucial resource for mul-
tilingual applications in natural language process-
ing including machine translation (Brown et al,
1990) and cross-lingual information retrieval (Nie
et al, 1999). A number of bilingual lexicons have
been constructed manually, despite their expensive
compilation costs. However, it is unrealistic to
build a bilingual lexicon for every language pair;
thus, comprehensible bilingual lexicons are avail-
able only for a limited number of language pairs.
One of the solutions is to build a bilingual lex-
icon of the source language L
f
and the target L
e
through a pivot language L
p
, when large bilingual
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
lexicons L
f
?L
p
and L
p
?L
e
are available. Numer-
ous researchers have explored the use of pivot lan-
guages (Tanaka and Umemura, 1994; Schafer and
Yarowsky, 2002; Zhang et al, 2005). This ap-
proach is advantageous because we can obtain a
bilingual lexicon between L
e
and L
f
, even if no
bilingual lexicon exists between these languages.
Pivot-based methods for dictionary construction
may produce incorrect translations when the word
w
e
is translated from a word w
f
by a polysemous
pivot word w
p
1
. Previous work addressed the poly-
semy problem in pivot-based methods (Tanaka and
Umemura, 1994; Schafer and Yarowsky, 2002).
Pivot-based methods also suffer from a mismatch
problem, in which a pivot word w
p
from a source
word w
f
does not exist in the bilingual lexicon L
p
?
L
e
2
. Moreover, a bilingual lexicon for technical
terms is prone to include a number of pivot terms
that are not included in another lexicon.
This paper proposes a method for building a
bilingual lexicon through a pivot language by us-
ing phrase-based statistical machine translation
(SMT) (Koehn et al, 2003). We build a transla-
tion model between L
f
and L
e
by assuming two
lexicons L
f
?L
p
and L
p
?L
e
as parallel corpora, in
order to increase the obtained lexicon size by han-
dling multi-word expressions appropriately. The
main advantage of this method is its ability to in-
corporate various translation models that associate
languages L
f
?L
e
; for example, we can further im-
prove the translation model by integrating a small
bilingual lexicon L
f
?L
e
.
1A Japanese term ????: dote, embankment, may be as-
sociated with a Chinese term ???,? y??ngha?ng: banking in-
stitution, using the pivot word bank in English.
2It is impossible to associate two translation pairs (???
??? (chikyu?-ondanka),? global warming), and (global heat-
ing, ????? (qua?nqiu?-bia`nnua?n)?) because of the differ-
ence in English (pivot) terms.
127
Lf-Lp lexicon Le-Lp lexicon
Lf-Lp translation 
phrase table
Le-Lp translation 
phrase table
Lf-Le translation 
phrase table
Phrase-based
SMT system
Word alignment &
grow-diag-final method
Merging phrase tables
Additional
features
INPUT OUTPUT
Le: translations of 
Lf-Lp lexicon
Figure 1: Framework of our approach
2 Merging two bilingual lexicons
We introduce phrase-based SMT for merging the
lexicons, in order to improve both the merged
lexicon size and its accuracy. Recently, several
researchers proposed the use of the pivot lan-
guage for phrase-based SMT (Utiyama and Isa-
hara, 2007; Wu and Wang, 2007). We employ a
similar approach for obtaining phrase translations
with the translation probabilities by assuming the
bilingual lexicons as parallel corpora. Figure 1 il-
lustrates the framework of our approach.
Let us suppose that we have two bilingual lex-
icons L
f
?L
p
and L
p
?L
e
. We obtain word align-
ments of these lexicons by applying GIZA++ (Och
and Ney, 2003), and grow-diag-final heuristics
(Koehn et al, 2007). Let w?
x
be a phrase that
represents a sequence of words in the language
L
x
. For phrase pairs (w?
p
, w?
f
) and (w?
e
, w?
p
), the
translation probabilities p(w?
p
|w?
f
) and p(w?
e
|w?
p
)
are computed using the maximum likelihood esti-
mation from the co-occurrence frequencies, con-
sistent with the word alignment in the bilingual
lexicons. We calculate the direct translation prob-
abilities between source and target phrases,
p(w?
e
|w?
f
) =
?
w?
p
p(w?
e
|w?
p
)p(w?
p
|w?
f
)
?
w?
?
e
?
w?
p
p(w?
?
e
|w?
p
)p(w?
p
|w?
f
)
. (1)
We employ the log-linear model of phrase-based
SMT (Och and Ney, 2002) for translating the
source term w?
f
in the lexicon L
f
?L
p
into the tar-
get language by finding a term ?w?
e
that maximizes
the translation probability,
?
w?
e
= argmax
w?
e
Pr(w?
e
|w?
f
)
= argmax
w?
e
M
?
m=1
?
m
h
m
(w?
e
, w?
f
), (2)
where we have M feature functions h
m
(w?
e
, w?
f
)
and model parameters ?
m
.
In addition to the typical features for the SMT
framework, we introduce two features: character-
based similarity, and additional bilingual lexicon.
We define a character-based similarity feature,
h
char sim
(w?
e
, w?
f
) = 1 ?
ED(w?
e
, w?
f
)
max(w?
e
, w?
f
)
, (3)
where ED(x, y) represents a Levenshtein distance
of characters between the two terms x and y3. We
also define an additional bilingual lexicon feature,
h
add lex
(w?
e
, w?
f
) =
?
i
log p
?
(w?
(i)
e
|w?
(i)
f
), (4)
where w?(i)
e
and w?(i)
f
represent an i-th translated
phrase pair on the term pair (w?
e
, w?
f
) during the
decoding, and p?(w?(i)
e
|w?
(i)
f
) represents the phrase
translation probabilities derived from the addi-
tional lexicon. The probability p?(w?(i)
e
|w?
(i)
f
) is cal-
culated using the maximum likelihood estimation.
3 Experiment
3.1 Data
For building a Chinese-to-Japanese lexicon, we
used the Japanese-English lexicon released by
JST4 (527,206 term pairs), and the Chinese-
English lexicon compiled by Wanfang Data5
(525,259 term pairs). Both cover a wide range
of named entities and technical terms that may
not be included in an ordinary dictionary. As an
additional lexicon, we used the Japanese-English-
Chinese trilingual lexicon6 (596,967 term pairs)
generated from EDR7 Japanese-English lexicon.
We lower-cased and tokenized all terms by the
following analyzers: JUMAN8 for Japanese, the
MEMM-based POS tagger9 for English, and cjma
(Nakagawa and Uchimoto, 2007) for Chinese.
3.2 The sizes and coverage of merged lexicons
Table 1 shows the distinct numbers of terms in
the original and merged lexicons, and the uti-
3We regard the different shapes of Han characters between
Chinese and Japanese as identical in our experiments.
4Japan Science and Technology Agency (JST)
http://pr.jst.go.jp/others/tape.html
5http://www.wanfangdata.com/
6This data was manually compiled by NICT, Japan.
7http://www2.nict.go.jp/r/r312/EDR/index.html
8http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
9http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/
128
Lexicon L
C
size L
E
size L
J
size
L
C
?L
E
375,990 429,807 -
L
E
?L
J
- 418,044 465,563
L
E
(distinct) - 783,414 -
Additional lex. 94,928 - 90,605
Exact matching 98,537 68,996 103,437
(26.2%) (22.2%)
Unique matching 4,875 4,875 4,875
(1.3%) (1.0%)
Table 1: The statistics of lexicons
lization ratio10 in the parentheses. For compari-
son, we prepared two baseline systems for build-
ing Chinese-Japanese lexicons. Exact matching
connects source and target terms that share at
least one common translation term in the pivot
language. Unique matching is an extreme ap-
proach for avoiding negative effects of polysemous
pivot terms: it connects source and target terms if
source, pivot, and target terms appear only once in
the corresponding lexicons.
Exact matching achieved 26.2% of the utiliza-
tion ratio in Japanese-to-Chinese translation, and
22.2% in Chinese-to-Japanese translation. These
figures imply that about 75% of the terms remained
unused in building the Japanese-Chinese lexicon.
With unique matching, as little as 1% of Japanese
and Chinese terms could be used. In contrast, our
method could cover 72.8% of Chinese terms by
generating Japanese terms, which was a drastic im-
provement in the utilization ratio.
3.3 Generating Japanese translations of the
Chinese-English lexicon
For evaluating the correctness of the merged lex-
icons, we assumed the lexicon generated by the
unique matching as a development/test set. Devel-
opment and test sets consist of about 2,400 term
pairs, respectively. Next, we input Chinese terms
in the development/test set into our system based
on Moses (Koehn et al, 2007), and obtained the
Japanese translations. We evaluated the perfor-
mance by using BLEU, NIST, and accuracy mea-
sures. Table 2 shows the evaluation results on the
test set. Our system could output correct trans-
lations for 68.5% of 500 input terms. The table
also reports that additional features were effective
in improving the performance.
We also conducted another experiment to gen-
erate Japanese translations for Chinese terms in-
cluded in an external resource. We randomly ex-
10The number of terms in the original lexicon used for
building the merged lexicon.
Features BLEU NIST Acc.
Typical features 0.4519 7.4060 0.676
w/ character similarity 0.4670 7.4963 0.682
w/ additional lexicon 0.4800 7.5907 0.674
All 0.4952 7.7046 0.685
Table 2: Translation performance on the test set
Features/Models Prec1 Prec10 MRR
Typical features 0.142 0.232 0.1719
w/ character similarity 0.136 0.224 0.1654
w/ additional lexicon 0.140 0.230 0.1704
All 0.140 0.230 0.1714
E-to-J translation 0.090 0.206 0.1256
Table 3: Evaluation results for the Eijiro dictionary
tracted 500 Chinese-English term pairs from the
Wanfang Data lexicon, for which the English term
cannot be mapped by the JST lexicon, but can be
mapped by another lexicon Eijiro11. Table 3 shows
the results for these 500 terms. Prec1 or Prec10 are
the precisions that the 1- or 10-best translations in-
clude the correct one, respectively. MRR (mean
reciprocal rank) is (1/500)?
i
(1/r
i
), where r
i
is
the highest rank of the correct translations for the
i-th term.
Since the input lexicons are Chinese-English
term pairs, their Japanese translations can be gen-
erated directly from the English terms by applying
an English-Chinese translation system. We com-
pared our system to an English-Japanese phrase-
based SMT system (E-to-J translation), con-
structed from the JST Japanese-English lexicon.
Table 3 shows that our system outperformed the
English-to-Japanese direct translation system.
Table 4 displays translation examples. The first
example shows that our system could output a cor-
rect translation (denoted by [T]); and the E-to-J
system failed to translate the source term ([F]),
because it could not reorder the source English
words and translate the word pubis correctly. In
the second example, our system could reproduce
Chinese characters ??? (fluid)?, but the E-to-J
system output a semantically acceptable but awk-
ward Japanese term. In the last example, the word
segmentation of the source Chinese term was in-
correct (???? (lumber)?? (lymph)?? is cor-
rect). Thus, our system received an invalid word ?
??? and could not find a translation for the word.
11http://www.eijiro.jp/
129
English Chinese Japanese (Eijiro) Japanese (C-to-J) Japanese (E-to-J)
symphysis pubis ???? ???? ???? [T] ??? (symphysis shame) [F]
ideal fluid dy-
namics
?? ??
???
?????? ?????? [T] ??? (fluid)?? [F]
intermediate
lumbar lymph
nodes
?? ??
??
??????? ?? ? ?? (inter-
mediate node [lumbar-
lymph]
INVALID
) [F]
??????? [T]
Table 4: Translation examples on Eijiro dictionary
4 Conclusion
This paper proposed a novel method for building a
bilingual lexicon by using a pivot language. Given
two bilingual lexicons L
f
?L
p
and L
p
?L
e
, we con-
structed a phrase-based SMT system from L
f
?L
e
by merging the lexicons into a phrase translation
table L
f
?L
e
. The experimental results demon-
strated that our method improves the utilization ra-
tio of given lexicons drastically. We also showed
that the pivot approach was more effective than the
SMT system that translates from L
p
to L
e
directly.
The future direction would be to introduce other
resources such as the parallel corpora and other
pivot languages into the SMT system for improv-
ing the precision and the coverage of the obtained
lexicon. We are also planning on evaluating a ma-
chine translation system that integrates our model.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan).
References
Brown, Peter F., John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demonstration ses-
sion, pages 177?180.
Nakagawa, Tetsuji and Kiyotaka Uchimoto. 2007. Hy-
brid approach to word segmentation and POS tag-
ging. In Companion Volume to the Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 217?220.
Nie, Jian-Yun, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language informa-
tion retrieval based on parallel texts and automatic
mining of parallel texts from the Web. In Proc. of
the 22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 74?81.
Och, Franz Josef and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Schafer, Charles and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proc. of the 6th Conference
on Natural Language Learning, volume 20, pages 1?
7.
Tanaka, Kumiko and Kyoji Umemura. 1994. Construc-
tion of a bilingual dictionary intermediated by a third
language. In Proc. of the 15th International Confer-
ence on Computational Linguistics, pages 297?303.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proc. of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 484?491.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proc. of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 856?
863.
Zhang, Yujie, Qing Ma, and Hitoshi Isahara. 2005.
Construction of a Japanese-Chinese bilingual dictio-
nary using English as an intermediary. International
Journal of Computer Processing of Oriental Lan-
guages, 18(1):23?39.
130
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447?456,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Discriminative Candidate Generator for String Transformations
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka?
yoshimasa.tsuruoka@manchester.ac.uk
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
String transformation, which maps a source
string s into its desirable form t?, is related
to various applications including stemming,
lemmatization, and spelling correction. The
essential and important step for string trans-
formation is to generate candidates to which
the given string s is likely to be transformed.
This paper presents a discriminative approach
for generating candidate strings. We use sub-
string substitution rules as features and score
them using an L1-regularized logistic regres-
sion model. We also propose a procedure to
generate negative instances that affect the de-
cision boundary of the model. The advantage
of this approach is that candidate strings can
be enumerated by an efficient algorithm be-
cause the processes of string transformation
are tractable in the model. We demonstrate
the remarkable performance of the proposed
method in normalizing inflected words and
spelling variations.
1 Introduction
String transformation maps a source string s into its
destination string t?. In the broad sense, string trans-
formation can include labeling tasks such as part-
of-speech tagging and shallow parsing (Brill, 1995).
However, this study addresses string transformation
in its narrow sense, in which a part of a source string
is rewritten with a substring. Typical applications of
this task include stemming, lemmatization, spelling
correction (Brill and Moore, 2000; Wilbur et al,
2006; Carlson and Fette, 2007), OCR error correc-
tion (Kolak and Resnik, 2002), approximate string
matching (Navarro, 2001), and duplicate record de-
tection (Bilenko and Mooney, 2003).
Recent studies have formalized the task in the dis-
criminative framework (Ahmad and Kondrak, 2005;
Li et al, 2006; Chen et al, 2007),
t? = argmax
t?gen(s)
P (t|s). (1)
Here, the candidate generator gen(s) enumerates
candidates of destination (correct) strings, and the
scorer P (t|s) denotes the conditional probability of
the string t for the given s. The scorer was modeled
by a noisy-channel model (Shannon, 1948; Brill and
Moore, 2000; Ahmad and Kondrak, 2005) and max-
imum entropy framework (Berger et al, 1996; Li et
al., 2006; Chen et al, 2007).
The candidate generator gen(s) also affects the
accuracy of the string transformation. Previous stud-
ies of spelling correction mostly defined gen(s),
gen(s) = {t | dist(s, t) < ?}. (2)
Here, the function dist(s, t) denotes the weighted
Levenshtein distance (Levenshtein, 1966) between
strings s and t. Furthermore, the threshold ? requires
the distance between the source string s and a can-
didate string t to be less than ?.
The choice of dist(s, t) and ? involves a tradeoff
between the precision, recall, and training/tagging
speed of the scorer. A less restrictive design of these
factors broadens the search space, but it also in-
creases the number of confusing candidates, amount
of feature space, and computational cost for the
scorer. Moreover, the choice is highly dependent on
the target task. It might be sufficient for a spelling
447
correction program to gather candidates from known
words, but a stemmer must handle unseen words ap-
propriately. The number of candidates can be huge
when we consider transformations from and to un-
seen strings.
This paper addresses these challenges by explor-
ing the discriminative training of candidate genera-
tors. More specifically, we build a binary classifier
that, when given a source string s, decides whether
a candidate t should be included in the candidate set
or not. This approach appears straightforward, but it
must resolve two practical issues. First, the task of
the classifier is not only to make a binary decision
for the two strings s and t, but also to enumerate a
set of positive strings for the string s,
gen(s) = {t | predict(s, t) = 1}. (3)
In other words, an efficient algorithm is necessary
to find a set of strings with which the classifier
predict(s, t) yields positive labels for the string s.
Another issue arises when we prepare a training
set. A discriminative model requires a training set
in which each instance (pair of strings) is annotated
with a positive or negative label. Even though some
existing resources (e.g., inflection table and query
log) are available for positive instances, such re-
sources rarely contain negative instances. Therefore,
we must generate negative instances that are effec-
tive for discriminative training.
To address the first issue, we design features that
express transformations from a source string s to its
destination string t. Feature selection and weight-
ing are performed using an L1-regularized logistic
regression model, which can find a sparse solution
to the classification model. We also present an al-
gorithm that utilizes the feature weights to enumer-
ate candidates of destination strings efficiently. We
deal with the second issue by generating negative
instances from unlabeled instances. We describe a
procedure to choose negative instances that affect
the decision boundary of the classifier.
This paper is organized as follows. Section 2 for-
malizes the task of the candidate generator as a bi-
nary classification modeled by logistic regression.
Features for the classifier are designed using the
rules of substring substitution. Therefore, we can
obtain, efficiently, candidates of destination strings
and negative instances for training. Section 3 re-
ports the remarkable performance of the proposed
method in various applications including lemmati-
zation, spelling normalization, and noun derivation.
We briefly review previous work in Section 4, and
conclude this paper in Section 5.
2 Candidate generator
2.1 Candidate classification model
In this section, we first introduce a binary classifier
that yields a label y ? {0, 1} indicating whether a
candidate t should be included in the candidate set
(1) or not (0), given a source string s. We express
the conditional probability P (y|s, t) using a logistic
regression model,
P (1|s, t) =
1
1 + exp (??TF (s, t))
, (4)
P (0|s, t) = 1? P (1|s, t). (5)
In these equations, F = {f1, ..., fK} denotes a vec-
tor of the Boolean feature functions; K is the num-
ber of feature functions; and ? = {?1, ..., ?K}
presents a weight vector of the feature functions.
We obtain the following decision rule to choose
the most probable label y? for a given pair ?s, t?,
y? = argmax
y?{0,1}
P (y|s, t) =
{
1
(
?TF (s, t) > 0
)
0 (otherwise)
.
(6)
Finally, given a source string s, the generator func-
tion gen(s) is defined to collect all strings to which
the classifier assigns positive labels:
gen(s) = {t | P (1|s, t) > P (0|s, t)}
= {t | ?TF (s, t) > 0}. (7)
2.2 Substitution rules as features
The binary classifier can include any arbitrary fea-
ture. This is exemplified by the Levenshtein dis-
tance and distributional similarity (Lee, 1999) be-
tween two strings s and t. These features can im-
prove the classification accuracy, but it is unrealistic
to compute these features for every possible string,
as in equation 7. For that reason, we specifically
examine substitution rules, with which the process
448
^oestrogen$
^estrogen$
^anaemia$
^anemia$
^studies$
^study$
('o', ''), ('^o', '^'), ('oe', 'e'),
('^oe', '^e'), ('^oes', '^es'), ...
('a', ''), ('na', 'n'), ('ae', 'e'),
('ana', 'an'), ('nae', 'ne'), ('aem', 'em'),
...
('ies', 'y'), ('dies', 'dy'), ('ies$', 'y$'),
('udies', 'udy'), ('dies$', 'dy$'), ...
S:
t:
S:
t:
S:
t:
(1)
(2)
(3)
Figure 1: Generating substitution rules.
of transforming a source string s into its destination
form t is tractable.
In this study, we assume that every string has a
prefix ??? and postfix ?$?, which indicate the head
and tail of a string. A substitution rule r = (?, ?)
replaces every occurrence of the substring ? in a
source string into the substring ?. Assuming that a
string s can be transformed into another string twith
a single substitution operation, substitution rules ex-
press the different portion between strings s and t.
Equation 8 defines a binary feature function with
a substitution rule between two strings s and t,
fk(s, t) =
{
1 (rule rk can convert s into t)
0 (otherwise)
.
(8)
We allow multiple substitution rules for a given pair
of strings. For instance, substitution rules (?a?,
??), (?na?, ?n?), (?ae?, ?e?), (?nae?, ?ne?), etc.
form feature functions that yield 1 for strings s =
??anaemia$? and t = ??anemia$?. Equation
6 produces a decision based on the sum of feature
weights, or scores of substitution rules, representing
the different portions between s and t.
Substitution rules for the given two strings s and
t are obtained as follows. Let l denote the longest
common prefix between strings s and t, and r the
longest common postfix. We define cs as the sub-
string in s that is not covered by the longest common
prefix l and postfix r, and define ct for t analogously.
In other words, strings s and t are divided into three
regions, lcsr and lctr, respectively. For strings s =
??anaemia$? and t = ??anemia$? in Figure 1
(2), we obtain cs = ?a? and ct = ?? because l =
??an? and r = ?emia$?.
Because substrings cs and ct express different
portions between strings s and t, we obtain the mini-
mum substitution rule (cs, ct), which can convert the
string s into t by replacing substrings cs in s with
ct; the minimum substitution rule for the same ex-
ample is (?a?, ??). However, replacing letters ?a?
in ??anaemia$? into empty letters does not pro-
duce the correct string ??anemia$? but ??nemi$?.
Furthermore, the rule might be inappropriate for ex-
pressing string transformation because it always re-
moves the letter ?a? from every string.
Therefore, we also obtain expanded substitution
rules, which insert postfixes of l to the head of min-
imum substitution rules, and/or append prefixes of
r to the rules. For example, we find an expanded
substitution rule (?na?, ?n?), by inserting a postfix
of l = ??an? to the head of the minimum substitu-
tion rule (?a?, ??); similarly, we obtain an expanded
substitution rule (?ae?, ?e?), by appending a prefix
of r = ?emia$? to the tail of the rule (?a?, ??).
Figure 1 displays examples of substitution rules
(the right side) for three pairs of strings (the left
side). Letters in blue, green, and red respectively
represent the longest common prefixes, longest com-
mon postfixes, and different portions. In this study,
we expand substitution rules such that the number of
letters in rules is does not pass a threshold ?1.
2.3 Parameter estimation
Given a training set that consists of N instances,
D =
(
(s(1), t(1), y(1)), ..., (s(N), t(N), y(N))
)
, we
optimize the feature weights in the logistic regres-
sion model by maximizing the log-likelihood of the
conditional probability distribution,
L? =
N?
i=1
logP (y(i)|s(i), t(i)). (9)
The partial derivative of the log-likelihood with re-
spect to a feature weight ?k is given as equation 10,
?L?
??k
=
N?
i=1
{
y(i) ? P (1|s(i), t(i))
}
fk(s
(i), t(i)).
(10)
The maximum likelihood estimation (MLE) is
known to suffer from overfitting the training set. The
1The number of letters for a substitution rule r = (?, ?) is
defined as the sum of the quantities of letters in ? and ?, i.e.,
|?|+ |?|. We determined the threshold ? = 12 experimentally.
449
common approach for addressing this issue is to use
the maximum a posteriori (MAP) estimation, intro-
ducing a regularization term of the feature weights
?, i.e., a penalty on large feature weights. In addi-
tion, the generation algorithm of substitution rules
might produce inappropriate rules that transform a
string incorrectly, or overly specific rules that are
used scarcely. Removing unnecessary substitution
rules not only speeds up the classifier but also the
algorithm for candidate generation, as presented in
Section 2.4.
In recent years, L1 regularization has received in-
creasing attention because it produces a sparse so-
lution of feature weights in which numerous fea-
ture weights are zero (Tibshirani, 1996; Ng, 2004).
Therefore, we regularize the log-likelihood with the
L1 norm of the weight vector ? and define the final
form the objective function to be minimized as
E? = ?L? +
|?|
?
. (11)
Here, ? is a parameter to control the effect of L1
regularization; the smaller the value we set to ?,
the more features the MAP estimation assigns zero
weights to: it removes a number of features from the
model. Equation 11 is minimized using the Orthant-
Wise Limited-memory Quasi-Newton (OW-LQN)
method (Andrew and Gao, 2007) because the second
term of equation 11 is not differentiable at ?k = 0.
2.4 Candidate generation
The advantage of our feature design is that we can
enumerate strings to which the classifier is likely to
assign positive labels. We start by observing the nec-
essary condition for t in equation 7,
?TF (s, t) > 0? ?k : fk(s, t) = 1 ? ?k > 0.
(12)
The classifier might assign a positive label to strings
s and t when at least one feature function whose
weight is positive can transform s to t.
Let R+ be a set of substitution rules to which
MAP estimation has assigned positive feature
weights. Because each feature corresponds to a sub-
stitution rule, we can obtain gen(s) for a given string
s by application of every substitution rule r ? R+,
gen(s) = {r(s) | r ? R+ ??TF (s, r(s)) > 0}.
(13)
Input: s = (s1, ..., sl): an input string s (series of letters)
Input: D: a trie dictionary containing positive features
Output: T : gen(s)
T = {};1
U = {};2
foreach i ? (1, ..., |s|) do3
F ? D.prefix search(s, i);4
foreach f ? F do5
if f /? U then6
t? f .apply(s);7
if classify(s, t) = 1 then8
add t to T ;9
end10
add f to U ;11
end12
end13
end14
return T ;15
Algorithm 1: A pseudo-code for gen(s).
Here, r(s) presents the string to which the substitu-
tion rule r transforms the source string s. We can
compute gen(s) with a small computational cost if
the MAP estimation with L1 regularization reduces
the number of active features.
Algorithm 1 represents a pseudo-code for obtain-
ing gen(s). To search for positive substitution rules
efficiently, the code stores a set of rules in a trie
structure. In line 4, the code obtains a set of positive
substitution rules F that can rewrite substrings start-
ing at offset #i in the source string s. For each rule
f ? F , we obtain a candidate string t by application
of the substitution rule f to the source string s (line
7). The candidate string t is qualified to be included
in gen(s) when the classifier assigns a positive label
to strings s and t (lines 8 and 9). Lines 6 and 11 pre-
vent the algorithm from repeating evaluation of the
same substitution rule.
2.5 Generating negative instances
The parameter estimation requires a training set D
in which each instance (pair of strings) is annotated
with a positive or negative label. Negative instances
(counter examples) are essential for penalizing in-
appropriate substitution rules, e.g. (?a?, ??). Even
though some existing resources (e.g. verb inflection
table) are available for positive instances, such re-
sources rarely contain negative instances.
A common approach for handling this situation
is to assume that every pair of strings in a resource
450
Input: D+ = [(s1, t1), ..., (sl, tl)]: positive instances
Input: V : a suffix array of all strings (vocabulary)
Output: D?: negative instances
Output: R: substitution rules (features)
D? = [];1
R = {};2
foreach d ? D+ do3
foreach r ? features(d) do4
add r to R;5
end6
end7
foreach r ? R do8
S ? V .search(r.src);9
foreach s ? S do10
t? r.apply(s);11
if (s, t) /? D+ then12
if t ? V then13
append (s, t) to D?;14
end15
end16
end17
end18
return D?, R;19
Algorithm 2: Generating negative instances.
is a negative instance; however, negative instances
amount to ca. V (V ? 1)/2, where V represents the
total number of strings. Moreover, substitution rules
expressing negative instances are innumerable and
sparse because the different portions are peculiar to
individual negative instances. For instance, the min-
imum substitution rule for unrelated words anaemia
and around is (?naemia?, ?round?), but the rule
cannot be too specific to generalize the conditions
for other negative instances.
In this study, we generate negative instances so
that they can penalize inappropriate rules and settle
the decision boundary of the classifier. This strat-
egy is summarized as follows. We consider every
pair of strings as candidates for negative instances.
We obtain substitution rules for the pair using the
same algorithm as that described in Section 2.2 if a
string pair is not included in the dictionary (i.e., not
in positive instances). The pair is used as a nega-
tive instance only when any substitution rule gener-
ated from the pair also exists in the substitution rules
generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-
ments the strategy for generating negative instances
efficiently. First, we presume that we have positive
instances D+ = [(s1, t1), ..., (sl, tl)] and unlabeled
Table Description # Entries
LRSPL Spelling variants 90,323
LRNOM Nominalizations (derivations) 14,029
LRAGR Agreement and inflection 910,854
LRWD Word index (vocabulary) 850,236
Table 1: Excerpt of tables in the SPECIALIST Lexicon.
Data set # + # - # Rules
Orthography 15,830 33,296 11,098
Derivation 12,988 85,928 5,688
Inflection 113,215 124,747 32,278
Table 2: Characteristics of datasets.
strings V . For example, positive instance D+ repre-
sent orthographic variants, and unlabeled strings V
include all possible words (vocabulary). We insert
the vocabulary into a suffix array, which is used to
locate every occurrence of substrings in V .
The algorithm first generates substitution rules R
only from positive instances D+ (lines 3 to 7). For
each substitution rule r ? R, we enumerate known
strings S that contain the source substring r.src (line
9). We apply the substitution rule to each string s ?
S and obtain its destination string t (line 11). If the
pair of strings ?s, t? is not included in D+ (line 12),
and if the destination string t is known (line 13), the
substitution rule r might associate incorrect strings
s and t, which do not exist in D+. Therefore, we
insert the pair to the negative set D? (line 14).
3 Evaluation
3.1 Experiments
We evaluated the candidate generator using three
different tasks: normalization of orthographic vari-
ants, noun derivation, and lemmatization. The
datasets for these tasks were obtained from the
UMLS SPECIALIST Lexicon2, a large lexicon that
includes both commonly occurring English words
and biomedical vocabulary. Table 1 displays the list
of tables in the SPECIALIST Lexicon that were used
in our experiments. We prepared three datasets, Or-
thography, Derivation, and Inflection.
The Orthography dataset includes spelling vari-
ants (e.g., color and colour) in the LRSPL table. We
2UMLS SPECIALIST Lexicon:
http://specialist.nlm.nih.gov/
451
chose entries as positive instances in which spelling
variants are caused by (case-insensitive) alphanu-
meric changes3. The Derivation dataset was built di-
rectly from the LRNOM table, which includes noun
derivations such as abandon ? abandonment. The
LRAGR table includes base forms and their inflec-
tional variants of nouns (singular and plural forms),
verbs (infinitive, third singular, past, past participle
forms, etc), and adjectives/adverbs (positive, com-
parative, and superlative forms). For the Inflection
dataset, we extracted the entries in which inflec-
tional forms differ from their base forms4, e.g., study
? studies.
For each dataset, we applied the algorithm de-
scribed in Section 2.5 to generate substitution rules
and negative instances. Table 2 shows the number of
positive instances (# +), negative instances (# -), and
substitution rules (# Rules). We evaluated the per-
formance of the proposed method in two different
goals of the tasks: classification (Section 3.2) and
normalization (Section 3.3).
3.2 Experiment 1: Candidate classification
In this experiment, we measured the performance
of the classification task in which pairs of strings
were assigned with positive or negative labels.
We trained and evaluated the proposed method
by performing ten-fold cross validation on each
dataset5. Eight baseline systems were prepared
for comparison: Levenshtein distance (LD), nor-
malized Levenshtein distance (NLD), Dice coef-
ficient on letter bigrams (DICE) (Adamson and
Boreham, 1974), Longest Common Substring Ra-
tio (LCSR) (Melamed, 1999), Longest Common
Prefix Ratio (PREFIX) (Kondrak, 2005), Porter?s
stemmer (Porter, 1980), Morpha (Minnen et al,
2001), and CST?s lemmatiser (Dalianis and Jonge-
3LRSPL table includes trivial spelling variants that can be
handled using simple character/string operations. For example,
the table contains spelling variants related to case sensitivity
(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).
4LRAGR table also provides agreement information even
when word forms do not change. For example, the table con-
tains an entry indicating that the first-singular present form of
the verb study is study, which might be readily apparent to En-
glish speakers.
5We determined the regularization parameter ? = 5 experi-
mentally. Refer to Figure 2 for the performance change.
jan, 2006)6.
The five systems LD, NLD, DICE, LCSR, and
PREFIX employ corresponding metrics of string
distance or similarity. Each system assigns a posi-
tive label to a given pair of strings ?s, t? if the dis-
tance/similarity of strings s and t is smaller/larger
than the threshold ? (refer to equation 2 for distance
metrics). The threshold of each system was chosen
so that the system achieves the best F1 score.
The remaining three systems assign a positive la-
bel only if the system transforms the strings s and
t into the identical string. For example, a pair of
two words studies and study is classified as positive
by Porter?s stemmer, which yields the identical stem
studi for these words. We trained CST?s lemmatiser
for each dataset to obtain flex patterns that are used
for normalizing word inflections.
To examine the performance of the L1-
regularized logistic regression as a discriminative
model, we also built two classifiers based on the
Support Vector Machine (SVM). These SVM
classifiers were implemented by the SVMperf 7 on
a linear kernel8. An SVM classifier employs the
same feature set (substitution rules) as the proposed
method so that we can directly compare the L1-
regularized logistic regression and the linear-kernel
SVM. Another SVM classifier incorporates the five
string metrics; this system can be considered as our
reproduction of the discriminative string similarity
proposed by Bergsma and Kondrak (2007).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) based on the number of correct de-
cisions for positive instances. The proposed method
outperformed the baseline systems, achieving 0.919,
0.888, and 0.984 of F1 scores, respectively. Porter?s
stemmer worked on the Inflection set, but not on
the Orthography set, which is beyond the scope of
the stemming algorithms. CST?s lemmatizer suf-
fered from low recall on the Inflection set because
it removed suffixes of base forms, e.g., (cloning,
clone) ? (clone, clo). Morpha and CST?s lemma-
6We used CST?s lemmatiser version 2.13:
http://www.cst.dk/online/lemmatiser/uk/
index.html
7SVM for Multivariate Performance Measures (SVMperf ):
http://svmlight.joachims.org/svm_perf.html
8We determined the parameter C = 500 experimentally; it
controls the tradeoff between training error and margin.
452
System Orthography Derivation Inflection
P R F1 P R F1 P R F1
Levenshtein distance (? = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565
Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646
Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673
Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645
LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645
PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645
Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881
Morpha (Minnen et al, 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902
CST?s lemmatiser (Dalianis et al 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290
Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984
Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983
+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983
Table 3: Performance of candidate classification
Rank Src Dst Weight Examples
1 uss us 9.81 focussing
2 aev ev 9.56 mediaeval
3 aen en 9.53 ozaena
4 iae$ ae$ 9.44 gadoviae
5 nni ni 9.16 prorennin
6 nne ne 8.84 connexus
7 our or 8.54 colour
8 aea ea 8.31 paean
9 aeu eu 8.22 stomodaeum
10 ooll ool 7.79 woollen
Table 4: Feature weights for the Orthography set
tizer were not designed for orthographic variants and
noun derivations.
Levenshtein distance (? = 1) did not work for
the Derivation set because noun derivations often
append two or more letters (e.g., happy ? happi-
ness). No string similarity/distance metrics yielded
satisfactory results. Some metrics obtained the best
F1 scores with extreme thresholds only to classify
every instance as positive. These results imply the
difficulty of the string metrics for the tasks.
The L1-regularized logistic regression was com-
parable to the SVM with linear kernel in this exper-
iment. However, the presented model presents the
advantage that it can reduce the number of active
features (features with non-zero weights assigned);
the L1 regularization can remove 74%, 48%, and
82% of substitution rules in each dataset. The
performance improvements by incorporating string
metrics as features were very subtle (less than 0.7%).
What is worse, the distance/similarity metrics do not
specifically derive destination strings to which the
classifier is likely to assign positive labels. There-
fore, we can no longer use the efficient algorithm
as a candidate generator (in Section 2.4) with these
features.
Table 4 demonstrates the ability of our approach
to obtain effective features; the table shows the top
10 features with high weights assigned for the Or-
thography data. An interesting aspect of the pro-
posed method is that the process of the orthographic
variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for
the Inflection data when we change the number of
active features (x-axis) by controlling the regular-
ization parameter ? from 0.001 to 100. The larger
the value we set for ?, the better the classifier per-
forms, generally, with more active features. In ex-
treme cases, the number of active features drops to
97 with ? = 0.01; nonetheless, the classifier still
achieves 0.961 of the F1 score. The result suggests
that a small set of substitution rules can accommo-
date most cases of inflectional variations.
3.3 Experiment 2: String transformation
The second experiment examined the performance
of the string normalization tasks formalized in equa-
tion 1. In this task, a system was given a string s and
was required to yield either its transformed form t?
(s 6= t?) or the string s itself when the transforma-
tion is unnecessary for s. The conditional probabil-
ity distribution (scorer) in equation 1 was modeled
453
System Orthography Derivation Inflection XTAG morph 1.5
P R F1 P R F1 P R F1 P R F1
Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688
CST?s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587
Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827
Table 5: Performance of string transformation
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0 1000 2000 3000 4000 5000 6000 7000
F1 s
core
Number of active features (with non-zero weights)
Spelling variation
Figure 2: Number of active features and performance.
by the maximum entropy framework. Features for
the maximum entropy model consist of: substitution
rules between strings s and t, letter bigrams and tri-
grams in s, and letter bigrams and trigrams in t.
We prepared four datasets, Orthography, Deriva-
tion, Inflection, and XTAG morphology. Each
dataset is a list of string pairs ?s, t? that indicate
the transformation of the string s into t. A source
string s is identical to its destination string t when
string s should not be changed. These instances
correspond to the case where string s has already
been lemmatized. For each string pair (s, t) in LR-
SPL9, LRNOM, and LRAGR tables, we generated
two instances ?s, t? and ?t, t?. Consequently, a sys-
tem is expected to leave the string t unchanged. We
also used XTAG morphology10 to perform a cross-
domain evaluation of the lemmatizer trained on the
Inflection dataset11. The entries in XTAG morphol-
9We define that s precedes t in dictionary order.
10XTAG morphology database 1.5:
ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.
5/morph-1.5.tar.gz
11We found that XTAG morphology contains numerous in-
ogy that also appear in the Inflection dataset were
39,130 out of 317,322 (12.3 %). We evaluated
the proposed method and CST?s lemmatizer by per-
forming ten-fold cross validation.
Table 5 reports the performance based on the
number of correct transformations. The proposed
method again outperformed the baseline systems
with a wide margin. It is noteworthy that the pro-
posed method can accommodate morphological in-
flections in the XTAG morphology corpus with no
manual tuning or adaptation.
Although we introduced no assumptions about
target tasks (e.g. a known vocabulary), the aver-
age number of positive substitution rules relevant
to source strings was as small as 23.9 (in XTAG
morphology data). Therefore, the candidate gen-
erator performed 23.9 substitution operations for a
given string. It applied the decision rules (equa-
tion 7) 21.3 times, and generated 1.67 candidate
strings per source string. The experimental results
described herein demonstrated that the candidate
generator was modeled successfully by the discrim-
inative framework.
4 Related work
The task of string transformation has a long history
in natural language processing and information re-
trieval. As described in Section 1, this task is re-
lated closely to various applications. Therefore, we
specifically examine several prior studies that are
relevant to this paper in terms of technical aspects.
Some researchers have reported the effectiveness
of the discriminative framework of string similarity.
MaCallum et al (2005) proposed a method to train
the costs of edit operations using Conditional Ran-
dom Fields (CRFs). Bergsma and Kondrak (2007)
correct comparative and superlative adjectives, e.g., unpopular
? unpopularer ? unpopularest and refundable ? refundabler
? refundablest. Therefore, we removed inflection entries for
comparative and superlative adjectives from the dataset.
454
presented an alignment-based discriminative string
similarity. They extracted features from substring
pairs that are consistent to a character-based align-
ment of two strings. Aramaki et al (2008) also used
features that express the different segments of the
two strings. However, these studies are not suited for
a candidate generator because the processes of string
transformations are intractable in their discrimina-
tive models.
Dalianis and Jongejan (2006) presented a lem-
matiser based on suffix rules. Although they pro-
posed a method to obtain suffix rules from a training
data, the method did not use counter-examples (neg-
atives) for reducing incorrect string transformations.
Tsuruoka et al (2008) proposed a scoring method
for discovering a list of normalization rules for dic-
tionary look-ups. However, their objective was to
transform given strings, so that strings (e.g., studies
and study) referring to the same concept in the dic-
tionary are mapped into the same string (e.g., stud);
in contrast, this study maps strings into their destina-
tion strings that were specified by the training data.
5 Conclusion
We have presented a discriminative approach for
generating candidates for string transformation.
Unlike conventional spelling-correction tasks, this
study did not assume a fixed set of destination
strings (e.g. correct words), but could even generate
unseen candidate strings. We used anL1-regularized
logistic regression model with substring-substitution
features so that candidate strings for a given string
can be enumerated using the efficient algorithm. The
results of experiments described herein showed re-
markable improvements and usefulness of the pro-
posed approach in three tasks: normalization of or-
thographic variants, noun derivation, and lemmati-
zation.
The method presented in this paper allows only
one region of change in string transformation. A
natural extension of this study is to handle mul-
tiple regions of changes for morphologically rich
languages (e.g. German) and to handle changes
at the phrase/term level (e.g., ?estrogen receptor?
and ?receptor of oestrogen?). Another direction
would be to incorporate the methodologies for semi-
supervised machine learning to accommodate situa-
tions in which positive instances and/or unlabeled
strings are insufficient.
Acknowledgments
This work was partially supported by Grants-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and for Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character struc-
ture to identify semantically related pairs of words and
document titles. Information Storage and Retrieval,
10(7-8):253?260.
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 955?962.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Machine
Learning (ICML 2007), pages 33?40.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), pages 48?55.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
656?663.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining (KDD 2003), pages 39?48.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on the As-
sociation for Computational Linguistics (ACL 2000),
pages 286?293.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
455
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the Sixth International Conference on
Machine Learning and Applications (ICMLA 2007),
pages 166?171.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 181?189.
Hercules Dalianis and Bart Jongejan. 2006. Hand-
crafted versus machine-learned inflectional rules: The
euroling-siteseeker stemmer and cst?s lemmatiser. In
In Proceedings of the 6th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 663?666.
Okan Kolak and Philip Resnik. 2002. OCR error correc-
tion using a noisy channel model. In Proceedings of
the second international conference on Human Lan-
guage Technology Research (HLT 2002), pages 257?
262.
Grzegorz Kondrak. 2005. Cognates and word alignment
in bitexts. In Proceedings of the Tenth Machine Trans-
lation Summit (MT Summit X), pages 305?312.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 1999),
pages 25?32.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association for
Computational Linguistics (Coling-ACL 2006), pages
1025?1032.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI 2005), pages 388?395.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys (CSUR),
33(1):31?88.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning (ICML 2004), pages 78?85.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27(3):379?423.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-
niadou. 2008. Normalizing biomedical terms by min-
imizing ambiguity and variability. BMC Bioinformat-
ics, Suppl 3(9):S2.
W. John Wilbur, Won Kim, and Natalie Xie. 2006.
Spelling correction in the PubMed search engine. In-
formation Retrieval, 9(5):543?564.
456
Feature Forest Models for Probabilistic
HPSG Parsing
Yusuke Miyao?
University of Tokyo
Jun?ichi Tsujii??
University of Tokyo
University of Manchester
Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit com-
plicated data structures, such as typed feature structures. This prevents us from applying
common methods of probabilistic modeling in which a complete structure is divided into sub-
structures under the assumption of statistical independence among sub-structures. For example,
part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing
is split into applications of CFG rules. These methods have relied on the structure of the target
problem, namely lattices or trees, and cannot be applied to graph structures including typed fea-
ture structures.
This article proposes the feature forest model as a solution to the problem of probabilistic
modeling of complex data structures including typed feature structures. The feature forest model
provides a method for probabilistic modeling without the independence assumption when prob-
abilistic events are represented with feature forests. Feature forests are generic data structures
that represent ambiguous trees in a packed forest structure. Feature forest models are maximum
entropy models defined over feature forests. A dynamic programming algorithm is proposed for
maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of
any data structures is possible when they are represented by feature forests.
This article also describes methods for representing HPSG syntactic structures and
predicate?argument structures with feature forests. Hence, we describe a complete strategy for
developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is
empirically evaluated through parsing experiments on the Penn Treebank, and the promise of
applicability to parsing of real-world sentences is discussed.
1. Introduction
Following the successful development of wide-coverage lexicalized grammars (Riezler
et al 2000; Hockenmaier and Steedman 2002; Burke et al 2004; Miyao, Ninomiya, and
? Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
E-mail: yusuke@is.s.u-tokyo.ac.jp.
?? Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
E-mail: tsujii@is.s.u-tokyo.ac.jp.
Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication:
5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.
This is because natural language processing applications usually require disambiguated
or ranked parse results, and statistical modeling of syntactic/semantic preference is one
of the most promising methods for disambiguation.
The focus of this article is the problem of probabilistic modeling of wide-coverage
HPSG parsing. Although previous studies have proposed maximum entropy mod-
els (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen,
Toutanova, et al 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003;
Malouf and van Noord 2004), the straightforward application of maximum entropy
models to wide-coverage HPSG parsing is infeasible because estimation of maximum
entropymodels is computationally expensive, especially when targeting wide-coverage
parsing. In general, complete structures, such as transition sequences inMarkovmodels
and parse trees, have an exponential number of ambiguities. This causes an exponential
explosion when estimating the parameters of maximum entropy models. We therefore
require solutions to make model estimation tractable.
This article first proposes feature forest models, which are a general solution to
the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).
Our algorithm avoids exponential explosion by representing probabilistic events with
feature forests, which are packed representations of tree structures. When complete
structures are represented with feature forests of a tractable size, the parameters of
maximum entropy models are efficiently estimated without unpacking the feature
forests. This is due to dynamic programming similar to the algorithm for computing
inside/outside probabilities in PCFG parsing.
The latter half of this article (Section 4) is on the application of feature forest
models to disambiguation in wide-coverage HPSG parsing. We describe methods for
representing HPSG parse trees and predicate?argument structures using feature forests
(Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the
parameter estimation algorithm for feature forest models, these methods constitute a
complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.
The methods we propose here were applied to an English HPSG parser, Enju (Tsujii
Laboratory 2004). We report on an extensive evaluation of the parser through parsing
experiments on theWall Street Journal portion of the Penn Treebank (Marcus et al 1994).
The content of this article is an extended version of our earlier work reported
in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The
major contribution of this article is a strict mathematical definition of the feature forest
model and the parameter estimation algorithm, which are substantially refined and
extended from Miyao and Tsujii (2002). Another contribution is that this article thor-
oughly discusses the relationships between the feature forest model and its application
to HPSG parsing. We also provide an extensive empirical evaluation of the resulting
HPSG parsing approach using real-world text.
Section 2 discusses a problem of conventional probabilistic models for lexicalized
grammars. Section 3 proposes feature forest models for solving this problem. Section 4
describes the application of feature forest models to probabilistic HPSG parsing. Sec-
tion 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6
introduces research related to our proposals. Section 7 concludes.
2. Problem
Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now be-
coming the de facto standard approach for disambiguation models for lexicalized or
36
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
feature structure grammars (Johnson et al 1999; Riezler et al 2000, 2002; Geman and
Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al 2004; Carroll and Oepen
2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al 2002;
Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord
2004) have also adopted log-linear models. This is because these grammar formalisms
exploit feature structures to represent linguistic constraints. Such constraints are known
to introduce inconsistencies in probabilistic models estimated using simple relative
frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable
choice for credible probabilistic models. It also allows various overlapping features to
be incorporated, and we can expect higher accuracy in disambiguation.
A maximum entropy model gives a probabilistic distribution that maximizes the
likelihood of training data under given feature functions. Given training data E =
{?x, y?}, a maximum entropy model gives conditional probability p(y|x) as follows.
Definition 1 (Maximum entropy model)
A maximum entropy model is defined as the solution of the following optimization
problem.
pM(y|x) = argmax
p
?
?
?
?
?
?x,y??E
p?(x, y) log p(y|x)
?
?
?
where:
p(y|x) = 1
Z(x)
exp
(
?
i
?i fi(x, y)
)
Z(x) =
?
y?Y(x)
exp
(
?
i
?i fi(x, y)
)
In this definition, p?(x, y) is the relative frequency of ?x, y? in the training data. fi is a
feature function, which represents a characteristic of probabilistic events by mapping
an event into a real value. ?i is the model parameter of a corresponding feature function
fi, and is determined so as to maximize the likelihood of the training data (i.e., the
optimization in this definition). Y(x) is a set of y for given x; for example, in parsing, x is
a given sentence and Y(x) is a parse forest for x. An advantage of maximum entropy
models is that feature functions can represent any characteristics of events. That is,
independence assumptions are unnecessary for the design of feature functions. Hence,
this method provides a principled solution for the estimation of consistent probabilistic
distributions over feature structure grammars.
The remaining issue is how to estimate parameters. Several numerical algorithms,
such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved
Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limited-
memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright
1999), have been proposed for parameter estimation. Although the algorithm proposed
in the present article is applicable to all of the above algorithms, we used L-BFGS for
experiments.
However, a computational problem arises in these parameter estimation algo-
rithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally
37
Computational Linguistics Volume 34, Number 1
very large. This is because local ambiguities in parse trees potentially cause exponential
growth in the number of structures assigned to sub-sequences of words, resulting in
billions of structures for whole sentences. For example, when we apply rewriting rule
S ? NP VP, and the left NP and the right VP, respectively, have n and m ambiguous
subtrees, the result of the rule application generates n?m trees.
This is problematic because the complexity of parameter estimation is proportional
to the size of Y(x). The cost of the parameter estimation algorithms is bound by the
computation ofmodel expectation, ?i, given as (Malouf 2002):
?i =
?
x?X
p?(x)
?
y?Y(x)
fi(x, y)p(y|x)
=
?
x?X
p?(x)
?
y?Y(x)
fi(x, y)
1
Z(x)
exp
?
?
?
j
?j fj(x, y)
?
? (1)
As shown in this definition, the computation of model expectation requires the summa-
tion over Y(x) for every x in the training data. The complexity of the overall estimation
algorithm is O( ?|Y| ?|F||E|), where ?|Y| and ?|F| are the average numbers of y and activated
features for an event, respectively, and |E| is the number of events. When Y(x) grows
exponentially, the parameter estimation becomes intractable.
In PCFGs, the problem of computing probabilities of parse trees is avoided by using
a dynamic programming algorithm for computing inside/outside probabilities (Baker
1979). With the algorithm, the computation becomes tractable. We can expect that the
same approach would be effective for maximum entropy models as well.
This notion yields a novel algorithm for parameter estimation for maximum en-
tropy models, as described in the next section.
3. Feature Forest Model
Our solution to the problem is a dynamic programming algorithm for computing
inside/outside ?-products. Inside/outside ?-products roughly correspond to inside/
outside probabilities in PCFGs. In maximum entropy models, a probability is defined
as a normalized product of ?
fj
j (= exp(?j fj)). Hence, similar to the algorithm of computing
inside/outside probabilities, we can compute exp
(
?
j ?j fj
)
, which we define as the
?-product, for each node in a tree structure. If we can compute ?-products at a tractable
cost, the model expectation ?i is also computed at a tractable cost.
We first define the notion of a feature forest, a packed representation of a set
of an exponential number of tree structures. Feature forests correspond to packed
charts in CFG parsing. Because feature forests are generalized representations of forest
structures, the notion is not only applicable to syntactic parsing but also to sequence
tagging, such as POS tagging and named entity recognition (which will be discussed in
Section 6). We then define inside/outside ?-products that represent the ?-products of
partial structures of a feature forest. Inside?-products correspond to inside probabilities
in PCFG, and represent the summation of ?-products of the daughter sub-trees. Outside
?-products correspond to outside probabilities in PCFG, and represent the summation
of ?-products in the upper part of the feature forest. Both can be computed incre-
mentally by a dynamic programming algorithm similar to the algorithm for computing
38
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
inside/outside probabilities in PCFG. Given inside/outside ?-products of all nodes in
a feature forest, the model expectation ?i is easily computed by multiplying them for
each node.
3.1 Feature Forest
To describe the algorithm, we first define the notion of a feature forest, the generalized
representation of features in a packed forest structure. Feature forests are used for
enumerating possible structures of events, that is, they correspond to Y(x) in Equation 1.
Definition 2 (Feature forest)
A feature forest ? is a tuple ?C,D, r,?, ??, where:
 C is a set of conjunctive nodes,
 D is a set of disjunctive nodes,
 r is the root node: r ? C,
 ? : D ? 2C is a conjunctive daughter function,
 ? : C ? 2D is a disjunctive daughter function.
We denote a feature forest for x as ?(x). For example, ?(x) can represent the set of all
possible tag sequences of a given sentence x, or the set of all parse trees of x. A feature
forest is an acyclic graph, and unpacked structures extracted from a feature forest are
trees. We also assume that terminal nodes of feature forests are conjunctive nodes. That
is, disjunctive nodes must have daughters (i.e., ?(d) = ? for all d ? D).
A feature forest represents a set of trees of conjunctive nodes in a packed structure.
Conjunctive nodes correspond to entities such as states in Markov chains and nodes
in CFG trees. Feature functions are assigned to conjunctive nodes and express their
characteristics. Disjunctive nodes are for enumerating alternative choices. Conjunctive/
disjunctive daughter functions represent immediate relations of conjunctive and dis-
junctive nodes. By selecting a conjunctive node as a child of each disjunctive node, we
can extract a tree consisting of conjunctive nodes from a feature forest.
Figure 1 shows an example of a feature forest. Each disjunctive node enumerates
alternative nodes, which are conjunctive nodes. Each conjunctive node has disjunctive
Figure 1
A feature forest.
39
Computational Linguistics Volume 34, Number 1
Figure 2
Unpacked trees.
nodes as its daughters. The feature forest in Figure 1 represents a set of 2? 2? 2 = 8
unpacked trees shown in Figure 2. For example, by selecting the left-most conjunctive
node at each disjunctive node, we extract an unpacked tree (c1, c2, c4, c6). An unpacked
tree is represented as a set of conjunctive nodes. Generally, a feature forest represents
an exponential number of trees with a polynomial number of nodes. Thus, complete
structures, such as tag sequences and parse trees with ambiguities, can be represented
in a tractable form.
Feature functions are defined over conjunctive nodes.1
Definition 3 (Feature function for feature forests)
A feature function for a feature forest is:
fi : C ? R
Hence, together with feature functions, a feature forest represents a set of trees of
features.
Feature forests may be regarded as a packed chart in CFG parsing. Although feature
forests have the same structure as PCFG parse forests, nodes in feature forests do not
necessarily correspond to nodes in PCFG parse forests. In fact, in Sections 4.2 and 4.3, we
will demonstrate that syntactic structures and predicate?argument structures in HPSG
can be represented with tractable-size feature forests. The actual interpretation of a node
in a feature forest may thus be ignored in the following discussion. Our algorithm is
applicable whenever feature forests are of a tractable size. The descriptive power of
feature forests will be discussed again in Section 6.
Asmentioned, a feature forest is a packed representation of trees of features.We first
define model expectations, ?i, on a set of unpacked trees, and then show that they can
be computed without unpacking feature forests. We denote an unpacked tree as a set,
c ? C, of conjunctive nodes. Our concern is only the set of features associated with each
conjunctive node, and the shape of the tree structure is irrelevant to the computation of
probabilities of unpacked trees. Hence, we do not distinguish an unpacked tree from a
set of conjunctive nodes.
The collection of unpacked trees represented by a feature forest is defined as amulti-
set of unpacked trees because we allow multiple occurrences of equivalent unpacked
1 Feature functions may also be conditioned on x. In this case, feature functions can be written as fi(c, x).
For simplicity, we omit x in the following discussion.
40
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
trees in a feature forest.2 Given multisets of unpacked trees, A,B, we define the union
and the product as follows.
A? B ? A ? B
A? B ? {a ? b|a ? A,b ? B}
Intuitively, the first operation is a collection of trees, and the second lists all combina-
tions of trees in A and B. It is trivial that they satisfy commutative, associative, and
distributive laws.
A? B = B? A
A? B = B? A
A? (B? C) = (A? B)? C
A? (B? C) = (A? B)? C
A? (B? C) = (A? B)? (A? C)
We denote a set of unpacked trees rooted at node n ? C ?D as ?(n). ?(n) is de-
fined recursively. For a terminal node c ? C, obviously ?(c) = {{c}}. For an internal
conjunctive node c ? C, an unpacked tree is a combination of trees, each of which is
selected from a disjunctive daughter. Hence, a set of all unpacked trees is represented
as a product of trees from disjunctive daughters.
?(c) = {{c}} ?
?
d??(c)
?(d)
A disjunctive node d ? D represents alternatives of packed trees, and obviously a set
of its unpacked trees is represented as a union of the daughter trees, that is, ?(d) =
?
c??(d)?(c).
To summarize, a set of unpacked trees is defined formally as follows.
Definition 4 (Unpacked tree)
Given a feature forest ? = ?C,D, r,?, ??, a set ?(n) of unpacked trees rooted at node
n ? C ?D is defined recursively as follows.
 If n ? C is a terminal, that is, ?(n) = ?,
?(n) ? {{n}}
 If n ? C,
?(n) ? {{n}} ?
?
d??(n)
?(d)
2 In fact, no feature forests include equivalent unpacked trees if no disjunctive nodes have identical
daughter nodes. Thus we may define a set of unpacked trees as an ordinary set, although the details
are omitted here for simplicity.
41
Computational Linguistics Volume 34, Number 1
 If n ? D,
?(n) ?
?
c??(n)
?(c)
Feature forests are directed acyclic graphs and, as such, this definition does not include
a loop. Hence, ?(n) is properly defined.
A set of all unpacked trees is then represented by ?(r); henceforth, we denote ?(r)
as ?(?), or just ?when it is not confusing in context. Figure 3 shows ?(?) of the feature
forest in Figure 1. Following Definition 4, the first element of each set is the root node,
c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}. Each set in
Figure 3 corresponds to a tree in Figure 2.
Given this formalization, the feature function for an unpacked tree is defined as
follows.
Definition 5 (Feature function for unpacked tree)
The feature function fi for an unpacked tree, c ? ?(?) is defined as:
fi(c) =
?
c?c
fi(c)
Because c ? ?(?) corresponds to y of the conventional maximum entropy model, this
function substitutes for fi(x, y) in the conventional model. Once a feature function for
an unpacked tree is given, a model expectation is defined as in the traditional model.
Definition 6 (Model expectation of feature forests)
The model expectation ?i for a set of feature forests {?(x)} is defined as:
?i =
?
x?X
p?(x)
?
c??(?(x))
fi(c)p(c|x)
=
?
x?X
p?(x)
?
c??(?(x))
fi(c)
1
Z(x)
exp
?
?
?
j
?j fj(c)
?
?
where Z(x) =
?
c??(?(x))
exp
?
?
?
j
?j fj(c)
?
?
It is evident that the naive computation of model expectations requires exponential
time complexity because the number of unpacked trees (i.e., |?(?)|) is exponentially
related to the number of nodes in the feature forest ?. We therefore need an algorithm
for computing model expectations without unpacking a feature forest.
Figure 3
Unpacked trees represented as sets of conjunctive nodes.
42
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 4
Inside/outside at node c2 in a feature forest.
3.2 Dynamic Programming
To efficiently compute model expectations, we incorporate an approach similar to the
dynamic programming algorithm for computing inside/outside probabilities in PCFGs.
We first define the notion of inside/outside of a feature forest. Figure 4 illustrates this
concept, which is similar to the analogous concept in PCFGs.3 Inside denotes a set of
partial trees (sets of conjunctive nodes) derived from node c2. Outside denotes a set of
partial trees that derive node c2. That is, outside trees are partial trees of complements
of inside trees.
We denote a set of inside trees at node n as ?(n), and that of outside trees as o(n).
Definition 7 (Inside trees)
We define a set ?(n) of inside trees rooted at node n ? C ?D as a set of unpacked trees
rooted at n.
?(n) ? ?(n)
Definition 8 (Outside trees)
We define a set o(n) of outside trees rooted at node n ? C ?D as follows.
o(r) ? {?}
o(c) ?
?
d???1(c)
o(d)
o(d) ?
?
c???1(d)
?
?
?
{{c}} ? o(c)?
?
d???(c),d? =d
?(d?)
?
?
?
3 A node may have multiple outside trees in general as in the case of CFGs, although Figure 4 shows only
one outside tree of c2 for simplicity.
43
Computational Linguistics Volume 34, Number 1
In the definition, ??1 and ??1 denote mothers of conjunctive and disjunctive nodes,
respectively. Formally,
??1(c) ? {d|c ? ?(d)}
??1(d) ? {c|d ? ?(c)}
Next, inside/outside ?-products are defined for conjunctive and disjunctive nodes.
The inside (or outside) ?-products are the summation of exp
(
?
j ?j fj(c)
)
of all inside (or
outside) trees c.
Definition 9 (Inside/outside ?-product)
An inside ?-product at conjunctive node c ? C is
?c =
?
c??(c)
exp
?
?
?
j
?j fj(c)
?
?
An outside ?-product is
?c =
?
c?o(c)
exp
?
?
?
j
?j fj(c)
?
?
Similarly, inside/outside ?-products at disjunctive node d ? D are defined as follows:
?d =
?
c??(d)
exp
?
?
?
j
?j fj(c)
?
?
?d =
?
c?o(d)
exp
?
?
?
j
?j fj(c)
?
?
We can derive that the model expectations of a feature forest are computed as the
product of the inside and outside ?-products.
Theorem 1 (Model expectation of feature forests)
The model expectation ?i of a feature forest?(x) = ?Cx,Dx, rx,?x, ?x? is computed as the
product of inside and outside ?-products as follows:
?i =
?
x?X
p?(x) 1
Z(x)
?
c?Cx
fi(c)?c?c
where Z(x) = ?rx
44
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 5
Incremental computation of inside ?-products at conjunctive node c2.
Figure 6
Incremental computation of inside ?-products at disjunctive node d4.
This equation shows a method for efficiently computing model expectations by
traversing conjunctive nodes without unpacking the forest, if the inside/outside
?-products are given. The remaining issue is how to efficiently compute inside/outside
?-products.
Fortunately, inside/outside ?-products can be incrementally computed by dynamic
programming without unpacking feature forests. Figure 5 shows the process of com-
puting the inside ?-product at a conjunctive node from the inside ?-products of its
daughter nodes. Because the inside of a conjunctive node is a set of the combinations of
all of its descendants, the ?-product is computed by multiplying the ?-products of the
daughter trees. The following equation is derived.
?c =
?
?
?
d??(c)
?d
?
? exp
?
?
?
j
?j fj(c)
?
?
The inside of a disjunctive node is the collection of the inside trees of its daughter nodes.
Hence, the inside ?-product at disjunctive node d ? D is computed as follows (Figure 6).
?d =
?
c??(d)
?c
45
Computational Linguistics Volume 34, Number 1
Theorem 2 (Inside ?-product)
The inside ?-product ?c at a conjunctive node c is computed by the following equation
if ?d is given for all daughter disjunctive nodes d ? ?(c).
?c =
?
?
?
d??(c)
?d
?
? exp
?
?
?
j
?j fj(c)
?
?
The inside ?-product ?d at a disjunctive node d is computed by the following equation
if ?c is given for all daughter conjunctive nodes c ? ?(d).
?d =
?
c??(d)
?c
The outside of a disjunctive node is equivalent to the outside of its daughter nodes.
Hence, the outside ?-product of a disjunctive node is propagated to its daughter con-
junctive nodes (Figure 7).
?c =
?
{d|c??(d)}
?d
The computation of the outside ?-product of a disjunctive node is somewhat com-
plicated. As shown in Figure 8, the outside trees of a disjunctive node are all com-
binations of
 the outside trees of the mother nodes, and
 the inside trees of the sister nodes.
Figure 7
Incremental computation of outside ?-products at conjunctive node c2.
46
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 8
Incremental computation of outside ?-products at disjunctive node d4.
From this, we find:
?d =
?
{c|d??(c)}
?
?
?
?
?
?
?
?c exp
?
?
?
j
?j fj(c)
?
?
?
d???(c)
d? =d
?d?
?
?
?
?
?
?
?
We finally find the following theorem for the computation of outside ?-products.
Theorem 3 (Outside ?-product)
The outside ?-product ?c at conjunctive node c is computed by the following equation
if ?d is given for all mother disjunctive nodes, that is, all d such that c ? ?(d).
?c =
?
{d|c??(d)}
?d
The outside ?-product ?d at disjunctive node d is computed by the following equation
if ?c is given for all mother conjunctive nodes, that is, all c such that d ? ?(c), and ?d?
for all sibling disjunctive nodes d?.
?d =
?
{c|d??(c)}
?
?
?
?
?
?
?
?c exp
?
?
?
j
?j fj(c)
?
?
?
d???(c)
d? =d
?d?
?
?
?
?
?
?
?
Figure 9 shows the overall algorithm for estimating the parameters, given a set
of feature forests. The key point of the algorithm is to compute inside ?-products ?
and outside ?-products ? for each node in C, and not for all unpacked trees. The func-
tions inside product and outside product compute ? and ? efficiently by dynamic
programming.
Note that the order in which nodes are traversed is important for incremental com-
putation, although it is not shown in Figure 9. The computation for the daughter
nodes and mother nodes must be completed before computing the inside and outside
47
Computational Linguistics Volume 34, Number 1
Figure 9
Algorithm for computing model expectations of feature forests.
?-products, respectively. This constraint is easily solved using any topological sort
algorithm. A topological sort is applied once at the beginning. The result of the sorting
does not affect the cost and the result of estimation. In our implementation, we assume
that conjunctive/disjunctive nodes are already ordered from the root node in input data.
The complexity of this algorithm is O(( ?|C|+ ?|D|) ?|F||E|), where ?|C| and ?|D| are the
average numbers of conjunctive and disjunctive nodes, respectively. This is tractable
when ?|C| and ?|D| are of a reasonable size. As noted in this section, the number of
48
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
nodes in a feature forest is usually polynomial even when that of the unpacked trees
is exponential. Thus we can efficiently compute model expectations with polynomial
computational complexity.
4. Probabilistic HPSG Parsing
Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al
2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van
Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The
probability, p(t|w), of producing parse result t of a given sentence w is defined as
p(t|w) = 1
Zw
p0(t|w) exp
(
?
i
?i fi(t,w)
)
where
Zw =
?
t??T(w)
p0(t
?|w) exp
(
?
i
?i fi(t
?,w)
)
where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution)
and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) rep-
resents the characteristics of t and w, and the corresponding model parameter ?i is its
weight. Model parameters that maximize the log-likelihood of the training data are
computed using a numerical optimization method (Malouf 2002).
Estimation of the model requires a set of pairs ?tw,T(w)?, where tw is the correct
parse for a sentencew. Whereas tw is provided by a treebank, T(w) has to be computed
by parsing each w in the treebank. Previous studies assumed T(w) could be enumer-
ated; however, this assumption is impractical because the size of T(w) is exponentially
related to the length of w.
Our solution here is to apply the feature forest model of Section 3 to the probabilistic
modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3
describe how to represent HPSG parse trees and predicate?argument structures by
feature forests. Together with the parameter estimation algorithm in Section 3, these
methods constitute a complete method for probabilistic disambiguation. We also ad-
dress a method for accelerating the construction of feature forests for all treebank
sentences in Section 4.4. The design of feature functions will be given in Section 4.5.
4.1 HPSG
HPSG (Pollard and Sag 1994; Sag,Wasow, and Bender 2003) is a syntactic theory that fol-
lows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases,
are denoted by signs, which are represented by typed feature structures (Carpenter
1992). Signs are a formal representation of combinations of phonological forms and
syntactic/semantic structures, and express which phonological form signifies which
syntactic/semantic structure. Figure 10 shows the lexical sign for loves. The geometry
of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word,
MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints
of a specifier, a syntactic subject, and complements, respectively. CONT denotes the
49
Computational Linguistics Volume 34, Number 1
Figure 10
Lexical entry for the transitive verb loves.
Figure 11
Simplified representation of the lexical entry in Figure 10.
predicate?argument structure of a phrase/sentence. The notation of CONT in this article
is borrowed from that of Minimal Recursion Semantics (Copestake et al 2006): HOOK
represents a structure accessed by other phrases, and RELS describes the remaining
structure of the semantics. In what follows, we represent signs in a reduced form as
shown in Figure 11, because of the large size of typical HPSG signs, which often include
information not immediately relevant to the point being discussed. We will only show
attributes that are relevant to an explanation, expecting that readers can fill in the values
of suppressed attributes.
50
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
In our actual implementation of the HPSG grammar, lexical/phrasal signs contain
additional attributes that are not defined in the standard HPSG theory but are used
by a disambiguation model. Examples include the surface form of lexical heads, and
the type of lexical entry assigned to lexical heads, which are respectively used for
computing the features WORD and LE introduced in Section 4.5. By incorporating ad-
ditional attributes into signs, we can straightforwardly compute feature functions for
each sign. This allows for a simple mapping between a parsing chart and a feature forest
as described subsequently. However, this might increase the size of parse forests and
therefore decrease parsing efficiency, because differences between additional attributes
interfere with equivalence relations for ambiguity packing.
4.2 Packed Representation of HPSG Parse Trees
We represent an HPSG parse tree with a set of tuples ?m, l, r?, where m, l, and r are the
signs of the mother, left daughter, and right daughter, respectively.4 In chart parsing,
partial parse candidates are stored in a chart, in which phrasal signs are identified and
packed into equivalence classes if they are judged to be equivalent and dominate the
sameword sequences. A set of parse trees is then represented as a set of relations among
equivalence classes.5
Figure 12 shows a chart for parsing he saw a girl with a telescope, where the modifiee
of with is ambiguous (saw or girl). Each feature structure expresses an equivalence class,
and the arrows represent immediate-dominance relations. The phrase, saw a girl with
a telescope, has two trees (A in the figure). Because the signs of the top-most nodes are
equivalent, they are packed into an equivalence class. The ambiguity is represented as
the two pairs of arrows leaving the node A.
A set of HPSG parse trees is represented in a chart as a tuple ?E,Er,??, where E is a
set of equivalence classes, Er ? E is a set of root nodes, and ? : E? 2E?E is a function
to represent immediate-dominance relations.
Our representation of a chart can be interpreted as an instance of a feature forest.
We map the tuple ?em, el, er?, which corresponds to ?m, l, r?, into a conjunctive node.
Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a feature
forest. Square boxes (ci) are conjunctive nodes, and di disjunctive nodes. A solid arrow
represents a disjunctive daughter function, and a dotted line expresses a conjunctive
daughter function.
Formally, a chart ?E,Er,?? is mapped into a feature forest ?C,D,R,?, ?? as follows.6
 C = {?em, el, er?|em ? E ? (el, er) ? ?(em)} ? {w|w ? w}
 D = E
 R = {?em, el, er?|em ? Er ? ?em, el, er? ? C}
4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n > 2) trees is trivial.
5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985),
and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that
parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and
Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests,
because they over-generate possible unpacked trees.
6 For ease of explanation, the definition of the root node is different from the original definition given
in Section 3. In this section, we define R as a set of conjunctive nodes rather than a single node r. The
definition here is translated into the original definition by introducing a dummy root node r? that has
no features and only one disjunctive daughter whose daughters are R.
51
Computational Linguistics Volume 34, Number 1
Figure 12
Chart for parsing he saw a girl with a telescope.
Figure 13
Feature forest representation of HPSG parse trees in Figure 12.
52
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
 ?(em) =
{
{?em, el, er?|(el, er) ? ?(em)} if ?(em) = ?
{w|em is a lexical entry for w} otherwise
 ?(c) =
{
{el, er} if c = ?em, el, er?
? if c ? w
Onemay claim that restricting the domain of feature functions to ?em, el, er? limits the
flexibility of feature design. Although this is true to some extent, it does not necessarily
mean the impossibility of incorporating features on nonlocal dependencies into the
model. This is because a feature forest model does not assume probabilistic indepen-
dence of conjunctive nodes. This means that we can unpack a part of the forest without
changing the model. Actually, we successfully developed a probabilistic model includ-
ing features on nonlocal predicate?argument dependencies, as described subsequently.
4.3 Packed Representation of Predicate?Argument Structures
With the method previously described, we can represent an HPSG parsing chart with
a feature forest. However, equivalence classes in a chart might increase exponentially
because predicate?argument structures in HPSG signs represent the semantic relations
of all words that the phrase dominates. For example, Figure 14 shows phrasal signs with
predicate?argument structures for saw a girl with a telescope. In the chart in Figure 12,
these signs are packed into an equivalence class. However, Figure 14 shows that the
values of CONT, that is, predicate?argument structures, have different values, and the
signs as they are cannot be equivalent. As seen in this example, predicate?argument
structures prevent us from packing signs into equivalence classes.
In this section, we apply the feature forest model to predicate?argument structures,
which may include reentrant structures and non-local dependencies. It is theoretically
difficult to apply the feature forest model to predicate?argument structures; a feature
forest cannot represent graph structures that include reentrant structures in a straight-
forward manner. However, if predicate?argument structures are constructed as in the
manner described subsequently, they can be represented by feature forests of a tracta-
ble size.
Feature forests can represent predicate?argument structures if we assume some
locality and monotonicity in the composition of predicate?argument structures.
Locality: In each step of composition of a predicate?argument structure, only a
limited depth of the daughters? predicate?argument structures are referred to.
That is, local structures in the deep descendent phrases may be ignored to
construct larger phrases. This assumption means that predicate?argument
structures can be packed into conjunctive nodes by ignoring local structures.
Figure 14
Signs with predicate?argument structures.
53
Computational Linguistics Volume 34, Number 1
Monotonicity: All relations in the daughters? predicate?argument structures
are percolated to the mother. That is, none of the predicate?argument
relations in the daughter phrases disappear in the mother. Thus
predicate?argument structures of descendent phrases can be located at
lower nodes in a feature forest.
Predicate?argument structures usually satisfy the above conditions, evenwhen they
include non-local dependencies. For example, Figure 15 shows HPSG lexical entries
for the wh-extraction of the object of love (left) and for the control construction of try
(right). The first condition is satisfied because both lexical entries refer to CONT|HOOK
of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly
access ARGX of the arguments. The second condition is also satisfied because the values
of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In
addition, the elements in CONT|RELS are percolated to the mother by the Semantic Prin-
ciple. Compositional semantics usually satisfies the above conditions, including MRS
(Copestake et al 1995, 2006). The composition of MRS refers to HOOK, and no internal
structures of daughters. The Semantic Principle of MRS also assures that all semantic
relations in RELS are percolated to the mother. When these conditions are satisfied,
semantics may include any constraints, such as selectional restrictions, although the
grammar we used in the experiments does not include semantic restrictions to constrain
parse forests.
Under these conditions, local structures of predicate?argument structures are en-
coded into a conjunctive node when the values of all of its arguments have been
instantiated. We introduce the notion of inactives to denote such local structures.
Definition 10 (Inactives)
An inactive is a subset of predicate?argument structures in which all arguments have
been instantiated.
Because inactive parts will not change during the rest of the parsing process, they can
be placed in a conjunctive node. By placing newly generated inactives into correspond-
ing conjunctive nodes, a set of predicate?argument structures can be represented in a
feature forest by packing local ambiguities, and non-local dependencies are preserved.
Figure 16 illustrates a process of parsing the sentence She ignored the fact that I wanted
to dispute, where dispute has an ambiguity (dispute1, intransitive, and dispute2, transitive)
Figure 15
Lexical entries including non-local relations.
54
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 16
Process of composing predicate?argument structures.
Figure 17
Predicate?argument structures of dispute.
and factmay optionally take a complementizer phrase.7 The predicate?argument struc-
tures for dispute1 and dispute2 are shown in Figure 17. Curly braces express the am-
biguities of partially constructed predicate?argument structures. The resulting feature
forest is shown in Figure 18. The boxes denote conjunctive nodes and dx represent
disjunctive nodes.
The clause I wanted to dispute has two possible predicate?argument structures: one
corresponding to dispute1 (? in Figure 16) and the other corresponding to dispute2 (?
in Figure 16). The nodes of the predicate?argument structure ? are all instantiated, that
is, it contains only inactives. The corresponding conjunctive node (?? in Figure 18) has
two inactives, for want and dispute1. The other structure ? has an unfilled object in the
argument (ARG28) of dispute2, which will be filled by the non-local dependency. Hence,
the corresponding conjunctive node ?? has only one inactive corresponding to want,
and the remaining part that corresponds to dispute2 is passed on for further processing.
When we process the phrase the fact that I wanted to dispute, the object of dispute2 is filled
by fact (? in Figure 16), and the predicate?argument structure of dispute2 is then placed
into a conjunctive node (?? in Figure 18).
7 In Figure 16, feature structures of different nodes of parse trees are assigned distinct variables, even when
they are from the same lexical entries. This is because feature structures are copied during chart parsing.
Although these variables are from the same lexical entry, it is copied to several chart items, and hence
there are no structure sharings among them.
8 ? (bottom) represents an uninstantiated value (Carpenter 1992).
55
Computational Linguistics Volume 34, Number 1
Figure 18
A feature forest representation of predicate?argument structures.
One of the beneficial characteristics of this packed representation is that the rep-
resentation is isomorphic to the parsing process, that is, a chart. Hence, we can assign
features of HPSG parse trees to a conjunctive node, together with features of predicate?
argument structures. In Section 5, we will investigate the contribution of features on
parse trees and predicate?argument structures to the disambiguation of HPSG parsing.
4.4 Filtering by Preliminary Distribution
The method just described is the essence of our solution for the tractable estimation
of maximum entropy models on exponentially many HPSG parse trees. However,
the problem of computational cost remains. Construction of feature forests requires
parsing of all of the sentences in a treebank. Despite the development of methods to
improve HPSG parsing efficiency (Oepen, Flickinger, et al 2002), exhaustive parsing of
all sentences is still expensive.
We assume that computation of parse trees with low probabilities can be omitted
in the estimation stage because T(w) can be approximated by parse trees with high
probabilities. To achieve this, we first prepared a preliminary probabilistic model whose
estimation did not require the parsing of a treebank. The preliminary model was used
to reduce the search space for parsing a training treebank.
The preliminary model in this study is a unigram model, p?(t|w) =
?
w?w p(l|w),
where w ? w is a word in the sentence w, and l is a lexical entry assigned to w. This
model is estimated by counting the relative frequencies of lexical entries used for w in
the training data. Hence, the estimation does not require parsing of a treebank. Actually,
we use a maximum entropymodel to compute this probability as described in Section 5.
56
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
The preliminarymodel is used for filtering lexical entries whenwe parse a treebank.
Given this model, we restrict the number of lexical entries used to parse a treebank.With
a threshold n for the number of lexical entries and a threshold  for the probability,
lexical entries are assigned to a word in descending order of probability, until the
number of assigned entries exceeds n, or the accumulated probability exceeds . If this
procedure does not assign a lexical entry necessary to produce a correct parse (i.e., an
oracle lexical entry), it is added to the list of lexical entries. It should be noted that oracle
lexical entries are given by the HPSG treebank. This assures that the filtering method
does not exclude correct parse trees from parse forests.
Figure 19 shows an example of filtering the lexical entries assigned to saw. With  =
0.95, four lexical entries are assigned. Although the lexicon includes other lexical entries,
such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they are
filtered out. Although this method reduces the time required for parsing a treebank, this
approximation causes bias in the training data and results in lower accuracy. The trade-
off between parsing cost and accuracy will be examined experimentally in Section 5.4.
We have several ways to integrate p? with the estimated model p(t|T(w)). In the
experiments, we will empirically compare the following methods in terms of accuracy
and estimation time.
Filtering only: The unigram probability p? is used only for filtering in training.
Product: The probability is defined as the product of p? and the estimated model p.
Reference distribution: p? is used as a reference distribution of p.
Feature function: log p? is used as a feature function of p. This method has been
shown to be a generalization of the reference distribution method (Johnson
and Riezler 2000).
4.5 Features
Feature functions in maximum entropy models are designed to capture the characteris-
tics of ?em, el, er?. In this article, we investigate combinations of the atomic features listed
Figure 19
Filtering of lexical entries for saw.
57
Computational Linguistics Volume 34, Number 1
Table 1
Templates for atomic features.
RULE name of the applied schema
DIST distance between the head words of the daughters
COMMA whether a comma exists between daughters and/or inside of daughter phrases
SPAN number of words dominated by the phrase
SYM symbol of the phrasal category (e.g., NP, VP)
WORD surface form of the head word
POS part-of-speech of the head word
LE lexical entry assigned to the head word
ARG argument label of a predicate
in Table 1. The following combinations are used for representing the characteristics of
binary/unary schema applications.
fbinary =
?
RULE,DIST,COMMA,
SPANl, SYMl, WORDl, POSl, LEl,
SPANr, SYMr, WORDr, POSr, LEr
?
funary = ?RULE,SYM,WORD,POS,LE?
where subscripts l and r denote left and right daughters.
In addition, the following is used for expressing the condition of the root node of
the parse tree.
froot = ?SYM,WORD,POS,LE?
Feature functions to capture predicate?argument dependencies are represented as
follows:
fpa =
?
ARG, DIST, WORDp, POSp, LEp, WORDa, POSa, LEa
?
where subscripts p and a represent predicate and argument, respectively.
Figure 20 shows examples: froot is for the root node, in which the phrase symbol
is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw,
VBD, and a transitive verb, respectively. fbinary is for the binary rule application to saw a
girl and with a telescope, in which the applied schema is the Head-Modifier Schema, the
left daughter is VP headed by saw, and the right daughter is PP headed by with, whose
part-of-speech is IN and whose lexical entry is a VP-modifying preposition.
Figure 21 shows example features for predicate?argument structures. The figure
shows features assigned to the conjunctive node denoted as ?? in Figure 18. Because
inactive structures in the node have three predicate?argument relations, three features
are activated. The first one is for the relation of want and I, where the label of the relation
is ARG1, the distance between the head words is 1, the surface string and the POS of
58
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 20
Example features for binary schema application and root condition.
Figure 21
Example features for predicate?argument structures.
the predicate are want and VBD, and those of the argument are I and PRP. The second
and the third features are for the other two relations. We may include features on more
than two relations, such as the dependencies among want, I, and dispute, although such
features are not incorporated currently.
In our implementation, some of the atomic features are abstracted (i.e., ignored) for
smoothing. Tables 2, 3, and 4 show the full set of templates of combined features used in
the experiments. Each row represents the template for a feature function. A check indi-
cates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.
59
Computational Linguistics Volume 34, Number 1
Table 2
Feature templates for binary schema (left) and unary schema (right).
RULE DIST COMMA SPAN SYM WORD POS LE
? ? ?
? ?
? ? ?
? ? ?
? ?
? ?
?? ? ?
? ?
?
?
?
? ? ?
?
? ?
? ??
?
? ?
?
? ? ?
?
?
? ?
?
? ?
??
?
? ?
?
?
?
?
?
?
? ? ? ?
? ?? ? ?
? ? ?
? ?
? ? ?
? ? ?
?
?? ? ?
? ? ? ?
?
? ? ?
?
?
? ? ??
?
? ?
? ?
? ?
?
?
? ?
? ?
?
??
?
? ?
? ? ?
?
?
?
? ? ?
? ? ?
RULE SYM WORD POS LE
?
?
? ? ?
?
?
? ?
??
?
?
?
?
? ? ?
? ??
? ?
? ?
?
? ?
?
??
? ? ?
?
? ?
? ? ?
Table 3
Feature templates for root condition.
SYM WORD POS LE
?
? ? ?
?
? ?
?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
?
?
? ? ?
?
?
? ? ?
Table 4
Feature templates for predicate?argument dependencies.
ARG DIST WORD POS LE
? ? ? ? ?
? ? ?
?
?
? ?
?
? ?
?
?
? ? ?
?
?
?
?
?
?
? ?
? ?
? ? ? ?
?? ? ?
? ?? ?
?
?
??
?
? ?
??
?
?
? ??
? ?
?
?
60
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
5. Experiments
This section presents experimental results on the parsing accuracy attained by the
feature forest models. In all of the following experiments, we use the HPSG grammar
developed by the method of Miyao, Ninomiya, and Tsujii (2005). Section 5.1 describes
how this grammarwas developed. Section 5.2 explains other aspects of the experimental
settings. In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.
5.1 The HPSG Grammar
In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a wide-
coverage HPSG grammar extracted from the Penn Treebank by the method of Miyao,
Ninomiya, and Tsujii (2005). In this method, we convert the Penn Treebank into an
HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG
treebank. Figure 22 illustrates the process of treebank conversion and lexicon collection.
We first convert and fertilize parse trees of the Penn Treebank. This step identifies
syntactic constructions that require special treatment in HPSG, such as raising/control
and long-distance dependencies. These constructions are then annotated with typed
feature structures so that they conform to the HPSG analysis. Next, we apply HPSG
schemas and principles, and obtain fully specified HPSG parse trees. This step solves
feature structure constraints given in the previous step, and fills unspecified constraints.
Failures of schema/principle applications indicate that the annotated constraints do not
Figure 22
Extracting HPSG lexical entries from the Penn Treebank.
61
Computational Linguistics Volume 34, Number 1
conform to the HPSG analysis, and require revisions. Finally, we obtain lexical entries
from the HPSG parse trees. The terminal nodes of HPSG parse trees are collected, and
they are generalized by removing word-specific or context-specific constraints.
An advantage of this method is that a wide-coverage HPSG lexicon is obtained
because lexical entries are extracted from real-world sentences. Obtained lexical entries
are guaranteed to construct well-formed HPSG parse trees because HPSG schemas
and principles are successfully applied during the development of the HPSG treebank.
Another notable feature is that we can additionally obtain an HPSG treebank, which
can be used as training data for disambiguation models. In the following experiments,
this HPSG treebank is used for the training of maximum entropy models.
The lexicon used in the following experiments was extracted from Sections 02?21
of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct
lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank
Section 23. This number expresses ?lexical coverage? in the strong sense defined by
Hockenmaier and Steedman (2002). In this notion of ?coverage,? this lexicon has 84.1%
sentential coverage, where this means that the lexicon can assign correct lexical entries
to all of the words in a sentence. Although the parser might produce parse results for
uncovered sentences, these parse results cannot be completely correct.
5.2 Experimental Settings
The data for the training of the disambiguation models was the HPSG treebank derived
from Sections 02?21 of the Wall Street Journal portion of the Penn Treebank, that is, the
same set used for lexicon extraction. For training of the disambiguation models, we
eliminated sentences of 40 words or more and sentences for which the parser could not
produce the correct parses. The resulting training set consists of 33,604 sentences (when
n = 10 and  = 0.95; see Section 5.4 for details). The treebanks derived from Sections
22 and 23 were used as the development and final test sets, respectively. Following
previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000),
accuracy is measured for sentences of less than 40 words and for those with less than
100 words. Table 5 shows the specifications of the test data.
The measure for evaluating parsing accuracy is precision/recall of predicate?
argument dependencies output by the parser. A predicate?argument dependency is
defined as a tuple ?wh,wn,?,??, where wh is the head word of the predicate, wn is the
head word of the argument, ? is the type of the predicate (e.g., adjective, intransitive
verb), and ? is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried
running has three dependencies as follows:
 ?tried, he, transitive verb,ARG1?
Table 5
Specification of test data for the evaluation of parsing accuracy.
No. of Sentences Avg. Length
Test set (Section 23, < 40 words) 2,144 20.52
Test set (Section 23, < 100 words) 2,299 22.23
Development set (Section 22, < 40 words) 1,525 20.69
Development set (Section 22, < 100 words) 1,641 22.43
62
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
 ?tried, running, transitive verb,ARG2?
 ?running, he, intransitive verb,ARG1?
Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser,
and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified
regardless of ? and ?. F-score is the harmonic mean of LP and LR. Sentence accuracy
is the exact match accuracy of complete predicate?argument relations in a sentence.
These measures correspond to those used in other studies measuring the accuracy of
predicate?argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman
2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al 2004),
although exact figures cannot be compared directly because the definitions of depen-
dencies are different. All predicate?argument dependencies in a sentence are the target
of evaluation except quotation marks and periods. The accuracy is measured by parsing
test sentences with gold-standard part-of-speech tags from the Penn Treebank unless
otherwise noted.
The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its
hyper-parameter was tuned for each model to maximize F-score for the development
set. The algorithm for parameter estimation was the limited-memory BFGS method
(Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with
the LiLFeS library (Makino et al 2002), and various speed-up techniques for HPSG
parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and
Tsujii 2004; Ninomiya et al 2005). Other efficient parsing techniques, including global
thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were
not used. The results obtained using these techniques are given in Ninomiya et al A
limit on the number of constituents was set for time-out; the parser stopped parsing
when the number of constituents created during parsing exceeded 50,000. In such a
case, the parser output nothing, and the recall was computed as zero.
Features occurring more than twice were included in the probabilistic models. A
method of filtering lexical entries was applied to the parsing of training data (Sec-
tion 4.4). Unless otherwise noted, parameters for filtering were n = 10 and  = 0.95, and
a reference distribution method was applied. The unigram model, p0(t|s), for filtering is
a maximum entropy model with two feature templates, ?WORD, POS, LE? and ?POS, LE?.
The model includes 24,847 features.
5.3 Efficacy of Feature Forest Models
Tables 6 and 7 show parsing accuracy for the test set. In the tables, ?Syntactic features?
denotes a model with syntactic features, that is, fbinary, funary, and froot introduced
Table 6
Accuracy of predicate?argument relations (test set, <40 words).
LP LR UP UR F-score Sentence acc.
Baseline 78.10 77.39 82.83 82.08 77.74 18.3
Syntactic features 86.92 86.28 90.53 89.87 86.60 36.3
Semantic features 84.29 83.74 88.32 87.75 84.01 30.9
All 86.54 86.02 90.32 89.78 86.28 36.0
63
Computational Linguistics Volume 34, Number 1
Table 7
Accuracy of predicate?argument relations (test set, <100 words).
LP LR UP UR F-score Sentence acc.
Baseline 77.58 76.84 82.22 81.43 77.21 17.1
Syntactic features 86.47 85.83 90.06 89.40 86.15 34.1
Semantic features 83.81 83.26 87.75 87.16 83.53 28.9
All 86.13 85.59 89.85 89.29 85.86 33.8
in Section 4.5. ?Semantic features? represents a model with features on predicate?
argument structures, that is, fpa given in Table 4. ?All? is a model with both syntactic
and semantic features. The ?Baseline? row shows the results for the reference model,
p0(t|s), used for lexical entry filtering in the estimation of the other models. This model
is considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1
for any rule r in the construction rules of the HPSG grammar.
The results demonstrate that feature forest models have significantly higher ac-
curacy than a baseline model. Comparing ?Syntactic features? with ?Semantic fea-
tures,? we see that the former model attained significantly higher accuracy than the
latter. This indicates that syntactic features are more important for overall accuracy.
We will examine the contributions of each atomic feature of the syntactic features in
Section 5.5.
Features on predicate?argument relations were generally considered as important
for the accurate disambiguation of syntactic structures. For example, PP-attachment
ambiguity cannot be resolved with only syntactic preferences. However, the results
show that a model with only semantic features performs significantly worse than one
with syntactic features. Even when combined with syntactic features, semantic features
do not improve accuracy. Obviously, semantic preferences are necessary for accurate
parsing, but the features used in this work were not sufficient to capture semantic pref-
erences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies
may be too sparse to capture semantic preferences.
For reference, our results are competitive with the best corresponding results re-
ported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our
results cannot be compared directly with other grammar formalisms because each
formalism represents predicate?argument dependencies differently. In contrast with the
results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly
lower than precision. This may have resulted from the HPSG grammar having stricter
feature constraints and the parser not being able to produce parse results for around
1% of the sentences. To improve recall, we need techniques to deal with these 1% of
sentences.
Table 8 gives the computation/space costs of model estimation. ?Estimation time?
indicates user times required for running the parameter estimation algorithm. ?No. of
feature occurrences? denotes the total number of occurrences of features in the training
data, and ?Data size? gives the sizes of the compressed files of training data. We can
conclude that feature forest models are estimated at a tractable computational cost and
a reasonable data size, even when a model includes semantic features including non-
local dependencies. The results reveal that feature forest models essentially solve the
problem of the estimation of probabilistic models of sentence structures.
64
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Table 8
Computation/space costs of model estimation.
No. of features Estimation No. of feature Data size
time (sec.) occurrences (MB)
Baseline 24,847 499 6,948,364 21
Syntactic features 599,104 511 127,497,615 727
Semantic features 334,821 278 176,534,753 375
All 933,925 716 304,032,368 1,093
Table 9
Estimation method vs. accuracy and estimation time.
LP LR F-score Estimation time (sec.)
Filtering only 51.70 49.89 50.78 449
Product 86.50 85.94 86.22 1,568
Reference distribution 86.92 86.28 86.60 511
Feature function 84.81 84.09 84.45 945
5.4 Comparison of Filtering Methods
Table 9 compares the estimation methods introduced in Section 4.4. In all of the follow-
ing experiments, we show the accuracy for the test set (<40 words) only. Table 9 reveals
that our method achieves significantly lower accuracy when it is used only for filtering
in the training phrase. One reason is that the feature forest model prefers lexical entries
that are filtered out in the training phase, because they are always oracle lexical entries
in the training. This means that we must incorporate the preference of filtering into the
final parse selection. As shown in Table 9, the models combined with a preliminary
model achieved sufficient accuracy. The reference distribution method achieved higher
accuracy and lower cost. The feature function method achieved lower accuracy in our
experiments. A possible reason for this is that a hyper-parameter of the prior was set to
the same value for all the features including the feature of the log-probability given by
the preliminary distribution.
Tables 10 and 11 show the results of changing the filtering threshold. We can
determine the correlation between the estimation/parsing cost and accuracy. In our
experiment, n ? 10 and  ? 0.90 seem necessary to preserve the F-score over 86.0.
5.5 Contribution of Features
Table 12 shows the accuracy with different feature sets. Accuracy was measured for 15
models with some atomic features removed from the final model. The last row denotes
the accuracy attained by the unigram model (i.e., the reference distribution). The num-
bers in bold type represent a significant difference from the final model according to
stratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value < .05
for 32 pairwise comparisons. The results indicate that DIST, COMMA, SPAN, WORD, and
65
Computational Linguistics Volume 34, Number 1
Table 10
Filtering threshold vs. accuracy.
n, LP LR F-score Sentence acc.
5, 0.80 85.09 84.30 84.69 32.4
5, 0.90 85.44 84.61 85.02 32.5
5, 0.95 85.52 84.66 85.09 32.7
5, 0.98 85.50 84.63 85.06 32.6
10, 0.80 85.60 84.65 85.12 32.5
10, 0.90 86.49 85.92 86.20 34.7
10, 0.95 86.92 86.28 86.60 36.3
10, 0.98 87.18 86.66 86.92 37.7
15, 0.80 85.59 84.63 85.11 32.4
15, 0.90 86.48 85.80 86.14 35.7
15, 0.95 87.21 86.68 86.94 37.0
15, 0.98 87.69 87.16 87.42 39.2
Table 11
Filtering threshold vs. estimation cost.
n, Estimation time (sec.) Parsing time (sec.) Data size (MB)
5, 0.80 108 5,103 341
5, 0.90 150 6,242 407
5, 0.95 190 7,724 469
5, 0.98 259 9,604 549
10, 0.80 130 6,003 370
10, 0.90 268 8,855 511
10, 0.95 511 15,393 727
10, 0.98 1,395 36,009 1,230
15, 0.80 123 6,298 372
15, 0.90 259 9,543 526
15, 0.95 735 20,508 854
15, 0.98 3,777 86,844 2,031
POS features contributed to the final accuracy, although the differences were slight. In
contrast, RULE, SYM, and LE features did not affect accuracy. However, when each was
removed together with another feature, the accuracy decreased drastically. This implies
that such features carry overlapping information.
5.6 Factors for Parsing Accuracy
Table 13 shows parsing accuracy for covered and uncovered sentences. As defined in
Section 5.1, ?covered? indicates that the HPSG lexicon has all correct lexical entries for a
sentence. In other words, for covered sentences, exactly correct parse trees are obtained
if the disambiguation model worked perfectly. The result reveals clear differences in
accuracy between covered and uncovered sentences. The F-score for covered sentences
is around 2.5 points higher than the overall F-score, whereas the F-score is more than
10 points lower for uncovered sentences. This result indicates improvement of lexicon
quality is an important factor for higher accuracy.
66
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Table 12
Accuracy with different feature sets.
Features LP LR F-score Sentence acc. No. of features
All 86.92 86.28 86.60 36.3 599,104
?RULE 86.83 86.19 86.51 36.3 596,446
?DIST 86.52 85.96 86.24 35.7 579,666
?COMMA 86.31 85.81 86.06 34.4 584,040
?SPAN 86.32 85.75 86.03 35.5 559,490
?SYM 86.74 86.16 86.45 35.4 406,545
?WORD 86.39 85.77 86.08 35.3 91,004
?POS 86.18 85.61 85.89 34.1 406,545
?LE 86.91 86.32 86.61 36.8 387,938
?DIST,SPAN 85.39 84.82 85.10 33.1 270,467
?DIST,SPAN,COMMA 83.75 83.25 83.50 28.9 261,968
?RULE,DIST,SPAN,COMMA 83.44 82.93 83.18 27.6 259,372
?WORD,LE 86.40 85.81 86.10 34.7 25,429
?WORD,POS 85.44 84.87 85.15 32.7 40,102
?WORD,POS,LE 84.68 84.12 84.40 31.1 8,899
?SYM,WORD,POS,LE 82.77 82.14 82.45 24.9 1,914
None 78.10 77.39 77.74 18.3 0
Table 13
Accuracy for covered/uncovered sentences.
LP LR F-score Sentence acc. No. of sentences
covered sentences 89.36 88.96 89.16 42.2 1,825
uncovered sentences 75.57 74.04 74.80 2.5 319
Figure 23 shows the learning curve. A feature set was fixed, and the parameter of
the Gaussian prior was optimized for each model. High accuracy is attained even with a
small training set, and the accuracy seems to be saturated. This indicates that we cannot
further improve the accuracy simply by increasing the size of the training data set. The
exploration of new types of features is necessary for higher accuracy. It should also be
noted that the upper bound of the accuracy is not 100%, because the grammar cannot
produce completely correct parse results for uncovered sentences.
Figure 24 shows the accuracy for each sentence length. It is apparent from this
figure that the accuracy is significantly higher for sentences with less than 10 words.
This implies that experiments with only short sentences overestimate the performance
of parsers. Sentences with at least 10 words are necessary to properly evaluate the
performance of parsing real-world texts. The accuracies for the sentences with more
than 10 words are not very different, although data points for sentences with more than
50 words are not reliable.
Table 14 shows the accuracies for predicate?argument relations when parts-
of-speech tags are assigned automatically by a maximum-entropy-based parts-of-
speech tagger (Tsuruoka and Tsujii 2005). The results indicate a drop of about three
points in labeled precision/recall (a two-point drop in unlabeled precision/recall).
A reason why we observed larger accuracy drops in labeled precision/recall is that
67
Computational Linguistics Volume 34, Number 1
Figure 23
Corpus size vs. accuracy.
Figure 24
Sentence length vs. accuracy.
predicate?argument relations are fragile with respect to parts-of-speech errors because
predicate types (e.g., adjective, intransitive verb) are determined depending on the
parts-of-speech of predicate words. Although our current parsing strategy assumes that
parts-of-speech are given beforehand, for higher accuracy in real application contexts,
we will need a method for determining parts-of-speech and parse trees jointly.
Table 14
Accuracy with automatic parts-of-speech tags (test set).
LP LR UP UR F-score Sentence acc.
<40 words 83.88 82.84 88.83 87.73 83.36 30.1
<100 words 83.45 82.40 88.37 87.26 82.92 28.2
68
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
5.7 Analysis of Disambiguation Errors
Table 15 shows amanual classification of the causes of disambiguation errors in 100 sen-
tences randomly chosen from Section 00. In our evaluation, one error source may cause
multiple dependency errors. For example, if an incorrect lexical entry is assigned to a
verb, all of the argument dependencies of the verb are counted as errors. The numbers
in the table include such double-counting. Figure 25 shows examples of disambiguation
errors. The figure shows output from the parser.
Major causes are classified into three types: attachment ambiguity, argument/
modifier distinction, and lexical ambiguity. As attachment ambiguities are well-known
error sources, PP-attachment is the largest source of errors in our evaluation. Our
disambiguation model cannot accurately resolve PP-attachment ambiguities because it
does not include dependencies among a modifiee and the argument of the preposition.
Because previous studies revealed that such dependencies are effective features for
PP-attachment resolution, we should incorporate them into our model. Some of the
attachment ambiguities, including adjective and adverb, should also be resolved
with an extension of features. However, we cannot identify any effective features
for the disambiguation of attachment of verbal phrases, including relative clauses,
verb phrases, subordinate clauses, and to-infinitives. For example, Figure 25 shows
an example error of the attachment of a relative clause. The correct answer is that the
Table 15
Classification of disambiguation errors.
Error cause No. of errors
Attachment ambiguity prepositional phrase 32
relative clause 14
adjective 7
adverb 6
verb phrase 5
subordinate clause 3
to-infinitive 3
others 6
Argument/modifier distinction to-infinitive 19
noun phrase 7
verb phrase 7
subordinate clause 7
others 9
Lexical ambiguity preposition/modifier 13
verb subcategorization frame 13
participle/adjective 12
others 6
Test set errors errors of treebank conversion 18
errors of Penn Treebank 4
Comma 32
Noun phrase identification 15
Coordination/insertion 15
Zero-pronoun resolution 9
Others 4
69
Computational Linguistics Volume 34, Number 1
Figure 25
Examples of disambiguation errors.
subject of yielded is acre, but this cannot be determined only by the relation among yield,
grapes, and acre. The resolution of these errors requires a novel type of feature function.
Errors of argument/modifier distinction are prominent in deep syntactic analysis,
because arguments and modifiers are not explicitly distinguished in the evaluation of
CFG parsers. Figure 25 shows an example of the argument/modifier distinction of a
to-infinitive clause. In this case, the to-infinitive clause is a complement of tempts. The
subcategorization frame of tempts seems responsible for this problem. However, the
disambiguation model wrongly assigned a lexical entry for a transitive verb because
of the sparseness of the training data (tempts occurred only once in the training data).
The resolution of this sort of ambiguity requires the refinement of a probabilistic model
of lexical entries. Errors of verb phrases and subordinate clauses are similar to this
example. Errors of argument/modifier distinction of noun phrases aremainly caused by
temporal nouns and cardinal numbers. The resolution of these errors seems to require
the identification of temporal expressions and usage of cardinal numbers.
Errors of lexical ambiguities were mainly caused by idioms. For example, in Fig-
ure 25, compared with is a compound preposition, but the parser recognized it as a
verb phrase. This indicates that the grammar or the disambiguation model requires
the special treatment of idioms. Errors of verb subcategorization frames were mainly
caused by difficult constructions such as insertions. Figure 25 shows that the parser
could not identify the inserted clause (says John Siegel. . .) and a lexical entry for a
declarative transitive verb was chosen.
Attachment errors of commas are also significant. It should be noted that commas
were ignored in the evaluation of CFG parsers. We did not eliminate punctuation
from the evaluation because punctuation sometimes contributes to semantics, as
in coordination and insertion. In this error analysis, errors of commas representing
coordination/insertion are classified into ?coordination/insertion,? and ?comma? in-
dicates errors that do not contribute to the computation of semantics.
Errors of noun phrase identification mean that a noun phrase was split into two
phrases. These errors were mainly caused by the indirect effects of other errors.
70
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Errors of identifying coordination/insertion structures sometimes resulted in
catastrophic analyses. While accurate analysis of such constructions is indispensable,
it is also known to be difficult because disambiguation of coordination/insertion
requires the computation of preferences over global structures, such as the similarity
of syntactic/semantic structure of coordinates. Incorporating features for representing
the similarity of global structures is difficult for feature forest models.
Zero-pronoun resolution is also a difficult problem. However, we found that
most were indirectly caused by errors of argument/modifier distinction in to-infinitive
clauses.
A significant portion of the errors discussed above cannot be resolved by the fea-
tures we investigated in this study, and the design of other features will be necessary
for improving parsing accuracy.
6. Discussion
6.1 Probabilistic Modeling of Complete Structures
The model described in this article was first published in Miyao and Tsujii (2002), and
has been applied to probabilistic models for parsing with lexicalized grammars. Appli-
cations to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al
2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained
higher accuracy than other models. These researchers applied feature forests to repre-
sentations of the packed parse results of LFG and the dependency/derivation structures
of CCG. Their work demonstrated the applicability and effectiveness of feature forest
models in parsing with wide-coverage lexicalized grammars. Feature forest models
were also shown to be effective for wide-coverage sentence realization (Nakanishi,
Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic
enough to be applied to natural language processing tasks other than parsing.
The work of Geman and Johnson (2002) independently developed a dynamic pro-
gramming algorithm for maximum entropy models. The solution was similar to our
approach, although their method was designed to traverse LFG parse results repre-
sented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).
The difference between the two approaches is that feature forests use a simpler generic
data structure to represent packed forest structures. Therefore, without assuming what
feature forests represent, our algorithm can be applied to various tasks, including
theirs.
Another approach to the probabilistic modeling of complete structures is a method
of approximation. The work on whole sentence maximum entropy models (Rosenfeld
1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate
parameters of maximum entropy models on whole sentence structures. However, the
algorithm suffered from slow convergence, and the model was basically a sequence
model. It could not produce a solution for complex structures as our model can.
We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum,
and Pereira 2001) for solving a similar problem in the context of maximum entropy
Markov models. Their solution was an algorithm similar to the computation of
forward/backward probabilities of hidden Markov models (HMMs). Their algorithm is
a special case of our algorithm in which each conjunctive node has only one daughter.
This is obvious because feature forests can represent Markov chains. In an analogy,
CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.
71
Computational Linguistics Volume 34, Number 1
Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also
regarded as instances of feature forest models. This fact implies that our algorithm is
applicable to not only parsing but also to other tasks. CRFs are now widely used for
sequence-based tasks, such as parts-of-speech tagging and named entity recognition,
and have been shown to achieve the best performance in various tasks (McCallum and
Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al 2003; Sha and Pereira
2003; Peng and McCallum 2004; Roark et al 2004; Settles 2004; Sutton, Rohanimanesh,
and McCallum 2004). These results suggest that the method proposed in the present
article will achieve high accuracy when applied to various statistical models with
tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton,
Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for
extending feature forest models. The purpose of dynamic CRFs is to incorporate feature
functions that are not represented locally, and the solution is to apply a variational
method, which is an algorithm of numerical computation, to obtain approximate so-
lutions. A similar method may be developed to overcome a bottleneck of feature forest
models, that is, the fact that feature functions are localized to conjunctive nodes.
The structure of feature forests is common in natural language processing and
computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse
trees are represented by feature forests. Furthermore, because conjunctive nodes do
not necessarily represent CFG nodes or rules and terminals of feature forests need
not be words, feature forests can express any forest structures in which ambiguities
are packed in local structures. Examples include the derivation trees of LTAG and
CCG. Chiang (2003) proved that feature forests could be considered as the derivation
forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi
1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and
CCG, while preserving polynomial-time complexity of parsing. This demonstrates that
feature forest models are applicable to probabilistic models far beyond PCFGs. Feature
forests are also isomorphic to support graphs (or explanation graphs) used in the graphical
EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic pro-
gramming language, PRISM (Sato and Kameya 1997), is converted into support graphs,
and parameters of probabilistic models are automatically learned by an EM algorithm.
Support graphs have been proved to represent various statistical structural models, in-
cluding HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato
and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability
of feature forest models to various real tasks.
Because feature forests have a structure isomorphic to parse forests of PCFG, it
might seem that they can represent only immediate dominance relations of CFG rules
as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein,
however, feature forests can represent structures beyond CFG parse trees. Furthermore,
because feature forests are a generalized representation of ambiguous structures, each
node in a feature forest need not correspond to a node in a PCFG parse forest. That is,
a node in a feature forest may represent any linguistic entity, including a fragment of a
syntactic structure, a semantic relation, or other sentence-level information.
The idea of feature forest models could be applied to non-probabilistic machine
learning methods. Taskar et al (2004) proposed a dynamic programming algorithm
for the learning of large-margin classifiers including support vector machines (Vapnik
1995), and presented its application to disambiguation in CFG parsing. Their algorithm
resembles feature forest models; an optimization function is computed by a dynamic
programing algorithmwithout unpacking packed forest structures. From the discussion
in this article, it is evident that if the main part of an update formula is represented
72
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
with (the exponential of) linear combinations, a method similar to feature forest models
should be applicable.
6.2 Probabilistic Parsing with Lexicalized Grammars
Before the advent of feature forest models, studies on probabilistic models of HPSG
adopted conventional maximum entropy models to select the most probable parse from
parse candidates given by HPSG grammars (Oepen, Toutanova, et al 2002; Toutanova
and Manning 2002; Baldridge and Osborne 2003). The difference between these studies
and our work is that we used feature forests to avoid the exponential increase in the
number of structures that results from unpacked parse results. These studies ignored
the problem of exponential explosion; in fact, training sets in these studies were very
small and consisted only of short sentences. A possible approach to avoid this problem
is to develop a fully restrictive grammar that never causes an exponential explosion, al-
though the development of such a grammar requires considerable effort and it cannot be
acquired from treebanks using existing approaches.We think that exponential explosion
is inevitable, particularly with the large-scale wide-coverage grammars required to an-
alyze real-world texts. In such cases, these methods of model estimation are intractable.
Another approach to estimating log-linear models for HPSG was to extract a small
informative sample from the original set T(w) (Osborne 2000). The method was suc-
cessfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible
problem with this method is in the approximation of exponentially many parse trees by
a polynomial-size sample. However, their method has an advantage in that any features
on parse results can be incorporated into a model, whereas our method forces feature
functions to be defined locally on conjunctive nodes. We will discuss the trade-off
between the approximation solution and the locality of feature functions in Section 6.3.
Non-probabilistic statistical classifiers have also been applied to disambiguation in
HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector
machines (Toutanova, Markova, and Manning 2004). However, the problem of expo-
nential explosion is also inevitable using their methods. As described in Section 6.1, an
approach similar to ours may be applied, following the study of Taskar et al (2004).
A series of studies on parsing with LFG (Johnson et al 1999; Riezler et al 2000,
2002) also proposed a maximum entropy model for probabilistic modeling of LFG pars-
ing. However, similarly to the previous studies on HPSG parsing, these groups had
no solution to the problem of exponential explosion of unpacked parse results. As dis-
cussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum
entropy estimation for packed representations of LFG parses.
Recent studies on CCG have proposed probabilistic models of dependency struc-
tures or predicate?argument dependencies, which are essentially the same as the
predicate?argument structures described in the present article. Clark, Hockenmaier, and
Steedman (2002) attempted the modeling of dependency structures, but the model was
inconsistent because of the violation of the independence assumption. Hockenmaier
(2003) proposed a consistent generative model of predicate?argument structures. The
probability of a non-local dependency was conditioned on multiple words to preserve
the consistency of the probability model; that is, probability p(I|want, dispute) in Sec-
tion 4.3 was directly estimated. The problem was that such probabilities could not be
estimated directly from the data due to data sparseness, and a heuristic method had
to be employed. Probabilities were therefore estimated as the average of individual
probabilities conditioned on a single word. Another problem is that the model is no
longer consistent when unification constraints such as those in HPSG are introduced.
73
Computational Linguistics Volume 34, Number 1
Our solution is free of these problems, and is applicable to various grammars, not only
HPSG and CCG.
Most of the state-of-the-art studies on parsing with lexicalized grammars have
adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al 2004; Riezler
and Vasserman 2004). Their methods of translating parse results into feature forests are
basically the same as our method described in Section 4, and details differ because
different grammar theories represent syntactic structures differently. They reported
higher accuracy in parsing the Penn Treebank than the previous methods introduced
herein, and these results attest the effectiveness of feature forest models in practical
deep parsing. A remaining problem is that no studies could provide empirical compar-
isons across grammar theories. The above studies and our research evaluated parsing
accuracy on their own test sets. The construction of theory-independent standard test
sets requires enormous effort because we must establish theory-independent criteria
such as agreed definitions of phrases and headedness. Although this issue is beyond
the scope of the present article, it is a fundamental obstacle to the transparency of these
studies on parsing.
Clark and Curran (2004a) described a method for reducing the cost of parsing a
training treebank without sacrificing accuracy in the context of CCG parsing. They first
assigned each word a small number of supertags, corresponding to lexical entries in
our case, and parsed supertagged sentences. Because they did not use the probabilities of
supertags in a parsing stage, their method corresponds to our ?filtering only? method.
The difference from our approach is that they also applied the supertagger in a parsing
stage. We suppose that this was crucial for high accuracy in their approach, although
empirical investigation is necessary.
6.3 Trade-Off between Dynamic Programming and Feature Locality
The proposed algorithm is an essential solution to the problem of estimating probabilis-
tic models on exponentially many complete structures. However, the applicability of
this algorithm relies on the constraint that features are defined locally in conjunctive
nodes. As discussed in Section 6.1, this does not necessarily mean that features in our
model can represent only the immediate-dominance relations of CFG rules, because
conjunctive nodesmay encode any fragments of complete structures. In fact, we demon-
strated in Section 4.3 that certain assumptions allowed us to encode non-local predicate?
argument dependencies in tractable-size feature forests. In addition, although in the
experiments we used only features on bilexical dependencies, the method described in
Section 4.3 allows us to define any features on a predicate and all of its arguments, such
as a ternary relation among a subject, a verb, and a complement (e.g., the relation among
I, want, and dispute1 in Figure 21), and a generalized relation among semantic classes
of a predicate and its arguments. This is because a predicate and all of its arguments
are included in a conjunctive node, and feature functions can represent any relations
expressed within a conjunctive node.
Whenwe definemore global features, such as co-occurrences of structures at distant
places in a sentence, conjunctive nodes must be expanded so that they include all
structures that are necessary to define these features. However, this obviously increases
the number of conjunctive nodes, and consequently, the cost of parameter estimation
increases. In an extreme case, for example, if we define features on any co-occurrences
of partial parse trees, the full unpacking of parse forests would be necessary, and pa-
rameter estimation would be intractable. This indicates that there is a trade-off between
the locality of features and the cost of estimation. That is, larger context features might
74
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
contribute to higher accuracy, while they inflate the size of feature forests and increase
the cost of parameter estimation.
Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000;
Malouf and van Noord 2004) allow us to define any features on complete structures
without any constraints. However, they force us to employ approximation methods
for tractable computation. The effectiveness of those techniques therefore relies on
convergence speed and approximation errors, which may vary depending on the char-
acteristics of target problems and features.
It is an open research question whether dynamic programming or sampling can
deliver a better balance of estimation efficiency and accuracy. The answer will differ in
different problems. When most effective features can be represented locally in tractable-
size feature forests, dynamic programming methods including ours are suitable.
However, when global context features are indispensable for high accuracy, sampling
methods might be better. We should also investigate compromise solutions such as
dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh,
and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson
2005). There is no analytical way of predicting the best solution, and it must be
investigated experimentally for each target task.
7. Conclusion
A dynamic programming algorithm was presented for maximum entropy modeling
and shown to provide a solution to the parameter estimation of probabilistic models of
complete structures without the independence assumption. We first defined the notion
of a feature forest, which is a packed representation of an exponential number of trees of
features. When training data is represented with feature forests, model parameters are
estimated at a tractable cost without unpacking the forests. Themethod provides a more
flexible modeling scheme than previous methods of application of maximum entropy
models to natural language processing. Furthermore, it is applicable to complex data
structures where an event is difficult to decompose into independent sub-events.
We also demonstrated that feature forest models are applicable to probabilistic mod-
eling of linguistic structures such as the syntactic structures of HPSG and predicate?
argument structures including non-local dependencies. The presented approach can
be regarded as a general solution to the probabilistic modeling of syntactic analysis
with lexicalized grammars. Table 16 summarizes the best performance of the HPSG
parser described in this article. The parser demonstrated impressively high coverage
and accuracy for real-world texts. We therefore conclude that the HPSG parser for
English is moving toward a practical level of use in real-world applications. Recently,
the applicability of the HPSG parser to practical applications, such as information
extraction and retrieval, has also been demonstrated (Miyao et al 2006; Yakushiji et al
2006; Chun 2007).
Table 16
Final results.
Parsing accuracy for Section 23 (<40 words)
# parsed sentences 2,137/2,144 (99.7%)
Precision/recall 87.69%/87.16%
Sentential accuracy 39.2%
75
Computational Linguistics Volume 34, Number 1
From our extensive investigation of HPSG parsing, we observed that exploration
of new types of features is indispensable to further improvement of parsing accuracy.
A possible research direction is to encode larger contexts of parse trees, which has
been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova,
and Manning 2004). Future work includes not only the investigation of these features
but also the abstraction of predicate?argument dependencies using semantic classes.
Experimental results also suggest that an improvement in grammar coverage is crucial
for higher accuracy. This indicates that an improvement in the quality of the grammar
is a key factor for the improvement of parsing accuracy.
The feature forest model provides new insight into the relationship between a
linguistic structure and a unit of probability. Traditionally, a unit of probability was
implicitly assumed to correspond to a meaningful linguistic structure; a tagging of a
word or an application of a rewriting rule. One reason for the assumption is to enable
dynamic programming algorithms, such as the Viterbi algorithm. The probability of a
complete structure must be decomposed into atomic structures in which ambiguities
are limited to a tractable size. Another reason is to estimate plausible probabilities.
Because a probability is defined over atomic structures, they should also be meaning-
ful so as to be assigned a probability. In feature forest models, however, conjunctive
nodes are responsible for the former, whereas feature functions are responsible for the
latter. Although feature functions must be defined locally in conjunctive nodes, they
are not necessarily equivalent. Conjunctive nodes may represent any fragments of a
complete structure, which are not necessarily linguistically meaningful. They should
be designed to pack ambiguities and enable us to define useful features. Meanwhile,
feature functions indicate an atomic unit of probability, and are designed to capture
statistical regularity of the target problem. We expect the separation of a unit of prob-
ability from linguistic structures to open up a new framework for flexible probabilistic
modeling.
Acknowledgments
The authors wish to thank the anonymous
reviewers of Computational Linguistics for
their helpful comments and discussions. We
would also like to thank Takashi Ninomiya
and Kenji Sagae for their precious support.
References
Abney, Steven P. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Baker, James K. 1979. Trainable grammars
for speech recognition. In Jared J. Wolf
and Dennis H. Klatt, editors, Speech
Communication Papers Presented at the 97th
Meeting of the Acoustical Society of America.
MIT Press, Cambridge, MA, pages 547?550.
Baldridge, Jason and Miles Osborne. 2003.
Active learning for HPSG parse selection.
In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL
2003, pages 17?24, Edmonton, Canada.
Berger, AdamL., StephenA. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview,
results and evaluation. In Proceedings
of the IJCNLP-04 Workshop ?Beyond Shallow
Analyses?, Hainan Island. Available
at www-tsujii.is.s.u-tokyo.ac.jp/bsa.
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge University
Press, Cambridge, England.
Carroll, John and Stephan Oepen. 2005.
High efficiency realization for a
wide-coverage unification grammar.
In Proceedings of the 2nd International
Joint Conference on Natural Language
Processing (IJCNLP-05), pages 165?176,
Jeju Island.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the First Conference on North American
Chapter of the Association for Computational
76
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Linguistics (NAACL 2000), pages 132?139,
Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 173?180, Ann Arbor, MI.
Chen, Stanley and Ronald Rosenfeld.
1999a. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMUCS-99-108, Carnegie
Mellon University.
Chen, Stanley F. and Ronald Rosenfeld.
1999b. Efficient sampling and feature
selection in whole sentence maximum
entropy language models. In Proceedings
of the 1999 IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 549?552, Phoenix, AZ.
Chiang, David. 2003. Mildly context sensitive
grammars for estimating maximum
entropy parsing models. In Proceedings of
the 8th Conference on Formal Grammar,
pages 19?31, Vienna.
Chun, Hong-Woo. 2007.Mining Literature for
Disease-Gene Relations. Ph.D. thesis,
University of Tokyo.
Clark, Stephen and James R. Curran. 2003.
Log-linear models for wide-coverage
CCG parsing. In Proceedings of the 2003
Conference on Empirical Methods in
Natural Language Processing (EMNLP 2003),
pages 97?104, Sapporo.
Clark, Stephen and James R. Curran.
2004a. The importance of supertagging
for wide-coverage CCG parsing.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING 2004), pages 282?288, Geneva.
Clark, Stephen and James R. Curran. 2004b.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 104?111,
Barcelona.
Clark, Stephen, Julia Hockenmaier, and
Mark Steedman. 2002. Building deep
dependency structures with a wide-
coverage CCG parser. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002),
pages 327?334, Philadephia.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics (ACL?97), pages 16?23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language
parsing. In Proceedings of the Seventeenth
International Conference on Machine
Learning, pages 175?182, Palo Alto, CA.
Collins,Michael. 2003. Head-driven statistical
models for natural language parsing.
Computational Linguistics, 29(4):589?637.
Copestake, Ann, Dan Flickinger,
Rob Malouf, Susanne Riehemann, and
Ivan Sag. 1995. Translation using minimal
recursion semantics. In Proceedings of the
Sixth International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI95), pages 15?32, Leuven.
Copestake, Ann, Dan Flickinger, Ivan A. Sag,
and Carl Pollard. 2006. Minimal recursion
semantics: An introduction. Research
on Language and Computation, 3(4):281?332.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. The Annals of Mathematical
Statistics, 43(5):1470?1480.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380?393.
Geman, Stuart and Mark Johnson. 2002.
Dynamic programming for parsing and
estimation of stochastic unification-based
grammars. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL 2002), pages 279?286,
Philadelphia, PA.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 167?202, Pittsburgh, PA.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate-argument
structure. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL 2003), pages 359?366,
Sapporo.
Hockenmaier, Julia and Mark Steedman.
2002. Acquiring compact lexicalized
grammars from a cleaner treebank.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC-2002), pages 1974?1981,
Las Palmas.
77
Computational Linguistics Volume 34, Number 1
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings
of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL?99),
pages 535?541, College Park, Maryland.
Johnson, Mark and Stefan Riezler. 2000.
Exploiting auxiliary distributions in
stochastic unification-based grammars.
In Proceedings of the First Conference
on North American Chapter of the
Association for Computational Linguistics,
pages 154?161, Seattle, WA.
Kameya, Yoshitaka and Taisuke Sato.
2000. Efficient EM learning with tabulation
for parameterized logic programs.
In Proceedings of the 1st International
Conference on Computational Logic
(CL2000), volume 1861 of Lecture Notes
in Artificial Intelligence (LNAI),
pages 269?294, Imperial College, London.
Kaplan, Ronald M., Stefan Riezler, Tracy H.
King, John T. Maxwell, III, Alexander
Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings
of the Human Language Technology
Conference and the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 97?104, Boston, MA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the International Conference
on Machine Learning 2001, pages 282?289,
Williams College, Williamstown, MA.
Makino, Takaki, Yusuke Miyao, Kentaro
Torisawa, and Jun?ichi Tsujii. 2002.
Native-code compilation of feature
structures. In Stephen Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans
Uszkoreit, editors, Collaborative
Language Engineering: A Case Study
in Efficient Grammar-based Parsing. CSLI
Publications, Palo Alto, CA, pages 49?80.
Malouf, Robert. 2002. A comparison
of algorithms for maximum entropy
parameter estimation. In Proceedings
of the Sixth Conference on Natural
Language Learning (CoNLL-2002),
pages 1?7, Taipei.
Malouf, Robert and Gertjan van Noord.
2004. Wide coverage parsing with
stochastic attribute value grammars.
In Proceedings of the IJCNLP-04 Workshop
?Beyond Shallow Analyses?, Hainan Island.
Available at www.tsujii.is.s.u-tokyo.
ac.jp/bsa.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate
argument structure. In Proceedings
of the Workshop on Human Language
Technology, pages 114?119, Plainsboro, NJ.
Maxwell John T., III and Ronald M. Kaplan.
1995. A method for disjunctive constraint
satisfaction. In Mary Dalrymple, Ronald
M. Kaplan, John T. Maxwell, III, and
Annie Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, number 47
in CSLI Lecture Notes Series. CSLI
Publications, Palo Alto, CA, chapter 14,
pages 381?481.
McCallum, Andrew and Wei Li. 2003.
Early results for named entity recognition
with conditional random fields, feature
induction and web-enhanced lexicons.
In Proceedings of the 7th Conference
on Natural Language Learning (CoNLL),
pages 188?191, Edmonton.
McCallum, Andrew, Khashayar
Rohanimanesh, and Charles Sutton.
2003. Dynamic conditional random fields
for jointly labeling multiple sequences. In
Proceedings of the Workshop on Syntax,
Semantics, Statistics at the 16th Annual
Conference on Neural Information Processing
Systems, Vancouver. Available at
www.cs.umasse.du/?mccallum/papers/
derf-nips03.pdf.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including
non-local dependencies. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing
(RANLP 2003), pages 285?291, Borovets.
Miyao, Yusuke, Takashi Ninomiya,
and Jun?ichi Tsujii. 2005. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Natural
Language Processing - IJCNLP 2004,
pages 684?693, Hainan Island.
Miyao, Yusuke, Tomoko Ohta,
Katsuya Masuda, Yoshimasa Tsuruoka,
Kazuhiro Yoshida, Takashi Ninomiya,
and Jun?ichi Tsujii. 2006. Semantic retrieval
for the accurate identification of relational
concepts in massive textbases. In
Proceedings of the Joint Conference of the 21st
International Conference on Computational
Linguistics and the 44th Annual Meeting of the
78
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Association for Computational Linguistics
(COLING-ACL 2006), pages 1017?1024,
Sydney.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of the Human
Language Technology Conference (HLT-2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii. 2003.
A model of syntactic disambiguation
based on lexicalized grammars. In
Proceedings of the Seventh Conference on
Computational Natural Language Learning
(CoNLL-2003), pages 1?8, Edmonton.
Miyao, Yusuke and Jun?ichi Tsujii. 2005.
Probabilistic disambiguation models
for wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 83?90, Ann Arbor, MI.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2005. Probabilistic models
for disambiguation of an HPSG-based
chart generator. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT 2005), pages 93?102,
Vancouver.
Ninomiya, Takashi, Yoshimasa Tsuruoka,
Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Efficacy of beam thresholding, unification
filtering and hybrid parsing in probabilistic
HPSG parsing. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 103?114, Vancouver.
Nocedal, Jorge. 1980. Updating
quasi-Newton matrices with limited
storage.Mathematics of Computation,
35:773?782.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
New York.
Oepen, Stephan and John Carroll. 2000.
Ambiguity packing in constraint-based
parsing: practical results. In Proceedings
of the First Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL 2000), pages 162?169,
Seattle, WA.
Oepen, Stephan, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors. 2002.
Collaborative Language Engineering: A Case
Study in Efficient Grammar-Based Processing.
CSLI Publications, Palo Alto, CA.
Oepen, Stephan, Kristina Toutanova,
Stuart Shieber, Christopher Manning, Dan
Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebankmotivation and
preliminary applications. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
volume 2, pages 1?5, Taipei.
Osborne, Miles. 2000. Estimation of
stochastic attribute-value grammar using
an informative sample. In Proceedings
of the 18th International Conference on
Computational Linguistics (COLING 2000),
volume 1, pages 586?592, Saarbru?cken.
Peng, Fuchun and Andrew McCallum. 2004.
Accurate information extraction from
research papers using conditional
random fields. In Proceedings of Human
Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT/
NAACL-04), pages 329?336, Boston, MA.
Pinto, David, Andrew McCallum, Xen Lee,
and W. Bruce Croft. 2003. Table extraction
using conditional random fields. In
Proceedings of the 26th Annual International
ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR 2003), pages 235?242, Toronto.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago, IL.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T.
Maxwell, III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using
a lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL 2002), pages 271?278,
Philadephia, PA.
Riezler, Stefan, Detlef Prescher, Jonas Kuhn,
and Mark Johnson. 2000. Lexicalized
stochastic modeling of constraint-based
grammars using log-linear measures
and EM training. In Proceedings of the
38th Annual Meeting of the Association
for Computational Linguistics (ACL 2000),
pages 480?487, Hong Kong.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed maximum-
entropy modeling. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 174?181, Barcelona.
Roark, Brian, Murat Saraclar, Michael Collins,
and Mark Johnson. 2004. Discriminative
language modeling with conditional
random fields and the perceptron
algorithm. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 47?54,
Barcelona.
79
Computational Linguistics Volume 34, Number 1
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding,
pages 230?237, Santa Barbara, CA.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Syntactic Theory: A Formal
Introduction. Number 152 in CSLI Lecture
Notes. CSLI Publications, Standford, CA.
Sarawagi, Sunita and William W. Cohen.
2004. Semi-Markov conditional random
fields for information extraction. In
Proceedings of the 18th Annual Conference
on Neural Information Processing
Systems, pages 1185?1192, Vancouver.
Sato, Taisuke. 2005. A generic approach to em
learning for symbolic-statistical models. In
Proceedings of the 4th Learning Language in
Logic Workshop (LLL05), pages 21?28, Bonn.
Sato, Taisuke and Yoshitaka Kameya. 1997.
PRISM: a language for symbolic-statistical
modeling. In Proceedings of the 15th
International Joint Conference on Artificial
Intelligence (IJCAI ?97), pages 1330?1335,
Nagoya.
Sato, Taisuke and Yoshitaka Kameya. 2001.
Parameter learning of logic programs for
symbolic-statistical modeling. Journal of
Artificial Intelligence Research, 15:391?454.
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random fields
and rich feature sets. In Proceedings of the
International Joint Workshop on Natural
Language Processing in Biomedicine and its
Applications (NLPBA), pages 104?107,
Geneva.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In Proceedings of the 2003
Human Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2003), pages 213?220, Edmonton.
Shieber, Stuart M. 1985. Using restriction
to extend parsing algorithms for
complex-feature-based formalisms. In
Proceedings of the 23rd Annual Meeting
on Association for Computational Linguistics,
pages 145?152, Chicago, IL.
Sutton, Charles, Khashayar Rohanimanesh,
and Andrew McCallum. 2004. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. In Proceedings
of the 21st International Conference
on Machine Learning (ICML 2004),
pages 783?790, Alberta.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Chris Manning.
2004. Max-margin parsing. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 1?8, Barcelona.
Toutanova, Kristina and Christopher
Manning. 2002. Feature selection for a
rich HPSG grammar using decision trees.
In Proceedings of the Sixth Conference on
Natural Language Lerning (CoNLL-2002),
pages 77?83, Taipei.
Toutanova, Kristina, Penka Markova,
and Christopher Manning. 2004. The
leaf projection path view of parse trees:
Exploring string kernels for HPSG parse
selection. In Proceedings of the 2004
Conference on Empirical Methods in
Natural Language Processing (EMNLP
2004), pages 166?173, Barcelona.
Tsujii Laboratory. 2004. Enju?A practical
HPSG parser. Available at http://www.
tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsuruoka, Yoshimasa, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Towards efficient
probabilistic HPSG parsing: Integrating
semantic and syntatic preference
to guide the parsing. In Proceedings
of the IJCNLP-04 Workshop ?Beyond Shallow
Analyses?, Hainan Island. Available
at www.tsujii.is.s.u-tokyo.ac.jp/bsa.
Tsuruoka, Yoshimasa and Jun?ichi Tsujii.
2005. Bidirectional inference with the
easiest-first strategy for tagging sequence
data. In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP 2005),
pages 467?474, Vancouver.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting
of the Association for Computational
Linguistics, pages 104?111, Palo Alto, CA.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Yakushiji, Akane, Yusuke Miyao,
Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2006. Automatic construction of
predicate-argument structure patterns for
biomedical information extraction.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2006), pages 284?292,
Sydney.
80

Generic NLP Technologies: Language, Knowledge and
Information Extraction
Junichi Tsujii
Department of Information Science, Faculty of Science
University of Tokyo, JAPAN
And
Centre for Computational Linguistics, UMIST, UK
1 Introduction
We have witnessed signicant progress in
NLP applications such as information ex-
traction (IE), summarization, machine trans-
lation, cross-lingual information retrieval
(CLIR), etc. The progress will be accelerated
by advances in speech technology, which not
only enables us to interact with systems via
speech but also to store and retrieve texts in-
put via speech.
The progress of NLP applications in this
decade has been mainly accomplished by the
rapid development of corpus-based and sta-
tistical techniques, while rather simple tech-
niques have been used as far as the structural
aspects of language are concerned.
In this paper, we will discuss how we
can combine more sophisticated, linguistically
elaborate techniques with the current statis-
tical techniques and what kinds of improve-
ment we can expect from such an integration
of dierent knowledge types and methods.
2 Argument against linguistically
elaborate techniques
Throughout the 80s, research based on lin-
guistics had ourished even in application ori-
ented NLP research such as machine transla-
tion. Eurotra, a European MT project, had
attracted a large number of theoretical lin-
guists into MT and the linguists developed
clean and linguistically elaborate frameworks
such as CTA-2, Simple Transfer, Eurotra-6,
etc.
ATR, a Japanese research institute for
telephone dialogue translation supported by
a consortium of private companies and the
Ministry of Post and Communication, also
adopted a linguistics-based framework, al-
though they changed their direction in the
later stage of the project. They also adopted
sophisticated plan-based dialogue models as
well at the initial stage of the project.
However, the trend changed rather dras-
tically in the early 90s and most research
groups with practical applications in mind
gave up such strategies and switched to more
corpus-oriented and statistical methods. In-
stead of sentential parsing based on linguis-
tically well founded grammar, for example,
they started to use simpler but more ro-
bust techniques based on nite-state models.
Neither did knowledge-based techniques like
plan-recognition, etc. survive, which presume
explicit representation of domain knowledge.
One of the major reasons for the failure
of these techniques is that, while these tech-
niques alone cannot solve the whole range of
problems that NLP application encounters,
both linguists and AI researchers made strong
claims that their techniques would be able to
solve most, if not all, of the problems. Al-
though formalisms based on linguistic theo-
ries can certainly contribute to the develop-
ment of clean and modular frameworks for
NLP, it is rather obvious that linguistics the-
ories alone cannot solve most of NLP's prob-
lems. Most of MT's problems, for example,
are related with semantics or interpretation
of language which linguistic theories of syntax
can hardly oer solutions for (Tsujii 1995).
However, this does not imply, either, that
frameworks based on linguistic theories are of
no use for MT or NLP application in general.
This only implies that we need techniques
complementary to those based on linguistic
theories and that frameworks based on lin-
guistic theories should be augmented or com-
bined with other techniques. Since techniques
from complementary elds such as statistical
or corpus-based ones have made signicant
progresses, it is our contention in this paper
that we should start to think seriously about
combining the fruits of the research results of
the 80s with those of the 90s.
The other claims against linguistics-based
and knowledge-based techniques which have
often been made by practical-minded people
are :
(1) Eciency: The techniques such as sen-
tential parsing and knowledge-based in-
ference, etc. are slow and require a large
amount of memory
(2) Ambiguity of Parsing: Sentential
parsing tends to generate thousands of
parse results from which systems cannot
choose the correct one.
(3) Incompleteness of Knowledge and
Robustness: In practice one cannot
provide systems with complete knowl-
edge. Defects in knowledge often cause
failures in processing, which result in the
fragile behavior of systems.
While these claims may have been the case
during the 80s, the steady progress of such
technologies have largely removed these dif-
culties. Instead, the disadvantages of cur-
rent technologies based on nite state tech-
nologies, etc. have increasingly become
clearer; the disadvantages such ad-hocness
and opaqueness of systems which prevent
them from being transferred from an appli-
cation in one domain to another domain.
3 The current state of the JSPS
project
In a ve-year project funded by JSPS (Japan
Society of Promotion of Science) which
started in September 1996, we have focussed
our research on generic techniques that will
be used for dierent kinds of NLP application
and domains.
The project comprises three university
groups from the University of Tokyo, Tokyo
Institute of Technology (Prof. Tokunaga)
and Kyoto University (Dr. Kurohashi), and
coordinated by myself (at the University of
Tokyo). The University of Tokyo has been
engaged in development of software infras-
tructure for ecient NLP, parsing technology
and ontology building from texts, while the
groups of Tokyo Institute of Technology and
Kyoto University have been responsible for
NLP application to IR and Knowledge-based
NLP techniques, respectively.
Since we have delivered promising results in
research on generic NLP methods, we are now
engaged in developing several application sys-
tems that integrate various research results to
show their feasibility in actual application en-
vironments. One such application is a system
that helps biochemists working in the eld of
genome research.
The system integrates various research re-
sults of our project such as new techniques
for query expansion and intelligent indexing
in IR, etc. The two results to be integrated
into the system that we focus on in this paper
are IE using a full-parser (sentential parser
based on grammar) and ontology building
from texts.
IE is very much in demand in genome re-
search, since quite a large portion of research
is now being targeted to construct systems
that model complete sequences of interac-
tion of various materials in biological organ-
isms. These systems require extraction of rel-
evant information from texts and its integra-
tion in xed formats. This entails that the
researchers there should have a model of in-
teraction among materials, into which actual
pieces of information extracted from texts are
tted. Such a model should have a set of
classes of interaction (event classes) and a set
of classes of entities that participate in events.
That is, the ontology of the domain should ex-
ist. However, since the building of an ultimate
ontology is, in a sense, the goal of science,
the explicit ontology exists only in a very re-
stricted and partial form. In other words, IE
and Ontology building are inevitably inter-
twined here.
In short, we found that IE and Ontology
building from texts in genome research pro-
vide an ideal test bed for our generic NLP
techniques, namely software infrastructure for
ecient NLP, parsing technology, and ontol-
ogy building from texts with initial partial
knowledge of the domain.
4 Software Infrastructure and
Parsing Technology
While tree structures are a versatile scheme
for linguistic representation, invention of fea-
ture structures that allow complex features
and reentrancy (structure sharing) makes
linguistic representation concise and allows
declarative specications of mutual relation-
ships among representation of dierent lin-
guistic levels (e.g.: morphology, syntax, se-
mantics, discourse, etc.). More importantly,
using bundles of features instead of simple
non-terminal symbols to characterize linguis-
tic objects allow us to use much richer statis-
tical means such as ME (maximum entropy
model), etc. instead of simple probabilistic
CFG. However, the potential has hardly been
pursued yet mostly due to the ineciency and
fragility of parsing based on feature-based for-
malisms.
In order to remove the eciency obstacle,
we have in the rst two years devoted our-
selves to the development of :
(A) Software infrastructure that makes pro-
cessing of feature-based formalisms e-
cient enough both for practical applica-
tion and for combining it with statistical
means.
(B) Grammar (Japanese and English) with
wide coverage for processing real world
texts (not examples in textbooks of lin-
guistics). At the same time, processing
techniques that make a system robust
enough for application.
(C) Ecient parsing algorithm for
linguistics-based frameworks, in particu-
lar HPSG.
We describe the current states of these three
in the following.
(A) Software Infrastructure (Miyao
2000):
We designed and develop a programming sys-
tem, LiLFeS, which is an extension of Pro-
log for expressing typed feature structures in-
stead of rst order terms. The system's core
engine is an abstract machine that can pro-
cess features and execute denite clause pro-
gram. While similar attempts treat feature
structure processing separately from that of
denite clause programs, the LiLFeS abstract
machine increases processing speed by seam-
lessly processing feature structures and de-
nite clause programs.
Diverse systems, such as large scale English
and Japanese grammar, a statistical disam-
biguation module for the Japanese parser, a
robust parser for English, etc., have already
been developed in the LiLFeS system.
We compared the performance of the sys-
tem with other systems, in particular with
LKB developed by CSLI, Stanford Univer-
sity, by using the same grammar (LinGo also
provided by Stanford University). A parsing
system in the LiLFeS system, which adopts
a naive CKY algorithm without any sophis-
tication, shows similar performance as that
of LKB which uses a more rened algorithm
to lter out unnecessary unication. The de-
tailed examination reveals that feature uni-
cation of the LiLFeS system is about four
times faster than LKB.
Furthermore, since LiLFeS has quite a few
built-in functions that facilitate fast sub-
sumption checking, ecient memory manage-
ment, etc., the performance comparison re-
veals that more advanced parsing algorithms
like the one we developed in (C) can benet
from the LiLFeS system. We have almost n-
ished the second version of the LiLFeS system
that uses a more ne-grained instruction set,
directly translatable to naive machine code of
a Pentium CPU. The new version shows more
than twice improvement in execution speed,
which means the naive CKY algorithm with-
out any sophistication in the LiLFeS system
will outperform LKB.
(B) Grammar with wide coverage
(Tateisi 1998; Mitsuishi 1998):
While LinGo that we used for comparison is
an interesting grammar from the view point
of linguistics, the coverage of the grammar is
rather restricted. We have cooperated with
the University of Pennsylvania to develop a
grammar with wide coverage. In this co-
operation, we translated an existing wide-
coverage grammar of XTAG to the framework
of HPSG, since our parsing algorithms in (C)
all assume that the grammar are HPSG. As
we discuss in the following section, we will
use this translated grammar as the core gram-
mar for information extraction from texts in
genome science.
As for wide-coverage Japanese Gram-
mar, we have developed our own grammar
(SLUNG) . SLUNG exploits the property
of HPSG that allows under-specied con-
straints. That is, in order to obtain wide-
coverage from the very beginning of grammar
development, we only give loose constraints
to individual words that may over-generate
wrong interpretations but nonetheless guar-
antee correct ones to be always generated.
Instead of rather rigid and strict con-
straints, we prepare 76 templates for lexical
entries that specify behaviors of words be-
longing to these 76 classes. The approach
is against the spirit of HPSG or lexicalized
grammar that emphasizes constraints specic
to individual lexical items. However, our
goal is rst to develop wide-coverage gram-
mar that can be improved by adding lexical-
item specic constraints in the later stage
of grammar development. The strategy has
proved to be eective and the current gram-
mar can produce successful parse results for
98.3 % of sentences in the EDR corpus with
high eciency (0.38 sec per sentence for the
EDR corpus). Since the grammar overgen-
erates, we have to choose single parse results
among a combinatorially large number of pos-
sible parses. However, an experiment shows
that a statistic method using ME (we use the
program for ME developed by NYU) can se-
lect around 88.6 % of correct analysis in terms
of dependency relationships among ! ! bun-
setsu's - the phrases in Japanese).
(C) Ecient parsing algorithm
(Torisawa 2000):
While feature structure representation pro-
vides an eective means of representing lin-
guistic objects and constraints on them,
checking satisability of constraints by lin-
guistic objects, i.e. unication, is computa-
tionally expensive in terms of time and space.
One way of improving the eciency is to avoid
unication operations as much as possible,
while the other way is to provide ecient soft-
ware infrastructure such as in (A). Once we
choose a specic task like parsing, genera-
tion, etc., we can devise ecient algorithms
for avoiding unication.
LKB accomplishes such reduction by in-
specting dependencies among features, while
the algorithm we chose is to reduce necessary
unication by compiling given HPSG gram-
mar into CFG. The CFG skeleton of given
HPSG, which is semi-automatically extracted
from the original HPSG, is applied to pro-
duce possible candidates of parse trees in the
rst phase. The skeletal parsing based on ex-
tracted CFG lters out the local constituent
structures which do not contribute to any
parse covering the whole sentence. Since a
large proportion of local constituent struc-
tures do not actually contribute to the whole
parse, this rst CFG phase helps the second
phase to avoid most of the globally mean-
ingless unication. The eciency gain by
this compilation technique depends on the na-
ture of the original grammar to be compiled.
While the eciency gain for SLUNG is just
two times, the gain for XHPSG (HPSG gram-
mar obtained by translating the XTAG gram-
mar into HPSG) is around 47 times for the
ATIS corpus (Tateisi 1998).
5 Information extraction by
sentential parsing
The basic arguments against use of sentential
parsing in practical application such as IE are
the ineciency in terms of time and space,
the fragility of systems based on linguistically
rigid frameworks and highly ambiguous parse
results that we often have as results of pars-
ing.
On the other hand, there are arguments
for sentential parsing or the deep analysis
approach. One argument is that an ap-
proach based on linguistically sound frame-
works makes systems transparent and easy to
re-use. The other is the limit on the qual-
ity that is achievable by the pattern match-
ing approach. While a higher recall rate of
IE requires a large amount of patterns to
cover diverse surface realization of the same
information, we have to widen linguistic con-
texts to improve the precision by preventing
extraction of false information. A pattern-
based system may end up with a set of pat-
terns whose complex mutual nullify the initial
appeal of simplicity of the pattern-based ap-
proach.
As we see in the previous section, the e-
ciency problem becomes less problematic by
utilizing the current parsing technology. It
is still a problem when we apply the deep
analysis to texts in the eld of genome sci-
ence, which tend to have much longer sen-
tences than in the ATIS corpus. However, as
in the pattern-based approach, we can reduce
the complexity of problems by combining dif-
ferent techniques.
In a preliminary experiment, we rst use a
shallow parser (ENGCG) to reduce part-of-
speech ambiguities before sentential parsing.
Unlike statistic POS taggers, the constraint
grammar adopted by ENGCG preserves all
possible POS interpretations just by dropping
interpretations that are impossible in given lo-
cal contexts. Therefore, the use of ENGCG
does not aect the soundness and complete-
ness of the whole system, while it reduces sig-
nicantly the local ambiguities that do not
contribute to the whole parse.
The experiment shows that ENGCG pre-
vents 60 % of edges produced by a parser
Based on naive CKY algorithm, when it is ap-
plied to 180 sentences randomly chosen from
MEDLINE abstracts (Yakushiji 2000). As a
result, the parsing by XHPSG becomes four
times faster from 20.0 seconds to 4.8 second
per sentence, which is further improved by us-
ing chunking based on the output of a Named
Entity recognition tool to 2.57 second per sen-
tence. Since the experiment was conducted
with a naive parser based on CYK and the
old version of LiLFeS, the performance can
be improved further.
The problems of fragility and ambiguity
still remain. XHPSG fails to produce parses
for about half of the sentences that cover
the whole. However, in application such as
IE, a system needs not have parses covering
the whole sentence. If the part in which the
relevant pieces of information appear can be
parsed, the system can extract them. This is
one of the major reasons why pattern-based
systems can work in a robust manner. The
same idea can be used in IE based on sen-
tential parser. That is, techniques that can
extract information from partial parse results
will make the system robust.
The problem of ambiguity can be treated in
a similar manner. In a pattern-based system,
the system extracts information when parts of
the text match with a pattern, independently
of whether other interpretations that compete
with the interpretation intended by the pat-
tern exist or not. In this way, a pattern-based
system treats ambiguity implicitly. In case
of the approach based on sentential parsing,
we treat the ambiguity problem by preference.
That is, an interpretation that indicates rel-
evant pieces of information exist is preferred
to other interpretations.
Although the methods illustrated in the
above make IE based on sentential pars-
ing similar to the pattern-based approach,
the approach retains the advantages over the
pattern-based one. For example, it can pre-
vent false extraction if the pattern that dic-
tates extraction contradicts with wider lin-
guistic structures or with the more preferred
interpretations. It keeps separate the general
linguistic knowledge embodied in the form of
XHPSG grammar that can be used in any do-
main. The mapping between syntactic struc-
tures to predicate structures can also be sys-
tematic.
6 Information extraction of named
entities using a hidden Markov
model
The named entity tool mentioned above,
called NEHMM (Collier 2000), has been de-
veloped as a generalizable supervised learning
method for identifying and classifying terms
given a training corpus of SGML marked-up
texts. HMMs themselves belong to a class of
learning algorithms that can be considered to
be stochastic nite state machines. They have
enjoyed success in a wide number of elds in-
cluding speech recognition and part of speech
tagging. We therefore consider their exten-
sion to the named entity task, which is es-
sentially a kind of semantic tagging of words
based on their class, to be quite natural.
NEHMM itself strives to be highly gen-
eralizable to terms in dierent domains and
the initial version uses bigrams based on lex-
ical and character features with one state
per name class. Data-sparseness is over-
come using the character features and linear-
interpolation.
Nobata et al (Nobata 1999) comment on
the particular diculties with identifying and
classifying terms in the biochemistry domain
including an open vocabulary and irregular
naming conventions as well as extensive cross-
over in vocabulary between classes. The irreg-
ular naming arises in part because of the num-
ber of researchers from dierent elds who
are working on the same knowledge discov-
ery area as well as the large number of pro-
teins, DNA etc. that need to be named. De-
spite the best eorts of major journals to stan-
dardize the terminology, there is also a sig-
nicant problem with synonymy so that of-
ten an entity has more than one name that is
widely used such as the protein names AKT
and PKB. Class cross-over of terms is another
problem that arises because many DNA and
RNA are named after the protein with which
they transcribe.
Despite the apparent simplicity of the
knowledge in NEHMM, the model has proven
to be quite powerful in application. In the
genome domain with only 80 training MED-
LINE abstracts it could achieve over 74% F-
score (a common metric for evaluation used in
IE that combines recall and precision). Simi-
lar performance has been found when training
using the dry-run and test set for MUC-6 (60
articles) in the news domain.
The next stage in the development of our
model is to train using larger test sets and
to incorporate wider contextual knowledge,
perhaps by marking-up for dependencies of
named-entities in the training corpus. This
extra level of structural knowledge should
help to constrain class assignment and also
to aid in higher levels of IE such as event ex-
traction.
7 Knowledge Building and Text
Annotation
Annotated corpora constitute not only an in-
tegral part of a linguistic investigation but
also an essential part of the design methodol-
ogy for an NLP systems. In particular, the de-
sign of IE systems requires clear understand-
ing of information formats of the domain, i.e.
what kinds of entities and events are consid-
ered as essential ingredients of information.
However, such information formats are often
implicit in the minds of domain specialists
and the process of annotating texts helps to
reveal them.
It is also the case that the mapping be-
tween information formats and surface lin-
guistic realization is not trivial and that cap-
turing the mapping requires empirical exam-
ination of actual corpora. While generic pro-
grams with learning ability may learn such
a mapping, learning algorithms need training
data, i.e. annotated corpora.
In order to design a NE recognition pro-
gram, for example, we have to have a reason-
able amount of annotated texts which show
in what linguistic contexts named entities ap-
pear and what internal structures typical lin-
guistic expressions of named entities of a given
eld have. Such human inspection of anno-
tated texts suggests feasible tools for NE (e.g.
HMM, ME, decision trees, dictionary look-up,
etc.) and a set of feasible features, if one uses
programs with learning ability. Human in-
spection of annotated corpora is still an in-
evitable step of feature selection, even if one
uses programs with learning ability.
More importantly, to determine classes of
named entities and events which should re-
ect the views of domain specialists requires
empirical investigation, since these often exist
implicitly only in the mind of specialists. This
is particularly the case in the eld of med-
ical and biological sciences, since they have
a much larger collection of terms (i.e. class
names) than, for example, mathematical sci-
ence, physics, etc.
In order to see the magnitude of the work
and diculties involved, we chose a well-
circumscribed eld and collected texts (MED-
LINE abstracts) in the eld to be annotated.
The eld is the reaction of transcription fac-
tors in human blood cells. The kinds of infor-
mation that we try to extract are the infor-
mation on protein-protein interactions.
The eld was chosen because a research
group of National Health Research Institute
of the Ministry of Health in Japan is building
a database called CSNDB (Cell Signal Net-
work DB), which gathers this type of infor-
mation. They read papers every week to ex-
tract relevant information and store them in
the database. IE of this eld can reduce the
work that is done manually at present.
We selected abstracts from MEDLINE by
the key words of "human", "transcription fac-
tors" and "blood cells", which yield 3300 ab-
stracts. The abstracts are from 100 to 200
words in length. 500 abstracts were chosen
randomly and annotated. Currently, seman-
tic annotation of 300 abstracts has been n-
ished and we expect 500 abstracts to be done
by April (Ohta 2000).
The task of annotation can be regarded as
identifying and classifying the terms that ap-
pear in texts according to a pre-dened clas-
sication scheme. The classication scheme,
in turn, reects the view of the elds that bio-
chemists have. That is, semantic tags we use
are the class names in an ontology of the eld.
Ontologies of biological terminology have
been created in projects such as the EU
funded GALEN project to provide a model
of biological concepts that can be used to
integrate heterogeneous information sources
while some ontologies such as MeSH are built
for the purpose of information retrieval Ac-
cording to their purposes, ontologies dier
from ne-grained to coarse ones and from as-
sociative to logical ones. Since there is no
appropriate ontology that covers the domain
that we are interested in, we decided to build
one for this specic domain.
The design of our ontology is in progress,
in which we distinguish classication based
on roles that proteins play in events from
that based on internal structures of proteins.
The former classication is closely linked with
classication of events. Since classication is
based on feature lattices, we plan to use the
LiLFeS system to dene these classication
schemes and their relationships among them.
8 Future Directions
While the researches of the 80s and 90s in
NLP focussed on dierent aspects of lan-
guage, they have been so far considered sepa-
rate development and no serious attempt has
been made to integrate them.
In the JSPS project, we have prepared nec-
essary background for such integration. Tech-
nological background such as ecient parsing,
a programming system based on types, etc.
will contribute to resolving eciency prob-
lems. The techniques such as NE recogni-
tion, staged architecture in conventional IE,
etc. will give hints on how to incorporate sev-
eral dierent techniques in the whole system.
A reasonable size of semantically annotated
texts, together with relevant ontology, have
been prepared.
We are engaged now in integrating these
components in the whole system, in order to
show how theoretical work, together with col-
lection of empirical data, can facilitate sys-
tematic development of NLP application sys-
tems.
References
Collier, N., Nobata, C., and Tsujii, J.: "Extract-
ing the Names of Genes and Gene products with
a Hidden Markov Model", COLING'2000 (Au-
gust), 2000
Mitsuishi, Y. et.al.: HPSG-style Underspecied
Japanese Grammar with Wide Coverage, in
Proc. of Coling-ACL 98, Montreal, 1998
Miyao, Y., Makino, T., et.al.: The LiLFeS Ab-
stract Machine and its Evaluation with LinGo,
A Special Issue on Ecient Processing of
HPSG, Journal of Natural Language Engineer-
ing, Cambridge University Press, 2000 (to ap-
pear)
Nobata, C., Collier, N., and Tsujii, J.: "Au-
tomatic Term Identication and Classication
in Biology Texts", in proceedings of the Nat-
ural Language Pacic Rim Symposium (NL-
PRS'99), Beijing, China, 1999.
Ohta, T., et.al.: A Semantically Tagged Corpus
based on an Ontology for Molecular Biology, in
Proc. of JSPS Symposium 2000, Tokyo, 2000
Tateisi, Y. et.al.: Translating the XTAG English
Grammar to HPSG, in Proc. of TAG+4 work-
shop, University of Pennsylvania, 1998
Torisawa, K. et.al.: An HPSG Parser with CFG
Filtering, A Special Issue on Ecient Process-
ing of HPSG, Journal of Natural language Pro-
cessing, Cambridge University Press, 2000 (to
appear)
Tsujii, J.: MT Research : Productivity and Con-
ventionality of Language, RANLP-95,Tzigov
Chark,Bulgaria,14-16 September,1995
Yakushiji, A.: Domain-Independent System for
Event Frame Extraction using an HPSG Parser,
Bsc Dissertation, Department of Information
Science, University of Tokyo, 2000
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1017?1024,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Retrieval for the Accurate Identification of Relational Concepts
in Massive Textbases
Yusuke Miyao? Tomoko Ohta? Katsuya Masuda? Yoshimasa Tsuruoka?
Kazuhiro Yoshida? Takashi Ninomiya? Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?Information Technology Center, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{yusuke,okap,kmasuda,tsuruoka,kyoshida,ninomi,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper introduces a novel framework
for the accurate retrieval of relational con-
cepts from huge texts. Prior to retrieval,
all sentences are annotated with predicate
argument structures and ontological iden-
tifiers by applying a deep parser and a term
recognizer. During the run time, user re-
quests are converted into queries of region
algebra on these annotations. Structural
matching with pre-computed semantic an-
notations establishes the accurate and effi-
cient retrieval of relational concepts. This
framework was applied to a text retrieval
system for MEDLINE. Experiments on
the retrieval of biomedical correlations re-
vealed that the cost is sufficiently small for
real-time applications and that the retrieval
precision is significantly improved.
1 Introduction
Rapid expansion of text information has motivated
the development of efficient methods of access-
ing information in huge texts. Furthermore, user
demand has shifted toward the retrieval of more
precise and complex information, including re-
lational concepts. For example, biomedical re-
searchers deal with a massive quantity of publica-
tions; MEDLINE contains approximately 15 mil-
lion references to journal articles in life sciences,
and its size is rapidly increasing, at a rate of more
than 10% yearly (National Library of Medicine,
2005). Researchers would like to be able to
search this huge textbase for biomedical correla-
tions such as protein-protein or gene-disease asso-
ciations (Blaschke and Valencia, 2002; Hao et al,
2005; Chun et al, 2006). However, the framework
of traditional information retrieval (IR) has diffi-
culty with the accurate retrieval of such relational
concepts because relational concepts are essen-
tially determined by semantic relations between
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
The present paper demonstrates a framework
for the accurate real-time retrieval of relational
concepts from huge texts. Prior to retrieval, we
prepare a semantically annotated textbase by ap-
plying NLP tools including deep parsers and term
recognizers. That is, all sentences are annotated
in advance with semantic structures and are stored
in a structured database. User requests are con-
verted on the fly into patterns of these semantic
annotations, and texts are retrieved by matching
these patterns with the pre-computed semantic an-
notations. The accurate retrieval of relational con-
cepts is attained because we can precisely describe
relational concepts using semantic annotations. In
addition, real-time retrieval is possible because se-
mantic annotations are computed in advance.
This framework has been implemented for a
text retrieval system for MEDLINE. We first ap-
ply a deep parser (Miyao and Tsujii, 2005) and
a dictionary-based term recognizer (Tsuruoka and
Tsujii, 2004) to MEDLINE and obtain annotations
of predicate argument structures and ontological
identifiers of genes, gene products, diseases, and
events. We then provide a search engine for these
annotated sentences. User requests are converted
into queries of region algebra (Clarke et al, 1995)
extended with variables (Masuda et al, 2006) on
these annotations. A search engine for the ex-
tended region algebra efficiently finds sentences
having semantic annotations that match the input
queries. In this paper, we evaluate this system with
respect to the retrieval of biomedical correlations
1017
Symbol CRP
Name C-reactive protein, pentraxin-related
Species Homo sapiens
Synonym MGC88244, PTX1
Product C-reactive protein precursor, C-reactive
protein, pentraxin-related protein
External links EntrezGene:1401, GDB:119071, . . .
Table 1: An example GENA entry
and examine the effects of using predicate argu-
ment structures and ontological identifiers.
The need for the discovery of relational con-
cepts has been investigated intensively in Infor-
mation Extraction (IE). However, little research
has targeted on-demand retrieval from huge texts.
One difficulty is that IE techniques such as pat-
tern matching and machine learning require heav-
ier processing in order to be applied on the fly.
Another difficulty is that target information must
be formalized beforehand and each system is de-
signed for a specific task. For instance, an IE
system for protein-protein interactions is not use-
ful for finding gene-disease associations. Apart
from IE research, enrichment of texts with vari-
ous annotations has been proposed and is becom-
ing a new research area for information manage-
ment (IBM, 2005; TEI, 2004). The present study
basically examines this new direction in research.
The significant contribution of the present paper,
however, is to provide the first empirical results of
this framework for a real task with a huge textbase.
2 Background: Resources and Tools for
Semantic Annotations
The proposed system for the retrieval of relational
concepts is a product of recent developments in
NLP resources and tools. In this section, ontology
databases, deep parsers, and search algorithms for
structured data are introduced.
2.1 Ontology databases
Ontology databases are collections of words and
phrases in specific domains. Such databases have
been constructed extensively for the systematic
management of domain knowledge by organizing
textual expressions of ontological entities that are
detached from actual sentences.
For example, GENA (Koike and Takagi, 2004)
is a database of genes and gene products that
is semi-automatically collected from well-known
databases, including HUGO, OMIM, Genatlas,
Locuslink, GDB, MGI, FlyBase, WormBase,
Figure 1: An output of HPSG parsing
Figure 2: A predicate argument structure
CYGD, and SGD. Table 1 shows an example of
a GENA entry. ?Symbol? and ?Name? denote
short forms and nomenclatures of genes, respec-
tively. ?Species? represents the organism species
in which this gene is observed. ?Synonym? is a
list of synonyms and name variations. ?Product?
gives a list of products of this gene, such as pro-
teins coded by this gene. ?External links? pro-
vides links to other databases, and helps to obtain
detailed information from these databases. For
biomedical terms other than genes/gene products,
the Unified Medical Language System (UMLS)
meta-thesaurus (Lindberg et al, 1993) is a large
database that contains various names of biomedi-
cal and health-related concepts.
Ontology databases provide mappings be-
tween textual expressions and entities in the real
world. For example, Table 1 indicates that CRP,
MGC88244, and PTX1 denote the same gene con-
ceptually. Hence, these resources enable us to
canonicalize variations of textual expressions of
ontological entities.
2.2 Parsing technologies
Recently, state-of-the-art CFG parsers (Charniak
and Johnson, 2005) can compute phrase structures
of natural sentences at fairly high accuracy. These
parsers have been used in various NLP tasks in-
cluding IE and text mining. In addition, parsers
that compute deeper analyses, such as predicate
argument structures, have become available for
1018
the processing of real-world sentences (Miyao and
Tsujii, 2005). Predicate argument structures are
canonicalized representations of sentence mean-
ings, and express the semantic relations of words
explicitly. Figure 1 shows an output of an HPSG
parser (Miyao and Tsujii, 2005) for the sentence
?A normal serum CRP measurement does not ex-
clude deep vein thrombosis.? The dotted lines ex-
press predicate argument relations. For example,
the ARG1 arrow coming from ?exclude? points
to the noun phrase ?A normal serum CRP mea-
surement?, which indicates that the subject of ?ex-
clude? is this noun phrase, while such relations are
not explicitly represented by phrase structures.
Predicate argument structures are beneficial for
our purpose because they can represent relational
concepts in an abstract manner. For example, the
relational concept of ?CRP excludes thrombosis?
can be represented as a predicate argument struc-
ture, as shown in Figure 2. This structure is univer-
sal in various syntactic expressions, such as pas-
sivization (e.g., ?thrombosis is excluded by CRP?)
and relativization (e.g., ?thrombosis that CRP ex-
cludes?). Hence, we can abstract surface varia-
tions of sentences and describe relational concepts
in a canonicalized form.
2.3 Structural search algorithms
Search algorithms for structured texts have been
studied extensively, and examples include XML
databases with XPath (Clark and DeRose, 1999)
and XQuery (Boag et al, 2005), and region alge-
bra (Clarke et al, 1995). The present study fo-
cuses on region algebra extended with variables
(Masuda et al, 2006) because it provides an ef-
ficient search algorithm for tags with cross bound-
aries. When we annotate texts with various levels
of syntactic/semantic structures, cross boundaries
are inherently nonnegligible. In fact, as described
in Section 3, our system exploits annotations of
predicate argument structures and ontological en-
tities, which include substantial cross boundaries.
Region algebra is defined as a set of operators
on regions, i.e., word sequences. Table 2 shows
operators of the extended region algebra, where
A and B denote regions, and results of operations
are also regions. For example, ?A & B? denotes a
region that includes both A and B. Four contain-
ment operators, >, >>, <, and <<, represent an-
cestor/descendant relations in XML. For example,
?A > B? indicates that A is an ancestor of B. In
[tag] Region covered with ?<tag>?
A > B A containing B
A >> B A containing B (A is not nested)
A < B A contained by B
A << B A contained by B (B is not nested)
A - B Starting with A and ending with B
A & B A and B
A | B A or B
Table 2: Operators of the extended region algebra
[sentence] >>
(([word arg1="$subject"] > exclude) &
([phrase id="$subject"] > CRP))
Figure 3: A query of the extended region algebra
Figure 4: Matching with the query in Figure 3
search algorithms for region algebra, the cost of
retrieving the first answer is constant, and that of
an exhaustive search is bounded by the lowest fre-
quency of a word in a query (Clarke et al, 1995).
Variables in the extended region algebra allow
us to express shared structures and are necessary
in order to describe predicate argument structures.
For example, Figure 3 shows a formula in the ex-
tended region algebra that represents the predicate
argument structure of ?CRP excludes something.?
This formula indicates that a sentence contains a
region in which the word ?exclude? exists, the
first argument (?arg1?) phrase of which includes
the word ?CRP.? A predicate argument relation is
expressed by the variable, ?$subject.? Figure 4
shows a situation in which this formula is satisfied.
Three horizontal bars describe regions covered by
<sentence>, <phrase>, and <word> tags,
respectively. The dotted line denotes the relation
expressed by this variable. Given this formula as a
query, a search engine can retrieve sentences hav-
ing semantic annotations that satisfy this formula.
3 A Text Retrieval System for MEDLINE
While the above resources and tools have been de-
veloped independently, their collaboration opens
up a new framework for the retrieval of relational
concepts, as described below (Figure 5).
Off-line processing: Prior to retrieval, a deep
parser is applied to compute predicate argument
1019
Figure 5: Framework of semantic retrieval
structures, and a term recognizer is applied to cre-
ate mappings from textual expressions into identi-
fiers in ontology databases. Semantic annotations
are stored and indexed in a structured database for
the extended region algebra.
On-line processing: User input is converted into
queries of the extended region algebra. A search
engine retrieves sentences having semantic anno-
tations that match the queries.
This framework is applied to a text retrieval en-
gine for MEDLINE. MEDLINE is an exhaustive
database covering nearly 4,500 journals in the life
sciences and includes the bibliographies of arti-
cles, about half of which have abstracts. Research
on IE and text mining in biomedical science has
focused mainly on MEDLINE. In the present pa-
per, we target al articles indexed in MEDLINE at
the end of 2004 (14,785,094 articles). The follow-
ing sections explain in detail off-/on-line process-
ing for the text retrieval system for MEDLINE.
3.1 Off-line processing: HPSG parsing and
term recognition
We first parsed all sentences using an HPSG parser
(Miyao and Tsujii, 2005) to obtain their predi-
cate argument structures. Because our target is
biomedical texts, we re-trained a parser (Hara et
al., 2005) with the GENIA treebank (Tateisi et
al., 2005), and also applied a bidirectional part-of-
speech tagger (Tsuruoka and Tsujii, 2005) trained
with the GENIA treebank as a preprocessor.
Because parsing speed is still unrealistic for
parsing the entire MEDLINE on a single ma-
chine, we used two geographically separated com-
puter clusters having 170 nodes (340 Xeon CPUs).
These clusters are separately administered and not
dedicated for use in the present study. In order to
effectively use such an environment, GXP (Taura,
2004) was used to connect these clusters and dis-
tribute the load among them. Our processes were
given the lowest priority so that our task would not
disturb other users. We finished parsing the entire
MEDLINE in nine days (Ninomiya et al, 2006).
# entries (genes) 517,773
# entries (gene products) 171,711
# entries (diseases) 148,602
# expanded entries 4,467,855
Table 3: Sizes of ontologies used for term recog-
nition
Event type Expressions
influence effect, affect, role, response, . . .
regulation mediate, regulate, regulation, . . .
activation induce, activate, activation, . . .
Table 4: Event expression ontology
Next, we annotated technical terms, such as
genes and diseases, to create mappings to onto-
logical identifiers. A dictionary-based term recog-
nition algorithm (Tsuruoka and Tsujii, 2004) was
applied for this task. First, an expanded term
list was created by generating name variations of
terms in GENA and the UMLS meta-thesaurus1.
Table 3 shows the size of the original database and
the number of entries expanded by name varia-
tions. Terms in MEDLINE were then identified
by the longest matching of entries in this expanded
list with words/phrases in MEDLINE.
The necessity of ontologies is not limited to
nominal expressions. Various verbs are used for
expressing events. For example, activation events
of proteins can be expressed by ?activate,? ?en-
hance,? and other event expressions. Although the
numbers of verbs and their event types are much
smaller than those of technical terms, verbal ex-
pressions are important for the description of rela-
tional concepts. Since ontologies of event expres-
sions in this domain have not yet been constructed,
we developed an ontology from scratch. We inves-
tigated 500 abstracts extracted from MEDLINE,
and classified 167 frequent expressions, including
verbs and their nominalized forms, into 18 event
types. Table 4 shows a part of this ontology. These
expressions in MEDLINE were automatically an-
notated with event types.
As a result, we obtained semantically annotated
MEDLINE. Table 5 shows the size of the orig-
inal MEDLINE and semantic annotations. Fig-
ure 6 shows semantic annotations for the sentence
in Figure 1, where ?-? indicates nodes of XML,2
1We collected disease names by specifying a query with
the semantic type as ?Disease or Syndrome.?
2Although this example is shown in XML, this textbase
contains tags with cross boundaries because tags for predicate
argument structures and technical terms may overlap.
1020
# papers 14,785,094
# abstracts 7,291,857
# sentences 70,935,630
# words 1,462,626,934
# successfully parsed sentences 69,243,788
# predicate argument relations 1,510,233,701
# phrase tags 3,094,105,383
# terms (genes) 84,998,621
# terms (gene products) 27,471,488
# terms (diseases) 19,150,984
# terms (event expressions) 51,810,047
Size of the original MEDLINE 9.3 GByte
Size of the semantic annotations 292 GByte
Size of the index file for region algebra 954 GByte
Table 5: Sizes of the original and semantically an-
notated MEDLINE textbases
- <sentence sentence_id="e6e525">
- <phrase id="0" cat="S" head="15" lex_head="18">
- <phrase id="1" cat="NP" head="4" lex_head="14">
- <phrase id="2" cat="DT" head="3" lex_head="3">
- <word id="3" pos="DT" cat="DT" base="a" arg1="4">
- A
- <phrase id="4" cat="NP" head="7" lex_head="14">
- <phrase id="5" cat="AJ" head="6" lex_head="6">
- <word id="6" pos="JJ" cat="AJ" base="normal" arg1="7">
- normal
- <phrase id="7" cat="NP" head="10" lex_head="14">
- <phrase id="8" cat="NP" head="9" lex_head="9">
- <word id="9" pos="NN" cat="NP" base="serum" mod="10">
- serum
- <phrase id="10" cat="NP" head="13" lex_head="14">
- <phrase id="11" cat="NP" head="12" lex_head="12">
- <entity_name id="entity-1" type="gene"
gene_id="GHS003134" gene_symbol="CRP"
gene_name="C-reactive protein, pentraxin-related"
species="Homo sapiens"
db_site="EntrezGene:1401|GDB:119071|GenAtlas:CRP">
- <word id="12" pos="NN" cat="NP" base="crp" mod="13">
- CRP
- <phrase id="13" cat="NP" head="14" lex_head="14">
- <word id="14" pos="NN" cat="NP" base="measurement">
- measurement
- <phrase id="15" cat="VP" head="16" lex_head="18">
- <phrase id="16" cat="VP" head="17" lex_head="18">
- <phrase id="17" cat="VP" head="18" lex_head="18">
- <word id="18" pos="VBZ" cat="VP" base="do"
arg1="1" arg2="21">
- does
- <phrase id="19" cat="AV" head="20" lex_head="20">
- <word id="20" pos="RB" cat="AV" base="not" arg1="21">
- not
- <phrase id="21" cat="VP" head="22" lex_head="23">
- <phrase id="22" cat="VP" head="23" lex_head="23">
- <word id="23" pos="VB" cat="VP" base="exclude"
arg1="1" arg2="24">
- exclude
...
Figure 6: A semantically annotated sentence
although the latter half of the sentence is omitted
because of space limitations. Sentences are an-
notated with four tags,3 ?phrase,? ?word,? ?sen-
tence,? and ?entity name,? and their attributes as
given in Table 6. Predicate argument structures are
annotated as attributes, ?mod? and ?argX ,? which
point to the IDs of the argument phrases. For ex-
ample, in Figure 6, the <word> tag for ?exclude?
has the attributes arg1="1" and arg2="24",
which denote the IDs of the subject and object
phrases, respectively.
3Additional tags exist for representing document struc-
tures such as ?title? (details omitted).
Tag Attributes
phrase id, cat, head, lex head
word id, cat, pos, base, mod, argX , rel type
sentence sentence id
entity name id, type, gene id/disease id, gene symbol,
gene name, species, db site
Attribute Description
id unique identifier
cat syntactic category
head head daughter?s ID
lex head lexical head?s ID
pos part-of-speech
base base form of the word
mod ID of modifying phrase
argX ID of the X-th argument of the word
rel type event type
sentence id sentence?s ID
type whether gene, gene prod, or disease
gene id ID in GENA
disease id ID in the UMLS meta-thesaurus
gene symbol short form of the gene
gene name nomenclature of the gene
species species that have this gene
db site links to external databases
Table 6: Tags (upper) and attributes (lower) for
semantic annotations
3.2 On-line processing
The off-line processing described above results in
much simpler on-line processing. User input is
converted into queries of the extended region al-
gebra, and the converted queries are entered into a
search engine for the extended region algebra. The
implementation of a search engine is described in
detail in Masuda et al (2006).
Basically, given subject x, object y, and verb v,
the system creates the following query:
[sentence] >>
([word arg1="$subject" arg2="$object"
base="v"] &
([phrase id="$subject"] > x) &
([phrase id="$object"] > y))
Ontological identifiers are substituted for x, y,
and v, if possible. Nominal keywords, i.e., x and
y, are replaced by [entity_name gene_id="n"]
or [entity_name disease_id="n"], where n is
the ontological identifier of x or y. For verbal key-
words, base="v" is replaced by rel_type="r",
where r is the event type of v.
4 Evaluation
Our system is evaluated with respect to speed and
accuracy. Speed is indispensable for real-time in-
teractive text retrieval systems, and accuracy is key
for the motivation of semantic retrieval. That is,
our motivation for employing semantic retrieval
1021
Query No. User input
1 something inhibit ERK2
2 something trigger diabetes
3 adiponectin increase something
4 TNF activate IL6
5 dystrophin cause disease
6 macrophage induce something
7 something suppress MAP phosphorylation
8 something enhance p53 (negative)
Table 7: Queries for experiments
[sentence] >>
([word rel_type="activation"] &
[entity_name type="gene" gene_id="GHS019685"] &
[entity_name type="gene" gene_id="GHS009426"])
[sentence] >>
([word arg1="$subject" arg2="$object"
rel_type="activation"] &
([phrase id="$subject"] >
[entity_name type="gene" gene_id="GHS019685"]) &
([phrase cat="np" id="$object"] >
[entity_name type="gene" gene_id="GHS009426"]))
Figure 7: Queries of the extended region algebra
for Query 4-3 (upper: keyword search, lower: se-
mantic search)
was to provide a device for the accurate identifica-
tion of relational concepts. In particular, high pre-
cision is desired in text retrieval from huge texts
because users want to extract relevant information,
rather than collect exhaustive information.
We have two parameters to vary: whether to
use predicate argument structures and whether to
use ontological identifiers. The effect of using
predicate argument structures is evaluated by com-
paring ?keyword search? with ?semantic search.?
The former is a traditional style of IR, in which
sentences are retrieved by matching words in a
query with words in sentences. The latter is a
new feature of the present system, in which sen-
tences are retrieved by matching predicate argu-
ment relations in a query with those in a semanti-
cally annotated textbase. The effect of using onto-
logical identifiers is assessed by changing queries
of the extended region algebra. When we use the
term ontology, nominal keywords in queries are
replaced with ontological identifiers in GENA and
the UMLS meta-thesaurus. When we use the event
expression ontology, verbal keywords in queries
are replaced with event types.
Table 7 is a list of queries used in the follow-
ing experiments. Words in italics indicate a class
of words: ?something? indicates that any word
can appear, and disease indicates that any dis-
ease expression can appear. These queries were
selected by a biologist, and express typical re-
lational concepts that a biologist may wish to
find. Queries 1, 3, and 4 represent relations of
genes/proteins, where ERK2, adiponectin, TNF,
and IL6 are genes/proteins. Queries 2 and 5 de-
scribe relations concerning diseases, and Query 6
is a query that is not relevant to genes or diseases.
Query 7 expresses a complex relation concern-
ing a specific phenomena, i.e., phosphorylation,
of MAP. Query 8 describes a relation concerning
a gene, i.e., p53, while ?(negative)? indicates that
the target of retrieval is negative mentions. This is
expressed by ?not? modifying a predicate.
For example, Query 4 attempts to retrieve sen-
tences that mention the protein-protein interaction
?TNF activates IL6.? This is converted into queries
of the extended region algebra given in Figure 7.
The upper query is for keyword search and only
specifies the appearances of the three words. Note
that the keywords are translated into the ontolog-
ical identifiers, ?activation,? ?GHS019685,? and
?GHS009426.? The lower query is for semantic
search. The variables in ?arg1? and ?arg2? indi-
cate that ?GHS019685? and ?GHS009426? are the
subject and object, respectively, of ?activation?.
Table 8 summarizes the results of the experi-
ments. The postfixes of query numbers denote
whether ontological identifiers are used. X-1 used
no ontologies, and X-2 used only the term ontol-
ogy. X-3 used both the term and event expression
ontologies4. Comparison of X-1 and X-2 clarifies
the effect of using the term ontology. Comparison
of X-2 and X-3 shows the effect of the event ex-
pression ontology. The results for X-3 indicate
the maximum performance of the current system.
This table shows that the time required for the se-
mantic search for the first answer, shown as ?time
(first)? in seconds, was reasonably short. Thus,
the present framework is acceptable for real-time
text retrieval. The numbers of answers increased
when we used the ontologies, and this result indi-
cates the efficacy of both ontologies for obtaining
relational concepts written in various expressions.
Accuracy was measured by judgment by a bi-
ologist. At most 100 sentences were retrieved for
each query, and the results of keyword search and
semantic search were merged and shuffled. A bi-
ologist judged the shuffled sentences (1,839 sen-
tences in total) without knowing whether the sen-
4Query 5-1 is not tested because ?disease? requires
the term ontology, and Query 6-2 is not tested because
?macrophage? is not assigned an ontological identifier.
1022
Query Keyword search Semantic search
No. # ans. time (first/all) precision n-precision # ans. time (first/all) precision relative recall
1-1 252 0.00/ 1.5 74/100 (74%) 74/100 (74%) 143 0.01/ 2.5 96/100 (96%) 51/74 (69%)
1-2 348 0.00/ 1.9 61/100 (61%) 61/100 (61%) 174 0.01/ 3.1 89/100 (89%) 42/61 (69%)
1-3 884 0.00/ 3.2 50/100 (50%) 50/100 (50%) 292 0.01/ 5.3 91/100 (91%) 21/50 (42%)
2-1 125 0.00/ 1.8 45/100 (45%) 9/ 27 (33%) 27 0.02/ 2.9 23/ 27 (85%) 17/45 (38%)
2-2 113 0.00/ 2.9 40/100 (40%) 10/ 26 (38%) 26 0.06/ 4.0 22/ 26 (85%) 19/40 (48%)
2-3 6529 0.00/ 12.1 42/100 (42%) 42/100 (42%) 662 0.01/1527.4 76/100 (76%) 8/42 (19%)
3-1 287 0.00/ 1.5 20/100 (20%) 4/ 30 (13%) 30 0.05/ 2.4 23/ 30 (80%) 6/20 (30%)
3-2 309 0.01/ 2.1 21/100 (21%) 4/ 32 (13%) 32 0.10/ 3.5 26/ 32 (81%) 6/21 (29%)
3-3 338 0.01/ 2.2 24/100 (24%) 8/ 39 (21%) 39 0.05/ 3.6 32/ 39 (82%) 8/24 (33%)
4-1 4 0.26/ 1.5 0/ 4 (0%) 0/ 0 (?) 0 2.44/ 2.4 0/ 0 (?) 0/ 0 (?)
4-2 195 0.01/ 2.5 9/100 (9%) 1/ 6 (17%) 6 0.09/ 4.1 5/ 6 (83%) 2/ 9 (22%)
4-3 2063 0.00/ 7.5 5/100 (5%) 5/ 94 (5%) 94 0.02/ 10.5 89/ 94 (95%) 2/ 5 (40%)
5-2 287 0.08/ 6.3 73/100 (73%) 73/100 (73%) 116 0.05/ 14.7 97/100 (97%) 37/73 (51%)
5-3 602 0.01/ 15.9 50/100 (50%) 50/100 (50%) 122 0.05/ 14.2 96/100 (96%) 23/50 (46%)
6-1 10698 0.00/ 42.8 14/100 (14%) 14/100 (14%) 1559 0.01/3014.5 65/100 (65%) 10/14 (71%)
6-3 42106 0.00/3379.5 11/100 (11%) 11/100 (11%) 2776 0.01/5100.1 61/100 (61%) 5/11 (45%)
7 87 0.04/ 2.7 34/ 87 (39%) 7/ 15 (47%) 15 0.05/ 4.2 10/ 15 (67%) 10/34 (29%)
8 1812 0.01/ 7.6 19/100 (19%) 17/ 84 (20%) 84 0.20/ 29.2 73/ 84 (87%) 7/19 (37%)
Table 8: Number of retrieved sentences, retrieval time, and accuracy
tence was retrieved by keyword search or semantic
search. Without considering which words actually
matched the query, a sentence is judged to be cor-
rect when any part of the sentence expresses all of
the relations described by the query. The modality
of sentences was not distinguished, except in the
case of Query 8. These evaluation criteria may be
disadvantageous for the semantic search because
its ability to exactly recognize the participants of
relational concepts is not evaluated. Table 8 shows
the precision attained by keyword/semantic search
and n-precision, which denotes the precision of
the keyword search, in which the same number,
n, of outputs is taken as the semantic search. The
table also gives the relative recall of the semantic
search, which represents the ratio of sentences that
are correctly output by the semantic search among
those correctly output by the keyword search. This
does not necessarily represent the true recall be-
cause sentences not output by keyword search are
excluded. However, this is sufficient for the com-
parison of keyword search and semantic search.
The results show that the semantic search exhib-
ited impressive improvements in precision. The
precision was over 80% for most queries and was
nearly 100% for Queries 4 and 5. This indicates
that predicate argument structures are effective for
representing relational concepts precisely, espe-
cially for relations in which two entities are in-
volved. Relative recall was approximately 30?
50%, except for Query 2. In the following, we
will investigate the reasons for the residual errors.
Table 9 shows the classifications of the errors of
Disregarding of noun phrase structures 45
Term recognition errors 33
Parsing errors 11
Other reasons 8
Incorrect human judgment 7
Nominal expressions 41
Phrasal verb expressions 26
Inference required 24
Coreference resolution required 19
Parsing errors 16
Other reasons 15
Incorrect human judgment 10
Table 9: Error analysis (upper: 104 false positives,
lower: 151 false negatives)
semantic retrieval. The major reason for false pos-
itives was that our queries ignore internal struc-
tures of noun phrases. The system therefore re-
trieved noun phrases that do not directly mention
target entities. For example, ?the increased mor-
tality in patients with diabetes was caused by . . . ?
does not indicate the trigger of diabetes. Another
reason was term recognition errors. For exam-
ple, the system falsely retrieved sentences con-
taining ?p40,? which is sometimes, but not nec-
essarily used as a synonym for ?ERK2.? Ma-
chine learning-based term disambiguation will al-
leviate these errors. False negatives were caused
mainly by nominal expressions such as ?the in-
hibition of ERK2.? This is because the present
system does not convert user input into queries
on nominal expressions. Another major reason,
phrasal verb expressions such as ?lead to,? is also
a shortage of our current strategy of query cre-
ation. Because semantic annotations already in-
1023
clude linguistic structures of these expressions, the
present system can be improved further by creat-
ing queries on such expressions.
5 Conclusion
We demonstrated a text retrieval system for MED-
LINE that exploits pre-computed semantic anno-
tations5. Experimental results revealed that the
proposed system is sufficiently efficient for real-
time text retrieval and that the precision of re-
trieval was remarkably high. Analysis of resid-
ual errors showed that the handling of noun phrase
structures and the improvement of term recogni-
tion will increase retrieval accuracy. Although
the present paper focused on MEDLINE, the NLP
tools used in this system are domain/task indepen-
dent. This framework will thus be applicable to
other domains such as patent documents.
The present framework does not conflict with
conventional IR/IE techniques, and integration
with these techniques is expected to improve the
accuracy and usability of the proposed system. For
example, query expansion and relevancy feedback
can be integrated in a straightforward way in order
to improve accuracy. Document ranking is useful
for the readability of retrieved results. IE systems
can be applied off-line, in the manner of the deep
parser in our system, for annotating sentences with
target information of IE. Such annotations will en-
able us to retrieve higher-level concepts, such as
relationships among relational concepts.
Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Systems
Genomics? (MEXT, Japan), Genome Network
Project (NIG, Japan), and Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
S. Boag, D. Chamberlin, M. F. Ferna?ndez, D. Florescu,
J. Robie, and J. Sime?on. 2005. XQuery 1.0: An
XML query language.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proc. ACL 2005.
5A web-based demo of our system is available on-line at:
http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
J. Clark and S. DeRose. 1999. XML Path Language
(XPath) version 1.0.
C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski.
1995. An algebra for structured text search and a
framework for its implementation. The Computer
Journal, 38(1):43?56.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
T. Hara, Y. Miyao, and J. Tsujii. 2005. Adapting
a probabilistic disambiguation model of an HPSG
parser to a new domain. In Proc. IJCNLP 2005.
IBM, 2005. Unstructed Information Management Ar-
chitecture (UIMA) SDK User?s Guide and Refer-
ence.
A. Koike and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. In Proc.
Biolink 2004, pages 9?16.
D. A. Lindberg, B. L. Humphreys, and A. T. Mc-
Cray. 1993. The Unified Medical Language Sys-
tem. Methods in Inf. Med., 32(4):281?291.
K. Masuda, T. Ninomiya, Y. Miyao, T. Ohta, and
J. Tsujii. 2006. Nested region algebra extended with
variables. In Preparation.
Y. Miyao and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proc. 43rd ACL, pages 83?90.
National Library of Medicine. 2005. Fact Sheet MED-
LINE. Available at http://www.nlm.nih.
gov/pubs/factsheets/medline.html.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, K. Taura, and
J. Tsujii. 2006. Fast and scalable HPSG parsing.
Traitement automatique des langues (TAL), 46(2).
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax annotation for the GENIA corpus. In Proc.
IJCNLP 2005, Companion volume, pages 222?227.
K. Taura. 2004. GXP : An interactive shell for the grid
environment. In Proc. IWIA2004, pages 59?67.
TEI Consortium, 2004. Text Encoding Initiative.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP 2005, pages
467?474.
1024
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93?102,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Probabilistic models for disambiguation of an HPSG-based chart generator
Hiroko Nakanishi
 
 
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku
Tokyo 113-0033, Japan
Yusuke Miyao
 

CREST, JST
Honcho 4-1-8, Kawaguchi-shi
Saitama 332-0012, Japan
n165, yusuke, tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii  

School of Informatics
University of Manchester
POBox 88, Sackville St
MANCHESTER M60 1QD, UK
Abstract
We describe probabilistic models for a
chart generator based on HPSG. Within
the research field of parsing with lex-
icalized grammars such as HPSG, re-
cent developments have achieved efficient
estimation of probabilistic models and
high-speed parsing guided by probabilis-
tic models. The focus of this paper is
to show that two essential techniques ?
model estimation on packed parse forests
and beam search during parsing ? are suc-
cessfully exported to the task of natural
language generation. Additionally, we re-
port empirical evaluation of the perfor-
mance of several disambiguation models
and how the performance changes accord-
ing to the feature set used in the models
and the size of training data.
1 Introduction
Surface realization is the final stage of natural lan-
guage generation which receives a semantic rep-
resentation and outputs a corresponding sentence
where all words are properly inflected and ordered.
This paper presents log-linear models to address the
ambiguity which arises when HPSG (Head-driven
Phrase Structure Grammar (Pollard and Sag, 1994))
is applied to sentence generation. Usually a single
semantic representation can be realized as several
sentences. For example, consider the following two
sentences generated from the same input.
 The complicated language in the huge new law
has muddied the fight.
 The language in the huge new law complicated
has muddied the fight.
The latter is not an appropriate realization because
?complicated? tends to be wrongly interpreted to
modify ?law?. Therefore the generator needs to se-
lect a candidate sentence which is more fluent and
easier to understand than others.
In principle, we need to enumerate all alternative
realizations in order to estimate a log-linear model
for generation. It therefore requires high compu-
tational cost to estimate a probabilistic model for a
wide-coverage grammar because there are consider-
able ambiguities and the alternative realizations are
hard to enumerate explicitly. Moreover, even after
the model has been estimated, to explore all possible
candidates in runtime is also expensive. The same
problems also arise with HPSG parsing, and recent
studies (Tsuruoka et al, 2004; Miyao and Tsujii,
2005; Ninomiya et al, 2005) proposed a number of
solutions including the methods of estimating log-
linear models using packed forests of parse trees and
pruning improbable candidates during parsing.
The aim of this paper is to apply these techniques
to generation. Since parsing and generation both
output the best probable tree under some constraints,
we expect that techniques that work effectively in
parsing are also beneficial for generation. First, we
enabled estimation of log-linear models with less
cost by representing a set of generation trees in a
packed forest. The forest representation was ob-
tained by adopting chart generation (Kay, 1996; Car-
93
roll et al, 1999) where ambiguous candidates are
packed into an equivalence class and mapping a
chart into a forest in the same way as parsing. Sec-
ond, we reduced the search space in runtime by
adopting iterative beam search (Tsuruoka and Tsu-
jii, 2004) that efficiently pruned improbable candi-
dates. We evaluated the generator on the Penn Tree-
bank (Marcus et al, 1993), which is highly reliable
corpus consisting of real-world texts.
Through a series of experiments, we compared
the performance of several disambiguation mod-
els following an existing study (Velldal and Oepen,
2005) and examined how the performance changed
according to the size of training data, the feature set,
and the beam width. Comparing the latter half of the
experimental results with those on parsing (Miyao
and Tsujii, 2005), we investigated similarities and
differences between probabilistic models for parsing
and generation. The results indicated that the tech-
niques exported from parsing to generation worked
well while the effects were slightly different in de-
tail.
The Nitrogen system (Langkilde and Knight,
1998; Langkilde, 2000) maps semantic relations to a
packed forest containing all realizations and selects
the best one with a bigram model. Our method ex-
tends their approach in that we can utilize syntactic
features in the disambiguation model in addition to
the bigram.
From the perspective of using a lexicalized gram-
mar developed for parsing and importing pars-
ing techniques, our method is similar to the fol-
lowing approaches. The Fergus system (Banga-
lore and Rambow, 2000) uses LTAG (Lexicalized
Tree Adjoining Grammar (Schabes et al, 1988))
for generating a word lattice containing realizations
and selects the best one using a trigram model.
White and Baldridge (2003) developed a chart gen-
erator for CCG (Combinatory Categorial Gram-
mar (Steedman, 2000)) and proposed several tech-
niques for efficient generation such as best-first
search, beam thresholding and chunking the input
logical forms (White, 2004). Although some of the
techniques look effective, the models to rank can-
didates are still limited to simple language mod-
els. Carroll et al (1999) developed a chart gen-
erator using HPSG. After the generator outputs all
the sentences the grammar allows, the ranking mod-
?????? xh eINDEXR EL??
??
??
?
?
??
??
??
?
?
p a s t
y
x
e
bu y
T ENS E
A R G 2
A R G 1
INDEX
R EL
??
?
?
?
??
?
?
?
y
z
t h e
A R G 1
INDEX
R EL ?????? ybookINDEXR EL
Figure 1: PASs for ?He bought the book.?
ule (Velldal and Oepen, 2005) selects the best one
using a log-linear model. Their model is trained us-
ing only 864 sentences where all realizations can be
explicitly enumerated.
As a grammar is extended to support more lin-
guistic phenomena and to achieve higher cover-
age, the number of alternative realizations increases
and the enumeration requires much higher compu-
tational cost. Moreover, using a variety of syntactic
features also increases the cost. By representing a
set of realizations compactly with a packed forest,
we trained the models with rich features on a large
corpus using a wide-coverage grammar.
2 Background
This section describes background of this work in-
cluding the representation of the input to our gener-
ator, the algorithm of chart generation, and proba-
bilistic models for HPSG.
2.1 Predicate-argument structures
The grammar we adopted is the Enju grammar,
which is an English HPSG grammar extracted from
the Penn Treebank by Miyao et al (2004). In
parsing a sentence with the Enju grammar, seman-
tic relations of words is output included in a parse
tree. The semantic relations are represented by a
set of predicate-argument structures (PASs), which
in turn becomes the input to our generator. Figure
1 shows an example input to our generator which
corresponds to the sentence ?He bought the book.?,
which consists of four predicates. REL expresses
the base form of the word corresponding to the pred-
icate. INDEX expresses a semantic variable to iden-
tify each word in the set of relations. ARG1 and
ARG2 express relationships between the predicate
and its arguments, e.g., the circled part in Figure 1
shows ?he? is the subject of ?buy? in this example.
The other constraints in the parse tree are omitted
in the input for the generator. Since PASs abstract
94
away superficial differences, generation from a set
of PASs contains ambiguities in the order of modi-
fiers like the example in Section 1 or the syntactic
categories of phrases. For example, the PASs in Fig-
ure 1 can generate the NP, ?the book he bought.?
When processing the input PASs, we split a single
PAS into a set of relations like (1) representing the
first PAS in Figure 1.
 
	
	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284?292,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Predicate-argument Structure Patterns
for Biomedical Information Extraction
Akane Yakushiji? ? Yusuke Miyao? Tomoko Ohta?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
? School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{akane, yusuke, okap, yucca, tsujii}@is.s.u-tokyo.ac.jp
Yuka Tateisi? ? Jun?ichi Tsujii? ?
Abstract
This paper presents a method of automat-
ically constructing information extraction
patterns on predicate-argument structures
(PASs) obtained by full parsing from a
smaller training corpus. Because PASs
represent generalized structures for syn-
tactical variants, patterns on PASs are ex-
pected to be more generalized than those
on surface words. In addition, patterns
are divided into components to improve
recall and we introduce a Support Vector
Machine to learn a prediction model using
pattern matching results. In this paper, we
present experimental results and analyze
them on how well protein-protein interac-
tions were extracted from MEDLINE ab-
stracts. The results demonstrated that our
method improved accuracy compared to a
machine learning approach using surface
word/part-of-speech patterns.
1 Introduction
One primitive approach to Information Extrac-
tion (IE) is to manually craft numerous extrac-
tion patterns for particular applications and this
is presently one of the main streams of biomedi-
cal IE (Blaschke and Valencia, 2002; Koike et al,
2003). Although such IE attempts have demon-
strated near-practical performance, the same sets
of patterns cannot be applied to different kinds of
information. A real-world task requires several
kinds of IE, thus manually engineering extraction
Current Affiliation:
? FUJITSU LABORATORIES LTD.
? Faculty of Informatics, Kogakuin University
patterns, which is tedious and time-consuming
process, is not really practical.
Techniques based on machine learning (Zhou et
al., 2005; Hao et al, 2005; Bunescu and Mooney,
2006) are expected to alleviate this problem in
manually crafted IE. However, in most cases, the
cost of manually crafting patterns is simply trans-
ferred to that for constructing a large amount of
training data, which requires tedious amount of
manual labor to annotate text.
To systematically reduce the necessary amount
of training data, we divided the task of construct-
ing extraction patterns into a subtask that general
natural language processing techniques can solve
and a subtask that has specific properties accord-
ing to the information to be extracted. The former
subtask is of full parsing (i.e. recognizing syntactic
structures of sentences), and the latter subtask is of
constructing specific extraction patterns (i.e. find-
ing clue words to extract information) based on the
obtained syntactic structures.
We adopted full parsing from various levels
of parsing, because we believe that it offers the
best utility to generalize sentences into normal-
ized syntactic relations. We also divided patterns
into components to improve recall and we intro-
duced machine learning with a Support Vector
Machine (SVM) to learn a prediction model us-
ing the matching results of extraction patterns. As
an actual IE task, we extracted pairs of interacting
protein names from biomedical text.
2 Full Parsing
2.1 Necessity for Full Parsing
A technique that many previous approaches have
used is shallow parsing (Koike et al, 2003; Yao
et al, 2004; Zhou et al, 2005). Their assertion is
284
Distance Count (%) Sum (%)
?1 54 5.0 5.0
0 8 0.7 5.7
1 170 15.7 21.4
2?5 337 31.1 52.5
6?10 267 24.6 77.1
11? 248 22.9 100.0
Distance ?1 means protein word has been annotated as in-
teracting with itself (e.g. ?actin polymerization?). Distance 0
means words of the interacting proteins are directly next to
one another. Multi-word protein names are concatenated as
long as they do not cross tags to annotate proteins.
Table 1: Distance between Interacting Proteins
that shallow parsers are more robust and would be
sufficient for IE. However, their claims that shal-
low parsers are sufficient, or that full parsers do
not contribute to application tasks, have not been
fully proved by experimental results.
Zhou et al (2005) argued that most informa-
tion useful for IE derived from full parsing was
shallow. However, they only used dependency
trees and paths on full parse trees in their experi-
ment. Such structures did not include information
of semantic subjects/objects, which full parsing
can recognize. Additionally, most relations they
extracted from the ACE corpus (Linguistic Data
Consortium, 2005) on broadcasts and newswires
were within very short word-distance (70% where
two entities are embedded in each other or sep-
arated by at most one word), and therefore shal-
low information was beneficial. However, Table 1
shows that the word distance is long between in-
teracting protein names annotated on the AImed
corpus (Bunescu and Mooney, 2004), and we have
to treat long-distance relations for information like
protein-protein interactions.
Full parsing is more effective for acquiring gen-
eralized data from long-length words than shallow
parsing. The sentences at left in Figure 1 exem-
plify the advantages of full parsing. The gerund
?activating? in the last sentence takes a non-local
semantic subject ?ENTITY1?, and shallow parsing
cannot recognize this relation because ?ENTITY1?
and ?activating? are in different phrases. Full pars-
ing, on the other hand, can identify both the sub-
ject of the whole sentence and the semantic subject
of ?activating? have been shared.
2.2 Predicate-argument Structures
We applied Enju (Tsujii Laboratory, 2005a) as
a full parser which outputs predicate-argument
structures (PASs). PASs are well normalized
forms that represent syntactic relations. Enju
is based on Head-driven Phrase Structure Gram-
mar (Sag and Wasow, 1999), and it has been
trained on the Penn Treebank (PTB) (Marcus et
al., 1994) and a biomedical corpus, the GENIA
Treebank (GTB) (Tsujii Laboratory, 2005b). We
used a part-of-speech (POS) tagger trained on the
GENIA corpus (Tsujii Laboratory, 2005b) as a
preprocessor for Enju. On predicate-argument re-
lations, Enju achieved 88.0% precision and 87.2%
recall on PTB, and 87.1% precision and 85.4% re-
call on GTB.
The illustration at right in Figure 1 is a PAS
example, which represents the relation between
?activate?, ?ENTITY1? and ?ENTITY2? of all sen-
tences to the left. The predicate and its argu-
ments are words converted to their base forms,
augmented by their POSs. The arrows denote
the connections from predicates to their arguments
and the types of arguments are indicated as arrow
labels, i.e., ARGn (n = 1, 2, . . .), MOD. For ex-
ample, the semantic subject of a transitive verb is
ARG1 and the semantic object is ARG2.
What is important here is, thanks to the strong
normalization of syntactic variations, that we can
expect that the construction algorithm for extract-
ing patterns that works on PASs will need a much
smaller training corpus than those working on
surface-word sequences. Furthermore, because of
the reduced diversity of surface-word sequences at
the PAS level, any IE system at this level should
demonstrate improved recall.
3 Related Work
Sudo et al (2003), Culotta and Sorensen (2004)
and Bunescu and Mooney (2005) acquired sub-
structures derived from dependency trees as ex-
traction patterns for IE in general domains. Their
approaches were similar to our approach using
PASs derived from full parsing. However, one
problem with their systems is that they could
not treat non-local dependencies such as seman-
tic subjects of gerund constructions (discussed in
Section 2), and thus rules acquired from the con-
structions were partial.
Bunescu and Mooney (2006) also learned ex-
traction patterns for protein-protein interactions
by SVM with a generalized subsequence kernel.
Their patterns are sequences of words, POSs, en-
tity types, etc., and they heuristically restricted
length and word positions of the patterns. Al-
285
ENTITY1 recognizes and activates ENTITY2.
ENTITY2 activated by ENTITY1 are not well characterized.
The herpesvirus encodes a functional ENTITY1 that activates human ENTITY2.
ENTITY1 can functionally cooperate to synergistically activate ENTITY2.
The ENTITY1 plays key roles by activating ENTITY2.
ENTITY1/NN
activate/VB ENTITY2/NN
ARG1 ARG2
Figure 1: Syntactical Variations of ?activate?
though they achieved about 60% precision and
about 40% recall, these heuristic restrictions could
not be guaranteed to be applied to other IE tasks.
Hao et al (2005) learned extraction patterns
for protein-protein interactions as sequences of
words, POSs, entity tags and gaps by dynamic
programming, and reduced/merged them using a
minimum description length-based algorithm. Al-
though they achieved 79.8% precision and 59.5%
recall, sentences in their test corpus have too
many positive instances and some of the pat-
terns they claimed to have been successfully con-
structed went against linguistic or biomedical in-
tuition. (e.g. ?ENTITY1 and interacts with EN-
TITY2? should be replaced by a more general pat-
tern because they aimed to reduce the number of
patterns.)
4 Method
We automatically construct patterns to extract
protein-protein interactions from an annotated
training corpus. The corpus needs to be tagged to
denote which protein words are interacting pairs.
We follow five steps in constructing extraction
patterns from the training corpus. (1) Sentences
in the training corpus are parsed into PASs and
we extract raw patterns from the PASs. (2) We
divide the raw patterns to generate both combi-
nation and fragmental patterns. Because obtained
patterns include inappropriate ones (wrongly gen-
erated or too general), (3) we apply both kinds of
patterns to PASs of sentences in the training cor-
pus, (4) calculate the scores for matching results
of combination patterns, and (5) make a prediction
model with SVM using these matching results and
scores.
We extract pairs of interacting proteins from a
target text in the actual IE phase, in three steps.
(1) Sentences in the target corpus are parsed into
PASs. (2) We apply both kinds of extraction pat-
terns to these PASs and (3) calculate scores for
combination pattern matching. (4) We use the pre-
diction model to predict interacting pairs.
ENTITY1
ENTITY2
CD4/NN protein/NN
interact/VB
with/IN polymorphic/JJ
region/NN
of/INMHCII/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
ARG1
Parsing Result
Raw Pattern
CD4 protein interacts with polymorphic regions of MHCII .
ENTITY1
ENTITY2
Sentence in Training Corpus
protein/NN
interact/VB
with/IN
region/NN
of/IN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
(1) (2) (3) (4) (5) (6)
p
0
p
1
p
2
p
3
p
4
p
5
p
6
ENTITY2/NN
ENTITY1/NN
Figure 2: Extraction of Raw Pattern
4.1 Full Parsing and Extraction of Raw
Patterns
As the first step in both the construction phase and
application phase of extraction patterns, we parse
sentences into PASs using Enju.1 We label all
PASs of the protein names as protein PASs.
After parsing, we extract the smallest set of
PASs, which connect words that denote interact-
ing proteins, and make it a raw pattern. We take
the same method to extract and refine raw patterns
as Yakushiji et al (2005). Connecting means we
can trace predicate-argument relations from one
protein word to the other in an interacting pair.
The procedure to obtain a raw pattern (p0, . . . , pn)
is as follows:
predicate(p): PASs that have p as their argument
argument(p): PASs that p has as its arguments
1. pi = p0 is the PAS of a word correspondent
to one of interacting proteins, and we obtain
candidates of the raw pattern as follows:
1-1. If pi is of the word of the other interact-
ing protein, (p0, . . . , pi) is a candidate
of the raw pattern.
1-2. If not, make pattern candidates
for each pi+1 ? predicate(pi) ?
argument(pi) ? {p0, . . . , pi} by
returning to 1-1.
2. Select the pattern candidate of the smallest
set as the raw pattern.
1Before parsing, we concatenate each multi-word protein
name into the one word as long as the concatenation does not
cross name boundaries.
286
3. Substitute variables (ENTITY1, ENTITY2) for
the predicates of PASs correspondent to the
interacting proteins.
The lower part of Figure 2 shows an example
of the extraction of a raw pattern. ?CD4? and
?MHCII? are words representing interacting pro-
teins. First, we set the PAS of ?CD4? as p0.
argument(p0) includes the PAS of ?protein?, and
we set it as p1 (in other words, tracing the arrow
(1)). Next, predicate(p1) includes the PAS of ?in-
teract? (tracing the arrow (2) back), so we set it
as p2. We continue similarly until we reach the
PAS of ?MHCII? (p6). The result of the extracted
raw pattern is the set of p0, . . . , p6 with substitut-
ing variables ENTITY1 and ENTITY2 for ?CD4?
and ?MHCII?.
There are some cases where an extracted raw
pattern is not appropriate and we need to re-
fine it. One case is when unnecessary coordi-
nations/parentheses are included in the pattern,
e.g. two interactions are described in a combined
representation (?ENTITY1 binds this protein and
ENTITY2?). Another is when two interacting pro-
teins are connected directly by a conjunction or
only one protein participates in an interaction. In
such cases, we refine patterns by unfolding of co-
ordinations/parentheses and extension of patterns,
respectively. We have omitted detailed explana-
tions because of space limitations. The details are
described in the work of Yakushiji et al (2005).
4.2 Division of Patterns
Division for generating combination patterns is
based on observation of Yakushiji et al (2005) that
there are many cases where combinations of verbs
and certain nouns form IE patterns. In the work
of Yakushiji et al (2005), we divided only patterns
that include only one verb. We have extended the
division process to also treat nominal patterns or
patterns that include more than one verb.
Combination patterns are not appropriate for
utilizing individual word information because they
are always used in rather strictly combined ways.
Therefore we have newly introduced fragmental
patterns which consist of independent PASs from
raw patterns, in order to use individual word infor-
mation for higher recall.
4.2.1 Division for Generating Combination
Patterns
Raw patterns are divided into some compo-
nents and the components are combined to con-
ENTITY1/NN protein/NN interact/VBwith/IN region/NN
of/IN
ENTITY2/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
*/VBwith/IN
ARG1ARG2
*/NN
ENTITY/NN
protein/NN
MOD
region/NN of/INENTITY/NN
ARG1
ARG2
interact/VB
ARG1
=
*/NN
*/VB
ARG1
*/NN
=
$X
$X
Main
Prep
Entity
Entity
Entity
MainEntity Main
Main
Entity
Raw Pattern
Combination Pattern
Figure 3: Division of Raw Pattern into Combina-
tion Pattern Components (Entity-Main-Entity)
struct combination patterns according to types of
the division. There are three types of division of
raw patterns for generating combination patterns.
These are:
(a) Two-entity Division
(a-1) Entity-Main-Entity Division
(a-2) Main-Entity-Entity Division
(b) Single-entity Division, and
(c) No Division (Naive Patterns).
Most raw patterns, where entities are at both
ends of the patterns, are divided into Entity-Main-
Entity. Main-Entity-Entity are for the cases where
there are PASs other than entities at the ends of
the patterns (e.g. ?interaction between ENTITY1
and ENTITY2?). Single-entity is a special Main-
Entity-Entity for interactions with only one partic-
ipant (e.g. ?ENTITY1 dimerization?).
There is an example of Entity-Main-Entity divi-
sion in Figure 3. First, the main component from
the raw pattern is the syntactic head PAS of the
raw pattern. If the raw pattern corresponds to a
sentence, the syntactic head PAS is the PAS of the
main verb. We underspecify the arguments of the
main component, to enable them to unify with the
PASs of any words with the same POSs. Next, if
there are PASs of prepositions connecting to the
main component, they become prep components.
If there is no PAS of a preposition next to the main
component on the connecting link from the main
component to an entity, we make the pseudo PAS
of a null preposition the prep component. The left
prep component ($X) in Figure 3 is a pseudo PAS
of a null preposition. We also underspecify the ar-
guments of prep components. Finally, the remain-
ing two parts, which are typically noun phrases, of
the raw pattern become entity components. PASs
287
corresponding to the entities of the original pair
are labeled as only unifiable with the entities of
other pairs.
Main-Entity-Entity division is similar, except
we distinguish only one prep component as a
double-prep component and the PAS of the coor-
dinate conjunction between entities becomes the
coord component. Single-entity division is simi-
lar to Main-Entity-Entity division and the differ-
ence is that single-entity division produces no co-
ord and one entity component. Naive patterns are
patterns without division, where no division can be
applied (e.g. ?ENTITY1/NN in/IN complexes/NN
with/IN ENTITY2/NN?).
All PASs on boundaries of components are la-
beled to determine which PAS on a boundary of
another component can be unified. Labels are rep-
resented by subscriptions in Figure 3. These re-
strictions on component connection are used in the
step of constructing combination patterns.
Constructing combination patterns by combin-
ing components is equal to reconstructing orig-
inal raw patterns with the original combination
of components, or constructing new raw patterns
with new combinations of components. For exam-
ple, an Entity-Main-Entity pattern is constructed
by combination of any main, any two prep and any
two entity components. Actually, this construction
process by combination is executed in the pattern
matching step. That is, we do not off-line con-
struct all possible combination patterns from the
components and only construct the combination
patterns that are able to match the target.
4.2.2 Division for Generating Fragmental
Patterns
A raw pattern is splitted into individual PASs
and each PAS becomes a fragmental pattern. We
also prepare underspecified patterns where one or
more of the arguments of the original are under-
specified, i.e., are able to match any words of
the same POSs and the same label of protein/not-
protein. We underspecify the PASs of entities in
fragmental patterns to enable them to unify with
any PASs with the same POSs and a protein la-
bel, although in combination patterns we retain the
PASs of entities as only unifiable with entities of
pairs. This is because fragmental patterns are de-
signed to be less strict than combination patterns.
4.3 Pattern Matching
Matching of combination patterns is executed as
a process to match and combine combination pat-
tern components according to their division types
(Entity-Main-Entity, Main-Entity-Entity, Single-
entity and No Division). Fragmental matching is
matching all fragmental patterns to PASs derived
from sentences.
4.4 Scoring for Combination Matching
We next calculate the score of each combination
matching to estimate the adequacy of the combina-
tion of components. This is because new combina-
tion of components may form inadequate patterns.
(e.g. ?ENTITY1 be ENTITY2? can be formed of
components from ?ENTITY1 be ENTITY2 recep-
tor?.) Scores are derived from the results of com-
bination matching to the source training corpus.
We apply the combination patterns to the train-
ing corpus, and count pairs of True Positives (TP)
and False Positives (FP). The scores are calculated
basically by the following formula:
Score = TP/(TP + FP ) + ? ? TP
This formula is based on the precision of the pat-
tern on the training corpus, i.e., an estimated pre-
cision on a test corpus. ? works for smoothing,
that is, to accept only patterns of large TP when
FP = 0. ? is set as 0.01 empirically. The formula
is similar to the Apriori algorithm (Agrawal and
Srikant, 1995) that learns association rules from a
database. The first term corresponds to the confi-
dence of the algorithm, and the second term corre-
sponds to the support.
For patterns where TP = FP = 0, which
are not matched to PASs in the training corpus
(i.e., newly produced by combinations of com-
ponents), we estimates TP ? and FP ? by using
the confidence of the main and entity compo-
nents. This is because main and entity components
tend to contain pattern meanings, whereas prep,
double-prep and coord components are rather
functional. The formulas to calculate the scores
for all cases are:
Score =
8
>
>
>
<
>
>
>
:
TP/(TP + FP ) + ? ? TP
(TP + FP ?= 0)
TP ?/(TP ? + FP ?)
(TP = FP = 0, TP ? + FP ? ?= 0)
0 (TP = FP = TP ? = FP ? = 0)
288
Combination Pattern
(1) Combination of components in combination
matching
(2) Main component in combination matching
(3) Entity components in combination matching
(4) Score for combination matching (SCORE)
Fragmental Pattern
(5) Matched fragmental patterns
(6) Number of PASs of example that are not matched
in fragmental matching
Raw Pattern
(7) Length of raw pattern derived from example
Table 2: Features for SVM Learning of Prediction
Model
TP ? =
8
>
<
>
:
TP ?main + TP ?entity1(+TP ?entity2)
(for Two-entity, Single-entity)
0 (for Naive)
FP ? = (similar to TP ? but TP ?x is replaced by FP ?x)
TP ?main =
8
>
>
>
>
>
<
>
>
>
>
>
:
TPmain:two/(TPmain:two + FPmain:two)
 
TPmain:two + FPmain:two ?= 0,
for Two-entity
!
TPmain:single/(TPmain:single + FPmain:single)
 
TPmain:single + FPmain:single ?= 0,
for Single-entity
!
0 (other cases)
TP ?entityi =
8
>
<
>
:
TPentityi/(TPentityi + FPentityi)
?
TPentityi + FPentityi ?= 0
?
0 (other cases)
FP ?x =
?
similar to TP ?x but TP ?y in the
numerators is replaced by FP ?y
?
? TP : number of TPs by the combination of components
? TPmain:two: sum of TPs by two-entity combinations
that include the same main component
? TPmain:single: sum of TPs by single-entity combina-
tions that include the same main component
? TPentityi: sum of TPs by combinations that include
the same entity component which is not the straight en-
tity component
? FPx: similar to TPx but TP is replaced by FP
The entity component ?ENTITY/NN?, which
only consists of the PAS of an entity, adds no infor-
mation to combinations of components. We call
this component a straight entity component and
exclude its effect from the scores.
4.5 Construction of Prediction Model
We use an SVM to learn a prediction model to de-
termine whether a new protein pair is interacting.
We used SV M light (Joachims, 1999) with an rbf
kernel, which is known as the best kernel for most
tasks. The prediction model is based on the fea-
tures of Table 2.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Prec
ision
Recall
ALLSCOREERK
Figure 4: Results of IE Experiment
ENTITY1
FGF-2/NN
bind well to FGFR1 but
interact/VB with/IN
poorly/RB
ENTITY2
KGFR/NN
ARG1 ARG1 ARG2
Figure 5: Example Demonstrating Advantages of
Full Parsing
5 Results and Discussion
5.1 Experimental Results on the AImed
Corpus
To evaluate extraction patterns automatically con-
structed with our method, we used the AImed cor-
pus, which consists of 225 MEDLINE (U.S. Na-
tional Library of Medicine, 2006) abstracts (1969
sentences) annotated with protein names and
protein-protein interactions, for the training/test
corpora. We used tags for the protein names given.
We measured the accuracy of the IE task using
the same criterion as Bunescu andMooney (2006),
who used an SVM to construct extraction patterns
on word/POS/type sequences from the AImed cor-
pus. That is, an extracted interaction from an ab-
stract is correct if the proteins are tagged as inter-
acting with each other somewhere in that abstract
(document-level measure).
Figure 4 plots our 10-fold cross validation and
the results of Bunescu and Mooney (2006). The
line ALL represents results when we used all fea-
tures for SVM learning. The line SCORE repre-
sents results when we extracted pairs with higher
combination matching scores than various thresh-
old values. And the line ERK represents results
by Bunescu and Mooney (2006).
The line ALL obtained our best overall F-
measure 57.3%, with 71.8% precision and 48.4%
recall. Our method was significantly better than
Bunescu and Mooney (2006) for precision be-
289
tween 50% and 80%. It also needs to be noted
that SCORE, which did not use SVM learning
and only used the combination patterns, achieved
performance comparable to that by Bunescu and
Mooney (2006) for the precision range from 50%
to 80%. And for this range, introducing the frag-
mental patterns with SVM learning raised the re-
call. This range of precision is practical for the
IE task, because precision is more important than
recall for significant interactions that tend to be
described in many abstracts (as shown by the
next experiment), and too-low recall accompa-
nying too-high precision requires an excessively
large source text.
Figure 5 shows the advantage of introducing
full parsing. ?FGF-2? and ?KGFR? is an interact-
ing protein pair. The pattern ?ENTITY1 interact
with ENTITY2? based on PASs successfully ex-
tracts this pair. However, it is difficult to extract
this pair with patterns based on surface words, be-
cause there are 5 words between ?FGF-2? and ?in-
teract?.
5.2 Experimental Results on Abstracts of
MEDLINE
We also conducted an experiment to extract in-
teracting protein pairs from a large amount of
biomedical text, i.e. about 14 million titles and
8 million abstracts in MEDLINE. We constructed
combination patterns from all 225 abstracts of the
AImed corpus, and calculated a threshold value
of combination scores that produced about 70%
precision and 30% recall on the training corpus.
We extracted protein pairs with higher combi-
nation scores than the threshold value. We ex-
cluded single-protein interactions to reduce time
consumption and we used a protein name recog-
nizer in this experiment2.
We compared the extracted pairs with a man-
ually curated database, Reactome (Joshi-Tope et
al., 2005), which published 16,564 human pro-
tein interaction pairs as pairs of Entrez Gene
IDs (U.S. National Library of Medicine, 2006).
We converted our extracted protein pairs into pairs
of Entrez Gene IDs by the protein name recog-
nizer.3 Because there may be pairs missed by Re-
2Because protein names were recognized after the pars-
ing, multi-word protein names were not concatenated.
3Although the same protein names are used for humans
and other species, these are considered to be human proteins
without checking the context. This is a fair assumption be-
cause Reactome itself infers human interaction events from
experiments on model organisms such as mice.
Total 89
Parsing Error/Failure 35
(Related to coordinations) (14)
Lack of Combination Pattern Component 33
Requiring Anaphora Resolution 9
Error in Prediction Model 8
Requiring Attributive Adjectives 5
Others 10
More than one cause can occur in one error, thus the sum of
all causes is larger than the total number of False Negatives.
Table 3: Causes of Error for FNs
actome or pairs that our processed text did not in-
clude, we excluded extracted pairs of IDs that are
not included in Reactome and excluded Reactome
pairs of IDs that do not co-occur in the sentences
of our processed text.
After this postprocessing, we found that we had
extracted 7775 human protein pairs. Of them, 155
pairs were also included in Reactome ([a] pseudo
TPs) and 7620 pairs were not included in Reac-
tome ([b] pseudo FPs). 947 pairs of Reactome
were not extracted by our system ([c] pseudo False
Negatives (FNs)). However, these results included
pairs that Reactome missed or those that only co-
occurred and were not interacting pairs in the text.
There may also have been errors with ID assign-
ment.
To determine such cases, a biologist investi-
gated 100 pairs randomly selected from pairs of
pseudo TPs, FPs and FNs retaining their ratio of
numbers. She also checked correctness of the as-
signed IDs. 2 pairs were selected from pseudo
TPs, 88 pairs were from pseudo FPs and 10 pairs
were from pseudo FNs. The biologist found that
57 pairs were actual TPs (2 pairs of pseudo TPs
and 55 pairs of pseudo FPs) and 32 pairs were ac-
tual FPs of the pseudo FPs. Thus, the precision
was 64.0% in this sample set. Furthermore, even
if we assume that all pseudo FNs are actual FNs,
the recall can be estimated by actual TPs / (actual
TPs + pseudo FNs) ? 100 = 83.8%.
These results mean that the recall of an IE sys-
tem for interacting proteins is improved for a large
amount of text even if it is low for a small corpus.
Thus, this justifies our assertion that a high degree
of precision in the low-recall range is important.
5.3 Error Analysis
Tables 3 and 4 list causes of error for FNs/FPs on
a test set of the AImed corpus using the predic-
tion model with the best F-measure with all the
290
Total 35
Requiring Attributive Adjectives 13
Corpus Error 11
Error in Prediction Model 5
Requiring Negation Words 2
Parsing Error 1
Others 3
Table 4: Causes of Error for FPs
features. Different to Subsection 5.1, we individ-
ually checked each occurring pair of interacting
proteins. The biggest problems were parsing er-
ror/failure, lack of necessary patterns and learning
of inappropriate patterns.
5.3.1 Parsing Error
As listed in Table 3, 14 (40%) of the 35 pars-
ing errors/failures were related to coordinations.
Many of these were caused by differences in the
characteristics of the PTB/GTB, the training cor-
pora for Enju, and the AImed Corpus. For ex-
ample, Enju failed to obtain the correct structure
for ?the ENTITY1 / ENTITY1 complex? because
words in the PTB/GTB are not segmented with
?/? and Enju could not be trained on such a case.
One method to solve this problem is to avoid seg-
menting words with ?/? and introducing extraction
patterns based on surface characters, such as ?EN-
TITY1/ENTITY2 complex?.
Parsing errors are intrinsic problems to IE meth-
ods using parsing. However, from Table 3, we can
conclude that the key to gaining better accuracy
is refining of the method with which the PAS pat-
terns are constructed (there were 46 related FNs)
rather than improving parsing (there were 35 FNs).
5.3.2 Lack of Necessary Patterns and
Learning of Inappropriate Patterns
There are two different reasons causing the
problems with the lack of necessary patterns and
the learning of inappropriate patterns: (1) the
training corpus was not sufficiently large to sat-
urate IE accuracy and (2) our method of pattern
construction was too limited.
Effect of Training Corpus Size To investigate
whether the training corpus was large enough to
maximize IE accuracy, we conducted experiments
on training corpora of various sizes. Figure 6 plots
graphs of F-measures by SCORE and Figure 7
plots the number of combination patterns on train-
ing corpora of various sizes. From Figures 6 and 7,
the training corpus (207 abstracts at a maximum)
 0.35
 0.4
 0.45
 0.5
 0.55
 0  50  100  150  200
F-m
eas
ure
 by
 SC
OR
E
Training Corpus Size (Number of Abstracts)
Figure 6: Effect of Training Corpus Size (1)
 0
 100
 200
 300
 400
 500
 600
 0  50  100  150  200
Nu
mb
er
Training Corpus Size (Number of Abstracts)
Raw Patterns (before division)Main ComponentEntity ComponentOther ComponentNaive Pattern
Figure 7: Effect of Training Corpus Size (2)
is not large enough. Thus increasing corpus size
will further improve IE accuracy.
Limitation of the Present Pattern Construc-
tion The limitations with our pattern construc-
tion method are revealed by the fact that we
could not achieve a high precision like Bunescu
and Mooney (2006) within the high-recall range.
Compared to theirs, one of our problems is that our
method could not handle attributives. One exam-
ple is ?binding property of ENTITY1 to ENTITY2?.
We could not obtain ?binding? because the small-
est set of PASs connecting ?ENTITY1? and ?EN-
TITY2? includes only the PASs of ?property?, ?of?
and ?to?. To handle these attributives, we need dis-
tinguish necessary attributives from those that are
general4 by semantic analysis or bootstrapping.
Another approach to improve our method is to
include local information in sentences, such as
surface words between protein names. Zhao and
Grishman (2005) reported that adding local infor-
mation to deep syntactic information improved IE
results. This approach is also applicable to IE in
other domains, where related entities are in a short
4Consider the case where a source sentence for a pattern is
?ENTITY1 is an important homodimeric protein.? (?homod-
imeric? represents that two molecules of ?ENTITY1? interact
with each other.)
291
distance like the work of Zhou et al (2005).
6 Conclusion
We proposed the use of PASs to construct pat-
terns as extraction rules, utilizing their ability to
abstract syntactical variants with the same rela-
tion. In addition, we divided the patterns for gen-
eralization, and used matching results for SVM
learning. In experiments on extracting of protein-
protein interactions, we obtained 71.8% precision
and 48.4% recall on a small corpus and 64.0% pre-
cision and 83.8% recall estimated on a large text,
which demonstrated the obvious advantages of our
method.
Acknowledgement
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
R. Agrawal and R Srikant. 1995. Mining Sequential
Patterns. In Proc. the 11th International Conference
on Data Engineering, pages 3?14.
Christian Blaschke and Alfonso Valencia. 2002. The
Frame-Based Module of the SUISEKI Informa-
tion Extraction System. IEEE Intelligent Systems,
17(2):14?20.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proc. ACL?04, pages 439?446.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proc. HLT/EMNLP 2005, pages 724?
731.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Advances
in Neural Information Processing Systems 18, pages
171?178. MIT Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL?04,
pages 423?429.
Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming
Li. 2005. Discovering patterns to extract protein-
protein interactions from the literature: Part II.
Bioinformatics, 21(15):3294?3300.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Advances in Kernel Methods
? Support Vector Learning. MIT-Press.
G Joshi-Tope, M Gillespie, I Vastrik, P D?Eustachio,
E Schmidt, B de Bono, B Jassal, GR Gopinath,
GR Wu, L Matthews, S Lewis, E Birney, and Stein
L. 2005. Reactome: a knowledgebase of biologi-
cal pathways. Nucleic Acids Research, 33(Database
Issue):D428?D432.
Asako Koike, Yoshiyuki Kobayashi, and Toshihisa
Takagi. 2003. Kinase Pathway Database: An
Integrated Protein-Kinase and NLP-Based Protein-
Interaction Resource. Genome Research, 13:1231?
1243.
Linguistic Data Consortium. 2005. ACE Program.
http://projects.ldc.upenn.edu/ace/.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proc. AAI ?94.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proc. ACL 2003, pages 224?231.
Tsujii Laboratory. 2005a. Enju - A practical HPSG
parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsujii Laboratory. 2005b. GENIA Project.
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.
U.S. National Library of Medicine. 2006. PubMed.
http://www.pubmed.gov.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns.
In Proc. SMBM 2005, pages 60?69.
Daming Yao, Jingbo Wang, Yanmei Lu, Nathan No-
ble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Don-
ald G. Payan, Ming Li, and Kunbin Qu. 2004. Path-
wayFinder: Paving The Way Towards Automatic
Pathway Extraction. In Bioinformatics 2004: Proc.
the 2nd APBC, volume 29 of CRPIT, pages 53?62.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL?05, pages 419?426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL?05, pages 427?434.
292
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 136?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
Subdomain adaptation of a POS tagger with a small corpus 
 
1 Introduction    
For the domain of biomedical research abstracts, 
two large corpora, namely GENIA (Kim et al
2003) and Penn BioIE (Kulik et al2004) are avail-
able. Both are basically in human domain and the 
performance of systems trained on these corpora 
when they are applied to abstracts dealing with 
other species is unknown. In machine-learning-
based systems, re-training the model with addition 
of corpora in the target domain has achieved prom-
ising results (e.g. Tsuruoka et al2005, Lease et al
2005). In this paper, we compare two methods for 
adaptation of POS taggers trained for GENIA and 
Penn BioIE corpora to Drosophila melanogaster 
(fruit fly) domain. 
2 Method 
Maximum Entropy Markov Models (MEMMs) 
(Ratnaparkhi 1996) and their extensions (Tutanova 
et al2003, Tsuruoka et al2005) have been success-
fully applied to English POS tagging. Here we use 
second-order standard MEMMs for learning POS. 
where the model parameters are determined with 
maximum entropy criterion in combination a regu-
larization method called inequality constraints 
(Kazama and Tsujii 2003). This regularization 
method has one non-negative meta-parameter 
called width-factor that controls the ?fitness? of the 
model parameters to the training data.
We used two methods of adapting a POS tagging 
model. One is to add the domain corpus to the 
training set. The other is to use the reference distri-
bution modeling, in which the training is per-
                                                          
    This work is partially supported by SORST program, Japan 
Science and Technology Agency. 
formed only on the domain corpus and the infor-
mation about the original training set is incorpo-
rated in the form of the reference distribution in 
the maximum entropy formulation (Johnson et al
2000, Hara et al2005). 
A set of 200 MEDLINE abstracts on D. 
melanogaster, was manually annotated with POS 
according to the scheme of the GENIA POS corpus 
(Tateisi et al2004) by one annotator. The new cor-
pus consists of 40,200 tokens in 1676 sentences. 
From this corpus which we call ?Fly? hereafter, 
1024 sentences are randomly taken and used for 
training. Half of the remaining is used for devel-
opment and the rest is used for testing.  
We measured the accuracy of the POS tagger 
trained in three settings:  
Original: The tagger is trained with the union of 
Wall Street Journal (WSJ) section of Penn 
Treebank (Marcus et al1993), GENIA, and 
Penn BioIE. In WSJ, Sections 0-18 for train-
ing, 19-21 for development, and 22-24 for 
test. In GENIA and Penn BioIE, 90% of the 
corpus is used for training and the rest is 
used for test. 
Combined: The tagger is trained with the union 
of the Original set plus N sentences from Fly.  
Refdist: Tagger is trained with N sentences 
from Fly, plus the Original set as reference. 
In Combined and Refdist settings, N is set to 8, 16, 
32, 64, 128, 256, 512, 1024 sentences to measure 
the learning curve. 
3 Results 
The accuracies of the tagger trained in the Origi-
nal setting were 96.4% on Fly, 96.7% on WSJ, 
Yuka Tateisi Yoshimasa Tsuruoka Jun-ichi Tsujii
Faculty of Informatics 
 Kogakuin University 
 Nishishinjuku 1-24-2 
Shinjuku-ku, Tokyo, 163-
8677, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K. 
Dept. of Computer Science 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, 
Tokyo 113-0033, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K.
136
98.1% on GENIA and 97.7% on Penn BioIE cor-
pora respectively. In the Combined setting, the ac-
curacies were 97.9% on Fly, 96.7% on WSJ, 
98.1% on GENIA and 97.7% on Penn BioIE. With 
Refdist setting, the accuracy on the Fly corpus was 
raised but those for WSJ and Penn BioIE corpora 
dropped from Original. When the width factor w 
was 10, the accuracy was 98.1% on Fly, but 95.4% 
on WSJ, 98.3% on GENIA and 96.6% on Penn 
BioIE. When the tagger was trained only on WSJ 
the accuracies were 88.7% on Fly, 96.9% on WSJ, 
85.0% on GENIA and 86.0% on Penn BioIE. 
When the tagger was trained only on Fly, the accu-
racy on Fly was even lower (93.1%). The learning 
curve indicated that the accuracies on the Fly cor-
pus were still rising in both Combined and Refdist 
settings, but both accuracies are almost as high as 
those of the original tagger on the original corpora 
(WSJ, GENIA and Penn BioIE), so in practical 
sense, 1024 sentences is a reasonable size for the 
additional corpus. When the width factor was 
smaller (2.5 and 5) the accuracies on the Fly cor-
pus were saturated with N=1024 with lower values 
(97.8% with w=2.5 and 98.0% with w=5).  
The amount of resources required for the Com-
bined and the Refdist settings were drastically dif-
ferent. In the Combined setting, the learning time 
was 30632 seconds and the required memory size 
was 6.4GB. On the other hand, learning in the Ref-
dist setting took only 21 seconds and the required 
memory size was 157 MB. 
The most frequent confusions involved the con-
fusion between FW (foreign words) with another 
class. Further investigation revealed that most of 
the error involved Linnaean names of species. Lin-
naean names are tagged differently in the GENIA 
and Penn BioIE corpora. In the GENIA corpus, 
tokens that constitute a Linnaean name are tagged 
as FW (foreign word) but in the Penn BioIE corpus 
they are tagged as NNP (proper noun). This seems 
to be one of the causes of the drop of accuracy on 
the Penn BioIE corpus when more sentences from 
the Fly corpus, whose tagging scheme follows that 
of GENIA, are added for training.
4 Conclusions 
We compared two methods of adapting a POS tag-
ger trained on corpora in human domain to fly do-
main. Training in Refdist setting required much 
smaller resources to fit to the target domain, but 
the resulting tagger is less portable to other do-
mains. On the other hand, training in Combined 
setting is slower and requires huge memory, but 
the resulting tagger is more robust, and fits rea-
sonably to various domains. 
References 
Tadayoshi Hara, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Adapting a probabilistic disambiguation model 
of an HPSG parser to a new domain. In Proceedings 
of  IJCNLP 2005, LNAI 3651, pp. 199-210. 
Mark Johnson and Stefan Riezler. 2000.  Exploiting 
auxiliary distributions in stochastic unification-based 
grammars. In Proceedings of 1st NAACL.  
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation 
and extension of maximum entropy models with ine-
quality constraints. In Proceedings of EMNLP 2003. 
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and 
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19(Suppl. 1):i180?i182. 
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, 
Ryan McDonald, Martha Palmer, Andrew Schein, 
Lyle Ungar, Scott Winters, and Pete White. 2004. In-
tegrated annotation for biomedical information ex-
traction. In Proceedings of BioLINK 2004, pp. 61?68. 
Matthew Lease and Eugene Charniak. 2005. Parsing 
Biomedical Literature, In Proceedings of  IJCNLP 
2005, LNAI 3651, pp. 58-69. 
Mitchell P. Marcus, Beatrice Sanorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank.  Computa-
tional Linguistics, Vol.19, pp. 313-330.  
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model 
for Part-Of-Speech Tagging. In Proceedings of 
EMNLP 1996. 
Yuka Tateisi and Jun'ichi Tsujii. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. In the 
Proceedings of LREC2004, vol. IV, pp. 1267-1270. 
Kristina Toutanova,  Dan Klein, Christopher Manning 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In Proceedings of HLT-NAACL 2003, pp. 173-180. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun'ichi Tsujii. 2005. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. In Proceed-
ings of 10th Panhellenic Conference on Informatics, 
LNCS 3746, pp. 382-392.  
137
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 467?474, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Bidirectional Inference with the Easiest-First Strategy
for Tagging Sequence Data
Yoshimasa Tsuruoka12 and Jun?ichi Tsujii231
1 CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan
2 Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
3 School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a bidirectional in-
ference algorithm for sequence label-
ing problems such as part-of-speech tag-
ging, named entity recognition and text
chunking. The algorithm can enumerate
all possible decomposition structures and
find the highest probability sequence to-
gether with the corresponding decomposi-
tion structure in polynomial time. We also
present an efficient decoding algorithm
based on the easiest-first strategy, which
gives comparably good performance to
full bidirectional inference with signifi-
cantly lower computational cost. Exper-
imental results of part-of-speech tagging
and text chunking show that the proposed
bidirectional inference methods consis-
tently outperform unidirectional inference
methods and bidirectional MEMMs give
comparable performance to that achieved
by state-of-the-art learning algorithms in-
cluding kernel support vector machines.
1 Introduction
The task of labeling sequence data such as part-of-
speech (POS) tagging, chunking (shallow parsing)
and named entity recognition is one of the most im-
portant tasks in natural language processing.
Conditional random fields (CRFs) (Lafferty et al,
2001) have recently attracted much attention be-
cause they are free from so-called label bias prob-
lems which reportedly degrade the performance of
sequential classification approaches like maximum
entropy markov models (MEMMs).
Although sequential classification approaches
could suffer from label bias problems, they have sev-
eral advantages over CRFs. One is the efficiency
of training. CRFs need to perform dynamic pro-
gramming over the whole sentence in order to com-
pute feature expectations in each iteration of numer-
ical optimization. Training, for instance, second-
order CRFs using a rich set of features can require
prohibitive computational resources. Max-margin
methods for structured data share problems of com-
putational cost (Altun et al, 2003).
Another advantage is that one can employ a vari-
ety of machine learning algorithms as the local clas-
sifier. There is huge amount of work about devel-
oping classification algorithms that have high gener-
alization performance in the machine learning com-
munity. Being able to incorporate such state-of-the-
art machine learning algorithms is important. In-
deed, sequential classification approaches with ker-
nel support vector machines offer competitive per-
formance in POS tagging and chunking (Gimenez
and Marquez, 2003; Kudo and Matsumoto, 2001).
One obvious way to improve the performance of
sequential classification approaches is to enrich the
information that the local classifiers can use. In stan-
dard decomposition techniques, the local classifiers
cannot use the information about future tags (e.g.
the right-side tags in left-to-right decoding), which
would be helpful in predicting the tag of the target
word. To make use of the information about fu-
ture tags, Toutanova et al proposed a tagging algo-
rithm based on bidirectional dependency networks
467
(Toutanova et al, 2003) and achieved the best ac-
curacy on POS tagging on the Wall Street Journal
corpus. As they pointed out in their paper, however,
their method potentially suffers from ?collusion? ef-
fects which make the model lock onto conditionally
consistent but jointly unlikely sequences. In their
modeling, the local classifiers can always use the in-
formation about future tags, but that could cause a
double-counting effect of tag information.
In this paper we propose an alternative way of
making use of future tags. Our inference method
considers all possible ways of decomposition and
chooses the ?best? decomposition, so the informa-
tion about future tags is used only in appropriate
situations. We also present a deterministic version
of the inference method and show their effective-
ness with experiments of English POS tagging and
chunking, using standard evaluation sets.
2 Bidirectional Inference
The task of labeling sequence data is to find the se-
quence of tags t1...tn that maximizes the following
probability given the observation o = o1...on
P (t1...tn|o). (1)
Observations are typically words and their lexical
features in the task of POS tagging. Sequential clas-
sification approaches decompose the probability as
follows,
P (t1...tn|o) =
n?
i=1
p(ti|t1...ti?1o). (2)
This is the left-to-right decomposition. If we
make a first-order markov assumption, the equation
becomes
P (t1...tn|o) =
n?
i=1
p(ti|ti?1o). (3)
Then we can employ a probabilistic classifier
trained with the preceding tag and observations in
order to obtain p(ti|ti?1o) for local classification. A
common choice for the local probabilistic classifier
is maximum entropy classifiers (Berger et al, 1996).
The best tag sequence can be efficiently computed
by using a Viterbi decoding algorithm in polynomial
time.
t1
(a)
t2 t3
o
t1
(b)
t2 t3
t1
(c)
t2 t3 t1
(d)
t2 t3
o
o o
Figure 1: Different structures for decomposition.
The right-to-left decomposition is
P (t1...tn|o) =
n?
i=1
p(ti|ti+1o). (4)
These two ways of decomposition are widely used
in various tagging problems in natural language pro-
cessing. The issue with such decompositions is that
you have only the information about the preceding
(or following) tags when performing local classifi-
cation.
From the viewpoint of local classification, we
want to give the classifier as much information as
possible because the information about neighboring
tags is useful in general.
As an example, consider the situation where we
are going to annotate a three-word sentence with
part-of-speech tags. Figure 1 shows the four possi-
ble ways of decomposition. They correspond to the
following equations:
(a) P (t1...t3|o) = P (t1|o)P (t2|t1o)P (t3|t2o) (5)
(b) P (t1...t3|o) = P (t3|o)P (t2|t3o)P (t1|t2o) (6)
(c) P (t1...t3|o) = P (t1|o)P (t3|o)P (t2|t3t1o) (7)
(d) P (t1...t3|o) = P (t2|o)P (t1|t2o)P (t3|t2o) (8)
(a) and (b) are the standard left-to-right and right-
to-left decompositions. Notice that in decomposi-
tion (c), the local classifier can use the information
about the tags on both sides when deciding t2. If,
for example, the second word is difficult to tag (e.g.
an unknown word), we might as well take the de-
composition structure (c) because the local classifier
468
can use rich information when deciding the tag of
the most difficult word. In general if we have an
n-word sentence and adopt a first-order markov as-
sumption, we have 2n?1 possible ways of decompo-
sition because each of the n ? 1 edges in the cor-
responding graph has two directions (left-to-right or
right-to-left).
Our bidirectional inference method is to consider
all possible decomposition structures and choose the
?best? structure and tag sequence. We will show in
the next section that this is actually possible in poly-
nomial time by dynamic programming.
As for the training, let us look at the equa-
tions of four different decompositions above. You
can notice that there are only four types of local
conditional probabilities: P (ti|ti?1o), P (ti|ti+1o),
P (ti|ti?1ti+1o), and P (ti|o).
This means that if we have these four types of lo-
cal classifiers, we can consider any decomposition
structures in the decoding stage. These local classi-
fiers can be obtained by training with corresponding
neighboring tag information. Training the first two
types of classifiers is exactly the same as the train-
ing of popular left-to-right and right-to-left sequen-
tial classification models respectively.
If we take a second-order markov assumption, we
need to train 16 types of local classifiers because
each of the four neighboring tags of a classification
target has two possibilities of availability. In gen-
eral, if we take a k-th order markov assumption, we
need to train 22k types of local classifies.
2.1 Polynomial Time Inference
This section describes an algorithm to find the de-
composition structure and tag sequence that give the
highest probability. The algorithm for the first-order
case is an adaptation of the algorithm for decoding
the best sequence on a bidirectional dependency net-
work introduced by (Toutanova et al, 2003), which
originates from the Viterbi decoding algorithm for
second-order markov models.
Figure 2 shows a polynomial time decoding al-
gorithm for our bidirectional inference. It enumer-
ates all possible decomposition structures and tag
sequences by recursive function calls, and finds the
highest probability sequence. Polynomial time is
achieved by caching. Note that for each local clas-
sification, the function chooses the appropriate local
function bestScore()
{
return bestScoreSub(n+2, ?end, end, end?, ?L,L?);
}
function bestScoreSub(i+1, ?ti?1, ti, ti+1?, ?di?1, di?)
{
// memorization
if (cached(i+1, ?ti?1, ti, ti+1?, ?di?1, di?))
return cache(i+1, ?ti?1, ti, ti+1?, ?di?1, di?);
// left boundary case
if (i = -1)
if (?ti?1, ti, ti+1? = ?start, start, start?) return 1;
else return 0;
// recursive case
P = localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?);
return maxdi?2 maxti?2 P?
bestScoreSub(i, ?ti?2, ti?1, ti?, ?di?2, di?1?);
}
function localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?)
{
if (di?1 = L & di = L) return P (ti|ti+1, o);
if (di?1 = L & di = R) return P (ti|o);
if (di?1 = R & di = L) return P (ti|ti?1ti+1, o);
if (di?1 = R & di = R) return P (ti|ti?1, o);
}
Figure 2: Pseudo-code for bidirectional inference
for the first-order conditional markov models. di is
the direction of the edge between ti and ti+1.
classifier by taking into account the directions of the
adjacent edges of the classification target.
The second-order case is similar but slightly more
complex. Figure 3 shows the algorithm. The recur-
sive function needs to consider the directions of the
four adjacent edges of the classification target, and
maintain the directions of the two neighboring edges
to enumerate all possible edge directions. In addi-
tion, the algorithm rules out cycles in the structure.
2.2 Decoding with the Easiest-First Strategy
We presented a polynomial time decoding algorithm
in the previous section. However, polynomial time is
not low enough in practice. Indeed, even the Viterbi
decoding of second-order markov models for POS
tagging is not practical unless some pruning method
is involved. The computational cost of the bidirec-
tional decoding algorithm presented in the previous
section is, of course, larger than that because it enu-
merates all possible directions of the edges on top of
the enumeration of possible tag sequences.
In this section we present a greedy version of the
decoding method for bidirectional inference, which
469
function bestScore()
{
return bestScoreSub(n+3, ?end, end, end, end, end?, ?L,L, L, L?, ?L,L?);
}
function bestScoreSub(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?)
{
// to avoid cycles
if (di?1 = di & di != d?i) return 0;
// memorization
if (cached(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?)
return cache(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?);
// left boundary case
if (i = -2)
if (?ti?2, ti?1, ti, ti+1, ti+2? = ?start, start, start, start, start?) return 1;
else return 0;
// recursive case
P = localClassification(i, ?ti?2, ti?1, ti, ti+1, ti+2?, ?d?i?1, di?1, di, d?i+1?);
return maxd?
i?2
maxdi?3 maxti?3 P? bestScoreSub(i+1, ?ti?3, ti?2, ti?1, titi+1?, ?d?i?2, di?2, di?1, d?i?, ?di?3, d?i?1?);
}
Figure 3: Pseudo-code for bidirectional inference for the second-order conditional markov models. di is the
direction of the edge between ti and ti+1. d?i is the direction of the edge between ti?1 and ti+1. We omit the
localClassification function because it is the obvious extension of that for the first-order case.
is extremely simple and significantly more efficient
than full bidirectional decoding.
Instead of enumerating all possible decomposi-
tion structures, the algorithm determines the struc-
ture by adopting the easiest-first strategy. The whole
decoding algorithm is given below.
1. Find the ?easiest? word to tag.
2. Tag the word.
3. Go back to 1. until all the words are tagged.
We assume in this paper that the ?easiest? word
to tag is the word for which the classifier outputs
the highest probability. In finding the easiest word,
we use the appropriate local classifier according to
the availability of the neighboring tags. Therefore,
in the first iteration, we always use the local classi-
fiers trained with no contextual tag information (i.e.
(P (ti|o)). Then, for example, if t3 has been tagged
in the first iteration in a three-word sentence, we use
P (t2|t3o) to compute the probability for tagging t2
in the second iteration (as in Figure 1 (b)).
A naive implementation of this algorithm requires
O(n2) invocations of local classifiers, where n is the
number of the words in the sentence, because we
need to update the probabilities over the words at
each iteration. However, a k-th order Markov as-
sumption obviously allows us to skip most of the
probability updates, resulting in O(kn) invocations
of local classifiers. This enables us to build a very
efficient tagger.
3 Maximum Entropy Classifier
For local classifiers, we used a maximum entropy
model which is a common choice for incorporating
various types of features for classification problems
in natural language processing (Berger et al, 1996).
Regularization is important in maximum entropy
modeling to avoid overfitting to the training data.
For this purpose, we use the maximum entropy
modeling with inequality constraints (Kazama and
Tsujii, 2003). The model gives equally good per-
formance as the maximum entropy modeling with
Gaussian priors (Chen and Rosenfeld, 1999), and
the size of the resulting model is much smaller than
that of Gaussian priors because most of the param-
eters become zero. This characteristic enables us
to easily handle the model data and carry out quick
decoding, which is convenient when we repetitively
perform experiments. This modeling has one param-
eter to tune, which is called the width factor. We
tuned this parameter using the development data in
each type of experiments.
470
Current word wi & ti
Previous word wi?1 & ti
Next word wi+1 & ti
Bigram features wi?1, wi & ti
wi, wi+1 & ti
Previous tag ti?1 & ti
Tag two back ti?2 & ti
Next tag ti+1 & ti
Tag two ahead ti+2 & ti
Tag Bigrams ti?2, ti?1 & ti
ti?1, ti+1 & ti
ti+1, ti+2 & ti
Tag Trigrams ti?2, ti?1, ti+1 & ti
ti?1, ti+1, ti+2 & ti
Tag 4-grams ti?2, ti?1, ti+1, ti+2 & ti
Tag/Word ti?1, wi & ti
combination ti+1, wi & ti
ti?1, ti+1, wi & ti
Prefix features prefixes of wi & ti
(up to length 10)
Suffix features suffixes of wi & ti
(up to length 10)
Lexical features whether wi has a hyphen & ti
whether wi has a number & ti
whether wi has a capital letter & ti
whether wi is all capital & ti
Table 1: Feature templates used in POS tagging ex-
periments. Tags are parts-of-speech. Tag features
are not necessarily used in all the models. For ex-
ample, ?next tag? features cannot be used in left-to-
right models.
4 Experiments
To evaluate the bidirectional inference methods pre-
sented in the previous sections, we ran experiments
on POS tagging and text chunking with standard En-
glish data sets.
Although achieving the best accuracy is not the
primary purpose of this paper, we explored useful
feature sets and parameter setting by using develop-
ment data in order to make the experiments realistic.
4.1 Part-of-speech tagging experiments
We split the Penn Treebank corpus (Marcus et al,
1994) into training, development and test sets as in
(Collins, 2002). Sections 0-18 are used as the train-
ing set. Sections 19-21 are the development set, and
sections 22-24 are used as the test set. All the ex-
periments were carried out on the development set,
except for the final accuracy report using the best
setting.
For features, we basically adopted the feature set
Method Accuracy Speed
(%) (tokens/sec)
Left-to-right (Viterbi) 96.92 844
Right-to-left (Viterbi) 96.89 902
Dependency Networks 97.06 1,446
Easiest-last 96.58 2,360
Easiest-first 97.13 2,461
Full bidirectional 97.12 34
Table 2: POS tagging accuracy and speed on the de-
velopment set.
Method Accuracy (%)
Dep. Networks (Toutanova et al, 2003) 97.24
Perceptron (Collins, 2002) 97.11
SVM (Gimenez and Marquez, 2003) 97.05
HMM (Brants, 2000) 96.48
Easiest-first 97.10
Full Bidirectional 97.15
Table 3: POS tagging accuracy on the test set (Sec-
tions 22-24 of the WSJ, 5462 sentences).
provided by (Toutanova et al, 2003) except for com-
plex features such as crude company-name detection
features because they are specific to the Penn Tree-
bank and we could not find the exact implementation
details. Table 1 lists the feature templates used in our
experiments.
We tested the proposed bidirectional methods,
conventional unidirectional methods and the bidirec-
tional dependency network proposed by Toutanova
(Toutanova et al, 2003) for comparison. 1. All
the models are second-order. Table 2 shows the
accuracy and tagging speed on the development
data 2. Bidirectional inference methods clearly out-
performed unidirectional methods. Note that the
easiest-first decoding method achieves equally good
performance as full bidirectional inference. Table 2
also shows that the easiest-last strategy, where we
select and tag the most difficult word at each itera-
tion, is clearly a bad strategy.
An example of easiest-first decoding is given be-
low:
1For dependency network and full bidirectional decoding,
we conducted pruning because the computational cost was too
large to perform exhaustive search. We pruned a tag candidate if
the zero-th order probability of the candidate P (ti|o) was lower
than one hundredth of the zero-th order probability of the most
likely tag at the token.
2Tagging speed was measured on a server with an AMD
Opteron 2.4GHz CPU.
471
The/DT/4 company/NN/7 had/VBD/11
sought/VBN/14 increases/NNS/13 total-
ing/VBG/12 $/$/2 80.3/CD/5 million/CD/8
,/,/1 or/CC/6 22/CD/9 %/NN/10 ././3
Each token represents Word/PoS/DecodingOrder.
Typically, punctuations and articles are tagged first.
Verbs are usually tagged in later stages because their
tags are likely to be ambiguous.
We applied our bidirectional inference methods
to the test data. The results are shown in Table 3.
The table also summarizes the accuracies achieved
by several other research efforts. The best accuracy
is 97.24% achieved by bidirectional dependency net-
works (Toutanova et al, 2003) with a richer set of
features that are carefully designed for the corpus. A
perceptron algorithm gives 97.11% (Collins, 2002).
Gimenez and Marquez achieve 97.05% with support
vector machines (SVMs). This result indicates that
bidirectional inference with maximum entropy mod-
eling can achieve comparable performance to other
state-of-the-art POS tagging methods.
4.2 Chunking Experiments
The task of chunking is to find non-recursive phrases
in a sentence. For example, a chunker segments the
sentence ?He reckons the current account deficit will
narrow to only 1.8 billion in September? into the fol-
lowing,
[NP He] [VP reckons] [NP the current account
deficit] [VP will narrow] [PP to] [NP only 1.8 bil-
lion] [PP in] [NP September] .
We can regard chunking as a tagging task by con-
verting chunks into tags on tokens. There are several
ways of representing text chunks (Sang and Veen-
stra, 1999). We tested the Start/End representation
in addition to the popular IOB2 representation since
local classifiers can have fine-grained information
on the neighboring tags in the Start/End represen-
tation.
For training and testing, we used the data set pro-
vided for the CoNLL-2000 shared task. The training
set consists of section 15-18 of the WSJ corpus, and
the test set is section 20. In addition, we made the
development set from section 21 3.
We basically adopted the feature set provided in
3We used the Perl script provided on
http://ilk.kub.nl/? sabine/chunklink/
Current word wi & ti
Previous word wi?1 & ti
Word two back wi?2 & ti
Next word wi+1 & ti
Word two ahead wi+2 & ti
Bigram features wi?2, wi?1 & ti
wi?1, wi & ti
wi, wi+1 & ti
wi+1, wi+2 & ti
Current POS pi & ti
Previous POS pi?1 & ti
POS two back pi?2 & ti
Next POS pi+1 & ti
POS two ahead pi+2 & ti
Bigram POS features pi?2, pi?1 & ti
pi?1, pi & ti
pi, pi+1 & ti
pi+1, pi+2 & ti
Trigram POS features pi?2, pi?1, pi & ti
pi?1, pi, pi+1 & ti
pi, pi+1, pi+2 & ti
Previous tag ti?1 & ti
Tag two back ti?2 & ti
Next tag ti+1 & ti
Tag two ahead ti+2 & ti
Bigram tag features ti?2, ti?1 & ti
ti?1, ti+1 & ti
ti+1, ti+2 & ti
Table 4: Feature templates used in chunking experi-
ments.
(Collins, 2002) and used POS-trigrams as well. Ta-
ble 4 lists the features used in chunking experiments.
Table 5 shows the results on the development set.
Again, bidirectional methods exhibit better perfor-
mance than unidirectional methods. The difference
is bigger with the Start/End representation. Depen-
dency networks did not work well for this chunking
task, especially with the Start/End representation.
We applied the best model on the development
set in each chunk representation type to the test
data. Table 6 summarizes the performance on the
test set. Our bidirectional methods achieved F-
scores of 93.63 and 93.70, which are better than the
best F-score (93.48) of the CoNLL-2000 shared task
(Sang and Buchholz, 2000) and comparable to those
achieved by other state-of-the-art methods.
5 Discussion
There are some reports that one can improve the
performance of unidirectional models by combining
outputs of multiple taggers. Shen et al (2003) re-
ported a 4.9% error reduction of supertagging by
472
Representation Method Order Recall Precision F-score Speed (tokens/sec)
IOB2 Left-to-right 1 93.17 93.05 93.11 1,775
2 93.13 92.90 93.01 989
Right-to-left 1 92.92 92.82 92.87 1,635
2 92.92 92.74 92.87 927
Dependency Networks 1 92.71 92.91 92.81 2,534
2 92.61 92.95 92.78 1,893
Easiest-first 1 93.17 93.04 93.11 2,441
2 93.35 93.32 93.33 1,248
Full Bidirectional 1 93.29 93.14 93.21 712
2 93.26 93.12 93.19 48
Start/End Left-to-right 1 92.98 92.69 92.83 861
2 92.96 92.67 92.81 439
Right-to-left 1 92.92 92.83 92.87 887
2 92.89 92.74 92.82 451
Dependency Networks 1 87.10 89.56 88.32 1,894
2 87.16 89.44 88.28 331
Easiest-first 1 93.33 92.95 93.14 1,950
2 93.31 92.95 93.13 1,016
Full Bidirectional 1 93.52 93.26 93.39 392
2 93.44 93.20 93.32 4
Table 5: Chunking F-scores on the development set.
Method Recall Precision F-score
SVM (Kudoh and Matsumoto, 2000) 93.51 93.45 93.48
SVM voting (Kudo and Matsumoto, 2001) 93.92 93.89 93.91
Regularized Winnow (with basic features) (Zhang et al, 2002) 93.60 93.54 93.57
Perceptron (Carreras and Marquez, 2003) 93.29 94.19 93.74
Easiest-first (IOB2, second-order) 93.59 93.68 93.63
Full Bidirectional (Start/End, first-order) 93.70 93.65 93.70
Table 6: Chunking F-scores on the test set (Section 20 of the WSJ, 2012 sentences).
pairwise voting between left-to-right and right-to-
left taggers. Kudo et al (2001) attained performance
improvement in chunking by conducting weighted
voting of multiple SVMs trained with distinct chunk
representations. The biggest difference between our
approach and such voting methods is that the lo-
cal classifier in our bidirectional inference methods
can have rich information for decision. Also, vot-
ing methods generally need many tagging processes
to be run on a sentence, which makes it difficult to
build a fast tagger.
Our algorithm can be seen as an ensemble classi-
fier by which we choose the highest probability one
among the different taggers with all possible decom-
position structures. Although choosing the highest
probability one is seemingly natural and one of the
simplest ways for combining the outputs of different
taggers, one could use a different method (e.g. sum-
ming the probabilities over the outputs which share
the same label sequence). Investigating the methods
for combination should be an interesting direction of
future work.
As for the computational cost for training, our
methods require us to train 22n types of classifiers
when we adopt an nth order markov assumption. In
many cases a second-order model is sufficient be-
cause further increase of n has little impact on per-
formance. Thus the training typically takes four or
16 times as much time as it would take for training a
single unidirectional tagger, which looks somewhat
expensive. However, because each type of classi-
fier can be trained independently, the training can
be performed completely in parallel and run with
the same amount of memory as that for training a
single classifier. This advantage contrasts with the
case for CRFs which requires substantial amount of
memory and computational cost if one tries to incor-
porate higher-order features about tag sequences.
Tagging speed is another important factor in
building a practical tagger for large-scale text min-
473
ing. Our inference algorithm with the easiest-first
strategy needs no Viterbi decoding unlike MEMMs
and CRFs, and makes it possible to perform very fast
tagging with high precision.
6 Conclusion
We have presented a bidirectional inference algo-
rithm for sequence labeling problems such as POS
tagging, named entity recognition and text chunk-
ing. The algorithm can enumerate all possible de-
composition structures and find the highest prob-
ability sequence together with the corresponding
decomposition structure in polynomial time. We
have also presented an efficient bidirectional infer-
ence algorithm based on the easiest-first strategy,
which gives comparable performance to full bidi-
rectional inference with significantly lower compu-
tational cost.
Experimental results of POS tagging and text
chunking show that the proposed bidirectional in-
ference methods consistently outperform unidi-
rectional inference methods and our bidirectional
MEMMs give comparable performance to that
achieved by state-of-the-art learning algorithms in-
cluding kernel support vector machines.
A natural extension of this work is to replace
the maximum entropy modeling, which was used as
the local classifiers, with other machine learning al-
gorithms. Support vector machines with appropri-
ate kernels is a good candidate because they have
good generalization performance as a single classi-
fier. Although SVMs do not output probabilities, the
easiest-first method would be easily applied by con-
sidering the margins output by SVMs as the confi-
dence of local classification.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. In Proceedings of ICML 2003, pages 3?10.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP Con-
ference (ANLP).
Xavier Carreras and Lluis Marquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS -99-108, Carnegie Mellon
University.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002, pages 1?8.
Jesus Gimenez and Lluis Marquez. 2003. Fast and accu-
rate part-of-speech tagging: The SVM approach revis-
ited. In Proceedings of RANLP 2003, pages 158?165.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP 2003.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
Taku Kudoh and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000, pages 142?144.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the conll-2000 shared task: Chunking.
In Proceedings of CoNLL-2000 and LLL-2000, pages
127?132.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL 1999,
pages 173?179.
Libin Shen and Aravind K. Joshi. 2003. A SNoW based
Supertagger with Application to NP Chunking. In
Proceedings of ACL 2003, pages 505?512.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Tong Zhang, Fred Damereau, and David Johnson. 2002.
Text chunking based on a generalization of winnow.
Journal of Machine Learning Research, 2:615?638.
474
Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain
Tadayoshi Hara1, Yusuke Miyao1, and Jun?ichi Tsujii1,2,3
1 Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
2 CREST, JST (Japan Science and Technology Agency),
Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
3 School of Informatics, University of Manchester,
POBox 88, Sackville St, Manchester, M60 1QD, UK
Abstract. This paper describes a method of adapting a domain-inde-
pendent HPSG parser to a biomedical domain. Without modifying the
grammar and the probabilistic model of the original HPSG parser, we
develop a log-linear model with additional features on a treebank of the
biomedical domain. Since the treebank of the target domain is limited, we
need to exploit an original disambiguation model that was trained on a
larger treebank. Our model incorporates the original model as a reference
probabilistic distribution. The experimental results for our model trained
with a small amount of a treebank demonstrated an improvement in
parsing accuracy.
1 Introduction
Natural language processing (NLP) is being demanded in various fields, such
as biomedical research, patent application, and WWW, because an unmanage-
able amount of information is being published in unstructured data, i.e., natural
language texts. To exploit latent information in these, the assistance of NLP
technologies is highly required. However, an obstacle is the lack of portability
of NLP tools. In general, NLP tools specialized to each domain were developed
from scratch, or adapted by considerable human effort. This is because linguistic
resources for each domain, such as a treebank, have not been sufficiently devel-
oped yet. Since dealing with various kinds of domains is an almost intractable
job, sufficient resources can not be expected.
The method presented in this paper is the development of disambiguation
models of an HPSG parser by combining a disambiguation model of an original
parser with a new model adapting to a new domain. Although the training of a
disambiguation model of a parser requires a sufficient amount of a treebank, its
construction requires a considerable human effort. Hence, we exploit the original
disambiguation model that was trained with a larger, but domain-independent
treebank. Since the original disambiguation model contains rich information of
general grammatical constraints, we try to use its information in developing a
disambiguation model for a new domain.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 199?210, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
200 T. Hara, Y. Miyao, and J. Tsujii
Our disambiguation model is a log-linear model into which the original disam-
biguation model is incorporated as a reference distribution. However, we cannot
simply estimate this model, because of the problem that has been discussed in
studies of the probabilistic modeling of unification-based grammars [1,2]. That
is, the exponential explosion of parse candidates assigned by the grammar makes
the estimation intractable. The previous studies solved the problem by applying
a dynamic programming algorithm to a packed representation of parse trees. In
this paper, we borrow their idea, and define reference distribution on a packed
structure. With this method, the log-linear model with a reference distribution
can be estimated by using dynamic programming.
In the experiments, we used an HPSG parser originally trained with the
Penn Treebank [3], and evaluated a disambiguation model trained with the GE-
NIA treebank [4], which consisted of abstracts of biomedical papers. First, we
measured the accuracy of parsing and the time required for parameter estima-
tion. For comparison, we also examined other possible models other than our
disambiguation model. Next, we varied the size of a training corpus in order to
evaluate the size sufficient for domain adaptation. Then, we varied feature sets
used for training and examined the parsing accuracy. Finally, we compared the
errors in the parsing results of our model with those of the original parser.
In Section 2, we introduce the disambiguation model of an HPSG parser. In
Section 3, we describe a method of adopting reference distribution for adapting
a probabilistic disambiguation model to a new domain. In Section 4, we examine
our method through experiments on the GENIA treebank.
2 An HPSG Parser
The HPSG parser used in this study is Enju [5]. The grammar of Enju was ex-
tracted from the Penn Treebank [3], which consisted of sentences collected from
The Wall Street Journal [6]. The disambiguation model of Enju was trained
on the same treebank. This means that the parser has been adapted to The
Wall Street Journal, and would be difficult to apply to other domains such
as biomedical papers that include different distribution of words and
their constraints.
In this study, we attempted the adaptation of a probabilistic disambiguation
model by fixing the grammar and the disambiguation model of the original
parser. The disambiguation model of Enju is based on a feature forest model
[2], which is a maximum entropy model [7] on packed forest structure. The
probability, pE(t|s), of producing the parse result t for a given sentence s is
defined as
pE(t|s) =
1
Zs
exp
(
?
i
?ifi(t, s)
)
Zs =
?
t??T (s)
exp
(
?
i
?ifi(t?, s)
)
,
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 201
Fig. 1. Chart for parsing ?he saw a girl with a telescope?
where T (s) is the set of parse candidates assigned to s. The feature function
fi(t, s) represents the characteristics of t and s, while the corresponding model
parameter ?i is its weight. Model parameters were estimated so as to maximize
the log-likelihood of the training data.
Estimation of the above model requires a set of training pairs ?ts, T (s)?, where
ts is the correct parse for the sentence s. While ts is provided by a treebank, T (s)
is computed by parsing each s in the treebank. However, the simple enumeration
of T (s) is impractical because the size of T (s) is exponential to the length of s.
To avoid an exponential explosion, Enju represented T (s) in a packed form of
HPSG parse trees [5]. In chart parsing, partial parse candidates are stored in a
chart, in which phrasal signs are identified and packed into an equivalence class
if they are determined to be equivalent and dominate the same word sequence.
A set of parse trees is then represented as a set of relations among equivalence
classes. Figure 1 shows a chart for parsing ?he saw a girl with a telescope?, where
the modifiee (?saw? or ?girl?) of ?with? is ambiguous. Each feature structure
expresses an equivalence class, and the arrows represent immediate-dominance
relations. The phrase, ?saw a girl with a telescope?, has two ambiguous subtrees
(A in the figure). Since the signs of the top-most nodes are equivalent, they are
packed into the same equivalence class. The ambiguity is represented as two
pairs of arrows that come out of the node.
A packed chart can be interpreted as an instance of a feature forest [2]. A
feature forest represents a set of exponentially-many trees in an ?and/or? graph
of a tractable size. A feature forest is formally defined as a tuple ?C, D, R, ?, ??,
where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R ? C
is a set of root nodes1, ? : D ? 2C is a conjunctive daughter function, and
? : C ? 2D is a disjunctive daughter function.
1 For the ease of explanation, the definition of root node is slightly different from the
original.
202 T. Hara, Y. Miyao, and J. Tsujii
HEAD  prep
MOD  NP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  prep
MOD  VP
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
he
saw
c1
c3
c2
c4
c5 c6
c8c7
Fig. 2. Packed representation of HPSG parse trees in Figure 1
Figure 2 shows (a part of) the HPSG parse trees in Figure 1 represented
as a feature forest. Square boxes are conjunctive nodes, dotted lines express a
disjunctive daughter function, and solid arrows represent a conjunctive daughter
function.
Based on the definition, parse tree t of sentence s can be represented as the
set of conjunctive nodes in the feature forest. The probability pE(t|s) is then
redefined as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
Zs =
?
t??T (s)
exp
(
?
c?t?
?
i
?ifi(c)
)
,
where fi(c) are alternative feature functions assigned to conjunctive nodes c ? C.
By using this redefined probability, a dynamic programming algorithm can be
applied to estimate p(t|T (s)) without unpacking the packed chart [2].
Feature functions in feature forest models are designed to capture the char-
acteristics of a conjunctive node. In HPSG parsing, it corresponds to a tuple of a
mother and its daughters. Enju uses features that are combinations of the atomic
features listed in Table 1. The following combinations are used for representing
the characteristics of the binary/unary rule applications.
fbinary =
?rule,dist,comma,
spanh, symh,wordh, posh, leh,
spann, symn, wordn, posn, len
?
funary = ?rule,sym,word,pos,le?
where suffixh andnmeans a headdaughter anda non-headdaughter, respectively.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 203
Table 1. Templates of atomic features
rule the name of the applied schema
dist the distance between the head words of the daughters
comma whether a comma exists between daughters and/or inside of daughter phrases
span the number of words dominated by the phrase
sym the symbol of the phrasal category (e.g. NP, VP)
word the surface form of the head word
pos the part-of-speech of the head word
le the lexical entry assigned to the head word
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
transitiveVBD,saw,S,root =f
prep-mod-vpwith,IN,PP,3,
,transitiveVBD,saw,VP,3,
mod,3,0,-head
binary =f
c1
c2
c3 c4
Fig. 3. Example features
In addition, the following feature is used for expressing the condition of the
root node of the parse tree.
froot = ?sym,word,pos,le?
Figure 3 shows example features: froot is the feature for the root node, in
which the phrase symbol is S and the surface form, part-of-speech, and lexical
entry of the lexical head are ?saw?, VBD, and a transitive verb, respectively.
The fbinary is the feature for the binary rule application to ?saw a girl? and
?with a telescope?, in which the applied schema is the Head-Modifier Schema, the
head daughter is VP headed by ?saw?, and the non-head daughter is PP headed
by ?with?, whose part-of-speech is IN and the lexical entry is a VP-modifying
preposition.
3 Re-training of Disambiguation Models
The method of domain adaptation is to develop a new maximum entropy model
with incorporating an original model as a reference probabilistic distribution.
The idea of adaptation using a reference distribution has already been presented
204 T. Hara, Y. Miyao, and J. Tsujii
in several studies [8,9]. When we have a reference probabilistic model p0(t|s) and
are making a new model pM (t|s), the probability is defined as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
j
?jgj(t?, s)
?
?
where Z ?s =
?
t??T (s)
p0(t?|s) exp
?
?
?
j
?jgj(t?, s)
?
? .
Model parameters, ?j, are estimated so as to maximize the likelihood of the
training data as in ordinary maximum entropy models. The maximization of the
likelihood with the above model is equivalent to finding the model pM that is
closest to the reference probability p0 in terms of the Kullback-Leibler distance.
However, we cannot simply apply the above method to our task because the
parameter estimation requires the computation of the above probability for all
parse candidates T (s). As discussed in Section 2, the size of T (s) is exponentially
related to the length of s. This imposes a new problem, that is, we need to
enumerate p0(t|s) for all candidate parses. Obviously, this is intractable.
Since Enju represented a probabilistic disambiguation model in a packed
forest structure, we exploit that structure to represent our probabilistic model.
That is, we redefine pM with feature functions gj on conjunctive nodes as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
c?t
?
j
?jgj(c)
?
?
where Z ?s =
?
t??T (s)
p0(t|s) exp
?
?
?
c?t?
?
j
?jgj(c)
?
? .
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
c1
c2
c3 c4
t1 selected
t2 selected
? j cgjj )( 1?
?i cfii )( 1?
? j cgjj )( 4?? j cgjj )( 3?? j cgjj )( 2?
?i cfii )( 2? ?i cfii )( 3? ?i cfii )( 4?
Fig. 4. Example of importing a reference distribution into each conjunctive node
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 205
As described in Section 2, the original model, pE(t|s), is expressed in a packed
structure as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
where Zs =
?
t??T (s)
exp
(
?
c?t
?
i
?ifi(c)
)
.
Then, p0(t|s) is substituted by pE(t|s), and pM (t|s) is formulated as
pM (t|s) =
1
Z ?s
{
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)}
exp
?
?
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ?s ? Zs
exp
?
?
?
c?t
?
i
?ifi(c) +
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ??s
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
where Z ??s = Zs ? Z ?s =
?
t?T (s)
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
.
With this form of pM (t|s), a dynamic programing algorithm can be applied.
For example, we show how to obtain probabilities of parse trees in the case of
Figure 4. For ease, we assume that there are only two disjunctive daughters
(dotted lines) that are of the top conjunctive node. The left disjunctive node
introduces a parse tree t1 that consists of conjunctive nodes {c1, c2, c3, . . . },
and the right one, t2 that consists of {c1, c2, c4, . . . }. To each conjunctive node
ck, a weight from the reference distribution
?
i ?ifi(ck) is assigned. Probability
pM (t1|s) and pM (t2|s) are then given as
pM (t1|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c3) +
?
j
?jgj(c3)
?
? + ? ? ?
?
?
?
pM (t2|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c4) +
?
j
?jgj(c4)
?
? + ? ? ?
?
?
?
.
206 T. Hara, Y. Miyao, and J. Tsujii
4 Experiments
We implemented the method described in Section 3. The original parser, Enju,
was developed on Section 02-21 of the Penn Treebank (39,832 sentences)[5]. For
the training of our model, we used the GENIA treebank [4], which consisted of
500 abstracts (4,446 sentences) extracted from MEDLINE. We divided the GENIA
treebank into three sets of 400, 50, and 50 abstracts (3,524, 455, and 467 sentences),
and these setswere used respectively as training, development, and final evaluation
data. The method of Gaussian MAP estimation [10] was used for smoothing.
The meta parameter ? of the Gaussian distribution was determined so as
to maximize the accuracy on the development set. In the following experiments,
we measured the accuracy of predicate-argument dependencies on the evaluation
set. The measure is labeled precision/recall (LP/LR), which is the same measure
as previous work [11,5] that evaluated the accuracy of lexicalized grammars on
the Penn Treebank.
First, we measured the accuracy of parsing and the time required for pa-
rameter estimation. Table 2 compares the results of the following estimation
methods.
Table 2. Accuracy and time cost for various estimation methods
F-score Training Parsing time (sec.)
GENIA Corpus Penn Treebank time (sec.) GENIA Corpus Penn Treebank
Our method 86.87 86.81 2,278 611 3,165
Combined 86.32 86.09 29,421 424 2,757
GENIA only 85.72 42.49 1,694 332 8,183
Original model 85.10 87.16 137,038 515 2,554
85
85.2
85.4
85.6
85.8
86
86.2
86.4
86.6
86.8
87
0 500 1000 1500 2000 2500 3000 3500
training sentences
F-
sc
ore
RULE WORDh + WORDn RULE + WORDh + WORDn
Fig. 5. Corpus size vs. Accuracy
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 207
Table 3. Accuracy with atomic feature templates
Features LP LR F-score diff.
RULE 85.42 84.87 85.15 +0.05
DIST 85.29 84.77 85.03 ?0.07
COMMA 85.45 84.86 85.15 +0.05
SPANh+SPANn 85.58 85.02 85.30 +0.20
SYMBOLh+SYMBOLn 85.01 84.56 84.78 ?0.32
WORDh+WORDn 86.59 86.07 86.33 +1.23
WORDh 85.48 84.98 85.23 +0.13
WORDn 85.44 84.64 85.04 ?0.06
POSh+POSn 85.23 84.77 85.00 ?0.10
LEh+LEn 85.42 85.06 85.24 +0.14
None 85.39 84.82 85.10
Table 4. Accuracy with the combination of RULE and other features
Features LP LR F-score diff.
RULE+DIST 85.41 84.85 85.13 +0.03
RULE+COMMA 85.92 85.15 85.53 +0.43
RULE+SPANh+SPANn 85.33 84.82 85.07 ?0.03
RULE+SYMBOLh+SYMBOLn 85.43 85.00 85.21 +0.11
RULE+WORDh+WORDn 87.12 86.62 86.87 +1.77
RULE + WORDh 85.74 84.94 85.34 +0.24
RULE + WORDn 85.10 84.60 84.85 ?0.25
RULE+POSh+POSn 85.51 85.08 85.29 +0.19
RULE+LEh+LEn 85.48 85.08 85.28 +0.18
None 85.39 84.82 85.10
Our method: training with our method
Combined: training Enju model with the training corpus replaced by the com-
bination of the GENIA corpus and the Penn Treebank
GENIA only: training Enju model with the training corpus replaced by the
GENIA corpus only
Original Model: training an original Enju model
The table shows the accuracy and the parsing time for the GENIA corpus and
the Penn Treebank Section 23, and also shows the time required for the training
of the model. The additional feature used in our method was RULE+WORDh+
WORDn, which will be explained later. In the ?Combined? method, we could
not train the model with the original training parameters (n = 20,  = 0.98 in
[5]) because the estimator ran out of memory. Hence, we reduced the parameters
to n = 10,  = 0.95.
For the GENIA corpus, our model gave the higher accuracy than the origi-
nal model and the other estimation methods, while for the Penn Treebank, our
model gave a little lower accuracy than the original model. This result indicates
that our model was more adapted to the specific domain. The ?GENIA only?
208 T. Hara, Y. Miyao, and J. Tsujii
Table 5. Accuracy with the combination of WORD and another feature
Features LP LR F-score diff.
WORDh+WORDn+RULE 87.12 86.62 86.87 +1.77
WORDh+WORDn+DIST 86.41 85.86 86.14 +1.04
WORDh+WORDn+COMMA 86.91 86.38 86.64 +1.54
WORDh+WORDn+SPANh+SPANn 85.77 85.22 85.49 +0.39
WORDh+WORDn+SYMBOLh+SYMBOLn 86.58 85.70 86.14 +1.04
WORDh+WORDn+POSh+POSn 86.53 85.99 86.26 +1.16
WORDh+WORDn+LEh+LEn 86.16 85.68 85.92 +0.82
None 85.39 84.82 85.10
Table 6. Errors in our model and Enju
Total errors Common errors Errors not in
the other model
Our model 1179 1050 129
Original model 1338 1050 288
method gave significantly lower accuracy. We expect that the method clearly
lacked the amount of the training corpus for obtaining generic grammatical
information.
The ?Combined? method achieved the accuracy close to our method. How-
ever, it is notable that our method took much less time for the training of the
model since ours did not need to handle the Penn Treebank. Instead, our method
exploited the original model of Enju, which was trained on the Penn Treebank,
and this resulted in much less cost of training.
Next, we changed the size of the GENIA treebank for training: 40, 80, 120,
160, 200, 240, 280, 320, 360, and 400 abstracts. Figure 5 shows the accuracy when
the size of the training data was changed. We can say that, for those feature sets
giving remarkable accuracy in the experiments, the accuracy edged upwards with
the size of the training corpus, and the trend does not seem to converge even if
more than 400 abstracts exist. If we choose more complex feature sets for higher
accuracy, data sparseness will occur and an even larger corpus will be needed.
These findings indicate that we can further improve the accuracy by using a
larger treebank and a proper feature set.
Table 3 shows the accuracy of models with only atomic feature templates.
The bottom of the table gives the accuracy attained by the original parser.
When we focus on the WORD features, we can see the combination of WORDh
and WORDn improved the accuracy significantly, although each of the features
by itself did not improve so much. DIST, SYMBOL, and POS feature templates
lowered the accuracy. The other feature templates improved the accuracy, though
not as well as the WORD templates.
Table 4 shows that the RULE feature combined with one or more other
features often gave a little higher accuracy than the RULE feature gave by
itself, though not as well as the WORD features.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 209
Table 5 shows that the WORD features combined with one or more other
features gave remarkable improvement to the accuracy as a whole. RULE and
COMMA features gave even higher accuracy than with only the WORD features.
Our results revealed that the WORD features were crucial for the adaptation to
the biomedical domain. We expect that this was because the biomedical domain
had a different distribution of words, while more generic grammatical constraints
were not significantly different from other domains.
Table 6 shows the comparison of the number of errors of our model with those
of the original model in parsing the GENIA corpus. Though our model gave less
errors than the original model, our model introduced a certain amount of new
errors. In future work, we need to investigate manually those errors to find more
suitable feature templates without losing the information in the original model.
5 Conclusions
We have presented a method of adapting a domain-independent HPSG parser
to a biomedical domain. Since the treebank of the new domain was limited,
we exploited an original disambiguation model. The new model was trained
on a biomedical treebank, and was combined with the original model by using
it as a reference distribution of a log-linear model. The experimental results
demonstrated our new model was adapted to the target domain, and was superior
to other adaptation methods in accuracy and the cost of training time. With our
model, the parsing accuracy for the target domain improved by 1.77 point with
the treebank of 3,524 sentences. Since the accuracy did not seem to saturate, we
will further improve the accuracy by increasing the size of the domain-dependent
treebank. In addition, the experimental results showed that the WORD feature
significantly contributed to the accuracy improvement.
We examined only a few feature templates, and we must search for further
more feature templates. Not only the new combinations of the atomic features
but also new types of features, which may be domain-dependent such as named
entities, will be possible.
References
1. Geman, S., Johnson, M.: Dynamic programming for parsing and estimation of
stochastic unification-based grammars. In: Proc. 40th ACL. (2002)
2. Miyao, Y., Tsujii, J.: Maximum entropy estimation for feature forests. In: Proc.
HLT 2002. (2002)
3. Marcus, M., Kim, G., Marcinkiewicz, M.A., MacIntyre, R., Bies, A., Ferguson, M.,
Katz, K., Schasberger, B.: The Penn Treebank: Annotating predicate argument
structure. In: ARPA Human Language Technology Workshop. (1994)
4. Kim, J.D., Ohta, T., Teteisi, Y., Tsujii, J.: Genia corpus - a semantically annotated
corpus for bio-textmining. Bioinformatics 19 (2003) i180?i182
5. Miyao, Y., Tsujii, J.: Probabilistic disambiguation models for wide-coverage HPSG
parsing. In: Proc. ACL 2005. (2005)
210 T. Hara, Y. Miyao, and J. Tsujii
6. Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development for
acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In:
Proc. IJCNLP-04. (2004)
7. Berger, A.L., Pietra, S.A.D., Pietra, V.J.D.: A maximum entropy approach to
natural language processing. Computational Linguistics 22 (1996) 39?71
8. Jelinek, F.: Statistical Methods for Speech Recognition. The MIT Press (1998)
9. Johnson, M., Riezler, S.: Exploiting auxiliary distributions in stochastic unification-
based grammars. In: Proc. 1st NAACL. (2000)
10. Chen, S., Rosenfeld, R.: A gaussian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon University (1999)
11. Clark, S., Curran, J.R.: Parsing the WSJ using CCG and log-linear models. In:
Proc. 42nd ACL. (2004)
Assigning Polarity Scores to Reviews
Using Machine Learning Techniques
Daisuke Okanohara1 and Jun?ichi Tsujii1,2,3
1 Department of Computer Science, University of Tokyo,
Hongo, 7-3-1, Bunkyo-ku, Tokyo 113-0013
2 CREST, JST, Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-0012
3 School of Informatics, University of Manchester,
POBox 88, Sackville St, Manchester, M60 1QD, UK
{hillbig, tsujii}@is.s.u-tokyo.ac.jp
Abstract. We propose a novel type of document classification task that
quantifies how much a given document (review) appreciates the target
object using not binary polarity (good or bad) but a continuous mea-
sure called sentiment polarity score (sp-score). An sp-score gives a very
concise summary of a review and provides more information than binary
classification. The difficulty of this task lies in the quantification of po-
larity. In this paper we use support vector regression (SVR) to tackle
the problem. Experiments on book reviews with five-point scales show
that SVR outperforms a multi-class classification method using support
vector machines and the results are close to human performance.
1 Introduction
In recent years, discussion groups, online shops, and blog systems on the Internet
have gained popularity and the number of documents, such as reviews, is growing
dramatically. Sentiment classification refers to classifying reviews not by their
topics but by the polarity of their sentiment (e.g, positive or negative). It is
useful for recommendation systems, fine-grained information retrieval systems,
and business applications that collect opinions about a commercial product.
Recently, sentiment classification has been actively studied and experimental
results have shown that machine learning approaches perform well [13,11,10,20].
We argue, however, that we can estimate the polarity of a review more finely. For
example, both reviews A and B in Table 1 would be classified simply as positive
in binary classification. Obviously, this classification loses the information about
the difference in the degree of polarity apparent in the review text.
We propose a novel type of document classification task where we evaluate
reviews with scores like five stars. We call this score the sentiment polarity score
(sp-score). If, for example, the range of the score is from one to five, we could
give five to review A and four to review B. This task, namely, ordered multi-class
classification, is considered as an extension of binary sentiment classification.
In this paper, we describe a machine learning method for this task. Our
system uses support vector regression (SVR) [21] to determine the sp-scores of
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 314?325, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Assigning Polarity Scores to Reviews 315
Table 1. Examples of book reviews
Example of Review binary sp-score
(1,...,5)
Review A I believe this is very good and a ?must read? plus 5
I can?t wait to read the next book in the series.
Review B This book is not so bad. plus 4
You may find some interesting points in the book.
Table 2. Corpus A: reviews for Harry Potter series book. Corpus B: reviews for all
kinds of books. The column of word shows the average number of words in a review,
and the column of sentences shows the average number of sentences in a review.
sp-score Corpus A Corpus B
review words sentences review words sentences
1 330 160.0 9.1 250 91.9 5.1
2 330 196.0 11.0 250 105.2 5.2
3 330 169.1 9.2 250 118.6 6.0
4 330 150.2 8.6 250 123.2 6.1
5 330 153.8 8.9 250 124.8 6.1
reviews. This method enables us to annotate sp-scores for arbitrary reviews such
as comments in bulletin board systems or blog systems. We explore several types
of features beyond a bag-of-words to capture key phrases to determine sp-scores:
n-grams and references (the words around the reviewed object).
We conducted experiments with book reviews from amazon.com each of
which had a five-point scale rating along with text. We compared pairwise sup-
port vector machines (pSVMs) and SVR and found that SVR outperformed
better than pSVMs by about 30% in terms of the squared error, which is close
to human performance.
2 Related Work
Recent studies on sentiment classification focused on machine learning ap-
proaches. Pang [13] represents a review as a feature vector and estimates the
polarity with SVM, which is almost the same method as those for topic classifi-
cation [1]. This paper basically follows this work, but we extend this task to a
multi-order classification task.
There have been many attempts to analyze reviews deeply to improve ac-
curacy. Mullen [10] used features from various information sources such as ref-
erences to the ?work? or ?artist?, which were annotated by hand, and showed
that these features have the potential to improve the accuracy. We use reference
features, which are the words around the fixed review target word (book), while
Mullen annotated the references by hand.
316 D. Okanohara and J. Tsujii
Turney [20] used semantic orientation, which measures the distance from
phrases to ?excellent? or ?poor? by using search engine results and gives the word
polarity. Kudo [8] developed decision stumps, which can capture substructures
embedded in text (such as word-based dependency), and suggested that subtree
features are important for opinion/modality classification.
Independently of and in parallel with our work, two other papers consider
the degree of polarity for sentiment classification. Koppel [6] exploited a neu-
tral class and applied a regression method as ours. Pang [12] applied a metric
labeling method for the task. Our work is different from their works in several
respects. We exploited square errors instead of precision for the evaluation and
used five distinct scores in our experiments while Koppel used three and Pang
used three/four distinct scores in their experiments.
3 Analyzing Reviews with Polarity Scores
In this section we present a novel task setting where we predict the degree of
sentiment polarity of a review. We first present the definition of sp-scores and
the task of assigning them to review documents. We then explain an evaluation
data set. Using this data set, we examined the human performance for this task
to clarify the difficulty of quantifying polarity.
3.1 Sentiment Polarity Scores
We extend the sentiment classification task to the more challenging task of as-
signing rating scores to reviews. We call this score the sp-score. Examples of
sp-scores include five-star and scores out of 100. Let sp-scores take discrete val-
ues1 in a closed interval [min...max]. The task is to assign correct sp-scores to
unseen reviews as accurately as possible. Let y? be the predicted sp-score and
y be the sp-score assigned by the reviewer. We measure the performance of an
estimator with the mean square error:
1
n
?n
i=1(y?i ? yi)2, (1)
where (x1, y1), ..., (xn, yn) is the test set of reviews. This measure gives a large
penalty for large mistakes, while ordered multi-class classification gives equal
penalties to any types of mistakes.
3.2 Evaluation Data
We used book reviews on amazon.com for evaluation data2 3. Each review has
stars assigned by the reviewer. The number of stars ranges from one to five:
1 We could allow sp-scores to have continuous values. However, in this paper we assume
sp-scores take only discrete values since the evaluation data set was annotated by
only discrete values.
2 http://www.amazon.com
3 These data were gathered from google cache using google API.
Assigning Polarity Scores to Reviews 317
one indicates the worst and five indicates the best. We converted the number
of stars into sp-scores {1, 2, 3, 4, 5} 4. Although each review may include several
paragraphs, we did not exploit paragraph information.
From these data, we made two data sets. The first was a set of reviews for
books in the Harry Potter series (Corpus A). The second was a set of reviews for
books of arbitrary kinds (Corpus B). It was easier to predict sp-scores for Corpus
A than Corpus B because Corpus A books have a smaller vocabulary and each
review was about twice as large. To create a data set with a uniform score distri-
bution (the effect of skewed class distributions is out of the scope of this paper),
we selected 330 reviews per sp-score for Corpus A and 280 reviews per sp-score
for Corpus B 5. Table 2 shows the number of words and sentences in the cor-
pora. There is no significant difference in the average number of words/sentences
among different sp-scores.
Table 3. Human performance of sp-score estimation. Test data: 100 reviews of Corpus
A with 1,2,3,4,5 sp-score.
Square error
Human 1 0.77
Human 2 0.79
Human average 0.78
cf. Random 3.20
All3 2.00
Table 4. Results of sp-score estimation: Human 1 (left) and Human 2 (right)
Assigned
1 2 3 4 5 Total
Correct
1 12 7 0 1 0 20
2 7 8 4 1 0 20
3 1 1 13 5 0 20
4 0 0 4 10 6 20
5 0 1 2 7 10 20
Total 20 17 23 24 16 100
Assigned
1 2 3 4 5 total
Correct
1 16 3 0 1 0 20
2 11 5 3 1 0 20
3 2 5 7 4 2 20
4 0 1 2 1 16 20
5 0 0 0 2 18 20
Total 29 14 12 9 36 100
3.3 Preliminary Experiments: Human Performance for Assigning
Sp-scores
We treat the sp-scores assigned by the reviewers as correct answers. However, the
content of a review and its sp-score may not be related. Moreover, sp-scores may
vary depending on the reviewers. We examined the universality of the sp-score.
4 One must be aware that different scales may reflect the different reactions than just
scales as Keller indicated [17].
5 We actually corrected 25000 reviews. However, we used only 2900 reviews since the
number of reviews with 1 star is very small. We examined the effect of the number
of training data is discussed in 5.3.
318 D. Okanohara and J. Tsujii
We asked two researchers of computational linguistics independently to assign
an sp-score to each review from Corpus A. We first had them learn the relation-
ship between reviews and sp-scores using 20 reviews. We then gave them 100
reviews with uniform sp-score distribution as test data. Table 3 shows the results
in terms of the square error. The Random row shows the performance achieved
by random assignment, and the All3 row shows the performance achieved by
assigning 3 to all the reviews. These results suggest that sp-scores would be
estimated with 0.78 square error from only the contents of reviews.
Table 4 shows the distribution of the estimated sp-scores and correct sp-
scores. In the table we can observe the difficulty of this task: the precise quantifi-
cation of sp-scores. For example, human B tended to overestimate the sp-score
as 1 or 5. We should note that if we consider this task as binary classifica-
tion by treating the reviews whose sp-scores are 4 and 5 as positive examples
and those with 1 and 2 as negative examples (ignoring the reviews whose sp-
scores are 3), the classification precisions by humans A and B are 95% and 96%
respectively.
4 Assigning Sp-scores to Reviews
This section describes a machine learning approach to predict the sp-scores of
review documents. Our method consists of the following two steps: extraction of
feature vectors from reviews and estimation of sp-scores by the feature vectors.
The first step basically uses existing techniques for document classification. On
the other hand, the prediction of sp-scores is different from previous studies
because we consider ordered multi-class classification, that is, each sp-score has
its own class and the classes are ordered. Unlike usual multi-class classification,
large mistakes in terms of the order should have large penalties. In this paper,
we discuss two methods of estimating sp-scores: pSVMs and SVR.
4.1 Review Representation
We represent a review as a feature vector. Although this representation ignores
the syntactic structure, word positions, and the order of words, it is known to
work reasonably well for many tasks such as information retrieval and document
classification. We use binary, tf, and tf-idf as feature weighting methods [15].
The feature vectors are normalized to have L2 norm 1.
4.2 Support Vector Regression
Support vector regression (SVR) is a method of regression that shares the un-
derlying idea with SVM [3,16]. SVR predicts the sp-score of a review by the
following regression:
f : Rn ? R, y = f(x) = ?w ? x? + b. (2)
Assigning Polarity Scores to Reviews 319
SVR uses an -insensitive loss function. This loss function means that all errors
inside the  cube are ignored. This allows SVR to use few support vectors and
gives generalization ability. Given a training set, (x1, y1), ...., (xn, yn), parame-
ters w and b are determined by:
minimize 12 ?w ? w? + C
?n
i=1(?i + ?
?
i )
subject to (?w ? x
i
? + b) ? yi ?  + ?i
yi ? (?w ? xi? + b) ?  + ??i
?(?)i ? 0 i = 1, ..., n. (3)
The factor C > 0 is a parameter that controls the trade-off between training
error minimization and margin maximization. The loss in training data increases
as C becomes smaller, and the generalization is lost as C becomes larger. More-
over, we can apply a kernel-trick to SVR as in the case with SVMs by using a
kernel function.
This approach captures the order of classes and does not suffer from data
sparseness. We could use conventional linear regression instead of SVR [4]. But
we use SVR because it can exploit the kernel-trick and avoid over-training.
Another good characteristic of SVR is that we can identify the features con-
tributing to determining the sp-scores by examining the coefficients (w in (2)),
while pSVMs does not give such information because multiple classifiers are in-
volved in determining final results. A problem in this approach is that SVR
cannot learn non-linear regression. For example, when given training data are
(x = 1, y = 1), (x = 2, y = 2), (x = 3, y = 8), SVR cannot perform regression
correctly without adjusting the feature values.
4.3 Pairwise Support Vector Machines
We apply a multi-class classification approach to estimating sp-scores. pSVMs
[7] considers each sp-score as a unique class and ignores the order among the
classes. Given reviews with sp-scores {1, 2, .., m}, we construct m ? (m ? 1)/2
SVM classifiers for all the pairs of the possible values of sp-scores. The classifier
for a sp-score pair (avsb) assigns the sp-score to a review with a or b. The class
label of a document is determined by majority voting of the classifiers. Ties in
the voting are broken by choosing the class that is closest to the neutral sp-score
(i.e, (1 + m)/2).
This approach ignores the fact that sp-scores are ordered, which causes the
following two problems. First, it allows large mistakes. Second, when the number
of possible values of the sp-score is large (e.g, n > 100), this approach suffers
from the data sparseness problem. Because pSVMs cannot employ examples that
have close sp-scores (e.g, sp-score = 50) for the classification of other sp-scores
(e.g, the classifier for a sp-score pair (51vs100)).
4.4 Features Beyond Bag-of-Words
Previous studies [9,2] suggested that complex features do not work as expected
because data become sparse when such features are used and a bag-of-words
320 D. Okanohara and J. Tsujii
Table 5. Feature list for experiments
Features Description Example in Fig.1 review 1
unigram single word (I) (believe) .. (series)
bigram pair of two adjacent words (I believe) ... (the series)
trigram adjacent three words (I believe this) ... (in the series)
inbook words in a sentence including ?book? (I) (can?t) ... (series)
aroundbook words near ?book? within two words. (the) (next) (in) (the)
approach is enough to capture the information in most reviews. Nevertheless,
we observed that reviews include many chunks of words such as ?very good? or
?must buy? that are useful for estimating the degree of polarity. We confirmed
this observation by using n-grams. Since the words around the review target
might be expected to influence the whole sp-score more than other words, we use
these words as features. We call these features reference. We assume the review
target is only the word ?book?, and we use ?inbook? and ?aroundbook? features.
The ?inbook? features are the words appear in the sentences which include the
word ?book?. The ?around book? are the words around the word ?book? within
two words. Table 5 summarizes the feature list for the experiments.
5 Experiments
We performed two series of experiments. First, we compared pSVMs and SVR.
Second, we examined the performance of various features and weighting methods.
We used Corpus A/B introduced in Sec. 3.2 for experiment data. We removed
all HTML tags and punctuation marks beforehand. We also applied the Porter
stemming method [14] to the reviews.
We divided these data into ten disjoint subsets, maintaining the uniform
class distribution. All the results reported below are the average of ten-fold cross-
validation. In SVMs and SVR, we used SVMlight6 with the quadratic polynomial
kernel K(x, z) = (?x ? z? + 1)2 and set the control parameter C to 100 in all the
experiments.
5.1 Comparison of pSVMs and SVR
We compared pSVMs and SVR to see differences in the properties of the regres-
sion approach compared with those of the classification approach. Both pSVMs
and SVR used unigram/tf-idf to represent reviews. Table 6 shows the square
error results for SVM, SVR and a simple regression (least square error) method
for Corpus A/B. These results indicate that SVR outperformed SVM in terms
of the square error and suggests that regression methods avoid large mistakes
by taking account of the fact that sp-scores are ordered, while pSVMs does not.
We also note that the result of a simple regression method is close to the result
of SVR with a linear kernel.
6 http://svmlight.joachims.org/
Assigning Polarity Scores to Reviews 321
Table 6. Comparison of multi-class SVM and SVR. Both use unigram/tf-idf.
Square error
Methods Corpus A Corpus B
pSVMs 1.32 2.13
simple regression 1.05 1.49
SVR (linear kernel) 1.01 1.46
SVR (polynomial kernel (?x ? z? + 1)2) 0.94 1.38
Figure 1 shows the distribution of estimation results for humans (top left:
human 1, top right: human 2), pSVMs (below left), and SVR (below right). The
horizontal axis shows the estimated sp-scores and the vertical axis shows the
correct sp-scores. Color density indicates the number of reviews. These figures
suggest that pSVMs and SVR could capture the gradualness of sp-scores better
than humans could. They also show that pSVMs cannot predict neutral sp-scores
well, while SVR can do so well.
5.2 Comparison of Different Features
We compared the different features presented in Section 4.4 and feature weight-
ing methods. First we compared different weighting methods. We used only
unigram features for this comparison. We then compared different features. We
used only tf-idf weighting methods for this comparison.
Table 7 summarizes the comparison results of different feature weighting
methods. The results show that tf-idf performed well on both test corpora.
We should note that simple representation methods, such as binary or tf, give
comparable results to tf-idf, which indicates that we can add more complex
features without considering the scale of feature values. For example, when we
add word-based dependency features, we have some difficulty in adjusting these
feature values to those of unigrams. But we could use these features together in
binary weighting methods.
Table 8 summarizes the comparison results for different features. For Corpus
A, unigram + bigram and unigram + trigram achieved high performance. The per-
formance of unigram + inbook was not good, which is contrary to our intuition that
the words that appear around the target object are more important than others.
For Corpus B, the results was different, that is, n-gram features could not predict
the sp-scores well. This is because the variety of words/phrases was much larger
than in Corpus A and n-gram features may have suffered from the data sparseness
problem. We should note that these feature settings are too simple, and we cannot
accept the result of reference or target object (inbook/aroundbook) directly.
Note that the data used in the preliminary experiments described in Section
3.3 are a part of Corpus A. Therefore we can compare the results for humans
with those for Corpus A in this experiment. The best result by the machine
learning approach (0.89) was close to the human results (0.78).
To analyze the influence of n-gram features, we used the linear kernel
k(x, z):= ?x ? z? in SVR training. We used tf-idf as feature weighting. We then
322 D. Okanohara and J. Tsujii
1 2 3 4 5
1
2
3
4
5
estimated sp-score
c o
rr
e c
t s
p-
sc
o
re
10-12
8-10
6-8
4-6
2-4
0-2
1 2 3 4 5
1
2
3
4
5
estimated sp-score
co
rr
ec
t s
p-
sc
or
e
16-18
14-16
12-14
10-1
8-10
6-8
4-6
2-4
0-2
1 2 3 4 5
1
2
3
4
5
estimated sp-score
c o
rr
e c
t s
p-
sc
o
re
14.0 -16.0 
12.0 -14.0 
10.0 -12.0 
8.0 -10.0 
6.0 -8.0 
4.0 -6.0 
2.0 -4.0 
0.0 -2.0 
1 2 3 4 5
1
2
3
4
5
estimated sp-score
co
rr
ec
t s
p-
sc
o
re
14.0 -16.0 
12.0 -14.0 
10.0 -12.0 
8.0 -10.0 
6.0 -8.0 
4.0 -6.0 
2.0 -4.0 
0.0 -2.0 
Fig. 1. Distribution of estimation results. Color density indicates the number of re-
views. Top left: Human A, top right: Human B, below left: pSVMs, below right: SVR.
examined each coefficient of regression. Since we used the linear kernel, the co-
efficient value of SVR showed the polarity of a single feature, that is, this value
expressed how much the occurrence of a feature affected the sp-score. Tables 9
shows the coefficients resulting from the training of SVR. These results show
that neutral polarity words themselves, such as ?all? and ?age?, will affect the
overall sp-scores of reviews with other neutral polarity words, such as, ?all ag
(age)?, ?can?t wait?, ?on (one) star?, and ?not interest?.
5.3 Learning Curve
We generated learning curves to examine the effect of the size of training data on
the performance. Figure 2 shows the results of a classification task using unigram
/tf-idf to represent reviews. The results suggest that the performance can still be
improved by increasing the training data.
Assigning Polarity Scores to Reviews 323
Table 7. Comparison results of different feature weighting methods. We used unigrams
as features of reviews.
Square error
Weighting methods (unigram) Corpus A Corpus B
tf 1.03 1.49
tf-idf 0.94 1.38
binary 1.04 1.47
Table 8. Comparison results of different features. For comparison of different features
we tf-idf as weighting methods.
Square error
Feature (tf-idf) Corpus A Corpus B
unigram (baseline) 0.94 1.38
unigram + bigram 0.89 1.41
unigram + trigram 0.90 1.42
unigram + inbook 0.97 1.36
unigram + aroundbook 0.93 1.37
Table 9. List of bigram features that have ten best/worst polarity values estimated by
SVR in Corpus A/B. The column of pol expresses the estimated sp-score of a feature,
i.e., only this feature is fired in a feature vector. (word stemming was applied)
Corpus A (best) Corpus B (best)
pol bigram pol bigram
1.73 best book 1.64 the best
1.69 is a 1.60 read it
1.49 read it 1.37 a great
1.44 all ag 1.34 on of
1.30 can?t wait 1.31 fast food
1.20 it is 1.22 harri potter
1.14 the sorcer?s 1.19 highli recommend
1.14 great ! 1.14 an excel
1.13 sorcer?s stone 1.12 to read
1.11 come out 1.01 in the
Corpus A (worst) Corpus B (worst)
pol bigram pol bigram
-1.61 at all -1.19 veri disappoint
-1.50 wast of -1.13 wast of
-1.38 potter book -0.98 the worst
-1.36 out of -0.97 is veri
-1.28 not interest -0.96 ! !
-1.18 on star -0.85 i am
-1.14 the worst -0.81 the exampl
-1.13 first four -0.79 bui it
-1.11 a wast -0.76 veri littl
-1.08 no on -0.74 onli to
6 Conclusion
In this paper, we described a novel task setting in which we predicted sp-scores
- degree of polarity - of reviews. We proposed a machine learning method using
SVR to predict sp-scores.
We compared two methods for estimating sp-scores: pSVMs and SVR. Exper-
imental results with book reviews showed that SVR performed better in terms
of the square error than pSVMs by about 30%. This result agrees with our
324 D. Okanohara and J. Tsujii
0
0.5
1
1.5
2
2.5
3
0 50 100 150 200 250 300 350
A number of training reviews per sp-score
S q
u
a r
e  
e r
ro
r
Corpus A
Corpus B
Fig. 2. Learning curve for our task setting for Corpus A and Corpus B. We used SVR
as the classifier and unigram/tf-idf to represent of reviews.
intuition that pSVMs does not consider the order of sp-scores, while SVR cap-
tures the order of sp-scores and avoids high penalty mistakes. With SVR, sp-
scores can be estimated with a square error of 0.89, which is very close to the
square error achieved by human (0.78).
We examined the effectiveness of features beyond a bag-of-words and refer-
ence features (the words around the reviewed objects.) The results suggest that
n-gram features and reference features contribute to improve the accuracy.
As the next step in our research, we plan to exploit parsing results such as
predicate argument structures for detecting precise reference information. We
will also capture other types of polarity than attitude, such as modality and
writing position [8], and we will consider estimating these types of polarity.
We plan to develop a classifier specialized for ordered multi-class classifica-
tion using recent studies on machine learning for structured output space [19,18]
or ordinal regression [5] because our experiments suggest that both pSVMs and
SVR have advantages and disadvantages. We will develop a more efficient clas-
sifier that outperforms pSVMs and SVR by combining these ideas.
References
1. T . Joachims. Learning to Classify Text Using Support Vector Machines. Kluwer,
2002.
2. C. Apte, F. Damerau, and S. Weiss. Automated learning of decision rules for text
categorization. Information Systems, 12(3):233?251, 1994.
3. N. Cristianini and J. S. Taylor. An Introduction to Support Vector Machines and
other Kernel-based Learning Methods. Cambridge University Press, 2000.
4. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning.
Springer, 2001.
5. Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank bound-
aries for ordinal regression. In Advances in Large Margin Classifiers, pages 115?132.
MIT press, 2000.
Assigning Polarity Scores to Reviews 325
6. Moshe Koppel and Jonathan Schler. The importance of neutral examples for learn-
ing sentiment. In In Workshop on the Analysis of Informal and Formal Information
Exchange during Negotiations (FINEXIN), 2005.
7. U. Kresel. Pairwise Classification and Support Vector Machines Methods. MIT
Press, 1999.
8. T. Kudo and Y. Matsumoto. A boosting algorithm for classification of semi-
structured text. In Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 301?308, 2004.
9. D. Lewis. An evaluation of phrasal and clustered representations on a text cate-
gorization task. In Proceedings of SIGIR-92, 15th ACM International Conference
on Research and Development in Information Retrieval, pages 37?50, 1992.
10. A. Mullen and N. Collier. Sentiment analysis using Support Vector Machines with
diverse information sources. In Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL), 2004.
11. B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL), pages 271?278, 2004.
12. B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales. In Proceedings of the 43nd Meeting of
the Association for Computational Linguistics (ACL), 2005.
13. B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 79?86, 2002.
14. M.F. Porter. An algorithm for suffix stripping, program. Program, 14(3):130?137,
1980.
15. F. Sebastiani. Machine learning in automated text categorization. ACM Computing
Surveys, 34(1):1?47, 2002.
16. A. Smola and B. Sch. A tutorial on Support Vector Regression. Technical report,
NeuroCOLT2 Technical Report NC2-TR-1998-030, 1998.
17. Antonella Sorace and Frank Keller. Gradience in linguistic data. Lingua,
115(11):1497?1524, 2005.
18. B. Taskar. Learning Structured Prediction Models: A Large Margin Approach. PhD
thesis, Stanford University, 2004.
19. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine
learning for interdependent and structured output spaces. In Machine Learning,
Proceedings of the Twenty-first International Conference (ICML), 2004.
20. P. D. Turney. Thumbs up or thumbs down? semantic orientation applied to un-
supervised classification of reviews. In Proceedings of the 40th Meeting of the
Association for Computational Linguistics (ACL), pages 417?424, 2002.
21. V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
Syntax annotation for the GENIA corpus  
Yuka Tateisi1 Akane Yakushiji2 Tomoko Ohta1 Jun?ichi Tsujii2,3,1
1 CREST, Japan Science and Technology Agency 
4-1-8, Honcho, Kawaguchi-shi, Saitama 332-0012 Japan 
2 Department of Computer Science, University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan 
3 School of Informatics, University of Manchester 
POBox 88, Sackville St, MANCHESTER M60 1QD, UK 
{yucca,akane,okap,tsujii}@is.s.u-tokyo.ac.jp 
 
Abstract 
Linguistically annotated corpus based 
on texts in biomedical domain has been 
constructed to tune natural language 
processing (NLP) tools for bio-
textmining. As the focus of information 
extraction is shifting from "nominal" 
information such as named entity to 
"verbal" information such as function 
and interaction of substances, applica-
tion of parsers has become one of the 
key technologies and thus the corpus 
annotated for syntactic structure of sen-
tences is in demand. A subset of the 
GENIA corpus consisting of 500 
MEDLINE abstracts has been anno-
tated for syntactic structure in an XML-
based format based on Penn Treebank 
II (PTB) scheme. Inter-annotator 
agreement test indicated that the writ-
ing style rather than the contents of the 
research abstracts is the source of the 
difficulty in tree annotation, and that 
annotation can be stably done by lin-
guists without much knowledge of bi-
ology with appropriate guidelines 
regarding to linguistic phenomena par-
ticular to scientific texts. 
1 Introduction 
Research and development for information ex-
traction from biomedical literature (bio-
textmining) has been rapidly advancing due to 
demands caused by information overload in the 
genome-related field. Natural language process-
ing (NLP) techniques have been regarded as 
useful for this purpose. Now that focus of in-
formation extraction is shifting from extraction 
of ?nominal? information such as named entity 
to ?verbal? information such as relations of enti-
ties including events and functions, syntactic 
analysis is an important issue of NLP applica-
tion in biomedical domain. In extraction of rela-
tion, the roles of entities participating in the 
relation must be identified along with the verb 
that represents the relation itself. In text analysis, 
this corresponds to identifying the subjects, ob-
jects, and other arguments of the verb. 
Though rule-based relation information ex-
traction systems using surface pattern matching 
and/or shallow parsing can achieve high-
precision (e.g. Koike et al, 2004) in a particular 
target domain, they tend to suffer from low re-
call due to the wide variation of the surface ex-
pression that describe a relation between a verb 
and its arguments. In addition,  the portability of 
such systems is low because the system has to 
be re-equipped with different set of rules when 
different kind of relation is to be extracted. One 
solution to this problem is using deep parsers 
which can abstract the syntactic variation of a 
relation between a verb and its arguments repre-
sented in the text, and constructing extraction 
rule on the abstract predicate-argument structure. 
To do so, wide-coverage and high-precision 
parsers are required. 
While basic NLP techniques are relatively 
general and portable from domain to domain, 
customization and tuning are inevitable, espe-
cially in order to apply the techniques effec-
tively to highly specialized literatures such as 
research papers and abstracts. As recent ad-
vances in NLP technology depend on machine-
learning techniques, annotated corpora from 
which system can acquire rules (including 
grammar rules, lexicon, etc.) are indispensable 
220
resources for customizing general-purpose NLP 
tools. In bio-textmining, for example, training 
on part-of-speech (POS)-annotated GENIA cor-
pus was reported to improve the accuracy of 
JunK tagger (English POS tagger) (Kazama et 
al., 2001) from  83.5% to 98.1% on MEDLINE 
abstracts (Tateisi and Tsujii, 2004), and the 
FraMed corpus (Wermter and Hahn, 2004) was 
used to train TnT tagger on German (Brants, 
2000) to improve its accuracy from 95.7% to 
98% on clinical reports and other biomedical 
texts. Corpus annotated for syntactic structures 
is expected to play a similar role in tuning pars-
ers to biomedical domain, i.e., similar improve-
ment on the performance of parsers is expected 
by using domain-specific treebank as a resource 
for learning. For this purpose, we construct 
GENA Treebank (GTB), a treebank on research 
abstracts in biomedical domain. 
2 Outline of the Corpus 
The base text of GTB is that of the GENIA cor-
pus constructed at University of Tokyo (Kim et 
al., 2003), which is a collection of research ab-
stracts selected from the search results of 
MEDLINE database with keywords (MeSH 
terms) human, blood cells and transcription fac-
tors. In the GENIA corpus, the abstracts are en-
coded in an XML scheme where each abstract is 
numbered with MEDLINE UID and contains 
title and abstract. The text  of title and abstract is 
segmented into sentences in which biological 
terms are annotated with their semantic classes. 
The GENIA corpus is also annotated for part-of-
speech (POS) (Tateisi and Tsujii, 2004), and 
coreference is also annotated in a part of the 
GENIA corpus by MedCo project at Institute for 
Infocomm Research, Singapore (Yang et al 
2004).  
GTB is the addition of syntactic information 
to the GENIA corpus. By annotating various 
linguistic information on a same set of text, the 
GENIA corpus will be a resource not only for 
individual purpose such as named entity extrac-
tion or training parsers but also for integrated 
systems such as information extraction using 
deep linguistic analysis. Similar attempt of con-
structing integrated corpora is being done in 
University of Pennsylvania, where a corpus of 
MEDLINE abstracts in CYP450 and oncology 
domains where annotated for named entities, 
POS, and tree structure of sentences (Kulick et 
al, 2004).  
2.1 Annotation Scheme 
The annotation scheme basically follows the 
Penn Treebank II (PTB) scheme (Beis et al 
1995), encoded in XML. A non-null constituent 
is marked as an element, with its syntactic cate-
gory (which may be combined with its function 
tags indicating grammatical roles such as  -SBJ, 
-PRD, and -ADV) used as tags. A null constitu-
ent is marked as a childless element whose tag 
corresponds to its categories. Other function tags 
are encoded as attributes. Figure 1 shows an ex-
ample of annotated sentence in XML, and the 
corresponding PTB notation. The label ?S? 
means ?sentence?, ?NP? noun phrase, ?PP? 
prepositional phrase, and ?VP? verb phrase.  
The label ?NP-SBJ? means that the element is 
an NP that serves as the subject of the sentence. 
A null element, the trace of the object of ?stud-
ied? moved by passivization, is denoted by 
? <NP NULL="NONE" ref="i55"/>? in XML 
and ?*-55? in PTB notation. The number ?55? 
which refers to the identifier of the moved ele-
ment, is denoted by ?id? and ?ref? attributes in 
XML, and is denoted as a part of a label in PTB. 
In addition to changing the encoding, we 
made some modifications to the scheme. First, 
analysis within the noun phrase is simplified. 
Second, semantic division of adverbial phrases 
such as ??TMP? (time) and ??MNR? (manner) 
are not used: adverbial constituents other than 
?ADVP? (adverbial phrases) or ?PP? used ad-
verbially are marked with ?ADV tags but not 
with semantic tags. Third, a coordination struc-
ture is explicitly marked with the attribute 
SYN=?COOD? whereas in the original PTB 
scheme it is not marked as such.   
 In our GTB scheme, ?NX? (head of a com-
plex noun phrase) and ?NAC? (a certain kind of 
nominal modifier within a noun phrase) of the 
PTB scheme are not used. A noun phrase is gen-
erally left unstructured. This is mainly in order 
to simplify the process of annotation. In case of 
biomedical abstracts, long noun phrases often 
involve multi-word technical terms whose syn-
tactic structure is difficult to determine without 
deep domain knowledge. However, the structure 
of noun phrases are usually independent of the 
structure outside the phrase, so that it would be 
221
easier to analyze the phrases involving such 
terms independently (e.g. by biologists) and 
later merge the two analysis together. Thus we 
have decided that we leave noun phrases un-
structured in GTB annotation unless their analy-
sis is necessary for determining the structure 
outside the phrase. One of the exception is the 
cases that involves coordination where it is nec-
essary to explicitly mark up the coordinated 
constituents. 
In addition, we have added special attributes 
?TXTERR?, ?UNSURE?,  and ?COMMENT? 
for later inspection. The ?TXTERR? is used 
when the annotator suspects that there is a 
grammatical error in the original text; the 
?UNSURE? attribute is used when the annotator 
is not confident; and the ?COMMENT? is used 
for free comments (e.g. reason of using 
?UNSURE?) by the annotator.  
2.2   Annotation Process 
The sentences in the titles and abstracts of the 
base text of GENIA corpus are annotated manu-
ally using an XML editor used for the Global 
Document Annotation project (Hasida 2000). 
Although the sentence boundaries were adopted 
from the corpus, the tree structure annotation 
was done independently of POS- and term- an-
notation already done on the GENIA corpus. 
The annotator was a Japanese non-biologist who 
has previously involved in the POS annotation 
of the GENIA corpus and accustomed to the 
style of research abstracts in English. Manually 
annotated abstracts are automatically converted 
to the PTB format, merged with the POS annota-
tion of the GENIA corpus (version 3.02). 
3 Annotation Results 
So far, 500 abstracts are annotated and con-
verted to the merged PTB format. In the merg-
ing process, we found several annotation errors. 
The 500 abstracts with correction of these errors 
are made publicly available as ?The GENIA 
Treebank Beta Version? (GTB-beta).   
For further clean-up, we also tried to parse 
the corpus by the Enju parser (Miyao and Tsujii 
2004), and identify the error of the corpus by 
investigating into the parse errors. Enju is an 
HPSG parser that can be trained with PTB-type 
corpora which is reported to have 87% accuracy 
on Wall Street Journal portion of Penn Treebank 
corpus. Currently the accuracy of the parser 
drops down to 82% on GTB-beta, and although 
proper quantitative analysis is yet to be done, it 
was found that the mismatches between labels of 
the treebank and the GENIA POS corpus (e.g. 
an ?ing form labeled as noun in the POS corpus 
and as the head of a verb phrase in the tree cor-
pus) are a major source of parse error. The cor-
rection is complicated because several errors in 
the GENIA POS corpus were found in this 
cleaning-up process. When the cleaning-up 
process is done, we will make the corpus pub-
licly available as the proper release. 
<S><PP>In <NP>the present paper </NP></PP>, 
<NP-SBJ id="i55"><NP>the binding 
</NP><PP>of <NP>a [125I]-labeled aldosterone 
derivative </NP></PP><PP>to <NP><NP>plasma 
membrane rich fractions </NP><PP>of HML 
</PP></NP></PP></NP-SBJ><VP>was 
<VP>studied <NP NULL="NONE" 
ref="i55"/></VP> 
</VP>.</S> 
 
4 Inter-Annotator Agreement 
We have also checked inter-annotator agreement. 
Although the PTB scheme is popular among 
natural language processing society, applicabil-
ity of the scheme to highly specialized text such 
as research abstract is yet to be discussed. Espe-
cially, when the annotation is done by linguists, 
lack of domain knowledge might decrease the 
stability and accuracy of annotation. 
A small part of the base text set (10 ab-
stracts) was annotated by another annotator. The 
10 abstracts were chosen randomly, had 6 to 17 
sentences per abstract (total 108 sentences). The 
new annotator had a similar background as the 
first annotator that she is a Japanese non-
biologist who has experiences in translation of 
(S (PP In (NP the present paper)), (NP-SBJ-55 (NP 
the binding) (PP of (NP a [125I]-labeled aldosterone 
derivative)) (PP to (NP (NP plasma membrane rich 
fractions) (PP of HML)))) (VP was (VP studied *-
55)).) 
Figure 1. The sentence ?In the present paper, the binding of 
a [125I]-labeled aldosterone derivative to plasma mem-
brane rich fractions of HML was studied? annotated in 
XML and PTB formats.  
222
technical documents in English and in corpus 
annotation of  English texts. 
The two results were examined manually, 
and there were 131 disagreements. Almost every 
sentence had at least one disagreement. We have 
made the ?gold standard? from the two sets of 
abstracts by resolving the disagreements, and the 
accuracy of the annotators against this gold 
standard were 96.7% for the first annotator and 
97.4% for the second annotator. 
 Of the disagreement, the most prominent 
were the cases involving coordination, espe-
cially the ones with ellipsis. For example, one 
annotator annotated the phrase ?IL-1- and IL-18-
mediated function? as in Figure 2a, the other 
annotated as Figure 2b.  
 Such problem is addressed in the PTB 
guideline and both formats are allowed as alter-
natives. As coordination with ellipsis occurs 
rather frequently in research abstracts, this kind 
of phenomena has higher effect on decrease of 
the agreement rate than in Penn Treebank. Of 
the 131 disagreements, 25 were on this type of 
coordination. 
Another source of disagreement is the at-
tachment of modifiers such as prepositional 
phrases and pronominal adjectives. However, 
most are ?benign ambiguity? where the differ-
ence of the structure does not affect on interpre-
tation, such as ?high expression of STAT in 
monocytes? where the prepositional phrase ?in 
monocytes? can attach to ?expression? or 
?STAT? without much difference in meaning, 
and ?is augmented when the sensitizing tumor is 
a genetically modified variant? where the wh-
clause can attach to ?is augmented? or ?aug-
mented? without changing the meaning. The 
PTB guideline states that the modifier should be 
attached at the higher level in the former case 
and at the lower case in the latter. In the annota-
tion results, one annotator consistently attached 
the modifiers in both cases at the higher level, 
and the other consistently at the lower level, in-
dicating that the problem is in understanding the 
scheme rather than understanding the sentence. 
Only 15 cases were true ambiguities that needed 
knowledge of biology to solve, in which 5 in-
volved coordination (e.g., the scope of ?various? 
in ?various T cell lines and peripheral blood 
cells?) .  
 Although the number was small, there were 
disagreements on how to annotate a mathemati-
cal formula such as ?n=2? embedded in the sen-
tence, since mathematical formulae were outside 
the scope of the original PTB scheme. One an-
notator annotated this kind of phrase consis-
tently as a phrase with ?=? as an adjective, the 
other annotated as phrase with ?=? as a verb. 
There were 6 such cases. Another disagreement 
particular to abstracts is a treatment of labeled 
sentences. There were 8 sentences in two ab-
stracts where there is a label like ?Background:?.  
One annotator included the colon (?:?) in the la-
bel, while the other did not. Yet another is that 
one regarded the phrase ?Author et al as coor-
dination, and the other regarded ?et al as a 
modifier.   
<NP SYN="COOD"> 
<NP><ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
         <NP NULL="RNR" ref="i20"/></NP> 
and  
<NP>IL-18-mediated <NP NULL="RNR" ref="i20"/></NP> 
<NP id="i20">function </NP> 
 Other disagreements are more general type 
such as regarding ?-ed? form of a verb as an ad-
jective or a participle, miscellaneous errors such 
as omission of a subtype of label (such as ?-
PRD? or ?-SBJ) or the position of <PRN> tags 
<NP> 
<ADJP SYN="COOD"> 
  <ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
  and  
  <ADJP>IL-18-mediated </ADJP></ADJP> 
function  
</NP> 
    NP    
       
  ADJP   Function  
       
ADJP     and ADJP    
         
IL-1  *   IL-18 mediated   
Figure 2a. Annotation of a coordinated phrase by the first 
annotator. A* denotes a null constituent. 
</NP> 
        NP     
       
 NP And NP   
         
    ADJP  *20 IL-18 meidiated NP  
          
IL-1 *      function20
Figure 2b. Annotation of the same phrase as in Figure 2a 
by the second annotator.  A * denotes a null constituent 
and ?20? denotes coindexing. 
223
with regards to ?,? for the inserted phrase, or the 
errors which look like just ?careless?. Such dis-
agreements and mistakes are at least partially 
eliminated when reliable taggers and parsers are 
available for preprocessing 
5 Discussion 
The result of the inter-annotator agreement 
test indicates that the writing style rather than 
the contents of the research abstracts is the 
source of the difficulty in tree annotation. Con-
trary to the expectation that the lack of domain 
knowledge causes a problem in annotation on 
attachments of modifiers, the number of cases 
where annotation of modifier attachment needs 
domain knowledge is small. This indicates that 
linguists can annotate most of syntactic structure 
without an expert level of domain knowledge.  
A major source of difficulty is coordination, 
especially the ones involving ellipsis. Coordina-
tion is reported to be difficult phenomena in an-
notation of different levels in the GENIA corpus 
(Tateisi and Tsujii, 2004), (Kim et al, 2003). In 
addition to the fact that this is the major source 
of inter-annotator agreement, the annotator often 
commented the coordinated structure as ?unsure?. 
The problem of coordination can be divided into 
two with different nature: one is that the annota-
tion policy is still not well-established for the 
coordination involving ellipsis, and the other is 
an ambiguity when the coordinated phrase has 
modifiers.  
Syntax annotation of coordination with ellip-
sis is difficult in general but the more so in an-
notation of abstracts than in the case of general 
texts, because in abstracts authors tend to pack 
information in limited number of words. The 
PTB guideline dedicates a long section for this 
phenomena and allows alternatives in annotation, 
but there are still cases which are not well-
covered by the scheme. For example, in addition 
to the disagreement, the phrase illustrated in 
Figure 2a and Figure 2b shows another problem 
of the annotation scheme. Both annotators fail to 
indicate that it is ?mediated? that was to be after 
?IL-1? because there is no mechanism of 
coindexing a null element with a part of a token.  
This problem of ellipsis can frequently occur 
in research abstracts, and it can be argued that 
the tokenization criteria must be changed for 
texts in biomedical domain (Yamamoto and Sa-
tou, 2004) so that such fragment as ?IL-18? and 
?mediated? in ?IL-18-ediated? should be regarede 
as separate tokens. The Pennsylvania biology 
corpus (Kulick et al, 2004) partially solves this 
problem by separating a token where two or 
more subtokens are connected with hyphens, but 
in the cases where a shared part of the word is 
not separated by a hyphen (e.g. ?metric? of ?ste-
reo- and isometric alleles?) the word including 
the part is left uncut. The current GTB follows 
the GENIA corpus that it retains the tokeniza-
tion criteria of the original Penn Treebank, but 
this must be reconsidered in future. 
 For analysis of coordination with ellipsis, if 
the information on full forms is available, one 
strategy would be to leave the inside structure of 
coordination unannotated in the treebank corpus 
(and in the phase of text analysis the structure is 
not established in the phase of parsing but with a 
different mechanism) and later merge it with the 
coordination structure annotation. The GENIA 
term corpus annotates the full form of a techni-
cal term whose part is omitted in the surface as 
an attribute of the ?<cons>? element indicating a 
technical term (Kim et al, 2003). In the above-
mentioned Pennsylvania corpus, a similar 
mechanism (?chaining?) is used for recovering 
the full form of named entities. However, in 
both corpora, no such information is available 
outside the terms/entities.  
The cases where scope of modification in 
coordinated phrases is problematic are few but 
they are more difficult in abstracts than in gen-
eral texts because the resolution of ambiguity 
needs domain knowledge. If term/entity annota-
tion is already done, that information can help 
resolve this type of ambiguity, but again the 
problem is that outside the terms/entities such 
information is not available. It would be practi-
cal to have the structure flat but specially 
marked when the tree annotators are unsure and 
have a domain expert resolve the ambiguity, as 
the sentences that needs such intervention seems 
few. Some cases of ambiguity in modifier at-
tachment (which do not involve coordination) 
can be solved with similar process. 
We believe that other type of disagreements 
can be solved with supplementing criteria for 
linguistic phenomena not well-covered by the 
scheme, and annotator training. Automatic pre-
processing by POS taggers and parsers can also 
help increase the consistent annotation. 
224
6 Conclusion 
A subset of the GENIA corpus is annotated 
for syntactic (tree) structure. Inter-annotator 
agreement test indicated that the annotation can 
be done stably by linguists without much 
knowledge in biology, provided that proper 
guideline is established for linguistic phenomena 
particular to scientific research abstracts. We 
have made the 500-abstract corpus in both XML 
and PTB formats and made it publicly available 
as ?the GENIA Treebank beta version? (GTB-
beta). We are in further cleaning up process of 
the 500-abstract set, and at the same time, initial 
annotation of the remaining abstracts is being 
done, so that the full GENIA set of 2000 ab-
stracts will be annotated with tree structure.  
For parsers to be useful for information ex-
traction, they have to establish a map between 
syntactic structure and more semantic predicate-
argument structure, and between the linguistic 
predicate-argument structures to the factual rela-
tion to be extracted. Annotation of various in-
formation on a same set of text can help 
establish these maps. For the factual relations, 
we are annotating relations between proteins and 
genes in cooperation with a group of biologists. 
For predicate-argument annotation, we are in-
vestigating the use of the parse results of the 
Enju parser. 
Acknowledgments 
The authors are grateful to annotators and col-
leagues that helped the construction of the cor-
pus. This work is partially supported by Grant-
in-Aid for Scientific Research on Priority Area 
C ?Genome Information Science? from the Min-
istry of Education, Culture, Sports, Science and 
Technology of Japan. 
References 
 Brants,T.(2000). TnT: a statistical part-of-speech 
tagger, Proceedings of the sixth conference on Ap-
plied natural language processing, pp.224-231, 
Morgan Kaufmann Publishers Inc.  
Beis.A., Ferguson,M., Katz,K., and Mac-
Intire,R.(1995). Bracketing Guidelines for Tree-
bank II Style: Penn Treebank Project, University 
of Pennsylvania 
Hasida, K. (2000). GDA: Annotated Document as 
Intelligent Content.  Proceedings of 
COLING?2000 Workshop on Semantic Annotation 
and Intelligent Content. 
Kazama,J., Miyao,Y., and Tsujii,J.(2001) A Maxi-
mum Entropy Tagger with Unsupervised Hidden 
Markov Models, Proceedings of the Sixth Natural 
Language Processing Pacific Rim Symposium, pp. 
333-340.  
Kim,J-D, Ohta,T., Tateisi,Y. and Tsujii,J. (2003). 
GENIA corpus - a semantically annotated corpus 
for bio-textmining. Bioinformatics. 19(suppl. 1). 
pp. i180-i182. Oxford University Press.  
Koike,A., Niwa,Y., and Takagi,T. (2004) Automatic 
extraction of gene/protein biological functions 
from biomedical text. Bioinformatics, Advanced 
Access published on October 27, 2004; 
doi:10.1093/bioinformatics/bti084.Oxford Univer-
sity Press. 
Kulick,S., Bies,A., Liberman,M., Mandel,M., 
McDonald,R., Palmer,M., Schein,A., Ungar,L., 
Winters,S. and White,P. (2004)  Integrated Anno-
tation for Biomedical Information Extraction. 
BioLINK 2004: Linking Biological Literature, On-
tologies, and Databases, pp. 61-68.Association for 
Computational Linguistics. 
Miyao,Y. and Tsujii,J. (2004a). Deep Linguistic 
Analysis for the Accurate Identification of Predi-
cate-Argument Relations. Proceedings of 
COLING 2004. pp. 1392-1397. 
Tateisi,Y. and Tsujii,J. (2004). Part-of-Speech Anno-
tation of Biology Research Abstracts. Proceedings 
of the 4th International Conference on Language 
Resource and Evaluation (LREC2004). IV. pp. 
1267-1270, European Language Resources Asso-
ciation. 
Wermter, J. and Hahn, U. (2004). An annotated Ger-
man-language medical text corpus. GMDS 2004 
meeting, 
http://www.egms.de/en/meetings/gmds2004/04gm
ds168.shtml. 
Yamamoto,K., and Satou,K (2004). Low-level Text 
Processing for Life Science, Proceedings of the 
SIG meeting on Natural Language Processing, In-
formation Processing Society of Japan, IPSJ-
SIGNL-159 (In Japanese). 
 Yang,XF., Zhou,GD., Su,J., and Tan.,CL (2004). 
Improving Noun Phrase Coreference Resolution 
by Matching Strings. Proceedings of 1st Interna-
tional Joint Conference on Natural Language 
Processing (IJCNLP'2004), pp226-233.
 
225
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103?114,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficacy of Beam Thresholding, Unification Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing
Takashi Ninomiya
CREST, JST
and
Department of Computer Science
The University of Tokyo
ninomi@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
CREST, JST
and
Department of Computer Science
The University of Tokyo
tsuruoka@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
The University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
The University of Tokyo
and
School of Informatics
University of Manchester
and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
We investigated the performance efficacy
of beam search parsing and deep parsing
techniques in probabilistic HPSG parsing
using the Penn treebank. We first tested
the beam thresholding and iterative pars-
ing developed for PCFG parsing with an
HPSG. Next, we tested three techniques
originally developed for deep parsing: quick
check, large constituent inhibition, and hy-
brid parsing with a CFG chunk parser. The
contributions of the large constituent inhi-
bition and global thresholding were not sig-
nificant, while the quick check and chunk
parser greatly contributed to total parsing
performance. The precision, recall and av-
erage parsing time for the Penn treebank
(Section 23) were 87.85%, 86.85%, and 360
ms, respectively.
1 Introduction
We investigated the performance efficacy of beam
search parsing and deep parsing techniques in
probabilistic head-driven phrase structure grammar
(HPSG) parsing for the Penn treebank. We first
applied beam thresholding techniques developed for
CFG parsing to HPSG parsing, including local
thresholding, global thresholding (Goodman, 1997),
and iterative parsing (Tsuruoka and Tsujii, 2005b).
Next, we applied parsing techniques developed for
deep parsing, including quick check (Malouf et al,
2000), large constituent inhibition (Kaplan et al,
2004) and hybrid parsing with a CFG chunk parser
(Daum et al, 2003; Frank et al, 2003; Frank, 2004).
The experiments showed how each technique con-
tributes to the final output of parsing in terms of
precision, recall, and speed for the Penn treebank.
Unification-based grammars have been extensively
studied in terms of linguistic formulation and com-
putation efficiency. Although they provide precise
linguistic structures of sentences, their processing is
considered expensive because of the detailed descrip-
tions. Since efficiency is of particular concern in prac-
tical applications, a number of studies have focused
on improving the parsing efficiency of unification-
based grammars (Oepen et al, 2002). Although sig-
nificant improvements in efficiency have been made,
parsing speed is still not high enough for practical
applications.
The recent introduction of probabilistic models of
wide-coverage unification-based grammars (Malouf
and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) has opened up the novel possibil-
ity of increasing parsing speed by guiding the search
path using probabilities. That is, since we often re-
quire only the most probable parse result, we can
compute partial parse results that are likely to con-
tribute to the final parse result. This approach has
been extensively studied in the field of probabilistic
103
CFG (PCFG) parsing, such as Viterbi parsing and
beam thresholding.
While many methods of probabilistic parsing for
unification-based grammars have been developed,
their strategy is to first perform exhaustive pars-
ing without using probabilities and then select the
highest probability parse. The behavior of their al-
gorithms is like that of the Viterbi algorithm for
PCFG parsing, so the correct parse with the high-
est probability is guaranteed. The interesting point
of this approach is that, once the exhaustive pars-
ing is completed, the probabilities of non-local de-
pendencies, which cannot be computed during pars-
ing, are computed after making a packed parse for-
est. Probabilistic models where probabilities are as-
signed to the CFG backbone of the unification-based
grammar have been developed (Kasper et al, 1996;
Briscoe and Carroll, 1993; Kiefer et al, 2002), and
the most probable parse is found by PCFG parsing.
This model is based on PCFG and not probabilis-
tic unification-based grammar parsing. Geman and
Johnson (Geman and Johnson, 2002) proposed a dy-
namic programming algorithm for finding the most
probable parse in a packed parse forest generated by
unification-based grammars without expanding the
forest. However, the efficiency of this algorithm is
inherently limited by the inefficiency of exhaustive
parsing.
In this paper we describe the performance of beam
thresholding, including iterative parsing, in proba-
bilistic HPSG parsing for a large-scale corpora, the
Penn treebank. We show how techniques developed
for efficient deep parsing can improve the efficiency
of probabilistic parsing. These techniques were eval-
uated in experiments on the Penn Treebank (Marcus
et al, 1994) with the wide-coverage HPSG parser de-
veloped by Miyao et al (Miyao et al, 2005; Miyao
and Tsujii, 2005).
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical en-
tries express word-specific characteristics. The struc-
tures of sentences are explained using combinations
of schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
Figure 1 shows an example of HPSG parsing of
the sentence ?Spring has come.? First, each of the
lexical entries for ?has? and ?come? is unified with a
daughter feature structure of the Head-Complement
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
1
=?
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing
Schema. Unification provides the phrasal sign of
the mother. The sign of the larger constituent is
obtained by repeatedly applying schemata to lexi-
cal/phrasal signs. Finally, the parse result is output
as a phrasal sign that dominates the sentence.
Given set W of words and set F of feature struc-
tures, an HPSG is formulated as a tuple, G = ?L,R?,
where
L = {l = ?w,F ?|w ? W, F ? F} is a set of lexical
entries, and
R is a set of schemata, i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of phrasal
signs, i.e., feature structures, as a result of parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Miyao et al, 2003; Mal-
ouf and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) defined a probabilistic model of
unification-based grammars as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability of parse result T assigned to given sen-
tence w = ?w1, . . . , wn? is
p(T |w) = 1Zw
exp
(
?
i
?ifi(T )
)
Zw =
?
T ?
exp
(
?
i
?ifi(T ?)
)
,
where ?i is a model parameter, and fi is a feature
function that represents a characteristic of parse tree
T . Intuitively, the probability is defined as the nor-
malized product of the weights exp(?i) when a char-
acteristic corresponding to fi appears in parse result
T . Model parameters ?i are estimated using numer-
104
ical optimization methods (Malouf, 2002) so as to
maximize the log-likelihood of the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the computa-
tion of p(T |w) for all parse candidates assigned to
sentence w. Because the number of parse candidates
is exponentially related to the length of the sentence,
the estimation is intractable for long sentences.
To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). They assumed that features are functions
on nodes in a packed parse forest. That is, parse tree
T is represented by a set of nodes, i.e., T = {c}, and
the parse forest is represented by an and/or graph
of the nodes. From this assumption, we can redefine
the probability as
p(T |w) = 1Zw
exp
(
?
c?T
?
i
?ifi(c)
)
Zw =
?
T ?
exp
(
?
c?T ?
?
i
?ifi(c)
)
.
A packed parse forest has a structure similar to a
chart of CFG parsing, and c corresponds to an edge
in the chart. This assumption corresponds to the
independence assumption in PCFG; that is, only
a nonterminal symbol of a mother is considered in
further processing by ignoring the structure of its
daughters. With this assumption, we can compute
the figures of merit (FOMs) of partial parse results.
This assumption restricts the possibility of feature
functions that represent non-local dependencies ex-
pressed in a parse result. Since unification-based
grammars can express semantic relations, such as
predicate-argument relations, in their structure, the
assumption unjustifiably restricts the flexibility of
probabilistic modeling. However, previous research
(Miyao et al, 2003; Clark and Curran, 2004; Kaplan
et al, 2004) showed that predicate-argument rela-
tions can be represented under the assumption of
feature locality. We thus assumed the locality of fea-
ture functions and exploited it for the efficient search
of probable parse results.
3 Techniques for efficient deep
parsing
Many of the techniques for improving the parsing
efficiency of deep linguistic analysis have been de-
veloped in the framework of lexicalized grammars
such as lexical functional grammar (LFG) (Bresnan,
1982), lexicalized tree adjoining grammar (LTAG)
(Shabes et al, 1988), HPSG (Pollard and Sag, 1994)
or combinatory categorial grammar (CCG) (Steed-
man, 2000). Most of them were developed for ex-
haustive parsing, i.e., producing all parse results that
are given by the grammar (Matsumoto et al, 1983;
Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer
et al, 1999; Malouf et al, 2000; Torisawa et al, 2000;
Oepen et al, 2002; Penn and Munteanu, 2003). The
strategy of exhaustive parsing has been widely used
in grammar development and in parameter training
for probabilistic models.
We tested three of these techniques.
Quick check Quick check filters out non-unifiable
feature structures (Malouf et al, 2000). Sup-
pose we have two non-unifiable feature struc-
tures. They are destructively unified by travers-
ing and modifying them, and then finally they
are found to be not unifiable in the middle of the
unification process. Quick check quickly judges
their unifiability by peeping the values of the
given paths. If one of the path values is not
unifiable, the two feature structures cannot be
unified because of the necessary condition of uni-
fication. In our implementation of quick check,
each edge had two types of arrays. One con-
tained the path values of the edge?s sign; we
call this the sign array. The other contained the
path values of the right daughter of a schema
such that its left daughter is unified with the
edge?s sign; we call this a schema array. When
we apply a schema to two edges, e1 and e2, the
schema array of e1 and the sign array of e2 are
quickly checked. If it fails, then quick check re-
turns a unification failure. If it succeeds, the
signs are unified with the schemata, and the re-
sult of unification is returned.
Large constituent inhibition (Kaplan et al,
2004) It is unlikely for a large medial edge to
contribute to the final parsing result if it spans
more than 20 words and is not adjacent to the
beginning or ending of the sentence. Large
constituent inhibition prevents the parser from
generating medial edges that span more than
some word length.
HPSG parsing with a CFG chunk parser A
hybrid of deep parsing and shallow parsing
was recently found to improve the efficiency
of deep parsing (Daum et al, 2003; Frank et
al., 2003; Frank, 2004). As a preprocessor, the
shallow parsing must be very fast and achieve
high precision but not high recall so that the
105
procedure Viterbi(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing
total parsing performance in terms of precision,
recall and speed is not degraded. Because there
is trade-off between speed and accuracy in
this approach, the total parsing performance
for large-scale corpora like the Penn treebank
should be measured. We introduce a CFG
chunk parser (Tsuruoka and Tsujii, 2005a) as a
preprocessor of HPSG parsing. Chunk parsers
meet the requirements for preprocessors; they
are very fast and have high precision. The
grammar for the chunk parser is automatically
extracted from the CFG treebank translated
from the HPSG treebank, which is generated
during grammar extraction from the Penn
treebank. The principal idea of using the chunk
parser is to use the bracket information, i.e.,
parse trees without non-terminal symbols, and
prevent the HPSG parser from generating edges
that cross brackets.
4 Beam thresholding for HPSG
parsing
4.1 Simple beam thresholding
Many algorithms for improving the efficiency of
PCFG parsing have been extensively investigated.
They include grammar compilation (Tomita, 1986;
Nederhof, 2000), the Viterbi algorithm, controlling
search strategies without FOM such as left-corner
parsing (Rosenkrantz and Lewis II, 1970) or head-
corner parsing (Kay, 1989; van Noord, 1997), and
with FOM such as the beam search, the best-first
search or A* search (Chitrao and Grishman, 1990;
Caraballo and Charniak, 1998; Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000; Roark, 2001; Klein
and Manning, 2003). The beam search and best-
first search algorithms significantly reduce the time
required for finding the best parse at the cost of los-
ing the guarantee of finding the correct parse.
The CYK algorithm, which is essentially a bottom-
up parser, is a natural choice for non-probabilistic
HPSG parsers. Many of the constraints are ex-
pressed as lexical entries in HPSG, and bottom-up
parsers can use those constraints to reduce the search
space in the early stages of parsing.
For PCFG, extending the CYK algorithm to out-
put the Viterbi parse is straightforward (Ney, 1991;
Jurafsky and Martin, 2000). The parser can effi-
ciently calculate the Viterbi parse by taking the max-
imum of the probabilities of the same nonterminal
symbol in each cell. With the probabilistic model
defined in Section 2, we can also define the Viterbi
search for unification-based grammars (Geman and
Johnson, 2002). Figure 2 shows the pseudo-code of
Viterbi algorithm. The pi[i, j] represents the set of
partial parse results that cover words wi+1, . . . , wj ,
and ?[i, j, F ] stores the maximum FOM of partial
parse result F at cell (i, j). Feature functions are
defined over lexical entries and results of rule appli-
cations, which correspond to conjunctive nodes in a
feature forest. The FOM of a newly created partial
parse, F , is computed by summing the values of ? of
the daughters and an additional FOM of F .
The Viterbi algorithm enables various pruning
techniques to be used for efficient parsing. Beam
thresholding (Goodman, 1997) is a simple and effec-
tive technique for pruning edges during parsing. In
each cell of the chart, the method keeps only a por-
tion of the edges which have higher FOMs compared
to the other edges in the same cell.
106
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?)
GlobalThresholding(n, ?)
procedure LocalThresholding(?, ?)
sort pi[i, j] according to ?[i, j, F ]
pi[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? pi[i, j]
if ?[i, j, F ] < ?max ? ?
pi[i, j]? pi[i, j]\{F}
procedure GlobalThresholding(n, ?)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
pi[i, j]? pi[i, j]\{F}
Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSG
parsing
107
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??
Figure 4: Pseudo-code of iterative beam thresholding
We tested three selection schemes for deciding
which edges to keep in each cell.
Local thresholding by number of edges Each
cell keeps the top ? edges based on their FOMs.
Local thresholding by beam width Each cell
keeps the edges whose FOM is greater than
?max ? ?, where ?max is the highest FOM
among the edges in the cell.
Global thresholding by beam width Each cell
keeps the edges whose global FOM is greater
than ?max??, where ?max is the highest global
FOM in the chart.
Figure 3 shows the pseudo-code of local beam
search, and global beam search algorithms for prob-
abilistic HPSG parsing. The code for local thresh-
olding is inserted at the end of the computation for
each cell. In Figure 3, pi[i, j]k denotes the k-th ele-
ment in sorted set pi[i, j]. We first take the first ?
elements that have higher FOMs and then remove
the elements with FOMs lower than ?max ? ?.
Global thresholding is also used for pruning edges,
and was originally proposed for CFG parsing (Good-
man, 1997). It prunes edges based on their global
FOM and the best global FOM in the chart. The
global FOM of an edge is defined as its FOM plus its
forward and backward FOMs, where the forward and
backward FOMs are rough estimations of the outside
FOM of the edge. The global thresholding is per-
formed immediately after each line of the CYK chart
is completed. The forward FOM is calculated first,
and then the backward FOM is calculated. Finally,
all edges with a global FOM lower than ?max ? ?
are pruned. Figure 3 gives further details of the al-
gorithm.
4.2 Iterative beam thresholding
We tested the iterative beam thresholding proposed
by Tsuruoka and Tsujii (2005b). We started the
parsing with a narrow beam. If the parser output
results, they were taken as the final parse results. If
the parser did not output any results, we widened the
Table 1: Abbreviations used in experimental results
num local beam thresholding by number
width local beam thresholding by width
global global beam thresholding by width
iterative iterative parsing with local beam
thresholding by number and width
chp parsing with CFG chunk parser
beam, and reran the parsing. We continued widen-
ing the beam until the parser output results or the
beam width reached some limit.
The pseudo-code is presented in Figure 4. It calls
the beam thresholding procedure shown in Figure 3
and increases parameters ? and ? until the parser
outputs results, i.e., pi[1, n] 6= ?.
Preserved iterative parsing Our implemented
CFG parser with iterative parsing cleared the
chart and edges at every iteration although the
parser regenerated the same edges using those
generated in the previous iteration. This is
because the computational cost of regenerating
edges is smaller than that of reusing edges to
which the rules have already been applied. For
HPSG parsing, the regenerating cost is even
greater than that for CFG parsing. In our
implementation of HPSG parsing, the chart
and edges were not cleared during the iterative
parsing. Instead, the pruned edges were marked
as thresholded ones. The parser counted the
number of iterations, and when edges were
generated, they were marked with the iteration
number, which we call the generation. If
edges were thresholded out, the generation was
replaced with the current iteration number plus
1. Suppose we have two edges, e1 and e2. The
grammar rules are applied iff both e1 and e2 are
not thresholded out, and the generation of e1
or e2 is equal to the current iteration number.
Figure 5 shows the pseudo-code of preserved
iterative parsing.
108
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?, iternum)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if gen[i, k, Fs] = iternum ? gen[k, j, Ft] = iternum
if F = r(Fs, Ft) has succeeded
gen[i, j, F ]? iternum
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?, iternum)
GlobalThresholding(n, ?, iternum)
procedure LocalThresholding(?, ?, iternum)
sort pi[i, j] according to ?[i, j, F ]
?[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? ?[i, j]
if ?[i, j, F ] < ?max ? ?
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure GlobalThresholding(n, ?, iternum)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? ?[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0; iternum = 0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?, iternum)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??; iternum? iternum + 1
Figure 5: Pseudo-code of preserved iterative parsing for HPSG
109
Table 2: Experimental results for development set (section 22) and test set (section 23)
Precision Recall F-score Avg. Time (ms) No. of failed sentences
development set 88.21% 87.32% 87.76% 360 12
test set 87.85% 86.85% 87.35% 360 15









        
  	 
  	      	  
         



	














        
  	 
  	      	  
         



	





Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)
and iterative parsing (iterative) (Right)

 
 
 


 
 
	 

 
 
               
                      Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 133?140,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chunk Parsing Revisited
Yoshimasa Tsuruoka
 
and Jun?ichi Tsujii  
 
CREST, JST (Japan Science and Technology Corporation)

Department of Computer Science, University of Tokyo

School of Informatics, University of Manchester

tsuruoka,tsujii  @is.s.u-tokyo.ac.jp
Abstract
Chunk parsing is conceptually appealing
but its performance has not been satis-
factory for practical use. In this pa-
per we show that chunk parsing can
perform significantly better than previ-
ously reported by using a simple sliding-
window method and maximum entropy
classifiers for phrase recognition in each
level of chunking. Experimental results
with the Penn Treebank corpus show that
our chunk parser can give high-precision
parsing outputs with very high speed (14
msec/sentence). We also present a pars-
ing method for searching the best parse by
considering the probabilities output by the
maximum entropy classifiers, and show
that the search method can further im-
prove the parsing accuracy.
1 Introduction
Chunk parsing (Tjong Kim Sang, 2001; Brants,
1999) is a simple parsing strategy both in imple-
mentation and concept. The parser first performs
chunking by identifying base phrases, and convert
the identified phrases to non-terminal symbols. The
parser again performs chunking on the updated se-
quence and convert the newly recognized phrases
into non-terminal symbols. The parser repeats this
procedure until there are no phrases to be chunked.
After finishing these chunking processes, we can
reconstruct the complete parse tree of the sentence
from the chunking results.
Although the conceptual simplicity of chunk pars-
ing is appealing, satisfactory performance for prac-
tical use has not yet been achieved with this pars-
ing strategy. Sang achieved an f-score of 80.49 on
the Penn Treebank by using the IOB tagging method
for each level of chunking (Tjong Kim Sang, 2001).
However, there is a very large gap between their per-
formance and that of widely-used practical parsers
(Charniak, 2000; Collins, 1999).
The performance of chunk parsing is heavily de-
pendent on the performance of phrase recognition in
each level of chunking. We show in this paper that
the chunk parsing strategy is indeed appealing in that
it can give considerably better performance than pre-
viously reported by using a different approach for
phrase recognition and that it enables us to build a
very fast parser that gives high-precision outputs.
This advantage could open up the possibility of
using full parsers for large-scale information extrac-
tion from the Web corpus and real-time information
extraction where the system needs to analyze the
documents provided by the users on run-time.
This paper is organized as follows. Section 2
introduces the overall chunk parsing strategy em-
ployed in this work. Section 3 describes the sliding-
window based method for identifying chunks. Two
filtering methods to reduce the computational cost
are presented in sections 4 and 5. Section 6 explains
the maximum entropy classifier and the feature set.
Section 7 describes methods for searching the best
parse. Experimental results on the Penn Treebank
corpus are given in Section 8. Section 10 offers
some concluding remarks.
133
Estimated  volume  was   a   light  2.4  million  ounces  .
VBN         NN    VBD DT  JJ    CD     CD NNS   .
QPNP
Figure 1: Chunk parsing, the 1st iteration.
volume          was   a   light    million       ounces .
NP             VBD DT  JJ          QP            NNS   .
NP
Figure 2: Chunk parsing, the 2nd iteration.
2 Chunk Parsing
For the overall strategy of chunk parsing, we fol-
low the method proposed by Sang (Tjong Kim Sang,
2001). Figures 1 to 4 show an example of chunk
parsing. In the first iteration, the chunker identifies
two base phrases, (NP Estimated volume) and (QP
2.4 million), and replaces each phrase with its non-
terminal symbol and head. The head word is identi-
fied by using the head-percolation table (Magerman,
1995). In the second iteration, the chunker identifies
(NP a light million ounces) and converts this phrase
into NP. This chunking procedure is repeated until
the whole sentence is chunked at the fourth itera-
tion, and the full parse tree is easily recovered from
the chunking history.
This parsing strategy converts the problem of full
parsing into smaller and simpler problems, namely,
chunking, where we only need to recognize flat
structures (base phrases). Sang used the IOB tag-
ging method proposed by Ramshow(Ramshaw and
Marcus, 1995) and memory-based learning for each
level of chunking and achieved an f-score of 80.49
on the Penn Treebank corpus.
3 Chunking with a sliding-window
approach
The performance of chunk parsing heavily depends
on the performance of each level of chunking. The
popular approach to this shallow parsing is to con-
vert the problem into a tagging task and use a variety
volume          was                    ounces          .
NP             VBD                    NP           .
VP
Figure 3: Chunk parsing, the 3rd iteration.
volume                           was                   .
NP                               VP                .
S
Figure 4: Chunk parsing, the 4th iteration.
of machine learning techniques that have been de-
veloped for sequence labeling problems such as Hid-
den Markov Models, sequential classification with
SVMs (Kudo and Matsumoto, 2001), and Condi-
tional Random Fields (Sha and Pereira, 2003).
One of our claims in this paper is that we should
not convert the chunking problem into a tagging
task. Instead, we use a classical sliding-window
method for chunking, where we consider all sub-
sequences as phrase candidates and classify them
with a machine learning algorithm. Suppose, for ex-
ample, we are about to perform chunking on the se-
quence in Figure 4.
NP-volume VBD-was .-.
We consider the following sub sequences as the
phrase candidates in this level of chunking.
1. (NP-volume) VBD-was .-.
2. NP-volume (VBD-was) .-.
3. NP-volume VBD-was (.-.)
4. (NP-volume VBD-was) .-.
5. NP-volume (VBD-was .-.)
6. (NP-volume VBD-was .-.)
The merit of taking the sliding window approach
is that we can make use of a richer set of features on
recognizing a phrase than in the sequential labeling
134
approach. We can define arbitrary features on the
target candidate (e.g. the whole sequence of non-
terminal symbols of the target) and the surrounding
context, which are, in general, not available in se-
quential labeling approaches.
We should mention here that there are some other
modeling methods for sequence labeling which al-
low us to define arbitrary features on the target
phrase. Semi-markov conditional random fields
(Semi-CRFs) are one of such modeling methods
(Sarawagi and Cohen, 2004). Semi-CRFs could
give better performance than the sliding-window
approach because they can incorporate features on
other phrase candidates on the same level of chunk-
ing. However, they require additional computational
resources for training and parsing, and the use of
Semi-CRFs is left for future work.
The biggest disadvantage of the sliding window
approach is the cost for training and parsing. Since
there are  

 	
 phrase candidates when the
length of the sequence is   , a naive application of
machine learning easily leads to prohibitive con-
sumption of memory and time.
In order to reduce the number of phrase candi-
dates to be considered by machine learning, we in-
troduce two filtering phases into training and pars-
ing. One is done by a rule dictionary. The other is
done by a naive Bayes classifier.
4 Filtering with the CFG Rule Dictionary
We use an idea that is similar to the method pro-
posed by Ratnaparkhi (Ratnaparkhi, 1996) for part-
of-speech tagging. They used a Tag Dictionary, with
which the tagger considers only the tag-word pairs
that appear in the training sentences as the candidate
tags.
A similar method can be used for reducing the
number of phrase candidates. We first construct a
rule dictionary consisting of all the CFG rules used
in the training data. In both training and parsing, we
filter out all the sub-sequences that do not match any
of the entry in the dictionary.
4.1 Normalization
The rules used in the training data do not cover all
the rules in unseen sentences. Therefore, if we take
a naive filtering method using the rule dictionary, we
Original Symbol Normalized Symbol
NNP, NNS, NNPS, PRP NN
RBR, RBS RB
JJR, JJS, PRP$ JJ
VBD, VBZ VBP
: ,
?, ? NULL
Table 1: Normalizing preterminals.
0
2000
4000
6000
8000
10000
12000
14000
16000
0 10000 20000 30000 40000
Si
ze
 o
f R
ul
e 
Di
ct
io
na
ry
Number of Sentences
Original
Normalized
Figure 5: Number of sentences vs the size of the rule
dictionary..
substantially lose recall in parsing unseen data.
To alleviate the problem of the coverage of rules,
we conduct normalization of the rules. We first con-
vert preterminal symbols into equivalent sets using
the conversion table provided in Table 1. This con-
version reduces the sparseness of the rules.
We further normalize the Right-Hand-Side (RHS)
of the rules with the following heuristics.

?X CC X? is converted to ?X?.

?X , X? is converted to ?X?.
Figure 5 shows the effectiveness of this normal-
ization method. The figure illustrates how the num-
ber of rules increases in the rule dictionary as we
add training sentences. Without the normalization,
the number of rules continues to grow rapidly even
when the entire training set is read. The normaliza-
tion methods reduce the growing rate, which con-
siderably alleviates the sparseness problem (i.e. the
problems of unknown rules).
135
5 Filtering with the Naive Bayes classifier
Although the use of the rule dictionary significantly
reduced the number of phrase candidates, we still
found it difficult to train the parser using the entire
training set when we used a rich set of features.
To further reduce the cost required for training
and parsing, we propose to use a naive Bayes classi-
fier for filtering the candidates. A naive Bayes clas-
sifier is simple and requires little storage and com-
putational cost.
We construct a binary naive Bayes classifier for
each phrase type using the entire training data. We
considered the following information as the features.
 The Right-Hand-Side (RHS) of the CFG rule
 The left-adjacent nonterminal symbol.
 The right-adjacent nonterminal symbol.
By assuming the conditional independence
among the features, we can compute the probability
for filtering as follows:
 
 	


 
	 

 


 
	


 
 

 
	 

 
 

 



 
 

 
 

 
 

 



where

is a binary output indicating whether the
candidate is a phrase of the target type or not,

is
the RHS of the CFG rule,

is the symbol on the
left, and

is the symbol on the right. We used
the Laplace smoothing method for computing each
probability. Note that the information about the re-
sult of the rule application, i.e., the LHS symbol, is
considered in this filtering scheme because different
naive Bayes classifiers are used for different LHS
symbols (phrase types).
Table 2 shows the filtering performance in train-
ing with sections 02-21 on the Penn Treebank. We
set the threshold probability for filtering to be 0.0001
for the experiments reported in this paper. The
naive Bayes classifiers effectively reduced the num-
ber of candidates with little positive samples that
were wrongly filtered out.
6 Phrase Recognition with a Maximum
Entropy Classifier
For the candidates which are not filtered out in the
above two phases, we perform classification with
maximum entropy classifiers (Berger et al, 1996).
We construct a binary classifier for each type of
phrases using the entire training set. The training
samples for maximum entropy consist of the phrase
candidates that have not been filtered out by the CFG
rule dictionary and the naive Bayes classifier.
One of the merits of using a maximum entropy
classifier is that we can obtain a probability from
the classifier in each decision. The probability of
each decision represents how likely the candidate is
a correct chunk. We accept a chunk only when the
probability is larger than the predefined threshold.
With this thresholding scheme, we can control the
trade-off between precision and recall by changing
the threshold value.
Regularization is important in maximum entropy
modeling to avoid overfitting to the training data.
For this purpose, we use the maximum entropy mod-
eling with inequality constraints (Kazama and Tsu-
jii, 2003). This modeling has one parameter to
tune as in Gaussian prior modeling. The parame-
ter is called the width factor. We set this parame-
ter to be 1.0 throughout the experiments. For nu-
merical optimization, we used the Limited-Memory
Variable-Metric (LMVM) algorithm (Benson and
More?, 2001).
6.1 Features
Table 3 lists the features used in phrase recognition
with the maximum entropy classifier. Information
about the adjacent non-terminal symbols is impor-
tant. We use unigrams, bigrams, and trigrams of the
adjacent symbols. Head information is also useful.
We use unigrams and bigrams of the neighboring
heads. The RHS of the CFG rule is also informa-
tive. We use the features on RHSs combined with
symbol features.
7 Searching the best parse
7.1 Deterministic parsing
The deterministic version of chunk parsing is
straight-forward. All we need to do is to repeat
chunking until there are no phrases to be chunked.
136
Symbol # candidates # remaining candidates # positives # false negative
ADJP 4,043,409 1,052,983 14,389 53
ADVP 3,459,616 1,159,351 19,765 78
NP 7,122,168 3,935,563 313,042 117
PP 3,889,302 1,181,250 94,568 126
S 3,184,827 1,627,243 95,305 99
VP 4,903,020 2,013,229 145,878 144
Table 2: Effectiveness of the naive Bayes filtering on some representative nonterminals.
Symbol Unigrams     ,    
Symbol Bigrams       ,   	       ,           ,         
Symbol Trigrams       	       ,               ,               ,             
Head Unigrams 
    , 
   
Head Bigrams 
  
   , 
	       , 
   
 
Symbol-Head Unigrams    
  ,    
  ,  

 

CFG Rule Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 465?472,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving the Scalability of Semi-Markov Conditional
Random Fields for Named Entity Recognition
Daisuke Okanohara? Yusuke Miyao? Yoshimasa Tsuruoka ? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
?SORST, Solution Oriented Research for Science and Technology
Honcho 4-1-8, Kawaguchi-shi, Saitama, Japan
{hillbig,yusuke,tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents techniques to apply
semi-CRFs to Named Entity Recognition
tasks with a tractable computational cost.
Our framework can handle an NER task
that has long named entities and many
labels which increase the computational
cost. To reduce the computational cost,
we propose two techniques: the first is the
use of feature forests, which enables us to
pack feature-equivalent states, and the sec-
ond is the introduction of a filtering pro-
cess which significantly reduces the num-
ber of candidate states. This framework
allows us to use a rich set of features ex-
tracted from the chunk-based representa-
tion that can capture informative charac-
teristics of entities. We also introduce a
simple trick to transfer information about
distant entities by embedding label infor-
mation into non-entity labels. Experimen-
tal results show that our model achieves an
F-score of 71.48% on the JNLPBA 2004
shared task without using any external re-
sources or post-processing techniques.
1 Introduction
The rapid increase of information in the biomedi-
cal domain has emphasized the need for automated
information extraction techniques. In this paper
we focus on the Named Entity Recognition (NER)
task, which is the first step in tackling more com-
plex tasks such as relation extraction and knowl-
edge mining.
Biomedical NER (Bio-NER) tasks are, in gen-
eral, more difficult than ones in the news domain.
For example, the best F-score in the shared task of
Bio-NER in COLING 2004 JNLPBA (Kim et al,
2004) was 72.55% (Zhou and Su, 2004) 1, whereas
the best performance at MUC-6, in which systems
tried to identify general named entities such as
person or organization names, was an accuracy of
95% (Sundheim, 1995).
Many of the previous studies of Bio-NER tasks
have been based on machine learning techniques
including Hidden Markov Models (HMMs) (Bikel
et al, 1997), the dictionary HMM model (Kou et
al., 2005) and Maximum Entropy Markov Mod-
els (MEMMs) (Finkel et al, 2004). Among these
methods, conditional random fields (CRFs) (Laf-
ferty et al, 2001) have achieved good results (Kim
et al, 2005; Settles, 2004), presumably because
they are free from the so-called label bias problem
by using a global normalization.
Sarawagi and Cohen (2004) have recently in-
troduced semi-Markov conditional random fields
(semi-CRFs). They are defined on semi-Markov
chains and attach labels to the subsequences of a
sentence, rather than to the tokens2. The semi-
Markov formulation allows one to easily construct
entity-level features. Since the features can cap-
ture all the characteristics of a subsequence, we
can use, for example, a dictionary feature which
measures the similarity between a candidate seg-
ment and the closest element in the dictionary.
Kou et al (2005) have recently showed that semi-
CRFs perform better than CRFs in the task of
recognition of protein entities.
The main difficulty of applying semi-CRFs to
Bio-NER lies in the computational cost at training
1Krauthammer (2004) reported that the inter-annotator
agreement rate of human experts was 77.6% for bio-NLP,
which suggests that the upper bound of the F-score in a Bio-
NER task may be around 80%.
2Assuming that non-entity words are placed in unit-length
segments.
465
Table 1: Length distribution of entities in the train-
ing set of the shared task in 2004 JNLPBA
Length # entity Ratio
1 21646 42.19
2 15442 30.10
3 7530 14.68
4 3505 6.83
5 1379 2.69
6 732 1.43
7 409 0.80
8 252 0.49
>8 406 0.79
total 51301 100.00
because the number of named entity classes tends
to be large, and the training data typically contain
many long entities, which makes it difficult to enu-
merate all the entity candidates in training. Table
1 shows the length distribution of entities in the
training set of the shared task in 2004 JNLPBA.
Formally, the computational cost of training semi-
CRFs is O(KLN), where L is the upper bound
length of entities, N is the length of sentence and
K is the size of label set. And that of training in
first order semi-CRFs is O(K2LN). The increase
of the cost is used to transfer non-adjacent entity
information.
To improve the scalability of semi-CRFs, we
propose two techniques: the first is to intro-
duce a filtering process that significantly re-
duces the number of candidate entities by using
a ?lightweight? classifier, and the second is to
use feature forest (Miyao and Tsujii, 2002), with
which we pack the feature equivalent states. These
enable us to construct semi-CRF models for the
tasks where entity names may be long and many
class-labels exist at the same time. We also present
an extended version of semi-CRFs in which we
can make use of information about a preceding
named entity in defining features within the frame-
work of first order semi-CRFs. Since the preced-
ing entity is not necessarily adjacent to the current
entity, we achieve this by embedding the informa-
tion on preceding labels for named entities into the
labels for non-named entities.
2 CRFs and Semi-CRFs
CRFs are undirected graphical models that encode
a conditional probability distribution using a given
set of features. CRFs allow both discriminative
training and bi-directional flow of probabilistic in-
formation along the sequence. In NER, we of-
ten use linear-chain CRFs, which define the con-
ditional probability of a state sequence y = y1, ...,
yn given the observed sequence x = x1,...,xn by:
p(y|x, ?) = 1
Z(x) exp(?
n
i=1?j?jfj(yi?1, yi, x, i)),
(1)
where fj(yi?1, yi,x, i) is a feature function and
Z(x) is the normalization factor over all the state
sequences for the sequence x. The model parame-
ters are a set of real-valued weights ? = {?j}, each
of which represents the weight of a feature. All the
feature functions are real-valued and can use adja-
cent label information.
Semi-CRFs are actually a restricted version of
order-L CRFs in which all the labels in a chunk are
the same. We follow the definitions in (Sarawagi
and Cohen, 2004). Let s = ?s1, ..., sp? denote a
segmentation of x, where a segment sj = ?tj , uj ,
yj? consists of a start position tj , an end position
uj , and a label yj . We assume that segments have a
positive length bounded above by the pre-defined
upper bound L (tj ? uj , uj ? tj + 1 ? L) and
completely cover the sequence x without overlap-
ping, that is, s satisfies t1 = 1, up = |x|, and
tj+1 = uj + 1 for j = 1, ..., p ? 1. Semi-CRFs
define a conditional probability of a state sequence
y given an observed sequence x by:
p(y|x, ?) = 1
Z(x) exp(?j?i?ifi(sj)), (2)
where fi(sj) := fi(yj?1, yj ,x, tj , uj) is a fea-
ture function and Z(x) is the normalization factor
as defined for CRFs. The inference problem for
semi-CRFs can be solved by using a semi-Markov
analog of the usual Viterbi algorithm. The com-
putational cost for semi-CRFs is O(KLN) where
L is the upper bound length of entities, N is the
length of sentence and K is the number of label
set. If we use previous label information, the cost
becomes O(K2LN).
3 Using Non-Local Information in
Semi-CRFs
In conventional CRFs and semi-CRFs, one can
only use the information on the adjacent previ-
ous label when defining the features on a certain
state or entity. In NER tasks, however, informa-
tion about a distant entity is often more useful than
466
O protein O O DNA
O protein O-protein O-protein DNA
Figure 1: Modification of ?O? (other labels) to
transfer information on a preceding named entity.
information about the previous state (Finkel et al,
2005). For example, consider the sentence ?... in-
cluding Sp1 and CP1.? where the correct labels of
?Sp1? and ?CP1? are both ?protein?. It would be
useful if the model could utilize the (non-adjacent)
information about ?Sp1? being ?protein? to clas-
sify ?CP1? as ?protein?. On the other hand, in-
formation about adjacent labels does not necessar-
ily provide useful information because, in many
cases, the previous label of a named entity is ?O?,
which indicates a non-named entity. For 98.0% of
the named entities in the training data of the shared
task in the 2004 JNLPBA, the label of the preced-
ing entity was ?O?.
In order to incorporate such non-local informa-
tion into semi-CRFs, we take a simple approach.
We divide the label of ?O? into ?O-protein? and
?O? so that they convey the information on the
preceding named entity. Figure 1 shows an ex-
ample of this conversion, in which the two labels
for the third and fourth states are converted from
?O? to ?O-protein?. When we define the fea-
tures for the fifth state, we can use the informa-
tion on the preceding entity ?protein? by look-
ing at the fourth state. Since this modification
changes only the label set, we can do this within
the framework of semi-CRF models. This idea is
originally proposed in (Peshkin and Pfeffer, 2003).
However, they used a dynamic Bayesian network
(DBNs) rather than a semi-CRF, and semi-CRFs
are likely to have significantly better performance
than DBNs.
In previous work, such non-local information
has usually been employed at a post-processing
stage. This is because the use of long distance
dependency violates the locality of the model and
prevents us from using dynamic programming
techniques in training and inference. Skip-CRFs
(Sutton and McCallum, 2004) are a direct imple-
mentation of long distance effects to the model.
However, they need to determine the structure
for propagating non-local information in advance.
In a recent study by Finkel et al, (2005), non-
local information is encoded using an indepen-
dence model, and the inference is performed by
Gibbs sampling, which enables us to use a state-
of-the-art factored model and carry out training ef-
ficiently, but inference still incurs a considerable
computational cost. Since our model handles lim-
ited type of non-local information, i.e. the label
of the preceding entity, the model can be solved
without approximation.
4 Reduction of Training/Inference Cost
The straightforward implementation of this mod-
eling in semi-CRFs often results in a prohibitive
computational cost.
In biomedical documents, there are quite a few
entity names which consist of many words (names
of 8 words in length are not rare). This makes
it difficult for us to use semi-CRFs for biomedi-
cal NER, because we have to set L to be eight or
larger, where L is the upper bound of the length of
possible chunks in semi-CRFs. Moreover, in or-
der to take into account the dependency between
named entities of different classes appearing in a
sentence, we need to incorporate multiple labels
into a single probabilistic model. For example, in
the shared task in COLING 2004 JNLPBA (Kim
et al, 2004) the number of labels is six (?pro-
tein?, ?DNA?, ?RNA?, ?cell line?, ?cell type?
and ?other?). This also increases the computa-
tional cost of a semi-CRF model.
To reduce the computational cost, we propose
two methods (see Figure 2). The first is employing
a filtering process using a lightweight classifier to
remove unnecessary state candidates beforehand
(Figure 2 (2)), and the second is the using the fea-
ture forest model (Miyao and Tsujii, 2002) (Fig-
ure 2 (3)), which employs dynamic programming
at training ?as much as possible?.
4.1 Filtering with a naive Bayes classifier
We introduce a filtering process to remove low
probability candidate states. This is the first step
of our NER system. After this filtering step, we
construct semi-CRFs on the remaining candidate
states using a feature forest. Therefore the aim of
this filtering is to reduce the number of candidate
states, without removing correct entities. This idea
467
(1) Enumerate
Candidate States
(2) Filtering by
Na?ve Bayes
(3) Construct feature forest
Training/
Inference
: other : entity
: other with preceding entity information
Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter
out low probability states by using a light-weight classifier, and represent them by using feature forest.
Table 2: Features used in the naive Bayes Classi-
fier for the entity candidate: ws, ws+1, ..., we. spi
is the result of shallow parsing at wi.
Feature Name Example of Features
Start/End Word ws, we
Inside Word ws, ws+1, ... , we
Context Word ws?1, we+1
Start/End SP sps, spe
Inside SP sps, sps+1, ..., spe
Context SP sps?1, spe+1
is similar to the method proposed by Tsuruoka and
Tsujii (2005) for chunk parsing, in which implau-
sible phrase candidates are removed beforehand.
We construct a binary naive Bayes classifier us-
ing the same training data as those for semi-CRFs.
In training and inference, we enumerate all possi-
ble chunks (the max length of a chunk is L as for
semi-CRFs) and then classify those into ?entity?
or ?other?. Table 2 lists the features used in the
naive Bayes classifier. This process can be per-
formed independently of semi-CRFs
Since the purpose of the filtering is to reduce the
computational cost, rather than to achieve a good
F-score by itself, we chose the threshold probabil-
ity of filtering so that the recall of filtering results
would be near 100 %.
4.2 Feature Forest
In estimating semi-CRFs, we can use an efficient
dynamic programming algorithm, which is simi-
lar to the forward-backward algorithm (Sarawagi
and Cohen, 2004). The proposal here is a more
general framework for estimating sequential con-
ditional random fields.
This framework is based on the feature forest
DNA
protein
Other
DNA
protein
Other
: or node (disjunctive node)
: and node (conjunctive node)
pos i i+1
??
Figure 3: Example of feature forest representation
of linear chain CRFs. Feature functions are as-
signed to ?and? nodes.
protein
O-protein
protein
u
j
=8 
prev-entity:protein
u
j
=  8
prev-entity: protein
packed
pos
87 9
Figure 4: Example of packed representation of
semi-CRFs. The states that have the same end po-
sition and prev-entity label are packed.
model, which was originally proposed for disam-
biguation models for parsing (Miyao and Tsujii,
2002). A feature forest model is a maximum en-
tropy model defined over feature forests, which are
abstract representations of an exponential number
of sequence/tree structures. A feature forest is
an ?and/or? graph: in Figure 3, circles represent
468
?and? nodes (conjunctive nodes), while boxes de-
note ?or? nodes (disjunctive nodes). Feature func-
tions are assigned to ?and? nodes. We can use
the information of the previous ?and? node for de-
signing the feature functions through the previous
?or? node. Each sequence in a feature forest is
obtained by choosing a conjunctive node for each
disjunctive node. For example, Figure 3 represents
3 ? 3 = 9 sequences, since each disjunctive node
has three candidates. It should be noted that fea-
ture forests can represent an exponential number
of sequences with a polynomial number of con-
junctive/disjunctive nodes.
One can estimate a maximum entropy model for
the whole sequence with dynamic programming
by representing the probabilistic events, i.e. se-
quence of named entity tags, by feature forests
(Miyao and Tsujii, 2002).
In the previous work (Lafferty et al, 2001;
Sarawagi and Cohen, 2004), ?or? nodes are con-
sidered implicitly in the dynamic programming
framework. In feature forest models, ?or? nodes
are packed when they have same conditions. For
example, ?or? nodes are packed when they have
same end positions and same labels in the first or-
der semi-CRFs,
In general, we can pack different ?or? nodes that
yield equivalent feature functions in the follow-
ing nodes. In other words, ?or? nodes are packed
when the following states use partial information
on the preceding states. Consider the task of tag-
ging entity and O-entity, where the latter tag is ac-
tually O tags that distinguish the preceding named
entity tags. When we simply apply first-order
semi-CRFs, we must distinguish states that have
different previous states. However, when we want
to distinguish only the preceding named entity tags
rather than the immediate previous states, feature
forests can represent these events more compactly
(Figure 4). We can implement this as follows. In
each ?or? node, we generate the following ?and?
nodes and their feature functions. Then we check
whether there exist ?or? node which has same con-
ditions by using its information about ?end posi-
tion? and ?previous entity?. If so, we connect the
?and? node to the corresponding ?or? node. If not,
we generate a new ?or? node and continue the pro-
cess.
Since the states with label O-entity and entity
are packed, the computational cost of training in
our model (First order semi-CRFs) becomes the
half of the original one.
5 Experiments
5.1 Experimental Setting
Our experiments were performed on the training
and evaluation set provided by the shared task in
COLING 2004 JNLPBA (Kim et al, 2004). The
training data used in this shared task came from
the GENIA version 3.02 corpus. In the task there
are five semantic labels: protein, DNA, RNA,
cell line and cell type. The training set consists
of 2000 abstracts from MEDLINE, and the evalu-
ation set consists of 404 abstracts. We divided the
original training set into 1800 abstracts and 200
abstracts, and the former was used as the training
data and the latter as the development data. For
semi-CRFs, we used amis3 for training the semi-
CRF with feature-forest. We used GENIA taggar4
for POS-tagging and shallow parsing.
We set L = 10 for training and evaluation when
we do not state L explicitly , where L is the upper
bound of the length of possible chunks in semi-
CRFs.
5.2 Features
Table 3 lists the features used in our semi-CRFs.
We describe the chunk-dependent features in de-
tail, which cannot be encoded in token-level fea-
tures.
?Whole chunk? is the normalized names at-
tached to a chunk, which performs like the closed
dictionary. ?Length? and ?Length and End-
Word? capture the tendency of the length of a
named entity. ?Count feature? captures the ten-
dency for named entities to appear repeatedly in
the same sentence.
?Preceding Entity and Prev Word? are fea-
tures that capture specifically words for conjunc-
tions such as ?and? or ?, (comma)?, e.g., for the
phrase ?OCIM1 and K562?, both ?OCIM1? and
?K562? are assigned cell line labels. Even if
the model can determine only that ?OCIM1? is a
cell line , this feature helps ?K562? to be assigned
the label cell line.
5.3 Results
We first evaluated the filtering performance. Table
4 shows the result of the filtering on the training
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
Note that the evaluation data are not used for training the GE-
NIA tagger.
469
Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words
at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is
the shallow parse result of wi.
Feature Name description of features
Non-Chunk Features
Word/POS/SC with Position BEGIN + ws, END + we, IN + ws+1, ..., IN + we?1, BEGIN + ps,...
Context Uni-gram/Bi-gram ws?1, we+1, ws?2 + ws?1, we+1 + we+2, ws?1 + we+1
Prefix/Suffix of Chunk 2/3-gram character prefix of ws, 2/3/4-gram character suffix of we
Orthography capitalization and word formation of ws...we
Chunk Features
Whole chunk ws + ws+1 + ... + we
Word/POS/SC End Bi-grams we?1 + we, pe?1 + pe, sce?1 + sce
Length, Length and End Word |s|, |s|+we
Count Feature the frequency of wsws+1..we in a sentence is greater than one
Preceding Entity Features
Preceding Entity /and Prev Word PrevState, PrevState + ws?1
Table 4: Filtering results using the naive Bayes
classifier. The number of entity candidates for the
training set was 4179662, and that of the develop-
ment set was 418628.
Training set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.984
1.0 ? 10?15 0.20 0.993
Development set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.985
1.0 ? 10?15 0.20 0.994
and evaluation data. The naive Bayes classifiers
effectively reduced the number of candidate states
with very few falsely removed correct entities.
We then examined the effect of filtering on the
final performance. In this experiment, we could
not examine the performance without filtering us-
ing all the training data, because training on all
the training data without filtering required much
larger memory resources (estimated to be about
80G Byte) than was possible for our experimental
setup. We thus compared the result of the recog-
nizers with and without filtering using only 2000
sentences as the training data. Table 5 shows the
result of the total system with different filtering
thresholds. The result indicates that the filtering
method achieved very well without decreasing the
overall performance.
We next evaluate the effect of filtering, chunk
information and non-local information on final
performance. Table 6 shows the performance re-
sult for the recognition task. L means the upper
bound of the length of possible chunks in semi-
CRFs. We note that we cannot examine the re-
sult of L = 10 without filtering because of the in-
tractable computational cost. The row ?w/o Chunk
Feature? shows the result of the system which does
not employ Chunk-Features in Table 3 at training
and inference. The row ?Preceding Entity? shows
the result of a system which uses Preceding En-
tity and Preceding Entity and Prev Word fea-
tures. The results indicate that the chunk features
contributed to the performance, and the filtering
process enables us to use full chunk representation
(L = 10). The results of McNemar?s test suggest
that the system with chunk features is significantly
better than the system without it (the p-value is
less than 1.0 < 10?4). The result of the preceding
entity information improves the performance. On
the other hand, the system with preceding infor-
mation is not significantly better than the system
without it5. Other non-local information may im-
prove performance with our framework and this is
a topic for future work.
Table 7 shows the result of the overall perfor-
mance in our best setting, which uses the infor-
mation about the preceding entity and 1.0?10?15
threshold probability for filtering. We note that the
result of our system is similar to those of other sys-
5The result of the classifier on development data is 74.64
(without preceding information) and 75.14 (with preceding
information).
470
Table 5: Performance with filtering on the development data. (< 1.0 ? 10?12) means the threshold
probability of the filtering is 1.0 ? 10?12.
Recall Precision F-score Memory Usage (MB) Training Time (s)
Small Training Data = 2000 sentences
Without filtering 65.77 72.80 69.10 4238 7463
Filtering (< 1.0 ? 10.0?12) 64.22 70.62 67.27 600 1080
Filtering (< 1.0 ? 10.0?15) 65.34 72.52 68.74 870 2154
All Training Data = 16713 sentences
Without filtering Not available Not available
Filtering (< 1.0 ? 10.0?12) 70.05 76.06 72.93 10444 14661
Filtering (< 1.0 ? 10.0?15) 72.09 78.47 75.14 15257 31636
Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks
in semi-CRFs.
Recall Precision F-score
L < 5 64.33 65.51 64.92
L = 10 + Filtering (< 1.0 ? 10.0?12) 70.87 68.33 69.58
L = 10 + Filtering (< 1.0 ? 10.0?15) 72.59 70.16 71.36
w/o Chunk Feature 70.53 69.92 70.22
+ Preceding Entity 72.65 70.35 71.48
tems in several respects, that is, the performance of
cell line is not good, and the performance of the
right boundary identification (78.91% in F-score)
is better than that of the left boundary identifica-
tion (75.19% in F-score).
Table 8 shows a comparison between our sys-
tem and other state-of-the-art systems. Our sys-
tem has achieved a comparable performance to
these systems and would be still improved by us-
ing external resources or conducting pre/post pro-
cessing. For example, Zhou et. al (2004) used
post processing, abbreviation resolution and exter-
nal dictionary, and reported that they improved F-
score by 3.1%, 2.1% and 1.2% respectively. Kim
et. al (2005) used the original GENIA corpus
to employ the information about other semantic
classes for identifying term boundaries. Finkel
et. al (2004) used gazetteers, web-querying, sur-
rounding abstracts, and frequency counts from
the BNC corpus. Settles (2004) used seman-
tic domain knowledge of 17 types of lexicon.
Since our approach and the use of external re-
sources/knowledge do not conflict but are com-
plementary, examining the combination of those
techniques should be an interesting research topic.
Table 7: Performance of our system on the evalu-
ation set
Class Recall Precision F-score
protein 77.74 68.92 73.07
DNA 69.03 70.16 69.59
RNA 69.49 67.21 68.33
cell type 65.33 82.19 72.80
cell line 57.60 53.14 55.28
overall 72.65 70.35 71.48
Table 8: Comparison with other systems
System Recall Precision F-score
Zhou et. al (2004) 75.99 69.42 72.55
Our system 72.65 70.35 71.48
Kim et.al (2005) 72.77 69.68 71.19
Finkel et. al (2004) 68.56 71.62 70.06
Settles (2004) 70.3 69.3 69.8
471
6 Conclusion
In this paper, we have proposed a single proba-
bilistic model that can capture important charac-
teristics of biomedical named entities. To over-
come the prohibitive computational cost, we have
presented an efficient training framework and a fil-
tering method which enabled us to apply first or-
der semi-CRF models to sentences having many
labels and entities with long names. Our results
showed that our filtering method works very well
without decreasing the overall performance. Our
system achieved an F-score of 71.48% without the
use of gazetteers, post-processing or external re-
sources. The performance of our system came
close to that of the current best performing system
which makes extensive use of external resources
and rule based post-processing.
The contribution of the non-local information
introduced by our method was not significant in
the experiments. However, other types of non-
local information have also been shown to be ef-
fective (Finkel et al, 2005) and we will examine
the effectiveness of other non-local information
which can be embedded into label information.
As the next stage of our research, we hope to ap-
ply our method to shallow parsing, in which seg-
ments tend to be long and non-local information is
important.
References
Daniel M. Bikel, Richard Schwartz, and Ralph
Weischedel. 1997. Nymble: a high-performance
learning name-finder. In Proc. of the Fifth Confer-
ence on Applied Natural Language Processing.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malv-
ina Nissim, Gail Sinclair, and Christopher Man-
ning. 2004. Exploiting context for biomedical en-
tity recognition: From syntax to the web. In Proc. of
JNLPBA-04.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proc. of ACL 2005, pages 363?370.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA-04, pages 70?75.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named
entity recognition using a hybrid method. In Proc. of
the Second International Joint Conference on Natu-
ral Language Processing (IJCNLP-05).
Zhenzhen Kou, William W. Cohen, and Robert F. Mur-
phy. 2005. High-recall protein entity recognition
using a dictionary. Bioinformatics 2005 21.
Micahel Krauthammer and Goran Nenadic. 2004.
Term identification in the biomedical literature. Jor-
nal of Biomedical Informatics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML 2001.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002.
Peshkin and Pfeffer. 2003. Bayesian information ex-
traction network. In IJCAI.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS 2004.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proc. of JNLPBA-04.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Sixth Message Understand-
ing Conference (MUC-6), pages 13?32.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML workshop on Sta-
tistical Relational Learning.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of the 9th Inter-
national Workshop on Parsing Technologies (IWPT
2005).
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proc. of JNLPBA-04.
472
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850?857,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Trimming CFG Parse Trees for Sentence Compression Using Machine
Learning Approaches
Yuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Jun?ichi Tsujii134
1Department of Computer Science, University of Tokyo
2Information Technology Center, University of Tokyo
3School of Informatics, University of Manchester
4SORST, JST
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
{unno, yusuke, tsujii}@is.s.u-tokyo.ac.jp
ninomi@r.dl.itc.u-tokyo.ac.jp
Abstract
Sentence compression is a task of creating
a short grammatical sentence by removing
extraneous words or phrases from an origi-
nal sentence while preserving its meaning.
Existing methods learn statistics on trim-
ming context-free grammar (CFG) rules.
However, these methods sometimes elim-
inate the original meaning by incorrectly
removing important parts of sentences, be-
cause trimming probabilities only depend
on parents? and daughters? non-terminals
in applied CFG rules. We apply a maxi-
mum entropy model to the above method.
Our method can easily include various
features, for example, other parts of a
parse tree or words the sentences contain.
We evaluated the method using manually
compressed sentences and human judg-
ments. We found that our method pro-
duced more grammatical and informative
compressed sentences than other methods.
1 Introduction
In most automatic summarization approaches, text
is summarized by extracting sentences from a
given document without modifying the sentences
themselves. Although these methods have been
significantly improved to extract good sentences
as summaries, they are not intended to shorten sen-
tences; i.e., the output often has redundant words
or phrases. These methods cannot be used to
make a shorter sentence from an input sentence or
for other applications such as generating headline
news (Dorr et al, 2003) or messages for the small
screens of mobile devices. We need to compress
sentences to obtain short and useful summaries.
This task is called sentence compression.
While several methods have been proposed for
sentence compression (Witbrock and Mittal, 1999;
Jing and McKeown, 1999; Vandeghinste and Pan,
2004), this paper focuses on Knight and Marcu?s
noisy-channel model (Knight and Marcu, 2000)
and presents an extension of their method. They
developed a probabilistic model for trimming a
CFG parse tree of an input sentence. Their
method drops words of input sentences but does
not change their order or change the words. They
use a parallel corpus that contains pairs of origi-
nal and compressed sentences. The method makes
CFG parse trees of both original and compressed
sentences and learns trimming probabilities from
these pairs. Although their method is concise and
well-defined, its accuracy is still unsatisfactory.
Their method has two problems. One is that prob-
abilities are calculated only from the frequencies
of applied CFG rules, and other characteristics like
whether the phrase includes negative words cannot
be introduced. The other problem is that the parse
trees of original and compressed sentences some-
times do not correspond.
To solve the former problem, we apply a maxi-
mum entropy model to Knight and Marcu?s model
to introduce machine learning features that are de-
fined not only for CFG rules but also for other
characteristics in a parse tree, such as the depth
from the root node or words it contains. To solve
the latter problem, we introduce a novel matching
method, the bottom-up method, to learn compli-
cated relations of two unmatched trees.
We evaluated each algorithm using the Ziff-
Davis corpus, which has long and short sentence
pairs. We compared our method with Knight and
Marcu?s method in terms of F -measures, bigram
F -measures, BLEU scores and human judgments.
850
2 Background
2.1 The Noisy-Channel Model for Sentence
Compression
Knight and Marcu proposed a sentence compres-
sion method using a noisy-channel model (Knight
and Marcu, 2000). This model assumes that a long
sentence was originally a short one and that the
longer sentence was generated because some un-
necessary words were added. Given a long sen-
tence l, it finds a short sentence s that maximizes
P (s|l). This is equivalent to finding the s that
maximizes P (s) ? P (l|s) in Bayes? Rule.
The expression P (s) is the source model, which
gives the probability that s is the original short
string. When s is ungrammatical, P (s) becomes
small. The expression P (l|s) is the channel
model, which gives the probability that s is ex-
panded to l. When s does not include important
words of l, P (l|s) has a low value.
In the Knight and Marcu?s model, a proba-
bilistic context-free grammar (PCFG) score and a
word-bigram score are incorporated as the source
model. To estimate the channel model, Knight
and Marcu used the Ziff-Davis parallel corpus,
which contains long sentences and corresponding
short sentences compressed by humans. Note that
each compressed sentence is a subsequence of the
corresponding original sentence. They first parse
both the original and compressed sentences using
a CFG parser to create parse trees. When two
nodes of the original and compressed trees have
the same non-terminals, and the daughter nodes of
the compressed tree are a subsequence of the orig-
inal tree, they count the node pair as a joint event.
For example, in Figure 1, the original parse tree
contains a rule rl = (B ? D E F ), and the com-pressed parse tree contains rs = (B ? D F ).They assume that rs was expanded into rl, andcount the node pairs as joint events. The expan-
sion probability of two rules is given by:
Pexpand (rl|rs) =
count(joint(rl, rs))
count(rs)
.
Finally, new subtrees grow from new daugh-
ter nodes in each expanded node. In Figure 1,
(E (G g) (H h)) grows from E. The PCFG
scores, Pcfg , of these subtrees are calculated.Then, each probability is assumed to be indepen-
dent of the others, and the channel model, P (l|s),
is calculated as the product of all expansion prob-
abilities of joint events and PCFG scores of new
A
B C
E FD
d
g h
f
c
A
B C
FD
d f
c
G H
Figure 1: Examples of original and compressed
parse trees.
subtrees:
P (l|s) =
?
(rl,rs)?R
Pexpand (rl|rs) ?
?
r?R?
Pcfg(r),
where R is the set of rule pairs, and R? is the set
of generation rules in new subtrees.
To compress an input sentence, they create a
tree with the highest score of all possible trees.
They pack all possible trees in a shared-forest
structure (Langkilde, 2000). The forest structure
is represented by an AND-OR tree, and it con-
tains many tree structures. The forest represen-
tation saves memory and makes calculation faster
because the trees share sub structures, and this can
reduce the total number of calculations.
They normalize each log probability using the
length of the compressed sentence; that is, they di-
vide the log probability by the length of the com-
pressed sentence.
Turner and Charniak (Turner and Charniak,
2005) added some special rules and applied this
method to unsupervised learning to overcome the
lack of training data. However their model also
has the same problem. McDonald (McDonald,
2006) independently proposed a new machine
learning approach. He does not trim input parse
trees but uses rich features about syntactic trees
and improved performance.
2.2 Maximum Entropy Model
The maximum entropy model (Berger et al, 1996)
estimates a probability distribution from training
data. The model creates the most ?uniform? distri-
bution within the constraints given by users. The
distribution with the maximum entropy is consid-
ered the most uniform.
Given two finite sets of event variables, X and
Y , we estimate their joint probability distribution,
P (x, y). An output, y (? Y), is produced, and
851
contextual information, x (? X ), is observed. To
represent whether the event (x, y) satisfies a cer-
tain feature, we introduce a feature function. A
feature function fi returns 1 iff the event (x, y) sat-isfies the feature i and returns 0 otherwise.
Given training data {(x1, y1), ? ? ? , (xn, yn)},we assume that the expectation of fi on the dis-tribution of the model conforms to that on the em-
pirical probability distribution P? (x, y). We select
the probability distribution that satisfies these con-
straints of all feature functions and maximizes its
entropy, H(P ) = ??x,y P (x, y) log (P (x, y)).
3 Methods
3.1 Maximum Entropy Model for Sentence
Compression
We describe a maximum entropy method as a
natural extension of Knight and Marcu?s noisy-
channel model (Knight and Marcu, 2000). Knight
and Marcu?s method uses only mother and daugh-
ter local relations in CFG parse trees. Therefore,
it sometimes eliminates the meanings of the origi-
nal sentences. For example, their method cannot
distinguish ?never? and ?always? because these
two adverbs are assigned the same non-terminals
in parse trees. However, if ?never? is removed
from a sentence, the meaning of the sentence com-
pletely changes. Turner and Charniak (Turner and
Charniak, 2005) revised and improved Knight and
Marcu?s algorithm; however, their algorithm also
uses only mother and daughter relations and has
the same problem. We use other information as
feature functions of the maximum entropy model,
and this model can deal with many features more
appropriately than using simple frequency.
Suppose that we trim a node in the original full
parse tree. For example, suppose we have a mother
node A and daughter nodes (B C D) that are de-
rived using a CFG rule. We must leave at least one
non-terminal in the daughter nodes. The trim can-
didates of this rule are the members of the set of
subsequences, Y , of (B C D), or the seven non-
terminal sequences below:
Y = {B,C,D,BC,BD,CD,BCD}.
For each y (? Y), such as (B C), the trimming
probability, P (y|Y) = Ptrim(A ? B C|A ?
B C D), is calculated by using the maximum en-
tropy model. We assume that these joint events are
independent of each other and calculate the proba-
bility that an original sentence, l, is compressed to
Description
1 the mother node
2 the current node
3 the daughter node sequence in the original sentence
and which daughters are removed
4 the daughter node sequence in the compressed sen-
tence
5 the number of daughter nodes
6 the depth from the root
7 the daughter non-terminals that are removed
8 the daughter terminals that are removed
9 whether the daughters are ?negative adverbs?, and
removed
10 tri-gram of daughter nodes
11 only one daughter exists, and its non-terminal is the
same as that of the current node
12 only one daughter exists, and its non-terminal is the
same as that of the mother node
13 how many daughter nodes are removed
14 the number of terminals the current node contains
15 whether the head daughter is removed
16 the left-most and the right-most daughters
17 the left and the right siblings
Table 1: Features for maximum entropy model.
s as the product of all trimming probabilities, like
in Knight and Marcu?s method.
P (s|l) =
?
(rs,rl)?R
Ptrim(rs|rl),
where R is the set of compressed and original rule
pairs in joint events. Note that our model does not
use Bayes? Rule or any language models.
For example, in Figure 1, the trimming proba-
bility is calculated as below:
P (s|l) = Ptrim(A ? B C|A ? B C)
?Ptrim(B ? D F |B ? D E F ).
To represent all summary candidates, we cre-
ate a compression forest as Knight and Marcu did.
We select the tree assigned the highest probability
from the forest.
Features in the maximum entropy model are de-
fined for a tree node and its surroundings. When
we process one node, or one non-terminal x, we
call it the current node. We focus on not only x
and its daughter nodes, but its mother node, its
sibling nodes, terminals of its subtree and so on.
The features we used are listed in Table 1.
Knight and Marcu divided the log probabilities
by the length of the summary. We extend this idea
so that we can change the output length flexibly.
We introduce a length parameter, ?, and define a
score S? as S?(s) = length(s)? log P (s|l), where
l is an input sentence to be shortened, and s is a
852
summary candidate. Because log P (s|l) is nega-
tive, short sentences obtain a high score for large
?, and long ones get a low score. The parameter
? can be negative or positive, and we can use it to
control the average length of outputs.
3.2 Bottom-Up Method
As explained in Section 2.1, in Knight and
Marcu?s method, both original and compressed
sentences are parsed, and correspondences of CFG
rules are identified. However, when the daugh-
ter nodes of a compressed rule are not a subse-
quence of the daughter nodes in the original one,
the method cannot learn this joint event. A com-
plex sentence is a typical example. A complex
sentence is a sentence that includes another sen-
tence as a part. An example of a parse tree of a
complex sentence and its compressed version is
shown in Figure 2. When we extract joint events
from these two trees, we cannot match the two
root nodes because the sequence of the daughter
nodes of the root node of the compressed parse
tree, (NP ADVP VP .), is not a subsequence
of the daughter nodes of the original parse tree,
(S , NP VP .). Turner and Charniak (Turner and
Charniak, 2005) solve this problem by appending
special rules that are applied when a mother node
and its daughter node have the same label. How-
ever, there are several types of such problems like
Figure 2. We need to extract these structures from
a training corpus.
We propose a bottom-up method to solve the
problem explained above. In our method, only
original sentences are parsed, and the parse trees
of compressed sentences are extracted from the
original parse trees. An example of this method
is shown in Figure 3. The original sentence is ?d
g h f c?, and its compressed sentence is ?d g c?.
First, each terminal in the parse tree of the original
sentence is marked if it exists in the compressed
sentence. In the figure, the marked terminals are
represented by circles. Second, each non-terminal
in the original parse tree is marked if it has at least
one marked terminal in its sub-trees. These are
represented as bold boxes in the figure. If non-
terminals contain marked non-terminals in their
sub-trees, these non-terminals are also marked re-
cursively. These marked non-terminals and termi-
nals compose a tree structure like that on the right-
hand side in the figure. These non-terminals rep-
resent joint events at each node.
S
S ,
,
NP VP
I said
.
.
S
.
.
NP VPADVP
I never think soNP VPADVP
I never think so
top top
Figure 2: Example of parse tree pair that cannot
be matched.
A
B C
E FD
G H
h
f
A
B C
ED
d
g
c
d
g
c
G
Figure 3: Example of bottom-up method.
Note that this ?tree? is not guaranteed to be
a grammatical ?parse tree? by the CFG gram-
mar. For example, from the tree of Figure 2,
(S (S ? ? ? ) (, , ) (NP I) (VP said) (. .)), a new
tree, (S (S ? ? ? ) (. .)), is extracted. However, the
rule (S ? S .) is ungrammatical.
4 Experiment
4.1 Evaluation Method
We evaluated each sentence compression method
using word F -measures, bigram F -measures, and
BLEU scores (Papineni et al, 2002). BLEU scores
are usually used for evaluating machine transla-
tion quality. A BLEU score is defined as the
weighted geometric average of n-gram precisions
with length penalties. We used from unigram to
4-gram precisions and uniform weights for the
BLEU scores.
ROUGE (Lin, 2004) is a set of recall-based cri-
teria that is mainly used for evaluating summa-
rization tasks. ROUGE-N uses average N-gram re-
call, and ROUGE-1 is word recall. ROUGE-L uses
the length of the longest common subsequence
(LCS) of the original and summarized sentences.
In our model, the length of the LCS is equal to
the number of common words, and ROUGE-L is
equal to the unigram F -measure because words
are not rearranged. ROUGE-L and ROUGE-1 are
supposed to be appropriate for the headline gener-
853
ation task (Lin, 2004). This is not our task, but it
is the most similar task in his paper.
We also evaluated the methods using human
judgments. The evaluator is not the author but not
a native English speaker. The judgment used the
same criteria as those in Knight and Marcu?s meth-
ods. We performed two experiments. In the first
experiment, evaluators scored from 1 to 5 points
the grammaticality of the compressed sentence. In
the second one, they scored from 1 to 5 points
how well the compressed sentence contained the
important words of the original one.
We used the parallel corpus used in Ref. (Knight
and Marcu, 2000). This corpus consists of sen-
tence pairs extracted automatically from the Ziff-
Davis corpus, a set of newspaper articles about
computer products. This corpus has 1087 sentence
pairs. Thirty-two of these sentences were used for
the human judgments in Knight and Marcu?s ex-
periment, and the same sentences were used for
our human judgments. The rest of the sentences
were randomly shuffled, and 527 sentence pairs
were used as a training corpus, 263 pairs as a de-
velopment corpus, and 264 pairs as a test corpus.
To parse these corpora, we used Charniak and
Johnson?s parser (Charniak and Johnson, 2005).
4.2 Settings of Two Experiments
We experimented with/without goal sentence
length for summaries.
In the first experiment, the system was given
only a sentence and no sentence length informa-
tion. The sentence compression problem without
the length information is a general task, but evalu-
ating it is difficult because the correct length of a
summary is not generally defined even by humans.
The following example shows this.
Original:?A font, on the other hand, is a subcate-
gory of a typeface, such as Helvetica Bold or Hel-
vetica Medium.?
Human: ?A font is a subcategory of a typeface,
such as Helvetica Bold.?
System: ?A font is a subcategory of a typeface.?
The ?such as? phrase is removed in this sys-
tem output, but it is not removed in the human
summary. Neither result is wrong, but in such
situations, the evaluation score of the system de-
creases. This is because the compression rate of
each algorithm is different, and evaluation scores
are affected by the lengths of system outputs. For
this reason, results with different lengths cannot be
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
F-
m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 4: F -measures and compression ratios.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
ig
ra
m
 F
-m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 5: Bigram F -measures and compression
ratios.
compared easily. We therefore examined the rela-
tions between the average compression ratios and
evaluation scores for all methods by changing the
system summary length with the different length
parameter ? introduced in Section 3.1.
In the second experiment, the system was given
a sentence and the length for the compressed sen-
tence. We compressed each input sentence to the
length of the sentence in its goal summary. This
sentence compression problem is easier than that
in which the system can generate sentences of any
length. We selected the highest-scored sentence
from the sentences of length l. Note that the re-
calls, precisions and F-measures have the same
scores in this setting.
4.3 Results of Experiments
The results of the experiment without the sen-
tence length information are shown in Figure 4,
5 and 6. Noisy-channel indicates the results of the
noisy-channel model, ME indicates the results of
the maximum-entropy method, and ME + bottom-
up indicates the results of the maximum-entropy
854
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 s
co
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 6: BLEU scores and compression ratios.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Noisy-channel ME ME+bottom-up
F-measure
bigram-F-measure
BLEU
Figure 7: Results of experiments with length in-
formation.
method with the bottom-up method. We used the
length parameter, ?, introduced in Section 3.1, and
obtained a set of summaries with different aver-
age lengths. We plotted the compression ratios
and three scores in the figures. In these figures,
a compression ratio is the ratio of the total num-
ber of words in compressed sentences to the total
number of words in the original sentences.
In these figures, our maximum entropy meth-
ods obtained higher scores than the noisy-channel
model at all compression ratios. The maximum
entropy method with the bottom-up method obtain
the highest scores on these three measures.
The results of the experiment with the sentence
length information are shown in Figure 7. In this
experiment, the scores of the maximum entropy
methods were higher than the scores of the noisy-
channel model. The maximum entropy method
with the bottom-up method achieved the highest
scores on each measure.
The results of the human judgments are shown
in Table 2. In this experiment, each length of out-
put is same as the length of goal sentence. The
Method Grammar Importance
Human 4.94 4.31
Noisy-channel 3.81 3.38
ME 3.88 3.38
ME + bottom-up 4.22 4.06
Table 2: Results of human judgments.
maximum entropy with the bottom-up method ob-
tained the highest scores of the three methods. We
did t-tests (5% significance). Between the noisy-
channel model and the maximum entropy with the
bottom-up method, importance is significantly dif-
ferent but grammaticality is not. Between the hu-
man and the maximum entropy with the bottom-
up method, grammaticality is significantly differ-
ent but importance is not. There are no significant
differences between the noisy-channel model and
the maximum entropy model.
4.3.1 Problem of Negative Adverbs
One problem of the noisy-channel model is that
it cannot distinguish the meanings of removed
words. That is, it sometimes removes semantically
important words, such as ?not? and ?never?, be-
cause the expansion probability depends only on
non-terminals of parent and daughter nodes.
For example, our test corpus includes 15 sen-
tences that contain ?not?. The noisy-channel
model removed six ?not?s, and the meanings of
the sentences were reversed. However, the two
maximum entropy methods removed only one
?not? because they have ?negative adverb? as a
feature in their models. The first example in Ta-
ble 3 shows one of these sentences. In this exam-
ple, only Noisy-channel removed ?not?.
4.3.2 Effect of Bottom-Up Method
Our bottom-up method achieved the highest
accuracy, in terms of F -measures, bigram F -
measures, BLEU scores and human judgments.
The results were fairly good, especially when it
summarized complex sentences, which have sen-
tences as parts. The second example in Table 3 is
a typical complex sentence. In this example, only
ME + bottom-up correctly remove ?he said?.
Most of the complex sentences were correctly
compressed by the bottom-up method, but a few
sentences like the third example in Table 3 were
not. In this example, the original sentence was
parsed as shown in Figure 8 (left). If this sen-
tence is compressed to the human output, its parse
tree has to be like that in Figure 8 (middle) using
855
Original a file or application ?? alias ??
similar in effect to the ms-dos path
statement provides a visible icon in
folders where an aliased application
does not actually reside .
Human a file or application alias provides
a visible icon in folders where an
aliased application does not actually
reside .
Noisy-
channel
a similar in effect to ms-dos
statement provides a visible icon in
folders where an aliased application
does reside .
ME a or application alias statement
provides a visible icon in folders
where an aliased application does not
actually reside .
ME +
bottom-up
a file or application statement
provides a visible icon in folders
where an aliased application does not
actually reside .
Original the user can then abort the
transmission , he said .
Human the user can then abort the
transmission .
Noisy-
channel
the user can abort the transmission
said .
ME the user can abort the transmission
said .
ME +
bottom-up
the user can then abort the
transmission .
Original it is likely that both companies will
work on integrating multimedia with
database technologies .
Human both companies will work on
integrating multimedia with database
technologies .
Noisy-
channel
it is likely that both companies will
work on integrating .
ME it is likely that both companies will
work on integrating .
ME +
bottom-up
it is will work on integrating
multimedia with database technologies
.
Table 3: Examples of compressed sentences.
our method. When a parse tree is too long from
the root to the leaves like this, some nodes are
trimmed but others are not because we assume that
each trimming probability is independent. The
compressed sentence is ungrammatical, as in the
third example in Table 3.
We have to constrain such ungrammatical sen-
tences or introduce another rule that reconstructs
a short tree as in Figure 8 (right). That is, we in-
troduce a new transformation rule that compresses
(A1 (B (C (A2 ? ? ? )))) to (A2 ? ? ? ).
4.4 Comparison with Original Results
We compared our results with Knight and Marcu?s
original results. They implemented two methods:
one is the noisy-channel model and the other is
a decision-based model. Each model produced
32 compressed sentences, and we calculated F -
measures, bigram F -measures, and BLEU scores.
We used the length parameter ? = 0.5 for the
maximum-entropy method and ? = ?0.25 for
S
VP
is ADJP SBAR
likely that S
both companies 
will ...
S
It
both companies 
will ...
S
VP
SBAR
S
both companies 
will ...
(left) (middle) (right)
Figure 8: Parse trees of complicated complex sen-
tences.
Method Comp. F-measure bigram F-
measure
BLEU
Noisy-
channel
70.19% 68.80 55.96 44.54
Decision-
based
57.26% 71.25 61.93 58.21
ME 66.51% 73.10 62.86 53.51
ME +
bottom-up
58.14% 78.58 70.30 65.26
Human 53.59%
Table 4: Comparison with original results.
the maximum-entropy method with the bottom-up
method. These two values were determined using
experiments on the development set, which did not
contain the 32 test sentences.
The results are shown in Table 4. Noisy-channel
indicates the results of Knight and Marcu?s noisy-
channel model, and Decision-based indicates the
results of Knight and Marcu?s decision-based
model. Comp. indicates the compression ratio of
each result. Our two methods achieved higher ac-
curacy than the noisy-channel model. The results
of the decision-based model and our maximum-
entropy method were not significantly different.
Our maximum-entropy method with the bottom-
up method achieved the highest accuracy.
4.5 Corpus Size and Output Accuracy
In general, using more training data improves the
accuracy of outputs and using less data results in
low accuracy. Our experiment has the problem
that the training corpus was small. To study the re-
lation between training corpus size and accuracy,
we experimented using different training corpus
sizes and compared accuracy of the output.
Figure 9 shows the relations between training
corpus size and three scores, F -measures, bigram
F -measures and BLEU scores, when we used the
maximum entropy method with the bottom-up
method. This graph suggests that the accuracy in-
856
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  100  200  300  400  500  600  700  800
Sc
or
e
Size of training corpus
BLEU score
F-measure
bigram F-measure
Figure 9: Relation between training corpus size
and evaluation score.
creases when the corpus size is increased. Over
about 600 sentences, the increase becomes slower.
The graph shows that the training corpus was
large enough for this study. However, if we intro-
duced other specific features, such as lexical fea-
tures, a larger corpus would be required.
5 Conclusion
We presented a maximum entropy model to ex-
tend the sentence compression methods described
by Knight and Marcu (Knight and Marcu, 2000).
Our proposals are two-fold. First, our maxi-
mum entropy model allows us to incorporate var-
ious characteristics, such as a mother node or the
depth from a root node, into a probabilistic model
for determining which part of an input sentence
is removed. Second, our bottom-up method of
matching original and compressed parse trees can
match tree structures that cannot be matched using
Knight and Marcu?s method.
The experimental results show that our maxi-
mum entropy method improved the accuracy of
sentence compression as determined by three eval-
uation criteria: F -measures, bigram F -measures
and BLEU scores. Using our bottom-up method
further improved accuracy and produced short
summaries that could not be produced by previ-
ous methods. However, we need to modify this
model to appropriately process more complicated
sentences because some sentences were not cor-
rectly summarized. Human judgments showed
that the maximum entropy model with the bottom-
up method provided more grammatical and more
informative summaries than other methods.
Though our training corpus was small, our ex-
periments demonstrated that the data was suffi-
cient. To improve our approaches, we can intro-
duce more feature functions, especially more se-
mantic or lexical features, and to deal with these
features, we need a larger corpus.
Acknowledgements
We would like to thank Prof. Kevin Knight and
Prof. Daniel Marcu for providing their parallel
corpus and the experimental results.
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.1996. A Maximum Entropy Approach to NaturalLanguage Processing. Computational Linguistics,22(1):39?71.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.
In Proc. of ACL?05, pages 173?180.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge Trim-
mer: A Parse-and-Trim Approach to Headline Gen-eration. In Proc. of DUC 2003, pages 1?8.
H. Jing and K. R. McKeown. 1999. The decomposi-
tion of human-written summary sentences. In Proc.of SIGIR?99, pages 129?136.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-marization - Step One: Sentence Compression. InProc. of AAAI/IAAI?00, pages 703?710.
I. Langkilde. 2000. Forest-Based Statistical SentenceGeneration. In Proc. of NAACL?00, pages 170?177.
C. Lin. 2004. ROUGE: A Package for AutomaticEvaluation of Summaries. In Text SummarizationBranches Out: Proc. of ACL?04 Workshop, pages
74?81.
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Evidence. In Proc. ofEACL?06.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a Method for Automatic Evaluation of Ma-chine Translation. In Proc. of ACL?02, pages 311?318.
J. Turner and E. Charniak. 2005. Supervised and Un-supervised Learning for Sentence Compression. InProc. of ACL?05, pages 290?297.
V. Vandeghinste and Y. Pan. 2004. Sentence Com-pression for Automated Subtitling: A Hybrid Ap-
proach. In Text Summarization Branches Out: Proc.of ACL?04 Workshop, pages 89?95.
M. J. Witbrock and V. O. Mittal. 1999. Ultra-Summarization: A Statistical Approach to Generat-ing Highly Condensed Non-Extractive Summaries.
In Proc. of SIGIR?99, pages 315?316.
857
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
Yoshimasa Tsuruoka
School of Informatics
University of Manchester
Yusuke Miyao
Department of Computer Science
University of Tokyo
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
1 Introduction
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ?supertags,? which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
155
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
156
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al, 1996).
The probability that a parse result T is assigned to
a given sentence w = ?w1, . . . , wn? is
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T , and Zw is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(?u) when a characteristic cor-
responding to fu appears in parse result T . The
model parameters, ?u, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T |w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T |w). Miyao and Tsujii
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
(Previous probabilistic HPSG)
phpsg?(T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in T
and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbinary and funary
are defined for constituents at binary and unary
branches, froot is a feature template set for the
root nodes of parse trees, and flex is a feature tem-
plate set for calculating the preliminary probabilis-
tic model. An example of features applied to the
parse tree for the sentence ?Spring has come? is
shown in Figure 2.
157
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Features.
3 Extremely lexicalized probabilistic
models
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = ?w1, . . . , wn?, and the probabilistic model
of lexical entry selection, p(li ? L|w, i), the first
model is formally defined as follows:
(Model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
(Model 2)
pmodel2(T |w) =
1
Zmodel2 pmodel1(T |w) exp
?
??
?
u
(fu?froot)
?ufu(T )
?
??
Zmodel2 =
?
T ?
pmodel1(T ?|w) exp
?
??
?
u
(fu?froot)
?ufu(T ?)
?
?? ,
where Zmodel2 is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
(Model 3)
pmodel3(T |w) =
1
Zmodel3 pmodel1(T |w) exp
(?
u
?ufu(T )
)
Zmodel3 =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic Model of Lexical Entry Selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
158
fexlex =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
combinations of feature templates
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing(?w1, . . . , wn?, ?L,R?, ?, ?, ?, ?, ?)
for i = 1 to n
foreach F ? ? {F |?wi, F ? ? L}
p =
?
u ?ufu(F
?)
pi[i? 1, i] ? pi[i? 1, i] ? {F ?}
if (p > ?[i? 1, i, F ?]) then
?[i? 1, i, F ?] ? p
LocalThresholding(i? 1, i,?, ?)
for d = 1 to n
for i = 0 to n? d
j = i+ d
for k = i+ 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if F = r(Fs, Ft) has succeeded
p = ?[i, k, Fs] + ?[k, j, Ft] +
?
u ?ufu(F )
pi[i, j] ? pi[i, j] ? {F}
if (p > ?[i, j, F ]) then
?[i, j, F ] ? p
LocalThresholding(i, j,?, ?)
GlobalThresholding(i, n, ?)
procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ??, ??, ??,
??, ??, ?last, ?last, ?last, ?last, ?last)? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0;
loop while ? ? ?last and ? ? ?last and ? ? ?last and ? ? ?last
and ? ? ?last
call Parsing(w, G, ?, ?, ?, ?, ?)
if pi[1, n] 6= ? then exit
? ? ?+??; ? ? ? +??;
? ? ?+??; ? ? ? +??; ? ? ? +??;
Figure 3: Pseudo-code of iterative parsing for
HPSG.
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
4 Experiments
4.1 Implementation
We implemented the iterative parsing algorithm
(Ninomiya et al, 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the pi[i, j] represents
the set of partial parse results that cover words
wi+1, . . . , wj , and ?[i, j, F ] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as ?u ?ufu(F ) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(F |w, i). The FOM of a newly created partial
parse, F , is computed by summing the values of
? of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of ? of the daughters; i.e., weights
exp(?u) in the figure are assigned zero. The terms
? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms ? and ? are the thresh-
olds of the number and the beam width of lexical
entries, and ? is the beam width for global thresh-
olding (Goodman, 1997).
4.2 Evaluation
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al, 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al, 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ?effective? tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
159
No. of tested sentences Total No. of Avg. length of tested sentences
? 40 words ? 100 words sentences ? 40 words ? 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (? 40 + Gold POSs) Section 23 (? 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (? 40 + POS tagger) Section 23 (? 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
Table 4: Experimental results for Section 23.
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: ?0 = 4,?? = 4, ?last =
20, ?0 = 1.0,?? = 2.5, ?last = 11.0, ?0 =
12,?? = 4, ?last = 28, ?0 = 6.0,?? =
2.25, ?last = 15.0, ?0 = 8.0,?? = 3.0, and
?last = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple ??,wh, a, wa?, where ? is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al, 2000) and large constituent inhibition (Kaplan et al,
2004) as described by Ninomiya et al (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
? 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of ? 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of ? 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of ? 40 and ? 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
160
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F
-
s
c
o
r
e
previous model
model 1
model 2
model 3
Figure 4: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values < 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of? 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
5 Discussion
5.1 Supertagging
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
Table 5: Accuracy of single-tag supertaggers. The
numbers under ?test data? are the PTB section
numbers of the test data.
? tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
Table 6: Accuracy of multi-supertagging.
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of ?, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
161
5.2 Efficacy of extremely lexicalized models
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
6 Conclusion
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597?
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL?05, pages 173?180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
of ACL?04, pages 104?111.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL?03, pages 91?98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of ACL ?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL?03,
pages 423?430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop ?Be-
yond Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,
2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103?114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL?00, pages
480?487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. of ACL?03, pages 505?512.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL?04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
163
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848
Manchester, August 2008
Modeling Latent-Dynamic in Shallow Parsing:
A Latent Conditional Model with Improved Inference
Xu Sun? Louis-Philippe Morency? Daisuke Okanohara? Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan
?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA
?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK
?{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.edu
Abstract
Shallow parsing is one of many NLP tasks
that can be reduced to a sequence la-
beling problem. In this paper we show
that the latent-dynamics (i.e., hidden sub-
structure of shallow phrases) constitutes a
problem in shallow parsing, and we show
that modeling this intermediate structure
is useful. By analyzing the automatically
learned hidden states, we show how the
latent conditional model explicitly learn
latent-dynamics. We propose in this paper
the Best Label Path (BLP) inference algo-
rithm, which is able to produce the most
probable label sequence on latent condi-
tional models. It outperforms two existing
inference algorithms. With the BLP infer-
ence, the LDCRF model significantly out-
performs CRF models on word features,
and achieves comparable performance of
the most successful shallow parsers on the
CoNLL data when further using part-of-
speech features.
1 Introduction
Shallow parsing identifies the non-recursive cores
of various phrase types in text. The paradigmatic
shallow parsing problem is noun phrase chunking,
in which the non-recursive cores of noun phrases,
called base NPs, are identified. As the represen-
tative problem in shallow parsing, noun phrase
chunking has received much attention, with the de-
velopment of standard evaluation datasets and with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
extensive comparisons among methods (McDon-
ald 2005; Sha & Pereira 2003; Kudo & Matsumoto
2001).
Syntactic contexts often have a complex under-
lying structure. Chunk labels are usually far too
general to fully encapsulate the syntactic behavior
of word sequences. In practice, and given the lim-
ited data, the relationship between specific words
and their syntactic contexts may be best modeled
at a level finer than chunk tags but coarser than
lexical identities. For example, in the noun phrase
(NP) chunking task, suppose that there are two lex-
ical sequences, ?He is her ?? and ?He gave her
? ?. The observed sequences, ?He is her? and
?He gave her?, would both be conventionally la-
beled by ?BOB?, where B signifies the ?beginning
NP?, and O the ?outside NP?. However, this label-
ing may be too general to encapsulate their respec-
tive syntactic dynamics. In actuality, they have dif-
ferent latent-structures, crucial in labeling the next
word. For ?He is her ??, the NP started by ?her? is
still incomplete, so the label for ? is likely to be I,
which conveys the continuation of the phrase, e.g.,
?[He] is [her brother]?. In contrast, for ?He gave
her ??, the phrase started by ?her? is normally self-
complete, and makes the next label more likely to
be B, e.g., ?[He] gave [her] [flowers]?.
In other words, latent-dynamics is an interme-
diate representation between input features and la-
bels, and explicitly modeling this can simplify the
problem. In particular, in many real-world cases,
when the part-of-speech tags are not available, the
modeling on latent-dynamics would be particu-
larly important.
In this paper, we model latent-dynamics in
shallow parsing by extending the Latent-Dynamic
Conditional Random Fields (LDCRFs) (Morency
et al 2007), which offer advantages over previ-
841
y y y1 2 m h h h1 2 m
y y y1 2 m
CRF LDCRF
x x x1 2 mx x x1 2 m
Figure 1: Comparison between CRF and LDCRF.
In these graphical models, x represents the obser-
vation sequence, y represents labels and h repre-
sents hidden states assigned to labels. Note that
only gray circles are observed variables. Also,
only the links with the current observation are
shown, but for both models, long range dependen-
cies are possible.
ous learning methods by explicitly modeling hid-
den state variables (see Figure 1). We expect LD-
CRFs to be particularly useful in those cases with-
out POS tags, though this paper is not limited to
this.
The inference technique is one of the most im-
portant components for a structured classification
model. In conventional models like CRFs, the op-
timal label path can be directly searched by using
dynamic programming. However, for latent condi-
tional models like LDCRFs, the inference is kind
of tricky, because of hidden state variables. In this
paper, we propose an exact inference algorithm,
the Best Label Path inference, to efficiently pro-
duce the optimal label sequence on LDCRFs.
The following section describes the related
work. We then review LDCRFs, and propose the
BLP inference. We further present a statistical
interpretation on learned hidden states. Finally,
we show that LDCRF-BLP is particularly effective
when pure word features are used, and when POS
tags are added, as existing systems did, it achieves
comparable results to the best reported systems.
2 Related Work
There is a wide range of related work on shallow
parsing. Shallow parsing is frequently reduced to
sequence labeling problems, and a large part of
previous work uses machine learning approaches.
Some approaches rely on k-order generative proba-
bilistic models of paired input sequences and label
sequences, such as HMMs (Freitag & McCallum
2000; Kupiec 1992) or multilevel Markov mod-
els (Bikel et al 1999). The generative model
provides well-understood training and inference
but requires stringent conditional independence as-
sumptions.
To accommodate multiple overlapping features
on observations, some other approaches view the
sequence labeling problem as a sequence of clas-
sification problems, including support vector ma-
chines (SVMs) (Kudo & Matsumoto 2001) and a
variety of other classifiers (Punyakanok & Roth
2001; Abney et al 1999; Ratnaparkhi 1996).
Since these classifiers cannot trade off decisions at
different positions against each other (Lafferty et
al. 2001), the best classifier based shallow parsers
are forced to resort to heuristic combinations of
multiple classifiers.
A significant amount of recent work has shown
the power of CRFs for sequence labeling tasks.
CRFs use an exponential distribution to model the
entire sequence, allowing for non-local dependen-
cies between states and observations (Lafferty et
al. 2001). Lafferty et al (2001) showed that CRFs
outperform classification models as well as HMMs
on synthetic data and on POS tagging tasks. As for
the task of shallow parsing, CRFs also outperform
many other state-of-the-art models (Sha & Pereira
2003; McDonald et al 2005).
When the data has distinct sub-structures, mod-
els that exploit hidden state variables are advanta-
geous in learning (Matsuzaki et al 2005; Petrov
et al 2007). Sutton et al (2004) presented an
extension to CRF called dynamic conditional ran-
dom field (DCRF) model. As stated by the authors,
training a DCRF model with unobserved nodes
(hidden variables) makes their approach difficult
to optimize. In the vision community, the LD-
CRF model was recently proposed by Morency et
al. (2007), and shown to outperform CRFs, SVMs,
and HMMs for visual sequence labeling.
In this paper, we introduce the concept of latent-
dynamics for shallow parsing, showing how hid-
den states automatically learned by the model
present similar characteristics. We will also pro-
pose an improved inference technique, the BLP,
for producing the most probable label sequence in
LDCRFs.
3 Latent-Dynamic Conditional Random
Fields
The task is to learn a mapping between a sequence
of observations x = x
1
, x
2
, . . . , x
m
and a sequence
of labels y = y
1
, y
2
, . . . , y
m
. Each y
j
is a class la-
842
bel for the j?th token of a word sequence and is a
member of a set Y of possible class labels. For
each sequence, the model also assumes a vector of
hidden state variables h = {h
1
, h
2
, . . . , h
m
}, which
are not observable in the training examples.
Given the above definitions, we define a latent
conditional model as follows:
P(y|x,?) =
?
h
P(y|h, x,?)P(h|x,?), (1)
where ? are the parameters of the model. The LD-
CRF model can seem as a natural extension of the
CRF model, and the CRF model can seem as a spe-
cial case of LDCRFs employing one hidden state
for each label.
To keep training and inference efficient, we re-
strict the model to have disjointed sets of hidden
states associated with each class label. Each h
j
is
a member of a set H
y
j
of possible hidden states for
the class label y
j
. We define H, the set of all pos-
sible hidden states to be the union of all H
y
j
sets.
Since sequences which have any h
j
< H
y
j
will by
definition have P(y|x,?) = 0, we can express our
model as:
P(y|x,?) =
?
h?H
y
1
?...?H
y
m
P(h|x,?), (2)
where P(h|x,?) is defined using the usual con-
ditional random field formulation: P(h|x,?) =
exp ??f(h|x)/
?
?h
exp ??f(h|x), in which f(h|x) is
the feature vector. Given a training set consisting
of n labeled sequences (x
i
, y
i
) for i = 1 . . . n, train-
ing is performed by optimizing the objective func-
tion to learn the parameter ?
?
:
L(?) =
n
?
i=1
log P(y
i
|x
i
,?) ? R(?). (3)
The first term of this equation is the conditional
log-likelihood of the training data. The second
term is the regularizer.
4 BLP Inference on Latent Conditional
Models
For testing, given a new test sequence x, we want
to estimate the most probable label sequence (Best
Label Path), y
?
, that maximizes our conditional
model:
y
?
= argmax
y
P(y|x,?
?
). (4)
In the CRF model, y
?
can be simply searched by
using the Viterbi algorithm. However, for latent
conditional models like LDCRF, the Best Label
Path y
?
cannot directly be produced by the Viterbi
algorithm because of the incorporation of hidden
states.
In this paper, we propose an exact inference al-
gorithm, the Best Label Path inference (BLP), for
producing the most probable label sequence y
?
on
LDCRF. In the BLP schema, top-n hidden paths
HP
n
= {h
1
,h
2
. . . h
n
} over hidden states are effi-
ciently produced by using A
?
search (Hart et al,
1968), and the corresponding probabilities of hid-
den paths P(h
i
|x,?) are gained. Thereafter, based
on HP
n
, the estimated probabilities of various la-
bel paths, P(y|x,?), can be computed by summing
the probabilities of hidden paths, P(h|x,?), con-
cerning the association between hidden states and
each class label:
P(y|x,?) =
?
h: h?H
y
1
?...?H
y
m
?h?HP
n
P(h|x,?). (5)
By using the A
?
search, HP
n
can be extended in-
crementally in an efficient manner, until the algo-
rithm finds that the Best Label Path is ready, and
then the search stops and ends the BLP inference
with success. The algorithm judges that y
?
is ready
when the following condition is achieved:
P(y
1
|x,?) ? P(y
2
|x,?) +
?
h<H
y
1
?...?H
y
m
P(h|x,?), (6)
where y
1
is the most probable label sequence, and
y
2
is the second ranked label sequence estimated
by using HP
n
. It would be straightforward to prove
that y
?
= y
1
, and further search is unnecessary, be-
cause in this case, the unknown probability mass
can not change the optimal label path. The un-
known probability mass can be computed by using
?
h<H
y
1
?...?H
y
m
P(h|x,?) = 1 ?
?
h?H
y
1
?...?H
y
m
P(h|x,?). (7)
The top-n hidden paths of HP
n
produced by the
A
?
-search are exact, and the BLP inference is ex-
act. To guarantee HP
n
is exact in our BLP in-
ference, an admissible heuristic function should
be used in A
?
search (Hart et al, 1968). We use
a backward Viterbi algorithm (Viterbi, 1967) to
compute the heuristic function of the forward A
?
search:
Heu
i
(h
j
) = max
h
?
i
=h
j
?h
?
i
?HP
|h|
i
P
?
(h
?
|x,?
?
), (8)
843
where h
?
i
= h
j
represents a partial hidden path
started from the hidden state h
j
, and HP
|h|
i
rep-
resents all possible partial hidden paths from the
position i to the ending position |h| . Heu
i
(h
j
) is
an admissible heuristic function for the A
?
search
over hidden paths, therefore HP
n
is exact and BLP
inference is exact.
The BLP inference is efficient when the prob-
ability distribution among the hidden paths is in-
tensive. By combining the forward A
?
with the
backward Viterbi algorithm, the time complexity
of producing HP
n
is roughly a linear complexity
concerning its size. In practice, on the CoNLL test
data containing 2,012 sentences, the BLP infer-
ence finished in five minutes when using the fea-
ture set based on both word and POS information
(see Table 3). The memory consumption is also
relatively small, because it is an online style algo-
rithm and it is not necessary to preserve HP
n
.
In this paper, to make a comparison, we also
study the Best Hidden Path inference (BHP):
y
BHP
= argmax
y
P(h
y
|x,?
?
), (9)
where h
y
? H
y
1
? . . . ?H
y
m
. In other words, the
Best Hidden Path is the label sequence that is di-
rectly projected from the most probable hidden
path h
?
.
In (Morency et al 2007), y
?
is estimated by us-
ing the Best Point-wise Marginal Path (BMP). To
estimate the label y
j
of token j, the marginal prob-
abilities P(h
j
= a|x,?) are computed for possible
hidden states a ? H. Then, the marginal probabili-
ties are summed and the optimal label is estimated
by using the marginal probabilities.
The BLP produces y
?
while the BHP and the
BMP perform an estimation on y
?
. We will make
an experimental comparison in Section 6.
5 Analyzing Latent-Dynamics
The chunks in shallow parsing are represented with
the three labels shown in Table 1, and shallow pars-
ing is treated as a sequence labeling task with those
three labels. A challenge for most shallow parsing
approaches is to determine the concepts learned by
the model. In this section, we show how we can
analyze the latent-dynamics.
5.1 Analyzing Latent-Dynamics
In this section, we show how to analyze the charac-
teristics of the hidden states. Our goal is to find the
words characterizing a specific hidden state, and
B words beginning a chunk
I words continuing a chunk
O words being outside a chunk
Table 1: Shallow parsing labels.
then look at the selected words with their associ-
ated POS tags to determine if the LDCRF model
has learned meaningful latent-dynamics.
In the experiments reported in this section, we
did not use the features on POS tags in order to
isolate the model?s capability of learning latent dy-
namics. In other words, the model could simply
learn the dynamics of POS tags as the latent dy-
namics if the model is given the information about
POS tags. The features used in the experiments are
listed on the left side (Word Features) in Table 3.
The main idea is to look at the marginal proba-
bilities P(h
j
= a|x,?) for each word j, and select
the hidden state a
?
with the highest probability. By
counting how often a specific word selected a as
the optimal hidden state, i.e., ?(w, a), we can cre-
ate statistics about the relationship between hidden
states and words. We define relative frequency as
the number of times a specific word selected a hid-
den state while normalized by the global frequency
of this word:
RltFreq(w, h
j
) =
Freq( ?(w, h
j
) )
Freq(w)
. (10)
5.2 Learned Latent-Dynamics from CoNLL
In this subsection, we show the latent-dynamics
learned automatically from the CoNLL dataset.
The details of these experiments are presented in
the following section.
The most frequent three words corresponding to
the individual hidden states of the labels, B and O,
are shown in Table 2. As shown, the automati-
cally learned hidden states demonstrate prominent
characteristics. The extrinsic label B, which begins
a noun phrase, is automatically split into 4 sub-
categories: wh-determiners (WDT, such as ?that?)
together with wh-pronouns (WP, such as ?who?),
the determiners (DT, such as ?any, an, a?), the per-
sonal pronouns (PRP, such as ?they, we, he?), and
the singular proper nouns (NNP, such as ?Nasdaq,
Florida?) together with the plural nouns (NNS,
such as ?cities?). The results of B1 suggests that
the wh-determiners represented by ?that?, and the
wh-pronouns represented by ?who?, perform simi-
844
Labels HidStat Words POS RltFreq
B
That WDT 0.85
B1 who WP 0.49
Who WP 0.33
any DT 1.00
B2 an DT 1.00
a DT 0.98
They PRP 1.00
B3 we PRP 1.00
he PRP 1.00
Nasdaq NNP 1.00
B4 Florida NNP 0.99
cities NNS 0.99
O
But CC 0.88
O1 by IN 0.73
or IN 0.67
4.6 CD 1.00
O2 1 CD 1.00
11 CD 0.62
were VBD 0.94
O3 rose VBD 0.93
have VBP 0.92
been VBN 0.97
O4 be VB 0.94
to TO 0.92
Table 2: Latent-dynamics learned automatically by
the LDCRF model. This table shows the top three
words and their gold-standard POS tags for each
hidden states.
lar roles in modeling the dynamics in shallow pars-
ing. Further, the singular proper nouns and the
plural nouns are grouped together, suggesting that
they may perform similar roles. Moreover, we can
notice that B2 and B3 are highly consistent.
The label O is automatically split into the coordi-
nating conjunctions (CC) together with the prepo-
sitions (IN) indexed by O1, the cardinal numbers
(CD) indexed by O2, the past tense verbs (VBD)
together with the personal verbs (VBP) indexed by
O3, and another sub-category, O4. From the results
we can find that gold-standard POS tags may not
be adequate in modeling latent-dynamics in shal-
low parsing, as we can notice that three hidden
states out of four (O1, O3 and O4) contains relat-
ing but different gold-standard POS tags.
6 Experiments
Following previous studies on shallow parsing, our
experiments are performed on the CoNLL 2000
Word Features:
{w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
, w
i?1
w
i
, w
i
w
i+1
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
POS Features:
{t
i?1
, t
i
, t
i+1
, t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
,
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
Table 3: Feature templates used in the experi-
ments. w
i
is the current word; t
i
is current POS
tag; and h
i
is the current hidden state (for the case
of latent models) or the current label (for the case
of conventional models).
data set (Sang & Buchholz 2000; Ramshow &
Marcus 1995). The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences. The standard evaluation metrics for this
task are precision p (the fraction of output chunks
matching the reference chunks), recall r (the frac-
tion of reference chunks returned), and the F-
measure given by F = 2pr/(p + r).
6.1 LDCRF for Shallow Parsing
We implemented LDCRFs in C++, and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions.
We employ similar predicate sets defined in Sha
& Pereira (2003). We follow them in using predi-
cates that depend on words as well as POS tags in
the neighborhood of a given position, taking into
account only those 417,835 features which occur
at least once in the training data. The features are
listed in Table 3.
As for numerical optimization (Malouf 2002;
Wallach 2002), we performed gradient decent with
the Limited-Memory BFGS (L-BFGS) optimiza-
tion technique (Nocedal & Wright 1999). L-BFGS
is a second-order Quasi-Newton method that nu-
merically estimates the curvature from previous
gradients and updates. With no requirement on
specialized Hessian approximation, L-BFGS can
handle large-scale problems in an efficient manner.
We implemented an L-BFGS optimizer in C++ by
modifying the OWLQN package (Andrew & Gao
2007) developed by Galen Andrew. In our exper-
iments, storing 10 pairs of previous gradients for
the approximation of the function?s inverse Hes-
sian worked well, making the amount of the ex-
tra memory required modest. Using more pre-
vious gradients will probably decrease the num-
845
ber of iterations required to reach convergence,
but would increase memory requirements signifi-
cantly. To make a comparison, we also employed
the Conjugate-Gradient (CG) optimization algo-
rithm. For details of CG, see Shewchuk (1994).
Since the objective function of the LDCRF
model is non-convex, it is suggested to use the ran-
dom initialization of parameters for the training.
To reduce overfitting, we employed an L
2
Gaus-
sian weight prior (Chen & Rosenfeld 1999). Dur-
ing training and validation, we varied the number
of hidden states per label (from 2 to 6 states per
label), and also varied the L
2
-regularization term
(with values 10
k
, k from -3 to 3). Our experiments
suggested that using 4 or 5 hidden states per label
for the shallow parser is a viable compromise be-
tween accuracy and efficiency.
7 Results and Discussion
7.1 Performance on Word Features
As discussed in Section 4, it is preferred to not
use the features on POS tags in order to isolate
the model?s capability of learning latent dynam-
ics. In this sub-section, we use pure word fea-
tures with their counts above 10 in the training data
to perform experimental comparisons among dif-
ferent inference algorithms on LDCRFs, including
BLP, BHP, and existing BMP.
Since the CRF model is one of the success-
ful models in sequential labeling tasks (Lafferty et
al. 2001; Sha & Pereira 2003; McDonald et al
2005), in this section, we also compare LDCRFs
with CRFs. We tried to make experimental results
more comparable between LDCRF and CRF mod-
els, and have therefore employed the same fea-
tures set, optimizer and fine-tuning strategy be-
tween LDCRF and CRF models.
The experimental results are shown in Table 4.
In the table, Acc. signifies ?label accuracy?, which
is useful for the significance test in the follow-
ing sub-section. As shown, LDCRF-BLP outper-
forms LDCRF-BHP and LDCRF-BMP, suggesting
that BLP inference
1
is superior. The superiority
of BLP is statistically significant, which will be
shown in next sub-section. On the other side, all
the LDCRF models outperform the CRF model. In
particular, the gap between LDCRF-BLP and CRF
is 1.53 percent.
1
In practice, for efficiency, we approximated the BLP on a
few sentences by limiting the number of search steps.
Models: WF Acc. Pre. Rec. F
1
LDCRF-BLP 97.01 90.33 88.91 89.61
LDCRF-BHP 96.52 90.26 88.21 89.22
LDCRF-BMP 97.26 89.83 89.06 89.44
CRF 96.11 88.12 88.03 88.08
Table 4: Experimental comparisons among differ-
ent inference algorithms on LDCRFs, and the per-
formance of CRFs using the same feature set on
pure word features. The BLP inference outper-
forms the BHP and BMP inference. LDCRFs out-
perform CRFs.
Models F
1
Gap Acc. Gap Sig.
BLP vs. BHP 0.39 0.49 1e-10
BLP vs. CRF 1.53 0.90 5e-13
Table 5: The significance tests. LDCRF-BLP is
significantly more accurate than LDCRF-BHP and
CRFs.
7.2 Labeling Accuracy and Significance Test
As shown in Table 4, the accuracy rate for individ-
ual labeling decisions is over-optimistic as a mea-
sure for shallow parsing. Nevertheless, since test-
ing the significance of shallow parsers? F-measures
is tricky, individual labeling accuracy provides a
more convenient basis for statistical significance
tests (Sha & Pereira 2003). One such test is the
McNemar test on paired observations (Gillick &
Cox 1989). As shown in Table 5, for the LD-
CRF model, the BLP inference schema is sta-
tistically more accurate than the BHP inference
schema. Also, Evaluations show that the McNe-
mar?s value on labeling disagreement between the
LDCRF-BLP and CRF models is 5e-13, suggest-
ing that LDCRF-BLP is significantly more accu-
rate than CRFs.
On the other hand, the accuracy rate of BMP in-
ference is a special case. Since the BMP inference
is essentially an accuracy-first inference schema,
the accuracy rate and the F-measure have a differ-
ent relation in BMP. As we can see, the individual
labeling accuracy achieved by the LDCRF-BMP
model is as high as 97.26%, but its F-measure is
still lower than LDCRF-BLP.
7.3 Convergence Speed
It would be interesting to compare the convergence
speed between the objective loss function of LD-
CRFs and CRFs. We apply the L-BFGS optimiza-
846
 150
 200
 250
 300
 350
 400
 450
 500
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
LDCRF
CRF
Figure 2: The value of the penalized loss based on
the number of iterations: LDCRFs vs. CRFs.
 160
 180
 200
 220
 240
 260
 280
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
L-BFGS
CG
Figure 3: Training the LDCRF model: L-BFGS
vs. CG.
tion algorithm to optimize the loss function of LD-
CRF and CRF models, making a comparison be-
tween them. We find that the iterations required
for the convergence of LDCRFs is less than for
CRFs (see Figure 2). Normally, the LDCRF model
arrives at the plateau of convergence in 120-150
iterations, while CRFs require 210-240 iterations.
When we replace the L-BFGS optimizer by the CG
optimization algorithm, we observed as well that
LDCRF converges faster on iteration numbers than
CRF does.
On the contrary, however, the time cost of the
LDCRF model in each iteration is higher than the
CRF model, because of the incorporation of hid-
den states. The time cost of the LDCRF model
in each iteration is roughly a quadratic increase
concerning the increase of the number of hidden
states. Therefore, though the LDCRF model re-
quires less passes for the convergence, it is practi-
cally slower than the CRF model. Improving the
scalability of the LDCRF model would be a inter-
esting topic in the future.
Furthermore, we make a comparison between
Models: WF+POS Pre. Rec. F
1
LDCRF-BLP 94.65 94.03 94.34
CRF
N/A N/A 93.6
(Vishwanathan et al 06)
CRF
94.57 94.00 94.29
(McDonald et al 05)
Voted perceptron
N/A N/A 93.53
(Collins 02)
Generalized Winnow
93.80 93.99 93.89
(Zhang et al 02)
SVM combination
94.15 94.29 94.22
(Kudo & Matsumoto 01)
Memo. classifier
93.63 92.89 93.26
(Sang 00)
Table 6: Performance of the LDCRF-BLP model,
and the comparison with CRFs and other success-
ful approaches. In this table, all the systems have
employed POS features.
the L-BFGS and the CG optimizer for LDCRFs.
We observe that the L-BFGS optimizer is slightly
faster than CG on LDCRFs (see Figure 3), which
echoes the comparison between the L-BFGS and
the CG optimizing technique on the CRF model
(Sha & Pereira 2003).
7.4 Comparisons to Other Systems with POS
Features
Performance of the LDCRF-BLP model and some
of the best results reported previously are summa-
rized in Table 6. Our LDCRF model achieved
comparable performance to those best reported
systems in terms of the F-measure.
McDonald et al (2005) achieved an F-measure
of 94.29% by using a CRF model. By employing a
multi-model combination approach, Kudo & Mat-
sumoto (2001) also achieved a good performance.
They use a combination of 8 kernel SVMs with
a heuristic voting strategy. An advantage of LD-
CRFs over max-margin based approaches is that
LDCRFs can output N-best label sequences and
their probabilities using efficient marginalization
operations, which can be used for other compo-
nents in an information extraction system.
8 Conclusions and Future Work
In this paper, we have shown that automatic model-
ing on ?latent-dynamics? can be useful in shallow
parsing. By analyzing the automatically learned
847
hidden states, we showed how LDCRFs can natu-
rally learn latent-dynamics in shallow parsing.
We proposed an improved inference algorithm,
the BLP, for LDCRFs. We performed experiments
using the CoNLL data, and showed how the BLP
inference outperforms existing inference engines.
When further employing POS features as other
systems did, the performance of the LDCRF-BLP
model is comparable to those best reported results.
The LDCRF model demonstrates a significant ad-
vantage over other models on pure word features
in this paper. We expect it to be particularly useful
in the real-world tasks without rich features.
The latent conditional model handles latent-
dynamics naturally, and can be easily extended to
other labeling tasks. Also, the BLP inference algo-
rithm can be extended to other latent conditional
models for producing optimal label sequences. As
a future work, we plan to further speed up the BLP
algorithm.
Acknowledgments
Many thanks to Yoshimasa Tsuruoka for helpful
discussions on the experiments and paper-writing.
This research was partially supported by Grant-
in-Aid for Specially Promoted Research 18002007
(MEXT, Japan). The work at the USC Institute for
Creative Technology was sponsored by the U.S.
Army Research, Development, and Engineering
Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
References
Abney, S. 1991. Parsing by chunks. In R. Berwick, S. Ab-
ney, and C. Tenny, editors, Principle-based Parsing. Kluwer
Academic Publishers.
Abney, S.; Schapire, R. E. and Singer, Y. 1999. Boosting
applied to tagging and PP attachment. In Proc. EMNLP/VLC-
99.
Andrew, G. and Gao, J. 2007. Scalable training of L1-
regularized log-linear models. In Proc. ICML-07.
Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.
An algorithm that learns what?s in a name. Machine Learning,
34: 211-231.
Chen, S. F. and Rosenfeld, R. 1999. A Gaussian prior
for smooth-ing maximum entropy models. Technical Report
CMU-CS-99-108, CMU.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP-02.
Freitag, D. and McCallum, A. 2000. Information extrac-
tion with HMM structures learned by stochastic optimization.
In Proc. AAAI-00.
Gillick, L. and Cox, S. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics Speech and Signal Process-
ing, v1, pages 532-535.
Hart, P.E.; Nilsson, N.J.; and Raphael, B. 1968. A formal
basis for the heuristic determination of minimum cost path.
IEEE Trans. On System Science and Cybernetics, SSC-4(2):
100-107.
Kudo, T. and Matsumoto, Y. 2001. Chunking with support
vector machines. In Proc. NAACL-01.
Kupiec, J. 1992. Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language.
6:225-242.
Lafferty, J.; McCallum, A. and Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML-01, pages 282-289.
Malouf, R. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-02.
Matsuzaki, T.; Miyao Y. and Tsujii, J. 2005. Probabilistic
CFG with Latent Annotations. In Proc. ACL-05.
McDonald, R.; Crammer, K. and Pereira, F. 2005. Flexible
Text Segmentation with Structured Multilabel Classification.
In Proc. HLT/EMNLP-05, pages 987- 994.
Morency, L.P.; Quattoni, A. and Darrell, T. 2007. Latent-
Dynamic Discriminative Models for Continuous Gesture
Recognition. In Proc. CVPR-07, pages 1- 8.
Nocedal, J. and Wright, S. J. 1999. Numerical Optimiza-
tion. Springer.
Petrov, S.; Pauls, A.; and Klein, D. 2007. Discriminative
log-linear grammars with latent variables. In Proc. NIPS-07.
Punyakanok, V. and Roth, D. 2001. The use of classifiers
in sequential inference. In Proc. NIPS-01, pages 995-1001.
MIT Press.
Ramshaw, L. A. and Marcus, M. P. 1995. Text chunking
using transformation-based learning. In Proc. Third Work-
shop on Very Large Corpora. In Proc. ACL-95.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP-96.
Sang, E.F.T.K. 2000. Noun Phrase Representation by Sys-
tem Combination. In Proc. ANLP/NAACL-00.
Sang, E.F.T.K and Buchholz, S. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-00,
pages 127-132.
Sha, F. and Pereira, F. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. HLT/NAACL-03.
Shewchuk, J. R. 1994. An introduction to the
conjugate gradient method without the agonizing pain.
http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.
Sutton, C.; Rohanimanesh, K. and McCallum, A. 2004.
Dynamic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML-04.
Viterbi, A.J. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory. 13(2):260-269.
Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W. and
Murphy, K. 2006. Accelerated training of conditional random
fields with stochastic meta-descent. In Proc. ICML-06.
Wallach, H. 2002. Efficient training of conditional random
fields. In Proc. 6th Annual CLUK Research Colloquium.
Zhang, T.; Damerau, F. and Johnson, D. 2002. Text chunk-
ing based on a generalization of winnow. Journal of Machine
Learning Research, 2:615-637.
848
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19?27,
Beijing, August 2010
Robust Measurement and Comparison of Context Similarity for Finding
Translation Pairs
Daniel Andrade?, Tetsuya Nasukawa?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, tsujii}@is.s.u-tokyo.ac.jp
?IBM Research - Tokyo
nasukawa@jp.ibm.com
Abstract
In cross-language information retrieval it
is often important to align words that are
similar in meaning in two corpora writ-
ten in different languages. Previous re-
search shows that using context similar-
ity to align words is helpful when no
dictionary entry is available. We sug-
gest a new method which selects a sub-
set of words (pivot words) associated with
a query and then matches these words
across languages. To detect word associa-
tions, we demonstrate that a new Bayesian
method for estimating Point-wise Mutual
Information provides improved accuracy.
In the second step, matching is done in
a novel way that calculates the chance of
an accidental overlap of pivot words us-
ing the hypergeometric distribution. We
implemented a wide variety of previously
suggested methods. Testing in two con-
ditions, a small comparable corpora pair
and a large but unrelated corpora pair,
both written in disparate languages, we
show that our approach consistently out-
performs the other systems.
1 Introduction
Translating domain-specific, technical terms from
one language to another can be challenging be-
cause they are often not listed in a general dictio-
nary. The problem is exemplified in cross-lingual
information retrieval (Chiao and Zweigenbaum,
2002) restricted to a certain domain. In this case,
the user might enter only a few technical terms.
However, jargons that appear frequently in the
data set but not in general dictionaries, impair the
usefulness of such systems. Therefore, various
means to extract translation pairs automatically
have been proposed. They use different clues,
mainly
? Spelling distance or transliterations, which
are useful to identify loan words (Koehn and
Knight, 2002).
? Context similarity, helpful since two words
with identical meaning are often used in sim-
ilar contexts across languages (Rapp, 1999).
The first type of information is quite specific; it
can only be helpful in a few cases, and can thereby
engender high-precision systems with low recall,
as described for example in (Koehn and Knight,
2002). The latter is more general. It holds for
most words including loan words. Usually the
context of a word is defined by the words which
occur around it (bag-of-words model).
Let us briefly recall the main idea for using
context similarity to find translation pairs. First,
the degree of association between the query word
and all content words is measured with respect to
the corpus at hand. The same is done for every
possible translation candidate in the target cor-
pus. This way, we can create a feature vector
for the query and all its possible translation can-
didates. We can assume that, for some content
words, we have valid translations in a general dic-
tionary, which enables us to compare the vectors
across languages. We will designate these content
words as pivot words. The query and its trans-
lation candidates are then compared using their
feature vectors, where each dimension in the fea-
ture vector contains the degree of association to
19
one pivot word. We define the degree of associa-
tion, as a measurement for finding words that co-
occur, or which do not co-occur, more often than
we would expect by pure chance.1
We argue that common ways for comparing
similarity vectors across different corpora perform
worse because they assume that degree of associa-
tions are very similar across languages and can be
compared without much preprocessing. We there-
fore suggest a new robust method including two
steps. Given a query word, in the first step we
determine the set of pivots that are all positively
associated with statistical significance. In the sec-
ond step, we compare this set of pivots with the set
of pivots extracted for a possible translation can-
didate. For extracting positively associated piv-
ots, we suggest using a new Bayesian method for
estimating the critical Pointwise Mutual Informa-
tion (PMI) value. In the second step, we use a
novel measure to compare the sets of extracted
pivot words which is based on an estimation of
the probability that pivot words overlap by pure
chance. Our approach engenders statistically sig-
nificant improved accuracy for aligning transla-
tion pairs, when compared to a variety of previ-
ously suggested methods. We confirmed our find-
ings using two very different pairs of comparable
corpora for Japanese and English.
In the next section, we review previous related
work. In Section 3 we explain our method in
detail, and argue that it overcomes subtle weak-
nesses of several previous efforts. In Section 4, we
show with a series of cross-lingual experiments
that our method, in some settings, can lead to con-
siderable improvement in accuracy. Subsequently
in Section 4.2, we analyze our method in contrast
to the baseline by giving two examples. We sum-
marize our findings in Section 5.
2 Related Work
Extracting context similarity for nouns and then
matching them across languages to find trans-
lation pairs was pioneered in (Rapp, 1999) and
(Fung, 1998). The work in (Chiao and Zweigen-
baum, 2002), which can be regarded as a varia-
1For example ?car? and ?tire? are expected to have a high
(positive) degree of association, and ?car? and ?apple? is ex-
pected to have a high (negative) degree of association.
tion of (Fung, 1998), uses tf.idf, but suggests to
normalize the term frequency by the maximum
number of co-occurrences of two words in the cor-
pus. All this work is closely related to our work
because they solely consider context similarity,
whereas context is defined using a word window.
The work in (Rapp, 1999; Fung, 1998; Chiao and
Zweigenbaum, 2002) will form the baselines for
our experiments in Section 4.2 This baseline is
also similar to the baseline in (Gaussier et al,
2004), which showed that it can be difficult to beat
such a feature vector approach.
In principle our method is not restricted to how
context is defined; we could also use, for exam-
ple, modifiers and head words, as in (Garera et
al., 2009). Although, we found in a preliminary
experiment that using a dependency parser to dif-
ferentiate between modifiers and head words like
in (Garera et al, 2009), instead of a bag-of-words
model, in our setting, actually decreased accuracy
due to the narrow dependency window. How-
ever, our method could be combined with a back-
translation step, which is expected to improve
translation quality as in (Haghighi et al, 2008),
which performs indirectly a back-translation by
matching all nouns mutually exclusive across cor-
pora. Notably, there also exist promising ap-
proaches which use both types of information,
spelling distance, and context similarity in a joint
framework, see (Haghighi et al, 2008), or (De?jean
et al, 2002) which include knowledge of a the-
saurus. In our work here, we concentrate on the
use of degrees of association as an effective means
to extract word translations.
In this application, to measure association ro-
bustly, often the Log-Likelihood Ratio (LLR)
measurement is suggested (Rapp, 1999; Morin et
al., 2007; Chiao and Zweigenbaum, 2002). The
occurrence of a word in a document is modeled
as a binary random variable. The LLR measure-
ment measures stochastic dependency between
2Notable differences are that we neglected word order, in
contrast to (Rapp, 1999), as it is little useful to compare it
between Japanese and English. Furthermore in contrast to
(Fung, 1998) we use only one translation in the dictionary,
which we select by comparing the relative frequencies. We
also made a second run of the experiments where we man-
ually selected the correct translations for the first half of the
most frequent pivots ? Results did not change significantly.
20
two such random variables (Dunning, 1993), and
is known to be equal to Mutual Information that is
linearly scaled by the size of the corpus (Moore,
2004). This means it is a measure for how much
the occurrence of word A makes the occurrence
of word B more likely, which we term positive
association, and how much the absence of word
A makes the occurrence of word B more likely,
which we term negative association. However, our
experiments show that only positive association is
beneficial for aligning words cross-lingually. In
fact, LLR can still be used for extracting posi-
tive associations by filtering in a pre-processing
step words with possibly negative associations
(Moore, 2005). Nevertheless a problem which
cannot be easily remedied is that confidence es-
timates using LLR are unreliable for small sample
sizes (Moore, 2004). We suggest a more princi-
pled approach that measures from the start only
how much the occurrence of word A makes the
occurrence of word B more likely, which is des-
ignated as Robust PMI.
Another point that is common to (Rapp, 1999;
Morin et al, 2007; Chiao and Zweigenbaum,
2002; Garera et al, 2009; Gaussier et al, 2004)
is that word association is compared in a fine-
grained way, i.e. they compare the degree of asso-
ciation3 with every pivot word, even when it is low
or exceptionally high. They suggest as a compar-
ison measurement Jaccard similarity, Cosine sim-
ilarity, and the L1 (Manhattan) distance.
3 Our Approach
We presume that rather than similarity between
degree (strength of) of associations, the existence
of common word associations is a more reliable
measure for word similarity because the degrees
of association are difficult to compare for the fol-
lowing reasons:
? Small differences in the degree of associa-
tion are not statistically significant
Taking, for example, two sample sets from
3To clarify terminology, where possible, we will try to
distinguish between association and degree of association.
For example word ?car? has the association ?tire?, whereas
the degree of association with ?tire? is a continuous number,
like 5.6.
the same corpus, we will in general measure
different degrees of association.
? Differences in sub-domains / sub-topics
Corpora sharing the same topic can still dif-
fer in sub-topics.
? Differences in style or language
Differences in word usage. 4
Other information that is used in vector ap-
proaches such as that in (Rapp, 1999) is nega-
tive association, although negative association is
less informative than positive. Therefore, if it is
used at all, it should be assigned a much smaller
weight.
Our approach caters to these points, by first de-
ciding whether a pivot word is positively associ-
ated (with statistical significance) or whether it
is not, and then uses solely this information for
finding translation pairs in comparable corpora. It
is divisible into two steps. In the first, we use a
Bayesian estimated PointwiseMutual Information
(PMI) measurement to find the pivots that are pos-
itively associated with a certain word with high
confidence. In the second step, we compare two
words using their associated pivots as features.
The similarity of feature sets is calculated using
pointwise entropy. The words for which feature
sets have high similarity are assumed to be related
in meaning.
3.1 Extracting positively associated words ?
Feature Sets
To measure the degree of positive association be-
tween two words x and y, we suggest the use
of information about how much the occurrence
of word x makes the occurrence of word y more
likely. We express this using Pointwise Mutual
Information (PMI), which is defined as follows:
PMI(x, y) = log p(x, y)p(x) ? p(y) = log
p(x|y)
p(x) .
Therein, p(x) is the probability that word x oc-
curs in a document; p(y) is defined analogously.
Furthermore, p(x, y) is the probability that both
4For example, ?stop? is not the only word to describe the
fact that a car halted.
21
words occur in the same document. A positive as-
sociation is given if p(x|y) > p(x). In related
works that use the PMI (Morin et al, 2007), these
probabilities are simply estimated using relative
frequencies, as
PMI(x, y) = log
f(x,y)
n
f(x)
n
f(y)
n
,
where f(x), f(y) is the document frequency
of word x and word y, and f(x, y) is the co-
occurrence frequency; n is the number of docu-
ments. However, using relative frequencies to es-
timate these probabilities can, for low-frequency
words, produce unreliable estimates for PMI
(Manning and Schu?tze, 2002). It is therefore nec-
essary to determine the uncertainty of PMI esti-
mates. The idea of defining confidence intervals
over PMI values is not new (Johnson, 2001); how-
ever, the problem is that exact calculation is very
computationally expensive if the number of docu-
ments is large, in which case one can approximate
the binomial approximation for example with a
Gaussian, which is, however only justified if n
is large and p, the probability of an occurrence,
is not close to zero (Wilcox, 2009). We suggest
to define a beta distribution over each probabil-
ity of the binary events that word x occurs, i.e.
[x], and analogously [x|y]. It was shown in (Ross,
2003) that a Bayesian estimate for Bernoulli trials
using the beta distribution delivers good credibil-
ity intervals5, importantly, when sample sizes are
small, or when occurrence probabilities are close
to 0. Therefore, we assume that
p(x|y) ? beta(??x|y, ??x|y), p(x) ? beta(??x, ??x)
where the parameters for the two beta distribu-
tions are set to
??x|y = f(x, y) + ?x|y ,
??x|y = f(y) ? f(x, y) + ?x|y , and
??x = f(x) + ?x, ??x = n ? f(x) + ?x .
Prior information related to p(x) and the con-
ditional probability p(x|y) can be incorporated
5In the Bayesian notation we refer here to credibility in-
tervals instead of confidence intervals.
by setting the hyper-parameters of the beta-
distribtutions.6 These can, for example, be
learned from another unrelated corpora pair and
then weighted appropriately by setting ?+ ?. For
our experiments, we use no information beyond
the given corpora pair; the conditional priors are
therefore set equal to the prior for p(x). Even if
we do not know which word x is, we have a notion
about p(x) because Zipf?s law indicates to us that
we should expect it to be small. A crude estima-
tion is therefore the mean word occurrence proba-
bility in our corpus as
? = 1|all words|
?
x?{all words}
f(x)
n .
We give this estimate a total weight of one obser-
vation. That is, we set
? = ? , ? = 1 ? ? .
From a practical perspective, this can be inter-
preted as a smoothing when sample sizes are
small, which is often the case for p(x|y). Because
we assume that p(x|y) and p(x) are random vari-
ables, PMI is consequently also a random variable
that is distributed according to a beta distribution
ratio.7 For our experiments, we apply a general
sampling strategy. We sample p(x|y) and p(x) in-
dependently and then calculate the ratio of times
PMI > 0 to determine P (PMI > 0).8 We will
refer to this method as Robust PMI (RPMI).
Finally we can calculate, for any word x, the set
of pivot words which have most likely a positive
association with word x. We require that this set
be statistically significant: the probability of one
or more words being not a positive association is
smaller than a certain p-value.9
6The hyper-parameters ? and ?, can be intuitively inter-
preted in terms of document frequency. For example ?x is
the number of times we belief the word x occurs, and ?x the
number of times we belief that x does not occur in a corpus.
Analogously ?x|y and ?x|y can be interpreted with respect
to the subset of the corpus where the word y occurs, instead
of the whole corpus. Note however, that ? and ? do not nec-
essarily have to be integers.
7The resulting distribution for the general case of a beta
distribution ratio was derived in (Pham-Gia, 2000). Unfortu-
nately, it involves the calculation of a Gauss hyper-geometric
function that is computationally expensive for large n.
8For experiments, we used 100, 000 samples for each es-
timate of P (PMI > 0).
9We set, for all of our experiments, the p-value to 0.01.
22
As an alternative for determining the probabil-
ity of a positive association using P (PMI > 0),
we calculate LLR and assume that approximately
LLR ? ?2 with one degree of freedom (Dunning,
1993). Furthermore, to ensure that only positive
association counts, we set the probability to zero
if p(x, y) < p(x) ? p(y), where the probabilities
are estimated using relative frequencies (Moore,
2005). We refer to this as LLR(P); lacking this
correction, it is LLR.
3.2 Comparing Word Feature Sets Across
Corpora
So far, we have explained a robust means to ex-
tract the pivot words that have a positive associa-
tion with the query. The next task is to find a sen-
sible way to use these pivots to compare the query
with candidates from the target corpus. A simple
means to match a candidate with a query is to see
how many pivots they have in common, i.e. using
the matching coefficient (Manning and Schu?tze,
2002) to score candidates. This similarity mea-
sure produces a reasonable result, as we will show
in the experiment section; however, in our error
analysis, we found out that this gives a bias to
candidates with higher frequencies, which is ex-
plainable as follows. Assuming that a word A has
a fixed number of pivots that are positively associ-
ated, then depending on the sample size?the doc-
ument frequency in the corpus?not all of these
are statistically significant. Therefore, not all true
positive associations are included in the feature
set to avoid possible noise. If the document fre-
quency increases, then we can extract more sta-
tistically significant positive associations and the
cardinality of the feature set increases. This con-
sequently increases the likelihood of having more
pivots that overlap with pivots from the query?s
feature set. For example, imagine two candidate
words A and B, for which feature sets of both in-
clude the feature set of the query, i.e. a complete
match, howeverA?s feature set is much larger than
B?s feature set. In this case, the information con-
veyed by having a complete match with the query
word?s feature set is lower in the case of A?s fea-
ture set than in case of B?s feature set. Therefore,
we suggest its use as a basis of our similarity mea-
sure, the degree of pointwise entropy of having an
estimate of m matches, as
Information(m, q, c) = ? log(P (matches = m)).
Therein, P (matches = m) is the likelihood that a
candidate word with c pivots has m matches with
the query word, which has q pivots. Letting w be
the total number of pivot words, we can then cal-
culate that the probability that the candidate with
c pivots was selected by chance
P (matches = m) =
( q
m
)
?
(w?q
c?m
)
(w
c
) .
Note that this probability equals a hypergeometric
distribution.10 The smaller P (matches = m) is,
the less likely it is that we obtain m matches by
pure chance. In other words, if P (matches = m)
is very small, m matches are more than we would
expect to occur by pure chance.11
Alternatively, in our experiments, we also con-
sider standard similarity measurements (Manning
and Schu?tze, 2002) such as the Tanimoto coeffi-
cient, which also lowers the score of candidates
that have larger feature sets.
4 Experiments
In our experiments, we specifically examine trans-
lating nouns, mostly technical terms, which occur
in complaints about cars collected by the Japanese
Ministry of Land, Infrastructure, Transport and
Tourism (MLIT)12, and in complaints about cars
collected by the USA National Highway Traffic
Safety Administration (NHTSA)13. We create for
each data collection a corpus for which a doc-
ument corresponds to one car customer report-
ing a certain problem in free text. The com-
plaints are, in general, only a few sentences long.
10` q
m
? is the number of possible combinations of pivots
which the candidate has in common with the query. There-
fore, ` qm
?
?
`w?q
c?m
? is the number of possible different feature
sets that the candidate can have such that it sharesm common
pivots with the query. Furthermore, `wc
? is the total number
of possible feature sets the candidate can have.
11The discussion is simplified here. It can also be that
P (matches = m) is very small, if there are less occur-
rences of m that we would expect to occur by pure chance.
However, this case can be easily identified by looking at the
gradient of P (matches = m).
12http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
13http://www-odi.nhtsa.dot.gov/downloads/index.cfm
23
To verify whether our results can be generalized
over other pairs of comparable corpora, we ad-
ditionally made experiments using two corpora
extracted from articles of Mainichi Shinbun, a
Japanese newspaper, in 1995 and English articles
from Reuters in 1997. There are two notable dif-
ferences between those two pairs of corpora: the
content is much less comparable, Mainichi re-
ports more national news than world news, and
secondly, Mainichi and Reuters corpora are much
larger than MLIT/NHTSA.14
For both corpora pairs, we extracted a
gold-standard semi-automatically by looking at
Japanese nouns and their translations with docu-
ment frequency of at least 50 for MLIT/NHTSA,
and 100 for Mainichi/Reuters. As a dictionary we
used the Japanese-English dictionary JMDic15.
In general, we preferred domain-specific terms
over very general terms, i.e. for example for
MLIT/NHTSA the noun ?? ?injection? was
preferred over ???? ?installation?. We ex-
tracted 100 noun pairs for MLIT/NHTSA and
Mainichi/Reuters, each. Each Japanese noun
which is listed in the gold-standard forms a query
which is input into our system. The resulting
ranking of the translation candidates is automat-
ically evaluated using the gold-standard. There-
fore, synonyms that are not listed in the gold stan-
dard are not recognized, engendering a conserva-
tive estimation of the translation accuracy. Be-
cause all methods return a ranked list of trans-
lation candidates, the accuracy is measured us-
ing the rank of the translation listed in the gold-
standard.16 The Japanese corpora are prepro-
cessed with MeCab (Kudo et al, 2004); the En-
glish corpora with Stepp Tagger (Tsuruoka et al,
2005) and Lemmatizer (Okazaki et al, 2008). As
a dictionary we use the Japanese-English dictio-
nary JMDic17. In line with related work (Gaussier
et al, 2004), we remove a word pair (Japanese
noun s, English noun t) from the dictionary, if s
occurs in the gold-standard. Afterwards we define
14MLIT/MLIT has each 20,000 documents.
Mainichi/Reuters corpora 75,935 and 148,043 documents,
respectively.
15http://www.csse.monash.edu.au/ jwb/edict doc.html
16In cases for which there are several translations listed for
one word, the rank of the first is used.
17http://www.csse.monash.edu.au/ jwb/edict doc.html
the pivot words by consulting the remaining dic-
tionary.
4.1 Crosslingual Experiment
We compare our approach used for extract-
ing cross-lingual translation pairs against several
baselines. We compare to LLR + Manhattan
(Rapp, 1999) and our variation LLR(P) + Man-
hattan. Additionally, we compare TFIDF(MSO)
+ Cosine, which is the TFIDF measure, whereas
the Term Frequency is normalized using the max-
imal word frequency and the cosine similarity
for comparison suggested in (Fung, 1998). Fur-
thermore, we implemented two variations of this,
TFIDF(MPO) + Cosine and TFIDF(MPO) + Jac-
card coefficient, which were suggested in (Chiao
and Zweigenbaum, 2002). In fact, TFIDF(MPO)
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word pair
frequency. The results are displayed in Figure 1.
Our approach clearly outperforms all baselines;
notably it has Top 1 accuracy of 0.14 and Top 20
accuracy of 0.55, which is much better than that
for the best baseline, which is 0.11 and 0.44, re-
spectively.
experiment that are similar to those of our cross-
lingual experi ent, we use the same pivot words
and the same gold standard as that used for the
MLIT/NHTSA experiments, for which a pair (A,
translation of A) is changed to (A, A): that is, the
word becomes the translation of itself. The result
of the monolingual experiment in Table 2 shows
that our method performs slightly worse than the
baseline, LLR + Manhattan, i.e. LLR with L1 nor-
malization and L1 distance(Rapp, 1999). Further-
more, LLR(P) + Manhattan using only positive as-
sociations also performs slightly worse.
Top 1 Top 10 Top 20
LLR + Manhattan 0.94 0.99 0.99
LLR(P) + Man attan 0.89 1.0 1.0
RPMI + Entropy 0.79 0.94 0.95
Table 2: Monolingual NHTSA experiment.
In our main experiment, we compare our ap-
proach used for extracting cross-lingual transla-
ti n pairs ag inst seve al baselines. As before,
we compare LLR + Manhattan (Rapp, 1999) and
the variation LLR(P) + Manhattan. Addition-
ally, we compare TFIDF(MSO) + Cosine, which
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word fre-
quency and the cosine similarity for comparison
suggested in (Fung, 1998). Furthermore, we im-
plemented two variations of this, TFIDF(MPO) +
Cosine and TFIDF(MPO) + Jaccard coefficient,
which were suggested in (Chiao and Zweigen-
baum, 2002). In fact, TFIDF(MPO) is the TFIDF
measure, whereas the Term Frequency is normal-
ized using the maximal word pair frequency.14
The results are displayed in Figure 1. Our ap-
proach clearly outperforms all baselines; notably
it has top 1 accuracy of 0.14 and top 20 accuracy
of 0.55, which is much better than that for the best
baseline, which is 0.11 and 0.44, respectively.
We next leave the proposed framework con-
stant, but change the mode of estimating positive
associations and the way to match feature sets.
As alternatives for estimating the probability that
there is a positive association, we test LLR(P) and
LLR. As alternatives for comparing feature sets,
we investigate the matching coefficient (match-
ing), cosine similarity (cosine), Tanimoto coeffi-
14We tried, like originally suggested, using maximum
count of every occurring word pair, i.e. (content word, con-
tent word), but using maximum of all pairs (content word,
pivot word) improves always slightly accuracy. Therefore for
we chose the latter as a baseline.
?
??
??
??
??
??
??
? ? ?? ?? ?? ??
???????????????????????????????????????????????????
Figure 1: Percentile ranking of our approach
RPMI + Entropy against various previous sug-
gested methods.
cient (tani), and overlap coefficient (over) (Man-
ning and Schu?tze, 2002). The result of every com-
bination is displayed concisely in Table 3 using the
median rank. In our experience, the median rank
is a good choice of measure of location for our
problem because we have, in general, a skewed
distribution over the ranks. The cases in which
the median ranks are close to RPMI + entropy are
magnified in 4.
It is readily apparent that most alternatives per-
form clearly worse. Looking at Table 4, we can
see that only RPMI + Entropy, and LLR(P) +
Entropy, perform similar. Pointwise entropy in-
creases the accuracy (Top 1) over the matching
coefficient and is clearly superior to other similar-
ity measures. Overlap similarity performs well in
contrast to other standard measurements because
other measures punish words with a high number
of associated pivots too severely. However, our
approach of using pointwise entropy as a measure
of similarity performs best because it more ade-
quately punishes words with a high number of as-
sociated pivots. Finally, LLR(P) presents a clear
edge over LLR, which suggests that indeed only
positive associations seem to matter in a cross-
lingual setting.
Entropy Matching Cosine Tani Over
RPMI 13.0 17.0 24.0, 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 3: Evaluation Matrix
Finally, we aim to clarify whether these re-
sults are specific to a certain type of compara-
ble corpora pair or if they hold more generally.
Therefore, we conduct the same experiments us-
ing the very different comparable corpora pair
Mainichi/Reuters. When comparing to the best
Figure 1: Crosslingual Experiment
MLIT/NHTSA ? Percentile Ranking of RPMI
+ Entropy Against Various Previous Suggested
Methods.
We next leave the proposed framework con-
stant, but change the mode of estimating posi-
tive associations and the way to match feature
sets. As alternatives for estimating the proba-
bility that there is a positive association, we test
LLR(P) and LLR. As alternatives for comparing
feature sets, we investigate the matching coef-
ficient (match), cosine similarity (cosine), Tan-
imoto coefficient (tani), and overlap coefficient
24
(over) (Manning and Schu?tze, 2002). The re-
sult of every combination is displayed concisely
in Table 1 using the median rank18. The cases
in which the median ranks are close to RPMI +
Entropy are magnified in Table 2. We can see
there that RPMI + Entropy, and LLR(P) + En-
tropy perform nearly equally. All other combina-
tions perform worse, especially in Top 1 accuracy.
Finally, LLR(P) presents a clear edge over LLR,
which suggests that indeed only positive associa-
tions seem to matter in a cross-lingual setting.
Entropy Match Cosine Tani Over
RPMI 13.0 17.0 24.0 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 1: Crosslingual experiment MLIT/NHTSA
? Evaluation matrix showing the median ranks of
several combinations of association and similarity
measures.
Top 1 Top 10 Top 20
RPMI + Entropy 0.14 0.46 0.55
RPMI + Matching 0.08 0.41 0.57
LLR(P) + Entropy 0.14 0.46 0.55
LLR(P) + Matching 0.08 0.44 0.55
Table 2: Accuracies for crosslingual experiment
MLIT/NHTSA.
Finally we conduct an another experiment using
the corpora pair Mainichi/Reuters which is quite
different from MLIT/NHTSA. When comparing
to the best baselines in Table 3 we see that our
approach again performs best. Furthermore, the
experiments displayed in Table 4 suggest that Ro-
bust PMI and pointwise entropy are better choices
for positive association measurement and similar-
ity measurement, respectively. We can see that
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
LLR(P) + Manhattan 0.10 0.26 0.33
TFIDF(MPO) + Cos 0.05 0.12 0.18
Table 3: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to best baselines.
18A median rank of i, means that 50% of the correct trans-
lations have a rank higher than i.
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
RPMI + Matching 0.08 0.30 0.35
LLR(P) + Entropy 0.13 0.36 0.47
LLR(P) + Matching 0.08 0.29 0.37
Table 4: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to alternatives.
the overall best baseline turns out to be LLR(P) +
Manhattan. Comparing the rank from each word
from the gold-standard pairwise, we see that our
approach, RPMI + Entropy, is significantly better
than this baseline in MLIT/NHTSA as well as in
Mainichi/Reuters.19
4.2 Analysis
In this section, we provide two representative ex-
amples extracted from the previous experiments
which sheds light into a weakness of the stan-
dard feature vector approach which was used as a
baseline before. The two example queries and the
corresponding responses of LLR(P) + Manhattan
and our approach are listed in Table 5. Further-
more in Table 6 we list the pivot words with the
highest degree of association (here LLR values)
for the query and its correct translation. We can
see that a query and its translation shares some
pivots which are associated with statistical signif-
icance20. However it also illustrates that the ac-
tual LLR value is less insightful and can hardly be
compared across these two corpora.
Let us analyze the two examples in more de-
tail. In Table 6, we see that the first query ??
?gear?21 is highly associated with??? ?shift?.
However, on the English side we see that gear is
most highly associated with the pivot word gear.
Note that here the word gear is also a pivot word
corresponding to the Japanese pivot word ??
?gear (wheel)?.22 Since in English the word gear
(shift) and gear (wheel) is polysemous, the surface
forms are the same leading to a high LLR value of
19Using pairwise test with p-value 0.05.
20Note that for example, an LLR value bigger than 11.0
means the chances that there is no association is smaller than
0.001 using that LLR ? ?2.
21For a Japanese word, we write the English translation
which is appropriate in our context, immediately after it.
22In other words, we have the entry (??, gear) in our
dictionary but not the entry (??, gear). The first pair is
used as a pivot, the latter word pair is what we try to find.
25
gear. Finally, the second example query ???
?pedal? shows that words which, not necessarily
always, but very often co-occur, can cause rela-
tively high LLR values. The Japanese verb ??
?to press? is associated with ??? with a high
LLR value ? 4 times higher than ?? ?return?
? which is not reflected on the English side. In
summary, we can see that in both cases the degree
of associations are rather different, and cannot be
compared without preprocessing. However, it is
also apparent that in both examples a simple L1
normalization of the degree of associations does
not lead to more similarity, since the relative dif-
ferences remain.
?? ?gear?
Method Top 3 candidates Rank
baseline jolt, lever, design 284
filtering reverse, gear, lever 2
??? ?pedal?
Method Top 3 candidates Rank
baseline mj, toyota, action 176
filtering pedal, situation, occasion 1
Table 5: List of translation suggestions using
LLR(P) + Manhattan (baseline) and our method
(filtering). The third column shows the rank of
the correct translation.
?? gear
Pivots LLR(P) Pivots LLR(P)
?? ?shift? 154 gear 7064
??? ?shift? 144 shift 1270
??? ?come out? 116 reverse 314
??? pedal
Pivots LLR(P) Pivots LLR(P)
?? ?press? 628 floor 1150
?? ?return? 175 stop 573
? ?foot? 127 press 235
Table 6: Shows the three pivot words which have
the highest degree of association with the query
(left side) and the correct translation (right side).
5 Conclusions
We introduced a new method to compare con-
text similarity across comparable corpora using a
Bayesian estimate for PMI (Robust PMI) to ex-
tract positive associations and a similarity mea-
surement based on the hypergeometric distribu-
tion (measuring pointwise entropy). Our experi-
ments show that, for finding cross-lingual trans-
lations, the assumption that words with similar
meaning share positive associations with the same
words is more appropriate than the assumption
that the degree of association is similar. Our ap-
proach increases Top 1 and Top 20 accuracy of
up to 50% and 39% respectively, when compared
to several previous methods. We also analyzed
the two components of our method separately. In
general, Robust PMI yields slightly better per-
formance than the popular LLR, and, in contrast
to LLR, allows to extract positive associations as
well as to include prior information in a principled
way. Pointwise entropy for comparing feature sets
cross-lingually improved the translation accuracy
clearly when compared with standard similarity
measurements.
Acknowledgment
We thank Dr. Naoaki Okazaki and the anony-
mous reviewers for their helpful comments. Fur-
thermore we thank Daisuke Takuma, IBM Re-
search - Tokyo, for mentioning previous work
on statistical corrections for PMI. This work was
partially supported by Grant-in-Aid for Specially
Promoted Research (MEXT, Japan). The first au-
thor is supported by the MEXT Scholarship and
by an IBM PhD Scholarship Award.
References
Chiao, Y.C. and P. Zweigenbaum. 2002. Looking
for candidate translational equivalents in special-
ized, comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics, pages 1?5. International Committee on Com-
putational Linguistics.
De?jean, H., E?. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
Proceedings of the International Conference on
Computational Linguistics, pages 1?7. International
Committee on Computational Linguistics.
Dunning, T. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Fung, P. 1998. A statistical view on bilingual
lexicon extraction: from parallel corpora to non-
parallel corpora. Lecture Notes in Computer Sci-
ence, 1529:1?17.
26
Garera, N., C. Callison-Burch, and D. Yarowsky.
2009. Improving translation lexicon induction from
monolingual corpora via dependency contexts and
part-of-speech equivalences. In Proceedings of the
Conference on Computational Natural Language
Learning, pages 129?137. Association for Compu-
tational Linguistics.
Gaussier, E., J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 526?
533. Association for Computational Linguistics.
Haghighi, A., P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 771?779. Association for Computa-
tional Linguistics.
Johnson, M. 2001. Trading recall for precision with
confidence-sets. Technical report, Brown Univer-
sity.
Koehn, P. and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, volume 34, pages 9?16. Association for Com-
putational Linguistics.
Kudo, T., K. Yamamoto, and Y. Matsumoto. 2004.
Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 230?237. Association for Com-
putational Linguistics.
Manning, C.D. and H. Schu?tze. 2002. Foundations
of Statistical Natural Language Processing. MIT
Press.
Moore, R.C. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 333?340. Association for
Computational Linguistics.
Moore, R.C. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Morin, E., B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining-using brain,
not brawn comparable corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, pages 664?671. As-
sociation for Computational Linguistics.
Okazaki, N., Y. Tsuruoka, S. Ananiadou, and J. Tsu-
jii. 2008. A discriminative candidate generator for
string transformations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 447?456. Association for Com-
putational Linguistics.
Pham-Gia, T. 2000. Distributions of the ratios of in-
dependent beta variables and applications. Com-
munications in Statistics. Theory and Methods,
29(12):2693?2715.
Rapp, R. 1999. Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 519?526. Association for Computational Lin-
guistics.
Ross, T.D. 2003. Accurate confidence intervals for
binomial proportion and Poisson rate estimation.
Computers in Biology and Medicine, 33(6):509?
531.
Tsuruoka, Y., Y. Tateishi, J. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. De-
veloping a robust part-of-speech tagger for biomed-
ical text. Lecture Notes in Computer Science,
3746:382?392.
Wilcox, R.R. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Ox-
ford University Press.
27
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779?787,
Beijing, August 2010
Evaluating Dependency Representation for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo
2School of Computer Science, University of Manchester
3National Center for Text Mining
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The detailed analyses of sentence struc-
ture provided by parsers have been applied
to address several information extraction
tasks. In a recent bio-molecular event ex-
traction task, state-of-the-art performance
was achieved by systems building specif-
ically on dependency representations of
parser output. While intrinsic evalua-
tions have shown significant advances in
both general and domain-specific pars-
ing, the question of how these translate
into practical advantage is seldom con-
sidered. In this paper, we analyze how
event extraction performance is affected
by parser and dependency representation,
further considering the relation between
intrinsic evaluation and performance at
the extraction task. We find that good
intrinsic evaluation results do not always
imply good extraction performance, and
that the types and structures of differ-
ent dependency representations have spe-
cific advantages and disadvantages for the
event extraction task.
1 Introduction
Advanced syntactic parsing methods have been
shown to effective for many information extrac-
tion tasks. The BioNLP 2009 Shared Task, a re-
cent bio-molecular event extraction task, is one
such task: analysis showed that the application of
a parser correlated with high rank in the task (Kim
et al, 2009). The automatic extraction of bio-
molecular events from text is important for a num-
ber of advanced domain applications such as path-
way construction, and event extraction thus a key
task in Biomedical Natural Language Processing
(BioNLP).
Methods building feature representations and
extraction rules around dependency representa-
tions of sentence syntax have been successfully
applied to a number of tasks in BioNLP. Several
parsers and representations have been applied in
high-performing methods both in domain studies
in general and in the BioNLP?09 shared task in
particular, but no direct comparison of parsers or
representations has been performed. Likewise,
a number of evaluation of parser outputs against
gold standard corpora have been performed in the
domain, but the broader implications of the results
of such intrinsic evaluations are rarely considered.
The BioNLP?09 shared task involved documents
contained also in the GENIA treebank (Tateisi et
al., 2005), creating an opportunity for direct study
of intrinsic and task-oriented evaluation results.
As the treebank can be converted into various de-
pendency formats using existing format conver-
sion methods, evaluation can further be extended
to cover the effects of different representations.
In this this paper, we consider three types of de-
pendency representation and six parsers, evaluat-
ing their performance from two different aspects:
dependency-based intrinsic evaluation, and effec-
tiveness for bio-molecular event extraction with a
state-of-the-art event extraction system. Compar-
ison of intrinsic and task-oriented evaluation re-
779
	








	
 


 
	

 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788?796,
Beijing, August 2010
Entity-Focused Sentence Simplification for Relation Extraction
Makoto Miwa1 Rune S?tre1 Yusuke Miyao2 Jun?ichi Tsujii1,3,4
1Department of Computer Science, The University of Tokyo
2National Institute of Informatics
3School of Computer Science, University of Manchester
4National Center for Text Mining
mmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,
yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jp
Abstract
Relations between entities in text have
been widely researched in the natu-
ral language processing and information-
extraction communities. The region con-
necting a pair of entities (in a parsed
sentence) is often used to construct ker-
nels or feature vectors that can recognize
and extract interesting relations. Such re-
gions are useful, but they can also incor-
porate unnecessary distracting informa-
tion. In this paper, we propose a rule-
based method to remove the information
that is unnecessary for relation extraction.
Protein?protein interaction (PPI) is used
as an example relation extraction problem.
A dozen simple rules are defined on out-
put from a deep parser. Each rule specif-
ically examines the entities in one target
interaction pair. These simple rules were
tested using several PPI corpora. The PPI
extraction performance was improved on
all the PPI corpora.
1 Introduction
Relation extraction (RE) is the task of finding a
relevant semantic relation between two given tar-
get entities in a sentence (Sarawagi, 2008). Some
example relation types are person?organization
relations (Doddington et al, 2004), protein?
protein interactions (PPI), and disease?gene as-
sociations (DGA) (Chun et al, 2006). Among
the possible RE tasks, we chose the PPI extrac-
tion problem. PPI extraction is a major RE task;
around 10 corpora have been published for train-
ing and evaluation of PPI extraction systems.
Recently, machine-learning methods, boosted
by NLP techniques, have proved to be effec-
tive for RE. These methods are usually intended
to highlight or select the relation-related regions
in parsed sentences using feature vectors or ker-
nels. The shortest paths between a pair of enti-
ties (Bunescu and Mooney, 2005) or pair-enclosed
trees (Zhang et al, 2006) are widely used as focus
regions. These regions are useful, but they can in-
clude unnecessary sub-paths such as appositions,
which cause noisy features.
In this paper, we propose a method to remove
information that is deemed unnecessary for RE.
Instead of selecting the whole region between
a target pair, the target sentence is simplified
into simpler, pair-related, sentences using general,
task-independent, rules. By addressing particu-
larly the target entities, the rules do not affect im-
portant relation-related expressions between the
target entities. We show how rules of two groups
can be easily defined using the analytical capabil-
ity of a deep parser with specific examination of
the target entities. Rules of the first group can re-
place a sentence with a simpler sentence, still in-
cluding the two target entities. The other group of
rules can replace a large region (phrase) represent-
ing one target entity, with just a simple mention of
that target entity. With only a dozen simple rules,
we show that we can solve several simple well-
known problems in RE, and that we can improve
the performance of RE on all corpora in our PPI
test-set.
788
2 Related Works
The general paths, such as the shortest path or
pair-enclosed trees (Section 1), can only cover
a part of the necessary information for relation
extraction. Recent machine-learning methods
specifically examine how to extract the missing
information without adding too much noise. To
find more representative regions, some informa-
tion from outside the original regions must be
included. Several tree kernels have been pro-
posed to extract such regions from the parse
structure (Zhang et al, 2006). Also the graph
kernel method emphasizes internal paths with-
out ignoring outside information (Airola et al,
2008). Composite kernels have been used to com-
bine original information with outside informa-
tion (Zhang et al, 2006; Miwa et al, 2009).
The approaches described above are useful,
but they can include unnecessary information that
distracts learning. Jonnalagadda and Gonzalez
(2009) applied bioSimplify to relation extraction.
BioSimplify is developed to improve their link
grammar parser by simplifying the target sentence
in a general manner, so their method might re-
move important information for a given target re-
lation. For example, they might accidentally sim-
plify a noun phrase that is needed to extract the
relation. Still, they improved overall PPI extrac-
tion recall using such simplifications.
To remove unnecessary information from a sen-
tence, some works have addressed sentence sim-
plification by iteratively removing unnecessary
phrases. Most of this work is not task-specific;
it is intended to compress all information in a tar-
get sentence into a few words (Dorr et al, 2003;
Vanderwende et al, 2007). Among them, Vickrey
and Koller (2008) applied sentence simplification
to semantic role labeling. With retaining all argu-
ments of a verb, Vickrey simplified the sentence
by removing some information outside of the verb
and arguments.
3 Entity-Focused Sentence
Simplification
We simplify a target sentence using simple rules
applicable to the output of a deep parser called
Mogura (Matsuzaki et al, 2007), to remove noisy
information for relation extraction. Our method
relies on the deep parser; the rules depend on the
Head-driven Phrase Structure Grammar (HPSG)
used by Mogura, and all the rules are written for
the parser Enju XML output format. The deep
parser can produce deep syntactic and semantic
information, so we can define generally applica-
ble comprehensive rules on HPSG with specific
examination of the entities.
For sentence simplification in relation extrac-
tion, the meaning of the target sentence itself is
less important than maintaining the truth-value of
the relation (interact or not). For that purpose,
we define rules of two groups: clause-selection
rules and entity-phrase rules. A clause-selection
rule constructs a simpler sentence (still includ-
ing both target entities) by removing noisy infor-
mation before and after the relevant clause. An
entity-phrase rule simplifies an entity-containing
region without changing the truth-value of the re-
lation. By addressing the target entities particu-
larly, we can define rules for many applications,
and we can simplify target sentences with less
danger of losing relation-related mentions. The
rules are summarized in Table 1.
Our method is different from the sentence sim-
plification in other systems (ref. Section 2). First,
our method relies on the parser, while bioSimplify
by Jonnalagadda and Gonzalez (2009) is devel-
oped for the improvement of their parser. Second,
our method tries to keep only the relation-related
regions, unlike other general systems including
bioSimplify which tried to keep all information in
a sentence. Third, our entity-phrase rules modify
only the entity-containing phrases, while Vickrey
and Koller (2008) tries to remove all information
outside of the target verb and arguments.
3.1 Clause-selection Rules
In compound or complex sentences, it is natural
to assume that one clause includes both the target
entities and the relation-related information. It can
also be assumed that the remaining sentence parts,
outside the clause, contain less related (or noisy)
information. The clause-selection rules simplify a
sentence by retaining only the clause that includes
the target entities (and by discarding the remain-
der of the sentence). We define three types of
789
Rule Group Rule Type # Example (original? simplified )
Sentence Clause 1 We show that A interacts with B.? A interacts with B.
Clause Selection Relative Clause 2 ... A that interacts with B.? A interacts with B.
Copula 1 A is a protein that interacts with B.? A interacts with B.
Apposition 2 a protein, A? A
Entity Phrase Exemplification 4 proteins, such as A? AParentheses 2 a protein (A)? A
Coordination 3 protein and A? A
Table 1: Rules for Sentence Simplification. (# is the rule count. A and B are the target entities.)
(a) S
bbbbbbb \\\\\\\
... VP
bbbbbbb \\\\\\\
N*
ccccc [[[[[
Vcc
77
(copular) ...
bbbbbbb \\\\\\\
... ENTITY ... N* S-REL
bbbbbbb \\\\\\\
NP-REL
NN
...
ccccc [[[[[
... ENTITY ...
A is a protein that interacts with B .
(b) S
bbbbbbb \\\\\\\
N*
ccccc [[[[[
...
ccccc [[[[[
... ENTITY ... ... ENTITY ...
A interacts with B .
Figure 1: Copula Rule. (a) is simplified to (b).
The arrows represent predicate?argument rela-
tions.
(a) N*
bbbbbbb \\\\\\\
N* ...
bbbbbbb ]]]]]]]]]]]]]
PN
RR
55(apposition) N*
ccccc [[[[[
... ENTITY ...
protein , A
(b) N*
ccccc [[[[[
... ENTITY ...
A
Figure 2: Apposition Rule.
clause-selection rules for sentence clauses, rela-
tive clauses, and copula. The sentence clause rule
finds the (smallest) clause that includes both tar-
get entities. It then replaces the original sentence
with the clause. The relative clause rules con-
struct a simple sentence from a relative clause and
the antecedent. If this simple sentence includes
the target entities, it is used instead of the orig-
inal sentence. We define two rules for the case
where the antecedent is the subject of the relative
clause. One rule is used when the relative clause
includes both the target entities. The other rule is
used when the antecedent includes one target en-
tity and the relative clause includes the other tar-
get entity. The copula rule is for sentences that
include copular verbs (e.g. be, is, become, etc).
The rule constructs a simple sentence from a rel-
ative clause with the subject of the copular verb
as the antecedent subject of the clause. The rule
replaces the target sentence with the constructed
sentence, if the relative clause includes one target
entity and the subject of a copular verb includes
the other target entity, as shown in Figure 1.
3.2 Entity-phrase Rules
Even the simple clauses (or paths between two
target entities) include redundant or noisy expres-
sions that can distract relation extraction. Some
of these expressions are related to the target enti-
ties, but because they do not affect the truth-value
of the relation, they can be deleted to make the
path simple and clear. The target problem affects
which expressions can be removed. We define
four types of rules for appositions, exemplifica-
tions, parentheses, and coordinations. Two appo-
sition rules are defined to select the correct ele-
ment from an appositional expression. One ele-
ment modifies or defines the other element in ap-
position, but the two elements represent the same
information from the viewpoint of PPI. If the tar-
get entity is in one of these elements, removing the
other element does not affect the truth-value of the
interaction. Many of these apposition expressions
are identified by the deep parser. The rule to se-
lect the last element is presented in Figure 2. Four
exemplification rules are defined for the two ma-
jor types of expressions using the phrases ?includ-
ing? or ?such as?. Exemplification is represented
by hyponymy or hypernymy. As for appositions,
the truth-value of the interaction does not change
whether we use the specific mention or the hyper-
class that the mention represents. Two parenthe-
ses rules are defined. Parentheses are useful for
synonyms, hyponyms, or hypernyms (ref. the two
790
1: S ? input sentence
2: repeat
3: reset rules {apply all the rules again}
4: P ? parse S
5: repeat
6: r ? next rule {null if no more rules}
7: if r is applicable to P then
8: P ? apply r to P
9: S ? sentence extracted from P
10: break (Goto 3)
11: end if
12: until r is null
13: until r is null
14: return S
Figure 3: Pseudo-code for sentence simplifica-
tion.
former rules). Three coordination rules are de-
fined. Removing other phrases from coordinated
expressions that include a target entity does not
affect the truth-value of the target relation. Two
rules are defined for simple coordination between
two phrases (e.g. select left or right phrase), and
one rule is defined to (recursively) remove one
element from lists of more than two coordinated
phrases (while maintaining the coordinating con-
junction, e.g. ?and?).
3.3 Sentence Simplification
To simplify a sentence, we apply rules repeatedly
until no more applications are possible as pre-
sented in Figure 3. After one application of one
rule, the simplified sentence is re-parsed before
attempting to apply all the rules again. This is be-
cause we require a consistent parse tree as a start-
ing point for additional applications of the rules,
and because a parser can produce more reliable
output for a partly simplified sentence than for the
original sentence. Using this method, we can also
backtrack and seek out conversion errors by exam-
ining the cascade of partly simplified sentences.
4 Evaluation
To elucidate the effect of the sentence simplifi-
cation, we applied the rules to five PPI corpora
and evaluated the PPI extraction performance. We
then analyzed the errors. The evaluation settings
will be explained in Section 4.1. The results of the
PPI extraction will be explained in Section 4.2. Fi-
nally, the deeper analysis results will be presented
in Section 4.3.
4.1 Experimental Settings
The state-of-the-art PPI extraction system
AkaneRE by Miwa et al (2009) was used to
evaluate our approach. The system uses a com-
bination of three feature vectors: bag-of-words
(BOW), shortest path (SP), and graph features.
Classification models are trained with a support
vector machine (SVM), and AkaneRE (with
Mogura) is used with default parameter settings.
The following two systems are used for a state-
of-the-art comparison: AkaneRE with multiple
parsers and corpora (Miwa et al, 2009), and
Airola et al (2008) single-parser, single-corpus
system.
The rules were evaluated on the BioIn-
fer (Pyysalo et al, 2007), AIMed (Bunescu et al,
2005), IEPA (Ding et al, 2002), HPRD50 (Fun-
del et al, 2006), and LLL (Ne?dellec, 2005) cor-
pora1. Table 2 shows the number of positive (in-
teracting) vs. all pairs. One duplicated abstract in
the AIMed corpus was removed.
These corpora have several differences in their
definition of entities and relations (Pyysalo et al,
2008). In fact, BioInfer and AIMed target al oc-
curring entities related to the corpora (proteins,
genes, etc). On the other hand, IEPA, HPRD50,
and LLL only use limited named entities, based
either on a list of entity names or on a named en-
tity recognizer. Only BioInfer is annotated for
other event types in addition to PPI, including
static relations such as protein family member-
ship. The sentence lengths are also different. The
duplicated pair-containing sentences contain the
following numbers of words on average: 35.8 in
BioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 in
HPRD50, and 33.4 in LLL.
For BioInfer, AIMed, and IEPA, each corpus is
split into training, development, and test datasets2.
The training dataset from AIMed was the only
training dataset used for validating the rules. The
development datasets are used for error analysis.
The evaluation was done on the test dataset, with
models trained using training and development
1http://mars.cs.utu.fi/PPICorpora/
GraphKernel.html
2This split method will be made public later.
791
BioInfer AIMed IEPA HPRD50 LLL
pos all pos all pos all pos all pos all
training 1,848 7,108 684 4,072 256 630 - - - -
development 256 928 102 608 23 51 - - - -
test 425 1,618 194 1,095 56 136 - - - -
all 2,534 9,653 980 5,775 335 817 163 433 164 330
Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.
BioInfer AIMed IEPA
Rule Applied F AUC Applied F AUC Applied F AUC
No Application 0 62.5 83.0 0 61.2 87.9 0 73.4 82.5
Clause Selection 4,313 63.5 83.9 2,569 62.5 88.2 307 75.0 83.7
Entity Phrase 22,066 60.5 80.9 7,784 61.2 86.1 1,031 72.7 83.3
ALL 26,281 62.9 82.1 10,783 60.2 85.7 1,343 75.4 85.7
Table 3: Performance of PPI Extraction on test datasets. ?Applied? represents the number of times the
rules are applied on the corpus. ?No Application? means PPI extraction without sentence simplification.
ALL is the case all rules are used. The top scores for each corpus are shown in bold.
datasets). Ten-fold cross-validation (CV) was
done to facilitate comparison with other existing
systems. For HPRD50 and LLL, there are insuf-
ficient examples to split the data, so we use these
corpora only for comparing the scores and statis-
tics. We split the corpora for the CV, and mea-
sured the F -score (%) and area under the receiver
operating characteristic (ROC) curve (AUC) as
recommended in (Airola et al, 2008). We count
each occurrence as one example because the cor-
rect interactions must be extracted for each occur-
rence if the same protein name occurs multiple
times in a sentence.
In the experiments, the rules are applied in the
following order: sentence?clause, exemplifica-
tion, apposition, parentheses, coordination, cop-
ula, and relative-clause rules. Furthermore, if the
same rule is applicable in different parts of the
parse tree, then the rule is first applied closest to
the leaf-nodes (deepest first). The order of the
rules is arbitrary; changing it does not affect the
results much. We conducted five experiments us-
ing the training and development dataset in IEPA,
each time with a random shuffling of the order of
the rules; the results were 77.8?0.26 in F -score
and 85.9?0.55 in AUC.
4.2 Performance of PPI Extraction
The performance after rule application was bet-
ter than the baseline (no application) on all the
corpora, and most rules could be frequently ap-
plied. We show the PPI extraction performance on
Rule Applied F AUC
No Application 0 72.9 84.5
Sentence Clause 145 71.6 83.8
Relative Clause 7 73.3 84.1
Copula 0 72.9 84.5
Clause Selection 152 71.4 83.4
Apposition 64 73.2 84.6
Exemplification 33 72.9 84.7
Parentheses 90 72.9 85.1
Coordination 417 73.6 85.4
Entity Phrase 605 74.1 86.6
ALL 763 75.0 86.6
Table 4: Performance of PPI Extraction on
HPRD50.
Rule Applied F AUC
No Application 0 79.0 84.6
Sentence Clause 135 81.3 85.2
Relative Clause 42 78.8 84.6
Copula 0 79.0 84.6
Clause Selection 178 81.0 85.6
Apposition 197 79.6 83.9
Exemplification 0 79.0 84.6
Parentheses 56 79.5 85.8
Coordination 322 84.2 89.4
Entity Phrase 602 83.8 90.1
ALL 761 82.9 90.5
Table 5: Performance of PPI Extraction on LLL.
BioInfer, AIMed, and IEPA with rules of different
groups in Table 3. The effect of using rules of
different types for PPI extraction from HPRD50
and LLL is reported in Table 4 and Table 5. Ta-
ble 6 shows the number of times each rule was
applied in an ?apply all-rules? experiment. The
usability of the rules depends on the corpus, and
different combinations of rules produce different
792
Rule B AIMed IEPA H LLL
S. Cl. 3,960 2,346 300 150 135
R. Cl. 287 212 17 5 24
Copula 60 57 1 0 0
Cl. Sel. 4,307 2,615 318 155 159
Appos. 3,845 1,100 99 69 198
Exempl. 383 127 11 33 0
Paren. 2,721 2,158 235 91 88
Coord. 15,025 4,783 680 415 316
E. Foc. 21,974 8,168 1,025 608 602
Sum 26,281 10,783 1,343 763 761
Table 6: Distribution of the number of rules ap-
plied when all rules are applied. B:BioInfer, and
H:HPRD50 corpora.
Rules Miwa et al Airola et al
F AUC F AUC F AUC
B 60.0 79.8 68.3 86.4 61.3 81.9
A 54.9 83.7 65.2 89.3 56.4 84.8
I 77.8 88.7 76.6 87.8 75.1 85.1
H 75.0 86.6 74.9 87.9 63.4 79.7
L 82.9 90.5 86.7 90.8 76.8 83.4
Table 7: Comparison with the results by Miwa et
al. (2009) and Airola et al (2008). The results
with all rules are reported.
results. For the clause-selection rules, the per-
formance was as good as or better than the base-
line for all corpora, except for HPRD50, which
indicates that the pair-containing clauses also in-
clude most of the important information for PPI
extraction. Clause selection rules alone could im-
prove the overall performance for the BioInfer and
AIMed corpora. Entity-phrase rules greatly im-
proved the performance on the IEPA, HPRD50,
and LLL corpora, although these rules degraded
the performance on the BioInfer and AIMed cor-
pora. These phenomena hold even if we use small
parts of the two corpora, so this is not because of
the size of the corpora.
We compare our results with the results by
Miwa et al (2009) and Airola et al (2008) in Ta-
ble 7. On three of five corpora, our method pro-
vides better results than the state-of-the-art system
by Airola et al (2008), and also provides com-
parable results to those obtained using multiple
parsers and corpora (Miwa et al, 2009) despite
the fact that our method uses one parser and one
corpus at a time. We cannot directly compare our
result with Jonnalagadda and Gonzalez (2009) be-
cause the evaluation scheme, the baseline system,
[FP?TN][Sentence, Parenthesis, Coordination] To
characterize the AAV functions mediating this effect,
cloned AAV type 2 wild-type or mutant genomes were
transfected into simian virus 40 (SV40)-transformed
hamster cells together with the six HSV replication genes
(encoding UL5, UL8, major DNA-binding protein, DNA
polymerase, UL42 , and UL52) which together are
necessary and sufficient for the induction of SV40 DNA
amplification (R. Heilbronn and H. zur Hausen, J. Virol.
63:3683-3692, 1989). (BioInfer.d760.s0)
[TP?FN][Coordination] Both the GT155-calnexin and
the GT155-CAP-60 interactions were dependent on the
presence of a correctly modified oligosaccharide group
on GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)
[TN?TN][Coordination, Parenthesis] Leptin may act as
a negative feedback signal to the hypothalamic control of
appetite through suppression of neuropeptide Y (NPY)
secretion and stimulation of cocaine and amphetamine
regulated transcript (CART) . (IEPA.d190.s454)
Figure 4: A rule-related error, a critical error, and
a parser-related error. Regions removed by the
rules are underlined, and target proteins are shown
in bold. Predictions, applied rules, and sentence
IDs are shown.
[FN?TP][Sentence, Coordination] WASp contains a
binding motif for the Rho GTPase CDC42Hs as well as
verprolin / cofilin-like actin-regulatory domains , but no
specific actin structure regulated by CDC42Hs-WASp has
been identified. (BioInfer.d795.s0)
[FN?TP][Parenthesis, Apposition] The protein Raf-1 , a
key mediator of mitogenesis and differentiation, associates
with p21ras (refs 1-3) . (AIMed.d124.s1055)
[FN?TP][Sentence, Parenthesis] On the basis of
far-Western blot and plasmon resonance (BIAcore)
experiments, we show here that recombinant bovine
prion protein (bPrP) (25-242) strongly interacts with the
catalytic alpha/alpha? subunits of protein kinase CK2
(also termed ?casein kinase 2?) (IEPA.d197.s479)
Figure 5: Correctly simplified cases. The first
sentence is a difficult (not PPI) relation, which is
typed as ?Similar? in the BioInfer corpus.
and test parts differ.
4.3 Analysis
We trained models using the training datasets
and classified the examples in the development
datasets. Two types of analysis were performed
based on these results: simplification-based and
classification-based analysis.
For the simplification-based analysis, we com-
pared positive (interacting) and negative pair sen-
tences that produce the exact same (inconsistent)
sentence after protein names normalization and
793
BioInfer AIMed IEPA
Before simplification FN FP TP TN FN FP TP TN FN FP TP TN Not AffectedAfter simplification TP TN FN FP TP TN FN FP TP TN FN FP
No Error 18 2 3 35 14 21 21 8 3 2 0 4 32
No Application 3 2 0 3 0 7 8 0 0 1 0 1 7
Number of Errors 0 2 0 32 4 2 1 4 0 0 0 0 1
Number of Pairs 21 6 3 70 18 30 30 12 3 3 0 5 40
Coordination 0 0 0 20 4 2 1 0 0 0 0 0 1
Sentence 0 2 0 4 0 0 0 4 0 0 0 0 0
Parenthesis 0 0 0 5 0 0 0 0 0 0 0 0 0
Exemplification 0 0 0 2 0 0 0 0 0 0 0 0 0
Apposition 0 0 0 1 0 0 0 0 0 0 0 0 0
Table 8: Distribution of sentence simplification errors compared to unsimplified predictions with their
types (on the three development datasets). TP, True Positive; TN, True Negative; FN, False Negative;
FP, False Positive. ?No Error? means that simplification was correct; ?No Application? means that no
rule could be applied; Other rule names mean that an error resulted from that rule application. ?Not
Affected? means that the prediction outcome did not change.
simplification in the training dataset. The numbers
of such inconsistent sentences are 7 for BioIn-
fer, 78 for AIMed, and 1 for IEPA. The few in-
consistencies in BioInfer and IEPA are from er-
rors by the rules, mainly triggered by parse errors.
The frequent inconsistencies in AIMed are mostly
from inconsistent annotations. For example, even
if all coordinated proteins are either interacting or
not, only the first protein mention is annotated as
interacting.
For the classification-based analysis, we
specifically examine simplified pairs that were
predicted differently before and after the simplifi-
cation. Pairs predicted differently before and after
rule application were selected: 100 random pairs
from BioInfer and all 90 pairs from AIMed. For
IEPA, all 51 pairs are reported. Simplified results
are classified as errors when the rules affect a re-
gion unrelated to the entities in the smallest sen-
tence clause. The results of analysis are shown in
Table 8. There were 34 errors in BioInfer, and 11
errors in AIMed. Among the errors, there were
five critical errors (in two sentences, in AIMed).
Critical errors mean that the pairs lost relation-
related mentions, and the errors are the only er-
rors which caused the changes in the truth-value
of the relation. There was also a rule-related er-
ror (in BioInfer), which means that rules with cor-
rect parse results affect a region unrelated to the
entities, and parse errors (parser-related errors).
Figure 4 shows the rule-related error in BioInfer,
one critical error in AIMed, and one parser-related
error in IEPA.
5 Discussion
Our end goal is to provide consistent relation
extraction for real tasks. Here we discuss the
?safety? of applying our simplification rules, the
difficulties in the BioInfer and AIMed corpora, the
reduction of errors, and the requirements for such
a general (PPI) extraction system.
Our rules are applicable to sentences, with little
danger of changing the relation-related mentions.
Figure 5 shows three successfully simplified cases
(?No Error? cases from Table 8). The sentence
simplification leaves sufficient information to de-
termine the value of the relation in these exam-
ples. Relation-related mentions remained for most
of the simplification error cases. There were only
five critical errors, which changed the truth-value
of the relation, out of 46 errors in 241 pairs shown
in Table 8. Please note that some rules can be
dangerous for other relation extraction tasks. For
example, the sentence clause rule could remove
modality information (negation, speculation, etc.)
modifying the clause, but there are few such cases
in the PPI corpora (see Table 8). Also, the task of
hedge detection (Morante and Daelemans, 2009)
can be solved separately, in the original sentences,
after the interacting pairs have been found. For
example, in the BioNLP shared task challenge
and the BioInfer corpus, interaction detection and
modality are treated as two different tasks. Once
other NLP tasks, like static relation (Pyysalo et
794
al., 2009) or coreference resolution, become good
enough, they can supplement or even substitute
some of the proposed rules.
There are different difficulties in the BioInfer
and AIMed corpora. BioInfer includes more com-
plicated sentences and problems than the other
corpora do, because 1) the apposition, coordi-
nation, and exemplification rules are more fre-
quently used in the BioInfer corpus than in the
other corpora (shown in Table 6), 2) there were
more errors in the BioInfer corpus than in other
corpora among the simplified sentences (shown
in Table 8), and 3) BioInfer has more words per
sentence and more relation types than the other
corpora. AIMed contains several annotation in-
consistencies as explained in Section 4.3. These
inconsistencies must be removed to properly eval-
uate the effect of our method.
Simplification errors are mostly caused by
parse errors. Our rule specifically examines a part
of parser output; a probability is attached to the
part. The probability is useful for defining the or-
der of rule applications, and the n-best results by
the parser are useful to fix major errors such as co-
ordination errors. By using these modifications of
rule applications and by continuous improvement
in parsing technology for the biomedical domain,
the performance on the BioInfer and AIMed cor-
pora will be improved also for the all rules case.
The PPI extraction system lost the ability to
capture some of the relation-related expressions
left by the simplification rules. This indicates
that the system used to extract some relations (be-
fore simplification) by using back-off features like
bag-of-words. The system can reduce bad effects
caused by parse errors, but it also captures the an-
notation inconsistencies in AIMed. Our simpli-
fication (without errors) can capture more general
expressions needed for relation extraction. To pro-
vide consistent PPI relation extraction in a general
setting (e.g. for multiple corpora or for other pub-
lic text collections), the parse errors must be dealt
with, and a relation extraction system that can cap-
ture (only) general relation-related expressions is
needed.
6 Conclusion
We proposed a method to simplify sentences, par-
ticularly addressing the target entities for relation
extraction. Using a few simple rules applicable
to the output of a deep parser called Mogura,
we showed that sentence simplification is effec-
tive for relation extraction. Applying all the rules
improved the performance on three of the five
corpora, while applying only the clause-selection
rules raised the performance for the remaining two
corpora as well. We analyzed the simplification
results, and showed that the simple rules are ap-
plicable with little danger of changing the truth-
values of the interactions.
The main contributions of this paper are: 1) ex-
planation of general sentence simplification rules
using HPSG for relation extraction, 2) presenting
evidence that application of the rules improve re-
lation extraction performance, and 3) presentation
of an error analysis from two viewpoints: simpli-
fication and classification results.
As future work, we are planning to refine and
complete the current set of rules, and to cover
the shortcomings of the deep parser. Using these
rules, we can then make better use of the parser?s
capabilities. We will also attempt to apply our
simplification rules to other relation extraction
problems than those of PPI.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientific Research (C) (General) (MEXT,
Japan).
795
References
Airola, Antti, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of the BioNLP 2008 work-
shop.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
724?731.
Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dictio-
naries and machine learning. In The Pacific Sympo-
sium on Biocomputing (PSB), pages 4?15.
Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) program: Tasks, data, and evalua-
tion. In Proceedings of LREC?04, pages 837?840.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In in Proceedings of Work-
shop on Automatic Summarization, pages 1?8.
Fundel, Katrin, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine, pages 109?114, November.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and cfg-filtering. In IJCAI?07: Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 1671?1676, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Miwa, Makoto, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, June.
Morante, Roser and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Ne?dellec, Claire. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Pyysalo, Sampo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece
in the biomedical information extraction puzzle.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1(3):261?
377.
Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606?1618.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 825?832. Association for
Computational Linguistics.
796
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851?859,
Beijing, August 2010
Simple and Efficient Algorithm
for Approximate Dictionary Matching
Naoaki Okazaki
University of Tokyo
okazaki@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
University of Tokyo
University of Manchester
National Centre for Text Mining
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a simple and effi-
cient algorithm for approximate dictio-
nary matching designed for similarity
measures such as cosine, Dice, Jaccard,
and overlap coefficients. We propose this
algorithm, called CPMerge, for the ? -
overlap join of inverted lists. First we
show that this task is solvable exactly by
a ? -overlap join. Given inverted lists re-
trieved for a query, the algorithm collects
fewer candidate strings and prunes un-
likely candidates to efficiently find strings
that satisfy the constraint of the ? -overlap
join. We conducted experiments of ap-
proximate dictionary matching on three
large-scale datasets that include person
names, biomedical names, and general
English words. The algorithm exhib-
ited scalable performance on the datasets.
For example, it retrieved strings in 1.1
ms from the string collection of Google
Web1T unigrams (with cosine similarity
and threshold 0.7).
1 Introduction
Languages are sufficiently flexible to be able to
express the same meaning through different dic-
tion. At the same time, inconsistency of surface
expressions has persisted as a serious problem in
natural language processing. For example, in the
biomedical domain, cardiovascular disorder can
be described using various expressions: cardio-
vascular diseases, cardiovascular system disor-
der, and disorder of the cardiovascular system. It
is a nontrivial task to find the entry from these sur-
face expressions appearing in text.
This paper addresses approximate dictionary
matching, which consists of finding all strings in
a string collection V such that they have similar-
ity that is no smaller than a threshold ? with a
query string x. This task has a broad range of ap-
plications, including spelling correction, flexible
dictionary look-up, record linkage, and duplicate
detection (Henzinger, 2006; Manku et al, 2007).
Formally, the task obtains a subset Yx,? ? V ,
Yx,? = {y ? V
?? sim(x, y) ? ?}, (1)
where sim(x, y) presents the similarity between x
and y. A na??ve solution to this task is to com-
pute similarity values |V | times, i.e., between x
and every string y ? V . However, this solution
is impractical when the number of strings |V | is
huge (e.g., more than one million).
In this paper, we present a simple and effi-
cient algorithm for approximate dictionary match-
ing designed for similarity measures such as co-
sine, Dice, Jaccard, and overlap coefficients. Our
main contributions are twofold.
1. We show that the problem of approximate
dictionary matching is solved exactly by a
? -overlap join (Sarawagi and Kirpal, 2004)
of inverted lists. Then we present CPMerge,
which is a simple and efficient algorithm for
the ? -overlap join. In addition, the algorithm
is easily implemented.
2. We demonstrate the efficiency of the al-
gorithm on three large-scale datasets with
person names, biomedical concept names,
851
and general English words. We com-
pare the algorithm with state-of-the-art al-
gorithms, including Locality Sensitive Hash-
ing (Ravichandran et al, 2005; Andoni and
Indyk, 2008) and DivideSkip (Li et al,
2008). The proposed algorithm retrieves
strings the most rapidly, e.g., in 1.1 ms from
Google Web1T unigrams (with cosine simi-
larity and threshold 0.7).
2 Proposed Method
2.1 Necessary and sufficient conditions
In this paper, we assume that the features of a
string are represented arbitrarily by a set. Al-
though it is important to design a string represen-
tation for an accurate similarity measure, we do
not address this problem: our emphasis is not on
designing a better representation for string simi-
larity but on establishing an efficient algorithm.
The most popular representation is given by n-
grams: all substrings of size n in a string. We
use trigrams throughout this paper as an example
of string representation. For example, the string
?methyl sulphone? is expressed by 17 elements
of letter trigrams1, {?$$m?, ?$me?, ?met?,
?eth?, ?thy?, ?hyl?, ?yl ?, ?l s?, ? su?,
?sul?, ?ulp?, ?lph?, ?pho?, ?hon?,
?one?, ?ne$?, ?e$$?}. We insert two $s be-
fore and after the string to denote the start or end
of the string. In general, a string x consisting of
|X| letters yields (|x| + n ? 1) elements of n-
grams. We call |x| and |X| the length and size,
respectively, of the string x.
Let X and Y denote the feature sets of the
strings x and y, respectively. The cosine similarity
between the two strings x and y is,
cosine(X,Y ) = |X ? Y |?
|X||Y |
. (2)
By integrating this definition with Equation 1, we
obtain the necessary and sufficient condition for
1In practice, we attach ordinal numbers to n-grams to rep-
resent multiple occurrences of n-grams in a string (Chaud-
huri et al, 2006). For example, the string ?prepress?, which
contains two occurrences of the trigram ?pre?, yields
the set {?$$p?#1, ?$pr?#1, ?pre?#1, ?rep?#1,
?epr?#1, ?pre?#2, ?res?#1, ?ess?#1, ?ss$?#1,
?s$$?#1}.
Table 1: Conditions for each similarity measure
Measure min |Y | max |Y | ?(= min |X ? Y |)
Dice ?2?? |X| 2??? |X| 12?(|X|+ |Y |)
Jaccard ?|X| |X|/? ?(|X|+|Y |)1+?
Cosine ?2|X| |X|/?2 ??|X||Y |
Overlap ? ? ?min{|X|, |Y |}
approximate dictionary matching,
?
?
?
|X||Y |
?
? |X ? Y | ? min{|X|, |Y |}.
(3)
This inequality states that two strings x and y must
have at least ? =
?
?
?
|X||Y |
?
features in com-
mon. When ignoring |X?Y | in the inequality, we
have an inequality about |X| and |Y |,
?
?2|X|
?
? |Y | ?
? |X|
?2
?
(4)
This inequality presents the search range for re-
trieving similar strings; that is, we can ignore
strings whose feature size is out of this range.
Other derivations are also applicable to similar-
ity measures, including Dice, Jaccard, and overlap
coefficients. Table 1 summarizes the conditions
for these similarity measures.
We explain one usage of these conditions. Let
query string x = ?methyl sulphone? and thresh-
old for approximate dictionary matching ? = 0.7
with cosine similarity. Representing the strings
with letter trigrams, we have the size of x, |X| =
17. The inequality 4 gives the search range of |Y |
of the retrieved strings, 9 ? |Y | ? 34. Presum-
ing that we are searching for strings of |Y | = 16,
we obtain the necessary and sufficient condition
for the approximate dictionary matching from the
inequality 3, ? = 12 ? |X ? Y |. Thus, we need
to search for strings that have at least 12 letter tri-
grams that overlap with X . When considering a
string y = ?methyl sulfone?, which is a spelling
variant of y (ph ? f), we confirm that the string
is a solution for approximate dictionary matching
because |X?Y | = 13 (? ? ). Here, the actual sim-
ilarity is cosine(X,Y ) = 13/?17? 16 = 0.788
(? ?).
2.2 Data structure and algorithm
Algorithm 1 presents the pseudocode of the ap-
proximate dictionary matching based on Table 1.
852
Input: V : collection of strings
Input: x: query string
Input: ?: threshold for the similarity
Output: Y: list of strings similar to the query
X ? string to feature(x);1
Y ?[];2
for l? min y(|X|, ?) to max y(|X|, ?) do3
? ? min overlap(|X|, l, ?);4
R? overlapjoin(X , ? , V , l);5
foreach r ? R do append r to Y;6
end7
return Y;8
Algorithm 1: Approximate dictionary
matching.
Given a query string x, a collection of strings V ,
and a similarity threshold ?, the algorithm com-
putes the size range (line 3) given by Table 1.
For each size l in the range, the algorithm com-
putes the minimum number of overlaps ? (line 4).
The function overlapjoin (line 5) finds sim-
ilar strings by solving the following problem (? -
overlap join): given a list of features of the query
string X and the minimum number of overlaps ? ,
enumerate strings of size l in the collection V such
that they have at least ? feature overlaps with X .
To solve this problem efficiently, we build an
inverted index that stores a mapping from the fea-
tures to their originating strings. Then, we can
perform the ? -overlap join by finding strings that
appear at least ? times in the inverted lists re-
trieved for the query features X .
Algorithm 2 portrays a na??ve solution for the
? -overlap join (AllScan algorithm). In this algo-
rithm, function get(V , l, q) returns the inverted
list of strings (of size l) for the feature q. In
short, this algorithm scans strings in the inverted
lists retrieved for the query features X , counts the
frequency of occurrences of every string in the
inverted lists, and returns the strings whose fre-
quency of occurrences is no smaller than ? .
This algorithm is inefficient in that it scans
all strings in the inverted lists. The number of
scanned strings is large, especially when some
query features appear frequently in the strings,
e.g., ?s$$? (words ending with ?s?) and ?pre?
(words with substring ?pre?). To make matters
worse, such features are too common for charac-
terizing string similarity. The AllScan algorithm
Input: X: array of features of the query string
Input: ? : minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
M ? {};1
R? [];2
foreach q ? X do3
foreach i ? get(V , l, q) do4
M [i]?M [i] + 1;5
if ? ?M [i] then6
append i to R;7
end8
end9
end10
return R;11
Algorithm 2: AllScan algorithm.
is able to maintain numerous candidate strings in
M , but most candidates are not likely to qualified
because they have few overlaps with X .
To reduce the number of the candidate strings,
we refer to signature-based algorithms (Arasu et
al., 2006; Chaudhuri et al, 2006):
Property 1 Let there be a set (of size h) X and a
set (of any size) Y . Consider any subset Z ? X of
size (h? ? +1). If |X ?Y | ? ? , then Z ?Y 6= ?.
We explain one usage of this property. Let query
string x = ?methyl sulphone? and its trigram set
X be features (therefore, |X| = h = 17). Pre-
suming that we seek strings whose trigrams are
size 16 and have 12 overlaps withX , then string y
must have at least one overlap with any subset of
size 6 (= 17 ? 12 + 1) of X . We call the subset
signatures. The property leads to an algorithmic
design by which we obtain a small set of candi-
date strings from the inverted lists for signatures,
(|X| ? ? + 1) features in X , and verify whether
each candidate string satisfies the ? overlap with
the remaining (? ? 1) n-grams.
Algorithm 3 presents the pseudocode employ-
ing this idea. In line 1, we arrange the features in
X in ascending order of the number of strings in
their inverted lists. We denote the k-th element in
the ordered features as Xk (k ? {0, ..., |X| ? 1}),
where the index number begins with 0. Based on
this notation,X0 andX|X|?1 are the most uncom-
mon and the most common features in X , respec-
tively.
In lines 2?7, we use (|X| ? ? + 1) features
853
Input: X: array of features of the query string
Input: ? : minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
sort elements in X by order of |get(V , l, Xk)|;1
M ? {};2
for k ? 0 to (|X| ? ?) do3
foreach s ? get(V , l, Xk) do4
M [s]?M [s] + 1;5
end6
end7
R? [];8
for k ? (|X| ? ? + 1) to (|X| ? 1) do9
foreach s ?M do10
if bsearch(get(V , l, Xk), s) then11
M [s]?M [s] + 1;12
end13
if ? ?M [s] then14
append s to R;15
remove s from M ;16
else if M [s] + (|X| ? k ? 1) < ? then17
remove s from M ;18
end19
end20
end21
return R;22
Algorithm 3: CPMerge algorithm.
X0, ..., X|X|?? to generate a compact set of can-
didate strings. The algorithm stores the occur-
rence count of each string s in M [s]. In lines 9?
21, we increment the occurrence counts if each
of X|X|??+1, ..., X|X|?1 inverted lists contain the
candidate strings. For each string s in the candi-
dates (line 10), we perform a binary search on the
inverted list (line 11), and increment the overlap
count if the string s exists (line 12). If the overlap
counter of the string reaches ? (line 14), then we
append the string s to the result list R and remove
s from the candidate list (lines 15?16). We prune
a candidate string (lines 17?18) if the candidate is
found to be unreachable for ? overlaps even if it
appears in all of the unexamined inverted lists.
3 Experiments
We report the experimental results of approximate
dictionary matching on large-scale datasets with
person names, biomedical names, and general En-
glish words. We implemented various systems of
approximate dictionary matching.
? Proposed: CPMerge algorithm.
? Naive: Na??ve algorithm that computes the
cosine similarity |V | times for every query.
? AllScan: AllScan algorithm.
? Signature: CPMerge algorithm without
pruning; this is equivalent to Algorithm 3
without lines 17?18.
? DivideSkip: our implementation of the algo-
rithm (Li et al, 2008)2.
? Locality Sensitive Hashing (LSH) (Andoni
and Indyk, 2008): This baseline system fol-
lows the design of previous work (Ravichan-
dran et al, 2005). This system approxi-
mately solves Equation 1 by finding dictio-
nary entries whose LSH values are within
the (bit-wise) hamming distance of ? from
the LSH value of a query string. To adapt
the method to approximate dictionary match-
ing, we used a 64-bit LSH function com-
puted with letter trigrams. By design, this
method does not find an exact solution to
Equation 1; in other words, the method can
miss dictionary entries that are actually sim-
ilar to the query strings. This system has
three parameters, ?, q (number of bit permu-
tations), and B (search width), to control the
tradeoff between retrieval speed and recall3.
Generally speaking, increasing these param-
eters improves the recall, but slows down the
speed. We determined ? = 24 and q = 24
experimentally4, and measured the perfor-
mance when B ? {16, 32, 64}.
The systems, excluding LSH, share the same
implementation of Algorithm 1 so that we can
specifically examine the differences of the algo-
rithms for ? -overlap join. The C++ source code of
the system used for this experiment is available5.
We ran all experiments on an application server
running Debian GNU/Linux 4.0 with Intel Xeon
5140 CPU (2.33 GHz) and 8 GB main memory.
2We tuned parameter values ? ? {0.01, 0.02, 0.04, 0.1,
0.2, 0.4, 1, 2, 4, 10, 20, 40, 100} for each dataset. We se-
lected the parameter with the fastest response.
3We followed the notation of the original pa-
per (Ravichandran et al, 2005) here. Refer to the original
paper for definitions of the parameters ?, q, and B.
4q was set to 24 so that the arrays of shuffled hash values
are stored in memory. We chose ? = 24 from {8, 16, 24} be-
cause it showed a good balance between accuracy and speed.
5http://www.chokkan.org/software/simstring/
854
3.1 Datasets
We used three large datasets with person names
(IMDB actors), general English words (Google
Web1T), and biomedical names (UMLS).
? IMDB actors: This dataset comprises actor
names extracted from the IMDB database6.
We used all actor names (1,098,022 strings;
18 MB) from the file actors.list.gz.
The average number of letter trigrams in the
strings is 17.2. The total number of trigrams
is 42,180. The system generated index files
of 83 MB in 56.6 s.
? Google Web1T unigrams: This dataset con-
sists of English word unigrams included in
the Google Web1T corpus (LDC2006T13).
We used all word unigrams (13,588,391
strings; 121 MB) in the corpus after remov-
ing the frequency information. The aver-
age number of letter trigrams in the strings
is 10.3. The total number of trigrams is
301,459. The system generated index files
of 601 MB in 551.7 s.
? UMLS: This dataset consists of English
names and descriptions of biomedical con-
cepts included in the Unified Medical Lan-
guage System (UMLS). We extracted all
English concept names (5,216,323 strings;
212 MB) from MRCONSO.RRF.aa.gz and
MRCONSO.RRF.ab.gz in UMLS Release
2009AA. The average number of letter tri-
grams in the strings is 43.6. The total number
of trigrams is 171,596. The system generated
index files of 1.1 GB in 1216.8 s.
For each dataset, we prepared 1,000 query
strings by sampling strings randomly from the
dataset. To simulate the situation where query
strings are not only identical but also similar to
dictionary entries, we introduced random noise
to the strings. In this experiment, one-third of
the query strings are unchanged from the original
(sampled) strings, one-third of the query strings
have one letter changed, and one-third of the
query strings have two letters changed. When
changing a letter, we randomly chose a letter po-
sition from a uniform distribution, and replaced
6ftp://ftp.fu-berlin.de/misc/movies/database/
the letter at the position with an ASCII letter ran-
domly chosen from a uniform distribution.
3.2 Results
To examine the scalability of each system, we
controlled the number of strings to be indexed
from 10%?100%, and issued 1,000 queries. Fig-
ure 1 portrays the average response time for re-
trieving strings whose cosine similarity values are
no smaller than 0.7. Although LSH (B=16) seems
to be the fastest in the graph, this system missed
many true positives7; the recall scores of approx-
imate dictionary matching were 15.4% (IMDB),
13.7% (Web1T), and 1.5% (UMLS). Increasing
the parameterB improves the recall at the expense
of the response time. LSH (B=64)8. It not only
ran slower than the proposed method, but also
suffered from low recall scores, 25.8% (IMDB),
18.7% (Web1T), and 7.1% (UMLS). LSH was
useful only when we required a quick response
much more than recall.
The other systems were guaranteed to find
the exact solution (100% recall). The proposed
algorithm was the fastest of all exact systems
on all datasets: the response times per query
(100% index size) were 1.07 ms (IMDB), 1.10 ms
(Web1T), and 20.37 ms (UMLS). The response
times of the Na??ve algorithm were too slow, 32.8 s
(IMDB), 236.5 s (Web1T), and 416.3 s (UMLS).
The proposed algorithm achieved substantial
improvements over the AllScan algorithm: the
proposed method was 65.3 times (IMDB), 227.5
times (Web1T), and 13.7 times (UMLS) faster
than the Na??ve algorithm. We observed that the
Signature algorithm, which is Algorithm 3 with-
out lines 17?18, did not perform well: The Sig-
nature algorithm was 1.8 times slower (IMDB),
2.1 times faster (Web1T), and 135.0 times slower
(UMLS) than the AllScan algorithm. These re-
sults indicate that it is imperative to minimize the
number of candidates to reduce the number of
binary-search operations. The proposed algorithm
was 11.1?13.4 times faster than DivideSkip.
Figure 2 presents the average response time
7Solving Equation 1, all systems are expected to retrieve
the exact set of strings retrieved by the Na??ve algorithm.
8The response time of LSH (B=64) on the IMDB dataset
was 29.72 ms (100% index size).
855
05
10
15
20
25
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
0
10
20
30
40
50
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
LSH (B=64)
0
10
20
30
40
50
60
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
(a) IMDB actors (b) Google Web1T unigrams (c) UMLS
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
LSH (B=64)
Figure 1: Average response time for processing a query (cosine similarity; ? = 0.7).
0
5
10
15
20
25
30
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
0
10
20
30
40
50
60
70
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
0
50
100
150
200
250
300
350
400
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
(a) IMDB actors (b) Google Web1T unigram (c) UMLS
Figure 2: Average response time for processing a query.
of the proposed algorithm for different similarity
measures and threshold values. When the similar-
ity threshold is lowered, the algorithm runs slower
because the number of retrieved strings |Y| in-
creases exponentially. The Dice coefficient and
cosine similarity produced similar curves.
Table 2 summarizes the run-time statistics of
the proposed method for each dataset (with co-
sine similarity and threshold 0.7). Using the
IMDB dataset, the proposed method searched for
strings whose size was between 8.74 and 34.06;
it retrieved 4.63 strings per query string. The
proposed algorithm scanned 279.7 strings in 4.6
inverted lists to obtain 232.5 candidate strings.
The algorithm performed a binary search on 4.3
inverted lists containing 7,561.8 strings in all.
In contrast, the AllScan algorithm had to scan
16,155.1 strings in 17.7 inverted lists and con-
sidered 9,788.7 candidate strings, and found only
4.63 similar strings.
This table clearly demonstrates three key con-
tributions of the proposed algorithm for efficient
approximate dictionary matching. First, the pro-
posed algorithm scanned far fewer strings than did
the AllScan algorithm. For example, to obtain
candidate strings in the IMDB dataset, the pro-
posed algorithm scanned 279.7 strings, whereas
the AllScan algorithm scanned 16,155.1 strings.
Therefore, the algorithm examined only 1.1%?
3.5% of the strings in the entire inverted lists in
the three datasets. Second, the proposed algo-
rithm considered far fewer candidates than did
the AllScan algorithm: the number of candidate
strings considered by the algorithm was 1.2%?
6.6% of those considered by the AllScan algo-
rithm. Finally, the proposed algorithm read fewer
inverted lists than did the AllScan algorithm. The
proposed algorithm actually read 8.9 (IMDB), 6.0
(Web1T), and 31.7 (UMLS) inverted lists during
the experiments9. These values indicate that the
proposed algorithm can solve ? -overlap join prob-
lems by checking only 50.3% (IMDB), 53.6%
(Web1T), and 51.9% of the total inverted lists re-
9These values are 4.6 + 4.3, 3.1 + 2.9, and 14.3 + 17.4.
856
Table 2: Run-time statistics of the proposed algorithm for each dataset
Averaged item IMDB Web1T UMLS Description
min |y| 8.74 5.35 21.87 minimum size of trigrams of target strings
max |y| 34.06 20.46 88.48 maximum size of trigrams of target strings
? 14.13 9.09 47.77 minimum number of overlaps required/sufficient per query
|Y| 4.63 3.22 111.79 number of retrieved strings per query
Total ? averaged for each query and target size:
# inverted lists 17.7 11.2 61.1 number of inverted lists retrieved for a query
# strings 16 155.1 52 557.6 49 561.4 number of strings in the inverted list
# unique strings 9 788.7 44 834.6 17 457.5 number of unique strings in the inverted list
Candidate stage ? averaged for each query and target size:
# inverted lists 4.6 3.1 14.3 number of inverted lists scanned for generating candidates
# strings 279.7 552.7 1 756.3 number of strings scanned for generating candidates
# candidates 232.5 523.7 1 149.7 number of candidates generated for a query
Validation stage ? averaged for each query and target size:
# inverted lists 4.3 2.9 17.4 number of inverted lists examined by binary search for a query
# strings 7 561.8 19 843.6 20 443.7 number of strings targeted by binary search
trieved for queries.
4 Related Work
Numerous studies have addressed approximate
dictionary matching. The most popular configu-
ration uses n-grams as a string representation and
the edit distance as a similarity measure. Gra-
vano et al (1998; 2001) presented various filter-
ing strategies, e.g., count filtering, position fil-
tering, and length filtering, to reduce the num-
ber of candidates. Kim et al (2005) proposed
two-level n-gram inverted indices (n-Gram/2L) to
eliminate the redundancy of position information
in n-gram indices. Li et al (2007) explored the
use of variable-length grams (VGRAMs) for im-
proving the query performance. Lee et al (2007)
extended n-grams to include wild cards and de-
veloped algorithms based on a replacement semi-
lattice. Xiao et al (2008) proposed the Ed-Join
algorithm, which utilizes mismatching n-grams.
Several studies addressed different paradigms
for approximate dictionary matching. Bocek et
al. (2007) presented the Fast Similarity Search
(FastSS), an enhancement of the neighborhood
generation algorithms, in which multiple variants
of each string record are stored in a database.
Wang et al (2009) further improved the technique
of neighborhood generation by introducing parti-
tioning and prefix pruning. Huynh et al (2006)
developed a solution to the k-mismatch problem
in compressed suffix arrays. Liu et al (2008)
stored string records in a trie, and proposed a
framework called TITAN. These studies are spe-
cialized for the edit distance measure.
A few studies addressed approximate dictio-
nary matching for similarity measures such as
cosine and Jaccard similarities. Chaudhuri et
al. (2006) proposed the SSJoin operator for sim-
ilarity joins with several measures including the
edit distance and Jaccard similarity. This algo-
rithm first generates signatures for strings, finds
all pairs of strings whose signatures overlap,
and finally outputs the subset of these candi-
date pairs that satisfy the similarity predicate.
Arasu et al (2006) addressed signature schemes,
i.e., methodologies for obtaining signatures from
strings. They also presented an implementation of
the SSJoin operator in SQL. Although we did not
implement this algorithm in SQL, it is equivalent
to the Signature algorithm in Section 3.
Sarawagi and Kirpal (2004) proposed the Mer-
geOpt algorithm for the ? -overlap join to approx-
imate string matching with overlap, Jaccard, and
cosine measures. This algorithm splits inverted
lists for a given query A into two groups, S and
L, maintains a heap to collect candidate strings on
S, and performs a binary search on L to verify the
condition of the ? -overlap join for each candidate
string. Their subsequent work includes an effi-
cient algorithm for the top-k search of the overlap
join (Chandel et al, 2006).
Li et al (2008) extended this algorithm to the
SkipMerge and DivideSkip algorithms. The Skip-
Merge algorithm uses a heap to compute the ? -
overlap join on entire inverted lists A, but has
an additional mechanism to increment the fron-
857
tier pointers of inverted lists efficiently based on
the strings popped most recently from the heap.
Consequently, SkipMerge can reduce the number
of strings that are pushed to the heap. Similarly
to the MergeOpt algorithm, DivideSkip splits in-
verted lists A into two groups S and L, but it ap-
plies SkipMerge to S. In Section 3, we reported
the performance of DivideSkip.
Charikar (2002) presented the Locality Sen-
sitive Hash (LSH) function (Andoni and Indyk,
2008), which preserves the property of cosine
similarity. The essence of this function is to map
strings into N -bit hash values where the bitwise
hamming distance between the hash values of two
strings approximately corresponds to the angle of
the two strings. Ravichandran et al (2005) ap-
plied LSH to the task of noun clustering. Adapting
this algorithm to approximate dictionary match-
ing, we discussed its performance in Section 3.
Several researchers have presented refined sim-
ilarity measures for strings (Winkler, 1999; Cohen
et al, 2003; Bergsma and Kondrak, 2007; Davis et
al., 2007). Although these studies are sometimes
regarded as a research topic of approximate dic-
tionary matching, they assume that two strings for
the target of similarity computation are given; in
other words, it is out of their scope to find strings
in a large collection that are similar to a given
string. Thus, it is a reasonable approach for an ap-
proximate dictionary matching to quickly collect
candidate strings with a loose similarity threshold,
and for a refined similarity measure to scrutinize
each candidate string for the target application.
5 Conclusions
We present a simple and efficient algorithm for
approximate dictionary matching with the co-
sine, Dice, Jaccard, and overlap measures. We
conducted experiments of approximate dictio-
nary matching on large-scale datasets with person
names, biomedical names, and general English
words. Even though the algorithm is very sim-
ple, our experimental results showed that the pro-
posed algorithm executed very quickly. We also
confirmed that the proposed method drastically re-
duced the number of candidate strings considered
during approximate dictionary matching. We be-
lieve that this study will advance practical NLP
applications for which the execution time of ap-
proximate dictionary matching is critical.
An advantage of the proposed algorithm over
existing algorithms (e.g., MergeSkip) is that it
does not need to read all the inverted lists retrieved
by query n-grams. We observed that the proposed
algorithm solved ? -overlap joins by checking ap-
proximately half of the inverted lists (with cosine
similarity and threshold ? = 0.7). This charac-
teristic is well suited to processing compressed
inverted lists because the algorithm needs to de-
compress only half of the inverted lists. It is nat-
ural to extend this study to compressing and de-
compressing inverted lists for reducing disk space
and for improving query performance (Behm et
al., 2009).
Acknowledgments
This work was partially supported by Grants-
in-Aid for Scientific Research on Priority Areas
(MEXT, Japan) and for Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
Andoni, Alexandr and Piotr Indyk. 2008. Near-
optimal hashing algorithms for approximate nearest
neighbor in high dimensions. Communications of
the ACM, 51(1):117?122.
Arasu, Arvind, Venkatesh Ganti, and Raghav Kaushik.
2006. Efficient exact set-similarity joins. In VLDB
?06: Proceedings of the 32nd International Confer-
ence on Very Large Data Bases, pages 918?929.
Behm, Alexander, Shengyue Ji, Chen Li, and Jiaheng
Lu. 2009. Space-constrained gram-based indexing
for efficient approximate string search. In ICDE
?09: Proceedings of the 2009 IEEE International
Conference on Data Engineering, pages 604?615.
Bergsma, Shane and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
ACL ?07: Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 656?663.
Bocek, Thomas, Ela Hunt, and Burkhard Stiller. 2007.
Fast similarity search in large dictionaries. Tech-
nical Report ifi-2007.02, Department of Informatics
(IFI), University of Zurich.
858
Chandel, Amit, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE ?06: Proceed-
ings of the 22nd International Conference on Data
Engineering.
Charikar, Moses S. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC ?02:
Proceedings of the thiry-fourth annual ACM sym-
posium on Theory of computing, pages 380?388.
Chaudhuri, Surajit, Venkatesh Ganti, and Raghav
Kaushik. 2006. A primitive operator for similar-
ity joins in data cleaning. In ICDE ?06: Proceed-
ings of the 22nd International Conference on Data
Engineering.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73?78.
Davis, Jason V., Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In ICML ?07: Proceedings of the
24th International Conference on Machine Learn-
ing, pages 209?216.
Gravano, Luis, Panagiotis G. Ipeirotis, H. V. Jagadish,
Nick Koudas, S. Muthukrishnan, and Divesh Srivas-
tava. 2001. Approximate string joins in a database
(almost) for free. In VLDB ?01: Proceedings of the
27th International Conference on Very Large Data
Bases, pages 491?500.
Henzinger, Monika. 2006. Finding near-duplicate
web pages: a large-scale evaluation of algorithms.
In SIGIR ?06: Proceedings of the 29th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 284?
291.
Huynh, Trinh N. D., Wing-Kai Hon, Tak-Wah Lam,
and Wing-Kin Sung. 2006. Approximate string
matching using compressed suffix arrays. Theoreti-
cal Computer Science, 352(1-3):240?249.
Kim, Min-Soo, Kyu-Young Whang, Jae-Gil Lee, and
Min-Jae Lee. 2005. n-Gram/2L: a space and time
efficient two-level n-gram inverted index structure.
In VLDB ?05: Proceedings of the 31st International
Conference on Very Large Data Bases, pages 325?
336.
Lee, Hongrae, Raymond T. Ng, and Kyuseok Shim.
2007. Extending q-grams to estimate selectivity of
string matching with low edit distance. In VLDB
?07: Proceedings of the 33rd International Confer-
ence on Very Large Data Bases, pages 195?206.
Li, Chen, Bin Wang, and Xiaochun Yang. 2007.
Vgram: improving performance of approximate
queries on string collections using variable-length
grams. In VLDB ?07: Proceedings of the 33rd In-
ternational Conference on Very Large Data Bases,
pages 303?314.
Li, Chen, Jiaheng Lu, and Yiming Lu. 2008. Effi-
cient merging and filtering algorithms for approx-
imate string searches. In ICDE ?08: Proceedings
of the 2008 IEEE 24th International Conference on
Data Engineering, pages 257?266.
Liu, Xuhui, Guoliang Li, Jianhua Feng, and Lizhu
Zhou. 2008. Effective indices for efficient approxi-
mate string search and similarity join. In WAIM ?08:
Proceedings of the 2008 The Ninth International
Conference on Web-Age Information Management,
pages 127?134.
Manku, Gurmeet Singh, Arvind Jain, and Anish
Das Sarma. 2007. Detecting near-duplicates for
web crawling. In WWW ?07: Proceedings of the
16th International Conference on World Wide Web,
pages 141?150.
Navarro, Gonzalo and Ricardo Baeza-Yates. 1998. A
practical q-gram index for text retrieval allowing er-
rors. CLEI Electronic Journal, 1(2).
Ravichandran, Deepak, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 622?629.
Sarawagi, Sunita and Alok Kirpal. 2004. Efficient
set joins on similarity predicates. In SIGMOD ?04:
Proceedings of the 2004 ACM SIGMOD interna-
tional conference on Management of data, pages
743?754.
Wang, Wei, Chuan Xiao, Xuemin Lin, and Chengqi
Zhang. 2009. Efficient approximate entity extrac-
tion with edit distance constraints. In SIGMOD
?09: Proceedings of the 35th SIGMOD Interna-
tional Conference on Management of Data, pages
759?770.
Winkler, William E. 1999. The state of record link-
age and current research problems. Technical Re-
port R99/04, Statistics of Income Division, Internal
Revenue Service Publication.
Xiao, Chuan, Wei Wang, and Xuemin Lin. 2008. Ed-
Join: an efficient algorithm for similarity joins with
edit distance constraints. In VLDB ?08: Proceed-
ings of the 34th International Conference on Very
Large Data Bases, pages 933?944.
859
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1281?1289,
Beijing, August 2010
Forest-guided Supertagger Training
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
Supertagging is an important technique
for deep syntactic analysis. A super-
tagger is usually trained independently
of the parser using a sequence labeling
method. This presents an inconsistent
training objective between the supertagger
and the parser. In this paper, we pro-
pose a forest-guided supertagger training
method to alleviate this problem by incor-
porating global grammar constraints into
the supertagging process using a CFG-
filter. It also provides an approach to
make the supertagger and the parser more
tightly integrated. The experiment shows
that using the forest-guided trained super-
tagger, the parser got an absolute 0.68%
improvement from baseline in F-score
for predicate-argument relation recogni-
tion accuracy and achieved a competi-
tive result of 89.31% with a faster pars-
ing speed, compared to a state-of-the-art
HPSG parser.
1 Introduction
Deep syntactic analysis by lexicalized grammar
parsing, which provides linguistic-rich informa-
tion for many NLP tasks, has recently received
more and more attention from the NLP commu-
nity. To use a deep parser in real large-scale ap-
plications, speed is an important issue to take into
consideration. Supertagging is one of the speed-
up technique widely used for lexicalized grammar
parsing. A supertagger is used to limit the number
of plausible lexical entries fed to the parser, this
can greatly reduce the search space for the parser.
Supertagging was first proposed for Lexicalized
Tree Adjoining Grammar (LTAG) (Bangalore and
Joshi, 1999), and then successfully applied to
Combinatory Categorial Grammar (CCG) (Clark,
2002) and Head-driven Phrase Structure Gram-
mar (HPSG) (Ninomiya et al, 2006). In addi-
tion, supertags can also be used for other NLP
tasks besides parsing, such as semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Birch et al, 2007; Hassan et al, 2007) to
utilize syntactic information in the supertags.
In lexicalized grammar parsing, supertagging is
usually treated as a sequence labeling task inde-
pendently trained from the parser. Previous re-
search (Clark, 2002) showed that even a point-
wise classifier not considering context edge fea-
tures is effective when used as a supertagger. To
make up for the insufficient accuracy as a single-
tagger, more than one supertag prediction is re-
served and the parser takes the burden of resolving
the rest of the supertag ambiguities.
A non-trivial problem raised by the separate
training of the supertagger is that the prediction
score provided by the supertagger might not be
suitable for direct use in the parsing process, since
a separately trained supertagger that does not take
into account grammar constraints has a training
objective which is inconsistent with the parser.
Although the scores provided by the supertagger
can be ignored (e.g., in some CCG parsers), this
may also discard some useful information for ef-
fective beam search and accurate disambiguation.
Based on this observation, we assume that
considering global grammar constraints during
the supertagger training process would make the
supertagger and the parser more tightly integrated.
1281
In this paper, we propose an on-line forest-guided
training method for a supertagger to make the
training objective of a supertagger more closely
related to the parsing task. We implemented this
method on a large-scale HPSG grammar. We
used a CFG grammar to approximate the original
HPSG grammar in the supertagging stage and ap-
plied best-first search to select grammar-satisfying
supertag sequences for the parameter updating.
The experiments showed that the HPSG parser is
improved by considering structure constraints in
the supertagging training process. For the stan-
dard test set (Penn Treebank Section 23), we ac-
complished an absolute 0.68% improvement from
baseline in F-score for predicate-argument rela-
tion recognition and got a competitive result of
89.31% with a faster parsing speed, compared to
a state-of-the-art HPSG parser.
The remainder of the paper is organized as
follows: in section 2 we provide the necessary
background regarding HPSG parsing. In section
3, we introduce the on-line forest-guided super-
tagger training method. Section 4 shows the ex-
periment results and the related analysis. Section
5 compares the proposed approach with related
work and section 6 presents our conclusions and
future work.
2 Background
2.1 Statistical HPSG Parsing
HPSG (Pollard and Sag, 1994) is a lexicalist
grammar framework. In HPSG, a large number
of lexical entries are used to express word-specific
characteristics, while only a small number of rule
schemata are used to describe general construc-
tion rules. Typed feature structures named ?signs?
are used to represent both lexical entries and
phrasal constituents. A classic efficient statisti-
cal HPSG parsing process is depicted in Figure 1.
Given a word and part-of-speech sequence (w, p)
as input, the first step (called ?supertagging?) in
HPSG parsing is to assign possible lexical entries.
In practice, for each word, more than one super-
tag is reserved for the parser. Then, the parser
searches the given lexical entry space to construct
a HPSG tree using the rule schemata to com-
bine possible signs. Constituent-based methods
and transition-based methods can be used for tree
structure disambiguation. This parsing framework
using supertagging is also used in other lexical-
ized grammars, such as LTAG and CCG.
2.2 HPSG Supertagging
Like other lexicalized grammar, the lexical en-
tries defined in HPSG are referred to as ?super-
tags?. For example, the word ?like? is assigned
a lexical entry for transitive verbs in non-3rd per-
son present form, which indicates that the head
syntactic category of ?like? is verb and it has
an NP subject and an NP complement. With
such fine-grained grammatical type distinctions,
the number of supertags is very large. Compared
to the 45 part-of-speech (POS) tags defined in the
PennTreebank, the HPSG grammar we used con-
tains 2,308 supertags. The large number and the
complexity of the supertags makes supertagging
harder than the POS tagging task.
Supertagging can be formulated as a sequence
labeling task. Here, we follow the definition of
Collins? perceptron (Collins, 2002). The train-
ing objective of supertagging is to learn the map-
ping from a POS-tagged word sentence w =
(w1/p1, ..., wn/pn) to a sequence of supertags
s = (s1, ..., sn). We use function GEN(w)
to indicate all candidates of supertag sequences
given input w. Feature function ? maps a sam-
ple (w, s) to a point in the feature space Rd. ? is
the vector of feature weights. Given an input w,
the most plausible supertag sequence is found by
the prediction function defined as follows:
F (w) = argmax
s?GEN(w)
? ? ?(w, s) (1)
2.3 CFG-filtering
CFG-filtering (Kiefer and Krieger, 2000) is a tech-
nique to find a superset of (packed) HPSG parse
trees that satisfy the constraints in a grammar. A
CFG that approximates the original HPSG gram-
mar is used for efficiently finding such trees with-
out doing full-fledged HPSG parsing that is com-
putationally demanding because the schema ap-
plication involves unification operations among
large feature structures (signs). The number of
possible signs is infinite in general and hence
1282
Figure 1: HPSG parsing for the sentence ?They like coffee.?
some features (e.g., the number agreement fea-
ture) are ignored in the approximating CFG so that
the set of possible signs can be approximated by
a finite set of non-terminal symbols in the CFG.
By this construction, some illegal trees may be
included in the set of trees licensed by the ap-
proximating CFG, but none of the well-formed
trees (i.e., those satisfying all constraints in the
grammar) are excluded by the approximation. We
use the algorithm described by Kiefer and Krieger
(2000) to obtain the approximating CFG for the
original HPSG. The technical details regarding
the algorithm can be found in Kiefer and Krieger
(2000).
3 Forest-guided Training for
Supertagging
3.1 Motivation
In lexicalized grammar parsing, a parser aims to
find the most plausible syntactic structure for a
given sentence based on the supertagging results.
One efficient parsing approach is to use predic-
tion scores provided by the supertagger. Usu-
ally, the supertagger is trained separately from the
structure disambiguation in a later stage. This
pipeline parsing strategy poses a potential prob-
lem in that the training objective of a supertagger
can deviate from the final parser, if the global
grammar constraints are not considered. For ex-
ample, the supertag predictions for some words
can contribute to high supertagging accuracy, but
cause the parser to fail. Therefore, considering the
global grammar constraints in the supertagging
training stage can make the supertagger and the
Algorithm 1: Forest-guided supertagger training
Input: Training Sample (wi, si)i=1,...,N ,
Number of iterations T
1: ? ? (0, ..., 0), ?sum ? (0, ..., 0)
2: for iterNum? 1 to T do
3: for i ? 1 to N do
4: Generate supertag lattice using
the point-wise classifier with current ?
5: Select s?i from the lattice
which can construct a tree
with largest sequence score
6: if( No s?i satisfied grammar constraints)
s?i ? argmaxs?GEN(wi) ?i ? ?(wi, si)
7: if s?i "= si then
8: ?i+1 ? ?i + ?(wi, si)? ?(wi, s?i)
9: ?sum ? ?sum + ?i+1
Return: ?sum/NT
parser more tightly related, which will contribute
towards the performance of the parser.
3.2 Training Algorithm
Based on the motivation above, we propose
a forest-guided supertagger training method to
make the supertagger more tightly integrated with
the parser. This method is based on the averaged
perceptron training algorithm. The training pro-
cess is given in Algorithm 1.
The most important difference of the proposed
algorithm compared to the traditional supertagger
training method is that the current best-scored
supertag sequence is searched only within the
space of the supertag sequences that are allowed
by the grammar. As for whether the grammar
1283
constraints are satisfied, we judge it by whether
a possible syntactic tree can be constructed using
the given supertag sequence. We do not require
the constructed syntactic tree to be identical to the
gold tree in the corpus. For this reason we call it
?forest-guided?.
In the forest-guided training of the supertagger,
an approximating CFG is used to filter out the
supertag sequences from which no well-formed
tree can be built. It is implemented as a best-first
CFG parser wherein the score of a constituent is
the score of the supertag (sub-)sequence on the
fringe of the constituent, which is calculated us-
ing the current value of the parameters. Note that
the best-first parser can find the best-scored super-
tag sequence very efficiently given proper scoring
for the candidate supertag set for each token; this
is actually the case in the course of training except
for the initial phase of the training, wherein the pa-
rameter values are not well-tuned. The efficiency
is due to the sparseness of the approximating CFG
(i.e., the production rule set includes only a tiny
fraction of the possible parent-children combina-
tions of symbols) and highest-scored supertags of-
ten have a well-formed tree on top of them.
As is clear from the above description, the use
of CFG-filter in the forest-guided training of the
supertagger is not essential but is only a subsidiary
technique to make the training faster. The im-
provement by the forest-guided training should
however depend on whether the CFG approxi-
mation is reasonably tight or not. Actually, we
managed to obtain a manageable size out of a
CFG grammar, which includes 80 thousand non-
terminal symbols and 10 million rules, by elimi-
nating only a small number of features (semantics,
case and number agreement, and fine distinctions
in nouns, adjectives and complementizers). We
thus believe that the approximation is fairly tight.
This training algorithm can also be explained
in a search-based learning framework (Hal Daume?
III and Daniel Marcu, 2005). In this framework,
the objective of learning is to optimize the ? for
the enqueue function to make the good hypothe-
ses rank high in the search queue. The rank score
r consists of two components: path score g and
heuristic score h. In the forest-guided training
method, r can be rewritten as follows:
r = g + h
= ? ? ?(x, y?) + [Tree(y?)] ? Penalty (2)
The heuristic part h checks whether the super-
tag candidate sequence satisfies the grammar con-
straints: if no CFG tree can be constructed, -?
penalty is imposed to the candidate sequence in
the forest-guided training method.
4 Experiments
We mainly evaluated the proposed forest-guided
supertagger training method on HPSG parsing.
Supertagging accuracy1 using different training
methods was also investigated.
4.1 Corpus Description
The HPSG grammar used in the experiments is
Enju version 2.32. It is semi-automatically con-
verted from the WSJ portion of PennTreebank
(Miyao, 2006). The grammar consists of 2,308
supertags in total. Sections 02-21 were used to
train different supertagging models and the HPSG
parser. Section 22 and section 23 were used as
the development set and the test set respectively.
We evaluated the HPSG parser performance by la-
beled precision (LP) and labeled recall (LR) of
predicate-argument relations of the parser?s out-
put as in previous works (Miyao, 2005). All ex-
periments were conducted on an AMD Opteron
2.4GHz server.
Template Type Template
Word wi,wi?1,wi+1,
wi?1&wi, wi&wi+1
POS pi, pi?1, pi?2, pi+1,
pi+2, pi?1&pi, pi?2&pi?1,
pi?1&pi+1, pi&pi+1,
pi+1&pi+2
Word-POS pi?1&wi, pi&wi, pi+1&wi
Table 1: Feature templates used for supertagging
models.
1?UNK? supertags are ignored in evaluation as in previ-
ous works.
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
1284
4.2 Baseline Models and Settings
We used a point-wise averaged perceptron (PW)
to train a baseline supertagger. Point-wise classi-
fiers have been reported to be very effective and
with competitive results for the supertagging task
(Clark, 2002; Zhang et al, 2009). The number of
training iterations was set to 5. The features used
in the supertaggers are described in Table 1. For
comparison, these features are identical to the fea-
tures used in the previous works (Matsuzaki et al,
2007; Ninomiya et al, 2007). To make the train-
ing efficient, we set the default chart size limit for
the forest-guided supertagger training to be 20k
by tuning it on the development set.
We combined the supertagger trained under
forest-guidance with a supertagging-based HPSG
parser (Matsuzaki et al, 2007) and evaluated the
contribution of the improved supertagger train-
ing procedure for the final HPSG parsing by the
accuracy of the predicate-argument relations out-
put of the parser. The parser crucially depends
on the supertagger?s performance in that it out-
puts the first well-formed tree successfully con-
structed on the highest scored supertag sequence.
The highest-scored supertag sequences are enu-
merated one by one in descending order in re-
gards to their score. The enumeration is actu-
ally implemented as n-best parsing on the super-
tag candidates using an approximating CFG. The
HPSG tree construction on a supertag sequence is
done using a shift-reduce style parsing algorithm
equipped with a classifier-based action selection
mechanism.
The automatically assigned POS tags were
given by a maximum entropy tagger with roughly
97% accuracy.
4.3 Supertagging Results
Although we mainly focused on improving the fi-
nal HPSG parsing performance through the im-
proved supertagger training, it is also very inter-
esting to investigate the supertagger performance
using different training methods. To evaluate the
forest-guided training method for a supertagger,
we also need to incorporate structure constraints
in the test stage. To make fair comparisons,
for the averaged perceptron trained supertagger
we also add structure constraints in its testing.
Model Name Acc%
FT+CFG 92.77
auto-POS PW+CFG 92.47
PW 91.14
ME 91.45
FT+CFG 93.98
gold-POS PW+CFG 93.70
PW 92.48
ME 92.78
Table 2: Supertagging results in section 23. ?FT?
represents the forest-guided trained supertagger.
?PW? is the baseline average perceptron trained
supertagger. ?ME? is the supertagger trained by
using the maximum entropy method. ?+CFG? in-
dicates the use of the CFG-filter for the super-
tagger results. The accuracy of automatically as-
signed POS tags in this section is 97.39%.
For simplicity, throughout this paper, we call the
forest-guided trained supertagger ?FT? in short,
while the ?PW? is used to represent the base-
line point-wise averaged perceptron supertagger.
?ME? is the re-implemented maximum entropy
supertagger described in Matsuzaki et al (2007).
For the PW supertagger, the performance was
roughly 0.3% below the ME supertagger. Simi-
lar results were reported by Zhang et al (2009),
which used a Bayes point machine to reduce the
gap between the averaged perceptron supertagger
and the maximum entropy supertagger. Although
we expected the ME supertagger using CFG-filter
to give better results than the PW supertagger, im-
plementing forest-guided supertagger training in
a maximum entropy framework is different and
more sophisticated than the current on-line train-
ing method. Considering that the performance of
the PW supertagger and the ME supertagger were
at a similar level, we chose the PW supertagger as
our baseline.
We used a CFG-filter to incorporate global
grammar constraints into both the training and
the testing phase. Compared to the PW super-
tagger, the PW+CFG supertagger incorporated
global grammar constraints only in the test phase,
while for the FT+CFG supertagger, the global
grammar constraints were incorporated both in
1285
!!!!!!!!!!!!!!!!Training Method
Iter NUM
1 2 3 4 5 Total Time
FT 6684s 4189s 3524s 3285s 3086s ? 5.8h
PW 99s 116s 117s 117s 117s ? 10 min
ME / ? 3h
Table 3: Supertagger training time on section 02-21. ?FT? and ?PW? represent forest-guided training
and point-wise averaged perceptron training separately. ?ME? is the point-wise maximum entropy
training reported in Matsuzaki et al (2007).
the training and the testing stage. The super-
tagging accuracy for different models is shown
in Table 2. Firstly, incorporating grammar con-
straints only in the testing phase (PW+CFG) gave
an absolute 1.22% (gold POS) and 1.33% (auto
POS) increase in F-score compared to the PW
supertagger. Secondly, incorporating grammar
constraints into both the training and the testing
stage (FT+CFG) gave an additional 0.28% (gold
POS) and 0.3% (auto POS) improvement over the
PW+CFG supertagger with p-values 0.0018 (gold
POS) and 0.0016 (auto POS).
This also indicates that the supertagger and the
parser are closely related to each other. The orig-
inal motivation for supertagging is using simple
models to resolve lexical ambiguities, which can
efficiently reduce the search space of the parser.
A better supertagger can contribute to more ef-
ficient and more accurate lexicalized grammar
parsing. Actually, a supertagger can act as a
coarse parser for the whole parsing process as
well, as long as the coarse parser is efficient. Since
supertag disambiguation is highly constrained by
the grammar, incorporating grammar constraints
into supertagging (including training and testing)
by using the CFG-filter can further improve the
supertagging performance, as shown in Table 2.
As for the supertagger training time, incorpo-
rating grammar constraints inevitably increases
the training time. As shown in Table 3, the to-
tal training time of forest-guided training (default
settings, with chart size limited to 20k) was about
5.8 hours. For each iteration of the FT model,
we find that the training time gradually decreases
with each successive iteration. This hints that we
can do better model initialization to further reduce
the training time.
4.4 HPSG Parsing Results
We evaluated the HPSG parsers using different
supertagger training methods. For the baseline
HPSG parser, a CFG-filter is already incorporated
to accelerate the parsing process. In the follow-
ing experiments, we fed the parser all the possi-
ble supertag candidates with the prediction scores
generated by the supertaggers. We controlled the
upper bound of the chart size in the CFG-filter to
make the parser more efficient.
Table 4 shows the results of the different pars-
ing models. We first compared the baseline
parsers using different supertaggers. The forest-
guided supertagger improved the final FT parser?s
F-score by 0.68% (statistically significant) over
the PW parser using the PW supertagger, which
did not consider global grammar constraints dur-
ing the supertagger training process. The parsing
time of the FT parser was very close to that of the
PW parser (108s vs. 106s), which was also ef-
ficient. The result empirically reflects that incor-
porating the global grammar constraints into the
supertagger training process can refine supertag
predicting scores, which become more consistent
and compatible with the parser.
We also compared our results with a state-of-
the-art HPSG parser using the same grammar.
Enju (Miyao, 2005; Ninomiya et al, 2007) is
a log-linear model based HPSG parser, which
uses a maximum entropy model for the struc-
ture disambiguation. In contrast to our baseline
parser, full HPSG grammar is directly used with
CKY algorithm in the parsing stage. As for the
parsing performance, our baseline PW parser us-
ing the PW supertagger was 0.23% below the
Enju parser. However, by using the forest-guided
trained supertagger, our improved FT parser per-
1286
Parser UP UR LP LR F-score Time ?
FT Parser 92.28 92.14 89.38 89.23 89.31 108s
PW Parser 91.88 91.63 88.75 88.51 88.63 106s
Enju 2.3 92.26 92.21 88.89 88.84 88.86 775s
Table 4: Parser performance on Section 23. ?FT Parser? represents baseline parser which uses forest-
guided trained supertagger. ?PW Parser? represents the baseline parser which uses the point-wise av-
eraged perceptron trained supertagger. (?) The time is the total time of both supertagging and parsing
and it was calculated on all 2291 sentences of the Section 23.
 85
 85.5
 86
 86.5
 87
 87.5
 88
 88.5
 89
0.5k
1k 1.5k
2k 2.5k
3k 3.5k
4k 10k
15k
20k
H
PS
G
 P
ar
si
ng
 F
-s
co
re
Chart size limit in the parsing
Parser using the PW supertagger
Parser using the 10k-train FT supertagger
Parser using the 20k-train FT supertagger
Figure 2: The F-score of the HPSG parsers on sec-
tion 22 using different settings for the chart size
limit in supertagger training and parsing.
formed 0.45% better than the Enju parser (default
settings) in F-score. In addition, our shift-reduce
style parser was faster than the Enju parser.
Beam size plays an important role for the
forest-guided supertagger training method, since a
larger beam size reduces the possibility of search
errors. Precisely speaking, we control the beam
size by limiting the number of edges in the chart
in both the forest-guided supertagger training pro-
cess and the final parsing. Figure 2 shows the re-
sults of setting different limits for the chart size
during supertagger training and parsing on the de-
velopment set. The X-axis represents the chart
size limitation for the parsing. ?10k-train? rep-
resents the chart size to be limited to 10k dur-
ing FT supertagger training phase. A similar
representation is used for ?20k-train?. There is
no tree structure search process for the baseline
PW supertagger. We evaluated the F-score of the
parsers using different supertaggers. As shown in
Figure 2, when the chart size of the parser was
more than 10k, the benefit of using forest-guided
supertaggers were obvious (around an absolute
0.5% improvement in F-score, compared to the
parser using the baseline PW supertagger). The
performance of the parser using ?10k-train? FT
supertagger was already approaching to that of the
parser using ?20k-train? FT supertagger. When
the chart size of the parser was less than 2000, the
forest-guided supertaggers were not work. Simi-
lar to the results showed in previous research (Hal
Daume? III and Daniel Marcu, 2005), it is better to
use the same chart size limit in the forest-guided
supertagger training and the final parsing.
5 Related Work
Since the supertagging technique is well known
to drastically improve the parsing speed and ac-
curacy, there is work concerned with tightly in-
tegrating a supertagger with a lexicalized gram-
mar parser. Clark and Curran (2004) investigated
a multi-tagger supertagging technique for CCG.
Based on the multi-tagging technique, supertagger
and parser are tightly coupled, in the sense that the
parser requests more supertags if it fails. They
(Clark and Curran, 2007) also used the percep-
tron algorithm to train a CCG parser. Differ-
ent from their work, we focused on improving
the performance of the deep parser by refining
the training method for supertagging. Ninomiya
et al (2007) used the supertagging probabili-
ties as a reference distribution for the log-linear
model for HPSG, which aimed to consistently
integrate supertagging into probabilistic HPSG
parsing. Prins et al (2001) trained a POS-
tagger on an automatic parser-generated lexical
entry corpus as a filter for Dutch HPSG parsing
to improve the parsing speed and accuracy.
1287
The existing work most similar to ours is
Boullier (2003). He presented a non-statistical
parsing-based supertagger for LTAG. Similar to
his method, we used a CFG to approximate the
original lexicalized grammar. The main difference
between these two methods is that we consider
the grammar constraints in the training phase of
the supertagger, not only in the supertagging test
phase and our main objective is to improve the
performance of the final parser.
6 Conclusions and Future Work
In this paper, based on the observation that su-
pertaggers are commonly trained separately from
lexicalized parsers without global grammar con-
straints, we proposed a forest-guided supertagger
training method to integrate supertagging more
tightly with deep parsing. We applied this method
to HPSG parsing and made further significant im-
provement for both supertagging (0.28%) and the
HPSG parsing (0.68%) compared to the baseline.
The improved parser also achieved a competitive
result (89.31%) with a faster parsing speed, com-
pared to a state-of-the-art HPSG parser.
For future work, we will try to weight the for-
est trees for the supertagger training and extend
this method to other lexicalized grammars, such
as LTAG and CCG.
Acknowledgments
We are grateful to the anonymous reviewers for
their valuable comments. We also thank Goran
Topic and Pontus Stenetorp for their help proof-
reading this paper. The first author was sup-
ported by The University of Tokyo Fellowship
(UT-Fellowship). This work was partially sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan).
References
Bangalore, Srinivas and Aravind K. Joshi. 1999.
Supertagging: An approach to almost parsing.
Computational Linguistics, 25:237?265.
Birch, Alexandra, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In In Proceedings IWPT-
2003, volume 3, pages 55?65.
Chen, John and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41?48.
Clark, Stephen and James R. Curran. 2004. The
importance of supertagging for wide-coverage ccg
parsing. In Proceedings of COLING-04, pages 282?
288.
Clark, S. and J.R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser.
In Proceedings of the Workshop on Deep Linguistic
Processing, pages 9?16.
Clark, Stephen. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19?24.
Collins, M. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-2002, pages 1?8.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In International
Conference on Machine Learning (ICML), pages
169?176.
Hassan, Hany, Mary Hearne, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of ACL-2007, pages 288?
295.
Kiefer, Bernd and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135?146.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Super-
tagging and CFG-filtering. In Proceedings of
IJCAI-07, pages 1671?1676.
Miyao, Yusuke. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Pro-
ceedings of the 43rd AnnualMeeting on Association
for Computational Linguistics, pages 83?90.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
1288
Ninomiya, Takashi, Yoshimasa Tsuruoka, Takuya
Matsuzaki, and Yusuke Miyao. 2006. Extremely
lexicalized models for accurate and fast HPSG pars-
ing. In Proceedings of EMNLP-2006, pages 155?
163.
Ninomiya, T., T. Matsuzaki, Y. Miyao, and J. Tsujii.
2007. A log-linear model with an n-gram reference
distribution for accurate HPSG parsing. In Proceed-
ings of IWPT-2007, pages 60?68.
Pollard, Carl and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago
/ CSLI.
Prins, R. and G. Van Noord. 2001. Unsupervised
Pos-Tagging Improves Parsing Accuracy And Pars-
ing Efficiency. In Proceedings of IWPT-2001, pages
154?165.
Zhang, Yao-zhong, Takuya Matsuzaki, and Jun?ichi
Tsujii. 2009. HPSG Supertagging: A Sequence La-
beling View. In Proceedings of IWPT-2009, pages
210?213.
1289
Coling 2010: Poster Volume, pages 851?859,
Beijing, August 2010
Imbalanced Classification Using Dictionary-based Prototypes and
Hierarchical Decision Rules for Entity Sense Disambiguation
Tingting Mu
National Centre for Text Mining
University of Manchester
tingting.mu@man.ac.uk
Xinglong Wang
National Centre for Text Mining
University of Manchester
xinglong.wang@man.ac.uk
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
tsujii@is.s.u-tokyo.ac.jp
Sophia Ananiadou
National Centre for Text Mining
University of Manchester
Sophia.Ananiadou@man.ac.uk
Abstract
Entity sense disambiguation becomes dif-
ficult with few or even zero training in-
stances available, which is known as im-
balanced learning problem in machine
learning. To overcome the problem, we
create a new set of reliable training in-
stances from dictionary, called dictionary-
based prototypes. A hierarchical classifi-
cation system with a tree-like structure is
designed to learn from both the prototypes
and training instances, and three different
types of classifiers are employed. In addi-
tion, supervised dimensionality reduction
is conducted in a similarity-based space.
Experimental results show our system out-
performs three baseline systems by at least
8.3% as measured by macro F1 score.
1 Introduction
Ambiguities in terms and named entities are a
challenge for automatic information extraction
(IE) systems. The problem is particularly acute
for IE systems targeting the biomedical domain,
where unambigiously identifying terms is of fun-
damental importance. In biomedical text, a term
(or its abbreviation (Okazaki et al, 2010)) may
belong to a wide variety of semantic categories
(e.g., gene, disease, etc.). For example, ER may
denote protein estrogen receptor in one context,
but cell subunit endoplasmic reticulum in another,
not to mention it can also mean emergency room.
In addition, same terms (e.g., protein) may be-
long to many model organisms, due to the nomen-
clature of gene and gene products, where genes
in model organisms other than human are given,
whenever possible, the same names as their hu-
man orthologs (Wain et al, 2002). On the other
hand, public biological databases keep species-
specific records for the same protein or gene,
making species disambiguation an inevitable step
for assigning unique database identifiers to entity
names in text (Hakenberg et al, 2008; Krallinger
et al, 2008).
One way to entity disambiguation is classify-
ing an entity into pre-defined semantic categories,
based on its context (e.g., (Bunescu and Pas?ca,
2006)). Existing classifiers, such as maximum
entropy model, achieved satisfactory results on
the ?majority? classes with abundant training in-
stances, but failed on the ?minority? ones with few
or even zero training instances, i.e., the knowl-
edge acquisition bottleneck (Agirre and Martinez,
2004). Furthermore, it is often infeasible to cre-
ate enough training data for all existing semantic
classes. In addition, too many training instances
for certain majority classes lead to increased com-
putational complexity for training, and a biased
system ignoring the minority ones. These corre-
spond to two previously addressed difficulties in
imbalanced learning: ?... either (i) you have far
more data than your algorithms can deal with,
851
and you have to select a sample, or (ii) you have
no data at all and you have to go through an in-
volved process to create them? (Provost, 2000).
Given an entity disambiguation task with imbal-
anced data, this paper explores how to create more
informative training instances for minority classes
and how to improve the large-scale training for
majority classes.
Previous research has shown that words denot-
ing class information in the surrounding context of
an entity can be an informative indicator for dis-
ambiguation (Krallinger et al, 2008; Wang et al,
2010). Such words are refered to as ?cue words?
throughout this paper. For example, to disam-
biguate the type of an entity, that is, whether it
is a protein, gene, or RNA, looking at words such
as protein, gene and RNA are very helpful (Hatzi-
vassiloglou et al, 2001). Similarly, for the task
of species disambiguation (Wang et al, 2010),
the occurrence of mouse p53 strongly suggests
that p53 is a mouse protein. In many cases, cue
words are readily available in dictionaries. Thus,
for the minority classes, instead of creating arti-
ficial training instances by commonly used sam-
pling methods (Haibo and Garcia, 2009), we pro-
pose to create a new set of real training instances
by modelling cue words from a dictionary, called
dictionary-based prototypes. To learn from both
the original training instances and the dictionary-
based prototypes, a hierarchical classification sys-
tem with a tree-like structure is designed. Further-
more, to cope with the large number of features
representing each instance, supervised orthogo-
nal locality preserving projection (SOLPP) is con-
ducted for dimensionality reduction, by simulta-
neously preserving the intrinsic structures con-
structed from both the features and labels. A new
set of lower-dimensional embeddings with better
discriminating power is obtained and used as in-
put to the classifier. To cope with the large num-
ber of training instances in some majority classes,
we propose a committee machine scheme to ac-
celerate training speed without sacrificing classi-
fication accuracy. The proposed method is evalu-
ated on a species disambiguation task, and the em-
pirical results are encouraging, showing at least
8.3% improvement over three different baseline
systems.
2 Related Work
Construction of a classification model using su-
pervised learning algorithms is popular for entity
disambiguation. A number of researchers have
tackled entity disambiguation in general text us-
ing wikipedia as a resource to learn classifica-
tion models (Bunescu and Pas?ca, 2006). Hatzi-
vassiloglou et al (2001) studied disambiguating
proteins, genes, and RNA in text by training var-
ious classifiers using entities with class informa-
tion provided by adjacent cue words. Wang et
al. (2010) proposed a ?hybird? system for species
disambiguation, which heuristically combines re-
sults obtained from classifying the context, and
those from modeling relations between cue words
and entities. Although satisfactory performance
was reported, their system incurs higher computa-
tional cost due to syntactic parsing and the binary
relation classifier.
Many imbalanced learning techniques, as re-
viewed by Haibo and Garcia (2009), can also be
used to achieve the same purpose. However, to
our knowledge, there is little research in apply-
ing these machine learning (ML) techniques to en-
tity disambiguation. It is worth mentioning that
although these ML techniques can improve the
learning performance to some extent, they only
consider the information contained in the origi-
nal training instances. The created instances do
not add new information, but instead utilize the
original training information in a more sophisti-
cated way. This motivates us to pursue a differ-
ent method of creating new training instances by
using information from a related and easily ob-
tained source (e.g., a dictionary), similar to trans-
fer learning (Pan and Yang, 2009).
3 Task and Corpus
In this work, we develop an entity disambiguation
technique with the use of cue words, as well as a
general ML algorithm for imbalanced classifica-
tion using a set of newly created dictionary-based
prototypes. These prototypes are represented with
different features from those used by the original
training instances. The proposed method is eval-
uated on a species disambiguation task: given a
text, in which mentions of biomedical named en-
852
tities are annotated, we assign a species identi-
fier to every entity mention. The types of entities
studied in this work are genes and gene products
(e.g., proteins), and we use the NCBI Taxonomy1
(taxon) IDs as species tags and to build the proto-
types. Note that this paper focuses on the task of
species disambiguation and makes the assumption
that the named entities are already recognised.
Consider the following sentence as an exam-
ple: if one searches the proteins (i.e., the under-
lined term) in a protein database, he/she will find
they belong to many model organisms. However,
in this particular context, CD200R-CD4d3+4 is
human and mouse protein, while rCD4d3+4 is
a rat one.2 We call such a task of assigning
species identifiers to entities, according to context,
as species disambiguation.
The amounts of human and mouse
CD200R-CD4d3+4 and rCD4d3+4
protein on the microarray spots were
similar as visualized by the red fluo-
rescence of OX68 mAb recognising
the CD4 tag present in each of the
recombinant proteins.
The informative cue words (e.g., mouse) used
to help species disambiguation are called species
words. In this work, species words are defined as
any word that indicates a model organism and also
appears in the organism dictionaries we use. They
may have various parts-of-speech, and may also
contain multiple tokens (despite the name species
word). For example, ?human?, ?mice?, ?bovine?
and ?E. Coli? are all species words. We detect
these words by automatic dictionary lookup: a
word is annotated as a species word if it matches
an entry in a list of organism names. Each entry in
the list contains a species word and its correspond-
ing taxon ID, and the list is merged from two dic-
tionaries: the NCBI Taxonomy and the UniProt
controlled vocabulary of species.3 The NCBI por-
tion is a flattened NCBI Taxonomy (i.e., without
hierarchy) including only the identifiers of genus
and species ranks. In total, the merged list con-
1http://www.ncbi.nlm.nih.gov/sites/entrez?db= taxon-
omy
2Prefix ?r? in ?rCD4d3+4? indicates that it is a rat protein.
3http://www.expasy.ch/cgi-bin/speclist
tains 356,387 unique species words and 272,991
unique species IDs. The ambiguity in species
words is low: 3.86% of species words map to mul-
tiple IDs, and on average each word maps to 1.043
IDs.
The proposed method was evaluated on the
corpus developed in (Wang et al, 2010), con-
taining 6, 223 genes and gene products, each of
which was manually assigned with either a taxon
ID or an ?Other? tag, with human being the
most frequent at 50.30%. With the extracted
features and the species ID tagged by domain
experts, each occurrence of named entities can
be represented as a d-dimensional vector with
a label. Species disambiguation can be mod-
elled as a multi-classification task: Given n train-
ing instances {xi}ni=1, their n ? d feature ma-
trix X = [xij ] and n-dimensional label vector
y = [y1, y2, . . . , yn]T are used to train a clas-
sifier C(?), where xi = [xi1, xi2, . . . , xid]T , yi ?
{1, 2, . . . , c}, and c denotes the number of ex-
isting species in total. Given m different query
instances {x?i}mi=1, their m ? d feature matrix
X? = [x?ij ] are used as the input to the trained
classifier, so that their labels can be predicted by
{C(x?i)}mi=1.
We used relatively simple contextual features
because this work was focused on developing a
ML framework. In more detail, we used the fol-
lowing features: 1) 200 words surrounding the en-
tity in question; 2) two nouns and two adjectives
at the entity?s left and right; 3) 5 species words
at the entity?s left and right. In addition, function
words and words that consist of only digits and
punctuations are filtered out. The final numeri-
cal dataset consists of 6,227 instances, each rep-
resented by 16,851 binary features and belonging
to one of the 13 classes. The dataset is highly im-
balanced: among the 13 classes, the numbers of
instances in the four majority classes vary from
449 to 3,220, while no more than 20 instances are
contained in the eight minority classes (see Table
1).
853
4 Proposed Method
4.1 Dictionary-based Prototypes
For each existing species, we create a b-
dimensional binary vector, given as pi =
[pi1, pi2, . . . , pib]T , using b different species
words listed in the dictionary as features, which
is called dictionary-based prototype. The binary
value pij denotes whether the jth species word
belongs to the ith species in the dictionary. This
leads to a c ? b feature matrix P = [pij ] for c
species.
Considering that the species words preceding
and appearing in the same sentence as an en-
tity can be informative indicators for the possible
species of this entity, we create two morem?b bi-
nary feature matrices for the query instances with
the same b species words as features: X?1 = [x?(1)ij ]
and X?2 = [x?(2)ij ], where x?(1)ij denotes whether the
jth species word is the preceding word of the ith
entity, and x?(2)ij denotes whether the jth species
word appears in the same sentence as the ith en-
tity but is not preceding word. Thus, the similar-
ity between each query entity and existing species
can be simply evaluated by calculating the inner-
product between the entity instance and the cor-
responding prototype. This leads to the following
m? c similarity matrix S? = [s?ij ]:
S? = ?X?1PT + (1? ?)X?2PT , (1)
where 0 ? ? ? 1 is a user-defined parameter con-
trolling the degree of indicating reliability of the
preceding word and the same-sentence word. The
n?c similarity matrix S = [sij ] between the train-
ing instances and the species can be constructed in
exactly the same way. Based on empirical expe-
rience, the preceding word indicates the entity?s
species more accurately than the same-sentence
word. Thus, ? is preferred to be set as greater
than 0.5. The obtained similarity matrix will be
used in the nearest neighbour classifier (see Sec-
tion 4.2.1).
Both the original training instances X and the
newly created prototypes P are used to train the
proposed hierarchical classification system. Sub-
ject to the nature of the classifier employed, it is
convenient to construct one single feature matrix
Nearest Neighbor Classifier(IT1)
Minority Classes
Majority Classes
SOLPP-FLDA Classifier(IT2)
Small-scale Majority Classes
Large-scale Majority Classes
Committee Classifier(END)
Yes
No Yes
No
Output: Instances with predicted labels belonging to MI Output: Instances with predicted labels belonging to SMA
Output: Instances with predicted labels belonging to LMA
Note: Definition of the minority,majority, small-scale majority, large-scale majority classes, as well as theIF-THEN rule 1 (IT1) and IF-THEN rule2 (IT2) are provided in the paper.
Figure 1: Structure of the proposed hierarchical
classification system
instead of using X and P individually. Aiming at
keeping the same similarity values between each
entity instance and the species prototype, we con-
struct the following (n+c)?(d+b) feature matrix
for both the training instances and prototypes:
F =
[ X ?X1 + (1? ?)X2
0 P
]
, (2)
where X1 and X2 are constructed in the same way
as X?1 and X?2 but for training instances. Their cor-
responding label vector is l = [yT , 1, 2, . . . , c]T .
4.2 Hierarchical Classification
Multi-stage or hierarchical classification (Giusti
et al, 2002; Podolak, 2007; Kurzyn?ski, 1988)
is widely used in many complex multi-category
classification tasks. Existing research shows such
techniques can potentially achieve right trade-off
between accuracy and resource allocation (Giusti
et al, 2002; Podolak, 2007). Our proposed hier-
archical system has a tree-like structure with three
different types of classifier at nodes (see Figure 1).
Different classes are organized in a hierarchical
order to be classified based on the corresponding
numbers of available training instances. Letting
ni denote the number of training instances avail-
able in the ithe class excluding the created proto-
types, we categorize the classes as follows:
? Minority Classes (MI): Classes with less
training instances than the threshold: MI =
{i : nin < ?1, i ? {1, 2 . . . , c}}.
854
? Majority Classes (MA): Classes with more
training instances than the threshold: MA =
{i : nin ? ?1, i ? {1, 2 . . . , c}}.
? Small-scale Majority Classes (SMA): Ma-
jority Classes with less training instances
than the threshold: SMA = {i : nin <
?2, i ? MA}.
? Large-scale Majority Classes (LMA): Ma-
jority Classes with more training instances
than the threshold: LMA = {i : nin ?
?2, i ? MA}.
Here, 0 < ?1 < 1 and 0 < ?2 < 1 are size
thresholds set by users. We have MI ?MA = ?,
SMA ? LMA = ?, and SMA ? LMA = MA.
The tree-like hierarchical structure of our sys-
tem is determined by MI, MA, SMA, and LMA.
We propose two IF-THEN rules to control the sys-
tem: Given a query instance x?i, the level 1 clas-
sifier C1 is used to predict whether x?i belongs to
MA or a specific class in MI, which wer call IF-
THEN rule 1 (IT1). If x?i belongs to MA, the level
2 classifier C2 is used to predict whether x?i be-
longs to LMA or a specific class in SMA, called
IF-THEN rule 2 (IT2). If x?i belongs to LMA, the
level 3 classifier C3 finally predicts the specific
class in LMA x?i belongs to. We explain in the
following sections how the classifiers C1, C2, and
C3 work in detail.
4.2.1 Nearest Neighbour Classifier
The goal of the nearest neighbour classifier, de-
noted by C1, is to decide whether the nearest-
neighbour prototype of the query instance be-
longs to MI. The only used training instances are
our created dictionary-based prototypes {pi}ci=1
with the label vector [1, 2, . . . , c]T . The nearest-
neighbour prototype of the query instance x?i pos-
sesses the maximum similarity to x?i:
NN(x?i) = arg maxj=1, 2, ..., c s?ij , (3)
where s?ij is obtained by Eq. (1). Consequently,
the output of the classifier C1 is given as
C1(x?i) =
{
NN(x?i), If NN(x?i) ? MI,
0, Otherwise.
(4)
The IF-THEN rule 1 can then be expressed as
Action(IT1) =
{ Go to C2, If C1(x?i) = 0,
Stop, Otherwise.
4.2.2 SOLPP-FLDA Classifier
The goal of the SOLPP-FLDA classifier, de-
noted by C2, is to predict whether the query in-
stance belongs to LMA or a specific class in SMA.
In this classifier, the used training instances are
the original training entities and the dictionary-
based prototypes, both belonging to MA. The fea-
ture matrix F and the label vector l defined in Sec-
tion 4.1 are used, but with instances from MI re-
moved (we use n? to denote the number of remain-
ing training instances, and the same symbol F for
feature matrix). The used label vector l? to train C2
should be re-defined as l?i = li if li ? SMA, and 0
otherwise.
First, we propose to implement orthog-
onal locality preserving projection (OLPP)
(Kokiopoulou and Saad, 2007) in a supervised
manner, leading to SOLPP, to obtain a smaller set
of more powerful features for classification. Also,
we conduct SOLPP in a similarity-based feature
space computed from (d + 2b) original features
by employing dot-product based similarity, given
by FFT . As explained later, to compute the
new features from FFT instead of the original
features F achieves reduced computational cost.
An n??k projection matrix V = [vij ] is optimized
in this n-dimensional similarity-based feature
space. The optimal projections are obtained by
minimizing the weighted distances between the
lower-dimensional embeddings so that ?similar?
instances are mapped together in the projected
feature space. Mathematically, this leads to the
following constrained optimization problem:
min
V?Rn??k,
VT V=Ik?k
tr[VTFTF(D?W)FFTV], (5)
where W = [wij ] denotes the n ? n weight ma-
trix with wij defining the degree of ?closeness? or
?similarity? between the ith and jth instances, D
is a diagonal matrix with {di =?n?j=1wij}n?i=1 as
the diagonal elements.
Usually, the weight matrix W is defined by
an adjacency graph constructed from the original
855
data, e.g. for OLPP. One common way to define
the adjacency is by including the K-nearest neigh-
bors (KNN) of a given node to its adjacency list,
which is also called the KNN-graph (Kokiopoulou
and Saad, 2007). There are two common ways to
define the weight matrix: constant value, where
wij = 1 if the ith and jth samples are adjacent,
while wij = 0 otherwise, and Gaussian kernel.
We will denote in the rest of the paper such a
weight matrix computed only from the features
as WX . Ideally, if the features can accurately
describe all the discriminating characteristics, the
samples that are close or similar enough to each
other should have the same label vectors. How-
ever, when processing real dataset, what may hap-
pen is that, in the d-dimensional feature space,
the data points that are close to each other may
belong to different categories, while on the con-
trary, the data points that are in a distant to each
other may belong to the same category. In the k-
dimensional projected feature space obtained by
OLPP, one may have the same problem. Because
OLPP solves the constrained optimization prob-
lem in Eq. (5) using WX : if two instances are
close or similar to each other in the original fea-
ture space, they will be the same close or simi-
lar to each other in the projected space. To solve
this problem, we decide to modify the ?closeness?
or ?similarity? between instances in the projected
feature space by considering the label informa-
tion. The following computation of a supervised
weight matrix is used for our SOLPP:
W = (1? ?)WX + ?LLT , (6)
where 0 ? ? ? 1 is a user-defined parameter
controlling the tradeoff between the label-based
and feature-based neighborhood structures, and
L = [lij ] is an n? ? c binary label matrix with
lij = 1 if the ith instance belongs to the jth class,
and lij = 0 otherwise.
The optimal solution of Eq. (5) is the top
(k + 1)th eigenvectors of the n? ? n? symmetric
matrix FTF(D ? W)FFT , corresponding to the
k + 1 smallest eigenvalues, but with the top one
eigenvector removed, denoted by V?. It is worth
to mention that if the original feature matrix F is
used as the input of SOLPP, one needs to com-
pute the eigen-decomposition of the (d + b) ?
(d+ b) symmetric matrix FT (D?W)F. The cor-
responding computation complexity increases in
O((d + b)3), which is unacceptable in practical
when d + b  n?. The projected features for the
training instances are computed by
Z = FFTV?. (7)
Given a different set of m query instances with an
m? (d+ b) feature matrix,
F? = [X?, ?X?1 + (1? ?)X?2], (8)
their embeddings can be easily obtained by
Z? = F?F?TV?. (9)
Then, the projected feature matrix Z and label
vector l? are used to train a multi-class classifier.
By employing the one-against-all scheme, differ-
ent binary classifiers {C(2)i }i?SMA?{0} with label
space {+1, ?1} are trained. For the ith class
(i ? SMA?{0}), the training instances belonging
to it are labeled as positive, otherwise negative. In
each binary classifier C(2)i , a separating function
f (2)i (x) = xTw
(2)
i + b
(2)
i (10)
is constructed, of which the optimal values of the
weight vector w(2)i and bias b(2)i are computed us-
ing Fisher?s linear discriminant analysis (FLDA)
(Fisher, 1936; Mu, 2008). Finally, the output of
the classifier C2 can be obtained by assigning the
most confident class label to the query instance x?i,
with the confidence value indicated by the value of
separating function:
C2(x?i) = arg max
j?SMA?{0}
f (2)j (x?i). (11)
The IF-THEN rule 2 can then be expressed as
Action(IT2) =
{ Go to C3, If C2(x?i) = 0,
Stop, Otherwise.
4.2.3 Committee Classifier
The goal of the committee classifier, denoted
by C3, is to predict the specific class in LMA
the query instance belongs to. The used training
856
instances are entities and dictionary-based proto-
types only belonging to LMA. With the same one-
against-all scheme, there are large number of pos-
itive and negative training instances to train a bi-
nary classifier for a class in LMA. To accelerate
the training procedure without sacrificing the ac-
curacy, the following scheme is designed.
Letting ne denote the number of experts in
committee, all the training instances are averagely
divided into ne+1 groups each containing similar
numbers of training instances from the same class.
The instances in the ith and the (i+1)th groups are
used to train the ith expert classifier. This achieves
overlapped training instances between expert clas-
sifiers. The output value of C(3)i is not the class in-
dex as used in C2, but the value of the separating
function of the most confident class, denoted by
f (3)i . Different from the commonly used majority
voting rule, we only trust the most confident ex-
pert. Thus, the output of C3 for a query instance
x?i can be obtained by
C3(x?i) = arg maxj=1, 2, ..., ne f
(3)
j (x?i). (12)
By using C3, different expert classifiers can be
trained in parallel. The total training time is equal
to that of the slowest expert classifier. The more
expert classifiers are used, the faster the system is,
however, the less accurate the system may become
due to the decrease of used training instances for
each expert, especially the positive instances in
the case of imbalanced classification. This is also
the reason we do not apply the committee scheme
to SMA classes.
5 Experiments
5.1 System Evaluation and Baseline
We evaluate the proposed method using 5-fold
cross validation, with around 4,980 instances for
training, and 1,245 instances for test in each trial.
We compute the F1 score for each species, and
employ macro- and micro- average scheme to
compute performance for all species. Three base-
lines for comparison include:
? Baseline 1 (B1) : A maximum entropy
model trained with training data only.
? Baseline 1 (B2) : Combination of B1 and
the species dictionary using rules employed
in Wang et al (2010).
? Baseline 2 (B3): The ?hybrid? system com-
bining B1, the dictionary and a relation
model 4 using rules (Wang et al, 2010).
Our hierarchical classification system were imple-
mented in two ways:
? HC: Only the training data on its own is used
to train the system.
? HC/D: Both the training data and the
dictionary-based prototypes are used to train
the system.
5.2 Results and Analysis
The proposed system was implemented with ? =
0.8, ? = 0.8, ne = 4, and k = 1000. The species
9606, 10090, 7227, and 4932 were categorized as
LMA, the species 10116 as SMA, and the rest sep-
cies as MI. To compute the supervised weight ma-
trix, the percentage of the used KNN in the KNN-
graph was 0.6. Parameters were not fine tuned, but
set based on our empirical experience on previous
classification research. As shown in Table 1: HC
and B1 were trained with the same instances and
features, and HC outperformed B1 in both macro
and micro F1. Both HC and B1 obtained zero F1
scores for most minority species, showing that it is
nearly impossible to correctly label the query in-
stances of minority classes, due to lack of training
data. By learning from a related resource, HC/D,
B2, and B3 yielded better macro performance. In
particular, while HC/D and B2 learned from the
same dictionary and training data, HC/D outper-
formed B2 by 19.1% in macro and 2.5% in mi-
cro F1. B3 aimed at improving the macro perfor-
mance by employing computationally expensive
syntactic parsers and also by training an extra re-
lation classifier. With the same goal, HC/D inte-
grated the cue word information into the ML clas-
sifier in a more general way, and yielded an 8.3%
improvement over B3, as measured by macro-F1.
4This is an SVM model predicting relations between en-
tities and nearby species words with positive output indicates
species words bear the semantic label of entities.
857
Species Name Cat. No. HC HC/D B1 B2 B3
Homo sapiens (9606) LMA 3220 87.39 87.48 86.06 85.43 86.48
Mus musculus (10090) LMA 1709 79.99 79.98 79.59 80.00 80.41
Drosophila melanogaster (7227) LMA 641 86.62 86.35 87.96 87.02 87.37
Saccharomyces cerevisiae (4932) LMA 499 90.24 90.24 83.35 81.64 84.64
Rattus norvegicus (10116) SMA 50 55.07 69.23 48.42 64.41 59.41
Escherichia coli K-12 (83333) MI 18 0.00 0.00 0.00 0.00 0.00
Xenopus tropicalis (8364) MI 8 0.00 40.00 0.00 41.67 36.36
Caenorhabditis elegans (6239) MI 7 0.00 22.22 0.00 28.57 22.22
Oryctolagus cuniculus (9986) MI 3 0.00 0.00 0.00 20.00 0.00
Bos taurus (9913) MI 3 0.00 50.00 0.00 0.00 100.00
Arabidopsis thaliana (3702) MI 2 0.00 0.00 0.00 0.00 66.67
Arthropoda (6656) MI 1 0.00 100.00 0.00 50.00 0.00
Martes zibellina (36722) MI 1 0.00 50.00 0.00 28.57 0.00
Micro-average N/A N/A 85.03 85.13 83.59 83.04 83.80
Macro-average N/A N/A 30.72 51.96 29.42 43.64 47.97
Table 1: Performance is compared in F1 (%), where ?No.? denotes the number of training instances
and ?Cat.? denotes the category of species class as defined in Section 4.2.
6 Conclusions and Future Work
Disambiguating bio-entities presents a challenge
for traditional supervised learning methods, due
to the high number of semantic classes and lack of
training instances for some classes. We have pro-
posed a hierarchical framework for imbalanced
learning, and evaluated it on the species disam-
biguation task. Our method automatically builds
training instances for the minority or missing
classes from a cue word dictionary, under the as-
sumption that cue words in the surrounding con-
text of an entity strongly indicate its semantic cat-
egory. Compared with previous work (Wang
et al, 2010; Hatzivassiloglou et al, 2001), our
method provides a more general way to integrate
the cue word information into a ML framework
without using deep linguistic information.
Although the species disambiguation task is
specific to bio-text, the difficulties caused by im-
balanced frequency of different senses are com-
mon in real application of sense disambiguation.
The proposed technique can also be applied to
other domains, providing the availability of a cue
word dictionary that encodes semantic informa-
tion regarding the target semantic classes. Build-
ing such a dictionary from scratch can be chal-
lenging, but may be easier compared to manual
annotation. In addition, such dictionaries may al-
ready exist in specialised domains.
Acknowledgment
The authors would like to thank the biologists who
annotated the species corpus, and National Cen-
tre for Text Mining. Funding: Pfizer Ltd.; Joint
Information Systems Committee (to UK National
Centre for Text Mining)
References
Agirre, E. and D. Martinez. 2004. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP.
Bunescu, R. and M. Pas?ca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL.
Fisher, R. A. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7(2):179?188.
Giusti, N., F. Masulli, and A. Sperduti. 2002. Theoret-
ical and experimental analysis of a two-stage system
for classification. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 24(7):893?904.
Haibo, H. and E. A. Garcia. 2009. Learning from
imbalanced data. IEEE Trans. on Knowledge and
Data Engineering, 21(9):1263?1284.
858
Hakenberg, J., C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of
gene mentions with GNAT. Bioinformatics, 24(16).
Hatzivassiloglou, V., PA Duboue?, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics,
17(Suppl 1).
Kokiopoulou, E. and Y. Saad. 2007. Orthogonal
neighborhood preserving projections: A projection-
based dimensionality reduction technique. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 29(12):2143?2156.
Krallinger, M., A. Morgan, L. Smith, F. Leitner,
L. Tanabe, J. Wilbur, L. Hirschman, and A. Valen-
cia. 2008. Evaluation of text-mining systems for
biology: overview of the second biocreative com-
munity challenge. Genome Biology, 9(Suppl 2).
Kurzyn?ski, M. W. 1988. On the multistage bayes clas-
sifier. Pattern Recognition, 21(4):355?365.
Mu, T. 2008. Design of machine learning algorithms
with applications to breast cancer detection. Ph.D.
thesis, University of Liverpool.
Okazaki, N., S. Ananiadou, and J. Tsujii. 2010.
Building a high quality sense inventory for im-
proved abbreviation disambiguation. Bioinformat-
ics, doi:10.1093/bioinformatics/btq129.
Pan, S. J. and Q. Yang. 2009. A survey on transfer
learning. IEEE Trans. on Knowledge and Data En-
gineering.
Podolak, I. T. 2007. Hierarchical rules for a hierarchi-
cal classifier. Lecture Notes in Computer Science,
4431:749?757.
Provost, F. 2000. Machine learning from imbalanced
data sets 101. In Proc. of Learning from Imbalanced
Data Sets: Papers from the Am. Assoc. for Artificial
Intelligence Workshop. (Technical Report WS-00-
05).
Wain, H., E. Bruford, R. Lovering, M. Lush,
M. Wright, and S. Povey. 2002. Guidelines for
human gene nomenclature. Genomics, 79(4):464?
470.
Wang, X., J. Tsujii, and S. Ananiadou. 2010. Dis-
ambiguating the species of biomedical named enti-
ties using natural language parsers. Bioinformatics,
26(5):661667.
859
Coling 2010: Poster Volume, pages 1417?1425,
Beijing, August 2010
Semi-automatically Developing Chinese HPSG Grammar from the Penn Chinese Treebank for Deep Parsing 
Kun Yu1 Yusuke Miyao2 Xiangli Wang1 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo 2. National Institute of Informatics yusuke@nii.ac.jp {kunyu, xiangli, matuzaki, tsujii} @is.s.u-tokyo.ac.jp 3. The University of Manchester  Abstract In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and anno-tation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexi-con is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accu-racy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. 1 Introduction Precise, in-depth syntactic and semantic analysis has become important in many NLP applications. Deep parsing provides a way of simultaneously obtaining both the semantic relation and syntac-tic structure. Thus, the method has become more popular among researchers recently (Miyao and Tsujii, 2006; Matsuzaki et al, 2007; Clark and Curran, 2004; Kaplan et al, 2004).  This paper introduces our recent work on deep parsing for Chinese, specifically focusing on the development of a large-scale grammar, based on the HPSG theory (Pollard and Sag, 1994). Be-cause it takes a decade to manually develop an HPSG grammar that achieves sufficient coverage for real-world text, we use a semi-automatic ap-proach, which has successfully been pursued for English (Miyao, 2006; Miyao et al, 2005; Xia, 1999; Hockenmaier and Steedman, 2002; Chen and Shanker, 2000; Chiang, 2000) and other lan-guages (Guo et al, 2007; Cramer and Zhang, 2009; Hockenmaier, 2006; Rehbei and Genabith, 2009; Schluter and Genabith, 2009).  The following lists our method of approach: (1) define a skeleton of the grammar (in this 
work, the structure of sign, grammatical princi-ples and schemas), (2) convert the CTB (Xue et al, 2002) into an HPSG-style treebank, (3) automatically extract a large-scale lexicon from the obtained treebank. Experiments were performed to evaluate the quality of the grammar developed from the CTB 6.0. More than 95% of the sentences in the CTB could be successfully converted, and the ex-tracted lexicon was 97.24% accurate. The ex-tracted lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. Since grammar engineering has many specific problems in each language, although we used the similar method applied in other languages to de-velop a Chinese HPSG grammar, it is very dif-ferent from applying, such as statistical parsing models, to a new language. Lots of efforts have been done for the specific characteristics of Chi-nese. The contribution of our work is to describe these issues. As a result, a skeleton design of Chinese HPSG is proposed, and for the first time, a robust and wide-coverage Chinese HPSG grammar is developed from real-world text.  2 Design of Grammatical Constraints for Chinese HPSG Because of the lack of a comprehensive HPSG-based syntactic theory for Chinese, we extended the original HPSG (Pollard and Sag, 1994) to analyze the specific linguistic phenomena in Chinese. Due to space limitations, we will pro-vide a brief sampling of our extensions, and dis-cuss several selected constructions.  2.1 Sign, Principles, and Schemas Sign, which is a data structure to express gram-matical constraints of words/phrases, is modified and extended for the analysis of Chinese specific constructions, as shown in Figure 1. PHON, MOD, SPEC, SUBJ, MARKING, and SLASH are 
1417
features defined in the original HPSG, and they represent the phonological information of a word, the constraints on the modifiee, the speci-fiee, the subject, the marker, and the long-distance dependency, respectively. COMPS, which represents the constraints on comple-ments, is divided into LCOMPS and RCOMPS, to distinguish between left and right comple-ments. Aspect, question, and negation particles are treated as markers as done in (Gao, 2000), which are distinguished by ASPECT, QUESTION, and NEGATION. CONT is also originated from Pollard and Sag (1994), although it is used to represent semantic structures with predicate-argument dependencies. TOPIC and CONJ are extended features that represent the constraints on the topic and the conjuncts of co-ordination. FILLER is another extended feature that records the grammatical function of the moved argument in a long-distance dependency. 
 Figure 1. HPSG sign for Chinese. The principles, including Phonology Princi-ple, Valence Principle, Head Feature Principle, and Nonlocal Feature Principle, are imple-mented in our Chinese HPSG grammar as de-fined in (Pollard and Sag, 1994). Semantic Principle is slightly modified so that it composes predicate-argument structures. 14 schemas are defined in our grammar, among which the Coord-Empty-Conj Schema, Relative-Head Schema, Empty-Relativizer Schema, and Topic-Head Schema are designed specifically for Chinese. The other 10 schemas are borrowed from the original HPSG theory. 15 Chinese constructions are considered in our current grammar (refer to Table 1). A detailed description of some particular constructions will be provided in the following subsection.  2.2 An HPSG Analysis for Chinese  2.2.1 BA Construction The BA construction moves the object of a verb to the pre-verbal position. For example, the sen-
tence in Figure 2 with the original word order is ??/I ?/read ? ?/book?. There were three popular ways to address the BA construction: as a verb (Huang, 1991; Bender, 2000), preposition (Gao, 1992), and case marker (Gao, 2000). Since the aspect markers, such as ???, cannot attach to BA, we exclude the analysis of treating BA as a verb. Because BA, like prepositions, always ap-pears before a noun phrase, we therefore follow the analysis in Gao (1992), and treat BA as a preposition. As shown in Figure 2, BA takes a moved object as a complement, and attaches to the verb as a left-complement. 
 (I read the book.) Figure 21. Analysis of BA construction. 2.2.2 BEI Construction The BEI construction is used to make the passive voice of a sentence. Because the aspect marker also cannot attach to BEI, we do not treat BEI as a verb, as done in the CTB. Similar to the analy-sis of BA construction, we regard BEI as a preposition that attaches to the verb as a left-complement. Additionally, because we can insert a clause ???/Li ?/send ?/person? between the moved object ??/he? and the verb ??/beat?, as is the case for ??/he ?/BEI ??/Li ?/send ?/person ?/beat ? (He was beaten by the person that is sent by Li)?, we treat the relation between the moved object and the verb as a long-distance dependency. Figure 3 exem-plifies our analysis of the BEI construction, in which the Filler-Head Schema is used to handle the long-distance dependency, and the FILLER feature is used to record that the role of the moved argument. 
 (The book is read by me.)  Figure 3. Analysis of BEI construction.  2.2.3 Topic Construction As indicated in Li and Thompson (1989), a topic refers to the theme of a sentence, which always                                                            1 In the figures in this paper, we will show only selected features that are relevant to the explanation.  
1418
appears before the subject. The difference be-tween the topic and subject is the subject must always have a direct semantic relationship with the verb in a sentence, whereas the topic does not. There are two types of topic constructions. In the first type, the topic does not fill any argu-ment slots of the verb, such as the topic ???/elephant? in Figure 4. In the second type, the topic has a semantic relationship with the verb. For example, in the sentence ??/he ?/I ??/like (I like him)?, the topic ??/he? is also an object of ???/like?. For the first type, we define the Topic-Head Schema to describe the topic construction (refer to Figure 4). For the second type, we follow the same analysis as in English, and use the Filler-Head Schema.   
 (The nose of an elephant is long.) Figure 4. Analysis of topic construction. 2.2.4 Serial Verb Construction In contrast to the definition of serial verb con-struction in Li and Thompson (1989), we specify a serial verb construction as a special type of verb phrase coordination, which describes sev-eral separate events with no conjunctions inside. Similar to ordinary coordination, the verb phrases in a serial verb construction share the same syntactic subject (Muller and Lipenkova, 2009), topic, and left-complement. We define Coord-Empty-Conj Schema to deal with it. Fig-ure 5 shows an example analysis. 
 (I go to the book store and buy a book.) Figure 5. Analysis of serial verb construction. 2.2.5 Relative Clause In Chinese, a relative clause is marked by a rela-tivizer ??? and exists in the left of the head noun. Because Chinese noun phrases are right-headed in general, we analyze a relative clause as a nominalization that modifies a head noun (Li and Thompson, 1989). Inside of a relative clause, the relativizer is treated as head. When the rela-tivizer is omitted, we define a unary schema, Empty-Relativizer Schema, which functions by combining a relative clause with an empty rela-
tivizer. Furthermore, we introduce a Relative-Head Schema to handle the long-distance de-pendency for the extracted argument2 (refer to Figure 6).  
 (the book that I buy) Figure 6. Analysis of relative clause. 3 Converting the CTB into an HPSG Treebank 3.1 Partially-specified Derivation Tree Annotation In order to convert the CTB into an HPSG tree-bank, we first annotate the bracketing trees in the CTB to be partially-specified derivation trees3, which conform to the grammatical constraints designed in Section 2. Three types of rules are defined to fulfill this annotation. 
 (I read the book that he wrote.) Figure 7. The CTB annotation for a sentence. 
 Figure 8. Partially-specified derivation tree for Figure 7. For example, Figure 7 shows the bracketing tree of a sentence in the CTB, while Figure 8 shows the partially-specified derivation tree after re-annotation.                                                            2 The extracted adjunct is not treated as a long-distance dependency in our current grammar. 3 Partially-specified derivation tree means a tree structure that is annotated with schema names and some features of the HPSG signs (Miyao, 2006). 
1419
3.1.1 Rules for Annotation Conversion  In the CTB, there exist some annotations that do not coincide with our HPSG analysis for Chi-nese. Therefore, we define pattern rules to con-vert the annotations in the CTB to fit with our HPSG analysis. 76 annotation rules are defined for 15 Chinese constructions (refer to Table 2). Due to page constraints, we focus on the constructions that we discussed in Section 2. Construction Rule # Relative clause 20 BEI construction 21 Coordination 7 Subject/object control 5 Non-verbal predicate 4 Logical subject 3 Right node raising 3 Parenthesis 3 BA construction 3 Aspect/question/negation particle 2 Subordination 1 Serial Verb construction 1 Modal verb 1 Topic construction 1 Apposition 1 Table 1. Chinese constructions and annotation rules. Rules for BA and BEI Construction As analyzed in Section 2, we treat BA and BEI as prepositions that attach to the verb as left-complements. However, in the CTB, BA and BEI are annotated as verbs that take a sentential complement (Xue and Xia, 2000). By applying the annotation rules, the BA/BEI and the subject of the sentential complement of BA/BEI are re-annotated as a prepositional phrase (as indicated in the dash-boxed part in Figure 9). 
 (I read the book.) Figure 9. Conversion of BA construction. 
 (He is regarded as a friend by me.) Figure 10. Verb division in BEI construction. In addition, in the CTB, some BA/BEI con-structions are not annotated with trace, which 
makes it difficult to retrieve the semantic relation between the verb and the moved object. The principal reason for this is that the moved object in these constructions has a semantic relation with only part of the verb. For example, in Fig-ure 10, the moved noun ??/he? is the object of ??/regard?, but not for ???/regard as?. Analy-sis shows that only a closed set of characters (e.g. ??/as?)  can be attached to verbs in such a case. Therefore, we manually collect these char-acters from the CTB, and then define pattern rules to automatically split the verb, which ends with the collected characters, in the BA and BEI construction. Finally, we annotate trace for the split verb. Figure 10 exemplifies the conversion of an example sentence. Rules for Topic Construction In the CTB, a functional tag ?TPC? is used to indicate a topic (Xue and Xia, 2000). Therefore, we use this functional tag to detect topic phrases during conversion. Rules for Serial Verb Construction We define pattern rules to detect the parallel verb phrases with no conjunction inside (as shown in Figure 11), and treat these verb phrases as a se-rial verb construction. However, when the verb in the first phrase is a modal verb, such as the case of ??/I ?/want to ??/sing (I want to sing)?, the parallel verb phrases should not be treated as a serial verb construction. Therefore, a list of modal verbs is manually collected from the CTB to filter out these exceptional cases dur-ing conversion.  
 (go downstairs and eat meal) Figure 11. An example of parallel verb phrases. Rules for Relative Clause  
 (the book that he wrote) Figure 12. Conversion of relative clause. We define annotation rules to slightly modify the annotation of a relative clause in CTB, as shown in Figure 12, to make the tree structure easy to be 
1420
analyzed. Furthermore, in CTB, relative clauses are annotated with both extracted arguments and extracted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. 3.1.2 Rules for Correcting Inconsistency  There are some inconsistencies in the annotation of the CTB, which presents difficulties for per-forming the derivation tree annotation. There-fore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (re-fer to Table 3).  3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to as-sign the schemas to corresponding constituents. Rule Type Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Rules for correcting inconsistent annotation Fix POS tag annotation 2 Slash recognization 27 Schema assignment 12 Head/Argument/Modifier marking 8 Rules for assisting  annotation Binarization 1 Table 2. Rules for correcting inconsistency and assisting annotation. 3.2 HPSG Treebank Acquisition In this phase, the schemas and principles are ap-plied to the annotated partially-specified trees, in order to fill out unspecified constraints and vali-date the consistency of the annotated constraints. In effect, an HPSG treebank is obtained. For instance, by applying the Head-Complement Schema to the dash-boxed nodes in Figure 8, the constraints of the right daughter are percolated to RCOMPS of the left daughter (as indicated as 4 in Figure 13). After applying the schemas and the principles to the whole tree in Figure 8, a HPSG derivation tree is acquired (re-fer to Figure 13).  3.3 Lexicon Extraction  With the HPSG treebank acquired in Section 3.2, we automatically collect lexical entries as the combination of words and lexical entry templates from the terminal nodes of the derivation trees. For example, from the HPSG derivation tree 
shown in Figure 13, we obtain a lexical entry for the word ??/write? as shown in Figure 14. 
 Figure 13. HPSG derivation tree for Figure 8. 
 Figure 14. Lexical entry extracted for the word ??/write?. 3.3.1 Lexical Entry Template Expansion 
 (a) Lexical entry template for the verb in BEI construction 
 (b) Lexical entry template for the verb in original word order Figure 15. Application of a lexical rule. Some Chinese constructions change the word order of sentences, such as the BA/BEI construc-tions. Therefore, we apply lexical rules (Naka-nishi et al, 2004) to the lexical entry templates to convert them into those for the original word order, and expand the lexical entry templates consequently. 18 lexical rules are defined for the verbs in the BA/BEI constructions. For example, by applying a lexical rule to the lexical entry template in Figure 15(a), the moved object indi-
1421
cated by SLASH is restored into RCOMPS, and the subject introduced by BEI in LCOMPS is restored into SUBJ (refer to Figure 15(b)). 3.3.2 Mapping of Semantics In our grammar, we use predicate-argument de-pendencies for semantic representation. 44 types of predicate-argument relations are defined to represent the semantic structures of 13 classes of words. For example, we define a predicate-argument relation ?verb_arg12?, in which a verb takes two arguments ?ARG1? and ?ARG2?, to ex-press the semantics of transitive verbs. 72 se-mantics mapping rules are defined to associate these predicate-argument relations with the lexi-cal entry templates. Figure 16 exemplifies a se-mantics mapping rule. The input of this rule is the lexical entry template (as shown in the left part), and the output is a predicate-argument rela-tion ?verb_arg12? (as shown in the right part), which associates the syntactic arguments SUBJ and SLASH with the semantic arguments ARG1 and ARG2 (as indicated by 1 and 2 in Figure 16). 
 Figure 16. A semantics mapping rule. 4 Evaluation 4.1 Experimental Setting We used the CTB 6.0 for HPSG grammar devel-opment and evaluation. We split the corpus into development, testing, and training data sets, fol-lowing the recommendation from the corpus author. The development data was used to tune the design of grammar constraints and the anno-tation rules. However, the testing data set was reserved for further evaluation on parsing. Thus, the training data was further divided into two parts for training and testing in this work. During the evaluation, unknown words were handled in the same way as done in (Hockenmaier and Steedman, 2002).  4.2 Evaluation Metrics In order to verify the quality of the grammar de-veloped in our work, we evaluated the extracted lexicon by the accuracy for assessing the semi-automatic conversion process, and the coverage for quantifying the upper-bound coverage of the future HPSG parser based on this grammar.  The accuracy of the extracted lexicon was evaluated by lexical accuracy, which counts the 
number of the correct lexical entries among all the obtained lexical entries.  In addition, two evaluation metrics as used in (Hockenmaier and Steedman, 2002; Xia, 1999; Miyao, 2006) were used to evaluate the coverage of the obtained lexicon. The first one is lexical coverage (Hockenmaier and Steedman, 2002; Xia, 1999), which means that the percentage that the lexical entries extracted from the testing data are covered by the lexical entries acquired from the training data. The second one is sentential coverage (Miyao, 2006): a sentence is consid-ered to be covered only when the lexical entries of all the words in this sentence are covered.  4.3 Results of Accuracy Since there was no gold standard data for the automatic evaluation of accuracy, we randomly selected 100 sentences from the testing data, and manually checked the lexical entries extracted from these sentences. Results show that 1,558 lexical entries were extracted at 97.24% (1,515/1,558) accuracy.  Error analysis shows all the incorrect lexical entries came from the error in the derivation tree annotation. For example, our current design failed to find the correct boundary of coordinated noun phrases when the word ??/etc? was at-tached at the end, such as ???/property right ??/selling ? ??/assets ??/renting ?/etc (property right selling and assets renting etc.)?. We will improve the derivation tree anno-tation to solve this issue. 4.4 Results of Coverage Table 3 shows the coverage of the extracted lexi-cal entries, which indicates that a large HPSG lexicon was successfully extracted from the CTB for unseen text, with reasonable coverage. The statistics of the HPSG lexicon extraction in our experiments (refer to Table 4) also indicates that we successfully extracted lexical entries from more than 95% of the sentences in the CTB.  Among all the uncovered lexical entries, 78.55% are for content words, such as verb and noun. In addition, the classification of uncovered lexical entries in Table 4 indicates that about 1/3 of the uncovered lexical entries came from the unknown lexical entry templates (?+w/-t?). We analyzed the 193 ?+w/-t? failures in the testing data, among which 169 failures resulted from the shortage of training data, which indicated that the correct lexical entry template did not appear in 
1422
the training data. The learning curve in Figure 17 shows that we can resolve this issue by enlarging the training data. The other 24 failures came from the error in the derivation tree annotation. For example, our current grammar failed at de-tecting the coordinated clauses when they were separated by a colon. We will be able to reduce this type of failure by improving the derivation tree annotation. Uncovered Lexical Entries Sent. Cov. Lex. Cov. +w/+t +w/-t 76.51% 98.51% 1.05% 0.43% Table 34. Coverage of extracted HPSG lexicon. Data Set Total Sent # Succeed Sent # Word # Lexical Entry Template # Training 20,230 19,257(95.19%) 510,815 4,836 Develop 2,067 2,009(97.19%) 55,714 1,582 Testing 2,000 1,941(97.05%) 44,924 1,163 Table 4. Statistics of HPSG lexicon extraction.  
 Figure 17. Lexical coverage (Y axis) vs. corpus size (X axis). 
 Figure 18. A lexical entry template extracted from testing data. The other type of failures (?+w/+t?) indicate that a word was incorrectly associated with a lexical entry template, even though both of them existed in the training data. Error analysis shows that 64.39% of failures were related to verbs. For example, for a relative clause ???/invest ??/Taiwan ? ??/businessman (the busi-nessman that invests Taiwan)? in the testing data, we associated a lexical entry template as shown in Figure 18 with the verb ???/invest?. In the training data, however, the lexical entry template shown in Figure 18 cannot be extracted for ???/invest?, since this word never appears in a relative clause with an extracted subject. Intro-ducing lexical rules to expand the lexical entry template of verbs in a relative clause is a possible way to solve this problem. 4.5 Comparison with Previous Work Guo?s work (Guo et al, 2007; Guo, 2009) is the only previous work on Chinese lexicalized                                                            4 ?+w/+t? means both the word and lexical entry template have been seen in the lexicon. ?+w/-t? means only the word has been seen in the lexicon (Hockenmaier and Steedman, 2002). 
grammar development from the CTB, which in-duced wide-coverage LFG resources from the CTB. By using the hand-made gold-standard f-structures of 200 sentences from the CTB 5.1, the LFG f-structures developed in Guo?s work achieved 96.34% precision and 96.46% recall for unseen text (Guo, 2009). In our work, we applied the similar strategy in evaluating the accuracy of the developed Chinese HPSG grammar, which achieved 97.24% lexical accuracy on 100 unseen sentences from the CTB 6.0. When evaluating the coverage of our grammar, we used a much larger data set (including 2,000 unseen sen-tences), and achieved 98.51% lexical coverage. Although these results cannot be compared to Guo?s work directly because of the different size and content of data set, it indicates that the Chi-nese HPSG grammar developed in our work is comparable in quality with Guo?s work. In addition, there were previous works about developing lexicalized grammar for English. Considering the small size of the CTB, in com-parison to the Penn Treebank used in the previ-ous works, the results listed in Table 5 verify that, the quality of the Chinese HPSG grammar developed in our work is comparable to these previous works.  Previous Work Sent. Cov. Lex. Cov. Miyao (2006) 82.50% 98.97% Hockenmaier and Steedman (2002) - 98.50% Xia (1999) - 96.20% Table 5. Evaluation results of previous work.  4.6 Discussion There are still some sentences in the CTB from which we failed to extract lexical entries. We analyzed the 59 failed sentences in the testing data and listed the reasons in Table 6.  Reason Sent # Error in the derivation tree annotation 31 Short of semantics mapping rule 23 Inconsistent annotation in the CTB 5 Table 6. Reasons for lexicon extraction failures. The principal reason for 31 sentence failures, is the error in the derivation tree annotation. For instance, our current annotation rules could con-vert the regular relative clause shown in Figure 12. Nonetheless, when the relative clause is in-side of a parenthesis, such as ?? ??/primitive ? ???/method (the method that is primi-tive)?, the annotation rules failed at finding the extracted head noun to create a derivation tree. This type of failure can be reduced by improving the annotation rules. 
1423
The second reason, for which 23 sentences failed, is the shortage of the semantics mapping rules. For example, we did not define semantics mapping rule for a classifier that acts as a predi-cate with two topics. This type of failure can be reduced by adding semantic mapping rules.  The last reason for sentence failures is incon-sistencies in the CTB annotation. In our future work, these inconsistencies will be collected to enrich our inconsistency correction rules.  In addition to the reasons above, some sen-tences with special constructions in the devel-opment and training data also could not be analyzed by our current grammar, since the spe-cial construction is difficult for the current HPSG to analyze. The special constructions include the argument-cluster coordination shown in Figure 19. Introducing the similar rules used in CCG (Hockenmaier and Steedman, 2002) could be a possible solution to this problem. 
 (have 177 intrant projects and 6.4 billion investments) Figure 19. An argument-cluster coordination in CTB. 5 Related Work  To the extent of our knowledge, the only previ-ous work about developing Chinese lexicalized grammar from treebanks is Guo?s work (Guo et al, 2007; Guo, 2009). An LFG-based parsing using wide-coverage LFG approximations in-duced from the CTB was done in this work. However, they did not train a deep parser based on the LFG resources obtained in their work, but relied on an external PCFG parser to create c-structure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). In contrast to Guo?s work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other lan-guages. For example, by using the Penn Tree-bank, Miyao et al (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG 
specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) ac-quired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained wide-coverage LFG resources from a French Tree-bank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a compara-tive result to state-of-the-art works for English.  There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HPSG grammar using the LinGO Grammar matrix (Bender et al, 2002). Only a few basic construc-tions were considered, and a small lexicon was constructed in this work. Li (1997) and Wang et al (2009) designed frameworks for Chinese HPSG grammar; however, only small grammars were implemented in these works. Furthermore, some linguistic works focused mainly on the discussion of specific Chinese constructions in the HPSG or LFG framework, without implementing a grammar for real-world text (Bender, 2000; Gao, 2000; Li and McFe-tridge, 1995; Li, 1995; Xue and McFetridge, 1995; Wang and Liu, 2007; Ng, 1997; Muller and Lipenkova, 2009; Liu, 1996; Kit, 1998). 6 Conclusion and Future Work  In this paper, we described the semi-automatic development of a Chinese HPSG grammar from the CTB. Grammatical constraints are first de-signed by hand. Then, we convert the bracketing trees in the CTB into an HPSG treebank, by us-ing pre-defined annotation rules. Lastly, we automatically extract lexical entries from the HPSG treebank. We evaluated our work on the CTB 6.0. Results indicated that a large HPSG lexicon was successfully extracted with a 97.24% accuracy. Furthermore, our grammar achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text.  This is an ongoing work, and there are some future works under consideration, including en-riching the design of annotation rules, introduc-ing more semantics mapping rules, and adding lexical rules. In addition, the work on Chinese HPSG parsing is on-going, within which the Chinese HPSG grammar developed in this work will be available soon. 
1424
References  Emily Bender. 2000. The Syntax of Madarin Ba: Reconsid-ering the Verbal Analysis. Journal of East Asian Lin-guistics. 9(2): 105-145. Emily Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-source Starter-lit for the Rapid Development of Cross-linguistically Consistent Broad-coverage Precision Grammars. Procedings of the Workshop on Grammar Engineering and Evaluation. John Chen and Vijay K. Shanker. 2004. Automated Extrac-tion of TAGs from the Penn Treebank. Proceedings of the 6th IWPT. David Chiang. 2000. Statistical Parsing with an Automati-cally-extracted Tree Adjoining Grammar. Proceedings of the 38th ACL. 456-463. Stephen Clark and James R. Curran. 2004. Parsing the WSJ Using CCG and Log-linear Models. Proceedings of the 42nd ACL. Bart Cramer and Yi Zhang. 2009. Construction of a German HPSG Grammar from a Detailed Treebank. Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks. Qian Gao. 1992. Chinese Ba Construction: its Syntax and Semantics. Technical report.  Qian Gao. 2000. Argument Structure, HPSG and Chinese Grammar. Ph.D. Thesis. Ohio State University. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. The-sis. Dublin City University. Yuqing Guo, Josef van Genabith and Haifeng Wang. 2007. Acquisition of Wide-Coverage, Robust, Probabilistic Lexical-Functional Grammar Resources for Chinese. Proceedings of the 12th International Lexical Functional Grammar Conference (LFG 2007). 214-232. Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German Proceedings of COLING/ACL 2006. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. C-R Huang. 1991. Madarin Chinese and the Lexical Map-ping Theory: A Study of the Interaction of Morphology and Argument Changing. Bulletin of the Institute of His-tory and Philosophy 62. Ronald M. Kaplan et al 2004. Speed and Accuracy in Shal-low and Deep Stochastic Parsing. Proceedings of HLT/NAACL 2004. Chunyu Kit. 1998. Ba and Bei as Multi-valence Preposi-tions in Chinese. Studia Linguistica Sinica: 497-522.  Wei Li. 1995. Esperanto Inflection and its Interface in HPSG. Proceedings of the 11th North West Linguistics Conference. Wei Li. 1997. Outline of an HPSG-style Chinese Reversible Grammar. Proceedings of the 13th North West Linguis-tics Conference. Wei Li and Paul McFetridge. 1995. Handling Chinese NP Predicate in HPSG. Proceedings of PACLING-II. Charles N. Li and Sandra A. Thompson. 1989. Mandarin Chinese: A Functional Reference Grammar. University of California Press, London, England. 
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2007. Efficient HPSG Parsing with Supertagging and CFG-filtering. Proceedings of the 20th IJCAI. Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D. Thesis. The University of Tokyo. Yusuke Miyao, Takashi Ninomiya and Junichi Tsujii. 2005. Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. Natural Language Processing - IJCNLP 2005: 684-693. Yusuke Miyao and Junichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics. 34(1): 35-80. Stefan Muller and Janna Lipenkova. 2009. Serial Verb Con-structions in Chinese: A HPSG Account. Proceedings of the 16th International Conference on Head-Driven Phrase Structure Grammar. 234-254. Hiroko Nakanishi, Yusuke Miyao and Junichi Tsujii. 2004. An Empirical Investigation of the Effect of Lexical Rules on Parsing with a Treebank Grammar. Proceed-ings of the 3rd TLT. 103-114. Say K. Ng. 1997. A Double-specifier Account of Chinese NPs Using Head-driven Phrase Structure Grammar. Master Thesis. Department of Linguistics, University of Edinburgh. Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press and CSLI Publications, Chicago, IL and Stanford, CA. Ines Rehbein and Josef van Genabith. 2009. Automatic Acquisition of LFG Resources for German ? As Good as it Gets. Proceedings of the 14th International Lexical Functional Grammar Conference (LFG 2009). Natalie Schluter and Josef van Genabith. 2008. Treebank-based Acquisition of LFG Parsing Resources for French. Proceedings of the 6th LREC. Mark Steedman. 2000. The Syntactic Process. The MIT Press. Xiangli Wang et al 2009. Design of Chinese HPSG Frame-work for Data-driven Parsing. Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Lulu Wang and Haitao Liu. 2007. A Description of Chinese NPs Using Head-driven Phrase Structure Grammar. Pro-ceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar. 287-305. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-scale Annotated Chinese Corpus. Pro-ceedings of COLING 2002. Ping Xue and Paul McFetridge. 1995. DP Structure, HPSG, and the Chinese NP. Proceedings of the 14th Annual Conference of Canadian Linguistics Association. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for the Penn Chinese Treebank. Yi Zhang. 2004. Starting to Implement Chinese Resource Grammar using LKB and LinGO Grammar Matrix. Technical report.   
1425
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1579?1590, Dublin, Ireland, August 23-29 2014.
Common Space Embedding of Primal-Dual Relation Semantic Spaces
Hidekazu Oiwa
?
The University of Tokyo
Tokyo, Japan
hidekazu.oiwa@gmail.com
Jun?ichi Tsujii
Microsoft Research
Beijing, China
jtsujii@microsoft.com
Abstract
Explicit continuous vector representation such as vector representation of words, phrases, etc. has
been proven effective for various NLP tasks. This paper proposes a novel method of constructing
such vector representation for both entity-pairs and relation expressions which link them in text.
Based on the insight of the duality of relations, the representation is constructed by embedding
of two separately constructed semantic spaces, one for entity-pairs and the other for relation
expressions, into a common semantic space. By representing the two different types of objects
(i.e. entity-pairs and relation expressions) in the same semantic space, we can treat the two tasks,
relation mining and relation expression mining (a.k.a. pattern mining), systematically and in a
unified manner. The approach is the first attempt to construct a continuous vector representation
for expressions whose validity can be explicitly checked by their proximities to known sets of
entity-pairs. We also experimentally validate the effectiveness of the common space for relation
mining and relation expression mining.
1 Introduction
Learning continuous vector representation for expressions which consist of more than one word has
gained attention in recent years. Various representations have been constructed and used to measure
semantic similarities between expressions in various tasks, such as analogical reasoning (Turney et al.,
2003; Mikolov et al., 2013) and sentiment analysis (Turney and Littman, 2003; Socher et al., 2012).
Many algorithms have been proposed to construct such continuous representations, depending on specific
tasks in mind. In this paper, we propose a method for constructing a vector representation for binary
relations, i.e., relations with two arguments. We demonstrate the effectiveness of the representation for
relation mining and relation expression mining.
The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses
the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the
two aspects of a given relation. We first construct two separate semantic spaces, one for pairs of named
entities and another for relation expressions in text which link an entity-pair. A relation is supposed to
correspond to a subset in each of these two spaces. The subset of entity-pairs is a set of pairs between
which the relation holds. The subset is called the extension set of the relation. The subset of relation
expressions consists a set of expressions which are used to link entity-pairs in the extension set.
The two semantic spaces are then embedded into a single common space. Figure 1 illustrates a brief
summary of constructing a common semantic space. While the subsets which correspond to a specific
relation are supposed to constitute natural clusters in the two original spaces, objects in the two spaces
exchange useful information to each other and form a tighter cluster in the common space. Exchange of
information takes place through common space embedding.
Since both entity-pairs and relation expressions have their vector representations in the common se-
mantic space, one can easily enumerate relation expressions specific to a certain set of entity-pairs (re-
?
This project was conducted while the first author stayed at Microsoft Research Asia.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1579
<iPad, Apple> <Stave Ballmer, Microsoft>{is product of} {is CEO of}{of}{is developed by} {become CEO of}
Entity-Pair Semantic Space
<iPad, Apple> <Stave Ballmer,Microsoft>
Relation  Semantic Space
{is developed by}{is product of}Co-occurring links
{is CEO of}{become CEO of}{of}
Common Semantic SpaceCommon Space Embedding
<Tim-Cook, Apple>
<Surface, Windows>
<Surface, Windows>
<Tim-Cook, Apple>
R: PRODUCT-OF R: CEO
Figure 1: Overview of our framework to construct common semantic space.
lation expression mining). Furthermore, unlike the conventional pattern-based relation mining, one can
perform relation mining in the common space without explicit reference to relation expressions.
2 Basic Framework
2.1 Duality of Relation and a Common Space
A binary relation is defined either extensionally by a set of pairs in the relation or intensionally by a set
of conditions which a pair in the relation should satisfy. However, in actual applications of text mining,
either of these definitions is given in a complete form. We are only given a subset of the whole set of pairs
and have to complete the set (i.e. relation mining). Instead of an explicit intensional definition, we only
have a set of observations in text where pairs in a relation are linked by certain linguistic expressions.
Based on such observations, we have to judge whether a given pair holds the relation or not. Though
some observed expressions are non-ambiguous and explicit for a relation (for example, ?the birth place
of A is B?), most of expressions are not (such as?A comes from B?).
We call a set of pairs which define a relation as Extension set of a relation, while we call their ob-
served expressions in text as Manifestation set. While these two sets are only partially given, they define
relations which we are interested in. Such duality of a relation has been recognized by many previous
work and has been exploited in relation mining and relation expression mining. (Bollegala et al., 2010),
for example, used the duality in their work on co-clustering of entity-pairs and relation expressions. (Ba-
roni and Lenci, 2010) presented a more general approach which defines a tensor associating a triplet
< e
1
, l, e
2
> with a weight. e
1
and e
2
are entity pairs, while l is a linking expression in text. By project-
ing the tensor to matrices, they showed that diverse concepts used in distributional semantics could be
captured in a unified manner. In particular, their tensors capture directly the duality of entity pairs and
their linking expressions (i.e. relation expressions).
These previous works implicitly assume that the semantic space of entity pairs and that of relation ex-
1580
pressions are tightly coupled. That is, the space of entity pairs is defined in terms of their co-occurrences
with linking expressions (or the weights in a tensor between them) and vice versa. However, such tight
coupling between the semantic spaces of entity pairs and relation expressions is not a logical necessity,
and harmful in the sense that it restricts available information only to their co-occurrences.
An entity pair and a linking expression are complex objects by themselves, and their semantic spaces
can be defined independently of each other. Two entities in a sentence, for example, are linked not only
by single verbs or predicates but by a long sequence of words. This means that we can define a semantic
space of linking expressions independently of entity pairs which they link. For example, one can use
sequence similarities of words among relation expressions. Since knowledge resources of large scale
have become available of late, we can define a semantic space of entity pairs by using paths in these
knowledge graphs, regardless of their textual occurrences with relation expressions.
In this paper, we first define two separate semantic spaces (i.e. dual primal spaces) for entity pairs and
relation expressions, and then use their textual co-occurrences to construct a common space consistent
with the two primal spaces. In this approach, the co-occurrences of entity pairs with relation expressions
play only an auxiliary role to project the two spaces into a common space.
The approach allows us to integrate information richer than mere co-occurrences of two objects (i.e.
entity pairs and relation expressions). Furthermore, the common space provides us with direct means by
which one can grasp finer grained relationships between two objects. Given a set of seed pairs of entities,
one can gather a set of relation expressions in their nearest neighbor in the common space. Another set
of seed pairs, even though conceptually they belong to the same relation, one may get a different set of
relation expressions. The previous approaches, in which the semantics of the two objects are captured
in two separate spaces, can capture only indirectly the hierarchical nature of natural relations, and how
such a hierarchy is mapped on association of extension sets with manifestation sets.
2.2 Extension set and Manifestation Set
Let E be a set of named entities. Let ?e
i
, e
j
? denote a pair of entities (e
i
, e
j
? E) and E
2
a set of all
entity-pairs. Then, a relation, R, is extensionally defined as a set of entity-pairs E
R
? E
2
, such as
CEO = { ?Tim-Cook, Apple?, ?Ballmer, Microsoft?, . . .}, COMPETE = {?Apple, Samsung?, ?Google,
Microsoft?, . . .} between which the relation holds. We call such a set of entity-pairs the extension set of
a relation R.
On the other hand, a relation R is manifested in text in various forms of expressions. For example, ?is
the CEO of? in ?Tim-Cook is the CEO of Apple? is a direct manifestation of the relation CEO. While
?overtook? in ?Samsung overtook Apple in the smartphone market in China? can be a manifestation of
the relation COMPETE, this manifestation is rather indirect, based on inference. We denote a relation
expression by r
i
and the whole set of relation expressions by D. We call a subset of relation expressions
which manifest, directly or indirectly, a relation R, as the manifestation set of R.
2.3 Primal-dual semantic spaces
A relation, R, is characterized by the two sets, the extension set and the manifestation set. In other words,
the two sets are implicitly associated with each other via the relation R. This association between the
two sets constitutes the foundation of the common semantic space to be constructed in this paper.
We first construct primal-dual semantic spaces, one for entity-pairs and another for relation expres-
sions. A sentence where two entities appear can be seen from two different perspectives. One view is to
see the sentence as characterization of the entity-pair, while the other takes the sentence as characteriza-
tion of the relation expression which links the two entities. Based on these two views, we construct two
semantic spaces from a given set of sentences (corpus). One space is for a set of entity-pairs (E
2
) and
the other for a set of relation expressions (D). e
2
? E
2
and r ? D are represented by vectors e
2
? E
2
and r ? D in the corresponding spaces. We assume that the two spaces are vector spaces, i.e., E
2
and D
are an n-dimensional vector space and an m-dimensional one, respectively.
1581
PRODUCT-OF CEO{is product of} <Stave Ballmer,Microsoft><iPad, Apple> {is CEO of}{of} dissimilardissimilar
{is developed by}
{become CEO of}Common Semantic Space
Ambiguous
Figure 2: Illustration of common semantic space defined by our approach.
2.4 Triplets
The two objects, entity-pairs and relation expressions, whose spaces are separate, are linked through
their co-occurrences in text. Co-occurrence of a relation expression (r) and an entity-pair (e
2
= ?e
2
1
, e
2
2
?)
means that r links in a sentence the entities of e
2
1
and e
2
2
. A triplet represents such a co-occurrence with
its frequency (f ? R) in text. An instance of triplets is denoted as ?e
2
, r, f? ? T . T indicates a set
of triplets. These co-occurrence frequencies between entity-pairs and relation expressions play a critical
role in common space embedding as linkage clues.
2.5 Common space embedding from E
2
and D
We use Multi-View Partial Least Squares (MVPLS) (Wu et al., 2013) as the basic framework to construct
a common space from E
2
? R
m
and D ? R
n
. MVPLS was originally developed for web search and
has been proven to be effective for embedding the semantic space of queries and that of documents into
a common space. This framework is an extension of the conventional well-used approach, Partial Least
Square. The framework is general enough to be used for our purpose.
Let k be the dimension of common latent space such that k ? m and k ? n. e
2
i
? E
2
is a i-th
entity-pair feature vector in the entity-pair space and r
i
? D is a i-th phrase feature vector. L
e
, L
r
are
linear projection matrices for embedding the original feature vector space into the common latent space.
L
e
is m? k and L
r
is n? k size matrices.
MVPLS learns these two projection matrices for generating a well-constructed common space from
the two separated spaces. Construction of latent common space can be formulated as an optimization
problem which maximizes the sum of the similarities between entity-pairs and relation expressions in
the common space when they co-occur. This optimization problem is as follows:
argmax
L
e
,L
r
?
(e
2
i
,r
i
,f
i
)?T
log(f
i
)r
T
i
L
r
L
T
e
e
2
i
s.t. L
T
e
L
e
= I, L
T
r
L
r
= I . (1)
Note that the similarity score is weighted by the logarithmic scale of the co-occurrence counts. The
outputs of this optimization problem are L
e
and L
r
which maximize the objective value where the or-
thogonal constraints on these matrices are satisfied. We do not necessarily solve (1) again when the
system receives a new instance because the derived matrices can be applied not only for the existing
entity-pairs and relation expressions but new ones. The problem is not convex, but Wu et al. (2013)
proved that the global optimal solution can be obtained by SVD of
?
T
log(f
i
)e
2
i
r
T
i
. L
e
corresponds to
left singular vectors and L
r
consists of right singular vectors.
2.6 Ambiguity of Relation Expressions in the Common Space
Due to the ambiguity of relation expressions, the assumption that the manifestation set of the same R
cluster around in proximity does not hold in reality. ?of? in ?Steve Ballmer of Microsoft? belongs to
1582
the manifestation set of CEO, while ?of? in ?iPad of Apple? belongs to the set of a different relation,
PRODUCT-OF. Indirect manifestation such as ?overtake? is another cause of ambiguity. Inference in-
volved here is abductive in nature and not always valid. We may be able to infer COMPETE relation from
?X overtake Y?, but ?X overtake Y? can be a consequence of another relation such as COOPERATE.
Such an ambiguous expression belongs to the manifestation sets of more than one relation and thus
would be located in a rather neutral position in the space. Since the common space reflects how frequently
certain expressions are used to link entity-pairs, their positions in the space reflect the relative specificity
to each relation cluster. Figure 2 illustrates how the ambiguity of a relation expression captured in the
common space.
3 Relation Mining and Relation Expression Mining
In an actual situation, both the extension set and the manifestation set of a relation R are only partially
known. To produce more comprehensive sets of these objects from large corpora is generally called
mining. Two mining tasks have been studied so far, which are different, though mutually related.
We define relation mining as a task which, given a relation R, enumerates entity-pairs in the extension
set. Another mining task (i.e. relation expression mining which is often performed as an auxiliary task
of relation mining) is to gather a set of relation expressions which are manifestations of a given R.
3.1 Relation Mining
Relation mining is the task to enumerate entity-pairs of a relation R from a small given set of objects
of a relation R. For example, if a set of relation expressions as the manifestation set of a relation R are
given, one can produce a set of entity-pairs simply by identifying occurrences of relation expressions in
text and producing the entity-pairs which are linked by them. Alternatively, if a small set of entity-pairs
as a subset of the extension set of a relation R are given, one can produce a set of entity-pairs simply by
gathering similar entity-pairs measured by relation expression co-occurrence vectors. These ideas have
been shared by many mining systems called pattern-based relation mining systems.
The recall and precision of such a system are determined by the quality and quantity of the given set.
If the given set is small, a system suffers low recall. On the other hand, if the set is large but contains
many ?ambiguous? or ?weak? objects, a system suffers low precision.
Therefore, one of the keys for success of relation mining is how to gather a large initial set, which are
effective, i.e. objects less ambiguous with high frequency. The common semantic space can be used not
only to generate a comprehensive set but to measure the specificity of objects in terms of a given R, it
also provides refined semantic measures between entity-pairs.
3.2 Relation Expression Mining
We have discussed semantic spaces of relation expressions and the common semantic space as if to
define what constitutes a relation expression is straightforward. However, it is not trivial to define what
constitutes a relation expression.
In the previous section, we treat ?overtake? in ?Apple overtook Samsung in the smart phone market? as
a relation expression which manifests the relation ?COMPETE?. However, one may argue that a pattern
such as ?X overtake Y in . . . market? should be treated as a basic unit of manifestation of the relation
COMPETE. This longer expression is less ambiguous and thus more effective than the shorter pattern of
?overtake?. On the other hand, the frequency of this pattern would be much less and thus less effective,
compared with the shorter version. Mining of effective relation expressions (sometimes called ?pattern
mining?) has to address the problem of balancing the specificity and generality of relation expressions.
Furthermore, one would like to identify the same relation expression in ?Apple announced yesterday that
it had overtaken Samsung which . . .? as in ?Apple overtook Samsung in the smart phone market?.
In the experiments, we do not treat the process of pattern mining seriously. Instead, we used two con-
ventional methods. The first method is to enumerate subsequences of words in the intervening part in a
sentence between two entities, and use them as relation expressions. We expect less effective expressions
as manifestation to be recognized in the common space. Another method is to use the shortest paths in
1583
dependency structures of sentences as relation expressions. Shortest paths can generalize surface variants
of essentially the same relation expressions and reduce unnecessary proliferation of relation expressions.
4 Experiments
This section empirically evaluates our approach of embedding the two original spaces into a common
space. We show that the common space provides a continuous vector space for relation expressions, in
which not only similarities among expressions but also their ambiguities are properly captured.
4.1 Experiment Settings
4.1.1 Dataset
Entity-pair Relation Triplet
Enumeration 12, 174 12, 185 521, 454
Shortest Path 10, 251 92, 797 130, 897
Table 1: The specifications of the ENT
dataset: Sizes of distinct entity-pairs, relation
expressions, and triplets. ?Enumeration? in-
dicates the results of pattern mining based on
word subsequences. ?Shortest Path? shows
that of shortest path extraction.
We use the ENT benchmark dataset (Bollegala et al.,
2009) for our experiments. The dataset consists of
661,502 snippets, which are brief summaries provided
by Web search engines. Most web search engines
provide links to webpages and snippets as search re-
sults and snippets contains a subset of texts including
the query words derived from the webpages. Table
1 shows how many distinct entity pairs, relation ex-
pressions and triplets were extracted as results of NER
and expression extraction (See Section 4.1.2 and 4.1.3).
The dataset is accompanied with 100 entity-pairs that
are classified into five semantic categories: ACQUISI-
TION, HEADQUARTERS, FIELD, CEO, and BIRTHPLACE. We use the ENT dataset not only for
evaluation of relation mining but also for examining the characteristics of the common space for rela-
tion expression mining. Note that, due to the nature of snippets, the dataset is very noisy. It contains
many non-sentences and even non-English texts, which may adversely affect the performance of mining
systems.
4.1.2 Entity and Entity-Pair Extraction
We first extracted entities from the ENT dataset. After splitting snippets into sentences, we applied
named entity recognizer (NER) (Finkel et al., 2005) to recognize entities in sentences. We used Stanford
Core NLP tools 2
1
for sentence splitting and NER. As relevant semantic classes for the ENT dataset,
entities which are recognized as ORGANIZATION, LOCATION, or PERSON are treated as entities in
the further process. We only used sentences in which at least two entities of these three classes appear.
4.1.3 Extraction of Relation Expressions
The definition of relation expressions which link two entities in text is not trivial. We adopt two methods
of extracting candidates of relation expressions, and compare them in experiments.
The first method is to use, as relation expressions, subsequences of words which appear between two
entities. We assume that two entities which appear apart in a sentence by more than 10 words are not
explicitly linked in the sentence. From the word sequence whose length is less than 10, we enumerate all
possible subsequences whose length is less than 6 words. Since a set of such subsequences include many
noises as relation expressions, we use only subsequences the frequency of which is higher than 100.
This shallow approach can be run very fast, thanks to the advances of sequential pattern mining (Pei
et al., 2004). Although the method is similar to that used in Bollegala et al. (2010) , we do not use any
further constraints based on part-of-speech tags, lexical-syntactic information, etc. Our contention is that
such ad-hoc constraints unnecessarily restrict a set of relation expressions. Our method treats ambiguous
expressions (e.g. ?of?, ?in?, ?with?, etc.) as relation expressions. Instead, the effectiveness or the degree
of ambiguities of a relation expression is captured in the common space after embedding.
1
http://nlp.stanford.edu/software/corenlp.shtml
1584
The second method is based on dependency parsing. We obtain the dependency tree of a sentence by a
publicly available deep parser, Enju3
2
(Miyao and Tsujii, 2005; Miyao and Tsujii, 2008), and then extract
shortest paths between two entities. Unlike the first method, this method uses linguistic information to
extract the skeleton of a relation expression.
Each node in shortest paths consists of a base form (e.g., ?like?, ?player?), syntactic category (e.g.,
?verb?, ?noun?), and predicate-argument links. The length of shortest paths was restricted to the range
from 1 to 6. Compared with the first method, a set of shortest paths contains much less noises, so that
we do not filter out those with low frequency. In the same way as the first method, a set of shortest paths
contains highly ambiguous paths (e.g. the path of ?of?).
4.1.4 Generation of the Space for Entity-Pairs
The primal semantic space for entity pairs can be constructed in several ways. The co-training method
constructed a space of entity pairs based on their co-occurrences with relation expressions. Their method
requires the two spaces of entity pairs and relation expressions have to be tightly coupled.
On the other hand, our approach allows us to design the two spaces independently. In addition to
the tightly coupled spaces, we design a new space for entity pairs based on the distributional hypothesis
(Harris, 1954). We used the point-wise mutual information (PMI) score of each word with an entity-pair.
PMI score is defined as PMI = log
e
p(w
a
|?e
i
, e
j
?)/p(w
a
) where p(w
a
) is an occurrence probability of a
word w
a
and p(w
a
|?e
i
, e
j
?) is a conditional probability with respect to an entity-pair ?e
i
, e
j
?. We filtered
words whose PMI scores were below 1.0 and all the rest were used as the features.
To maximize the effectiveness of the space, we performed preliminary experiments by changing pa-
rameters in the definition of context in the distributional hypothesis, such as how the context around
entities is distinguished, whether the whole of a sentence or limited windows around entities are used
as context, etc. As a result, we chose the settings in which right, left, and intervening contexts are dis-
tinguished. We used three different window sizes as the context (e.g. 4, 5 and 6 words). That is, when
we set the window size to 4, we used the four words in the left side of the first entity as the left context,
those in the right side of the second entity as the right context, and the words in the intervening part
as the intervening context. If the intervening part consists of more than 8 words, the four words in the
neighborhood of the two entities are used as the intervening context.
4.1.5 Generation of the Space for Relation Expressions
Following the work (Lin and Pantel, 2001), we constructed a simple space, in which a relation expression
is characterized by the entities which it links. We counted the entities in the left-hand side and the right
hand side of a relation expression. The same as the vector of an entity-pair, we used the PMI score as the
feature value. As for feature selection, we chose the entities whose PMI scores are no less than 1.0
3
.
4.1.6 Dimension Reduction
After generating vectors for entity-pairs and relation expressions, we applied a dimension reduction.
Since both of the primary semantic spaces use surface words or entities, their vectors tend to have a very
large dimension (i.e. about 100, 000 for entity pairs and about 2, 500 for relation expressions). Since
the cardinalities of the two sets of distinct entity pairs and relations expressions are also very high (See
Table 1 of the specification of the ENT dataset), the high dimensions of the two spaces would make the
computation cost of MVPLS embedding in terms of time and space prohibitively high.
To take advantage of the sparseness of both spaces, we used Randomized SVD (Halko et al., 2011)
which can produce low-dimensional feature vectors from a large-scale sparse feature matrix efficiently.
We produced spaces with 3, 000-dimensions for entity-pairs and 1, 000 for relation expressions.
4.1.7 Common Space Embedding
Lastly, we applied MVPLS (1) to construct common space projection matrices. We set the dimension
of common space as 1, 000. We verified that the dimension does not affect much the evaluation results,
2
http://www.nactem.ac.uk/enju/
3
Other than context-based characterization methods, we have applied path kernel method (Reichartz et al., 2009; Reichartz
et al., 2010) to shortest path relations as preliminary works, however, their performances were definitely worse.
1585
Window Size 4 5 6
VSM (Turney, 2005) 0.68
LRA (Turney, 2005) 0.68
(Bollegala et al., 2010) 0.76
Relation (1, 000) 0.82
Original (1, 000) 0.88 0.88 0.88
Embedded (1, 000) 0.90 0.89 0.90
Table 2: Entity-Pair space evaluation results (Enu-
meration) : Each figure shows the average pre-
cision. The best figures in each window size are
written in bold. Figures in parentheses denote the
number of dimensions.
Window Size 4 5 6
VSM (Turney, 2005) 0.68
LRA (Turney, 2005) 0.68
(Bollegala et al., 2010) 0.76
Relation (1, 000) 0.62
Original (1, 000) 0.91 0.90 0.91
Embedded (1, 000) 0.91 0.91 0.91
Table 3: Entity-Pair space evaluation results
(Shortest Path). Each figure shows the average
precision. The best figures in each window size
are written in bold. Figures in parentheses denote
the number of dimensions.
when we set it to larger than 300. So we used a common space with 1, 000 dimensions for the sake of
comparison with the original spaces.
4.2 Relation Mining Evaluation
We evaluated the embedding approach by a quantitative analysis on the relation mining task used in
(Bollegala et al., 2010). The experimental setting is the same as the previous work. The objective is
to assess whether the derived common semantic space provides a good space for measuring semantic
distances among entity-pairs. We expected that in a good semantic space, entity-pairs which belong to
the same semantic category would be clustered in proximity.
We used the ENT dataset (Bollegala et al., 2009). We used the same evaluation measures used in
(Bollegala et al., 2010). The measure assumes that a semantic space would be judged as appropriate if it
assigned higher similarity scores to entity-pairs the relationships of which belong to the same category.
Therefore, the measure evaluated the top 10 similar pairs to each entity-pair and calculated average
precision defined as
?
10
t=1
Rel(t) ? Pre(t)/10. Here, Rel(t) is a binary valued function that returns 1 if
the entity-pair at rank t and ?e
i
, e
j
? have the same semantic category. Pre(t) is the precision at rank t,
which is defined by the percentage of correct objects in top t pairs.
For the sake of comparison, we prepared several models, which used different semantic spaces for
entity pairs. One space (called Relation) is to characterize an entity pair by the relation expressions
which it co-occur. Another space (called Original) is to characterize an entity pair by the context vector
discussed in Section 4.1.4. There are three Original spaces which use different window sizes (4, 5 and 6
words). Then, the final space is the common space obtained by embedding (called Embedded).
Table 2 and 3 correspond to the experiment results using the two definitions of relation expressions,
one by enumerated word sequences and the other by shortest paths. We note that the previous works
only use co-occurrences information and cannot use any context information. The previous work and
Relation have no ways of changing the size of windows. Therefore, these results are independent of
the window size. These tables show the limitation of co-training which can only use tightly coupled
vector spaces for entity pairs and relation expressions. Both the original and the common embedded
space outperform significantly the performance obtained by previous works, regardless of the definitions
of relation expressions (i.e. enumerated subsequence and shortest paths). Since the space for relation
expressions is simple and poor, we expected that it would hardly add extra information to the space of
entity pairs. However, the common space embedded from the two spaces improve the performance.
4.3 Relation Expression Mining
While the primary space for relation expressions is rather poor, vector representations of relation ex-
pressions are much richer in the common space. This is because they receive extra information from
the rich space of entity-pairs through their co-occurrences. For evaluation, we first chose representative
relation expressions, and then gathered relations that are close to them in the primary space of relation
1586
{announce acquisition} {president ,}
Embedded Original Embedded Original
{announce that have acquire} {announce that have acquire} {chairman ,} {?s president be}
{complete acquisition} {acquire} {, ceo &} {would say}
{say have it buy} {pay} {?s president ,} {would that say}
{acquire} {buy} {ceo &} {?s blue and}
{pay} {compra} {chief ,} {?s chairman ,}
{?s acquisition} {buy company} {, ceo )} {chairman ,}
{?s out of} {say that it buy} {chief ,} {palmisano}
{?s purchase} {nor} {would that say} {,}
{acquisition} {do} {executive ,} {, reader ,}
{?s takeover} {announce be buy} {ceo become} {palmisano include door ?}
Table 4: Evaluation of similarity measure between relation expressions. This table shows the top-10
ranked relation expressions that are closest to two representative relation expressions.
expressions and in the common space. If our expectation was correct, the list of expressions close to the
chosen expression in the common space should be more appropriate than that in the primary space.
We show the result of the experiment in which we use shortest paths as relation expressions. We used
the same dataset as the previous experiment. We removed shortest paths with frequency less than 10. As
for the primary space for entity pairs, we use the one with the window size of 5. We used {announce
acquisition} and {president ,} as two representatives.
Table 4 shows the lists of relation expressions closest to the chosen representatives in the common
space and the primary space. For the ease of interpretation, we do not show syntactic categories and
predicates attached to the shortest paths. One can easily see that the common space successfully moved
down many ambiguous expressions such as {compra} and {nor} in {announce acquisition}, and {would
say} and{,} in {president ,}. On the other hand, some relation expressions which are specific and se-
mantically similar to the chosen ones moved up in the rank, for example {?s purchase} and {chief ,}.
We have also conducted the same experiment for relation expressions produced by the enumeration
method. While the enumeration method improves the relation mining which gathering similar entity-
pairs, it gave much poorer results to expressing mining than the shortest paths. This is because the
enumeration method generated a large amount of non-meaningful relation expressions. For example, to
generate a complex relation expression such as {say have it buy} appeared in Table 4, the enumeration
method has to generate a large variety of noisy ones that co-occur with a complex relation expression.
4.4 Similarity measure between entity-pair and relation expressions
The major advantage of embedding over co-training is that it produces where the two different types of
objects, entity-pairs and relation expressions, are treated in the exactly the same vector space. Therefore,
we can easily gather a set of relation expressions relevant to a given prototype entity pair of a relation. In
this experiment, instead of representative relation expressions, we gave entity pairs which are prototyp-
ical examples of certain relations. As in the previous experiment, we used the shortest paths as relation
expressions, and ignored relation expressions with frequency under 10.
Table 5 shows the list of relation expressions for two prototypical entity-pairs used in the ENT dataset,
?charlie chaplin, london? as a representative entity-pair for BIRTHPLACE and ?facebook inc, mark
zuckerberg? as CEO relation semantics. The table shows that the top-10 frequently co-occurring relation
expressions. While many noisy relation expressions (i.e. ambiguous expressions) appear by extracting
expressions based on their co-occurrence frequency with ?charlie chaplin, london?, these ambiguous
expressions disappear in the proximity of the entity-pair in the common space. Moreover, the result of
?facebook inc. mark zuckerberg? shows that some relation expressions that do not co-occur with the
prototype entity-pair were successfully extracted, such as {?s executive ,}.
5 Related Work
Bollegala et al. (2010) proposed a simple sequential co-clustering framework of entity-pairs and relation
expressions for objects sharing the same semantic relation to be clustered. Our definition of primal-dual
1587
?charlie chaplin, london? ?facebook inc, mark zuckerberg?
Embedded Co-occurrence Embedded Co-occurrence
{bear walworth} {bear} {?s executive ,} {, ceo}
{bear april} {?s ? arrangement while lay orchestra} {ceo be} {, ceo (}
{play} {,} {ceo} {founder and}
{bear} {reception} {,} {everything , ceo}
{bear} {?s} {?s president ,} {andceo}
{bear woolsthorpe ,} {?s} {have say} {ceo ,}
{bear woolthrope} {be when} {, ceo ,} {-}
{be member parliament} {and} {, ceo} N/A
{bear woolsthorpe} {bear april street , walworth ,} {ceo become} N/A
{?s} {walk ,} {buy} N/A
Table 5: Relation expressions gathered by prototype entity-pairs on the ENT dataset. This table shows
the top-10 ranked relation expressions that are closest to the representative entity-pairs ?charlie chap-
lin, london? as BIRTHPLACE and ?facebook inc, mark zuckerberg? as CEO. ?facebook inc, mark
zuckerberg? co-occurred with only seven discrete relation expressions.
semantic space and common space embedding approach can be viewed as extensions of their work by
introducing feature spaces as characterizations. This extension enables to utilize each space?s character-
izations and calculate similarity between different types of objects. Baroni and Lenci (2010) proposed a
framework that analyze triplets as a third-order tensor, called ?distributional memory?. By matricizing
the tensor to second-order tensors, that is matrices, this framework can utilize the relationship between
entity-pairs and relation expressions. They also propose the procedure for generating continuous vec-
tor representations of entities and relation expressions through the tensor decomposition techniques.
However, this framework cannot use semantic spaces independently defined, therefore it is difficult to
incorporate the similarity information between entity-pairs or similarities between relation expressions
into the decomposition procedure in contract to our framework based on MVPLS. Lin and Pantel (2001)
proposed a weakly supervised framework of mining paraphrases based on shortest paths as basic units
to be mined. Our work can be viewed as an extension by mixing entity-pair characterizations with the
extended distributional hypothesis by embedding.
Many other previous work have been proposed to construct a knowledge base, including relation
expressions (Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2012). However, they cannot
interactively predict semantic meanings of objects through labeled objects of the other space.
As for treatment of ambiguity, some previous work has focused on triplet clustering to disambiguate
each triplet object known as relation extraction. Unlike other mining tasks, this task requires a system
to disambiguate the meaning of a relation expression r in ?r, e
1
, e
2
? which appears in a specific context.
We did not treat this task in this paper, however, our framework would discharge the burden by showing
the insight of ambiguities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed
a new triplet clustering method through a generative probabilistic model. The model used surrounding
contexts as features in both a sentence and document level to identify the meaning of each triplet. They
demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT
(Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in
an unsupervised way and enables to incorporate various resources about entity and relation expressions.
Chen et al. (2006) proposed a label propagation algorithm for relation extraction as a semi-supervised
learning method by utilizing the information of parsing.
6 Conclusion
We propose a common space embedding framework which constructs a semantic space in which both
entity-pairs and relation expressions are represented. We showed that our framework is effective to con-
struct the extension set and the manifestation set of a relation R in this space. The results of experiments
showed that the common space is further refined for tasks such as relation and relation expression min-
ing, compared with the original two spaces. Moreover, we showed relation expressions collected from a
small set of entity-pairs through the common space, which share the same semantics as being relevant.
1588
There are several interesting future topics:
? how to iteratively collect objects from a dual object, like bootstrapping
? how to reduce surface diversities of relation expressions which are not abstracted away by simple
method or shortest paths (by using methods such as SOL Pattern Model (Nakashole et al., 2012))
? How to combine a ground truth and non-textual knowledge stored in knowledge bases for charac-
terizing entity-pairs with our framework
? How to extend the framework in order to deal with n-ary relations
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?721.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2009. Measuring the similarity between implicit
semantic relations from the web. In Proc. of WWW.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2010. Relational duality: unsupervised extraction
of semantic relations between entities on the web. In Proc. of WWW.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Toward an architecture for never-ending language learning. In Proc. of AAAI.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2006. Relation extraction using label propagation
based semi-supervised learning. In Proc. of ACL.
Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction.
In Proc. of EMNLP.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proc. of ACL.
Nathan Halko, Per G. Martinsson, and Joel A. Tropp. 2011. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217?288.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proc. of KDD.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality. In Proceedings of NIPS.
Bonan Min, Shuming Shi, Ralph Grishman, and Chin-Yew Lin. 2012. Ensemble semantics for large-scale unsu-
pervised relation extraction. In Proc. of EMNLP-CoNLL.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage hpsg parsing. In
Proc. of ACL.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns
with semantic types. In Proc. of EMNLP-CoNLL.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal, and
Mei-Chun Hsu. 2004. Mining sequential patterns by pattern-growth: The prefixspan approach. IEEE Transac-
tions on Knowledge and Data Engineering, 16(11):1424?1440.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proc. of EMNLP.
Frank Reichartz, Hannes Korte, and Gerhard Paass. 2009. Dependency tree kernels for relation extraction from
natural language text. In Proc. of ECML/PKDD (2).
1589
Frank Reichartz, Hannes Korte, and Gerhard Paass. 2010. Semantic relation extraction with kernels over typed
dependency trees. In Proc. of KDD.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, pages 1201?1211.
Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Systems, 21(4):315?346.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy problems. In RANLP, pages 482?489.
Peter D. Turney. 2005. Measuring semantic similarity by latent relational analysis. In IJCAI, pages 1136?1141.
Wei Wu, Hang Li, and Jun Xu. 2013. Learning query and document similarities from click-through bipartite graph
with metadata. In Proc. of WSDM.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using
generative models. In Proc. of EMNLP.
Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Unsupervised relation discovery with sense disam-
biguation. In Proc. of ACL.
1590
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701?1712,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining String and Context Similarity
for Bilingual Term Alignment from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
Automatically compiling bilingual dictio-
naries of technical terms from comparable
corpora is a challenging problem, yet with
many potential applications. In this paper,
we exploit two independent observations
about term translations: (a) terms are of-
ten formed by corresponding sub-lexical
units across languages and (b) a term and
its translation tend to appear in similar lex-
ical context. Based on the first observa-
tion, we develop a new character n-gram
compositional method, a logistic regres-
sion classifier, for learning a string similar-
ity measure of term translations. Accord-
ing to the second observation, we use an
existing context-based approach. For eval-
uation, we investigate the performance of
compositional and context-based methods
on: (a) similar and unrelated languages,
(b) corpora of different degree of compa-
rability and (c) the translation of frequent
and rare terms. Finally, we combine the
two translation clues, namely string and
contextual similarity, in a linear model and
we show substantial improvements over
the two translation signals.
1 Introduction
Bilingual dictionaries of technical terms are re-
sources useful for various tasks, such as computer-
aided human translation (Dagan and Church,
1994; Fung and McKeown, 1997), Statistical Ma-
chine Translation (Och and Ney, 2003) and Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997). In the last two decades, researchers
have focused on automatically compiling bilingual
term dictionaries either from parallel (Smadja et
al., 1996; Van der Eijk, 1993) or comparable cor-
pora (Rapp, 1999; Fung and Yee, 1998). While
parallel corpora contain the same sentences in two
languages, comparable corpora consist of bilin-
gual pieces of text that share some features, only,
such as topic, domain, or time period. Comparable
corpora can be constructed more easily than paral-
lel corpora. Freely available, up-to-date, on-line
resources (e.g., Wikipedia) can be employed.
In this paper, we exploit two different sources
of information to extract bilingual terminology
from comparable corpora: the compositional and
the contextual clue. The compositional clue is
the hypothesis that the representations of a term
in any pair of languages tend to consist of cor-
responding lexical or sub-lexical units, e.g., pre-
fixes, suffices and morphemes. In order to cap-
ture associations of textual units across languages,
we investigate three different character n-gram ap-
proaches, namely a Random Forest (RF) classifier
(Kontonatsios et al., 2014), Support Vector Ma-
chines with an RBF kernel (SVM-RBF) and a Lo-
gistic Regression (LogReg) classifier. Whilst the
previous approaches take as an input monolingual
features and then try to find cross-lingual map-
pings, our proposed method (LogReg classifier)
considers multilingual features, i.e., tuples of co-
occurring n-grams.
The contextual clue is the hypothesis that mu-
tual translations of a term tend to occur in similar
lexical context. Context-based approaches are un-
supervised methods that compare the context dis-
tributions of a source and a target term. A bilin-
gual seed dictionary is used to map context vec-
tor dimensions of two languages. Li and Gaussier
(2010) suggested that the seed dictionary can be
used to estimate the degree of comparability of a
bilingual corpus. Given a seed dictionary, the cor-
pus comparability is the expectation of finding for
each word of the source corpus, its translation in
the target part of the corpus. The performance of
context-based methods has been shown to depend
on the frequency of terms to be translated and the
1701
corpus comparability. In this work, we use an ex-
isting distributional semantics approach to locate
term translations.
Furthermore, we hypothesise that the compo-
sitional and contextual clue are orthogonal, since
the former considers the internal structure of terms
while the latter exploits the surrounding lexical
context. Based on the above hypothesis, we com-
bine the two translation clues in a linear model.
For experimentation, we construct compara-
ble corpora for four language pairs (English-
Spanish, English-French, English-Greek and
English-Japanese) of the biomedical domain.
We choose this domain because a large propor-
tion of the medical terms tends to composition-
ally translate across languages (Lovis et al., 1997;
Namer and Baud, 2007). Additionally, given the
vast amount of newly introduced terms (neolo-
gisms) in the medical domain (Pustejovsky et al.,
2001), term alignment methods are needed in or-
der to automatically update existing resources.
We investigate the following aspects of term
alignment: (a) the performance of compositional
methods on closely related and on distant lan-
guages, (b) the performance of context vectors and
compositional methods when translating frequent
or rare terms, (c) the degree to which the corpus
comparability affects the performance of context-
based and compositional methods (d) the improve-
ments that we can achieve when we combine the
compositional and context clue.
Our experiments show that the performance of
compositional methods largely depends on the dis-
tance between the two languages. The perfor-
mance of the context-based approach is greatly
affected by corpus-specific parameters (the fre-
quency of occurrence of the terms to be translated
and the degree of corpora comparability). It is also
shown that the combination of compositional and
contextual methods performs better than each of
the clues, separately. Combined systems can be
deployed in application environments with differ-
ent language pairs, comparable corpora and seeds
dictionaries.
The LogReg, dictionary extraction method de-
scribed in this paper is freely available
1
.
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/LogReg-TermAlign.tar.gz
2 Related Work
Context-based methods (Fung and Yee, 1998;
Rapp, 1999) adapt the Distributional Hypothesis
(Harris, 1954), i.e., words that occur in similar
lexical context tend to have the same meaning, in
a multilingual environment. They represent the
context of each term t as a context vector, usu-
ally following the bag-of-words model. Each di-
mension of the vector corresponds to a context
word occurring within a predefined window, while
the corresponding value is computed by a corre-
lation metric, e.g., Log-Likelihood Ratio (Morin
et al., 2007; Chiao and Zweigenbaum, 2002) or
Point-wise Mutual Information (Andrade et al.,
2010). A general bilingual dictionary is then used
to translate/project the target context vectors into
the source language. As a result, the source and
target context vectors become directly compara-
ble. In a final step, candidate translations are being
ranked according to a distance metric, e.g., cosine
similarity (Tamura et al., 2012) or Jaccard index
(Zanzotto et al., 2010; Apidianaki et al., 2012).
Whilst context-based methods have become a
common practise for bilingual dictionary extrac-
tion from comparable corpora, nonetheless, their
performance is subject to various factors, one of
which is the quality of the comparable corpus. Li
and Gaussier (2010) introduced the corpus com-
parability metric and showed that it is related to
the performance of context vectors. The higher
the corpus comparability is, the higher the perfor-
mance of context vectors is. Furthermore, context
vector approaches are sensitive to the frequency of
terms. For frequent terms, distributional seman-
tics methods exhibit robust performance since the
corresponding context is more informative. Chiao
and Zweigenbaum (2002) reported an accuracy of
91% for the top 20 candidates when translating
terms that occur 100 times or more. However,
the performance of context vectors drastically de-
creases for lower frequency terms (Kontonatsios et
al., 2014; Morin and Daille, 2010).
Our work is more closely related to a second
class of term alignment methods that exploits the
internal structure of terms between a source and
a target language. Compositional translation al-
gorithms are based on the principal of composi-
tionality (Keenan and Faltz, 1985), which claims
that the translation of the whole is a function of
the translation of its parts. Lexical (Morin and
Daille, 2010; Daille, 2012; Robitaille et al., 2006;
1702
Tanaka, 2002) and sub-lexical (Delpech et al.,
2012) compositional algorithms are knowledge-
rich approaches that proceed in two steps, namely
generation and selection. In the generation step,
an input source term is segmented into basic trans-
lation units: words (lexical compositional meth-
ods) or morphemes (sub-lexical methods). Then
a pre-compiled, seed dictionary of words or mor-
phemes is used to translate the components of the
source term. Finally, a permutation function gen-
erates candidate translations using the list of the
translated segments. In the selection step, candi-
date translations are ranked according to their fre-
quency (Morin and Daille, 2010; Robitaille et al.,
2006) or their context similarity with the source
term (Tanaka, 2002). The performance of the
compositional translation algorithms is bound to
the coverage of the seed dictionary (Daille, 2012).
Delpech et al. (2012) noted that 30% of untrans-
lated terms were due to the low coverage of the
seed dictionary.
Kontonatsios et al. (2014) introduced a Random
Forest (RF) classifier that learns correspondences
of character n-grams between a source and target
language. Unlike lexical and sub-lexical compo-
sitional methods, a RF classifier does not require
a bilingual dictionary of translation units. The
model is able to automatically build correlation
paths between source and target sub-lexical seg-
ments that best discriminate translation from non-
translation pairs. However, being a supervised
method, it still requires a seed bilingual dictio-
nary of technical terms for training. The RF classi-
fier was previously applied on an English-Spanish
comparable corpus and it was shown to signifi-
cantly outperform context-based approaches.
3 Methods
In this section we describe the character n-gram
models, the context vector method and the hybrid
system. The lexicon induction task is formalised
as a two-class classification problem. Given a pair
of terms in a source and a target language, the out-
put is a prediction of whether the terms are mutual
translations are not. Furthermore, each term align-
ment method implements a ranking function that
calculates a similarity score between a source and
a target term. The methods rank target terms ac-
cording to the similarity score and select the top N
ranked terms as candidate translations. The rank-
ing functions will be discussed in the following
subsections.
3.1 Character n-gram models
Let s be a source term containing p character n-
grams (s={s
1
, s
2
, ..., s
p
} s
i
? S, ?i ? [1, p])
and t a target term of q n-grams (t={t
1
, t
2
, ..., t
q
}
t
i
? T , ?i ? [1, q]). We extract charac-
ter n-grams by considering any contiguous, non-
linguistically motivated sequence of characters
that occurs within a window size of [2 ? 5]
2
) for
English, French and Greek. For Japanese, uni-
grams are included (window size of [1 ? 5] be-
cause Japanese terms often contain Kanji (Chi-
nese) characters.
Given the two lists of source and target n-grams,
our objective is to find an underlying relationship
between S and T that best discriminates trans-
lation from non-translation pairs. The RF clas-
sifier was previously shown to exhibit such be-
haviour (Kontonatsios et al., 2014). An RF clas-
sifier (Breiman, 2001) is a collection of decision
trees voting for the most popular class. For a pair
of source and target terms ?s, t?, the RF method
creates feature vectors of a fixed size 2r, i.e., first
order feature space. The first r features are ex-
tracted from the source term, while the last r fea-
tures from the target term. Each feature has a
boolean value (0 or 1) that designates the pres-
ence/absence of the corresponding n-gram in the
input instance.
The ability of the RF to detect latent associa-
tions between S and T relies on the decision trees.
The internal nodes of a decision tree represent the
n-gram features that are linked together in the tree-
hierarchy. Each leaf node of the trees is labelled as
translation or non-translation indicating whether
the parent path of n-gram features is positively or
negatively associated. The classification margin
that we use to rank the candidate translations is
given by a margin function (Breiman, 2001):
mg(X,Y ) = av(I(x) = 1)?av(I(x)) = 0) (1)
where x is an instance ?s, t?, y ? Y = {0, 1} the
class label, I(?) : (s, t) ?? {0, 1} is the indicator
function of a decision tree and av(I(?)) the aver-
age number of trees voting for the same class la-
bel. In our experiments, we used the same settings
as the ones reported in Kontonatsios et al. (2014).
2
we have experiments with larger and narrower window
sizes but this setting resulted in better translation accuracy
1703
We used 140 decision trees and log
2
|2q| + 1 ran-
dom features. For training an RF model, we used
the WEKA platform (Hall et al., 2009).
The second class of machine learning algo-
rithms that we investigate is Support Vector Ma-
chines (SVMs). The simplest version of SVMs
is a linear classifier (linear-SVM) that tries to
place a hyperplane, a decision boundary, that sepa-
rates translation from non-translation instances. A
linear-SVM is a feature agnostic method since the
model only exploits the position of the vectors in
the hyperspace to achieve class separation (Hastie
et al., 2009).
The first order feature representation used with
the RF classifier does not model associations be-
tween S and T . Hence, intuitively, a first or-
der feature space is not linearly separable, i.e.,
there exists no decision boundary that divides the
data points into translations and non-translations.
3
. To solve non-linear classification problems,
SVMs employ non-linear kernels. A kernel func-
tion projects input instances into a higher dimen-
sional space to discover non-linear associations
between the initial features. In this new, projected
feature space, the SVM attempts to define a sep-
arating plane. For training an non-linear SVM on
the first order feature space, we used the LIBSVM
package (Chang and Lin, 2011) with a radial ba-
sis function (RBF) kernel. For ranking candidate
translations, we used the decision value given by
LIBSVM which represents the distance between
an instance and the hyperplane. To translate a
source term, the method ranks candidate transla-
tions by decision value and suggests as best trans-
lation the candidate with the maximum distance
(maximum margin).
While the first order models try to find cross-
lingual mappings between monolingual features,
our proposed method follows a different approach.
It models cross-lingual links between the source
and target character n-grams and uses them as
second order features to train a linear classifier.
A second order feature is a tuple of n-grams in
S and T , respectively, that co-occur in a train-
ing, translation instance. Second order feature
3
We applied a linear-SVM with the first order feature
representation on the four comparable corpora for English-
French, English-Spanish, English-Greek and English-
Japanese. In all cases, the best accuracies achieved were close
to zero. Additionally, the ranked list of candidate translations
was the same for all source terms. Hence, we can empiri-
cally suggest that the linear-SVM cannot exploit a first order
feature space.
values are boolean. Given a translation instance
?s, t? of p source and q target n-grams, there are
p?q second order features. For dimensionality re-
duction, we consider as second order features the
most frequent out of all possible first order feature
combinations, only. Experiments indicate that a
large number of features needs to be considered
to achieve robust performance. To cope with the
high dimensional second order space, we use LI-
BLINEAR (Fan et al., 2008), which is designed
to solve large-scale, linear classifications prob-
lems. LIBLINEAR implements two linear clas-
sification algorithms: LogReg and linear-SVM.
Both models solve the same optimisation problem,
i.e., determine the optimal separating plane, but
they adopt different loss functions. Since LIBLIN-
EAR does not support decision value estimations
for the linear-SVM, we only experimented with
LogReg. Similarly to SVM-RBF, LogReg ranks
candidate translations by classification margin.
3.2 Context vectors
We follow a standard approach to calculate context
similarity of source and target terms (Rapp, 1999;
Morin and Daille, 2010; Morin and Prochasson,
2011a; Delpech et al., 2012). Context vectors
of candidate terms in the source and target lan-
guage are populated after normalising each bilin-
gual corpus, separately. Normalisation consists
of stop-word filtering, tokenisation, lemmatisa-
tion and Part-of-Speech (PoS) tagging. For En-
glish, Spanish and French we used the TreeTagger
(Schmid, 1994) while for Greek we used the ILSP
toolkit (Papageorgiou et al., 2000). The Japanese
corpus was segmented and PoS-tagged using Ju-
man (Kurohashi and Kawahara, 2005).
In succession, monolingual context vectors are
compiled by considering all lexical units that oc-
cur within a window of 3 words before or af-
ter a term (a seven-word window). Only lexical
units (seeds) that occur in a bilingual dictionary
are retained The values in context vectors are Log-
Likelihood Ratio associations (Dunning, 1993) of
the term and a seed lexical unit occurring in it. In
a second step, we use the translations in the seed
dictionary to map target context vectors into the
source vector space. If there are several transla-
tions for a term, they are all considered with equal
weights. Finally, candidate translations are ranked
in descending order of the cosine of the angle be-
tween the mapped target vectors and the source
1704
Trainingcorpus
Testcorpus
character n-grammodel context vectors
hybrid model
Annotate Annotate
Train Project
seed termdictionary seed worddictionary
Figure 1: Architecture of the hybrid term align-
ment system.
vector.
3.3 Hybrid term alignment system
Figure 1 illustrates a block diagram of our term
alignment system. We use two bilingual seed dic-
tionaries: (a) a dictionary of term translation pairs
to train the n-gram models and (b) a dictionary of
word-to-word correspondences to translate target
context vectors. The n-gram and context vector
methods are used separately to score term pairs.
The n-gram model computes the value of the com-
positional clue while the context vector estimates
the score of the contextual clue. The hybrid model
combines both methods by using the correspond-
ing scores as features to train a linear classifier.
For this, we used a linear-SVM of the LIBSVM
package with default values for all parameters.
4 Data
Following previous research (Prochasson and
Fung, 2011; Irvine and Callison-Burch, 2013;
Klementiev et al., 2012), we construct compara-
ble biomedical corpora using Wikipedia as a freely
available resource.
Starting with a list of 4K biomedical English
terms (query-terms), we collected 4K English
Wikipedia articles, by matching query-terms to the
topic signatures of articles. Then, we followed
the Wikipedia interlingual links to retrieve the-
matically related articles in each target language.
Since not all English articles contain links for all
four target languages (Spanish, French, Greek and
Japanese), we used a different list of query-terms
for each language pair. Corpora were randomly
divided into training and testing parts. For train-
ing we used 3K documents and for testing the re-
maining 1K. Table 1 shows the size of corpora in
terms of numbers of source (SW) and target words
(TW).
4.1 Seed dictionaries
As shown in Figure 1, the term alignment methods
require two seed bilingual dictionaries: a term and
a word dictionary. The character n-gram models
rely on a bilingual term dictionary to learn asso-
ciations of n-grams that appear often in technical
terms. The dictionary may contain both single-
word and multi-word terms. For English-Spanish
and English-French we used UMLS (Bodenreider,
2004) while for English-Japanese we used an elec-
tronic dictionary of medical terms (Denshika and
Kenkyukai, 1991).
An English-Greek biomedical dictionary was
not available at the time of conducting these ex-
periments, thus we automatically compiled a dic-
tionary from a parallel corpus. For this, we trained
a standard Statistical Machine Translation system
(Koehn et al., 2007) on EMEA (Tiedemann, 2009),
a biomedical parallel corpus containing sentence-
aligned documents from the European Medicines
Agency. Then, we extracted all English-Greek
pairs for which: (a) the English sequence was
listed in UMLS and (b) the translation probability
was equal or higher to 0.7.
The sizes of the seed term dictionaries vary sig-
nificantly, e.g., 500K entries for English-French
but only 20K entries for English-Greek. How-
ever, the character n-gram models require a rela-
tively small portion of the corresponding dictio-
nary to converge. In the reported experiments,
we used 10K translation pairs as positive, train-
ing instances. In addition, we generated an equal
number of pseudo-negative instances by randomly
matching non-translation terms.
Morin and Prochasson (2011b) showed that the
translation accuracy of context vectors is higher
when using bilingual dictionaries that contain both
general language entries and technical terms rather
than general or domain-specific dictionaries, sep-
1705
Training corpus Test Corpus
# SW # TW # SW # TW
en-fr 4.8M 2.2M 1.9M 1.1M
en-es 4.9M 2.5M 1.8M 0.9M
en-el 10.2M 2.4M 3.3M 1.3M
en-jpn 5.3M 2.4M 2.3M 1.2M
Table 1: Statistics of the English-French (en-
fr), Engish-Spanish (en-es), English-Greek (en-
el) and English-Japanese (en-jpn) Wikipedia com-
parable corpora. SW: source words, TW: target
words
Corpus Seed words
Comparability in dictionary
en-fr 0.71 66K
en-es 0.75 40K
en-el 0.68 22K
en-jpn 0.49 57K
Table 2: Corpus comparability and number of fea-
tures of the seed word dictionaries
arately. In a mixed dictionary, lexical units are
either single-word technical terms, such as ?dis-
ease? and ?patient?, or general language words,
such as ?occur? and ?high?. Note that we have
already compiled a seed term dictionary for each
pair of languages. Following the suggestion of
Morin and Prochasson (2011b), we attempt to en-
rich the seed term dictionaries with general lan-
guage entries. For this, we extracted bilingual
word dictionaries for English-Spanish, English-
French and English-Greek by applying GIZA++
(Och and Ney, 2003) on the EMEA corpus. We
then concatenated the word with the term dictio-
naries to obtain enhanced seeds for the three lan-
guage pairs. For English-Japanese, we only used
the term dictionary to translate the target context
vectors.
Once the word dictionaries have been compiled,
we compute the corpus comparability measure. Li
and Gaussier (2010) define corpus comparability
as the percentage of words that can be translated
bi-directionally, given a seed dictionary.
Table 2 shows corpus comparability scores of
the four corpora accompanied with the number
of English, single words in the seed dictionar-
ies. It can be observed that seed dictionary sizes
are not necessarily proportional to the correspond-
ing corpus comparability scores. As expected, for
English-Japanese, corpus comparability is low be-
cause the dictionary contains single-word terms,
only. The English-Spanish dictionary is smaller
than the English-French but achieved higher cor-
pus comparability, i.e., a higher percentage of
words can be bi-directionally translated using the
corresponding seed dictionary. A possible ex-
planation is that the comparable corpora were
constructed using different lists of query-terms.
Hence, the query-terms used for English-Spanish
retrieved a more coherent corpus. The resulting
values of corpus comparability indicate that the
context vectors will perform the best for English-
Spanish while for English-Japanese the perfor-
mance is expected to be substantially lower.
4.2 Training and evaluation datasets
For evaluation, we construct a test dataset of
single-word terms, in particular nouns or adjec-
tives. The dataset contains 1K terms that occur
more frequently than 20 but not more than 200
times and are listed in the English part of the
UMLS. In order to extract candidate translations,
we considered all nouns or adjectives that occur
at least 5 times in the target part of the corpus.
Furthermore, we do not constraint the evaluation
datasets only to those terms whose corresponding
translation occurs in the corpus.
The hybrid model that combines the composi-
tional and context clue, is based on a two-feature
model. Therefore, the model converges using only
a few hundred instances. For training a hybrid
model, we used 1K translation instances that oc-
curred in the training comparable corpora. Sim-
ilarly, to the character n-gram models, pseudo-
negative instances were generated by randomly
coupling non-translation terms. The ratio of posi-
tive to negative instances is 1 : 1.
5 Experiments
In this section, we present three experiments con-
ducted to evaluate the character n-gram, con-
text vector and hybrid methods. Firstly, we
examine the performance of the n-gram mod-
els on closely related language pairs (English-
French, English-Spanish), on a distant language
pair (English-Greek) and on an unrelated language
pair (English-Japanese). English and Greek are
not unrelated because they are members of the
same language family, but also not closely re-
lated because they use different scripts. Secondly,
1706
we compare the character n-gram methods against
context vectors when translating frequent or rare
terms and on comparable corpora of similar lan-
guage pairs (English-French, English-Spanish) but
of different corpus comparability scores. Thirdly,
we evaluate the hybrid method on all four com-
parable corpora and investigate the improvement
margin of combining the contextual with the com-
positional clue.
As evaluation metrics, we adopt the top-N
translation accuracy, following most previous ap-
proaches (Rapp, 1999; Chiao and Zweigenbaum,
2002; Morin et al., 2007; Tamura et al., 2012). The
top-N translation accuracy is defined as the per-
centage of source terms for which a given method
has output the correct translation among the top N
candidate translations.
5.1 Character n-gram models
In the first experiment, we investigate the perfor-
mance of the character n-gram models consider-
ing an increasing number of features. The features
were sorted in order of decreasing frequency of oc-
currence. Starting from the top of the list, more
features were incrementally added and translation
accuracy was recorded.
Figure 2 shows the top-20 translation accu-
racy of single-word terms on an increasing num-
ber of first and second order features. With re-
gards to the first order models (Subfigure 2a),
the Random Forest (RF) classifier outperforms
our baseline method (SVM-RBF) for all four lan-
guage pairs. The largest margin between RF and
SVM-RBF can be observed for the English-Greek
dataset while for closely related language pairs,
i.e., English-French and English-Spanish, the mar-
gin is smaller. Furthermore, it can be noted that
using only a small number of first order features,
1K features (500 for the source and 500 for the
target language, both n-gram models reach a sta-
ble performance.
In contrast to the first order models, the Lo-
gReg classifier requires a large number of sec-
ond order features to achieve a robust performance
(Subfigure 2b). Starting from 100K features, the
translation accuracy continuously increases. The
best performance is observed for a total number
of 4M second order features when considering
the English-French, English-Spanish and English-
Greek datasets. For English-Japanese, the best
performance is achieved for 2M features. Beyond
this point, translation accuracy decreases slightly.
After feature selection is performed, we directly
compare all the character n-gram models. Table 3
summarises performance achieved by the LogReg,
RF and SVM-RBF models. It can be noted that
LogReg and RF performed similarly for closely
related languages (no statistically significant dif-
ferences were observed) while both methods out-
performed the SVM-RBF. However, for English-
Greek and English-Japanese, LogReg achieved
a statistically significant improvement over the
translation accuracy of RF and SVM-RBF. Lo-
gReg outperformed RF by 7% for English-Greek,
while for English-Japanese the improvement was
10% and 17% percent for top-1 and top-20 accu-
racy, respectively. Finally, it can be observed that
the more distant the language pair is, the lower the
performance.
5.2 N-gram methods and context vectors
In this experiment, we compare the n-gram meth-
ods against context vectors with regards to two pa-
rameters: (a) the frequency of source terms to be
translated and (b) corpus comparability. English-
French and English-Spanish are similar language
pairs but the corresponding corpora are of dif-
ferent corpus comparability scores. To investi-
gate how performance is affected by term occur-
rence frequency, we compiled an additional test
dataset of 1K rare English terms in the frequency
range [10, 20]. Our intuition is, that character n-
gram methods will perform similarly for all set-
tings since character n-grams are corpus indepen-
dent features.
We compare (a) the character n-gram models
(LogReg, RF and SVM-RBF) with (b) the con-
text vector method (context) and (c) an upper
bound. The latter represents the percentage of
source terms for which a reference translation ac-
tually occurs in the target corpus. Hence, the up-
per bound is the maximum performance achiev-
able according to the reference evaluation.
Figure 3a shows the top-20 translation accu-
racy for high and medium frequency terms, within
the frequency range [20, 200]. Context vectors
achieved a robust performance of 52% and 45%
for English-Spanish and English-French, respec-
tively. The difference in corpus comparability
can explain this 7% margin between these perfor-
mances. As shown in Table 2, the corpus com-
parability scores for English-Spanish and English-
1707
0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 400 600 800 1000
trans
lation
 accu
racy 
@ 20
# first order features
RF (en-jpn)SVM-RBF (en-jpn)RF (en-el)SVM-RBF (en-el)
RF (en-fr)SVM-RBF (en-fr)RF (en-es)SVM-RBF (en-es)
(a) First order n-gram models
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
100 200 300 400 500 1000 2000 3000 4000
trans
lation
 accu
racy 
@ 20
# second order features (x10^3)
en-jpn en-el en-fr en-es
(b) Second order n-gram model
Figure 2: Top-20 translation accuracy of models trained on (a) first and (b) second order features
English-French English-Spanish English-Greek English-Japanese
acc@1 acc@20 acc@1 acc@20 acc@1 acc@20 acc@1 acc@20
LogReg 0.45 0.61 0.42 0.62 0.3 0.48 0.25 0.41
RF 0.47 0.58 0.43 0.59 0.23 0.41 0.15 0.24
SVM-RBF 0.38 0.51 0.33 0.53 0.1 0.25 0.06 0.16
Table 3: Top-1 (acc@1) and top-20 (acc@20) translation accuracy of LogReg, RF and SVM-RBF
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(a) Test terms with frequency [20, 200]
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(b) Test terms with frequency [10, 20]
Figure 3: Top-20 translation accuracy of terms in the frequency range of [10, 200] and [10, 20]
French are 0.75 and 0.71, respectively. In contrast
to context vectors, the character n-gram methods
performed comparably.
A second factor that affects the performance of
context vectors, is the frequency of the terms to
be translated. The translation of rare terms has
been shown to be a challenging case for context
vectors. For example, Morin and Daille (2010)
reported low accuracy (21% for the top-20 can-
didates) of context vectors for terms occurring 20
times or less. In our experiments, Figure 3b illus-
trates accuracies achieved for less frequent terms
([10, 20]). The performance of context vectors is
significantly lower, 26% for English-Spanish and
21% for English-French. Furthermore, the trans-
lation accuracy of the n-gram methods decreases
slightly (? 5% to 8%). This can be explained
by the decrease of the upper bound for lower fre-
quency terms (? 3% to 6%).
5.3 Combining internal and contextual
similarity
We have hypothesised that the compositional and
contextual clue are orthogonal, i.e., they convey
1708
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(a) Top-20 accuracy (acc@20)
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(b) Top-1 accuracy (acc@1)
Figure 4: Overall performance. Top-20 and top-1 translation accuracy
different and possibly complimentary information.
To investigate this intuition, we evaluate the hybrid
model on all four comparable corpora, for term oc-
currence frequencies in [20, 200].
Figure 4a illustrates top-20 translation accu-
racy scores for (a) the character n-gram models,
(b) the context vector method and (c) the hy-
brid models, i.e., LogReg+Context, RF+Context,
SVM-RBF+Context. We observe that the com-
bination of the compositional and contextual clue
improved the performance of all methods. The hy-
brid model largely improved the performance of
the SVM-RBF (? 14% to 20%). With regards
to the combined signals the translation accuracy
of LogReg and RF increased by ? 4% for the
English-Japanese corpus and ? 8% for all other
corpora.
For the top 1 candidate translation, we observe
in Figure 4 smaller improvements achieved by the
hybrid model in comparison to the top-20 accu-
racy. Interestingly, the RF classifier performed
slightly better on its own for English-French,
English-Spanish and English-Japanese. This in-
dicates that the hybrid method ranks more correct
translations in the top 20 candidates but it does not
always assign the best score to the correct answer.
6 Discusion and Future work
In this paper, we investigated a compositional
and a context-based approach useful for compil-
ing bilingual dictionaries of terms automatically
from comparable corpora. Compositional transla-
tion methods exploit the internal structure of terms
across languages while context-based approaches
investigate the surrounding lexical context.
We proposed a character n-gram composi-
tional method, i.e., a Logistic Regression clas-
sifier, which uses a multilingual representation,
i.e., source and target terms. Experimental evi-
dence showed that the LogReg classifier signifi-
cantly outperformed the baseline methods on dis-
tant languages. For closely related languages, Lo-
gReg performed comparably to an existing n-gram
method based on a Random Forest classifier.
Furthermore, we compared the n-gram models
against a context-based approach under different
corpus-specific parameters: (a) corpus compara-
bility, which is relevant to the seed dictionary, and
(b) the occurrence frequency of the terms to be
translated. It was shown that the performance of
n-gram methods was not affected by different pa-
rameter settings. Only small fluctuations were ob-
served, since the n-gram methods are based on
corpus-independent features, only. In contrast,
the context-based method was affected by corpus
comparability scores. The corresponding transla-
tion accuracy declined significantly for rare terms.
Finally, we hypothesised that the n-gram and
context-based methods provide complimentary in-
formation. To test this hypothesis, we developed a
hybrid method that combines compositional and
contextual similarity scores as features in a lin-
ear classifier. The hybrid model achieved signif-
icantly better top-20 translation accuracy than the
two methods separately but minor improvements
were observed in terms of top-1 accuracy.
As future work, we plan to improve the qual-
ity of the extracted dictionary further by exploiting
additional translation signals. For example, previ-
ous works (Schafer and Yarowsky, 2002; Klemen-
tiev et al., 2012) have reported that the temporal
and topic similarity are clues that indicate transla-
tion equivalence. It would be interesting to investi-
gate the contribution of different clues for various
1709
experimental parameters, e.g., domain, distance of
languages, types of comparable corpora.
Acknowledgements
The authors would like to thank Dr. Danushka
Bollegala for providing feedback on this paper
and the three anonymous reviewers for their useful
comments and suggestions. This work was funded
by the European Community?s Seventh Frame-
work Program (FP7/2007-2013) [grant number
318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages 19?27. Asso-
ciation for Computational Linguistics.
Marianna Apidianaki, Nikola Ljube?sic, and Darja
Fi?ser. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267?
D270.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Ido Dagan and Ken Church. 1994. Termight: Identi-
fying and translating technical terminology. In Pro-
ceedings of the fourth conference on Applied natural
language processing, pages 34?40. Association for
Computational Linguistics.
Emmanuel Morin B??eatrice Daille. 2012. Revising the
compositional method for terminology acquisition
from comparable corpora. COLING 2012, 1810.
Estelle Delpech, B?eatrice Daille, Emmanuel Morin,
and Claire Lemaire. 2012. Extraction of domain-
specific bilingual lexicon from comparable corpora:
Compositional translation and ranking. In COLING,
pages 745?762.
Igakuyo Denshika and Jisho Kenkyukai. 1991.
250,000 medical term dictionary (in japanese).
Nichigai Associates, Inc.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational lin-
guistics, 19(1):61?74.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Pascale Fung and Kathleen McKeown. 1997. A
technical word-and term-translation aid using noisy
parallel corpora across language groups. Machine
Translation, 12(1-2):53?87.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Z.S. Harris. 1954. Distributional structure. Word.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of NAACL-
HLT, pages 518?523.
Edward L Keenan and Leonard M Faltz. 1985.
Boolean semantics for natural language, volume 23.
Springer.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
1710
pages 177?180. Association for Computational Lin-
guistics.
G. Kontonatsios, I. Korkontzelos, J. Tsujii, and S. Ana-
niadou. 2014. Using a random forest classifier
to compile bilingual dictionaries of technical terms
from comparable corpora. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 111?116. Association for Com-
putational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2005.
Japanese morphological analysis system juman ver-
sion 5.1 manual.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 644?652. Association for Computational
Linguistics.
Christian Lovis, R Baud, PA Michel, JR Scherrer, and
AM Rassinoux. 1997. Building medical dictionar-
ies for patient encoding systems: A methodology. In
Artificial Intelligence in Medicine, pages 373?380.
Springer.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011a.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34. Association for Computational Lin-
guistics.
Emmanuel Morin and Emmanuel Prochasson. 2011b.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2):226?
233.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Harris Papageorgiou, Prokopis Prokopidis, Voula
Giouli, and Stelios Piperidis. 2000. A unified pos
tagging architecture and its application to greek. In
Proceedings of the 2nd Language Resources and
Evaluation Conference, pages 1455?1462, Athens,
June. European Language Resources Association.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned compara-
ble documents. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1327?1335. Association for Computational
Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling french-japanese terminologies from the
web. In EACL.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In proceedings of the 6th con-
ference on Natural language learning-Volume 20,
pages 1?7. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Frank Smadja, Kathleen R McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational linguistics, 22(1):1?38.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Takaaki Tanaka. 2002. Measuring the similarity be-
tween compound nouns in different languages us-
ing non-parallel corpora. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, pages 1?7. Association for
Computational Linguistics.
1711
J?org Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248.
Pim Van der Eijk. 1993. Automating the acquisition of
bilingual terminology. In Proceedings of the sixth
conference on European chapter of the Association
for Computational Linguistics, pages 113?119. As-
sociation for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1263?1271, Stroudsburg, PA,
USA. Association for Computational Linguistics.
1712
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 430?438,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Coordination Structure Analysis using Dual Decomposition
Atsushi Hanamoto 1 Takuya Matsuzaki 1
1. Department of Computer Science, University of Tokyo, Japan
2. Web Search & Mining Group, Microsoft Research Asia, China
{hanamoto, matuzaki}@is.s.u-tokyo.ac.jp
jtsujii@microsoft.com
Jun?ichi Tsujii 2
Abstract
Coordination disambiguation remains a dif-
ficult sub-problem in parsing despite the
frequency and importance of coordination
structures. We propose a method for disam-
biguating coordination structures. In this
method, dual decomposition is used as a
framework to take advantage of both HPSG
parsing and coordinate structure analysis
with alignment-based local features. We
evaluate the performance of the proposed
method on the Genia corpus and the Wall
Street Journal portion of the Penn Tree-
bank. Results show it increases the per-
centage of sentences in which coordination
structures are detected correctly, compared
with each of the two algorithms alone.
1 Introduction
Coordination structures often give syntactic ambi-
guity in natural language. Although a wrong anal-
ysis of a coordination structure often leads to a
totally garbled parsing result, coordination disam-
biguation remains a difficult sub-problem in pars-
ing, even for state-of-the-art parsers.
One approach to solve this problem is a gram-
matical approach. This approach, however, of-
ten fails in noun and adjective coordinations be-
cause there are many possible structures in these
coordinations that are grammatically correct. For
example, a noun sequence of the form ?n0 n1
and n2 n3? has as many as five possible struc-
tures (Resnik, 1999). Therefore, a grammatical
approach is not sufficient to disambiguate coor-
dination structures. In fact, the Stanford parser
(Klein and Manning, 2003) and Enju (Miyao and
Tsujii, 2004) fail to disambiguate a sentence I am
a freshman advertising and marketing major. Ta-
ble 1 shows the output from them and the correct
coordination structure.
The coordination structure above is obvious to
humans because there is a symmetry of conjuncts
(-ing) in the sentence. Coordination structures of-
ten have such structural and semantic symmetry
of conjuncts. One approach is to capture local
symmetry of conjuncts. However, this approach
fails in VP and sentential coordinations, which
can easily be detected by a grammatical approach.
This is because conjuncts in these coordinations
do not necessarily have local symmetry.
It is therefore natural to think that consider-
ing both the syntax and local symmetry of con-
juncts would lead to a more accurate analysis.
However, it is difficult to consider both of them
in a dynamic programming algorithm, which has
been often used for each of them, because it ex-
plodes the computational and implementational
complexity. Thus, previous studies on coordina-
tion disambiguation often dealt only with a re-
stricted form of coordination (e.g. noun phrases)
or used a heuristic approach for simplicity.
In this paper, we present a statistical analysis
model for coordination disambiguation that uses
the dual decomposition as a framework. We con-
sider both of the syntax, and structural and se-
mantic symmetry of conjuncts so that it outper-
forms existing methods that consider only either
of them. Moreover, it is still simple and requires
onlyO(n4) time per iteration, where n is the num-
ber of words in a sentence. This is equal to that
of coordination structure analysis with alignment-
based local features. The overall system still has a
quite simple structure because we need just slight
modifications of existing models in this approach,
430
Stanford parser/Enju
I am a ( freshman advertising ) and (
marketing major )
Correct coordination structure
I am a freshman ( ( advertising and mar-
keting ) major )
Table 1: Output from the Stanford parser, Enju and the
correct coordination structure
so we can easily add other modules or features for
future.
The structure of this paper is as follows. First,
we describe three basic methods required in the
technique we propose: 1) coordination structure
analysis with alignment-based local features, 2)
HPSG parsing, and 3) dual decomposition. Fi-
nally, we show experimental results that demon-
strate the effectiveness of our approach. We com-
pare three methods: coordination structure anal-
ysis with alignment-based local features, HPSG
parsing, and the dual-decomposition-based ap-
proach that combines both.
2 Related Work
Many previous studies for coordination disam-
biguation have focused on a particular type of NP
coordination (Hogan, 2007). Resnik (1999) dis-
ambiguated coordination structures by using se-
mantic similarity of the conjuncts in a taxonomy.
He dealt with two kinds of patterns, [n0 n1 and
n2 n3] and [n1 and n2 n3], where ni are all nouns.
He detected coordination structures based on sim-
ilarity of form, meaning and conceptual associa-
tion between n1 and n2 and between n1 and n3.
Nakov and Hearst (2005) used the Web as a train-
ing set and applied it to a task that is similar to
Resnik?s.
In terms of integrating coordination disam-
biguation with an existing parsing model, our ap-
proach resembles the approach by Hogan (2007).
She detected noun phrase coordinations by find-
ing symmetry in conjunct structure and the depen-
dency between the lexical heads of the conjuncts.
They are used to rerank the n-best outputs of the
Bikel parser (2004), whereas two models interact
with each other in our method.
Shimbo and Hara (2007) proposed an
alignment-based method for detecting and dis-
ambiguating non-nested coordination structures.
They disambiguated coordination structures
based on the edit distance between two conjuncts.
Hara et al(2009) extended the method, dealing
with nested coordinations as well. We used their
method as one of the two sub-models.
3 Background
3.1 Coordination structure analysis with
alignment-based local features
Coordination structure analysis with alignment-
based local features (Hara et al 2009) is a hy-
brid approach to coordination disambiguation that
combines a simple grammar to ensure consistent
global structure of coordinations in a sentence,
and features based on sequence alignment to cap-
ture local symmetry of conjuncts. In this section,
we describe the method briefly.
A sentence is denoted byx = x1...xk, where xi
is the i-th word of x. A coordination boundaries
set is denoted by y = y1...yk, where
yi =
?
?
?
?
?
?
?
?
?
?
?
(bl, el, br, er) (if xi is a coordinating
conjunction having left
conjunct xbl ...xel and
right conjunct xbr ...xer)
null (otherwise)
In other words, yi has a non-null value
only when it is a coordinating conjunction.
For example, a sentence I bought books and
stationary has a coordination boundaries set
(null, null, null, (3, 3, 5, 5), null).
The score of a coordination boundaries set is
defined as the sum of score of all coordinating
conjunctions in the sentence.
score(x,y) =
k
?
m=1
score(x, ym)
=
k
?
m=1
w ? f(x, ym) (1)
where f(x, ym) is a real-valued feature vector of
the coordination conjunct xm. We used almost the
same feature set as Hara et al(2009): namely, the
surface word, part-of-speech, suffix and prefix of
the words, and their combinations. We used the
averaged perceptron to tune the weight vector w.
Hara et al(2009) proposed to use a context-
free grammar to find a properly nested coordina-
tion structure. That is, the scoring function Eq (1)
431
COORD Coordination.
CJT Conjunct.
N Non-coordination.
CC Coordinating conjunction like ?and?.
W Any word.
Table 2: Non-terminals
Rules for coordinations:
COORDi,m ? CJTi,jCCj+1,k?1CJTk,m
Rules for conjuncts:
CJTi,j ? (COORD|N)i,j
Rules for non-coordinations:
Ni,k ? COORDi,jNj+1,k
Ni,j ?Wi,i(COORD|N)i+1,j
Ni,i ?Wi,i
Rules for pre-terminals:
CCi,i ? (and|or|but|, |; |+|+/?)i
CCi,i+1 ? (, |; )i(and|or|but)i+1
CCi,i+2 ? (as)i(well)i+1(as)i+2
Wi,i ? ?i
Table 3: Production rules
is only defined on the coordination structures that
are licensed by the grammar. We only slightly ex-
tended their grammar for convering more variety
of coordinating conjunctions.
Table 2 and Table 3 show the non-terminals and
production rules used in the model. The only ob-
jective of the grammar is to ensure the consistency
of two or more coordinations in a sentence, which
means for any two coordinations they must be ei-
ther non-overlapping or nested coordinations. We
use a bottom-up chart parsing algorithm to out-
put the coordination boundaries with the highest
score. Note that these production rules don?t need
to be isomorphic to those of HPSG parsing and
actually they aren?t. This is because the two meth-
ods interact only through dual decomposition and
the search spaces defined by the methods are con-
sidered separately.
This method requires O(n4) time, where n is
the number of words. This is because there are
O(n2) possible coordination structures in a sen-
tence, and the method requires O(n2) time to get
a feature vector of each coordination structure.
3.2 HPSG parsing
HPSG (Pollard and Sag, 1994) is one of the
linguistic theories based on lexicalized grammar
sign
PHON list of string
SYNSEM
synsem
LOCAL
local
CAT
category
HEAD
head
MODL synsem
MODR synsem
SUBJ list of synsem
COMPS list of synsem
SEM semantics
NONLOC
nonlocal
REL list of local
SLASH list of local
Figure 1: HPSG sign
2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >
1
HEAD
SUBJ    < >
COMPS  < >
1
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)
and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema
and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing
of the sentence ?Spring has come.? First, each
of the lexical entries for ?has? and ?come? are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.
3 Acquiring HPSG from the Penn
Treebank
As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are
1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.
Spring
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS <                        >
5
has
HEAD  verb
SUBJ  <                         >
COMPS  < >
come
HEAD  verb
SUBJ  <    >
COMPS  < >
5
HEAD  noun
SUBJ < >
COMPS  < >
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
UnifyUnify
Head-complement
schema
Lexical entries
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 3: HPSG parsing
required because HPSG schemata are not injec-
tive, i.e., daughters? signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-
ment in terms of the four phases: specification,
externalization, extraction, and verification.
3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-
Figure 1: subject-head schema (left) and head-
complement schema (right); taken from Miyao et al
(2004).
formalism. In a lexicalized grammar, quite a
small numbers of schemata are used to explain
general grammatical constraints, compared with
other theories. On the other hand, rich word-
specific characteristics are embedded in lexical
entries. Both of schemata and lexical entries
are represented by typed feature structures, and
constraints in parsing are checked by unification
among them. Figure 1 shows examples of HPSG
schema.
Figure 2 shows an HPSG parse tree of the s n-
tence ?Spring has come.? Fi st, the lexical en-
tries of ?has? and ?come? are joined by head-
complement schema. Unification gives the HPSG
sign of mother. After applying schemata to HPSG
signs repeatedly, the HPSG sign of the whole sen-
tence is output.
We use Enju for an English HPSG parser
(Miyao et al 2004). Figure 3 shows how a co-
ordination tructure is built in the Enju grammar.
First, a coordinating conju ction and the right
conjunct are joined by coord right schema. Af-
terwards, the parent and the left conjunct are
joined by coord left schema.
The Enju parser is equipped with a disam-
biguation model trained by the maximum entropy
method (Miyao and Tsujii, 2008). Since we do
not need the probability of each parse tree, we
treat the model just as a linear model that defines
the score of a parse tree as the sum of feature
weights. The features of the model are defined
on local subtrees of a parse tree.
The Enju parser takes O(n3) time since it uses
the CKY algorithm, and each cell in the CKY
parse table has at most a constant number of edges
because we use beam search algorithm. Thus, we
can regard the parser as a decoder for a weighted
CFG.
3.3 Dual decomposition
Dual decomposition is a classical method to solve
complex optimization problems that can be de-
432
sign
PHON list of string
SYNSEM
synsem
LOCAL
local
CAT
category
HEAD
head
MODL synsem
MODR synsem
SUBJ list of synsem
COMPS list of synsem
SEM semantics
NONLOC
nonlocal
REL list of local
SLASH list of local
Figure 1: HPSG sign
2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >
1
HEAD
SUBJ    < >
COMPS  < >
1
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)
and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema
and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing
of the sentence ?Spring has come.? First, each
of the lexical entries for ?has? and ?come? are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.
3 Acquiring HPSG from the Penn
Treebank
As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are
1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.
Spring
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS <                        >
5
has
HEAD  verb
SUBJ  <                         >
COMPS  < >
come
HEAD  verb
SUBJ  <    >
COMPS  < >
5
HEAD  noun
SUBJ < >
COMPS  < >
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
UnifyUnify
Head-complement
schema
Lexical entries
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 3: HPSG parsing
required because HPSG schemata are not injec-
tive, i.e., daughters? signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-
ment in terms of the four phases: specification,
externalization, extraction, and verification.
3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-
Figure 2: HPSG parsing; taken from Miyao et al
(2004).
Coordina(on
Le3,Conjunct Par(al,Coordina(on
Coordina(ng,Conjunc(on Right,Conjunct
? coord_right_schema 
? coord_left_schema 
Figure 3: Construction of coordination in Enju
composed into efficiently solvable sub-problems.
It is becoming popular in the NLP community
and has been shown to work effectively on sev-
eral NLP tasks (Rush et al 2010).
We consider an optimization problem
argmax
x
(f(x) + g(x)) (2)
which is difficult to solve (e.g. NP-hard), while
argmaxx f(x) and argmaxx g(x) are effectively
solvable. In dual decomposition, we solve
min
u
max
x,y
(f(x) + g(y) + u(x? y))
instead of the original problem.
To find the minimum value, we can use a sub-
gradient method (Rush et al 2010). The subgra-
dient method is given in Table 4. As the algorithm
u(1) ? 0
for k = 1 to K do
x(k) ? argmaxx(f(x) + u(k)x)
y(k) ? argmaxy(g(y)? u(k)y)
if x = y then
return u(k)
end if
u(k+1) ? uk ? ak(x(k) ? y(k))
end for
return u(K)
Table 4: The subgradient method
shows, you can use existing algorithms and don?t
need to have an exact algorithm for the optimiza-
tion problem, which are features of dual decom-
position.
If x(k) = y(k) occurs during the algorithm, then
we simply take x(k) as the primal solution, which
is the exact answer. If not, we simply take x(K),
the answer of coordination structure analysis with
alignment-based features, as an approximate an-
swer to the primal solution. The answer does not
always solve the original problem Eq (2), but pre-
vious works (e.g., (Rush et al 2010)) has shown
that it is effective in practice. We use it in this
paper.
4 Proposed method
In this section, we describe how we apply dual
decomposition to the two models.
4.1 Notation
We define some notations here. First we describe
weighted CFG parsing, which is used for both
coordination structure analysis with alignment-
based features and HPSG parsing. We follows the
formulation by Rush et al (2010). We assume a
context-free grammar in Chomsky normal form,
with a set of non-terminals N . All rules of the
grammar are either the form A? BC or A? w
where A,B,C ? N and w ? V . For rules of the
form A? w we refer to A as the pre-terminal for
w.
Given a sentence with n words, w1w2...wn, a
parse tree is a set of rule productions of the form
?A ? BC, i, k, j? where A,B,C ? N , and
1 ? i ? k ? j ? n. Each rule production rep-
resents the use of CFG rule A? BC where non-
terminal A spans words wi...wj , non-terminal B
433
spans word wi...wk, and non-terminal C spans
word wk+1...wj if k < j, and the use of CFG
rule A? wi if i = k = j.
We now define the index set for the coordina-
tion structure analysis as
Icsa = {?A? BC, i, k, j? : A,B,C ? N,
1 ? i ? k ? j ? n}
Each parse tree is a vector y = {yr : r ? Icsa},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Therefore, each parse tree is repre-
sented as a vector in {0, 1}m, where m = |Icsa|.
We use Y to denote the set of all valid parse-tree
vectors. The set Y is a subset of {0, 1}m.
In addition, we assume a vector ?csa = {?csar :
r ? Icsa} that specifies a score for each rule pro-
duction. Each ?csar can take any real value. The
optimal parse tree is y? = argmaxy?Y y ? ?csa
where y ? ?csa =
?
r yr ? ?csar is the inner product
between y and ?csa.
We use similar notation for HPSG parsing. We
define Ihpsg , Z and ?hpsg as the index set for
HPSG parsing, the set of all valid parse-tree vec-
tors and the weight vector for HPSG parsing re-
spectively.
We extend the index sets for both the coor-
dination structure analysis with alignment-based
features and HPSG parsing to make a constraint
between the two sub-problems. For the coor-
dination structure analysis with alignment-based
features we define the extended index set to be
I ?csa = Icsa
?
Iuni where
Iuni = {(a, b, c) : a, b, c ? {1...n}}
Here each triple (a, b, c) represents that word
wc is recognized as the last word of the right
conjunct and the scope of the left conjunct or
the coordinating conjunction is wa...wb1. Thus
each parse-tree vector y will have additional com-
ponents ya,b,c. Note that this representation is
over-complete, since a parse tree is enough to
determine unique coordination structures for a
sentence: more explicitly, the value of ya,b,c is
1This definition is derived from the structure of a co-
ordination in Enju (Figure 3). The triples show where
the coordinating conjunction and right conjunct are in
coord right schema, and the left conjunct and partial coor-
dination are in coord left schema. Thus they alone enable
not only the coordination structure analysis with alignment-
based features but Enju to uniquely determine the structure
of a coordination.
1 if rule COORDa,c ? CJTa,bCC , CJT ,c or
COORD ,c ? CJT , CCa,bCJT ,c is in the parse
tree; otherwise it is 0.
We apply the same extension to the HPSG in-
dex set, also giving an over-complete representa-
tion. We define za,b,c analogously to ya,b,c.
4.2 Proposed method
We now describe the dual decomposition ap-
proach for coordination disambiguation. First, we
define the set Q as follows:
Q = {(y, z) : y ? Y, z ? Z, ya,b,c = za,b,c
for all (a, b, c) ? Iuni}
Therefore, Q is the set of all (y, z) pairs that
agree on their coordination structures. The coor-
dination structure analysis with alignment-based
features and HPSG parsing problem is then to
solve
max
(y,z)?Q
(y ? ?csa + ?z ? ?hpsg) (3)
where ? > 0 is a parameter dictating the relative
weight of the two models and is chosen to opti-
mize performance on the development test set.
This problem is equivalent to
max
z?Z
(g(z) ? ?csa + ?z ? ?hpsg) (4)
where g : Z ? Y is a function that maps a
HPSG tree z to its set of coordination structures
z = g(y).
We solve this optimization problem by using
dual decomposition. Figure 4 shows the result-
ing algorithm. The algorithm tries to optimize
the combined objective by separately solving the
sub-problems again and again. After each itera-
tion, the algorithm updates the weights u(a, b, c).
These updates modify the objective functions for
the two sub-problems, encouraging them to agree
on the same coordination structures. If y(k) =
z(k) occurs during the iterations, then the algo-
rithm simply returns y(k) as the exact answer. If
not, the algorithm returns the answer of coordina-
tion analysis with alignment features as a heuristic
answer.
It is needed to modify original sub-problems
for calculating (1) and (2) in Table 4. Wemodified
the sub-problems to regard the score of u(a, b, c)
as a bonus/penalty of the coordination. The mod-
ified coordination structure analysis with align-
ment features adds u(k)(i, j,m) and u(k)(j+1, l?
434
u(1)(a, b, c)? 0 for all (a, b, c) ? Iuni
for k = 1 to K do
y(k) ? argmaxy?Y(y ? ?csa ?
?
(a,b,c)?Iuni u
(k)(a, b, c)ya,b,c) ... (1)
z(k) ? argmaxz?Z(z ? ?hpsg +
?
(a,b,c)?Iuni u
(k)(a, b, c)za,b,c) ... (2)
if y(k)(a, b, c) = z(k)(a, b, c) for all (a, b, c) ? Iuni then
return y(k)
end if
for all (a, b, c) ? Iuni do
u(k+1)(a, b, c)? u(k)(a, b, c)? ak(y(k)(a, b, c)? z(k)(a, b, c))
end for
end for
return y(K)
Figure 4: Proposed algorithm
w ? f(x, (i, j, l,m)) to the score of the sub-
tree, when the rule production COORDi,m ?
CJTi,jCCj+1,l?1CJTl,m is applied.
The modified Enju adds u(k)(i, j, l) when co-
ord left schema is applied, where word wc
is recognized as a coordinating conjunction
and left side of its scope is wa...wb, or co-
ord right schema is applied, where word wc
is recognized as a coordinating conjunction and
right side of its scope is wa...wb.
5 Experiments
5.1 Test/Training data
We trained the alignment-based coordination
analysis model on both the Genia corpus (?)
and the Wall Street Journal portion of the Penn
Treebank (?), and evaluated the performance of
our method on (i) the Genia corpus and (ii) the
Wall Street Journal portion of the Penn Treebank.
More precisely, we used HPSG treebank con-
verted from the Penn Treebank and Genia, and
further extracted the training/test data for coor-
dination structure analysis with alignment-based
features using the annotation in the Treebank. Ta-
ble ?? shows the corpus used in the experiments.
The Wall Street Journal portion of the Penn
Treebank has 2317 sentences from WSJ articles,
and there are 1356 COOD tags in the sentences,
while the Genia corpus has 1754 sentences from
MEDLINE abstracts, and there are 1848 COOD
tags in the sentences. COOD tags are further
subcategorized into phrase types such as NP-
COOD or VP-COOD. Table ?? shows the per-
centage of each phrase type in all COOD tags.
It indicates the Wall Street Journal portion of the
COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5
Table 6: The percentage of each conjunct type (%) of
each test set
Penn Treebank has more VP-COOD tags and S-
COOD tags, while the Genia corpus has more
NP-COOD tags and ADJP-COOD tags.
5.2 Implementation of sub-problems
We used Enju (?) for the implementation of
HPSG parsing, which has a wide-coverage prob-
abilistic HPSG grammar and an efficient parsing
algorithm, while we re-implemented Hara et al
(2009)?s algorithm with slight modifications.
5.2.1 Step size
We used the following step size in our algo-
rithm (Figure ??). First, we initialized a0, which
is chosen to optimize performance on the devel-
opment set. Then we defined ak = a0 ? 2??k ,
where ?k is the number of times that L(u(k
?)) >
L(u(k
??1)) for k? ? k.
5.3 Evaluation metric
We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level brack-
eting (?); i.e., we count each of the coordination
scopes as one output of the system, and the system
Figure 4: Proposed algorithm
1,m), as well as adding w ? f(x, (i, j, l,m)) to
the score of the subtree, when the rule produc-
tion COORDi,m ? CJTi,jCCj+1,l?1CJTl,m is
applied.
The modified Enju adds u(k)(a, b, c) when
coord right schema is applied, where word
wa...wb is recognized as a coordinating conjunc-
tion and the last word of the right conjunct is
wc, or coord left schema is applied, where word
wa...wb is recognized as the left conjunct and the
last word of the right conjunct is wc.
5 Experiments
5.1 Test/Training data
We trained the alignment-based coordination
analysis model on both the Geni corpus (Kim
et al 2003) and the Wall Street Jour al p rtion
of the Penn Treebank (Marcus et al 1993), and
evaluated the performance of our method on (i)
the Genia corpus and (ii) the Wall Street Jour-
nal portion of the Penn Tre bank. More precisely,
we used HPSG treebank onverted from the Penn
Treebank and Genia, and further extracted the
training/test data for c ordinati n structure analy-
sis with alignment-based features usi g the anno-
tation in the reebank. Table 5 shows the corpus
used in the experiments.
The Wall Street Journal portion of the Penn
Treebank in the test set ha 2317 sentences from
WSJ articles, and there are 1356 coordinations
in the sentences, while the Genia corpus in the
test set has 1764 sentences from MEDLINE ab-
stracts, and there are 1848 coordinations in the
sentences. Coor inations are further subcatego-
COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5
Table 6: The percentage of each conjunct type (%) of
each test set
rized into phrase types such as a NP coordination
or PP coordination. Table 6 shows the percentage
of each phrase type in all coordianitons. It indi-
cates the Wall Street Journal portion of the Penn
Treebank has more VP coordinations and S co-
ordianitons, while the Genia corpus has more NP
coordianitons and ADJP coordiations.
5.2 Implementation of sub-problems
We used Enju (Miyao and Tsujii, 2004) for
the implementation of HPSG parsing, which has
a wide-coverage probabilistic HPSG grammar
and an efficient parsing algorithm, while we re-
implemented Hara t al., (2009)?s algorithm with
slight modificatio s.
5.2.1 Step size
We used the following step size in our algo-
rithm (Figure 4). First, we initialized a0, which
is chosen to optimize performance on th devel-
opment set. Then we defined ak = a0 ? 2??k ,
where ?k is the number of times that L(u(k
?)) >
L(u(k??1)) for k? ? k.
435
Task (i) Task (ii)
Training WSJ (sec. 2?21) + Genia (No. 1?1600) WSJ (sec. 2?21)
Development Genia (No. 1601?1800) WSJ (sec. 22)
Test Genia (No. 1801?1999) WSJ (sec. 23)
Table 5: The corpus used in the experiments
Proposed Enju CSA
Precision 72.4 66.3 65.3
Recall 67.8 65.5 60.5
F1 70.0 65.9 62.8
Table 7: Results of Task (i) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
5.3 Evaluation metric
We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level bracket-
ing (Shimbo and Hara, 2007); i.e., we count each
of the coordination scopes as one output of the
system, and the system output is regarded as cor-
rect if both of the beginning of the first output
conjunct and the end of the last conjunct match
annotations in the Treebank (Hara et al 2009).
5.4 Experimental results of Task (i)
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. We found the two
sub-problems return the same answer during the
algorithm in over 95% of sentences.
We compare the accuracy of the dual decompo-
sition approach to two baselines: Enju and coor-
dination structure analysis with alignment-based
features. Table 7 shows all three results. The dual
decomposition method gives a statistically signif-
icant gain in precision and recall over the two
methods2.
Table 8 shows the recall of coordinations of
each type. It indicates our re-implementation of
CSA and Hara et al(2009) have a roughly simi-
lar performance, although their experimental set-
tings are different. It also shows the proposed
method took advantage of Enju and CSA in NP
coordination, while it is likely just to take the an-
swer of Enju in VP and sentential coordinations.
This means we might well use dual decomposi-
2p < 0.01 (by chi-square test)
60%$
65%$
70%$
75%$
80%$
85%$
90%$
95%$
100%$
1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$accuracy certificates 
Figure 5: Performance of the approach as a function of
K of Task (i) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is obtained.
tion only on NP coordinations to have a better re-
sult.
Figure 5 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The graphs show
that values of K much less than 50 produce al-
most identical performance to K = 50 (with
K = 50, the accuracy of the method is 73.4%,
with K = 20 it is 72.6%, and with K = 1 it
is 69.3%). This means you can use smaller K in
practical use for speed.
5.5 Experimental results of Task (ii)
We also ran the dual decomposition algorithm
with a limit of K = 50 iterations on Task (ii).
Table 9 and 10 show the results of task (ii). They
show the proposed method outperformed the two
methods statistically in precision and recall3.
Figure 6 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The convergence
speed for WSJ was faster than that for Genia. This
is because a sentence of WSJ often have a simpler
coordination structure, compared with that of Ge-
nia.
3p < 0.01 (by chi-square test)
436
COORD # Proposed Enju CSA # Hara et al(2009)
Overall 1848 67.7 63.3 61.9 3598 61.5
NP 1213 67.5 61.4 64.1 2317 64.2
VP 208 79.8 78.8 66.3 456 54.2
ADJP 193 58.5 59.1 54.4 312 80.4
S 111 51.4 52.3 34.2 188 22.9
PP 110 64.5 59.1 57.3 167 59.9
Others 13 78.3 73.9 65.2 140 49.3
Table 8: The number of coordinations of each type (#), and the recall (%) for the proposed method, Enju,
coordination structure analysis with alignment-based features (CSA) , and Hara et al(2009) of Task (i) on the
development set. Note that Hara et al(2009) uses a different test set and different annotation rules, although its
test data is also taken from the Genia corpus. Thus we cannot compare them directly.
Proposed Enju CSA
Precision 76.3 70.7 66.0
Recall 70.6 69.0 60.1
F1 73.3 69.9 62.9
Table 9: Results of Task (ii) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
COORD # Proposed Enju CSA
Overall 1017 71.6 68.1 60.7
NP 573 76.1 71.0 67.7
VP 187 62.0 62.6 47.6
ADJP 73 82.2 75.3 79.5
S 141 64.5 62.4 42.6
PP 19 52.6 47.4 47.4
Others 24 62.5 70.8 54.2
Table 10: The number of coordinations of each type
(#), and the recall (%) for the proposed method, Enju,
and coordination structure analysis with alignment-
based features (CSA) of Task (ii) on the development
set.
6 Conclusion and Future Work
In this paper, we presented an efficient method for
detecting and disambiguating coordinate struc-
tures. Our basic idea was to consider both gram-
mar and symmetries of conjuncts by using dual
decomposition. Experiments on the Genia corpus
and the Wall Street Journal portion of the Penn
Treebank showed that we could obtain statisti-
cally significant improvement in accuracy when
using dual decomposition.
We would need a further study in the follow-
ing points of view: First, we should evaluate our
60%$
65%$
70%$
75%$
80%$
85%$
90%$
95%$
100%$
1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$accuracy certificates 
Figure 6: Performance of the approach as a function of
K of Task (ii) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is provided.
method with corpus in different domains. Be-
cause characteristics of coordination structures
differs from corpus to corpus, experiments on
other corpus would lead to a different result. Sec-
ond, we would want to add some features to coor-
dination structure analysis with alignment-based
local features such as ontology. Finally, we can
add other methods (e.g. dependency parsing) as
sub-problems to our method by using the exten-
sion of dual decomposition, which can deal with
more than two sub-problems.
Acknowledgments
The second author is partially supported by KAK-
ENHI Grant-in-Aid for Scientific Research C
21500131 and Microsoft CORE project 7.
437
References
Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and
Yuji Matsumoto. 2009. Coordinate structure analy-
sis with global structural constraints and alignment-
based local features. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 967?975, Aug.
Deirdre Hogan. 2007. Coordinate noun phrase dis-
ambiguation in a generative parsing model. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL 2007),
pages 680?687.
Jun-Dong Kim, Tomoko Ohta, and Jun?ich Tsujii.
2003. Genia corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15:3?10.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Yusuke Miyao and Jun?ich Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proceeding of
COLING 2004, pages 1392?1397.
Yusuke Miyao and Jun?ich Tsujii. 2008. Feature
forest models for probabilistic hpsg parsing. MIT
Press, 1(34):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP 2004).
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835?842.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.
Philip Resnik. 1999. Semantic similarity in a takon-
omy. Journal of Artificial Intelligence Research,
11:95?130.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natu-
ral language processing. In Proceeding of the con-
ference on Empirical Methods in Natural Language
Processing.
Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 610?619, Jun.
438
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102?107,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
BRAT: a Web-based Tool for NLP-Assisted Text Annotation
Pontus Stenetorp1? Sampo Pyysalo2,3? Goran Topic?1
Tomoko Ohta1,2,3 Sophia Ananiadou2,3 and Jun?ichi Tsujii4
1Department of Computer Science, The University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, UK
3National Centre for Text Mining, University of Manchester, Manchester, UK
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,goran,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We introduce the brat rapid annotation tool
(BRAT), an intuitive web-based tool for text
annotation supported by Natural Language
Processing (NLP) technology. BRAT has
been developed for rich structured annota-
tion for a variety of NLP tasks and aims
to support manual curation efforts and in-
crease annotator productivity using NLP
techniques. We discuss several case stud-
ies of real-world annotation projects using
pre-release versions of BRAT and present
an evaluation of annotation assisted by se-
mantic class disambiguation on a multi-
category entity mention annotation task,
showing a 15% decrease in total annota-
tion time. BRAT is available under an open-
source license from: http://brat.nlplab.org
1 Introduction
Manually-curated gold standard annotations are
a prerequisite for the evaluation and training of
state-of-the-art tools for most Natural Language
Processing (NLP) tasks. However, annotation is
also one of the most time-consuming and finan-
cially costly components of many NLP research
efforts, and can place heavy demands on human
annotators for maintaining annotation quality and
consistency. Yet, modern annotation tools are
generally technically oriented and many offer lit-
tle support to users beyond the minimum required
functionality. We believe that intuitive and user-
friendly interfaces as well as the judicious appli-
cation of NLP technology to support, not sup-
plant, human judgements can help maintain the
quality of annotations, make annotation more ac-
cessible to non-technical users such as subject
?These authors contributed equally to this work
Figure 1: Visualisation examples. Top: named en-
tity recognition, middle: dependency syntax, bot-
tom: verb frames.
domain experts, and improve annotation produc-
tivity, thus reducing both the human and finan-
cial cost of annotation. The tool presented in
this work, BRAT, represents our attempt to realise
these possibilities.
2 Features
2.1 High-quality Annotation Visualisation
BRAT is based on our previously released open-
source STAV text annotation visualiser (Stene-
torp et al 2011b), which was designed to help
users gain an understanding of complex annota-
tions involving a large number of different se-
mantic types, dense, partially overlapping text an-
notations, and non-projective sets of connections
between annotations. Both tools share a vector
graphics-based visualisation component, which
provide scalable detail and rendering. BRAT in-
tegrates PDF and EPS image format export func-
tionality to support use in e.g. figures in publica-
tions (Figure 1).
102
Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the
annotations for ?moving? and ?Citibank?.
2.2 Intuitive Annotation Interface
We extended the capabilities of STAV by imple-
menting support for annotation editing. This was
done by adding functionality for recognising stan-
dard user interface gestures familiar from text ed-
itors, presentation software, and many other tools.
In BRAT, a span of text is marked for annotation
simply by selecting it with the mouse by ?drag-
ging? or by double-clicking on a word. Similarly,
annotations are linked by clicking with the mouse
on one annotation and dragging a connection to
the other (Figure 2).
BRAT is browser-based and built entirely using
standard web technologies. It thus offers a fa-
miliar environment to annotators, and it is pos-
sible to start using BRAT simply by pointing a
standards-compliant modern browser to an instal-
lation. There is thus no need to install or dis-
tribute any additional annotation software or to
use browser plug-ins. The use of web standards
also makes it possible for BRAT to uniquely iden-
tify any annotation using Uniform Resource Iden-
tifiers (URIs), which enables linking to individual
annotations for discussions in e-mail, documents
and on web pages, facilitating easy communica-
tion regarding annotations.
2.3 Versatile Annotation Support
BRAT is fully configurable and can be set up to
support most text annotation tasks. The most ba-
sic annotation primitive identifies a text span and
assigns it a type (or tag or label), marking for e.g.
POS-tagged tokens, chunks or entity mentions
(Figure 1 top). These base annotations can be
connected by binary relations ? either directed or
undirected ? which can be configured for e.g. sim-
ple relation extraction, or verb frame annotation
(Figure 1 middle and bottom). n-ary associations
of annotations are also supported, allowing the an-
notation of event structures such as those targeted
in the MUC (Sundheim, 1996), ACE (Doddington
et al 2004), and BioNLP (Kim et al 2011) In-
formation Extraction (IE) tasks (Figure 2). Addi-
tional aspects of annotations can be marked using
attributes, binary or multi-valued flags that can
be added to other annotations. Finally, annotators
can attach free-form text notes to any annotation.
In addition to information extraction tasks,
these annotation primitives allow BRAT to be
configured for use in various other tasks, such
as chunking (Abney, 1991), Semantic Role La-
beling (Gildea and Jurafsky, 2002; Carreras
and Ma`rquez, 2005), and dependency annotation
(Nivre, 2003) (See Figure 1 for examples). Fur-
ther, both the BRAT client and server implement
full support for the Unicode standard, which al-
low the tool to support the annotation of text us-
ing e.g. Chinese or Devana?gar?? characters. BRAT
is distributed with examples from over 20 cor-
pora for a variety of tasks, involving texts in seven
different languages and including examples from
corpora such as those introduced for the CoNLL
shared tasks on language-independent named en-
tity recognition (Tjong Kim Sang and De Meul-
der, 2003) and multilingual dependency parsing
(Buchholz and Marsi, 2006).
BRAT also implements a fully configurable sys-
tem for checking detailed constraints on anno-
tation semantics, for example specifying that a
TRANSFER event must take exactly one of each
of GIVER, RECIPIENT and BENEFICIARY argu-
ments, each of which must have one of the types
PERSON, ORGANIZATION or GEO-POLITICAL
ENTITY, as well as a MONEY argument of type
103
Figure 3: Incomplete TRANSFER event indicated
to the annotator
MONEY, and may optionally take a PLACE argu-
ment of type LOCATION (LDC, 2005). Constraint
checking is fully integrated into the annotation in-
terface and feedback is immediate, with clear vi-
sual effects marking incomplete or erroneous an-
notations (Figure 3).
2.4 NLP Technology Integration
BRAT supports two standard approaches for inte-
grating the results of fully automatic annotation
tools into an annotation workflow: bulk anno-
tation imports can be performed by format con-
version tools distributed with BRAT for many
standard formats (such as in-line and column-
formatted BIO), and tools that provide standard
web service interfaces can be configured to be in-
voked from the user interface.
However, human judgements cannot be re-
placed or based on a completely automatic analy-
sis without some risk of introducing bias and re-
ducing annotation quality. To address this issue,
we have been studying ways to augment the an-
notation process with input from statistical and
machine learning methods to support the annota-
tion process while still involving human annotator
judgement for each annotation.
As a specific realisation based on this approach,
we have integrated a recently introduced ma-
chine learning-based semantic class disambigua-
tion system capable of offering multiple outputs
with probability estimates that was shown to be
able to reduce ambiguity on average by over 75%
while retaining the correct class in on average
99% of cases over six corpora (Stenetorp et al
2011a). Section 4 presents an evaluation of the
contribution of this component to annotator pro-
ductivity.
2.5 Corpus Search Functionality
BRAT implements a comprehensive set of search
functions, allowing users to search document col-
Figure 4: The BRAT search dialog
lections for text span annotations, relations, event
structures, or simply text, with a rich set of search
options definable using a simple point-and-click
interface (Figure 4). Additionally, search results
can optionally be displayed using keyword-in-
context concordancing and sorted for browsing
using any aspect of the matched annotation (e.g.
type, text, or context).
3 Implementation
BRAT is implemented using a client-server ar-
chitecture with communication over HTTP using
JavaScript Object Notation (JSON). The server is
a RESTful web service (Fielding, 2000) and the
tool can easily be extended or adapted to switch
out the server or client. The client user interface is
implemented using XHTML and Scalable Vector
Graphics (SVG), with interactivity implemented
using JavaScript with the jQuery library. The
client communicates with the server using Asyn-
chronous JavaScript and XML (AJAX), which
permits asynchronous messaging.
BRAT uses a stateless server back-end imple-
mented in Python and supports both the Common
Gateway Interface (CGI) and FastCGI protocols,
the latter allowing response times far below the
100 ms boundary for a ?smooth? user experience
without noticeable delay (Card et al 1983). For
server side annotation storage BRAT uses an easy-
to-process file-based stand-off format that can be
converted from or into other formats; there is no
need to perform database import or export to in-
terface with the data storage. The BRAT server in-
104
Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational
Modifications event extraction task.
stallation requires only a CGI-capable web server
and the set-up supports any number of annotators
who access the server using their browsers, on any
operating system, without separate installation.
Client-server communication is managed so
that all user edit operations are immediately sent
to the server, which consolidates them with the
stored data. There is no separate ?save? operation
and thus a minimal risk of data loss, and as the
authoritative version of all annotations is always
maintained by the server, there is no chance of
conflicting annotations being made which would
need to be merged to produce an authoritative ver-
sion. The BRAT client-server architecture also
makes real-time collaboration possible: multiple
annotators can work on a single document simul-
taneously, seeing each others edits as they appear
in a document.
4 Case Studies
4.1 Annotation Projects
BRAT has been used throughout its development
during 2011 in the annotation of six different cor-
pora by four research groups in efforts that have
in total involved the creation of well-over 50,000
annotations in thousands of documents compris-
ing hundreds of thousands of words.
These projects include structured event an-
notation for the domain of cancer biology,
Japanese verb frame annotation, and gene-
mutation-phenotype relation annotation. One
prominent effort making use of BRAT is the
BioNLP Shared Task 2011,1 in which the tool was
used in the annotation of the EPI and ID main
task corpora (Pyysalo et al 2012). These two
information extraction tasks involved the annota-
tion of entities, relations and events in the epige-
netics and infectious diseases subdomains of biol-
ogy. Figure 5 shows an illustration of shared task
annotations.
Many other annotation efforts using BRAT are
still ongoing. We refer the reader to the BRAT
1http://2011.bionlp-st.org
Mode Total Type Selection
Normal 45:28 13:49
Rapid 39:24 (-6:04) 09:35 (-4:14)
Table 1: Total annotation time, portion spent se-
lecting annotation type, and absolute improve-
ment for rapid mode.
website2 for further details on current and past an-
notation projects using BRAT.
4.2 Automatic Annotation Support
To estimate the contribution of the semantic class
disambiguation component to annotation produc-
tivity, we performed a small-scale experiment in-
volving an entity and process mention tagging
task. The annotation targets were of 54 dis-
tinct mention types (19 physical entity and 35
event/process types) marked using the simple
typed-span representation. To reduce confound-
ing effects from annotator productivity differ-
ences and learning during the task, annotation was
performed by a single experienced annotator with
a Ph.D. in biology in a closely related area who
was previously familiar with the annotation task.
The experiment was performed on publication
abstracts from the biomolecular science subdo-
main of glucose metabolism in cancer. The texts
were drawn from a pool of 1,750 initial candi-
dates using stratified sampling to select pairs of
10-document sets with similar overall statistical
properties.3 Four pairs of 10 documents (80 in to-
tal) were annotated in the experiment, with 10 in
each pair annotated with automatic support and 10
without, in alternating sequence to prevent learn-
ing effects from favouring either approach.
The results of this experiment are summarized
in Table 1 and Figure 6. In total 1,546 annotations
were created in normal mode and 1,541 annota-
2http://brat.nlplab.org
3Document word count and expected annotation count,
were estimated from the output of NERsuite, a freely avail-
able CRF-based NER tagger: http://nersuite.nlplab.org
105
0500
1000
1500
2000
2500
3000
Normal Mode Rapid Mode
Tim
e(
se
co
nd
s)
Figure 6: Allocation of annotation time. GREEN
signifies time spent on selecting annotation type
and BLUE the remaining annotation time.
tions in rapid mode; the sets are thus highly com-
parable. We observe a 15.4% reduction in total
annotation time, and, as expected, this is almost
exclusively due to a reduction in the time the an-
notator spent selecting the type to assign to each
span, which is reduced by 30.7%; annotation time
is otherwise stable across the annotation modes
(Figure 6). The reduction in the time spent in se-
lecting the span is explained by the limiting of the
number of candidate types exposed to the annota-
tor, which were decreased from the original 54 to
an average of 2.88 by the semantic class disam-
biguation component (Stenetorp et al 2011a).
Although further research is needed to establish
the benefits of this approach in various annotation
tasks, we view the results of this initial experi-
ment as promising regarding the potential of our
approach to using machine learning to support an-
notation efforts.
5 Related Work and Conclusions
We have introduced BRAT, an intuitive and user-
friendly web-based annotation tool that aims to
enhance annotator productivity by closely inte-
grating NLP technology into the annotation pro-
cess. BRAT has been and is being used for several
ongoing annotation efforts at a number of aca-
demic institutions and has so far been used for
the creation of well-over 50,000 annotations. We
presented an experiment demonstrating that inte-
grated machine learning technology can reduce
the time for type selection by over 30% and over-
all annotation time by 15% for a multi-type entity
mention annotation task.
The design and implementation of BRAT was
informed by experience from several annotation
tasks and research efforts spanning more than
a decade. A variety of previously introduced
annotation tools and approaches also served to
guide our design decisions, including the fast an-
notation mode of Knowtator (Ogren, 2006), the
search capabilities of the XConc tool (Kim et al
2008), and the design of web-based systems such
as MyMiner (Salgado et al 2010), and GATE
Teamware (Cunningham et al 2011). Using ma-
chine learning to accelerate annotation by sup-
porting human judgements is well documented in
the literature for tasks such as entity annotation
(Tsuruoka et al 2008) and translation (Mart??nez-
Go?mez et al 2011), efforts which served as in-
spiration for our own approach.
BRAT, along with conversion tools and exten-
sive documentation, is freely available under the
open-source MIT license from its homepage at
http://brat.nlplab.org
Acknowledgements
The authors would like to thank early adopters of
BRAT who have provided us with extensive feed-
back and feature suggestions. This work was sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan), the UK Biotechnology
and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event
Extraction from the Literature for Drug Discov-
ery (reference number: BB/G013160/1), and the
Royal Swedish Academy of Sciences.
106
References
Steven Abney. 1991. Parsing by chunks. Principle-
based parsing, 44:257?278.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Stuart K. Card, Thomas P. Moran, and Allen Newell.
1983. The psychology of human-computer interac-
tion. Lawrence Erlbaum Associates, Hillsdale, New
Jersey.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic Role
Labeling. In Proceedings of the 9th Conference on
Natural Language Learning, pages 152?164. Asso-
ciation for Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 837?
840.
Roy Fielding. 2000. REpresentational State Trans-
fer (REST). Architectural Styles and the Design
of Network-based Software Architectures. Univer-
sity of California, Irvine, page 120.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(1):10.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 1?6, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical
report, Linguistic Data Consortium.
Pascual Mart??nez-Go?mez, Germa?n Sanchis-Trilles,
and Francisco Casacuberta. 2011. Online learn-
ing via dynamic reranking for computer assisted
translation. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 93?105. Springer Berlin / Heidelberg.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, pages 149?160.
Philip V. Ogren. 2006. Knowtator: A prote?ge? plug-in
for annotated corpus construction. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Companion Volume:
Demonstrations, pages 273?275, New York City,
USA, June. Association for Computational Linguis-
tics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Junichi Tsujii, and Sophia Ananiadou. 2012.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(suppl.
8):S2.
David Salgado, Martin Krallinger, Marc Depaule,
Elodie Drula, and Ashish V Tendulkar. 2010.
Myminer system description. In Proceedings of the
Third BioCreative Challenge Evaluation Workshop
2010, pages 157?158.
Pontus Stenetorp, Sampo Pyysalo, Sophia Ananiadou,
and Jun?ichi Tsujii. 2011a. Almost total recall: Se-
mantic category disambiguation using large lexical
resources and approximate string matching. In Pro-
ceedings of the Fourth International Symposium on
Languages in Biology and Medicine.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011b. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Beth M. Sundheim. 1996. Overview of results of
the MUC-6 evaluation. In Proceedings of the Sixth
Message Understanding Conference, pages 423?
442. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2008. Accelerating the annotation of
sparse named entities by dynamic sentence selec-
tion. BMC Bioinformatics, 9(Suppl 11):S8.
107
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111?116,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Using a Random Forest Classifier to Compile Bilingual Dictionaries of
Technical Terms from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We describe a machine learning approach,
a Random Forest (RF) classifier, that is
used to automatically compile bilingual
dictionaries of technical terms from com-
parable corpora. We evaluate the RF clas-
sifier against a popular term alignment
method, namely context vectors, and we
report an improvement of the translation
accuracy. As an application, we use the
automatically extracted dictionary in com-
bination with a trained Statistical Machine
Translation (SMT) system to more accu-
rately translate unknown terms. The dic-
tionary extraction method described in this
paper is freely available
1
.
1 Background
Bilingual dictionaries of technical terms are im-
portant resources for many Natural Language
Processing (NLP) tasks including Statistical Ma-
chine Translation (SMT) (Och and Ney, 2003) and
Cross-Language Information Retrieval (Balles-
teros and Croft, 1997). However, manually cre-
ating and updating such resources is an expensive
process. In addition to this, new terms are con-
stantly emerging. Especially in the biomedical
domain, which is the focus of this work, there is
a vast number of neologisms, i.e., newly coined
terms, (Pustejovsky et al., 2001).
Early work on bilingual lexicon extraction
focused on clean, parallel corpora providing
satisfactory results (Melamed, 1997; Kay and
R?oscheisen, 1993). However, parallel corpora are
expensive to construct and for some domains and
language pairs are scarce resources. For these rea-
sons, the focus has shifted to comparable corpora
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/RF-TermAlign.tar.gz
that are more readily available, more up-to-date,
larger and cheaper to construct than parallel data.
Comparable corpora are collections of monolin-
gual documents in a source and target language
that share the same topic, domain and/or docu-
ments are from the same period, genre and so
forth.
Existing methods for bilingual lexicon extrac-
tion from comparable corpora are mainly based
on the same principle. They hypothesise that a
word and its translation tend to appear in simi-
lar lexical context (Fung and Yee, 1998; Rapp,
1999; Morin et al., 2007; Chiao and Zweigen-
baum, 2002). Context vector methods are reported
to achieve robust performance on terms that occur
frequently in the corpus. Chiao and Zweigenbaum
(2002) achieved a performance of 94% accuracy
on the top 20 candidates when translating high fre-
quency, medical terms (frequency of 100 or more).
In contrast, Morin and Daille (2010) reported an
accuracy of 21% for multi-word terms occurring
20 times or less, noting that translating rare terms
is a challenging problem for context vectors.
Kontonatsios et al. (2013) introduced an RF
classifier that is able to automatically learn as-
sociation rules of textual units between a source
and target language. However, they applied their
method only on artificially constructed datasets
containing an equal number of positive and neg-
ative instances. In the case of comparable cor-
pora, the datasets are highly unbalanced (given n,
m source and target terms respectively, we need to
classify n?m instances). In this work, we incor-
porate the classification margin into the RF model,
to allow the method to cope with the skewed dis-
tribution of positive and negative instances that oc-
curs in comparable corpora.
Our proposed method ranks candidate transla-
tions using the classification margin and suggests
as the best translation the candidate with the max-
imum margin. We evaluate our method on an
111
English-Spanish comparable corpus of Wikipedia
articles that are related to the medical sub-domain
of ?breast cancer?. Furthermore, we show that dic-
tionaries extracted from comparable corpora can
be used to dynamically augment an SMT sys-
tem in order to better translate Out-of-Vocabulary
(OOV) terms.
2 Methodology
A pair of terms in a source and target language is
represented as a feature vector where each dimen-
sion corresponds to a unique character n-gram.
The value of each dimension is 0 or 1 and desig-
nates the occurrence of the corresponding n-gram
in the input terms. The feature vectors that we
use contain 2q dimensions where the first q dimen-
sions correspond to the n-gram features extracted
from the source terms and the last q dimensions to
those from the target terms. In the reported experi-
ments, we use the 600 (300 source and 300 target)
most frequently occurring n-grams.
The underlying mechanism that allows the RF
method to learn character gram mappings between
terms of a source and target language is the de-
cision trees. A node in the decision tree is a
unique character n-gram. The nodes are linked
through the branches of the trees and therefore the
two sub-spaces of q source and q target charac-
ter grams are combined. Each decision tree in the
forest is constructed as follows: every node is split
by considering |?| random n-gram features of the
initial feature set ?, and a decision tree is fully
grown. This process is repeated |? | times and con-
structs |? | decision trees. We tuned the RF clas-
sifier using 140 random trees where we observed
a plateau in the classification performance. Fur-
thermore, we set the number of random features
using |?| = log
2
|?|+ 1 as suggested by Breiman
(2001).
The classification margin that we use to rank
the candidate translations is calculated by simply
subtracting the average number of trees predicting
that the input terms are not translations from the
average number of decision trees predicting that
the terms are mutual translations. A larger classi-
fication margin means that more decision trees in
the forest classify an instance as a translation pair.
For training an RF model, we use a bilingual
dictionary of technical terms. When the dictionary
lists more than one translation for an English term,
we randomly select only one. Negative instances
are created by randomly matching non-translation
pairs of terms. We used an equal number of posi-
tive and negative instances for training the model.
Starting from 20, 000 translation pairs we gener-
ated a training dataset of 40, 000 positive and neg-
ative instances.
2.1 Baseline method
The context projection method was first pro-
posed by (Fung and Yee, 1998; Rapp, 1999) and
since then different variations have been suggested
(Chiao and Zweigenbaum, 2002; Morin et al.,
2007; Andrade et al., 2010; Morin and Prochas-
son, 2011). Our implementation more closely
follows the context vector method introduced by
(Morin and Prochasson, 2011).
As a preprocessing step, stop words are re-
moved using an online list
2
and lemmatisation
is performed using TreeTagger (Schmid, 1994) on
both the English and Spanish part of the compa-
rable corpus. Afterwards, the method proceeds
in three steps. Firstly, for each source and target
term of the comparable corpus, i.e., i, we collect
all lexical units that: (a) occur within a window
of 3 words around i (a seven-word window) and
(b) are listed in the seed bilingual dictionary. The
lexical units that satisfy the above two conditions
are the dimensions of the context vectors. Each
dimension has a value that indicates the correla-
tion between the context lexical unit and the term
i. In our approach, we use the log-likelihood ra-
tio. In the second step, the seed dictionary is used
to translate the lexical units of the Spanish context
vectors. In this way the Spanish and English vec-
tors become comparable. When several transla-
tions are listed in the seed dictionary, we consider
all of them. In the third step, we compute the con-
text similarity, i.e., distance metric, between the
vector of an English term to be translated with ev-
ery projected, Spanish context vector. For this we
use the cosine similarity.
3 Experiments
In this section, we evaluate the two dictionary ex-
traction methods, namely context vectors and RF,
on a comparable corpus of Wikipedia articles.
For the evaluation metric, we use the top-k
translation accuracy
3
and the mean reciprocal
2
http://members.unine.ch/jacques.savoy/clef/index.html
3
the percentage of English terms whose top k candidates
contain a correct translation
112
rank (MRR)
4
as in previous approaches (Chiao
and Zweigenbaum, 2002; Chiao and Zweigen-
baum, 2002; Morin and Prochasson, 2011; Morin
et al., 2007; Tamura et al., 2012). As a refer-
ence list, we use the UMLS metathesaurus
5
. In
addition to this, considering that in several cases
the dictionary extraction methods retrieved syn-
onymous translations that do not appear in the ref-
erence list, we manually inspected the answers.
Finally, unlike previous approaches (Chiao and
Zweigenbaum, 2002), we do not restrict the test
list only to those English terms whose Spanish
translations are known to occur in the target cor-
pus. In such cases, the performance of dictionary
extraction methods have been shown to achieve a
lower performance (Tamura et al., 2012).
3.1 Data
We constructed a comparable corpus of Wikipedia
articles. For this, we used Wikipedia?s search en-
gine
6
and submitted the queries ?breast cancer?
and ?c?ancer de mama? for English and Spanish
respectively. From the returned list of Wikipedia
pages, we used the 1, 000 top articles for both lan-
guages.
The test list contains 1, 200 English single-word
terms that were extracted by considering all nouns
that occur more than 10 but not more than 200
times and are listed in UMLS. For the Spanish part
of the corpus, we considered all nouns as candi-
date translations (32, 347 in total).
3.2 Results
Table 1 shows the top-k translation accuracy and
the MRR of RF and context vectors.
Acc
1
Acc
10
Acc
20
MRR
RF 0.41 0.57 0.59 0.47
Cont.
Vectors 0.1 0.21 0.26 0.11
Table 1: top-k translation accuracy and MRR of
RF and context vectors on 1, 200 English terms
We observe that the proposed RF method
achieves a considerably better top-k translation ac-
4
MRR =
1
|Q|
?
Q
i=1
1
rank
i
where |Q| is the number of
English terms for which we are extracting translations and
rank
i
is the position of the first correct translation from re-
turned list of candidates
5
nlm.nih.gov/research/umls
6
http://en.wikipedia.org/wiki/Help:Searching
curacy and MRR than the baseline method. More-
over, we segmented the 1, 200 test terms into 7
frequency ranges
7
, from high-frequency to rare
terms. Figure 1 shows the translation accuracy at
top 20 candidates for the two methods. We note
Figure 1: Translation accuracy of top 20 candi-
dates on different frequency ranges
that for high frequency terms, i.e. [100,200] range,
the performance achieved by the two methods is
similar (53% and 52% for the RF and context vec-
tors respectively). However, for lower frequency
terms, the translation accuracy of the context vec-
tors continuously declines. This confirms that con-
text vectors do not behave robustly for rare terms
(Morin and Daille, 2010). In contrast, the RF
slightly fluctuates over different frequency ranges
and presents approximately the same translation
accuracy for both frequent and rare terms.
4 Application
As an application of our method, we use the pre-
viously extracted dictionaries to on-line augment
the phrase table of an SMT system and observe
the translation performance on test sentences that
contain OOV terms. For the translation probabil-
ities in the phrase table, we use the distance met-
ric given by the dictionary extraction methods i.e.,
classification margin and cosine similarity of RF
and context vectors respectively, normalised by
the uniform probability (if a source term has m
candidate translations, we normalise the distance
metric by dividing by m as in (Wu et al., 2008) .
4.1 Data and tools
We construct a parallel, sentence-aligned corpus
from the biomedical domain, following the pro-
cess described in (Wu et al., 2011; Yepes et al.,
2013). The parallel corpus comprises of article ti-
tles indexed by PubMed in both English and Span-
ish. We collect 120K parallel sentences for train-
7
each frequency range contains 100 randomly sampled
terms
113
ing the SMT and 1K sentences for evaluation. The
test sentences contain 1, 200 terms that do not ap-
pear in the training parallel corpus. These terms
occur in the Wikipedia comparable corpus. Hence,
the previously extracted dictionaries list a possible
translation. Using the PubMed parallel corpus, we
train Moses (Koehn et al., 2007), a phrase-based
SMT system.
4.2 Results
We evaluated the translation performance of the
SMT that uses the dictionary extracted by the RF
against the following baselines: (i) Moses using
only the training parallel data (Moses), (ii) Moses
using the dictionary extracted by context vectors
(Moses+context vector). The evaluation metric is
BLEU (Papineni et al., 2002).
Table 2 shows the BLEU score achieved by the
SMT systems when we append the top-k transla-
tions to the phrase table.
BLEU
on top-k translations
1 10 20
Moses 24.22 24.22 24.22
Moses+
RF 25.32 24.626 24.42
Moses+
Context Vectors 23.88 23.69 23.74
Table 2: Translation performance when adding
top-k translations to the phrase table
We observe that the best performance is
achieved by the RF when we add the top 1 trans-
lation with a total gain of 1.1 BLEU points over
the baseline system. In contrast, context vec-
tors decreased the translation performance of the
SMT system. This indicates that the dictionary ex-
tracted by the context vectors is too noisy and as
a result the translation performance dropped. Fur-
thermore, it is noted that the augmented SMT sys-
tems achieve the highest performance for the top 1
translation while for k greater than 1, the transla-
tion performance decreases. This behaviour is ex-
pected since the target language model was trained
only on the training Spanish sentences of the par-
allel corpus. Hence, the target language model
does not have a prior knowledge of the OOV trans-
lations and as a result it cannot choose the correct
translation among k candidates.
To further investigate the effect of the language
model on the translation performance of the aug-
mented SMT systems, we conducted an oracle ex-
periment. In this ideal setting, we assume a strong
language model, that is trained on both training
and test Spanish sentences of the parallel corpus,
in order to assign a higher probability to a correct
translation if it exists in the deployed dictionary.
As we observe in Table 3, a strong language model
can more accurately select the correct translation
among top-k candidates. The dictionary extracted
by the RF improved the translation performance
by 2.5 BLEU points for the top-10 candidates and
context vectors by 0.45 for the top-20 candidates.
BLEU
on top-k translations
1 10 20
Moses 28.85 28.85 28.85
Moses+
RF 30.98 31.35 31.2
Moses+
Context Vectors 28.18 29.17 29.3
Table 3: Translation performance when adding
top-k translations to the phrase table. SMT sys-
tems use a language model trained on training and
test Spanish sentences of the parallel corpus.
5 Discussion
In this paper, we presented an RF classifier that
is used to extract bilingual dictionaries of techni-
cal terms from comparable corpora. We evaluated
our method on a comparable corpus of Wikipedia
articles. The experimental results showed that our
proposed method performs robustly when translat-
ing both frequent and rare terms.
As an application, we used the automatically
extracted dictionary to augment the phrase table of
an SMT system. The results demonstrated an im-
provement of the overall translation performance.
As future work, we plan to integrate the RF clas-
sifier with context vectors. Intuitively, the two
methods are complementary considering that the
RF exploits the internal structure of terms while
context vectors use the surrounding lexical con-
text. Therefore, it will be interesting to investigate
how we can incorporate the two feature spaces in
a machine learner.
114
6 Acknowledgements
This work was funded by the European Commu-
nity?s Seventh Framework Program (FP7/2007-
2013) [grant number 318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 19?
27, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
Martin Kay and Martin R?oscheisen. 1993. Text-
translation alignment. computational Linguistics,
19(1):121?142.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Georgios Kontonatsios, Ioannis Korkontzelos, Jun?ichi
Tsujii, and Sophia Ananiadou. 2013. Using ran-
dom forest to recognise translation equivalents of
biomedical terms across languages. In Proceed-
ings of the Sixth Workshop on Building and Using
Comparable Corpora, pages 95?104. Association
for Computational Linguistics, August.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 305?312. Association for
Computational Linguistics.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 993?1000. Association for Computational
Linguistics.
115
Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.
2011. Statistical machine translation for biomedical
text: are we there yet? In AMIA Annual Sympo-
sium Proceedings, volume 2011, page 1290. Ameri-
can Medical Informatics Association.
Antonio Jimeno Yepes,
?
Elise Prieur-Gaston, and
Aur?elie N?ev?eol. 2013. Combining medline and
publisher data to create parallel corpora for the auto-
matic translation of biomedical text. BMC bioinfor-
matics, 14(1):146.
116
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645?648,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Simple Approach for HPSG Supertagging Using Dependency Information
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
In a supertagging task, sequence labeling
models are commonly used. But their lim-
ited ability to model long-distance informa-
tion presents a bottleneck to make further im-
provements. In this paper, we modeled this
long-distance information in dependency for-
malism and integrated it into the process of
HPSG supertagging. The experiments showed
that the dependency information is very in-
formative for supertag disambiguation. We
also evaluated the improved supertagger in the
HPSG parser.
1 Introduction
Supertagging is a widely used speed-up technique
for lexicalized grammar parsing. It was first
proposed for lexicalized tree adjoining grammar
(LTAG) (Bangalore and Joshi, 1999), then extended
to combinatory categorial grammar (CCG) (Clark,
2002) and head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). For deep parsing,
supertagging is an important preprocessor: an ac-
curate supertagger greatly reduces search space of
a parser. Not limited to parsing, supertags can be
used for NP chunking (Shen and Joshi, 2003), se-
mantic role labeling (Chen and Rambow, 2003) and
machine translation (Birch et al, 2007; Hassan et
al., 2007) to explore rich syntactic information con-
tained in them.
Generally speaking, supertags are lexical tem-
plates extracted from a grammar. These templates
encode possible syntactic behavior of a word. Al-
though the number of supertags is far larger than the
45 POS tags defined in Penn Treebank, sequence la-
beling techniques are still effective for supertagging.
Previous research (Clark, 2002) showed that a POS
sequence is very informative for supertagging, and
some extent of local syntactic information can be
captured by the context of surrounding words and
POS tags. However, since the context window
length is limited for the computational cost reasons,
there are still long-range dependencies which are not
easily captured in sequential models (Zhang et al,
2009). In practice, the multi-tagging technique pro-
posed by Clark (2002) assigned more than one su-
pertag to each word and let the ambiguous supertags
be selected by the parser. As for other NLP applica-
tions which use supertags, resolving more supertag
ambiguities in supertagging stage is preferred. With
this consideration, we focus on supertagging and
aim to make it as accurate as possible.
In this paper, we incorporated long-distance in-
formation into supertagging. First, we used depen-
dency parser formalism to model long-distance re-
lationships between the input words, which is hard
to model in sequence labeling models. Then, we
combined the dependency information with local
context in a simple point-wise model. The experi-
ments showed that dependency information is very
informative for supertagging and we got a compet-
itive 93.70% on supertagging accuracy (fed golden
POS). In addition, we also evaluated the improved
supertagger in the HPSG parser.
2 HPSG Supertagging and Dependency
2.1 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a lexicalist gram-
mar framework. In HPSG, a large number of
lexical entries is used to describe word-specific
syntactic characteristics, while only a small num-
ber of schemas is used to explain general con-
struction rules. These lexical entries are called
?HPSG supertags?. For example, one possi-
ble supertag for the word ?like? is written like
?[NP.nom<V.bse>NP.acc] lxm?, which indicates
645
the head syntactic category of ?like? is verb in base
form. It has a NP subject and a NP complement.
With such fine-grained grammatical type distinc-
tions, the number of supertags is much larger than
the number of tags used in other sequence labeling
tasks. The HPSG grammar used in our experiment
includes 2,308 supertags. This increases computa-
tional cost of sequence labeling models.
2.2 Why Use Dependency in Supertagging
By analyzing the internal structure of the supertags,
we found that subject and complements are two im-
portant syntactic properties for each supertag. If
we could predict subject and complements of the
word well, supertagging would be an easier job to
do. However, current widely used sequence labeling
models have the limited ability to catch these long-
distance syntactic relations. In supertagging stage,
tree structures are still not constructed. Dependency
formalism is an alternative way to describe these two
syntactic properties. Based on this observation, we
think dependency information could assist supertag
prediction.
Figure 1: Model structure of incorporating dependency
information into the supertagging stage. Dotted arrows
describe the augmented long distance dependency infor-
mation provided for supertag prediction.
3 Our Method
3.1 Modeling Dependency for Supertags
First of all, we need to characterize the dependency
between words for supertagging. Since exact de-
pendency locations are not encoded in supertags, to
make use of state-of-the-art dependency parser, we
recover HPSG supertag dependencies with the aid
of HPSG treebanks. The dependencies are extracted
from each branch in the HPSG trees by regarding
the non-head daughter as the modifier of the head-
daughter. HPSG schemas are expressed in depen-
dency arcs.
To model the dependency, we follow mainstream
dependency parsing formalism. Two representa-
tive methods for dependency parsing are transition-
based model like MaltParser (Nivre, 2003) and
graph-based model like MSTParser1 (McDonald et
al., 2005). Previous research (Nivre and McDon-
ald, 2008) showed that MSTParser is more accurate
than MaltParser for long dependencies. Since our
motivation is to capture long-distance dependency
as a complement for local supertagging models, we
use the projective MSTParser formalism to model
dependencies.
{(pi ? pj)&sj |(j, i) ? E}
MOD-IN {(pi ? wj)&sj|(j, i) ? E}
{(wi ? pj)&sj|(j, i) ? E}
{(wi ? wj)&sj |(j, i) ? E}
{(pi ? pj)&si|(i, j) ? E}
MOD-OUT {(pi ? wj)&si|(i, j) ? E}
{(wi ? pj)&si|(i, j) ? E}
{(wi ? wj)&si|(i, j) ? E}
Table 1: Non-local feature templates used for super-
tagging. Here, p, w and s represent POS, word
and schema respectively. Direction (Left/Right) from
MODIN/MODOUTword to the current word is also con-
sidered in the feature templates.
3.2 Integrating Dependency into Supertagging
There are several ways to combine long-distance
dependency into supertagging. Integrating depen-
dency information into training process would be
more intuitive. Here, we use feature-based integra-
tion. The base model is a point-wise averaged per-
ceptron (PW-AP) which has been shown very ef-
fective (Zhang et al, 2009). The improved model
structure is described in Figure 1. The long-distance
information is formalized as first-order dependency.
For the word being predicted, we extract its modi-
fiers (MODIN) and its head (MODOUT) (Table 1)
based on first-order dependency arcs. Then MODIN
and MODOUT relations are combined as features
with local context for supertag prediction. To com-
pare with previous work, the basic local context fea-
tures are the same as in Matsuzaki et al (2007).
1http://sourceforge.net/projects/mstparser/
646
4 Experiments
We evaluated dependency-informed supertagger
(PW-DEP) both by supertag accuracy 2 and by a
HPSG parser. The experiments were conducted on
WSJ-HPSG treebank (Miyao, 2006). Sections 02-
21 were used to train the dependency parser, the
dependency-informed supertagger and the HPSG
parser. Section 23 was used as the testing set. The
evaluation metric for HPSG parser is the accuracy
of predicate-argument relations in the parser?s out-
put, as in previous work (Sagae et al, 2007).
Model Dep Acc%? Acc%
PW-AP / 91.14
PW-DEP 90.98 92.18
PW-AP (gold POS) / 92.48
PW-DEP (gold POS) 92.05 93.70
100 97.43
Table 2: Supertagging accuracy on section 23. (?)
Dependencies are given by MSTParser evaluated with
labeled accuracy. PW-AP is the baseline point-wise
averaged perceptron model. PW-DEP is point-wise
dependency-informed model. The automatically tagged
POS tags were given by a maximum entropy tagger with
97.39% accuracy.
4.1 Results on Supertagging
We first evaluated the upper-bound of dependency-
informed supertagging model, given gold standard
first-order dependencies. As shown in Table 2,
with such long-distance information supertagging
accuracy can reach 97.43%. Comparing to point-
wise model (PW-AP) which only used local con-
text (92.48%), this absolute 4.95% gain indicated
that dependency information is really informative
for supertagging. When automatically predicted de-
pendency relations were given, there still were ab-
solute 1.04% (auto POS) and 1.22% (gold POS) im-
provements from baseline PW-AP model.
We also compared supertagging results with pre-
vious works (reported on section 22). Here we
mainly compared the dependency-informed point-
wise models with perceptron-based Bayes point ma-
chine (BPM) plus CFG-filter (Zhang et al, 2009).
To the best of our knowledge, these are the state-of-
the-art results on the same dataset with gold POS
2?UNK? supertags are ignored in evaluation as previous.
Figure 2: HPSG Parser F-score on section 23, given au-
tomatically tagged POS.
tags. CFG-filtering can be considered as an al-
ternative way of incorporating long-distance con-
straints on supertagging results. Although our base-
line system was slightly behind (PW-AP: 92.16%
vs. BPM:92.53%), the final accuracies of grammati-
cally constrained models were very close (PW-DEP:
93.53% vs. BPM-CFG: 93.60%); They were not sta-
tistically significantly different (P-value is 0.26). As
the result of oracle PW-DEP indicated, supertagging
accuracy can be further improved with better depen-
dency modeling (e.g., with a semi-supervised de-
pendency parser), which makes it more extensible
and attractive than using CFG-filter after the super-
tagging process.
4.2 HPSG parsing results
We also evaluated the dependency-informed su-
pertagger in a HPSG parser. Considering the effi-
ciency, we use the HPSG parser3 described by Ma-
tsuzaki et al (2007).
In practice, several supertag candidates are re-
served for each word to avoid parsing failure. To
evaluate the quality of the two supertaggers, we re-
stricted the number of each word?s supertag candi-
dates fed to the HPSG parser. As shown in Figure 2,
for the case when only one supertag was predicted
for each word, F-score of the HPSG parser using
dependency-informed supertagger is 5.06% higher
than the parser using the baseline supertagger mod-
ule. As the candidate number increased, the gap nar-
rowed: when all candidates were given, the gains
gradually came down to 0.2%. This indicated that
3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.
647
improved supertagger can optimize the search space
of the deep parser, which may contribute to more ac-
curate and fast deep parsing. From another aspect,
supertagging can be viewed as an interface to com-
bine different types of parsers.
As for the overall parsing time, we didn?t opti-
mize for speed in current setting. The parsing time4
saved by using the improved supertagger (around
6.0 ms/sen, 21.5% time reduction) can not compen-
sate for the extra cost of MSTParser (around 73.8
ms/sen) now. But there is much room to improve the
final speed (e.g., optimizing the dependency parser
for speed or reusing acquired dependencies for ef-
fective pruning). In addition, small beam-size can be
?safely? used with improved supertagger for speed.
Using shallow dependencies in deep HPSG pars-
ing has been previously explored by Sagae et al
(2007), who used dependency constraints in schema
application stage to guide HPSG tree construction
(F-score was improved from 87.2% to 87.9% with
a single shift-reduce dependency parser). Since the
baseline parser is different, we didn?t make a direct
comparison here. However, it would be interesting
to compare these two different ways of incorporat-
ing the dependency parser into HPSG parsing. We
left it as further work.
5 Conclusions
In this paper, focusing on improving the accu-
racy of supertagging, we proposed a simple but
effective way to incorporate long-distance depen-
dency relations into supertagging. The experiments
mainly showed that these long-distance dependen-
cies, which are not easy to model in traditional se-
quence labeling models, are very informative for su-
pertag predictions. Although these were preliminary
results, the method shows its potential strength for
related applications. Not limited to HPSG, it can be
extended to other lexicalized grammar supertaggers.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. We also thank Goran Topic for his self-
less help. The first author was supported by The
University of Tokyo Fellowship (UT-Fellowship).
4Tested on section 23 (2291 sentences) using an AMD
Opteron 2.4GHz server, given all supertag candidates.
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Super-
tagging: An approach to almost parsing. Computa-
tional Linguistics, 25:237?265.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorial grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks (TAG+ 6), pages 19?24.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, pages 288?295.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI-07.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL-05.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Develop-
ment and Feature Forest Model. Ph.D. Dissertation,
The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-
zaki, and Yusuke Miyao. 2006. Extremely lexicalized
models for accurate and fast hpsg parsing. In Proceed-
ings of EMNLP-2006, pages 155?163.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT-03, pages
149?160. Citeseer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago / CSLI.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints. In
Proceedings of ACL-07.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2009. Hpsg supertagging: A sequence labeling
view. In Proceedings of IWPT-09, Paris, France.
648
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325?334,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fine-grained Tree-to-String Translation Rule Extraction
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
Tree-to-string translation rules are widely
used in linguistically syntax-based statis-
tical machine translation systems. In this
paper, we propose to use deep syntac-
tic information for obtaining fine-grained
translation rules. A head-driven phrase
structure grammar (HPSG) parser is used
to obtain the deep syntactic information,
which includes a fine-grained description
of the syntactic property and a semantic
representation of a sentence. We extract
fine-grained rules from aligned HPSG
tree/forest-string pairs and use them in
our tree-to-string and string-to-tree sys-
tems. Extensive experiments on large-
scale bidirectional Japanese-English trans-
lations testified the effectiveness of our ap-
proach.
1 Introduction
Tree-to-string translation rules are generic and ap-
plicable to numerous linguistically syntax-based
Statistical Machine Translation (SMT) systems,
such as string-to-tree translation (Galley et al,
2004; Galley et al, 2006; Chiang et al, 2009),
tree-to-string translation (Liu et al, 2006; Huang
et al, 2006), and forest-to-string translation (Mi et
al., 2008; Mi and Huang, 2008). The algorithms
proposed by Galley et al (2004; 2006) are fre-
quently used for extracting minimal and composed
rules from aligned 1-best tree-string pairs. Deal-
ing with the parse error problem and rule sparse-
ness problem, Mi and Huang (2008) replaced the
1-best parse tree with a packed forest which com-
pactly encodes exponentially many parses for tree-
to-string rule extraction.
However, current tree-to-string rules only make
use of Probabilistic Context-Free Grammar tree
fragments, in which part-of-speech (POS) or
koroshita korosareta
(active) (passive)
VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4)
VBN(killed:active) 5 (5/6,5/6) 1 (1/6,1/4)
VBN(killed:passive) 1 (1/4,1/6) 3 (3/4,3/4)
Table 1: Bidirectional translation probabilities of
rules, denoted in the brackets, change when voice
is attached to ?killed?.
phrasal tags are used as the tree node labels. As
will be testified by our experiments, we argue that
the simple POS/phrasal tags are too coarse to re-
flect the accurate translation probabilities of the
translation rules.
For example, as shown in Table 1, sup-
pose a simple tree fragment ?VBN(killed)? ap-
pears 6 times with ?koroshita?, which is a
Japanese translation of an active form of ?killed?,
and 4 times with ?korosareta?, which is a
Japanese translation of a passive form of ?killed?.
Then, without larger tree fragments, we will
more frequently translate ?VBN(killed)? into ?ko-
roshita? (with a probability of 0.6). But,
?VBN(killed)? is indeed separable into two fine-
grained tree fragments of ?VBN(killed:active)?
and ?VBN(killed:passive)?1. Consequently,
?VBN(killed:active)? appears 5 times with ?ko-
roshita? and 1 time with ?korosareta?; and
?VBN(killed:passive)? appears 1 time with ?ko-
roshita? and 3 times with ?korosareta?. Now, by
attaching the voice information to ?killed?, we are
gaining a rule set that is more appropriate to reflect
the real translation situations.
This motivates our proposal of using deep syn-
tactic information to obtain a fine-grained trans-
lation rule set. We name the information such as
the voice of a verb in a tree fragment as deep syn-
tactic information. We use a head-driven phrase
structure grammar (HPSG) parser to obtain the
1For example, ?John has killed Mary.? versus ?John was
killed by Mary.?
325
deep syntactic information of an English sentence,
which includes a fine-grained description of the
syntactic property and a semantic representation
of the sentence. We extract fine-grained trans-
lation rules from aligned HPSG tree/forest-string
pairs. We localize an HPSG tree/forest to make
it segmentable at any nodes to fit the extraction
algorithms described in (Galley et al, 2006; Mi
and Huang, 2008). We also propose a linear-time
algorithm for extracting composed rules guided
by predicate-argument structures. The effective-
ness of the rules are testified in our tree-to-string
and string-to-tree systems, taking bidirectional
Japanese-English translations as our test cases.
This paper is organized as follows. In Section 2,
we briefly review the tree-to-string and string-to-
tree translation frameworks, tree-to-string rule ex-
traction algorithms, and rich syntactic information
previously used for SMT. The HPSG grammar and
our proposal of fine-grained rule extraction algo-
rithms are described in Section 3. Section 4 gives
the experiments for applying fine-grained transla-
tion rules to large-scale Japanese-English transla-
tion tasks. Finally, we conclude in Section 5.
2 Related Work
2.1 Tree-to-string and string-to-tree
translations
Tree-to-string translation (Liu et al, 2006; Huang
et al, 2006) first uses a parser to parse a source
sentence into a 1-best tree and then searches for
the best derivation that segments and converts the
tree into a target string. In contrast, string-to-tree
translation (Galley et al, 2004; Galley et al, 2006;
Chiang et al, 2009) is like bilingual parsing. That
is, giving a (bilingual) translation grammar and a
source sentence, we are trying to construct a parse
forest in the target language. Consequently, the
translation results can be collected from the leaves
of the parse forest.
Figure 1 illustrates the training and decoding
processes of bidirectional Japanese-English trans-
lations. The English sentence is ?John killed
Mary? and the Japanese sentence is ?jyon ha mari
wo koroshita?, in which the function words ?ha?
and ?wo? are not aligned with any English word.
2.2 Tree/forest-based rule extraction
Galley et al (2004) proposed the GHKM algo-
rithm for extracting (minimal) tree-to-string trans-
lation rules from a tuple of ?F,Et, A?, where F =
 
x0 ? x1 
x0 x1 
x1 ? x0 
NP 
John 
??? 
V 
killed ??? 
NP 
Mary ??? 
NP 
V NP 
VP 
S 
John killed Mary 
??? ? ??? ? ??? 
NP VP 
S 
V NP 
VP 
x0 x1 
Training 
Aligned tree-string pair: 
Extract 
rules  
John killed Mary 
??? ? ??? ? ??? 
CKY decoding 
Testing 
NP V NP 
VP 
S 
John killed Mary 
NP 
VP 
V NP 
Apply  
rules  
?? 
 
jyon   ha    mari  wo  koroshita 
parsing 
Bottom-up 
decoding 
tree-to-string string-to-tree 
Figure 1: Illustration of the training and decod-
ing processes for tree-to-string and string-to-tree
translations.
fJ1 is a sentence of a foreign language other than
English,Et is a 1-best parse tree of an English sen-
tence E = eI1, and A = {(j, i)} is an alignment
between the words in F and E.
The basic idea of GHKM algorithm is to de-
compose Et into a series of tree fragments, each
of which will form a rule with its corresponding
translation in the foreign language. A is used as a
constraint to guide the segmentation procedure, so
that the root node of every tree fragment of Et ex-
actly corresponds to a contiguous span on the for-
eign language side. Based on this consideration, a
frontier set (fs) is defined to be a set of nodes n in
Et that satisfies the following constraint:
fs = {n|span(n) ? comp span(n) = ?}. (1)
Here, span(n) is defined by the indices of the first
and last word in F that are reachable from a node
n, and comp span(n) is defined to be the comple-
ment set of span(n), i.e., the union of the spans
of all nodes n? in Et that are neither descendants
nor ancestors of n. span(n) and comp span(n)
of each n can be computed by first a bottom-up
exploration and then a top-down traversal of Et.
By restricting each fragment so that it only takes
326
John 
CAT       N 
POS      NNP 
BASE    john 
LEXENTRY [D< 
N.3sg>]_lxm 
PRED  noun_arg0  
t0 
HEAD           t0 
SEM_HEAD t0   
CAT           NX   
XCAT  
 
c2 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom  
<V.bse> NP.acc] 
_lxm-past_verb_rule 
PRED  verb_arg12   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
ARG2       c5 
t1 
HEAD           t1 
SEM_HEAD t1  
CAT           VX  
XCAT    
 
c4 
HEAD           c6 
SEM_HEAD c6   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c5 
HEAD           t2 
SEM_HEAD t2  
CAT          NX  
XCAT    
 
c6 
HEAD           c3 
SEM_HEAD c3   
CAT              S   
XCAT    
SCHEMA subj_head  
 
c0 
HEAD           c2 
SEM_HEAD c2   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c1 
HEAD           c4 
SEM_HEAD c4   
CAT              VP   
XCAT    
SCHEMA  head_comp  
c3 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
[D<N.3sg>]_lxm 
PRED    noun_arg0  
 
t2 
??? ? ??? ? ??? 
1. c0(x0:c1, x1:c3)  x0 ? x1 
2. c1(x0:c2)  x0 
3. c2(t0)  ??? 
4. c3(x0:c4, x1:c5)  x1 ? x0 
5. c4(t1)  ??? 
6. c5(x0:c6)  x0 
7. c6(t2)  ??? 
c0 
c1 c3 
c4 c5 
t1 
minimum  
covering tree 
x0 ? x1 ? ??? 
An HPSG-tree based minimal rule set A PAS-based composed rule  
John killed Mary 
HEAD           c8 
SEM_HEAD c8   
CAT              S   
XCAT    
SCHEMA head_mod  
 
c7 
HEAD           c9 
SEM_HEAD c9   
CAT              S   
XCAT    
SCHEMA  subj_head  
 
c8 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom 
<V.bse>]_lxm-
past_verb_rule 
PRED  verb_arg1   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
t3 
HEAD           t3 
SEM_HEAD t3  
CAT           VP  
XCAT    
 
c9 
HEAD           c11 
SEM_HEAD c11   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c10 
HEAD           t4 
SEM_HEAD t4  
CAT          NX  
XCAT    
 
c11 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
V[D<N.3sg>] 
PRED    noun_arg0  
 
t4 
 
2.77 4.52 
0.81 2.25 
0 
0.00 
-3.47 -0.03 
0 
-2.82 
-0.07 -0.001 
Figure 2: Illustration of an aligned HPSG forest-string pair. The forest includes two parse trees by taking
?Mary? as a modifier (t3, t4) or an argument (t1, t2) of ?killed?. Arrows with broken lines denote the PAS
dependencies from the terminal node t1 to its argument nodes (c1 and c5). The scores of the hyperedges
are attached to the forest as well.
the nodes in fs as the root and leaf nodes, a well-
formed fragmentation of Et is generated. With
fs computed, rules are extracted through a depth-
first traversal of Et: we cut Et at all nodes in fs
to form tree fragments and extract a rule for each
fragment. These extracted rules are calledminimal
rules (Galley et al, 2004). For example, the 1-
best tree (with gray nodes) in Figure 2 is cut into 7
pieces, each of which corresponds to the tree frag-
ment in a rule (bottom-left corner of the figure).
In order to include richer context information
and account for multiple interpretations of un-
aligned words of foreign language, minimal rules
which share adjacent tree fragments are connected
together to form composed rules (Galley et al,
2006). For each aligned tree-string pair, Gal-
ley et al (2006) constructed a derivation-forest,
in which composed rules were generated, un-
aligned words of foreign language were consis-
tently attached, and the translation probabilities
of rules were estimated by using Expectation-
Maximization (EM) (Dempster et al, 1977) train-
ing. For example, by combining the minimal rules
of 1, 4, and 5, we obtain a composed rule, as
shown in the bottom-right corner of Figure 2.
Considering the parse error problem in the
1-best or k-best parse trees, Mi and Huang
(2008) extracted tree-to-string translation rules
from aligned packed forest-string pairs. A for-
est compactly encodes exponentially many trees
327
rather than the 1-best tree used by Galley et al
(2004; 2006). Two problems were managed to
be tackled during extracting rules from an aligned
forest-string pair: where to cut and how to cut.
Equation 1 was used again to compute a frontier
node set to determine where to cut the packed
forest into a number of tree-fragments. The dif-
ference with tree-based rule extraction is that the
nodes in a packed forest (which is a hypergraph)
now are hypernodes, which can take a set of in-
coming hyperedges. Then, by limiting each frag-
ment to be a tree and whose root/leaf hypernodes
all appearing in the frontier set, the packed forest
can be segmented properly into a set of tree frag-
ments, each of which can be used to generate a
tree-to-string translation rule.
2.3 Rich syntactic information for SMT
Before describing our approaches of applying
deep syntactic information yielded by an HPSG
parser for fine-grained rule extraction, we would
like to briefly review what kinds of deep syntactic
information have been employed for SMT.
Two kinds of supertags, from Lexicalized Tree-
Adjoining Grammar and Combinatory Categorial
Grammar (CCG), have been used as lexical syn-
tactic descriptions (Hassan et al, 2007) for phrase-
based SMT (Koehn et al, 2007). By introduc-
ing supertags into the target language side, i.e.,
the target language model and the target side
of the phrase table, significant improvement was
achieved for Arabic-to-English translation. Birch
et al (2007) also reported a significant improve-
ment for Dutch-English translation by applying
CCG supertags at a word level to a factorized SMT
system (Koehn et al, 2007).
In this paper, we also make use of supertags
on the English language side. In an HPSG
parse tree, these lexical syntactic descriptions
are included in the LEXENTRY feature (re-
fer to Table 2) of a lexical node (Matsuzaki
et al, 2007). For example, the LEXEN-
TRY feature of ?t1:killed? takes the value of
[NP.nom<V.bse>NP.acc]_lxm-past
_verb_rule in Figure 2. In which,
[NP.nom<V.bse>NP.acc] is an HPSG
style supertag, which tells us that the base form
of ?killed? needs a nominative NP in the left hand
side and an accessorial NP in the right hand side.
The major differences are that, we use a larger
feature set (Table 2) including the supertags for
fine-grained tree-to-string rule extraction, rather
than string-to-string translation (Hassan et al,
2007; Birch et al, 2007).
The Logon project2 (Oepen et al, 2007) for
Norwegian-English translation integrates in-depth
grammatical analysis of Norwegian (using lexi-
cal functional grammar, similar to (Riezler and
Maxwell, 2006)) with semantic representations in
the minimal recursion semantics framework, and
fully grammar-based generation for English using
HPSG. A hybrid (of rule-based and data-driven)
architecture with a semantic transfer backbone is
taken as the vantage point of this project. In
contrast, the fine-grained tree-to-string translation
rule extraction approaches in this paper are to-
tally data-driven, and easily applicable to numer-
ous language pairs by taking English as the source
or target language.
3 Fine-grained rule extraction
We now introduce the deep syntactic informa-
tion generated by an HPSG parser and then de-
scribe our approaches for fine-grained tree-to-
string rule extraction. Especially, we localize an
HPSG tree/forest to fit the extraction algorithms
described in (Galley et al, 2006; Mi and Huang,
2008). Also, we propose a linear-time com-
posed rule extraction algorithm by making use of
predicate-argument structures.
3.1 Deep syntactic information by HPSG
parsing
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, lin-
guistic entities such as words and phrases are rep-
resented by a data structure called a sign. A sign
gives a factored representation of the syntactic fea-
tures of a word/phrase, as well as a representation
of their semantic content. Phrases and words rep-
resented by signs are composed into larger phrases
by applications of schemata. The semantic rep-
resentation of the new phrase is calculated at the
same time. As such, an HPSG parse tree/forest
can be considered as a tree/forest of signs (c.f. the
HPSG forest in Figure 2).
An HPSG parse tree/forest has two attractive
properties as a representation of an English sen-
tence in syntax-based SMT. First, we can carefully
control the condition of the application of a trans-
lation rule by exploiting the fine-grained syntactic
2http://www.emmtee.net/
328
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 2: Syntactic/semantic features extracted
from HPSG signs that are included in the output
of Enju. Features in phrasal nodes (top) and lexi-
cal nodes (bottom) are listed separately.
description in the English parse tree/forest, as well
as those in the translation rules. Second, we can
identify sub-trees in a parse tree/forest that cor-
respond to basic units of the semantics, namely
sub-trees covering a predicate and its arguments,
by using the semantic representation given in the
signs. We expect that extraction of translation
rules based on such semantically-connected sub-
trees will give a compact and effective set of trans-
lation rules.
A sign in the HPSG tree/forest is represented by
a typed feature structure (TFS) (Carpenter, 1992).
A TFS is a directed-acyclic graph (DAG) wherein
the edges are labeled with feature names and the
nodes (feature values) are typed. In the original
HPSG formalism, the types are defined in a hierar-
chy and the DAG can have arbitrary shape (e.g., it
can be of any depth). We however use a simplified
form of TFS, for simplicity of the algorithms. In
the simplified form, a TFS is converted to a (flat)
set of pairs of feature names and their values. Ta-
ble 2 lists the features used in this paper, which
are a subset of those in the original output from an
HPSG parser, Enju3. The HPSG forest shown in
Figure 2 is in this simplified format. An impor-
tant detail is that we allow a feature value to be a
pointer to another (simplified) TFS. Such pointer-
valued features are necessary for denoting the se-
mantics, as explained shortly.
In the Enju English HPSG grammar (Miyao et
3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
 
 
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 3: Predicate argument structures for the
sentences of ?John killed Mary? and ?She ignored
the fact that I wanted to dispute?.
al., 2003) used in this paper, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 3 shows the PAS
of the example sentence in Figure 2, ?John killed
Mary?, and a more complex PAS for another sen-
tence, ?She ignored the fact that I wanted to dis-
pute?, which is adopted from (Miyao et al, 2003).
In an HPSG tree/forest, each leaf node generally
introduces a predicate, which is represented by
the pair of LEXENTRY (lexical entry) feature and
PRED (predicate type) feature. The arguments of
a predicate are designated by the pointers from the
ARG?x? features in a leaf node to non-terminal
nodes.
3.2 Localize HPSG forest
Our fine-grained translation rule extraction algo-
rithm is sketched in Algorithm 1. Considering that
a parse tree is a trivial packed forest, we only use
the term forest to expand our discussion, hereafter.
Recall that there are pointer-valued features in the
TFSs (Table 2) which prevent arbitrary segmenta-
tion of a packed forest. Hence, we have to localize
an HPSG forest.
For example, there are ARG pointers from t1 to
c1 and c5 in the HPSG forest of Figure 2. How-
ever, the three nodes are not included in one (min-
imal) translation rule. This problem is caused
by not considering the predicate argument depen-
dency among t1, c1, and c5 while performing the
GHKM algorithm. We can combine several min-
imal rules (Galley et al, 2006) together to ad-
dress this dependency. Yet we have a faster way
to tackle PASs, as will be described in the next
subsection.
Even if we omit ARG, there are still two kinds
of pointer-valued features in TFSs, HEAD and
SEM HEAD. Localizing these pointer-valued fea-
tures is straightforward, since during parsing, the
HEAD and SEM HEAD of a node are automati-
cally transferred to its mother node. That is, the
syntactic and semantic head of a node only take
329
Algorithm 1 Fine-grained rule extraction
Input: HPSG tree/forest Ef , foreign sentence F , and align-
ment A
Output: a PAS-based rule set R1 and/or a tree-rule set R2
1: if Ef is an HPSG tree then
2: E
?
f = localize Tree(Ef )
3: R1 = PASR extraction(E
?
f , F , A) ? Algorithm 2
4: E
??
f = ignore PAS(E
?
f )
5: R2 = TR extraction(E
??
f , F , A) ? composed rule ex-
traction algorithm in (Galley et al, 2006)
6: else if Ef is an HPSG forest then
7: E
?
f = localize Forest(Ef );
8: R2 = forest based rule extraction(E
?
f , F , A) ? Algo-
rithm 1 in (Mi and Huang, 2008)
9: end if
the identifier of the daughter node as the values.
For example, HEAD and SEM HEAD of node c0
take the identical value to be c3 in Figure 2.
To extract tree-to-string rules from the tree
structures of an HPSG forest, our solution is to
pre-process an HPSG forest in the following way:
? for a phrasal hypernode, replace its HEAD
and SEM HEAD value with L, R, or S,
which respectively represent left daughter,
right daughter, or single daughter (line 2 and
7); and,
? for a lexical node, ARG?x? and PRED fea-
tures are ignored (line 4).
A pure syntactic-based HPSG forest without any
pointer-valued features can be yielded through this
pre-processing for the consequent execution of the
extraction algorithms (Galley et al, 2006; Mi and
Huang, 2008).
3.3 Predicate-argument structures
In order to extract translation rules from PASs,
we want to localize a predicate word and its ar-
guments into one tree fragment. For example, in
Figure 2, we can use a tree fragment which takes
c0 as its root node and c1, t1, and c5 on its yield (=
leaf nodes of a tree fragment) to cover ?killed? and
its subject and direct object arguments. We define
this kind of tree fragment to be a minimum cov-
ering tree. For example, the minimum covering
tree of {t1, c1, c5} is shown in the bottom-right
corner of Figure 2. The definition supplies us a
linear-time algorithm to directly find the tree frag-
ment that covers a PAS during both rule extracting
and rule matching when decoding an HPSG tree.
Algorithm 2 PASR extraction
Input: HPSG tree Et, foreign sentence F , and alignment A
Output: a PAS-based rule set R
1: R = {}
2: for node n ? Leaves(Et) do
3: if Open(n.ARG) then
4: Tc = MinimumCoveringTree(Et, n, n.ARGs)
5: if root and leaf nodes of Tc are in fs then
6: generate a rule r using fragment Tc
7: R.append(r)
8: end if
9: end if
10: end for
See (Wu, 2010) for more examples of minimum
covering trees.
Taking a minimum covering tree as the tree
fragment, we can easily build a tree-to-string
translation rule that reflects the semantic depen-
dency of a PAS. The algorithm of PAS-based
rule (PASR) extraction is sketched in Algorithm
2. Suppose we are given a tuple of ?F,Et, A?.
Et is pre-processed by replacing HEAD and
SEM HEAD to be L, R, or S, and computing the
span and comp span of each node.
We extract PAS-based rules through one-time
traversal of the leaf nodes in Et (line 2). For each
leaf node n, we extract a minimum covering tree
Tc if n contains at least one argument. That is, at
least one ARG?x? takes the value of some node
identifier, where x ranges 1 over 4 (line 3). Then,
we require the root and yield nodes of Tc being in
the frontier set of Et (line 5). Based on Tc, we can
easily build a tree-to-string translation rule by fur-
ther completing the right-hand-side string by sort-
ing the spans of Tc?s leaf nodes, lexicalizing the
terminal node?s span(s), and assigning a variable
to each non-terminal node?s span. Maximum like-
lihood estimation is used to calculate the transla-
tion probabilities of each rule.
An example of PAS-based rule is shown in the
bottom-right corner of Figure 2. In the rule, the
subject and direct-object of ?killed? are general-
ized into two variables, x0 and x1.
4 Experiments
4.1 Translation models
We use a tree-to-string model and a string-to-tree
model for bidirectional Japanese-English transla-
tions. Both models use a phrase translation table
(PTT), an HPSG tree-based rule set (TRS), and
a PAS-based rule set (PRS). Since the three rule
sets are independently extracted and estimated, we
330
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the weights of the features from the
three rule sets on the development set.
Given a 1-best (localized) HPSG tree Et, the
tree-to-string decoder searches for the optimal
derivation d? that transforms Et into a Japanese
string among the set of all possible derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ log s(d|Et)}. (2)
Here, the first item is the language model (LM)
probability where ?(d) is the target string of
derivation d; the second item is the translation
length penalty; and the third item is the transla-
tion score, which is decomposed into a product of
feature values of rules:
s(d|Et) =
?
r?d
f(r?PTT )f(r?TRS)f(r?PRS).
This equation reflects that the translation rules in
one d come from three sets. Inspired by (Liu et
al., 2009b), it is appealing to combine these rule
sets together in one decoder because PTT provides
excellent rule coverages while TRS and PRS offer
linguistically motivated phrase selections and non-
local reorderings. Each f(r) is in turn a product of
five features:
f(r) = p(s|t)?3 ? p(t|s)?4 ? l(s|t)?5 ? l(t|s)?6 ? e?7 .
Here, s/t represent the source/target part of a rule
in PTT, TRS, or PRS; p(?|?) and l(?|?) are transla-
tion probabilities and lexical weights of rules from
PTT, TRS, and PRS. The derivation length penalty
is controlled by ?7.
In our string-to-tree model, for efficient decod-
ing with integrated n-gram LM, we follow (Zhang
et al, 2006) and inversely binarize all translation
rules into Chomsky Normal Forms that contain
at most two variables and can be incrementally
scored by LM. In order to make use of the bina-
rized rules in the CKY decoding, we add two kinds
of glues rules:
S ? Xm(1), Xm(1);
S ? S(1)Xm(2), S(1)Xm(2).
Here Xm ranges over the nonterminals appearing
in a binarized rule set. These glue rules can be
seen as an extension from X to {Xm}of the two
glue rules described in (Chiang, 2007).
The string-to-tree decoder searches for the op-
timal derivation d? that parses a Japanese string
F into a packed forest of the set of all possible
derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ ?3g(d) + log s(d|F )}. (3)
This formula differs from Equation 2 by replacing
Et with F in s(d|?) and adding g(d), which is the
number of glue rules used in d. Further definitions
of s(d|F ) and f(r) are identical with those used
in Equation 2.
4.2 Decoding algorithms
In our translation models, we have made use
of three kinds of translation rule sets which are
trained separately. We perform derivation-level
combination as described in (Liu et al, 2009b) for
mixing different types of translation rules within
one derivation.
For tree-to-string translation, we use a bottom-
up beam search algorithm (Liu et al, 2006) for
decoding an HPSG tree Et. We keep at most 10
best derivations with distinct ?(d)s at each node.
Recall the definition of minimum covering tree,
which supports a faster way to retrieve available
rules from PRS without generating all the sub-
trees. That is, when node n fortunately to be the
root of someminimum covering tree(s), we use the
tree(s) to seek available PAS-based rules in PRS.
We keep a hash-table with the key to be the node
identifier of n and the value to be a priority queue
of available PAS-based rules. The hash-table is
easy to be filled by one-time traversal of the termi-
nal nodes in Et. At each terminal node, we seek
its minimum covering tree, retrieve PRS, and up-
date the hash-table. For example, suppose we are
decoding an HPSG tree (with gray nodes) shown
in Figure 2. At t1, we can extract its minimum
covering tree with the root node to be c0, then take
this tree fragment as the key to retrieve PRS, and
consequently put c0 and the available rules in the
hash-table. When decoding at c0, we can directly
access the hash-table looking for available PAS-
based rules.
In contrast, we use a CKY-style algorithm with
beam-pruning and cube-pruning (Chiang, 2007)
to decode Japanese sentences. For each Japanese
sentence F , the output of the chart-parsing algo-
rithm is expressed as a hypergraph representing a
set of derivations. Given such a hypergraph, we
331
Train Dev. Test
# of sentences 994K 2K 2K
# of Jp words 28.2M 57.4K 57.1K
# of En words 24.7M 50.3K 49.9K
Table 3: Statistics of the JST corpus.
use the Algorithm 3 described in (Huang and Chi-
ang, 2005) to extract its k-best (k = 500 in our
experiments) derivations. Since different deriva-
tions may lead to the same target language string,
we further adopt Algorithm 3?s modification, i.e.,
keep a hash-table to maintain the unique target
sentences (Huang et al, 2006), to efficiently gen-
erate the unique k-best translations.
4.3 Setups
The JST Japanese-English paper abstract corpus4,
which consists of one million parallel sentences,
was used for training and testing. This corpus
was constructed from a Japanese-English paper
abstract corpus by using the method of Utiyama
and Isahara (2007). Table 3 shows the statistics
of this corpus. Making use of Enju 2.3.1, we suc-
cessfully parsed 987,401 English sentences in the
training set, with a parse rate of 99.3%. We mod-
ified this parser to output a packed forest for each
English sentence.
We executed GIZA++ (Och and Ney, 2003) and
grow-diag-final-and balancing strategy (Koehn et
al., 2007) on the training set to obtain a phrase-
aligned parallel corpus, from which bidirectional
phrase translation tables were estimated. SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) was em-
ployed to train 5-gram English and Japanese LMs
on the training set. We evaluated the translation
quality using the case-insensitive BLEU-4 metric
(Papineni et al, 2002). The MERT toolkit we used
is Z-mert5 (Zaidan, 2009).
The baseline system for comparison is Joshua
(Li et al, 2009), a freely available decoder for hi-
erarchical phrase-based SMT (Chiang, 2005). We
respectively extracted 4.5M and 5.3M translation
rules from the training set for the 4K English and
Japanese sentences in the development and test
sets. We used the default configuration of Joshua,
expect setting the maximum number of items/rules
and the k of k-best outputs to be the identical
4http://www.jst.go.jp. The corpus can be conditionally
obtained from NTCIR-7 patent translation workshop home-
page: http://research.nii.ac.jp/ntcir/permission/ntcir-7/perm-
en-PATMT.html.
5http://www.cs.jhu.edu/ ozaidan/zmert/
PRS CS3 C3 FS F
tree nodes TFS POS TFS POS TFS
# rules 0.9 62.1 83.9 92.5 103.7
# tree types 0.4 23.5 34.7 40.6 45.2
extract time 3.5 - 98.6 - 121.2
Table 4: Statistics of several kinds of tree-to-string
rules. Here, the number is in million level and the
time is in hour.
200 for English-to-Japanese translation and 500
for Japanese-to-English translation.
We used four dual core Xeon machines
(4?3.0GHz?2CPU, 4?64GB memory) to run all
the experiments.
4.4 Results
Table 4 illustrates the statistics of several transla-
tion rule sets, which are classified by:
? using TFSs or simple POS/phrasal tags (an-
notated by a superscript S) to represent tree
nodes;
? composed rules (PRS) extracted from the
PAS of 1-best HPSG trees;
? composed rules (C3), extracted from the tree
structures of 1-best HPSG trees, and 3 is the
maximum number of internal nodes in the
tree fragments; and
? forest-based rules (F ), where the packed
forests are pre-pruned by the marginal
probability-based inside-outside algorithm
used in (Mi and Huang, 2008).
Table 5 reports the BLEU-4 scores achieved by
decoding the test set making use of Joshua and our
systems (t2s = tree-to-string and s2t = string-to-
tree) under numerous rule sets. We analyze this
table in terms of several aspects to prove the effec-
tiveness of deep syntactic information for SMT.
Let?s first look at the performance of TFSs. We
take CS3 and FS as approximations of CFG-based
translation rules. Comparing the BLEU-4 scores
of PTT+CS3 and PTT+C3, we gained 0.56 (t2s)
and 0.57 (s2t) BLEU-4 points which are signifi-
cant improvements (p < 0.05). Furthermore, we
gained 0.50 (t2s) and 0.62 (s2t) BLEU-4 points
from PTT+FS to PTT+F , which are also signif-
icant improvements (p < 0.05). The rich fea-
tures included in TFSs contribute to these im-
provements.
332
Systems BLEU-t2s Decoding BLEU-s2t
Joshua 21.79 0.486 19.73
PTT 18.40 0.013 17.21
PTT+PRS 22.12 0.031 19.33
PTT+CS3 23.56 2.686 20.59
PTT+C3 24.12 2.753 21.16
PTT+C3+PRS 24.13 2.930 21.20
PTT+FS 24.25 3.241 22.05
PTT+F 24.75 3.470 22.67
Table 5: BLEU-4 scores (%) achieved by Joshua
and our systems under numerous rule configura-
tions. The decoding time (seconds per sentence)
of tree-to-string translation is listed as well.
Also, BLEU-4 scores were inspiringly in-
creased 3.72 (t2s) and 2.12 (s2t) points by append-
ing PRS to PTT, comparing PTT with PTT+PRS.
Furthermore, in Table 5, the decoding time (sec-
onds per sentence) of tree-to-string translation by
using PTT+PRS is more than 86 times faster than
using the other tree-to-string rule sets. This sug-
gests that the direct generation of minimum cover-
ing trees for rule matching is extremely faster than
generating all subtrees of a tree node. Note that
PTT performed extremely bad compared with all
other systems or tree-based rule sets. The major
reason is that we did not perform any reordering
or distorting during decoding with PTT.
However, in both t2s and s2t systems, the
BLEU-4 score benefits of PRS were covered by
the composed rules: both PTT+CS3 and PTT+C3
performed significant better (p < 0.01) than
PTT+PRS, and there are no significant differences
when appending PRS to PTT+C3. The reason is
obvious: PRS is only a small subset of the com-
posed rules, and the probabilities of rules in PRS
were estimated by maximum likelihood, which is
fast but biased compared with EM based estima-
tion (Galley et al, 2006).
Finally, by using PTT+F , our systems achieved
the best BLEU-4 scores of 24.75% (t2s) and
22.67% (s2t), both are significantly better (p <
0.01) than that achieved by Joshua.
5 Conclusion
We have proposed approaches of using deep syn-
tactic information for extracting fine-grained tree-
to-string translation rules from aligned HPSG
forest-string pairs. The main contributions are the
applications of GHKM-related algorithms (Galley
et al, 2006; Mi and Huang, 2008) to HPSG forests
and a linear-time algorithm for extracting com-
posed rules from predicate-argument structures.
We applied our fine-grained translation rules to a
tree-to-string system and an Hiero-style string-to-
tree system. Extensive experiments on large-scale
bidirectional Japanese-English translations testi-
fied the significant improvements on BLEU score.
We argue the fine-grained translation rules are
generic and applicable to many syntax-based SMT
frameworks such as the forest-to-string model (Mi
et al, 2008). Furthermore, it will be interesting
to extract fine-grained tree-to-tree translation rules
by integrating deep syntactic information in the
source and/or target language side(s). These tree-
to-tree rules are applicable for forest-to-tree trans-
lation models (Liu et al, 2009a).
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japanese/Chinese Machine Translation
Project in Special Coordination Funds for Pro-
moting Science and Technology (MEXT, Japan),
and Microsoft Research Asia Machine Translation
Theme. The first author thanks Naoaki Okazaki
and Yusuke Miyao for their help and the anony-
mous reviewers for improving the earlier version.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?
16, June.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 218?
226, June.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Lingustics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
333
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of ACL, pages 288?295, June.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, pages 177?180.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Demonstration of joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Au-
gust.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI, pages 1671?
1676, January.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, and Victoria Rose?n. 2007. Towards hy-
brid quality-oriented machine translation - on lin-
guistics and probabilities in mt. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI-07), September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Stefan Riezler and John T. Maxwell, III. 2006. Gram-
matical machine translation. In Proceedings of HLT-
NAACL, pages 248?255.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT Summit XI, pages 475?482, Copenhagen.
Xianchao Wu. 2010. Statistical Machine Transla-
tion Using Large-Scale Lexicon and Deep Syntactic
Structures. Ph.D. dissertation. Department of Com-
puter Science, The University of Tokyo.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT-NAACL,
pages 256?263, June.
334
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22?31,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Effective Use of Function Words for Rule Generalization
in Forest-Based Translation
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
In the present paper, we propose the ef-
fective usage of function words to generate
generalized translation rules for forest-based
translation. Given aligned forest-string pairs,
we extract composed tree-to-string translation
rules that account for multiple interpretations
of both aligned and unaligned target func-
tion words. In order to constrain the ex-
haustive attachments of function words, we
limit to bind them to the nearby syntactic
chunks yielded by a target dependency parser.
Therefore, the proposed approach can not
only capture source-tree-to-target-chunk cor-
respondences but can also use forest structures
that compactly encode an exponential num-
ber of parse trees to properly generate target
function words during decoding. Extensive
experiments involving large-scale English-to-
Japanese translation revealed a significant im-
provement of 1.8 points in BLEU score, as
compared with a strong forest-to-string base-
line system.
1 Introduction
Rule generalization remains a key challenge for
current syntax-based statistical machine translation
(SMT) systems. On the one hand, there is a ten-
dency to integrate richer syntactic information into
a translation rule in order to better express the trans-
lation phenomena. Thus, flat phrases (Koehn et al,
2003), hierarchical phrases (Chiang, 2005), and syn-
tactic tree fragments (Galley et al, 2006; Mi and
Huang, 2008; Wu et al, 2010) are gradually used in
SMT. On the other hand, the use of syntactic phrases
continues due to the requirement for phrase cover-
age in most syntax-based systems. For example,
Mi et al (2008) achieved a 3.1-point improvement
in BLEU score (Papineni et al, 2002) by including
bilingual syntactic phrases in their forest-based sys-
tem. Compared with flat phrases, syntactic rules are
good at capturing global reordering, which has been
reported to be essential for translating between lan-
guages with substantial structural differences, such
as English and Japanese, which is a subject-object-
verb language (Xu et al, 2009).
Forest-based translation frameworks, which make
use of packed parse forests on the source and/or tar-
get language side(s), are an increasingly promising
approach to syntax-based SMT, being both algorith-
mically appealing (Mi et al, 2008) and empirically
successful (Mi and Huang, 2008; Liu et al, 2009).
However, forest-based translation systems, and, in
general, most linguistically syntax-based SMT sys-
tems (Galley et al, 2004; Galley et al, 2006; Liu
et al, 2006; Zhang et al, 2007; Mi et al, 2008;
Liu et al, 2009; Chiang, 2010), are built upon word
aligned parallel sentences and thus share a critical
dependence on word alignments. For example, even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, and un-
aligned words can result in an exponentially large
set of extractable rules for the interpretation of these
unaligned words (Galley et al, 2006).
What makes word alignment so fragile? In or-
der to investigate this problem, we manually ana-
lyzed the alignments of the first 100 parallel sen-
tences in our English-Japanese training data (to be
shown in Table 2). The alignments were generated
by running GIZA++ (Och and Ney, 2003) and the
grow-diag-final-and symmetrizing strategy (Koehn
et al, 2007) on the training set. Of the 1,324 word
alignment pairs, there were 309 error pairs, among
22
which there were 237 target function words, which
account for 76.7% of the error pairs1. This indicates
that the alignments of the function words are more
easily to be mistaken than content words. More-
over, we found that most Japanese function words
tend to align to a few English words such as ?of?
and ?the?, which may appear anywhere in an English
sentence. Following these problematic alignments,
we are forced to make use of relatively large English
tree fragments to construct translation rules that tend
to be ill-formed and less generalized.
This is the motivation of the present approach of
re-aligning the target function words to source tree
fragments, so that the influence of incorrect align-
ments is reduced and the function words can be gen-
erated by tree fragments on the fly. However, the
current dominant research only uses 1-best trees for
syntactic realignment (Galley et al, 2006; May and
Knight, 2007; Wang et al, 2010), which adversely
affects the rule set quality due to parsing errors.
Therefore, we realign target function words to a
packed forest that compactly encodes exponentially
many parses. Given aligned forest-string pairs, we
extract composed tree-to-string translation rules that
account for multiple interpretations of both aligned
and unaligned target function words. In order to con-
strain the exhaustive attachments of function words,
we further limit the function words to bind to their
surrounding chunks yielded by a dependency parser.
Using the composed rules of the present study in
a baseline forest-to-string translation system results
in a 1.8-point improvement in the BLEU score for
large-scale English-to-Japanese translation.
2 Backgrounds
2.1 Japanese function words
In the present paper, we limit our discussion
on Japanese particles and auxiliary verbs (Martin,
1975). Particles are suffixes or tokens in Japanese
grammar that immediately follow modified con-
tent words or sentences. There are eight types of
Japanese function words, which are classified de-
pending on what function they serve: case markers,
parallel markers, sentence ending particles, interjec-
1These numbers are language/corpus-dependent and are not
necessarily to be taken as a general reflection of the overall qual-
ity of the word alignments for arbitrary language pairs.
tory particles, adverbial particles, binding particles,
conjunctive particles, and phrasal particles.
Japanese grammar also uses auxiliary verbs to
give further semantic or syntactic information about
the preceding main or full verb. Alike English, the
extra meaning provided by a Japanese auxiliary verb
alters the basic meaning of the main verb so that the
main verb has one or more of the following func-
tions: passive voice, progressive aspect, perfect as-
pect, modality, dummy, or emphasis.
2.2 HPSG forests
Following our precious work (Wu et al, 2010), we
use head-drive phrase structure grammar (HPSG)
forests generated by Enju2 (Miyao and Tsujii, 2008),
which is a state-of-the-art HPSG parser for English.
HPSG (Pollard and Sag, 1994; Sag et al, 2003) is a
lexicalist grammar framework. In HPSG, linguistic
entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are collected into larger phrases by the appli-
cations of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse forest can be considered to
be a forest of signs. Making use of these signs in-
stead of part-of-speech (POS)/phrasal tags in PCFG
results in a fine-grained rule set integrated with deep
syntactic information.
For example, an aligned HPSG forest3-string pair
is shown in Figure 1. For simplicity, we only draw
the identifiers for the signs of the nodes in the HPSG
forest. Note that the identifiers that start with ?c? de-
note non-terminal nodes (e.g., c0, c1), and the iden-
tifiers that start with ?t? denote terminal nodes (e.g.,
t3, t1). In a complete HPSG forest given in (Wu et
al., 2010), the terminal signs include features such
as the POS tag, the tense, the auxiliary, the voice of
a verb, etc.. The non-terminal signs include features
such as the phrasal category, the name of the schema
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
3The forest includes three parse trees rooted at c0, c1, and
c2. In the 1-best tree, ?by? modifies the passive verb ?verified?.
Yet in the 2- and 3-best tree, ?by? modifies ?this result was ver-
ified?. Furthermore, ?verified? is an adjective in the 2-best tree
and a passive verb in the 3-best tree.
23
 
 
jikken niyotte kono kekka ga sa re ta kensyou 
Realign target function words 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 ? 6 
 
? 7 
 
? 8 
 
?? 5 
 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c10 c16 c22 c4 c21 c12 c18 c19 c14 c15
c23 
c8 
c13 
c5 c17 
c3 
c6 
c2 
c7 
c11 
c0 
c20 
c1 
1-best tree 2-best tree 3-best tree 
experiments 
 
by 
 
this 
 
result 
 
 
 
verified 
 
c1 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 
 
? 6 
 
? 7 
 
? 8 
 
?? 5 
 C1 C2 C3 C4 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c16 c22 c45-7 | 5-8 c125-7 | 5-8  c18 c19 c14 c15
c2 c0 
c215-7 | 5-8 
c23 c8 
c13 
c5 c17 
c3 
c6 
c7 
c11 c20 
c103 | 3-4 
Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation. The chunk-level
dependency tree for the Japanese sentence is shown as well.
applied in the node, etc..
3 Composed Rule Extraction
In this section, we first describe an algorithm that
attaches function words to a packed forest guided
by target chunk information. That is, given a triple
?FS , T, A?, namely an aligned (A) source forest
(FS) to target sentence (T ) pair, we 1) tailor the
alignment A by removing the alignments for tar-
get function words, 2) seek attachable nodes in the
source forest FS for each function word, and 3) con-
struct a derivation forest by topologically travers-
ing FS . Then, we identify minimal and composed
rules from the derivation forest and estimate the
probabilities of rules and scores of derivations us-
ing the expectation-maximization (EM) (Dempster
et al, 1977) algorithm.
3.1 Definitions
In the proposed algorithm, we make use of the fol-
lowing definitions, which are similar to those de-
scribed in (Galley et al, 2004; Mi and Huang, 2008):
? s(?): the span of a (source) node v or a (target)
chunk C, which is an index set of the words that
24
v or C covers;
? t(v): the corresponding span of v, which is an
index set of aligned words on another side;
? c(v): the complement span of v, which is the
union of corresponding spans of nodes v? that
share an identical parse tree with v but are nei-
ther antecedents nor descendants of v;
? PA: the frontier set of FS , which contains
nodes that are consistent with an alignment A
(gray nodes in Figure 1), i.e., t(v) ?= ? and
closure(t(v)) ? c(v) = ?.
The function closure covers the gap(s) that may
appear in the interval parameter. For example,
closure(t(c3)) = closure({0-1, 4-7}) = {0-7}.
Examples of the applications of these functions can
be found in Table 1. Following (Galley et al,
2006), we distinguish between minimal and com-
posed rules. The composed rules are generated by
combining a sequence of minimal rules.
3.2 Free attachment of target function words
3.2.1 Motivation
We explain the motivation for the present research
using an example that was extracted from our train-
ing data, as shown in Figure 1. In the alignment of
this example, three lines (in dot lines) are used to
align was and the with ga (subject particle), and was
with ta (past tense auxiliary verb). Under this align-
ment, we are forced to extract rules with relatively
large tree fragments. For example, by applying the
GHKM algorithm (Galley et al, 2004), a rule rooted
at c0 will take c7, t4, c4, c19, t2, and c15 as the
leaves. The final tree fragment, with a height of 7,
contains 13 nodes. In order to ensure that this rule
is used during decoding, we must generate subtrees
with a height of 7 for c0. Suppose that the input for-
est is binarized and that |E| is the average number
of hyperedges of each node, then we must generate
O(|E|26?1) subtrees4 for c0 in the worst case. Thus,
4For one (binarized) hyperedge e of a node, suppose there
are x subtrees in the left tail node and y subtrees in the right tail
node. Then the number of subtrees guided by e is (x + 1) ?
(y+1). Thus, the recursive formula is Nh = |E|(Nh?1 +1)2,
where h is the height of the hypergraph and Nh is the number
of subtrees. When h = 1, we let Nh = 0.
the existence of these rules prevents the generaliza-
tion ability of the final rule set that is extracted.
In order to address this problem, we tailor the
alignment by ignoring these three alignment pairs in
dot lines. For example, by ignoring the ambiguous
alignments on the Japanese function words, we en-
large the frontier set to include from 12 to 19 of the
24 non-terminal nodes. Consequently, the number
of extractable minimal rules increases from 12 (with
three reordering rules rooted at c0, c1, and c2) to
19 (with five reordering rules rooted at c0, c1, c2,
c5, and c17). With more nodes included in the fron-
tier set, we can extract more minimal and composed
monotonic/reordering rules and avoid extracting the
less generalized rules with extremely large tree frag-
ments.
3.2.2 Why chunking?
In the proposed algorithm, we use a target chunk
set to constrain the attachment explosion problem
because we use a packed parse forest instead of a 1-
best tree, as in the case of (Galley et al, 2006). Mul-
tiple interpretations of unaligned function words for
an aligned tree-string pair result in a derivation for-
est. Now, we have a packed parse forest in which
each tree corresponds to a derivation forest. Thus,
pruning free attachments of function words is prac-
tically important in order to extract composed rules
from this ?(derivation) forest of (parse) forest?.
In the English-to-Japanese translation test case of
the present study, the target chunk set is yielded
by a state-of-the-art Japanese dependency parser,
Cabocha v0.535 (Kudo and Matsumoto, 2002). The
output of Cabocha is a list of chunks. A chunk con-
tains roughly one content word (usually the head)
and affixed function words, such as case markers
(e.g., ga) and verbal morphemes (e.g., sa re ta,
which indicate past tense and passive voice). For
example, the Japanese sentence in Figure 1 is sepa-
rated into four chunks, and the dependencies among
these chunks are identified by arrows. These arrows
point out the head chunk that the current chunk mod-
ifies. Moreover, we also hope to gain a fine-grained
alignment among these syntactic chunks and source
tree fragments. Thereby, during decoding, we are
binding the generation of function words with the
generation of target chunks.
5http://chasen.org/?taku/software/cabocha/
25
Algorithm 1 Aligning function words to the forest
Input: HPSG forest FS , target sentence T , word alignment
A = {(i, j)}, target function word set {fw} appeared in
T , and target chunk set {C}
Output: a derivation forest DF
1: A? ? A \ {(i, s(fw))} ? fw ? {fw}
2: for each node v ? PA? in topological order do
3: Tv ? ? ? store the corresponding spans of v
4: for each function word fw ? {fw} do
5: if fw ? C and t(v)?(C) ?= ? and fw are not attached
to descendants of v then
6: append t(v) ? {s(fw)} to Tv
7: end if
8: end for
9: for each corresponding span t(v) ? Tv do
10: R ? IDENTIFYMINRULES(v, t(v), T ) ? range
over the hyperedges of v, and discount the factional
count of each rule r ? R by 1/|Tv|
11: create a node n in DF for each rule r ? R
12: create a shared parent node ? when |R| > 1
13: end for
14: end for
3.2.3 The algorithm
Algorithm 1 outlines the proposed approach to
constructing a derivation forest to include multiple
interpretations of target function words. The deriva-
tion forest is a hypergraph as previously used in
(Galley et al, 2006), to maintain the constraint that
one unaligned target word be attached to some node
v exactly once in one derivation tree. Starting from
a triple ?FS , T, A?, we first tailor the alignment A
to A? by removing the alignments for target function
words. Then, we traverse the nodes v ? PA? in topo-
logical order. During the traversal, a function word
fw will be attached to v if 1) t(v) overlaps with the
span of the chunk to which fw belongs, and 2) fw
has not been attached to the descendants of v.
We identify translation rules that take v as the root
of their tree fragments. Each tree fragment is a fron-
tier tree that takes a node in the frontier set PA?
of FS as the root node and non-lexicalized frontier
nodes or lexicalized non-frontier nodes as the leaves.
Also, a minimal frontier tree used in a minimal rule
is limited to be a frontier tree such that all nodes
other than the root and leaves are non-frontier nodes.
We use Algorithm 1 described in (Mi and Huang,
2008) to collect minimal frontier trees rooted at v in
FS . That is, we range over each hyperedges headed
at v and continue to expand downward until the cur-
A ? (A?)
node s(?) t(?) c(?) consistent
c0 0-6 0-8(0-3,5-7) ? 1
c1 0-6 0-8(0-3,5-7) ? 1
c2 0-6 0-8(0-3,5-7) ? 1
c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0
c4 3 5-7 0,8(0-3) 1
c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c7 0-1 2-3 0-1,4-8(0-1,5-7) 1
c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)
c9 0 2 0-1,3-8(0-1,3,5-7) 1
c10 1 3 0-2,4-8(0-2,5-7) 1
c11 2-6 0-1,4-8(0-1,5-7) 2-3 0
c12 3 5-7 0,8(0-3) 1
c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)
c14 5 4(?) 0-8(0-3,5-7) 0
c15 6 0 1-8(1-3,5-7) 1
c16 2 4,8(?) 0-7(0-3,5-7) 0
c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c18 4 1 0,2-8(0,2-3,5-7) 1
c19 4 1 0,2-8(0,2-3,5-7) 1
c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c21 3 5-7 0,8(0-3) 1
c22 2 4,8(?) 0-7(0-3,5-7) 0
c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)
Table 1: Change of node attributes after alignment modi-
fication from A to A? of the example in Figure 1. Nodes
with * superscripts are consistent with A? but not consis-
tent with A.
rent set of hyperedges forms a minimal frontier tree.
In the derivation forest, we use ? nodes to man-
age minimal/composed rules that share the same
node and the same corresponding span. Figure 2
shows some minimal rule and ? nodes derived from
the example in Figure 1.
Even though we bind function words to their
nearby chunks, these function words may still be at-
tached to relative large tree fragments, so that richer
syntactic information can be used to predict the
function words. For example, in Figure 2, the tree
fragments rooted at node c0?80 can predict ga and/or
ta. The syntactic foundation behind is that, whether
to use ga as a subject particle or to use wo as an ob-
ject particle depends on both the left-hand-side noun
phrase (kekka) and the right-hand-side verb (kensyou
sa re ta). This type of node v? (such as c0?80 ) should
satisfy the following two heuristic conditions:
? v? is included in the frontier set PA? of FS , and
? t(v?) covers the function word, or v? is the root
node ofFS if the function word is the beginning
or ending word in the target sentence T .
Starting from this derivation forest with minimal
26
 c103-4 
t13: result 
kekka ga  
 
* c103 
t13: result 
kekka  
 
c92 
t32: the 
kono  
 
c72-3 
c103 c92 
x0 x1 
 x0 
 x1  
c72-4 
c103-4 c92 
x0 x1 
 x0  x1  *
c62-7 
c85-7 c72-3 
x0 ga x1  
 x0 
 x1  
* c62-7 
c85-7 c72-4 
x0 x1  
 x0 
 x1  * 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-4 c11 x2 x0 x1 ta 
 
x0 
 
x1 
 
x2 
 
+ 
* 
c00-8 
c16 
c45-8 c50-1 
c3 
c72-4 c11 x2 x0 x1  
 
x0 
 
x1 
 
x2 
 + 
* 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 ta 
 x0  
x1 
 
x2 
 
* + c00-8 
c16 
c45-8 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 
 x0  
x1 
 
x2 
 
* 
+ 
t4{}:was t4{}:was t4{}:was t4{}:was 
Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s).
Nodes annotated by ?*? include ga, and nodes annotated by ?+? include ta.
rules as nodes, we can further combine two or more
minimal rules to form composed rules nodes and can
append these nodes to the derivation forest.
3.3 Estimating rule probabilities
We use the EM algorithm to jointly estimate 1)
the translation probabilities and fractional counts of
rules and 2) the scores of derivations in the deriva-
tion forests. As reported in (May and Knight, 2007),
EM, as has been used in (Galley et al, 2006) to es-
timate rule probabilities in derivation forests, is an
iterative procedure and prefers shorter derivations
containing large rules over longer derivations con-
taining small rules. In order to overcome this bias
problem, we discount the fractional count of a rule
by the product of the probabilities of parse hyper-
edges that are included in the tree fragment of the
rule.
4 Experiments
4.1 Setup
We implemented the forest-to-string decoder de-
scribed in (Mi et al, 2008) that makes use of forest-
based translation rules (Mi and Huang, 2008) as
the baseline system for translating English HPSG
forests into Japanese sentences. We analyzed the
performance of the proposed translation rule sets by
Train Dev. Test
# sentence pairs 994K 2K 2K
# En 1-best trees 987,401 1,982 1,984
# En forests 984,731 1,979 1,983
# En words 24.7M 50.3K 49.9K
# Jp words 28.2M 57.4K 57.1K
# Jp function words 8.0M 16.1K 16.1K
Table 2: Statistics of the JST corpus. Here, En = English
and Jp = Japanese.
using the same decoder.
The JST Japanese-English paper abstract corpus6
(Utiyama and Isahara, 2007), which consists of one
million parallel sentences, was used for training,
tuning, and testing. Table 2 shows the statistics of
this corpus. Note that Japanese function words oc-
cupy more than a quarter of the Japanese words.
Making use of Enju 2.3.1, we generated 987,401
1-best trees and 984,731 parse forests for the En-
glish sentences in the training set, with successful
parse rates of 99.3% and 99.1%, respectively. Us-
ing the pruning criteria expressed in (Mi and Huang,
2008), we continue to prune a parse forest by set-
ting pe to be 8, 5, and 2, until there are no more than
e10 = 22, 026 trees in a forest. After pruning, there
are an average of 82.3 trees in a parse forest.
6http://www.jst.go.jp
27
C3-T M&H-F Min-F C3-F
free fw Y N Y Y
alignment A? A A? A?
English side tree forest forest forest
# rule 86.30 96.52 144.91 228.59
# reorder rule 58.50 91.36 92.98 162.71
# tree types 21.62 93.55 72.98 120.08
# nodes/tree 14.2 42.1 26.3 18.6
extract time 30.2 52.2 58.6 130.7
EM time 9.4 - 11.2 29.0
# rules in dev. 0.77 1.22 1.37 2.18
# rules in test 0.77 1.23 1.37 2.15
DT(sec./sent.) 2.8 15.7 22.4 35.4
BLEU (%) 26.15 27.07 27.93 28.89
Table 3: Statistics and translation results for four types of
tree-to-string rules. With the exception of ?# nodes/tree?,
the numbers in the table are in millions and the time is in
hours. Here, fw denotes function word, and DT denotes
the decoding time, and the BLEU scores were computed
on the test set.
We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) on the training set to obtain
alignments. The SRI Language Modeling Toolkit
(Stolcke, 2002) was employed to train a five-gram
Japanese LM on the training set. We evaluated the
translation quality using the BLEU-4 metric (Pap-
ineni et al, 2002).
Joshua v1.3 (Li et al, 2009), which is a
freely available decoder for hierarchical phrase-
based SMT (Chiang, 2005), is used as an external
baseline system for comparison. We extracted 4.5M
translation rules from the training set for the 4K En-
glish sentences in the development and test sets. We
used the default configuration of Joshua, with the ex-
ception of the maximum number of items/rules, and
the value of k (of the k-best outputs) is set to be 200.
4.2 Results
Table 3 lists the statistics of the following translation
rule sets:
? C3-T: a composed rule set extracted from the
derivation forests of 1-best HPSG trees that
were constructed using the approach described
in (Galley et al, 2006). The maximum number
of internal nodes is set to be three when gen-
erating a composed rule. We free attach target
function words to derivation forests;
0
5
10
15
20
25
2 12 22 32 42 52 62 72 82 92
# o
f ru
les 
(M
)
# of tree nodes in rule
M&H-F
Min-F
C3-T
C3-F
Figure 3: Distributions of the number of tree nodes in the
translation rule sets. Note that the curves of Min-F and
C3-F are duplicated when the number of tree nodes being
larger than 9.
? M&H-F: a minimal rule set extracted from
HPSG forests using the extracting algorithm of
(Mi and Huang, 2008). Here, we make use of
the original alignments. We use the two heuris-
tic conditions described in Section 3.2.3 to at-
tach unaligned words to some node(s) in the
forest;
? Min-F: a minimal rule set extracted from the
derivation forests of HPSG forests that were
constructed using Algorithm 1 (Section 3).
? C3-F: a composed rule set extracted from the
derivation forests of HPSG forests. Similar to
C3-T, the maximum number of internal nodes
during combination is three.
We investigate the generalization ability of these
rule sets through the following aspects:
1. the number of rules, the number of reordering
rules, and the distributions of the number of
tree nodes (Figure 3), i.e., more rules with rel-
atively small tree fragments are preferred;
2. the number of rules that are applicable to the
development and test sets (Table 3); and
3. the final translation accuracies.
Table 3 and Figure 3 reflect that the generalization
abilities of these four rule sets increase in the or-
der of C3-T < M&H-F < Min-F < C3-F. The ad-
vantage of using a packed forest for re-alignment is
verified by comparing the statistics of the rules and
28
0
10
20
30
40
50
0.0
0.5
1.0
1.5
2.0
2.5
C3-T M&H-F Min-F C3-F
Dec
odi
ng t
ime
 (se
c./s
ent
.)
# o
f ru
les 
(M)
# rules (M)
DT
Figure 4: Comparison of decoding time and the number
of rules used for translating the test set.
the final BLEU scores of C3-T with Min-F and C3-
F. Using the composed rule set C3-F in our forest-
based decoder, we achieved an optimal BLEU score
of 28.89 (%). Taking M&H-F as the baseline trans-
lation rule set, we achieved a significant improve-
ment (p < 0.01) of 1.81 points.
In terms of decoding time, even though we used
Algorithm 3 described in (Huang and Chiang, 2005),
which lazily generated the N-best translation can-
didates, the decoding time tended to be increased
because more rules were available during cube-
pruning. Figure 4 shows a comparison of decoding
time (seconds per sentence) and the number of rules
used for translating the test set. Easy to observe that,
decoding time increases in a nearly linear way fol-
lowing the increase of the number of rules used dur-
ing decoding.
Finally, compared with Joshua, which achieved
a BLEU score of 24.79 (%) on the test set with
a decoding speed of 8.8 seconds per sentence, our
forest-based decoder achieved a significantly better
(p < 0.01) BLEU score by using either of the four
types of translation rules.
5 Related Research
Galley et al (2006) first used derivation forests of
aligned tree-string pairs to express multiple inter-
pretations of unaligned target words. The EM al-
gorithm was used to jointly estimate 1) the trans-
lation probabilities and fractional counts of rules
and 2) the scores of derivations in the derivation
forests. By dealing with the ambiguous word align-
ment instead of unaligned target words, syntax-
based re-alignment models were proposed by (May
and Knight, 2007; Wang et al, 2010) for tree-based
translations.
Free attachment of the unaligned target word
problem was ignored in (Mi and Huang, 2008),
which was the first study on extracting tree-to-string
rules from aligned forest-string pairs. This inspired
the idea to re-align a packed forest and a target sen-
tence. Specially, we observed that most incorrect or
ambiguous word alignments are caused by function
words rather than content words. Thus, we focus on
the realignment of target function words to source
tree fragments and use a dependency parser to limit
the attachments of unaligned target words.
6 Conclusion
We have proposed an effective use of target function
words for extracting generalized transducer rules for
forest-based translation. We extend the unaligned
word approach described in (Galley et al, 2006)
from the 1-best tree to the packed parse forest. A
simple yet effective modification is that, during rule
extraction, we account for multiple interpretations
of both aligned and unaligned target function words.
That is, we chose to loose the ambiguous alignments
for all of the target function words. The consider-
ation behind is in order to generate target function
words in a robust manner. In order to avoid gener-
ating too large a derivation forest for a packed for-
est, we further used chunk-level information yielded
by a target dependency parser. Extensive experi-
ments on large-scale English-to-Japanese translation
resulted in a significant improvement in BLEU score
of 1.8 points (p < 0.01), as compared with our
implementation of a strong forest-to-string baseline
system (Mi et al, 2008; Mi and Huang, 2008).
The present work only re-aligns target function
words to source tree fragments. It will be valuable
to investigate the feasibility to re-align all the tar-
get words to source tree fragments. Also, it is in-
teresting to automatically learn a word set for re-
aligning7. Given source parse forests and a target
word set for re-aligning beforehand, we argue our
approach is generic and applicable to any language
pairs. Finally, we intend to extend the proposed
approach to tree-to-tree translation frameworks by
7This idea comes from one reviewer, we express our thank-
fulness here.
29
re-aligning subtree pairs (Liu et al, 2009; Chiang,
2010) and consistency-to-dependency frameworks
by re-aligning consistency-tree-to-dependency-tree
pairs (Mi and Liu, 2010) in order to tackle the rule-
sparseness problem.
Acknowledgments
The present study was supported in part by a Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan), by the Japanese/Chinese Machine Transla-
tion Project through Special Coordination Funds for
Promoting Science and Technology (MEXT, Japan),
and by Microsoft Research Asia Machine Transla-
tion Theme.
Wu (wu.xianchao@lab.ntt.co.jp) has
moved to NTT Communication Science Laborato-
ries and Tsujii (junichi.tsujii@live.com)
has moved to Microsoft Research Asia.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63?69. Taipei, Taiwan.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Demonstration of joshua: An open source
toolkit for parsing-based machine translation. In Pro-
ceedings of the ACL-IJCNLP 2009 Software Demon-
strations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Samuel E. Martin. 1975. A Reference Grammar of
Japanese. New Haven, Conn.: Yale University Press.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 360?368, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1433?1442, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
30
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In Proceedings of MT
Summit XI, pages 475?482, Copenhagen.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 325?334, Uppsala, Sweden, July. Association
for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceedings
of HLT-NAACL, pages 245?253.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535?542, Copen-
hagen, Denmark, September.
31
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045?1053,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Incremental Joint Approach to Word Segmentation, POS Tagging, and
Dependency Parsing in Chinese
Jun Hatori1 Takuya Matsuzaki2 Yusuke Miyao2 Jun?ichi Tsujii3
1University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan
2National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan
3Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R. China
hatori@is.s.u-tokyo.ac.jp
{takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.com
Abstract
We propose the first joint model for word segmen-
tation, POS tagging, and dependency parsing for
Chinese. Based on an extension of the incremental
joint model for POS tagging and dependency pars-
ing (Hatori et al, 2011), we propose an efficient
character-based decoding method that can combine
features from state-of-the-art segmentation, POS
tagging, and dependency parsing models. We also
describe our method to align comparable states in
the beam, and how we can combine features of dif-
ferent characteristics in our incremental framework.
In experiments using the Chinese Treebank (CTB),
we show that the accuracies of the three tasks can
be improved significantly over the baseline models,
particularly by 0.6% for POS tagging and 2.4% for
dependency parsing. We also perform comparison
experiments with the partially joint models.
1 Introduction
In processing natural languages that do not include
delimiters (e.g. spaces) between words, word seg-
mentation is the crucial first step that is necessary
to perform virtually all NLP tasks. Furthermore, the
word-level information is often augmented with the
POS tags, which, along with segmentation, form the
basic foundation of statistical NLP.
Because the tasks of word segmentation and POS
tagging have strong interactions, many studies have
been devoted to the task of joint word segmenta-
tion and POS tagging for languages such as Chi-
nese (e.g. Kruengkrai et al (2009)). This is because
some of the segmentation ambiguities cannot be re-
solved without considering the surrounding gram-
matical constructions encoded in a sequence of POS
tags. The joint approach to word segmentation and
POS tagging has been reported to improve word seg-
mentation and POS tagging accuracies by more than
1% in Chinese (Zhang and Clark, 2008). In addition,
some researchers recently proposed a joint approach
to Chinese POS tagging and dependency parsing (Li
et al, 2011; Hatori et al, 2011); particularly, Ha-
tori et al (2011) proposed an incremental approach
to this joint task, and showed that the joint approach
improves the accuracies of these two tasks.
In this context, it is natural to consider further
a question regarding the joint framework: how
strongly do the tasks of word segmentation and de-
pendency parsing interact? In the following Chinese
sentences:
S? ?sV  ?s ?Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 127?132,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Akamon: An Open Source Toolkit
for Tree/Forest-Based Statistical Machine Translation?
Xianchao Wu?, Takuya Matsuzaki?, Jun?ichi Tsujii?
? Baidu Inc.
?National Institute of Informatics
? Microsoft Research Asia
wuxianchao@gmail.com,takuya-matsuzaki@nii.ac.jp,jtsujii@microsoft.com
Abstract
We describe Akamon, an open source toolkit
for tree and forest-based statistical machine
translation (Liu et al, 2006; Mi et al, 2008;
Mi and Huang, 2008). Akamon implements
all of the algorithms required for tree/forest-
to-string decoding using tree-to-string trans-
lation rules: multiple-thread forest-based de-
coding, n-gram language model integration,
beam- and cube-pruning, k-best hypotheses
extraction, and minimum error rate training.
In terms of tree-to-string translation rule ex-
traction, the toolkit implements the tradi-
tional maximum likelihood algorithm using
PCFG trees (Galley et al, 2004) and HPSG
trees/forests (Wu et al, 2010).
1 Introduction
Syntax-based statistical machine translation (SMT)
systems have achieved promising improvements in
recent years. Depending on the type of input, the
systems are divided into two categories: string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar (Wu, 1997; Chiang, 2005; Galley et al,
2006; Shen et al, 2008), and tree/forest-based sys-
tems whose input is already a parse tree or a packed
forest to be directly converted into a target tree or
string (Ding and Palmer, 2005; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al, 2008;
Mi and Huang, 2008; Zhang et al, 2009; Wu et al,
2010; Wu et al, 2011a).
?Work done when all the authors were in The University of
Tokyo.
Depending on whether or not parsers are explic-
itly used for obtaining linguistically annotated data
during training, the systems are also divided into two
categories: formally syntax-based systems that do
not use additional parsers (Wu, 1997; Chiang, 2005;
Xiong et al, 2006), and linguistically syntax-based
systems that use PCFG parsers (Liu et al, 2006;
Huang et al, 2006; Galley et al, 2006; Mi et al,
2008; Mi and Huang, 2008; Zhang et al, 2009),
HPSG parsers (Wu et al, 2010; Wu et al, 2011a), or
dependency parsers (Ding and Palmer, 2005; Quirk
et al, 2005; Shen et al, 2008). A classification1 of
syntax-based SMT systems is shown in Table 1.
Translation rules can be extracted from aligned
string-string (Chiang, 2005), tree-tree (Ding and
Palmer, 2005) and tree/forest-string (Galley et al,
2004; Mi and Huang, 2008; Wu et al, 2011a)
data structures. Leveraging structural and linguis-
tic information from parse trees/forests, the latter
two structures are believed to be better than their
string-string counterparts in handling non-local re-
ordering, and have achieved promising translation
results. Moreover, the tree/forest-string structure is
more widely used than the tree-tree structure, pre-
sumably because using two parsers on the source
and target languages is subject to more problems
than making use of a parser on one language, such
as the shortage of high precision/recall parsers for
languages other than English, compound parse error
rates, and inconsistency of errors. In Table 1, note
that tree-to-string rules are generic and applicable
to many syntax-based models such as tree/forest-to-
1This classification is inspired by and extends the Table 1 in
(Mi and Huang, 2008).
127
Source-to-target Examples (partial) Decoding Rules Parser
tree-to-tree (Ding and Palmer, 2005) ? dep.-to-dep. DG
forest-to-tree (Liu et al, 2009a) ? ?? tree-to-tree PCFG
tree-to-string (Liu et al, 2006) ? tree-to-string PCFG
(Quirk et al, 2005) ? dep.-to-string DG
forest-to-string (Mi et al, 2008) ? ?? tree-to-string PCFG
(Wu et al, 2011a) ? ?? tree-to-string HPSG
string-to-tree (Galley et al, 2006) CKY tree-to-string PCFG
(Shen et al, 2008) CKY string-to-dep. DG
string-to-string (Chiang, 2005) CKY string-to-string none
(Xiong et al, 2006) CKY string-to-string none
Table 1: A classification of syntax-based SMT systems. Tree/forest-based and string-based systems are split by a line.
All the systems listed here are linguistically syntax-based except the last two (Chiang, 2005) and (Xiong et al, 2006),
which are formally syntax-based. DG stands for dependency (abbreviated as dep.) grammar. ? and ? denote top-down
and bottom-up traversals of a source tree/forest.
string models and string-to-tree model.
However, few tree/forest-to-string systems have
been made open source and this makes it diffi-
cult and time-consuming to testify and follow exist-
ing proposals involved in recently published papers.
The Akamon system2, written in Java and follow-
ing the tree/forest-to-string research direction, im-
plements all of the algorithms for both tree-to-string
translation rule extraction (Galley et al, 2004; Mi
and Huang, 2008; Wu et al, 2010; Wu et al, 2011a)
and tree/forest-based decoding (Liu et al, 2006; Mi
et al, 2008). We hope this system will help re-
lated researchers to catch up with the achievements
of tree/forest-based translations in the past several
years without re-implementing the systems or gen-
eral algorithms from scratch.
2 Akamon Toolkit Features
Limited by the successful parsing rate and coverage
of linguistic phrases, Akamon currently achieves
comparable translation accuracies compared with
the most frequently used SMT baseline system,
Moses (Koehn et al, 2007). Table 2 shows the auto-
matic translation accuracies (case-sensitive) of Aka-
mon and Moses. Besides BLEU and NIST score, we
further list RIBES score3, , i.e., the software imple-
mentation of Normalized Kendall?s ? as proposed by
(Isozaki et al, 2010a) to automatically evaluate the
translation between distant language pairs based on
rank correlation coefficients and significantly penal-
2Code available at https://sites.google.com/site/xianchaowu2012
3Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes
izes word order mistakes.
In this table, Akamon-Forest differs from
Akamon-Comb by using different configurations:
Akamon-Forest used only 2/3 of the total training
data (limited by the experiment environments and
time). Akamon-Comb represents the system com-
bination result by combining Akamon-Forest and
other phrase-based SMT systems, which made use
of pre-ordering methods of head finalization as de-
scribed in (Isozaki et al, 2010b) and used the total 3
million training data. The detail of the pre-ordering
approach and the combination method can be found
in (Sudoh et al, 2011) and (Duh et al, 2011).
Also, Moses (hierarchical) stands for the hi-
erarchical phrase-based SMT system and Moses
(phrase) stands for the flat phrase-based SMT sys-
tem. For intuitive comparison (note that the result
achieved by Google is only for reference and not a
comparison, since it uses a different and unknown
training data) and following (Goto et al, 2011), the
scores achieved by using the Google online transla-
tion system4 are also listed in this table.
Here is a brief description of Akamon?s main fea-
tures:
? multiple-thread forest-based decoding: Aka-
mon first loads the development (with source
and reference sentences) or test (with source
sentences only) file into memory and then per-
form parameter tuning or decoding in a paral-
lel way. The forest-based decoding algorithm
is alike that described in (Mi et al, 2008),
4http://translate.google.com/
128
Systems BLEU NIST RIBES
Google online 0.2546 6.830 0.6991
Moses (hierarchical) 0.3166 7.795 0.7200
Moses (phrase) 0.3190 7.881 0.7068
Moses (phrase)* 0.2773 6.905 0.6619
Akamon-Forest* 0.2799 7.258 0.6861
Akamon-Comb 0.3948 8.713 0.7813
Table 2: Translation accuracies of Akamon and the base-
line systems on the NTCIR-9 English-to-Japanese trans-
lation task (Wu et al, 2011b). * stands for only using
2 million parallel sentences of the total 3 million data.
Here, HPSG forests were used in Akamon.
i.e., first construct a translation forest by ap-
plying the tree-to-string translation rules to the
original parsing forest of the source sentence,
and then collect k-best hypotheses for the root
node(s) of the translation forest using Algo-
rithm 2 or Algorithm 3 as described in (Huang
and Chiang, 2005). Later, the k-best hypothe-
ses are used both for parameter tuning on addi-
tional development set(s) and for final optimal
translation result extracting.
? language models: Akamon can make use of
one or many n-gram language models trained
by using SRILM5 (Stolcke, 2002) or the Berke-
ley language model toolkit, berkeleylm-1.0b36
(Pauls and Klein, 2011). The weights of multi-
ple language models are tuned under minimum
error rate training (MERT) (Och, 2003).
? pruning: traditional beam-pruning and cube-
pruning (Chiang, 2007) techniques are incor-
porated in Akamon to make decoding feasi-
ble for large-scale rule sets. Before decoding,
we also perform the marginal probability-based
inside-outside algorithm based pruning (Mi et
al., 2008) on the original parsing forest to con-
trol the decoding time.
? MERT: Akamon has its own MERT module
which optimizes weights of the features so as
to maximize some automatic evaluation metric,
such as BLEU (Papineni et al, 2002), on a de-
velopment set.
5http://www.speech.sri.com/projects/srilm/
6http://code.google.com/p/berkeleylm/
 
 
 
e.tok 
corpus 
f.seg 
tokenize word segment 
e.tok.lw f.seg.lw 
lowercase lowercase 
clean 
e.clean f.clean 
GIZA++ 
alignment 
Rule set 
rule extraction 
SRILM 
Akamon Decoder (MERT) 
N-gram LM 
e.tok 
dev.e 
tokenize 
e.tok.lw 
lowercase 
e.forests 
Enju 
e.forests 
Enju 
dev 
f.seg 
dev.f 
word 
segmentation 
f.seg.lw 
lowercase 
pre-processing 
Figure 1: Training and tuning process of the Akamon sys-
tem. Here, e = source English language, f = target foreign
language.
? translation rule extraction: as former men-
tioned, we extract tree-to-string translation
rules for Akamon. In particular, we imple-
mented the GHKM algorithm as proposed by
Galley et al (2004) from word-aligned tree-
string pairs. In addition, we also implemented
the algorithms proposed by Mi and Huang
(2008) and Wu et al (2010) for extracting rules
from word-aligned PCFG/HPSG forest-string
pairs.
3 Training and Decoding Frameworks
Figure 1 shows the training and tuning progress of
the Akamon system. Given original bilingual par-
allel corpora, we first tokenize and lowercase the
source and target sentences (e.g., word segmentation
of Chinese and Japanese, punctuation segmentation
of English).
The pre-processed monolingual sentences will be
used by SRILM (Stolcke, 2002) or BerkeleyLM
(Pauls and Klein, 2011) to train a n-gram language
model. In addition, we filter out too long sentences
129
here, i.e., only relatively short sentence pairs will be
used to train word alignments. Then, we can use
GIZA++ (Och and Ney, 2003) and symmetric strate-
gies, such as grow-diag-final (Koehn et al, 2007),
on the tokenized parallel corpus to obtain a word-
aligned parallel corpus.
The source sentence and its packed forest, the tar-
get sentence, and the word alignment are used for
tree-to-string translation rule extraction. Since a 1-
best tree is a special case of a packed forest, we will
focus on using the term ?forest? in the continuing
discussion. Then, taking the target language model,
the rule set, and the preprocessed development set
as inputs, we perform MERT on the decoder to tune
the weights of the features.
The Akamon forest-to-string system includes the
decoding algorithm and the rule extraction algorithm
described in (Mi et al, 2008; Mi and Huang, 2008).
4 Using Deep Syntactic Structures
In Akamon, we support the usage of deep syn-
tactic structures for obtaining fine-grained transla-
tion rules as described in our former work (Wu et
al., 2010)7. Similarly, Enju8, a state-of-the-art and
freely available HPSG parser for English, can be
used to generate packed parse forests for source
sentences9. Deep syntactic structures are included
in the HPSG trees/forests, which includes a fine-
grained description of the syntactic property and a
semantic representation of the sentence. We extract
fine-grained rules from aligned HPSG forest-string
pairs and use them in the forest-to-string decoder.
The detailed algorithms can be found in (Wu et al,
2010; Wu et al, 2011a). Note that, in Akamon, we
also provide the codes for generating HPSG forests
from Enju.
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, linguis-
tic entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
7However, Akamon still support PCFG tree/forest based
translation. A special case is to yield PCFG style trees/forests
by ignoring the rich features included in the nodes of HPSG
trees/forests and only keep the POS tag and the phrasal cate-
gories.
8http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
9Until the date this paper was submitted, Enju supports gen-
erating English and Chinese forests.
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 3: Syntactic/semantic features extracted from
HPSG signs that are included in the output of Enju. Fea-
tures in phrasal nodes (top) and lexical nodes (bottom)
are listed separately.
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are composed into larger phrases by applica-
tions of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse tree/forest can be considered
as a tree/forest of signs (c.f. the HPSG forest in Fig-
ure 2 in (Wu et al, 2010)).
An HPSG parse tree/forest has two attractive
properties as a representation of a source sentence
in syntax-based SMT. First, we can carefully control
the condition of the application of a translation rule
by exploiting the fine-grained syntactic description
in the source parse tree/forest, as well as those in the
translation rules. Second, we can identify sub-trees
in a parse tree/forest that correspond to basic units
of the semantics, namely sub-trees covering a pred-
icate and its arguments, by using the semantic rep-
resentation given in the signs. Extraction of trans-
lation rules based on such semantically-connected
sub-trees is expected to give a compact and effective
set of translation rules.
A sign in the HPSG tree/forest is represented by a
typed feature structure (TFS) (Carpenter, 1992). A
TFS is a directed-acyclic graph (DAG) wherein the
edges are labeled with feature names and the nodes
130
  
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 2: Predicate argument structures for the sentences
of ?John killed Mary? and ?She ignored the fact that I
wanted to dispute?.
(feature values) are typed. In the original HPSG for-
malism, the types are defined in a hierarchy and the
DAG can have arbitrary shape (e.g., it can be of any
depth). We however use a simplified form of TFS,
for simplicity of the algorithms. In the simplified
form, a TFS is converted to a (flat) set of pairs of
feature names and their values. Table 3 lists the fea-
tures used in our system, which are a subset of those
in the original output from Enju.
In the Enju English HPSG grammar (Miyao et
al., 2003) used in our system, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 2 shows the PAS
of a simple sentence, ?John killed Mary?, and a more
complex PAS for another sentence, ?She ignored the
fact that I wanted to dispute?, which is adopted from
(Miyao et al, 2003). In an HPSG tree/forest, each
leaf node generally introduces a predicate, which
is represented by the pair of LEXENTRY (lexical
entry) feature and PRED (predicate type) feature.
The arguments of a predicate are designated by the
pointers from the ARG?x? features in a leaf node
to non-terminal nodes. Consequently, Akamon in-
cludes the algorithm for extracting compact com-
posed rules from these PASs which further lead to
a significant fast tree-to-string decoder. This is be-
cause it is not necessary to exhaustively generate the
subtrees for all the tree nodes for rule matching any
more. Limited by space, we suggest the readers to
refer to our former work (Wu et al, 2010; Wu et al,
2011a) for the experimental results, including the
training and decoding time using standard English-
to-Japanese corpora, by using deep syntactic struc-
tures.
5 Content of the Demonstration
In the demonstration, we would like to provide a
brief tutorial on:
? describing the format of the packed forest for a
source sentence,
? the training script on translation rule extraction,
? the MERT script on feature weight tuning on a
development set, and,
? the decoding script on a test set.
Based on Akamon, there are a lot of interesting
directions left to be updated in a relatively fast way
in the near future, such as:
? integrate target dependency structures, espe-
cially target dependency language models, as
proposed by Mi and Liu (2010),
? better pruning strategies for the input packed
forest before decoding,
? derivation-based combination of using other
types of translation rules in one decoder, as pro-
posed by Liu et al (2009b), and
? taking other evaluation metrics as the opti-
mal objective for MERT, such as NIST score,
RIBES score (Isozaki et al, 2010a).
Acknowledgments
We thank Yusuke Miyao and Naoaki Okazaki for
their invaluable help and the anonymous reviewers
for their comments and suggestions.
References
Bob Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge University Press.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Lingustics, 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammers. In Proceedings of ACL, pages 541?
548, Ann Arbor.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2011. Generalized
minimum bayes risk system combination. In Proceed-
ings of IJCNLP, pages 1356?1360, November.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
131
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic eval-
uation of translation quality for distant language pairs.
In Proc.of EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMT-MetricsMATR, pages 244?251, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of ACL,
pages 1433?1442, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings of
RANLP, pages 285?291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL-HLT,
pages 258?267, June.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun?ichi Tsujii. 2011. Ntt-ut statistical machine trans-
lation in ntcir-9 patentmt. In Proceedings of NTCIR-9
Workshop Meeting, pages 585?592, December.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of ACL, pages 325?334, July.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011a. Effective use of function words for rule gen-
eralization in forest-based translation. In Proceedings
of ACL-HLT, pages 22?31, June.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011b. Smt systems in the university of tokyo for
ntcir-9 patentmt. In Proceedings of NTCIR-9 Work-
shop Meeting, pages 666?672, December.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proceedings of COLING-
ACL, pages 521?528, July.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of ACL-
IJCNLP, pages 172?180, Suntec, Singapore, August.
132
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29?35
Manchester, August 2008
Parser Evaluation across Frameworks without Format Conversion
Wai Lok Tam
Interfaculty Initiative in
Information Studies
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Yo Sato
Dept of Computer Science
Queen Mary
University of London
Mile End Road
London E1 4NS, U.K.
Yusuke Miyao
Dept of Computer Science
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Jun-ichi Tsujii
Abstract
In the area of parser evaluation, formats
like GR and SD which are based on
dependencies, the simplest representation
of syntactic information, are proposed as
framework-independent metrics for parser
evaluation. The assumption behind these
proposals is that the simplicity of depen-
dencies would make conversion from syn-
tactic structures and semantic representa-
tions used in other formalisms to GR/SD a
easy job. But (Miyao et al, 2007) reports
that even conversion between these two
formats is not easy at all. Not to mention
that the 80% success rate of conversion
is not meaningful for parsers that boast
90% accuracy. In this paper, we make
an attempt at evaluation across frame-
works without format conversion. This
is achieved by generating a list of names
of phenomena with each parse. These
names of phenomena are matched against
the phenomena given in the gold stan-
dard. The number of matches found is used
for evaluating the parser that produces the
parses. The evaluation method is more ef-
fective than evaluation methods which in-
volve format conversion because the gen-
eration of names of phenomena from the
output of a parser loaded is done by a rec-
ognizer that has a 100% success rate of
recognizing a phenomenon illustrated by a
sentence. The success rate is made pos-
sible by the reuse of native codes: codes
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
used for writing the parser and rules of the
grammar loaded into the parser.
1 Introduction
The traditional evaluation method for a deep parser
is to test it against a list of sentences, each of which
is paired with a yes or no. The parser is evaluated
on the number of grammatical sentences it accepts
and that of ungrammatical sentences it rules out.
A problem with this approach to evaluation is that
it neither penalizes a parser for getting an analy-
sis wrong for a sentence nor rewards it for getting
it right. What prevents the NLP community from
working out a universally applicable reward and
penalty scheme is the absence of a gold standard
that can be used across frameworks. The correct-
ness of an analysis produced by a parser can only
be judged by matching it to the analysis produced
by linguists in syntactic structures and semantic
representations created specifically for the frame-
work on which the grammar is based. A match or
a mismatch between analyses produced by differ-
ent parsers based on different frameworks does not
lend itself for a meaningful comparison that leads
to a fair evaluation of the parsers. To evaluate two
parsers across frameworks, two kinds of methods
suggest themselves:
1. Converting an analysis given in a certain for-
mat native to one framework to another na-
tive to a differernt framework (e.g. converting
from a CCG (Steedman, 2000) derivation tree
to an HPSG (Pollard and Sag, 1994) phrase
structure tree with AVM)
2. Converting analyses given in different
framework-specific formats to some simpler
format proposed as a framework-independent
evaluation schema (e.g. converting from
29
HPSG phrase structure tree with AVM to GR
(Briscoe et al, 2006))
However, the feasibility of either solution is
questionable. Even conversion between two eval-
uation schemata which make use of the simplest
representation of syntactic information in the form
of dependencies is reported to be problematic by
(Miyao et al, 2007).
In this paper, therefore, we propose a different
method of parser evaluation that makes no attempt
at any conversion of syntactic structures and se-
mantic representations. We remove the need for
such conversion by abstracting away from com-
parison of syntactic structures and semantic rep-
resentations. The basic idea is to generate a list
of names of phenomena with each parse. These
names of phenomena are matched against the phe-
nomena given in the gold standard for the same
sentence. The number of matches found is used
for evaluating the parser that produces the parse.
2 Research Problem
Grammar formalisms differ in many aspects. In
syntax, they differ in POS label assignment, phrase
structure (if any), syntactic head assignment (if
any) and so on, while in semantics, they differ
from each other in semantic head assignment, role
assignment, number of arguments taken by pred-
icates, etc. Finding a common denominator be-
tween grammar formalisms in full and complex
representation of syntactic information and seman-
tic information has been generally considered by
the NLP community to be an unrealistic task, al-
though some serious attempts have been made re-
cently to offer simpler representation of syntactic
information (Briscoe et al, 2006; de Marneffe et
al., 2006).
Briscoe et al(2006)?s Grammatical Rela-
tion (GR) scheme is proposed as a framework-
independent metric for parsing accuracy. The
promise of GR lies actually in its dependence on
a framework that makes use of simple representa-
tion of syntactic information. The assumption be-
hind the usefulness of GR for evaluating the out-
put of parsers is that most conflicts between gram-
mar formalisms would be removed by discarding
less useful information carried by complex syn-
tactic or semantic representations used in gram-
mar formalisms during conversion to GRs. But
is this assumption true? The answer is not clear.
A GR represents syntactic information in the form
of a binary relation between a token assigned as
the head of the relation and other tokens assigned
as its dependents. Notice however that grammar
frameworks considerably disagree in the way they
assign heads and non-heads. This would raise the
doubt that, no matter how much information is re-
moved, there could still remain disagreements be-
tween grammar formalisms in what is left.
The simplicity of GR, or other dependency-
based metrics, may give the impression that con-
version from a more complex representation into
it is easier than conversion between two complex
representations. In other words, GRs or a sim-
ilar dependency relation looks like a promising
candidate for lingua franca of grammar frame-
works. However the experiment results given by
Miyao et al(2007) show that even conversion into
GRs of predicate-argument structures, which is not
much more complex than GRs, is not a trivial task.
Miyao et al(2007) manage to convert 80% of the
predicate-argument structures outputted by their
deep parser, ENJU, to GRs correctly. However the
parser, with an over 90% accuracy, is too good for
the 80% conversion rate. The lesson here is that
simplicity of a representation is a different thing
from simplicity in converting into that representa-
tion.
3 Outline of our Solution
The problem of finding a common denominator for
grammar formalisms and the problem of conver-
sion to a common denominator may be best ad-
dressed by evaluating parsers without making any
attempt to find a common denominator or conduct
any conversion. Let us describe briefly in this sec-
tion how such evaluation can be realised.
3.1 Creating the Gold Standard
The first step of our evaluation method is to con-
struct or find a number of sentences and get an an-
notator to mark each sentence for the phenomena
illustrated by each sentence. After annotating all
the sentences in a test suite, we get a list of pairs,
whose first element is a sentence ID and second is
again a list, one of the corresponding phenomena.
This list of pairs is our gold standard. To illustrate,
suppose we only get sentence 1 and sentence 2 in
our test suite.
(1) John gives a flower to Mary
(2) John gives Mary a flower
30
Sentence 1 is assigned the phenomena: proper
noun, unshifted ditransitive, preposition. Sentence
2 is assigned the phenomena: proper noun, dative-
shifted ditransitive. Our gold standard is thus the
following list of pairs:
?1, ?proper noun, unshifted ditransitive, preposition? ?,
?2, ?proper noun,dative-shifted ditransitive? ?
3.2 Phenomena Recognition
The second step of our evaluation method requires
a small program that recognises what phenomena
are illustrated by an input sentence taken from the
test suite based on the output resulted from pars-
ing the sentence. The recogniser provides a set
of conditions that assign names of phenomena to
an output, based on which the output is matched
with some framework-specific regular expressions.
It looks for hints like the rule being applied at a
node, the POS label being assigned to a node, the
phrase structure and the role assigned to a refer-
ence marker. The names of phenomena assigned
to a sentence are stored in a list. The list of phe-
nomena forms a pair with the ID of the sentence,
and running the recogniser on multiple outputs ob-
tained by batch parsing (with the parser to be eval-
uated) will produce a list of such pairs, in exactly
the same format as our gold standard. Let us illus-
trate this with a parser that:
1. assigns a monotransitive verb analysis to
?give? and an adjunct analysis to ?to Mary? in
1
2. assigns a ditransitive verb analysis to ?give? in
2
The list of pairs we obtain from running the
recogniser on the results produced by batch pars-
ing the test suite with the parser to be evaluated is
the following:
?1,?proper noun,monotransitive,preposition,adjunct??,
?2, ?proper noun,dative-shifted ditransitive? ?
3.3 Performance Measure Calculation
Comparing the two list of pairs generated from the
previous steps, we can calculate the precision and
recall of a parser using the following formulae:
Precision = (
n
?
i=1
| R
i
?A
i
|
| R
i
|
)? n (1)
Recall = (
n
?
i=1
| R
i
?A
i
|
| A
i
|
)? n (2)
where list R
i
is the list generated by the recogniser
for sentence i, list A
i
is the list produced by anno-
tators for sentence i, and n the number of sentences
in the test suite.
In our example, the parser that does a good job
with dative-shifted ditransitives but does a poor job
with unshifted ditranstives would have a precision
of:
(
2
4
+
2
2
)? 2 = 0.75
and a recall of:
(
2
3
+
2
2
)? 2 = 0.83
4 Refining our Solution
In order for the precision and recall given above to
be a fair measure, it is necessary for both the recog-
niser and the annotators to produce an exhaustive
list of the phenomena illustrated by a sentence.
But we foresee that annotation errors are likely
to be a problem of exhaustive annotation, as is re-
ported in Miyao et al(2007) for the gold standard
described in Briscoe et al(2006). Exhaustive an-
notation procedures require annotators to repeat-
edly parse a sentence in search for a number of
phenomena, which is not the way language is nor-
mally processed by humans. Forcing annotators to
do this, particularly for a long and complex sen-
tence, is a probable reason for the annotation er-
rors in the gold standard described in (Briscoe et
al., 2006).
To avoid the same problem in our creation of a
gold standard, we propose to allow non-exhaustive
annotation. In fact, our proposal is to limit the
number of phenomena assigned to a sentence to
one. This decision on which phenomenon to be as-
signed is made, when the test suite is constructed,
for each of the sentences contained in it. Follow-
ing the traditional approach, we include every sen-
tence in the test suite, along with the core phe-
nomenon we intend to test it on (Lehmann and
Oepen, 1996). Thus, Sentence 1 would be as-
signed the phenomenon of unshifted ditransitive.
Sentence 2 would be assigned the phenomenon of
31
dative-shifted ditransitive. This revision of anno-
tation policy removes the need for exhaustive an-
notation. Instead, annotators are given a new task.
They are asked to assign to each sentence the most
common error that a parser is likely to make. Thus
Sentence 1 would be assigned adjunct for such an
error. Sentence 2 would be assigned the error of
noun-noun compound. Note that these errors are
also names of phenomena.
This change in annotation policy calls for a
change in the calculation of precision and recall.
We leave the recogniser as it is, i.e. to produce an
exhaustive list of phenomena, since it is far beyond
our remit to render it intelligent enough to select a
single, intended, phenomenon. Therefore, an in-
correctly low precision would result from a mis-
match between the exhaustive list generated by the
recogniser and the singleton list produced by an-
notators for a sentence. For example, suppose we
only have sentence 2 in our test suite and the parser
correctly analyses the sentence. Our recogniser as-
signs two phenomena (proper noun, dative-shifted
ditransitive) to this sentence as before. This would
result in a precision of 0.5.
Thus we need to revise our definition of preci-
sion, but before we give our new definition, let us
define a truth function t:
t(A ? B) =
{
1 A ? B
0 A ?B = ?
t(A ?B = ?) =
{
0 A ?B 6= ?
1 A ?B = ?
Now, our new definition of precision and recall
is as follows:
Precision (3)
=
(
?
n
i=1
t(R
i
?AP
i
)+t(R
i
?AN
i
=?)
2
)
n
Recall (4)
=
(
?
n
i=1
|R
i
?AP
i
|
|AP
i
|
)
n
where list AP
i
is the list of phenomena produced
by annotators for sentence i, and list AN
i
is the list
of errors produced by annotators for sentence i.
While the change in the definition of recall is
trivial, the new definition of precision requires
some explanation. The exhaustive list of phenom-
ena generated by our recogniser for each sentence
is taken as a combination of two answers to two
questions on the two lists produced by annotators
for each sentence. The correct answer to the ques-
tion on the one-item-list of phenomenon produced
by annotators for a sentence is a superset-subset re-
lation between the list generated by our recogniser
and the one-item-list of phenomenon produced by
annotators. The correct answer to the question on
the one-item-list of error produced by annotators
for a sentence is the non-existence of any common
member between the list generated by our recog-
niser and the one-item-list of error produced by an-
notators.
To illustrate, let us try a parser that does a good
job with dative-shifted ditransitives but does a poor
job with unshifted ditranstives on both 2 and 1.
The precision of such a parser would be:
(
0
2
+
2
2
)? 2 = 0.5
and its recall would be:
(
0
1
+
1
1
)? 2 = 0.5
5 Experiment
For this abstract, we evaluate ENJU (Miyao,
2006), a released deep parser based on the HPSG
formalism and a parser based on the Dynamic Syn-
tax formalism (Kempson et al, 2001) under devel-
opment against the gold standard given in table 1.
The precision and recall of the two parsers
(ENJU and DSPD, which stands for ?Dynamic
Syntax Parser under Development?) are given in
table 3:
The experiment that we report here is intended
to be an experiment with the evaluation method de-
scribed in the last section, rather than a very seri-
ous attempt to evaluate the two parsers in question.
The sentences in table 1 are carefully selected to
include both sentences that illustrate core phenom-
ena and sentences that illustrate rarer but more in-
teresting (to linguists) phenomena. But there are
too few of them. In fact, the most important num-
ber that we have obtained from our experiment is
the 100% success rate in recognizing the phenom-
ena given in table 1.
32
ID Phenomenon Error
1 unshifted ditransi-
tive
adjunct
2 dative-shifted di-
transitive
noun-noun com-
pound
3 passive adjunct
4 nominal gerund verb that takes
verbal comple-
ment
5 verbal gerund imperative
6 preposition particle
7 particle preposition
8 adjective with ex-
trapolated senten-
tial complement
relative clause
9 inversion question
10 raising control
Figure 1: Gold Standard for Parser Evaluation
ID Sentence
1 John gives a flower to Mary
2 John give Mary a flower
3 John is dumped by Mary
4 Your walking me pleases me
5 Abandoning children increased
6 He talks to Mary
7 John makes up the story
8 It is obvious that John is a fool
9 Hardly does anyone know Mary
10 John continues to please Mary
Figure 2: Sentences Used in the Gold Standard
Measure ENJU DSPD
Precision 0.8 0.7
Recall 0.7 0.5
Figure 3: Performance of Two Parsers
6 Discussion
6.1 Recognition Rate
The 100% success rate is not as surprising as it
may look. We made use of two recognisers, one
for each parser. Each of them is written by the
one of us who is somehow involved in the devel-
opment of the parser whose output is being recog-
nised and familiar with the formalism on which the
output is based. This is a clear advantage to for-
mat conversion used in other evaluation methods,
which is usually done by someone familiar with ei-
ther the source or the target of conversion, but not
both, as such a recogniser only requires knowledge
of one formalism and one parser. For someone
who is involved in the development of the gram-
mar and of the parser that runs it, it is straight-
forward to write a recogniser that can make use
of the code built into the parser or rules included
in the grammar. We can imagine that the 100%
recognition rate would drop a little if we needed
to recognise a large number of sentences but were
not allowed sufficient time to write detailed regular
expressions. Even in such a situation, we are con-
fident that the success rate of recognition would be
higher than the conversion method.
Note that the effectiveness of our evaluation
method depends on the success rate of recognition
to the same extent that the conversion method em-
ployed in Briscoe et al (2006) and de Marneff et
al. (2006) depends on the conversion rate. Given
the high success rate of recognition, we argue that
our evaluation method is more effective than any
evaluation method which makes use of a format
claimed to be framework independent and involves
conversion of output based on a different formal-
ism to the proposed format.
6.2 Strictness of Recognition and Precision
There are some precautions regarding the use of
our evaluation method. The redefined precision 4
is affected by the strictness of the recogniser. To
illustrate, let us take Sentence 8 in Table 1 as an
example. ENJU provides the correct phrase struc-
ture analysis using the desired rules for this sen-
tence but makes some mistakes in assigning roles
to the adjective and the copular verb. The recog-
niser we write for ENJU is very strict and refuses
to assign the phenomenon ?adjective with extrap-
olated sentential complement? based on the output
given by ENJU. So ENJU gets 0 point for its an-
swer to the question on the singleton list of phe-
33
nomenon in the gold standard. But it gets 1 point
for its answer to the question on the singleton list
of error in the gold standard because it does not
go to the other extreme: a relative clause analysis,
yielding a 0.5 precision. In this case, this value is
fair for ENJU, which produces a partially correct
analysis. However, a parser that does not accept
the sentence at all, a parser that fails to produce
any output or one that erroneously produces an un-
expected phenomenon would get the same result:
for Sentence 8, such a parser would still get a pre-
cision of 0.5, simply because its output does not
show that it assigns a relative clause analysis.
We can however rectify this situation. For the
lack of parse output, we can add an exception
clause to make the parser automatically get a 0 pre-
cision (for that sentence). Parsers that make unex-
pected mistakes are more problematic. An obvi-
ous solution to deal with these parsers is to come
up with an exhaustive list of mistakes but this is an
unrealistic task. For the moment, a temporary but
realistic solution would be to expand the list of er-
rors assigned to each sentence in the gold standard
and ask annotators to make more intelligent guess
of the mistakes that can be made by parsers by con-
sidering factors such as similarities in phrase struc-
tures or the sharing of sub-trees.
6.3 Combining Evaluation Methods
For all measures, some distortion is unavoidable
when applied to exceptional cases. This is true for
the classical precision and recall, and our redefined
precision and recall is no exception. In the case of
the classical precision and recall, the distortion is
countered by the inverse relation between them so
that even if one is distorted, we can tell from the
other that how well (poorly) the object of evalua-
tion performs. Our redefined precision and recall
works pretty much the same way.
What motivates us to derive measures so closely
related to the classical precision and recall is the
ease to combine the redefined precision and recall
obtained from our evaluation method with the clas-
sical precision and recall obtained from other eval-
uation methods, so as to obtain a full picture of
the performance of the object of evaluation. For
example, our redefined precision and recall figures
given in Table 3 (or figures obtained from running
the same experiment on a larger test set) for ENJU
can be combined with the precision and recall fig-
ures given in Miyao et al (2006) for ENJU, which
is based on a evaluation method that compares its
predicate-argument structures those given in Penn
Treebank. Here the precision and recall figures are
calculated by assigning an equal weight to every
sentence in Section 23 of Penn Treebank. This
means that different weights are assigned to dif-
ferent phenomena depending on their frequency in
the Penn Treebank. Such assignment of weights
may not be desirable for linguists or developers
of NLP systems who are targeting a corpus with a
very different distribution of phenomena from this
particular section of the Penn Treebank. For exam-
ple, a linguist may wish to assign an equal weight
across phenomena or more weights to ?interesting?
phenomena. A developer of a question-answering
system may wish to give more weights to question-
related phenomena than other phenomena of less
interest which are nevertheless attested more fre-
quently in the Penn Treebank.
In sum, the classical precision and recall fig-
ures calculated by assigning equal weight to ev-
ery sentence could be considered skewed from the
perspective of phenomena, whereas our redefined
precision and recall figures may be seen as skewed
from the frequency perspective. Frequency is rela-
tive to domains: less common phenomena in some
domains could occur more often in others. Our re-
defined precision and recall are not only useful for
those who want a performance measure skewed the
way they want, but also useful for those who want
a performance measure as ?unskewed? as possible.
This may be obtained by combining our redefined
precision and recall with the classical precision
and recall yielded from other evaluation methods.
7 Conclusion
We have presented a parser evaluation method
that addresses the problem of conversion between
frameworks by totally removing the need for that
kind of conversion. We do some conversion but
it is a different sort. We convert the output of a
parser to a list of names of phenomena by drawing
only on the framework that the parser is based on.
It may be inevitable for some loss or inaccuracy
to occur during this kind of intra-framework con-
version if we try our method on a much larger test
set with a much larger variety of longer sentences.
But we are confident that the loss would still be
far less than any inter-framework conversion work
done in other proposals of cross-framework evalu-
ation methods. What we believe to be a more prob-
34
lematic area is the annotation methods we have
suggested. At the time we write this paper based
on a small-scale experiment, we get slightly bet-
ter result by asking our annotator to give one phe-
nomenon and one common mistake for each sen-
tence. This may be attributed to the fact that he
is a member of the NLP community and hence he
gets the knowledge to identify the core phenom-
ena we want to test and the common error that
parsers tend to make. If we expand our test set
and includes longer sentences, annotators would
make more mistakes whether they attempt exhaus-
tive annotation or non-exhaustive annotation. It
is difficult to tell whether exhaustive annotation
or non-exhaustive annotation would be better for
large scale experiments. As future work, we intend
to try our evaluation method on more test data to
determine which one is better and find ways to im-
prove the one we believe to be better for large scale
evaluation.
References
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL 2006.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Kempson, Ruth, Wilfried Meyer-Viol, and Dov Gab-
bay. 2001. Dynamic Syntax: The Flow of Language
Understanding. Blackwell.
Lehmann, Sabine and Stephan Oepen. 1996. TSNLP
test suites for natural language processing. In Pro-
ceedings of COLING 1996.
Miyao, Yusuke, Kenji Sagae, and Junichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of GEAF 2007.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Steedman, Mark. 2000. Syntactic Process. MIT Press.
35
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 123?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Deep Re-annotation in a Chinese Scientific Treebank  
Kun Yu1 Xiangli Wang1 Yusuke Miyao2 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {kunyu, xiangli, matuzaki, tsujii}@is.s.u-tokyo.ac.jp 2. National Institute of Informatics, Hitotsubashi 2-1-2, Chiyoda-ku, Tokyo, 101-8430, Japan yusuke@nii.ac.jp 3. The University of Manchester, Oxford Road, Manchester, M13 9PL, UK  
Abstract 
In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific tree-bank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 1 Introduction A Chinese scientific Treebank (called the NICT Chinese Treebank) has been developed by the National Institute of Information and Communi-cations Technology of Japan (NICT). This tree-bank annotates the word segmentation, pos-tags, and bracketing structures according to the anno-tation guideline of the Penn Chinese Treebank (Xia, 2000(a); Xia, 2000(b); Xue and Xia, 2000). Contrary to the Penn Chinese Treebank in news domain, the NICT Chinese Treebank includes sentences that are manually translated from Japanese scientific papers. Currently, the NICT Chinese Treebank includes around 8,000 Chinese sentences. The annotation of more sen-tences in the science domain is ongoing.  The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep informa-tion, which includes both the grammatical func-tional tags and the traces, are omitted in the an-notation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies.  Gabbard et al (2006) and Blaheta and Charniak (2000) applied machine learning mod-els to automatically assign the empty categories and functional tags to an English treebank.  
However, considering about the different do-mains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chi-nese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applica-tions, such as the lexicalized grammar develop-ment from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002).  Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chi-nese Treebank, we will continue this re-annotation work on large-scale sentences.  2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annota-tion criteria in the Penn Chinese Treebank.  Figure 1 exemplifies our re-annotation to a sentence in the NICT Chinese Treebank. In this example, we first re-annotate the trace (as indi-cated by the italicized part in Figure 1(b)) for the extracted head noun ??/word?. Furthermore, we re-annotate the functional tag of the trace (as indicated by the dashed-box in Figure 1(b)), to indicate that the extracted head noun should be restored into the relative clause as a topic. There are 26 functional tags in the Penn Chi-nese Treebank (Xue and Xia, 2000), in which seven functional tags describe the grammatical 
123
roles and one functional tag (i.e. LGS) indicates a logical subject. Since the eight functional tags are crucial for obtaining the grammatical func-tion of constituents, we re-annotate the eight functional tags (refer to Table 1) to the NICT Chinese Treebank. (NP (CP (IP (NP (NN ??)                             (NN ???))                               (VP (VA ?)))                                  (DEC ?))                           (NP (NN ?))) (the word of which the word cohesion is high) (a) A relative clause in the NICT Chinese Treebank          (NP (CP (WHNP-1 (-NONE- *OP*)      (CP (IP (NP-TPC (-NONE- *T*-1))                   (NP (NN ??)                           (NN ???))                   (VP (VA ?)))                                (DEC ?)))                (NP (NN ?))) (b) The relative clause after re-annotation Figure 1.  Our re-annotation to a relative clause. Functional Tag Description IO indirect object OBJ direct object EXT post-verbal complement that describes the extent, frequency, or quantity FOC object fronted to a pre-verbal but post-subject position PRD non-verbal predicate SBJ surface subject TPC topic LGS logical subject Table 1. Functional tags that we re-annotate.                 (IP (NP-TPC-1 (NN ??))                 (VP (ADVP (AD ??))                        (VP (ADVP (AD ??))                               (VP (VV ??)                                      (NP-OBJ (-NONE- *T*-1))))))                            (It is easier to obtain information.) (a) A topic construction with long-distance dependency after re-annotation of functional tag and trace          (IP (NP-TPC (DP (DT ?))                                (NP (NN ??)))                (NP-SBJ (NP (PN ?))                               (NP (NN ???)))                (VP (ADVP (AD ?))                        (VP (VV ??)                               (VV ??))))                (The rationality of this algorithm has been verified.)  (b) A topic construction without long-distance dependency after re-annotation of functional tag Figure 2. Our re-annotation to topic constructions. In addition, in the annotation guideline of the Penn Chinese Treebank, four constructions are annotated with traces: BA-construction, BEI-construction, topic construction and relative clause. The BEI-construction and relative 
clause introduce long-distance dependency. Therefore, we re-annotate the traces for the two constructions. The topic construction introduces the topic phrase. For the topic constructions that contain long-distance dependency, we re-annotate both the traces and the functional tags (refer to the italicized part in Figure 2(a)). Some topic constructions, however, do not include long-distance dependency. In such cases, we only re-annotate the functional tag to indicate that it is a topic (refer to the italicized part in Figure 2(b)). In addition, the BA-construction moves the object to a pre-verbal position. Al-though the BA-construction does not contain long-distance dependency, we still re-annotate the trace to acquire the original position of the moved object in the sentence. 3 Issues and Solutions 3.1 Trace re-annotation in the BA/BEI construction The NICT Chinese Treebank follows the word segmentation and pos-tag annotation guideline of the Penn Chinese Treebank. Therefore, there are some BA-constructions and BEI-constructions that cannot be re-annotated with traces. The principle reason for this is that the moved object has semantic relations with only part of the verb. For example, in the sentence shown in Figure 3(a), the moved head noun ???/hometown? is the object of ??/construct?, but not for ???/construct to be?.  (VP (BA ?)                (IP (NP (NN ??))                      (VP (VV ??)                             (NP (NN ??))))) (construct the hometown to be a garden) (a) The annotation in the NICT Chinese Treebank  (VP (BA ?)                            (IP (NP-SBJ-1 (NN ??))                                  (VP (VV ?)                                         (NP-OBJ (-NONE- *-1))                                         (AM ?)                                         (NP (NN ??))))) (b) Our proposed re-annotation of functional tag and trace Figure 3.  Our re-annotation to a BA construction with split verb. Our analysis of the Penn Chinese Treebank shows that only a closed list of characters (such as ??/to be?) can be attached to verbs in such a case. Therefore, we solve the problem by fol-lowing four steps (for an example, refer to Fig-ure 3(b)): 
124
(1) A linguist manually collects the characters that can be attached to verbs in such a case from the Penn Chinese Treebank and assigns them a new pos-tag ?AM (argument marker)?.  (2) The annotators use the character list as a reference during the re-annotation. When the verb in a BA/BEI construction ends with a char-acter in the list, and the annotators think the verb should be split, the annotators record the sentence ID without performing any re-annotation.  (3) The linguist collects all of the recorded sentences, and defines pattern rules to automati-cally split the verbs in the BA/BEI construc-tions. (4) The annotators annotate trace for the sen-tences with the split verbs. This step will be fin-ished in our future work. 3.2 Topic detection In the annotation guideline of the Penn Chinese Treebank, a topic is defined as ?the element that appears before the subject in a declarative sen-tence?. However, the NICT Chinese Treebank does not annotate the omitted subject. Therefore, we could not use the position of the subject as a criterion for topic detection.  In order to resolve this issue, we define some heuristic rules based on both the meaning and the bracketing structure of phrases, to help de-tect the topic phrase. Only the phrase that satis-fies all the rules will be re-annotated as a topic. The following exemplifies some rules: (1) If there is a phrase before a subject, the phrase is probably a topic. (2) A topic phrase must be parallel to the fol-lowing verb phrase. (3) The preposition phrase and localization phrase describing the location or time are not topics. 3.3 Inconsistent annotation in the NICT Chinese Treebank There are some inconsistent annotations in the NICT Chinese Treebank, which makes our re-annotation work difficult.  These inconsistencies include: (1) Inconsistent word segmentation, such as segmenting the word ???? /corresponding? into two words ???/opposite? and ??/ought?. (2) Inconsistent pos-tag annotation. For ex-ample, when the word  ???  exists between two noun phrases, it should be tagged as an associa-tive marker (i.e. DEG), according to the guide-
line of the Penn Chinese Treebank. However, in the NICT Chinese Treebank, sometimes it is tagged as a nominalizer (i.e. DEC).  (3) Inconsistent bracketing annotation. Fig-ure 4(a) shows the annotation of a relative clause in the NICT Chinese Treebank. In this annotation, the noun phrase ???/Osaka ??/subway? is incorrectly treated as the extracted head; furthermore, the adverb ???/by hand? that modifies the verb ???/make? is incor-rectly annotated as an adjective that modifies the noun ????/deformation graph?. After cor-recting these inconsistencies, the relative clause should be annotated as shown in Figure 4(b). (NP (QP (CD ??))              (ADJP (JJ ??))              (DNP (NP (CP (IP (VP (VV ??)))                                       (DEC ?))                                (NP (NR ??)                                       (NN ??)))                         (DEG ?))              (NP (NN ???))) (many deformation graphs of Osaka subway that are made by hand)  (a) The inconsistent annotation of a relative clause (NP (QP (CD ??))        (NP (CP (IP (VP (ADVP (AD ??))                                     (VP (VV ??))))                       (DEC ?))                (NP (DNP (NP (NR ??)                                         (NN ??))                                  (DEG ?))                       (NP (NN ???)))))  (b) The annotation after correcting the inconsistencies Figure 4. An inconsistent annotation in the NICT Chinese Treebank and its correction. In our re-annotation, these inconsistently an-notated sentences in the NICT Chinese Tree-bank were recorded by the annotators. We then sent them back to NICT for further verification. 4 Process of Re-annotation 4.1 Annotation Guideline  During the re-annotation, we basically follow the annotation guideline of the Penn Chinese Treebank (Xue and Xia, 2000). However, in order to fit with the characteristics of scientific sentences in the NICT Chinese Treebank, some constraints are added to the guideline.  For example, in the science domain, the rela-tive clause is often used to describe a phenome-non, in which the extracted head noun is usually an abstract noun, and the relative clause is an appositive of the extracted head noun. Figure 5 shows an example in which the relative clause ???/system ??/stop ??/working? is a de-
125
scription of the extracted head noun ???/phenomenon?. In such a case, the head noun cannot be restored into the clause. Therefore, we add the following restriction in our re-annotation guideline: Do not re-annotate the trace when the head noun of a relative clause is an abstract noun and it is an appositive of the relative clause.         (NP (CP (IP (NP (NN ??))                              (VP (VV ??)                                      (NP (NN ??))))                        (DEC ?))                 (NP (NN ??))) (the phenomenon that the system stops working) Figure 5. A relative clause in the NICT Chinese Treebank. 4.2 Quality Control Several processes were undertaken to guarantee the quality of our re-annotation:  (1) We chose graduate students who major in Chinese for all of the annotators.  (2) A visualization tool - XConc Suite (Kim et al, 2008) was used as assistance during the re-annotation.  (3) Only 2,363 sentences with good transla-tion quality in the NICT Chinese Treebank were chosen for re-annotation in the current phase.   (4) Before starting the re-annotation, a lin-guist selected 200 representative sentences, which contain all the linguistic phenomena that we want to re-annotate, from among the 2,363 sentences in the NICT Chinese Treebank. The selected 200 sentences were manually re-annotated by the linguist, and were split into two sets for training the annotators sequentially. We evaluated the annotation quality of the anno-tators during training. The average annotation quality of all the annotators after training is shown in Table 2. Annotation Quality Inter-annotator Consistency Precision Recall Precision Recall 70.71% 70.75% 61.59% 61.59% Table 2. The average annotation quality of the annotators after training.      (5) After training, the remaining sentences were split into several parts and assigned to the annotators for re-annotation. In each part, there were around 20% sentences that were shared by all of the annotators. These shared sentences were used to check and guarantee inter-annotator consistency during the re-annotation.  5 Conclusion and Future Work  We re-annotated the deep information, which includes eight types of grammatical functional 
tags and the traces in four constructions, to a Chinese scientific treebank, i.e. the NICT Chi-nese Treebank. Since the NICT Chinese Tree-bank is based on manually translated sentences, only 2,363 sentences with good translation qual-ity were re-annotated in the current phase to guarantee the re-annotation quality.  In the future, we will finish the trace annota-tion for the BA and BEI constructions with split verbs. Furthermore, we will continue our re-annotation on more sentences in the NICT Chi-nese Treebank. Acknowledgments We would like to thank Dr. Kiyotaka Uchimoto and Dr. Junichi Kazama for providing the NICT Chinese Treebank. References  Don Blaheta and Eugene Charniak. 2000. Assigning Func-tion Tags to Parsed Text. Proceedings of NAACL 2000. Ryan Gabbard, Seth Kulick and Mitchell Marcus. 2006. Fully Parsing the Penn Treebank. Proceedings of HLT-NAACL 2006. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. Thesis. Dublin City University. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. Jindong Kim, Tomoko Ohta, and Junichi Tsujii. 2008. Corpus Annotation for Mining Biomedical Events from Literature. BMC Bioinformatics, 9(10).  Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D Thesis. The University of Tokyo. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Fei Xia. 2000 (a). The Segmentation Guidelines for the Penn Chinese Treebank (3.0). Fei Xia. 2000 (b). The Part-of-speech Tagging Guidelines for the Penn Chinese Treebank (3.0). Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-Scale Annotated Chinese Corpus. Proceedings of COLING 2002. Nianwen Xue and Fei Xia. 2000. The Bracketing Guide-lines for the Penn Chinese Treebank. Shiwen Yu et al 2002. The Basic Processing of Contempo-rary Chinese Corpus at Peking University Specification. Journal of Chinese Information Processing, 16 (5). Qiang Zhou. 2004. Annotation Scheme for Chinese Tree-bank. Journal of Chinese Information Processing, 18 (4). 
126
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 19?27,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Event Extraction for Post-Translational Modifications
Tomoko Ohta? Sampo Pyysalo? Makoto Miwa? Jin-Dong Kim? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,smp,mmiwa,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We consider the task of automatically
extracting post-translational modification
events from biomedical scientific publica-
tions. Building on the success of event
extraction for phosphorylation events in
the BioNLP?09 shared task, we extend the
event annotation approach to four major
new post-transitional modification event
types. We present a new targeted corpus of
157 PubMed abstracts annotated for over
1000 proteins and 400 post-translational
modification events identifying the modi-
fied proteins and sites. Experiments with
a state-of-the-art event extraction system
show that the events can be extracted with
52% precision and 36% recall (42% F-
score), suggesting remaining challenges
in the extraction of the events. The an-
notated corpus is freely available in the
BioNLP?09 shared task format at the GE-
NIA project homepage.1
1 Introduction
Post-translational-modifications (PTM), amino
acid modifications of proteins after translation, are
one of the posterior processes of protein biosyn-
thesis for many proteins, and they are critical
for determining protein function such as its ac-
tivity state, localization, turnover and interac-
tions with other biomolecules (Mann and Jensen,
2003). Since PTM alter the properties of a pro-
tein by attaching one or more biochemical func-
tional groups to amino acids, understanding of
the mechanism and effects of PTM are a major
goal in the recent molecular biology, biomedicine
and pharmacology fields. In particular, epige-
netic (?outside conventional genetics?) regulation
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
of gene expression has a crucial role in these fields
and PTM-like modifications of biomolecules are a
burning issue. For instance, tissue specific or con-
text dependent expression of many proteins is now
known to be controlled by specific PTM of his-
tone proteins, such as Methylation and Acetylation
(Jaenisch and Bird, 2003). This Methylation and
Acetylation of specific amino acid residues in his-
tone proteins are strongly implicated in unwinding
the nucleosomes and exposing genes to transcrip-
tion, replication and DNA repairing machinery.
The recent BioNLP?09 Shared Task on Event
Extraction (Kim et al, 2009a) (below, BioNLP
shared task) represented the first community-wide
step toward the extraction of fine-grained event
representations of information from biomolecular
domain publications (Ananiadou et al, 2010). The
nine event types targeted in the task included one
PTM type, Phosphorylation, whose extraction in-
volved identifying the modified protein and, when
stated, the specific phosphorylated site. The re-
sults of the shared task showed this PTM event to
be single most reliably extracted event type in the
data, with the best-performing system for the event
type achieving 91% precision and 76% recall
(83% F-score) in the extraction of phosphorylation
events (Buyko et al, 2009). The results suggest
both that the event representation is well applica-
ble to PTM and that current extraction methods are
capable of reliable PTM extraction. Most of the
proposed state-of-the-art methods for event extrac-
tion are further largely machine-learning based.
This suggest that the coverage of many existing
methods could be straightforwardly extended to
new event types and domains by extending the
scope of available PTM annotations and retrain-
ing the methods on newly annotated data. In this
study, we take such an annotation-based approach
to extend the extraction capabilities of state of the
art event extraction methods for PTM.
19
Term Count
Phosphorylation 172875 50.90%
Methylation 49780 14.66%
Glycosylation 36407 10.72%
Hydroxylation 20141 5.93%
Acetylation 18726 5.51%
Esterification 7836 2.31%
Ubiquitination 6747 1.99%
ADP-ribosylation 5259 1.55%
Biotinylation 4369 1.29%
Sulfation 3722 1.10%
. . .
TOTAL 339646 100%
Table 1: PTM mentions in PubMed. The number
of citations returned by the PubMed search engine
for each PTM term shown together with the frac-
tion of the total returned for all searches. Searches
were performed with the terms as shown, allow-
ing MeSH term expansion and other optimizations
provided by the Entrez search.
2 Corpus Annotation
We next discuss the selection of the annotated
PTM types and source texts and present the rep-
resentation and criteria used in annotation.
2.1 Event Types
A central challenge in the automatic extraction
of PTMs following the relatively data-intensive
BioNLP shared task model is the sheer number
of different modifications: the number of known
PTM types is as high as 300 and constantly grow-
ing (Witze et al, 2007). Clearly, the creation of
a manually annotated resource with even mod-
est coverage of statements of each of the types
would be a formidable undertaking. We next
present an analysis of PTM statement occurrences
in PubMed as the first step toward resolving this
challenge.
We estimated the frequency of mentions of
prominent PTM types by combining MeSH
ontology2 PTM terms with terms occurring
in the post-translational protein
modification branch of the Gene Ontology
(The Gene Ontology Consortium, 2000). After
removing variants (e.g. polyamination for amina-
tion or dephosphorylation for phosphorylation)
and two cases judged likely to occur frequently
2http://www.nlm.nih.gov/mesh/meshhome.
html
in non-PTM contexts (hydration and oxidation),
we searched PubMed for the remaining 31 PTM
types. The results for the most frequent types
are shown in Table 1. We find a power-law
- like distribution with phosphorylation alone
accounting for over 50% of the total, and the top
6 types together for over 90%. By contrast, the
bottom ten types together represent less than a
percent of total occurrences.
This result implies that fair coverage of individ-
ual PTM event mentions can be achieved without
considering even dozens of different PTM event
types, let alne hundreds. Thus, as a step toward
extending the coverage of event extraction systems
for PTM, we chose to focus limited resources on
annotating a small selection of types so that a num-
ber of annotations sufficient for supervised learn-
ing and stable evaluation can be provided. To
maximize the utility of the created annotation, the
types were selected based on their frequency of oc-
currence.
2.2 Text Selection
Biomedical domain corpora are frequently anno-
tated from selections of texts chosen as a sample
of publications in a particular subdomain of inter-
est. While several areas in present-day molecu-
lar biology are likely to provide ample source data
for PTM statements, a sample of articles from any
subdomain is unlikely to provide a well-balanced
distribution of event types: for example, the most
frequent PTM event type annotated in the GENIA
event corpus occurs more than 10 times as often
as the second most frequent (Kim et al, 2008).
Further, avoiding explicit subdomain restrictions
is not alone sufficient to assure a balanced distri-
bution of event types: in the BioInfer corpus, for
which sentences were selected on the basis of their
containing mentions of protein pairs known to in-
teract, the most frequent PTM type is again anno-
tated nearly four times as often as the second most
frequent (Pyysalo et al, 2007).
To focus annotation efforts on texts relevant to
PTM and to guarantee that the annotation results
in relatively balanced numbers of PTM events of
each targeted type, we decided to annotate a tar-
geted set of source texts instead of a random sam-
ple of texts for a particular subdomain. This type
of targeted annotation involves a risk of introduc-
ing bias: a badly performed selection could pro-
duce a corpus that is not representative of the
20
PTM type AB FT
Acetylation 103 128
Glycosylation 226 336
Methylation 72 69
Phosphorylation 186 76
Hydroxylation 71 133
Table 2: Number of abstracts (AB) and full-text ar-
ticles (FT) tagged in PIR as containing PTM state-
ments.
statements expressing PTMs in text and thus poor
material for either meaningful evaluation or for
training methods with good generalization perfor-
mance.3 To avoid such bias, we decided to base
our selection of the source texts on an indepen-
dently annotated PTM resource with biological (as
opposed to textual) criteria for inclusion. Owing
in part to the recent interest in PTMs, there are
currently a wealth of resources providing different
levels of annotation for PTMs.
Here, we have chosen to base initial annotation
on corpora provided by the Protein Information
Resource4 (PIR) (Wu et al, 2003). These corpora
contain annotation for spans with evidence for five
different PTM types (Table 2), corresponding to
the five PTMs found above to occur in PubMed
with the highest frequency. A key feature setting
this resource apart from others we are aware of is
that it provides text-bound annotations identifying
the statement by which a PTM record was made in
the context of the full publication abstracts. While
this annotation is less specific and detailed than
the full BioNLP shared task markup, it could both
serve as an initial seed for annotation and assure
that the annotation agrees with relevant database
curation criteria. The PIR corpora have also been
applied in previous PTM extraction studies (e.g.
(Hu et al, 2005; Narayanaswamy et al, 2005)).
We judged that the annotated Phosphorylation
events in the BioNLP shared task data provide
sufficient coverage for the extraction of this PTM
type, and chose to focus on producing annota-
tion for the four other PTM types in the PIR data.
As the high extraction performance for phospho-
rylation events in the BioNLP shared task was
3One could easily gather PTM-rich texts by performing
protein name tagging and searching for known patterns such
as ?[PROTEIN] methylates [PROTEIN]?, but a corpus cre-
ated in this way would not necessarily provide significant
novelty over the original search patterns.
4http://pir.georgetown.edu
Protein Site PTM Count
collagen lysine Hydroxylate 44
myelin arginine Methylate 17
M protein N-terminal Glycosylate 2
EF-Tu lysine Methylate 1
Actobindin NH2 terminus Acetylate 0
Table 3: Example queried triples and match counts
from Medie.
achieved with annotated training data containing
215 PTM events, in view of the available resources
we set as an initial goal the annotation of 100
events of each of the four PTM types. To assure
that the annotated resource can be made publicly
available, we chose to use only the part of the PIR
annotations that identified sections of PubMed ab-
stracts, excluding full-text references and non-
PubMed abstracts. Together with the elimination
of duplicates and entries judged to fall outside of
the event annotation criteria (see Section 2.4), this
reduced the number of source texts below our tar-
get, necessitating a further selection strategy.
For further annotation, we aimed to select ab-
stracts that contain specific PTM statements iden-
tifying both the name of a modified protein and the
modified site. As for the initial selection, we fur-
ther wished to avoid limiting the search by search-
ing for any specific PTM expressions. To imple-
ment this selection, we used the Medie system5
(Ohta et al, 2006; Miyao et al, 2006) to search
PubMed for sentences where a specific protein and
a known modified site were found together in a
sentence occurring in an abstract annotated with a
specific MeSH term. The (protein name, modified
site, MeSH term) triples were extracted from PIR
records, substituting the appropriate MeSH term
for each PTM type. Some examples with the num-
ber of matching documents are shown in Table 3.
As most queries returned either no documents or a
small number of hits, we gave priority to responses
to queries that returned a small number of docu-
ments to avoid biasing the corpus toward proteins
whose modifications are frequently discussed.
We note that while the PIR annotations typically
identified focused text spans considerably shorter
than a single sentence and sentence-level search
was used in the Medie-based search to increase the
likelihood of identifying relevant statements, after
selection all annotation was performed to full ab-
stracts.
5http://www-tsujii.is.s.u-tokyo.ac.jp/
medie/
21
Event type Count
Protein modification 38
Phosphorylation 546
Dephosphorylation 28
Acetylation 7
Deacetylation 1
Ubiquitination 6
Deubiquitination 0
Table 4: GENIA PTM-related event types and
number of events in the GENIA event corpus.
Type names are simplified: the full form of e.g.
the Phosphorylation type in the GENIA event on-
tology is Protein amino acid phosphorylation.
Event type Arguments Count
Protein modification Theme 31
Phosphorylation Theme 261
Phosphorylation Theme, Site 230
Phosphorylation Site 20
Phosphorylation Theme, Cause 14
Dephosphorylation Theme 16
Table 5: GENIA PTM-related event arguments.
Only argument combinations appearing more than
10 times in the corpus shown.
2.3 Representation
The employed event representation can capture
the association of varying numbers of participants
in different roles. To apply an event extraction
approach to PTM, we must first define the tar-
geted representation, specifying the event types,
the mandatory and optional arguments, and the ar-
gument types ? the roles that the participants play
in the events. In the following, we discuss alterna-
tives and present the representation applied in this
work.
The GENIA Event ontology, applied in the
annotation of the GENIA Event corpus (Kim
et al, 2008) that served as the basis of the
BioNLP shared task data, defines a general Pro-
tein modification event type and six more specific
modification subtypes, shown in Table 4. While
the existing Acetylation type could thus be applied
together with the generic Protein modification
type to capture all the annotated PTMs, we be-
lieve that identification of the specific PTM type
is not only important to users of extracted PTM
events but also a relatively modest additional bur-
den for automatic extraction, owing to the unam-
biguous nature of typical expressions used to state
Figure 1: Alternative representations for PTM
statements including a catalyst in GENIA Event
corpus. PTM events can be annotated with a di-
rect Cause argument (top, PMID 9374467) or us-
ing an additional Regulation event (middle, PMID
10074432). The latter annotation can be applied
also in cases where there is no expression directly
?triggering? the secondary event (bottom, PMID
7613138).
PTMs in text. We thus chose to introduce three
additional specific modification types, Glycosyla-
tion, Hydroxylation and Methylation for use in the
annotation.
The GENIA Event corpus annotation allows
PTM events to take Theme, Site and Cause argu-
ments specifying the event participants, where the
Theme identifies the entity undergoing the mod-
ification, Site the specific region being modified,
and Cause an entity or event leading to the modi-
fication. Table 5 shows frequent argument combi-
nations appearing in the annotated data. We note
that while Theme is specified in the great majority
of events and Site in almost half, Cause is anno-
tated for less than 5% of the events. However, the
relative sparsity of Cause arguments in modifica-
tion events does not imply that e.g. catalysts of the
events are stated only very rarely, but instead re-
flects also the use of an alternative representation
for capturing such statements without a Cause ar-
gument for the PTM event. The GENIA event an-
notation specifies a Regulation event (with Posi-
tive regulation and Negative regulation subtypes),
used to annotate not only regulation in the biolog-
ical sense but also statements of general causality
between events: Regulation events are used gen-
erally to connect entities or events stated to other
events that they are stated to cause. Thus, PTM
22
events with a stated cause (e.g. a catalyst) can be
alternatively represented with a Cause argument
on the PTM event or using a separate Regulation
event (Figure 1). The interpretation of these event
structures is identical, and from an annotation per-
spective there are advantages to both. However,
for the purpose of automatic extraction it is impor-
tant to establish a consistent representation, and
thus only one should be used.
In this work, we follow the latter representation,
disallowing Cause arguments for annotated PTM
events and applying separate Regulation events
to capture e.g. catalyst associations. This choice
has the benefits of providing an uniform repre-
sentation for catalysis and inhibition (one involv-
ing a Positive regulation and the other a Nega-
tive regulation event), reducing the sparseness of
specific event structures in the data, and matching
the representation chosen in the BioNLP shared
task, thus maintaining compatibility with exist-
ing event extraction methods. Finally, we note
that while we initially expected that glycosylation
statements might frequently identify specific at-
tached side chains, necessitating the introduction
of an additional argument type to accurately cap-
ture all the stated information regarding Glycosy-
lation events, the data contained too few examples
for either training material or to justify the mod-
ification of the event model. We adopt the con-
straints applied in the BioNLP shared task regard-
ing the entity types allowed as specific arguments.
Thus, the representation we apply here annotated
PTM events with specific types, taking as Theme
argument a gene/gene product type entity and as
Site argument a physical (non-event) entity that
does not need to be assigned a specific type.
2.4 Annotation criteria
To create PTM annotation compatible with the
event extraction systems introduced for the
BioNLP shared task, we created annotation fol-
lowing the GENIA Event corpus annotation cri-
teria (Kim et al, 2008), as adapted for the shared
task. The criteria specify that annotation should be
applied to statements that involve the occurrence
of a change in the state of an entity ? even if stated
as having occurred in the past, or only hypotheti-
cally ? but not in cases merely discussing the state
or properties of entities, even if these can serve as
the basis for inference that a specific change has
occurred. We found that many of the spans an-
notated in PIR as evidence for PTM did not ful-
fill the criteria for event annotation. The most fre-
quent class consisted of cases where the only evi-
dence for a PTM was in the form of a sequence of
residues, for example
Characterization [. . . ] gave the follow-
ing sequence, Gly-Cys-Hyp-D-Trp-Glu-
Pro-Trp-Cys-NH2 where Hyp = 4-trans-
hydroxyproline. (PMID 8910408)
Here, the occurrence of hydroxyproline in the se-
quence implies that the protein has been hydrox-
ylated, but as the hydroxylation event is only im-
plied by the protein state, no event is annotated.
Candidates drawn from PIR but not fulfilling
the criteria were excluded from annotation. While
this implies that the general class of event extrac-
tion approaches considered here will not recover
all statements providing evidence of PTM to bi-
ologists (per the PIR criteria), several factors mit-
igate this limitation of their utility. First, while
PTMs implied by sequence only are relatively fre-
quent in PIR, its selection criteria give emphasis
to publications initially reporting the existence of a
PTM, and further publications discussing the PTM
are not expected to state it as sequence only. Thus,
it should be possible to extract the correspond-
ing PTMs from later sources. Similarly, one of
the promises of event extraction approaches is the
potential to extract associations of multiple enti-
ties and extract causal chains connecting events
with others (e.g. E catalyzes the hydroxylation of
P, leading to . . . ), and the data indicates that the
sequence-only statements typically provide little
information on the biological context of the modi-
fication beyond identifying the entity and site. As
such non-contextual PTM information is already
available in multiple databases, this class of state-
ments may not be of primary interest for event ex-
traction.
2.5 Annotation results
The new PTM annotation covers 157 PubMed
abstracts. Following the model of the BioNLP
shared task, all mentions of specific gene or gene
product names in the abstracts were annotated, ap-
plying the annotation criteria of (Ohta et al, 2009).
This new named entity annotation covers 1031
gene/gene product mentions, thus averaging more
than six mentions per annotated abstract. In to-
tal, 422 events of which 405 are of the novel PTM
23
Event type Count
Glycosylation 122
Hydroxylation 103
Methylation 90
Acetylation 90
Positive reg. 12
Phosphorylation 3
Protein modification 2
TOTAL 422
Table 6: Statistics of the introduced event annota-
tion.
Arguments Count
Theme, Site 363
Theme 36
Site 6
Table 7: Statistics for the arguments of the anno-
tated PTM events.
types were annotated, matching the initial annota-
tion target in number and giving a well-balanced
distribution of the specific PTM types (Table 6).
Reflecting the selection of the source texts, the
argument structures of the annotated PTM events
(Table 7) show a different distribution from those
annotated in the GENIA event corpus (Table 5):
whereas less than half of the GENIA event corpus
PTM events include a Site argument, almost 90%
of the PTM events in the new data include a Site.
PTM events identifying both the modified protein
and the specific modified site are expected to be
of more practical interest. However, we note that
the greater number of multi-argument events is ex-
pected to make the dataset more challenging as an
extraction target.
3 Evaluation
To estimate the capacity of the newly annotated
resource to support the extraction of the targeted
PTM events and the performance of current event
extraction methods at open-domain PTM extrac-
tion, we performed a set of experiments using an
event extraction method competitive with the state
of the art, as established in the BioNLP shared task
on event extraction (Kim et al, 2009a; Bjo?rne et
al., 2009).
3.1 Methods
We adopted the recently introduced event extrac-
tion system of Miwa et al (2010). The system
applies a pipeline architecture consisting of three
supervised classification-based modules: a trig-
ger detector, an event edge detector, and an event
detector. In evaluation on the BioNLP shared
task test data, the system extracted phosphory-
lation events at 75.7% precision and 85.2% re-
call (80.1% F-score) for Task 1, and 75.7% preci-
sion and 83.3% recall (79.3% F-score) for Task 2,
showing performance comparable to the best re-
sults reported in the literature for this event class
(Buyko et al, 2009). We assume three precondi-
tions for the PTM extraction: proteins are given,
all PTMs have Sites, and all arguments in a PTM
co-occur in sentence scope. The first of these is
per the BioNLP shared task setup, the second fixed
based the corpus statistics, and the third a property
intrinsic to the extraction method, which builds on
analysis of sentence structure.6 In the experiments
reported here, only the four novel PTM event types
with Sites in the corpus are regarded as a target for
the extraction.
The system extracted PTMs as follows: the
trigger detector detected the entities (triggers and
sites) of the PTMs, the event edge detector de-
tected the edges in the PTMs, and the event de-
tector detected the PTMs. The evaluation setting
was the same as the evaluation in (Miwa et al,
2010) except for the threshold. The thresholds in
the three modules were tuned with the develop-
ment data set.
Performance evaluation is performed using the
BioNLP shared task primary evaluation criteria,
termed the ?Approximate Span Matching? crite-
rion. This criterion relaxes the requirements of
strict matching in accepting extracted event trig-
gers and entities as correct if their span is inside
the region of the corresponding region in the gold
standard annotation.
3.2 Data Preparation
The corpus data was split into training and test sets
on the document level with a sampling strategy
that aimed to preserve a roughly 3:1 ratio of oc-
currences of each event type between training and
test data. The test data was held out during sys-
tem development and parameter selection and only
applied in a single final experiment. The event ex-
traction system was trained using the 112 abstracts
of the training set, further using 24 of the abstracts
6We note that in the BioNLP shared task data, all argu-
ments were contained within single sentences for 95% of
events.
24
Figure 2: Performance of PTM extraction on the
development data set.
Event type Prec Rec F
Acetylation 69.6% 36.7% 48.1%
Methylation 50.0% 34.2% 40.6%
Glycosylation 36.7% 42.5% 39.4%
Hydroxylation 57.1% 29.3% 38.7%
Overall 52.1% 35.7% 42.4%
Table 8: Event extraction results on the test set.
as a development test set.
3.3 Results
We first performed parameter selection, setting the
machine learning method parameter by estimating
performance on the development data set. Figure 2
shows the performance of PTM extraction on the
development data set with different values of pa-
rameter. The threshold value corresponding to the
best performance (0.3) was then applied for an ex-
periment on the held-out test set.
Performance on the test set was evaluated as
52% precision and 36% recall (42% F-score),
matching estimates on the development data. A
breakdown by event type (Table 8) shows that
Acetylation is most reliably extracted with extrac-
tion for the other three PTM types showing sim-
ilar F-scores despite some variance in the preci-
sion/recall balance. We note that while these re-
sults fall notably below the best result reported
for Phosphorylation events in the BioNLP shared
task, they are comparable to the best results re-
ported in the task for Regulation and Binding
events (Kim et al, 2009a), suggesting that the
dataset alows the extraction of the novel PTM
events with Theme and Site arguments at levels
comparable to multi-argument shared task events.
Figure 3: Learning curve of PTM extraction on the
development data set.
Further, a learning curve (Figure 3) plotted on
the development data suggests roughly linearly
increasing performance over most of the curve.
While the increase appears to be leveling off to
an extent when using all of the available data, the
learning curve indicates that performance can be
further improved by increasing the size of the an-
notated dataset.
4 Discussion
Post-translational modifications have been a fo-
cus of interest in the biomedical text mining com-
munity, and a number of resources and systems
targeting PTM have been proposed. The GE-
NIES and GeneWays systems (Friedman et al,
2001; Rzhetsky et al, 2004) targeted PTM events
such as phosphorylation and dephosphorylation
under the more general createbond and breakbond
types. Hu et al (2005) introduce the RLIMS-P
rule-based system for mining the substrates and
sites for phosphorylation, which is extended with
the capacity to extract intra-clausal statements by
Narayanaswamy et al (2005). Saric et al (2006)
present an extension of their rule-based STRING-
IE system for extracting regulatory networks to
capture phosphorylation and dephosphorylation
events. Lee et al (2008) present E3Miner, a tool
for automatically extracting information related to
ubiquitination, and Kim et al (2009b) present a
preliminary study adapting the E3Miner approach
to the mining of acetylation events.
It should be noted that while studies target-
ing single specific PTM types report better re-
sults than found in the initial evaluation presented
here (in many cases dramatically so), different
25
extraction targets and evaluation criteria compli-
cate direct comparison. Perhaps more importantly,
our aim here is to extend the capabilities of gen-
eral event extraction systems targeting multiple
types of structured events. Pursuing this broader
goal necessarily involves some compromise in the
ability to focus on the extraction of individual
event types, and it is expected that highly focused
systems will provide better performance than re-
trained general systems.
The approach to PTM extraction adopted here
relies extensively on the availability of annotated
resources, the creation of which requires consider-
able effort and expertise in understanding the tar-
get domain as well as the annotation methodology
and tools. The annotation created in this study,
performed largely on the basis of partial existing
annotations drawn from PIR data, involved an es-
timated three weeks of full-time effort from an ex-
perienced annotator. As experiments further in-
dicated that a larger corpus may be necessary for
reliable annotation, we can estimate that extending
the approach to sufficient coverage of each of hun-
dreds of PTM types without a partial initial anno-
tation would easily require several person-years of
annotation efforts. We thus see a clear need for the
development of unsupervised or semisupervised
methods for PTM extraction to extend the cover-
age of event extraction systems to the full scale of
different PTM types. Nevertheless, even if reliable
methods for PTM extraction that entirely avoid the
need for annotated training data become available,
a manually curated reference standard will still be
necessary for reliable estimation of their perfor-
mance. To efficiently support the development of
event extraction systems capable of capturing the
full variety of PTM events, it may be beneficial to
reverse the approach taken here: instead of anno-
tating hundreds of examples of a small number of
PTM types, annotate a small number of each of
hundreds of PTM types, thus providing both seed
data for semisupervised approaches as well as ref-
erence data for the evaluation of broad-coverage
PTM event extraction systems.
5 Conclusions and Future Work
We have presented an event extraction approach
to automatic PTM recognition, building on the
model introduced in the BioNLP shared task on
event extraction. By annotating a targeted cor-
pus for four prominent PTM types not considered
in the BioNLP shared task data, we have created
a resource that can be straightforwardly used to
extend the capability of event extraction systems
for PTM extraction. We estimated that while sys-
tems trained on the original shared task dataset
could not recognize more than 50% of PTM men-
tions due to their types, the introduced annotation
increases this theoretical upper bound to nearly
90%. An initial experiment on the newly intro-
duced dataset using a state-of-the-art method indi-
cated that straightforward adoption of the dataset
as training data to extend coverage of PTM events
without specific adaptations of the method is feasi-
ble, although the measured performance indicates
remaining challenges for reliable extraction. Fur-
ther, while the experiments were performed on a
dataset selected to avoid bias toward e.g. a partic-
ular subdomain or specific forms of event expres-
sions, it remains an open question how extraction
performance generalizes to biomedical literature
beyond the selected sample. As experiments in-
dicated clear remaining potential for the improve-
ment of extraction performance from more train-
ing data, the extension of the annotated dataset is
a natural direction for future work. We considered
also the possiblity of extending annotation to cover
small numbers of each of a large variety of PTM
types, which would place focus on the challenges
of event extraction with little or no training data
for specific event types.
The annotated corpus covering over 1000 gene
and gene product entities and over 400 events is
freely available in the widely adopted BioNLP
shared task format at the GENIA project home-
page.7
Acknowledgments
We would like to thank Goran Topic for automat-
ing Medie queries to identify target abstracts.
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japan-Slovenia Research Cooperative
Program (JSPS, Japan and MHEST, Slovenia).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
7http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
26
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 19?27, Boulder, Colorado, June. Association
for Computational Linguistics.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: A natural-language processing system for the
extraction of molecular pathways from journal arti-
cles. Bioinformatics, 17(Suppl. 1):S74?S82.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
Rudolf Jaenisch and Adrian Bird. 2003. Epigenetic
regulation of gene expression: how the genome in-
tegrates intrinsic and environmental signals. Nature
Genetics, 33:245?254.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009a. Overview
of bionlp?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Youngrae Kim, Hodong Lee, and Gwan-Su Yi. 2009b.
Literature mining for protein acetylation. In Pro-
ceedings of LBM?09.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein
ligases. Nucl. Acids Res., 36(suppl.2):W416?422.
Matthias Mann and Ole N. Jensen. 2003. Proteomic
analysis of post-translational modifications. Nature
Biotechnology, 21:255?261.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131?146, February.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya, and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proceedings of
COLING-ACL 2006, pages 1017?1024.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2005. Beyond the clause: extraction
of phosphorylation information from medline ab-
stracts. Bioinformatics, 21(suppl.1):i319?327.
Tomoko Ohta, Yusuke Miyao, Takashi Ninomiya,
Yoshimasa Tsuruoka, Akane Yakushiji, Katsuya
Masuda, Jumpei Takeuchi, Kazuhiro Yoshida, Ta-
dayoshi Hara, Jin-Dong Kim, Yuka Tateisi, and
Jun?ichi Tsujii. 2006. An Intelligent Search Engine
and GUI-based Efficient MEDLINE Search Tool
Based on Deep Syntactic Parsing. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, pages 17?20.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Jasmin Saric, Lars Juhl Jensen, Rossitza Ouzounova,
Isabel Rojas, and Peer Bork. 2006. Extraction
of regulatory gene/protein networks from Medline.
Bioinformatics, 22(6):645?650.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25:25?29.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
27
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 28?36,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Scaling up Biomedical Event Extraction to the Entire PubMed
Jari Bjo?rne?, ,1,2 Filip Ginter,?,1 Sampo Pyysalo,?,3 Jun?ichi Tsujii,3,4 Tapio Salakoski1,2
1Department of Information Technology, University of Turku, Turku, Finland
2Turku Centre for Computer Science (TUCS), Turku, Finland
3Department of Computer Science, University of Tokyo, Tokyo, Japan
4National Centre for Text Mining, University of Manchester, Manchester, UK
jari.bjorne@utu.fi,ginter@cs.utu.fi,smp@is.s.u-tokyo.ac.jp
tsujii@is.s.u-tokyo.ac.jp,tapio.salakoski@it.utu.fi
Abstract
We present the first full-scale event extrac-
tion experiment covering the titles and ab-
stracts of all PubMed citations. Extraction
is performed using a pipeline composed
of state-of-the-art methods: the BANNER
named entity recognizer, the McClosky-
Charniak domain-adapted parser, and the
Turku Event Extraction System. We an-
alyze the statistical properties of the re-
sulting dataset and present evaluations of
the core event extraction as well as nega-
tion and speculation detection components
of the system. Further, we study in de-
tail the set of extracted events relevant
to the apoptosis pathway to gain insight
into the biological relevance of the result.
The dataset, consisting of 19.2 million oc-
currences of 4.5 million unique events,
is freely available for use in research at
http://bionlp.utu.fi/.
1 Introduction
There has recently been substantial interest in
event models in biomedical information extraction
(IE). The expressive event representation captures
extracted knowledge as structured, recursively
nested, typed associations of arbitrarily many par-
ticipants in specific roles. The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), the
first large scale evaluation of biomedical event
extraction systems, drew the participation of 24
groups and established a standard event represen-
tation scheme and datasets. The training and test
data of the Shared Task comprised 13,623 manu-
ally annotated events in 1,210 PubMed citation ab-
stracts, and on this data the top performing system
of Bjo?rne et al (2009; 2010b) achieved an overall
F-score of 51.95% (Kim et al, 2009).
?Equal contribution by first three authors.
The issue of the scalability and generalization
ability of the introduced event extraction systems
beyond the domain of the GENIA corpus on which
the Shared Task was based has remained largely
an open question. In a prior study, we have es-
tablished on a 1% random sample of PubMed ti-
tles and abstracts that the event extraction system
of Bjo?rne et al is able to scale up to PubMed-
wide extraction without prohibitive computational
time requirements, however, the actual extraction
from the entire PubMed was left as a future work
(Bjo?rne et al, 2010a). Thus, the top-ranking event
extraction systems in the Shared Task have, in fact,
not been used so far for actual mass-scale event ex-
traction beyond the carefully controlled setting of
the Shared Task itself. Further, since an automated
named entity recognition step was not part of the
Shared Task, the interaction of the event extrac-
tion systems with gene/protein name recognizers
remains largely unexplored as well.
In this study, we address some of these ques-
tions by performing a mass-scale event extraction
experiment using the best performing system1 of
the Shared Task (Bjo?rne et al, 2009; Bjo?rne et al,
2010b), and applying it to the entire set of titles
and abstracts of the nearly 18 million citations in
the 2009 distribution of PubMed. The extraction
result, containing 19.2 million event occurrences,
is the largest dataset of its type by several orders
of magnitude and arguably represents the state-of-
the-art in automatic event extraction with respect
to both accuracy and size.
To support emerging community efforts in tasks
that build on event extraction output, such as event
network refinement, hypothesis generation, path-
way extraction, and others, we make the entire
resulting dataset freely available for research pur-
poses. This allows researchers interested in ques-
tions involving text mining, rather than initial in-
1Available at http://bionlp.utu.fi/
28
Event type Example
Gene expression 5-LOX is expressed in leukocytes
Transcription promoter associated with IL-4 gene
transcription
Localization phosphorylation and nuclear translo-
cation of STAT6
Protein catabolism I kappa B-alpha proteolysis by
phosphorylation.
Phosphorylation BCL-2 was phosphorylated at the
G(2)/M phase
Binding Bcl-w forms complexes with Bax and
Bak
Regulation c-Met expression is regulated by Mitf
Positive regulation IL-12 induced STAT4 binding
Negative regulation DN-Rac suppressed NFAT activation
Table 1: Targeted event types with brief example
statements expressing an event of each type. In the
examples, the word or words marked as triggering
the presence of the event are shown in italics and
event participants underlined. The event types are
grouped by event participants, with the first five
types taking one theme, binding events taking mul-
tiple themes and the regulation types theme and
cause participants. Adapted from (Bjo?rne et al,
2009).
formation extraction, to make use of the many fa-
vorable statistical properties of the massive dataset
without having to execute the laborious and time-
consuming event extraction pipeline.
In the following, we describe the Shared Task
event representation applied throughout this study,
the event extraction pipeline itself, and a first set
of analyzes of multiple aspects of the resulting
dataset.
2 Event extraction
The event extraction pipeline follows the model of
the BioNLP?09 Shared Task in its representation
of extracted information. The primary extraction
targets are gene or gene product-related entities
and nine fundamental biomolecular event types in-
volving these entities (see Table 1 for illustration).
Several aspects of the event representation, as
defined in the context of the Shared Task, differ-
entiate the event extraction task from the body of
domain IE studies targeting e.g. protein?protein
interactions and gene?disease relations, including
previous domain shared tasks (Ne?dellec, 2005;
Krallinger et al, 2008). Events can have an ar-
bitrary number of participants with specified roles
(e.g. theme or cause), making it possible to cap-
ture n-ary associations and statements where some
participants occur in varying roles or are only oc-
Regulation
NNP NN VB NNP CC .conj_and>
<nn dobj><nsubj NNP
ProteinSTAT3 Phosphorylationphosphorylation Regulationinvolve ProteinVav and ProteinRac-1 .
Cause>
Cause><Theme
event detectionC
B
A
dobj>
named entity recognition
ProteinSTAT3 phosphorylation involve ProteinVav and ProteinRac-1 .
STAT3 phosphorylation involve Vav and Rac-1 .Ser(727) mayNNP
appos> <auxMD
Ser(727) may
Ser(727) mayEntity
<Theme<Site <Theme
Regulation
ProteinSTAT3 Phosphorylationphosphorylation involve ProteinVav and ProteinRac-1 .
Cause>
Cause><Theme
speculation and negation detectionD
Ser(727) mayEntity
<Theme<Site <Theme
RegulationSpec
Spec
STAT3 phosphorylation involve Vav and Rac-1 .Ser(727) may
parsing
Figure 1: Event extraction. A multi-stage sys-
tem produces an event graph for each sentence.
Named entities are detected (A) using BANNER.
Independently of named entity detection, sen-
tences are parsed (B) to produce a dependency
parse. Event detection (C) uses the named entities
and the parse in predicting the trigger nodes and
argument edges that form the events. Finally, po-
larity and certainty (D) are predicted for the gen-
erated events. Adapted from (Bjo?rne et al, 2009).
casionally mentioned. A further important prop-
erty is that event participants can be other events,
resulting in expressive, recursively nested struc-
tures. Finally, events are given GENIA Event on-
tology types drawn from the community-standard
Gene Ontology (The Gene Ontology Consortium,
2000), giving each event well-defined semantics.
2.1 Event Extraction Pipeline
The event extraction pipeline applied in this work
consists of three main processing steps: named en-
tity recognition, syntactic parsing, and event ex-
traction. The process is illustrated in Figure 1.
For named entity recognition, we use the BAN-
NER system of Leaman and Gonzales (2008),
which in its current release achieves results close
to the best published on the standard GENETAG
dataset and was reported to have the best perfor-
mance in a recent study comparing publicly avail-
able taggers (Kabiljo et al, 2009). Titles and ab-
stracts of all 17.8M citations in the 2009 distribu-
tion of PubMed are processed through the BAN-
NER system.
Titles and abstracts of PubMed citations in
which at least one named entity was identified, and
29
which therefore contain a possible target for event
extraction, are subsequently split into sentences
using a maximum-entropy based sentence splitter
trained on the GENIA corpus (Kazama and Tsujii,
2003) with limited rule-based post-processing for
some common errors.
All sentences containing at least one named
entity are then parsed with the domain-adapted
McClosky-Charniak parser (McClosky and Char-
niak, 2008; McClosky, 2009), which has achieved
the currently best published performance on the
GENIA Treebank (Tateisi et al, 2005). The con-
stituency parse trees are then transformed to the
collapsed-ccprocessed variant of the Stanford De-
pendency scheme using the conversion tool2 intro-
duced by de Marneffe et al (2006).
Finally, events are extracted using the Turku
Event Extraction System of Bjo?rne et al which
achieved the best performance in the BioNLP?09
Shared Task and remains fully competitive with
even the most recent advances (Miwa et al, 2010).
We use a recent publicly available revision of the
event extraction system that performs also extrac-
tion of Shared Task subtask 2 and 3 information,
providing additional event arguments relevant to
event sites and localization (site, atLoc, and toLoc
role types in the Shared Task) as well as informa-
tion on event polarity and certainty (Bjo?rne et al,
2010b).
2.2 Extraction result and computational
requirements
Named entity recognition using the BANNER sys-
tem required in total roughly 1,800 CPU-hours
and resulted in 36,454,930 named entities identi-
fied in 5,394,350 distinct PubMed citations.
Parsing all 20,037,896 sentences with at least
one named entity using the McClosky-Charniak
parser and transforming the resulting constituency
trees into dependency analyzes using the Stanford
conversion tool required about 5,000 CPU-hours,
thus averaging 0.9 sec per sentence. Even though
various stability and scalability related problems
were met during the parsing process, we were able
to successfully parse 20,020,266 (99.91%) of all
sentences.
Finally, the event extraction step required ap-
proximately 1,500 CPU-hours and resulted in
19,180,827 event instances. In total, the entire cor-
2http://www-nlp.stanford.edu/
downloads/lex-parser.shtml
pus of PubMed titles and abstracts was thus pro-
cessed in roughly 8,300 CPU-hours, or, 346 CPU-
days, the most time-consuming step by far being
the syntactic parsing.
We note that, even though the components used
in the pipeline are largely well-documented and
mature, a number of technical issues directly re-
lated to, or at least magnified by, the untypi-
cally large dataset were met at every point of the
pipeline. Executing the pipeline was thus far from
a trivial undertaking. Due to the computational re-
quirements of the pipeline, cluster computing sys-
tems were employed at every stage of the process.
2.3 Evaluation
We have previously evaluated the Turku Event
Extraction System on a random 1% sample of
PubMed citations, estimating a precision of 64%
for event types and arguments pertaining to sub-
task 1 of the Shared Task (Bjo?rne et al, 2010a),
which compares favorably to the 58% precision
the system achieves on the Shared Task dataset it-
self (Bjo?rne et al, 2009).
To determine precision on subtasks 2 and 3
on PubMed citations, we manually evaluate 100
events with site and location arguments (sub-
task 2) and 100 each of events predicted to be
speculated or negated (subtask 3).
Subtask 2 site and location arguments are
mostly external to the events they pertain to and
therefore were evaluated independently of their
parent event. Their precision is 53% (53/100),
comparable to the 58% precision established on
the BioNLP?09 Shared Task development set, us-
ing the same parent-independent criterion.
To estimate the precision of the negation detec-
tion (subtask 3), we randomly select 100 events
predicted to be negated. Of these, 9 were incor-
rect as events to such an extent that the correct-
ness of the predicted negation could not be judged
and, among the remaining 91 events, the negation
was correctly predicted in 82% of the cases. Sim-
ilarly, to estimate the precision of speculation de-
tection, we randomly select 100 events predicted
to be speculated, of which 20 could not be judged
for correctness of speculation. Among the remain-
ing 80, 88% were correctly predicted as specula-
tive events. The negations were mostly signalled
by explicit statements such as is not regulated, and
speculation by statements, such as was studied,
that defined the events as experimental questions.
30
For comparison, on the BioNLP?09 Shared Task
development set, for correctly predicted events,
precision for negation examples was 83% (with
recall of 53%) and for speculation examples 77%
(with recall of 51%).
In the rest of this paper, we turn our attention to
the extraction result.
3 Term-NE mapping
As the event types are drawn from the Gene On-
tology and the original data on which the system
is trained has been annotated with reference to the
GO definitions, the events targeted by the extrac-
tion system have well-defined biological interpre-
tations. The meaning of complete event struc-
tures depends also on the participating entities,
which are in the primary event extraction task con-
strained to be of gene/gene product (GGP) types,
as annotated in the GENIA GGP corpus (Ohta et
al., 2009a). The simple and uniform nature of
these entities makes the interpretation of complete
events straightforward.
However, the semantics of the entities au-
tomatically tagged in this work are somewhat
more openly defined. The BANNER system was
trained on the GENETAG corpus, annotated for
?gene/protein entities? without differentiating be-
tween different entity types and marking entities
under a broad definition that not only includes
genes and gene products but also related entities
such as gene promoters and protein complexes,
only requiring that the tagged entities be specific
(Tanabe et al, 2005). The annotation criteria of
the entities used to train the BANNER system as
well as the event extraction system also differ in
the extent of the marked spans, with GENIA GGP
marking the minimal name and GENETAG allow-
ing also the inclusion of head nouns when a name
occurs in modifier position. Thus, for example, the
latter may annotate the spans p53 gene, p53 pro-
tein, p53 promoter and p53 mutations in contexts
where the former would in each case mark only
the substring p53.
One promising future direction for the present
effort is to refine the automatically extracted data
into an event network connected to specific entries
in gene/protein databases such as Entrez Gene and
UniProt. To achieve this goal, the resolution of
the tagged entities can be seen to involve two re-
lated but separate challenges. First, identifying
the specific database entries that are referred to
Relation Examples
Equivalent GGP gene, wild-type GGP
Class-Subclass human GGP, HIV-1 GGP
Object-Variant
GGP-Isoform GGP isoform
GGP-Mutant dominant-negative GGP
GGP-Recombinant GGP expression plasmid
GGP-Precursor GGP precursor, pro-GGP
Component-Object
GGP-Amino acid GGP-Ile 729
GGP-AA motif GGP NH2-terminal
GGP-Reg. element GGP proximal promoter
GGP-Flanking region GGP 5? upstream sequence
Object-Component
GGP-Protein Complex GGP homodimers
Place-Area
GGP-Locus GGP loci
Member-Collection
GGP-Group GGP family members
Table 2: Gene/gene product NE-term relation
types with examples. Top-level relations in the re-
lation type hierarchy shown in bold, specific NE
names in examples replaced with GGP. Intermedi-
ate levels in the hierarchy and a number of minor
relations omitted. Relation types judged to allow
remapping (see text) underlined.
by the genes/proteins named in the tagged enti-
ties, and second, mapping from the events involv-
ing automatically extracted terms to ones involv-
ing the associated genes/proteins. The first chal-
lenge, gene/protein name normalization, is a well-
studied task in biomedical NLP for which a num-
ber of systems with promising performance have
been proposed (Morgan and Hirschman, 2007).
The second we believe to be novel. In the follow-
ing, we propose a method for resolving this task.
We base the decision on how to map events ref-
erencing broadly defined terms to ones referencing
associated gene/protein names in part on a recently
introduced dataset of ?static relations? (Pyysalo et
al., 2009) between named entities and terms (Ohta
et al, 2009b). This dataset was created based on
approximately 10,000 cases where GGP NEs, as
annotated in the GENIA GGP corpus (Ohta et al,
2009a), were embedded in terms, as annotated in
the GENIA term corpus (Ohta et al, 2002). For
each such case, the relation between the NE and
the term was annotated using a set of introduced
relation types whose granularity was defined with
reference to MeSH terms (see Table 2, Ohta et al,
2009b). From this data, we extracted prefix and
suffix strings that, when affixed to a GGP name,
produced a term with a predictable relation (within
the dataset) to the GGP. Thus, for example, the
31
term GGP
p53 protein p53
p53 gene p53
human serum albumin serum albumin
wild-type p53 p53
c-fos mRNA c-fos
endothelial NO synthase NO synthase
MHC cl. II molecules MHC cl. II
human insulin insulin
HIV-1 rev.transcriptase rev.transcriptase
hepatic lipase lipase
p24 antigen p24
tr. factor NF-kappaB NF-kappaB
MHC molecules MHC
PKC isoforms PKC
HLA alleles HLA
RET proto-oncogene RET
ras oncogene ras
SV40 DNA SV40
EGFR tyrosine kinase EGFR
Table 3: Examples of frequently applied map-
pings. Most frequent term for each mapping is
shown. Some mention strings are abbreviated for
space.
Mentions Types
Total 36454930 4747770
Mapped 2212357 (6.07%) 547920 (11.54%)
Prefix 430737 (1.18%) 129536 (2.73%)
Suffix 1838646 (5.04%) 445531 (9.38%)
Table 4: Statistics for applied term-GGP map-
pings. Tagged mentions and types (unique men-
tions) shown separately. Overall total given for
reference, for mappings overall for any mapping
shown and further broken down into prefix-string
and suffix-string based.
prefix string ?wild-type? was associated with the
Equivalent relation type and the suffix string ?ac-
tivation sequence? with the GGP-Regulatory ele-
ment type. After filtering out candidates shorter
than 3 characters as unreliable (based on prelim-
inary experiments), this procedure produced a set
of 68 prefix and 291 suffix strings.
To make use of the data for predicting relations
between GGP names and the terms formed by af-
fixing a prefix or suffix string, it is necessary to
first identify name-term pairs. Candidates can be
generated simply by determining the prefix/suffix
strings occurring in each automatically tagged en-
tity and assuming that what remains after remov-
ing the prefixes and suffixes is a GGP name. How-
ever, this naive strategy often fails: while remov-
ing ?protein? from ?p53 protein? correctly identi-
fies ?p53? as the equivalent GGP name, for ?cap-
sid protein? the result, ?capsid? refers not to a
GGP but to the shell of a virus ? ?protein? is prop-
erly part of the protein name. To resolve this is-
sue, we drew on the statistics of the automatically
tagged entities, assuming that if a prefix/suffix
string is not a fixed part of a name, the name will
appear tagged also without that string. As the tag-
ging covers the entire PubMed, this is likely to
hold for all but the very rarest GGP names. To
compensate for spurious hits introduced by tag-
ging errors, we specifically required that to accept
a candidate prefix/suffix string-name pair, the can-
didate name should occur more frequently without
the prefix/suffix than with it. As the dataset is very
large, this simple heuristic often gives the right de-
cision with secure margins: for example, ?p53?
was tagged 117,835 times but ?p53 protein? only
11,677, while ?capsid? was (erroneously) tagged
7 times and ?capsid protein? tagged 1939 times.
A final element of the method is the definition
of a mapping to events referencing GGP NEs from
the given events referencing terms, the NEs con-
tained in the terms, and the NE-term relations. In
this work, we apply independently for each term a
simple mapping based only on the relation types,
deciding for each type whether replacing refer-
ence to a term with reference to a GGP holding
the given relation to the term preserves event se-
mantics (to an acceptable approximation) or not.
For the Equivalent relation this holds by defini-
tion. We additionally judged all Class-Subclass
and Component-Object relations to allow remap-
ping (accepting e.g. P1 binds part of P2 ? P1
binds P2) as well as selected Object-Variant rela-
tions (see Table 2). For cases judged not to allow
remapping, we simply left the event unmodified.
Examples of frequently applied term-GGP map-
pings are shown in Table 3, and Table 4 shows
the statistics of the applied mappings. We find
that suffix-based mappings apply much more fre-
quently than prefix-based, perhaps reflecting also
the properties of the source dataset. Overall, the
number of unique tagged types is reduced by over
10% by this procedure. It should be noted that the
applicability of the method could likely be consid-
erably extended by further annotation of NE-term
relations in the dataset of Ohta et al (2009b): the
current data is all drawn from the GENIA corpus,
drawn from the subdomain of transcription factors
in human blood cells, and its coverage of PubMed
is thus far from exhaustive.
32
4 Event recurrence
Given a dataset of events extracted from the en-
tire PubMed, we can study whether, and to what
extent, events are re-stated in multiple PubMed ci-
tations. This analysis may shed some light ? nat-
urally within the constraints of an automatically
extracted dataset rather than gold-standard anno-
tation ? on the often (informally) discussed hy-
pothesis that a high-precision, low recall system
might be a preferred choice for large-scale extrac-
tion as the lower recall would be compensated by
the redundancy of event statements in PubMed.
In order to establish event recurrence statistics,
that is, the number of times a given event is re-
peated in the corpus, we perform a limited normal-
ization of tagged entities consisting of the Term-
NE mapping presented in Section 3 followed
by lowercasing and removal of non-alphanumeric
characters. Two named entities are then consid-
ered equal if their normalized string representa-
tions are equal. For instance, the two names IL-
2 gene and IL2 would share the same normalized
form il2 and would thus be considered equal.
For the purpose of recurrence statistics, two
events are considered equal if their types are equal,
and all their Theme and Cause arguments, which
can be other events, are recursively equal as well.
A canonical order of arguments is used in the com-
parison, thus e.g. the following events are consid-
ered equal:
regulation(Cause:A, Theme:binding(Theme:B, Theme:C))
regulation(Theme:binding(Theme:C, Theme:B), Cause:A)
In total, the system extracted 19,180,827 instances
of 4,501,883 unique events. On average, an
event is thus stated 4.2 times. The distribution
is, however, far from uniform and exhibits the
?long tail? typical of natural language phenom-
ena, with 3,484,550 (77%) of events being single-
ton occurrences. On the other hand, the most fre-
quent event, localization(Theme:insulin), occurs
as many as 59,821 times. The histogram of the
number of unique events with respect to their oc-
currence count is shown in Figure 2.
The total event count consists mostly of sim-
ple one-argument events. The arguably more
interesting category of events that involve at
least two different named entities constitutes
2,064,278 instances (11% of the 19.2M total)
of 1,565,881 unique events (35% of the 4.5M
total). Among these complex events, recur-
 10
 100
1K
10K
100K
1M
 1  10  100 1K 10KUn
iqu
e e
ven
ts w
ith 
giv
en 
occ
urre
nce
 co
unt
Event occurrence count
Figure 2: Number of unique events (y-axis) with a
given occurrence count (x-axis).
R P N L B E T C H
R 561 173 128 42 63 83 30 16 17
P 173 1227 192 58 99 143 39 20 23
N 128 192 668 46 73 98 31 17 18
L 42 58 46 147 57 75 25 15 15
B 63 99 73 57 1023 134 35 20 21
E 83 143 98 75 134 705 49 22 24
T 30 39 31 25 35 49 79 11 11
C 16 20 17 15 20 22 11 39 7
H 17 23 18 15 21 24 11 7 49
Table 5: Event type confusion matrix. Each el-
ement contains the number of unique events, in
thousands, that are equal except for their type.
The matrix is symmetric and its diagonal sums to
4,5M, the total number of extracted unique events.
The event types are (R)egulation, (P)ositive
regulation, (N)egative regulation, (L)ocalization,
(B)inding, gene (E)xpression, (T)ranscription,
protein (C)atabolism, and p(H)osphorylation.
rence is thus considerably lower, an event be-
ing stated on average 1.3 times. The most
frequent complex event, with 699 occurrences,
is positive-regulation(Cause:GnRG,Theme:local-
ization(Theme:LH)), reflecting the well-known
fact that GnRG causes the release of LH, a hor-
mone important in human reproduction.
To gain an additional broad overview of the
characteristics of the extracted events, we com-
pute an event type confusion matrix, shown in Ta-
ble 5. In this matrix, we record for each pair of
event types T1 and T2 the number of unique events
of type T1 for which an event of type T2 can be
found such that, apart for the type difference, the
events are otherwise equal. While e.g. a posi-
tive regulation-negative regulation pair is at least
unusual, in general these event pairs do not sug-
gest extraction errors: for instance the existence
33
of the event expression(Theme:A) does not in any
way prevent the existence of the event localiza-
tion(Theme:A), and regulation subsumes positive-
regulation. Nevertheless, Table 5 shows a clear
preference for a single type for the events.
5 Case Study: The apoptosis pathway
In this section, we will complement the preceding
broad statistical overview of the extracted events
with a detailed study of a specific pathway, the
apoptosis pathway, determining how well the ex-
tracted events cover its interactions (Figure 3).
To create an event network, the events must be
linked through their protein arguments. In addi-
tion to the limited named entity normalization in-
troduced in Section 4, we make use of a list of syn-
onyms for each protein name in the apoptosis path-
way, obtained manually from protein databases,
such as UniProt. Events whose protein arguments
correspond to any of these known synonyms are
then used for reconstructing the pathway.
The apoptosis pathway consists of several over-
lapping signaling routes and can be defined on
different levels of detail. To have a single, ac-
curate and reasonably high-level definition, we
based our pathway on a concisely presentable sub-
set of the KEGG human apoptosis pathway (entry
hsa04210) (Kanehisa and Goto, 2000). As seen
in Figure 3, the extracted dataset contains events
between most interaction partners in the pathway.
The constructed pathway also shows that the ex-
tracted events are not necessarily interactions in
the physical sense. Many ?higher level? events
are extracted as well. For example, the extracel-
lular signaling molecule TNF? can trigger path-
ways leading to the activation of Nf-?B. Although
the two proteins are not likely to interact directly,
it can be said that TNF? upregulates NF-?B, an
event actually extracted by the system. Such state-
ments of indirect interaction co-exist with state-
ments of actual, physical interactions in the event
data.
6 Conclusions
In this paper, we have presented the result of pro-
cessing the entire, unabridged set of PubMed titles
and abstracts with a state-of-the-art event extrac-
tion pipeline as a new resource for text mining in
the biomedical domain. The extraction result ar-
guably represents the best event extraction output
achievable with currently available tools.
The primary contribution of this work is the set
of over 19M extracted event instances of 4.5M
unique events. Of these, 2.1M instances of 1.6M
unique events involve at least two different named
entities. These form an event network several
orders of magnitude larger than those previously
available. The data is intended to support re-
search in biological hypothesis generation, path-
way extraction, and similar higher-level text min-
ing tasks. With the network readily available in an
easy-to-process format under an open license, re-
searchers can focus on the core tasks of text min-
ing without the need to perform the tedious and
computationally very intensive task of event ex-
traction with a complex IE pipeline.
In addition to the extracted events, we make
readily available the output of the BANNER sys-
tem on the entire set of PubMed titles and abstracts
as well as the parser output of the McClosky-
Charniak domain-adapted parser (McClosky and
Charniak, 2008; McClosky, 2009) further trans-
formed to the Stanford Dependency representa-
tion using the tools of de Marneffe et al (2006)
for nearly all (99.91%) sentences with at least one
named entity identified. We expect this data to be
of use for the development and application of sys-
tems for event extraction and other BioNLP tasks,
many of which currently make extensive use of
dependency syntactic analysis. The generation of
this data having been far from a trivial technical
undertaking, its availability as-is can be expected
to save substantial duplication of efforts in further
research.
A manual analysis of extracted events relevant
to the apoptosis pathway demonstrates that the
event data can be used to construct detailed bio-
logical interaction networks with reasonable accu-
racy. However, accurate entity normalization, in
particular taking into account synonymous names,
seems to be a necessary prerequisite and remains
among the most important future work directions.
In the current study, we take first steps in this di-
rection in the form of a term-NE mapping method
in event context. The next step will be the applica-
tion of a state-of-the-art named entity normaliza-
tion system to obtain biological database identities
for a number of the named entities in the extracted
event network, opening possibilities for combin-
ing the data in the network with other biological
information. A further practical problem to ad-
dress will be that of visualizing the network and
34
ev
n
nv
v
v v
n e t
n n v
v nv
v
vn
n
n
n
n
n
n
n n
n
n
n
n n
n
nn
 
n
d
n
e
n
c
v
n
v
t
n
nvnv
n
n
n
v
t
en
ct
nncv
n
n
n e n v
n
n
vnd
n
 
n
v
n t
n
vv
v
n
v
t
n
t
e t
e n v
e
t
n
v
v
n
v
v
nv
n
n v
v
n v
n
c
n
n
v
n v
nn
c
v
nn
vn
v
v
n
v
v
n
e v
vn
IL-1 TNF? TRAIL Fas-L
IL-1R TNF-R1 TRAIL-R Fas
FADDTRADDRIP1MyD88IRAK
NIK
IKK
I?B? NF-?B
CASP10CASP8
FLIP
CASP3CASP7
dioamayorgsrdapuaporrlpaop
ugugpp
TRAF2
IAP
doiiraii
Figure 3: Extracted apoptosis event network. Events shown in the figure are selected on their
prominence in the data or correspondence to known apoptosis interactions. Events corresponding
to KEGG apoptosis pathway interaction partners are highlighted with a light grey background. The
event types are (P)ositive regulation, (N)egative regulation, (R)egulation, gene (E)xpression, (B)inding,
p(H)osphorylation, (L)ocalization and protein (C)atabolism.
presenting the information in a biologically mean-
ingful manner.
The introduced dataset is freely available for
research purposes at http://bionlp.utu.
fi/.
Acknowledgments
This work was supported by the Academy of
Finland and by Grant-in-Aid for Specially Pro-
moted Research (MEXT, Japan). Computational
resources were provided by CSC ? IT Center for
Science, Ltd., a joint computing center for Finnish
academia and industry. We thank Robert Leaman
for advance access and assistance with the newest
release of BANNER.
35
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado. Association for
Computational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010a. Complex event ex-
traction at PubMed scale. In Proceedings of the 18th
Annual International Conference on Intelligent Sys-
tems for Molecular Biology (ISMB 2010). In press.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2010b. Ex-
tracting contextualized complex biological events
with rich graph-based feature sets. Computational
Intelligence. In press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Renata Kabiljo, Andrew Clegg, and Adrian Shepherd.
2009. A realistic assessment of methods for extract-
ing gene/protein interactions from free text. BMC
Bioinformatics, 10(1):233.
M. Kanehisa and S. Goto. 2000. KEGG: kyoto ency-
clopedia of genes and genomes. Nucleic Acids Res.,
28:27?30, Jan.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137?144.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado. ACL.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
R. Leaman and G. Gonzalez. 2008. BANNER: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomput-
ing, pages 652?663.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
David McClosky. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event Extraction With Com-
plex Event Classification Using Rich Features. J
Bioinform Comput Biol, 8:131?146.
Alexander A. Morgan and Lynette Hirschman. 2007.
Overview of BioCreative II gene normalization. In
Proceedings of BioCreative II, pages 101?103.
Claire Ne?dellec. 2005. Learning Language in
Logic - Genic Interaction Extraction Challenge. In
J. Cussens and C. Ne?dellec, editors, Proceedings
of the 4th Learning Language in Logic Workshop
(LLL05), pages 31?37.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: An annotated re-
search abstract corpus in molecular biology domain.
In Proceedings of the Human Language Technology
Conference (HLT?02), pages 73?77.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009a. Incorporating
genetag-style annotation to genia corpus. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 106?
107, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tomoko Ohta, Sampo Pyysalo, Kim Jin-Dong, and
Jun?ichi Tsujii. 2009b. A re-evaluation of biomedi-
cal named entity - term relations. In Proceedings of
LBM?09.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 1?9,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne
Matten, and W John Wilbur. 2005. GENETAG: A
tagged corpus for gene/protein named entity recog-
nition. BMC Bioinformatics, 6(Suppl. 1):S3.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proceedings of the IJCNLP
2005, Companion volume, pages 222?227.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature ge-
netics, 25:25?29.
36
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37?45,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Syntactic Parsers for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The extraction of bio-molecular events
from text is an important task for a number
of domain applications such as pathway
construction. Several syntactic parsers
have been used in Biomedical Natural
Language Processing (BioNLP) applica-
tions, and the BioNLP 2009 Shared Task
results suggest that incorporation of syn-
tactic analysis is important to achieving
state-of-the-art performance. Direct com-
parison of parsers is complicated by to dif-
ferences in the such as the division be-
tween phrase structure- and dependency-
based analyses and the variety of output
formats, structures and representations ap-
plied. In this paper, we present a task-
oriented comparison of five parsers, mea-
suring their contribution to bio-molecular
event extraction using a state-of-the-art
event extraction system. The results show
that the parsers with domain models using
dependency formats provide very similar
performance, and that an ensemble of dif-
ferent parsers in different formats can im-
prove the event extraction system.
1 Introduction
Bio-molecular events are useful for modeling and
understanding biological systems, and their au-
tomatic extraction from text is one of the key
tasks in Biomedical Natural Language Process-
ing (BioNLP). In the BioNLP 2009 Shared Task
on event extraction, participants constructed event
extraction systems using a variety of different
parsers, and the results indicated that the use of
a parser was correlated with high ranking in the
task (Kim et al, 2009). By contrast, the results
did not indicate a clear preference for a particular
parser, and there has so far been no direct compar-
ison of different parsers for event extraction.
While the outputs of parsers applying the same
out format can be compared using a gold standard
corpus, it is difficult to perform meaningful com-
parison of parsers applying different frameworks.
Additionally, it is still an open question to what ex-
tent high performance on a gold standard treebank
correlates with usefulness at practical tasks. Task-
based comparisons of parsers provide not only a
way to asses parsers across frameworks but also a
necessary measure of their practical applicability.
In this paper, five different parsers are com-
pared on the bio-molecular event extraction task
defined in the BioNLP 2009 Shared Task using a
state-of-the-art event extraction system. The data
sets share abstracts with GENIA treebank, and the
treebank is used as an evaluation standard. The
outputs of the parsers are converted into two de-
pendency formats with the help of existing conver-
sion methods, and the outputs are compared in the
two dependency formats. The evaluation results
show that different syntactic parsers with domain
models in the same dependency format achieve
closely similar performance, and that an ensemble
of different syntactic parsers in different formats
can improve the performance of an event extrac-
tion system.
2 Bio-molecular Event Extraction with
Several Syntactic Parsers
This paper focuses on the comparison of several
syntactic parsers on a bio-molecular event extrac-
tion task with a state-of-the-art event extraction
system. This section explains the details of the
comparison. Section 2.1 presents the event ex-
37
traction task setting, following that of the BioNLP
2009 Shared Task. Section 2.2 then summa-
rizes the five syntactic parsers and three formats
adopted for the comparison. Section 2.3 described
how the state-of-the-art event extraction system of
Miwa et al (2010) is modified and used for the
comparison.
2.1 Bio-molecular Event Extraction
The bio-molecular event extraction task consid-
ered in this study is that defined in the BioNLP
2009 Shared Task (Kim et al, 2009)1. The shared
task provided common and consistent task defi-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task consists of
three subtasks: core event extraction (Task 1),
augmenting events with secondary arguments
(Task 2), and the recognition of speculation and
negation of the events (Task 3) (Kim et al, 2009).
In this paper we consider Task 1 and Task 2. The
shared task defined nine event types, which can be
divided into five simple events (Gene expression,
Transcription, Protein catabolism, Phosphoryla-
tion, and Localization) that take one core argu-
ment, a multi-participant binding event (Bind-
ing), and three regulation events (Regulation, Pos-
itive regulation, and Negative regulation) that can
take other events as arguments.
In the two tasks considered, events are repre-
sented with a textual trigger, type, and arguments,
where the trigger is a span of text that states the
event in text. In Task 1 the event arguments that
need to be extracted are restricted to the core ar-
guments Theme and Cause, and secondary argu-
ments (locations and sites) need to be attached in
Task 2.
2.2 Parsers and Formats
Five parsers and three formats are adopted for
the evaluation. The parsers are GDep (Sagae and
Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel,
2004)3, the Charniak-Johnson reranking parser,
using David McClosky?s self-trained biomedi-
cal parsing model (MC) (McClosky, 2009)4, the
C&C CCG parser, adapted to biomedical text
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
2http://www.cs.cmu.edu/?sagae/parser/
gdep/
3http://www.cis.upenn.edu/?dbikel/
software.html
4http://www.cs.brown.edu/?dmcc/
biomedical.html
	 
     Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 132?140,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards Event Extraction from Full Texts on Infectious Diseases
Sampo Pyysalo? Tomoko Ohta? Han-Cheol Cho? Dan Sullivan?
Chunhong Mao? Bruno Sobral? Jun?ichi Tsujii??? Sophia Ananiadou??
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, USA
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{smp,okap,priancho,tsujii}@is.s.u-tokyo.ac.jp
{dsulliva,cmao,sobral}@vbi.vt.edu
Sophia.Ananiadou@manchester.ac.uk
Abstract
Event extraction approaches based on ex-
pressive structured representations of ex-
tracted information have been a significant
focus of research in recent biomedical nat-
ural language processing studies. How-
ever, event extraction efforts have so far
been limited to publication abstracts, with
most studies further considering only the
specific transcription factor-related subdo-
main of molecular biology of the GENIA
corpus. To establish the broader relevance
of the event extraction approach and pro-
posed methods, it is necessary to expand
on these constraints. In this study, we pro-
pose an adaptation of the event extraction
approach to a subdomain related to infec-
tious diseases and present analysis and ini-
tial experiments on the feasibility of event
extraction from domain full text publica-
tions.
1 Introduction
For most of the previous decade, biomedical In-
formation Extraction (IE) efforts have focused pri-
marily on tasks that allow extracted information
to be represented as simple pairs of related enti-
ties. This representation is applicable to many IE
targets of interest, such as gene-disease associa-
tions (Chun et al, 2006) and protein-protein inter-
actions (Ne?dellec, 2005; Krallinger et al, 2007).
However, it has limited applicability to advanced
applications such as semantic search, Gene On-
tology term annotation, and pathway extraction,
tasks for which and relatively few resources or sys-
tems (e.g. (Rzhetsky et al, 2004)) have been intro-
duced. A number of recent studies have proposed
more expressive representations of extracted in-
formation, introducing resources supporting ad-
vanced IE approaches (Pyysalo et al, 2007; Kim
et al, 2008; Thompson et al, 2009; Ananiadou
et al, 2010a). A significant step in the develop-
ment of domain IE methods capable of extract-
ing this class of representations was taken in the
BioNLP?09 shared task on event extraction, where
24 teams participated in an IE task setting requir-
ing the extraction of structured representations of
multi-participant biological events of several types
(Kim et al, 2009).
While the introduction of structured event ex-
traction resources and methods has notably ad-
vanced the state of the art in biomedical IE rep-
resentations, the focus of event extraction studies
carries other limitations frequently encountered in
domain IE efforts. Specifically, resources anno-
tated for biomedical events contain exclusively
texts from publication abstracts, typically further
drawn from small subdomains of molecular biol-
ogy. These choices constrain not only the types of
texts but also the types of events considered, re-
stricting the applicability of event extraction. This
paper presents results from one ongoing effort to
extend an event extraction approach over these
boundaries, toward event extraction from full text
documents in the domain of infectious diseases.
In this study, we consider the subdomain related
to Type IV secretion systems as a model subdo-
main of interest within the broad infectious dis-
eases domain. Type IV secretion systems (T4SS)
are mechanisms for transferring DNA and pro-
teins across cellular boundaries. T4SS are found
in a broad range of Bacteria and in some Ar-
chaea. These translocation systems enable gene
transfer across cellular membranes thus contribut-
ing to the spread of antibiotic resistance and viru-
132
Figure 1: Event representation example. Inhibition of binding caused by phosphorylation is represented
using three events. The shaded text background identifies the text bindings of the events and entities.
lence genes making them an especially important
mechanism in infectious disease research (Juhas et
al., 2008). Type IV secretion systems are found in
plant pathogens, such as Agrobacterium tumefa-
ciens, the cause of crown gall disease as well as in
animal pathogens, such as Helicobacter pylori, a
cause of severe gastric disease. The study of T4SS
has been hampered by the lack of consistent termi-
nology to describe genes and proteins associated
with the translocation mechanism thus motivating
the use of natural language processing techniques
to enhance information retrieval and information
extraction from relevant literature.
2 Event Extraction for the T4SS Domain
This section presents the application of an event
extraction approach to the T4SS domain.
2.1 Event Extraction
We base our information extraction approach on
the model introduced in the BioNLP?09 shared
task on event extraction. Central to this approach
is the event representation, which can capture
the association of multiple participants in varying
roles and numbers and treats events as primary ob-
jects of annotation, thus allowing events to be par-
ticipants in other events. Further, both entities and
events are text-bound, i.e. anchored to specific ex-
pressions in text (Figure 1).
The BioNLP?09 shared task defined nine event
types and five argument types (roles): Theme spec-
ifies the core participant(s) that an event affects,
Cause the cause of the the event, Site a specific
domain or region on a participant involved in the
event, and ToLoc and AtLoc locations associated
with localization events (Table 1). Theme and
Cause arguments may refer to either events or
gene/gene product entities, and other arguments
refer to other physical entities. The Theme ar-
gument is always mandatory, while others can be
omitted when a relevant participant is not stated.
The event types were originally defined to cap-
ture statements of biologically relevant changes in
Event type Args Example
Gene expression T 5-LOX is coexpressed
Transcription T IL-4 transcription
Protein catabolism T IkB-A proteolysis
Localization T,L translocation of STAT6
Phosphorylation T,S NF90 was phosphorylated
Binding T+,S+ Nmi interacts with STAT
Regulation T,C,S IL-4 gene control
Positive regulation T,C,S IL-12 induced binding
Negative regulation T,C,S suppressed dimerization
Table 1: Event types targeted in the BioNLP?09
shared task and their arguments, with minimal
examples of each event type. Arguments ab-
breviate for (T)heme, (C)ause, (S)ite and L for
ToLoc/AtLoc, with ?+? identifying arguments
than can occur multiple times. The expression
marked as triggering the event shown in italics.
the state of entities in a target subdomain involv-
ing transcription factors in human blood cells. In
adapting the approach to new domains, some ex-
tension of the event types is expected to be nec-
essary. By contrast, the argument types and the
general design of the representation are intended
to be general, and to maintain compatibility with
existing systems we aim to avoid modifying these.
2.2 T4SS Domain
A corpus of full-text publications relating to the
T4SS subdomain of the infectious diseases do-
main annotated for biological entities and terms of
interest to domain experts was recently introduced
by (Ananiadou et al, 2010b). In the present study,
we use this corpus as a reference standard defin-
ing domain information needs. In the following
we briefly describe the corpus annotation and the
view it provides of the domain.
The T4SS corpus annotation covers four classes
of tagged entities and terms: Bacteria, Cellular
components, Biological Processes, and Molecular
functions. The latter three correspond to the three
Gene Ontology (GO) (Ashburner et al, 2000) top-
level sub-ontologies, and terms of these types were
annotated with reference to both GO and relevance
to the interests of domain experts, with guidelines
133
Bacterium
A. tumefaciens 32.7%
H. pylori 20.0%
L. pneumophila 16.3%
E. coli 12.3%
B. pertussis 3.0%
Cell component
T4SS 5.2%
Ti plasmid 5.1%
outer membrane 4.2%
membrane 3.5%
genome 3.4%
Biological process
virulence 14.1%
conjugation 7.9%
localization 6.1%
nuclear import 5.8%
transfer 5.1%
Molecular function
nucleotide-binding 20.3%
ATPase activity 17.3%
NTP-binding 14.7%
ATP-binding 12.2%
DNA-binding 9.1%
Table 2: Most frequently tagged terms (after normalization) and their relative frequencies of all tagged
entities of each of the four types annotated in the T4SS corpus.
Type Annotations
Bacteria 529
Cellular component 2237
Biological process 1873
Molecular function 197
Table 3: Statistics for the existing T4SS corpus
annotation.
requiring that marked terms be both found in GO
and associated with T4SS. These constraints as-
sure that the corpus is relevant to the informa-
tion needs of biologists working in the domain and
that it can be used as a reference for the study of
automatic GO annotation. In the work introduc-
ing the corpus, the task of automatic GO anno-
tation was studied as facilitating improved infor-
mation access, such as advanced search function-
ality: GO annotation can allow for search by se-
mantic classes or co-occurrences of terms of speci-
fied classes. The event approach considered in this
study further extends on these opportunities in in-
troducing a model allowing e.g. search by specific
associations of the concepts of interest.
The previously created annotation of the T4SS
corpus covers 27 full text publications totaling
15143 pseudo-sentences (text sentences plus table
rows, references, etc.) and 244942 tokens.1 A to-
tal of nearly 5000 entities and terms are annotated
in these documents; Table 2 shows the most fre-
quently tagged terms of each type after basic nor-
malization of different surface forms, and Table 3
gives the per-class statistics. Domain characteris-
tics are clearly identifiable in the first three tagged
types, showing disease-related bacteria, their ma-
jor cellular components, and processes related to
movement, reproduction and infection. The last
term type is dominated by somewhat more generic
binding-type molecular functions.
In addition to the four annotated types it was
1While the document count is modest compared to that
of abstract-based corpora, we estimate that in terms of the
amount of text (tokens) the corpus corresponds to over 1000
abstracts, comparable in size to e.g. the GENIA event corpus
(Kim et al, 2008).
recognized during the original T4SS corpus anno-
tation that genes and gene products are centrally
important for domain information needs, but their
annotation was deferred to focus on novel cate-
gories. As part of the present study, we introduce
annotation for gene/gene product (GGP) mentions
(Section 3.2), and in the following discussion of
applying an event extraction approach to the do-
main the availability of this class annotation as an
additional category is assumed.
2.3 Adaptation of the Event Model
The event model involves two primary categories
of representation: physical entities such as genes
and proteins are elementary (non-structured) and
their mentions annotated as typed spans of text,2
and events and processes (?things that happen?)
are represented using the structured event repre-
sentation described in Section 2.1. This division
applies straightforwardly to the T4SS annotations,
suggesting an approach where bacteria and cell
components retain their simple tagged-term repre-
sentation and the biological processes and molec-
ular functions are given an event representation.
In the following, we first analyze correspondences
between the latter two classes and BioNLP?09
shared task events, and then proceed to study the
event arguments and their roles as steps toward a
complete event model for the domain.
Molecular functions, the smallest class tagged
in the T4SS corpus, are highly uniform: almost
75% involve binding, immediately suggesting rep-
resentation using the Binding class of events de-
fined in the applied event extraction model. The
remaining functions are ATPase activity, together
with its exact GO synonyms (e.g. ATP hydrolase
activity) accounting for 19% of the terms, the gen-
eral type hydrolysis (4.5%), and a small number
of rare other functions. While these have no cor-
respondence with previously defined event types,
2Normalization identifying e.g. the Uniprot entry corre-
sponding to a protein mention may also be necessary, but here
excluded from consideration an independent issue.
134
Class Category Freq
Location
Transfer 27.6%
Localization 15.6%
Import/export 14.5%
Virulence 14.1%
High-level Assembly 8.7%
process Conjugation 8.3%
Secretion 8.1%
(Other) 1.8%
Table 4: Categorization of T4SS corpus biologi-
cal processes and relative frequency of mentions
of each category of the total tagged.
their low overall occurrence counts make them of
secondary interest as extraction targets.
The biological processes are considerably more
diverse. To identify general categories, we per-
formed a manual analysis of the 217 unique nor-
malized terms annotated in the corpus as biologi-
cal processes (Table 4). We find that the majority
of the instances (58%) relate to location or move-
ment. As related types of statements are anno-
tated as Localization events in the applied model,
we propose to apply this event type and differen-
tiate between the specific subtypes on the basis of
the event arguments. A further 39% are of cate-
gories that can be viewed as high-level processes.
These are distinct from the events considered in
the BioNLP?09 shared task in involving coarser-
grained events and larger-scale participants than
the GGP entities considered in the task: for ex-
ample, conjugation occurs between bacteria, and
virulence may involve a human host.
To analyze the role types and arguments char-
acteristic of domain events, we annotated a small
sample of tagged mentions for the most fre-
quent types in the broad classification discussed
above: Binding for Molecular function, Transfer
for Location-related, and Virulence for High-level
process. The statistics of the annotated 65 events
are shown in Tables 5, 6 and 7. For Binding, we
find that while an estimated 90% of events in-
volve a GGP argument, the other participant of
the binding is in all cases non-GGP, most fre-
quently of Nucleotide type (e.g. NTP/ATP). While
only GGP Binding arguments were considered in
the shared task events, the argument structures are
typical of multi-participant binding and this class
of expressions are in scope of the original GE-
NIA Event corpus annotation (Kim et al, 2008).
Event annotations could thus potentially be de-
rived from existing data. Localization event
arguments show substantially greater variety and
Freq Arguments
78% Theme: GGP, Theme: Nucleotide
5.5% Theme: GGP, Theme: DNA
5.5% Theme: GGP, Theme: Sugar
5.5% Theme: Protein family, Theme: DNA
5.5% Theme: Protein, Theme: Nucleotide
Table 5: Binding event arguments.
Freq Arguments
16% Theme: DNA, From/To: Organism
16% Theme: DNA
16% Theme: Cell component
12% Theme: DNA, To: Organism
8% Theme: Protein family, From/To: Organism
4% Theme: GGP
4% Theme: GGP, To: Organism
4% Theme: GGP, From: Organism
4% Theme: Protein family, From: Organism
4% Theme: Protein family
4% Theme: Organism, To: Cell component
4% Theme: DNA From: Organism, To: Cell component
4% (no arguments)
Table 6: Localization (Transfer) event arguments.
Freq Arguments
64% Cause: GGP
16% Theme:Organism, Cause: GGP
8% Cause: Organism
8% (no arguments)
4% Cause: Protein family
Table 7: Process (Virulence) arguments.
some highly domain-specific argument combina-
tions, largely focusing on DNA and Cell compo-
nent (e.g. phagosome) transfer, frequently involv-
ing transfer between different organisms. While
the participants are almost exclusively of types
that do not appear in Localization events in exist-
ing annotations, the argument structures are stan-
dard and in our judgment reasonably capture the
analyzed statements, supporting the applicability
of the general approach. Finally, the argument
analysis shown in Table 7 supports the previous
tentative observation that the high-level biologi-
cal processes are notably different from previously
considered event types: for over 80% of these pro-
cesses no overtly stated Theme could be identified.
We take this to indicate that the themes ? the core
participants that the processes concern ? are ob-
vious in the discourse context and their overt ex-
pression would be redundant. (For example, in
the context virulence obviously involves a host and
conjugation involves bacteria.) By contrast, in the
corpus the entities contributing to these processes
are focused: a participant we have here analyzed
as Cause is stated in over 90% of cases. This
135
Sentences Tokens
Abstracts 150 3789
Full texts 448 13375
Total 598 17164
Table 8: Statistics for the selected subcorpus.
novel pattern of event arguments suggests that the
event model should be augmented to capture this
category of high-level biological processes. Here,
we propose an event representation for these pro-
cesses that removes the requirement for a Theme
and substitutes instead a mandatory Cause as the
core argument. In the event annotation and exper-
iments, we focus on this newly proposed class.
3 Annotation
This section describes the new annotation intro-
duced for the T4SS corpus.
3.1 Text Selection
The creation of exhaustive manual annotation for
the full T4SS corpus represents a considerable an-
notation effort. Due to resource limitations, for
this study we did not attempt full-scope annota-
tion but instead selected a representative subset of
the corpus texts. We aimed to select texts that pro-
vide good coverage of the text variety in the T4SS
corpus and can be freely redistributed for use in re-
search. We first selected for annotation all corpus
documents with at least a freely available PubMed
abstract, excluding 3 documents. As the corpus
only included a single freely redistributable Open
Access paper, we extended full text selection to
manuscripts freely available as XML/HTML (i.e.
not only PDF) via PubMed Central. While these
documents cannot be redistributed in full, their
text can be reliably combined with standoff anno-
tations to recreate the annotated corpus.
In selected full-text documents, to focus anno-
tation efforts on sections most likely to contain re-
liable new information accessible to natural lan-
guage processing methods, we further selected the
publication body text, excluding figures and tables
and their captions, and removed Methods and Dis-
cussion sections. We then removed artifacts such
as page numbers and running heads and cleaned
remaining errors from PDF conversion of the orig-
inal documents. This selection produced a subcor-
pus of four full-text documents and 19 abstracts.
The statistics for this corpus are shown in Table 8.
GGP GGP/sentence
Abstracts 124 0.82
Full texts 394 0.88
Total 518 0.87
Table 9: Statistics for the GGP annotation.
3.2 Gene/Gene Product Annotation
As gene and gene product entities are central to
domain information needs and the core entities of
the applied event extraction approach, we first in-
troduced annotation for this entity class. We cre-
ated manual GGP annotation following the an-
notation guidelines of the GENIA GGP Corpus
(Ohta et al, 2009). As this corpus was the source
of the gene/protein entity annotation provided as
the basis of the BioNLP shared task on event ex-
traction, adopting its annotation criteria assures
compatibility with recently introduced event ex-
traction methods. Briefly, the guidelines spec-
ify tagging for minimal continuous spans of spe-
cific gene/gene product names, without differen-
tiating between DNA/RNA/protein. A ?specific
name? is understood to be a a name that allows
a domain expert to identify the entry in a rele-
vant database (Entrez gene/Uniprot) that the name
refers to. Only GGP names are tagged, excluding
descriptive references and the names of related en-
tities such as complexes, families and domains.
The annotation was created on the basis of an
initial tagging created by augmenting the output
of the BANNER tagger (Leaman and Gonzalez,
2008) by dictionary- and regular expression-based
tagging. This initial high-recall markup was then
corrected by a human annotator. To confirm that
the annotator had correctly identified subdomain
GGPs and to check against possible error intro-
duced through the machine-assisted tagging, we
performed a further verification of the annotation
on approx. 50% of the corpus sentences: we com-
bined the machine- and human-tagged annotations
as candidates, removed identifying information,
and asked two domain experts to identify the cor-
rect GGPs. The two sets of independently pro-
duced judgments showed very high agreement:
holding one set of judgments as the reference stan-
dard, the other would achieve an f-score of 97%
under the criteria presented in Section 4.2. We
note as one contributing factor to the high agree-
ment that the domain has stable and systematically
applied GGP naming criteria. The statistics of the
full GGP annotation are shown in Table 9.
136
Events Event/sentence
Abstracts 15 0.1
Full texts 5 0.01
Additional 80 2.2
Total 100 0.16
Table 10: Statistics for the event annotation.
3.3 Event Annotation
Motivated by the analysis described in Section 2.3,
we chose to focus on the novel category of asso-
ciations of GGP entities in high-level processes.
Specifically, we chose to study biological pro-
cesses related to virulence, as these are the most
frequent case in the corpus and prototypical of the
domain. We adopted the GENIA Event corpus an-
notation guidelines (Kim et al, 2008), marking as-
sociations between specific GGPs and biological
processes discussed in the text even when these
are stated speculatively or their existence explic-
itly denied. As the analysis indicated this category
of processes to typically involve a single stated
participant in a fixed role, annotations were ini-
tially recorded as (GGP, process) pairs and later
converted into an event representation.
During annotation, the number of annotated
GGP associations with the targeted class of pro-
cesses in the T4SS subcorpus was found to be too
low to provide material for both training and test-
ing a supervised learning-based event extraction
approach. To extend the source data, we searched
PubMed for cases where a known T4SS-related
protein co-occurred with an expression known to
relate to the targeted process class (e.g. virulence,
virulent, avirulent, non-virulent) and annotated a
further set of sentences from the search results for
both GGPs and their process associations. As the
properties of these additional examples could not
be assured to correspond to those of the targeted
domain texts, we used these annotations only as
development and training data, performing evalu-
ation on cases drawn from the T4SS subcorpus.
As the annotation target was novel, we per-
formed two independent sets of judgments for all
annotated cases, jointly resolving disagreements.
Although initial agreement was low, for a final set
of judgments we measured high agreement, corre-
sponding to 93% f-score when holding one set of
judgments as the gold standard. The statistics of
the annotation are shown in Table 10. Annotations
are sparse in the T4SS subcorpus and, as expected,
very dense in the targeted additional data.
4 Experiments
4.1 Methods
For GGP tagging experiments, we applied a state-
of-the-art tagger with default settings as reference
and a custom tagger for adaptation experiments.
As the reference tagger, we applied a recent re-
lease of BANNER (Leaman and Gonzalez, 2008)
trained on the GENETAG corpus (Tanabe et al,
2005). The corpus is tagged for gene and protein-
related entities and its texts drawn from a broad
selection of PubMed abstracts. The current revi-
sion of the tagger3 achieves an f-score of 86.4%
on the corpus, competitive with the best result re-
ported in the BioCreative II evaluation (Wilbur et
al., 2007), 87.2%. The custom tagger4 follows the
design of BANNER in both the choice of Con-
ditional Random Fields (Lafferty et al, 2001) as
the applied learning method and the basic feature
design, but as a key extension can further adopt
features from external dictionaries as both positive
and negative indicators of tagged entities. Tagging
experiments were performed using a document-
level 50/50 split of the GGP-annotated subcorpus.
For event extraction, we applied an adapta-
tion of the approach of the top-ranking system in
the BioNLP?09 shared task (Bjo?rne et al, 2009):
all sentences in the input text were parsed with
the McClosky-Charniak (2008) parser and the re-
sulting phrase structure analyses then converted
into the Stanford Dependency representation us-
ing conversion included in the Stanford NLP tools
(de Marneffe et al, 2006). Trigger recognition
was performed with a simple regular expression-
based tagger covering standard surface form vari-
ation. Edge detection was performed using a su-
pervised machine learning approach, applying the
LibSVM (Chang and Lin, 2001) Support Vector
Machine implementation with a linear kernel and
the feature representation of Bjo?rne et al (2009),
building largely around the shortest dependency
path connecting a detected trigger with a candi-
date participant. The SVM regularization parame-
ter was selected by a sparse search of the parame-
ter space with evaluation using cross-validation on
the training set. As the class of events targeted for
extraction in this study are of a highly restricted
type, each taking only of a single mandatory Cause
argument, the construction of events from detected
3http://banner.sourceforge.net
4http://www-tsujii.is.s.u-tokyo.ac.jp/
NERsuite/
137
Precision Recall F-score
Abstracts 68.1% 89.5% 77.3%
Full texts 56.9% 80.7% 66.7%
Total 59.4% 82.8% 69.2%
Table 11: Initial GGP tagging results.
triggers and edges could be implemented as a sim-
ple deterministic rule.
4.2 Evaluation Criteria
For evaluating the performance of the taggers we
apply a relaxed matching criterion that accepts a
match between an automatically tagged and a gold
standard entity if the two overlap at least in part.
This relaxation is adopted to focus on true tagging
errors. The GENETAG entity span guidelines dif-
fer from the GENIA GGP guidelines adopted here
in allowing the inclusion of e.g. head nouns when
names appear in modifier position, while the an-
notation guidelines applied here require marking
only the minimal name.5 When applying strict
matching criteria, a substantial number of errors
may trace back to minor boundary differences
(Wang et al, 2009), which we consider of sec-
ondary interest to spurious or missing tags. Over-
all results are microaverages, that is, precision, re-
call and f-score are calculated from the sum of true
positive etc. counts over individual documents.
For event extraction, we applied the BioNLP?09
shared task event extraction criteria (Kim et al,
2009) with one key change: to make it possible
to evaluate the extraction of the high-level pro-
cess participants, we removed the requirement that
all events must define a Theme as their core argu-
ment.
4.3 Gene/Gene Product Tagging
The initial GGP tagging results using BANNER
are shown in Table 11. We find that even for the
relaxed overlap matching criterion, the f-score is
nearly 10% points lower than reported on GENE-
TAG in the evaluation on abstracts. For full texts,
performance is lower yet by a further 10% points.
In both cases, the primary problem is the poor
precision of the tagger, indicating that many non-
GGPs are spuriously tagged.
To determine common sources of error, we per-
formed a manual analysis of 100 randomly se-
lected falsely tagged strings (Table 12). We find
5GENETAG annotations include e.g. human ets-1 protein,
whereas the guidelines applied here would require marking
only ets-1.
Category Freq Examples
GGP family or group 34% VirB, tmRNA genes
Figure/table 26% Fig. 1B, Table 1
Cell component 10% T4SS, ER vacuole
Species/strain 9% E. coli, A348deltaB4.5
Misc. 9% step D, Protocol S1
GGP domain or region 4% Pfam domain
(Other) 8% TrIP, LGT
Table 12: Common sources of false positives in
GGP tagging.
Precision Recall F-score
Abstracts 90.5% 95.7% 93.1%
Full texts 90.0% 93.2% 91.6%
Total 90.1% 93.8% 91.9%
Table 13: GGP tagging results with domain adap-
tation.
that the most frequent category consists of cases
that are arguably correct by GENETAG annota-
tion criteria, which allow named protein families
of groups to be tagged. A similar argument can
be made for domains or regions. Perhaps not sur-
prisingly, a large number of false positives relate
to features common in full texts but missing from
the abstracts on which the tagger was trained, such
as figure and table references. Finally, systematic
errors are made for entities belonging to other cat-
egories such as named cell components or species.
To address these issues, we applied a domain-
adapted custom tagger that largely replicates the
features of BANNER, further integrating infor-
mation from the UMLS Metathesaurus,6 which
provides a large dictionary containing terms cov-
ering 135 different semantic classes, and a cus-
tom dictionary of 1081 domain GGP names, com-
piled by (Ananiadou et al, 2010b). The non-GGP
UMLS Metathesaurus terms provided negative in-
dicators for reducing spurious taggings, and the
custom dictionary positive indicators. Finally, we
augmented the GENETAG training data with 10
copies7 of the training half of the T4SS GGP cor-
pus as in-domain training data.
Table 13 shows the results with the domain-
adapted tagger. We find dramatically improved
performance for both abstracts and full texts,
showing results competitive with the state of the
art performance on GENETAG (Wilbur et al,
2007). Thus, while the performance of an un-
adapted tagger falls short of both results reported
6http://www.nlm.nih.gov/research/umls/
7As the GENETAG corpus is considerably larger than the
T4SS GGP corpus, replication was used to assure that suffi-
cient weight is given to the in-domain data in training.
138
Precision Recall F-score
Co-occurrence 65% 100% 78%
Machine learning 81% 85% 83%
Table 14: Event extraction results.
on GENETAG and levels necessary for practi-
cal application, adaptation addressing common
sources of error through the adoption of general
and custom dictionaries and the use of a small
set of in-domain training data was successful in
addressing these issues. The performance of the
adapted tagger is notably high given the modest
size of the in-domain data, perhaps again reflect-
ing the consistent GGP naming conventions of the
subdomain.
4.4 Event Extraction
We performed an event extraction experiment fol-
lowing the training and test split described in Sec-
tion 3.3. Table 14 shows the results of the ap-
plied machine learning-based method contrasted
with a co-occurrence baseline replacing the edge
detection with a rule that extracts a Cause edge for
all trigger-GGP combinations co-occurring within
sentence scope. This approach achieves 100% re-
call as the test data was found to only contain
events where the arguments are stated in the same
sentence as the trigger.
The results show that the machine learning ap-
proach achieves very high performance, matching
the best results reported for any single event type
in the BioNLP?09 shared task (Kim et al, 2009).
The very high co-occurrence baseline result sug-
gests that the high performance largely reflects the
relative simplicity of the task. With respect to
the baseline result, the machine-learning approach
achieves a 21% relative reduction in error.
While this experiment is limited in both scope
and scale, it suggests that the event extraction ap-
proach can be beneficially applied to detect do-
main events represented by novel argument struc-
tures. As a demonstration of feasibility the result
is encouraging for both the applicability of event
extraction to this specific new domain and for the
adaptability of the approach to new domains in
general.
5 Discussion and Conclusions
We have presented a study of the adaptation of an
event extraction approach to the T4SS subdomain
as a step toward the introduction of event extrac-
tion to the broader infectious diseases domain. We
applied a previously introduced corpus of subdo-
main full texts annotated for mentions of bacte-
ria and terms from the three top-level Gene On-
tology subontologies as a reference defining do-
main information needs to study how these can
be met through the application of events defined
in the BioNLP?09 Shared Task on event extrac-
tion. Analysis indicated that with minor revision
of the arguments, the Binding and Localization
event types could account for the majority of both
biological processes and molecular functions of
interest. We further identified a category of ?high-
level? biological processes such as the virulence
process typical of the subdomain, which necessi-
tated extension of the considered event extraction
model.
Based on argument analysis, we proposed a rep-
resentation for high-level processes in the event
model that substitutes Cause for Theme as the
core argument. We further produced annotation
allowing an experiment on the extraction of the
dominant category of virulence processes with
gene/gene product (GGP) causes, annotating 518
GGP mentions and 100 associations between these
and the processes. Experiments indicated that with
annotated in-domain resources both the GGP enti-
ties and their associations with processes could be
extracted with high reliability.
In future work we will extend the model and
annotation proposed in this paper to the broader
infectious diseases domain, introducing annotated
resources and extraction methods for advanced in-
formation access. All annotated resources intro-
duced in this study are available from the GENIA
project homepage.8
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
the National Institutes of Health, grant number
HHSN272200900040C, and the Joint Information
Systems Committee (JISC, UK).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010a. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
139
Sophia Ananiadou, Dan Sullivan, Gina-Anne Levow,
Joseph Gillespie, Chunhong Mao, Sampo Pyysalo,
Jun?ichi Tsujii, and Bruno Sobral. 2010b. Named
entity recognition for bacterial type IV secretion sys-
tems. (manuscript in review).
M Ashburner, CA Ball, JA Blake, D Botstein, H But-
ler, JM Cherry, AP Davis, K Dolinski, SS Dwight,
JT Eppig, MA Harris, DP Hill, L Issel-Tarver,
A Kasarskis, S Lewis, JC Matese, JE Richardson,
M Ringwald, GM Rubin, and G Sherlock. 2000.
Gene ontology: tool for the unification of biology.
Nature genetics, 25:25?29.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Hong-Woo Chun, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dic-
tionaries and machine learning. In Proceedings of
the Pacific Symposium on Biocomputing (PSB?06),
pages 4?15.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Mario Juhas, Derrick W. Crook, and Derek W. Hood.
2008. Type IV secretion systems: tools of bacterial
horizontal gene transfer and virulence. Cellular mi-
crobiology, 10(12):2377?2386.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the Second BioCreative
PPI task: Automatic Extraction of Protein-Protein
Interactions. In L. Hirschman, M. Krallinger, and
A. Valencia, editors, Proceedings of Second BioCre-
ative Challenge Evaluation Workshop, pages 29?39.
John D. Lafferty, Andrew McCallum, and Fernando
C . N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML ?01: Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
R. Leaman and G. Gonzalez. 2008. Banner: an ex-
ecutable survey of advances in biomedical named
entity recognition. Pacific Symposium on Biocom-
puting, pages 652?663.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in
Logic - Genic Interaction Extraction Challenge. In
J. Cussens and C. Ne?dellec, editors, Proceedings
of the 4th Learning Language in Logic Workshop
(LLL05), pages 31?37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur. 2005. Genetag: a tagged
corpus for gene/protein named entity recognition.
BMC Bioinformatics, 6(Suppl 1):S3.
Paul Thompson, Syed Iqbal, John McNaught, and
Sophia Ananiadou. 2009. Construction of an anno-
tated corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10(1):349.
Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo
Pyysalo, and Jun?ichi Tsujii. 2009. Investigat-
ing heterogeneous protein annotations toward cross-
corpora utilization. BMC Bioinformatics, 10(1):403.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, ed-
itors, Proceedings of Second BioCreative Challenge
Evaluation Workshop, pages 7?16.
140
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 65?73,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic Acquisition of Huge Training Data
for Bio-Medical Named Entity Recognition
Yu Usami? ? Han-Cheol Cho? Naoaki Okazaki? and Jun?ichi Tsujii?
?Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Inui Laboratory, Department of System Information Sciences, Tohoku University, Sendai, Japan
? Microsoft Research Asia, Beijing, China
{yusmi, hccho}@is.s.u-tokyo.ac.jp
okazaki@ecei.tohoku.ac.jp
jtsujii@microsoft.com
Abstract
Named Entity Recognition (NER) is an im-
portant first step for BioNLP tasks, e.g., gene
normalization and event extraction. Employ-
ing supervised machine learning techniques
for achieving high performance recent NER
systems require a manually annotated corpus
in which every mention of the desired seman-
tic types in a text is annotated. However, great
amounts of human effort is necessary to build
and maintain an annotated corpus. This study
explores a method to build a high-performance
NER without a manually annotated corpus,
but using a comprehensible lexical database
that stores numerous expressions of seman-
tic types and with huge amount of unanno-
tated texts. We underscore the effectiveness of
our approach by comparing the performance
of NERs trained on an automatically acquired
training data and on a manually annotated cor-
pus.
1 Introduction
Named Entity Recognition (NER) is the task widely
used to detect various semantic classes such as
genes (Yeh et al, 2005), proteins (Tanabe and
Wilbur, 2002), and diseases in the biomedical field.
A na??ve approach to NER handles the task as a
dictionary-matching problem: Prepare a dictionary
(gazetteer) containing textual expressions of named
entities of specific semantic types. Scan an input
text, and recognize a text span as a named entity if
the dictionary includes the expression of the span.
Although this approach seemingly works well, it
presents some critical issues. First, the dictionary
must be comprehensive so that every NE mention
can be found in the dictionary. This requirement
for dictionaries is stringent because new terminol-
ogy is being produced continuously, especially in
the biomedical field. Second, this approach might
suffer from an ambiguity problem in which a dic-
tionary includes an expression as entries for multi-
ple semantic types. For this reason, we must use
the context information of an expression to make
sure that the expression stands for the target seman-
tic type.
Nadeau and Sekine (2007) reported that a strong
trend exists recently in applying machine learning
(ML) techniques such as Support Vector Machine
(SVM) (Kazama et al, 2002; Isozaki and Kazawa,
2002) and Conditional Random Field (CRF) (Set-
tles, 2004) to NER, which can address these issues.
In this approach, NER is formalized as a classifi-
cation problem in which a given expression is clas-
sified into a semantic class or other (non-NE) ex-
pressions. Because the classification problem is usu-
ally modeled using supervised learning methods, we
need a manually annotated corpus for training NER
classifier. However, preparing manually annotated
corpus for a target domain of text and semantic types
is cost-intensive and time-consuming because hu-
man experts are needed to reliably annotate NEs in
text. For this reason, manually annotated corpora
for NER are often limited to a specific domain and
covers a small amount of text.
In this paper we propose a novel method for au-
tomatically acquiring training data for NER from a
comprehensible lexical database and huge amounts
of unlabeled text. This paper presents four contribu-
65
Gene or Protein name
Official name
Aliases
References
Figure 1: Example of an Entrez Gene record.
tions:
1. We show the ineffectiveness of a na??ve
dictionary-matching for acquiring a training
data automatically and the significance of the
quality of training data for supervised NERs
2. We explore the use of reference information
that bridges the lexical database and unlabeled
text for acquiring high-precision and low-recall
training data
3. We develop two strategies for expanding NE
annotations, which improves the recall of the
training data
4. The proposed method acquires a large amount
of high-quality training data rapidly, decreasing
the necessity of human efforts
2 Proposed method
The proposed method requires two resources to ac-
quire training data automatically: a comprehen-
sive lexical database and unlabeled texts for a tar-
get domain. We chose Entrez Gene (National Li-
brary of Medicine, 2005) as the lexical database be-
cause it provides rich information for lexical entries
and because genes and proteins constitute an im-
portant semantic classes for Bio NLP. Entrez Gene
consists of more than six million gene or protein
records, each of which has various information such
as the official gene (protein) name, synonyms, or-
ganism, description, and human created references.
Figure 1 presents an example of an Entrez Gene
record. We created a dictionary by collecting offi-
cial gene (protein) names and their synonyms from
the Entrez Gene records. For unlabeled text, we use
the all 2009 release MEDLINE (National Library
of Medicine, 2009) data. MEDLINE consists of
about ten million abstracts covering various fields of
biomedicine and health. In our study, we focused on
recognizing gene and protein names within biomed-
ical text.
Our process to construct a NER classifier is as fol-
lows: We apply the GENIA tagger (Tsuruoka et al,
2005) to split the training data into tokens and to at-
tach part of speech (POS) tags and chunk tags. In
this work, tokenization is performed by an external
program that separates tokens by a space, hyphen,
comma, period, semicolon, or colon character. Part
of speech tags present grammatical roles of tokens,
e.g. verbs, nouns, and prepositions. Chunk tags
compose tokens into syntactically correlated seg-
ments, e.g. verb phrases, noun phrases, and preposi-
tional phrases. We use the IOBES notation (Ratinov
and Roth, 2009) to represent NE mentions with label
sequences, thereby NER is formalized as a multi-
class classification problem in which a given token
is classified into IOBES labels. To classify labels of
tokens, we use a linear kernel SVM which applies
the one-vs.-the-rest method (Weston and Watkins,
1999) to extend binary classification to multi-class
classification. Given the t-th token xt in a sentence,
we predict the label yt,
yt = argmax
y
s(y|xt, yt?1).
In this equation, s(y|xt, yt?1) presents the score
(sum of feature weights) when the token xt is la-
beled y. We use yt?1 (the label of the previous to-
ken) to predict yt, expecting that this feature behaves
as a label bigram feature (also called translation fea-
ture) in CRF. If the sentence consists of x1 to xT , we
repeat prediction of labels sequentially from the be-
ginning (y1) to the end (yT ) of a sentence. We used
LIBLINEAR (Fan et al, 2008) as an SVM imple-
mentation.
Table 1 lists the features used in the classifier
modeled by SVM. For each token (?Human? in the
example of Table 1), we created several features in-
cluding: token itself (w), lowercase token (wl), part
of speech (pos), chunk tag (chk), character pattern of
66
Name Description Example Value
w token Human
wl token in small letters human
pos part of speech NNP
chk chunk tag B-NP
shape entity pattern ULLLL
shaped entity pattern 2 UL
type token type InitCap
pn(n = 1...4) prefix n characters (H,Hu,Hum,Huma)
sn(n = 1...4) suffix n characters (n,an,man,uman)
Table 1: Example of features used in machine learning
process.
token (shape), character pattern designated (shaped),
token type (type), prefixes of length n (pn), and suf-
fixes of length n (sn). More precisely, the character
pattern of token (shape) replaces each character in
the token with either an uppercase letter (U), a low-
ercase letter (L), or a digit (D). The character pat-
tern designated (shaped) is similar to a shape feature,
but the consecutive character types are reduced to
one symbol, for example, ?ULLLL? (shape) is rep-
resented with ?UL? (shaped) in the example of Ta-
ble 1). The token type (type) represents whether the
token satisfies some conditions such as ?begins with
a capital letter?, ?written in all capitals?, ?written
only with digits?, or ?contains symbols?. We created
unigram features and bigram features (excluding wl,
pn, sn) from the prior 2 to the subsequent 2 tokens
of the current position.
2.1 Preliminary Experiment
As a preliminary experiment, we acquired training
data using a na??ve dictionary-matching approach.
We obtained the training data from all 2009 MED-
LINE abstracts with an all gene and protein dictio-
nary in Entrez Gene. The training data consisted of
nine hundred million tokens. We constructed a NER
classifier using only four million tokens of the train-
ing data because of memory limitations. For evalua-
tion, we used the Epigenetics and Post-translational
Modification (EPI) corpus BioNLP 2011 Shared
Task (SIGBioMed, 2011). Only development data
and training data are released as the EPI corpus at
present, we used both of the data sets for evalua-
tion in this experiment. Named entities in the corpus
are annotated exhaustively and belong to a single se-
mantic class, Gene or Gene Product (GGP) (Ohta
et al, 2009). We evaluated the performance of the
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
trained on acquired data 85.76 10.18 23.83 14.27
Table 2: Results of the preliminary experiment.
(a) It is clear that in culture media of AM,
cystatin C and cathepsin B are present as
proteinase?antiproteinase complexes.
(b) Temperature in the puerperium is higher
in AM, and lower in PM.
Figure 2: Dictionary-based gene name annotating exam-
ple (annotated words are shown in italic typeface).
NER on four measures: Accuracy (a), Precision (P),
Recall (R), and F1-measure (F1). We used the strict
matching criterion that a predicted named entity is
correct if and only if the left and the right bound-
aries are both correct.
Table 2 presents the evaluation results of this ex-
periment. The first model ?dictionary matching?
performs exact dictionary-matching on the test cor-
pus. It achieves a 40.78 F1-score. The second model
?trained on acquired data? uses the training data
acquired automatically for constructing NER clas-
sifier. It scores very low-performance (14.27 F1-
score), even compared with the simple dictionary-
matching NER. Exploring the annotated training
data, we investigate why this machine learning ap-
proach shows extremely low performance.
Figure 2 presents an example of the acquired
training data. The word ?AM? in the example (a)
is correct because it is gene name, although ?AM?
in the example (b) is incorrect because ?AM? in (b)
is the abbreviation of ante meridiem, which means
before noon. This is a very common problem, espe-
cially with abbreviations and acronyms. If we use
this noisy training data for learning, then the result
of NER might be low because of such ambiguity. It
is very difficult to resolve errors in the training data
even with the help of machine learning methods.
2.2 Using Reference Information
To obtain high-precision data, we used reference in-
formation included with each record in Entrez Gene.
Figure 3 portrays a simple example of reference in-
formation. It shows the reference information of the
67
 PMID 1984484: 
 It is clear that in culture media of AM, 
cystatin C and cathepsin B are present as 
proteinase-antiproteinase complexes.
Gene: AM
Entrez Gene Records
MEDLINE Abstracts
 PMID 23456:
 Temperature in puerperium is higher in AM, 
lower in PM.
Reference
Figure 3: Reference to MEDLINE abstract example.
Entrez Gene record which describes that the gene
?AM?. The reference information indicates PMIDs
in which the gene or protein is described.
We applied the rule whereby we annotated a
dictionary-matching in each MEDLINE abstract
only if they were referred by the Entrez Gene
records. Figure 3 shows that the gene ?AM? has
reference to the MEDLINE abstract #1984484 only.
Using this reference information between the En-
trez Gene record ?AM? and the MEDLINE abstract
#1984484, we can annotate the expansion ?AM? in
MEDLINE abstract #1984484 only. In this way, we
can avoid incorrect annotation such as example b in
Figure 2.
We acquired training data automatically using ref-
erence information, as follows:
1. Construct a gene and protein dictionary includ-
ing official names, synonyms and reference in-
formation in Entrez Gene
2. Apply a dictionary-matching on the all MED-
LINE abstracts with the dictionary
3. Annotate the MEDLINE abstract only if it was
referred by the Entrez Gene records which de-
scribe the matched expressions
We obtained about 48,000,000 tokens of training
data automatically by using this process using all the
2009 MEDLINE data. This training data includes
about 3,000,000 gene mentions.
? ... in the following order: tna, gltC, gltS,
pyrE; gltR is located near ...
? The three genes concerned (designated
entA, entB and entC) ...
? Within the hypoglossal nucleus large
amounts of acetylcholinesterase (AChE)
activity are ...
Figure 4: False negative examples.
2.3 Training Data Expansion
In the previous section, we were able to obtain train-
ing data with high-precision by exploiting reference
information in the Entrez Gene. However, the result-
ing data include many false negatives (low-recall),
meaning that correct gene names in the data are
unannotated. Figure 4 presents an example of miss-
ing annotation. In this figure, all gene mentions
are shown in italic typeface. The underlined en-
tities were annotated by using the method in Sec-
tion 2.2, because they were in the Entrez Gene dic-
tionary and this MEDLINE abstract was referred by
these entities. However, the entities in italic type-
face with no underline were not annotated, because
these gene names in Entrez Gene have no link to
this MEDLINE abstract. Those expressions became
false negatives and became noise for learning. This
low-recall problem occurred because no guarantee
exists of exhaustiveness in Entrez Gene reference in-
formation.
To improve the low-recall while maintaining
high-precision, we focused on coordination struc-
tures. We assumed that coordinated noun phrases
belong to the same semantic class. Figure 5 portrays
the algorithm for the annotation expansion based
on coordination analysis. We expanded training
data annotation using this coordination analysis al-
gorithm to improve annotation recall. This algo-
rithm analyzes whether the words are reachable or
not through coordinate tokens such as ?,?, ?.?, or
?and? from initially annotated entities. If the words
are reachable and their entities are in the Entrez
Gene records (ignoring reference information), then
they are annotated.
68
Input: Sequence of sentence tokens S, Set of
symbols and conjunctions C, Dictionary with-
out reference D, Set of annotated tokens A
Output: Set of Annotated tokens A
begin
for i = 1 to |S| do
if S[i] ? A then
j ? i? 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j + 1] ? C do
A? A ? {S[j]}
j ? j ? 2
end while
j ? i + 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j ? 1] ? C do
A? A ? {S[j]}
j ? j + 2
end while
end if
end for
Output A
end
Figure 5: Coordination analysis algorithm.
2.4 Self-training
The method described in Section 2.3 reduces false
negatives based on coordination structures. How-
ever, the training data contain numerous false neg-
atives that cannot be solved through coordination
analysis. Therefore, we used a self-training algo-
rithm to automatically correct the training data. In
general, a self-training algorithm obtains training
data with a small amount of annotated data (seed)
and a vast amount of unlabeled text, iterating this
process (Zadeh Kaljahi, 2010):
1. Construct a classification model from a seed,
then apply the model on the unlabeled text.
2. Annotate recognized expressions as NEs.
3. Add the sentences which contain newly anno-
tated expressions to the seed.
In this way, a self-training algorithm obtains a huge
amount of training data.
Input: Labeled training data D, Machine
learning algorithm A, Iteration times n,
Threshold ?
Output: Training data Tn
begin
T0 ? A seed data from D
i? 0
D ? D\T0
while i 6= n do
Mi ? Construct model with Ti
U ? Sample some amount of data from D
L? Annotate U with model Mi
Unew ?Merge U with L if their confidence
values are larger than ?
Ti+1 ? Ti ? Unew
D ? D\U
i? i + 1
end while
Output Tn
end
Figure 6: Self-training algorithm.
In contrast, our case is that we have a large
amount of training data with numerous false neg-
atives. Therefore, we adapt a self-training algo-
rithm to revise the training data obtained using the
method described in Section 2.3. Figure 6 shows
the algorithm. We split the data set (D) obtained in
Section 2.3 into a seed set (T0) and remaining set
(D\T0). Then, we iterate the cycle (0 ? i ? n):
1. Construct a classification model (Mi) trained
on the training data (Ti).
2. Sample some amount of data (U ) from the re-
maining set (D).
3. Apply the model (Mi) on the sampled data (U ).
4. Annotate entities (L) recognized by this model.
5. Merge newly annotated expressions (L) with
expressions annotated in Section 2.3 (U ) if
their confidence values are larger than a thresh-
old (?).
6. Add the merged data (Unew) to the training data
(Ti).
69
In this study, we prepared seed data of 683,000 to-
kens (T0 in Figure 6). In each step, 227,000 tokens
were sampled from the remaining set (U ).
Because the remaining set U has high precision
and low recall, we need not revise NEs that were
annotated in Section 2.3. It might lower the qual-
ity of the training data to merge annotated entities,
thus we used confidence values (Huang and Riloff,
2010) to revise annotations. Therefore, we retain the
NE annotations of the remaining setU and overwrite
a span of a non-NE annotation only if the current
model predicts the span as an NE with high confi-
dence. We compute the confidence of the prediction
(f(x)) which a token x is predicted as label y as,
f(x) = s(x, y)?max(?z 6=ys(x, z)).
Here, s(x, y) denotes the score (the sum of feature
weights) computed using the SVM model described
in the beginning of Section 2. A confidence score
presents the difference of scores between the pre-
dicted (the best) label and the second-best label. The
confidence value is computed for each token label
prediction. If the confidence value is greater than
a threshold (?) and predicted as an NE of length 1
token (label S in IOBES notation), then we revise
the NE annotation. When a new NE with multiple
tokens (label B, I, or E in IOBES notation) is pre-
dicted, we revise the NE annotation if the average
of confidence values is larger than a threshold (?).
If a prediction suggests a new entity with multiple
tokens xi, ..., xj , then we calculate the average of
confidence values as
f(xi, ..., xj) =
1
j ? i + 1
j
?
k=i
f(xk).
The feature set presented in the beginning of Sec-
tion 2 uses information of the tokens themselves.
These features might overfit the noisy seed set, even
if we use regularization in training. Therefore, when
we use the algorithm of Figure 6, we do not gen-
erate token (w) features from tokens themselves but
only from tokens surrounding the current token. In
other words, we hide information from the tokens of
an entity, and learn models using information from
surrounding words.
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
svm 85.76 10.18 23.83 14.27
+ reference 93.74 69.25 39.12 50.00
+ coordination 93.97 66.79 47.44 55.47
+ self-training 93.98 63.72 51.18 56.77
Table 3: Evaluation results.
3 Experiment
The training data automatically generated using the
proposed method have about 48,000,000 tokens and
3,000,000 gene mentions. However, we used only
about 10% of this data because of the computational
cost. For evaluation, we chose to use the BioNLP
2011 Shared Task EPI corpus and evaluation mea-
sures described in Section 2.1.
3.1 Evaluation of Proposed Methods
In the previous section, we proposed three methods
for automatic training data acquisition. We first in-
vestigate the effect of these methods on the perfor-
mance of NER. Table 3 presents evaluation results.
The first method ?dictionary matching? simply
performs exact string matching with the Entrez Gene
dictionary on the evaluation corpus. It achieves a
40.78 F1-measure; this F1-measure will be used as
the baseline performance. The second method, as
described in Section 2.1, ?svm? uses training data
generated automatically from the Entrez Gene and
unlabeled texts without reference information of the
Entrez Gene. The third method, ?+ reference? ex-
ploits the reference information of the Entrez Gene.
This method drastically improves the performance.
As shown in Table 3, this model achieves the highest
precision (69.25%) with comparable recall (39.12%)
to the baseline model with a 50.00 F1-measure. The
fourth method, ?+ coordination?, uses coordination
analysis results to expand the initial automatic an-
notation. Compared to the ?+ reference? model, the
annotation expansion based on coordination analy-
sis greatly improves the recall (+8.32%) with only
a slight decrease of the precision (-2.46%). The
last method ?+ self-training? applies a self-training
technique to improve the performance further. This
model achieves the highest recall (51.18%) among
all models with a reasonable cost in the precision.
70
Figure 7: Results of self-training.
To analyze the effect of self-training, we evalu-
ated the performance of this model for each itera-
tion. Figure 7 shows the F1-measure of the model
as iterations increase. The performance improved
gradually. It did not converge even for the last iter-
ation. The size of the training data at the 17th itera-
tion was used in Table 3 experiment. It is the same
to the size of the training data for other methods.
3.2 Comparison with a Manually Annotated
Corpus
NER systems achieving state-of-the-art performance
are based mostly on supervised machine learn-
ing trained on manually annotated corpus. In
this section, we present a comparison of our best-
performing NER model with a NER model trained
on manually annotated corpus. In addition to the
performance comparison, we investigate how much
manually annotated data is necessary to outperform
our best-performing system. In this experiment, we
used only the development data for evaluation be-
cause the training data are used for training the NER
model.
We split the training data of EPI corpus randomly
into 20 pieces and evaluated the performance of
the conventional NER system as the size of manu-
ally annotated corpus increases. Figure 8 presents
the evaluation results. The performance of our our
best-performing NER is a 62.66 F1-measure; this
is shown as horizontal line in Figure 8. The NER
model trained on the all training data of EPI cor-
Figure 8: Manual annotation vs. our method.
pus achieves a 67.89 F1-measure. The result shows
that our best-performing models achieve compara-
ble performance to that of the NER model when us-
ing about 40% (60,000 tokens, 2,000 sentences) of
the manually annotated corpus.
3.3 Discussion
Although the proposed methods help us to obtain
training data automatically with reasonably high
quality, we found some shortcomings in these meth-
ods. For example, the annotation expansion method
based on coordination analysis might find new enti-
ties in the training data precisely. However, it was
insufficient in the following case.
tna loci, in the following order: tna, gltC,
gltS, pyrE; gltR is located near ...
In this example, all gene mentions are shown in
italic typeface. The words with underline were ini-
tial annotation with reference information. The sur-
rounding words represented in italic typeface are an-
notated by annotation expansion with coordination
analysis. Here, the first word ?tna? shown in italic
typeface in this example is not annotated, although
its second mention is annotated at the annotation ex-
pansion step. We might apply the one sense per dis-
course (Gale et al, 1992) heuristic to label this case.
Second, the improvement of self-training tech-
niques elicited less than a 1.0 F1-measure. To as-
certain the reason for this small improvement, we
analyzed the distribution of entity length both origi-
71
Original
Added
0% 25% 50% 75% 100%
Length 1 Length 2 Length 3 More than 4
Figure 9: Distribution of entity length.
nally included entities and newly added entities dur-
ing self-training, as shown in Figure 9. They repre-
sent the ratio of entity length to the number of total
entities. Figure 9 shows the added distribution of
entity length (Added) differs from the original one
(Original). Results of this analysis show that self-
training mainly annotates entities of the length one
and barely recognizes entities of the length two or
more. It might be necessary to devise a means to fol-
low the corpus statistics of the ratio among the num-
ber of entities of different length as the self-training
iteration proceeds.
4 Related Work
Our study focuses mainly on achieving high per-
formance NER without manual annotation. Several
previous studies aimed at reducing the cost of man-
ual annotations.
Vlachos and Gasperin (2006) obtained noisy
training data from FlyBase1 with few manually an-
notated abstracts from FlyBase. This study sug-
gested the possibility of acquiring high-quality train-
ing data from noisy training data. It used a boot-
strapping method and a highly context-based classi-
fiers to increase the number of NE mentions in the
training data. Even though the method achieved a
high-performance NER in the biomedical domain, it
requires curated seed data.
Whitelaw et al (2008) attempted to create ex-
tremely huge training data from the Web using a
seed set of entities and relations. In generating train-
ing data automatically, this study used context-based
tagging. They reported that quite a few good re-
sources (e.g., Wikipedia2) listed entities for obtain-
ing training data automatically.
1http://flybase.org/
2http://www.wikipedia.org/
Muramoto et al (2010) attempted to create train-
ing data from Wikipedia as a lexical database and
blogs as unlabeled text. It collected about one mil-
lion entities from these sources, but they did not re-
port the performance of the NER in their paper.
5 Conclusions
This paper described an approach to the acquisi-
tion of huge amounts of training data for high-
performance Bio NER automatically from a lexical
database and unlabeled text. The results demon-
strated that the proposed method outperformed
dictionary-based NER. Utilization of reference in-
formation greatly improved its precision. Using co-
ordination analysis to expand annotation increased
recall with slightly decreased precision. Moreover,
self-training techniques raised recall. All strategies
presented in the paper contributed greatly to the
NER performance.
We showed that the self-training algorithm
skewed the length distribution of NEs. We plan
to improve the criteria for adding NEs during self-
training. Although we obtained a huge amount of
training data by using the proposed method, we
could not utilize all of acquired training data be-
cause they did not fit into the main memory. A fu-
ture direction for avoiding this limitation is to em-
ploy an online learning algorithm (Tong and Koller,
2002; Langford et al, 2009), where updates of fea-
ture weights are done for each training instance. The
necessity of coordination handling and self-training
originates from the insufficiency of reference infor-
mation in the lexical database, which was not de-
signed to be comprehensive. Therefore, establish-
ing missing reference information from a lexical
database to unlabeled texts may provide another so-
lution for improving the recall of the training data.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233?237.
72
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275?285.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proceedings of the 19th international conference on
Computational linguistics - Volume 1, pages 1?7.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector ma-
chines for biomedical named entity recognition. In
Proceedings of the ACL-02 workshop on Natural lan-
guage processing in the biomedical domain - Volume
3, pages 1?8.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. J. Mach.
Learn. Res., 10:777?801.
Hideki Muramoto, Nobuhiro Kaji, Naoki Suenaga, and
Masaru Kitsuregawa. 2010. Learning semantic cat-
egory tagger from unlabeled data. In The Fifth NLP
Symposium for Yung Researchers. (in Japanese).
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30(1):3?26.
National Library of Medicine. 2005. Entrez Gene. avail-
able at http://www.ncbi.nlm.nih.gov/gene.
National Library of Medicine. 2009. MEDLINE. avail-
able at http://www.ncbi.nlm.nih.gov/.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
genetag-style annotation to genia corpus. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 106?107.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 147?155.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications, pages 104?107.
SIGBioMed. 2011. BioNLP 2011 Shared Task.
http://sites.google.com/site/bionlpst/.
Lorraine K. Tanabe and W. John Wilbur. 2002. Tagging
gene and protein names in biomedical text. Bioin-
formatics/computer Applications in The Biosciences,
18:1124?1132.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun ?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, volume 3746, pages 382?392.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
Jason Weston and Chris Watkins. 1999. Support vec-
tor machines for multi-class pattern recognition. In
ESANN?99, pages 219?224.
Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,
and Lyle Ungar. 2008. Web-scale named entity recog-
nition. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 123?
132.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. Biocreative task 1a: gene
mention finding evaluation. BMC Bioinformatics,
6(1):S2.
Rasoul Samad Zadeh Kaljahi. 2010. Adapting self-
training for semantic role labeling. In Proceedings of
the ACL 2010 Student Research Workshop, pages 91?
96.
73
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 105?113,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
From Pathways to Biomolecular Events: Opportunities and Challenges
Tomoko Ohta? Sampo Pyysalo? Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Microsoft Research Asia, Beijing, China
{okap,smp}@is.s.u-tokyo.ac.jp, jtsujii@microsoft.com
Abstract
The construction of pathways is a major fo-
cus of present-day biology. Typical pathways
involve large numbers of entities of various
types whose associations are represented as
reactions involving arbitrary numbers of reac-
tants, outputs and modifiers. Until recently,
few information extraction approaches were
capable of resolving the level of detail in text
required to support the annotation of such
pathway representations. We argue that event
representations of the type popularized by the
BioNLP Shared Task are potentially applica-
ble for pathway annotation support. As a step
toward realizing this possibility, we study the
mapping from a formal pathway representa-
tion to the event representation in order to
identify remaining challenges in event extrac-
tion for pathway annotation support. Follow-
ing initial analysis, we present a detailed study
of protein association and dissociation reac-
tions, proposing a new event class and repre-
sentation for the latter and, as a step toward
its automatic extraction, introduce a manu-
ally annotated resource incorporating the type
among a total of nearly 1300 annotated event
instances. As a further practical contribu-
tion, we introduce the first pathway-to-event
conversion software for SBML/CellDesigner
pathways and discuss the opportunities arising
from the ability to convert the substantial ex-
isting pathway resources to events.
1 Introduction
For most of the previous decade of biomedical in-
formation extraction (IE), efforts have focused on
foundational tasks such as named entity detection
and their database normalization (Krallinger et al,
2008) and simple IE targets, most commonly bi-
nary entity relations representing associations such
as protein-protein interactions (Pyysalo et al, 2008;
Tikk et al, 2010). In recent years, an increasing
number of resources and methods pursuing more de-
tailed representations of extracted information are
becoming available (Pyysalo et al, 2007; Kim et al,
2008; Thompson et al, 2009; Bjo?rne et al, 2010).
The main thrust of this move toward structured, fine-
grained information extraction falls under the head-
ing of event extraction (Ananiadou et al, 2010), an
approach popularized and represented in particular
by the BioNLP Shared Task (BioNLP ST) (Kim et
al., 2009a; Kim et al, 2011).
While a detailed representation of extracted in-
formation on biomolecular events has several po-
tential applications ranging from semantic search to
database curation support (Ananiadou et al, 2010),
the number of practical applications making use of
this technology has arguably so far been rather lim-
ited. In this study, we pursue in particular the op-
portunities that event extraction holds for pathway
annotation support,1 arguing that the match between
1Throughout this paper, we call the projected task pathway
annotation support. There is no established task with this label,
and we do not envision this to be a specific single task. Rather,
we intend the term to refer to a set of tasks where information
extraction/text mining methods are applied in some role to con-
tribute directly to pathway curation, including, for example, the
identification of specific texts in the literature relevant to anno-
tated reactions, the automatic suggestion of further entities or
reactions to add to a pathway, or even the fully automatic gen-
eration of entire pathways from scratch.
105
representations that biologists employ to capture re-
actions between biomolecules in pathways and the
event representation of the BioNLP ST task makes
pathway-oriented applications a potential ?killer ap-
plication? for event extraction technology.
The fit between these representations is not ac-
cidental ? the design of the BioNLP ST event rep-
resentation has been informed by that of popular
pathway models ? nor is it novel to suggest to sup-
port pathway extraction through information meth-
ods in general (see e.g. (Rzhetsky et al, 2004))
or through event extraction specifically (Oda et al,
2008). However, our study differs from previous
efforts in two key aspects. First, instead of being
driven by information extraction and defining a rep-
resentation fitting its results, we specifically adopt
the perspective and model of a widely applied stan-
dard database representation and proceed from the
pathway to events in text. Second, while previous
work on event extraction for pathway annotation has
been exploratory in nature or has otherwise had lim-
ited practical impact, we introduce and release a
first software implementation of a conversion from
a standard pathway format to the event format, thus
making a large amount of pathway data available for
use in event extraction and taking a concrete step
toward reliable, routine mappings between the two
representations.
2 Representations and Resources
Before proceeding to consider the mapping between
the two, we first briefly introduce the pathway and
event representations in focus in this study and the
applied pathway resources.
2.1 Pathways
The biomolecular curation community has created
and made available an enormous amount of path-
way resources: for example, as of April 2011, the
Pathguide pathway resource list2 includes references
to 325 pathway-related resources ? many of which
are themselves pathway databases containing hun-
dreds of individual models. These resources in-
volve a formidable variety of different, largely inde-
pendently developed formats and representations of
which only few pairs have tools supporting mutual
2http://www.pathguide.org/
conversion. To address the challenges of interoper-
ability that this diversity implies, a number of stan-
dardization efforts for pathway representations have
been introduced.
In this work, we consider two widely adopted
pathway representation formats: Systems Biol-
ogy Markup Language (SBML)3 (Hucka et al,
2003) and Biological Pathway Exchange (BioPAX)4
(Demir et al, 2010). SBML is an XML-based
machine-readable data exchange format that sup-
ports a formal mathematical representation of chem-
ical reactions (including e.g. kinetic parameters),
allowing biochemical simulation. BioPAX is an
RDF/OWL-based standard language to represent
bio-molecular and cellular networks designed to en-
able data integration, exchange, visualization and
analysis. Despite significantly different choices in
storage format, the represented information content
of the two is broadly compatible. In the follow-
ing, we refer to established correspondences and
mappings when relating the two (see e.g. (Mi and
Thomas, 2009)).
As an interchange format aimed to support a large
variety of specific representations, the SBML stan-
dard itself does not define a fixed set of types of
physical entities or biochemical reactions. However,
the standard defines an extension mechanism allow-
ing additional information, including such types, to
be defined. As specific, fixed types with established
semantics are a requirement for practical conversion
between the different representations, we thus rely
in this work not only on SBML core, but also a min-
imal set of the extensions introduced by the popu-
lar CellDesigner pathway modeling tool (Funahashi
et al, 2008). In the following, we assume through-
out the availability of CellDesigner extensions when
discussing SBML features.
For pathway data, in this study we use the full
set of pathways contained in the Panther and Payao
pathway repositories in SBML form. Panther (Pro-
tein ANalysis THrough Evolutionary Relationships)
is a gene function-based classification system that
hosts a large collection of pathways. The Panther
repository consists of 165 pathways, including 153
signaling and 12 metabolic pathways. All pathways
3http://sbml.org
4http://www.biopax.org
106
Figure 1: Illustration of the event representation.
were drawn on CellDesigner by manual curation
and thus include CellDesigner SBML extensions
(Mi and Thomas, 2009). Payao is a community-
based SBML model tagging platform (Matsuoka et
al., 2010) that allows a community to share models,
tag and add comments, and search relevant literature
(Kemper et al, 2010). Currently, 28 models are reg-
istered in Payao. As in Panther, all Payao pathways
include CellDesigner extensions.
2.2 Event Representation
The application of event representations in biomed-
ical IE is a relatively recent development, follow-
ing the introduction of corpus resources annotating
structured, n-ary associations of entities with de-
tailed types (Pyysalo et al, 2007; Kim et al, 2008;
Thompson et al, 2009)) and popularized in particu-
lar by the BioNLP Shared Task (BioNLP ST) events
(Kim et al, 2009b; Kim et al, 2011). In this pa-
per, we use event in the BioNLP ST sense, to refer
specifically to the representation where each event
is assigned a type from a fixed ontology, bound to a
specific expression in text stating its occurrence (the
trigger or text binding), and associated with an ar-
bitrary number of participants (similarly text-bound
entities or other events), for which the roles in which
they are involved in the event are defined from a
fixed small inventory of event argument types (e.g.
Theme, Cause, Site). These concepts are illustrated
in Figure 1.
3 Analysis of Pathway-Event Mapping
We next present an analysis of the relationship be-
tween the two representations, considering features
required from IE systems for efficient support of
pathway annotation support.
We assume throughout that the target on the path-
way side is restricted to the broad, central biologi-
cal content of pathways, excluding information only
related to e.g. simulation support or pathway visual-
ization/layout.
Figure 2: Illustration of a generalized pathway reaction.
3.1 Top-level concepts
Both SBML and BioPAX involve two (largely com-
parable) top-level concepts that form the core of the
representation: entity (species/physical entity) and
reaction (interaction). In the following we focus pri-
marily on entities and reactions, deferring consider-
ation of detailed concepts such as modification state
and compartment localization to Section 3.3.
The concept of a reaction in the considered path-
way representations centrally involves three sets of
entities: reactants, products, and modifiers. As the
names suggest, the reaction produces the set of prod-
uct entities from the reactant entities and is affected
by the modifiers. Figure 2 shows an illustration of a
generalized reaction. Pathway reactions find a rea-
sonably good analogy in events in the event repre-
sentation. While the event representation does not
differentiate ?reactants? from ?products? in these
terms, the roles assigned to event participants al-
low comparable interpretation. There is no single
concept in the event representation directly compa-
rable to reaction modifiers. However, the semantics
of specific modification types (see Section 3.3) cor-
respond broadly to those of regulation in the event
representation, suggesting that modification be rep-
resented using a separate event of the appropriate
type with the modifying entities participating in the
Cause role (Kim et al, 2008). Figure 3 illustrates the
event structure proposed to correspond to the reac-
tion of Figure 2, with the added assumptions that the
reaction and modification types (unspecified in Fig-
ure 2) are Association (BioPAX:ComplexAssembly)
and Modulation (BioPAX:Control).
107
Figure 3: Illustration of a generalized event structure with four entities and two events (REGULATION and BINDING).
Note that the text is only present as filler to satisfy the requirement that events are bound to specific expressions in
text. The Product role is not a standard role in event representation but newly proposed in this study.
Pathway Event
CellDesigner BioPAX ST?09 ST?11 GENIA
Protein Protein Protein Protein Protein
RNA RNA Protein Protein RNA
AntiSenseRNA RNA Protein Protein RNA
Gene DNA Protein Protein DNA
Simple molecule Small molecule - Chemical Inorganic compound
Ion Small molecule - Chemical Inorganic compound
Drug PhysicalEntity - Chemical Inorganic compound
Hetero/homodimer Complex - - Protein complex
Table 1: Entity type comparison between pathways and events.
The mapping of top-level concepts that we con-
sider thus unifies physical entities in pathways with
the entities of the BioNLP ST representation, and
pathway reaction with event.5
To be able to efficiently support (some aspect of)
pathway annotation through IE, the applied extrac-
tion model should be able, for both entities and reac-
tions, to 1) recognize mentions of all relevant types
of entity/reaction and 2) differentiate between en-
tity/reaction types at the same or finer granularity as
the pathway representation. For example, an IE sys-
tem that does not detect mentions of protein com-
plexes cannot efficiently support aspects of pathway
annotation that involve this type; a system that de-
tects proteins and complexes with no distinction be-
tween the two will be similarly limited. In the fol-
lowing, we consider entity and reaction types sep-
arately to determine to what extent these require-
ments are filled by presently available resources for
event extraction, in particular the GENIA corpus
(Kim et al, 2008) and the BioNLP ST 2009 (Kim
et al, 2009b) and 2011 corpora.
5Pathways and IE/text mining use many of the same terms
with (sometimes subtly) different meanings. We use largely IE
terminology, using e.g. entity instead of species (SBML) and
entity type instead of physical entity class (BioPAX) / species
type (SBML) For the pathway associations, we have adopted
reaction (SBML term) in favor of interaction (BioPAX). With
event, we refer to the BioNLP ST sense of the word; we make
no use of the SBML ?event? concept.
3.2 Entities
Table 1 shows a comparison of the primary entity
types between SBML/CellDesigner, BioPAX, and
the event representations. There is significant dif-
ference in the resolution of gene and gene product
types between the pathway representations and that
applied in ST?09 and ST?11: while both pathway
representations and the GENIA corpus differenti-
ate the DNA, RNA and protein forms, the STs fold
the three types into a single one, PROTEIN.6 The
CHEMICAL type defined in ST?11 (ID task) overlaps
largely with BioPAX SMALL MOLECULE, a type
that SBML/CellDesigner further splits into two spe-
cific types, and further partly covers the definition of
the SBML/CellDesigner type Drug. The same holds
(with somewhat less specificity) for GENIA INOR-
GANIC COMPOUND. Finally, although annotated in
GENIA, the category of protein complexes has no
correspondence among the entities considered in the
BioNLP ST representation.
Thus, information extraction systems applying
the core BioNLP ST entity types will entirely lack
coverage for protein complexes and will not be able
6While the term PROTEIN appears to suggest that the class
consists only of protein forms, these entities are in fact anno-
tated in the BioNLP ST data according to the GENIA gene/gene
product guidelines (Ohta et al, 2009) and thus include also
DNA and RNA forms. The type could arguably more accurately
be named GENE OR GENE PRODUCT.
108
Pathway Event
CellDesigner BioPAX ST?09 ST?11 GENIA
State transition BiochemicalReaction (see Table 3)
Truncation BiochemicalReaction Catabolism Catabolism Catabolism
Transcription BiochemicalReaction Transcription Transcription Transcription
Translation BiochemicalReaction - - Translation
Association ComplexAssembly Binding Binding Binding
Dissociation ComplexAssembly - - -
Transport Transport w/reaction Localization Localization Localization
Degradation Degradation Catabolism Catabolism Catabolism
Catalysis Catalysis Positive regulation Positive regulation Positive regulation
Physical stimulation Control Positive regulation Positive regulation Positive regulation
Modulation Control Regulation Regulation Regulation
Trigger Control Positive regulation Positive regulation Positive regulation
Inhibition Control Negative regulation Negative regulation Negative regulation
Table 2: Reaction type comparison between pathways and events.
to fully resolve the detailed type of gene and gene
product types applied in the pathway representa-
tions. While these distinctions exist in the full GE-
NIA corpus, it has not been frequently applied in
event extraction in its complete form and is un-
likely to be adopted over the widely applied ST
resources. Finally, none of the event representa-
tions differentiate the pathway small molecule/drug
types. We discuss the implications of these ambigu-
ities in detail below. By contrast, we note that both
SBML/CellDesigner and BioPAX entity types cover
the scope of the major BioNLP ST types and have
comparable or finer granularity in each case.
3.3 Reactions
Table 2 shows a comparison between the reaction
types of the two considered pathway representations
and those of the BioNLP ST event representation.
The full semantics of the generic reaction type State
transition (BioPAX: BiochemicalReaction) cannot
be resolved from the type alone; we defer discussion
of this type.
Contrary to the event types, we find that for re-
action types even the least comprehensive BioNLP
ST?09 event representation has high coverage of the
pathway reaction types as well as a largely compa-
rable level of granularity in its types. While neither
of the BioNLP ST models defines a TRANSLATION
type, the adoption of the GENIA representation ?
matching that for TRANSCRIPTION ? for this simple
and relatively rare event type would likely be rela-
tively straightforward. A more substantial omission
in all of the event representations is the lack of a
Dissociation event type. As dissociation is the ?re-
verse? reaction of (protein) BINDING and central to
many pathways, its omission from the event model
is both surprising as well as potentially limiting for
applications of event extraction to pathway annota-
tion support.
The detailed resolution of pathway reactions pro-
vided by the event types has implications on the
impact of the ambiguity noted between the sin-
gle type covering genes and gene products in the
event representation as opposed to the distinct
DNA/RNA/protein types applied in the pathways.
Arguably, for many practical cases the specific type
of an entity of the broad gene/gene product type is
unambiguously resolved by the events it participates
in: for example, any gene/gene product that is mod-
ified through phosphorylation (or similar reaction)
is necessarily a protein.7 Similarly, only proteins
will be involved in e.g. localization between nucleus
and cytoplasm. On a more detailed level, BIND-
ING events resolves their arguments in part through
their Site argument: binding to a promoter implies
DNA, while binding to a C-terminus implies pro-
tein. Thus, we can (with some reservation) forward
the argument that it is not necessary to disambiguate
all gene/gene product mentions on the entity level
for pathway annotation support, and that success-
ful event extraction will provide disambiguation in
cases where the distinction matters.
7DNA methylation notwithstanding; the BioNLP ST?11 EPI
task demonstrated that protein and DNA methylation can be dis-
ambiguated on the event type level without entity type distinc-
tions.
109
Pathway Event
SBML/CellDesigner ST?09 ST?11 GENIA
in:Compartment1 ? in:Compartment2 Localization Localization Localization
residue:state:? ? residue:state:Phosphorylated Phosphorylation Phosphorylation Phosphorylation
residue:state:Phosphorylated ? residue:state:? - Dephosphorylation Dephosphorylation
residue:state:? ? residue:state:Methylated - Methylation Methylation
residue:state:Methylated ? residue:state:? - Demethylation Demethylation
residue:state:? ? residue:state:Ubiquitinated - Ubiquitination Ubiquitination
residue:state:Ubiquitinated ? residue:state:? - Deubiquitination Deubiquitination
species:state:inactive ? species:state:active Positive regulation Positive regulation Positive regulation
species:state:active ? species:state:inactive Negative regulation Negative regulation Negative regulation
Table 3: Interpretation and comparison of state transitions.
Finally, the pathway representations de-
fine generic reaction types (State transi-
tion/BiochemicalReaction) that do not alone
have specific interpretations. To resolve the event
involved in these reactions it is necessary to com-
pare the state of the reactants against that of the
matching products. Table 3 shows how specific state
transitions map to event types (this detailed compar-
ison was performed only for SBML/CellDesigner
pathways). We find here a good correspondence for
transitions affecting a single aspect of entity state.
While generic pathway transitions can change any
number of such aspects, we suggest that decomposi-
tion into events where one event corresponds to one
point change in state is a reasonable approximation
of the biological interpretation: for example, a reac-
tion changing one residue state into Methylated and
another into Phosphorylated would map into two
events, METHYLATION and PHOSPHORYLATION.
In summary of the preceding comparison of the
core pathway and event representations, we found
that in addition to additional ambiguity in e.g. gene
and gene product types, the popular BioNLP ST rep-
resentations lack a protein complex type and further
that none of the considered event models define a
(protein) dissociation event. To address these latter
omissions, we present in the following section a case
study of dissociation reactions as a step toward their
automatic extraction. We further noted that pathway
types cover the event types well and have similar or
higher granularity in nearly all instances. This sug-
gests to us that mapping from the pathway repre-
sentation to events is more straightforward than vice
versa. To follow up on these opportunities, we intro-
duce such a mapping in Section 5, in following the
correspondences outlined above.
4 Protein Association and Dissociation
In the analysis presented above, we noted a major re-
action type defined in both considered pathway rep-
resentations that had no equivalent in the event rep-
resentation: dissociation. In this section, we present
a study of this reaction type and its expression as
statements in text through the creation of event-style
annotation for dissociation statements.
4.1 Target data
Among the large set of pathways available, we chose
to focus on the Payao mTOR pathway (Caron et al,
2010) because it is a large, recently introduced path-
way with high-quality annotations that involves nu-
merous dissociation reactions. The Payao pathways
are further annotated with detailed literature refer-
ences, providing a PubMed citation for nearly each
entity and reaction in the pathway. To acquire texts
for event annotation, we followed the references in
the pathway annotation and retrieved the full set of
PubMed abstracts associated with the pathway, over
400 in total. We then annotated 60 of these abstracts
that were either marked as relevant to dissociation
events in the pathway or were found to include dis-
sociation statements in manual analysis. These ab-
stracts were not included in any previously anno-
tated domain corpus. Further, as we aimed specifi-
cally to be able to identify event structures for which
no previous annotations exist, we could not rely on
(initial) automatic annotation.
4.2 Annotation guidelines
We performed exhaustive manual entity and event
annotation in the event representation for the se-
lected 60 abstracts. For entity annotation, we ini-
110
tially considered adopting the gene/gene product an-
notation guidelines (Ohta et al, 2009) applied in
the BioNLP ST 2009 as well as in the majority
of the 2011 tasks. However, the requirement of
these guidelines to mark only specific gene/protein
names would exclude a substantial number of the
entities marked in the pathway, as many refer to
gene/protein families or groups instead of specific
individual genes or proteins. We thus chose to adopt
the pathway annotation itself for defining the scope
of our entity annotation: we generated a listing of all
the names appearing in the target pathway and an-
notated their mentions, extrapolating from this rich
set of examples to guide us in decisions on how to
annotate references to entities not appearing in the
pathway. For event annotation, we adapted the GE-
NIA event corpus annotation guidelines (Kim et al,
2008), further developing a specific representation
and guidelines for annotating dissociation events
based on an early iteration of exploratory annotation.
Annotation was performed by a single biology
PhD with extensive experience in event annotation
(TO). While we could thus not directly assess inter-
annotator consistency, we note that our recent com-
parable efforts have been evaluated by comparing
independently created annotations at approximately
90% F-score for entity annotations and approxi-
mately 80% F-score for event annotations (BioNLP
Shared Task primary evaluation criteria) (Pyysalo et
al., 2011; Ohta et al, 2011).
4.3 Representing Association and Dissociation
Based on our analysis of 107 protein dissociation
statements annotated in the corpus and a correspond-
ing study of the ?reverse?, statements of protein as-
sociation in the corpus, we propose the following
extensions for the BioNLP ST event representation.
First, the introduction of the event type DISSOCIA-
TION, taking as its primary argument a single Theme
identifying a participating entity of the type COM-
PLEX. Second, we propose the new role type Prod-
uct, in the annotation of DISSOCIATION events an
optional (secondary) argument identifying the PRO-
TEIN entities that are released in the dissociation
event. This argument should be annotated (or ex-
tracted) only when explicitly stated in text. Third,
for symmetry in the representation, more detail in
extracted information, and to have a representation
Figure 4: Examples annotated with the proposed event
representation for DISSOCIATION and BINDING events
with the proposed Product role marking formed complex.
Item Count
Abstract 60
Word 11960
Protein 1483
Complex 201
Event 1284
Table 4: Annotation statistics.
more compatible with the pathway representation
for protein associations, we propose to extend the
representation for BINDING, adding Product as an
optional argument identifying a COMPLEX partici-
pant in BINDING events marking statements of com-
plex formation stating the complex. The extended
event representations are illustrated in Figure 4.
4.4 Annotation statistics
Table 4 presents the statistics of the created annota-
tion. While covering a relatively modest number of
abstracts, the annotation density is very high, relat-
ing perhaps in part to the fact that many of the ref-
erenced documents are reviews condensing a wealth
of information into the abstract.
5 Pathway-to-event conversion
As an additional practical contribution and out-
come of our analysis of the mapping from the path-
way representation to the event representation, we
created software implementing this mapping from
SBML with CellDesigner extensions to the event
representation. This conversion otherwise follows
111
the conventions of the event model, but lacks spe-
cific text bindings for the mentioned entities and
event expressions (triggers). To maximize the appli-
cability of the conversion, we chose to forgo e.g. the
CellDesigner plugin architecture and to instead cre-
ate an entirely standalone software based on python
and libxml2. We tested this conversion on the 165
Panther and 28 Payao pathways to assure its robust-
ness.
Conversion from pathways into the event repre-
sentation opens up a number of opportunities, such
as the ability to directly query large-scale event
repositories (e.g. (Bjo?rne et al, 2010)) for specific
pathway reactions. For pathways where reactions
are marked with literature references, conversion
further allows event annotations relevant to specific
documents to be created automatically, sparing man-
ual annotation costs. While such event annotations
will not be bound to specific text expressions, they
could be used through the application of techniques
such as distant supervision (Mintz et al, 2009). As a
first attempt, the conversion introduced in this work
is limited in a number of ways, but we hope it can
serve as a starting point for both wider adoption
of pathway resources for event extraction and fur-
ther research into accurate conversions between the
two. The conversion software, SBML-to-event,
is freely available for research purposes.
6 Discussion and Conclusions
Over the last decade, the bio-community has in-
vested enormous efforts in the construction of de-
tailed models of the function of a large variety of bi-
ological systems in the form of pathways. These ef-
forts toward building systemic understanding of the
functioning of organisms remain a central focus of
present-day biology, and their support through infor-
mation extraction and text mining perhaps the great-
est potential contribution that the biomedical natural
language processing community could make toward
the broader bio-community.
We have argued that while recent developments
in BioNLP are highly promising for approaching
practical support of pathway annotation through in-
formation extraction, the BioNLP community has
not yet made the most of the substantial resources
in the form of existing pathways and that pursu-
ing mapping from pathways to the event represen-
tation might be both more realistic and more fruit-
ful than the other way around. As a first step in
what we hope will lead to broadened understand-
ing of the different perspectives, communication be-
tween the communities, and better uses resources,
we have introduced a fully automatic mapping from
SBML/CellDesigner pathways into the BioNLP ST-
style event representation. As a first effort this map-
ping has many limitations and imperfections that we
hope the BioNLP community will take as a chal-
lenge to do better.
Noting in analysis that dissociation reactions are
not covered in previously proposed event represen-
tations, we also presented a detailed case study fo-
cusing on statements describing protein association
and dissociation reactions in PubMed abstracts rel-
evant to the mTOR pathway. Based on exploratory
annotation, we proposed a novel event class DIS-
SOCIATION, thus taking a step toward covering this
arguably most significant omission in the event rep-
resentation.
The pathway-bound event annotations created
in this study, exhaustive annotation of all rel-
evant entities and events in 60 abstracts, con-
sist in total of annotation identifying nearly
1500 protein and 200 complex mentions and
over 1200 events involving these entities in text.
These annotations are freely available for use
in research at http://www-tsujii.is.s.
u-tokyo.ac.jp/GENIA.
Acknowledgments
We would like to thank Hiroaki Kitano, Yukiko Mat-
suoka and Samik Ghosh of the Systems Biology In-
stitute for their generosity in providing their time
and expertise in helping us understand the CellDe-
signer and SBML pathway representations. This
work was partially supported by Grant-in-Aid for
Specially Promoted Research (MEXT, Japan).
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
112
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
E. Caron, S. Ghosh, Y. Matsuoka, D. Ashton-Beaucage,
M. Therrien, S. Lemieux, C. Perreault, P.P. Roux, and
H. Kitano. 2010. A comprehensive map of the mTOR
signaling network. Molecular Systems Biology, 6(1).
E. Demir, M.P. Cary, S. Paley, K. Fukuda, C. Lemer,
I. Vastrik, G. Wu, P. D?Eustachio, C. Schaefer, J. Lu-
ciano, et al 2010. The BioPAX community stan-
dard for pathway data sharing. Nature biotechnology,
28(9):935?942.
A. Funahashi, Y. Matsuoka, A. Jouraku, M. Morohashi,
N. Kikuchi, and H. Kitano. 2008. CellDesigner 3.5:
a versatile modeling tool for biochemical networks.
Proceedings of the IEEE, 96(8):1254?1265.
M. Hucka, A. Finney, H. M. Sauro, H. Bolouri, J. C.
Doyle, and H Kitano et al 2003. The systems biol-
ogy markup language (SBML): a medium for repre-
sentation and exchange of biochemical network mod-
els. Bioinformatics, 19(4):524?531.
B. Kemper, T. Matsuzaki, Y. Matsuoka, Y. Tsuruoka,
H. Kitano, S. Ananiadou, and J. Tsujii. 2010. Path-
Text: a text mining integrator for biological pathway
visualizations. Bioinformatics, 26(12):i374.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009a. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP 2009 Shared Task.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009b. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of BioNLP 2011.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
Y. Matsuoka, S. Ghosh, N. Kikuchi, and H. Kitano. 2010.
Payao: a community platform for SBML pathway
model curation. Bioinformatics, 26(10):1381.
H. Mi and P. Thomas. 2009. PANTHER pathway: an
ontology-based pathway database coupled with data
analysis tools. Methods Mol. Biol, 563:123?140.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL?09, pages
1003?1011.
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and
Jun?ichi Tsujii. 2008. New challenges for text min-
ing: Mapping between text and manually curated path-
ways. BMC Bioinformatics, 9(Suppl 3):S5.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of BioNLP 2011.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Antti Airola, Juho Heimonen, and Jari
Bjo?rne. 2008. Comparative analysis of five protein-
protein interaction corpora. BMC Bioinformatics,
9(Suppl. 3):S6.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of BioNLP
2011.
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Morris,
Hong Yu, Pablo Ariel Duboue?, Wubin Weng, W. John
Wilbur, Vasileios Hatzivassiloglou, and Carol Fried-
man. 2004. GeneWays: A system for extracting, ana-
lyzing, visualizing, and integrating molecular pathway
data. Journal of Biomedical Informatics, 37(1):43?53.
Paul Thompson, Syed Iqbal, John McNaught, and Sophia
Ananiadou. 2009. Construction of an annotated
corpus to support biomedical information extraction.
BMC Bioinformatics, 10(1):349.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehen-
sive benchmark of kernel methods to extract protein-
protein interactions from literature. PLoS Comput
Biol, 6(7):e1000837, 07.
113
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 114?123,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Towards Exhaustive Protein Modification Event Extraction
Sampo Pyysalo? Tomoko Ohta? Makoto Miwa? Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Microsoft Research Asia, Beijing, China
{smp,okap,mmiwa}@is.s.u-tokyo.ac.jp, jtsujii@microsoft.com
Abstract
Protein modifications, in particular post-
translational modifications, have a central role
in bringing about the full repertoire of pro-
tein functions, and the identification of spe-
cific protein modifications is important for
understanding biological systems. This task
presents a number of opportunities for the au-
tomatic support of manual curation efforts.
However, the sheer number of different types
of protein modifications is a daunting chal-
lenge for automatic extraction that has so far
not been met in full, with most studies focus-
ing on single modifications or a few prominent
ones. In this work, aim to meet this challenge:
we analyse protein modification types through
ontologies, databases, and literature and intro-
duce a corpus of 360 abstracts manually anno-
tated in the BioNLP Shared Task event repre-
sentation for over 4500 mentions of proteins
and 1000 statements of modification events of
nearly 40 different types. We argue that to-
gether with existing resources, this corpus pro-
vides sufficient coverage of modification types
to make effectively exhaustive extraction of
protein modifications from text feasible.
1 Introduction
In the decade following the sequencing of the hu-
man genome, the critical role of protein modifica-
tions in establishing the full set of protein functions
from forms transcribed from the fixed DNA is in-
creasingly appreciated, reflected in the rise of pro-
teomics as an extension and complement to genetics
in efforts to understand gene and protein functions.
The mapping of the space of modifications of spe-
cific proteins is a formidable undertaking: the num-
ber of known types of post-translational modifica-
tions (PTMs) is as high as 300 (Witze et al, 2007)
with new types identified regularly (e.g. (Brennan
and Barford, 2009)), and the number of specific
molecular variants of proteins in cells may be several
orders of magnitude larger than that encoded in the
genome; up to millions for humans (Walsh, 2006).
Automatic extraction of protein modifications from
the massive literature on the topic could contribute
significantly to addressing these challenges.
Biomedical information extraction (IE) has ad-
vanced substantially in recent years, shifting from
the detection of simple binary associations such
as protein-protein interactions toward resources and
methods for the extraction of multiple types of struc-
tured associations of varying numbers participants in
specific roles. These IE approaches are frequently
termed event extraction (Ananiadou et al, 2010).
While protein modifications have been considered
in numerous IE studies in the domain (e.g. (Fried-
man et al, 2001; Rzhetsky et al, 2004; Hu et al,
2005; Narayanaswamy et al, 2005; Saric et al,
2006; Yuan et al, 2006; Lee et al, 2008; Ohta et
al., 2010), event extraction efforts have brought in-
creased focus also on the extraction of protein modi-
fications: in the BioNLP Shared Task series that has
popularized event extraction, the 2009 shared task
(Kim et al, 2009) involved the extraction of nine
event types including one PTM, and in the 2011
follow-up event (Kim et al, 2011) the Epigenet-
ics and Post-translational modifications (EPI) task
(Ohta et al, 2011) targeted six PTM types, their re-
114
verse reactions, and statements regarding their catal-
ysis. The results of these tasks were promising, sug-
gesting that the single PTM type could be extracted
at over 80% F-score (Buyko et al, 2009) and the
core arguments of the larger set at nearly 70% F-
score (Bjo?rne and Salakoski, 2011).
The increasing availability of systems capable of
detailed IE for protein modifications, their high per-
formance also for multiple modifications types, and
demonstrations of the scalability of the technology
to the full scale of the literature (Bjo?rne et al, 2010)
are highly encouraging for automatic extraction of
protein modifications. However, previous efforts
have been restricted by the relatively narrow scope
of targeted modification types. In the present study,
we seek to address the task in full by identifying
all modifications of substantial biological signifi-
cance and creating an annotated resource with effec-
tively complete type-level coverage. We addition-
ally present preliminary extraction results to assess
the difficulty of exhaustive modification extraction.
2 Event representation
To be able to benefit from the substantial number of
existing resources and systems for event extraction,
we apply the event representation of the BioNLP
Shared Task (ST) for annotating protein modifica-
tions. Specifically, we directly extend the approach
of the BioNLP ST 2011 EPI task (Ohta et al, 2011).
In brief, in the applied representation, each event
is marked as being expressed by a specific span of
text (the event trigger) and assigned a type from a
fixed ontology defining event types. Events can take
a conceptually open-ended number of participants,
each of which is similarly bound to a specific tex-
tual expression and marked as participating in the
event in a specific role. In this work, we apply three
roles: Theme identifies the entity or event that is af-
fected by the event (e.g. the protein that is modified),
Cause its cause, and Site specifies a specific part on
the Theme participant that is affected, i.e. the mod-
ification site or region. Further, events are primary
objects of annotation and can thus in turn be par-
ticipants in other events as well as being marked as
e.g. explicitly negated (?is not phosphorylated?) or
stated speculatively (?may be phosphorylated?). An
event annotation example is shown in Figure 1.
Figure 1: Illustration of the event representation. An
event of type ADP-RIBOSYLATION (expressed through
the text ?ADP-ribosylation?) with a PROTEIN (?P2X7?)
participant in the Theme role is in turn the Theme of a
CATALYSIS event with another PROTEIN (?ART2?) as its
Cause.
3 Protein Modifications
We next present our selection of protein modifica-
tion types relevant to event annotation and an ex-
tended analysis of their relative prominence.
3.1 Protein Modifications in Ontologies
For mapping and structuring the space of protein
modification concepts, we primarily build on the
community-standard Gene Ontology (GO) (Ash-
burner et al, 2000). GO has substantial represen-
tation of protein modifications: the sub-ontology
rooted at protein modification process
(GO:0006464) in the GO biological process ontol-
ogy contains 805 terms1 (including both leaf and in-
ternal nodes). This set of terms is the starting point
for our selection of modifications types to target.
First, many specific GO terms can be excluded
due to the different approach to semantic representa-
tion taken in event annotation: while GO terms rep-
resent detailed concepts without explicit structure
(see e.g. (Ogren et al, 2004)), the event representa-
tion is structured, allowing more general terms to be
applied while capturing the same information. For
example, many GO modification terms have child
nodes that identify the target (substrate) of modifica-
tion, e.g. protein phosphorylation has the
child actin phosphorylation. In the event
representation, the target of modification is cap-
tured through the Theme argument. Similarly, GO
terms may identify the site or region of modifica-
tion, which becomes a Site argument in the event
representation (see Figure 2). To avoid redundancy,
we exclude GO terms that differ from a more gen-
eral included term only in specifying a substrate or
modification site. We similarly exclude terms that
specify a catalyst or refer to regulation of modifi-
1GO structure and statistics from data retrieved Dec. 2010.
115
Figure 2: Comparison of hypothetical text-bound GO an-
notation with specific terms (top) and event annotation
with general GO terms (bottom).
cation, as these are captured using separate events
in the applied representation, as illustrated in Fig-
ure 1. For an analogous reason, we do not separately
include type-level distinctions for ?magnitude?
variants of terms (e.g. monoubiquitination,
polyubiquitination) as these can be system-
atically modeled as aspects that can mark any event
(cf. the low/neutral/high Manner of Nawaz et al
(2010)).
Second, a number of the GO terms identify reac-
tions that are in scope of previously defined (non-
modification) event types in existing resources. To
avoid introducing redundant or conflicting annota-
tion with e.g. the GENIA Event corpus (Kim et al,
2008) or BioNLP ST resources, we excluded terms
that involve predominantly (or exclusively) non-
covalent binding (included in the scope of the event
type BINDING) and terms involving the removal of
or binding between the amino acids of a protein, in-
cluding protein maturation by peptide bond cleav-
age (annotated ? arguably somewhat inaccurately ?
as PROTEIN CATABOLISM in GENIA/BioNLP ST
data). By contrast, we do differentiate between re-
actions involving the addition of chemical groups or
small proteins and those involving their removal, in-
cluding e.g. PALMITOYLATION and DEPALMITOY-
LATION as distinct types. To preserve the ontology
structure, we further include also internal nodes ap-
pearing in GO for the purposes of structuring the
ontology (e.g. small protein conjugation
or removal), although we only apply more spe-
cific leaf nodes in event annotation.
This selection, aiming to identify the maximal
subset of the protein modification branch of the GO
ontology relevant to event annotation, resulted in
the inclusion of 74 terms, approximately 9% of the
branch total. Table 1 shows the relevant part of
the GO protein modification subontology
term structure, showing each term only once2 and
excluding very rare terms for space. (A detailed de-
scription of other information in the table is given in
the following sections.)
In addition to GO, we consider protein modifica-
tions in the MeSH ontology,3 used to index PubMed
citations with concepts relevant to them. Further, for
resolving cases not appearing in GO, we refer to the
Uniprot controlled vocabulary of posttranslational
modifications4 and the Proteomics Standards Ini-
tiative Protein Modification Ontology5 (PSI-MOD)
(Montecchi-Palazzi et al, 2008).
3.2 Protein Modifications in Databases
A substantial number of databases tracking pro-
tein modifications from a variety of perspectives ex-
ist, and new ones are introduced regularly. The
databases range from the specific (e.g. (Gupta et al,
1999; Diella et al, 2004; Zhang et al, 2010)) to the
broad in scope (Lee et al, 2005; Li et al, 2009). In-
formation on protein modifications is also found in
general protein knowledge resources such as Swiss-
Prot (Boeckmann et al, 2003) and PIR (Wu et al,
2003). The relative number of entries relevant to
each protein modification in such resources is one
possible proxy for the biological significance of the
various modifications. We apply two such estimates
in this work.
One of the primary applications of GO is the use
of the ontology terms to annotate gene products,
identifying their functions. These annotations, pro-
vided by a variety of groups in different efforts (e.g.
(Camon et al, 2004)), are readily available in GO
and used in various GO tools as a reflection of the
prominence of each of the ontology concepts. As
GO is a community standard with wide participa-
tion and a primary source in this work, we give these
annotation numbers priority in introducing an addi-
tional filter: we exclude from detailed analysis any
term that has no gene product association annota-
tions, taking this as an indication that the modifica-
2GO allows multiple inheritance, and e.g. protein
palmitoylation occurs under both protein
lipidation and protein acylation reflecting
the biological definition.
3http://www.nlm.nih.gov/mesh/meshhome.
html
4http://www.uniprot.org/docs/ptmlist
5http://www.psidev.info/MOD
116
G
PA
Sy
sP
TM
Pu
bM
ed
G
EN
IA
O
ht
a?
10
EP
I
Th
is
st
ud
y
Term GO ID
phosphorylation GO:0006468 8246 24705 93584 546 3 130 85
small protein conj./removal GO:0070647
small protein conjugation GO:0032446
ubiquitination GO:0016567 1724 439 4842 6 - 340 52
sumoylation GO:0016925 121 260 886 - - - 101
neddylation GO:0045116 66 2 100 - - - 52
ufmylation GO:0071569 33 - 1 - - - -
urmylation GO:0032447 16 - 7 - - - -
pupylation GO:0070490 11 - 15 - - - -
small protein removal GO:0070646
deubiquitination GO:0016579 360 - 206 0 - 17 2
deneddylation GO:0000338 45 - 39 - - - 8
desumoylation GO:0016926 20 - 45 - - - 3
dephosphorylation GO:0006470 1479 121 8339 28 - 3 1
glycosylation GO:0006486 1145 2982 12619 - 122 347 62
acylation GO:0043543 1 - 1728 - - - 71
acetylation GO:0006473 522 2000 4423 7 90 337 17
palmitoylation GO:0018345 49 198 1009 - - - 187
myristoylation GO:0018377 27 150 895 - - - 34
octanoylation GO:0018190 4 - 11 - - - -
palmitoleylation GO:0045234 3 - 0 - - - -
alkylation GO:0008213 0
methylation GO:0006479 552 499 9749 - 90 374 18
lipidation GO:0006497 34 51 258 - - - 16
prenylation GO:0018342 64 111 822 - - - 71
farnesylation GO:0018343 19 - 118 - - - 48
geranylgeranylation GO:0018344 26 - 79 - - - 30
deacylation GO:0035601 1 - 331 - - - 1
deacetylation GO:0006476 320 6 1056 1 - 50 4
depalmitoylation GO:0002084 9 - 81 - - - 9
ADP-ribosylation GO:0006471 261 9 3113 - - - 52
cofactor linkage GO:0018065
lipoylation GO:0009249 53 - 49 - - - 14
FAD linkage GO:0018293 46 - 6 - - - -
pyridoxal-5-phosphate linkage GO:0018352 6 - 0 - - - -
dealkylation GO:0008214 0
demethylation GO:0006482 116 - 1465 - - 13 1
deglycosylation GO:0006517 22 1 1204 - - 27 0
ISG15-protein conjugation GO:0032020 20 - 3 - - - -
arginylation GO:0016598 20 - 46 - - - -
hydroxylation GO:0018126 20 226 2948 - 103 139 3
sulfation GO:0006477 18 132 960 - - - 37
carboxylation GO:0018214 17 7 595 - - - 34
nucleotidylation GO:0018175 0
adenylylation GO:0018117 16 - 116 - - - -
uridylylation GO:0018177 1 - 105 - - - -
polyglycylation GO:0018094 17 - 14 - - - -
de-ADP-ribosylation GO:0051725 16 - 7 - - - 5
nitrosylation GO:0017014 14 - 670 - - - -
glutathionylation GO:0010731 11 - 279 - - - -
biotinylation GO:0009305 8 - 1247 - - - 4
deglutathionylation GO:0080058 3 - 42 - - - -
delipidation GO:0051697 3 - 303 - - - -
oxidation GO:0018158 3 475 23413 - - - 21
phosphopantetheinylation GO:0018215 3 - 26 - - - -
tyrosinylation GO:0018322 2 - 2 - - - -
deamination GO:0018277 1 - 840 - - - -
esterification GO:0018350 1 - 1180 - - - -
glucuronidation GO:0018411 1 - 705 - - - -
polyamination GO:0018184 1 - 13 - - - -
Table 1: Protein modifications and protein modification resources. GO terms shown abbreviated, mostly by removing
?protein? (e.g. ?acylation? instead of ?protein acylation?). Terms with 0 GPA not shown except when required for
structure. Columns: GPA: number of Gene Product Associations for each term in GO (not including counts of more
specific child nodes), SysPTM: number of SysPTM modification entries (excluding sites), PubMed: PubMed query
matches (see Section 3.3), GENIA: GENIA corpus (Kim et al, 2008), Ohta?10: corpus introduced in Ohta et al
(2010), EPI: BioNLP ST?11 EPI task corpus (Ohta et al, 2011) (excluding test set).
117
tion is not presently established as having high bio-
logical significance.6
In addition to the GO associations, we include
an estimate based on dedicated protein modification
databases. We chose to use the integrated SysPTM
resource (Li et al, 2009), which incorporates data
from five databases, four webservers, and manual
extraction from the literature. In its initial release,
SysPTM included information on ?nearly 50 modifi-
cation types? on over 30,000 proteins. The columns
labeled GPA and SysPTM in Table 1 show the num-
ber of gene product associations for each selected
type in GO and entries per type in SysPTM, respec-
tively.
3.3 Protein Modifications in domain literature
As a final estimate of the relative prominence of the
various protein modification types, we estimated the
relative frequency with which they are discussed in
the literature through simple PubMed search, query-
ing the Entrez system for each modification in its
basic nominalized form (e.g. phosphorylation) in a
protein-related article. Specifically, for each modifi-
cation string MOD we searched Entrez for
?MOD?[TIAB] AND ?protein?[TIAB]
The modifier [TIAB] specifies to search the title and
abstract. The literal string ?protein? is included to
improve the estimate by removing references that
involve the modification of non-proteins or related
concepts that happen to share the term.7 While this
query is far from a perfect estimate of the actual
number of protein modifications, we expect it to be
a useful as a rough indicator of their relative fre-
quencies and more straightforward to assess than
more involved statistical analyses (e.g. (Pyysalo et
al., 2010)). The results for these queries are given in
the PubMed column of Table 1.
6We are also aware that GO coverage of protein modifica-
tions is not perfect: for example, citrullination, eliminylation,
sialylation, as well as a number of reverse reactions for addi-
tion reactions in the ontology (e.g. demyristoylation) are not
included at the time of this writing. As for terms with no gene
product associations, we accept these omissions as indicating
that these modifications are not biologically prominent.
7For example, search for only dehydration ? a modification
with zero GPA in GO ? matches nearly 10 times as many doc-
uments as search including protein, implying that most of the
hits for the former query likely do not concern protein modi-
fication by dehydration. By contrast, the majority of hits for
phosphorylation match also phosphorylation AND protein.
3.4 Protein Modifications in Event Resources
The rightmost four columns of Table 1 present the
number of annotations for each modification type
in previously introduced event-annotated resources
following the BioNLP ST representation as well as
those annotated in the present study. While modi-
fication annotations are found also in other corpora
(e.g. (Wu et al, 2003; Pyysalo et al, 2007)), we
only include here resources readily compatible with
the BioNLP ST representation.
Separating for the moment from consideration the
question of what level of practical extraction per-
formance can be supported by these event annota-
tions, we can now provide an estimate of the up-
per bound on the coverage of relevant modifica-
tion statements for each of the three proxies (GO
GPA, SysPTM DB entries, PubMed query hits) sim-
ply by dividing the sum of instances of modifica-
tions for which annotations exist by the total. Thus,
for example, there are 8246 GPA annotations for
Phosphorylation and a total of 15597 GPA an-
notations, so the BioNLP ST?09 data (containing
only PHOSPHORYLATION events) could by the GPA
estimate cover 8246/15597, or approximately 53%
of individual modifications.8
For the total coverage of the set of types for which
event annotation is available given the corpus in-
troduced in this study, the coverage estimates are:
GO GPA: 98.2%, SysPTM 99.6%, PubMed 97.5%.
Thus, we estimate that correct extraction of the in-
cluded types would, depending on whether one takes
a gene association, database entry, or literature men-
tion point of view, cover between 97.5% to 99.6%
of protein modification instances ? a level of cov-
erage we suggest is effectively exhaustive for most
practical purposes. We next briefly describe our an-
notation effort before discarding the assumption that
correct extraction is possible and measuring actual
extraction performance.
4 Annotation
This section presents the entity and event annotation
approach, document selection, and the statistics of
the created annotation.
8The remarkably high coverage for a single type reflects the
Zipfian distribution of the modification types; see e.g. Ohta et
al. (2010).
118
4.1 Entity and Event Annotation
To maximize compatibility with existing event-
annotated resources, we chose to follow the gen-
eral representation and annotation guidelines ap-
plied in the annotation of GENIA/BioNLP ST re-
sources, specifically the BioNLP ST 2011 EPI task
corpus. Correspondingly, we followed the GE-
NIA gene/gene product (Ohta et al, 2009) annota-
tion guidelines for marking protein mentions, ex-
tended the GENIA event corpus guidelines (Kim et
al., 2008) for the annotation of protein modification
events, and marked CATALYSIS events following the
EPI task representation. For compatibility, we also
marked event negation and speculation as in these
resources. We followed the GO definitions for in-
dividual modification types, and in the rare cases
where a modification discussed in text had no ex-
isting GO definition, we extrapolated from the way
in which protein modifications are generally defined
in GO, consulting other domain ontologies and re-
sources (Section 3.1) as necessary.
4.2 Document Selection
As the distribution of protein modifications in
PubMed is extremely skewed, random sampling
would recover almost solely instances of major
types such as phosphorylation. As we are inter-
ested also in the extraction of very rare modifica-
tions, we applied a document selection strategy tar-
geted at individual modification types. We applied
one of two primary strategies depending on whether
each targeted modification type had a correspond-
ing MeSH term or not. If a MeSH term specific
to the modification exists, we queried PubMed for
the MeSH term, thus avoiding searches for spe-
cific forms of expression that might bias the search.
In cases where no specific MeSH term existed,
we searched the text of documents marked with
the generic MeSH term protein processing,
post-translational for mentions of likely
forms of expression for the modification.9 Fi-
nally, in a few isolated instances we applied cus-
tom text-based PubMed searches with broader cov-
9Specifically, we applied a regular expression incorporating
the basic form of modification expression and allowing variance
through relevant affixes and inflections derived from an initial
set of annotations for documents for which MeSH terms were
defined.
Item Count
Abstract 360
Word 76806
Protein 4698
Event type 37
Event instance 1142
Table 2: Annotation statistics.
erage. Then, as many of the modifications are not
limited to protein substrates, to select documents re-
lating specifically to protein modification we pro-
ceeded to tagged a large random sample of selected
documents with the BANNER named entity tagger
(Leaman and Gonzalez, 2008) trained on the GENE-
TAG corpus (Tanabe et al, 2005) and removed doc-
uments with fewer than five automatically tagged
gene/protein-related entities. The remaining docu-
ments were then randomly sampled for annotation.10
4.3 Corpus Statistics
We initially aimed to annotate balanced numbers of
modification types in order of their estimated promi-
nence, with particular focus on previously untar-
geted reaction types involving the addition of chem-
ical groups or small proteins. However, it became
apparent in the annotation process that the extreme
rarity of some of the modifications as well as the
tendency for more frequent modifications to be dis-
cussed in texts mentioning rare ones made this im-
possible. Thus, while preserving the goal of es-
tablishing broadly balanced numbers of major new
modifications, we allowed the number of rare reac-
tions to remain modest.
Table 2 summarizes the statistics of the final cor-
pus, and the rightmost column of Table 1 shows
per-type counts. We note that as reactions involv-
ing the removal of chemical groups or small pro-
teins were not separately targeted, only few events
of such types were annotated. We did not sepa-
rately measure inter-annotator agreement for this ef-
fort, but note that this work is an extension of the
EPI corpus annotation, for which comparison of in-
dependently created event annotations indicated an
F-score of 82% for the full task and 89% for the core
targets (see Section 5.1) (Ohta et al, 2011).
10This strategy, including MeSH-based search, was applied
also in the BioNLP Shared Task 2011 EPI task document selec-
tion.
119
5 Experiments
To assess actual extraction performance, we per-
formed experiments using a state-of-the art event ex-
traction system.
5.1 Experimental Setup
We first split the corpus into a training/development
portion and a held out set for testing, placing half of
the abstracts into each set. The split was stratified
by event type to assure that relatively even numbers
of each event type were present in both sets. All
development was performed using cross-validation
on the visible portion of the data, and a single final
experiment was performed on the test dataset.
To assure that our results are comparable with
those published in recent event extraction stud-
ies, we adopted the standard evaluation crite-
ria of the BioNLP Shared Task. The evalua-
tion is event instance-based and uses the standard
precision/recall/F1-score metrics. We modified the
shared task evaluation software to support the newly
defined event types and ran experiments with the
standard approximate span matching and partial re-
cursive matching criteria (see (Kim et al, 2009)).
We further follow the EPI task evaluation in re-
porting results separately for the extraction of only
Theme and Cause arguments (core task) and for the
full argument set.
5.2 Event extraction method
We applied the EventMine event extraction system
(Miwa et al, 2010a; Miwa et al, 2010b), an SVM-
based pipeline system using an architecture similar
to that of the best-performing system in the BioNLP
ST?09 (Bjo?rne et al, 2009); we refer to the studies
of Miwa et al for detailed description of the base
system. For analysing sentence structure, we applied
the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and
GDep beta2 (Sagae and Tsujii, 2007) parsers.
For the present study, we modified the base Event-
Mine system as follows. First, to improve efficiency
and generalizability, instead of using all words as
trigger candidates as in the base system, we filtered
candidates using a dictionary extracted from train-
ing data and expanded by using the UMLS specialist
lexicon (Bodenreider, 2004) and the ?hypernyms?
and ?similar to? relations in WordNet (Fellbaum,
1998). Second, to allow generalization across ar-
gument types, we added support for solving a single
classification problem for event argument detection
instead of solving multiple classification problems
separated by argument types. Finally, to facilitate
the use of other event resources for extraction, we
added functionality to incorporate models trained by
other corpora as reference models, using predictions
from these models as features in classification.
5.3 Experimental results
We first performed a set of experiments to determine
whether models can beneficially generalize across
different modification event types. The EventMine
pipeline has separate classification stages for event
trigger detection, event-argument detection, and the
extraction of complete event structures. Each of
these stages involves a separate set of features and
output labels, some of which derive directly from
the involved event types: for example, in deter-
mining whether a specific entity is the Theme of
an event triggered by the string ?phosphorylation?,
the system by default uses the predicted event type
(PHOSPHORYLATION) among its features. It is pos-
sible to force the model to generalize across event
types by replacing specific types with placehold-
ers, for example replacing PHOSPHORYLATION,
METHYLATION, etc. with MODIFICATION.
In preliminary experiments on the development
set, we experimented with a number of such gener-
alizations. Results indicated that while some gen-
eralization was essential for achieving good ex-
traction performance, most implementation variants
produced broadly comparable results. We chose the
following generalizations for the final test: in the
trigger detection model, no generalization was per-
formed (allowing specific types to be extracted), for
argument detection, all instances of event types were
replaced with a generic type (EVENT), and for event
structure prediction, all instances of specific modi-
fication event types (but not CATALYSIS) were re-
placed with a generic type (MODIFICATION). Re-
sults comparing the initial, ungeneralized model to
the generalized one are shown in the top two rows
of Table 3. The results indicate that generalization is
clearly beneficial: attempting to learn each of the
event types in isolation leaves F-score results ap-
proximately 4-5% points lower than when general-
120
Core Full
Initial 39.40 / 46.36 / 42.60 31.39 / 38.88 / 34.74
Generalized 39.02 / 61.18 / 47.65 31.07 / 51.89 / 38.87
+Model 41.28 / 61.28 / 49.33 33.66 / 53.06 / 41.19
+Ann 38.46 / 66.99 / 48.87 32.36 / 59.17 / 41.84
+Model +Ann 41.84 / 66.17 / 51.26 33.98 / 56.00 / 42.30
Test data 45.69 / 62.35 / 52.74 38.03 / 54.57 / 44.82
Table 3: Experimental results.
izing across types. A learning curve for the gen-
eralized model is shown in Figure 3. While there
is some indication of decreasing slope toward use
of the full dataset, the curve suggests performance
could be further improved through additional anno-
tation efforts.
In a second set of experiments, we investigated
the compatibility of the newly introduced annota-
tions with existing event resources by incorporat-
ing their annotations either directly as training data
(+Ann) or indirectly through features from predic-
tions from a model trained on existing resources
(+Model), as well as their combination. We per-
formed experiments with the BioNLP Shared Task
2011 EPI task corpus11 and the generalized setting.
The results of these experiments are given in the
middle rows of Table 3. We find substantial bene-
fit from either form of existing resource integration
alone, and, interestingly, an indication that the ben-
efits of the two approaches can be combined. This
result indicates that the newly introduced corpus is
compatible with the EPI corpus, a major previously
introduced resource for protein modification event
extraction. Evaluation on the test data (bottom row
of Table 3) confirmed that development data results
were not overfit and generalized well to previously
unseen data.
6 Discussion and Conclusions
We have presented an effort to directly address the
challenges involved in the exhaustive extraction of
protein modifications in text. We analysed the Gene
Ontology protein modification process
subontology from the perspective of event extraction
for information extraction, arguing that due largely
to the structured nature of the event representation,
11When combining EPI annotations directly as additional
training abstracts, we filtered out abstracts including possible
?missing? annotations for modification types not annotated in
EPI data using a simple regular expression.
Figure 3: Learning curve.
74 of the 805 ontology terms suffice to capture the
general modification types included. Through an
analysis of the relative prominence of protein modi-
fications in ontology annotations, domain databases,
and literature, we then filtered and prioritized these
types, estimating that correct extraction of the most
prominent half of these types would give 97.5%-
99.6% coverage of protein modifications, a level that
is effectively exhaustive for practical purposes.
To support modification event extraction and to
estimate actual extraction performance, we then
proceeded to manually annotate a corpus of 360
PubMed abstracts selected for relevance to the se-
lected modification types. The resulting corpus an-
notation marks over 4500 proteins and over 1000 in-
stances of modification events and more than triples
the number of specific protein modification types for
which text-bound event annotations are available.
Experiments using a state-of-the-art event extraction
system showed that a machine learning method can
beneficially generalize features across different pro-
tein modification event types and that incorporation
of BioNLP Shared Task EPI corpus annotations can
improve performance, demonstrating the compati-
bility of the created resource with existing event cor-
pora. Using the best settings on the test data, we
found that the core extraction task can be performed
at 53% F-score.
The corpus created in this study is freely available
for use in research from http://www-tsujii.
is.s.u-tokyo.ac.jp/GENIA.
Acknowledgments
We would like to thank Yo Shidahara and Yoshihiro
Okuda of NalaPro Technologies for their efforts in
creating the corpus annotation. This work was sup-
ported by Grant-in-Aid for Specially Promoted Re-
search (MEXT, Japan).
121
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of BioNLP?09 Shared
Task, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Scaling up biomed-
ical event extraction to the entire pubmed. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing, pages 28?36.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32(Database
issue):D267?70.
B. Boeckmann, A. Bairoch, R. Apweiler, M.C. Blat-
ter, A. Estreicher, E. Gasteiger, M.J. Martin, K. Mi-
choud, C. O?Donovan, I. Phan, et al 2003. The
SWISS-PROT protein knowledgebase and its supple-
ment TrEMBL in 2003. Nucleic acids research,
31(1):365.
D.F. Brennan and D. Barford. 2009. Eliminylation:
a post-translational modification catalyzed by phos-
phothreonine lyases. Trends in biochemical sciences,
34(3):108?114.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In Proceedings of the BioNLP?09
Shared Task, pages 19?27, Boulder, Colorado, June.
Association for Computational Linguistics.
Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-
vian Lee, Emily Dimmer, John Maslen, David Binns,
Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.
2004. The Gene Ontology Annotation (GOA)
Database: sharing knowledge in Uniprot with Gene
Ontology. Nucl. Acids Res., 32(suppl 1):D262?266.
Francesca Diella, Scott Cameron, Christine Gemund,
Rune Linding, Allegra Via, Bernhard Kuster, Thomas
Sicheritz-Ponten, Nikolaj Blom, and Toby Gibson.
2004. Phospho.elm: A database of experimentally
verified phosphorylation sites in eukaryotic proteins.
BMC Bioinformatics, 5(1):79.
C. Fellbaum. 1998. Wordnet: an electronic lexical
database. In International Conference on Computa-
tional Linguistics.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: A natural-language processing system for the
extraction of molecular pathways from journal articles.
Bioinformatics, 17(Suppl. 1):S74?S82.
Ramneek Gupta, Hanne Birch, Kristoffer Rapacki, Sren
Brunak, and Jan E. Hansen. 1999. O-glycbase version
4.0: a revised database of o-glycosylated proteins. Nu-
cleic Acids Research, 27(1):370?372.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of bionlp
shared task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Proceedings of PSB?08,
pages 652?663.
Tzong-Yi Lee, Hsien-Da Huang, Jui-Hung Hung, Hsi-
Yuan Huang, Yuh-Shyong Yang, and Tzu-Hao Wang.
2005. dbPTM: an information repository of protein
post-translational modification. Nucleic Acids Re-
search, 34(suppl 1):D622?D627.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein lig-
ases. Nucl. Acids Res., 36(suppl.2):W416?422.
Hong Li, Xiaobin Xing, Guohui Ding, Qingrun Li, Chuan
Wang, Lu Xie, Rong Zeng, and Yixue Li. 2009.
Sysptm: A systematic resource for proteomic research
on post-translational modifications. Molecular & Cel-
lular Proteomics, 8(8):1839?1849.
Takuya Matsuzaki and Yusuke Miyao. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
In In Proceedings of IJCAI-07, pages 1671?1676.
122
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. Evaluating dependency rep-
resentations for event extraction. In Proceedings of
Coling?10, pages 779?787.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146.
Luisa Montecchi-Palazzi, Ron Beavis, Pierre-Alain Binz,
Robert Chalkley, John Cottrell, David Creasy, Jim
Shofstahl, Sean Seymour, and John Garavelli. 2008.
The PSI-MOD community standard for representation
of protein modification data. Nature Biotechnology,
26:864?866.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2005. Beyond the clause: extraction of
phosphorylation information from medline abstracts.
Bioinformatics, 21(suppl.1):i319?327.
R. Nawaz, P. Thompson, J. McNaught, and S. Ananiadou.
2010. Meta-Knowledge Annotation of Bio-Events.
Proceedings of LREC 2010, pages 2498?2507.
P.V. Ogren, K.B. Cohen, G.K. Acquaah-Mensah, J. Eber-
lein, and L. Hunter. 2004. The compositional struc-
ture of Gene Ontology terms. In Pacific Symposium
on Biocomputing, page 214.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii. 2010.
An analysis of gene/protein associations at pubmed
scale. In Proceedings of the fourth International Sym-
posium for Semantic Mining in Biomedicine (SMBM
2010).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Morris,
Hong Yu, Pablo Ariel Duboue?, Wubin Weng, W. John
Wilbur, Vasileios Hatzivassiloglou, and Carol Fried-
man. 2004. GeneWays: A system for extracting, ana-
lyzing, visualizing, and integrating molecular pathway
data. Journal of Biomedical Informatics, 37(1):43?53.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of EMNLP-CoNLL?07,
pages 1044?1050.
Jasmin Saric, Lars Juhl Jensen, Rossitza Ouzounova, Is-
abel Rojas, and Peer Bork. 2006. Extraction of regu-
latory gene/protein networks from Medline. Bioinfor-
matics, 22(6):645?650.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Christopher Walsh. 2006. Posttranslational modification
of proteins: expanding nature?s inventory. Roberts &
Company Publishers.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
X. Yuan, ZZ Hu, HT Wu, M. Torii, M. Narayanaswamy,
KE Ravikumar, K. Vijay-Shanker, and CH Wu. 2006.
An online literature mining tool for protein phospho-
rylation. Bioinformatics, 22(13):1668.
Yan Zhang, Jie Lv, Hongbo Liu, Jiang Zhu, Jianzhong Su,
Qiong Wu, Yunfeng Qi, Fang Wang, and Xia Li. 2010.
Hhmd: the human histone modification database. Nu-
cleic Acids Research, 38(suppl 1):D149?D154.
123
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 136?145,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
SimSem: Fast Approximate String Matching in Relation to Semantic
Category Disambiguation
Pontus Stenetorp?? Sampo Pyysalo? and Jun?ichi Tsujii?
? Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp}@is.s.u-tokyo.ac.jp
jtsujii@microsoft.com
Abstract
In this study we investigate the merits of
fast approximate string matching to address
challenges relating to spelling variants and to
utilise large-scale lexical resources for seman-
tic class disambiguation. We integrate string
matching results into machine learning-based
disambiguation through the use of a novel set
of features that represent the distance of a
given textual span to the closest match in each
of a collection of lexical resources. We col-
lect lexical resources for a multitude of se-
mantic categories from a variety of biomedi-
cal domain sources. The combined resources,
containing more than twenty million lexical
items, are queried using a recently proposed
fast and efficient approximate string match-
ing algorithm that allows us to query large
resources without severely impacting system
performance. We evaluate our results on six
corpora representing a variety of disambigua-
tion tasks. While the integration of approxi-
mate string matching features is shown to sub-
stantially improve performance on one corpus,
results are modest or negative for others. We
suggest possible explanations and future re-
search directions. Our lexical resources and
implementation are made freely available for
research purposes at: http://github.com/ninjin/
simsem
1 Introduction
The use of dictionaries for boosting performance has
become commonplace for Named Entity Recogni-
tion (NER) systems (Torii et al, 2009; Ratinov and
Roth, 2009). In particular, dictionaries can give an
initial improvement when little or no training data
is available. However, no dictionary is perfect, and
all resources lack certain spelling variants and lag
behind current vocabulary usage and thus are un-
able to cover the intended domain in full. Further,
due to varying dictionary curation and corpus anno-
tation guidelines, the definition of what constitutes
a semantic category is highly unlikely to precisely
match for any two specific resources (Wang et al,
2009). Ideally, for applying a lexical resource to an
entity recognition or disambiguation task to serve as
a definition of a semantic category there would be
a precise match between the definitions of the lexi-
cal resource and target domain, but this is seldom or
never the case.
Most previous work studying the use of dictionary
resources in entity mention-related tasks has focused
on single-class NER, in particular this is true for
BioNLP where it has mainly concerned the detec-
tion of proteins. These efforts include Tsuruoka and
Tsujii (2003), utilising dictionaries for protein de-
tection by considering each dictionary entry using a
novel distance measure, and Sasaki et al (2008), ap-
plying dictionaries to restrain the contexts in which
proteins appear in text. In this work, we do not
consider entity mention detection, but instead focus
solely on the related task of disambiguating the se-
mantic category for a given continuous sequence of
characters (a textual span), doing so we side-step the
issue of boundary detection in favour of focusing on
novel aspects of semantic category disambiguation.
Also, we are yet to see a high-performing multi-class
biomedical NER system, this motivates our desire to
include multiple semantic categories.
136
2 Methods
In this section we introduce our approach and the
structure of our system.
2.1 SimSem
Many large-scale language resources are available
for the biomedical domain, including collections
of domain-specific lexical items (Ashburner et al,
2000; Bodenreider, 2004; Rebholz-Schuhmann et
al., 2010). These resources present obvious opportu-
nities for semantic class disambiguation. However,
in order to apply them efficiently, one must be able
to query the resources taking into consideration both
lexical variations in dictionary entries compared to
real-world usage and the speed of look-ups.
We can argue that each resource offers a differ-
ent view of what constitutes a particular semantic
category. While these views will not fully overlap
between resources even for the same semantic cate-
gory, we can expect a certain degree of agreement.
When learning to disambiguate between semantic
categories, a machine learning algorithm could be
expected to learn to identify a specific semantic cat-
egory from the similarity between textual spans an-
notated for the category and entries in a related lex-
ical resource. For example, if we observe the text
?Carbonic anhydrase IV? marked as PROTEIN and
have an entry for ?Carbonic anhydrase 4? in a lexical
resource, a machine learning method can learn to as-
sociate the resource with the PROTEIN category (at
specific similarity thresholds) despite syntactic dif-
ferences.
In this study, we aim to construct such a system
and to demonstrate that it outperforms strict string
matching approaches. We refer to our system as
SimSem, as in ?Similarity? and ?Semantic?.
2.2 SimString
SimString1 is a software library utilising the CP-
Merge algorithm (Okazaki and Tsujii, 2010) to en-
able fast approximate string matching. The software
makes it possible to find matches in a collection with
over ten million entries using cosine similarity and
a similarity threshold of 0.7 in approximately 1 mil-
lisecond with modest modern hardware. This makes
it useful for querying a large collection of strings to
1http://www.chokkan.org/software/simstring/
find entries which may differ from the query string
only superficially and may still be members of the
same semantic category.
As an example, if we construct a SimString
database using an American English wordlist2 and
query it using the cosine measure and a threshold of
0.7. For the query ?reviewer? SimString would re-
turn the following eight entries: review, viewer, pre-
view, reviewer, unreviewed, televiewer, and review-
eress. We can observe that most of the retrieved en-
tries share some semantic similarity with the query.
2.3 Machine Learning
For the machine learning component of our system
we use the L2-regularised logistic regression im-
plementation of the LIBLINEAR3 software library
(Fan et al, 2008). We do not normalise our feature
vectors and optimise our models? penalty parameter
using k-fold cross-validation on the training data. In
order to give a fair representation of the performance
of other systems, we use a rich set of features that are
widely applied for NER (See Table 1).
Our novel SimString features are generated as fol-
lows. We query each SimString database using the
cosine measure with a sliding similarity threshold,
starting at 1.0 and ending at 0.7, lowering the thresh-
old by 0.1 per query. If a query is matched, we gen-
erate a feature unique for that database and thresh-
old, we also generate the same feature for each step
from the current threshold to the cut-off of 0.7 (a
match at e.g. 0.9 similarity also implies matches at
0.8 and 0.7).
The cut-off is motivated by the fact that very
low thresholds introduces a large degree of noise.
For example, for our American English wordlist
the query ?rejection? using threshold 0.1 and the
cosine measure will return 13,455 results, among
them ?questionableness? which only have a single
sequence ?ion? in common.
It is worthwhile to note that during our prelimi-
nary experiments we failed to establish a consistent
benefit from contextual features across our develop-
ment sets. Thus, contextual features are not included
in our feature set and instead our study focuses only
2/usr/share/dict/web2 under FreeBSD 8.1-RELEASE, based
on Webster?s Second International dictionary from 1934
3We used version 1.7 of LIBLINEAR for our experiments
137
Feature Type Input Value(s)
Text Text Flu Flu
Lower-cased Text DNA dna
Prefixes: sizes 3 to 5 Text bull bul, . . .
Suffixes: sizes 3 to 5 Text bull ull, . . .
Stem (Porter, 1993) Text performing perform
Is a pair of digits Bool 42 True
Is four digits Bool 4711 True
Letters and digits Bool C4 True
Digits and hyphens Bool 9-12 True
Digits and slashes Bool 1/2 True
Digits and colons Bool 3,1 True
Digits and dots Bool 3.14 True
Upper-case and dots Bool M.C. True
Initial upper-case Bool Pigeon True
Only upper-case Bool PMID True
Only lower-case Bool pure True
Only digits Bool 131072 True
Only non-alpha-num Bool #*$! True
Contains upper-case Bool gAwn True
Contains lower-case Bool After True
Contains digits Bool B52 True
Contains non-alpha-num Bool B52;s True
Date regular expression4 Bool 1989-01-30 True
Pattern Text 1B-zz 0A-aa
Collapsed Pattern Text 1B-zz 0A-a
Table 1: Basic features used for classification
the features that are generated solely from the tex-
tual span which has been annotated with a semantic
category (span-internal features) and the comparison
of approximate and strict string matching.
3 Resources
This section introduces and discusses the prepro-
cessing and statistics of the lexical and corpus re-
sources used in our experiments.
3.1 Lexical Resources
To generate a multitude of SimString databases cov-
ering a wide array of semantic categories we employ
several freely available lexical resources (Table 2).
The choice of lexical resources was initially made
with the aim to cover commonly annotated domain
semantic categories: the CHEBI and CHEMICAL
subsets of JOCHEM for chemicals, LINNAEUS for
species, Entrez Gene and SHI for proteins. We then
4A simple regular expression matching dates:
?(19|20)\d\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])$
from http://www.regular-expressions.info/dates.html
expanded the selection based on error analysis to in-
crease our coverage of a wider array of semantic cat-
egories present in our development data.
We used the GO version from March 2011, ex-
tracting all non-obsolete terms from the ontology
and separating them into the three GO subontolo-
gies: biological process (BP), cellular component
(CC) and molecular function (MF). We then created
an additional three resources by extracting all exact
synonyms for each entry. Lastly, we expanded these
six resources into twelve resources by applying the
GO term variant generation technique described by
Beisswanger et al (2008).
UMLS, a collection of various resources, contain
135 semantic categories (e.g. Body Location or Re-
gion and Inorganic Chemical) which we use to cre-
ate a database for each category.
For Entrez Gene we extracted all entries for the
following types: gene locus, protein name, protein
description, nomenclature symbol and nomenclature
fullname, creating a SimString database for each.
This leaves some parts of Entrez Gene unutilised,
but we deemed these categories to be sufficient for
our experiments.
The Turku Event Corpus is a resource created by
applying an automated event extraction system on
the full release of PubMed from 2009. As a pre-
condition for the event extraction system to operate,
protein name recognition is necessary; for this cor-
pus, NER has been performed by the corpus curators
using the BANNER (Leaman and Gonzalez, 2008)
NER system trained on GENETAG (Tanabe et al,
2005). We created a database (PROT) containing
all protein annotations, extracted all event triggers
(TRIG) and created a database for each of the event
types covered by the event extraction system.
For the AZDC corpus, we extracted each anno-
tated textual span since the corpus covers only a sin-
gle semantic category. Similarly, the LINNAEUS
dictionary was converted into a single database since
it covers the single category ?species?.
Table 3 contains the statistics per dictionary re-
source and the number of SimString databases cre-
ated for each resource. Due to space requirements
we leave out the full details for GO BP, GO CC,
GO MF, UMLS, Entrez Gene and TURKU TRIG,
and instead give the total entries for all the databases
generated from these resources.
138
Name Abbreviation Semantic Categories Publication
Gene Ontology GO Multiple Ashburner et al (2000)
Protein Information Resource PIR Proteins Wu et al (2003)
Unified Medical Language System UMLS Multiple Bodenreider (2004)
Entrez Gene ? Proteins Maglott et al (2005)
Automatically generated dictionary SHI Proteins Shi and Campagne (2005)
Jochem JOCHEM Multiple Hettne et al (2009)
Turku Event Corpus TURKU Proteins and biomolecular events Bjo?rne et al (2010)
Arizona Disease Corpus AZDC Diseases Chowdhury and Lavelli (2010)
LINNAEUS Dictionary LINNAEUS Species Gerner et al (2010)
Webster?s International Dictionary WID Multiple ?
Table 2: Lexical resources gathered for our experiments
Resource Unique Entries Databases
GO BP 67,411 4
GO CC 5,993 4
GO MF 55,595 4
PIR 691,577 1
UMLS 5,902,707 135
Entrez Gene 3,602,757 5
SHI 61,676 1
CHEBI 187,993 1
CHEMICAL 1,527,751 1
TURKU PROT 4,745,825 1
TURKU TRIG 130,139 10
AZDC 1,195 1
LINNAEUS 3,119,005 1
WID 235,802 1
Total: 20, 335, 426 170
Table 3: Statistics per dictionary resource
3.2 Corpora
To evaluate our approach we need a variety of cor-
pora annotated with multiple semantic categories.
For this purpose we selected the six corpora listed
in Table 4.
The majority of our corpora are available in the
common stand-off style format introduced for the
BioNLP 2009 Shared Task (BioNLP?09 ST) (Kim
et al, 2009). The remaining two, NLPBA and
CALBC CII, were converted into the BioNLP?09 ST
format so that we could process all resources in the
same manner for our experimental set-up.
In addition to physical entity annotations, the
GREC, EPI, ID and GENIA corpora incorporate
event trigger annotations (e.g. Gene Regulatory
Event (GRE) for GREC). These trigger expressions
carry with them a specific semantic type (e.g. ?in-
teract? can carry the semantic type BINDING for
GENIA), allowing us to enrich the data sets with
additional semantic categories by including these
types in our dataset as distinct semantic categories.
This gave us the following increase in semantic cat-
egories: GREC one, EPI 15, ID ten, GENIA nine.
The original GREC corpus contains an exception-
ally wide array of semantic categories. While this
is desirable for evaluating the performance of our
approach under different task settings, the sparsity
of the data is a considerable problem; the majority
of categories do not permit stable evaluation as they
have only a handful of annotations each. To alleviate
this problem we used the five ontologies defined in
the GREC annotation guidelines5, collapsing the an-
notations into five semantic super categories to cre-
ate a resource we refer to as Super GREC. This pre-
processing conforms with how the categories were
used when annotating the GREC corpus (Thompson
et al, 2009). This resource contains sufficient anno-
tations for each semantic category to enable evalua-
tion on a category-by-category basis. Also, for the
purpose of our experiments we removed all ?SPAN?
type annotations since they themselves carry no se-
mantic information (cf. GREC annotation guide-
lines).
CALBC CII contains 75,000 documents, which
is more than enough for our experiments. In order
to maintain balance in size between the resources in
our experiments, we sampled a random 5,000 docu-
ments and used these as our CALBC CII dataset.
5http://www.nactem.ac.uk/download.php?target=GREC/
Event annotation guidelines.pdf
139
Name Abbreviation Publication
BioNLP/NLPBA 2004 Shared Task Corpus NLPBA Kim et al (2004)
Gene Regulation Event Corpus GREC Thompson et al (2009)
Collaborative Annotation of a Large Biomedical Corpus CALBC CII Rebholz-Schuhmann et al (2010)
Epigenetics and Post-Translational Modifications EPI Ohta et al (2011)
Infectious Diseases Corpus ID Pyysalo et al (2011)
Genia Event Corpus GENIA Kim et al (2011)
Table 4: Corpora used for evaluation
3.3 Corpus Statistics
In this section we present statistics for each of our
datasets. For resources with a limited number of se-
mantic categories we use pie charts to illustrate their
distribution (Figure 1). For the other corpora we use
tables to illustrate this. Tables for the corpora for
which pie charts are given has been left out due to
space requirements.
The NLPBA corpus (Figure 1a) with 59,601 to-
kens annotated, covers five semantic categories, with
a clear majority of protein annotations. While
NLPBA contains several semantic categories, they
are closely related, which is expected to pose chal-
lenges for disambiguation. This holds in particular
for proteins, DNA and RNA, which commonly share
names.
Our collapsed version of GREC, Super GREC
(see Figure 1b), contains 6,777 annotated tokens and
covers a total of six semantic categories: Regulatory
Event (GRE), nucleic acids, proteins, processes, liv-
ing system and experimental. GREC is an interest-
ing resource in that its classes are relatively distinct
and four of them are evenly distributed.
CALBC CII is balanced among its annotated cat-
egories, as illustrated in Figure 1c. The 6,433 to-
kens annotated are of the types: proteins and genes
(PRGE), species (SPE), disorders (DISO) and chem-
icals and drugs (CHED). We note that we have in-
troduced lexical resources covering each of these
classes (Section 3.1).
For the BioNLP?11 ST resources EPI (Table 5),
GENIA (Figure 1d and contains 27,246 annotated
tokens) and ID (Table 6), we observe a very skewed
distribution due to our decision to include event
types as distinct classes; The dominating class for
all the datasets are proteins. For several of these
categories, learning accurate disambiguation is ex-
Type Ratio Annotations
Acetylation 2.3% 294
Catalysis 1.4% 186
DNA demethylation 0.1% 18
DNA methylation 2.3% 301
Deacetylation 0.3% 43
Deglycosylation 0.2% 26
Dehydroxylation 0.0% 1
Demethylation 0.1% 12
Dephosphorylation 0.0% 3
Deubiquitination 0.1% 13
Entity 6.6% 853
Glycosylation 2.3% 295
Hydroxylation 0.9% 116
Methylation 2.5% 319
Phosphorylation 0.9% 112
Protein 77.7% 10,094
Ubiquitination 2.3% 297
Total: 12,983
Table 5: Semantic categories in EPI
pected to be very challenging if not impossible due
to sparsity: For example, Dehydroxylation in EPI
has a single annotation.
ID is of particular interest since it contains a con-
siderable amount of annotations for more than one
physical entity category, including in addition to
protein also organism and a minor amount of chem-
ical annotations.
4 Experiments
In this section we introduce our experimental set-up
and discuss the outcome of our experiments.
4.1 Experimental Set-up
To ensure that our results are not biased by over-
fitting on a specific set of data, all data sets were
separated into training, development and test sets.
140
(a) NLPBA
(b) Super GREC
(c) CALBC CII
(d) GENIA
Figure 1: Semantic category distributions
NLPBA defines only a training and test set, GREC
and CALBC CII are provided as resources and lack
any given division, and for the BioNLP?11 ST data
the test sets are not distributed. Thus, we combined
all the available data for each dataset and separated
the documents into fixed sets with the following ra-
tios: 1/2 training, 1/4 development and 1/4 test.
Type Ratio Annotations
Binding 1.0% 102
Chemical 6.8% 725
Entity 0.4% 43
Gene expression 3.3% 347
Localization 0.3% 36
Negative regulation 1.6% 165
Organism 25.5% 2,699
Phosphorylation 0.5% 54
Positive regulation 2.5% 270
Process 8.0% 843
Protein 43.1% 4,567
Protein catabolism 0.0% 5
Regulation 1.8% 188
Regulon-operon 1.1% 121
Transcription 0.4% 47
Two-component-system 3.7% 387
Total: 10,599
Table 6: Semantic categories in ID
We use a total of six classifiers for our experi-
ments. First, a naive baseline (Naive): a majority
class voter with a memory based on the exact text
of the textual span. The remaining five are ma-
chine learning classifiers trained using five differ-
ent feature sets: gazetteer features constituting strict
string matching towards our SimString databases
(Gazetteer), SimString features generated from our
SimString databases (SimString), the span internal
features listed in Table 1 (Internal), the span inter-
nal and gazetteer features (Internal-Gazetteer) and
the span internal and SimString features (Internal-
SimString).
We evaluate performance using simple instance-
level accuracy (correct classifications / all classifica-
tions). Results are represented as learning curves for
each data set.
4.2 Results
From our experiments we find that ? not surpris-
ingly ? the performance of the Naive, Gazetteer and
SimString classifiers alone is comparatively weak.
Their performance is illustrated in Figure 2. We can
briefly summarize the results for these methods by
noting that the SimString classifier outperforms the
Gazetteer by a large margin for every dataset.6 From
6Due to space restrictions we do not include further analysis
or charts.
141
Figure 2: SimString, Gazetteer and Naive for ID
Figure 3: Learning curve for NLPBA
here onwards we focus on the performance of the In-
ternal classifier in combination with Gazetteer and
SimString features.
For NLPBA (Figure 3), GENIA (Figure 4) and ID
(Figure 5) our experiments show no clear systematic
benefit from either SimString or Gazetteer features.
For Super GREC (Figure 6) and EPI (Figure 7)
classifiers with Gazetteer and SimString features
consistently outperform the Internal classifier, and
the SimString classifier further shows some benefit
over Gazetteer for EPI.
The only dataset for which we see a clear benefit
from SimString features over Gazetteer and Internal
is for CALBC CII (Figure 8).
5 Discussion and Conclusions
While we expected to see clear benefits from both
using Gazetteers and SimString features, our exper-
Figure 4: Learning curve for GENIA
Figure 5: Learning curve for ID
iments returned negative results for the majority of
the corpora. For NLPBA, GENIA and ID we are
aware that most of the instances are either proteins
or belong to event trigger classes for which we may
not have had adequate lexical resources for disam-
biguation. By contrast, for Super GREC there are
several distinct classes for which we expected lex-
ical resources to have fair coverage for SimString
and Gazetteer features. While an advantage over In-
ternal was observed for Super GREC, SimString fea-
tures showed no benefit over Gazetteer features. The
methods exhibited the expected result on only one of
the six corpora, CALBC CII, where there is a clear
advantage for Gazetteer over Internal and a further
clear advantage for SimString over Gazetteer.
Disappointingly, we did not succeed in establish-
ing a clear improvement for more than one of the six
corpora. Although we have not been successful in
142
Figure 6: Learning curve for Super GREC
Figure 7: Learning curve for EPI
proving our initial hypothesis we argue that our re-
sults calls for further study due to several concerns
raised by the results remaining unanswered. It may
be that our notion of distance to lexical resource en-
tries is too naive. A possible future direction would
be to compare the query string to retrieved results us-
ing a method similar to that of Tsuruoka and Tsujii
(2003). This would enable us to retain the advantage
of fast approximate string matching, thus being able
to utilise larger lexical resources than if we were to
calculate sophisticated alignments for each lexical
entry.
Study of the confusion matrices revealed that
some event categories such as negative regulation,
positive regulation and regulation for ID are com-
monly confused by the classifiers. Adding addi-
tional resources or contextual features may alleviate
these problems.
Figure 8: Learning curve for CALBC CII
To conclude, we have found a limited advantage
but failed to establish a clear, systematic benefit
from approximate string matching for semantic class
disambiguation. However, we have demonstrated
that approximate string matching can be used to gen-
erate novel features for classifiers and allow for the
utilisation of large scale lexical resources in new and
potentially interesting ways. It is our hope that by
making our findings, resources and implementation
available we can help the BioNLP community to
reach a deeper understanding of how best to incor-
porate our proposed features for semantic category
disambiguation and related tasks.
Our system and collection of resources are freely
available for research purposes at http://github.com/
ninjin/simsem
Acknowledgements
The authors would like to thank Dietrich Rebholz-
Schuhmann and the CALBC organisers for allowing
us the use of their data. and Jari Bjo?rne for answer-
ing questions regarding the Turku Event Corpus. We
would also like to thank the anonymous reviewers
and Luke McCrohon for their insightful and exten-
sive feedback, which has considerably helped us to
improve this work. Lastly the first author would
like to thank Makoto Miwa and Jun Hatori for their
timely and helpful advice on machine learning meth-
ods.
This work was supported by the Swedish Royal
Academy of Sciences and by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan).
143
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, et al 2000. Gene ontology: tool for the
unification of biology. The Gene Ontology Consor-
tium. Nature genetics, 25(1):25.
E. Beisswanger, M. Poprat, and U. Hahn. 2008. Lexical
Properties of OBO Ontology Class Names and Syn-
onyms. In 3rd International Symposium on Semantic
Mining in Biomedicine.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Scaling up biomedical event ex-
traction to the entire PubMed. In Proceedings of the
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 28?36. Association for Computational
Linguistics.
O Bodenreider. 2004. The unified medical language sys-
tem (umls): integrating biomedical terminology. Nu-
cleic Acids Research, 32:D267?D270.
M.F.M. Chowdhury and A. Lavelli. 2010. Disease Men-
tion Recognition with Specific Features. ACL 2010,
page 83.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010.
LINNAEUS: A species name identification system for
biomedical literature. BMC bioinformatics, 11(1):85.
K.M. Hettne, R.H. Stierum, M.J. Schuemie, P.J.M. Hen-
driksen, B.J.A. Schijvenaars, E.M. Mulligen, J. Klein-
jans, and J.A. Kors. 2009. A dictionary to identify
small molecules and drugs in free text. Bioinformat-
ics, 25(22):2983.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA), pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihasi Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
R. Leaman and G. Gonzalez. 2008. BANNER: an exe-
cutable survey of advances in biomedical named entity
recognition. In Pacific Symposium on Biocomputing,
volume 13, pages 652?663. Citeseer.
D. Maglott, J. Ostell, K.D. Pruitt, and T. Tatusova. 2005.
Entrez Gene: gene-centered information at NCBI. Nu-
cleic Acids Research, 33(suppl 1):D54.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Naoaki Okazaki and Jun?ichi Tsujii. 2010. Simple and
efficient algorithm for approximate dictionary match-
ing. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 851?859, Beijing, China, August.
M.F. Porter. 1993. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130?137.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 147?155.
Association for Computational Linguistics.
D. Rebholz-Schuhmann, A.J.J. Yepes, E.M. Van Mul-
ligen, N. Kang, J. Kors, D. Milward, P. Corbett,
E. Buyko, E. Beisswanger, and U. Hahn. 2010.
CALBC silver standard corpus. Journal of bioinfor-
matics and computational biology, 8(1):163?179.
Y. Sasaki, Y. Tsuruoka, J. McNaught, and S. Ananiadou.
2008. How to make the most of NE dictionaries in
statistical NER. BMC bioinformatics, 9(Suppl 11):S5.
L. Shi and F. Campagne. 2005. Building a protein name
dictionary from full text: a machine learning term ex-
traction approach. BMC bioinformatics, 6(1):88.
L. Tanabe, N. Xie, L. Thom, W. Matten, and W.J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC bioinfor-
matics, 6(Suppl 1):S3.
P. Thompson, S.A. Iqbal, J. McNaught, and S. Anani-
adou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
144
M. Torii, Z. Hu, C.H. Wu, and H. Liu. 2009. BioTagger-
GM: a gene/protein name recognition system. Jour-
nal of the American Medical Informatics Association,
16(2):247.
Y. Tsuruoka and J. Tsujii. 2003. Boosting precision and
recall of dictionary-based protein name recognition.
In Proceedings of the ACL 2003 workshop on Natural
language processing in biomedicine-Volume 13, pages
41?48. Association for Computational Linguistics.
Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo Pyysalo,
and Jun?ichi Tsujii. 2009. Investigating heteroge-
neous protein annotations toward cross-corpora uti-
lization. BMC Bioinformatics, 10(1):403.
C.H. Wu, L.S.L. Yeh, H. Huang, L. Arminski, J. Castro-
Alvear, Y. Chen, Z. Hu, P. Kourtesis, R.S. Ledley, B.E.
Suzek, et al 2003. The protein information resource.
Nucleic Acids Research, 31(1):345.
145
Proceedings of the Fifth Law Workshop (LAW V), pages 56?64,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Collaborative Annotation between Human Annotators and a Statistical
Parser
Shun?ya Iwasawa Hiroki Hanaoka Takuya Matsuzaki
University of Tokyo
Tokyo, Japan
{iwasawa,hkhana,matuzaki}@is.s.u-tokyo.ac.jp
Yusuke Miyao
National Institute of Informatics
Tokyo, Japan
yusuke@nii.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
Beijing, P.R.China
jtsujii@microsoft.com
Abstract
We describe a new interactive annotation
scheme between a human annotator who
carries out simplified annotations on CFG
trees, and a statistical parser that converts
the human annotations automatically into a
richly annotated HPSG treebank. In order
to check the proposed scheme?s effectiveness,
we performed automatic pseudo-annotations
that emulate the system?s idealized behavior
and measured the performance of the parser
trained on those annotations. In addition,
we implemented a prototype system and con-
ducted manual annotation experiments on a
small test set.
1 Introduction
On the basis of the success of the research on the
corpus-based development in NLP, the demand for
a variety of corpora has increased, for use as both a
training resource and an evaluation data-set. How-
ever, the development of a richly annotated cor-
pus such as an HPSG treebank is not an easy task,
since the traditional two-step annotation, in which
a parser first generates the candidates and then an
annotator checks each candidate, needs intensive ef-
forts even for well-trained annotators (Marcus et al,
1994; Kurohashi and Nagao, 1998). Among many
NLP problems, adapting a parser for out-domain
texts, which is usually referred to as domain adap-
tation problem, is one of the most remarkable prob-
lems. The main cause of this problem is the lack
of corpora in that domain. Because it is difficult to
prepare a sufficient corpus for each domain without
reducing the annotation cost, research on annotation
methodologies has been intensively studied.
There has been a number of research projects
to efficiently develop richly annotated corpora with
the help of parsers, one of which is called a
discriminant-based treebanking (Carter, 1997). In
discriminant-based treebanking, the annotation pro-
cess consists of two steps: a parser first generates
the parse trees, which are annotation candidates,
and then a human annotator selects the most plau-
sible one. One of the most important characteristics
of this methodology is to use easily-understandable
questions called discriminants for picking up the fi-
nal annotation results. Human annotators can per-
form annotations simply by answering those ques-
tions without closely examining the whole tree. Al-
though this approach has been successful in break-
ing down the difficult annotations into a set of easy
questions, specific knowledge about the grammar,
especially in the case of a deep grammar, is still re-
quired for an annotator. This would be the bottle-
neck to reduce the cost of annotator training and can
restrict the size of annotations.
Interactive predictive parsing (Sa?nchez-Sa?ez et
al., 2009; Sa?nchez-Sa?ez et al, 2010) is another ap-
proach of annotations, which focuses on CFG trees.
In this system, an annotator revises the currently
proposed CFG tree until he or she gets the correct
tree by using a simple graphical user interface. Al-
though our target product is a more richly anno-
tated treebanks, the interface of CFG can be useful
to develop deep annotations such as HPSG features
by cooperating with a statistical deep parser. Since
CFG is easier to understand than HPSG, it can re-
56
duce the cost of annotator training; non-experts can
perform annotations without decent training. As a
result, crowd-sourcing or similar approach can be
adopted and the annotation process would be accel-
erated.
Before conducting manual annotation, we sim-
ulated the annotation procedure for validating our
system. In order to check whether the CFG-based
annotations can lead to sufficiently accurate HPSG
annotations, several HPSG treebanks were created
with various qualities of CFG and evaluated by their
HPSG qualities.
We further conducted manual annotation experi-
ments by two human annotators to evaluate the ef-
ficiency of the annotation system and the accuracy
of the resulting annotations. The causes of annota-
tion errors were analyzed and future direction of the
further development is discussed.
2 Statistical Deep Parser
2.1 HPSG
Head-Driven Phrase Structure Grammar (HPSG)
is one of the lexicalized grammatical formalisms,
which consists of lexical entries and a collection of
schemata. The lexical entries represent the syntac-
tic and semantic characteristics of words, and the
schemata are the rules that construct larger phrases
from smaller phrases. Figure 1 shows the mecha-
nism of the bottom-up HPSG parsing for the sen-
tence ?Dogs run.? First, a lexical entry is as-
signed to each word, and then, the lexical signs
for ?Dogs? and ?run? are combined by Subject-
Head schema. In this way, lexical signs and phrasal
signs are combined until the whole sentence be-
comes one sign. Compared to Context Free Gram-
mar (CFG), since each sign of HPSG has rich infor-
mation about the phrase, such as subcategorization
frame or predicate-argument structure, a corpus an-
notated in an HPSG manner is more difficult to build
than CFG corpus. In our system, we aim at building
HPSG treebanks with low-cost in which even non-
experts can perform annotations.
2.2 HPSG Deep Parser
The Enju parser (Ninomiya et al, 2007) is a statis-
tical deep parser based on the HPSG formalism. It
produces an analysis of a sentence that includes the
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Dogs
2
64
HEAD verb
SUBJ < noun >
COMPS <>
3
75
Drung
?
2
64
HEAD verb
SUBJ <>
COMPS <>
3
75
Subject
1
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Headj
2
664
HEAD verb
SUBJ < 1 >
COMPS <>
3
775
Figure 1: Example of HPSG parsing for ?Dogs run.?
syntactic structure (i.e., parse tree) and the semantic
structure represented as a set of predicate-argument
dependencies. The grammar design is based on
the standard HPSG analysis of English (Pollard and
Sag, 1994). The parser finds a best parse tree
scored by a maxent disambiguation model using a
CKY-style algorithm and beam search. We used
a toolkit distributed with the Enju parser for ex-
tracting a HPSG lexicon from a PTB-style treebank.
The toolkit initially converts the PTB-style treebank
into an HPSG treebank and then extracts the lexi-
con from it. The HPSG treebank converted from the
test section is also used as the gold standard in the
evaluation.
2.3 Evaluation Metrics
In the experiments shown below, we evaluate the ac-
curacy of an annotation result (i.e., an HPSG deriva-
tion on a sentence) by evaluating the accuracy of
the semantic description produced by the deriva-
tion, as well as a more traditional metrics such
as labeled bracketing accuracy of the tree struc-
ture. Specifically, we used labeled and unlabeled
precision/recall/F-score of the predicate-argument
dependencies and the labeled brackets compared
against a gold-standard annotation obtained by using
the Enju?s treebank conversion tool. A predicate-
argument dependency is represented as a tuple of
?wp, wa, r?, where wp is the predicate word, wa
is the argument word, and r is the label of the
predicate-argument relation, such as verb-ARG1
(semantic subject of a verb) and prep-MOD (modi-
57
fiee of a prepositional phrase). As for the bracketing
accuracies, the label of a bracket is obtained by pro-
jecting the sign corresponding to the phrase into a
simple phrasal labels such as S, NP, and VP.
3 Proposed Annotation System
In our system, a human annotator and a statistical
deep parser cooperate to build a treebank. Our sys-
tem uses CFG as user interface and bridges a gap be-
tween CFG and HPSG with a statistical CKY parser.
Following the idea of the discriminant-based tree-
banking model, the parser first generates candidate
trees and then an annotator selects the correct tree in
the form of a packed forest. For selecting the correct
tree, the annotator only edits a CFG tree projected
from an HPSG tree through pre-defined set of oper-
ations, to eventually give the constraints onto HPSG
trees. This is why annotators can annotate HPSG
trees without HPSG knowledge. The current system
is implemented based on the following client-server
model.
3.1 Client: Annotator Interface
The client-side is an annotator?s interface imple-
mented with Ajax technique, on which annotator?s
revision is carried out through Web-Browser. When
the client-side receives the data of the current best
tree from the server-side, it shows an annotator the
CFG representation of the tree. Then, an annotator
adds revisions to the CFG tree using the same GUI,
until the current best tree has the CFG structure that
exactly matches the annotators? interpretation of the
sentence. Finally, the client-side sends the annota-
tor?s revision as a CGI query to the server. Based
on interactive predicative parsing system, two kinds
of operations are implemented in our system: ?span
modification? and ?label substitution?, here abbrevi-
ated as ?S? and ?L? operations:
?S? operation modify span(left, right)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified span, by sequentially clicking the leaf
nodes at the left and right boundaries.
?L? operation modify label(pos, label)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified label, by inputting a label and clicking the
node position.
In addition to ?S? and ?L? operations, one more
operation, ?tree fixation?, abbreviated ?F?, is imple-
mented for making annotation more efficient. Our
system computes the best tree under the current con-
straints, which are specified by the ?S? and ?L? op-
erations that the annotator has given so far. It means
other parts of the tree that are not constrained may
change after a new operation by the annotator. This
change may lead to a structure that the annotator
does not want. To avoid such unexpected changes,
an annotator can specify a subtree which he or she
does not want to change by ?tree fixation? operation:
?F? operation fix tree(pos = i)
An annotator can specify a subtree as correct
and not to be changed. The specified subtree
does not change and always appears in the best
tree.
3.2 Server: Parsing Constraints
In our annotation system, the server-side carries out
the conversion of annotator?s constraints into HPSG
grammatical constraints on CKY chart and the re-
computation of the current best tree under the con-
straints added so far. The server-side works in the
following two steps. The first step is the conversion
of the annotator?s revision into a collection of dead
edges or dead cells; a dead edge means the edge
must not be a part of the correct tree, and a dead cell
means all edges in the cell are dead. As mentioned
in the background section, Enju creates a CKY chart
during the parsing where all the terminal and non-
terminal nodes are stored with the information of its
sign and links to daughter edges. In our annotation
system, to change the best tree according to the an-
notator?s revision, we determine whether each edge
in the chart is either alive or dead. The server-side
re-constructs the best tree under the constraints that
all the edges used in the tree are alive. The sec-
ond step is the computation of the best tree by re-
constructing the tree from the chart, under the con-
straint that the best tree contains only the alive edges
as its subconstituents. Re-construction includes the
following recursive process:
1. Start from the root edge.
58
2. Choose the link which has the highest probabil-
ity among the links and whose daughter edges
are all alive.
3. If there is such a link, recursively carry out the
process for the daughter edge.
4. If all the links from the edge are dead, go back
to the previous edge.
Note that our system parses a sentence only once,
the first time, instead of re-parsing the sentence after
each revision. Now, we are going to list the revision
operations again and explain how the operations are
interpreted as the constraints in the CKY chart. In
the description below, label(x) means the CFG-
symbol that corresponds to edge x. Note that there
is in principle an infinite variety of possible HPSG
signs. The label function maps this multitude of
signs onto a small set of simple CFG nonterminal
symbols.
?S? operation span(left = i, right = j)
When the revision type is ?S? and the left and
right boundary of the specified span is i and j
in the CGI query, we add the cells which satisfy
the following formula to the list of dead edges.
Suppose the sentence length is L, then the set
of new dead cells is defined as:
{cell(a, b) | 0 ? a < i,i ? b < j }
? {cell(c, d) | i+ 1 ? c ? j,j + 1 ? d ? n },
where the first set means the inhibition of the
edges that span across the left boundary of the
specified span. The second set means a similar
conditions for the right span.
?L? operation fix label(position = i, label = l)
When the revision type is ?L?, the node posi-
tion is i and the label is l in the CGI query, we
determine the set of new dead edges and dead
cells as follows:
1. let cell(a, b) = the cell including i
2. mark those cells that are generated by
span(a, b) as defined above to be dead,
and
3. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= l
?F? operation fix tree(position = i)
(a) prob = 0.4 (b) prob = 0.3 (c) prob = 0.2
NP
NX
NP
Time
NX
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
Time
VP
VP
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
NP
Time
NX
flies
VP
VX
like
NP
DP
an
NX
arrow
Figure 2: Three parse tree candidates of ?Time flies like
an arrow.?
When the revision type is ?F? and the target
node position is i in the CGI query, we carry
out the following process to determine the new
dead edges and cells:
1. for each edge e in the subtree rooted at
node i,
2. let cell(a, b) = the cell including e
3. mark those cells that are generated by
span(a, b) as defined above to be dead
4. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= label(e)
The above procedure adds the constraints so
that the correct tree includes a subtree that has
the same CFG-tree representation as the sub-
tree rooted at i in the current tree.
Finally we show how the best tree for the sentence
?Time flies like an arrow.? changes with the anno-
tator?s operations. Let us assume that the chart in-
cludes the three trees shown (in the CFG representa-
tion) in (Figure 2), and that there are no dead edges.
Let us further assume that the probability of each
tree is as shown in the figure and hence the current
best tree is (a). If the annotator wants to select (b)
as the best tree, s/he can apply ?L? operation on the
root node. The operation makes some of the edges
dead, which include the root edge of tree (a) (see
Figure 3). Accordingly, the best tree is now selected
from (b), (c), etc., and tree (b) will be selected as the
next best tree.
4 Validation of CFG-based Annotation
Because our system does not present HPSG anno-
tations to the annotators, there is a risk that HPSG
annotations are wrong even when their projections
to CFG trees are completely correct. Our expecta-
59
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
fix label
(root,S)
?
Figure 3: Chart constraints by ?L? operation. Solid lines
represent the link of the current best tree and dashed lines
represent the second best one. Dotted lines stand for an
unavailable link due to the death of the source edge.
tion is that the stochastic model of the HPSG parser
properly resolves the remaining ambiguities in the
HPSG annotation within the constraints given by a
part of the CFG trees. In order to check the validity
of this expectation and to measure to what extent the
CFG-based annotations can achieve correct HPSG
annotations, we performed a pseudo-annotation ex-
periment.
In this experiment, we used bracketed sentences
in the Brown Corpus (Kuc?era and Francis, 1967),
and a court transcript portion of the Manually An-
notated Sub-Corpus (MASC) (Ide et al, 2010). We
automatically created HPSG annotations that mimic
the annotation results by an ideal annotator in the
following four steps. First, HPSG treebanks for
these sentences are created by the treebank conver-
sion program distributed with the Enju parser. This
program converts a syntactic tree annotated by Penn
Treebank style into an HPSG tree. Since this pro-
gram cannot convert the sentences that are not cov-
ered by the basic design of the grammar, we used
only those that are successfully converted by the
program throughout the experiments and considered
this converted treebank as the gold-standard tree-
bank for evaluation. Second, the same sentences are
parsed by the Enju parser and the results are com-
pared with the gold-standard treebank. Then, CFG-
level differences between the Enju parser?s outputs
and the gold-standard trees are translated into oper-
ation sequences of the annotation system. For ex-
ample, ?L? operation of NX ? VP at the root node
is obtained in the case of Figure 4. Finally, those
operation sequences are executed on the annotation
system and HPSG annotations are produced.
total size ave. s. l. convertible
Brown 24,243 18.94 22,214
MASC 1,656 14.81 1,353
Table 1: Corpus and experimental data information (s. l.
means ?sentence length.?)
(a) NX
NX PP
PX NP
(b) VP
VP PP
PX NP
Figure 4: CFG representation of parser output (a) and
gold-standard tree (b)
4.1 Relationship between CFG and HPSG
Correctness
We evaluated the automatically produced annota-
tions in terms of three measures: the labeled brack-
eting accuracies of their projections to CFG trees,
the accuracy of the HPSG lexical entry assignments
to the words, and the accuracy of the semantic de-
pendencies extracted from the annotations. The
CFG-labeled bracketing accuracies are defined in
the same way as the traditional PARSEVAL mea-
sures. The HPSG lexical assignment accuracy is
the ratio of words to which the correct HPSG lex-
ical entry is assigned, and the semantic dependency
accuracy is defined as explained in Section 2.3. In
this experiment, we cut off sentences longer than 40
words for time reasons. We split the Brown Cor-
pus into three parts: training, development test and
evaluation, and evaluated the automatic annotation
results only for the training portion.
We created three sets of automatic annotations as
follows:
Baseline No operation; default parsing results are
considered as the annotation results.
S-full Only ?S? operations are used; the tree struc-
tures of the resulting annotations should thus be
identical to the gold-standard annotations.
SL-full ?S? and ?L? operations are used; the la-
beled tree structures of the resulting anno-
tations should thus be identical to the gold-
standard annotations.
Before showing the evaluation results, splitting of
the data should be described here. Our system as-
sumes that the correct tree is included in the parser?s
60
CKY chart; however, because of the beam-search
limitation and the incomplete grammar coverage, it
does not always hold true. In this paper, such sit-
uations are called ?out-chart?. Conversely, the sit-
uations in which the parser does include the cor-
rect tree in the CKY chart are ?in-chart?. The re-
sults of ?in-chart? are here considered to be the re-
sults in the ideal situation of the perfect parser. In
our experimental setting, the training portion of the
Brown Corpus has 10,576 ?in-chart? and 7,208 ?out-
chart? sentences, while the MASC portion has 864
?in-chart? and 489 ?out-chart? sentences (Table 2).
Under ?out-chart? situations, we applied the opera-
tions greedily for calculating S-full and SL-full; that
is, all operations are sequentially applied and an op-
eration is skipped when there are no HPSG trees in
the CKY chart after applying that operation.
Table 3 shows the results of our three measures:
the CFG tree bracketing accuracy, the accuracy of
HPSG lexical entry assignment and that of the se-
mantic dependency. In both of S-full and SL-full,
the improvement from the baseline is significant.
Especially, SL-full for ?in-chart? data has almost
complete agreement with the gold-standard HPSG
annotations. The detailed figures are shown in Ta-
ble 4. Therefore, we can therefore conclude that
high quality CFG annotations lead to high quality
HPSG annotations when the are combined with a
good statistical HPSG parser.
4.2 Domain Adaptation
We evaluated the parser accuracy adapted with the
automatically created treebank on the Brown Cor-
pus. In this experiment, we used the adaptation al-
gorithm by (Hara et al, 2007), with the same hyper-
parameters used there. Table 5 shows the result of
the adapted parser. Each line of this table stands for
the parser adapted with different data. ?Gold? is the
result adapted on the gold-standard annotations, and
?Gold (only covered)? is that adapted on the gold
data which is covered by the original Enju HPSG
grammar that was extracted from the WSJ portion
of the Penn Treebank. ?SL-full? is the result adapted
on our automatically created data. ?Baseline? is the
result by the original Enju parser, which is trained
only on the WSJ-PTB and whose grammar was ex-
tracted from the WSJ-PTB. The table shows SL-full
slightly improves the baseline results, which indi-
#operations
S L F Avg. Time
Brown A. 1 122 1 0 1.19 43.32A. 2 91 4 1 0.94 41.77
MASC A. 1 275 2 5 2.76 33.33A. 2 52 2 0 0.51 35.13
Table 6: The number of operations and annotation time
by human annotators. ?Annotator? is abbreviated as A.
Avg. is the average number of operations per sentence
and Time is annotation time per sentence [sec.].
cates our annotation system can be useful for do-
main adaptation. Because we used mixed data of
?in-chart? and ?out-chart? in this experiment, there
still is much room for improvement by increasing
the ratio of the ?in-chart? sentences using a larger
beam-width.
5 Interactive Annotation on a
Prototype-system
We developed an initial version of the annotation
system described in Section 3, and annotated 200
sentences in total on the system. Half of the sen-
tences were taken from the Brown corpus and the
other half were taken from a court-debate section of
the MASC corpus. All of the sentences were an-
notated twice by two annotators. Both of the anno-
tators has background in computer science and lin-
guistics.
Table 6 shows the statistics of the annotation pro-
cedures. This table indicates that human annotators
strongly prefer ?S? operation to others, and that the
manual annotation on the prototype system is at least
comparable to the recent discriminant-based annota-
tion system by (Zhang and Kordoni, 2010), although
the comparison is not strict because of the difference
of the text.
Table 7 shows the automatic evaluation results.
We can see that the interactive annotation gave slight
improvements in all accuracy metrics. The improve-
ments were however not as much as we desired.
By classifying the remaining errors in the anno-
tation results, we identified several classes of major
errors:
1. Truly ambiguous structures, which require the
context or world-knowledge to correctly re-
solve them.
61
in out in+out
Brown (train.) 10,576 / 10,394 7,190 / 6,464 17,766 / 16,858
MASC 864 / 857 489 / 449 1,353 / 1,306
Table 2: The number of ?in-chart? and ?out-chart? sentences (total / 1-40 length)
in out in+out
Brown
SL-full 100.00 / 99.31 / 99.60 88.67 / 83.95 / 82.00 94.91 / 92.21 / 92.24
S-full 98.46 / 96.64 / 96.83 89.60 / 82.02 / 81.20 94.48 / 89.88 / 90.29
Baseline 92.39 / 92.69 / 90.54 82.10 / 78.38 / 73.80 87.78 / 86.07 / 83.54
MASC
SL-full 100.00 / 99.13 / 99.30 85.91 / 80.75 / 78.80 93.38 / 90.55 / 91.02
S-full 98.71 / 96.88 / 96.73 86.95 / 79.14 / 77.43 93.18 / 88.60 / 88.93
Baseline 93.98 / 93.51 / 91.56 80.00 / 75.89 / 72.22 87.43 / 85.30 / 83.75
Table 3: Evaluation of the automatic annotation sets. Each cell has the score of CFG F1 / Lex. Acc. / Dep. F1.
CFG tree accuracy
Brown MASC
A. 1 90.55 / 90.83 / 90.69 90.62 / 90.80 / 90.71
A. 2 91.01 / 91.09 / 91.05 91.01 / 91.09 / 91.05
Enju 89.70 / 89.74 / 89.72 90.02 / 90.20 / 90.11
PAS dependency accuracy
Brown MASC
A. 1 87.48 / 87.55 / 87.52 86.02 / 86.02 / 86.02
A. 2 88.42 / 88.27 / 88.34 85.28 / 91.01 / 85.32
Enju 87.12 / 86.91 / 87.01 84.81 / 84.26 / 84.53
Table 7: Automatic evaluation of the annotation results
(LP / LR / F1)
CFG tree accuracy
in-chart out-chart
A. 1 94.52 / 94.65 / 94.58 83.95 / 84.44 / 84.19
A. 2 95.07 / 95.14 / 95.10 84.22 / 84.32 / 84.27
Enju 94.44 / 94.37 / 94.40 81.81 / 82.00 / 81.90
PAS dependency accuracy
in-chart out-chart
A. 1 92.85 / 92.85 / 92.85 77.47 / 77.65 / 77.56
A. 2 93.34 / 93.34 / 93.34 79.17 / 78.80 / 78.98
Enju 92.73 / 92.73 / 92.73 76.57 / 76.04 / 76.30
Table 8: Automatic evaluation of the annotation results
(LP/LR/F1); in-chart sentences (left-column) and out-
chart sentences (right column) both from Brown
2. Purely grammar-dependent analyses, which re-
quire in-depth knowledge of the specific HPSG
grammar behind the simplified CFG-tree repre-
sentation given to the annotators.
3. Discrepancy between human intuition and the
convention in the HPSG grammar introduced
by the automatic conversion.
4. Apparently wrong analysis left untouched due
to the limitation of the annotation system.
We suspect some of the errors of type 1 have been
caused by the experimental setting of the annotation;
we gave the test sentences randomly drawn from
the corpus in a randomized order. This would have
made it difficult for the annotators to interpret the
sentences correctly. We thus expect this kind of er-
rors would be reduced by doing the annotation on a
larger chunk of text.
The second type of the errors are due to the fact
that the annotators are not familiar with the details
of the Enju English HPSG grammar. For example,
one of the annotators systematically chose a struc-
ture like (NP (NP a cat) (PP on the mat)). This struc-
ture is however always analysed as (NP a (NP? cat
(PP on the mat))) by the Enju grammar. The style of
the analysis implemented in the grammar thus some-
times conflicts with the annotators? intuition and it
introduces errors in the annotation results.
Our intention behind the design of the annotation
system was to make the annotation system more ac-
cessible to non-experts and reduce the cost of the
annotation. To reduce the type 2 errors, rather than
the training of the annotators for a specific gram-
mar, we plan to introduce another representation
system in which the grammar-specific conventions
become invisible to the annotators. For example, the
above-shown difference in the bracketing structures
of a determiner-noun-PP sequence can be hidden by
showing the noun phrase as a ternary branch on the
three children: (NP a cat (PP on the mat)).
The third type of the errors are mainly due to the
rather arbitrary choice of the HPSG analysis intro-
duced through the semi-automatic treebank conver-
sion used to extract the HPSG grammar. For in-
stance, the Penn Treebank annotates a structure in-
cluding an adverb that intervenes an auxiliary verb
62
Lex-Acc Dep-LP Dep-LR Dep-UP Dep-UR Dep-F1 Dep-EM
Brown 99.26 99.61 99.59 99.69 99.67 99.60 95.80
MASC 99.13 99.26 99.33 99.42 99.49 99.30 95.68
Table 4: HPSG agreement of SL-full for ?in-chart? data (EM means ?Exact Match.?)
LP LR UP UR F1 EM
Gold 85.62 85.41 89.70 69.47 85.51 45.07
Gold (only covered) 84.32 84.01 88.72 88.40 84.17 42.52
SL-full 83.27 82.88 87.93 87.52 83.08 40.19
Baseline 82.64 82.20 87.50 87.03 82.42 37.63
Table 5: Domain Adaptation Results
and a following verb as in (VP is (ADVP already)
installed). The attachment direction of the adverb is
thus left unspecified. Such structures are however
indistinguishably transformed to a binary structure
like (VP (VP? is already) installed) in the course of
the conversion to HPSG analysis since there is no
way to choose the proper direction only with the
information given in the source corpus. This de-
sign could be considered as a best-effort, systematic
choice under the insufficient information, but it con-
flicts with the annotators? intuition in some cases.
We found in the annotation results that the anno-
tators have left apparently wrong analyses on some
sentences, either those remaining from the initial
output proposed by the parser or a wrong structure
appeared after some operations by the annotators
(error type 4). Such errors are mainly due to the
fact that for some sentences a correct analysis cannot
be found in the parser?s CKY chart. This can hap-
pen either when the correct analysis is not covered
by the HPSG grammar, or the correct analysis has
been pruned by the beam-search mechanism in the
parser. To correct a wrong analysis from the insuffi-
cient grammar coverage, an expansion of the gram-
mar is necessary, either in the form of the expan-
sion of the lexicon, or an introduction of a new lex-
ical type. For the other errors from the beam-search
limitation, there is a chance to get a correct analysis
from the parser by enlarging the beam size as nec-
essary. The introduction of a new lexical type def-
initely requires a deep knowledge on the grammar
and thus out of the scope of our annotation frame-
work. The other cases can in principle be handled in
the current framework, e.g., by a dynamic expansion
of the lexicon (i.e., an introduction of a new associ-
ation between a word and known lexical type), and
by a dynamic tuning of the beam size.
To see the significance of the last type of the er-
ror, we re-evaluated the annotation results on the
Brown sentences after classifying them into: (1)
those for which the correct analyses were included
in the parser?s chart (in-chart, 65 sentences) and (2)
those for which the correct analyses were not in the
chart (out-chart, 35 sentences), either because of the
pruning effect or the insufficient grammar coverage.
The results shown in Table 8 clearly show that there
is a large difference in the accuracy of the annota-
tion results between these two cases. Actually, on
the in-chart sentences, the parser has returned the
correct analysis as the initial solution for over 50%
of the sentences, and the annotators saved it without
any operations. Thus, we believe it is quite effective
to add the above-mentioned functionalities to reduce
this type of errors.
6 Conclusion and Future Work
We proposed a new annotation framework for deep
grammars by using statistical parsers. From the the-
oretical point of view, we can achieve significantly
high quality HPSG annotations only by CFG annota-
tions, and the products can be useful for the domain
adaptation task. On the other hand, preliminary ex-
periments of a manual annotation show some diffi-
culties about CFG annotations for non-experts, es-
pecially grammar-specific ones. We hence need to
develop some bridging functions reducing such dif-
ficulties. One possible strategy is to introduce an-
other representation such as flat CFG than binary
CFG. While we adopted CFG interface in our first
prototype system, our scheme can be applied to an-
other interface such as dependency as long as there
exist some relatedness over syntax or semantics.
63
References
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Workshop On
Computational Environments For Grammar Develop-
ment And Linguistic Engineering, pages 9?15.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of the 10th International Con-
ference on Parsing Technologies, pages 11?22, Prague,
Czech Republic.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a japanese parsed corpus while improving the parsing
system. In Proceedings of the NLPRS, pages 719?724.
Henry Kuc?era and W. Nelson Francis. 1967. Compu-
tational Analysis of Present Day American English.
Brown University Press, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on Human Language
Technology, pages 114?119.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of the 10th International Confer-
ence on Parsing Technologies, pages 60?68.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez, and Jose?-
Miguel Bened??. 2009. Interactive predictive parsing.
In Proceedings of the 11th International Conference
on Parsing Technologies, pages 222?225.
Ricardo Sa?nchez-Sa?ez, Luis A. Leiva, Joan-Andreu
Sa?nchez, and Jose?-Miguel Bened??. 2010. Interactive
predictive parsing using a web-based architecture. In
Proceedings of the NAACL HLT 2010 Demonstration
Session, pages 37?40.
Yi Zhang and Valia Kordoni. 2010. Discriminant rank-
ing for efficient treebanking. In Coling 2010: Posters,
pages 1453?1461, Beijing, China, August. Coling
2010 Organizing Committee.
64
Learning the Optimal use of Dependency-parsing Information for Finding
Translations with Comparable Corpora
Daniel Andrade?, Takuya Matsuzaki?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp
?Microsoft Research Asia, Beijing
jtsujii@microsoft.com
Abstract
Using comparable corpora to find new word
translations is a promising approach for ex-
tending bilingual dictionaries (semi-) auto-
matically. The basic idea is based on the
assumption that similar words have similar
contexts across languages. The context of
a word is often summarized by using the
bag-of-words in the sentence, or by using
the words which are in a certain dependency
position, e.g. the predecessors and succes-
sors. These different context positions are
then combined into one context vector and
compared across languages. However, previ-
ous research makes the (implicit) assumption
that these different context positions should be
weighted as equally important. Furthermore,
only the same context positions are compared
with each other, for example the successor po-
sition in Spanish is compared with the suc-
cessor position in English. However, this is
not necessarily always appropriate for lan-
guages like Japanese and English. To over-
come these limitations, we suggest to perform
a linear transformation of the context vec-
tors, which is defined by a matrix. We de-
fine the optimal transformation matrix by us-
ing a Bayesian probabilistic model, and show
that it is feasible to find an approximate solu-
tion using Markov chain Monte Carlo meth-
ods. Our experiments demonstrate that our
proposed method constantly improves transla-
tion accuracy.
1 Introduction
Using comparable corpora to automatically extend
bilingual dictionaries is becoming increasingly pop-
ular (Laroche and Langlais, 2010; Andrade et al,
2010; Ismail and Manandhar, 2010; Laws et al,
2010; Garera et al, 2009). The general idea is
based on the assumption that similar words have
similar contexts across languages. The context of
a word can be described by the sentence in which
it occurs (Laroche and Langlais, 2010) or a sur-
rounding word-window (Rapp, 1999; Haghighi et
al., 2008). A few previous studies, like (Garera et
al., 2009), suggested to use the predecessor and suc-
cessors from the dependency-parse tree, instead of a
word window. In (Andrade et al, 2011), we showed
that including dependency-parse tree context posi-
tions together with a sentence bag-of-words context
can improve word translation accuracy. However
previous works do not make an attempt to find an
optimal combination of these different context posi-
tions.
Our study tries to find an optimal weighting and
aggregation of these context positions by learning
a linear transformation of the context vectors. The
motivation is that different context positions might
be of different importance, e.g. the direct predeces-
sors and successors from the dependency tree might
be more important than the larger context from the
whole sentence. Another motivation is that depen-
dency positions cannot be always compared across
different languages, e.g. a word which tends to oc-
cur as a modifier in English, can tend to occur in
Japanese in a different dependency position.
As a solution, we propose to learn the optimal
combination of dependency and bag-of-words sen-
tence information. Our approach uses a linear trans-
formation of the context vectors, before comparing
10
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 10?18,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
them using the cosine similarity. This can be con-
sidered as a generalization of the cosine similarity.
We define the optimal transformation matrix by the
maximum-a-posterior (MAP) solution of a Bayesian
probabilistic model. The likelihood function for a
translation matrix is defined by considering the ex-
pected achieved translation accuracy. As a prior, we
use a Dirichlet distribution over the diagonal ele-
ments in the matrix and a uniform distribution over
its non-diagonal elements. We show that it is fea-
sible to find an approximation of the optimal so-
lution using Markov chain Monte Carlo (MCMC)
methods. In our experiments, we compare the pro-
posed method, which uses this approximation, with
the baseline method which uses the cosine similarity
without any linear transformation. Our experiments
show that the translation accuracy is constantly im-
proved by the proposed method.
In the next section, we briefly summarize the most
relevant previous work. In Section 3, we then ex-
plain the baseline method which is based on previ-
ous research. Section 4 explains in detail our pro-
posed method, followed by Section 5 which pro-
vides an empirical comparison to the baseline, and
analysis. We summarize our findings in Section 6.
2 Previous Work
Using comparable corpora to find new translations
was pioneered in (Rapp, 1999; Fung, 1998). The ba-
sic idea for finding a translation for a word q (query),
is to measure the context of q and then to compare
the context with each possible translation candidate,
using an existing dictionary. We will call words
for which we have a translation in the given dic-
tionary, pivot words. First, using the source cor-
pus, they calculate the degree of association of a
query word q with all pivot words. The degree of
association is a measure which is based on the co-
occurrence frequency of q and the pivot word in a
certain context position. A context (position) can be
a word-window (Rapp, 1999), sentence (Utsuro et
al., 2003), or a certain position in the dependency-
parse tree (Garera et al, 2009; Andrade et al, 2011).
In this way, they get a context vector for q, which
contains the degree of association to the pivot words
in different context positions. Using the target cor-
pus, they then calculate a context vector for each
possible translation candidate x, in the same way.
Finally, they compare the context vector of q with
the context vector of each candidate x, and retrieve
a ranked list of possible translation candidates. In
the next section, we explain the baseline which is
based on that previous research.
The general idea of learning an appropriate
method to compare high-dimensional vectors is not
new. Related research is often called ?metric-
learning?, see for example (Xing et al, 2003; Basu
et al, 2004). However, for our objective function it
is difficult to find an analytic solution. To our knowl-
edge, the idea of parameterizing the transformation
matrix, in the way we suggest in Section 4, and to
learn an approximate solution with a fast sampling
strategy is new.
3 Baseline
Our baseline measures the degree of association be-
tween the query word q and each pivot word with
respect to several context positions. As a context
position we consider the predecessors, successors,
siblings with respect to the dependency parse tree,
and the whole sentence (bag-of-words). The depen-
dency information which is used is also illustrated in
Figure 1. As a measure of the degree of association
we use the Log-odds-ratio as proposed in (Laroche
and Langlais, 2010).
Figure 1: Example of the dependency information used
by our approach. Here, from the perspective of ?door?.
Next, we define the context vector which contains
the degree of association between the query and each
pivot in several context positions. First, for each
11
context position i we define a vector qi which con-
tains the degree of association with each pivot word
in the context position i. If we number the pivot
words from 1 to n, then this vector can be writ-
ten as qi = (q1i , . . . , qni ). Note that in our case i
ranges from 1 to 4, representing the context posi-
tions predecessors (1), successors (2), siblings (3),
and the sentence bag-of-words (4). Finally, the com-
plete context vector for the query q is a long vector
q which appends each qi, i.e.: q = (q1, . . . ,q4).
Next, in the same way as before, we create a con-
text vector x for each translation candidate x in the
target language. For simplicity, we assume that each
pivot word in the source language has only one cor-
responding translation in the target language. As
a consequence, the dimensions of q and x are the
same. Finally we can score each translation candi-
date by using the cosine similarity between q and
x.
We claim that all of the context positions (1 to 4)
can contain information which is helpful to identify
translation candidates. However, we do not know
about their relative importance, neither do we know
whether these dependency positions can be com-
pared across language pairs as different as Japanese
and English. The cosine similarity simply weights
all dependency position equally important and ig-
nores problems which might occur when comparing
dependency positions across languages.
4 Proposed Method
Our proposed method tries to overcome the short-
comings of the cosine-similarity by using the fol-
lowing generalization:
sim(q,x) = qAx
T
?qAqT?xAxT , (1)
where A is a positive-definite matrix in Rdn?dn, and
T is the transpose of a vector. This can also be con-
sidered as linear transformation of the vectors using?A before using the normal cosine similarity, see
also (Basu et al, 2004).1
The challenge is to find an appropriate matrix A
which is expected to take the correlations between
1Therefore, exactly speaking A is not the transformation
matrix, however it defines uniquely the transformation matrix?
A.
the different dimensions into account, and which op-
timally weights the different dimensions. Note that,
if we set A to the identity matrix, we recover the
normal cosine similarity, which is our baseline.
Clearly, finding an optimal matrix in Rdn?dn is
infeasible due to the high dimensionality. We will
therefore restrict the structure of A.
Let I be the identity matrix in Rn?n , then we
define the matrix A, as follows:
A =
?
???
d1I z1,2I z1,3I z1,4I
z1,2I d2I z2,3I z2,4I
z1,3I z2,3I d3I z3,4I
z1,4I z2,4I z3,4I d4I
?
???
It is clear from this definition that d1, . . . , d4 weights
the context positions 1 to 4. Furthermore, zi,j can
be interpreted as a the confusion coefficient between
context position i and j. For example, a high value
for z2,3 means that a pivot word which occurs in
the sibling position in Japanese (source language),
might not necessarily occur in the sibling position in
English (target language), but instead in the succes-
sor position. However, in order to reduce the dimen-
sionality of the parameter space further, we assume
that each such zi,j has the same value z. Therefore,
matrix A becomes
A =
?
???
d1I zI zI zI
zI d2I zI zI
zI zI d3I zI
zI zI zI d4I
?
??? .
In the next subsection we will explain how we de-
fine an optimal solution for A.
4.1 Optimal solution for A
We use a Bayesian probabilistic model in order to
define the optimal solution for A. Formally we try
to find the maximum-a-posterior (MAP) solution of
A, i.e.:
argmax
A
p(A|data, ?). (2)
The posterior probability is defined by
p(A|data, ?) ? fauc(data|A) ? p(A|?) . (3)
fauc(data|A) is the (unnormalized) likelihood func-
tion. p(A|?) is the prior that captures our prior be-
liefs about A, and which is parameterized by a hy-
perparameter ?.
12
4.1.1 The likelihood function fauc(data|A)
As a likelihood function we use a modification
of the area under the curve (AUC) of the accuracy-
vs-rank graph. The accuracy-vs-rank graph shows
the translation accuracy at different ranks. data
refers to the part of the gold-standard which is used
for training. Our complete gold-standard contains
443 domain-specific Japanese nouns (query words).
Each Japanese noun in the gold standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We de-
note the accuracy at rank r, by accr. The accuracy
accr is determined by counting how often the cor-
rect answer is listed in the top r translation candi-
dates suggested for a query, divided by the number
of all queries in data. The likelihood function is
now defined as follows:
fauc(data|A) =
20?
r=1
accr ? (21 ? r) . (4)
That means fauc(data|A) accumulates the accura-
cies at the ranks from 1 to 20, where we weight ac-
curacies at top ranks higher.
4.1.2 The prior p(A|?)
The prior over the transformation matrix is factor-
ized in the following manner:
p(A|?) = p(z|d1, . . . , d4) ? p(d1, . . . , d4|?) .
The prior over the diagonal is defined as a Dirichlet
distribution:
p(d1, . . . , d4|?) = 1B(?)
4?
i=1
d??1i
where ? is the concentration parameter of the sym-
metric Dirichlet, and B(?) is the normalization con-
stant. The prior over the non-diagonal value a is de-
fined as:
p(z|d1, . . . , d4) = 1? ? 1[0,?](z) (5)
where ? = min{d1, . . . , d4}.
First, note that our prior limits the possible matri-
ces A to matrices which have diagonal entries which
are between 0 and 1. This is not a restriction since
the ranking of the translation candidates induced by
the parameterized cosine similarity will not change
if A is multiplied by a constant c > 0 . To see this,
note that
sim(q,x) = q(c ?A)x?q(c ?A)q?x(c ?A)x
= qAx?qAq?xAx .
Second, note that our prior limits A further, by re-
quiring, in Equation (5), that every non-diagonal el-
ement is smaller or equal than any diagonal element.
That requirement is sensible since we do not expect
that a optimal similarity measure between English
and Japanese will prefer context which is similar in
different dependency positions, over context which
is similar in the same context positions. To see this,
imagine the extreme case where for example d1 is 0,
and instead z12 is 1. In that case the similarity mea-
sure would ignore any similarity in the predecessor
position, but would instead compare the predeces-
sors in Japanese with the successors in English.
Finally, note that our prior puts probability mass
over a subset of the positive-definite matrices in
R4?4, and puts no probability mass on matrices
which are not positive-definite. As a consequence,
the similarity measure in Equation (1) is ensured to
be well-defined.
4.2 Training
In the following we explain how we use the training
data in order to find a good solution for the matrix
A.
4.2.1 Setting hyperparameter ?
Recall, that ? weights our prior belief about how
strong we think that the different context positions
should be weighted equally. From a practical point-
of-view, we do not know how strong we should
weight that prior belief. We therefore use empirical
Bayes to estimate ?, that is we use part of the train-
ing data to set ?. First, using half of the training
set, we find the A which maximizes p(A|data, ?)
for several ?. Then, the remaining half of the train-
ing set is used to evaluate fauc(data|A) to find the
best ?. Note that the prior p(A|?) can also be con-
sidered as a regularization to prevent overfitting. In
the next sub-section we will explain how to find an
approximation ofAwhich maximizes p(A|data, ?).
13
4.2.2 Finding a MAP solution for A
Recall that matrix A is defined by using only five
parameters. Since the problem is low-dimensional,
we can therefore expect to find a reasonable solution
using sampling methods. For finding an approxima-
tion of the maximum-a-posteriori (MAP) solution of
p(A|data, ?), we use the following Markov chain
Monte Carlo procedure:
1. Initialize d1, . . . , d4 and z.
2. Leave z constant, and run Simulated-
Annealing to find the d1, . . . , d4 which
maximize p(A|data, ?).
3. Given d1, . . . , d4, sample from the uniform dis-
tribution [1,min(d1, . . . d4)] in order to find the
z which maximizes p(A|data, ?).
The steps 2. and 3. are repeated till the convergence
of the parameters.
Concerning step 2., we use Simulated-
Annealing for finding a (local) maximum of
p(d1, . . . , d4|data, ?) with the following settings:
As a jumping distribution we use a Dirichlet distri-
bution which we update every 1000 iterations. The
cooling rate is set to 1iteration .
For step 2. and 3. it is of utmost importance to
be able to evaluate p(A|data, ?) fast. The com-
putationally expensive part of p(A|data, ?) is to
evaluate fauc(data|A). In order to quickly evalu-
ate fauc(data|A), we need to pre-calculate part of
sim(q, x) for all queries q and all translation can-
didates x. To illustrate the basic idea, consider
sim(q, x) without the normalization of q and xwith
respect to A, i.e.:
sim(q, x) = qAxT = (q1, . . . ,q4)A(x1, . . . ,x4)T .
Let us denote I?dn a block matrix in Rdn?dn whichcontains in each n ? n block the identity matrix ex-
cept in its diagonal; the diagonal of I?dn contains the
n ? n matrix which is zero in all entries. We can
now rewrite matrix A as:
A =
?
???
d1I 0 0 0
0 d2I 0 0
0 0 d3I 0
0 0 0 d4I
?
???+ z ? I?dn .
And finally we can factor out the parameters
(d1, . . . d4) and z in the following way:
sim(q, x) = (d1, . . . , d4)?
?
??
q1xT1...
q4xT4
?
??+z?(qI?dnxT )
By pre-calculating
?
??
q1xT1...
q4xT4
?
?? and qI?dnxT , we can
make the evaluation of each sample, in steps 2. and
3., computationally feasible.
5 Experiments
In the experiments of the present study, we used
a collection of complaints concerning automobiles
compiled by the Japanese Ministry of Land, Infras-
tructure, Transport and Tourism (MLIT)2 and an-
other collection of complaints concerning automo-
biles compiled by the USA National Highway Traf-
fic Safety Administration (NHTSA)3. Both corpora
are publicly available. The corpora are non-parallel,
but are comparable in terms of content. The part
of MLIT and NHTSA which we used for our ex-
periments, contains 24090 and 47613 sentences, re-
spectively. The Japanese MLIT corpus was mor-
phologically analyzed and dependency parsed using
Juman and KNP4. The English corpus NHTSA was
POS-tagged and stemmed with Stepp Tagger (Tsu-
ruoka et al, 2005; Okazaki et al, 2008) and depen-
dency parsed using the MST parser (McDonald et
al., 2005). Using the Japanese-English dictionary
JMDic5, we found 1796 content words in Japanese
which have a translation which is in the English cor-
pus. These content words and their translations cor-
respond to our pivot words in Japanese and English,
respectively.6
2http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
3http://www-odi.nhtsa.dot.gov/downloads/index.cfm
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html and http://www-lab25.kuee.kyoto-
u.ac.jp/nl-resource/knp.html
5http://www.csse.monash.edu.au/ jwb/edict doc.html
6Recall that we assume a one-to-one correspondence be-
tween a pivot in Japanese and English. If a Japanese pivot word
as more than one English translation, we select the translation
for which the relative frequency in the target corpus is closest
to the pivot in the source corpus.
14
5.1 Evaluation
For the evaluation we extract a gold-standard which
contains Japanese and English noun pairs that ac-
tually occur in both corpora.7 The gold-standard
is created with the help of the JMDic dictionary,
whereas we correct apparently inappropriate trans-
lations, and remove general nouns such as ???
(possibility) and ambiguous words such as? (rice,
America). In this way, we obtain a final list of 443
domain-specific Japanese nouns.
Each Japanese noun in the gold-standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We divide
the gold-standard into two halves. The first half is
used for for learning the matrix A, the second part
is used for the evaluation. In general, we expect that
the optimal transformation matrixA depends mainly
on the languages (Japanese and English) and on the
corpora (MLIT and NHTSA). However, in practice,
the optimal matrix can also vary depending on the
part of the gold-standard which is used for training.
These random variations are especially large, if the
part of the gold-standard which is used for training
or testing is small.
In order to take these random effects into ac-
count, we perform repeated subsampling of the
gold-standard. In detail, we randomly split the gold-
standard into equally-sized training and test set. This
is repeated five times, leading to five training and
five test sets. The performance on each test set is
shown in Table 1. OPTIMIZED-ALL marks the re-
sult of our proposed method, where matrix A is opti-
mized using the training set. The optimization of the
diagonal elements d1, . . . , d4, and the non-diagonal
value z is as described in Section 4.2. Finally, the
baseline method, as described in 3, corresponds to
OPTIMIZED-ALL where d1, . . . , d4 are set to 1,
and z is set to 0. This baseline is denoted as NOR-
MAL. We can see that the overall translation accu-
racy varies across the test sets. However, we see that
in all test sets our proposed method OPTIMIZED-
ALL performs better than the baseline NORMAL.
7Note that if the current query (Japanese noun) is a pivot
word, then the word is not considered as a pivot word.
5.2 Analysis
In the previous section, we showed that the cosine-
similarity is sub-optimal for comparing context vec-
tors which contain information from different con-
text positions. We showed that it is possible to find
an approximation of a matrix A which optimally
weights, and combines the different context posi-
tions. Recall, that the matrix A is described by the
parameters d1 . . . d4 and z, which can interpreted as
context position weights and a confusion coefficient,
respectively. Therefore, by looking at these parame-
ters which we learned using each training set, we can
get some interesting insights. Table 2 shows theses
parameters learned for each training set.
We can see that the parameters, across the train-
ing sets, are not as stable as we wish. For example
the weight for the predecessor position ranges from
0.27 to 0.44. As a consequence, the average values,
shown in the last row of Table 2, have to be inter-
preted with care. We expect that the variance is due
to the limited size of the training set, 220 <query,
answers> pairs.
Nevertheless, we can draw some conclusions with
confidence. For example, we see that the prede-
cessor and successor positions are the most impor-
tant contexts, since the weights for both are al-
ways higher than for the other context positions.
Furthermore, we clearly see that the sibling and
sentence (bag-of-words) contexts, although not as
highly weighted as the former two, can be consid-
ered to be relevant, since each has a weight of around
0.20. Finally, we see that z, the confusion coeffi-
cient, is around 0.03, which is small.8 Therefore,
we verify z?s usefulness with another experiment.
We additionally define the method OPTIMIZED-
DIAG which uses the same matrix as OPTIMIZED-
ALL except that the confusion coefficient z is set
to zero. In Table 1, we can see that the accu-
racy of OPTIMIZED-DIAG is constantly lower than
OPTIMIZED-ALL.
Furthermore, we are interested in the role of the
whole sentence (bag-of-words) information which is
in the context vector (in position d4 of the block vec-
tor). Therefore, we excluded the sentence informa-
8In other words, z is around 17% of its maximal possible
value. The maximal possible value is around 0.18, since, recall
that z is, by definition, smaller or equal to min{d1 . . . d4}.
15
Test Set Method Top-1 Top-5 Top-10 Top-15 Top-20Accuracy Accuracy Accuracy Accuracy Accuracy
1
OPTIMIZED-ALL 0.20 0.37 0.47 0.50 0.54
OPTIMIZED-DIAG 0.20 0.34 0.43 0.48 0.51
NORMAL 0.18 0.32 0.43 0.47 0.50
2
OPTIMIZED-ALL 0.20 0.35 0.43 0.48 0.52
OPTIMIZED-DIAG 0.19 0.33 0.42 0.46 0.52
NORMAL 0.18 0.34 0.42 0.47 0.49
3
OPTIMIZED-ALL 0.17 0.31 0.37 0.44 0.48
OPTIMIZED-DIAG 0.17 0.27 0.36 0.41 0.45
NORMAL 0.16 0.27 0.36 0.41 0.44
4
OPTIMIZED-ALL 0.14 0.30 0.38 0.43 0.46
OPTIMIZED-DIAG 0.14 0.26 0.34 0.4 0.43
NORMAL 0.15 0.29 0.37 0.41 0.44
5
OPTIMIZED-ALL 0.18 0.34 0.42 0.46 0.51
OPTIMIZED-DIAG 0.17 0.30 0.38 0.43 0.48
NORMAL 0.19 0.31 0.40 0.44 0.48
average
OPTIMIZED-ALL 0.18 0.33 0.41 0.46 0.50
OPTIMIZED-DIAG 0.17 0.30 0.39 0.44 0.48
NORMAL 0.17 0.31 0.40 0.44 0.47
Table 1: Shows the accuracy at different ranks for all test sets, and, in the last column, the average over all test sets.
The proposed method OPTIMIZED-ALL is compared to the baseline NORMAL. Furthermore, for analysis, the results
when optimizing only the diagonal are marked as OPTIMIZED-DIAG.
Training Set d1 d2 d3 d4 zpredecessor successor sibling sentence confusion coefficient
1 0.35 0.26 0.19 0.20 0.03
2 0.27 0.29 0.21 0.23 0.03
3 0.35 0.31 0.16 0.18 0.02
4 0.44 0.24 0.17 0.16 0.04
5 0.39 0.28 0.20 0.13 0.03
average 0.36 0.28 0.19 0.18 0.03
Table 2: Shows the parameters which were learned using each training set. d1 . . . d4 are the weights of the context
positions, which sum up to 1. z marks the degree to which it is useful to compare context across different positions.
tion from the context vector. The accuracy results,
averaged over the same test sets as before, are shown
in Table 3. We can see that the accuracies are clearly
lower than before (compare to Table 1). This clearly
justifies to include additionally sentence information
into the context vector. It is also interesting to note
that the average z value is now 0.14.9 This is consid-
erable higher than before, and shows that a bag-of-
words model can partly make the use of z redundant.
However, note that the sentence bag-of-words model
covers a broader context, beyond the direct prede-
cessors, successor and siblings, which explains why
9That is 48% of its maximal possible value. Since for the
dependency positions predecessor, successor and sibling we get
the average weights 0.38, 0.33 and 0.29, respectively.
a small z value is still relevant in the situation where
we include sentence bag-of-words into the context
vector.
Finally, to see why it can be helpful to compare
different dependency positions from the context vec-
tors of Japanese and English, we looked at concrete
examples. We found, for example, that the trans-
lation accuracy of the query word ???? (disc)
improved when using OPTIMIZED-ALL instead of
OPTIMIZED-DIAG. The pivot word ?? (wrap)
tends together with both the Japanese query ??
?? (disc), and with the correct translation ?disc?
in English. However, that pivot word occurs in
Japanese and English in different context positions.
In the Japanese corpus ?? (wrap) tends to occur
16
Method Top-1 Top-5 Top-10 Top-15 Top-20
OPT-DEP 0.13 0.25 0.34 0.38 0.41
NOR-DEP 0.12 0.23 0.29 0.33 0.38
Table 3: The proposed method, but without the sentence
information in the context vector, is denoted OPT-DEP.
The baseline method, but without the sentence informa-
tion in the context vector, is denoted NOR-DEP.
together with the query???? (disc) in sentences
like for example the following:
????? (break)???? (disc)???
(wrap)???? (occured)??
That Japanese sentence can be literally translated as
?A wrap occured in the brake disc.?, where ?wrap?
is the sibling of ?disc? in the dependency tree. How-
ever, in English, considered out of the perspective
of ?disc?, the pivot word ?wrap? tends to occur in a
different dependency position. For example, the fol-
lowing sentence can be found in the English corpus:
?Front disc wraps.?
In English ?wrap? tends to occur as a successor of
?disc?. A non-zero confusion coefficient allows us
to account some degree of similarity to situations
where the query (here ??????(disc)) and the
translation candidate (here ?disc?) tend to occur with
the same pivot word (here ?wrap?), but in different
dependency positions.
6 Conclusions
Finding new translations of single words using com-
parable corpora is a promising method, for exam-
ple, to assist the creation and extension of bilin-
gual dictionaries. The basic idea is to first create
context vectors of the query word, and all the can-
didate translations, and then, in the second step,
to compare these context vectors. Previous work
(Laroche and Langlais, 2010; Fung, 1998; Garera
et al, 2009) suggests that for this task the cosine-
similarity is a good choice to compare context vec-
tors. For example, Garera et al (2009) include the
information of various context positions from the
dependency-parse tree in one context vector, and, af-
terwards, compares these context vectors using the
cosine-similarity. However, this makes the implicit
assumption that all context positions are equally im-
portant, and, furthermore, that context from differ-
ent context positions does not need to be compared
with each other. To overcome these limitations, we
suggested to use a generalization of the cosine simi-
larity which performs a linear transformation of the
context vectors, before applying the cosine similar-
ity. The linear transformation can be described by a
positive-definite matrix A. We defined the optimal
matrix A by using a Bayesian probabilistic model.
We demonstrated that it is feasible to approximate
the optimal matrix A by using MCMC-methods.
Our experimental results suggest that it is bene-
ficial to weight context positions individually. For
example, we found that predecessor and successor
should be stronger weighted than sibling, and sen-
tence information. Whereas, the latter two are also
important, having a total weight of around 40%.
Furthermore, we showed that for languages as dif-
ferent as Japanese and English it can be helpful to
compare also different context positions across both
languages. The proposed method constantly outper-
formed the baseline method. Top 1 accuracy in-
creased by up to 2% percent points and Top 20 by
up to 4% percent points.
For future work, we consider to use different pa-
rameterizations of the matrix A which could lead to
even higher improvement in accuracy. Furthermore,
we consider to include, and weight additional fea-
tures like transliteration similarity.
Acknowledgment
We would like to thank the anonymous reviewers
for their helpful comments. This work was partially
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan). The first author is sup-
ported by the MEXT Scholarship and by an IBM
PhD Scholarship Award.
References
D. Andrade, T. Nasukawa, and J. Tsujii. 2010. Robust
measurement and comparison of context similarity for
finding translation pairs. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 19?27.
D. Andrade, T. Matsuzaki, and J. Tsujii. 2011. Effec-
tive use of dependency structure for bilingual lexicon
17
creation. In Proceedings of the International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, Lecture Notes in Computer Science, pages
80?92. Springer Verlag.
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59?68.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1?17.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 129?137. Association for Computational Lin-
guistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771?779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 ? 489.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 ? 625.
F. Laws, L. Michelbacher, B. Dorow, C. Scheible,
U. Heid, and H. Schu?tze. 2010. A linguistically
grounded graph model for bilingual lexicon extrac-
tion. In Proceedings of the International Conference
on Computational Linguistics, pages 614?622. Inter-
national Committee on Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics, pages 91?98. Association
for Computational Linguistics.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447?456. Association for Computational Lin-
guistics.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519?526. Asso-
ciation for Computational Linguistics.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382?392.
T. Utsuro, T. Horiuchi, K. Hino, T. Hamamoto, and
T. Nakayama. 2003. Effect of cross-language IR
in bilingual lexicon acquisition from comparable cor-
pora. In Proceedings of the conference on European
chapter of the Association for Computational Linguis-
tics, pages 355?362. Association for Computational
Linguistics.
E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell. 2003.
Distance metric learning with application to clustering
with side-information. Advances in Neural Informa-
tion Processing Systems, pages 521?528.
18
Proceedings of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Sampo Pyysalo
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
smp@is.s.u-tokyo.ac.jp
Tomoko Ohta
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
okap@is.s.u-tokyo.ac.jp
Robert Bossy
National Institute for Agricultural Research
78352 Jouy en Josas, Cedex
Robert.Bossy@jouy.inra.fr
Ngan Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
The BioNLP Shared Task 2011, an informa-
tion extraction task held over 6 months up to
March 2011, met with community-wide par-
ticipation, receiving 46 final submissions from
24 teams. Five main tasks and three support-
ing tasks were arranged, and their results show
advances in the state of the art in fine-grained
biomedical domain information extraction and
demonstrate that extraction methods success-
fully generalize in various aspects.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
series represents a community-wide move toward
fine-grained information extraction (IE), in particu-
lar biomolecular event extraction (Kim et al, 2009;
Ananiadou et al, 2010). The series is complemen-
tary to BioCreative (Hirschman et al, 2007); while
BioCreative emphasizes the short-term applicability
of introduced IE methods for tasks such as database
curation, BioNLP-ST places more emphasis on the
measurability of the state-of-the-art and traceabil-
ity of challenges in extraction through an approach
more closely tied to text.
These goals were pursued in the first event,
BioNLP-ST 2009 (Kim et al, 2009), through high
quality benchmark data provided for system devel-
opment and detailed evaluation performed to iden-
tify remaining problems hindering extraction perfor-
mance. Also, as the complexity of the task was high
and system development time limited, we encour-
aged focus on fine-grained IE by providing gold an-
notation for named entities as well as various sup-
porting resources. BioNLP-ST 2009 attracted wide
attention, with 24 teams submitting final results. The
task setup and data since have served as the basis
for numerous studies (Miwa et al, 2010b; Poon and
Vanderwende, 2010; Vlachos, 2010; Miwa et al,
2010a; Bjo?rne et al, 2010).
As the second event of the series, BioNLP-ST
2011 preserves the general design and goals of the
previous event, but adds a new focus on variabil-
ity to address a limitation of BioNLP-ST 2009: the
benchmark data sets were based on the Genia corpus
(Kim et al, 2008), restricting the community-wide
effort to resources developed by a single group for
a small subdomain of molecular biology. BioNLP-
ST 2011 is organized as a joint effort of several
groups preparing various tasks and resources, in
which variability is pursued in three primary direc-
tions: text types, event types, and subject domains.
Consequently, generalization of fine grained bio-IE
in these directions is emphasized as the main theme
of the second event.
This paper summarizes the entire BioNLP-ST
2011, covering the relationships between tasks and
similar broad issues. Each task is presented in detail
in separate overview papers and extraction systems
in papers by participants.
1
2 Main tasks
BioNLP-ST 2011 includes four main tracks (with
five tasks) representing fine-grained bio-IE.
2.1 Genia task (GE)
The GE task (Kim et al, 2011) preserves the task
definition of BioNLP-ST 2009, arranged based on
the Genia corpus (Kim et al, 2008). The data repre-
sents a focused domain of molecular biology: tran-
scription factors in human blood cells. The purpose
of the GE task is two-fold: to measure the progress
of the community since the last event, and to eval-
uate generalization of the technology to full papers.
For the second purpose, the provided data is com-
posed of two collections: the abstract collection,
identical to the BioNLP-ST 2009 data, and the new
full paper collection. Progress on the task is mea-
sured through the unchanged task definition and the
abstract collection, while generalization to full pa-
pers is measured on the full paper collection. In this
way, the GE task is intended to connect the entire
event to the previous one.
2.2 Epigenetics and post-translational
modification task (EPI)
The EPI task (Ohta et al, 2011) focuses on IE for
protein and DNA modifications, with particular em-
phasis on events of epigenetics interest. While the
basic task setup and entity definitions follow those of
the GE task, EPI extends on the extraction targets by
defining 14 new event types relevant to task topics,
including major protein modification types and their
reverse reactions. For capturing the ways in which
different entities participate in these events, the task
extends the GE argument roles with two new roles
specific to the domain, Sidechain and Contextgene.
The task design and setup are oriented toward the
needs of pathway extraction and curation for domain
databases (Wu et al, 2003; Ongenaert et al, 2008)
and are informed by previous studies on extraction
of the target events (Ohta et al, 2010b; Ohta et al,
2010c).
2.3 Infectious diseases task (ID)
The ID task (Pyysalo et al, 2011a) concerns the ex-
traction of events relevant to biomolecular mecha-
nisms of infectious diseases from full-text publica-
tions. The task follows the basic design of BioNLP-
ST 2009, and the ID entities and extraction targets
are a superset of the GE ones. The task extends
considerably on core entities, adding to PROTEIN
four new entity types, including CHEMICAL and
ORGANISM. The events extend on the GE defini-
tions in allowing arguments of the new entity types
as well as in introducing a new event category for
high-level biological processes. The task was im-
plemented in collaboration with domain experts and
informed by prior studies on domain information ex-
traction requirements (Pyysalo et al, 2010; Anani-
adou et al, 2011), including the support of systems
such as PATRIC (http://patricbrc.org).
2.4 Bacteria track
The bacteria track consists of two tasks, BB and BI.
2.4.1 Bacteria biotope task (BB)
The aim of the BB task (Bossy et al, 2011) is to ex-
tract the habitats of bacteria mentioned in textbook-
level texts written for non-experts. The texts are
Web pages about the state of the art knowledge about
bacterial species. BB targets general relations, Lo-
calization and PartOf , and is challenging in that
texts contain more coreferences than usual, habitat
references are not necessarily named entities, and,
unlike in other BioNLP-ST 2011 tasks, all entities
need to be recognized by participants. BB is the first
task to target phenotypic information and, as habi-
tats are yet to be normalized by the field community,
presents an opportunity for the BioNLP community
to contribute to the standardization effort.
2.4.2 Bacteria interaction task (BI)
The BI task (Jourde et al, 2011) is devoted to the ex-
traction of bacterial molecular interactions and reg-
ulations from publication abstracts. Mainly focused
on gene transcriptional regulation in Bacillus sub-
tilis, the BI corpus is provided to participants with
rich semantic annotation derived from a recently
proposed ontology (Manine et al, 2009) defining
ten entity types such as gene, protein and deriva-
tives as well as DNA sites/motifs. Their interactions
are described through ten relation types. The BI
corpus consists of the sentences of the LLL corpus
(Ne?dellec, 2005), provided with manually checked
linguistic annotations.
2
Task Text Focus #
GE abstracts, full papers domain (HT) 9
EPI abstracts event types 15
ID full papers domain (TCS) 10
BB web pages domain (BB) 2
BI abstracts domain (BS) 10
Table 1: Characteristics of BioNLP-ST 2011 main tasks.
?#?: number of event/relation types targeted. Domains:
HT = human transcription factors in blood cells, TCS
= two-component systems, BB = bacteria biology, BS =
Bacillus subtilis
2.5 Characteristics of main tasks
The main tasks are characterized in Table 1. From
the text type perspective, BioNLP-ST 2011 gener-
alizes from abstracts in 2009 to full papers (GE and
ID) and web pages (BB). It also includes data collec-
tions for a variety of specific subject domains (GE,
ID, BB an BI) and a task (EPI) whose scope is not
defined through a domain but rather event types. In
terms of the target event types, ID targets a superset
of GE events and EPI extends on the representation
for PHOSPHORYLATION events of GE. The two bac-
teria track tasks represent an independent perspec-
tive relatively far from other tasks in terms of their
target information.
3 Supporting tasks
BioNLP-ST 2011 includes three supporting tasks
designed to assist in primary the extraction tasks.
Other supporting resources made available to par-
ticipants are presented in (Stenetorp et al, 2011).
3.1 Protein coreference task (CO)
The CO task (Nguyen et al, 2011) concerns the
recognition of coreferences to protein references. It
is motivated from a finding from BioNLP-ST 2009
result analysis: coreference structures in biomedical
text hinder the extraction results of fine-grained IE
systems. While finding connections between event
triggers and protein references is a major part of
event extraction, it becomes much harder if one is
replaced with a coreferencing expression. The CO
task seeks to address this problem. The data sets for
the task were produced based on MedCO annotation
(Su et al, 2008) and other Genia resources (Tateisi
et al, 2005; Kim et al, 2008).
Event Date Note
Sample Data 31 Aug. 2010
Support. Tasks
Train. Data 27 Sep. 2010 7 weeks for development
Test Data 15 Nov. 2010 4 days for submission
Submission 19 Nov. 2010
Evaluation 22 Nov. 2010
Main Tasks
Train. Data 1 Dec. 2010 3 months for development
Test Data 1 Mar. 2011 9 days for submission
Submission 10 Mar. 2011 extended from 8 Mar.
Evaluation 11 Mar. 2011 extended from 10 Mar.
Table 2: Schedule of BioNLP-ST 2011
3.2 Entity relations task (REL)
The REL task (Pyysalo et al, 2011b) involves the
recognition of two binary part-of relations between
entities: PROTEIN-COMPONENT and SUBUNIT-
COMPLEX. The task is motivated by specific chal-
lenges: the identification of the components of pro-
teins in text is relevant e.g. to the recognition of
Site arguments (cf. GE, EPI and ID tasks), and re-
lations between proteins and their complexes rele-
vant to any task involving them. REL setup is in-
formed by recent semantic relation tasks (Hendrickx
et al, 2010). The task data, consisting of new anno-
tations for GE data, extends a previously introduced
resource (Pyysalo et al, 2009; Ohta et al, 2010a).
3.3 Gene renaming task (REN)
The REN task (Jourde et al, 2011) objective is to ex-
tract renaming pairs of Bacillus subtilis gene/protein
names from PubMed abstracts, motivated by dis-
crepancies between nomenclature databases that in-
terfere with search and complicate normalization.
REN relations partially overlap several concepts:
explicit renaming mentions, synonymy, and renam-
ing deduced from biological proof. While the task
is related to synonymy relation extraction (Yu and
Agichtein, 2003), it has a novel definition of renam-
ing, one name permanently replacing the other.
4 Schedule
Table 2 shows the task schedule, split into two
phases to allow the use of supporting task results in
addressing the main tasks. In recognition of their
higher complexity, a longer development period was
arranged for the main tasks (3 months vs 7 weeks).
3
Team GE EPI ID BB BI CO REL REN
UTurku 1 1 1 1 1 1 1 1
ConcordU 1 1 1 1 1 1
UMass 1 1 1
Stanford 1 1 1
FAUST 1 1 1
MSR-NLP 1 1
CCP-BTMG 1 1
Others 8 0 2 2 0 4 2 1
SUM 15 7 7 3 1 6 4 3
Table 3: Final submissions to BioNLP-ST 2011 tasks.
5 Participation
BioNLP-ST 2011 received 46 submissions from 24
teams (Table 3). While seven teams participated in
multiple tasks, only one team, UTurku, submitted fi-
nal results to all the tasks. The remaining 17 teams
participated in only single tasks. Disappointingly,
only two teams (UTurku, and ConcordU) performed
both supporting and main tasks, and neither used
supporting task analyses for the main tasks.
6 Results
Detailed evaluation results and analyses are pre-
sented in individual task papers, but interesting ob-
servations can be obtained also by comparisons over
the tasks. Table 4 summarizes best results for vari-
ous criteria (Note that the results shown for e.g. GEa,
GEf and GEp may be from different teams).
The community has made a significant improve-
ment in the repeated GE task, with an over 10%
reduction in error from ?09 to GEa. Three teams
achieved better results than M10, the best previously
reported individual result on the ?09 data. This in-
dicates a beneficial role from focused efforts like
BioNLP-ST. The GEf and ID results show that
generalization to full papers is feasible, with very
modest loss in performance compared to abstracts
(GEa). The results for PHOSPHORYLATION events
in GE and EPI are comparable (GEp vs EPIp), with
the small drop for the EPI result, suggesting that
the removal of the GE domain specificity does not
compromise extraction performance. EPIc results
indicate some challenges in generalization to simi-
lar event types, and EPIf suggest substantial further
challenges in additional argument extraction. The
complexity of ID is comparable to GE, also reflected
to their final results, which further indicate success-
Task Evaluation Results
BioNLP-ST 2009 (?09) 46.73 / 58.48 / 51.95
Miwa et al (2010b) (M10) 48.62 / 58.96 / 53.29
LLL 2005 (LLL) 53.00 / 55.60 / 54.30
GE abstracts (GEa) 50.00 / 67.53 / 57.46
GE full texts (GEf) 47.84 / 59.76 / 53.14
GE PHOSPHORYLATION (GEp) 79.26 / 86.99 / 82.95
GE LOCALIZATION (GEl) 37.88 / 77.42 / 50.87
EPI full task (EPIf) 52.69 / 53.98 / 53.33
EPI core task (EPIc) 68.51 / 69.20 / 68.86
EPI PHOSPHORYLATION (EPIp) 86.15 / 74.67 / 80.00
ID full task (IDf) 48.03 / 65.97 / 55.59
ID core task (IDc) 50.62 / 66.06 / 57.32
BB 45.00 / 45.00 / 45.00
BB PartOf (BBp) 32.00 / 83.00 / 46.00
BI 71.00 / 85.00 / 77.00
CO 22.18 / 73.26 / 34.05
REL 50.10 / 68.00 / 57.70
REN 79.60 / 95.90 / 87.00
Table 4: Best results for various (sub)tasks (recall / preci-
sion / f-score (%)). GEl: task 2 without trigger detection.
ful generalization to a new subject domain as well
as to new argument (entity) types. The BB task is
in part comparable to GEl and involves a represen-
tation similar to REL, with lower results likely in
part because BB requires entity recognition. The BI
task is comparable to LLL Challenge, though BI in-
volves more entity and event types. The BI result
is 20 points above the LLL best result, indicating a
substantial progress of the community in five years.
7 Discussion and Conclusions
Meeting with wide participation from the commu-
nity, BioNLP-ST 2011 produced a wealth of valu-
able resources for the advancement of fine-grained
IE in biology and biomedicine, and demonstrated
that event extraction methods can successfully gen-
eralize to new text types, event types, and domains.
However, the goal to observe the capacity of sup-
porting tasks to assist the main tasks was not met.
The entire shared task period was very long, more
than 6 months, and the complexity of the task was
high, which could be an excessive burden for partic-
ipants, limiting the application of novel resources.
There have been ongoing efforts since BioNLP-ST
2009 to develop IE systems based on the task re-
sources, and we hope to see continued efforts also
following BioNLP-ST 2011, especially exploring
the use of supporting task resources for main tasks.
4
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O. Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 33?
38, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
A.P. Manine, E. Alphonse, and Bessie`res P. 2009. Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. International Journal of
Medical Informatics, 78(12):e31?38.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Ne?dellec. 2005. Learning Language in Logic ? Genic
Interaction Extraction Challenge. In Proceedings of
4th Learning Language in Logic Workshop (LLL?05),
pages 31?37.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010a. A re-evaluation of biomedical
named entity-term relations. Journal of Bioinformat-
ics and Computational Biology (JBCB), 8(5):917?928.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010b. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2010c. Event extraction for dna
methylation. In Proceedings of SMBM?10.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methylation
database combining text-mining and expert annota-
tion. Nucleic Acids Research, 36(suppl 1):D842?846.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
5
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9, Boulder, Colorado. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucleic Acids Research, 31(1):345?347.
H. Yu and E. Agichtein. 2003. Extracting synony-
mous gene and protein terms from biological litera-
ture. Bioinformatics, 19(suppl 1):i340.
6
Proceedings of BioNLP Shared Task 2011 Workshop, pages 16?25,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Epigenetics and Post-translational Modifications (EPI) task
of BioNLP Shared Task 2011
Tomoko Ohta? Sampo Pyysalo? Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Microsoft Research Asia, Beijing, China
{okap,smp}@is.s.u-tokyo.ac.jp, jtsujii@microsoft.com
Abstract
This paper presents the preparation, resources,
results and analysis of the Epigenetics and
Post-translational Modifications (EPI) task, a
main task of the BioNLP Shared Task 2011.
The task concerns the extraction of detailed
representations of 14 protein and DNA modifi-
cation events, the catalysis of these reactions,
and the identification of instances of negated
or speculatively stated event instances. Seven
teams submitted final results to the EPI task in
the shared task, with the highest-performing
system achieving 53% F-score in the full task
and 69% F-score in the extraction of a simpli-
fied set of core event arguments.
1 Introduction
The Epigenetics and Post-translational Modifica-
tions (EPI) task is a shared task on event extrac-
tion from biomedical domain scientific publications,
first introduced as a main task in the BioNLP Shared
Task 2011 (Kim et al, 2011a).
The EPI task focuses on events relating to epige-
netic change, including DNA methylation and hi-
stone methylation and acetylation (see e.g. (Hol-
liday, 1987; Jaenisch and Bird, 2003)), as well
as other common protein post-translational modi-
fications (PTMs) (Witze et al, 2007). PTMs are
chemical modifications of the amino acid residues
of proteins, and DNA methylation a parallel mod-
ification of the nucleotides on DNA. While these
modifications are chemically simple reactions and
can thus be straightforwardly represented in full de-
tail, they have a crucial role in the regulation of
gene expression and protein function: the modifi-
cations can alter the conformation of DNA or pro-
teins and thus control their ability to associate with
other molecules, making PTMs key steps in protein
biosynthesis for introducing the full range of protein
functions. For instance, protein phosphorylation ?
the attachment of phosphate ? is a common mecha-
nism for activating or inactivating enzymes by alter-
ing the conformation of protein active sites (Stock
et al, 1989; Barford et al, 1998), and protein ubiq-
uitination ? the post-translational attachment of the
small protein ubiquitin ? is the first step of a major
mechanism for the destruction (breakdown) of many
proteins (Glickman and Ciechanover, 2002).
Many of the PTMs targeted in the EPI task in-
volve modification of histone, a core protein that
forms an octameric complex that has a crucial role in
packaging chromosomal DNA. The level of methy-
lation and acetylation of histones controls the tight-
ness of the chromatin structure, and only ?unwound?
chromatin exposes the gene packed around the hi-
stone core to the transcriptional machinery. Since
histone modification is of substantial current inter-
est in epigenetics, we designed aspects of the EPI
task to capture the full detail in which histone mod-
ification events are stated in text. Finally, the DNA
methylation of gene regulatory elements controls the
expression of the gene by altering the affinity with
which DNA-binding proteins (including transcrip-
tion factors) bind, and highly methylated genes are
not transcribed at all (Riggs, 1975; Holliday and
Pugh, 1975). DNA methylation can thus ?switch
off? genes, ?removing? them from the genome in a
way that is reversible through DNA demethylation.
16
Figure 1: Three views of protein methylation. a)
chemical formula b) event representation c) modification
database entry.
The BioNLP?09 Shared Task on Event Extrac-
tion (Kim et al, 2009), the first task in the present
shared task series, involved the extraction of nine
event types including one PTM type, PHOSPHORY-
LATION. The results of the shared task showed this
PTM event to be the single most reliably extracted
event type in the task, with the best-performing
system for the type achieving 91% precision and
76% recall (83% F-score) in its extraction (Buyko
et al, 2009). The results suggest both that the
event representation is well applicable to PTM ex-
traction and that current extraction methods are ca-
pable of reliable PTM extraction. The EPI task
follows up on these opportunities, introducing spe-
cific, strongly biologically motivated extraction tar-
gets that are expected to be both feasible for high-
accuracy event extraction, relevant to the needs of
present-day molecular biology, and closely appli-
cable to biomolecular database curation needs (see
Figure 1) (Ohta et al, 2010a).
2 Task Setting
The EPI task is an event extraction task in the sense
popularized by a number of recent domain resources
and challenges (e.g. (Pyysalo et al, 2007; Kim et al,
2008; Thompson et al, 2009; Kim et al, 2009; Ana-
niadou et al, 2010)). In broad outline, the task fo-
cuses on the extraction of information on statements
regarding change in the state or properties of (physi-
cal) entities, modeled using an event representation.
Figure 2: Illustration of the event representation. An
event of type METHYLATION (expressed through the text
?methylation?) with two participants of the types PRO-
TEIN (?histone H3?) and ENTITY (?Lys9?), participating
in the event in Theme and Site roles, respectively.
In this representation, events are typed n-ary asso-
ciations of participants (entities or other events) in
specific roles. Events are bound to specific expres-
sions in text (the event trigger or text binding) and
are primary objects of annotation, allowing them to
be marked in turn e.g. as negated or as participants
in other events. Figure 2 illustrates these concepts.
In its specific formulation, EPI broadly follows
the definition of the BioNLP?09 shared task on event
extraction. Basic modification events are defined
similarly to the PHOSPHORYLATION event type tar-
geted in the ?09 and the 2011 GE and ID tasks (Kim
et al, 2011b; Pyysalo et al, 2011b), with the full
task extending previously defined arguments with
two additional ones, Sidechain and Contextgene.
2.1 Entities
The EPI task follows the general policy of the
BioNLP Shared Task in isolating the basic task of
named entity recognition from the event extraction
task by providing task participants with manually
annotated gene and gene product entities as a start-
ing point for extraction. The entity types follow the
BioNLP?09 Shared Task scheme, where genes and
their products are simply marked as PROTEIN.1
In addition to the given PROTEIN entities, some
events involve other entities, such as the modifica-
tion Site. These entities are not given and must thus
be identified by systems targeting the full task (see
Section 4). In part to reduce the demands of this
entity recognition component of the task, these ad-
ditional entities are not given specific types but are
generically marked as ENTITY.
1While most of the modifications targeted in the task involve
proteins, this naming is somewhat inaccurate for the Themes of
DNA METHYLATION and DNA DEMETHYLATION events and
for Contextgene arguments, which refer to genes. Despite this
inaccuracy, we chose to follow this naming scheme for consis-
tency with other tasks.
17
Type Core arguments Additional arguments
HYDROXYLATION Theme(PROTEIN) Site(ENTITY)
PHOSPHORYLATION Theme(PROTEIN) Site(ENTITY)
UBIQUITINATION Theme(PROTEIN) Site(ENTITY)
DNA METHYLATION Theme(PROTEIN) Site(ENTITY)
GLYCOSYLATION Theme(PROTEIN) Site(ENTITY), Sidechain(ENTITY)
ACETYLATION Theme(PROTEIN) Site(ENTITY), Contextgene(PROTEIN)
METHYLATION Theme(PROTEIN) Site(ENTITY), Contextgene(PROTEIN)
CATALYSIS Theme(Event), Cause(PROTEIN)
Table 1: Event types and their arguments. The type of entity allowed as argument is specified in parenthesis. For each
event type except CATALYSIS, the reverse reaction (e.g. DEACETYLATION for ACETYLATION) is also defined, with
identical arguments. The total number of event types in the task is thus 15.
2.2 Relations
The EPI task does not define any explicit relation
extraction targets. However, the task annotation in-
volves one relation type, EQUIV. This is a binary,
symmetric, transitive relation between entities that
defines two entities to be equivalent (Hoehndorf et
al., 2010). The relation is used in the gold annota-
tion to mark local aliases such as the full and abbre-
viated forms of a protein name as referring to the
same real-world entity. While the ?09 task only rec-
ognized equivalent PROTEIN entities, EPI extends
on the scope of EQUIV annotations in allowing enti-
ties of any type to be marked equivalent. In evalua-
tion, references to any of a set of equivalent entities
are treated identically.
2.3 Events
While the EPI task entity definition closely follows
that of the previous shared task, the task introduces
considerable novelty in the targeted events, adding a
total of 14 novel event types and two new participant
roles. Table 1 summarizes the targeted event types
and their arguments.
As in the BioNLP?09 shared task, Theme argu-
ments identify the entity that the event is about, such
as the protein that is acetylated in an acetylation
event. A Theme is always mandatory for all EPI task
events. Site arguments identify the modification site
on the Theme entity, such as a specific residue on a
modified protein or a specific region on a methylated
gene. The Sidechain argument, specific to GLYCO-
SYLATION and DEGLYCOSYLATION among the tar-
geted events, identifies the moiety attached or re-
moved in the event (in glycosylation, the sugar).2 Fi-
nally, the Contextgene argument, specific to ACETY-
LATION and METHYLATION events and their re-
verse reactions, identifies the gene whose expression
is controlled by these modifications. This argument
applies specifically for histone protein modification:
the modification of the histones that form the nu-
cleosomes that structure DNA are key to the epige-
netic control of gene expression. The Site, Sidechain
and Contextgene arguments are not mandatory, and
should only be extracted when explicitly stated.
For CATALYSIS events, representing the cataly-
sis of protein or DNA modification by another pro-
tein, both Theme and Cause are mandatory. While
CATALYSIS is a new event type, it is related to
the ?09 POSITIVE REGULATION type by a class-
subclass relation: any CATALYSIS event is a POS-
ITIVE REGULATION event in the ?09 task terms (but
not vice versa).
2.4 Event modifications
In addition to events, the EPI task defines two
event modification extraction targets: NEGATION
and SPECULATION. Both are represented as simple
binary ?flags? that apply to events, marking them as
being explicitly negated (e.g. H2A is not methylated)
or stated in a speculative context (e.g. H2A may be
methylated). Events may be both negated and spec-
ulated.
2Note that while arguments similar to Sidechain could be
defined for other event types also, their extraction would pro-
vide no additional information: the attached molecule is always
acetyl in acetylation, methyl in methylation, etc.
18
3 Data
The primary EPI task data were annotated specifi-
cally for the BioNLP Shared Task 2011 and are not
based on any previously released resource. Before
starting this annotation effort, we performed two
preparatory studies using in part previously released
related datasets: in (Ohta et al, 2010a) we consid-
ered the extraction of four protein post-translational
modifications event types with reference to annota-
tions originally created for the Protein Information
Resource3 (PIR) (Wu et al, 2003), and in (Ohta et
al., 2010b) we studied the annotation and extraction
of DNA methylation events with reference to anno-
tations created for the PubMeth4 (Ongenaert et al,
2008) database. The corpus text selection and anno-
tation scheme were then defined following the un-
derstanding formed in these studies.
3.1 Document selection
The texts for the EPI task corpus were drawn from
PubMed abstracts. In selecting the primary cor-
pus texts, we aimed to gather a representative sam-
ple of all PubMed documents relevant to selected
modification events, avoiding bias toward, for ex-
ample, specific genes/proteins, species, forms of
event expression, or subdomains. We primarily tar-
geted DNA methylation and the ?prominent PTM
types? identified in (Ohta et al, 2010a). We de-
fined the following document selection protocol: for
each of the targeted event types, 1) Select a ran-
dom sample of PubMed abstracts annotated with the
MeSH term corresponding to the target event (e.g.
Acetylation) 2) Automatically tag protein/gene
entities in the selected abstracts, removing ones
where fewer than a specific cutoff are found 3) Per-
form manual filtering removing documents not rele-
vant to the targeted topic (optional).
MeSH is a controlled vocabulary of over 25,000
terms that is used to manually annotate each docu-
ment in PubMed. By performing initial document
retrieval using MeSH terms it is possible to se-
lect relevant documents without bias toward specific
expressions in text. While search for documents
tagged with e.g. the Acetylation MeSH term is
sufficient to select documents relevant to the modi-
3http://pir.georgetown.edu
4http://www.pubmeth.org/
fication, not all such documents necessarily concern
specifically protein modification, necessitating a fil-
tering step. Following preliminary experiments, we
chose to apply the BANNER named entity tagger
(Leaman and Gonzalez, 2008) trained on the GENE-
TAG corpus (Tanabe et al, 2005) and to filter docu-
ments where fewer than five entities were identified.
Finally, for some modification types this protocol se-
lected also a substantial number of non-relevant doc-
uments. In these cases a manual filtering step was
performed prior to full annotation to avoid marking
large numbers of non-relevant abstracts.
This primary corpus text selection protocol does
not explicitly target reverse reactions such as
deacetylation, and the total number of these events
in the resulting corpus was low for many types. To
be able to measure the extraction performance for
these types, we defined a secondary selection pro-
tocol that augmented the primary protocol with a
regular expression-based filter removing documents
that did not (likely) contain mentions of reverse re-
actions. This protocol was used to select a secondary
set of test abstracts enriched in mentions of reverse
reactions. Performance on this secondary test set
was also evaluated, but is not part of the primary task
evaluation. Due to space considerations, we only
present the primary test set results in this paper, re-
ferring to the shared task website for the secondary
results.
3.2 Annotation
Annotation was performed manually. The
gene/protein entities automatically detected in
the document selection step were provided to
annotators for reference for creating PROTEIN
annotations, but all entity annotations were checked
and revised to conform to the specific guidelines for
the task.5 For the annotation of PROTEIN entities,
we adopted the GENIA gene/gene product (GGP)
annotation guidelines (Ohta et al, 2009), adding
one specific exception: while the primary guidelines
require that only specific individual gene or gene
product names are annotated, we allowed also the
annotation of mentions of groups of histones or
5This revision was substantial: only approximately 65% of
final PROTEIN annotations exactly match an automatically pre-
dicted one due to differences in annotation criteria (Wang et al,
2009).
19
the entire histone protein family to capture histone
modification events also in cases where only the
group is mentioned.
All event annotations were created from scratch
without automatic support to avoid bias toward spe-
cific automatic extraction methods or approaches.
The event annotation follows the GENIA event cor-
pus annotation guidelines (Kim et al, 2008) as they
apply to protein modifications, with CATALYSIS be-
ing annotated following the criteria for the POSI-
TIVE REGULATION event type with the additional
constraints that the Cause of the event is a gene or
gene product entity and the form of regulation is
catalysis of a modification reaction.
The manual annotation was performed by three
experienced annotators with a molecular biology
background, with one chief annotator with extensive
experience in domain event annotation organizing
and supervising the annotator training and the over-
all process. After completion of primary annotation,
we performed a final check targeting simple human
errors using an automatic extraction system.6 This
correction process resulted in the revision of approx-
imately 2% of the event annotations. To evaluate the
consistency of the annotation, we performed inde-
pendent event annotation (taking PROTEIN annota-
tions as given) for a random sample of 10% of the
corpus documents. Comparison of the two manually
created sets of event annotations under the primary
task evaluation criteria gave an F-score of 82% for
the full task and 89% for the core task.7 We found
that CATALYSIS events were particularly challeng-
ing, showing just 65% agreement for the core task.
Table 2 shows the statistics of the primary task
data. We note that while the corpus is broadly com-
parable in size to the BioNLP?09 shared task dataset
(Kim et al, 2009) in terms of the number of ab-
stracts and annotated entities, the number of anno-
tated events in the EPI corpus is approximately 20%
of that in the ?09 dataset, reflecting the more focused
event types.
6High-confidence system predictions differing from gold
annotations were provided to a human annotator, not used di-
rectly to change corpus data. To further reduce the risk of bias,
we only informed the annotator of the entities involved, not of
the predicted event structure.
7Due to symmetry of precision/recall and the applied crite-
ria, this score was not affected by the choice of which set of
annotations to consider as ?gold? for the comparison.
Item Training Devel Test
Abstract 600 200 400
Word 127,312 43,497 82,819
Protein 7,595 2,499 5,096
Event 1,852 601 1,261
Modification 173 79 117
Table 2: Statistics of the EPI corpus. Test set statistics
shown only for the primary test data.
4 Evaluation
Evaluation is instance- and event-oriented and based
on the standard precision/recall/F-score8 metrics.
The primary evaluation criteria are the same as in the
BioNLP?09 shared task, incorporating the ?approx-
imate span matching? and ?approximate recursive
matching? variants to strict matching. In brief, un-
der these criteria text-bound annotations (event trig-
gers and entities) in a submission are considered to
match a corresponding gold annotation if their span
is contained within the (mildly extended) span of
the gold annotation, and events that refer to other
events as arguments are considered to match if the
Theme arguments of the recursively referred events
match, that is, non-Theme arguments are ignored in
recursively referred events. For a detailed descrip-
tion of these evaluation criteria, we refer to (Kim et
al., 2009).
In addition to the primary evaluation criteria, we
introduced a new relaxed evaluation criterion we
term single partial penalty. Under the primary cri-
teria, when a predicted event matches a gold event
in some of its arguments but lacks one or more ar-
guments of the gold event, the submission is ar-
guably given a double penalty: the predicted event
is counted as a false positive (FP), and the gold
event is counted as a false negative (FN). Under the
single partial penalty evaluation criterion, predicted
events that match a gold event in all their arguments
are not counted as FP, although the corresponding
gold event still counts as FN (the ?single penalty?).
Analogously, gold events that partially match a pre-
dicted event are not counted as FN, although the cor-
responding predicted event with ?extra? arguments
counts as FP. This criterion can give a more nuanced
view of performance for partially correctly predicted
events.
8Specifically F1. F is used for short throughout.
20
NLP Events Other resources
Rank Team Org word parse trigger arg group modif. corpora other
1 UTurku 1BI Porter McCCJ + SD SVM SVM SVM SVM - hedge words
2 FAUST 3NLP
CoreNLP,
SnowBall
McCCJ + SD (UMass+Stanford as features) - - word clusters
3 MSR-NLP
1SDE,
3NLP
Porter,
custom
McCCJ + SD,
Enju
SVM SVM SVM - -
triggers, word
clusters
4 UMass 1NLP
CoreNLP,
SnowBall
McCCJ + SD Joint, dual decomposition - - -
5 Stanford 3NLP custom McCCJ + SD MaxEnt Joint, MSTParser - - word clusters
6 CCP-BTMG 3BI
Porter,
WN-lemma
Stanford + SD Graph extraction & matching - - -
7 ConcordU 2NLP - McCCJ + SD Dict Rules Rules Rules -
triggers and
hedge words
Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Lan-
guage Processing researcher, SDE=Software Development Engineer, CoreNLP=Stanford CoreNLP, Porter=Porter
stemmer, Snowball=Snowball stemmer, WN-lemma=WordNet lemmatization, McCCJ=McClosky-Charniak-Johnson
parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, Dict=Dictionary
The full EPI task involves many partially indepen-
dent challenges, incorporating what were treated in
the BioNLP?09 shared task as separate subtasks: the
identification of additional non-Theme event partic-
ipants (Task 2 in ?09) and the detection of negated
and speculated events (Task 3 in ?09). The EPI task
does not include explicit subtasks. However, we
specifies minimal core extraction targets in addition
to the full task targets. Results are reported sepa-
rately for core targets and full task, allowing partic-
ipants to choose to only extract core targets. The
full task results are considered the primary evalua-
tion for the task e.g. for the purposes of determining
the ranking of participating systems.
5 Results
5.1 Participation
Table 3 summarizes the participating groups and the
features of their extraction systems. We note that,
similarly to the ?09 task, machine learning-based
systems remain dominant overall, although there is
considerable divergence in the specific methods ap-
plied. In addition to domain mainstays such as sup-
port vector machines and maximum entropy mod-
els, we find increased application of joint models
(Riedel et al, 2011; McClosky et al, 2011; Riedel
and McCallum, 2011) as opposed to pure pipeline
systems (Bjo?rne and Salakoski, 2011; Quirk et al,
2011) . Remarkably, the application of full pars-
ing together with dependency-based representations
of syntactic analyses is adopted by all participants,
with the parser of Charniak and Johnson (2005) with
the biomedical domain model of McClosky (2009)
is applied in all but one system (Liu et al, 2011) and
the Stanford Dependency representation (de Marn-
effe et al, 2006) in all. These choices may be mo-
tivated in part by the success of systems using the
tools in the previous shared task and the availability
of the analyses as supporting resources (Stenetorp et
al., 2011).
Despite the availability of PTM and DNA methy-
lation resources other than those specifically intro-
duced for the task and the PHOSPHORYLATION an-
notations in the GE task (Kim et al, 2011b), no par-
ticipant chose to apply other corpora for training.
With the exception of externally acquired unlabeled
data such as PubMed-derived word clusters applied
by three groups, the task results thus reflect a closed
task setting in which only the given data is used for
training.
5.2 Evaluation results
Table 4 presents a the primary results by event type,
and Table 5 summarizes these results. We note
that only two teams, UTurku (Bjo?rne and Salakoski,
2011) and ConcordU (Kilicoglu and Bergler, 2011),
predicted event modifications, and only UTurku pre-
dicted additional (non-core) event arguments (data
not shown). The other five systems thus addressed
21
MSR-
NLP
CCP-
BTMG
Con-
cordUUTurku FAUST UMass Stanford Size
HYDROXYLATION 42.25 10.26 10.20 12.80 9.45 12.84 6.32 139
DEHYDROXYLATION - - - - - - - 1
PHOSPHORYLATION 67.12 51.61 50.00 49.18 40.98 47.06 44.44 130
DEPHOSPHORYLATION 0.00 0.00 0.00 0.00 0.00 50.00 0.00 3
UBIQUITINATION 75.34 72.95 67.88 72.94 67.44 70.87 69.97 340
DEUBIQUITINATION 54.55 40.00 0.00 31.58 0.00 42.11 14.29 17
DNA METHYLATION 60.21 31.21 34.54 23.82 31.02 15.65 8.22 416
DNA DEMETHYLATION 26.67 0.00 0.00 0.00 0.00 0.00 0.00 21
Simple event total 63.05 45.17 44.97 43.01 40.96 40.62 37.84 1067
GLYCOSYLATION 49.43 41.10 38.87 40.00 37.22 25.62 25.94 347
DEGLYCOSYLATION 40.00 35.29 0.00 38.10 30.00 35.29 26.67 27
ACETYLATION 57.22 40.00 41.42 40.25 35.12 37.50 38.19 337
DEACETYLATION 54.90 28.00 31.82 29.17 21.74 24.56 27.27 50
METHYLATION 57.67 24.82 19.57 23.67 18.54 16.99 15.50 374
DEMETHYLATION 35.71 0.00 0.00 0.00 0.00 0.00 0.00 13
Non-simple event total 54.36 33.86 31.85 33.07 29.28 25.06 25.10 1148
CATALYSIS 7.06 6.58 7.75 5.00 2.84 7.58 1.74 238
Subtotal 55.02 36.93 36.17 35.30 32.85 30.58 28.92 2453
NEGATION 18.60 0.00 0.00 0.00 0.00 0.00 26.51 149
SPECULATION 37.65 0.00 0.00 0.00 0.00 0.00 6.82 103
Modification total 28.07 0.00 0.00 0.00 0.00 0.00 16.37 252
Total 53.33 35.03 34.27 33.52 31.22 28.97 27.88 2705
Addition total 59.33 40.27 39.05 38.65 36.03 32.75 31.50 2038
Removal total 44.29 22.41 15.73 22.76 14.41 23.53 17.48 132
Table 4: Primary evaluation F-scores by event type. The ?size? column gives the number of annotations of each type
in the given data (training+development). Best result for each type shown in bold. For DEHYDROXYLATION, no
examples were present in the test data and none were predicted by any participant.
Team recall prec. F-score
UTurku 52.69 53.98 53.33
FAUST 28.88 44.51 35.03
MSR-NLP 27.79 44.69 34.27
UMass 28.08 41.55 33.52
Stanford 26.56 37.85 31.22
CCP-BTMG 23.44 37.93 28.97
ConcordU 20.83 42.14 27.88
Table 5: Primary evaluation results
only the core task. For the full task, this differ-
ence in approach is reflected in the substantial per-
formance advantage for the UTurku system, which
exhibits highest performance overall as well as for
most individual event types.
Extraction performance for simple events tak-
ing only Theme and Site arguments is consistently
higher than for other event types, with absolute F-
score differences of over 10% points for many sys-
tems. Similar notable performance differences are
seen between the addition events, for which am-
ple training data was available, and the removal
types for which data was limited. This effect is
particularly noticeable for DEPHOSPHORYLATION,
DNA DEMETHYLATION and DEMETHYLATION,
for which the clear majority of systems failed to pre-
dict any correct events. Extraction performance for
CATALYSIS events is very low despite a relatively
large set of training examples, indicating that the
extraction of nested event structures remains very
challenging. This low performance may also be re-
lated to the fact that CATALYSIS events are often
triggered by the same word as the catalysed mod-
ification (e.g. Figure 1b), requiring the assignment
of multiple event labels to a single word in typical
system architectures.
Table 6 summarizes the full task results with the
addition of the single partial penalty criterion. The
F-scores for the seven participants under this crite-
22
Team recall prec. F-score ?
UTurku 54.79 58.42 56.55 3.22
FAUST 28.88 72.05 41.24 6.21
MSR-NLP 27.79 66.72 39.24 4.97
UMass 28.08 63.28 38.90 5.38
Stanford 26.56 56.83 36.20 4.98
CCP-BTMG 23.44 50.79 32.08 3.11
ConcordU 20.83 60.55 30.99 3.11
Table 6: Full task evaluation results for primary criteria
and with single partial penalty. The ? column gives F-
score difference to the primary results.
rion are on average over 4% points higher than un-
der the primary criteria, with the most substantial
increases seen for high-ranking participants only ad-
dressing the core task: for example, the precision
of the FAUST system (Riedel et al, 2011) is nearly
30% higher under the relaxed criterion. These re-
sults provide new perspective deserving further de-
tailed study into the question of what are the most
meaningful criteria for event extraction system eval-
uation.
Table 7 summarizes the core task results. While
all systems show notably higher performance than
for the full task, high-ranking participants focusing
on the core task gain most dramatically, with the
FAUST system core task F-score essentially match-
ing that of the top system (UTurku). For the core
task, all participants achieve F-scores over 50% ?
a result achieved by only a single system in the ?09
task ? and the top four participants average over 65%
F-score. These results confirm that current event
extraction technology is well applicable to the core
PTM extraction task even when the number of tar-
geted event types is relatively high and may be ready
to address the challenges of exhaustive PTM extrac-
tion (Pyysalo et al, 2011a). The best core tasks re-
sults, approaching 70% F-score, are particularly en-
couraging as the level of performance is comparable
to or better than state-of-the-art results for many ref-
erence resources for protein-protein interaction ex-
traction (see e.g. Tikk et al (2010))) using the simple
untyped entity pair representation, a standard task
that has been extensively studied in the domain.
6 Discussion and Conclusions
This paper has presented the preparation, resources,
results and analysis of the BioNLP Shared Task
Team recall prec. F-score ?1 ?2
UTurku 68.51 69.20 68.86 15.53 12.31
FAUST 59.88 80.25 68.59 33.56 27.35
MSR-NLP 55.70 77.60 64.85 30.58 25.61
UMass 57.04 73.30 64.15 30.63 25.25
Stanford 56.87 70.22 62.84 31.62 26.64
ConcordU 40.28 76.71 52.83 24.95 21.84
CCP-BTMG 45.06 63.37 52.67 23.70 20.59
Table 7: Core task evaluation results. The ?1 column
gives F-score difference to primary full task results, ?2
to full task results with single partial penalty.
2011 Epigenetics and Post-translational modifica-
tions (EPI) main task. The results demonstrate that
the core extraction target of identifying statements
of 14 different modification types with the modified
gene or gene product can be reliably addressed by
current event extraction methods, with two systems
approaching 70% F-score at this task. Nevertheless,
challenges remain in detecting statements regarding
the catalysis of these events as well as in resolving
the full detail of such modification events, a task at-
tempted by only one participant and at which perfor-
mance remains at somewhat above 50% in F-score.
Detailed evaluation showed that the highly com-
petitive participating systems differ substantially in
their relative strengths, indicating potential for fur-
ther development at protein and DNA modification
event detection. The task results are available in
full detail from the shared task webpage, http:
//sites.google.com/site/bionlpst/.
In the future, we will follow the example of the
BioNLP?09 shared task in making the data and re-
sources of the EPI task open to all interested par-
ties to encourage further study of event extraction
for epigenetics and post-translational modification
events, to facilitate system comparison on a well-
defined standard task, and to support the develop-
ment of further applications of event extraction tech-
nology in this important area of biomolecular sci-
ence.
Acknowledgments
We would like to thank Yoshiro Okuda and Yo Shi-
dahara of NalaPro Technologies for their efforts in
producing the EPI task annotation. This work was
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan).
23
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
D. Barford, A.K. Das, and M.P. Egloff. 1998. The struc-
ture and mechanism of protein phosphatases: insights
into catalysis and regulation. Annual review of bio-
physics and biomolecular structure, 27(1):133?164.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In Proceedings of BioNLP Shared
Task 2009, pages 19?27.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL?05, pages 173?
180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
M.H. Glickman and A. Ciechanover. 2002. The
ubiquitin-proteasome proteolytic pathway: destruction
for the sake of construction. Physiological reviews,
82(2):373.
R. Hoehndorf, A.C.N. Ngomo, S. Pyysalo, T. Ohta,
A. Oellrich, and D. Rebholz-Schuhmann. 2010.
Applying ontology design patterns to the imple-
mentation of relations in GENIA. In Proceedings
of the Fourth Symposium on Semantic Mining in
Biomedicine SMBM 2010.
Robin Holliday and JE Pugh. 1975. Dna modification
mechanisms and gene activity during development.
Science, 187:226?232.
Robin Holliday. 1987. The inheritance of epigenetic de-
fects. Science, 238:163?170.
Rudolf Jaenisch and Adrian Bird. 2003. Epigenetic reg-
ulation of gene expression: how the genome integrates
intrinsic and environmental signals. Nature Genetics,
33:245?254.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biological
event extraction. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
R. Leaman and G. Gonzalez. 2008. Banner: an exe-
cutable survey of advances in biomedical named entity
recognition. Proceedings of the Pacific Symposium on
Biocomputing (PSB?08), pages 652?663.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing
for bionlp 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
David McClosky. 2009. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010a. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2010b. Event extraction for dna
methylation. In Proceedings of SMBM?10.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
24
Van Criekinge. 2008. PubMeth: a cancer methy-
lation database combining text-mining and expert
annotation. Nucl. Acids Res., 36(suppl 1):D842?846.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and
Jun?ichi Tsujii. 2011a. Towards exhaustive protein
modification event extraction. In Proceedings of the
BioNLP 2011 Workshop, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011b.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. Msr-nlp entry in bionlp
shared task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decompo-
sition and minimal domain adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in bionlp 2011. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
A.D. Riggs. 1975. X inactivation, differentiation, and
dna methylation. Cytogenetic and Genome Research,
14:9?25.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
JB Stock, AJ Ninfa, and AM Stock. 1989. Protein
phosphorylation and regulation of adaptive responses
in bacteria. Microbiology and Molecular Biology Re-
views, 53(4):450.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Paul Thompson, Syed Iqbal, John McNaught, and Sophia
Ananiadou. 2009. Construction of an annotated
corpus to support biomedical information extraction.
BMC Bioinformatics, 10(1):349.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehen-
sive benchmark of kernel methods to extract protein-
protein interactions from literature. PLoS Comput
Biol, 6(7):e1000837, 07.
Yue Wang, Jin-Dong Kim, Rune S?tre, Sampo Pyysalo,
and Jun?ichi Tsujii. 2009. Investigating heteroge-
neous protein annotations toward cross-corpora uti-
lization. BMC Bioinformatics, 10(403), Dec. ISSN:
1471-2105.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
25
Proceedings of BioNLP Shared Task 2011 Workshop, pages 26?35,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011
Sampo Pyysalo? Tomoko Ohta? Rafal Rak?? Dan Sullivan? Chunhong Mao?
Chunxia Wang? Bruno Sobral? Jun?ichi Tsujii? Sophia Ananiadou??
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, USA
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
?Microsoft Research Asia, Beijing, China
{smp,okap}@is.s.u-tokyo.ac.jp jtsujii@microsoft.com
{dsulliva,cmao,cwang,sobral}@vbi.vt.edu
{rafal.rak,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper presents the preparation, resources,
results and analysis of the Infectious Diseases
(ID) information extraction task, a main task
of the BioNLP Shared Task 2011. The ID
task represents an application and extension
of the BioNLP?09 shared task event extrac-
tion approach to full papers on infectious dis-
eases. Seven teams submitted final results to
the task, with the highest-performing system
achieving 56% F-score in the full task, com-
parable to state-of-the-art performance in the
established BioNLP?09 task. The results in-
dicate that event extraction methods general-
ize well to new domains and full-text publi-
cations and are applicable to the extraction of
events relevant to the molecular mechanisms
of infectious diseases.
1 Introduction
The Infectious Diseases (ID) task of the BioNLP
Shared Task 2011 (Kim et al, 2011a) is an infor-
mation extraction task focusing on the biomolecu-
lar mechanisms of infectious diseases. The primary
target of the task is event extraction (Ananiadou et
al., 2010), broadly following the task setup of the
BioNLP?09 Shared Task (BioNLP ST?09) (Kim et
al., 2009).
The task concentrates on the specific domain of
two-component systems (TCSs, or two-component
regulatory systems), a mechanism widely used by
bacteria to sense and respond to the environment
(Thomason and Kay, 2000). Typical TCSs con-
sist of two proteins, a membrane-associated sensor
kinase and a cytoplasmic response regulator. The
sensor kinase monitors changes in the environment
while the response regulator mediates an adaptive
response, usually through differential expression of
target genes (Mascher et al, 2006). TCSs have many
functions, but those of particular interest for infec-
tious disease researchers include virulence, response
to antibiotics, quorum sensing, and bacterial cell at-
tachment (Krell et al, 2010). Not all TCS functions
are well known: in some cases, TCSs are involved
in metabolic processes that are difficult to precisely
characterize (Wang et al, 2010). TCSs are of in-
terest also as drugs designed to disrupt TCSs may
reduce the virulence of bacteria without killing it,
thus avoiding the potential selective pressure of an-
tibiotics lethal to some pathogenic bacteria (Gotoh
et al, 2010). Information extraction techniques may
support better understanding of these fundamental
systems by identifying and structuring the molecu-
lar processes underlying two component signaling.
The ID task seeks to address these opportuni-
ties by adapting the BioNLP ST?09 event extraction
model to domain scientific publications. This model
was originally introduced to represent biomolecu-
lar events relating to transcription factors in human
blood cells, and its adaptation to a domain that cen-
trally concerns both bacteria and their hosts involves
a variety of novel aspects, such as events concerning
whole organisms, the chemical environment of bac-
teria, prokaryote-specific concepts (e.g. regulons as
elements of gene expression), as well as the effects
of biomolecules on larger-scale processes involving
hosts such as virulence.
26
2 Task Setting
The ID task broadly follows the task definition and
event types of the BioNLP ST?09, extending it with
new entity categories, correspondingly broadening
the scope of events, and introducing a new class of
events, high-level biological processes.
2.1 Entities
The ID task defines five core types of entities:
genes/gene products, two-component systems, reg-
ulons/operons, chemicals, and organisms. Follow-
ing the general policy of the BioNLP Shared Task,
the recognition of the core entities is not part of
the ID task. As named entity recognition (NER)
is considered in other prominent domain evaluations
(Krallinger et al, 2008), we have chosen to isolate
aspects of extraction performance relating to NER
from the main task of interest, event extraction, by
providing participants with human-created gold an-
notations for core entities. These annotations are
briefly presented in the following.
Mentions of names of genes and their products
(RNA and proteins) are annotated with a single
type, without differentiating between subtypes, fol-
lowing the guidelines of the GENIA GGP corpus
(Ohta et al, 2009). This type is named PRO-
TEIN to maintain consistency with related tasks
(e.g. BioNLP ST?09), despite slight inaccuracy
for cases specifically referencing RNA or DNA
forms. Two-component systems, consisting of two
proteins, frequently have names derived from the
names of the proteins involved (e.g. PhoP-PhoR
or SsrA/SsrB). Mentions of TCSs are annotated as
TWO-COMPONENT-SYSTEM, nesting PROTEIN an-
notations if present. Regulons and operons are col-
lections of genes whose expression is jointly regu-
lated. Like the names of TCSs, their names may de-
rive from the names of the involved genes and pro-
teins, and are annotated as embedding PROTEIN an-
notations when they do. The annotation does not
differentiate between the two, marking both with a
single type REGULON-OPERON.
In addition to these three classes relating to genes
and proteins, the core entity annotation recognizes
the classes CHEMICAL and ORGANISM. All men-
tions of formal and informal names of atoms, inor-
ganic compounds, carbohydrates and lipids as well
as organic compounds other than amino acid and nu-
cleic acid compounds (i.e. gene/protein-related com-
pounds) are annotated as CHEMICAL. Mentions of
names of families, genera, species and strains as
well as non-name references with comparable speci-
ficity are annotated as ORGANISM.
Finally, the non-specific type ENTITY1 is defined
for marking entities that specify additional details of
events such as the binding site in a BINDING event or
the location an entity moves to in a LOCALIZATION
event. Unlike the core entities, annotations of the
generic ENTITY type are not provided for test data
and must be detected by participants addressing the
full task.
2.2 Relations
The ID task involves one relation, EQUIV, defin-
ing entities (of any of the core types) to be equiv-
alent. This relation is used to annotate abbreviations
and local aliases and it is not a target of extraction,
but provided for reference and applied in evaluation,
where references to any of a set of equivalent entities
are treated identically.
2.3 Events
The primary extraction targets of the ID task are the
event types summarized in Table 1. These are a su-
perset of those targeted in the BioNLP ST?09 and its
repeat, the 2011 GE task (Kim et al, 2011b). This
design makes it possible to study aspects of domain
adaptation by having the same extraction targets in
two subdomains of biomedicine, that of transcrip-
tion factors in human blood cells (GE) and infectious
diseases. The events in the ID task extend on those
of GE in the inclusion of additional entity types
as participants in previously considered event types
and the introduction of a new type, PROCESS. We
next briefly discuss the semantics of these events,
defined (as in GE) with reference to the community-
standard Gene Ontology (Ashburner et al, 2000).
We refer to (Kim et al, 2008; Kim et al, 2009) for
the ST?09/GE definitions.
1In terms of the GENIA ontology, ENTITY is used to mark
e.g. PROTEIN DOMAIN OR REGION references. Specific types
were applied in manual annotation, but these were replaced
with the generic ENTITY in part to maintain consistency with
BioNLP ST?09 data and to reduce the NER-related demands
on participating systems by not requiring the assignment of de-
tailed types.
27
Type Core arguments Additional arguments
GENE EXPRESSION Theme(PROTEIN or REGULON-OPERON)
TRANSCRIPTION Theme(PROTEIN or REGULON-OPERON)
PROTEIN CATABOLISM Theme(PROTEIN)
PHOSPHORYLATION Theme(PROTEIN) Site(ENTITY)
LOCALIZATION Theme(Core entity) AtLoc(ENTITY), ToLoc(ENTITY)
BINDING Theme(Core entity)+ Site(ENTITY)+
PROCESS Participant(Core entity)?
REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
POSITIVE REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
NEGATIVE REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
Table 1: Event types and their arguments. The type of entity allowed as argument is specified in parenthesis. ?Core en-
tity? is any of PROTEIN, TWO-COMPONENT-SYSTEM, REGULON-OPERON, CHEMICAL, or ORGANISM. Arguments
that can be filled multiple times marked with ?+?, non-mandatory core arguments with ??? (all additional arguments
are non-mandatory).
The definitions of the first four types in Table 1
are otherwise unchanged from the ST?09 definitions
except that GENE EXPRESSION and TRANSCRIP-
TION extend on the former definition in recogniz-
ing REGULON-OPERON as an alternative unit of ex-
pression. LOCALIZATION, taking only PROTEIN
type arguments in the ST?09 definition, is allowed
to take any core entity argument. This expanded
definition remains consistent with the scope of the
corresponding GO term (GO:0051179). BINDING
is similarly extended, giving it a scope largely con-
sistent with GO:0005488 (binding) but also encom-
passing GO:0007155 (cell adhesion) (e.g. a bac-
terium binding another) and protein-organism bind-
ing. The three regulation types (REGULATION,
POSITIVE REGULATION, and NEGATIVE REGULA-
TION) likewise allow the new core entity types as
arguments, but their definitions are otherwise un-
changed from those in ST?09, that is, the GENIA on-
tology definitions. As in these resources, regulation
types are used not only for the biological sense but
also to capture statements of general causality (Kim
et al, 2008). As in ST?09, all events of types dis-
cussed above require a Theme argument: only events
involving an explicitly stated theme (of an appropri-
ate type) should be extracted. All other arguments
are optional.
The PROCESS type, new to ID, is used to annotate
high-level processes such as virulence, infection and
resistance that involve infectious organisms. This
type differs from the others in that it has no manda-
tory arguments: the targeted processes should be ex-
tracted even if they have no explicitly stated partici-
pants, reflecting that they are of interest even without
the further specification. When stated, the involved
participants are captured using the generic role type
Participant. Figure 1 shows an illustration of some
of the the ID task extraction targets.
We term the first five event types in Table 1 taking
exactly one Theme argument as their core argument
simple events. In analysis we further differentiate
non-regulation events (the first seven) and regulation
(the last three), which is known to represent partic-
ular challenges for extraction in involving events as
arguments, thus creating nested event structures.
2.4 Event modifications
The ID task defines two event modification ex-
traction targets, NEGATION and SPECULATION.
These modifications mark events as being explic-
itly negated (e.g. virB is not expressed) or stated in
a speculative context (e.g. virB may be expressed).
Both may apply simultaneously. The modification
definitions are identical to the ST?09 ones, includ-
ing the representation in which modifications (un-
like events) are not assigned text bindings.
3 Data
The ID task data were newly annotated for the
BioNLP Shared Task and are not based on any previ-
ously released resource. Annotation was performed
by two teams, one in Tsujii laboratory (University
of Tokyo) and one in Virginia Bioinformatics Insti-
tute (Virginia Tech). The entity and event annotation
28
Figure 1: Example event annotation. The association of a TCS with an organism is captured through an event structure
involving a PROCESS (?virulence?) and POSITIVE REGULATION. Regulation types are used to capture also statements
of general causality such as ?is essential for? here. (Simplified from PMC ID 2358977)
Journal # Published
PLoS Pathogens 9 2006?2010
PLoS One 7 2008?2010
BMC Genomics 3 2008?2010
PLoS Genetics 2 2007?2010
Open Microbiology J. 2 2008?2010
BMC Microbiology 2 2008?2009
Other 5 2007?2008
Table 2: Corpus composition. Journals in which selected
articles were published with number of articles (#) and
publication years.
design was guided by previous studies on NER and
event extraction in a closely related domain (Pyysalo
et al, 2010; Ananiadou et al, 2011).
3.1 Document selection
The training and test data were drawn from the pri-
mary text content of recent full-text PMC open ac-
cess documents selected by infectious diseases do-
main experts (Virginia Tech team) as representative
publications on two-component regulatory systems.
Table 2 presents some characteristics of the corpus
composition. To focus efforts on natural language
text likely to express novel information, we excluded
tables, figures and their captions, as well as methods
sections, acknowledgments, authors? contributions,
and similar meta-content.
3.2 Annotation
Annotation was performed in two primary stages,
one for marking core entities and the other for events
and secondary entities. As a preliminary processing
step, initial sentence segmentation was performed
with the GENIA Sentence Splitter2. Segmentation
errors were corrected during core entity annotation.
Core entity annotation was performed from the
basis of an automatic annotation created using se-
lected existing taggers for the target entities. The
2http://www-tsujii.is.s.u-tokyo.ac.jp/
?y-matsu/geniass/
Entity type prec. rec. F
PROTEIN 54.64 39.64 45.95
CHEMICAL 32.24 19.05 23.95
ORGANISM 90.38 47.70 62.44
TWO-COMPONENT-SYSTEM 87.69 47.24 61.40
Table 3: Automatic core entity tagging performance.
following tools and settings were adopted, with pa-
rameters tuned on initial annotation for two docu-
ments:
PROTEIN: NeMine (Sasaki et al, 2008) trained on
the JNLPBA data (Kim et al, 2004) with threshold
0.05, filtered to only GENE and PROTEIN types.
ORGANISM: Linnaeus (Gerner et al, 2010) with
?variant matching? for species names variants.
CHEMICAL: OSCAR3 (Corbett and Murray-Rust,
2006) with confidence 90%.
TWO-COMPONENT-SYSTEM: Custom regular ex-
pressions.
Initial automatic tagging was not applied for en-
tities of the REGULON-OPERON type or the generic
ENTITY type (for additional event arguments). All
automatically generated annotations were at least
confirmed through manual inspection, and the ma-
jority of the automatic annotations were revised in
manual annotation. Table 3 summarizes the tag-
ging performance of the automatic tools as measured
against the final human-annotated training and de-
velopment datasets.3
Annotation for the task extraction targets ? events
and event modifications ? was created entirely man-
ually without automatic annotation support to avoid
any possible bias toward specific extraction meth-
ods or approaches. The Tsujii laboratory team orga-
3It should be noted that these results are low in part due to
differences in annotation criteria (see e.g. (Wang et al, 2009))
and to data tagged using the ID task annotation guidelines not
being applied for training; training on the newly annotated data
is expected to allow notably more accurate tagging.
29
Item Train Devel Test Total
Articles 15 5 10 30
Sentences 2,484 709 1,925 5118
Words 74,439 21,225 57,489 153,153
Core entities 6,525 1,976 4,239 12,740
Events 2,088 691 1,371 4150
Modifications 95 45 74 214
Table 4: Statistics of the ID corpus.
nized the annotation effort, with a coordinating an-
notator with extensive experience in event annota-
tion (TO) leading annotator training and annotation
scheme development. Detailed annotation guide-
lines (Pyysalo et al, 2011) extending on the GE-
NIA annotation guidelines were developed jointly
with all annotators and refined throughout the an-
notation effort. Based on measurements of inter-
annotator consistency between annotations indepen-
dently created by the two teams, made throughout
annotator training and primary annotation (exclud-
ing final corpus cleanup), we estimate the consis-
tency of the final entity annotation to be no lower
than 90% F-score and that of the event annotation to
be no lower than 75% F-score for the primary eval-
uation criteria (see Section 4).
3.3 Datasets and statistics
Initial annotation was produced for the selected sec-
tions (see Section 3.1) in 33 full-text articles, of
which 30 were selected for the final dataset as repre-
sentative of the extraction targets. These documents
were split into training, development and test sets of
15, 5 and 10 documents, respectively. Participants
were provided with all training and development set
annotations and test set core entity annotations. The
overall statistics of the datasets are given in Table 4.
As the corpus consists of full-text articles, it con-
tains a somewhat limited number of articles, but in
other terms it is of broadly comparable size to the
largest of the BioNLP ST corpora: the corpus word
count, for example, corresponds to that of a cor-
pus of approximately 800 PubMed abstracts, and the
core entity count is comparable to that in the ST?09
data. However, for reasons that may relate in part to
the domain, the event count is approximately a third
of that for the ST?09 data. In addition to having less
training data, the entity/event ratio is thus consider-
ably higher (i.e. there are more candidates for each
true target), suggesting that the ID data could be ex-
pected to provide a more challenging extraction task.
4 Evaluation
The performance of participating systems was
evaluated in terms of events using the standard
precision/recall/F-score metrics. For the primary
evaluation, we adopted the standard criteria defined
in the BioNLP?09 shared task. In brief, for deter-
mining whether a reference annotation and a pre-
dicted annotation match, these criteria relax exact
matching for event triggers and arguments in two
ways: matching of text-bound annotation (event
triggers and ENTITY type entities) allows limited
boundary variation, and only core arguments need to
match in nested event arguments for events to match.
For details of the matching criteria, please refer to
Kim et al (2009).
The primary evaluation for the task requires the
extraction of all event arguments (both core and ad-
ditional; see Table 1) as well as event modifications
(NEGATION and SPECULATION). This is termed
the full task. We additionally report extraction re-
sults for evaluation where both the gold standard ref-
erence data and the submission events are reduced
to only core arguments, event modifications are re-
moved, and resulting duplicate events removed. We
term this the core task. In terms of the subtask divi-
sion applied in the BioNLP?09 Shared Task and the
GE task of 2011, the core task is analogous to sub-
task 1 and the full task analogous to the combination
of subtasks 1?3.
5 Results
5.1 Participation
Final results to the task were successfully submitted
by seven participants. Table 5 summarizes the in-
formation provided by the participating teams. We
note that full parsing is applied in all systems, with
the specific choice of the parser of Charniak and
Johnson (2005) with the biomedical domain model
of McClosky (2009) and conversion into the Stan-
ford Dependency representation (de Marneffe et al,
2006) being adopted by five participants. Further,
five of the seven systems are predominantly machine
learning-based. These can be seen as extensions of
trends that were noted in analysis of the BioNLP
30
NLP Events Other resources
Rank Team Org Word Parse Trig. Arg. Group. Modif. Corpora Other
1 FAUST 3NLP
CoreNLP,
SnowBall
McCCJ + SD (UMass+Stanford as features) GE word clusters
2 UMass 1NLP
CoreNLP,
SnowBall
McCCJ + SD Joint, dual dec.+MIRA 1-best - GE -
3 Stanford 3NLP CoreNLP McCCJ + SD MaxEnt Joint, MSTParser - GE word clusters
4 ConcordU 2NLP - McCCJ + SD dict rules rules rules -
triggers and
hedge words
5 UTurku 1BI Porter McCCJ + SD SVM SVM SVM SVM - hedge words
6 PNNL
1CS, 1NLP,
2BI
Porter Stanford SVM SVM rules - GE UMLS, triggers
7 PredX 1CS, 1NLP LGP LGP dict rules rules - - UMLS, triggers
Table 5: Participants and summary of system descriptions. Abbreviations: Trig./Arg./Group./Modif.=event trigger
detection/argument detection/argument grouping/modification detection, BI=Bioinformatician, NLP=Natural Lan-
guage Processing researcher, CS=Computer scientist, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snow-
ball=Snowball stemmer McCCJ=McClosky-Charniak-Johnson parser, LGP=Link Grammar Parser, SD=Stanford De-
pendency conversion, UMLS=UMLS resources (e.g. lexicon, metamap)
ST?09 participation. In system design choices, we
note an indication of increased use of joint models
as opposed to pure pipeline designs, with the three
highest-ranking systems involving a joint model.
Several participants compiled dictionaries of
event trigger words and two dictionaries of hedge
words from the data. Four teams, including the three
top-ranking, used the GE task corpus as supplemen-
tary material, indicating that the GE annotations are
largely compatible with ID ones (see detailed results
below). This is encouraging for future applications
of the event extraction approach: as manual annota-
tion requires considerable effort and time, the ability
to use existing annotations is important for the feasi-
bility of adaptation of the approach to new domains.
While several participants made use of support-
ing syntactic analyses provided by the organizers
(Stenetorp et al, 2011), none applied the analyses
for supporting tasks, such as coreference or entity
relation extraction results ? at least in cases due to
time constraints (Kilicoglu and Bergler, 2011).
5.2 Evaluation results
Table 6 presents the primary results by event type,
and Table 7 summarizes these results. The full
task requires the extraction of additional arguments
and event modifications and involves multiple novel
challenges from previously addressed domain tasks
including a new subdomain, full-text documents,
several new entity types and a new event category.
Team recall prec. F-score
FAUST 48.03 65.97 55.59
UMass 46.92 62.02 53.42
Stanford 46.30 55.86 50.63
ConcordU 49.00 40.27 44.21
UTurku 37.85 48.62 42.57
PNNL 27.75 52.36 36.27
PredX 22.56 35.18 27.49
Table 7: Primary evaluation results.
Nevertheless, extraction performance for the top
systems is comparable to the state-of-the-art results
for the established BioNLP ST?09 task (Miwa et al,
2010) as well as its repetition as the 2011 GE task
(Kim et al, 2011b), where the highest overall result
for the primary evaluation criteria was also 56% F-
score for the FAUST system (Riedel et al, 2011).
This result is encouraging regarding the ability of
the extraction approach and methods to generalize
to new domains as well as their applicability specifi-
cally to texts on the molecular mechanisms of infec-
tious diseases.
We note that there is substantial variation in the
relative performance of systems for different en-
tity types. For example, Stanford (McClosky et
al., 2011) has relatively low performance for simple
events but achieves the highest result for PROCESS,
while UTurku (Bjo?rne and Salakoski, 2011) results
show roughly the reverse. This suggests further po-
tential for improvement from system combinations.
31
FAUST UMass Stanford ConcordU UTurku PNNL PredX Size
GENE EXPRESSION 70.68 66.43 54.00 56.57 64.88 53.33 0.00 512
TRANSCRIPTION 69.66 68.24 60.00 70.89 57.14 0.00 53.85 77
PROTEIN CATABOLISM 75.00 72.73 20.00 66.67 33.33 11.76 0.00 33
PHOSPHORYLATION 64.00 66.67 40.00 54.55 60.61 64.29 40.00 69
LOCALIZATION 33.33 14.29 31.58 20.00 66.67 20.69 0.00 49
Simple event total 68.47 63.55 52.72 56.78 62.67 43.87 18.18 740
BINDING 31.30 34.62 23.44 40.00 22.22 20.00 28.28 156
PROCESS 65.69 62.26 73.57 67.17 41.57 51.04 53.27 901
Non-regulation total 63.78 60.68 63.59 62.43 46.39 47.34 43.65 1797
REGULATION 35.44 30.49 17.67 19.43 22.96 0.00 2.16 267
POSITIVE REGULATION 47.50 49.49 34.78 23.41 41.28 24.60 21.02 455
NEGATIVE REGULATION 58.86 60.45 44.44 47.96 52.11 25.70 9.49 260
Regulation total 47.07 46.65 33.02 28.87 39.49 18.45 9.71 982
Subtotal 57.28 55.03 52.09 46.60 43.33 37.53 28.38 2779
NEGATION 0.00 0.00 0.00 22.92 32.91 0.00 0.00 96
SPECULATION 0.00 0.00 0.00 3.23 15.00 0.00 0.00 44
Modification total 0.00 0.00 0.00 11.82 26.89 0.00 0.00 140
Total 55.59 53.42 50.63 44.21 42.57 36.27 27.49 2919
Table 6: Primary evaluation F-scores by event type. The ?size? column gives the number of annotations of each type
in the given data (training+development). Best result for each type shown in bold.
The best performance for simple events and for
PROCESS approaches or exceeds 70% F-score, ar-
guably approaching a sufficient level for user-facing
applications of the extraction technology. By con-
trast, BINDING and regulation events, found chal-
lenging in ST?09 and GE, remain problematic also
in the ID task, with best overall performance below
50% F-score. Only two teams, UTurku and Con-
cordU (Kilicoglu and Bergler, 2011), attempted to
extract event modifications, with somewhat limited
performance. The difficulty of correct extraction of
event modifications is related in part to the recursive
nature of the problem (similarly as for nested reg-
ulation events): to extract a modification correctly,
the modified event must also be extracted correctly.
Further, only UTurku predicted any instances of sec-
ondary arguments. Thus, teams other than UTurku
and ConcordU addressed only the core task extrac-
tion targets. With the exception of ConcordU, all
systems clearly favor precision over recall (Table 7),
in many cases having over 15% point higher preci-
sion than recall. This a a somewhat unexpected in-
version, as the ConcordU system is one of the two
rule-based in the task, an approach typically associ-
ated with high precision.
The five top-ranking systems participated also in
the GE task (Kim et al, 2011b), which involves a
subset of the ID extraction targets. This allows ad-
ditional perspective into the relative performance of
the systems. While there is a 13% point spread in
overall results for the top five systems here, in GE
all these systems achieved F-scores ranging between
50?56%. The results for FAUST, UMass and Stan-
ford were similar in both tasks, while the ConcordU
result was 6% points higher for GE and the UTurku
result over 10% points higher for GE, ranking third
after FAUST and UMass. These results suggest that
while the FAUST and UMass systems in particular
have some systematic (e.g. architectural) advantage
at both tasks, much of the performance difference
observed here between the top three systems and
those of ConcordU and UTurku is due to strengths
or weaknesses specific to ID. Possible weaknesses
may relate to the treatment of multiple core entity
types (vs. only PROTEIN in GE) or challenges re-
lated to nested entity annotations (not appearing in
GE). A possible ID-specific strength of the three
top-ranking systems is the use of GE data for train-
ing: Riedel and McCallum (2011) report an esti-
mated 7% point improvement and McClosky et al
(2011) a 3% point improvement from use of this
data; McGrath et al (2011) estimate a 1% point im-
provement from direct corpus combination. The in-
tegration strategies applied in training these systems
32
Team recall prec. F-score ?
FAUST 50.62 66.06 57.32 1.73
UMass 49.45 62.11 55.06 1.64
Stanford 48.87 56.03 52.20 1.57
ConcordU 50.77 43.25 46.71 2.50
UTurku 38.79 49.35 43.44 0.87
PNNL 29.36 52.62 37.69 1.42
PredX 23.67 35.18 28.30 0.81
Table 8: Core task evaluation results. The ? column
gives the F-score difference to the corresponding full task
(primary) result.
could potentially be applied also with other systems,
an experiment that could further clarify the relative
strengths of the various systems. The top-ranking
five systems all participated also in the EPI task
(Ohta et al, 2011), for which UTurku ranked first
with FAUST having comparable performance for the
core task. While this supports the conclusion that
ID performance differences do not reflect a simple
universal ranking of the systems, due to many sub-
stantial differences between the ID and EPI setups it
is not straightforward to identify specific reasons for
relative differences to performance at EPI.
Table 8 summarizes the core task results. There
are only modest and largely consistent differences to
the corresponding full task results, reflecting in part
the relative sparseness of additional arguments: in
the training data, for example, only approximately
3% of instances of event types that can potentially
take additional arguments had at least one additional
argument. While event modifications represent a
further 4% of full task extraction targets not required
for the core task, the overall low extraction perfor-
mance for additional arguments and modifications
limits the practical effect of these annotation cate-
gories on the performance difference between sys-
tems addressing only the core targets and those ad-
dressing the full task.
6 Discussion and Conclusions
We have presented the preparation, resources, re-
sults and analysis of the Infectious Diseases (ID)
task of the BioNLP Shared Task 2011. A corpus
of 30 full-text publications on the two-component
systems subdomain of infectious diseases was cre-
ated for the task in a collaboration of event annota-
tion and domain experts, adapting and extending the
BioNLP?09 Shared Task (ST?09) event representa-
tion to the domain.
Seven teams submitted final results to the ID task.
Despite the novel challenges of full papers, four new
entity types, extension of event scopes and the intro-
duction of a new event category for high-level pro-
cesses, the highest results for the full ID task were
comparable to the state-of-the-art performance on
the established ST?09 data, showing that the event
extraction approach and present systems generalize
well and demonstrating the feasibility of event ex-
traction for the infectious diseases domain. Analy-
sis of results suggested further opportunities for im-
proving extraction performance by combining the
strengths of various systems and the use of other
event resources.
The task design takes into account the needs
of supporting practical applications, and its results
and findings will be adopted in future development
of the Pathosystems Resource Integration Center4
(PATRIC). Specifically, PATRIC will combine do-
main named entity recognition and event extraction
to mine the virulence factor literature and integrate
the results with literature search and retrieval ser-
vices, protein feature analysis, and systems such as
Disease View.5 Present and future advances at the
ID event extraction task can thus assist biologists in
efforts of substantial public health interest.
The ID task will be continued as an open
shared task challenge with data, supporting re-
sources, and evaluation tools freely available from
the shared task site, http://sites.google.
com/site/bionlpst/.
Acknowledgments
This work was supported by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan). This
project has been funded in whole or in part with Fed-
eral funds from the National Institute of Allergy and
Infectious Diseases, National Institutes of Health,
Department of Health and Human Services, under
Contract No. HHSN272200900040C, awarded to
BWS Sobral.
4http://patricbrc.org
5See for example http://patricbrc.org/portal/
portal/patric/DiseaseOverview?cType=
taxon&cId=77643
33
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Peter Corbett and Peter Murray-Rust. 2006. High-
throughput identification of chemistry in life science
texts. Computational Life Sciences II, pages 107?118.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Martin Gerner, Goran Nenadic, and Casey M. Bergman.
2010. LINNAEUS: a species name identification sys-
tem for biomedical literature. BMC bioinformatics,
11(1):85+, February.
Yasuhiro Gotoh, Yoko Eguchi, Takafumi Watanabe, Sho
Okamoto, Akihiro Doi, and Ryutaro Utsumi. 2010.
Two-component signal transduction as potential drug
targets in pathogenic bacteria. Current Opinion in Mi-
crobiology, 13(2):232?239. Cell regulation.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biological
event extraction. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier, editors. 2004. Intro-
duction to the bio-entity recognition task at JNLPBA,
Geneva, Switzerland.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
Tino Krell, Jess Lacal, Andreas Busch, Hortencia Silva-
Jimnez, Mara-Eugenia Guazzaroni, and Juan Luis
Ramos. 2010. Bacterial sensor kinases: Diversity in
the recognition of environmental signals. Annual Re-
view of Microbiology, 64(1):539?559.
Thorsten Mascher, John D. Helmann, and Gottfried Un-
den. 2006. Stimulus perception in bacterial signal-
transducing histidine kinases. Microbiol. Mol. Biol.
Rev., 70(4):910?938.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing
for bionlp 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
David McClosky. 2009. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Liam McGrath, Kelly Domico, Courtney Corley, and
Bobbie-Jo Webb-Robertson. 2011. Complex biologi-
cal event extraction from full text using signatures of
linguistic and semantic features. In Proceedings of
34
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In Proceedings of COL-
ING?10, pages 779?787.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sulli-
van, Chunhong Mao, Chunxia Wang, Bruno Sobral,
Jun?ichi Tsujii, and Sophia Ananiadou. 2011. An-
notation guidelines for infectious diseases event cor-
pus. Technical report, Tsujii Laboratory, University of
Tokyo. To appear.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decompo-
sition and minimal domain adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in bionlp 2011. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Yutaka Sasaki, Yoshimasa Tsuruoka, John McNaught,
and Sophia Ananiadou. 2008. How to make the most
of NE dictionaries in statistical NER. BMC bioinfor-
matics, 9 Suppl 11.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Peter Thomason and Rob Kay. 2000. Eukaryotic sig-
nal transduction via histidine-aspartate phosphorelay.
J Cell Sci, 113(18):3141?3150.
Yue Wang, Jin-Dong Kim, Rune S?tre, Sampo Pyysalo,
and Jun?ichi Tsujii. 2009. Investigating heteroge-
neous protein annotations toward cross-corpora uti-
lization. BMC Bioinformatics, 10(403).
Chunxia Wang, Jocelyn Kemp, Isabel O. Da Fonseca,
Raymie C. Equi, Xiaoyan Sheng, Trevor C. Charles,
and Bruno W. S. Sobral. 2010. Sinorhizobium
meliloti 1021 loss-of-function deletion mutation in
chvi and its phenotypic characteristics. Molecular
Plant-Microbe Interactions, 23(2):153?160.
35
Proceedings of BioNLP Shared Task 2011 Workshop, pages 74?82,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Protein Coreference task in BioNLP Shared Task 2011
Ngan Nguyen
University of Tokyo
Hongo 7-3-1, Bunkyoku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jin-Dong Kim
Database Center for Life Science
Yayoi 2-11-16, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
This paper summarizes the Protein Coref-
erence Resolution task of BioNLP Shared
Task 2011. After 7 weeks of system devel-
opment period, the task received final sub-
missions from 6 teams. Evaluation results
show that state-of-the-art performance on the
task can find 22.18% of protein coreferences
with the precision of 73.26%. Analysis of
the submissions shows that several types of
anaphoric expressions including definite ex-
pressions, which occupies a significant part of
the problem, have not yet been solved.
1 Introduction
While named entity recognition (NER) and relation
or event extraction are regarded as standard tasks
of information extraction (IE), coreference resolu-
tion (Ng, 2010; Bejan and Harabagiu, 2010) is more
and more recognized as an important component of
IE for a higher performance. Without coreference
resolution, the performance of IE is often substan-
tially limited due to an abundance of coreference
structures in natural language text, i.e. information
pieces written in text with involvement of a corefer-
ence structure are hard to be captured (Miwa et al,
2010). There have been several attempts for coref-
erence resolution, particularly for newswire texts
(Strassel et al, 2008; Chinchor, 1998). It is also one
of the lessons from BioNLP Shared Task (BioNLP-
ST, hereafter) 2009 that coreference structures in
biomedical text substantially hinder the progress of
fine-grained IE (Kim et al, 2009).
To address the problem of coreference resolution
in molecular biology literature, the Protein Corefer-
ence (COREF) task is arranged in BioNLP-ST 2011
as a supporting task. While the task itself is not
an IE task, it is expected to be a useful compo-
nent in performing the main IE tasks more effec-
tively. To establish a stable evaluation and to observe
the effect of the results of the task to the main IE
tasks, the COREF task particularly focuses on find-
ing anaphoric protein references.
The benchmark data sets for developing and test-
ing coreference resolution system were developed
based on various manual annotations made to the
Genia corpus (Ohta et al, 2002). After 7 weeks of
system development phase, for which training and
development data sets with coreference annotation
were given, six teams submitted their prediction of
coreferences for the test data. The best system ac-
cording to our primary evaluation criteria is evalu-
ated to find 22.18% of anaphoric protein references
at the precision of 73.26%.
This paper presents overall explanation of the
COREF task, which includes task definition (Sec-
tion 2), data preparation (Section 4), evaluation
methods (Section 5), results (Section 7), and thor-
ough analyses (Section 8) to figure out what are
remaining problems for coreference resolution in
biomedical text.
2 Problem Definition
This section provides an explanation of the corefer-
ence resolution task in our focus, through examples.
Figure 1 shows an example text segmented into
four sentences, S2 - S5, where anaphoric corefer-
ences are illustrated with colored extends and ar-
rows. In the figure, protein names are highlighted in
purple, T4 - T10, and anaphoric protein references,
e.g. pronouns and definite noun phrases, are high-
lighted in red, T27, T29, T30, T32, of which the an-
74
Figure 1: Protein coreference annotation
tecedents are indicated by arrows if found in the text.
In the example, the definite noun phrase (NP), this
transcription factor (T32), is a coreference to p65
(T10). Without knowing the coreference structure,
it becomes hard to capture the information written
in the phrase, nuclear exclusion of this transcription
factor, which is localization of p65 (out of nucleus)
according to the framework of BioNLP-ST.
A standard approach would include a step to find
candidate anaphoric expressions that may refer to
proteins. In this task, pronouns, e.g. it or they, and
definite NPs that may refer to proteins, e.g. the tran-
scription factor or the inhibitor are regarded as can-
didates of anaphoric protein references. This step
corresponds to markable detection and anaphoric-
ity determination steps in the jargon of MUC. The
next step would be to find the antecedents of the
anaphoric expressions. This step corresponds to
anaphora resolution in the jargon of MUC.
3 Task Setting
In the task, the training, development and test data
sets are provided in three types of files: the text, the
protein annotation, and the coreference annotation
files. The text files contain plain texts which are tar-
get of annotation. The protein annotation files pro-
vide gold annotation for protein names in the texts,
and the coreference annotation files provide gold an-
notation for anaphoric references to those protein
names. The protein annotation files are given to the
participants, together with all the training, develop-
ment and test data sets. The coreference annotation
files are not given with the test data set, and the task
for the participants is to produce them automatically.
In protein annotation files, annotations for protein
names are given in a stand-off style encoding. For
example, those highlighted in purple in Figure 1 are
protein names, which are given in protein annotation
files as follows:
T4 Protein 275 278 p65
T5 Protein 294 297 p50
T6 Protein 367 372 v-rel
T7 Protein 406 409 p65
T8 Protein 597 600 p50
T9 Protein 843 848 MAD-3
T10 Protein 879 882 p65
The first line indicates there is a protein reference
in the span that begins at 275th character and ends
before 278th character, of which the text is ?p65?,
and the annotation is identified by the id, ?T4?
The coreference annotation files include three sort
of annotations. First, annotations for anaphoric pro-
tein references are given. For example, those in red
in Figure 1 are anaphoric protein references:
T27 Exp 179 222 the N.. 215 222 complex
T29 Exp 307 312 which
T30 Exp 459 471 this .. 464 471 complex
T32 Exp 1022 1047 this .. 1027 1047 tra..
The first line indicates that there is an anaphoric
protein reference in the specified span, of which the
text is ?the NF-kappa B transcription factor com-
plex? (truncated due to limit of space), and that its
minimal expression is ?complex?. Second, noun
phrases that are antecedents of the anaphoric refer-
ences are also given in the coreference annotation
file. For example, T28 and T31 (highlighted in blue)
are antecedents of T29 and T32, respectively, and
thus given in the file:
T28 Exp 264 297 NF-ka..
T31 Exp 868 882 NF-ka..
Third, the coreference relation between the
anaphoric expressions and their antecedents are
given in predicate-argument expressions1:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
The first line indicates there is a coreference rela-
tion, R1, of which the anaphor is T29 and the an-
tecedent is T28, and the relation involves two protein
names, T5 and T4.
Note that, sometimes, an anaphoric expression,
e.g. which (T29), is connected to more than one
protein names, e.g. p65 (T4) and p50 (T5). Some-
times, coreference structures do not involve any spe-
cific protein names, e.g. T30 and T27. In order
1Due to limitation of space, argument names are abbrevi-
ated, e.g. ?Ana? for ?Anaphora?, and ?Ant? for ?Antecedent?
75
to establish a stable evaluation, our primary evalu-
ation will focus only on coreference structures that
involve specific protein names, e.g. T29 and T28,
and T32 and T31. Among the three, only two, R1
and R3, involves specific protein references, T4 and
T5, and T10. Thus, finding of R2 will be ignored
in the primary evaluation. However, those not in-
volving specific protein references are also provided
in the training data to help system development,
and will be considered in the secondary evaluation
mode. See section 5 for more detail.
4 Data Preparation
The data sets for the COREF task are produced
based on three resources: MedCO coreference an-
notation (Su et al, 2008), Genia event annotation
(Kim et al, 2008), and Genia Treebank (Tateisi et
al., 2005). Although the three have been developed
independently from each other, they are annotations
made to the same corpus, the Genia corpus (Kim et
al., 2008). Since COREF was focused on finding
anaphoric references to proteins (or genes), only rel-
evant annotations were extracted from the MedCO
corpus though the following process:
1. From MedCo annotation, coreference entities that
were pronouns and definite base NPs were ex-
tracted, which became candidate anaphoric expres-
sions. The base NPs were determined by consulting
Genia Tree Bank.
2. Among the candidate anaphoric expressions, those
that could not be protein references were filtered
out. This process was done by checking the head
noun of NPs. For example, definite NPs with ?cell?
as their head noun were filtered out. The remaining
ones became candidate protein coreferences.
3. The candidate protein coreferences and their an-
tecedents according to MedCo annotation were in-
cluded in the data files for COREF task.
4. The protein name annotations from Genia event
annotation were added to the data files to deter-
mine which coreference expressions involve protein
name references.
Table 1 summarizes the coreference entities in the
training, development, and test sets for COREF task.
In the table, the anaphoric entities are classified into
four types as follows:
RELAT indicates relative pronouns or relative adjec-
tives, e.g. that, which, or whose.
PRON indicates pronouns, e.g. it.
Type Train Dev Test
RELAT 1193 254 349
PRON 738 149 269
Anaphora DNP 296 58 91
APPOS 9 1 3
N/C 11 1 2
Antecedent 2116 451 674
TOTAL 4363 914 1388
Table 1: Statistics of coreference entities in COREF data
sets: N/C = not-classified.
DNP indicates definite NPs or demonstrative NPs, e.g.
NPs that begin with the, this, etc.
APPOS indicates coreferences in apposition.
5 Evaluation
The coreference resolution performance is evaluated
in two modes.
The Surface coreference mode evaluates the per-
formance of finding anaphoric protein references
and their antecedents, regardless whether the an-
tecedents actually embed protein names or not. In
other words, it evaluates the ability to predict the
coreference relations as provided in the gold coref-
erence annotation file, which we call surface coref-
erence links.
The protein coreference mode evaluates the per-
formance of finding anaphoric protein references
with their links to actual protein names (protein
coreference links). In the implementation of the
evaluation, the chain of surface coreference linkes
is traced until an antecedent embedding a protein
name is found. If a protein-name-embedding an-
tecedent is connected to an anaphora through only
one surfs link, we call the antecedent a direct pro-
tein antecedent. If a protein-name-embedding an-
teceden is connected to an anaphora through more
than one surface link, we call it an indirect protein
antecedent, and the antecedents in the middle of the
chain intermediate antecedents. The performance
evaluated in this mode may be directly connected
to the potential performance in main IE tasks: the
more the (anaphoric) protein references are found,
the more the protein-related events may be found.
For this reason, the protein coreference mode is cho-
sen as the primary evaluation mode.
Evaluation results for both evaluation modes are
76
given in traditional precision, recall and f-score,
which are similar to (Baldwin, 1997).
5.1 Surface coreference
A response expression is matched with a gold ex-
pression following partial match criterion. In par-
ticular, a response expression is considered cor-
rect when it covers the minimal boundary, and is
included in the maximal boundary of expression.
Maximal boundary is the span of expression anno-
tation, and minimal boundary is the head of ex-
pression, as defined in MUC annotation schemes
(Chinchor, 1998). A response link is correct when
its two argument expressions are correctly matched
with those of a gold link.
5.2 Protein coreference
This is the primary evaluation perspective of the pro-
tein coreference task. In this mode, we ignore coref-
erence links that do not reference to proteins. Inter-
mediate antecedents are also ignored.
Protein coreference links are generated from the
surface coreference links. A protein coreference link
is composed of an anaphoric expression and a pro-
tein reference that appears in its direct or indirect
antecedent. Below is an example.
Example:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
R4 Coref Ana:T33 Ant:T32
In this example, supposing that there are four surface
links in the coreference annotation file (T29,T28),
(T30,T27), (T32,T31), and (T33, T32), in which
T28 contains two protein mentions T5, T4, and T31
contains one protein mention T10; thus, the protein
coreference links generated from these surface links
are (T29,T4), (T29,T5), (T32,T10), and (T33, T10).
Notice that T33 is connected with T10 through the
intermediate expression T32.
Response expressions and generated response re-
sult links are matched with gold expressions and
links correspondingly in a way similar to the surface
coreference evaluation mode.
6 Participation
We received submissions from six teams. Each team
was requested to submit a brief description of their
team, which was summarized in Table 2.
Team Member Approach & Tools
UU 1 NLP ML (Yamcha SVM,
Reconcile)
UZ 5 NLP RB (-)
CU 2 NLP RB (-)
UT 1 biochemist ML (SVM-Light)
US 2 AI ML (SVM-Light)
UC 3 NLP, 1 BioNLP ML (Weka SVM)
Table 2: Participation. UU = UofU, UZ = UZH,
CU=ConcordU, UT = UTurku, UZ = UZH, US =
Uszeged, UC = UCD SCI, RB = Rule-based, ML = Ma-
chine learning-based.
TEAM RESP C P R F
UU 86 63 73.26 22.18 34.05
UZ 110 61 55.45 21.48 30.96
CU 87 55 63.22 19.37 29.65
UT 61 41 67.21 14.44 23.77
US 259 9 3.47 3.17 3.31
UC 794 2 0.25 0.70 0.37
Table 3: Protein coreference results. Total num-
ber of gold link = 284. RESP=response, C=correct,
P=precision, R=recall, F=fscore
The tool column shows the external tools used
in resolution processing. Among these tools,
there is only one team used an external coref-
erence resolution framework, Reconcile, which
achieved the state-of-the-art performance for super-
vised learning-based coreference resolution (Stoy-
anov et al, 2010b).
7 Results
7.1 Protein coreference results
Evaluation results in the protein coreference mode
are shown in Table 3. The UU team got the high-
est f-score 34.05%. The UZ and CU teams are
the second- and third-best teams with 30.96% and
29.65% f-score correspondingly, which are compa-
rable to each other. Unfortunately, two teams, US
and UC could not produce meaningful results, and
the other four teams show performance optimized
for high precision. It was expected that the 22.18%
of protein coreferences may contribute to improve
the performance on main task, which was not ob-
served this time, unfortunately.
The first ranked system by UU utilized Recon-
77
TEAM RESP C P R F
UU 360 43 11.94 20.48 15.09
UZ 736 51 6.93 24.29 10.78
CU 365 36 9.86 17.14 12.52
UT 452 50 11.06 23.81 15.11
US 259 4 1.54 1.90 1.71
UC 797 1 0.13 0.48 0.20
Table 4: Surface coreference results. Total num-
ber of gold link = 210. RESP=response, C=correct,
P=precision, R=recall, F=fscore
UU UT
S-correct & P-missing 8 29
S-missing & P-correct 16 5
Table 5: Count of anaphors that have different status in
different evaluation modes. S = surface coreference eval-
uation mode, P = protein coreference evaluation mode
cile which was originally developed for newswire
domain. It supports the hypothesis that machine
learning-based coreference resolution tool trained
on different domains can be helpful for the bio med-
ical domain; however, it still requires some adapta-
tions.
7.2 Surface coreference results
Table 4 shows the evaluation results in the surface
link mode. The overall performances of all the sys-
tems are low, in which recalls are much higher than
the precisions. One possible reason of the low re-
sults is because most of the teams focus on resolv-
ing pronominal coreference; however, they failed to
solve some difficult types of pronoun such as ?it?,
?its?, ?these?, ?them?, and ?which?, which occupy
the majority of anaphoric pronominal expressions
(Table 1). Definite anaphoric expressions were ig-
nored by almost all of the systems (except one sub-
mission).
The results show that the protein coreference res-
olution is not a trivial task; and many parts remains
challenging. In next section, we analyze about po-
tential reason of the low results, and discuss possible
directions for further improvement.
Ex 1 GOLD
T5 DQalpha and DQbeta trans heterodimeric
HLA-DQ molecules
T6 such trans-dimers
T7 which
R1 T6 T5 [T3, T4]
R2 T7 T6
RESP
T5 such trans-dimers
T6 which
R1 T6 T5
Ex 2 GOLD
T18 Five members of this family
(MYC, SCL, TAL-2, LYL-1 and E2A)
T20 their
R3 T20 T18 [T3, T2, T5, T4]
RESP
T19 Five members
T20 their
R2 T20 T19
Table 6: Example of surface-correct & protein-missing
cases. Protein names are underlined, and the min-values
are in italic.
8 Analysis
8.1 Why the rankings based on the two
evaluation methods are not the same?
Comparing with the protein coreference mode, we
can see the rankings based on two evaluation meth-
ods are different. In order to find out what led to
this interesting difference, we further analyzed the
submissions from the two teams UT and UU. The
UT team achieved the highest f-score in the surface
evaluation mode, but was in the fourth rank in the
protein evaluation mode. Meanwhile, the score of
UU team was slightly less than the UT team in the
former mode, but got the highest in the later (Table
3 and Table 4). In other words, there is no clear cor-
relation between the two evaluation results.
Because the two precisions in surface evaluation
mode are not much different, the recalls were the
main contribution in the difference of f-score. An-
alyzing the correct and missing examples in both
evaluation modes, we found that there are anaphors
whose surface links are correct, while the protein
links with the same anaphors are evaluated as miss-
ing; and vice versa with missing surface links and
correct protein links. Counts of anaphors of each
78
type are shown in Table 5. In this table, the cell
at column UT and row S-correct and P-missing can
be interpreted as following. There are 29 anaphors
in the UT response whose surface links are correct
but protein links are missing, which contributes pos-
itively to the recall in surface coreference mode, and
negatively to that in protein coreference mode.
Table 6 shows two examples of S-correct and
P-missing. In the first example, we can see that
the gold antecedent proteins are contained in an in-
direct antecedent. Therefore, when the interme-
diate antecedent is correctly detected by the sur-
face link R1, but the indirect antecedent is not de-
tected, the anaphor is not linked to it antecedent
proteins ?DQalpha? and ?DQbeta?. Another reason
is because response antecedents do not include an-
tecedent proteins. This is actually the problem of
expression boundary detection. An example of this
is example 2 (Table 6), in which the response sur-
face link R2 is correct, but the protein links to the
four proteins are not detected, because the response
antecedent ?five members? does not include the pro-
tein mentions ?SCL, TAL-2, LYL-1 and E2A?. How-
ever, the response antecedent expression is correct
because it contains the minimal boundary ?mem-
bers?.
For S-missing and P-correct, we found that
anaphors are normally directly linked to antecedent
proteins. In other words, expression boundary is
same as protein boundary. Another case is that re-
sponse antecedents contain the antecedent proteins,
but are evaluated as incorrect because the expres-
sion boundary of the response expression is larger
than the gold expression. An example is shown in
Table 7 where the response expression ?a second
GCR, termed GCRbeta? includes the gold expres-
sion ?GCRbeta?. Therefore, although the surface
link is incorrect because the response expression is
evaluated as incorrect, the protein coreference link
receives a full score .
The difference reflects the characteristics of the
two evaluation methods. The analysis result also
shows the affect of markable detection or expression
detection on the resolution evaluation result.
8.2 Protein coreference analysis
We want to see how well each system performs on
each type of anaphor. However, the type information
Ex 3 GOLD
T17 GCRbeta
T18 which
R2 T18 T17 [T4]
RESP
T16 a second GCR, termed GCRbeta
T19 which
R2 T19 T16
Table 7: Examples of S-missing and P-correct
is not explicitly included in the response, so it has
to be induced automatically. We done this by find-
ing the first word of anaphoric expression; then, we
combine it with 1 if the expression is a single-word
expression, or 2 if the expression is multi-word, to
create a sub type value for each anaphor of both
gold and response anaphors. After that, subtypes are
mapped with the anaphor types specified in Section
4 using the mapping in Table 10.
Protein coreference resolution results by sub type
are given in Table 9 and 8. It can be easily seen in
Table 9 which team performed well on which type
of anaphor. In particular, the CU system was good at
resolving the RELAT, APPOS and other types. The
UU team performed well on the DNP type. And for
the PRON type, UZ was the best team. In theory,
knowing this, we can combine strengths of the teams
to tackle all the types.
We analyzed false positive protein anaphora links
to see what types of anaphora are solved by each
system. The recalls in Table 11 are calculated based
on the anaphor type information manually annotated
in the gold data. Comparing with those in Table 9,
there is a small difference due to the automatic in-
duction of anaphoric types based on sub types. It
can be seen in the table 11 that only 77.5 percent of
RELAT-typed anaphora links were resolved (by CU
team), although this type is supposed to be the eas-
iest type. Examining the output data, we found that
the system tends to choose the nearest expression
as the antecedent of a relative pronoun; however,
this is not always correct, as in the following exam-
ples from the UofU submission: ?We also identified
functional Aiolos-binding sites1a in the Bcl-2 pro-
moter1b, which1 are able to activate the luciferase
reporter gene.?, and ?Furthermore, the analysis of
IkappaBalpha turnover demonstrated an increased
79
PRON P- P- P- P- P- P- DNP D- RELAT R-
both-2 it-1 its-1 one-2 that-1 their-1 these-2 this-2 those-1 which-1 whose-1 N/C
UU 36.4 64.4 2 13.3 18.2 62 5 30.8
UZ 46.2 35.7 53.3 7.1 12.5 5.4 59 66.7 15.4
CU 62 70.9 5 42.1
UT 9.5 36.8 10 34.6 9.5 5 30.8
US 13.9 22.9
UC 28.6 9.1
Table 8: Fine-grained results (f-score, %)
Team PRON P- P- DNP D- D- RELAT R- R- Others O- O-
P R F P R F P R F P R F
UU 79.0 11.5 20.1 66.7 5.9 10.8 71.3 56.0 62.7 100.0 18.3 30.8
UZ 62.9 16.9 26.7 12.5 4.4 6.5 71.4 46.7 56.5 50.0 9.1 15.4
CU 64.6 68.0 66.2 50.0 36.4 42.1
UT 72.7 12.3 21.1 14.3 1.5 2.7 73.3 29.3 41.9 100.0 18.2 30.8
US 27.3 6.9 11.0
UC 9.1 1.5 2.6
Table 9: Protein coreference results by coreference type (fscore, %). P = precision, R = recall, F = f-score. O = Others.
TEAM A R D P O
UU 0.0 62.0 5.7 11.1 0.0
UZ 0.0 49.3 4.3 17.0 0.0
CU 0.0 77.5 0.0 0.0 0.0
UT 0.0 32.4 1.4 11.9 14.3
US 0.0 0.0 0.0 6.7 0.0
UC 0.0 0.0 1.4 0.7 0.0
Table 11: Exact recalls by anaphor type, based on man-
ual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=OTHER
degradation of IkappaBalpha2a in HIV-1-infected
cells2b that2 may account for the constitutive DNA
binding activity.?. Expressions with the same index
are coreferential expressions. The a subscript indi-
cates correct antecedent, and b subscript indicates
the wrong one. In these examples, the relative pro-
noun that and which are incorrectly linked with the
nearest expression, which is actually part of post-
modifier or the correct antecedent expression.
For the DNP type, recall of the best system is less
than 6 percent (Table 11), although it is an impor-
tant type which occupies almost one fifth of all pro-
tein links (Table 1). There is only one team, the UC
team, attempted to tackle the anaphor; however, it
resulted in many spurious links. The other teams
did not make any prediction on this type. A possi-
ble reason of this is because there are much more
non-anaphoric definite noun phrases than anaphoric
ones, which making it difficult to train an effective
classier for anaphoricity determination. We have to
seek for a better method for solving the DNP links,
in order to significantly improve protein coreference
resolution system.
Concerning the PRON type, Table 8 shows that
except for that-1, no other figures are higher than
50 percent f-score. This is an interesting obser-
vation because pronominal anaphora problem has
been reported with much higher results on other
domains(Raghunathan et al, 2010), and also on
other bio data (hsiang Lin and Liang, 2004). One
of the reasons for the low recall is because target
anaphoric pronouns in the bio domain are neutral-
gender and third-person pronouns(Nguyen and Kim,
2008), which are difficult to resolve than other types
of pronouns(Stoyanov et al, 2010a).
8.3 Protein coreference analysis - Intermediate
antecedent
As mentioned in the task setting, anaphors can di-
rectly link to their antecedent, or indirectly link via
one or more intermediate antecedents. We counted
the numbers of correct direct and indirect protein
coreference links in each submission (Table 12).
80
Sub type Type Count Sub type Type Count Sub type Type Count
both 1 PRON 2 both 2 PRON 4 either 1 PRON 0
it 1 PRON 17 its 1 PRON 61 one 2 PRON 1
such 2 DNP 2 that 1 RELAT 37 the 2 DNP 20
their 1 PRON 27 them 1 PRON 1 these 1 PRON 1
these 2 DNP 26 they 1 PRON 5 this 1 PRON 1
this 2 DNP 20 those 1 PRON 9 which 1 RELAT 37
whose 1 RELAT 1 whose 2 RELAT 0 (others) N/C 11
Table 10: Mapping from sub type to coreference type. Count = number of anaphors
TEAM A R R D D P P O
Di Di In Di In Di In Di
UU 44 4 15
UZ 35 2 1 23
CU 54 1
UT 22 1 1 16 1
US 8 1
UC 1 1
Total 1 64 7 65 5 126 9 7
Table 12: Numbers of correct protein coreference links
by anaphor type and by number of antecedents, based on
manual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=Others. Di=direct, In=indirect.
APPOS and Others types do not have any intermedi-
ate antecedent, thus there is only one column marked
with D (direct protein coreference link). We can
see in this table that very few indirect links were
detected. Therefore, there is place to improve our
resolution system by focusing on detection of such
links.
8.4 Surface coreference results
Because inclusion of all expressions was not a re-
quirement of shared task submission, the submitted
results may not contain expressions that do not in-
volve in any coreference links. Therefore, it is un-
fair to evaluate expression detection based on the re-
sponse expressions.
Evaluation results for anaphoricity determination
are shown in Table 13. The calculation is performed
as following. Supposing that every anaphor has a
response link, the number of anaphors is number
of distinct anaphoric expressions inferred from the
response links, which is given in the first column.
The total number of gold anaphors are also calcu-
lated in similar way. Since response expressions
are lined with gold expressions before evaluation,
Team Resp Align P R F
UU 360 94.2 19.4 33.3 24.6
UZ 736 75.8 22.0 77.1 34.2
CU 365 89.6 15.3 26.7 19.5
UT 452 92.0 18.1 39.0 24.8
US 259 9.3 6.2 7.6 6.8
UC 797 6.8 1.1 4.3 1.8
Table 13: Anaphoricity determination results. Total num-
ber of gold anaphors = 210. Resp = number of response
anchors, Align = alignment rate(%), P = precision (%), R
= recall (%), F = f-score (%)
we provided the alignment rate for reference in the
second column of the table. The third and forth
columns show the precisions and recalls. In theory,
low anaphoricity determination precision results in
many spurious response links, while low recall be-
comes the bottle neck for the overall coreference
resolution recall. Therefore, we can conclude that
the low performance of anaphoricity determination
contribute to the low coreference evaluation results
(Table 4, Table 3).
9 Conclusion
The coreference resolution supporting task of
BioNLP Shared Task 2011 has drawn attention from
researchers of different interests. Although the over-
all results are not good enough to be helpful for the
main shared tasks as expected, the analysis results in
this paper shows the coreference types which have
and have not yet been successfully solved. Tack-
ling the remained problems in expression bound-
ary detection, anaphoricity determination and reso-
lution algorithms for difficult types of anaphors such
as definite noun phrases should be the future work.
Then, it would be interesting to see how much coref-
erence can contribute to event extraction.
81
References
B. Baldwin. 1997. Cogniac: High precision with limited
knowledge and linguistic resources. In Proceedings of
the ACL?97/EACL?97 Workshop on Operational Fac-
tors in Practical, Robust Anaphora Resolution, pages
38?45, Madrid, Spain.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguis-
tic features. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1412?1422, Uppsala, Sweden, July. Association
for Computational Linguistics.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Yu hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature.
In In Proceedings of ROCLING XVI: Conference on
Computational Linguistics and Speech Processing.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
ACL, pages 1396?1411.
Ngan Nguyen and Jin-Dong Kim. 2008. Exploring do-
main differences for the design of a pronoun resolution
system for biomedical texts. In Proceedings of 22nd
International Conference on Computational Linguis-
tics (COLING-2008).
T Ohta, Y Tateisi, H Mima, and J Tsujii. 2002. Ge-
nia corpus: an annotated research abstract corpus in
molecular biology domain. Proceedings of the Hu-
man Language Technology Conference (HLT 2002),
San Diego, California, pages 73?77.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501, October.
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010a. Coreference resolution with
reconcile. In Proceedings of the Conference of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2010).
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010b. Reconcile: A coreference res-
olution platform. In Tech Report - Cornell University.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the genia
corpus. In International Joint Conference on Natu-
ral Language Processing, pages 222?227, Jeju Island,
Korea, October.
82
Proceedings of BioNLP Shared Task 2011 Workshop, pages 83?88,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Entity Relations (REL) supporting task of
BioNLP Shared Task 2011
Sampo Pyysalo? Tomoko Ohta? Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Microsoft Research Asia, Beijing, China
{smp,okap}@is.s.u-tokyo.ac.jp, jtsujii@microsoft.com
Abstract
This paper presents the Entity Relations
(REL) task, a supporting task of the BioNLP
Shared Task 2011. The task concerns the ex-
traction of two types of part-of relations be-
tween a gene/protein and an associated en-
tity. Four teams submitted final results for the
REL task, with the highest-performing system
achieving 57.7% F-score. While experiments
suggest use of the data can help improve event
extraction performance, the task data has so
far received only limited use in support of
event extraction. The REL task continues as
an open challenge, with all resources available
from the shared task website.
1 Introduction
The BioNLP Shared Task 2011 (BioNLP ST?11)
(Kim et al, 2011a), the follow-up event to the
BioNLP?09 Shared Task (Kim et al, 2009), was
organized from August 2010 (sample data release)
to March 2011. The shared task was divided into
two stages, with supporting tasks carried out be-
fore the main tasks. The motivation for this task
setup drew in part from analysis of the results of the
previous shared task, which suggested that events
that involve coreference or entity relations repre-
sent particular challenges for extraction. To help ad-
dress these challenges and encourage modular ex-
traction approaches, increased sharing of successful
solutions, and an efficient division of labor, the two
were separated into independent supporting tasks on
Coreference (CO) (Nguyen et al, 2011) and Entity
Relations in BioNLP ST?11. This paper presents the
Entity Relations (REL) supporting task.
2 Task Setting
In the design of the REL task, we followed the gen-
eral policy of the shared task in assuming named
entity recognition (NER) as a given starting point:
participants were provided with manually annotated
gold standard annotations identifying gene/protein
names in all of the training, development, and final
test data. By limiting effects due to NER perfor-
mance, the task remains more specifically focused
on the key challenge studied.
Following the results and analysis from previous
studies (Pyysalo et al, 2009; Ohta et al, 2010), we
chose to limit the task specifically to relations in-
volving a gene/protein named entity (NE) and one
other entity. Fixing one entity involved in each re-
lation to an NE helps assure that the relations are
?anchored? to real-world entities, and the specific
choice of the gene/protein NE class further pro-
vides a category with several existing systems and
substantial ongoing efforts addressing the identifica-
tion of those referents through named entity recog-
nition and normalization (Leaman and Gonzalez,
2008; Hakenberg et al, 2008; Krallinger et al, 2008;
Morgan et al, 2008; Wermter et al, 2009). The
recognition of biologically relevant associations of
gene/protein NEs is a key focus of the main event
extraction tasks of the shared task. By contrast, in
the REL task setting, only one participant in each
binary relation is a gene/protein NE, while the other
can be either a non-name reference such as promoter
or the name of an entity not of the gene/protein type
(e.g. a complex).1 Motivated in part by the relatively
limited number of existing methods for the detec-
1Pronominal references are excluded from annotation scope.
83
Figure 1: Simple REL annotation example showing a
PROTEIN-COMPONENT (PR-CO) relation between ?hi-
stone H3? and ?lysine 9?. An associated METHYLATION
event and its arguments (shaded, not part of the REL task
targets) shown for context.
tion of such entity references, their detection is in-
cluded in the task: participants must recognize these
secondary entities in addition to extracting the rela-
tions they participate in. To limit the demands of this
NER-type task, these entities are not assigned spe-
cific types but rather the generic type ENTITY, and
exact matching of their boundaries is not required
(see Section 4).
The general task setting encompasses a rich set
of potential relation extraction targets. For the task,
we aimed to select relations that minimize overlap
between the targets of other tasks while maintain-
ing relevance as a supporting goal. As the main
tasks primarily target events (?things that happen?)
involving change in entities, we chose to focus in
the REL task on what we have previously termed
?static relations? (Pyysalo et al, 2009), that is, rela-
tions such as part-of that hold between entities with-
out necessary implication of causality or change. A
previous study by Van Landeghem et al (2010) in-
dicated that this class of relations may benefit event
extraction. We based our choice of specific target
relation on previous studies of entity relations do-
main texts (Pyysalo et al, 2009; Ohta et al, 2010),
which indicated that part-whole relations are by far
the most frequent class of relevant relations for the
task setting and proposed a classification of these
relations for biomedical entities. We further found
that ? in terms of the taxonomy of Winston et al
(1987) ? object-component and collection-member
relations account for the the great majority of part-
of relations relevant to the domain. For REL, we
chose to omit collection-member relations in part to
minimize overlap with the targets of the coreference
task. Instead, we focused on two specific types of
object-component relations, that holding between a
gene or protein and its part (domain, regions, pro-
moters, amino acids, etc.) and that between a protein
Item Training Devel Test
Abstract 800 150 260
Word 176,146 33,827 57,256
Protein 9,297 2,080 3,589
Relation 1,857 480 497
PROTEIN-COMPONENT 1,302 314 334
SUBUNIT-COMPLEX 555 166 163
Table 1: REL dataset statistics.
and a complex that it is a subunit of. Following the
biological motivation and the general practice in the
shared task to term genes and gene products PRO-
TEIN for simplicity, we named these two relations
PROTEIN-COMPONENT and SUBUNIT-COMPLEX.
Figure 1 shows an illustration of a simple relation
with an associated event (not part of REL). Events
with Site arguments such as that shown in the figure
are targeted in the GE, EPI, and ID tasks (Kim et al,
2011b; Ohta et al, 2011; Pyysalo et al, 2011) that
REL is intended to support.
3 Data
The task dataset consists of new annotations for
the GENIA corpus (Kim et al, 2008), building on
the existing biomedical term annotation (Ohta et
al., 2002), the gene and gene product name annota-
tion (Ohta et al, 2009) and the syntactic annotation
(Tateisi et al, 2005) of the corpus. The general fea-
tures of the annotation are presented by Pyysalo et
al. (2009), describing a previous release of a subset
of the data. The REL task annotation effort extended
the coverage of the previously released annotation to
all relations of the targeted types stated within sen-
tence scope in the GENIA corpus.
For compatibility with the BioNLP ST?09 and its
repeat as the GE task in 2011 (Kim et al, 2011b),
the REL task training/development/test set division
of the GENIA corpus abstracts matches that of the
BioNLP ST?09 data. The statistics of the corpus are
presented in Table 1. We note that both in terms of
training examples and the data available in the given
development set, the number of examples of the
PROTEIN-COMPONENT relation is more than twice
that for SUBUNIT-COMPLEX. Thus, at least for
methods based on machine learning, we might gen-
erally expect to find higher extraction performance
for the former relation.
84
NLP Extraction Other resources
Rank Team Org Word Parse Entities Relations Corpora Other
1 UTurku 1BI Porter McCCJ + SD SVM SVM - -
2 VIBGhent 1NLP, 1ML, 1BI Porter McCCJ + SD SVM SVM GENIA, PubMed word similarities
3 ConcordU 2NLP - McCCJ + SD Dict Rules - -
3 HCMUS 6L OpenNLP OpenNLP Dict Rules - -
Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural
Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,
McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary
UTurku VIBGhent ConcordU HCMUS
PROTEIN-COMPONENT 50.90 / 68.57 / 58.43 47.31 / 36.53 / 41.23 23.35 / 52.05 / 32.24 20.96 / 21.63 / 21.29
SUBUNIT-COMPLEX 48.47 / 66.95 / 56.23 47.85 / 38.12 / 42.43 26.38 / 39.81 / 31.73 4.91 / 66.67 / 9.14
Total 50.10 / 68.04 / 57.71 47.48 / 37.04 / 41.62 24.35 / 46.85 / 32.04 15.69 / 23.26 / 18.74
Table 3: Primary evaluation results for the REL task. Results given as recall / precision / F-score.
4 Evaluation
The evaluation of the REL task is relation-based and
uses the standard precision/recall/F1-score metrics.
Similarly to the BioNLP?09 ST and most of the 2011
main tasks, the REL task relaxes the equality criteria
for matching text-bound annotations: for a submis-
sion entity to match an entity in the gold reference
annotation, it is sufficient that the span of the sub-
mitted entity (i.e. its start and end positions in text)
is entirely contained within the span of the gold an-
notation. This corresponds largely to the approxi-
mate span matching criterion of the 2009 task (Kim
et al, 2009), although the REL criterion is slightly
stricter in not involving testing against an extension
of the gold entity span. Relation matching is exact:
for a submitted relation to match a gold one, both its
type and the related entities must match.
5 Results
5.1 Participation
Table 2 summarizes the participating groups and ap-
proaches. We find a remarkable number of sim-
ilarities between the approaches of the systems,
with all four utilizing full parsing and a depen-
dency representation of the syntactic analysis, and
the three highest-ranking further specifically the
phrase structure parser of Charniak and Johnson
(2005) with the biomedical domain model of Mc-
Closky (2009), converted into Stanford Dependency
form using the Stanford tools (de Marneffe et al,
2006). These specific choices may perhaps be influ-
enced by the success of systems building on them
in the 2009 shared task (e.g. Bjo?rne et al (2009)).
While UTurku (Bjo?rne and Salakoski, 2011) and
VIBGhent (Van Landeghem et al, 2011) further
agree in the choice of Support Vector Machines for
the recognition of entities and the extraction of rela-
tions, ConcordU (Kilicoglu and Bergler, 2011) and
HCMUS (Le Minh et al, 2011) pursue approaches
building on dictionary- and rule-based extraction.
Only the VIBGhent system makes use of resources
external to those provided for the task, extracting
specific semantic entity types from the GENIA cor-
pus as well as inducing word similarities from a
large unannotated corpus of PubMed abstracts.
5.2 Evaluation results
Table 3 shows the results of the REL task. We find
that the four systems diverge substantially in terms
of overall performance, with all pairs of systems
of neighboring ranks showing differences approach-
ing or exceeding 10% points in F-score. While
three of the systems notably favor precision over re-
call, VIBGhent shows a decided preference for re-
call, suggesting a different approach from UTurku in
design details despite the substantial similarities in
overall system architecture. The highest-performing
85
system, UTurku, shows an F-score in the general
range of state-of-the-art results in the main event
extraction task, which could be taken as an indica-
tion that the reliability of REL task analyses created
with presently available methods may not be high
enough for direct use as a building block for the
main tasks. However, the emphasis of the UTurku
system on precision is encouraging for such ap-
plications: nearly 70% of the entity-relation pairs
that the system predicts are correct. The two top-
ranking systems show similar precision and recall
results for the two relation types. The submission of
HCMUS shows a decided advantage for PROTEIN-
COMPONENT relation extraction as tentatively pre-
dicted from the relative numbers of training exam-
ples (Section 3 and Table 1), but their rule-based
approach suggests training data size is likely not
the decisive factor. While the limited amount of
data available prevents strong conclusions from be-
ing drawn, overall the lack of correlation between
training data size and extraction performance sug-
gests that performance may not be primarily limited
by the size of the available training data.
6 Discussion
The REL task was explicitly cast in a support role
for the main event extraction tasks, and REL par-
ticipants were encouraged to make their predictions
of the task extraction targets for the various main
task datasets available to main task participants. The
UTurku team responded to this call for supporting
analyses, running their top-ranking REL task sys-
tem on all main task datasets and making its output
available as a supporting resource (Stenetorp et al,
2011). In the main tasks, we are so far aware of
one application of this data: the BMI@ASU team
(Emadzadeh et al, 2011) applied the UTurku REL
predictions as part of their GE task system for re-
solving the Site arguments in events such as BIND-
ING and PHOSPHORYLATION (see Figure 1). While
more extensive use of the data would have been de-
sirable, we find this application of the REL analyses
very appropriate to our general design for the role of
the supporting and main tasks and hope to see other
groups pursue similar possibilities in future work.
7 Conclusions
We have presented the preparation, resources, re-
sults and analysis of the Entity Relations (REL) task,
a supporting task of the BioNLP Shared Task 2011
involving the recognition of two specific types of
part-of relations between genes/proteins and associ-
ated entities. The task was run in a separate early
stage in the overall shared task schedule to allow
participants to make use of methods and analyses for
the task as part of their main task submissions.
Of four teams submitting finals results, the
highest-performing system, UTurku, achieved a pre-
cision of 68% at 50% recall (58% F-score), a
promising level of performance given the relative
novelty of the specific extraction targets and the
short development period. Nevertheless, challenges
remain for achieving a level of reliability that would
allow event extraction systems to confidently build
on REL analyses to address the main information
extraction tasks. The REL task submissions, repre-
senting four independent perspectives into the task,
are a valuable resource for further study of both the
original task data as well as the relative strengths and
weaknesses of the participating systems. In future
work, we will analyse this data in detail to better
understand the challenges of the task and effective
approached for addressing them.
The UTurku team responded to a call for sup-
porting analyses by providing predictions from their
REL system for all BioNLP Shared Task main task
datasets. These analyses were adopted by at least
one main task participant as part of their system,
and we expect that this resource will continue to
serve to facilitate the study of the position of part-
of relations in domain event extraction. The REL
task will continue as an open shared challenge, with
all task data, evaluation software, and analysis tools
available to all interested parties from http://
sites.google.com/site/bionlpst/.
Acknowledgments
We would like to thank the UTurku team for their
generosity with their time and tools in providing
REL task analyses for all the BioNLP Shared Task
2011 main task datasets. This work was supported
by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan).
86
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder,
and G. Gonzalez. 2008. Inter-species normaliza-
tion of gene mentions with GNAT. Bioinformatics,
24(16):i126.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biological
event extraction. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for biomedical event anno-
tation. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
R. Leaman and G. Gonzalez. 2008. Banner: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomputing,
pages 652?663.
David McClosky. 2009. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
A.A. Morgan, Z. Lu, X. Wang, A.M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, et al 2008. Overview of BioCreative II gene
normalization. Genome biology, 9(Suppl 2):S3.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: An annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence (HLT?02), pages 73?77.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. A re-evaluation of biomedical
named entity-term relations. Journal of Bioinformat-
ics and Computational Biology (JBCB), 8(5):917?928.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
87
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9, Boulder, Colorado. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the GE-
NIA corpus. In Proceedings of IJCNLP?05, pages
222?227.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing, pages 144?152.
Sofie Van Landeghem, Thomas Abeel, Bernard De Baets,
and Yves Van de Peer. 2011. Detecting entity rela-
tions as a supporting task for bio-molecular event ex-
traction. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguis-
tics.
J. Wermter, K. Tomanek, and U. Hahn. 2009. High-
performance gene name normalization with GeNo.
Bioinformatics, 25(6):815.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11.
88
Proceedings of BioNLP Shared Task 2011 Workshop, pages 112?120,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011: Supporting Resources
Pontus Stenetorp: Goran Topic? Sampo Pyysalo
Tomoko Ohta Jin-Dong Kim; and Jun?ichi Tsujii$
Tsujii Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
:Aizawa Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
; Database Center for Life Science,
Research Organization of Information and Systems, Tokyo, Japan
$Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,goran,smp,okap}@is.s.u-tokyo.ac.jp
jdkim@dbcls.rois.ac.jp
jtsujii@microsoft.com
Abstract
This paper describes the supporting resources
provided for the BioNLP Shared Task 2011.
These resources were constructed with the
goal to alleviate some of the burden of sys-
tem development from the participants and al-
low them to focus on the novel aspects of con-
structing their event extraction systems. With
the availability of these resources we also seek
to enable the evaluation of the applicability of
specific tools and representations towards im-
proving the performance of event extraction
systems. Additionally we supplied evaluation
software and services and constructed a vi-
sualisation tool, stav, which visualises event
extraction results and annotations. These re-
sources helped the participants make sure that
their final submissions and research efforts
were on track during the development stages
and evaluate their progress throughout the du-
ration of the shared task. The visualisation
software was also employed to show the dif-
ferences between the gold annotations and
those of the submitted results, allowing the
participants to better understand the perfor-
mance of their system. The resources, evalu-
ation tools and visualisation tool are provided
freely for research purposes and can be found
at http://sites.google.com/site/bionlpst/
1 Introduction
For the BioNLP?09 Shared Task (Kim et al, 2009),
the first in the ongoing series, the organisers pro-
vided the participants with automatically generated
syntactic analyses for the sentences from the anno-
tated data. For evaluation purposes, tools were made
publicly available as both distributed software and
online services. These resources were well received.
A majority of the participants made use of one or
more of the syntactic analyses, which have remained
available after the shared task ended and have been
employed in at least two independent efforts study-
ing the contribution of different tools and forms of
syntactic representation to the domain of informa-
tion extraction (Miwa et al, 2010; Buyko and Hahn,
2010). The evaluation software for the BioNLP?09
Shared Task has also been widely adopted in subse-
quent studies (Miwa et al, 2010; Poon and Vander-
wende, 2010; Bjo?rne et al, 2010).
The reception and research contribution from pro-
viding these resources encouraged us to continue
providing similar resources for the BioNLP Shared
Task 2011 (Kim et al, 2011a). Along with the
parses we also encouraged the participants and ex-
ternal groups to process the data with any NLP (Nat-
ural Language Processing) tools of their choice and
make the results available to the participants.
We provided continuous verification and evalua-
tion of the participating systems using a suite of in-
house evaluation tools. Lastly, we provided a tool
for visualising the annotated data to enable the par-
ticipants to better grasp the results of their experi-
ments and to help gain a deeper understanding of
the underlying concepts and the annotated data. This
paper presents these supporting resources.
2 Data
This section introduces the data resources provided
by the organisers, participants and external groups
for the shared task.
112
Task Provider Tool
CO University of Utah Reconcile
CO University of Zu?rich UZCRS
CO University of Turku TEES
REL University of Turku TEES
Table 1: Supporting task analyses provided, TEES
is the Turku Event Extraction System and UZCRS
is the University of Zu?rich Coreference Resolution
System
2.1 Supporting task analyses
The shared task included three Supporting Tasks:
Coreference (CO) (Nguyen et al, 2011), Entity re-
lations (REL) (Pyysalo et al, 2011b) and Gene re-
naming (REN) (Jourde et al, 2011). In the shared
task schedule, the supporting tasks were carried out
before the main tasks (Kim et al, 2011b; Pyysalo
et al, 2011a; Ohta et al, 2011; Bossy et al, 2011)
in order to allow participants to make use of analy-
ses from the systems participating in the Supporting
Tasks for their main task event extraction systems.
Error analysis of BioNLP?09 shared task sub-
missions indicated that coreference was the most
frequent feature of events that could not be cor-
rectly extracted by any participating system. Fur-
ther, events involving statements of non-trivial rela-
tions between participating entities were a frequent
cause of extraction errors. Thus, the CO and REL
tasks were explicitly designed to support parts of
the main event extraction tasks where it had been
suggested that they could improve the system per-
formance.
Table 1 shows the supporting task analyses pro-
vided to the participants. For the main tasks, we
are currently aware of one group (Emadzadeh et al,
2011) that made use of the REL task analyses in their
system. However, while a number of systems in-
volved coreference resolution in some form, we are
not aware of any teams using the CO task analyses
specifically, perhaps due in part to the tight sched-
ule and the somewhat limited results of the CO task.
These data will remain available to allow future re-
search into the benefits of these resources for event
extraction.
2.2 Syntactic analyses
For syntactic analyses we provided parses for all
the task data in various formats from a wide range
of parsers (see Table 2). With the exception of
the Pro3Gres1 parser (Schneider et al, 2007), the
parsers were set up and run by the task organisers.
The emphasis was put on availability for research
purposes and variety of parsing models and frame-
works to allow evaluation of their applicability for
different tasks.
In part following up on the results of Miwa et al
(2010) and Buyko and Hahn (2010) regarding the
impact on performance of event extraction systems
depending on the dependency parse representation,
we aimed to provide several dependency parse for-
mats. Stanford Dependencies (SD) and Collapsed
Stanford Dependencies (SDC), as described by de
Marneffe et al (2006), were generated by convert-
ing Penn Treebank (PTB)-style (Marcus et al, 1993)
output using the Stanford CoreNLP Tools2 into the
two dependency formats. We also provided Confer-
ence on Computational Natural Language Learning
style dependency parses (CoNLL-X) (Buchholz and
Marsi, 2006) which were also converted from PTB-
style output, but for this we used the conversion
tool3 from Johansson and Nugues (2007). While
this conversion tool was not designed with convert-
ing the output from statistical parsers in mind (but
rather to convert between treebanks), it has previ-
ously been applied successfully for this task (Miyao
et al, 2008; Miwa et al, 2010).
The text from all documents provided were split
into sentences using the Genia Sentence Splitter4
(S?tre et al, 2007) and then postprocessed using a
set of heuristics to correct frequently occurring er-
rors. The sentences were then tokenised using a to-
kenisation script created by the organisers intended
to replicate the tokenisation of the Genia Tree Bank
(GTB) (Tateisi et al, 2005). This tokenised and
sentence-split data was then used as input for all
parsers.
We used two deep parsers that provide phrase
structure analysis enriched with deep sentence struc-
1https://files.ifi.uzh.ch/cl/gschneid/parser/
2http://nlp.stanford.edu/software/corenlp.shtml
3http://nlp.cs.lth.se/software/treebank converter/
4http://www-tsujii.is.s.u-tokyo.ac.jp/y-matsu/geniass/
113
Name Format(s) Model Availability BioNLP?09
Berkeley PTB, SD, SDC, CoNLL-X News Binary, Source No
C&C CCG, SD Biomedical Binary, Source Yes
Enju HPSG, PTB, SD, SDC, CoNLL-X Biomedical Binary No
GDep CoNLL-X Biomedical Binary, Source Yes
McCCJ PTB, SD, SDC, CoNLL-X Biomedical Source Yes
Pro3Gres Pro3Gres Combination ? No
Stanford PTB, SD, SDC, CoNLL-X Combination Binary, Source Yes
Table 2: Parsers, the formats for which their output was provided and which type of model that was used. The
availability column signifies public availability (without making an explicit request) for research purposes
tures, for example predicate-argument structure for
Head-Driven Phrase Structure Grammar (HPSG).
First we used the C&C Combinatory Categorial
Grammar (CCG) parser5 (C&C) by Clark and Cur-
ran (2004) using the biomedical model described in
Rimell and Clark (2009) which was trained on GTB.
Unlike all other parsers for which we supplied SD
and SDC dependency parses, the C&C output was
converted from its native format using a separate
conversion script provided by the C&C authors. Re-
grettably we were unable to provide CoNLL-X for-
mat output for this parser due to the lack of PTB-
style output. The other deep parser used was the
HPSG parser Enju6 by Miyao and Tsujii (2008), also
trained on GTB.
We also applied the frequently adopted Stanford
Parser7 (Klein and Manning, 2003) using a mixed
model which includes data from the biomedical do-
main, and the Charniak Johnson re-ranking parser8
(Charniak and Johnson, 2005) using the self-trained
biomedical model from McClosky (2009) (McCCJ).
For the BioNLP?09 shared task it was observed
that the Bikel parser9 (Bikel, 2004), which used a
non-biomedical model and can be argued that it uses
the somewhat dated Collins? parsing model (Collins,
1996), did not contribute towards event extraction
performance as strongly as other parses supplied for
the same data. We therefore wanted to supply a
parser that can compete with the ones above in a do-
main which is different from the biomedical domain
to see whether conclusions could be drawn as to the
5http://svn.ask.it.usyd.edu.au/trac/candc/
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
7http://nlp.stanford.edu/software/lex-parser.shtml
8ftp://ftp.cs.brown.edu/pub/nlparser/
9http://www.cis.upenn.edu/dbikel/software.html
importance of using a biomedical model. For this
we used the Berkeley parser10 (Petrov et al, 2006).
Lastly we used a native dependency parser, the GE-
NIA Dependency parser (GDep) by Sagae and Tsujii
(2007).
At least one team (Choudhury et al, 2011) per-
formed experiments on some of the provided lexi-
cal analyses and among the 14 submissions for the
EPI and ID tasks, 13 submissions utilised tools for
which resources were provided by the organisers of
the shared task. We intend to follow up on whether
or not the majority of the teams ran the tools them-
selves or used the provided analyses.
2.3 Other analyses
The call for analyses was open to all interested par-
ties and all forms of analysis. In addition to the Sup-
porting Task analyses (CO and REL) and syntactic
analyses provided by various groups, the University
of Antwerp CLiPS center (Morante et al, 2010) re-
sponded to the call providing negation/speculation
analyses in the BioScope corpus format (Szarvas et
al., 2008).
Although this resource was not utilised by the par-
ticipants for the main task, possibly due to a lack of
time, it is our hope that by keeping the data available
it can lead to further development of the participat-
ing systems and analysis of BioScope and BioNLP
ST-style hedging annotations.
3 Tools
This section presents the tools produced by the or-
ganisers for the purpose of the shared task.
10http://code.google.com/p/berkeleyparser/
114
1 10411007-E1 Regulation <Exp>regulate[26-34] <Theme>TNF-alpha[79-88] ?
?<Excerpt>[regulate] an enhancer activity in the third intron of [TNF-alpha]
2 10411007-E2 Gene_expression <Exp>activity[282-290] <Theme>TNF-alpha[252-261] ?
?<Excerpt>[TNF-alpha] gene displayed weak [activity]
3 10411007-E3 +Regulation <Exp>when[291-295] <Theme>E2 <Excerpt>[when]
Figure 1: Text output from the BioNLP?09 Shared Event Viewer with line numbering and newline markings
Figure 2: An illustration of collective (sentence 1)
and distributive reading (sentence 2). ?Theme? is
abbreviated as ?Th? and ?Protein? as ?Pro? when
there is a lack of space
3.1 Visualisation
The annotation data in the format specified by the
shared task is not intended to be human-readable ?
yet researchers need to be able to visualise the data
in order to understand the results of their experi-
ments. However, there is a scarcity of tools that can
be used for this purpose. There are three available
for event annotations in the BioNLP ST format that
we are aware of.
One is the BioNLP?09 Shared Task Event
Viewer11, a simple text-based annotation viewer: it
aggregates data from the annotations, and outputs it
in a format (Figure 1) that is meant to be further pro-
cessed by a utility such as grep.
Another is What?s Wrong with My NLP12, which
visualises relation annotations (see Figure 3a) ? but
is unable to display some of the information con-
tained in the Shared Task data. Notably, the distribu-
tive and collective readings of an event are not dis-
tinguished (Figure 2). It also displays all annotations
on a single line, which makes reading and analysing
longer sentences, let alne whole documents, some-
what difficult.
The last one is U-Compare13 (Kano et al, 2009),
11http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/
downloads.shtml
12http://code.google.com/p/whatswrong/
13http://u-compare.org/bionlp2009.html
which is a comprehensive suite of tools designed for
managing NLP workflows, integrating many avail-
able services. However, the annotation visualisation
component, illustrated in Figure 3b, is not optimised
for displaying complex event structures. Each anno-
tation is marked by underlining its text segment us-
ing a different colour per annotation type, and a role
in an event is represented by a similarly coloured arc
between the related underlined text segments. The
implementation leaves some things to be desired:
there is no detailed information added in the display
unless the user explicitly requests it, and then it is
displayed in a separate panel, away from the text it
annotates. The text spacing makes no allowance for
the annotations, with opaque lines crossing over it,
with the effect of making both the annotations and
the text hard to read if the annotations are above a
certain degree of complexity.
As a result of the difficulties of these existing
tools, in order to extract a piece of annotated text
and rework it into a graph that could be embedded
into a publication, users usually read off the annota-
tions, then create a graph from scratch using vector
drawing or image editing software.
To address these issues, we created a visualisa-
tion tool named stav (stav Text Annotation Visual-
izer), that can read the data formatted according to
the Shared Task specification and aims to present it
to the user in a form that can be grasped at a glance.
Events and entities are annotated immediately above
the text, and the roles within an event by labelled
arcs between them (Figure 3c). In a very complex
graph, users can highlight the object or association
of interest to follow it even more easily. Special fea-
tures of annotations, such as negation or speculation,
are shown by unique visual cues, and more in-depth,
technical information that is usually not required can
be requested by floating the mouse cursor over the
annotation (as seen in Figure 5).
We took care to minimise arc crossovers, and to
115
(a) Visualisation using What?s Wrong with My NLP
(b) Visualisation using U-Compare
(c) Visualisation using stav
Figure 3: Different visualisations of complex textual annotations of Dickensheets et al (1999)
116
Figure 4: A screenshot of the stav file-browser
keep them away from the text itself, in order to main-
tain text readability. The text is spaced to accommo-
date the annotations between the rows. While this
does end up using more screen real-estate, it keeps
the text legible, and annotations adjacent to the text.
The text is broken up into lines, and each sentence
is also forced into a new line, and given a numer-
ical identifier. The effect of this is that the text is
laid out vertically, like an article would be, but with
large spacing to accomodate the annotations. The
arcs are similarly continued on successive lines, and
can easily be traced ? even in case of them spanning
multiple lines, by the use of mouseover highlight-
ing. To preserve the distributionality information of
the annotation, any event annotations are duplicated
for each event, as demonstrated in the example in
Figure 2.
stav is not limited to the Shared Task datasets with
appropriate configuration settings, it could also vi-
sualise other kinds of relational annotations such as:
frame structures (Fillmore, 1976) and dependency
parses (de Marneffe et al, 2006).
To achieve our objectives above, we use the Dy-
namic Scalable Vector Graphics (SVG) functional-
ity (i.e. SVG manipulated by JavaScript) provided
by most modern browsers to render the WYSIWYG
(What You See Is What You Get) representation of
the annotated document. An added benefit from
this technique is that the installation process, if any,
is very simple: although not all browsers are cur-
rently supported, the two that we specifically tested
against are Safari14 and Google Chrome15; the for-
mer comes preinstalled with the Mac OS X oper-
ating system, while the latter can be installed even
by relatively non-technical users. The design is kept
modular using a dispatcher pattern, in order to al-
low the inclusion of the visualiser tool into other
JavaScript-based projects. The client-server archi-
tecture also allows centralisation of data, so that ev-
ery user can inspect an uploaded dataset without the
hassle of downloading and importing into a desktop
application, simply by opening an URL which can
uniquely identify a document, or even a single an-
notation. A screenshot of the stav file browser can
be seen in Figure 4.
3.2 Evaluation Tools
The tasks of BioNLP-ST 2011 exhibit very high
complexity, including multiple non-trivial subprob-
lems that are partially, but not entirely, independent
of each other. With such tasks, the evaluation of par-
ticipating systems itself becomes a major challenge.
Clearly defined evaluation criteria and their precise
implementation is critical not only for the compari-
son of submissions, but also to help participants fol-
low the status of their development and to identify
the specific strengths and weaknesses of their ap-
proach.
A further challenge arising from the complexity
of the tasks is the need to process the relatively in-
tricate format in which annotations are represented,
which in turn carries a risk of errors in submissions.
To reduce the risk of submissions being rejected or
the evaluation showing poor results due to format-
ting errors, tools for checking the validity of the file
format and annotation semantics are indispensable.
For these reasons, we placed emphasis in the or-
ganisation of the BioNLP-ST?11 on making tools for
format checking, validation and evaluation available
to the participants already during the early stages of
system development. The tools were made avail-
able in two ways: as downloads, and as online ser-
vices. With downloaded tools, participants can per-
form format checking and evaluation at any time
without online access, allowing more efficient op-
timisation processes. Each task in BioNLP-ST also
14http://www.apple.com/safari
15http://www.google.com/chrome
117
Figure 5: An example of a false negative illustrated by the evaluation tools in co-ordination with stav
maintained an online evaluation tool for the develop-
ment set during the development period. The online
evaluation is intended to provide an identical inter-
face and criteria for submitted data as the final on-
line submission system, allowing participants to be
better prepared for the final submission. With on-
line evaluation, the organisers could also monitor
submissions to ensure that there were no problems
in, for example, the evaluation software implemen-
tations.
The system logs of online evaluation systems
show that the majority of the participants submit-
ted at least one package with formatting errors, con-
firming the importance of tools for format checking.
Further, most of the participants made use of the on-
line development set evaluation at least once before
their final submission.
To enhance the evaluation tools we drew upon the
stav visualiser to provide a view of the submitted re-
sults. This was done by comparing the submitted
results and the gold data to produce a visualisation
where errors are highlighted, as illustrated in Fig-
ure 5. This experimental feature was available for
the EPI and ID tasks and we believe that by doing so
it enables participants to better understand the per-
formance of their system and work on remedies for
current shortcomings.
4 Discussion and Conclusions
Among the teams participating in the EPI and ID
tasks, a great majority utilised tools for which re-
sources were made available by the organisers. We
hope that the continued availability of the parses will
encourage further investigation into the applicability
of these and similar tools and representations.
As for the analysis of the supporting analyses pro-
vided by external groups and the participants, we are
so far aware of only limited use of these resources
among the participants, but the resources will re-
main available and we are looking forward to see
future work using them.
To enable reproducibility of our resources, we
provide a publicly accessible repository containing
the automated procedure and our processing scripts
used to produce the released data. This repository
also contains detailed instructions on the options and
versions used for each parser and, if the software li-
cense permits it, includes the source code or binary
that was used to produce the processed data. For the
cases where the license restricts redistribution, in-
structions and links are provided on how to obtain
the same version that was used. We propose that us-
ing a multitude of parses and formats can benefit not
just the task of event extraction but other NLP tasks
as well.
We have also made our evaluation tools and visu-
alisation tool stav available along with instructions
on how to run it and use it in coordination with the
shared task resources. The responses from the par-
ticipants in relation to the visualisation tool were
very positive, and we see this as encouragement to
advance the application of visualisation as a way to
better reach a wider understanding and unification
of the concept of events for biomedical event extrac-
tion.
All of the resources described in this paper are
available at http://sites.google.com/site/bionlpst/.
118
Acknowledgements
We would like to thank Jari Bjo?rne of the Uni-
versity of Turku BioNLP group; Gerold Schneider,
Fabio Rinaldi, Simon Clematide and Don Tuggener
of the Univerity of Zurich Computational Linguis-
tics group; Roser Morante of University of Antwerp
CLiPS center; and Youngjun Kim of the Univer-
sity of Utah Natural Language Processing Research
Group for their generosity with their time and exper-
tise in providing us with supporting analyses.
This work was supported by Grant-in-Aid for
Specially Promoted Research (MEXT, Japan) and
the Royal Swedish Academy of Sciences.
References
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Complex event extraction at
PubMed scale. Bioinformatics, 26(12):i382.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
E. Buyko and U. Hahn. 2010. Evaluating the impact
of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 982?992. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Pallavi Choudhury, Michael Gamon, Chris Quirk, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 103. Association for Com-
putational Linguistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
H.L. Dickensheets, C. Venkataraman, U. Schindler, and
R.P. Donnelly. 1999. Interferons inhibit activation of
STAT6 by interleukin 4 in human monocytes by in-
ducing SOCS-1 gene expression. Proceedings of the
National Academy of Sciences of the United States of
America, 96(19):10800.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. A generalizable and efficient ma-
chine learning approach for biological event extraction
from text. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy of
Sciences, 280(1):20?32.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics, 25(15):1997?1998, May.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
119
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
M.P Marcus, B. Santorini, and M.A Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Tree Bank. Computational Linguistics,
pages 313?318.
D. McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Ph. D. thesis, Department of Computer
Science, Brown University.
M. Miwa, S. Pyysalo, T. Hara, and J. Tsujii. 2010. Eval-
uating Dependency Representation for Event Extrac-
tion. In In the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 779?
787.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
H. Poon and L. Vanderwende. 2010. Joint inference
for knowledge extraction from biomedical literature.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 813?
821. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, 42(5):852 ? 865.
Biomedical Natural Language Processing.
R. S?tre, K. Yoshida, A. Yakushiji, Y. Miyao, Y. Matsub-
yashi, and T. Ohta. 2007. AKANE system: protein-
protein interaction pairs in BioCreAtIvE2 challenge,
PPI-IPS subtask. In Proceedings of the Second
BioCreative Challenge Workshop, pages 209?212.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL 2007 Shared
Task.
G. Schneider, M. Hess, and P. Merlo. 2007. Hybrid
long-distance functional dependency parsing. Unpub-
lished PhD thesis, Institute of Computational Linguis-
tics, University of Zurich.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scope in biomedical texts. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 38?45. Associa-
tion for Computational Linguistics.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax Annotation for the GENIA corpus. In Proceed-
ings of the IJCNLP, pages 222?227.
120
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 47?56, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Bridging the Gap Between Scope-based and Event-based
Negation/Speculation Annotations: A Bridge Not Too Far
Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3
Sophia Ananiadou2,3 and Jun?ichi Tsujii2,3,4
1Department of Computer Science, University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, United Kingdom
3National Centre for Text Mining, University of Manchester, Manchester, United Kingdom
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We study two approaches to the marking of
extra-propositional aspects of statements in
text: the task-independent cue-and-scope rep-
resentation considered in the CoNLL-2010
Shared Task, and the tagged-event representa-
tion applied in several recent event extraction
tasks. Building on shared task resources and
the analyses from state-of-the-art systems rep-
resenting the two broad lines of research, we
identify specific points of mismatch between
the two perspectives and propose ways of ad-
dressing them. We demonstrate the feasibility
of our approach by constructing a method that
uses cue-and-scope analyses together with a
small set of features motivated by data anal-
ysis to predict event negation and speculation.
Evaluation on BioNLP Shared Task 2011 data
indicates the method to outperform the nega-
tion/speculation components of state-of-the-
art event extraction systems.
The system and resources introduced in this
work are publicly available for research pur-
poses at: https://github.com/ninjin/eepura
1 Introduction
Understanding extra-propositional aspects of texts
is key to deeper understanding of statements con-
tained in natural language texts. Extra-propositional
aspects such as the polarity of key statements have
long been acknowledged to be critical for user-
facing applications such as information retrieval
(Friedman et al, 1994; Hersh, 1996). In recogni-
tion of this need, a number of recent information
extraction (IE) resources involving structured repre-
sentations of text statements have explicitly included
some marking of certainty and polarity (LDC, 2005;
Kim et al, 2009; Saur and Pustejovsky, 2009; Kim
et al, 2011a; Thompson et al, 2011).
Although extra-propositional aspects are recog-
nised as important, there is no clear consensus on
how to address their annotation and extraction from
text. Some comparatively early efforts focused on
the detection of negation cue phrases associated with
specific (previously detected) terms through regu-
lar expression-based rules (Chapman et al, 2001).
A number of later efforts identified the scope of
negation cues with phrases in constituency analy-
ses in sentence structure (Huang and Lowe, 2007).
Drawing in part on this work, the BioScope corpus
(Vincze et al, 2008) applied a representation where
both cues and their associated scopes are marked as
contiguous spans of text (Figure 1 bottom). This ap-
proach was also applied in the CoNLL-2010 Shared
Task (Farkas et al, 2010), in which 13 participat-
ing groups proposed approaches for Task 2, which
required the identification of uncertainty cues and
their associated scopes in text. In the following,
we will term this task-independent, linguistically-
motivated approach as the cue-and-scope represen-
tation (please see Vincze et al (2008) for details re-
garding the representation).
For IE efforts, more task-oriented representations
are commonly applied. In an effort to formalise
and drive research for extracting structured repre-
sentations of statements regarding molecular biol-
ogy, the ongoing series of BioNLP shared tasks
have addressed biomedical Event Extraction (EE)
(Kim et al, 2009; Kim et al, 2011a). The extra-
propositional targets of negation and speculation
47
Figure 1: Example illustrating cue-and-scope and
event-based negation marking. ?Crossing-out?
marks events as negated. PRO, TH and NEG are ab-
breviations for PROTEIN, THEME and NEGATION,
respectively.
of extracted events were already included in the
first task in the series, using a representation where
events can be assigned ?flags? to mark them as being
negated, speculated, or both (Figure 1 upper). Due
to space limitations we refer the reader to Kim et al
(2009) for a detailed explanation of the representa-
tion; similar representations have been applied also
in previous event extraction tasks (LDC, 2005).
There are a number of ways in which task-
oriented, event-based approaches could benefit from
the existing linguistically-oriented cue-and-scope
methods for identifying extra-propositional aspects
of text statements. However, there has been sur-
prisingly little work exploring the combination of
the approaches, and comparatively few methods ad-
dressing the latter task in detail. Only three out
of the 24 participants in the BioNLP Shared Task
2009 submitted results for the non-mandatory nega-
tion/speculation task, and although negation and
speculation were also considered in three main tasks
for the 2011 follow-up event (Kim et al, 2011a),
the trend continued, with only two participants ad-
dressing the negation/speculation aspects of the task.
We are aware of only two studies exploring the rela-
tionship between the cue-and-scope and event-based
representations: in a manual analysis of scope over-
lap with tagged events, Vincze et al (2011) identi-
fied a number of issues and mismatches in annota-
tion scope and criteria, which may explain in part
the lack of methods combining these two lines of
research. Kilicoglu and Bergler (2010) approached
the problem from the opposite direction and used an
existing EE system to extract cue-and-scope annota-
tions in the CoNLL-2010 Shared Task.
In this work, we take a high-level perspective,
seeking to bridge the linguistically oriented frame-
work and the more application-oriented event frame-
work to overcome the mismatches demonstrated
by Vincze et al (2011). Specifically, we aim to
determine how cue-and-scope recognition systems
can be used to produce a state-of-the-art nega-
tion/speculation detection system for the EE task.
2 Resources
Several existing resources can support the investiga-
tion of the relationship between the linguistically-
oriented and task-oriented perspectives on nega-
tion/speculation detection. In this study, we make
use of the following resources.
First, we study the three BioNLP 2011 Shared
Task corpora that include annotation for negation
and speculation: the GE, EPI and ID main task cor-
pora (Table 1). Second, we make use of support-
ing analyses provided for these corpora in response
to a call sent by the BioNLP Shared Task organis-
ers to the developers of third-party systems (Stene-
torp et al, 2011). Specifically, we use the output
of the BiographTA NeSp Scope Labeler (here re-
ferred to as CLiPS-NESP) (Morante and Daelemans,
2009; Morante et al, 2010) provided by the Univer-
sity of Antwerp CLiPS center. This system provides
cue-and-scope analyses for negation and speculation
and was demonstrated to have state-of-the-art per-
formance at the relevant CoNLL-2010 Shared Task.
Finally, we make use of the event analyses created
by systems that participated in the BioNLP Shared
Task, made available to the research community for
the majority of the shared task submissions (Pyysalo
et al, 2012). These analyses represent the state-
of-the-art in event extraction and their capability to
detect event structures as well as marking them for
negation and speculation.
The above three resources present us with many
opportunities to relate scope-based annotations to
three highly relevant event-based corpora containing
negation/speculation annotations.
3 Manual Analysis
To gain deeper insight into the data and the chal-
lenges in combining the cue-and-scope and event-
oriented perspectives, we performed a manual anal-
ysis of the corpus annotations using the manually
48
Name Negated Events Speculated Events Negated Spans Speculated Spans Publication
EPI 103 (5.6%) 70 (3.8%) 561 1,032 Ohta et al (2011)
GE 759 (7.4%) 623 (6.0%) 1,308 1,968 Kim et al (2011b)
ID 69 (3.3%) 26 (1.2%) 415 817 Pyysalo et al (2011)
Table 1: Corpora used for our experiments along with annotation statistics for their respective training sets.
The parenthesised values are the relative proportion of negated/speculated event annotations.
Occ. (Ratio) EPI ID
Covered 26 (15.03%) 52 (56.52%)
Not-covered 135 (78.03%) 38 (41.30%)
Error-in-gold 12 (6.94%) 2 (2.18%)
Morphological 48 (27.75%) 11 (11.96%)
Hypothesis 44 (25.43%) 15 (16.30%)
Ellipsis 5 (2.89%) 0 (0.00%)
Argument-only 2 (1.16%) 10 (10.87%)
Table 2: Results from the Manual Data Analysis of
the EPI and ID test sets.
created BioNLP Shared Task training data event an-
notations, and the automatic annotations created for
this data by the CLiPS-NESP system. The test
data was held out and was not directly examined
at any point of our study. We performed the anal-
ysis specifically on the EPI and ID corpora, as the
GE corpus training set texts overlap with the train-
ing data for the CLiPS-NESP system (BioScope cor-
pus), and results on this data would thus not reflect
the performance of the system on unseen data, and
a comparison of the GE and BioScope gold anno-
tations was previously performed by Vincze et al
(2011).
The analysis was performed by an experienced
annotator with a doctoral degree in a related field
in biology, who individually examined each of the
events marked as negated and speculated in the
EPI and ID training corpora. For the analysis,
the CLiPS-NESP system output was super-imposed
onto the BioNLP Shared Task event annotations.
The annotator was asked to assign three primary
flags for each event that was marked as negated or
speculated: Covered if the event trigger was covered
by span(s) of the correct type with a correct cue in
the cue-and-span analysis, Not-covered if not Cov-
ered, and Error-in-gold if the negation/speculation
flag on the event annotation was itself incorrect. We
also identified a number of additional properties that
initial analysis suggested to frequently characterise
instances where the coverage of the cue-and-scope
system is lacking: Morphological was assigned if
the negation/speculation of an event could be in-
ferred only from the morphology of the word ex-
pressing the event, rather than from cue words in its
context (e.g. unphosphorylated, non-glycosylated);
Hypothesis for cases where speculation is marked
for events stated as hyphotheses1 under consider-
ation, e.g. ?We analysed the methylation status of
MGMT?; Ellipsis for cases where the modified ex-
pression is elided (e.g. ?A was phosphorylated but B
was not?); and Argument-only if the CLiPS-NESP
output had marked the argument of an event as
negated rather than the event trigger (we use argu-
ment in the sense it is used in the BioNLP Shared
Tasks, for example, in Figure 1 upper, the two argu-
ments of the event are ?fMimR? and ?fimA?).
The results of the analysis are summarised in Ta-
ble 2. We find that that the system shows a clear dif-
ference in coverage depending on the dataset. For
the ID dataset, a majority of the annotations are cov-
ered by the appropriate spans, while only a small mi-
nority are covered for EPI. Instead, the EPI dataset
contains a significant portion of events where extra-
propositional aspects can only be distinguished by
the morphology of the word expressing the event
(all Morphological cases were negation) as well as
events marked as speculated due to being expressed
as hypotheses under study.
The analysis thus identified specific ways in
which the applicability of negation-detection sys-
tems using a span-and-scope representation could be
improved for some tasks.
1While it is arguable whether such cases represent specula-
tion (Vincze et al, 2008), separation from affirmatively made
claims is clearly motivated for many applications.
49
Event-based
Scope-based
Negation/speculationdetection
Eventextraction
Oursystem
Figure 2: An illustration of our approach.
4 Methods
We next introduce the methods we apply for as-
signing negation and speculation flags to extracted
events.
4.1 Approach
To focus on the extra-propositional aspects of event
extraction, we only consider the assignment of the
negation and speculation flags, not the extraction of
the event structures that these mark. To our knowl-
edge, no previous work studying this subtask in iso-
lation from event extraction exists. Thus, in order to
be able to relate the performance of the methods we
consider to the performance of previously proposed
approaches, it is necessary to base the negation and
speculation detection on an event extraction analy-
sis. For this reason, we construct our methods us-
ing system outputs for systems participating in the
BioNLP Shared Task 2011, in effect creating a nega-
tion/speculation processing stage for a pipeline sys-
tem where the previous stage is the completion of
event analysis without negation/speculation detec-
tion (Figure 2).
Our methods thus take extracted events as input
and attempt to enrich the output with negation and
speculation annotations. This enables us to produce
a general system with the potential to be applied
together with any existing event extraction system.
Additionally, this allows us to directly compare our
system output with that of the negation/speculation
components of previously proposed monolithic sys-
tems by removing the existing negation and spec-
ulation output from submissions including this and
recreating these annotations using our methods.
4.2 Rule-based Methods
The most straightforward way of carrying over in-
formation from scope-based to event-based annota-
tions is to consider any event structure for which the
word or words stating the event (i.e. the event trig-
ger) is within the scope of a negation or speculation
be negated or speculated (respectively). We imple-
mented this simple heuristic as our initial rule-based
method.
One relatively common category of cases where
this heuristic fails that was identified in analysis re-
lates to events that take other events as arguments.
Consider, for example, the case illustrated in Fig-
ure 3. The speculation span is correctly identified as
covering the statement ?FimR modulates mfa1 ex-
pression?, and the event expressed through ?mod-
ulates? is identified as speculated. However, the
nested event, the expression of mfa1, is not spec-
ulated. To cover this case, we implemented what
we refer to as the root-heuristic, which prevents the
propagation of negation/speculation marking from
scopes to events that are the arguments of another
event contained in the same scope. The second rule-
based method we consider incorporates this addi-
tional heuristic.
Preliminary development set experiments indi-
cated that while the root-heuristic could improve
precision, the performance of the rule-based meth-
ods remained poor, in particular on the EPI dataset.
The results of the manual analysis (Section 3) sug-
gested this to trace in particular to two main issues,
namely differences between annotation criteria be-
tween BioScope and the shared task data (as noted
also by Vincze et al (2011)) and events which are
negated not by external cues but by morphological
alternations of the event trigger, such as ?unphos-
phorylated? expressing the absence of phosphory-
lation. As it would have been difficult to system-
atically incorporate both morphology and context
into the rule-based method without compromising
the generality of the approach, we opted to move to a
machine learning framework for further method de-
velopment. This allows us to continue to make use
of the existing cue-and-scope annotations while ex-
ploring the effects of other aspects of the text and
maintaining generality through retraining.
4.3 Machine Learning-based Methods
In developing a machine learning-based approach to
the negation/speculation task, we aimed to identify
and evaluate a minimal set of features directly mo-
50
Feature Example Value(s)
Heuristic ROOT/NON-ROOT
Heuristic-Cue possibility
Heuristic-Span One, possibility, . . .
Trigger-Text non-phosphorylated
Trigger-Prefixes no, non, non-, . . .
Trigger-Preceding-Context is, that, . . .
Trigger-Proceeding-Context mfa1, expression, . . .
Table 3: Machine learning features. The fea-
tures are categorised into three groups: features
based on cue-and-scope based heuristics (top), non-
contextual features derived from the event trigger
(middle), and features derived from the context of
the event trigger (bottom). These three feature sets
are abbreviated as E, M and C, respectively.
Figure 3: Example of a speculation span containing
two events, of which only one is speculated (marked
by a dashed border).
tivated by the analysis of the data and to use the
cue-and-scope analyses as much as possible. In par-
ticular, we wanted to avoid features requiring com-
putationally expensive analyses such as full pars-
ing or replicating the type of analyses performed by
the CLiPS-NESP system, focusing rather on specific
points where its output does not meet the needs of
the event-based approach.
We introduced features representing the heuristics
described in Section 4.2, marking each case as be-
ing either a root or non-root event in its scope (if
any). Drawing further on the cue-and-scope analy-
sis, we included as features the cue word and bag-of-
words features for all tokens in the scope (using sim-
ple white-space tokenisation). To address the issues
identified in manual analysis, we introduced features
for the event trigger text as well as character-based
prefixes of lengths 2 to 7 of the, intended primarily
to capture morphological negation.
All features presented above are derived only
from those parts of the sentence already marked ei-
ther by the event extraction or the cue-and-scope
system. However, due to the differences in anno-
tation guidelines for speculation annotations, we ex-
pect that the scope-based system will fail to mark
a significant portion of the speculation annotations.
To allow the system to learn to detect these, we in-
troduce a minimal set of contextual features, limited
to a bag-of-words representation of the three words
preceding and following the event trigger.
5 Experiments
We perform two sets of experiments, the first to eval-
uate our approach on gold annotations to give a fair
upper-limit to how well our negation/speculation de-
tection system could perform under ideal settings,
and the second to enrich the output of an event ex-
traction system with negation and speculation an-
notations, to evaluate real-world performance and
to allow direct comparison of our methods with
those incorporated in monolithic event extraction
and negation/speculation detection systems.
5.1 Corpora
For our experiments we used the GE, EPI and ID
corpora of the BioNLP Shared Task 2011 (Table 1).
We note that while the GE training set texts overlap
with the BioScope corpus used to train the CLiPS-
NESP system, the GE test set does not, and thus test
set results are not expected to be overfit.
We noted when performing development set
experiments that training machine learning-based
methods on the negation/speculation annotations of
the event-annotated corpora was problematic due to
the sparseness of these flags in the annotation. To
address this issue, we merge the training data of the
three corpora in all experiments with machine learn-
ing methods.
5.2 Baseline methods
We use the event analyses created by the UTurku
(Bjo?rne and Salakoski, 2011) and UConcordia (Kil-
icoglu and Bergler, 2011) systems for the BioNLP
2011, the only systems that included negation and
speculation analyses. To investigate the impact on a
system that did not include a negation/speculation
component, we further consider analyses created
51
Negation (R/P/F) EPI GE ID
H 29.23/31.67/30.40 53.92/52.84/53.38 44.00/31.88/36.97
HR 27.69/32.73/30.00 53.24/71.89/61.18 44.00/37.93/40.74
M 47.69/20.00/28.18 43.00/25.25/31.82 46.00/26.74/33.82
ME 60.00/66.10/62.90 58.36/70.08/63.69 54.00/69.23/60.67
MC 40.00/74.29/52.00 58.36/76.34/66.15 52.00/61.90/56.52
MCE 58.46/73.08/64.96 61.77/83.03/70.84 58.00/70.73/63.74
Table 4: Results for Negation for our two heuristics and the four combinations of machine learning features.
Speculation (R/P/F) EPI GE ID
H 13.46/6.48/8.75 33.77/18.12/23.58 54.17/6.50/11.61
HR 11.54/5.66/7.59 32.79/29.45/31.03 54.17/7.98/13.90
M 1.92/0.62/0.93 25.65/10.84/15.24 45.83/10.58/17.19
ME 3.85/12.50/5.88 22.08/42.24/29.00 29.17/28.00/28.57
MC 51.92/52.94/52.43 27.27/50.30/35.37 37.50/31.03/33.96
MCE 48.08/51.02/49.50 31.82/53.85/40.00 33.33/42.11/37.21
Table 5: Results for Speculation for our two heuristics and the four combinations of ML features.
by the FAUST system, which achieved the high-
est performance at two of the three tasks consid-
ered (Riedel et al, 2011). The UTurku system is
a pipeline ML-based EE system, while the UCon-
cordia system is strictly rule-based. FAUST is an
ML-based model combination system incorporating
information from the parser-based Stanford system
(McClosky et al, 2011) and the jointly-modelled
UMass system (Riedel and McCallum, 2011).
We also performed preliminary experiments for
the other released submissions to the BioNLP 2011
Shared Task, but due to space limitations focus only
on the three above-mentioned systems.
5.3 Evaluation criteria
We use the primary evaluation criteria of the
BioNLP 2011 Shared Task (Kim et al, 2011a) to
assure comparability, reporting all results using the
standard precision, recall and their harmonic mean
(F-score).
5.4 Methods
We apply the rule-based simple heuristic method
and its root extension (Section 4.2) as well as Sup-
port Vector Machines (SVM) trained with the fea-
tures introduced in Section 4.3. For the SVM, we
separately evaluate models based on all permuta-
tions of the feature sets introduced in Table 3. In the
results tables we abbreviate the feature set names as
done in Table 3 and use H for the heuristic method
and R for its root extension. As our machine learn-
ing component we use LIBLINEAR (Fan et al,
2008) with a L2-regularised L2-loss SVM model.
We optimise the SVM regularisation parameter C
using 10-fold cross-validation on the training data.
We use the training, development and test set par-
tition provided by the shared task organisers. In line
with standard ML methodology the test set was held
out during development and was only used when
carrying out the final experiments prior to submit-
ting the manuscript.
6 Results and Discussion
Our initial experiments, building on gold event data
(Tables 4 and 5), support our manual analysis, show-
ing nearly uniform performance improvement with
additional features. First, we find that the root-
heuristic gives an improvement over the original
heuristic in four out of six cases. To justify our us-
age of the cue-and-scope based heuristic feature (E)
we find that adding it as a feature improves on the M
feature set and the MC feature set, showing that even
given context, the cue-and-scope perspective is still
useful. The only anomaly is for speculation on the
EPI dataset, where adding this heuristic feature ac-
tually hampers performance, possibly relating to the
52
Negation (R/P/F) EPI GE ID
UConcordia 16.92/61.11/26.51 18.43/43.44/25.88 22.00/23.91/22.92
UConcordia* 20.00/70.59/31.17 20.14/42.96/27.42 28.00/31.58/29.68
UTurku 12.31/38.10/18.60 22.87/48.85/31.15 26.00/44.83/32.91
UTurku* 43.08/48.28/45.53 21.16/38.56/27.33 26.00/41.94/32.10
FAUST* 29.23/59.38/39.18 21.50/41.18/28.25 28.00/46.67/35.00
Table 6: Results of the Negation enrichment experiment.
Speculation (R/P/F) EPI GE ID
UConcordia 5.77/8.33/6.82 21.10/38.46/27.25 8.33/2.00/3.23
UConcordia* 1.92/4.55/2.70 12.99/29.20/17.98 8.33/2.22/3.51
UTurku 30.77/48.48/37.65 17.86/32.54/23.06 12.50/18.75/15.00
UTurku* 46.15/47.06/46.60 11.04/26.56/15.60 8.33/3.33/4.76
FAUST* 36.54/48.72/41.76 10.39/26.50/14.93 12.50/12.50/12.50
Table 7: Results of the Speculation enrichment experiment.
(R/P/F) EPI ID
UConcordia 20.83/42.14/27.88 49.00/40.27/44.21
UConcordia* 20.83/42.94/28.05 49.20/41.78/45.19
UTurku 52.69/53.98/53.33 37.85/48.62/42.57
UTurku* 54.72/53.86/54.29 37.79/47.76/42.19
FAUST 28.88/44.51/35.03 48.03/65.97/55.59
FAUST* 31.64/45.17/37.21 49.20/64.66/55.88
Table 8: Overall scores for the EPI and ID data sets.
sparseness of useful annotations due to the differing
annotation guidelines, as noted in manual analysis.
The numbers from these initial experiments serve as
an upper bound when we proceed to our enrichment
experiments, as they do not suffer from the possibil-
ity of producing false positives negation/speculation
annotations for false positive event structures.
In addition to the above in preliminary experi-
ments we also considered two features inspired by
findings made by Vincze et al (2011). A distance-
based feature, measuring the distance in tokens be-
tween the cue-word and the event trigger, and also
trigger suffixes to capture some cases of morpholog-
ical speculation (?induced? vs. ?inducible?). How-
ever, we failed to establish any consistent benefits
from these features and only for the EPI dataset did
the suffix features improve performance.
For the enrichment evaluation, adding nega-
F EPI GE ID
UConcordia 57.43 60.68 67.28
UTurku 81.31 66.27 55.84
FAUST 74.91 66.14 67.13
Table 9: Estimated F-score upper-bound for an ora-
cle system precision assigning negation/speculation
annotations to events predicted by an up-stream EE
system.
tion/speculation flags to the output of event extrac-
tion systems (Tables 6 and 7), our results are some-
what more modest. For negation we see an improve-
ment in four out of six cases, and for speculation in
two out of six. Despite the fact that a major limi-
tation to our approach are the false positive events
that are propagated from the original EE system, we
manage to improve the global score for all data sets
where a global score is provided by the organisers
(Table 8). We improve a full point in F-score for
UTurku on EPI, but only sub-percentage for Faust
on ID, the latter most likely since ID contains fewer
negation and speculation annotations and the global
scores are microaverages over all annotations.
As a final analysis we estimate the upper-bound
in F-score performance for all three EE systems
(Table 9). We do so by assuming that the recall
for events marked by negation and speculation is
53
equal to that of the overall recall of the up-stream
EE system and that negation/speculation annotations
assigned by an oracle. What we can see is that
there is still room for improvement, both for our
enrichment approach and for the EE system?s inter-
nal negation/speculation components, although re-
call of the EE output is a limiting factor we can
expect further efforts towards improving the extra-
propositional aspects of the system to yield perfor-
mance improvements.
7 Conclusions and Future Work
In this study, we have considered two broad lines
of research on extra-propositional aspects of key
statements in text, one using the task-independent,
linguistically-motivated cue-and-scope representa-
tion applied in the recent CoNLL-2010 Shared Task,
and the other using the task-oriented flagged-event
representation applied e.g. in the ACE and BioNLP
Shared Task evaluations. We presented a detailed
manual analysis exploring points of disagreement
and evaluated in detail rule-based and machine
learning-based methods joining state-of-the-art sys-
tems representing the two approaches.
Our manual analysis identified a number of phe-
nomena that limit the applicability of existing cue-
and-scope based systems to the event extraction
task, such as negation expressed through morpho-
logical change of words expressing events (e.g. un-
phosphorylated). To address these issues, we pro-
posed a combination of heuristics and simple lexical
features, carefully selected to address differences in
perspective between the cue-and-scope and event-
based frameworks and aiming to complement cue-
and-scope analyses for creating task-oriented out-
puts.
To test our approach, we created a method suit-
able for use as a component of an event extraction
pipeline that incorporates information from a previ-
ously proposed state-of-the-art cue-and-scope based
negation/speculation detection system and a mini-
mal set of features in an SVM-based system that was
shown to enhance and in several cases improve upon
the output of existing EE systems. Experiments on
the BioNLP Shared Task 2011 EPI and ID datasets
demonstrated that the combined approach could im-
prove the results of the best-performing systems at
the original task in 5 out of 6 cases, outperforming
the highest results reported for any system for these
two tasks.
There exist several potential targets for future
work on improving our introduced system and
to join cue-and-scope and event-based approaches.
Since none of the existing EE corpora was con-
structed with the aim to solely cover negation and
speculation annotations and taking into account our
finding that merging datasets to compensate for data
sparseness is beneficial, it might be worth consid-
ering other possible corpora or resources and how
they can be used for training our machine learning
system.
Also, it would be worthwhile to attempt to com-
bine an existing EE system capable of detect-
ing negation/speculation with our proposed method.
Combining the two could yield an ensemble, im-
proving upon an already strong system by bridging
the differences in perspectives and tapping into the
potential benefits of both approaches.
The system and all resources introduced in this
work are publicly available for research purposes at:
https://github.com/ninjin/eepura
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their many insightful comments and sug-
gestions for improvements.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 183?191.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of biomedi-
cal informatics, 34(5):301?310.
54
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12.
Carol Friedman, Philip O. Alderson, John H.M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
general natural-language text processor for clinical ra-
diology. Journal of the American Medical Informatics
Association, 1(2):161?174.
William R. Hersh. 1996. Information retrieval: a health
care perspective. Springer.
Yuang Huang and Henry J. Lowe. 2007. A novel hybrid
approach to automated negation detection in clinical
radiology reports. Journal of the American Medical
Informatics Association, 14(3):304?311.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and their
Scopes. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
70?77.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
pages 173?182.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of the BioNLP 2011 Workshop Companion Vol-
ume for Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 7?15.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical re-
port, Linguistic Data Consortium.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of BioNLP 2011,
pages 41?45.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 28?
36.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scopes of hedge cues. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning ? Shared Task, CoNLL 2010: Shared
Task, pages 40?47.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 16?25.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 26?35.
Sampo Pyysalo, Pontus Stenetorp, Tomoka Ohta, Jin-
Dong Kim, and Sophia Ananiadou. 2012. New Re-
sources and Perspectives for Biomedical Event Extrac-
tion. In Proceedings of BioNLP 2012 Workshop. to
appear.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 46?50.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 51?55.
Roser Saur and James Pustejovsky. 2009. Fact-
Bank: a corpus annotated with event factuality.
Language Resources and Evaluation, 43:227?268.
10.1007/s10579-009-9089-9.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, pages 112?120.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedical
event corpus with meta-knowledge annotation. BMC
Bioinformatics, 12(1):393.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
55
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
Veronika Vincze, Gyorgy Szarvas, Gyorgy Mora,
Tomoko Ohta, and Richard Farkas. 2011. Linguis-
tic scope-based and biological event-based specula-
tion and negation annotations in the BioScope and Ge-
nia Event corpora. Journal of Biomedical Semantics,
2(Suppl 5):S8.
56
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 27?36,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Open-domain Anatomical Entity Mention Detection
Tomoko Ohta 1 Sampo Pyysalo 1 Jun?ichi Tsujii 2 Sophia Ananiadou 1
1National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK
2Microsoft Research Asia, Beijing, China
okap.tiffany@gmail.com, sampo.pyysalo@gmail.com
jtsujii@microsoft.com, sophia.ananiadou@manchester.ac.uk
Abstract
Anatomical entities such as kidney, muscle
and blood are central to much of biomedical
scientific discourse, and the detection of men-
tions of anatomical entities is thus necessary
for the automatic analysis of the structure of
domain texts. Although a number of resources
and methods addressing aspects of the task
have been introduced, there have so far been
no annotated corpora for training and evaluat-
ing systems for broad-coverage, open-domain
anatomical entity mention detection. We in-
troduce the AnEM corpus, a domain- and
species-independent resource manually anno-
tated for anatomical entity mentions using a
fine-grained classification system. The cor-
pus texts are selected randomly from citation
abstracts and full-text papers with the aim of
making the corpus representative of the en-
tire available biomedical scientific literature.
We demonstrate the use of the corpus through
an evaluation of the broad-coverage MetaMap
tagger and a CRF-based system trained on the
corpus data, considering also a combination
of these two methods. The combined sys-
tem demonstrates a promising level of per-
formance, approaching 80% F-score for men-
tion detection for a relaxed matching criterion.
The corpus and other introduced resources are
available under open licences from http://
www.nactem.ac.uk/anatomy/.
1 Introduction
Entity mention detection is a prerequisite for most
efforts to systematically analyse and represent the
structure of scientific discourse. In the life sciences,
a comprehensive analysis must include entities at
multiple levels of biological organization, from the
molecular to the organism level. The detection of
references to anatomical entities such as ?kidney?
and ?blood? is thus required for the automatic struc-
tured analysis of biomedical scientific text.
Although a wealth of lexical and ontological re-
sources covering anatomical entities are available
(Rosse and Mejino, 2003; Smith et al, 2007; Boden-
reider, 2004; Haendel et al, 2009), such resources
do not alone confer the ability to reliably detect
mentions of anatomical entities in natural language
(Gerner et al, 2010a; Travillian et al, 2011; Pyysalo
et al, 2012b). To support the development and eval-
uation of reliable anatomical entity mention detec-
tion methods, corpus resources annotated specifi-
cally for the task are necessary.
In this study, we aim to create a reference standard
for evaluating methods for anatomical entity men-
tion detection and for training machine learning-
based methods for the task. We seek to select
a set of texts that are representative of the rele-
vant scientific literature, i.e. open-domain in the
sense of avoiding bias toward, for example, specific
species, levels of biological organization (e.g. sub-
cellular or gross anatomy), parts of documents (e.g.
abstracts), or subdomains of life science. In sup-
port of our annotation, we draw on a granularity-
based, species-independent upper-level ontology of
anatomy as well as relevant species-specific onto-
logical resources.
The overall aim of our efforts is to create methods
and resources for comprehensive event-based anal-
ysis (Ananiadou et al, 2010) of biomedical scien-
tific discourse involving anatomy-level entities and
processes. In aiming to establish a stable basis
for anatomical entity mention detection, the present
study is an important step toward this goal.
27
Label Ontology classes Examples
A
na
to
m
ic
al
en
ti
ty
A
na
to
m
ic
al
st
ru
ct
ur
e ORGANISM SUBDIVISION organism subdivision CARO head, limb
ANATOMICAL SYSTEM anatomical system CARO vascular system
ORGAN compound organ CARO liver, heart
MULTI-TISSUE STRUCTURE multi-tissue structure CARO artery
TISSUE portion of tissue CARO epithelium
CELL cell CARO epithelial cell
DEVELOPING ANATOMICAL STRUCTURE developing anatomical structure UBERON embryo
CELLULAR COMPONENT cellular component GO mitochondrion
ORGANISM SUBSTANCE portion of organism substance CARO blood
IMMATERIAL ANATOMICAL ENTITY immaterial anatomical entity CARO lumen
PATHOLOGICAL FORMATION - carcinoma
Table 1: Annotations targets with applied label, corresponding ontology classes, and common examples.
2 Corpus Annotation
2.1 Ontological Basis
Following our previous efforts on anatomical en-
tity classification (Pyysalo et al, 2012b), we base
our definition of annotated mention scope, the sub-
division of anatomical entities into classes, and
the class labels applied in our annotation primar-
ily on the Common Anatomy Reference Ontology
(CARO) (Haendel et al, 2008). CARO is a small,
species-independent ontology of anatomical entities
based on the upper-level structure of the Founda-
tional Model of Anatomy (FMA) ontology of hu-
man anatomy (Rosse and Mejino, 2003; Rosse and
Mejino, 2008). CARO has been proposed as a stan-
dard for unifying the upper-level structure of the
various existing species-specific ontologies and is
adopted by many of the over 40 ontologies involv-
ing the anatomy domain in the Open Biomedical
Ontologies (OBO) foundry1 (Smith et al, 2007).
CARO adheres to disjoint classes and single inher-
itance, and divides anatomical structures primarily
by granularity (Kumar et al, 2004), a systematic no-
tion familiar to those working in the life sciences.
Although we draw primarily on CARO, we fol-
low the well-established cellular component subon-
tology of the Gene Ontology (GO) (Ashburner et
al., 2000) in grouping sub-cellular structures under
a single upper-level category. For developing struc-
tures that resist granularity-based categorization due
to occupying different levels at different stages of
development, we adopt a separate DEVELOPING
ANATOMICAL STRUCTURE category, as done also
in e.g. Uberon (Haendel et al, 2009).
1http://obofoundry.org/
2.2 Annotation Scope
We diverge from the scope of anatomy ontologies in
two important aspects in our annotation.
First, ontologies of anatomy commonly incorpo-
rate everything from molecules to whole organisms
within their scope. However, in entity mention de-
tection, many molecular level anatomical entities
fall within the scope of the established gene/protein
mention detection tasks (e.g. (Kim et al, 2004; Tan-
abe et al, 2005)), and whole organism mentions
similarly largely within what is covered by existing
methods and resources for organism mention detec-
tion (Gerner et al, 2010b; Naderi et al, 2011). To
avoid overlap with established tasks and to focus on
the novel aspects of anatomical entity mention de-
tection, we exclude biological macromolecules and
mentions of organism names from the scope of our
annotation, as argued in (Pyysalo et al, 2012b).
Second, these ontologies typically represent
canonical anatomy, an idealized state that is rarely
(if ever) encountered in reality (Bada and Hunter,
2011). As our annotation is intended to cover ref-
erences to real-world anatomy, we explicitly include
in the scope of our annotation also healthy as well as
pathological variants of canonical anatomy. We in-
clude also entities derived from these anatomical en-
tities through (planned) processing such as surgical
or laboratory procedures, even when these processed
entities are no longer properly part of the original
organism. Finally, we annotate pathological forma-
tions such as scars and carcinomas that are part of
individual organisms but have no correspondence in
canonical anatomy (Smith et al, 2005).
Table 1 presents the class labels applied in the an-
notation with the corresponding ontology classes.
28
In contrast, the 3 cases of metastatic cancer of the GB had no blood flow signal in the wall of the GB
Pathological form Organ OSubst MTS OrganPart-ofPart-of
Figure 1: Example sentence with annotation. OSUBST and MTS abbreviate for ORGANISM SUBSTANCE and MULTI-
TISSUE STRUCTURE, respectively.
2.3 Representation
The primary corpus annotation marks mentions of
anatomical entities as contiguous spans of characters
in text, each of which is assigned a type (Figure 1).
As the CARO-based categorization has comprehen-
sive coverage and disjoint classes, each annotation
can be assigned exactly one type (class label).
In addition to identifying and typing anatomical
entity mentions, we further apply binary attributes
(?flags?) marking the following characteristics of
each mention:
DEVELOPING developing variant of anatomical
entity, e.g. fetal liver
PATHOLOGICAL pathological variant of anatomi-
cal entity, e.g. carcinoma cell
PLANT anatomical entity that is part of a plant
(member of the Viridiplantae kingdom), e.g.
roots, leaf
PROCESSED variant of anatomical entity that has
undergone planned processing, e.g. tissue spec-
imen
Any combination of attributes can apply to a single
mention. These attributes allow the identification of
subsets of annotations that may be out of scope for
some efforts (e.g. pathological or processed entities)
and facilitate the analysis of mention detection sys-
tem performance by identifying particular problem-
atic categories.
2.4 Annotation Criteria
In very brief summary, we annotate spans of text that
refer to anatomical entities as defined above. Men-
tions that involve only metaphorical senses of such
entities (?on the other hand?) or artificial analogues
(?artificial heart?) are not annotated.
The primary targets of our annotation are anatom-
ical entity names (e.g. ?lymphocyte?) and nominal
mentions of anatomical entities (e.g. ?muscle tis-
sue?). Both names and nominal mentions are anno-
tated similarly, without distinction. We exclude pro-
nouns (it, that) from annotation even when they un-
cytoplasm of phagocytic microglia
Organism substance CellPart-of
thyroid and eye muscle membranes
Tissue TissueFrag
Figure 2: Part-of relation marking entity mention span-
ning a prepositional phrase (above) and Frag relation
marking coordination with ellipsis (below).
ambiguously refer to an anatomical entity; we con-
sider the identification and resolution of such men-
tions part of the distinct coreference resolution task
(see e.g. Pradhan et al (2011)).
In addition to names and nominal mentions, we
mark adjectives that have an unambiguous sense
of relating to a specific anatomical entity. Thus,
for example, both ?kidney? and ?renal? (relating to
the kidneys) are annotated as ORGAN in expres-
sions such as ?kidney failure? and ?renal failure?.
The choice to annotate adjectival references is mo-
tivated by the expected needs of applications mak-
ing use of automatically detected anatomical entity
mentions. For example, for semantic search target-
ing documents relating to organ failure, a document
discussing ?renal failure? is obviously relevant and
should be recovered.
Syntactically, annotations mainly cover base
noun phrases without determiners, i.e. nouns with
premodifiers relevant to identifying the specific
anatomical entity referred to. We exclude noun
phrase postmodifiers such as prepositional phrases
from the span of single annotations, but apply a
separate level of annotation for part-of relations
that allow such alternate spans to be recovered
when they identify an anatomical entity (Figure 2
top). Similarly, we decompose coordinated ref-
erences to anatomical entities involving ellipsis to
non-overlapping spans, but mark the cases using a
frag(ment) relation type (Figure 2 bottom). (Due to
space considerations, we omit detailed discussion of
these relation annotations.) Together with the prop-
erties described in Section 2.3, these constraints as-
sure that any single token is assigned at most one
class label and allow the annotation to be repre-
29
Matching criterion
Task Strict Left boundary Right boundary
Mention detection (single class) 89.2%/ 82.0%/ 85.4% 93.0%/ 85.5%/ 89.1% 94.6%/ 86.9%/ 90.6%
Detection and classification (multi-class) 85.6%/ 78.7%/ 82.0% 87.0%/ 80.0%/ 83.3% 90.2%/ 82.9%/ 86.4%
Table 2: Inter-annotator agreement results (precision / recall / F-score).
sented in the standard BIO format and to be straight-
forwardly applied with many existing entity mention
taggers.
By contrast to previously introduced domain re-
sources for e.g. molecular entity and organism men-
tion detection (Tanabe et al, 2005; Gerner et al,
2010b), we do not incorporate any specificity con-
straints in our annotation criteria. That is, non-
specific expressions such as ?tissue? and ?organ? are
marked identically to specific ones such as ?epithe-
lium? and ?heart?. This choice seeks to assure the
generality of the task and methods for addressing it.
2.5 Text Selection
Texts for the corpus were drawn from two sources:
the PubMed2 database of publication abstracts, and
the PubMed Central3 (PMC) Open Access subset
of full-text publications. PubMed, containing more
than 20 million citations, has a very broad coverage
of domain scientific texts but is limited to publica-
tion abstracts, while PMC has lower coverage but
does provide over 400,000 full-text documents un-
der open licenses. By sampling both sources, we
seek to assure the corpus is relevant to IE efforts re-
gardless of their choice of texts.
To avoid bias toward e.g. subdomains of biol-
ogy or specific species, we selected texts from both
sources by random sampling. For PubMed, we sim-
ply selected a random set of citations and extracted
their abstract and title texts. For PMC, we initially
extracted all non-overlapping section texts (PMC
XML <sec> elements) as well as caption texts
(<caption> elements), and then selected a ran-
dom set of extracts. This selection seeks to maxi-
mize the diversity of the texts in the full-text sec-
tion of the corpus, and the selection of extracts larger
than isolated sentences aims to allow the corpus to
be used to study methods making use of broader
context, e.g. by incorporating constraints such as
one sense per discourse (Gale et al, 1992).
2http://pubmed.com
3http://www.ncbi.nlm.nih.gov/pmc/
We selected a total of 500 documents using this
protocol, half from PubMed and half from PMC
document extracts. (Descriptive statistics of the ab-
stracts and full-text extracts subcorpora are given
later in Table 3.)
2.6 Annotation Process
Primary annotation was created by a PhD biologist
with extensive experience in domain information ex-
traction and text annotation (TO). The use of any rel-
evant resources, such as the full article being anno-
tated or species-specific anatomy ontologies in the
OBO foundry, was encouraged for resolving unclear
or ambiguous cases during annotation. Initial anno-
tation was produced entirely manually. To further
assure the quality of the annotation, a series of au-
tomatic tests was performed and used as the basis
of a further manual round of revision.4 Annotation
guidelines were initially created based on those cre-
ated by our previous domain-specific effort (Pyysalo
et al, 2012a) and revised throughout the annotation
effort to document specific decisions made during
annotation. The annotations were created using the
BRAT annotation tool (Stenetorp et al, 2012).
To evaluate the annotation consistency, we per-
formed an inter-annotator agreement (IAA) exper-
iment. After brief training with annotation guide-
lines provided by the primary annotator, a random
10% of the corpus was independently annotated by
a PhD computer scientist with experience in domain
text annotation and anatomy ontologies (SP). IAA
was evaluated using the same criteria as applied in
experiments (see Section 3.4), holding the primary
annotation as gold. The results are shown in Table 2.
We find very good agreement both for mention de-
tection (ignoring classification) as well as for the full
task, indicating that the task is well defined and the
annotation consistency high.
4No automatically suggested annotations were incorporated
into the corpus without manual verification.
30
3 Methods
We next present the methods applied in our anatomi-
cal entity mention detection experiments. We aim to
evaluate the capacity of the newly annotated corpus
to support reliable mention detection and to estab-
lish initial baseline results for the newly introduced
resource, and thus focus only on relatively straight-
forward applications of existing methods.
3.1 MetaMap
MetaMap5 (Aronson, 2001) is a tool capable of
detecting mentions of concepts from the exten-
sive UMLS Metathesaurus (Bodenreider, 2004)
in text. The metathesaurus and MetaMap have
broad coverage of concepts relevant to biology
and medicine and provide a categorization of
concepts into 133 semantic types, ranging from
Amino Acid to Health Care Activity to
Vertebrate, many directly relevant to anatomi-
cal entities. MetaMap is a key component of the
process used by the National Library of Medicine
(NLM) to index publications in the PubMed
database and has been applied in numerous other in-
formation extraction and information retrieval tasks
(Aronson and Lang, 2010).
In initial experiments, we applied MetaMap to
training set documents to identify the subset of the
133 semantic classes relevant to anatomy, select-
ing 14 classes (including e.g. Cell, Tissue and
Body Substance) for final experiments.6 Dur-
ing testing, we used command-line arguments to re-
strict output to the selected semantic classes. The
core tagging functionality of MetaMap is rule-based,
and it does not support training on tagged data
for concept mention detection. With the exception
of the semantic class selection, the evaluation of
MetaMap reflects an ?off-the-shelf? application of
the general-purpose tool.
3.2 CRF tagging
Conditional Random Fields (CRF) (Lafferty et al,
2001) are graphical models that are frequently ap-
5http://metamap.nlm.nih.gov/
6In brief, we tagged the training data with MetaMap, ex-
tracted the subset of semantic classes giving more than 5%
precision against the gold annotations, and manually analysed
these to select this subset. The selected classes are detailed in
supplementary material available on the project webpage.
plied to sequence labeling tasks, and CRFs form
the basis of state-of-the-art methods for many en-
tity mention tagging tasks. We performed experi-
ments using the NERsuite entity mention recogni-
tion toolkit, based on the CRFsuite implementation
of CRFs (Okazaki, 2007). NERsuite provides an
extensive set of features applied in entity mention
detection, allowing the tool to achieve performance
competitive with state-of-the-art methods for many
biomedical domain tasks through retraining with-
out task-specific adaptation7. Retraining the tool for
new tasks is also straightforward, allowing applica-
tion to new tasks with modest effort.
We set the L2 regularization parameter of the
learning method using held-out evaluation with
training set data, picking out of a set of values 2n
(n ? Z) the one giving best performance.8 Other
learning method parameters were left at default val-
ues.
3.3 System combination
As a third system, we apply a straightforward com-
bination of the MetaMap and CRF tagging systems,
where we initially tag the data using MetaMap and
then incorporate the classes assigned by MetaMap
as features for training and testing with NERsuite
(stacking). More specifically, we create a BIO-
tagged version of MetaMap output segmented to
match NERsuite tokenization, and assign each token
the BIO tag based on the MetaMap semantic type
code (e.g. B-cell) as a feature.
Excepting for the addition of these MetaMap-
derived features, NERsuite is applied as described
above (Section 3.2).
3.4 Experimental setting
We split the corpus data into two primary parts: a
training set consisting of 60% of the documents and
a test set of the remaining 40%. The data splits
were performed independently for the two subcor-
pora (abstracts and full-text extracts), using strati-
fied sampling to assure broadly comparable statisti-
cal properties between the sets. The test set was held
out during development and only applied for the fi-
nal experiments.
7http://nersuite.nlplab.org/
8Specifically, C2 = 2?5 was selected.
31
Dataset
Source Item Train Test Total
Abst.
Document 150 100 250
Word 28,960 18,199 47,159
Entity 1,182 764 1,946
FTE
Document 150 100 250
Word 26,306 17,955 44,261
Entity 697 492 1,189
Total
Document 300 200 500
Word 55,266 36,154 91,420
Entity 1,879 1,256 3,135
Table 3: Overall corpus statistics. Statistics given sepa-
rately for the abstracts (abst.) and full-text extracts (FTE)
subcorpora as well as for the total.
We perform experiments in two settings: a single-
class setting where the task is restricted to the detec-
tion of anatomical entity mentions without classifi-
cation, and a multi-class setting where the correct
class label must further be assigned to each detected
mention. As MetaMap uses UMLS semantic classes
that do not fully align with the applied CARO-based
classes, MetaMap is only applied in the single-class
setting.
For evaluation, we adopted the protocol, crite-
ria and metrics of the established BioNLP/JNLPBA
shared task 2004 (Kim et al, 2004). To assure com-
patibility, we created our evaluation tool on the ba-
sis of the shared task evaluation script. The eval-
uation is thus based on entity-wise (microaverage)
precision/recall/F-score metrics, and tagging perfor-
mance is separately evaluated under strict match, left
boundary match and right boundary match criteria.
In the former setting, a predicted entity must exactly
match the extent of a gold standard entity, while in
the latter two settings, it is enough that the left/right
boundary matches.
3.5 Format
The annotation is distributed in the standard column-
based BIO format applied for e.g. CoNLL 2003
(Tjong Kim Sang and De Meulder, 2003) and
JNLPBA (Kim et al, 2004) data, among other es-
tablished datasets.
4 Results
4.1 Corpus statistics
Table 3 presents the overall corpus statistics. We
note that the abstracts and full-text extracts (FTE)
Type Count
CELL 776
MULTI-TISSUE STRUCTURE 639
ORGAN 381
PATHOLOGICAL FORMATION 368
ORGANISM SUBSTANCE 291
CELLULAR COMPONENT 199
TISSUE 169
ORGANISM SUBDIVISION 162
IMMATERIAL ANATOMICAL ENTITY 60
ANATOMICAL SYSTEM 51
DEVELOPING ANATOMICAL STRUCTURE 39
Table 4: Annotation statistics by type.
subcorpora are of comparable size in terms of their
word counts, but the number of annotations is 1.6
times higher in the abstracts subcorpus (1.5 cor-
recting for number of words). This difference in
anatomical entity mention density between abstracts
and full texts parallels the findings of Cohen et al
(2010) on the relative density of gene, drug and dis-
ease mentions. We further note that the estimated
density of anatomical entity mentions in abstracts
(approx. 41 per 1000 words) and full texts (27 per
1000) are broadly comparable to the gene mention
density estimates of Cohen et al (61 and 47 for ab-
stracts and full texts, respectively).
Table 4 presents a breakdown by annotation type.
There are large differences in the number of anno-
tations by type, with the majority class CELL out-
numbering the rarest type 20-fold. While the total
number of annotated examples is likely to be suf-
ficient for training machine learning-based taggers
and most of the classes contain a respectable num-
ber of examples, the statistics suggest that the least
frequently annotated types may represent challenges
for learning.
4.2 Entity Mention Detection
Table 5 presents the experimental results for anatom-
ical entity mention detection (single-class). In terms
of F-score, we find the same ranking of the three
methods for all three criteria, with the CRF-based
tagger outperforming the rule-based MetaMap, and
the combination method outperforming its compo-
nents. Although it is not surprising that a dedicated
machine learning-based system is capable of outper-
forming a general-purpose, largely rule-based sys-
tem, this result does reflect positively on both the
32
Matching criterion
Method Strict Left boundary Right boundary
MetaMap 50.78% / 64.49% / 56.82% 54.67% / 69.43% / 61.17% 58.18% / 73.89% / 65.10%
NERsuite 77.98% / 52.15% / 62.50% 81.43% / 54.46% / 65.27% 90.00% / 60.19% / 72.14%
MetaMap + NERsuite 82.09% / 62.42% / 70.92% 84.61% / 64.33% / 73.09% 90.68% / 68.95% / 78.34%
Table 5: Overall single-class anatomical entity mention detection results (precision / recall / F-score).
Matching criterion
Method Strict Left boundary Right boundary
NERsuite 72.07% / 42.12% / 53.17% 72.75% / 42.52% / 53.67% 85.69% / 50.08% / 63.22%
MetaMap + NERsuite 75.41% / 51.75% / 61.38% 76.45% / 52.47% / 62.23% 83.99% / 57.64% / 68.37%
Table 6: Overall anatomical entity mention detection and classification results (precision / recall / F-score).
consistency of the annotation as well as the suffi-
ciency of the size of the newly introduced corpus.
In this application, we find that MetaMap tends to
favor recall over precision ? perhaps reflecting its
focus on IR applications (Aronson and Lang, 2010)
? while the trained machine learning-based models
are clearly biased in favor of high precision.
As expected on the basis of the results of previous
evaluations using similar experimental setups (Kim
et al, 2004), results are notably better under the re-
laxed matching criteria. In particular, requiring only
the right boundaries of annotations to match yields
F-scores nearly 10% points higher than under strict
matching. Recalling that the annotations primar-
ily mark base noun phrases, this suggests that the
systems comparatively frequently identify the head
word of an anatomical entity mention correctly but
differ from gold annotation regarding the choice of
premodifiers included in the span of the annotation.
As limited variation in premodifier selection is ar-
guably acceptable for many applications and relaxed
matching criteria are frequently applied in domain
tagging tasks (Kim et al, 2004; Wilbur et al, 2007),
we propose to consider performance under the re-
laxed right boundary match criterion as the primary
result for evaluation using the new corpus.
Table 6 presents the results for anatomical entity
mention detection and classification using the 11-
class categorization used in annotation.9 While per-
formance in terms of F-score is approximately 10%
points lower than for the single-class task, this drop
is comparatively modest given the large number of
9Note that evaluation using MetaMap only is not possible as
its semantic classes differ from those used in the annotation.
distinct classes, indicating that the number of an-
notations of most individual classes is sufficient for
learning.
While these initial results are not as high as for
established entity mention detection tasks in the do-
main (Wilbur et al, 2007; Rebholz-Schuhmann et
al., 2011), we consider the level of performance
quite good given the many new challenges relat-
ing to the task. Further, as the mention detection
methods were also applied with only modest specific
adaptation to the task, we believe there remain many
opportunities for further development of methods
for the task.
4.3 Discussion
Many commonly targeted mention types in both
the ?general? and the biological domain are fre-
quently characterized by obvious surface features:
the names of people and locations are capitalized in
many languages, as are genera in scientific species?
names, and many gene and chemical names have
comparable features distinguishing them from com-
mon nouns (consider e.g. p53, IgE, c-myc, Ca2+,
H2SO4). By contrast, many typical anatomical en-
tity mentions are common noun compounds lacking
obvious distinguishing surface features. This fact
likely contributes to the comparatively low perfor-
mance of the CRF-based tagger when applied with-
out support from lexical resources.
A further challenge that arises comparatively fre-
quently in anatomical entity mention detection is
ambiguity between entity mentions and other words
sharing the same surface form. For example, while
Barack Obama, Sweden, p53 and H2SO4 can be
33
safely identified as mentions of a person, country,
gene, and chemical without reference to context,
face should not be marked as an anatomical entity
mention in face the facts, nor should Airways in
British Airways. Thus, approaches relying on simple
matching against lexical resources will not suffice
for accurate anatomical entity mention detection.
Our evaluation results demonstrated a clear ad-
vantage to combining detection based on lexical re-
sources with machine learning-based tagging, an ap-
proach we believe will be key to the further develop-
ment of reliable anatomical entity mention tagging
that we will seek to explore in detail in future work.
To facilitate analysis of the performance of the meth-
ods, we provide the predictions of each method in
supplementary data on the project homepage.
5 Related work
A number of domain corpora such as GENIA (Ohta
et al, 2002), BioInfer (Pyysalo et al, 2007), and the
recently introduced CellFinder corpus (Neves et al,
2012) include annotation for at least some classes
of anatomical entities. However, such corpora typ-
ically cover only specific subdomains of the litera-
ture, such as transcription factors in human blood
cells (GENIA), protein-protein interactions (BioIn-
fer), or stem cells (CellFinder). To the best of our
knowledge, this is the first effort introducing a cor-
pus annotated for anatomical entity mentions that
specifically aims to be representative of the entire
available literature. We note that there is a well-
established precedent to this goal: sentences for
the de facto standard corpus for gene/protein name
recognition, GENETAG (Tanabe et al, 2005), were
similarly selected from PubMed abstracts without
domain restrictions.
The BioNLP/JNLPBA shared task 2004 (Kim et
al., 2004) targeted the detection of mentions of five
types of biological entities, including two that would
fall within in the scope of our CELL annotation
(?Cell type? and ?Cell line?). Other than this com-
paratively early shared task, collaborative domain
efforts such as BioCreative (Krallinger et al, 2008)
and CALBC (Rebholz-Schuhmann et al, 2011) have
not targeted anatomical entity mentions.
Some recent studies have considered the use of
ontological resources for the detection of anatomi-
cal entity mentions in natural language expressions.
In previous work (Pyysalo et al, 2012b), we studied
the classification of isolated noun phrases extracted
from PubMed to identify anatomy terms. Travillian
et al (2011) considered two lexical matching appli-
cations to detect anatomical entities from two OBO
resources in user-provided terms. However, these
efforts have not involved the annotation or detection
of mentions in context, which we view as critical for
real-world entity mention detection method devel-
opment and evaluation.
6 Conclusions
We have introduced a manually annotated corpus for
open-domain anatomical entity mention detection,
consisting of 500 documents (over 90,000 words)
drawn from publication abstracts and full texts. The
primary corpus annotation consists of the identifi-
cation of over 3,000 references to both healthy and
pathological anatomical entities, marked using a de-
tailed 11-class categorization based on established
biomedical domain ontologies. We demonstrated
the use of the new corpus through a comparative
evaluation of MetaMap, a general semantic class
tagger; NERsuite, a CRF-based machine learning
system; and a stacked combination of the two, find-
ing that under a relaxed matching criterion, the com-
bination approaches 80% F-score at mention detec-
tion and 70% F-score at mention detection and clas-
sification. This level of performance is encourag-
ing for a first application and suggests that reliable
open-domain anatomical entity mention detection is
not an unrealistic target.
We hope that the introduced corpus can serve as a
reference standard for the further development and
evaluation of methods for anatomical entity men-
tion detection. This corpus, the introduced evalua-
tion tools, and other resources created in this study
are made available under open licences from http:
//www.nactem.ac.uk/anatomy/.
Acknowledgments
This work was funded by UK Biotechnology and Bi-
ological Sciences Research Council (BBSRC) under
project Automated Biological Event Extraction from
the Literature for Drug Discovery (reference num-
ber: BB/G013160/1).
34
References
S. Ananiadou, S. Pyysalo, J. Tsujii, and D.B. Kell. 2010.
Event extraction for systems biology by text mining
the literature. Trends in Biotechnology, 28(7):381?
390.
A.R. Aronson and F.M. Lang. 2010. An overview of
MetaMap: historical perspective and recent advances.
Journal of the American Medical Informatics Associa-
tion, 17(3):229?236.
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of AMIA, pages 17?21.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
M. Bada and L. Hunter. 2011. Desiderata for ontologies
to be used in semantic annotation of biomedical docu-
ments. Journal of Biomedical Informatics, 44(1):94?
101.
O. Bodenreider. 2004. The unified medical language
system (UMLS): integrating biomedical terminology.
Nucleic acids research, 32(suppl 1):D267?D270.
K.B. Cohen, H. Johnson, K. Verspoor, C. Roeder, and
L. Hunter. 2010. The structural and content aspects of
abstracts versus bodies of full text journal articles are
different. BMC bioinformatics, 11(1):492.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. One
sense per discourse. In Proceedings of the workshop
on Speech and Natural Language, pages 233?237.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010a. An
exploration of mining gene expression mentions and
their anatomical locations from biomedical text. In
BioNLP?10, pages 72?80.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010b.
LINNAEUS: a species name identification system
for biomedical literature. BMC bioinformatics,
11(1):85+.
M.A. Haendel, F. Neuhaus, D. Osumi-Sutherland, P.M.
Mabee, J.L.V. Mejino, C.J. Mungall, and B. Smith.
2008. CARO?the common anatomy reference ontol-
ogy. Anatomy Ontologies for Bioinformatics, pages
327?349.
M.A. Haendel, G.G. Gkoutos, S.E. Lewis, and
C. Mungall. 2009. Uberon: towards a comprehensive
multi-species anatomy ontology. Nature precedings.
J-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task at
JNLPBA. In Proceedings JNLPBA?04.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
A. Kumar, B. Smith, and D.D. Novotny. 2004. Biomed-
ical informatics and granularity. Comparative and
functional genomics, 5(6-7):501?508.
J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289.
N. Naderi, T. Kappler, C.J.O. Baker, and R. Witte.
2011. OrganismTagger: Detection, normalization, and
grounding of organism entities in biomedical docu-
ments. Bioinformatics.
M. Neves, A. Damaschun, A. Kurtz, and U. Leser. 2012.
Annotating and evaluating text for stem cell research.
In Third Workshop on Building and Evaluation Re-
sources for Biomedical Text Mining (BioTxtM 2012).
(to appear).
T Ohta, Y Tateisi, H Mima, and J Tsujii. 2002. GE-
NIA corpus: an annotated research abstract corpus in
molecular biology domain. Proceedings of the Human
Language Technology Conference (HLT 2002), pages
73?77.
N. Okazaki. 2007. CRFsuite: a fast imple-
mentation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
ontonotes. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1?27.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. BioInfer: A cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, T. Ohta, M. Miwa, H-C. Cho, J. Tsujii, and
S. Ananiadou. 2012a. Event extraction across mul-
tiple levels of biological organization. (manuscript in
review).
S. Pyysalo, T. Ohta, J. Tsujii, and S. Ananiadou. 2012b.
Learning to classify anatomical entities using open
biomedical ontologies. Journal of Biomedical Seman-
tics. (to appear).
D. Rebholz-Schuhmann, A. Yepes, C. Li, S. Kafkas,
I. Lewin, N. Kang, P. Corbett, D. Milward, E. Buyko,
E. Beisswanger, K. Hornbostel, A. Kouznetsov,
R. Witte, J. Laurila, C. Baker, C. Kuo, S. Clematide,
F. Rinaldi, R. Farkas, G. Mora, K. Hara, L.I. Fur-
long, M. Rautschka, M. Neves, A. Pascual-Montano,
35
Q. Wei, N. Collier, M. Chowdhury, A. Lavelli,
R. Berlanga, R. Morante, V. Van Asch, W. Daelemans,
J. Marina, E. van Mulligen, J. Kors, and U. Hahn.
2011. Assessment of NER solutions against the first
and second calbc silver standard corpus. Journal of
Biomedical Semantics, 2(Suppl 5):S11.
C. Rosse and J.L.V. Mejino. 2003. A reference on-
tology for biomedical informatics: the foundational
model of anatomy. Journal of Biomedical Informat-
ics, 36(6):478?500.
C. Rosse and J.L.V. Mejino. 2008. The foundational
model of anatomy ontology. Anatomy Ontologies for
Bioinformatics, pages 59?117.
B. Smith, A. Kumar, W. Ceusters, and C. Rosse. 2005.
On carcinomas and other pathological entities. Com-
parative and functional genomics, 6(7-8):379?387.
B. Smith, M. Ashburner, C. Rosse, J. Bard, W. Bug,
W. Ceusters, L. J Goldberg, K. Eilbeck, A. Ireland,
C.J. Mungall, N. Leontis, P. Rocca-Serra, A. Rut-
tenberg, S-A Sansone, R.H. Scheuermann, N. Shah,
P.L. Whetzel, and S. Lewis. 2007. The OBO
Foundry: coordinated evolution of ontologies to sup-
port biomedical data integration. Nature biotechnol-
ogy, 25(11):1251?1255.
P. Stenetorp, S. Pyysalo, G. Topic?, T. Ohta, S. Ananiadou,
and J. Tsujii. 2012. brat: a web-based tool for NLP-
assisted text annotation. In Proceedings of the EACL
2012 Demonstrations, pages 102?107.
L. Tanabe, N. Xie, L. Thom, W. Matten, and W.J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC bioinfor-
matics, 6(Suppl 1):S3.
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003, pages 142?147.
R. Travillian, T. Adamusiak, T. Burdett, M. Gruenberger,
J. Hancock, A-M. Mallon, J. Malone, P. Schofield, and
H. Parkinson. 2011. Anatomy ontologies and poten-
tial users: bridging the gap. Journal of Biomedical
Semantics, 2(Suppl 4):S3.
J. Wilbur, L. Smith, and L. Tanabe. 2007. BioCre-
ative 2 Gene Mention Task. In Proceedings of Second
BioCreative Challenge Evaluation Workshop, pages
7?16.
36
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 67?75,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of the Pathway Curation (PC) task of BioNLP Shared Task 2013
Tomoko Ohta 1, Sampo Pyysalo 1, Rafal Rak 1, Andrew Rowley1, Hong-Woo Chun2,
Sung-Jae Jung 2,3, Chang-Hoo Jeong 2 Sung-Pil Choi 2,3, Jun?ichi Tsujii 4,Sophia Ananiadou 1
1National Centre for Text Mining and School of Computer Science, University of Manchester
2Software Research Center, Korea Institute of Science and Technology Information (KISTI)
3Department of Applied Information Science, University of Science and Technology (UST)
4Microsoft Research Asia, Beijing, China
Abstract
We present the Pathway Curation (PC)
task, a main event extraction task of
the BioNLP shared task (ST) 2013.
The PC task concerns the automatic ex-
traction of biomolecular reactions from
text. The task setting, representation
and semantics are defined with respect
to pathway model standards and ontolo-
gies (SBML, BioPAX, SBO) and docu-
ments selected by relevance to specific
model reactions. Two BioNLP ST 2013
participants successfully completed the
PC task. The highest achieved F-
score, 52.8%, indicates that event extrac-
tion is a promising approach to support-
ing pathway curation efforts. The PC
task continues as an open challenge with
data, resources and tools available from
http://2013.bionlp-st.org/
1 Introduction
Following developments in molecular biology, bi-
ological phenomena are increasingly understood
on the molecular level, as the products of complex
systems of molecular reactions. Pathway mod-
els formalizing biomolecules and their reactions
in machine readable representations are a key way
of sharing and communicating human understand-
ing of these phenomena and of developing com-
putational models of biological systems (Kitano,
2002). Many pathway models integrate knowl-
edge from hundreds or thousands of scientific pub-
lications, and their curation requires substantial
manual effort. To support this effort, we have de-
veloped PathText (Kemper et al, 2010) which pro-
vides a seamless environment integrating a path-
way visualizer, text mining systems and annota-
tion tools. Furthermore, automatic processing of
the domain literature could thus potentially play
pyruvate kinase catalyzes the conversion of PEP to pyruvate.
GGP +Regulation Conversion Chem ChemicalThemeCause Theme Product
Figure 1: Event representation for a conversion re-
action.
an important role in the support of pathway cura-
tion.
Information extraction targeting biomolecular
reactions has been a major focus of efforts in
biomedical natural language processing, with sev-
eral tasks, resources, and tools addressing in par-
ticular protein-protein interactions (Krallinger et
al., 2007; Pyysalo et al, 2008; Tikk et al, 2010).
However, most such efforts have employed sim-
ple representations, such as entity pairs, that are
not sufficient for capturing molecular reactions to
the level of detail required to support the curation
of pathway models. Additionally, previous efforts
have not directly involved the semantics (e.g. re-
action type definitions) of such models. Perhaps
in part due to these reasons, natural language pro-
cessing and information extraction methods have
not been widely embraced by biomedical pathway
curation communities (Ohta et al, 2011c; Ohta et
al., 2011a).
We believe that the extraction of structured
event representations (Figure 1) pursued in the
BioNLP Shared Tasks offers many opportuni-
ties to make significant contributions to support
the development, evaluation and maintenance of
biomolecular pathways. The Pathway Curation
(PC) task, a main task of the BioNLP Shared Task
2013, is proposed as a step toward realizing these
opportunities. The PC task aims to evaluate the ap-
plicability of event extraction systems to pathway
curation and to encourage the further development
of methods for related tasks. The design of the
task aims to address current issues in information
extraction for pathway curation by explicitly bas-
ing its representation and extraction targets on ma-
67
GTP GDP
GAPs
re1
re1 Protein Molecule MoleculeReactantModifier ProductConversion GAPs catalyze the hydrolysis of GTP to GDP.GGP +Reg Conversion Chem Chem
Cause ThemeTheme Product
(a) CONVERSION
p38 gamma Pp38 gamma
MKK6
re1
re1
MKK6 phosphorylates p38 gamma.Protein Protein
Protein
Modifier Reactant
Product
Phosphorylation MKK6 phosphorylates p38 gamma.GGP Phosphorylation GGP
Cause Theme
(b) PHOSPHORYLATION
NF-kappaB
p65
p50
p65
p50re1
p65 binds to p50.
GGP Bind GGPTheme Theme2
p65-p50 complex formation.
Complex BindingProduct
p65 and p50 form p65-p50 complex.
Protein Protein NC binding ComplexReactant2 Product
Reactant
(c) BINDING
Figure 2: Illustration of pathway reaction (left), matching representation as an idealized text-bound event
structure (middle) and applied event representation for statements actually appearing in text (right).
jor standards developed in the biomolecular path-
way curation community, such as SBML (Hucka
et al, 2003) and BioPAX (Mi et al, 2011), and
ontologies such as the Systems Biology Ontology1
(SBO) (Courtot et al, 2011). Further, The corpus
texts are selected on the basis of relevance to a se-
lection of pathway models from PANTHER Path-
way DB2 (Mi and Thomas, 2009) and BioMod-
els3 (Li et al, 2010) repositories. The PC task set-
ting and its document selection protocol aim to ac-
count for both signalling and metabolic pathways,
the latter of which has received comparatively lit-
tle attention in recent domain IE efforts (Li et al,
2013).
2 Task setting
The PC task is formulated as an event extraction
task (Ananiadou et al, 2010) following the general
representation and task setting first introduced in
the BioNLP ST 2009 (Kim et al, 2011). The pri-
mary aim is the extraction of event structures, or
events, each of which can involve any number of
physical entities or other events in specific roles.
The event representation is sufficiently expres-
sive to allow the definition of event structures that
closely parallel the definition of reactions in path-
way representations such as SBML and BioPAX.
These pathway representations differentiate be-
tween three primary groups of reaction partici-
pants: reactants (?inputs?), products (?outputs?),
and modifiers, where the specific roles of modi-
fiers can be further identified to differentiate e.g.
1http://www.ebi.ac.uk/sbo/main/
2http://www.pantherdb.org/pathway/
3http://www.ebi.ac.uk/biomodels-main/
reaction catalysts from inhibitors. Correspond-
ingly, the PC task applies the Theme role defined
in previous BioNLP ST tasks to capture reactants,
introduces a new Product role for products, and
applies the previously defined Cause role and reg-
ulatory events to capture modifiers (Figure 2; see
also Section 2.3).
It is important to note that while the event repre-
sentation allows a one-to-one mapping to reactions
in principle, an annotation scheme cannot guar-
antee that actual statements in text map to fully
specified reactions: in free-form text, authors fre-
quently omit mention of some entities taking part
in reactions, perhaps most typically to avoid re-
dundancies such as in ?p38? is phosphorylated
into phospho-p38?? (Figure 2b). Representations
extracted from explicit statements in text will thus
in some cases omit aspects of the corresponding
complete reactions in pathway models.
Systems addressing the PC task are expected to
extract events of specific types given 1) free-form
text and 2) gold standard annotation for mentions
of physical entities in that text. The task annota-
tions also include equivalence relations and event
modifications, a secondary extraction target. The
annotation types are detailed below.
2.1 Entities
The entity annotation marks mentions of physical
entities using start and end offsets in text (contigu-
ous span) and a type selected from a fixed set. The
following four entity types are marked in the PC
task: SIMPLE CHEMICAL, annotated with refer-
ence to the Chemical Entities of Biological Inter-
est (ChEBI) resource (Degtyarenko et al, 2008);
68
Entity type Scope Reference Ontology ID
SIMPLE CHEMICAL simple, non-repetitive chemical entities ChEBI SBO:0000247
GENE OR GENE PRODUCT genes, RNA and proteins gene/protein DBs SBO:0000246
COMPLEX entities of non-covalently linked components complex DBs SBO:0000253
CELLULAR COMPONENT parts of cell and extracellular environment GO-CC SBO:0000290
Table 1: Entity types, definitions, and reference resources.
Event type Core arguments Additional arguments Ontology ID
CONVERSION Theme:Molecule, Product:Molecule SBO:0000182
PHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000216
DEPHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000330
(Other modifications, such as ACETYLATION, defined similarly.)
LOCALIZATION Theme:Molecule At/From/ToLoc:CELL. COMP. GO:0051179
TRANSPORT Theme:Molecule From/ToLoc:CELL. COMP. SBO:0000185
GENE EXPRESSION Theme:GENE OR GENE PRODUCT GO:0010467
TRANSCRIPTION Theme:GENE OR GENE PRODUCT SBO:0000183
TRANSLATION Theme:GENE OR GENE PRODUCT SBO:0000184
DEGRADATION Theme:Molecule SBO:0000179
BINDING Theme:Molecule, Product:COMPLEX SBO:0000177
DISSOCIATION Theme:COMPLEX, Product:Molecule SBO:0000180
REGULATION Theme:ANY, Cause:ANY GO:0065007
POSITIVE REGULATION Theme:ANY, Cause:ANY
GO:0048518,
GO:0044093
ACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
NEGATIVE REGULATION Theme:ANY, Cause:ANY
GO:0048519,
GO:0044092
INACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
PATHWAY Participant:Molecule SBO:0000375
Table 2: Event types and arguments. ?Molecule? refers to an entity annotation of any of the types
SIMPLE CHEMICAL, GENE OR GENE PRODUCT, or COMPLEX, and ?ANY? refers to an annotation of
any type, either entity or event. The indentation corresponds to ontological relationships between the
event types: for example, PHOSPHORYLATION is-a CONVERSION and TRANSCRIPTION part-of
GENE EXPRESSION.
GENE OR GENE PRODUCT, annotated with refer-
ence to gene and protein databases such as UniProt
(Consortium, 2011), Entrez Gene (Maglott et al,
2005) and PFam (Finn et al, 2010); COMPLEX,
annotated with reference to database resources
covering complexes; and CELLULAR COMPO-
NENT, annotated following the scope of the Gene
Ontology cellular component subontology
(Ashburner et al, 2000) (Table 1). For discussion
of the relation between these types and the repre-
sentations applied in pathway models, we refer to
Ohta et al (2011c).
In terms of mention types in text, the annotation
for SIMPLE CHEMICAL, GENE OR GENE PROD-
UCT and COMPLEX covers entity name mentions
only, while the annotation for CELLULAR COM-
PONENT covers entity name mentions, nominal
mentions, and adjectival references (e.g. ?mito-
chondrial?).
2.2 Relations
The PC task defines one relation type, Equiv
(equivalence), which can hold between entity
mitogen-activated protein kinase (MAPK, also known as ERK)
Gene or gene product GGP GGPEquivEquiv
Figure 3: Example Equiv annotation.
mentions of the same type and specifies that they
refer to the same real-world entity (Figure 3).
These relations are only applied to determine if
two events match during evaluation, where entities
connected by an Equiv relation are considered in-
terchangeable. Gold standard Equiv relations are
applied also for test data, and systems participat-
ing in the task are not expected to extract these
relations.
2.3 Events
The event annotation marks references to reac-
tions, processes and comparable associations in
scope of the annotation using the event represen-
tation. For the definition and scope of the event
annotation, we rely primarily on the Systems Biol-
ogy Ontology (SBO), drawing some general types
not in scope of this ontology from the Gene Ontol-
ogy (GO). Table 2 presents the event types anno-
69
Pathway Repository ID Publication
mTOR BioModels MODEL1012220002 (Caron et al, 2010)
mTORC1 upstream regulators BioModels MODEL1012220003 (Caron et al, 2010)
TLR BioModels MODEL2463683119 (Oda and Kitano, 2006)
Yeast Cell Cycle BioModels MODEL1011020000 (Kaizu et al, 2010)
Rb BioModels MODEL4132046015 (Calzone et al, 2008)
EGFR BioModels MODEL2463576061 (Oda et al, 2005)
Human Metabolic Network BioModels MODEL6399676120 (Duarte et al, 2007)
NF-kappaB pathway - - (Oda et al, 2008)
p38 MAPK PANTHER DB P05918 -
p53 PANTHER DB P00059 -
p53 feedback loop pathway PANTHER DB P04392 -
Wnt signaling pathway PANTHER DB P00057 -
Table 3: Pathway models used to select documents for the task, with pathway repository model identifiers
and publications presenting each model (when applicable).
tated in the PC task and their arguments. We refer
again to Ohta et al (2011c) for detailed discussion
of the relation between these types and other rep-
resentations applied in pathway models.
The role in which each event argument (entity
or other event) participates in an event is specified
as one of the following:
Theme entity/event that undergoes the effects of
the event. For example, the entity that is tran-
scribed in a TRANSCRIPTION event or transported
in a TRANSPORT event.
Cause entity/event that is causally active in the
event. Marks, for example, ?P1? in ?P1 inhibits P2
expression?.
AtLoc,FromLoc,ToLoc : location in which the
Theme entity of a LOCALIZATION event is local-
ized (At) in LOCALIZATION events not involving
movement or is transported (or moves) from/to
(From/To) in LOCALIZATION and TRANSPORT
events involving movement.
Site site on the Theme entity that is modified in
the event. Can be specified for modification events
such as PHOSPHORYLATION.
Participant general role type identifying an en-
tity that participates in some underspecified way in
a high-level process. Only applied for the PATH-
WAY type.
2.4 Event modifications
In addition to events, the PC task defines a sec-
ondary extraction target, event modifications. Two
modification types are defined: NEGATION and
SPECULATION. Both are binary flags that mod-
ify events, the former marking an event as be-
ing explicitly stated as not occurring (e.g. ?P is
not phosphorylated?) and the latter as being stated
in a speculative context (?P may be phosphory-
lated.?). Both are defined in terms of annotation
scope and semantics identically as in the BioNLP
ST?09 (Kim et al, 2009).
2.5 Evaluation
The PC task evaluation applies the standard evalu-
ation criteria established in the BioNLP ST 2009.
These criteria relax exact matching between gold
and predicted events in two aspects: approximate
trigger boundary matching, and approximate re-
cursive event matching. The former allows pre-
dicted event triggers to differ from gold triggers
by one word, and the latter requires recursively re-
ferred events to only match in their core arguments
(see Table 2). We refer to Kim et al (2011) for a
detailed definition of these criteria.
3 Corpus
This section presents the PC task corpus and its
annotation process.
3.1 Document selection
To assure that the documents annotated for the PC
task corpus are relevant to pathway reactions, we
applied two complementary approaches, both se-
lecting documents on the basis of relevance to a
specific pathway reaction. First, we selected from
the BioModels repository those pathway models
with the largest numbers of manually created an-
notations referencing a specific PubMed document
identifier. For each of these models, we extracted
literature references, selected a random subset,
downloaded the documents, and manually filtered
to select abstracts that explicitly discuss relevant
molecular reactions. Second, as only a small sub-
set of models include explicit references to the
70
literature providing evidence for specific pathway
reactions, we applied an alternative strategy where
reactions from a selection of PANTHER DB mod-
els were entered into the PathText system (Kem-
per et al, 2010),4 which is capable of suggest-
ing documents relevant to given reactions based
on an SBML model. We then selected a random
set of reactions to query the system, and manually
evaluated the highest-ranking documents to iden-
tify those whose abstracts explicitly discuss the se-
lected reaction. We refer to Miwa et al (2013a)
for a detailed description of this approach. Table 3
presents the pathway models on which the docu-
ment selection was based.
3.2 Annotation process
The base entity annotation for the PC corpus was
created automatically using state-of-the-art entity
mention taggers for each of the targeted entity
types. For SIMPLE CHEMICAL tagging, the OS-
CAR4 system (Jessop et al, 2011) trained on
the chemical named entity recognition corpus of
Corbett and Copestake (2008) was applied. For
GENE OR GENE PRODUCT mention detection, the
NERsuite5 system trained on the BioCreative 2
Gene Mention task (Wilbur et al, 2007) corpus
was used. NERsuite was also applied for CEL-
LULAR COMPONENT mention detection, for this
task trained on the Anatomical Entity Mention
(AnEM) corpus (Ohta et al, 2012). Finally, COM-
PLEX annotations were created using a combi-
nation of a dictionary and heuristics making use
of the GENE OR GENE PRODUCT annotation (for
mentions such as ?cyclin E/CDK2 complex?). To
support the curation process, these tools were in-
tegrated into the NaCTeM text-analysis workflow
system Argo (Rak et al, 2012).
Based on the evaluations of each of these tools
in the studies presenting them, we expected initial
automatic tagging performance to be in the range
80-90% in both precision and recall. Following
initial automatic annotation, the entity mention an-
notation was manually revised to improve quality
and consistency. As the entity annotation is not
itself a target of extraction in the shared task, we
did not separately evaluate the consistency of the
revised entity mention annotation.
To assure that the quality and consistency of
the event annotation are as high as possible, ini-
4http://nactem.ac.uk/pathtext/
5http://nersuite.nlplab.org/
Item Train Devel Test Total
Documents 260 90 175 525
Words 53811 18579 35966 108356
Entities 7855 2734 5312 15901
Events 5992 2129 4004 12125
Modifications 317 80 174 571
Table 4: PC corpus statistics
tial event annotation was created entirely man-
ually, without automatic support. This annota-
tion effort was carried out using the BRAT anno-
tation tool (Stenetorp et al, 2012) by a group of
biologists in collaboration between NaCTeM and
KISTI. Following initial annotator training and re-
finement of guidelines based on the event type def-
initions provided by the reference ontologies, the
primary event annotation was created by three bi-
ologists. To evaluate and maintain annotation con-
sistency, a random 20% of documents were an-
notated redundantly by all annotators, and these
overlapping annotations were periodically evalu-
ated and differences in annotation were discussed
between the annotators and annotation coordina-
tors. Following initial annotation, a round of semi-
automatic consistency checks were applied using
BRAT. Evaluation of the redundantly annotated
documents using the primary task evaluation cri-
teria gave an inter-annotator agreement of 61.0%
in F-score. For the final corpus, the redundantly
annotated documents were evaluated separately by
an annotation coordinator to select the best of each
set.6
The overall statistics of the corpus are summa-
rized in Table 4. We note that the among the
previous BioNLP ST corpora, only the GENIA
(GE) task corpus has a larger number of annotated
events than the PC corpus.
4 Results
4.1 Participation
Two groups submitted final results to the PC
task, one from the National Centre for Text Min-
ing (NaCTeM) and one from the University of
Turku BioNLP group (TEES-2.1) (Table 5). Both
participants applied their well-established, state-
of-the-art event extraction systems, EventMine7
(Miwa et al, 2012) (NaCTeM) and the Turku
6This selection implies that the consistency of the event
annotation of the final corpus is expected to exceed the 61%
F-score of the IAA experiment. Consistency after selection
was not separately evaluated.
7http://nactem.ac.uk/EventMine/
71
NLP Events Other resources
Rank Team Org Word Parse Trig. Arg. Group. Modif. Corpora Other
1 NaCTeM 1NLP Snowball Enju, GDep SVM SVM SVM SVM (see text) triggers
2 TEES-2.1 1BI Porter McCCJ + SD SVM SVM SVM SVM GE hedge words
Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician,
NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Char-
niak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.
Team recall prec. F-score
NaCTeM 52.23 53.48 52.84
TEES-2.1 47.15 55.78 51.10
Table 6: Primary evaluation results
Event Extraction System8 (Bjo?rne et al, 2011)
(TEES). The two systems share the same over-
all architecture, a one-best pipeline with SVM-
based stages for event trigger detection, trigger-
argument relation detection, argument grouping
into event structures, and modification prediction.
The feature representations of both systems draw
on substructures of dependency-like representa-
tions of sentence syntax, derived from full parses
of input sentences. TEES applies the Charniak
and Johnson (2005) parser with the McClosky
(2009) biomedical model, converting the phrase-
structure parses into dependencies using the Stan-
ford tools (de Marneffe et al, 2006). By contrast,
EventMine uses a combination of the predicate-
argument structure analyses created by the deep
parser Enju (Miyao and Tsujii, 2008) and the out-
put of the the GDep best-first shift-reduce depen-
dency parser (Sagae and Tsujii, 2007). All three
parsers have models trained in part on the biomed-
ical domain GENIA treebank (Tateisi et al, 2005).
Interestingly, both systems make use of the GE
task data, but the application of EventMine ex-
tends on this considerably by applying a stacked
model (Miwa et al, 2013b) with predictions also
from models trained on the BioNLP ST 2011 EPI
and ID tasks (Pyysalo et al, 2012) as well as from
four corpora introduced outside of the shared tasks
by Thompson et al (2011), Pyysalo et al (2011),
Ohta et al (2011b) and Ohta et al (2011c).
4.2 Evaluation results
Table 6 summarizes the primary evaluation results.
The two systems demonstrate broadly similar per-
formance in terms of F-scores, with NaCTeM
achieving an 1.7% point higher overall result.
8http://jbjorne.github.io/TEES/
However, the systems show quite different per-
formance in terms of the precision/recall balance:
while the NaCTeM system has little difference
between precision and recall, TEES-2.1 shows a
clear preference for precision, with 8.6% lower re-
call than precision.
Results are shown separately for each event type
in Table 7. The results largely mirror the over-
all performance, with the NaCTeM system show-
ing better performance for 13 out of the 21 event
types present in the test data and more balanced
precision and recall than TEES-2.1, which em-
phasizes precision over recall for almost all event
types. Although the results do not include evalu-
ation of EventMine with a reduced set of stacked
models in training, the modest difference in per-
formance suggests that comprehensive use of pre-
viously released event resources in EventMine did
not confer a decisive advantage, perhaps in part
due to differences in the event definitions between
the PC task and previous resources.
Overall, the two systems appear quite similar
not only in architecture but also performance, with
the clearest systematic difference observed being
the different emphases on precision vs. recall. As
both systems are based on machine learning meth-
ods with real-valued outputs, it would be relatively
straightforward to use prediction confidences to
analyse performance over the entire precision-
recall curve instead of a single fixed point. Such
analysis could provide further insight into the rel-
ative strengths and weaknesses of these two sys-
tems.
5 Discussion
Although participation in this initial run of the PC
task was somewhat limited, the two participating
systems have been applied to a large variety of
event extraction tasks over the last years and have
shown consistently competitive performance with
the state of the art (Bjo?rne and Salakoski, 2011;
Miwa et al, 2012). It is thus reasonable to as-
sume that the higher performance achieved by the
72
NaCTeM TEES-2.1
Event recall prec. F-score recall prec. F-score
CONVERSION 34.33 35.48 34.90 35.82 42.86 39.02
PHOSPHORYLATION 62.46 55.94 59.02 53.40 66.00 59.03
DEPHOSPHORYLATION 45.00 56.25 50.00 35.00 77.78 48.28
ACETYLATION 69.57 72.73 71.11 82.61 76.00 79.17
DEACETYLATION 33.33 33.33 33.33 0.00 0.00 0.00
METHYLATION 42.86 60.00 50.00 57.14 80.00 66.67
DEMETHYLATION 100.00 100.00 100.00 100.00 100.00 100.00
UBIQUITINATION 52.94 64.29 58.06 58.82 76.92 66.67
DEUBIQUITINATION 100.00 100.00 100.00 100.00 100.00 100.00
LOCALIZATION 42.25 61.22 50.00 43.66 54.39 48.44
TRANSPORT 65.52 61.29 63.33 56.55 59.85 58.16
GENE EXPRESSION 90.65 83.15 86.74 84.55 79.39 81.89
TRANSCRIPTION 71.15 82.22 76.29 57.69 73.17 64.52
TRANSLATION 0.00 0.00 0.00 50.00 100.00 66.67
Simple-total 66.42 64.80 65.60 60.40 67.87 63.92
DEGRADATION 78.57 89.19 83.54 78.57 78.57 78.57
ACTIVATION 78.54 70.96 74.56 72.06 72.06 72.06
INACTIVATION 44.62 55.77 49.57 38.46 45.45 41.67
BINDING 64.96 47.30 54.74 53.96 53.96 53.96
DISSOCIATION 38.46 46.88 42.25 35.90 45.16 40.00
PATHWAY 84.91 75.50 79.93 70.94 75.50 73.15
General-total 69.07 62.69 65.72 61.16 65.74 63.37
REGULATION 33.33 33.97 33.65 29.73 39.51 33.93
POSITIVE REGULATION 35.49 42.81 38.81 34.51 45.45 39.23
NEGATIVE REGULATION 45.75 50.64 48.07 41.02 47.37 43.97
Regulation-total 37.73 42.79 40.10 35.17 44.76 39.39
Sub-total 53.47 53.96 53.72 48.23 56.22 51.92
NEGATION 24.52 35.87 29.13 25.16 41.30 31.27
SPECULATION 15.79 22.22 18.46 0.00 0.00 0.00
Modification-total 23.56 34.65 28.05 22.41 40.00 28.73
Total 52.23 53.48 52.84 47.15 55.78 51.10
Table 7: Primary evaluation results by event type.
task participants, a balanced F-score of 52.8%, is
a good estimate of the performance level that can
be attained for this task by present event extraction
technology.
The results achieved by the two systems are
broadly comparable to the best results achieved by
any system in similar previously introduced event
extraction tasks (Kim et al, 2012; Pyysalo et al,
2012). Given the novelty of the task domain and
reference resource and the broad selection of doc-
uments, we find the results highly encouraging re-
garding the applicability of event extraction tech-
nology to supporting the development, evaluation,
and maintenance of pathway models.
6 Conclusions
This paper presented the Pathway Curation (PC)
task, a main event extraction task of the BioNLP
ST 2013. The task was organized in collaboration
between groups with an interest in pathway cura-
tion with the aim of evaluating and advancing the
state of the art in event extraction toward methods
for developing, evaluating and maintaining formal
pathway models in representations such as SBML
and BioPAX. We introduced an event extraction
task setting with reference to pathway model stan-
dards and the Systems Biology Ontology, selected
a set of 525 publication abstracts relevant to spe-
cific model reactions, and created fully manual
73
event annotation marking over 12,000 event struc-
tures in the corpus.
Two participants in the BioNLP ST 2013 sub-
mitted final predictions to the PC task, applying
established, state-of-the-art event extraction sys-
tems, EventMine and the Turku Event Extrac-
tion System. Both systems achieved F-scores
over 50%, with the EventMine system achiev-
ing the best overall result of 52.8%. This level
of performance is broadly comparable with re-
sults achieved in comparable previously proposed
tasks, indicating that current event extraction tech-
nology is applicable to the projected pathway cu-
ration support tasks.
To allow the further development and evalua-
tion of event extraction methods for the task, the
PC task continues as an open challenge to all inter-
ested participants, with the annotated corpus data,
supporting resources, and evaluation tools avail-
able under open licenses from the task homepage,
http://2013.bionlp-st.org/
Acknowledgments
We would like to thank Yonghwa Jo, Hyeyeon
Choi, Jeong-Ik Lee and Ssang-Goo Cho of
Konkuk University for their contribution to the de-
velopment of the relevance judgment annotation
criteria. We also wish to thank Hyun Uk Kim,
Jinki Kim and Kyusang Hwang of KAIST for
their efforts in producing the PC task annotation.
This work is a part of joint research of KISTI and
NaCTeM, and partially supported by the Biotech-
nology and Biological Sciences Research Council
(BBSRC) [BB/G53025X/1].
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and Dou-
glas B. Kell. 2010. Event extraction for systems biology
by text mining the literature. Trends in Biotechnology,
28(7):381?390.
Michael Ashburner, Catherine A. Ball, Judith A. Blake,
David Botstein, Heather Butler, J. Michael Cherry, Al-
lan P. Davis, Kara Dolinski, et al 2000. Gene ontology:
tool for the unification of biology. Nature genetics, 25:25?
29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 183?191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio
Pahikkala, and Tapio Salakoski. 2011. Extracting contex-
tualized complex biological events with rich graph-based
feature sets. Computational Intelligence, 27(4):541?557.
Laurence Calzone, Ame?lie Gelay, Andrei Zinovyev, Franc?ois
Radvanyl, and Emmanuel Barillot. 2008. A comprehen-
sive modular map of molecular interactions in rb/e2f path-
way. Molecular systems biology, 4(1).
Etienne Caron, Samik Ghosh, Yukiko Matsuoka, Dariel
Ashton-Beaucage, Marc Therrien, Se?bastien Lemieux,
Claude Perreault, Philippe P Roux, and Hiroaki Kitano.
2010. A comprehensive map of the mtor signaling net-
work. Molecular systems biology, 6(1).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
n-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL?05, pages 173?180.
The UniProt Consortium. 2011. Ongoing and future devel-
opments at the universal protein resource. Nucleic Acids
Research, 39(suppl 1):D214?D219.
Peter Corbett and Ann Copestake. 2008. Cascaded classifiers
for confidence-based chemical named entity recognition.
BMC Bioinformatics, 9(Suppl 11):S4.
Me?lanie Courtot, Nick Juty, Christian Knu?pfer, Dagmar Wal-
temath, Anna Zhukova, Andreas Dra?ger, Michel Dumon-
tier, Andrew Finney, Martin Golebiewski, Janna Hastings,
et al 2011. Controlled vocabularies and semantics in sys-
tems biology. Molecular systems biology, 7(1).
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of
LREC, volume 6, pages 449?454.
Kirill Degtyarenko, Paula De Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, Alan Mcnaught, Rafael
Alca?ntara, Michael Darsow, Mickae?l Guedj, and Michael
Ashburner. 2008. Chebi: a database and ontology for
chemical entities of biological interest. Nucleic acids re-
search, 36(suppl 1):D344?D350.
Natalie C Duarte, Scott A Becker, Neema Jamshidi, Ines
Thiele, Monica L Mo, Thuy D Vo, Rohith Srivas, and
Bernhard ? Palsson. 2007. Global reconstruction of
the human metabolic network based on genomic and bib-
liomic data. Proceedings of the National Academy of Sci-
ences, 104(6):1777?1782.
Robert D. Finn, Jaina Mistry, John Tate, Penny Coggill, An-
dreas Heger, Joanne E. Pollington, O. Luke Gavin, Prasad
Gunasekaran, et al 2010. The Pfam protein families
database. Nucleic Acids Research, 38(suppl 1):D211?
D222.
Michael Hucka, Andrew Finney, Herbert M Sauro, Hamid
Bolouri, John C Doyle, Hiroaki Kitano, Adam P Arkin,
Benjamin J Bornstein, et al 2003. The systems biology
markup language (SBML): a medium for representation
and exchange of biochemical network models. Bioinfor-
matics, 19(4):524?531.
David M. Jessop, Sam Adams, Egon L. Willighagen, Lezan
Hawizy, and Peter Murray-Rust. 2011. Oscar4: a flexible
architecture for chemical text-mining. Journal of chemin-
formatics, 3(1):1?12.
Kazunari Kaizu, Samik Ghosh, Yukiko Matsuoka, Hisao
Moriya, Yuki Shimizu-Yoshida, and Hiroaki Kitano.
2010. A comprehensive molecular interaction map of the
budding yeast cell cycle. Molecular systems biology, 6(1).
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka, Yoshi-
masa Tsuruoka, Hiroaki Kitano, Sophia Ananiadou, and
Jun?ichi Tsujii. 2010. Pathtext: a text mining integra-
tor for biological pathway visualizations. Bioinformatics,
26(12):i374?i381.
74
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In Proceedings of
BioNLP?09.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Junichi Tsujii. 2011. Extracting bio-molecular
events from literature ? the bionlp?09 shared task. Com-
putational Intelligence, 27(4):513?540.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsujii,
Toshihisa Takagi, and Akinori Yonezawa. 2012. The
genia event and protein coreference tasks of the bionlp
shared task 2011. BMC bioinformatics, 13(Suppl 11):S1.
Hiroaki Kitano. 2002. Systems biology: a brief overview.
Science, 295(5560):1662?1664.
Martin Krallinger, Florian Leitner, and Alfonso Valencia.
2007. Assessment of the Second BioCreative PPI task:
Automatic Extraction of Protein-Protein Interactions. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of BioCreative II, pages 29?39.
Chen Li, Marco Donizelli, Nicolas Rodriguez, Harish
Dharuri, Lukas Endler, Vijayalakshmi Chelliah, Lu Li,
Enuo He, et al 2010. BioModels Database: An enhanced,
curated and annotated resource for published quantitative
kinetic models. BMC Systems Biology, 4:92.
Chen Li, Maria Liakata, and Dietrich Rebholz-Schuhmann.
2013. Biological network extraction from scientific litera-
ture: state of the art and challenges. Briefings in bioinfor-
matics.
Donna Maglott, Jim Ostell, Kim D. Pruitt, and Tatiana
Tatusova. 2005. Entrez gene: gene-centered information
at ncbi. Nucleic Acids Research, 33(suppl 1):D54.
David McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing. Ph.D.
thesis, Brown University.
Huaiyu Mi and Paul Thomas. 2009. PANTHER pathway: an
ontology-based pathway database coupled with data anal-
ysis tools. In Protein Networks and Pathway Analysis,
pages 123?140. Springer.
Huaiyu Mi, Anushya Muruganujan, Emek Demir, Yukiko
Matsuoka, Akira Funahashi, Hiroaki Kitano, and Paul D
Thomas. 2011. Biopax support in celldesigner. Bioinfor-
matics, 27(24):3437?3438.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the liter-
ature using domain adaptation and coreference resolution.
Bioinformatics, 28(13):1759?1765.
Makoto Miwa, Tomoko Ohta, Rafal Rak, Andrew Rowley,
Douglas B. Kell, Sampo Pyysalo, and Sophia Ananiadou.
2013a. A method for integrating and ranking the evidence
for biochemical pathways by mining reactions from text.
Bioinformatics. in press.
Makoto Miwa, Sampo Pyysalo, Tomoko Ohta, and Sophia
Ananiadou. 2013b. Wide coverage biomedical event
extraction using multiple partially overlapping corpora.
BMC bioinformatics, 14(1):175.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest mod-
els for probabilistic HPSG parsing. Computational Lin-
guistics, 34(1):35?80.
Kanae Oda and Hiroaki Kitano. 2006. A comprehensive
map of the toll-like receptor signaling network. Molecular
Systems Biology, 2(1).
Kanae Oda, Yukiko Matsuoka, Akira Funahashi, and Hiroaki
Kitano. 2005. A comprehensive pathway map of epider-
mal growth factor receptor signaling. Molecular systems
biology, 1(1).
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and Jun?ichi
Tsujii. 2008. New challenges for text mining: mapping
between text and manually curated pathways. BMC bioin-
formatics, 9(Suppl 3):S5.
Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Ju-
nichi Tsujii. 2011a. Pathway curation support as an infor-
mation extraction task. Proceedings of LBM?11.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and Jun?ichi
Tsujii. 2011b. Event extraction for dna methylation.
Journal of Biomedical Semantics, 2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c.
From pathways to biomolecular events: opportunities and
challenges. In Proceedings of BioNLP?11, pages 105?
113.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia
Ananiadou. 2012. Open-domain anatomical entity men-
tion detection. In Proceedings of DSSD?12, pages 27?36.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari Bjo?rne,
Filip Ginter, and Tapio Salakoski. 2008. Comparative
analysis of five protein-protein interaction corpora. BMC
Bioinformatics, 9(Suppl 3):S6.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun?ichi
Tsujii. 2011. Towards exhaustive event extraction for pro-
tein modifications. In Proceedings of BioNLP?11, pages
114?123.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sullivan,
Chunhong Mao, Chunxia Wang, Bruno Sobral, Jun?ichi
Tsujii, and Sophia Ananiadou. 2012. Overview of the id,
epi and rel tasks of bionlp shared task 2011. BMC bioin-
formatics, 13(Suppl 11):S2.
Rafal Rak, Andrew Rowley, William Black, and Sophia Ana-
niadou. 2012. Argo: an integrative, interactive, text
mining-based workbench supporting curation. Database:
The Journal of Biological Databases and Curation, 2012.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1044?1050.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. Brat:
a web-based tool for nlp-assisted text annotation. In Pro-
ceedings of EACL?12, pages 102?107.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Junichi
Tsujii. 2005. Syntax annotation for the genia corpus. In
Proceedings of IJCNLP, volume 5, pages 222?227.
Paul Thompson, Raheel Nawaz, John McNaught, and Sophia
Ananiadou. 2011. Enriching a biomedical event corpus
with meta-knowledge annotation. BMC Bioinformatics,
12(1):393.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg Haken-
berg, and Ulf Leser. 2010. A comprehensive benchmark
of kernel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6(7):e1000837, 07.
John Wilbur, Lawrence Smith, and Lorraine Tanabe. 2007.
BioCreative 2. Gene Mention Task. In L. Hirschman,
M. Krallinger, and A. Valencia, editors, Proceedings of
BioCreative II, pages 7?16.
75
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95?104,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using a Random Forest Classifier to recognise translations of biomedical
terms across languages
Georgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun?ichi Tsujii3 Sophia Ananiadou1,2
National Centre for Text Mining, University of Manchester, Manchester, UK1
School of Computer Science, University of Manchester, Manchester, UK2
Microsoft Research Asia, Beijing, China3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We present a novel method to recognise
semantic equivalents of biomedical terms
in language pairs. We hypothesise that
biomedical term are formed by seman-
tically similar textual units across lan-
guages. Based on this hypothesis, we
employ a Random Forest (RF) classifier
that is able to automatically mine higher
order associations between textual units
of the source and target language when
trained on a corpus of both positive and
negative examples. We apply our method
on two language pairs: one that uses the
same character set and another with a dif-
ferent script, English-French and English-
Chinese, respectively. We show that
English-French pairs of terms are highly
transliterated in contrast to the English-
Chinese pairs. Nonetheless, our method
performs robustly on both cases. We eval-
uate RF against a state-of-the-art align-
ment method, GIZA++, and we report a
statistically significant improvement. Fi-
nally, we compare RF against Support
Vector Machines and analyse our results.
1 Introduction
Given a term in a source language and term in a
target language the task of this paper is to classify
this pair as a translation or not. We investigate the
performance of the proposed classifier by apply-
ing it on a balanced classification problem, i.e. our
experimental datasets contain an equal number of
positive and negative examples. The proposed
classification model can be used as a component of
a larger system that automatically compiles bilin-
gual dictionaries of technical terms across lan-
guages. Bilingual dictionaries of terms are impor-
tant resources for many Natural Language Pro-
cessing (NLP) applications including Statistical
Machine Translation (SMT) (Feng et al, 2004;
Huang and Vogel, 2002; Wu et al, 2008), Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997) and Question Answering systems
(Al-Onaizan and Knight, 2002). Especially in the
biomedical domain, manually creating and more
importantly updating such resources is an expen-
sive process, due to the vast amount of neologisms,
i.e. newly introduced terms (Pustejovsky et al,
2001). The UMLS metathesaurus which is one the
most popular hub of multilingual resources in the
biomedical domain, contains technical terms in 21
languages that are linked together using a con-
cept identifier. In Spanish, the second most popu-
lar language in UMLS, only 16.44% of the 7.6M
English terms are covered while other languages
fluctuate between 0.0052% (for Hebrew terms) to
3.26% (for Japanese terms). Hence, these lex-
ica are far for complete and methods that semi-
automatically (i.e., in a post-processing step, cu-
rators can manually remove erroneous dictionary
entries) discover pairs of terms across languages
are needed to enrich such multilingual resources.
Our method can be applied to parallel, aligned cor-
pora, where we expect approximately the same,
balanced classification problem. However, in
comparable corpora the search space of candidate
alignments is of vast size, i.e., quadratic the the
size of the input data. To cope with this heavily
unbalanced classification problem, we would need
to narrow down the number of negative instances
before classification.
We hypothesise that there are language in-
dependent rules that apply to biomedical terms
across many languages. Often the same or simi-
lar textual units (e.g., morphemes and suffixes) are
concatenated to realise the same terms in different
languages. For example, Table 1 illustrates how
a morpheme expressing pain (ache in English) is
used to realise the same terms in English, Chinese
and French. The realisations of the term ?head-
95
English Morpheme: -ache Chinese Morpheme: ? French Morpheme: -mal
head-ache ?-? mal de te?te
back-ache ?-? mal au dos
ear-ache ??-? mal d?oreille
Table 1: An example of English, Chinese and French terms consisting of the same morphemes
ache? is expected to consist of the units for ?head?
and ?ache? regardless of the language of realisa-
tion. Hence, knowing the translations of ?head?
and ?ache? allows the reconstruction ?headache?
in a target language.
In our method, we use a Random Forest (RF) clas-
sifier (Breiman, 2001) to learn the underlying rules
according to which terms are being constructed
across languages. An RF is an ensemble of De-
cision Trees voting for the most popular class. RF
classifiers are popular in the biomedical domain
for various tasks: classification of microarray data
(D??az-Uriarte and De Andres, 2006), compound
classification in cheminformatics (Svetnik et al,
2003), classification of microRNA data (Jiang et
al., 2007) and protein-protein interactions in Sys-
tems Biology (Chen and Liu, 2005). In NLP, RF
classifiers have been used for: Language Mod-
elling (Xu and Jelinek, 2004) and semantic pars-
ing (Nielsen and Pradhan, 2004). To the best of
the authors? knowledge, this is the first attempt to
employ RF for identifying translation equivalents
of biomedical terms.
We prefer RF over other traditional machine learn-
ing approaches such as Support Vector Machines
(SVMs) for a number of reasons. Firstly, RF is
able to automatically construct correlation paths
from the feature space, i.e. decision rules that cor-
respond to the translation rules that we intend
to capture. Secondly, RF is considered one of
the most accurate classifier available (D??az-Uriarte
and De Andres, 2006; Jiang et al, 2007). Finally,
RF is reported to cope well with datasets where the
number of features is larger than the number of ob-
servations (D??az-Uriarte and De Andres, 2006). In
our dataset, the number of features is almost four
times more than that of the observations.
We represent pairs of terms using character gram
features (i.e., first order features). Such shal-
low features have been proven effective in a num-
ber of NLP applications including: Named En-
tity Recognition (Klein et al, 2003), Multilin-
gual Named Entity Transliteration (Klementiev
and Roth, 2006; Freitag and Khadivi, 2007) and
predicting authorship (Stamatatos, 2006). In ad-
dition, by selecting character n-grams instead of
word n-grams, one avoids to segment words in
Chinese which has been proven to be a challenging
topic (Sproat and Emerson, 2003). We evaluate
our proposed method on two datasets of biomed-
ical terms (English-French and English-Chinese)
that contain equal numbers of positive and neg-
ative instances. RF achieves higher classifica-
tion performance than baseline methods. To boost
SVM?s performance further, we used a second or-
der feature space to represent the data. It consists
of pairs of character grams that co-occur in trans-
lation pairs. In the second order feature space, the
performance of SVMs improved significantly.
The rest of the paper is structured as follows. In
Section 2, we present previous approaches in iden-
tifying translation equivalents of terms or named
entities. In Section 3, we define the classifica-
tion problem, we formulate the RF classifier and
we discuss the first and second order feature space
that we use to represent pairs of terms. In Sec-
tion 4, we show that RF achieves superior classi-
fication performance. In Section 5, we overview
our method and we discuss how it can be used to
compile large-scale bilingual dictionaries of terms
from comparable corpora.
2 Related Work
In this section, we review previous approaches
that exploit the internal structure of sequences to
align terms or named entities across languages.
(Klementiev and Roth, 2006; Freitag and Khadivi,
2007) use character gram features, similar to the
feature space that we propose in this paper, to train
discriminative, supervised models. Klementiev
and Roth (2006) introduce a supervised Percep-
tron model for English and Russian named enti-
ties. They construct a character gram feature space
as follows: firstly, they extract all distinct charac-
ter grams from both source and target named en-
tity. Then, they pair character grams of the source
named entity with character grams of the corre-
sponding target named entity into features. In or-
96
der to reduce the number of features, they link
only those character grams whose position offsets
in the source and target sequence differs by -1, 0
or 1. Freitag and Khadivi (2007) employ the same
character gram feature space but they do not con-
straint the included character-grams to their rela-
tive position offsets in the source and target se-
quence. The boolean features are defined for ev-
ery distinct character-grams observed in the data
of length k or shorter. Using this feature space
they train an Averaged Perceptron model, able to
incorporate an arbitrary number of features in the
input vectors, for English and Arabic named en-
tities. The above character gram based methods
mainly focused on aligning named entities of the
general domain, i.e. person names, locations, or-
ganizations, etc., that are transliterated, i.e. present
phonetic similarities, across languages.
SMT-based approaches built on top of existing
SMT frameworks to identify translation pairs of
terms (Tsunakawa et al, 2008; Wu et al, 2008).
Tsunakawa et al (2008), align terms between
a source language Ls and a target language Lt
using a pivot language Lp. They assume that
two bilingual dictionaries exist: from Ls to Lp
and from Lp to Lt. Then, they train GIZA++
(Och and Ney, 2003) on both directions and they
merge the resulting phrase tables into one table
between Ls and Lt, using grow-diag-final heuris-
tics (Koehn et al, 2007). Wu et al (2008), use
morphemes instead of words as translation units
to train a phrase based SMT system for technical
terms in English and Chinese. The use of shorter
lexical fragments, e.g. lemmas, stems and suf-
fixes, as translation units has reportedly reduced
the Out-Of-Vocabulary problem (Virpioja et al,
2007; Popovic and Ney, 2004; Oflazer and El-
Kahlout, 2007).
Hybrid methods exploit that a term or a named en-
tity can be translated in various ways across lan-
guages (Shao and Ng, 2004; Feng et al, 2004; Lu
and Zhao, 2006). For instance, person names are
usually translated by transliteration (i.e., words
exhibiting pronunciation similarities across lan-
guages, are likely to be mutual translations) while
technical terms are likely to be translated by
meaning (i.e., the same semantic units are used to
generate the translation of the term in the target
language). The resulting hybrid systems were re-
ported to perform at least as well as existing SMT
systems (Feng et al, 2004).
Lepage and Denoual (2005) presented an analog-
ical learning machine translation system as part
of the IWSLT task (Eck and Hori, 2005) that re-
quires no training process and it is able to achieve
state-of-the art performance. The core method
of their system models relationships between se-
quences of characters, e.g., sentences, phrases or
words, across languages using proportional analo-
gies, i.e., [a : b = c : d], ?a is to b as c is to d?, and
is able to solve unknown analogical equations,
i.e., [x : y = z :?] (Lepage, 1998). Analogical
learning has been proven effective in translating
unseen words (Langlais and Patry, 2007). Further-
more, analogical learning is reported to achieve a
better precision but a lower recall than a phrase-
based machine translation system when translating
medical terms (Langlais et al, 2009).
3 Methodology
Let em = (e1, ? ? ? , em) be an English term
consisting of m translation units and fn =
(f1, ? ? ? , fn) a French or Chinese term consist-
ing of n units. As translation units, we con-
sider character grams. We define a function f :
(em, fn) ?? {0, 1}:
f(em, fn) =
{
1, if em translates into fn
0, otherwise
The function can be learned by training a Random
Forest (RF) classifier1. Let N be the number of
training instances, |?| the total number of features,
i.e. the number of dimensions of the feature space,
|? | a predefined number of random decision trees
and |?| a predefined number of random features.
An RF classifier is defined as a collection of fully
grown decision tree classifiers, ?i(X) (Breiman,
2001):
RF = {?1(X), ? ? ? , ?? (X)}, X = (e
m, chn)
(1)
A pair of terms is classified as a translation pair
if the majority of the trees is voting for this class
label. Let I(?i(X)) be the vote of the ith tree
in the forest and avj?{0,1} the average number of
votes for class labels 0 (translation) and 1 (non-
translation). The function f of ? decision trees
can be written as the majority function:
f(em, chn) = Maj (I(?1(X)), ? ? ? , I(?? (X)))
=
?
1
2
??
1 I(?i(X)) + 1/2(?1)
r
?
?
(2)
1The WEKA implementation (Hall et al, 2009) of RF was
used for all experiments of this paper.
97
The majority function returns 1 if the majority
of I(?i(X)) is 1, or returns 0 if the majority of
I(?i(X)) is 0. Adding or subtracting 1/2 controls
whether a tie is resolved towards 1 or 0, respec-
tively. In RF ties are resolved randomly. To rep-
resent this, the negative unit (?1) is raised to a
randomly chosen positive integer r ? N+.
We tuned the RF classifier using 140 random
trees and |?| = log2 |?|+ 1 features as suggested
in Breiman (Breiman, 2001).
The RF mechanism that triggers term construction
rules across languages lies in the decision trees.
A RF grows a decision tree by selecting the most
informative feature, i.e. corresponding to the
lowest entropy, out of ? random features. For
each selected feature, a node is created and this
process is repeated for all ? random features of
the unprunned decision trees. In other words, the
process starts with the most informative feature
and builds association rules between all random
features. These are the construction rules that
we are interested in. Figure 1 illustrates a path
in one of the decision trees of an RF classifier
taken from the experiments we conducted on
the English-Chinese dataset. In only one of
thousands of branches of the forest, the classifier
is able to partially trigger the construction rule of
kinase, a type of enzyme, between English and
Chinese. The translation rule correctly associates
the English n-grams kin and as with their Chinese
translation ??. In addition, the translation rule
contains both positive and negative associations
between features. The English n-grams ing and
or are negatively correlated with the term kinase.
3.1 Feature Engineering
Each pair of terms is represented as a feature vec-
tor of character n-grams. We further define two
types of character n-gram features, namely first
order and second order. First order character n-
grams are boolean features that designate the oc-
currence of a corresponding character gram of pre-
defined length in the input term. These features are
monolingual, extracted separately from the source
and target term. The RF classifier is shown to ben-
efit from only monolingual features and achieves
the best observed performance. In contrast, SVMs
were shown not to perform well using the first or-
der feature space because they cannot directly as-
sociate the source with the target character grams.
To enhance the performance of SVMs, we con-
structed a second order feature space that contains
associations between first order features. A sec-
ond order feature is a tuple of a source and a tar-
get character gram that co-occur in one or more
translation pairs. Table 2 illustrates an example.
Second order character n-grams are multilingual
features and are defined over true translation pairs.
For this reason, we extract second order features
from the training data only.
In all experiments, the features were sorted in de-
creasing order of frequency of occurrence. We
trained a RF and two SVM classifiers, namely
linear-SVM and RBF-SVM, using a gradually in-
creasing number of features, always starting from
the top of the list. SMT frameworks cannot be
trained on an increasing number of features be-
cause each training instance needs to correspond
to at least one known translation unit (i.e., first or-
der features). Therefore, GIZA++ is trained on the
complete set of translation units.
4 Experiments
In this section, we discuss the employed datasets
of biomedical terms in English-French and
English-Chinese and three baseline methods. We
compare and discuss RF and SVMs trained on the
first order and second order features. Finally, we
report results of all classification methods evalu-
ated on the same datasets.
4.1 Datasets
For our experiments, we used an online bilin-
gual dictionary2 for English-Chinese terms and the
UMLS metathesaurus3 for English-French terms.
The former contains 31, 700 entries while the lat-
ter is a much larger dictionary containing 84, 000
entries. For training, we used the same number of
instances for both language pairs (i.e., 21, 000 en-
tries) in order not to bias the performance towards
the larger English-French dataset. The remain-
ing instances were used for testing (i.e., 10, 7000
and 63, 000 English-Chinese and English-French
respectively). In the case where a source term cor-
responded to more that one target terms according
to the seed dictionary, we randomly selected only
one translation. Negative instances were created
by randomly matching non-translation pairs of
terms. Since we are dealing with a balanced clas-
2www2.chkd.cnki.net/kns50/
3nlm.nih.gov/research/umls
98
Figure 1: Example of a term construction rule as a branch in a decision tree.
Input pair of English-French terms : (e1, e2, e3, f1, f2, f3)
English first order French first order Second order
?1(e1, e2) ?1(f1, f2) ?1(e1e2, f1f2), ?1(e1e2, f2f3)
?1(e2, e3) ?1(f2, f3) ?1(e2e3, f1f2), ?1(e2e3, f2f3)
Table 2: Example of first and second order features using a predefined n-gram size of 2.
sification problem, we created as many negative
instances as the positive ones in all our datasets.
In all experiments we performed a 3-fold cross-
validation.
4.2 Baselines
We evaluated RF against three classification meth-
ods, namely SVMs, GIZA++ and a Levenshtein
distance-based classifier.
SVMs coordinate a hyperplane in the hyperspace
defined by the features to best separate the posi-
tive and negative instances, i.e. aligned from non-
aligned pairs. In contrast to RF, SVMs do not sup-
port building association rules between features,
i.e., translation units, which in our task seems to be
a deficiency. SVMs produce one final association
rule, i.e. the classification boundary which sepa-
rates positive from negative examples. Its abil-
ity to distinguish aligned from non-aligned pair
of terms depends on how separable the two clus-
ters are. We evaluated several settings for the
SVM classifier. Apart from the default linear ker-
nel function, we applied a radial basis function,
i.e. RBF-SVM. RBF-SVM uses the kernel trick to
project the instances in a higher dimensional space
to better separate the two clusters. While tuning
the SVM?s classification cost C, we observed op-
timal performance for a value of 100. Secondly,
we seeded the association rules of translation units
to the SVM classifier by creating a second or-
der feature space, discussed in detail in section
3.1. We employed the LIBSVM implementation
(Chang and Lin, 2011) of SVMs using both the
linear and RBF kernels.
The second baseline method is GIZA++, an
open source implementation of the 5 IBM-models
(Brown et al, 1993). GIZA++ is traditionally
trained on a bilingual, parallel corpus of aligned
sentences and estimates the probability P (s|t) of a
source translation unit (typically a word), s, given
a target unit t. To apply GIZA++ on our dataset,
we consider the list of terms as parallel sentences.
GIZA++, trained on a list of terms, estimates
the alignment probability of English-Chinese and
English-French textual units, i.e. character n-
grams. Each entry i, j in the translation table
is the probability P (si|tj), where si and tj are
the source and target character n-grams in row i
and column j, respectively. Further details about
training a SMT toolkit for aligning technical terms
can be found in (Tsunakawa et al, 2008; Freitag
and Khadivi, 2007; Wu et al, 2008). After train-
ing GIZA++ we estimate the posterior probabil-
ity P (cfn|em) that a test, Chinese or French term
cfn = {cf1, ? ? ? , cfn} is aligned with a given En-
glish term em = {e1, ? ? ? , em} as follows:
p(cfn|em) = n?m
n?
i=1
m?
j=1
P (cfi|ej) (3)
A threshold ? was defined to classify a pair of
terms into translations or non-translations:
f(em, cfn) =
{
1, if p(cfn|em) ? ?
0, otherwise
(4)
We experimented with different values of ?
(greedy search) and we selected a value that max-
imizes classification performance.
In order to estimate how phonetically similar the
two language pairs are, we employed a third base-
99
(a) English-French dataset (b) English-Chinese dataset
Figure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the first
order dataset
line method that uses the Edit/Levenshtein dis-
tance of pairs of terms to classify instances as
translations or not. The Levenshtein distance is
defined as the minimum edit operations, i.e., inser-
tion, deletions and substitution, required to trans-
form one sequence of characters to another. We
cannot directly calculate the Levenshtein distance
between English-Chinese pairs of terms since the
two languages are using different scripts. There-
fore, before we applied the Levenshtein distance-
based classifier, we converted the Chinese terms
to their pinyin form, i.e., Romanization system of
Chinese characters. As with GIZA++, we selected
a threshold ? that maximizes the performance of
the classifier.
4.3 Results
We hypothesise that a RF classifier is able to form
association paths between first order features. We
also have the theoretical intuition that SVM clas-
sifiers are not able to form such association paths.
As a result, we expect limited performance on the
first order feature set, because it does not contain
any associations among character grams.
Figure 2 shows the F-Score achieved by RF, linear-
SVM, RBF-SVM, GIZA++ and Levenshtein/Edit
distance-based classifier on the English-French
and English-Chinese datasets. RF and SVMs are
trained on an increasing number of features. The
behaviour of the classifiers is approximately the
same in both datasets. Performance is greater on
the English-French dataset since English is more
similar to French than to Chinese.
We also observe that linear-SVM and RBF-SVM
do not behave consistently. RBF-SVM?s perfor-
mance quickly climbs to a maximum and after-
wards it declines while linear-SVM?s performance
is constantly increasing until it balances to a very
high error rate, almost corresponding to random
classification. The linear-SVM classifier performs
poorly using first order features only, indicating
that this feature space is non-linearly separable,
i.e. there exists no hyperplane that separates trans-
lation from non-translation instances. Contrary,
RBF-SVM is able to construct a higher dimen-
sional space by applying the kernel trick so as
to take full advantage of a small number of fre-
quent and informative first order features. In this
higher dimensional space of few but informative
first order features, the RBF-SVM classifier coor-
dinates a hyperplane that effectively separates pos-
itive from negative instances. However, increas-
ing the number of features introduces noise that
affects the performance.
The RF is able to profit from larger sets of first
order features; thus, its performance is continu-
ously increasing until it stabilises at 6, 000 fea-
tures. The branches of the decision trees are shown
to manage features correctly to construct most of
the translation rules. Increasing the size of the fea-
ture space minimises the classification error, be-
cause more translation rules that generalize well
on unseen data are constructed.
The bilingual dictionary that we use for our
experiments contains heterogeneous biomedical
terms of diverse semantic categories. For ex-
ample, our data-set contains common medical
terms such as Intellectual Products (e.g. Pain
Management, prise en charge de la douleur, ?
???) or complex biological concepts such
as Enzymes (e.g. homogentisate 1,2-dioxygenase,
100
(a) English-French dataset (b) English-Chinese dataset
Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second
order dataset
English-French pairs English-Chinese pairs
P R F1 P R F1
GIZA++ 0.901 0.826 0.862 0.907 0.742 0.816
Levenshtein Distance 0.762 0.821 0.791 0.501 0.990 0.668
SVM -RBFsecond-order 0.946 0.884 0.914 0.750 0.899 0.818
Linear-SVMsecond-order 0.866 0.887 0.8763 0.765 0.893 0.824
RFfirst-order 0.962 0.874 0.916 0.779 0.940 0.851
Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance
acide homogentisique-oxydase, ???1,2-??
?). Therefore, we would expect poor perfor-
mance of the supervised methods using only a
small portion of the total set of first order features
due to the high diversity of the terms. For exam-
ple the morpheme ache/ mal/ ? is more frequent
in Disease or Syndrome named entities rather than
Enzyme named entities. However, the results indi-
cate that RF can generalize well on heterogeneous
terms. Figure 2 shows that the RF classifier out-
performs SMT based methods, using only 1000
features.
The Levenshtein distance-based classifier per-
forms considerably better in the English-French
dataset than in English-Chinese. In fact, its best
performance for the English-Chinese dataset is
achieved when classifying every pair of terms as
a translation, i.e. 100% recall but 50% precision.
In a second experiment, we attempted to explore
whether the performance of SVMs can be im-
proved by providing cross-language association
features. We employed the second order feature
set discussed in subsection 3.1. We used a constant
number of 6, 000 first order features, the num-
ber of features that achieved maximum F-Score
for RF in the previous experiment. Besides these
first order features, we added an increasing num-
ber of second order ones. Figure 3 shows the F-
Score curves of the RF, linear-SVM, RBF-SVM,
GIZA++ and Levenshtein distance using this fea-
ture space.
We observe that second order features improved
the performance of both SVMs considerably. In
contrast to the previous experiment, the two SVMs
present consistent bevaviour. Interestingly, the
performance of the RF slightly decreased when
using a small number of second order features.
A possible explanation of this behaviour is that
the second order associative features added noise,
since the RF had already formed the association
rules from first order features. In addition, for m
English and n Chinese or French first order fea-
tures there were m ? n possible combinations of
second order features as explained in Subsection
3.1. Hence, there was a large number of second
order features that we excluded from the train-
ing process. Consequently, decision tree branches
were populated with incomplete association rules
while the RF was able to form these associa-
tions automatically. Nevertheless, as more sec-
ond order features were added, more association
rules were explored and the RF performance in-
101
creased. Table 3 summarises the highest perfor-
mance achieved by the RF, SVMs, GIZA++ and
Levenshtein distance all trained and tested on the
same dataset. The resulting performance of the RF
compared with GIZA++ is statistically significant
(p < 0.0001) in all experiments. Comparing the
RF with the SVMs, we note that in the English-
French dataset, the performance of the SVM-RBF
is approximately the same with the performance
of our proposed method. However, this comes
with a cost. Firstly, SVMs can possibly achieve
a comparable performance to the RF when us-
ing multilingual, second order features. In con-
trast, our experiments show that RF benefit from
monolingual, first order features only. Secondly,
SVMs need a large number of additional multi-
lingual features, (6.000 second order features or
more) to perform similarly to RF. As a conse-
quence, the resulting models of the SVM classi-
fiers are more complex. We measured the aver-
age time needed by the two classifiers to decide
for a single pair of terms. The RF is approx-
imately 30 times faster than SVMs (on average
0.010 and 0.292 seconds, respectively). Finally,
in the English-Chinese dataset the RF performed
significantly better than both SVMs.
5 Discussion And Future Work
In this paper, we presented a novel classification
method that uses Random Forest (RF) to recognise
translations of biomedical terms across languages.
Our approach is based on the hypothesis that in
many languages, there exist some rules for com-
bining textual units, e.g. n-grams, to form biomed-
ical terms. Based on this assumption, we de-
fined a first order feature space of character grams
and demonstrated that an RF classifier is able to
discover such cross language translation rules for
terms. We experimented with two diverse lan-
guage pairs: English-French and English-Chinese.
In the former case, pairs of terms exhibit high pho-
netic similarity while in the latter case they do not.
Our results showed that the proposed method per-
forms robustly in both cases and achieves a signif-
icantly better performance than GIZA++. We also
evaluated Support Vector Machines (SVM) clas-
sifiers on the same first order feature space and
showed that they fail to form translation rules in
both language pairs, possibly because it cannot
associate first order features with each other suc-
cessfully. We attempted to boost the performance
of the SVM classifier by adding association evi-
dence of textual units to the features. We extracted
second order features from the training data and
we defined a new feature set consisting of both first
order and second order features. In this feature
space, the performance of the SVMs improved sig-
nificantly.
In addition to this, we observe from the reported
experiments that RF achieves a better F-Score per-
formance than GIZA++ in all datasets. Nonethe-
less, GIZA++ presents a better precision (but
lower recall) in one dataset, i.e., English/Chinese.
Based on this observation we plan to investigate
the performance of a hybrid system combining RF
with MT approaches.
One trivial approach to apply the proposed method
for compiling large-scale bilingual dictionaries of
terms from comparable corpora would be to di-
rectly classify all possible pairs of terms into
translations or non-translations. However, in
comparable corpora, the size of the search space
is quadratic to the input data. Therefore, the clas-
sification task is much more challenging since the
distribution of positive and negative instances is
highly skewed. To cope with the vast search space
of comparable corpora, we plan to incorporate
context-based approaches with the RF classifica-
tion method. Context-based approaches, such as
distributional vector similarity (Fung and McKe-
own, 1997; Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008), can be used to limit the
number of candidate translations by filtering out
pairs of terms with low contextual similarity.
Finally, the proposed method can be also used to
online augment the phrase table of Statistical Ma-
chine Translation (SMT) in order to better han-
dle the Out-of-Vocabulary problem i.e. inability
to translate textual units that consist of one or
more words and do not occur in the training data
(Habash, 2008).
Acknowledgements
The work described in this paper is partially
funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
102
References
Y. Al-Onaizan and K. Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 400?408. Association for Computational Lin-
guistics.
L. Ballesteros and W.B. Croft. 1997. Phrasal trans-
lation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Fo-
rum, volume 31, pages 84?91. ACM.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3):27.
X.W. Chen and M. Liu. 2005. Prediction of protein?
protein interactions using random decision forest
framework. Bioinformatics, 21(24):4394?4400.
R. D??az-Uriarte and S.A. De Andres. 2006. Gene se-
lection and classification of microarray data using
random forest. BMC bioinformatics, 7(1):3.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?22.
D. Feng, Y. Lv, and M. Zhou. 2004. A new approach
for english-chinese named entity alignment. In Em-
pirical Methods in Natural Language Processing,
pages 372?379.
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247.
P. Fung and K. McKeown. 1997. A technical word-
and term-translation aid using noisy parallel cor-
pora across language groups. Machine Translation,
12(1):53?87.
N. Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons
from monolingual corpora. Proceedings of ACL-08:
HLT, pages 771?779.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
F. Huang and S. Vogel. 2002. Improved named en-
tity translation and bilingual named entity extrac-
tion. In International Conference on Multimodal In-
teraction, pages 253?258. IEEE.
P. Jiang, H. Wu, W. Wang, W. Ma, X. Sun, and Z. Lu.
2007. Mipred: classification of real and pseudo
microrna precursors using random forest prediction
model with combined features. Nucleic acids re-
search, 35(suppl 2):W339?W344.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning.
2003. Named entity recognition with character-level
models. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL, pages
180?183. Association for Computational Linguis-
tics.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 817?
824. Association for Computational Linguistics.
P. Koehn and K. Knight. 2002. Learning a transla-
tion lexicon from monolingual corpora. In Proceed-
ings of the ACL-02 workshop on Unsupervised lex-
ical acquisition-Volume 9, pages 9?16. Association
for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
Proceedings of EMNLP-CoNLL, pages 877?886.
Philippe Langlais, Franc?ois Yvon, and Pierre Zweigen-
baum. 2009. Improvements in analogical learning:
application to translating multi-terms of the medical
domain. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 487?495. Association
for Computational Linguistics.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 728?734. Association for Computational Lin-
guistics.
M. Lu and J. Zhao. 2006. Multi-feature based chinese-
english named entity extraction from comparable
corpora. pages 131?141.
103
R.D. Nielsen and S. Pradhan. 2004. Mixing weak
learners in semantic parsing. In Empirical Methods
in Natural Language Processing.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
K. Oflazer and I.D. El-Kahlout. 2007. Exploring
different representational units in english-to-turkish
statistical machine translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 25?32. Association for Computational
Linguistics.
Maja Popovic and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference
on Language Resources and Evaluation (LREC),
pages 1585?1588, Lisbon,Portugal.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
and M. Morrell. 2001. Automatic extraction
of acronym-meaning pairs from medline databases.
Studies in health technology and informatics,
(1):371?375.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
L. Shao and H.T. Ng. 2004. Mining new word trans-
lations from comparable corpora. In Proceedings
of the 20th international conference on Computa-
tional Linguistics, page 618. Association for Com-
putational Linguistics.
R. Sproat and T. Emerson. 2003. The first international
chinese word segmentation bakeoff. In Proceedings
of the second SIGHAN workshop on Chinese lan-
guage processing-Volume 17, pages 133?143. Asso-
ciation for Computational Linguistics.
Efstathios Stamatatos. 2006. Ensemble-based author
identification using character n-grams. In In Proc.
of the 3rd Int. Workshop on Textbased Information
Retrieval, pages 41?46.
V. Svetnik, A. Liaw, C. Tong, J.C. Culberson, R.P.
Sheridan, and B.P. Feuston. 2003. Random forest:
a classification and regression tool for compound
classification and qsar modeling. Journal of chemi-
cal information and computer sciences, 43(6):1947?
1958.
T. Tsunakawa, N. Okazaki, and J. Tsujii. 2008.
Building bilingual lexicons using lexical translation
probabilities via pivot languages. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, may.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsu-
pervised manner. Machine Translation Summit XI,
2007:491?498.
X. Wu, N. Okazaki, T. Tsunakawa, and J. Tsujii. 2008.
Improving English-to-Chinese Translation for Tech-
nical Terms Using Morphological Information. In
AMTA-2008. MT at work: Proceedings of the Eighth
Conference of the Association for Machine Trans-
lation in the Americas, pages 202?211, Waikiki,
Hawai?i, October.
P. Xu and F. Jelinek. 2004. Random forests in lan-
guage modeling. In Empirical Methods in Natural
Language Processing, pages 325?332. Association
for Computational Linguistics.
104
