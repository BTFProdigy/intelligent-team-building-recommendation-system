Proceedings of the 43rd Annual Meeting of the ACL, pages 565?572,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Instance-based Sentence Boundary Determination by Optimization for
Natural Language Generation
Shimei Pan and James C. Shaw
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
{shimei,shawjc}@us.ibm.com
Abstract
This paper describes a novel instance-
based sentence boundary determination
method for natural language generation
that optimizes a set of criteria based on
examples in a corpus. Compared to exist-
ing sentence boundary determination ap-
proaches, our work offers three signifi-
cant contributions. First, our approach
provides a general domain independent
framework that effectively addresses sen-
tence boundary determination by balanc-
ing a comprehensive set of sentence com-
plexity and quality related constraints.
Second, our approach can simulate the
characteristics and the style of naturally
occurring sentences in an application do-
main since our solutions are optimized
based on their similarities to examples
in a corpus. Third, our approach can
adapt easily to suit a natural language gen-
eration system?s capability by balancing
the strengths and weaknesses of its sub-
components (e.g. its aggregation and re-
ferring expression generation capability).
Our final evaluation shows that the pro-
posed method results in significantly bet-
ter sentence generation outcomes than a
widely adopted approach.
1 Introduction
The problem of sentence boundary determination in
natural language generation exists when more than
one sentence is needed to convey multiple concepts
and propositions. In the classic natural language
generation (NLG) architecture (Reiter, 1994), sen-
tence boundary decisions are made during the sen-
tence planning stage in which the syntactic struc-
ture and wording of sentences are decided. Sentence
boundary determination is a complex process that
directly impacts a sentence?s readability (Gunning,
1952), its semantic cohesion, its syntactic and lex-
ical realizability, and its smoothness between sen-
tence transitions. Sentences that are too complex are
hard to understand, so are sentences lacking seman-
tic cohesion and cross-sentence coherence. Further
more, bad sentence boundary decisions may even
make sentences unrealizable.
To design a sentence boundary determination
method that addresses these issues, we employ an
instance-based approach (Varges and Mellish, 2001;
Pan and Shaw, 2004). Because we optimize our so-
lutions based on examples in a corpus, the output
sentences can demonstrate properties, such as simi-
lar sentence length distribution and semantic group-
ing similar to those in the corpus. Our approach
also avoids problematic sentence boundaries by op-
timizing the solutions using all the instances in the
corpus. By taking a sentence?s lexical and syntac-
tic realizability into consideration, it can also avoid
sentence realization failures caused by bad sentence
boundary decisions. Moreover, since our solution
can be adapted easily to suit the capability of a natu-
ral language generator, we can easily tune the algo-
rithm to maximize the generation quality. To the best
of our knowledge, there is no existing comprehen-
sive solution that is domain-independent and pos-
sesses all the above qualities. In summary, our work
offers three significant contributions:
1. It provides a general and flexible sentence
565
boundary determination framework which
takes a comprehensive set of sentence com-
plexity and quality related criteria into consid-
eration and ensures that the proposed algorithm
is sensitive to not only the complexity of the
generated sentences, but also their semantic co-
hesion, multi-sentence coherence and syntactic
and lexical realizability.
2. Since we employ an instance-based method,
the proposed solution is sensitive to the style
of the sentences in the application domain in
which the corpus is collected.
3. Our approach can be adjusted easily to suit
a sentence generation system?s capability and
avoid some of its known weaknesses.
Currently, our work is embodied in a multimodal
conversation application in the real-estate domain in
which potential home buyers interact with the sys-
tem using multiple modalities, such as speech and
gesture, to request residential real-estate informa-
tion (Zhou and Pan, 2001; Zhou and Chen, 2003;
Zhou and Aggarwal, 2004). After interpreting the
request, the system formulates a multimedia pre-
sentation, including automatically generated speech
and graphics, as the response (Zhou and Aggarwal,
2004). The proposed sentence boundary determi-
nation module takes a set of propositions selected
by a content planner and passes the sentence bound-
ary decisions to SEGUE (Pan and Shaw, 2004), an
instance-based sentence generator, to formulate the
final sentences. For example, our system is called
upon to generate responses to a user?s request: ?Tell
me more about this house.? Even though not all of
the main attributes of a house (more than 20) will be
conveyed, it is clear that a good sentence boundary
determination module can greatly ease the genera-
tion process and improve the quality of the output.
In the rest of the paper, we start with a discussion
of related work, and then describe our instance-base
approach to sentence boundary determination. Fi-
nally, we present our evaluation results.
2 Related Work
Existing approaches to sentence boundary determi-
nation typically employ one of the following strate-
gies. The first strategy uses domain-specific heuris-
tics to decide which propositions can be combined.
For example, Proteus (Davey, 1979; Ritchie, 1984)
produces game descriptions by employing domain-
specific sentence scope heuristics. This approach
can work well for a particular application, however,
it is not readily reusable for new applications.
The second strategy is to employ syntactic, lex-
ical, and sentence complexity constraints to con-
trol the aggregation of multiple propositions (Robin,
1994; Shaw, 1998). These strategies can generate
fluent complex sentences, but they do not take other
criteria into consideration, such as semantic cohe-
sion. Further more, since these approaches do not
employ global optimization as we do, the content of
each sentence might not be distributed evenly. This
may cause dangling sentence problem (Wilkinson,
1995).
Another strategy described in Mann and
Moore(1981) guided the aggregation process by
using an evaluation score that is sensitive to the
structure and term usage of a sentence. Similar to
our approach, they rely on search to find an optimal
solution. The main difference between this approach
and ours is that their evaluation score is computed
based on preference heuristics. For example, all
the semantic groups existing in a domain have to
be coded specifically in order to handle semantic
grouping. In contrast, in our framework, the score is
computed based on a sentence?s similarity to corpus
instances, which takes advantage of the naturally
occurring semantic grouping in the corpus.
Recently, Walker (2002) and Stent (2004) used
statistical features derived from corpus to rank gen-
erated sentence plans. Because the plan ranker was
trained with existing examples, it can choose a plan
that is consistent with the examples. However, de-
pending on the features used and the size of the train-
ing examples, it is unclear how well it can capture
patterns like semantic grouping and avoid problems
likes dangling sentences.
3 Examples
Before we describe our approach in detail, we start
with a few examples from the real-estate domain
to demonstrate the properties of the proposed ap-
proach.
First, sentence complexity impacts sentence
boundary determination. As shown in Table 1, af-
ter receiving a user?s request (U1) for the details of a
house, the content planner asked the sentence plan-
ner to describe the house with a set of attributes in-
cluding its asking price, style, number of bedrooms,
number of bathrooms, square footage, garage, lot
size, property tax, and its associated town and school
566
Example Turn Sentence
E1 U1 Tell me more about this house
S1 This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial
with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk
and in the Byram Hills school district.
S2 This is a 1 million dollar house. This is a 3 bedroom house. This is a 2 bathroom
house. This house has 2000 square feet. This house has 2 acres of land.
This house has 2 car garage. This is a colonial house. The annual taxes are 8000 dollars.
This house is in Armonk. This house is in the Byram Hills school district.
S3 This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk
with 2 acres of land. The asking price is 1 million dollar and the annual taxes
are 8000 dollars. The house is located in the Byram Hills School District.
E2 S4 This is a 1 million dollar 3 bedroom house. This is a 2 bathroom house with
annual taxes of 8000 dollars.
S5 This is a 3 bedroom and 2 bathroom house. Its price is 1 million dollar and
its annual taxes are 8000 dollars.
E3 S6 The tax rate of the house is 3 percent.
S7 The house has an asphalt roof.
E4 S8 This is a 3 bedroom, 2 bathroom colonial with 2000 square feet and 2 acres of land.
S9 The house has 2 bedrooms and 3 bathrooms. This house is a colonial.
It has 2000 square feet. The house is on 2 acres of land.
Table 1: Examples
district name. Without proper sentence boundary
determination, a sentence planner may formulate a
single sentence to convey all the information, as in
S1. Even though S1 is grammatically correct, it
is too complex and too exhausting to read. Simi-
larly, output like S2, despite its grammatical correct-
ness, is choppy and too tedious to read. In contrast,
our instance-based sentence boundary determination
module will use examples in a corpus to partition
those attributes into several sentences in a more bal-
anced manner (S3).
Semantic cohesion also influences the quality of
output sentences. For example, in the real-estate
domain, the number of bedrooms and number of
bathrooms are two closely related concepts. Based
on our corpus, when both concepts appear, they al-
most always conveyed together in the same sen-
tence. Given this, if the content planner wants to
convey a house with the following attributes: price,
number of bedrooms, number of bathrooms, and
property tax, S4 is a less desirable solution than S5
because it splits these concepts into two separate
sentences. Since we use instance-based sentence
boundary determination, our method generates S5 to
minimize the difference from the corpus instances.
Sentence boundary placement is also sensitive to
the syntactic and lexical realizability of grouped
items. For example, if the sentence planner asks the
surface realizer to convey two propositions S6 and
S7 together in a sentence, a realization failure will
be triggered because both S6 and S7 only exist in
the corpus as independent sentences. Since neither
of them can be transformed into a modifier based on
the corpus, S6 and S7 cannot be aggregated in our
system. Our method takes a sentence?s lexical and
syntactic realizability into consideration in order to
avoid making such aggregation request to the sur-
face realizer in the first place.
A generation system?s own capability may also
influence sentence boundary determination. Good
sentence boundary decisions will balance a system?s
strengths and weaknesses. In contrast, bad decisions
will expose a system?s venerability. For example, if
a sentence generator is good at performing aggre-
gations and weak on referring expressions, we may
avoid incoherence between sentences by preferring
aggregating more attributes in one sentence (like in
S8) rather than by splitting them into multiple sen-
tences (like in S9).
In the following, we will demonstrate how our ap-
proach can achieve all the above goals in a unified
instance-based framework.
4 Instance-based boundary determination
Instance-based generation automatically creates
sentences that are similar to those generated by hu-
mans, including their way of grouping semantic con-
tent, their wording and their style. Previously, Pan
and Shaw (2004) have demonstrated that instance-
based learning can be applied successfully in gen-
erating new sentences by piecing together existing
words and segments in a corpus. Here, we want to
demonstrate that by applying the same principle, we
can make better sentence boundary decisions.
567
The key idea behind the new approach is to find a
sentence boundary solution that minimizes the ex-
pected difference between the sentences resulting
from these boundary decisions and the examples in
the corpus. Here we measure the expected differ-
ence based a set of cost functions.
4.1 Optimization Criteria
We use three sentence complexity and quality re-
lated cost functions as the optimization criteria: sen-
tence boundary cost, insertion cost and deletion cost.
Sentence boundary cost (SBC): Assuming P is
a set of propositions to be conveyed and S is a col-
lection of example sentences selected from the cor-
pus to convey P . Then we say P can be realized
by S with a sentence boundary cost that is equal to
(|S| ? 1) ? SBC in which |S| is the number of sen-
tences and SBC is the sentence boundary cost. To
use a specific example from the real-estate domain,
the input P has three propositions:
p
1
. House1 has-attr (style=colonial).
p
2
. House1 has-attr(bedroom=3).
p
3
. House1 has-attr(bathroom=2).
One solution, S, contains 2 sentences:
s
1
. This is a 3 bedroom, 2 bathroom house.
s
2
. This is a colonial house.
Since only one sentence boundary is involved, S is a
solution containing one boundary cost. In the above
example, even though both s
1
and s
2
are grammati-
cal sentences, the transition from s
1
to s
2
is not quite
smooth. They sound choppy and disjointed. To pe-
nalize this, whenever there is a sentence break, there
is a SBC. In general, the SBC is a parameter that is
sensitive to a generation system?s capability such as
its competence in reference expression generation.
If a generation system does not have a robust ap-
proach for tracking the focus across sentences, it is
likely to be weak in referring expression generation
and adding sentence boundaries are likely to cause
fluency problems. In contrast, if a generation sys-
tem is very capable in maintaining the coherence be-
tween sentences, the proper sentence boundary cost
would be lower.
Insertion cost: Assume P is the set of propo-
sitions to be conveyed, and Ci is an instance in
the corpus that can be used to realize P by insert-
ing a missing proposition pj to Ci, then we say P
can be realized using Ci with an insertion cost of
icost(CH , pj), in which CH is the host sentence in
the corpus containing proposition pj . Using an ex-
ample from our real-estate domain, assume the input
P=(p
2
, p
3
, p
4
), where
p
4
. House1 has-attr (square footage=2000).
Assume Ci is a sentence selected from the cor-
pus to realize P : ?This is 3 bedroom 2 bathroom
house?. Since Ci does not contain p4, p4 needs to
be added. We say that P can be realized using Ci
by inserting a proposition p
4
with an insertion cost
of icost(CH , p4), in which CH is a sentence in the
corpus such as ?This is a house with 2000 square
feet.?
The insertion cost is influenced by two main fac-
tors: the syntactic and lexical insertability of the
proposition pj and a system?s capability in aggre-
gating propositions. For example, if in the corpus,
the proposition pj is always realized as an indepen-
dent sentence and never as a modifier, icost(?, pj)
should be extremely high, which effectively pro-
hibit pj from becoming a part of another sen-
tence. icost(?, pj) is defined as the minimum in-
sertion cost among all the icost(CH , pj). Currently
icost(CH , pj) is computed dynamically based on
properties of corpus instances. In addition, since
whether a proposition is insertable depends on how
capable an aggregation module can combine propo-
sitions correctly into a sentence, the insertion cost
should be assigned high or low accordingly.
Deletion cost: Assume P is a set of input proposi-
tions to be conveyed and Ci is an instance in the cor-
pus that can be used to convey P by deleting an un-
needed proposition pj in Ci. Then, we say P can be
realized using Ci with a deletion cost dcost(Ci, pj).
As a specific example, assuming the input is P=(p
2
,
p
3
, p
4
), Ci is an instance in the corpus ?This is a
3 bedroom, 2 bathroom, 2000 square foot colonial
house.? In addition to the propositions p
2
, p
3
and
p
4
, Ci also conveys a proposition p1. Since p1 is
not needed when conveying P , we say that P can be
realized using Ci by deleting proposition p1 with a
deletion cost of dcost(Ci, p1).
The deletion cost is affected by two main fac-
tors as well: first the syntactic relation between
pj and its host sentence. Given a new instance
Ci, ?This 2000 square foot 3 bedroom, 2 bathroom
house is a colonial?, deleting p
1
, the main object
568
of the verb, will make the rest of the sentence in-
complete. As a result, dcost(Ci, p1) is very expen-
sive. In contrast, dcost(Ci, p4) is low because the
resulting sentence is still grammatically sound. Cur-
rently dcost(Ci, pj) is computed dynamically based
on properties of corpus instances. Second, the ex-
pected performance of a generation system in dele-
tion also impacts the deletion cost. Depending on
the sophistication of the generator to handle various
deletion situations, the expected deletion cost can
be high if the method employed is naive and error
prone, or is low if the system can handle most cases
accurately.
Overall cost: Assume P is the set of propositions
to be conveyed and S is the set of instances in the
corpus that are chosen to realize P by applying a set
of insertion, deletion and sentence breaking opera-
tions, the overall cost of the solution
Cost(P ) =
?
C
i
(Wi ?
?
j
icost(CHj , pj)
+Wd ?
?
k
dcost(Ci, pk))
+(Nb ? 1) ? SBC
in which Wi, Wd and SBC are the insertion weight,
deletion weight and sentence boundary cost; Nb is
the number of sentences in the solution, Ci is a cor-
pus instance been selected to construct the solution
and CHj is the host sentence that proposition pj be-
longs.
4.2 Algorithm: Optimization based on overall
cost
We model the sentence boundary determination pro-
cess as a branch and bound tree search problem. Be-
fore we explain the algorithm itself, first a few no-
tations. The input P is a set of input propositions
chosen by the content planner to be realized. ? is
the set of all possible propositions in an application
domain. Each instance Ci in the corpus C is repre-
sented as a subset of ?. Assume S is a solution to
P , then it can be represented as the overall cost plus
a list of pairs like (Cis, Ois), in which Cis is one
of the instances selected to be used in that solution,
Ois is a set of deletion, insertion operations that can
be applied to Cis to transform it to a subsolution Si.
To explain this representation further, we use a spe-
cific example in which P=(a, d, e, f), ?=(a, b, c, d,
e, f g, h, i). One of the boundary solution S can be
represented as
S = (Cost(S), (S1, S2))
S
1
= (C
1
= (a, b, d, i), delete(b, i)),
S
2
= (C
2
= (e), insert(f as in C
3
= (f, g)))
Cost(S) = Wd ? (dcost(C1, b) + dcost(C1, i)) +
Wi ? icost(C3, f) + 1 ? SBC
in which C
1
and C
2
are two corpus instances se-
lected as the bases to formulate the solution and C
3
is the host sentence containing proposition f .
The general idea behind the instance-based
branch and bound tree search algorithm is that given
an input, P , for each corpus instance Ci, we con-
struct a search branch, representing all possible
ways to realize the input using the instance plus
deletions, insertions and sentence breaks. Since
each sentence break triggers a recursive call to
our sentence boundary determination algorithm, the
complexity of the algorithm is NP-hard. To speed up
the process, for each iteration, we prune unproduc-
tive branches using an upper bound derived by sev-
eral greedy algorithms. The details of our sentence
boundary determination algorithm, sbd(P ), are de-
scribed below. P is the set of input propositions.
1. Set the current upper bound, UB, to the mini-
mum cost of solutions derived by greedy algo-
rithms, which we will describe later. This value
is used to prune unneeded branches to make the
search more efficient.
2. For each instance Ci in corpus C in which (Ci?
P ) 6= ?, loop from step 3 to 9. The goal here
is to identify all the useful corpus instances for
realizing P .
3. Delete all the propositions pj ? D in which
D = Ci ? P (D contains propositions in Ci
but not exist in P) with cost Costd(P ) = Wd ?
?
P
j
?D dcost(Ci, pj). This step computes the
deletion operators and their associated costs.
4. Let I = P ? Ci (I contains propositions in P
but not in Ci). For each subset Ej ? I (Ej in-
cludes ? and I itself), iterate through step 5 to
9. These steps figure out all the possible ways
to add the missing propositions, including in-
serting into the instance Ci and separating the
rest as independent sentence(s).
569
5. Generate a solution in which ?pk ? Ej , insert
pk to Ci. All the propositions in Q = I ? Ej
will be realized in different sentences, thus in-
curring a SBC.
6. We update the cost Cost(P ) to
Costd(P ) + Wi ?
?
p
k
?E
j
icost(?, pk)+
SBC + Cost(Q)
in which Cost(Q) is the cost of sbd(Q) which
recursively computes the best solution for input
Q and Q ? P . To facilitate dynamic program-
ming, we remember the best solution for Q de-
rived by sbd(Q) in case Q is used to formulate
other solutions.
7. If the lower bound for Cost(P) is greater than
the established upper bound UB, prune this
branch.
8. Using the notation described in the beginning
of Sec. 4.2, we update the current solution to
sbd(P ) = (Cost(P ), (Ci, delete
?p
j
?D(pj),
insert
?p
k
?E
j
(pk)))
?
sbd(Q)
in which
?
is an operator that composes two
partial solutions.
9. If sbd(P) is a complete solution (either Q is
empty or have a known best solution) and
Cost(P ) < UB, update the upper bound
UB = Cost(P ).
10. Output the solution with the lowest overall cost.
To establish the initial UB for pruning, we use the
minimum of the following three bounds. In general,
the tighter the UB is, the more effective the pruning
is.
Greedy set partition: we employ a greedy set
partition algorithm in which we first match the set
S ? P with the largest |S|. Repeat the same process
for P ? where P ? = P ? S. The solution cost is
Cost(P ) = (N ? 1) ? SBC , and N is the number
of sentences in the solution. The complexity of this
computation is O(|P |), where |P | is the number of
propositions in P .
Revised minimum set covering: we employ a
greedy minimum set covering algorithm in which
we first find the set S in the corpus that maximizes
the overlapping of propositions in the input P . The
unwanted propositions in S ? P are deleted. As-
sume P ? = P ? S, repeat the same process to P?
until P ? is empty. The only difference between this
and the previous approach is that S here might not
be a subset of P . The complexity of this computa-
tion is O(|P |).
One maximum overlapping sentence: we first
identify the instance Ci in corpus that covers the
maximum number of propositions in P . To arrive
at a solution for P , the rest of the propositions not
covered by Ci are inserted into Ci and all the un-
wanted propositions in Ci are deleted. The cost of
this solution is
Wd ?
?
p
j
?D
dcost(Ci, pj) + Wi ?
?
p
k
?I
icost(?, pk)
in which D includes proposition in Ci but not in P ,
and I includes propositions in P but not in Ci.
Currently, we update UB only after a complete
solution is found. It is possible to derive better UB
by establishing the upper bound for each partial so-
lution, but the computational overhead might not
justify doing so.
4.3 Approximation Algorithm
Even with pruning and dynamic programming, the
exact solution still is very expensive computation-
ally. Computing exact solution for an input size
of 12 propositions has over 1.6 millions states and
takes more than 30 minutes (see Figure 1). To make
the search more efficient for tasks with a large num-
ber of propositions in the input, we naturally seek
a greedy strategy in which at every iteration the al-
gorithm myopically chooses the next best step with-
out regard for its implications on future moves. One
greedy search policy we implemented explores the
branch that uses the instance with maximum over-
lapping propositions with the input and ignores all
branches exploring other corpus instances. The in-
tuition behind this policy is that the more overlap
an instance has with the input, the less insertions or
sentence breaks are needed.
Figure 1 and Figure 2 demonstrate the trade-
off between computation efficiency and accuracy.
In this graph, we use instances from the real-
estate corpus with size 250, we vary the input sen-
tence length from one to twenty and the results
shown in the graphs are average value over sev-
eral typical weight configurations ((Wd,Wi,SBC)=
570
(1,3,5),(1,3,7),(1,5,3),(1,7,3),(1,1,1)). Figure 2 com-
pares the quality of the solutions when using exact
solutions versus approximation. In our interactive
multimedia system, we currently use exact solution
for input size of 7 propositions or less and switch to
greedy for any larger input size to ensure sub-second
performance for the NLG component.
0
20
40
60
80
100
120
140
160
180
200
2 4 6 8 9 10 12 14 16 18 20
# of Propositions in Input
Ex
ec
ut
io
n 
Ti
m
e 
(Se
co
nd
s)
Greedy
Exact
Figure 1: Speed difference between exact solutions
and approximations
0
2
4
6
8
10
12
14
16
18
20
2 4 6 8 9 10 12 14 16 18 20
# of Proposition in Input
Co
st Greedy
Exact
Figure 2: Cost difference between exact solutions
and approximations
Measures Ours B-3 B-6
Dangling sentence (7) 0 100% 100%
Split Semantic Group 1% 61% 21%
Realization Failure 0 56% 72%
Fluency 59% 4% 8%
Table 2: Comparisons
5 Evaluations
To evaluate the quality of our sentence boundary de-
cisions, we implemented a baseline system in which
boundary determination of the aggregation module
is based on a threshold of the maximum number
of propositions allowed in a sentence (a simplified
version of the second strategy in Section 2. We
have tested two threshold values, the average (3) and
maximum (6) number of propositions among cor-
pus instances. Other sentence complexity measures,
such as the number of words and depth of embed-
ding are not easily applicable for our comparison
because they require the propositions to be realized
first before the boundary decisions can be made.
We tune the relative weight of our approach to
best fit our system?s capability. Currently, the
weights are empirically established to Wd = 1,
Wi = 3 and SBC = 3. Based on the output gen-
erated from both systems, we derive four evaluation
metrics:
1. Dangling sentences: We define dangling sen-
tences as the short sentences with only one
proposition that follow long sentences. This
measure is used to verify our claim that because
we use global instead of local optimization,
we can avoid generating dangling sentences by
making more balanced sentence boundary de-
cisions. In contrast, the baseline approaches
have dangling sentence problem when the in-
put proposition is 1 over the multiple of the
threshold values. The first row of Table 2 shows
that when the input proposition length is set
to 7, a pathological case, among the 200 input
proposition sets randomly generated, the base-
line approach always produce dangling sen-
tences (100%). In contrast, our approach al-
ways generates more balanced sentences (0%).
2. Semantic group splitting. Since we use an
instance-based approach, we can maintain the
semantic cohesion better. To test this, we
randomly generated 200 inputs with up to 10
propositions containing semantic grouping of
both the number of bedrooms and number of
bathrooms. The second row, Split Semantic
Group, in Table 2 shows that our algorithm can
maintain semantic group much better than the
baseline approach. Only in 1% of the output
sentences, our algorithm generated number of
bedrooms and number of bathrooms in separate
sentences. In contrast, the baseline approaches
did much worse (61% and 21%).
3. Sentence realization failure. This measure is
used to verify that since we also take a sen-
tence?s lexical and syntactical realizability into
consideration, our sentence boundary decisions
will result in less sentence realization failures.
571
An realization failure occurs when the aggre-
gation module failed to realize one sentence
for all the propositions grouped by the sentence
boundary determination module. The third row
in Table 2, Realization Failure, indicates that
given 200 randomly generated input proposi-
tion sets with length from 1 to 10, howmany re-
alization happened in the output. Our approach
did not have any realization failure while for the
baseline approaches, there are 56% and 72%
outputs have one or more realization failures.
4. Fluency. This measure is used to verify our
claim that since we also optimize our solutions
based on boundary cost, we can reduce incoher-
ence across multiple sentences. Given 200 ran-
domly generated input propositions with length
from 1 to 10, we did a blind test and presented
pairs of generated sentences to two human sub-
jects randomly and asked them to rate which
output is more coherent. The last row, Flu-
ency, in Table 2 shows how often the human
subjects believe that a particular algorithm gen-
erated better sentences. The output of our al-
gorithm is preferred for more than 59% of the
cases, while the baseline approaches are pre-
ferred 4% and 8%, respectively. The other per-
centages not accounted for are cases where the
human subject felt there is no significant differ-
ence in fluency between the two given choices.
The result from this evaluation clearly demon-
strates the superiority of our approach in gener-
ating coherent sentences.
6 Conclusion
In the paper, we proposed a novel domain indepen-
dent instance-based sentence boundary determina-
tion algorithm that is capable of balancing a com-
prehensive set of generation capability, sentence
complexity, and quality related constraints. This
is the first domain-independent algorithm that pos-
sesses many desirable properties, including balanc-
ing a system?s generation capabilities, maintaining
semantic cohesion and cross sentence coherence,
and preventing severe syntactic and lexical realiza-
tion failures. Our evaluation results also demon-
strate the superiority of the approach over a rep-
resentative domain independent sentence boundary
solution.
References
Anthony C. Davey. 1979. Discourse Production. Edin-
burgh University Press, Edinburgh.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill.
William C. Mann and James A. Moore. 1981. Computer
generation of multiparagraph English text. American
Journal of Computational Linguistics, 7(1):17?29.
Shimei Pan and James Shaw. 2004. SEGUE: A hy-
brid case-based surface natural language generator. In
Proc. of ICNLG, Brockenhurst, U.K.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proc. of INLG, Kennebunkport, Maine.
Graeme D. Ritchie. 1984. A rational reconstruction of
the Proteus sentence planner. In Proc. of the COLING
and the ACL, Stanford, CA.
Jacques Robin. 1994. Automatic generation and revi-
sion of natural language summaries providing histori-
cal background. In Proc. of the Brazilian Symposium
on Artificial Intelligence, Fortaleza, CE, Brazil.
James Shaw. 1998. Segregatory coordination and ellipsis
in text generation. In Proc. of the COLING and the
ACL., Montreal, Canada.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Proc. of the ACL, Barcelona, Spain.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In Proc. of the
NAACL, Pittsburgh, PA.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. Computer Speech and Language.
John Wilkinson. 1995. Aggregation in natural language
generation: Another look. Co-op work term report,
Dept. of Computer Science, University of Waterloo.
Michelle Zhou and Vikram Aggarwal. 2004. An
optimization-based approach to dynamic data content
selection in intelligent multimedia interfaces. In Proc.
of the UIST, Santa Fe, NM.
Michelle X. Zhou and Min Chen. 2003. Automated
generation of graphic sketches by example. In IJCAI,
Acapulco, Mexico.
Michelle X. Zhou and Shimei Pan. 2001. Automated
authoring of coherent multimedia discourse in conver-
sation systems. In ACM Multimedia, Ottawa, Canada.
572
Modeling Local Context for Pitch Accent Prediction
Shimei Pan
Department of Computer Science
Columbia University
New York, NY, 10027, USA
pan@cs.columbia.edu
Julia Hirschberg
AT&T Labs-Research
Florham Park, NJ, 07932-0971, USA
julia@research.att.com
Abstract
Pitch accent placement is a major
topic in intonational phonology re-
search and its application to speech
synthesis. What factors inuence
whether or not a word is made
intonationally prominent or not is
an open question. In this paper,
we investigate how one aspect of a
word's local context | its colloca-
tion with neighboring words | inu-
ences whether it is accented or not.
Results of experiments on two tran-
scribed speech corpora in a medical
domain show that such collocation
information is a useful predictor of
pitch accent placement.
1 Introduction
In English, speakers make some words more
intonationally prominent than others. These
words are said to be accented or to bear
pitch accents. Accented words are typically
louder and longer than their unaccented coun-
terparts, and their stressable syllable is usu-
ally aligned with an excursion in the funda-
mental frequency. This excursion will dier
in shape according to the type of pitch ac-
cent. Pitch accent type, in turn, inuences
listeners' interpretation of the accented word
or its larger syntactic constituent. Previous
research has associated pitch accent with vari-
ation in various types of information status,
including the given/new distinction, focus, and
contrastiveness, inter alia. Assigning pitch ac-
cent in speech generation systems which em-
ploy speech synthesizers for output is thus crit-
ical to system performance: not only must one
convey meaning naturally, as humans would,
but one must avoid conveying mis-information
which reliance on the synthesizers' defaults
may result in.
The speech generation work discussed here
is part of a larger eort in developing an intel-
ligent multimedia presentation generation sys-
tem called MAGIC (Medical Abstract Gen-
eration for Intensive Care) (Dalal et al,
1996). In MAGIC, given a patient's medical
record stored at Columbia Presbyterian Medi-
cal Center (CPMC)'s on-line database system,
the system automatically generates a post-
operative status report for a patient who has
just undergone bypass surgery. There are two
media-specic generators in MAGIC: a graph-
ics generator which automatically produces
graphical presentations from database entities,
and a spoken language generator which auto-
matically produces coherent spoken language
presentations from these entities. The graph-
ical and the speech generators communicate
with each other on the y to ensure that the
nal multimedia output is synchronized.
In order to produce natural and coherent
speech output, MAGIC's spoken language gen-
erator models a collection of speech features,
such as accenting and intonational phrasing,
which are critical to the naturalness and intel-
ligibility of output speech. In order to assign
these features accurately, the system needs to
identify useful correlates of accent and phrase
boundary location to use as predictors. This
work represents part of our eorts in identi-
fying useful predictors for pitch accent place-
ment.
Pitch accent placement has long been a re-
search focus for scientists working on phonol-
ogy, speech analysis and synthesis (Bolinger,
1989; Ladd, 1996). In general, syntactic fea-
tures are the most widely used features in
pitch accent predication. For example, part-
of-speech is traditionally the most useful sin-
gle pitch accent predictor (Hirschberg, 1993).
Function words, such as prepositions and ar-
ticles, are less likely to be accented, while
content words, such as nouns and adjectives,
are more likely to be accented. Other lin-
guistic features, such as inferred given/new
status (Hirschberg, 1993; Brown, 1983), con-
trastiveness (Bolinger, 1961), and discourse
structure (Nakatani, 1998), have also been ex-
amined to explain accent assignment in large
speech corpora. In a previous study (Pan and
McKeown, 1998; Pan andMcKeown, 1999), we
investigated how features such as deep syntac-
tic/semantic structure and word informative-
ness correlate with accent placement. In this
paper, we focus on how local context inuences
accent patterns. More specically, we investi-
gate how word collocation inuences whether
nouns are accented or not.
Determining which nouns are accented and
which are not is challenging, since part-of-
speech information cannot help here. So, other
accent predictors must be found. There are
some advantages in looking only at one word
class. We eliminate the interaction between
part-of-speech and collocation, so that the in-
uence of collocation is easier to identify. It
also seems likely that collocation may have a
greater impact on content words, like nouns,
than on function words, like prepositions.
Previous researchers have speculated that
word collocation aects stress assignment of
noun phrases in English. For example, James
Marchand (1993) notes how familiar colloca-
tions change their stress, witness the American
pronunciation of `Little House' [in the televi-
sion series Little House on the Prairie], where
stress used to be on HOUSE, but now, since the
series is so familiar, is placed on the LITTLE.
That is, for collocated words, stress shifts to
the left element of the compound. However,
there are numerous counter-examples: con-
sider apple PIE, which retains a right stress
pattern, despite the collocation. So, the ex-
tent to which collocational status aects ac-
cent patterns is still unclear.
Despite some preliminary investigation
(Liberman and Sproat, 1992), word colloca-
tion information has not, to our knowledge,
been successfully used to model pitch accent
assignment; nor has it been incorporated into
any existing speech synthesis systems. In this
paper, we empirically verify the usefulness of
word collocation for accent prediction. In Sec-
tion 2, we describe our annotated speech cor-
pora. In Section 3, we present a description of
the collocation measures we investigated. Sec-
tion 4 to 7 describe our analyses and machine
learning experiments in which we attempt to
predict accent location. In Section 8 we sum
up our results and discuss plans for further re-
search.
2 Speech Corpora
From the medical domain described in Section
1, we collected two speech corpora and one text
corpus for pitch accent modeling. The speech
corpora consist of one multi-speaker sponta-
neous corpus, containing twenty segments and
totaling fty minutes, and one read corpus of
ve segments, read by a single speaker and to-
taling eleven minutes of speech. The text cor-
pus consists of 3.5 million words from 7375 dis-
charge summaries of patients who had under-
gone surgery. The speech corpora only cover
cardiac patients, while the text corpus covers
a larger group of patients and the majority of
them have also undergone cardiac surgery.
The speech corpora were rst transcribed or-
thographically and then intonationally, using
the ToBI convention for prosodic labeling of
standard American English (Silverman et al,
1992). For this study, we used only binary ac-
cented/deaccented decisions derived from the
ToBI tonal tier, in which location and type
of pitch accent is marked. After ToBI label-
ing, each word in the corpora was tagged with
part-of-speech, from a nine-element set: noun,
verb, adjective, adverb, article, conjunction,
pronoun, cardinal, and preposition. The spon-
taneous corpus was tagged by hand and the
read tagged automatically. As noted above,
we focus here on predicting whether nouns are
accented or not.
3 Collocation Measures
We used three measures of word collocation to
examine the relationship between collocation
and accent placement: word bigram pre-
dictability, mutual information, and the
Dice coefficient. While word predictabil-
ity is not typically used to measure collocation,
there is some correlation between word collo-
cation and predictability. For example, if two
words are collocated, then it will be easy to
predict the second word from the rst. Sim-
ilarly, if one word is highly predictable given
another word, then there is a higher possibility
that these two words are collocated. Mutual
information (Fano, 1961) and the Dice coe?-
cient (Dice, 1945) are two standard measures
of collocation. In general, mutual information
measures uncertainty reduction or departure
from independence. The Dice coe?cient is a
collocation measure widely used in information
retrieval. In the following, we will give a more
detailed denitions of each.
Statistically, bigram word predictability is
dened as the log conditional probability of
word w
i
, given the previous word w
i 1
:
Pred(w
i
) = log(Prob(w
i
jw
i 1
))
Bigram predictability directly measures the
likelihood of seeing one word, given the
occurrence of the previous word. Bi-
gram predictability has two forms: abso-
lute and relative. Absolute predictability is
the value directly computed from the for-
mula. For example, given four adjacent
words w
i 1
; w
i
; w
i+1
and w
i+2
, if we assume
Prob(w
i
jw
i 1
) = 0:0001, Prob(w
i+1
jw
i
) =
0:001, and Prob(w
i+2
jw
i+1
) = 0:01, the abso-
lute bigram predictability will be -4, -3 and
-2 for w
i
; w
i+1
and w
i+2
. The relative pre-
dictability is dened as the rank of absolute
predictability among words in a constituent.
In the same example, the relative predictabil-
ity will be 1, 2 and 3 for w
i
; w
i+1
and w
i+2
,
where 1 is associated with the word with the
lowest absolute predictability. In general, the
higher the rank, the higher the absolute pre-
dictability. Except in Section 7, all the pre-
dictability measures mentioned in this paper
use the absolute form.
We used our text corpus to compute bigram
word predictability for our domain. When cal-
culating the word bigram predictability, we
rst ltered uncommon words (words occur-
ring 5 times or fewer in the corpus) then used
the Good-Turing discount strategy to smooth
the bigram. Finally we calculated the log con-
ditional probability of each word as the mea-
sure of its bigram predictability.
Two measures of mutual information were
used for word collocation: pointwise mu-
tual information, which is dened as :
I
1
(w
i 1
;w
i
) = log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
and average mutual information, which
is dened as:
I
2
(w
i 1
;w
i
) =
P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
The same text corpus was used to compute
both mutual information measures. Only word
pairs with bigram frequency greater than ve
were retained.
The Dice coe?cient is dened as:
Dice(w
i 1
; w
i
) =
2 P
r
(w
i 1
; w
i
)
P
r
(w
i 1
) + P
r
(w
i
)
Here, we also use a cut o threshold of ve to
lter uncommon bigrams.
Although all these measures are correlated,
one measure can score word pairs quite dier-
ently from another. Table 1 shows the top ten
collocations for each metric.
In the predictability top ten list, we have
pairs like scarlet fever where fever is very pre-
dictable from scarlet (in our corpus, scarlet is
always followed by fever), thus, it ranks high-
est in the predictability list. Since scarlet can
be di?cult to predict from fever, these types
of pairs will not receive a very high score us-
ing mutual information (in the top 5% in I
1
sorted list and in the top 20% in I
2
list) and
Dice coe?cient (top 22%). From this table, it
is also quite clear that I
1
tends to rank un-
common words high. All the words in the top
ten I
1
list have a frequency less than or equal
Pred I
1
I
2
Dice
chief complaint polymyalgia rheumatica The patient greeneld lter
cerebrospinal uid hemiside stepper present illness Guillain Barre
folic acid Pepto Bismol hospital course Viet Nam
periprocedural complications Glen Cove p o Neo Synephrine
normoactive bowel hydrogen peroxide physical exam polymyalgia rheumatica
uric acid Viet Nam i d hemiside stepper
postpericardiotomy syndrome Neo Synephrine coronary artery Pepto Bismol
Staten Island otitis media postoperative day Glen Cove
scarlet fever Lo Gerfo saphenous vein present illness
pericardiotomy syndrome Chlor Trimeton medical history chief complaint
Table 1: Top Ten Most Collocated Words for Each Measure
to seven (we lter all the pairs occurring fewer
than six times).
Of the dierent metrics, only bigram pre-
dictability is a unidirectional measure. It cap-
tures how the appearance of one word aects
the appearance of the following word. In con-
trast, the other measures are all bidirectional
measures, making no distinction between the
relative position of elements of a pair of col-
located items. Among the bidirectional mea-
sures, point-wise mutual information is sensi-
tive to marginal probabilities P
r
(word
i 1
) and
P
r
(word
i
). It tends to give higher values as
these probabilities decrease, independently of
the distribution of their co-occurrence. The
Dice coe?cient, however, is not sensitive to
marginal probability. It computes conditional
probabilities which are equally weighted in
both directions.
Average mutual information measures the
reduction in the uncertainty, of one word,
given another, and is totally symmetric. Since
I
2
(word
i 1
; word
i
)=I
2
(word
i
;word
i 1
), the
uncertainty reduction of the rst word, given
the second word, is equal to the uncer-
tainty reduction of the second word, given the
rst word. Further more, because I
2
(word
i
;
word
i 1
) = I
2
(word
i
;word
i 1
), the uncer-
tainty reduction of one word, given another,
is also equal to the uncertainty reduction of
failing to see one word, having failed to see
the other.
Since there is considerable evidence that
prior discourse context, such as previous men-
tion of a word, aects pitch accent decisions,
it is possible that symmetric measures, such
as mutual information and the Dice coe?-
cient, may not model accent placement as
well as asymmetric measures, such as bigram
predictability. Also, the bias of point-wise
mutual information toward uncommon words
can aect its ability to model accent assign-
ment, since, in general, uncommon words are
more likely to be accented (Pan and McKe-
own, 1999). Since this metric disproportion-
ately raises the mutual information for un-
common words, making them more predictable
than their appearance in the corpus warrants,
it may predict that uncommon words are more
likely to be deaccented than they really are.
4 Statistical Analyses
In order to determine whether word collo-
cation is useful for pitch accent prediction,
we rst employed Spearman's rank correlation
test (Conover, 1980).
In this experiment, we employed a unigram
predictability-based baseline model. The un-
igram predictability of a word is dened as
the log probability of a word in the text cor-
pus. The maximum likelihood estimation of
this measure is:
log
Freq(w
i
)
P
i
Freq(w
i
)
The reason for choosing this as the baseline
model is not only because it is context inde-
pendent, but also because it is eective. In
a previous study (Pan and McKeown, 1999),
we showed that when this feature is used, it
is as powerful a predictor as part-of-speech.
When jointly used with part-of-speech infor-
mation, the combined model can perform sig-
nicantly better than each individual model.
When tested on a similar medical corpus, this
combined model also outperforms a compre-
hensive pitch accent model employed by the
Bell Labs' TTS system (Sproat et al, 1992;
Hirschberg, 1993; Sproat, 1998), where dis-
course information, such as given/new, syntac-
tic information, such as POS, and surface in-
formation, such as word distance, are incorpo-
rated. Since unigram predictability is context
independent. By comparing other predictors
to this baseline model, we can demonstrate the
impact of context, measured by word colloca-
tion, on pitch accent assignment.
Table 2 shows that for our read speech
corpus, unigram predictability, bigram pre-
dictability and mutual information are all sig-
nicantly correlated (p < 0:001) with pitch ac-
cent decision.
1
However, the Dice coe?cient
shows only a trend toward correlation (p <
0:07). In addition, both bigram predictabil-
ity and (pointwise) mutual information show a
slightly stronger correlation with pitch accent
than the baseline. When we conducted a sim-
ilar test on the spontaneous corpus, we found
that all but the baseline model are signicantly
correlated with pitch accent placement. Since
all three models incorporate a context word
while the baseline model does not, these re-
sults suggest the usefulness of context in ac-
cent prediction. Overall, for all the dierent
measures of collocation, bigram predictability
explains the largest amount of variation in ac-
cent status for both corpora. We conducted a
similar test using trigram predictability, where
two context words, instead of one, were used
to predict the current word. The results are
slightly worse than bigram predictability (for
the read corpus r =  0:167, p < 0:0001; for
the spontaneous r =  0:355, p < 0:0001).
The failure of the trigram model to improve
over the bigram model may be due to sparse
data. Thus, in the following analysis, we focus
on bigram predictability. In order to further
verify the eectiveness of word predictability
in accent prediction, we will show some exam-
ples in our speech corpora rst. Then we will
describe how machine learning helps to derive
pitch accent prediction models using this fea-
ture. Finally, we show that both absolute pre-
dictability and relative predictability are use-
ful for pitch accent prediction.
1
Since pointwise mutual information performed con-
sistently better than average mutual information in our
experiment, we present results only for the former.
5 Word Predictability and Accent
In general, nouns, especially head nouns, are
very likely to be accented. However, cer-
tain nouns consistently do not get accented.
For example, Table 3 shows some collocations
containing the word cell in our speech cor-
pus. For each context, we list the collocated
pair, its most frequent accent pattern in our
corpus (upper case indicates that the word
was accented and lower case indicates that
it was deaccented), its bigram predictability
(the larger the number is, the more predictable
the word is), and the frequency of this ac-
cent pattern, as well as the total occurrence
of the bigram in the corpus. In the rst ex-
Word Pair Pred(cell) Freq
[of] CELL -3.11 7/7
[RED] CELL -1.119 2/2
[PACKED] cell -0.5759 4/6
[BLOOD] cell -0.067 2/2
Table 3: cell Collocations
ample, cell in [of ] CELL is very unpredictable
from the occurrence of of and always receives a
pitch accent. In [RED] CELL, [PACKED] cell,
and [BLOOD] cell, cell has the same semantic
meaning, but dierent accent patterns: cell in
[PACKED] cell and [BLOOD] cell is more pre-
dictable and deaccented, while in [RED] CELL
it is less predictable and is accented. These
examples show the inuence of context and
its usefulness for bigram predictability. Other
predictable nouns, such as saver in CELL
saver usually are not accented even when they
function as head nouns. Saver is deaccented in
ten of the eleven instances in our speech cor-
pus. Its bigram score is -1.5517, which is much
higher than that of CELL (-4.6394{3.1083 de-
pending upon context). Without collocation
information, a typical accent prediction sys-
tem is likely to accent saver, which would be
inappropriate in this domain.
6 Accent Prediction Models
Both the correlation test results and direct ob-
servations provide some evidence on the useful-
ness of word predictability. But we still need to
demonstrate that we can successfully use this
feature in automatic accent prediction. In or-
der to achieve this, we used machine learning
Corpus Read Spontaneous
r p-value r p-value
Baseline (Unigram) r =  0:166 p = 0:0002 r =  0:02 p = 0:39
Bigram Predictability r =  0:236 p < 0:0001 r =  0:36 p < 0:0001
Pointwise Mutual Information r =  0:185 p < 0:0001 r =  0:177 p < 0:0001
Dice Coe?cient r =  0:079 p = 0:066 r =  0:094 p < 0:0001
Table 2: Correlation of Dierent Collocation Measures with Accent Decision
techniques to automatically build accent pre-
diction models using bigram word predictabil-
ity scores.
We used RIPPER (Cohen, 1995b) to ex-
plore the relations between predictability and
accent placement. RIPPER is a classication-
based rule induction system. From annotated
examples, it derives a set of ordered if-then
rules, describing how input features can be
used to predict an output feature. In order
to avoid overtting, we use 5-fold cross valida-
tion. The training data include all the nouns in
the speech corpora. The independent variables
used to predict accent status are the unigram
and bigram predictability measures, and the
dependent variable is pitch accent status. We
used a majority-based predictability model as
our baseline (i.e. predict accented).
In the combined model, both unigram and
bigram predictability are used together for ac-
cent prediction. From the results in Table 4,
we see that the bigram model consistently out-
performs the unigram model, and the com-
bined model achieves the best performance.
To evaluate the signicance of the improve-
ments achieved by incorporating a context
word, we use the standard error produced by
RIPPER. Two results are statistically signif-
icant when the results plus or minus twice
the standard error do not overlap (Cohen,
1995a). As shown in Table 4, for the read
corpus, except for the unigram model, all the
models with bigram predictability performed
signicantly better than the baseline model.
However, the bigram model and the combined
model failed to improve signicantly over the
unigram model. This may result from too
small a corpus. For the spontaneous corpus,
the unigram, bigram and the combined model
all achieved signicant improvement over the
baseline. The bigram also performed signi-
cantly better than the unigram model. The
combined model had the best performance. It
also achieved signicant improvement over the
unigram model.
The improvement of the combined model
over both unigram and bigram models may
be due to the fact that some accent patterns
that are not captured by one are indeed cap-
tured by the other. For example, accent pat-
terns for street names have been extensively
discussed in the literature (Ladd, 1996). For
example, street in phrases like (e.g. FIFTH
street) is typically deaccented while avenue
(e.g. Fifth AVENUE) is accented. While it
seems likely that the conditional probability
of P
r
(StreetjFifth) is no higher than that of
P
r
(AvenuejFifth), the unigram probability of
P
r
(street) is probably higher than that of av-
enue P
r
(avenue).
2
. So, incorporating both
predictability measures may tease apart these
and similar cases.
7 Relative Predictability
In the our previous analysis, we showed the ef-
fectiveness of absolute word predictability. We
now consider whether relative predictability is
correlated with a larger constituent's accent
pattern. The following analysis focuses on ac-
cent patterns of non-trivial base NPs.
3
For
this study we labeled base NPs by hand for
the corpora described in Section 2. For each
base NP, we calculate which word is the most
predictable and which is the least. We want
to see, when comparing with its neighboring
2
For example, in a 7.5M word general news corpus
(from CNN and Reuters), street occurs 2115 times and
avenue just 194. Therefore, the unigram predictabil-
ity of street is higher than that of avenue. The most
common bigram with street is Wall Street which occurs
116 times and the most common bigram with avenue is
Pennsylvania Avenue which occurs 97. In this domain,
the bigram predictability for street in Fifth Street is ex-
tremely low because this combination never occurred,
while that for avenue in Fifth Avenue is -3.0995 which
is the third most predictable bigrams with avenue as
the second word.
3
Non-recursive noun phrases containing at least two
elements.
Corpus Predictability Model Performance Standard Error
baseline model 81.98%
unigram model 82.86%  0.93
Read bigram predictability model 84.41%  1.10
unigram+bigram model 85.03%  1.04
baseline model 70.03%
unigram model 72.22%  0.62
Spontaneous bigram model 74.46%  0.30
unigram+bigram model 77.43%  0.51
Table 4: Ripper Results for Accent Status Prediction
Model Predictability Total Accented Word Not Accented Accentability
unigram Least Predictable 1206 877 329 72.72%
Most Predictable 1198 485 713 40.48%
bigram Least Predictable 1205 965 240 80.08%
Most Predictable 1194 488 706 40.87%
Table 5: Relative Predictability and Accent Status
words, whether the most predictable word is
more likely to be deaccented. As shown in Ta-
ble 5, the \total" column represents the total
number of most (or least) predictable words
in all baseNPs
4
. The next two columns indi-
cate how many of them are accented and deac-
cented. The last column is the percentage of
words that are accented. Table 5 shows that
the probability of accenting a most predictable
word is between 40:48% and 45:96% and that
of a least predictable word is between 72:72%
and 80:08%. This result indicates that rela-
tive predictability is also a useful predictor for
a word's accentability.
8 Discussion
It is di?cult to directly compare our results
with previous accent prediction studies, to
determine the general utility of bigram pre-
dictability in accent assignment, due to dif-
ferences in domain and the scope of our task.
For example, Hirschberg (1993) built a com-
prehensive accent prediction model using ma-
chine learning techniques for predicting ac-
cent status for all word classes for a text-to-
speech system, employing part-of-speech, var-
ious types of information status inferred from
the text, and a number of distance metrics,
as well as a complex nominal predictor devel-
oped by Sproat (1992). An algorithm making
use of these features achieved 76.5%-80% ac-
cent prediction accuracy for a broadcast news
4
The total number of most predictable words is not
equal to that of least predictable words due to ties.
corpus, 85% for sentences from the ATIS cor-
pus of spontaneous elicited speech, and 98.3%
success on a corpus of laboratory read sen-
tences. Liberman and Sproat's (1992) success
in predicting accent patterns for complex nom-
inals alone, using rules combining a number
of features, achieved considerably higher suc-
cess rates (91% correct, 5.4% acceptable, 3.6%
unacceptable when rated by human subjects)
for 500 complex nominals of 2 or more ele-
ments chosen from the AP Newswire. Our re-
sults, using bigram predictability alone, 77%
for the spontaneous corpus and 85% for the
read corpus, and using a dierent success es-
timate, while not as impressive as (Liberman
and Sproat, 1992)'s, nonetheless demonstrate
the utility of a relatively untested feature for
this task.
In this paper, we have investigated several
collocation-based measures for pitch accent
prediction. Our initial hypothesis was that
word collocation aects pitch accent place-
ment, and that the more predictable a word
is in terms of its local lexical context, the
more likely it is to be deaccented. In order
to verify this claim, we estimated three col-
location measures: word predictability, mu-
tual information and the Dice coe?cient. We
then used statistical techniques to analyze the
correlation between our dierent word collo-
cation metrics and pitch accent assignment
for nouns. Our results show that, of all the
collocation measures we investigated, bigram
word predictability has the strongest correla-
tion with pitch accent assignment. Based on
this nding, we built several pitch accent mod-
els, assessing the usefulness of unigram and
bigram word predictability {as well as a com-
bined model{ in accent predication. Our re-
sults show that the bigram model performs
consistently better than the unigram model,
which does not incorporate local context in-
formation. However, our combined model per-
forms best of all, suggesting that both con-
textual and non-contextual features of a word
are important in determining whether or not
it should be accented.
These results are particularly important for
the development of future accent assignment
algorithms for text-to-speech. For our contin-
uing research, we will focus on two directions.
The rst is to combine our word predictability
feature with other pitch accent predictors that
have been previously used for automatic accent
prediction. Features such as information sta-
tus, grammatical function, and part-of-speech,
have also been shown to be important deter-
minants of accent assignment. So, our nal
pitch accent model should include many other
features. Second, we hope to test whether the
utility of bigram predictability can be gener-
alized across dierent domains. For this pur-
pose, we have collected an annotated AP news
speech corpus and an AP news text corpus,
and we will carry out a similar experiment in
this domain.
9 Acknowledgments
Thanks for C. Jin, K. McKeown, R. Barzi-
lay, J. Shaw, N. Elhadad, M. Kan, D. Jor-
dan, and anonymous reviewers for the help on
data preparation and useful comments. This
research is supported in part by the NSF Grant
IRI 9528998, the NLM Grant R01 LM06593-01
and the Columbia University Center for Ad-
vanced Technology in High Performance Com-
puting and Communications in Healthcare.
References
D. Bolinger. 1961. Contrastive accent and con-
trastive stress. language, 37:83{96.
D. Bolinger. 1989. Intonation and Its Uses. Stan-
ford University Press.
G. Brown. 1983. Prosodic structure and the
given/new distinction. In A. Cutler and D.R.
Ladd, ed., Prosody: Models and Measurements,
pages 67{78. Springer-Verlag, Berlin.
P. Cohen. 1995a. Empirical methods for articial
intelligence. MIT press, Cambridge, MA.
W. Cohen. 1995b. Fast eective rule induction.
In Proc. of the 12th International Conference on
Machine Learning.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
M. Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou,
T. Hoellerer, J. Shaw, Y. Feng, and J. Fromer.
1996. Negotiation for automated generation of
temporal multimedia presentations. In Proc. of
ACM Multimedia 96, pages 55{64.
Lee R. Dice. 1945. Measures of the amount of
ecologic association between species. Journal of
Ecology, 26:297{302.
Robert M. Fano. 1961. Transmission of Informa-
tion: A Statistical Theory of Communications.
MIT Press, Cambridge, MA.
J. Hirschberg. 1993. Pitch accent in context: pre-
dicting intonational prominence from text. Ar-
ticial Intelligence, 63:305{340.
D. Robert Ladd. 1996. Intonational Phonology.
Cambridge University Press, Cambridge.
M. Liberman and R. Sproat. 1992. The stress and
structure of modied noun phrases in English.
In I. Sag, ed., Lexical Matters, pages 131{182.
University of Chicago Press.
J. Marchand. 1993. Message posted on HUMAN-
IST mailing list, April.
C. Nakatani. 1998. Constituent-based accent pre-
diction. In Proc. of COLING/ACL'98, pages
939{945, Montreal, Canada.
S. Pan and K. McKeown. 1998. Learning intona-
tion rules for concept to speech generation. In
Proc. of COLING/ACL'98, Montreal, Canada.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC, pages 148{157.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: a standard for
labeling English prosody. In Proc. of ICSLP92.
R. Sproat, J. Hirschberg, and D. Yarowsky. 1992.
A corpus-based synthesizer. In Proc. of IC-
SLP92, pages 563{566, Ban.
R. Sproat, ed. 1998. Multilingual Text-to-Speech
Synthesis: The Bell Labs Approach. Kluwer.
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30?33,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Active Learning with Constrained Topic Model
Yi Yang
Northwestern University
yiyang@u.northwestern.edu
Shimei Pan
IBM T. J. Watson Research Center
shimei@us.ibm.com
Doug Downey
Northwestern University
ddowney@eecs.northwestern.edu
Kunpeng Zhang
University of Illinois at Chicago
kzhang6@uic.edu
Abstract
Latent Dirichlet Allocation (LDA) is a topic
modeling tool that automatically discovers
topics from a large collection of documents.
It is one of the most popular text analysis
tools currently in use. In practice however,
the topics discovered by LDA do not al-
ways make sense to end users. In this ex-
tended abstract, we propose an active learn-
ing framework that interactively and itera-
tively acquires user feedback to improve the
quality of learned topics. We conduct exper-
iments to demonstrate its effectiveness with
simulated user input on a benchmark dataset.
1 Introduction
Statistical topic models such as Latent Dirichlet Al-
location (LDA) (Blei et al., 2003) provide powerful
tools for uncovering hidden thematic patterns in text
and are useful for representing and summarizing the
contents of large document collections. However,
when using topic models in practice, users often face
one critical problem: topics discovered by the model
do not always make sense. A topic may contain the-
matically unrelated words. Moreover, two thematic
related words may appear in different topics. This
is mainly because the objective function optimized
by LDA may not reflect human judgments of topic
quality (Boyd-Graber et al., 2009).
Potentially, we can solve these problems by incor-
porating additional user guidance or domain knowl-
edge in topic modeling. With standard LDA how-
ever, it is impossible for users to interact with the
model and provide feedback. (Hu et al., 2011) pro-
posed an interactive topic modeling framework that
allows users to add word must-links. However, it
has several limitations. Since the vocabulary size of
a large document collection can be very large, users
may need to annotate a large number of word con-
straints for this method to be effective. Thus, this
process can be very tedious. More importantly, it
cannot handle polysemes. For example, the word
?pound? can refer to either a currency or a unit of
mass. If a user adds a must-link between ?pound?
and another financial term, then he/she cannot add
a must-link between ?pound? and any measurement
terms. Since word must-links are added without
context, there is no way to disambiguate them. As a
result, word constraints frequently are not as effec-
tive as document constraints.
Active learning (Settles, 2010) provides a use-
ful framework which allows users to iteratively give
feedback to the model to improve its quality. In gen-
eral, with the same amount of human labeling, ac-
tive learning often results in a better model than that
learned by an off-line method.
In this extended abstract, we propose an active
learning framework for LDA. It is based on a new
constrained topic modeling framework which is ca-
pable of handling pairwise document constraints.
We present several design choices and the pros and
cons of each choice. We also conduct simulated ex-
periments to demonstrate the effectiveness of the ap-
proach.
2 Active Learning With Constrained Topic
Modeling
In this section, we first summarize our work on con-
strained topic modeling. Then, we introduce an
active topic learning framework that employs con-
strained topic modeling.
In LDA, a document?s topic distribution
~
? is
drawn from a Dirichlet distribution with prior ~?.
A simple and commonly used Dirichlet distribution
uses a symmetric ~? prior. However, (Wallach et al.,
2009) has shown that an asymmetric Dirichlet prior
over the document-topic distributions
~
? and a sym-
metric Dirichlet prior over the topic-word distribu-
tions
~
? yield significant improvements in model per-
formance. Our constrained topic model uses asym-
metric priors to encode constraints.
To incorporate user feedback, we focus on two
1
30
Figure 1: Diagram illustrating the topic model active learning framework.
types of document constraints. A must-link be-
tween two documents indicates that they belong to
the same topics, while a cannot-link indicates that
they belong to different topics.
Previously, we proposed a constrained LDA
framework called cLDA,
1
which is capable of incor-
porating pairwise document constraints. Given pair-
wise document constraints, the topic distribution of
a document cannot be assumed to be independently
sampled. More specifically, we denote the collection
of documents as D = {d
1
, d
2
, ..., d
N
}. We also de-
noteM
i
? D as the set of documents sharing must-
links with document d
i
, and C
i
? D as the set of
documents sharing cannot-links with document d
i
.
~
?
i
is the topic distribution of d
i
, and ~? is the global
document-topic hyper-parameter shared by all doc-
uments.
Given the documents inM
i
, we introduce an aux-
iliary variable ~?
M
i
:
~?
i
M
= T ?
1
|M
i
|
?
j?M
i
~
?
j
, (1)
where T controls the concentration parameters. The
larger the value of T is, the closer
~
?
i
is to the average
of
~
?
j
?s.
Given the documents in C
i
, we introduce another
auxiliary variable:
~?
i
C
= T ? arg
~
?
i
maxmin
j?C
i
KL(
~
?
i
,
~
?
j
), (2)
whereKL(
~
?
i
,
~
?
j
) is the KL-divergence between two
distributions
~
?
i
and
~
?
j
. This means we choose a vec-
tor that is maximally far away from C
i
, in terms of
KL divergence to its nearest neighbor in C
i
.
In such a way, we force documents sharing must-
links to have similar topic distributions while docu-
ments sharing cannot-links to have dissimilar topic
distributions. Note that it also encodes constraint as
soft preference rather than hard constraint. We use
Collapsed Gibbs Sampling for LDA inference. Dur-
ing Gibbs Sampling, instead of always drawing
~
?
i
1
currently in submission.
from Dirichlet(~?), we draw
~
?
i
based on the fol-
lowing distribution:
~
?
i
? Dir(?~?+?
M
~?
i
M
+?
C
~?
i
C
) = Dir(~?
i
). (3)
Here, ?
g
, ?
M
and ?
C
are the weights to control the
trade-off among the three terms. In our experiment,
we choose T = 100, ?
g
= ?
M
= ?
C
= 1.
Our evaluation has shown that cLDA is effective
in improving topic model quality. For example, it
achieved a significant topic classification error re-
duction on the 20 Newsgroup dataset. Also, top-
ics learned by cLDA are more coherent than those
learned by standard LDA.
2.1 Active Learning with User Interaction
In this subsection, we present an active learning
framework to iteratively acquire constraints from
users. As shown in Figure 1, given a document col-
lection, the framework first runs standard LDA with
a burnin component. Since it uses a Gibbs sampler
(Griffiths and Steyvers, 2004) to infer topic samples
for each word token, it usually takes hundreds of it-
erations for the sampler to converge to a stable state.
Based on the results of the burnt-in model, the sys-
tem generates a target document and a set of anchor
documents for a user to annotate. Target document is
a document on which the active learner solicits user
feedback, and anchor documents are representatives
of a topic model?s latent topics. If a large portion of
the word tokens in a document belongs to topic i, we
say the document is an anchor document for topic i.
A user judges the content of the target and the
anchor documents and then informs the system
whether the target document is similar to any of the
anchor documents. The user interface is designed
so that the user can drag the target document near
an anchor document if she considers both to be the
same topic. Currently, one target document can be
must-linked to only one anchor document. Since
it is possbile to have multiple topics in one docu-
ment, in the future, we will allow user to add must
links between one target and mulitple anchor doc-
uments. After adding one or more must-links, the
31
system automatically adds cannot-links between the
target document and the rest anchor documents.
Given this input, the system adds them to a con-
straint pool. It then uses cLDA to incorporate these
constraints and generates an updated topic model.
Based on the new topic model, the system chooses a
new target document and several new anchor docu-
ments for the user to annotate. This process contin-
ues until the user is satisfied with the resulting topic
model.
How to choose the target and anchor documents
are the key questions that we consider in the next
subsections.
2.2 Target Document Selection
A target document is defined as a document on
which the active learner solicits user feedback. We
have investigated several strategies for selecting a
target document.
Random: The active learner randomly selects a doc-
ument from the corpus. Although this strategy is
the simplest, it may not be efficient since the model
may have enough information about the document
already.
MaxEntropy: The entropy of a document d is com-
puted as H
d
= ?
?
K
i=1
?
dk
log ?
dk
, where K is the
number of topics, and ? is model?s document-topic
distribution. Therefore, the system will select a doc-
ument about which it is most confused. A uniform
? implies that the model has no topic information
about the document and thus assigns equal probabil-
ity to all topics.
MinLikelihood: The likelihood of a document d is
computed as L
d
= (
?
N
i=1
?
K
k=1
?
ki
?
dk
)/N , where
N is the number of tokens in d, and ? is model?s
topic-word distribution. Since the overall likeli-
hood of the input documents is the objective func-
tion LDA aims to maximize, using this criteria, the
system will choose a document that is most difficult
for which the current model achieves the lowest ob-
jective score.
2.3 Anchor Documents Selection
Given a target document d, the active learner then
generates one or more anchor documents based on
the target document?s topic distribution ?
d
. It filters
out topics with trivial value in ?
d
and extracts an an-
chor topic set T
anc
which only contains topics with
non-trivial value in ?
d
. A trivial ?
di
means that the
mass of ith component in ?
d
is neglectable, which
indicates that the model rarely assign topic i to doc-
ument d. For each topic t in T
anc
, the active learner
selects an anchor document who has minimum Eu-
clidean distance with an ideal anchor ?
?
t
. In the ideal
anchor ?
?
t
, all the components are zero except the
value of the t
th
component is 1. For example, if a
target document d?s ?
d
is {0.5, 0.3, 0.03, 0.02, 0.15}
in a K = 5 topic model, the active learner would
generate T
anc
= {0, 1, 4} and for each t in T
anc
, an
anchor document.
However, it is possible that some topics learned
by LDA are only ?background? topics which have
significant non-trivial probabilities over many doc-
uments (Song et al., 2009). Since background top-
ics are often uninteresting ones, we use a weighted
anchor topic selection method to filter them. A
weighted k
th
component of ?
?
dk
for document d is
defined as follows: ?
?
dk
= ?
dk
/
?
D
i=0
?
ik
. There-
fore, instead of keeping the topics with non-trivial
values, we keep those whose weighted values are
non-trivial.
3 Evaluation
In this section, we evaluate our active learning
framework. Topic models are often evaluated us-
ing perplexity on held-out test data. However, re-
cent work (Boyd-Graber et al., 2009; Chuang et al.,
2013) has shown that human judgment sometimes
is contrary to the perplexity measure. Following
(Mimno et al., 2011), we employ Topic Coherence,
a metric which was shown to be highly consistent
with human judgment, to measure a topic model?s
quality. It relies upon word co-occurrence statistics
within documents, and does not depend on external
resources or human labeling.
We followed (Basu et al., 2004) to create a Mix3
sub-dataset from the 20 Newsgroups data
2
, which
consists of two newsgroups with similar topics
(rec.sport.hockey, rec.sport.baseball) and one with
a distinctive topic (sci.space). We use this dataset
to evaluate the effectiveness of the proposed frame-
work.
3.1 Simulated Experiments
We first burn-in LDA for 500 iterations. Then for
each additional iteration, the active learner generates
one query which consists of one target document and
one or more anchor documents. We simulate user
feedback using the documents? ground truth labels.
If a target document has the same label as one of
the anchor documents, we add a must-link between
them. We also add cannot-links between the target
document and the rest of the anchor documents. All
these constraints are added into a constraint pool.
We also augment the constraint pool with derived
constraints. For example, due to transitivity, if there
is a must-link between (a, b) and (b, c), then we add
2
Available at http://people.csail.mit.edu/
jrennie/20Newsgroups
32
Topic Words
1 writes, like, think, good, know, better, even, people, run, hit
2 space, nasa, system, gov, launch, orbit, moon, earth, access, data
3 game, play, hockey, season, league, fun, wing, cup, shot, score
1 baseball, hit, won, shot, hitter, base, pitching, cub, ball, yankee
2 space, nasa, system, gov, launch, obit, moon, earth, mission, shuttle
3 hockey, nhl, playoff, star, wing, cup, king, detroit, ranger
Table 1: Ten most probable words of each topic before (above) and after active learning (below).
a must link between (a, c). We simulate the process
for 100 iterations to acquire constraints. After that,
we keep cLDA running for 400 more iterations with
the acquired constraints until it converges.
Figure 2: Topic coherence with different number of
iterations.
Figure 2 shows the topic coherence scores for dif-
ferent target document selection strategies. This re-
sult indicates 1). MaxEntropy has the best topic co-
herence score. 2). All active learning strategies out-
perform standard LDA, and the results are statisti-
cally significant at p = 0.05. With standard LDA,
500 more iterations without any constraints does not
improve the topic coherence. However, by active
learning with cLDA for 500 iterations, the topic co-
herences are significantly improved.
Using MaxEntropy target document selection
method, we demonstrate the improvement of the
most probable topic keywords before and after ac-
tive learning. Table 1 shows that before active learn-
ing, topic 1?s most probable words are incoherent
and thus it is difficult to determine the meaning of
the topic . After active learning, in contrast, topic 1?s
most probable words become more consistent with
a ?baseball? topic. This example suggests that the
active learning framework that interactively and it-
eratively acquires pairwise document constraints is
effective in improving the topic model?s quality.
4 Conclusion
We presented a novel active learning framework for
LDA that employs constrained topic modeling to
actively incorporate user feedback encoded as pair-
wise document constraints. With simulated user in-
put, our preliminary results demonstrate the effec-
tiveness of the framework on a benchmark dataset.
In the future, we will perform a formal user study
in which real users will interact with the system to
iteratively refine topic models.
Acknowledgments
This work was supported in part by DARPA contract
D11AP00268.
References
Sugato Basu, A. Banjeree, ER. Mooney, Arindam Baner-
jee, and Raymond J. Mooney. 2004. Active semi-
supervision for pairwise constrained clustering. In
SDM, pages 333?344.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In NIPS.
Jason Chuang, Sonal Gupta, Christopher D. Manning,
and Jeffrey Heer. 2013. Topic model diagnostics:
Assessing domain relevance via topical alignment. In
ICML.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In ACL, pages 248?
257.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP, pages 262?272.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin Madison.
Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X.
Zhou, and Weihong Qian. 2009. Topic and keyword
re-ranking for lda-based topic modeling. In CIKM,
pages 1757?1760.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter. In
NIPS, pages 1973?1981.
33
