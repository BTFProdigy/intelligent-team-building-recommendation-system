Proposition Bank II:  Delving Deeper 
 
 
Olga Babko-Malaya, Martha Palmer, Nianwen Xue, Aravind Joshi1, Seth Kulick 
University of Pennsylvania 
{malayao/mpalmer/xueniwen/joshi/skulick}@linc.cis.upenn.edu 
                                                          
1 Associated with Penn Discourse Treebank (PDTB). Other members of the project are Eleni Miltsakaki, Rashmi Prasad, 
(Univ. of PA) and Bonnie Webber (Univ. of Edinburgh) 
 
 
Abstract 
The PropBank project is creating a corpus of 
text annotated with information about basic 
semantic propositions. PropBank I (Kingsbury 
& Palmer, 2002) added a layer of predicate-
argument information, or semantic roles, to 
the syntactic structures of the English Penn 
Treebank.   This paper presents an overview 
of the second phase of PropBank Annotation, 
PropBank II, which is being applied to Eng-
lish and Chinese, and includes (Neodavid-
sonian) eventuality variables, nominal 
references, sense tagging, and connections to 
the Penn Discourse Treebank (PDTB), a pro-
ject for annotating discourse connectives and 
their arguments. 
1 Introduction 
An important question is the degree to which current 
statistical NLP systems can be made more domain-
independent without prohibitive costs, either in terms of 
engineering or annotation.  The Proposition Bank is 
designed as a broad-coverage resource to facilitate the 
development of more general systems.  It focuses on the 
argument structure of verbs, and provides a complete 
corpus annotated with semantic roles, including partici-
pants traditionally viewed as arguments and ad-
juncts.  Correctly identifying the semantic roles of the 
sentence constituents is a crucial part of interpreting 
text, and in addition to forming a component of the in-
formation extraction problem, can serve as an interme-
diate step in machine translation or automatic 
summarization. 
 
The Proposition Bank project takes a practical approach 
to semantic representation, adding a layer of predicate-
argument information, or semantic roles, to the syntactic 
structures of the Penn Treebank.  The resulting resource 
can be thought of as shallow, in that it does not repre-
sent co-reference, quantification, and many other 
higher-order phenomena, but also broad, in that it cov-
ers every verb in the corpus and allows representative 
statistics to be calculated. The semantic annotation pro-
vided by PropBank is only a first approximation at cap-
turing the full richness of semantic representation. 
Additional annotation of nominalizations and other 
noun predicates has already begun at NYU. This paper 
presents an overview of the second phase of PropBank 
Annotation, PropBank II, which is being applied to Eng-
lish and Chinese and includes (Neodavidsonian) eventu-
ality variables, nominal references, sense tagging, and 
discourse connectives.   
2 PropBank I 
PropBank (Kingsbury & Palmer, 2002) is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II (Marcus, 1994) with `predicate-argument' structures, 
using sense tags for highly polysemous words and se-
mantic role labels for each argument. An important goal 
is to provide consistent semantic role labels across dif-
ferent syntactic realizations of the same verb, as in the 
window in [ARG0 John] broke [ARG1 the window] and 
[ARG1 The window] broke. PropBank can provide fre-
quency counts for (statistical) analysis or generation 
components in a machine translation system, but pro-
vides only a shallow semantic analysis in that the anno-
tation is close to the syntactic structure and each verb is 
its own predicate. 
 
In PropBank, semantic roles are defined on a verb-by-
verb basis.  An individual verb's semantic arguments are 
simply numbered, beginning with 0.  Polysemous verbs 
have several Framesets, corresponding to a relatively 
coarse notion of word senses, with a separate set of 
numbered roles, a roleset, defined for each Frameset.  
For instance, leave has both a DEPART Frameset ([ARG0 
John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 
I] left [ARG1 my pearls] [ARG2 to my daughter-in-law] 
[ARGM-LOC in my will].)   While most Framesets have 
three or four numbered roles, as many as six can appear, 
in particular for certain verbs of motion. Verbs can take 
any of a set of general, adjunct-like arguments 
(ARGMs), such as LOC (location), TMP (time), DIS 
(discourse connectives), PRP (purpose) or DIR (direc-
tion).  Negations (NEG) and modals (MOD) are also 
marked. 
 
The same annotation philosophy has been extended to 
the Penn Chinese Proposition Bank (Xue and Palmer, 
2003). The Chinese PropBank annotation is performed 
on a smaller (250k words) and yet growing corpus an-
notated with syntactic structures (Xue et al2004). The 
same syntactic alternations that form the basis for the 
English PropBank annotation also exist in robust quanti-
ties in Chinese, even though it may not be the case that 
the same exact verbs (meaning verbs that are close 
translations of one another) have the exact same range 
of syntactic realization for Chinese and English.  For 
example, in (1), "xin-nian/New Year  zhao-dai-
hui/reception" plays the same role in (a) and (b), which 
is the event or activity held,  even though it occurs in 
different syntactic positions. Assigning the same argu-
ment label, Arg1, to both instances, captures this regu-
larity. It is worth noting that the predicate ?ju-
xing/hold" does not have passive morphology in (1a), 
despite of what its English translation suggests. Like the 
English PropBank, the adjunct-like elements receive 
more general labels like TMP or LOC, as also illustrated 
in (1). The tag set for Chinese and English PropBanks 
are to a large extent similar and more details can be 
found in (Xue and Palmer, 2003). 
  
(1) a. [ARG1 xin-nian/New Year zhao-dai-
hui/reception] [ARGM-TMP jin-tian/today] [ARGM-
LOC zai/at diao-yu-tai/Diaoyutai guo-bin-guan/state 
guest house ju-xing/hold]  
"The New Year reception was held in Diaoyutai State 
Guest House today." 
 
          b. [ARG0 tang-jia-xuan/Tang Jiaxuan] [ARGM-
TMP jin-tian/today] [ARGM-LOC zai/at diao-yu-
tai/Diaoyutai guo-bin-guan/state guest house] ju-
xing/hold [arg1 xin-nian/New Year zhao-dai-
hui/reception] 
"Tang Jiaxuan was holding the New Year Reception in 
Diaoyutai State Guest House today." 
 
For polysemous verbs that take different sets of seman-
tic roles, we also distinguish different Framesets. (2) 
and (3) illustrate the different Framesets of "tong-
guo/pass", which correspond loosely with major senses 
of the verb.  The Frameset in (2) roughly means "pass 
by voting" while the Frameset illustrated by (3) means 
"pass through". The different Framesets are generally 
reflected in the different alternation patterns, which can 
serve as a cue for statistical systems performing Frame-
set disambiguation. (2) is similar to the causa-
tive/inchoative alternation (Levin, 1993). In contrast, (3) 
shows object drop. 
 
(2) a. [ARG0 guo-hui/Congress] zui-jin/recently tong-
guo/pass le/ASP [ARG1 zhou-ji/interstate yin-hang-
fa/banking law] 
    "The U.S. Congress recently passed the inter-state 
banking law." 
      b. [ARG1 zhou-ji/interstate yin-hang-fa/banking 
law] zui-jin/recently tong-guo/pass le/ASP 
       "The inter-state banking law passed recently." 
 
(3) a. [ARG0 huo-che/train] zheng-zai/now tong-
guo/pass [ARG1 sui-dao/tunnel] 
       "The train is passing through the tunne." 
        b. [ARG0 huo-che/train]  zheng-zai/now  tong-
guo/pass. 
       "The train is passing." 
 
There are also some notable differences between Chi-
nese PropBank and English PropBank. In general, the 
verbs in the Chinese PropBank are less polysemous, 
with the vast majority of the verbs having just one 
Frameset. On the other hand, the Chinese PropBank has 
more verbs (including static verbs which are generally 
translated into adjectives in English) normalized by the 
corpus size.  
3 Adding Event Variables to PropBank 
Event variables provide a rich analytical tool for analyz-
ing verb meaning. Positing that there is an event vari-
able allows for a straightforward representation of the 
logical form of adverbial modifiers, the capturing of 
pronominal reference to events, and the representation 
of nouns that refer to events. For example, event vari-
ables make it possible to have direct reference to an 
event with a noun phrase, as in (4a) destruction, and to 
refer back to an event with a pronoun (as illustrated in 
(4b) That): 
 
(4) a. The destruction of Pompeii happened in the 1st 
century. 
       b. Brutus stabbed Caesar. That was a pivotal event 
in history. 
 
PropBank I annotations can be translated straightfor-
wardly into logical representations with event variables, 
as illustrated in (5), with relations being defined as 
predicates of events, and Args and ArgMs representing 
relations between event variables and corresponding 
phrases.   
 
(5) a. Mr. Bush met him privately, in the White House,          
on Thursday. 
 
     b. PropBank annotation 
  Rel:  met                      
       Arg0: Mr. Bush  
       ArgM-MNR: privately 
       ArgM-LOC: in the White House 
       ArgM-TMP: on Thursday 
 
     c. Logical representation with an event variable  
 ?e meeting(e) & Arg0(e, Mr. Bush) & Arg1(e, he) 
& MNR(e, privately) & LOC(e, ?in the White 
House?) & TIME(e, ?on Thursday?) 
 
As the representation in (5c) shows, we adopt Neo-
davidsonian analysis of events, which follows Parsons 
(1990) in treating arguments on a par with modifiers in 
the event structure. An alternative analysis is the origi-
nal Davidsonian analysis of events (Davidson 1967), 
where the arguments of the verb are analyzed as its 
logical arguments. 
  
Our choice of a Neodavidsonian representation is moti-
vated by its predictions with respect to obligatoriness of 
arguments. Under the Davidsonian approach, arguments 
are logical arguments of the verb and thus must be im-
plied by the meaning of the sentence, either explicitly or 
implicitly (i.e. existentially quantified). On the other 
hand, it has been a crucial assumption in PropBank that 
not all roles must necessarily be present in each sen-
tence. For example, the Frameset for the verb serve, 
shown in (6a) has three roles: Arg0, Arg1, and Arg2. 
Actual usages of the verb, on the other hand, do not 
require the presence of all three roles. For example, the 
sentence in (6b), as its PropBank annotation in (6c) 
shows, does not include Arg1. 
 
(6)  a.  serve.01 "act, work":            
Arg0:worker 
Arg1:job, project 
Arg2:employer 
 
     b.  Each new trading roadblock is likely to be beaten     
by institutions seeking better ways *trace* to serve 
their high-volume clients.  
 
c. Arg0:  *trace* -> institutions 
     REL:    serve 
     Arg2:   their high-volume clients 
 
As the representations in (7) illustrate, only the Neo-
davidsonian representation gives the correct interpreta-
tion of this sentence. 
 
(7) Davidsonian representation: 
 ?e ?z serve(e, institutions, z, their high-volume 
clients) 
 
Neodavidsonian representation: 
     ?e serve(e)&Arg0(e, institutions)&Arg2(e, their 
high-volume clients) 
 
Assuming a Neodavidsonian representation, we can 
analyze all Args and certain types of modifiers as predi-
cates of events.  The types of ArgMs that can be ana-
lyzed as predicates of event variables are shown below: 
 
? MNR:   to manage businesses profitably 
? TMP:    to run the company for 23 years 
? LOC:    to use the notes on the test 
? DIR:     to jump up 
? CAU:    because of ? 
? PRP:      in order to ? 
 
Whereas for the most part, translating these adverbials 
into modifiers of event variables does not require man-
ual annotation, certain constructions need human revi-
sion. For example, in the sentence in (8a) the temporal 
ArgM ?for the past five years? does not modify the event 
variable e introduced by the verb manage, as our auto-
matic translation would predict. The revised analysis of 
this sentence, given in (8b), follows Krifka 1989, who 
proposed that negated sentences refer to maximal events 
? events that have everything that happened during their 
running time as a part. Annotation of this sentence 
would thus require us to introduce an additional event 
variable, the maximal event e?, which has a duration 
?for the past five years? and has no event of unions 
managing wage increases as part. 
 
(8)  a. For the past five years, unions have not managed 
to win wage increases. 
       b. ?e? TMP(e?, ?for the past five years?) &  
??e(e<e? & managing(e) & Arg0(e, unions) & 
Arg1(e, ?win wage increases?)) 
 
Further annotation involves linking empty categories in 
PropBank to event variables in cases of control, as illus-
trated in (9), where event variables can be viewed as the 
appropriate antecedents for PRO, marked as ?*? below: 
    
(9) The car collided with a lorry, * killing both drivers. 
 
And, finally, we will consider tagging variables accord-
ing to the aspectual class of the eventuality they denote, 
such as states or events. Events, such as John built a 
house, involve some kind of change and usually imply 
that some condition, which obtains when the event be-
gins, is terminated by the event. States, on the other 
hand, do not involve any change and hold for varying 
amounts of time. It does not make sense to ask how long 
a state took (as opposed to events), and whether the 
state is culminated or finished.  
 
This distinction between states and events plays an im-
portant role for the temporal analysis of discourse, as 
the following examples (from Kamp and Reyle 1993) 
illustrate: 
 
(10) a. A man entered the White Hart. Bill served him a 
beer. 
       b. I arrived at the Olivers? cottage on Friday night. 
It was not a propitious beginning to my visit. She 
was ill and he in a foul mood. 
 
If a non-initial sentence denotes an event, then it is typi-
cally understood as following the event described by the 
preceding sentence. For example, in (10a), the event of 
Bill serving a beer is understood as taking place after 
the event of ?a man entering the White Hart? was com-
pleted.  On the other hand, states are interpreted as tem-
porally overlapping with the time of the preceding 
sentence, as illustrated in (10b). The sentences she was 
ill and he was in a foul mood seem to describe a state of 
affairs obtaining at the time of the speaker?s arrival.  
 
As this example illustrates, there are different types of 
temporal relations between eventualities (as we will call 
both events and states) and adverbials that modify them, 
such as temporal overlap and temporal containment. 
Furthermore, these relations crucially depend on the 
aspectual properties of the sentence. Translation of PB 
annotations to logical representations with eventuality 
variables and tagging these variables according to their 
aspectual type would thus make it possible to provide an 
analysis of temporal relations. This analysis should also 
be compatible with a higher level of annotation of tem-
poral structure (e.g. Ferro et al 2001).   
4 Annotation of Nominal Coreference 
Our approach to coreference annotation is based on the 
recognition of the different types of relationships that 
might be called "coreference".  The most straightfor-
ward case is that of two semantically definite NPs that 
refer to identical entities, as in (11).  Anaphoric rela-
tions (very broadly defined) are those in which one NP 
(or possessive adjective) has no referential value of its 
own but depends on an antecedent for its interpretation. 
In some cases this can be relatively simple, as in (12), in 
which the pronoun He takes John Smith as its antece-
dent.  However, in some cases, as in (13), the antecedent 
may not even be a referring expression, or can, as in 
(14), refer to an entity that may or may not exist, with 
the non-existent a car being the antecedent of it.  The 
anaphor does not have to be an NP, as in (15), in which 
the possessive their, which takes many companies as its 
antecedent, is an adjective. 
 
(11) John Smith of Company X arrived yesterday.  Mr. 
Smith said that..." 
(12) John Smith of Company X arrived yesterday.  He 
said that..." 
(13) No team spoke about its system. 
(14) I want to buy a car.  I need it to go to work. 
(15) Many companies raised their payouts by more than 
10%. 
 
Another level of complexity is raised by NPs that are 
not anaphors, in that they have their own reference (per-
haps abstract or nonexistent), but are not in an identity 
relationship with an antecedent, but rather describe a 
property of that antecedent.  Typical cases of this are 
predicate nominals, as in (16), or appositives, as in (17), 
and other cases as in (18). 
 
(16) Larry is a university lecturer. 
(17) Larry, the chair of his department, became presi-
dent. 
(18) The stock price fell from $4.02 to $3.85 
 
As has been discussed (e.g., van Deemter & Kibble, 
2001), such cases have fundamentally different proper-
ties than either the identity relationships of (11) or the 
anaphoric relationships of (12)-(15).   
 
Annotation of nominal co-reference is being done in 
two passes. The first pass involves annotation of true 
co-reference between semantically definite NPs`. The 
issue here is to consider what the semantically definite 
nouns are.  Initially, they are defined as proper nouns 
(named entities), either as NPs (America) or prenominal 
adjectives (American politicians).   
 
(19) The last time the S&P 500 yield dropped below 3% 
was in the summer of 1987... There have been only 
seven other times when the yield on the S&P 500 
dropped....   
 
It is reasonable to expand this to definite descriptions, 
so that in (19), the S&P 500 yield and the yield on the 
S&P 500 are marked as coreferring.  However, some 
definite NPs can refer to clauses, not NPs, such as The 
pattern in (20), and we will not do such cases of clausal 
antecedents on the first pass. 
 
(20) The index fell 40% in 1975 and jumped 80% in 
1976.  The pattern is an unusual one. 
 
Anaphoric relations are being done on a "need-to-
annotate" basis.   For each anaphoric NP or possessive 
adjective, the annotator needs to determine its antece-
dent.  As discussed, this is a different type of relation 
than identity, and this distinction will be noted in the 
annotation. The issue here is what we consider an ana-
phoric element to be.  We consider all cases of pro-
nouns, possessives, reflexives, and NPs with that/those 
to be potential cases of anaphors (again, broadly de-
fined). However, as with definite NPs, we only mark 
those that have an NP antecedent, and not clausal ante-
cedents.  For example, in (21), it refers to the current 
3.3% reading, and so would be marked as being in an 
antecedent-anaphor relation. In (22), it refers to having 
the dividend increases, which is not an NP, and so 
would not be marked as being in an anaphor relation in 
the first pass. Similar considerations apply to potential 
anaphors like those NP, that NP, etc. 
 
 (21) ...the current 3.3% reading isn't as troublesome as 
it might have been. 
(22) Having the dividend increases is a supportive ele-
ment in the market outlook, but I don't think it's a 
main consideration". 
 
Note that placing the burden on the anaphors to deter-
mine what gets marked as being in an anaphor-
antecedent leaves it open as to what the antecedent 
might be, other than the requirement just mentioned of it 
being an NP.  Not only might it be non-referring NPs as 
in  (13) or (14), it could even be a generic, as in (23), in 
which books is the antecedent for they. 
 
(23) I like books.  They make me smile. 
 
The second pass will tackle the more difficult issues: 
 
1. Descriptive NPs, as in (16)-(18).  While the informa-
tion provided by these cases would be extremely valu-
able for information extraction and other systems, there 
are some uncertain issues here, mostly focusing on how 
such descriptors describe the antecedent at different 
moments in time and/or space.  The crucial question is 
therefore what to take the descriptor to be.   
 
(24) Henry Higgins might become the president of 
Dreamy Detergents. 
 
For example, in (18), it can't be just $4.02 and $3.85, 
since this does not include information about *when* 
the stock price had such values. The same issue arises 
for (17).  As van Deemter & Kibble point out, such 
cases can interact with issues of modality in uncertain 
ways, as illustrated in (24).  Just saying that in (24) the 
president of Dreamy Detergents is in the same type of 
relationship with Henry Higgins as a university lecturer 
is with Larry in (16) would be very misleading. 
 
2. Clausal antecedents - Here we will handle cases of it 
and other anaphor elements and definite NPs referring 
to non-NPs as antecedents, as in (21).  This will most 
likely be done by referring to the eventuality variable 
associated with the antecedent. 
5 Linking to the Penn Discourse Treebank 
(PDTB)  
The Penn Discourse Treebank (PDTB) is currently be-
ing built by the PDTB team at the University of Penn-
sylvania, providing the next appropriate level of  
annotation: the annotation of the predicate argument 
structure of connectives (Miltsakaki et al2004a/b). The 
PDTB project is based on the idea that discourse con-
nectives can be thought of as predicates with their asso-
ciated argument structure. This perspective of discourse 
is based on a series of papers extending lexicalized tree-
adjoining grammar (LTAG) to discourse (DLTAG), 
beginning with Webber and Joshi (1998).2  This level of 
annotation is quite complex for a variety of reasons, 
such as the lack of available literature describing dis-
course connectives and frequent occurrences of empty 
(lexically null) connectives between two sentences that 
cannot be ignored. Also, unlike the predicates at the 
sentence level, some of the discourse connectives, espe-
cially discourse adverbials, take their arguments ana-
phorically and not structurally, requiring an intimate 
association with event variable representation.  
 
The long-range goal of the PDTB project is to develop a 
large scale and reliably annotated corpus that will en-
code coherence relations associated with discourse con-
nectives, including their argument structure and 
anaphoric links, thus exposing a clearly defined level of 
discourse structure and supporting the extraction of a 
range of inferences associated with discourse connec-
tives. This annotation will reference the Penn Treebank 
(PTB) annotations as well as PropBank. 
 
In PDTB, a variety of connectives are considered, such 
as subordinate and coordinate conjunctions, adverbial 
connectives and implicit connectives amounting to a 
total of approximately 20,000 annotations; 10,000 im-
                                                          
2 The PDTB annotations are deliberately kept independ-
ent of DLTAG framework for two reasons: (1) to make the 
annotated corpus widely useful to researchers working in 
different frameworks and (2) to make the annotation task 
easier, thereby increasing interannotator reliability. 
plicit connectives and 10,000 annotations of the 250 
explicit connectives identified in the corpus (for details 
see (Miltsakaki et al2004a and Miltsakaki et al2004b).  
Current annotations in PDTB are performed by four 
annotators. Individual annotation proceeds one connec-
tive at a time. This way, the annotators quickly gain 
experience with that connective and develop a better 
understanding of its predicate-argument characteristics. 
For the annotation of implicit connectives, the annota-
tors are required to provide an explicit connective that 
best expressed the inferred relation. 
 
The PDTB is expected to be released by November 
2005. The final version of the corpus  will also contain 
characterizations of the semantic roles associated with 
the arguments of each type of connective as well as 
links to PropBank. 
6.   Annotation of Word Senses 
The critical question with respect to sense tagging in-
volves the choice of senses.  In other words, which 
sense inventory, and which level of granularity with 
respect to that sense inventory?  The PropBank Frames 
Files for the verbs include coarse-grained sense distinc-
tions based primarily on usages of a verb that have dif-
ferent numbers of predicate-arguments. These are 
termed Framesets ? referring to the set of roles for each 
one and the corresponding set of syntactic frames.  We 
are currently sense-tagging the annotated predicates for 
lemmas with multiple Framesets, which can be done 
quickly and accurately with an inter-annotator agree-
ment of over 90%.  The distinctions made by the 
Framesets are very coarse, and each one would map to 
several standard dictionary entries for the lemma in 
question.  More fine-grained sense distinctions could be 
useful for Automatic Content Extraction, yet it remains 
to be determined exactly which distinctions are neces-
sary and what methodology should be followed to pro-
vide additional word sense annotation. 
 
Palmer et al(2004b) present an hierarchical approach to 
verb senses, where different levels of sense distinctions, 
from PropBank Framesets to WordNet senses, form a 
continuum of granularity. At the intermediate level of 
sense hierarchy we are considering manual groupings of 
the SENSEVAL-2 verb senses (Palmer, et.al., 2004a), 
developed in a separate project. Given a large disagree-
ment rate between annotators (average inter-annotator 
agreement rate for Senseval-2 verbs was only 71%), 
verbs were grouped by two or more people into sets of 
closely related senses, with grouping differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. These groupings of 
WordNet senses were shown to reconcile a substantial 
portion of the manual and automatic tagging disagree-
ments, showing that many of these disagreements are 
fairly subtle.  Using the groups as a more coarse-grained 
set of sense distinctions improved ITA and system 
scores by almost 10%, to 82% and 69%, respectively 
(Palmer, et. al. 2004a). 
 
We have been investigating whether or not the groups 
can provide an intermediate level of hierarchy in be-
tween the PropBank Framesets and the WN 1.7 senses.  
Based on our existing WN 1.7 tags and Frameset tags of 
the Senseval2 verbs in the Penn Treebank, 95% of the 
verb instances map directly from sense groups to 
Framesets, with each Frameset typically corresponding 
to two or more sense groups. Using the PropBank 
coarse-grained senses as a starting place, and WordNet 
sense tagging for over 1000 verbs produced automati-
cally through mapping VerbNet to PropBank (Kipper, 
et. al., 2004), we have the makings of a large scale tag-
ging experiment on the Penn Treebank.  This will en-
able investigations into the applicability of clearly 
defined criteria for sense distinctions at varying levels 
of granularity, and produce a large, 1M word corpus of 
sense-tagged text for training WSD systems 
 
The hierarchical approach to verb senses, as utilized by 
most standard dictionaries as well as Hector (Atkins, 
?93), and as applied to SENSEVAL-2, presents obvious 
advantages for the problem of Word Sense Disambigua-
tion. The human annotation task is simplified, since 
there are fewer choices at each level and clearer distinc-
tions between them.  The automated systems can com-
bine training data from closely related senses to 
overcome the sparse data problem, and both humans 
and systems can back off to a more coarse-grained 
choice when fine-grained choices prove too difficult.  
 
Conclusion 
This paper has presented an overview of the second 
phase of PropBank Annotation, PropBank II, which is 
being applied to English and Chinese. It  includes (Neo-
davidsonian) eventuality variables, nominal references, 
an hierarchical approach to sense tagging, and connec-
tions to the Penn Discourse Treebank (PDTB), a project 
for annotating discourse connectives and their argu-
ments. 
References 
 
Atkins, S. (1993) Tools for computer-aided corpus lexi-
cography: The Hector Project.  Actu Linguistica Hunguricu, 
41:5-72. 
 
Carlson, L., Marcu, D. and Okurowski, M. E. (2002). 
Building a Discourse-Tagged Corpus in the Framework of 
Rhetorical Structure Theory. In Current Directions in Dis-
course and Dialogue, Jan van Kuppevelt and Ronnie Smith 
eds., Kluwer Academic Publishers. To appear. 
 
Davidson, D. 1967.  The Logical Form of Action Sen-
tences. In The Logic of Decision and Action, ed. Nicholas 
Rescher.  81--95. Pittsburgh: University of Pittsburgh 
Press.  Republished in Donald Davidson, Essays on Actions 
and Events,Oxford University Press, Oxford, 1980. 
 
Edmonds, P. and Cotton, S. 2001. SENSEVAL-2: Over-
view. In Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, ACL-SIGLEX, Toulouse, France. 
 
Ferro L, I. Mani, B. Sundheim and G.Wilson 2001 TIDES 
Temporal Annotation Guidelines, MITRE Technical Report, 
MTR 01W0000041.  
 
Kamp, H. and U.Reyle. 1993. From Discourse to Logic, 
Kluwer, Dordrecht. 
 
Kingsbury, P. and Palmer, M, (2002), From TreeBank to 
PropBank, Third International Conference on Language  Re-
sources and Evaluation, LREC-02, Las Palmas, Canary Is-
lands, Spain, May 28- June 3. 
 
Kilgarriff, A. and Palmer, M.. 2000. Introduction to the 
special issue on Senseval, Computers and the Humanities, 
34(1-2):1-13. 
 
Kipper K., B. Snyder, and M. Palmer. (to appear, 2004) 
"Extending a verb-lexicon using a semantically annotated 
corpus". Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC-04). Lisbon, 
Portugal, 2004. 
 
Krifka, M. 1989. Nominalreferenz und Zeitkonstitution. 
M?nchen, Wilhelm Fink Verlag 
 
Levin, B. 1993. English Verb Classes and Alternations: a 
Preliminary Investigation. Chicago: The University of Chi-
cago Press. 
 
Mann, W. and S. Thompson. 1986. ?Relational Proposi-
tions in Discourse?, Discourse Processes 9, 57-90. 
Marcu, D. 2000. The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press. 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004a. 
The Penn Discourse Treebank. In Proceedings of the 4th In-
ternational Conference on Language Resources and Evalua-
tion (LREC 2004), Lisbon. 
 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004b. 
Annotation of Discourse Connectives and Their Arguments, in 
Proceedings of the HLT-EACL Workshop on Frontiers in 
Corpus Annotation, Boston, Massachussetts. 
      Palmer, M., Dang, H. T, and Fellbaum, C., 2004a.  Making 
fine-grained and coarse-grained sense distinctions, both manu-
ally and automatically, under revision for Natural Language 
Engineering. 
Palmer, M., Babko-Malaya, O., Dang, H. T., 2004b. Dif-
ferent Sense Granularities for Different Applications, to ap-
pear in the Scalable Natural Language Understanding 
Workshop, held in conjunction with HLT/NAACL-04, May, 
2004. 
 
Parsons, T. 1990.  Events in the Semantics of Eng-
lish.  Cambridge, MA: MIT Press. 
  
van Deemter, K. and R. Kibble. 2000. ?On Coreferring: 
Coreference in MUC and Related Annotation Schemes?, 
Computational Linguistics 26:629-637. 
 
Webber B. and A. Joshi. 1998. Anchoring a lexicalized 
tree-adjoining grammar for discourse. In ACL/COLING 
Workshop on Discourse Relations and Discourse Markers, 
Montreal, Canada, pp. 41-48. 
 
 
Xue, N. and Palmer, M. 2003. Annotating the Propositions 
in the Penn Chinese Treebank. In the Proceedings of the Sec-
ond SIGHAN Workshop on Chinese Language Processing.  
Sapporo, Japan. 
  
Xue, Nianwen, Xia, Fei, Chiou, Fu-dong and Palmer, 
Martha. 2004. The Penn Chinese Treebank: phrase structure 
annotation of a large corpus. Natural Language Engineering, 
10(4):1-30, June 2004.  
 
 
Different Sense Granularities for Different Applications 
 
 
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang 
University of Pennsylvania 
{mpalmer/malayao/htd}@linc.cis.upenn.edu 
 
 
Abstract 
This paper describes an hierarchical approach 
to WordNet sense distinctions that provides 
different types of automatic Word Sense Dis-
ambiguation (WSD) systems, which perform 
at varying levels of accuracy.  For tasks where 
fine-grained sense distinctions may not be es-
sential, an accurate coarse-grained WSD sys-
tem may be sufficient. The paper discusses the 
criteria behind the three different levels of 
sense granularity, as well as the machine learn-
ing approach used by the WSD system. 
1 Introduction 
The difficulty of finding consistent criteria for making 
sense distinctions has been thoroughly attested to in the 
literature (Kilgarriff, ?97, Hanks, ?00).  Difficulties have 
been found with truth-theoretical criteria, linguistic crite-
ria and definitional criteria (Sparck-Jones, ?86, Geer-
aerts, ?93).  In spite of the proliferation of dictionaries, 
there is no methodology by which two lexicographers 
working independently are guaranteed to derive the same 
set of distinctions for a given word, with objects and 
events vying for which is the most difficult to character-
ize (Cruse, ?86, Apresjan, ?74, Pustejovsky, ?91, ?95).   
 
On the other hand, accurate Word Sense Disambiguation 
(WSD) could significantly improve the precision of In-
formation Retrieval by ensuring that the senses of verbs 
in the retrieved documents match the sense of the verb in 
the query.  For example, the two queries What do you 
call a successful movie? and Whom do you call for a 
successful movie? submitted to AskJeeves both retrieve 
the same set of documents, even though they are asking 
quite different questions, referencing very different 
senses of call.  The documents retrieved are also not very 
relevant, again because they do not distinguish which 
matches contain relevant senses and which do not. 
 
Tips on Being a Successful Movie Vampire ... I shall 
call the police. 
 
Successful Casting Call & Shoot for ``Clash of Em-
pires'' ... thank everyone for their participation in the 
making of yesterday's movie. 
 
Demme's casting is also highly entertaining, although I 
wouldn't go so far as to call it successful. This  movie's 
resemblance to its predecessor is pretty vague... 
 
VHS Movies: Successful Cold Call Selling: Over 100 
New Ideas, Scripts, and Examples from the Nation's 
Foremost Sales Trainer. 
 
The two senses of call in the two queries can be easily 
distinguished by their differing predicate-argument 
structures.  They are also separate senses in WordNet, 
but WordNet has an additional 26 senses for call, and the 
current best performance of an automatic Word Sense 
Disambiguation system this type of polysemous verb is 
only 60.2% (Dang and Palmer, 2002).  Is it possible that 
sense distinctions that are less fine-grained than Word-
Net?s distinctions could be made more reliably, and 
could still benefit this type of NLP application?   
 
The idea of underspecification as a solution to WSD has 
been proposed in Buitelaar 2000 (among others), who 
pointed out that for some applications, such as document 
categorization, information retrieval, and information 
extraction it may be sufficient to know if a given word 
belongs to a certain class of WordNet senses or under-
specified sense. On the other hand, there is evidence that 
machine translation of languages as diverse as Chinese 
and English will require all of the fine-grained sense 
distinctions that WordNet is capable of providing, and 
even more (Ng, et al2003, Palmer, et. al., to appear).   
 
An hierarchical approach to verb senses, of the type dis-
cussed in this paper, presents obvious advantages for the 
problem of word sense disambiguation. The human an-
notation task is simplified, since there are fewer choices 
at each level and clearer distinctions between them.  The 
automated systems can combine training data from 
closely related senses to overcome the sparse data prob-
lem, and both humans and systems can back off to a 
more coarse-grained choice when fine-grained choices 
prove too difficult. 
 
The approach to verb senses presented in this paper as-
sumes three different levels of sense distinctions: Prop-
Bank Framesets, WordNet groupings, and WordNet 
senses.  In a project for the semantic annotation of predi-
cate-argument structure, PropBank, we have made 
coarse-grained sense distinctions for the 700 most 
polysemous verbs in the Penn TreeBank (Kingsbury and 
Palmer, ?02).  These distinctions are based primarily on 
different subcategorization frames that require different 
argument label annotations. In a separate project, as dis-
cussed in Palmer et al2004, we have grouped 
SENSEVAL-2 verb senses (which came from WordNet 
1.7). These manual groupings were shown to reconcile a 
substantial portion of the manual and automatic tagging 
disagreements, showing that many of these disagree-
ments are fairly subtle (Palmer, et.al., ?04).   
 
The tree levels of sense distinctions form a continuum of 
granularity. Our criterion for the Framesets, being pri-
marily syntactic, is also the most clear cut. These distinc-
tions are based primarily on usages of a verb that have 
different numbers of predicate-arguments, however they 
also separate verb senses on semantic grounds, if these 
senses are not closely related. Sense groupings provide 
an intermediate level of hierarchy, where groups are 
distinguished by more fine-grained criteria.  Both 
Frameset and grouping distinctions can be made consis-
tently by humans and systems (over 90% accuracy for 
Framesets and 82% for groupings) and are surprisingly 
compatible; 95% of our groups map directly onto a sin-
gle PropBank sense.   
 
2  Background 
2.1 Propbank 
PropBank [Kingsbury & Palmer, 2002] is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II [Marcus, 1994] with dependency structures (or `predi-
cate-argument' structures), using sense tags for highly 
polysemous words and semantic role labels for each de-
pendency. An important goal is to provide consistent 
semantic role labels across different syntactic realiza-
tions of the same verb, as in the window in [ARG0 John] 
broke [ARG1 the window] and [ARG1 The window] broke. 
PropBank can provide frequency counts for (statistical) 
analysis or generation components in a machine transla-
tion system, but provides only a shallow semantic analy-
sis in that the annotation is close to the syntactic 
structure and each verb is its own predicate. 
 
In addition to the annotated corpus, PropBank provides a 
lexicon that lists, for each broad meaning of each anno-
tated verb, its Frameset, i.e., the possible arguments in 
the predicate and their labels and all possible syntactic 
realizations.  The notion of ``meaning'' used is fairly 
coarse-grained, and it is typically motivated from differ-
ing syntactic behavior.  The Frameset alo includes a 
``descriptor'' field for each role which is intended for use 
during annotation and as documentation, but which does 
not have any theoretical standing. The collection of 
Frameset entries for a verb is referred to as the verb's 
frame.  As an example of a PropBank entry, we give the 
frame for the verb leave below.  Currently, there are 
frames for over 3,000 verbs, with a total of just over 
4,300 Framesets described.  Of these 3,000 verb frames, 
only a small percentage 21.8 % (700) have more than 
one Frameset, with less than 100 verbs with 4 or more.  
The process of sense-tagging the PropBank corpus with 
the Frameset tags has just been completed. 
 
The criteria used for the Framesets are primarily syntac-
tic and clear cut. The guiding principle is that two verb 
meanings are distinguished as different framesets if they 
have distinct subcategorization frames. For example, the 
verb ?leave? has 2 framesets with the following frames, 
illustrated by the examples in (1) and (2): 
 
Frameset 1:  move away from 
Arg0:entity leaving 
Arg1:place left 
 
Frameset 2:  give  
Arg0:giver / leaver 
Arg1:thing given 
Arg2:benefactive / given-to 
 
(1) John left the room. 
(2) Mary left her daughter-in-law her pearls in her will 
 
2.2 WordNet  Sense Groupings 
In a separate project, as part of Senseval tagging exer-
cises, we have developed a lexicon with another level of 
coarse-grained distinctions, as described below. 
 
The Senseval-1 workshop (Kilgarriff and Palmer, 2000) 
provided convincing evidence that supervised automatic 
systems can perform word sense disambiguation (WSD) 
satisfactorily, given clear, consistent sense distinctions 
and suitable training data.  However, the Hector lexicon 
that was used as the sense inventory was very small and 
under proprietary constraints, and the question remained 
whether it was possible to have a publicly available, 
broad-coverage lexical resource for English and other 
languages, with the requisite clear, consistent sense dis-
tinctions. 
 
Subsequently, the Senseval-2 (Edmonds and Cotton, 
2001) exercise was run, which included WSD tasks for 
10 languages.  A concerted effort was made to use exist-
ing WordNets as sense inventories because of their 
widespread popularity and availability. Each language 
had a choice between the lexical sample task and the all-
words task.  The most polysemous words in the English 
Lexical Sample task are the 29 verbs, with an average 
polysemy of 16.28 senses using the pre-release version 
of WordNet 1.7.  Double blind annotation by two lin-
guistically trained annotators was performed on corpus 
instances, with a third linguist adjudicating between in-
ter-annotator differences to create the ?Gold Standard.?  
The average inter-annotator agreement rate was only 
71%, which is comparable to the 73% agreement for all 
words in SemCor, with a much lower average polysemy. 
However, a comparison of system performance on words 
of similar polysemy in Senseval-1 and Senseval-2 
showed very little difference in accuracy (Palmer et al, 
submitted).  In spite of the lower inter-annotator agree-
ment figures for Senseval-2, the double blind annotation 
and adjudication provided a reliable enough filter to en-
sure consistently tagged data with WordNet senses.  
Even so, the high polysemy of the WordNet 1.7 entries 
on average poses a challenge for automatic word sense 
disambiguation.  In addition, WordNet only gives a flat 
listing of alternative senses, unlike most standard dic-
tionaries which are more structured and often provide 
hierarchical entries. To address this lack, the verbs were 
grouped by two or more people, with differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. 
 
The criteria used for groupings included syntactic and 
semantic ones. Syntactic structure performed two dis-
tinct functions in our groupings. Recognizable alterna-
tions with similar corresponding predicate-argument 
structures were often a factor in choosing to group 
senses together, as in the Levin classes and PropBank, 
whereas distinct subcategorization frames were also of-
ten a factor in putting senses in separate groups.  Fur-
thermore, senses were grouped together if they were 
more specialized versions of a general sense.  The se-
mantic criteria for grouping senses separately included 
differences in semantic classes of arguments (abstract 
versus concrete, animal versus human, animacy versus 
inanimacy, different instrument types...), differences in 
the number and type of arguments (often reflected in the 
subcategorization frame as discussed above), differences 
in entailments (whether an argument refers to a created 
entity or a resultant state), differences in the type of 
event (abstract, concrete, mental, emotional...), whether 
there is a specialized subject domain, etc.   
 
Senseval-2 verb inter-annotator disagreements were re-
duced by more than a third when evaluated against the 
groups, from 29% to 18%, and by over half in a separate 
study, from 28% to 12%.    A similar number of random 
groups provided almost no benefit to the inter-annotator 
agreement figures (74% instead of 71%), confirming the 
greater coherence of the manual groupings. 
3 Mapping of Sense Groups to Framesets  
Groupings of senses for Senseval-2, as discussed above, 
use both syntactic and semantic criteria.  Propbank, on 
the other hand, uses mostly syntactic cues to divide verb 
senses into framesets. As a result, framesets are more 
general than sense-groups and usually incorporate sev-
eral sense groups. We have been investigating whether 
or not the groups developed for SENSEVAL-2 can provide 
an intermediate level of hierarchy in between the Prop-
Bank Framesets and the WN 1.7 senses, and our initial 
results are promising.  Based on our existing WN 1.7 
tags and frameset tags of the Senseval2 verbs in the Penn 
TreeBank, 95% of the verb instances map directly from 
sense groups to framesets, with each frameset typically 
corresponding to two or more sense groups, as illustrated 
by the tables 1-4 for the verbs ?serve?, ?leave?, ?pull?, and 
?see?1  below. 
 
As the tables 1-4 illustrate, the criteria used to split the 
Framesets into groups are as follows:  
  
 1) Syntactic Frames. Most verb senses which allow syn-
tactic alternations (such as transitive/inchoative, unspeci-
fied object deletion, etc) are analyzed as one sense 
group. However, in some cases, as illustrated by the verb 
leave, intransitive and transitive uses are distinguished as 
different sense groups: 
 
Group 1: DEPART (Ship leaves at midnight) 
Group 2: LEAVE BEHIND (She left a mess.) 
 
The DEPART sense of the verb can be used transitively if 
the object specifies the place of departure. The LEAVE 
BEHIND sense is more general and allows syntactic varia-
tion as well as different semantic types of NPs. In Prop-
Bank, these groups are unified as one frameset (Frameset 
1 MOVE AWAY FROM). 
                                                          
1 All these verbs have one or more additional framesets, which 
correspond to one group or sense, and therefore are not in-
cluded here 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:   
WN1 (function) 
WN3(contribute to) 
WN12 (answer) 
 
His freedom served him well 
The scandal served to increase his popularity 
Nothing else will serve 
GROUP 2:   
WN2 (do duty) 
WN13 (do military service) 
 
She served in Congress 
She served in Vietnam 
GROUP 5:    
WN7 (devote one?s efforts) 
WN10 (attend to) 
 
She served the art of music 
May I serve you? 
serve 01:  Act, work 
 
Roles: 
Arg0:worker 
Arg1:job,  project 
Arg2:employer 
 
GROUP 3:   
WN4 (be used by) 
WN8 (serve well)                
WN14 (service) 
 
 
The garage served to shelter horses 
Art serves commerce 
Male animals serve the females for breeding 
purposes 
 
Table 1. Frameset  serve 01.
            
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 2:   
WN2 (leave behind)  
 WN12 (be survived by)  
 WN14  (forget) 
 
She left a mess 
He left six children I left my keys 
GROUP 1:   
WN1  (go away) 
 
WN5 (exit, go out) 
WN8 (depart) 
 
The ship leaves at midnight 
Leave the room 
The teenager left home 
GROUP 3:   
WN3 (to act)   
WN7 (result in) 
 
The inflation left them penniless 
Her blood left a stain on the napkin 
SINGLETON  
WN4 (leave behind) 
 
Leave it as is 
leave 02: Move away 
from  
 
Roles:  
Arg0:entity leaving 
Arg1:thing left 
Arg2 :attribute / sec-
ondary predication 
 
SINGLETON  
WN6 (allow for, provide) 
 
Leave lots of time for the trip 
 
Table 2. Frameset leave 02. 
 
 
 
2. Optional Arguments.  In PropBank verbs of manner 
of motion and verbs of directed motion are usually 
grouped into one frameset. For example, one of the 
framesets of the verb pull (TRY) TO CAUSE MOTION 
unifies the following two group senses: 
Group 1: MOVE ALONG (pull a sled) 
Group 2: MOVE INTO A CERTAIN DIRECTION (The van 
pulled up) 
    
Although the frame for the frameset 1 of the verb pull 
has a ?direction? argument, this argument does not 
have to be present (or implied), and verbs with this 
frame can also be understood as verbs of manner of 
motion in PropBank. 
 
3) Syntactic variation of arguments. Syntactic varia-
tion in objects can also be used to distinguish sense 
groups, but are not taken into consideration for distin-
guishing framesets.  Here both noun phrases and sen-
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (draw)  
WN4(apply force)  
WN9 (cause to move) 
WN10 (operate) 
WN13 (hit) 
 
Pull a sled 
Pull the rope 
A declining dollar pulled  down the export figures 
Pull the oars 
Pull the ball 
GROUP 2:   
WN2 (attract) 
WN12 (rip) 
 
The ad pulled in many potential customers 
 Pull the cooked chicken into strips 
GROUP 3:   
WN3 (move) 
WN7 (steer) 
 
The car pulls to the right 
 Pull the car over 
pull.01:  try to cause motion 
 
Roles:  
Arg0:puller 
Arg1:thing pulled 
Arg2: direction or predication 
Arg3:extent, distance moved 
  
 
GROUP 4:   
WN6 (pull out) 
WN15 (extract) 
WN17(take away) 
 
The mugger pulled a knife on his victim 
 Pull weeds 
 Pull the old soup cans from the shelf 
 
Table 3. Frameset pull 01. 
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (perceive by sight)  
WN7 (watch)  
WN19 (observe as if with an eye) 
 WN20 (examine)       
 
Can you the bird? 
See a movie 
The camera saw the burglary 
I must see your passport 
GROUP 3:   
WN3 (witness) 
 WN6 (learn) 
 
I want to see the results 
I see that you have been promoted 
GROUP 4:   
WN5 (consider) 
WN24 (interpret) 
 
I don?t see the situation quite as negatively 
What message do you see in this letter? 
GROUP 5:   
WN8 (determine) 
WN10 (check) 
WN14 (attend) 
 
See whether it works 
See that the curtains are closed 
Could you see about lunch? 
see.01: view  
 
Roles:  
Arg0:viewer 
Arg1:thing viewed 
Arg2:secondary attribute 
 
GROUP 6:   
WN11 (see a professional) 
WN15 (receive as a guest) 
 
You should see a lawyer 
The doctor will see you now 
 
Table 4. Frameset see 01. 
 
tential complements are contained in the same frame-
set.  These could also be distinguished by the type of 
event, a physical perception vs. an abstract or mental 
perception, but these would also not distinguished by 
PropBank. 
   
Group 1: PERCEIVE BY SIGHT (Can you see the bird?) 
Group 5: DETERMINE, CHECK (See whether it works) 
 
4) Semantic classes of arguments. Differences in se-
mantic classes of arguments, such as ANIMACY versus 
INANIMACY, are also not considered for distinguishing 
framesets. The verb serve, for example, has the follow-
ing group senses, the second of which requires an 
ANIMATE agent, which are unified as one frameset in 
PropBank: 
 
Group 1: FUNCTION (His freedom served him well) 
Group 2: WORK (He served in Congress)   
 
Most of the criteria which are used to split Framesets 
into groupings, as the tables above illustrate, are se-
mantic. These distinctions, although more fine-grained 
than Framesets, are still more easily distinguished than 
WordNet senses. 
 
Mismatches between Framesets and groupings usually 
occur for the following two reasons. First, some senses 
can be missing in the PropBank, if they do not occur in 
the corpus.  Second, given that PropBank is an annota-
tion of the Wall Street Journal, it often distinguishes 
obscure financial senses of the verb as separate senses.  
4 Experiments with Automatic WSD  
We have also been investigating the suitability of these 
distinctions for training automatic Word Sense 
Disambiguation systems.  The system that we used to 
tag verbs with their frameset is the same maximum 
entropy system as that of Dang and Palmer (2002), 
including both topical and local features. Topical 
features looked for the presence of keywords occurring 
anywhere in the sentence and any surrounding 
sentences provided as context (usually one or two 
sentences).  The set of keywords is specific to each 
lemma to be disambiguated, and is determined 
automatically from training data so as to minimize the 
entropy of the probability of the senses conditioned on 
the keyword.  
The local features for a verb w in a particular sentence 
tend to look only within the smallest clause containing 
w.  They include collocational features requiring no 
linguistic prepro essing beyond part-of-speech tagging 
(1), syntactic features that capture relations 
between the verb and its complements (2-4), and se-
mantic features that incorporate information about 
noun classes for objects (5-6): 
1) the word w, the part of speech of w, and 
words at positions -2, -1, +1, +2, relative to w 
2) whether or not the sentence is passive 
3) whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree) 
4) the words (if any) in the positions of subject, 
direct object, indirect object, particle, preposi-
tional complement (and its object) 
5) a Named Entity tag (PERSON, 
ORGANIZATION, LOCATION) for proper 
nouns appearing in (4). 
6) all possible WordNet synsets and hypernyms 
for the nouns appearing in (4). 
The system performed well on the English verbs in 
Senseval-2, achieving an accuracy of 60.2% when tag-
ging verbs with their fine-grained WordNet senses, and 
70.2% when tagging with the more coarse-grained 
sense groups. 
 
 
Verb Framesets Instances Accuracy
call 11 522 0.835 
carry 4 195 0.933 
develop 2 240 0.938 
draw 3 94 0.926 
dress 3 15 0.800 
drive 2 99 0.808 
keep 5 136 0.919 
leave 3 147 0.762 
live 4 125 0.888 
play 5 98 0.806 
pull 6 88 0.784 
see 2 187 0.995 
serve 2 150 0.967 
strike 10 59 0.610 
train 2 17 0.941 
treat 2 51 0.863 
turn 14 141 0.638 
use 2 820 0.988 
wash 2 8 0.875 
work 7 398 0.955 
Table 5.  Frameset tagging results 
 
For frameset tagging, we collected a total of 3590 in-
stances of 20 verbs in the PropBank corpus that had 
been annotated with their framesets.  The verbs all had 
more than one possible frameset and were a subset of 
the ones used for the English lexical sample task of 
Senseval-2.  Local features for frameset taging were 
extracted using the gold-standard part-of-speech tags 
and bracketing of the Penn Treebank.  Table 5 shows 
the number of framesets, the number of instances, and 
the system accuracy for each verb using 10-fold cross-
validation. The overall accuracy of our automatic 
frameset tagging was 90.0%, compared to a baseline 
accuracy of 73.5% if verbs are tagged with their most 
frequent frameset. While the data is only a subset of 
that used in Senseval-2, it is clear that framesets can be 
much more reliably tagged than fine-grained WordNet 
senses and even sense groups. 
Conclusion 
This paper described an hierarchical approach to 
WordNet sense distinctions that provided different 
types of automatic Word Sense Disambiguation (WSD) 
systems, which perform at varying levels of accuracy. 
We have described three different levels of sense 
granularity, with PropBank Framesets being the most 
syntactic, the most coarse-grained, and most easily 
reproduced.  A set of manual groupings devised for 
Senseval2 provides a middle level of granularity that 
mediates between Framesets and WordNet.   For tasks 
where fine-grained sense distinctions may not be essen-
tial such as an AskJeeves information retrieval task, an 
accurate coarse-grained WSD system such as our 
Frameset tagger may be sufficient. There is evidence, 
however, that machine translation of languages as di-
verse as Chinese and English might require all of the 
fine-grained sense distinctions of WordNet, and even 
more (Ng, et al2003, Palmer, et. al., to appear).   
References 
 
Apresjan, J. D. .(1974) Regular polysemy, Linguistics, 
142:5?32. 
 
Atkins, S. (1993) Tools for computer-aided corpus 
lexicography: The Hector Project.  Actu Linguis-
tica Hunguricu, 41:5-72. 
 
Buitelaar, P.P (2000). Reducing Lexical Semantic 
Complexity with Systematic Polysemous Classes 
and Underspecification. In Poceedings of the 
ANLP Workshop on Syntactic and Semantic Com-
plexity in NLP Systems. Seattle, WA. 
 
Cruse, D. A., (1986), Lexical Semantics, Cambridge 
University Press, Cambridge, UK, 1986. 
 
Dang, H. T. and Palmer, M., (2002). Combining Con-
textual Features for Word Sense Disambiguation. 
In Proceedings of the Workshop on Word Sense 
Disambiguation: Recent Successes and Future Di-
rections, Philadelphia, Pa. 
 
Edmonds, P. and Cotton, S. (2001). SENSEVAL-2: 
Overview. In Proceedings of SENSEVAL-2: Sec-
ond International Workshop on Evaluating Word 
Sense Disambiguation Systems, ACL-SIGLEX, 
Toulouse, France. 
 
Hanks, P., (2000), Do word meanings exist? Com-
puters and the Humanities, Special Issue on 
SENSEVAL, 34(1-2). 
 
Geeraerts, D., (1993), Vagueness's puzzles, polysemy's 
vagaries, Cognitive Linguistics, 4. 
 
Kilgarriff, A., (1997), I don't believe in word senses, 
Computers and the Humanities, 31(2). 
 
Kilgarriff, A. and Palmer, M., (2000), Introduction to 
the special issue on Senseval, Computers and the 
Humanities, 34(1-2):1-13. 
 
Kingsbury, P., and Palmer, M, (2002), From TreeBank 
to PropBank, Third International Conference on 
Language  Resources and Evaluation, LREC-02, 
Las Palmas, Canary Islands, Spain, May 28- June 
3. 
 
Marcus, M, (1994), The Penn TreeBank: A revised 
corpus design for extracting predicate argument 
structure, In Proceedings of the ARPA Human 
Language Technology Workshop, Princeton, NJ.   
 
Ng, H. T., & Wang, B., & Chan, Y. S. (2003). 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In the Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL-03). Sapporo, 
Japan, July. 
Palmer, M., Dang, H. T., and Fellbaum, C., (to appear, 
2004), Making fine-grained and coarse-grained 
sense distinctions, both manually and automati-
cally, under revision for Natural Language Engi-
neering. 
Pustejovsky, J. (1991) The Generative Lexicon,  in 
Computational Linguistics 17(4).   
Pustejovsky, J. (1995) The Generative Lexicon, Cam-
bridge, MIT Press, Mass. 
 
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 61?67,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Parallel Proposition Bank II for Chinese and English?
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, Benjamin Snyder
Department of Computer and Information Science
University of Pennsylvania
{mpalmer/xueniwen/malayao/Jinying/bsnyder3}@linc.cis.upenn.edu
Abstract
The Proposition Bank (PropBank) project
is aimed at creating a corpus of text an-
notated with information about seman-
tic propositions. The second phase of
the project, PropBank II adds additional
levels of semantic annotation which in-
clude eventuality variables, co-reference,
coarse-grained sense tags, and discourse
connectives. This paper presents the re-
sults of the parallel PropBank II project,
which adds these richer layers of semantic
annotation to the first 100K of the Chinese
Treebank and its English translation. Our
preliminary analysis supports the hypoth-
esis that this additional annotation recon-
ciles many of the surface differences be-
tween the two languages.
1 Introduction
There is a pressing need for a consensus on a task-
oriented level of semantic representation that can en-
able the development of powerful new semantic ana-
lyzers in the same way that the Penn Treebank (Mar-
cus et al, 1993) enabled the development of sta-
tistical syntactic parsers (Collins, 1999; Charniak,
2001). We believe that shallow semantics expressed
as a dependency structure, i.e., predicate-argument
structure, for verbs, participial modifiers, and nom-
inalizations provides a feasible level of annotation
that would be of great benefit. This annotation, cou-
pled with word senses, minimal co-reference links,
?This work is funded by the NSF via Grant EIA02-05448 .
event identifiers, and discourse and temporal rela-
tions, could provide the foundation for a major ad-
vance in our ability to automatically extract salient
relationships from text. This will in turn facilitate
breakthroughs in message understanding, machine
translation, fact retrieval, and information retrieval.
The Proposition Bank project is a major step towards
providing this type of annotation. It takes a prac-
tical approach to semantic representation, adding a
layer of predicate argument information, or seman-
tic roles, to the syntactic structures of the Penn Tree-
bank (Palmer et al, 2005). The Frame Files that
provide guidance to the annotators constitute a rich
English lexicon with explicit ties between syntac-
tic realizations and coarse-grained senses, Frame-
sets. PropBank Framesets are distinguished primar-
ily by syntactic criteria such as differences in sub-
categorization frames, and can be seen as the top-
level of an hierarchy of sense distinctions. Group-
ings of fine-grained WordNet senses, such as those
developed for Senseval2 (Palmer et al, to appear)
provide an intermediate level, where groups are dis-
tinguished by either syntactic or semantic criteria.
WordNet senses constitute the bottom level. The
PropBank Frameset distinctions, which can be made
consistently by humans and systems (over 90% ac-
curacy for both), are surprisingly compatible with
the groupings; 95% of the groups map directly onto
a single PropBank frameset sense (Palmer et al,
2004).
The semantic annotation provided by PropBank
is only a first approximation at capturing the full
richness of semantic representation. Additional an-
notation of nominalizations and other noun pred-
61
icates has already begun at NYU. This paper de-
scribes the results of PropBank II, a project to pro-
vide richer semantic annotation to structures that
have already been propbanked, specifically, eventu-
ality ID.s, coreference, coarse-grained sense tags,
and discourse connectives. Of special interest to the
machine translation community is our finding, pre-
sented in this paper, that PropBank II annotation rec-
onciles many of the surface differences of the two
languages.
2 PropBank I
PropBank (Palmer et al, 2005) is an annotation of
the Wall Street Journal portion of the Penn Treebank
II (Marcus et al, 1994) with ?predicate-argument?
structures, using sense tags for highly polysemous
words and semantic role labels for each argument.
An important goal is to provide consistent seman-
tic role labels across different syntactic realizations
of the same verb, as in the window in [ARG0 John]
broke [ARG1 the window] and [ARG1 The window]
broke. PropBank can provide frequency counts for
(statistical) analysis or generation components in
a machine translation system, but provides only a
shallow semantic analysis in that the annotation is
close to the syntactic structure and each verb is its
own predicate.
In PropBank, semantic roles are defined on a
verb-by-verb basis. An individual verb?s seman-
tic arguments are simply numbered, beginning with
0. Polysemous verbs have several framesets, cor-
responding to a relatively coarse notion of word
senses, with a separate set of numbered roles, a role-
set, defined for each Frameset. For instance, leave
has both a DEPART Frameset ([ARG0 John] left
[ARG1 the room]) and a GIVE Frameset, ([ARG0
I] left [ARG1 my pearls] [ARG2 to my daughter-in-
law] [ARGM-LOC in my will].) While most Frame-
sets have three or four numbered roles, as many
as six can appear, in particular for certain verbs of
motion. Verbs can take any of a set of general,
adjunct-like arguments (ARGMs), such as LOC (lo-
cation), TMP (time), DIS (discourse connectives),
PRP (purpose) or DIR (direction). Negations (NEG)
and modals (MOD) are also marked.
There are several other annotation projects,
FrameNet (Baker et al, 1998), Salsa (Ellsworth et
al., 2004), and the Prague Tectogrammatics (Haji-
cova and Kucerova, 2002), that share similar goals.
Berkeley.s FrameNet project, (Baker et al, 1998;
Fillmore and Atkins, 1998; Johnson et al, 2002)
is committed to producing rich semantic frames on
which the annotation is based, but it is less con-
cerned with annotating complete texts, concentrat-
ing instead on annotating a set of examples for each
predicator (including verbs, nouns and adjectives),
and attempting to describe the network of relations
among the semantic frames. For instance, the buyer
of a buy event and the seller of a sell event would
both be Arg0.s (Agents) in PropBank, while in
FrameNet one is the BUYER and the other is the
SELLER. The Salsa project (Ellsworth et al, 2004)
in Germany is producing a German lexicon based
on the FrameNet semantic frames and annotating a
large German newswire corpus. PropBank style an-
notation is being used for verbs which do not yet
have FrameNet frames defined.
The PropBank annotation philosophy has been
extended to the Penn Chinese Proposition Bank
(Xue and Palmer, 2003). The Chinese PropBank an-
notation is performed on a smaller (250k words) and
yet growing corpus annotated with syntactic struc-
tures (Xue et al, To appear). The same syntac-
tic alternations that form the basis for the English
PropBank annotation also exist in robust quantities
in Chinese, even though it may not be the case that
the same exact verbs (meaning verbs that are close
translations of one another) have the exact same
range of syntactic realization for Chinese and En-
glish. For example, in (1), ?#c/New Year???/
reception? plays the same role in (a) and (b), which
is the event or activity held, even though it occurs in
different syntactic positions. Assigning the same ar-
gument label, Arg1, to both instances, captures this
regularity. It is worth noting that the predicate /?
1/hold? does not have passive morphology in (1a),
despite what its English translation suggests. Like
the English PropBank, the adjunct-like elements re-
ceive more general labels like TMP or LOC, as also
illustrated in (1). The functional tags for Chinese
and English PropBanks are to a large extent similar
and more details can be found in (Xue and Palmer,
2003).
(1) a. [ARG1 #c/New Year ???/reception] [ARGM-
TMP 8 U/today] [ARGM-LOC 3/at M ~
62
/DiaoyutaiIU,/state guest house ?1/hold]
?The New Year reception was held in Diao-yutai
State Guest House today.?
b. [ARG0 /[^/Tang Jiaxuan] [ARGM-TMP 8
U/today] [ARGM-LOC 3/at M~/Diaoyutai I
U,/state guest house] ?1/ hold [arg1 #c/New
Year???/reception]
?Tang Jiaxuan was holding the New Year reception in
Diaoyutai State Guest House today.?
3 A Parallel PropBank II
As discussed above, PropBank II adds richer se-
mantic annotation to the PropBank I predicate ar-
gument structures, notably eventuality variables,
co-references, coarse-grained sense tags (Babko-
Malaya et al, 2004; Babko-Malaya and Palmer,
2005), and discourse connectives (Xue, To appear)
To create our parallel PropBank II, we began with
the first 100K words of the Chinese Treebank which
had already been propbanked, and which we had
had translated into English. The English transla-
tion was first treebanked and then propbanked, and
we are now in the process of adding the PropBank
II annotation to both the English and the Chinese
propbanks. We will discuss our progress on each of
the three individual components of PropBank II in
turn, bringing out translation issues along the way
that have been highlighted by the additional anno-
tation. In general we find that this level of abstrac-
tion facilitates the alignment of the source and tar-
get language descriptions: event ID.s and event
coreferences simplify the mappings between verbal
and nominal events; English coarse-grained sense
tags correspond to unique Chinese lemmas; and dis-
course connectives correspond well.
3.1 Eventuality variables
Positing eventuality1 variables provides a straight-
forward way to represent the semantics of adver-
bial modifiers of events and capture nominal and
pronominal references to events. Given that the ar-
guments and adjuncts for the verbs are already an-
notated in Propbank I, adding eventuality variables
is for the most part straightforward. The example
in (2) illustrates a Propbank I annotation, which is
identified with a unique event id in Propbank II.
1The term ?eventuality? is used here to refer to events and
states.
(2) a. Mr. Bush met him privately in the White House on
Thursday.
b. Propbank I: Rel: met, Arg0: Mr. Bush, Arg1: him,
ArgM-MNR: privately, ArgM-LOC: in the White
House, ArgM-TMP: on Thursday.
c. Propbank II: ?e meeting(e) & Arg0(e,Mr. Bush) &
Arg1(e, him) & MNR (e, privately) & LOC(e, in the
White House) & TMP (e, on Thursday).
Annotation of event variables starts by auto-
matically associating all Propbank I annotations
with potential event ids. Since not all annotations
actually denote eventualities, we manually filter
out selected classes of verbs. We further attempt
to identify all nouns and nominals which describe
eventualities as well as all sentential arguments of
the verbs which refer to events. And, finally, part
of the PropBank II annotation involves tagging of
event coreference for pronouns as well as empty
categories. All these tasks are discussed in more
detail below.
Identifying event modifiers. The actual annota-
tion starts from the presumption that all verbs are
events or states and nouns are not. All the verbs in
the corpus are automatically assigned a unique event
identifier and the manual part of the task becomes (i)
identification of verbs or verb senses that do not de-
note eventualities, (ii) identification of nouns that do
denote events. For example, in (3), begin is an as-
pectual verb that does not introduce an event vari-
able, but rather modifies the verb -take., as is
supported by the fact that it is translated as an ad-
verb ??/initially? in the corresponding Chinese sen-
tence.
(3) ?:/key u?/develop /DE ??/medicine ?/and )
?/biology E?/technology, #/new E?/technology,
#/new ?/material, O ? ?/computer 9/and A
^/application, 1/photo >/electric ?Nz/integration
/etc. ?/industry ?/already ?/initially ?/take 5
/shape.
/Key developments in industries such as medicine,
biotechnology, new materials, computer and its applica-
tions, protoelectric integration, etc. have begun to take
shape.0
Nominalizations as events Although most nouns
do not introduce eventualities, some do and these
nouns are generally nominalizations2 . This is true
2The problem of identifying nouns which denote events is
addressed as part of the sense-tagging tagging. Detailed discus-
sion can be found in (Babko-Malaya and Palmer, 2005).
63
for both English and Chinese, as is illustrated in (4).
Both /u?/develop0and /\/deepening0are
nominalized verbs that denote events. Having a par-
allel propbank annotated with event variables allows
us to see how events are lined up in the two lan-
guages and how their lexical realizations can vary.
The nominalized verbs in Chinese can be translated
into verbs or their nominalizations, as is shown in
the alternative translations of the Chinese original
in (4). What makes this particular example even
more interesting is the fact that the adjective mod-
ifier of the events, /??/continued0, can ac-
tually be realized as an aspectual verb in English.
The semantic representations of the Propbank II an-
notation, however, are preserved: both the aspec-
tual verb /continue0in English and the adjective
/??/continued0in Chinese are modifiers of the
events denoted by /u?/development0and /
\/deepening0.
(4) ? X/with ? I/China ? L/economy /DE ?
?/continuedu?/development ?/and?/to	/outside
m?/open/DE??/continued\/deepen ,
/As China.s economy continues to develop and
its practice of opening to the outside continues to
deepen,0
/With the continued development of China.s economy
and the continued deepening of its practice of opening to
the outside,0
Event Coreference Another aspect of the event
variable annotation involves identifying pronominal
expressions that corefer with events. These pronom-
inal expressions may be overt, as in the Chinese ex-
ample in (5), while others correspond to null pro-
nouns, marked as pro3. in the Treebank annotations,
as in (6):
(5) ?/additionally, ? ?/export ??/commodity (
/structure UY/continue ` z/optimize, c/last
year ? ?/industry ? ? ?/finished product ?
?/export /quota ?/account for I/entire country
? ?/export o /quantity /DE ' ?/proportion
?/reach z??l??:8/85.6 percent, ?/this ?
?/clearly L?/indicate ?I/China ??/industry 
?/product/DE?E/produce Y?/level'/compared
with L /past k/have 
/LE ?/very ?/big J
p/improvement.
/Moreover, the structure of export com-modities
continues to optimize, and last year.s export volume
of manufactured products ac-counts for 85.6 percent of
3The small *pro* and big *PRO* distinction made in the
Chinese Treebank is exploratory in nature. The idea is that it is
easier to erase this distinction if it turns out to be implausible or
infeasible than to add it if it turns out to be important.
the whole countries.export, *pro* clearly indicating
that China.s industrial product manufacturing level has
improved.0
(6) ?
/these ?J/achievement ?/among k/have ?
z n? l/138 ?/item Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 70?77,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Issues in Synchronizing the English Treebank and PropBank 
 
Olga Babko-Malayaa, Ann Biesa, Ann Taylorb, Szuting Yia, Martha Palmerc,  
Mitch Marcusa, Seth Kulicka and Libin Shena 
aUniversity of Pennsylvania, bUniversity of York, cUniversity of Colorado 
{malayao,bies}@ldc.upenn.edu, {szuting,mitch,skulick,libin}@linc.cis.upenn.edu,
at9@york.ac.uk, Martha.Palmer@colorado.edu
 
Abstract 
The PropBank primarily adds semantic 
role labels to the syntactic constituents in 
the parsed trees of the Treebank. The 
goal is for automatic semantic role label-
ing to be able to use the domain of local-
ity of a predicate in order to find its ar-
guments. In principle, this is exactly what 
is wanted, but in practice the PropBank 
annotators often make choices that do not 
actually conform to the Treebank parses. 
As a result, the syntactic features ex-
tracted by automatic semantic role label-
ing systems are often inconsistent and 
contradictory. This paper discusses in de-
tail the types of mismatches between the 
syntactic bracketing and the semantic 
role labeling that can be found, and our 
plans for reconciling them. 
1 Introduction 
The PropBank corpus annotates the entire Penn 
Treebank with predicate argument structures by 
adding semantic role labels to the syntactic 
constituents of the Penn Treebank.  
Theoretically, it is straightforward for PropBank 
annotators to locate possible arguments based on 
the syntactic structure given by a parse tree, and 
mark the located constituent with its argument 
label. We would expect a one-to-one mapping 
between syntactic constituents and semantic 
arguments. However, in practice, PropBank 
annotators often make choices that do not 
actually conform to the Penn Treebank parses. 
The discrepancies between the PropBank and 
the Penn Treebank obstruct the study of the syn-
tax and semantics interface and pose an immedi-
ate problem to an automatic semantic role label-
ing system. A semantic role labeling system is 
trained on many syntactic features extracted from 
the parse trees, and the discrepancies make the 
training data inconsistent and contradictory. In 
this paper we discuss in detail the types of mis-
matches between the syntactic bracketing and the 
semantic role labeling that can be found, and our 
plans for reconciling them. We also investigate 
the sources of the disagreements, which types of 
disagreements can be resolved automatically, 
which types require manual adjudication, and for 
which types an agreement between syntactic and 
semantic representations cannot be reached. 
1.1 Treebank  
The Penn Treebank annotates text for syntactic 
structure, including syntactic argument structure 
and rough semantic information. Treebank anno-
tation involves two tasks: part-of-speech tagging 
and syntactic annotation. 
The first task is to provide a part-of-speech tag 
for every token. Particularly relevant for Prop-
Bank work, verbs in any form (active, passive, 
gerund, infinitive, etc.) are marked with a verbal 
part of speech (VBP, VBN, VBG, VB, etc.). 
(Marcus, et al 1993; Santorini 1990) 
The syntactic annotation task consists of 
marking constituent boundaries, inserting empty 
categories (traces of movement, PRO, pro), 
showing the relationships between constituents 
(argument/adjunct structures), and specifying a 
particular subset of adverbial roles. (Marcus, et 
al. 1994; Bies, et al 1995) 
Constituent boundaries are shown through 
syntactic node labels in the trees. In the simplest 
case, a node will contain an entire constituent, 
complete with any associated arguments or 
modifiers. However, in structures involving syn-
tactic movement, sub-constituents may be dis-
placed. In these cases, Treebank annotation 
represents the original position with a trace and 
shows the relationship as co-indexing. In (1) be-
low, for example, the direct object of entail is 
shown with the trace *T*, which is coindexed to 
the WHNP node of the question word what. 
 
(1) (SBARQ (WHNP-1 (WP What ))
(SQ (VBZ does )
(NP-SBJ (JJ industrial )
(NN emigration ))
(VP (VB entail)
(NP *T*-1)))
(. ?))
70
In (2), the relative clause modifying a journal-
ist has been separated from that NP by the prepo-
sitional phrase to al Riyadh, which is an argu-
ment of the verb sent. The position where the 
relative clause originated or ?belongs? is shown 
by the trace *ICH*, which is coindexed to the 
SBAR node containing the relative clause con-
stituent. 
 
(2)(S (NP-SBJ You)  
(VP sent
(NP (NP a journalist)
(SBAR *ICH*-2))
(PP-DIR to
(NP al Riyadh))
(SBAR-2
(WHNP-3 who)
(S (NP-SBJ *T*-3)
(VP served
(NP (NP the name)
(PP of
(NP Lebanon)))
(ADVP-MNR
magnificently))))))
 
Empty subjects which are not traces of move-
ment, such as PRO and pro, are shown as * (see 
the null subject of the infinite clause in (4) be-
low). These null subjects are coindexed with a 
governing NP if the syntax allows. The null sub-
ject of an infinitive clause complement to a noun 
is, however, not coindexed with another node in 
the tree in the syntax. This coindexing is shown 
as a semantic coindexing in the PropBank anno-
tation. 
The distinction between syntactic arguments 
and adjuncts of the verb or verb phrase is made 
through the use of functional dashtags rather than 
with a structural difference. Both arguments and 
adjuncts are children of the VP node. No distinc-
tion is made between VP-level modification and 
S-level modification. All constituents that appear 
before the verb are children of S and sisters of 
VP; all constituents that appear after the verb are 
children of VP.  
Syntactic arguments of the verb are NP-SBJ, 
NP (no dashtag), SBAR (either ?NOM-SBJ or no 
dashtag), S (either ?NOM-SBJ or no dashtag),  
-DTV, -CLR (closely/clearly related), -DIR with 
directional verbs. 
Adjuncts or modifiers of the verb or sentence 
are any constituent with any other adverbial 
dashtag, PP (no dashtag), ADVP (no dashtag). 
Adverbial constituents are marked with a more 
specific functional dashtag if they belong to one 
of the more specific types in the annotation sys-
tem (temporal ?TMP, locative ?LOC, manner  
?MNR, purpose ?PRP, etc.). 
Inside NPs, the argument/adjunct distinction is 
shown structurally. Argument constituents (S and 
SBAR only) are children of NP, sister to the head 
noun. Adjunct constituents are sister to the NP 
that contains the head noun, child of the NP that 
contains both:  
 
(NP (NP head)
(PP adjunct)) 
1.2 PropBank   
PropBank is an annotation of predicate-argument 
structures on top of syntactically parsed, or Tree-
banked, structures. (Palmer, et al 2005; Babko-
Malaya, 2005). More specifically, PropBank 
annotation involves three tasks: argument 
labeling, annotation of modifiers, and creating 
co-reference chains for empty categories.  
The first goal is to provide consistent argu-
ment labels across different syntactic realizations 
of the same verb, as in   
 
(3) [ARG0 John] broke [ARG1 the window]   
 [ARG1 The window] broke.  
 
As this example shows, semantic arguments 
are tagged with numbered argument labels, such 
as Arg0, Arg1, Arg2, where these labels are de-
fined on a verb by verb basis.  
The second task of the PropBank annotation 
involves assigning functional tags to all modifi-
ers of the verb, such as MNR (manner), LOC 
(locative), TMP (temporal), DIS (discourse con-
nectives), PRP (purpose) or DIR (direction) and 
others. 
And, finally, PropBank annotation involves 
finding antecedents for ?empty? arguments of the 
verbs, as in (4). The subject of the verb leave in 
this example is represented as an empty category 
[*] in Treebank. In PropBank, all empty catego-
ries which could be co-referred with a NP within 
the same sentence are linked in ?co-reference? 
chains:  
 
(4) I made a decision [*] to leave 
 
Rel:    leave,   
Arg0: [*] -> I 
 
As the following sections show, all three tasks 
of PropBank annotation result in structures 
which differ in certain respects from the corre-
sponding Treebank structures. Section 2 presents 
71
our approach to reconciling the differences be-
tween Treebank and PropBank with respect to 
the third task, which links empty categories with 
their antecedents. Section 3 introduces mis-
matches between syntactic constituency in Tree-
bank and PropBank. Mismatches between modi-
fier labels are not addressed in this paper and are 
left for future work. 
2 Coreference and syntactic chains  
PropBank chains include all syntactic chains 
(represented in the Treebank) plus other cases of 
nominal semantic coreference, including those  
in which the coreferring NP is not a syntactic 
antecedent. For example, according to PropBank 
guidelines, if a trace is coindexed with a NP in 
Treebank, then the chain should be reconstructed: 
 
(5) What-1 do you like [*T*-1]? 
 
Original PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*] -> What 
 
Such chains usually include traces of A and A? 
movement and PRO for subject and object con-
trol. On the other hand, not all instances of PROs 
have syntactic antecedents. As the following ex-
ample illustrates, subjects of infinitival verbs and 
gerunds might have antecedents within the same 
sentence, which cannot be linked as a syntactic 
chain. 
 
(6) On the issue of abortion , Marshall Coleman 
wants  to take away your right  [*] to choose 
and give it to the politicians .  
 
ARG0:          [*] -> your 
REL:           choose 
 
Given that the goal of PropBank is to find all 
semantic arguments of the verbs, the links be-
tween empty categories and their coreferring NPs 
are important, independent of whether they are 
syntactically coindexed or not. In order to recon-
cile the differences between Treebank and Prop-
Bank annotations, we decided to revise Prop-
Bank annotation and view it as a 3 stage process. 
First, PropBank annotators should not recon-
struct syntactic chains, but rather tag empty cate-
gories as arguments. For example, under the new 
approach annotators would simply tag the trace 
as the Arg1 argument in (7): 
(7) What-1 do you like [*T*-1]? 
 
Revised PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*]  
  
As the second stage, syntactic chains will be re-
constructed automatically, based on the 
coindexation provided by Treebank (note that the 
trace is coindexed with the NP What in (7)). And, 
finally, coreference annotation will be done on 
top of the resulting resource, with the goal of 
finding antecedents for the remaining empty 
categories, including empty subjects of infinitival 
verbs and gerunds.   
One of the advantages of this approach is that 
it allows us to distinguish different types of 
chains, such as syntactic chains (i.e., chains 
which are derived as the result of syntactic 
movement, or control coreference), direct 
coreference chains (as illustrated by the example 
in (6)), and semantic type links for other ?indi-
rect? types of links between an empty category 
and its antecedent.  
Syntactic chains are annotated in Treebank, 
and are reconstructed automatically in PropBank. 
The annotation of direct coreference chains is 
done manually on top of Treebank, and is re-
stricted to empty categories that are not 
coindexed with any NP in Treebank. And, finally, 
as we show next, a semantic type link is used for 
relative clauses and a coindex link for verbs of 
saying. 
A semantic type link is used when the antece-
dent and the empty category do not refer to the 
same entity, but do have a certain kind of rela-
tionship. For example, consider the relative 
clause in (8):  
 
(8) Answers that we?d like to have 
 
Treebank annotation: 
(NP (NP answers)
(SBAR (WHNP-6 which)
(S (NP-SBJ-3 we)
(VP 'd
(VP like
(S (NP-SBJ *-3)
(VP to
(VP have
(NP *T*-6)
))))))))
 
In Treebank, the object of the verb have is a trace, 
which is coindexed with the relative pronoun. In 
72
the original PropBank annotation, a further link 
is provided, which specifies the relative pronoun 
as being of ?semantic type? answers.  
 
(9) Original PropBank annotation: 
Arg1:    [NP *T*-6] -> which -> answers 
 rel:         have 
 Arg0:     [NP-SBJ *-3] -> we 
 
This additional link between which and answers 
is important for many applications that make use 
of preferences for semantic types of verb argu-
ments, such as Word Sense Disambiguation 
(Chen & Palmer 2005). In the new annotation 
scheme, annotators will first label traces as ar-
guments: 
 
(10) Revised PropBank annotation (stage 1): 
Rel:  have 
Arg1: [*T*-6]  
Arg0: [NP-SBJ *-3] 
 
As the next stage, the trace [*T*-6] will be 
linked to the relative pronoun automatically (in 
addition to the chain [NP-SBJ *-3] -> we being 
automatically reconstructed). As the third stage, 
PropBank annotators will link which to answers. 
However, this chain will be labeled as a ?seman-
tic type? to distinguish it from direct coreference 
chains and to indicate that there is no identity 
relation between the coindexed elements. 
Verbs of saying illustrate another case of links 
rather than coreference chains. In many sen-
tences with direct speech, the clause which intro-
duces a verb of saying is ?embedded? into the 
utterance. Syntactically this presents a problem 
for both Treebank and Propbank annotation. In 
Treebank, the original annotation style required a 
trace coindexed to the highest S node as the ar-
gument of the verb of saying, indicating syntactic 
movement. 
 
(11) Among other things, they said  [*T*-1] , Mr. 
Azoff would develop musical acts for a new 
record label . 
 
Treebank annotation: 
(S-1 (PP Among
(NP other things))
(PRN ,
(S (NP-SBJ they)
(VP said
(SBAR 0
(S *T*-1))))
,)
(NP-SBJ Mr. Azoff)
(VP would
(VP develop
(NP (NP musical acts)
(PP for
(NP a new record
label)))))
.)
In PropBank, the different pieces of the utterance, 
including the trace under the verb said, were 
concatenated 
 
(12) Original PropBank annotation: 
ARG1:      [ Among other things] [ Mr. 
Azoff] [ would develop musical acts for a 
new record label] [ [*T*-1]] 
ARG0:       they 
rel:        said 
 
Under the new approach, in stage one, Tree-
bank annotation will introduce not a trace of the 
S clause, but rather *?*, an empty category indi-
cating ellipsis. In stage three, PropBank annota-
tors will link this null element to the S node, but 
the resulting chain will not be viewed as  ?direct? 
coreference. A special tag will be used for this 
link, in order to distinguish it from other types of 
chains. 
 
(13) Revised PropBank  annotation: 
ARG1:      [*?*] (-> S) 
ARG0:       they 
rel:        said 
3 Differences in syntactic constituency  
3.1 Extractions of mismatches between 
PropBank and Treebank 
In order to make the necessary changes to both 
the Treebank and the PropBank, we have to first 
find all instances of mismatches. We have used 
two methods to do this: 1) examining the argu-
ment locations; 2) examining the discontinuous 
arguments. 
 
Argument Locations  In a parse tree which ex-
presses the syntactic structure of a sentence, a 
semantic argument occupies specific syntactic 
locations: it appears in a subject position, a verb 
complement location or an adjunct location. 
Relative to the predicate, its argument is either a 
sister node, or a sister node of the predicate?s 
ancestor. We extracted cases of PropBank argu-
ments which do not attach to the predicate spine, 
and filtered out VP coordination cases. For ex-
ample, the following case is a problematic one 
because the argument PP node is embedded too 
73
deeply in an NP node and hence it cannot find a 
connection with the main predicate verb lifted. 
This is an example of a PropBank annotation 
error. 
 
(14) (VP (VBD[rel] lifted) 
(NP us) )
(NP-EXT
(NP a good 12-inches)
(PP-LOC[ARGM-LOC] above
(NP the water level))))
 
However, the following case is not problem-
atic because we consider the ArgM PP to be a 
sister node of the predicate verb given the VP 
coordination structure:  
 
(15) (VP (VP (VB[rel] buy)  
(NP the basket of ? )
(PP in whichever market ?))
(CC and)
(VP (VBP sell)
(NP them)
(PP[ARGM] in the more
expensive market)))
 
Discontinuous Arguments happen when Prop-
Bank annotators need to concatenate several 
Treebank constituents to form an argument.  Dis-
continuous arguments often represent different 
opinions between PropBank and Treebank anno-
tators regarding the interpretations of the sen-
tence structure. 
For example, in the following case, the Prop-
Bank concatenates the NP and the PP to be the 
Arg1. In this case, the disagreement on PP at-
tachment is simply a Treebank annotation error. 
 
(16) The region lacks necessary mechanisms for 
handling the aid and accounting items. 
 
Treebank annotation: 
(VP lacks
(NP necessary mechanisms)
(PP for
(NP handing the aid?)))
 
PropBank annotation: 
REL: lacks 
Arg1: [NP necessary mechanisms][PP for 
handling the aid and accounting items] 
 
All of these examples have been classified into 
the following categories: (1) attachment ambi-
guities, (2) different policy decisions, and (3) 
cases where one-to-one mapping cannot be pre-
served. 
3.2 Attachment ambiguities  
Many cases of mismatches between Treebank 
and PropBank constituents are the result of am-
biguous interpretations. The most common ex-
amples are cases of modifier attachment ambi-
guities, including PP attachment. In cases of am-
biguous interpretations, we are trying to separate 
cases which can be resolved automatically from 
those which require manual adjudication. 
 
PP-Attachment  The most typical case of PP 
attachment annotation disagreement is shown in 
(17).  
 
(17) She wrote a letter for Mary. 
 
Treebank annotation: 
(VP wrote
(NP (NP a letter)
(PP for
(NP Mary))))
 
PropBank annotation: 
REL: write 
Arg1: a letter 
Arg2: for Mary 
 
In (17), the PP ?for Mary? is attached to the 
verb in PropBank and to the NP in Treebank. 
This disagreement may have been influenced by 
the set of roles of the verb ?write?, which in-
cludes a beneficiary as its argument.  
 
(18) Frameset write:  Arg0: writer 
   Arg1: thing written 
   Arg2: beneficiary 
 
Examples of this type cannot be automatically 
resolved and require manual adjudication. 
Adverb Attachment  Some cases of modifier 
attachment ambiguities, on the other hand, could 
be automatically resolved. Many cases of mis-
matches are of the type shown in (19), where a 
directional adverbial follows the verb. In Tree-
bank, this adverbial is analyzed as part of an 
ADVP which is the argument of the verb in 
question. However, in PropBank, it is annotated 
as a separate ArgM-DIR.  
(19) Everything is going back to Korea or Japan. 
 
 
74
Treebank annotation:  
(S (NP-SBJ (NN Everything) )
(VP (VBZ is)
(VP (VBG[rel] going)
(ADVP-DIR
(RB[ARGM-DIR] back)
(PP[ARG2] (TO to)
(NP (NNP Korea)
(CC and)
(NNP Japan)
))))) (. .))
 
Original PropBank annotation: 
Rel: going 
ArgM-DIR: back 
Arg2: to Korea or Japan 
 
For examples of this type, we have decided to 
automatically reconcile PropBank annotations to 
be consistent with Treebank, as shown in (20). 
 
(20) Revised PropBank annotation: 
Rel:  going 
Arg2: back to Korea or Japan 
3.3 Sentential complements 
Another area of significant mismatch between 
Treebank and PropBank annotation involves sen-
tential complements, both infinitival clauses and 
small clauses. In general, Treebank annotation 
allows many more verbs to take sentential com-
plements than PropBank annotation. 
For example, the Treebank annotation of the 
sentence in (21) gives the verb keep a sentential 
complement which has their markets active un-
der the S as the subject of the complement clause. 
PropBank annotation, on the other hand, does not 
mark the clause but rather labels each subcon-
stituent as a separate argument. 
 
(21)  ?keep their markets active 
 
Treebank annotation: 
(VP keep
(S (NP-SBJ their markets)
(ADJP-PRD active)))
 
PropBank annotation: 
REL: keep 
Arg1: their markets 
Arg2: active 
 
In Propbank, an important criterion for decid-
ing whether a verb takes an S argument, or de-
composes it into two arguments (usually tagged 
as Arg1 and Arg2) is based on the semantic in-
terpretation of the argument, e.g. whether the 
argument can be interpreted as an event or pro-
position. 
For example, causative verbs (e.g. make, get), 
verbs of perception (see, hear), and intensional 
verbs (want, need, believe), among others, are 
analyzed as taking an S clause, which is inter-
preted as an event in the case of causative verbs 
and verbs of perception, and as a proposition in 
the case of intensional verbs. On the other hand, 
?label? verbs (name, call, entitle, label, etc.), do 
not select for an event or proposition and are 
analyzed as having 3 arguments: Arg0, Arg1, 
and Arg2. 
Treebank criteria for distinguishing arguments, 
on the other hand, were based on syntactic 
considerations, which did not always match with 
Propbank. For example, in Treebank, evidence of 
the syntactic category of argument that a verb 
can take is used as part of the decision process 
about whether to allow the verb to take a small 
clause. Verbs that take finite or non-finite (verbal) 
clausal arguments, are also treated as taking 
small clauses. The verb find takes a finite clausal 
complement as in We found that the book was 
important and also a non-finite clausal comple-
ment as in We found the book to be important. 
Therefore, find is also treated as taking a small 
clause complement as in We found the book 
important.  
 
(22) (S (NP-SBJ We) 
(VP found
(S (NP-SBJ the book)
(ADJP-PRD important))))
 
The obligatory nature of the secondary predi-
cate in this construction also informed the deci-
sion to use a small clause with a verb like find. In 
(22), for example, important is an obligatory part 
of the sentence, and removing it makes the sen-
tence ungrammatical with this sense of find (?We 
found the book? can only be grammatical with a 
different sense of find, essentially ?We located 
the book?). 
With verbs that take infinitival clausal com-
plements, however, the distinction between a 
single S argument and an NP object together 
with an S argument is more difficult to make. 
The original Treebank policy was to follow the 
criteria and the list of verbs taking both an NP 
object and an infinitival S argument given in 
Quirk, et al (1985).  
Resultative constructions are frequently a 
source of mismatch between Treebank annota-
75
tion as a small clause and PropBank annotation 
with Arg1 and Arg2. Treebank treated a number 
of resultative as small clauses, although certain 
verbs received resultative structure annotation, 
such as the one in (23). 
 
(23) (S (NP-SBJ They) 
(VP painted
(NP-1 the apartment)
(S-CLR (NP-SBJ *-1)
(ADJP-PRD orange))))
 
In all the mismatches in the area of sentential 
complementation, Treebank policy tends to 
overgeneralize S-clauses, whereas Propbank 
leans toward breaking down clauses into separate 
arguments.  
This type of mismatch is being resolved on a 
verb-by-verb basis. Propbank will reanalyze 
some of the verbs (like consider and find), which 
have been analyzed as having 3 arguments, as 
taking an S argument. Treebank, on the other 
hand, will change the analysis of label verbs like 
call, from a small clause analysis to a structure 
with two complements. 
Our proposed structure for label verbs, for ex-
ample, is in (24). 
 
(24) (S (NP-SBJ[Arg0] his parents) 
(VP (VBD called)
(NP-1[Arg1] him)
(S-CLR[Arg2]
(NP-SBJ *-1)
(NP-PRD John))))
 
This structure will accommodate both Treebank 
and PropBank requirements for label verbs. 
4 Where Syntax and Semantics do not 
match  
Finally, there are some examples where the dif-
ferences seem to be impossible to resolve with-
out sacrificing some important features of Prop-
Bank or Treebank annotation. 
4.1 Phrasal verbs   
PropBank has around 550 phrasal verbs like 
keep up, touch on, used to and others, which are 
analyzed as separate predicates in PropBank. 
These verbs have their own set of semantic roles, 
which is different from the set of roles of the cor-
responding ?non-phrasal? verbs, and therefore 
they require a separate PropBank entry. In Tree-
bank, on the other hand, phrasal verbs are not 
distinguished. If the second part of the phrasal 
verb is labeled as a verb+particle combination in 
the Treebank, the PropBank annotators concate-
nate it with the verb as the REL. If Treebank la-
bels the second part of the ?phrasal verb? as part 
of a prepositional phrase, there is no way to re-
solve the inconsistency.   
 
(25) But Japanese institutional investors are used 
to quarterly or semiannual payments on their in-
vestments, so ?  
 
Treebank annotation: 
(VBN used)
(PP (TO to)
(NP quarterly or ?
on their investments))
 
PropBank annotation: 
      Arg1: quarterly or ? on their investments 
 Rel: used to (?used to? is a separate predi-
cate in PropBank) 
4.2 Conjunction  
In PropBank, conjoined NPs and clauses are 
usually analyzed as one argument, parallel to 
Treebank. For example, in John and Mary came, 
the NP John and Mary is a constituent in Tree-
bank and it is also marked as Arg0 in PropBank. 
However, there are a few cases where one of the 
conjuncts is modified, and PropBank policy is to 
mark these modifiers as ArgMs. For example, in 
the following NP, the temporal ArgM now modi-
fies a verb, but it only applies to the second con-
junct.  
 
(26) 
(NP (NNP Richard)
(NNP Thornburgh) )
(, ,)
(SBAR
(WHNP-164 (WP who))
(S
(NP-SBJ-1 (-NONE- *T*-164))
(VP
(VBD went)
(PRT (RP on) )
(S
(NP-SBJ (-NONE- *-1))
(VP (TO to)
(VP (VB[rel] become)
(NP-PRD
(NP[ARG2]
(NP (NN governor))
(PP (IN of)
(NP
(NNP
Pennsylvania))))
76
(CC and)
(PRN (, ,)
(ADVP-TMP (RB now))
(, ,) )
(NP[ARG2] (NNP U.S.)
(NNP Attorney)
(NNP General))
)))))))
 
In PropBank, cases like this can be decom-
posed into two propositions: 
 
(27) Prop1:      rel: become    
                Arg1: attorney general         
                Arg0: [-NONE- *-1]                       
         
   Prop2: rel:  become    
  ArgM-TMP: now   
  Arg0: [-NONE- *-1] 
Arg1: a governor               
 
In Treebank, the conjoined NP is necessarily 
analyzed as one constituent. In order to maintain 
the one-to-one mapping between PropBank and 
Treebank, PropBank annotation would have to 
be revised in order to allow the sentence to have 
one proposition with a conjoined phrase as an 
argument. Fortunately, these types of cases do 
not occur frequently in the corpus. 
4.3 Gapping 
Another place where the one-to-one mapping 
is difficult to preserve is with gapping construc-
tions. Treebank annotation does not annotate the 
gap, given that gaps might correspond to differ-
ent syntactic categories or may not even be a 
constituent. The policy of Treebank, therefore, is 
simply to provide a coindexation link between 
the corresponding constituents:  
 
(28) Mary-1 likes chocolates-2 and  
 Jane=1 ? flowers=2 
 
This policy obviously presents a problem for 
one-to-one mapping, since Propbank annotators 
tag Jane and flowers as the arguments of an im-
plied second likes relation, which is not present 
in the sentence. 
5 Summary 
In this paper we have considered several types 
of mismatches between the annotations of the 
English Treebank and the PropBank: coreference 
and syntactic chains, differences in syntactic 
constituency, and cases in which syntax and se-
mantics do not match. We have found that for the 
most part, such mismatches arise because Tree-
bank decisions are based primarily  on syntactic 
considerations while PropBank decisions give 
more weight  to semantic representation.. 
In order to reconcile these differences we have 
revised the annotation policies of both the Prop-
Bank and Treebank in appropriate ways. A 
fourth source of mismatches is simply annotation 
error in either the Treebank or PropBank. Look-
ing at the mismatches in general has allowed us 
to find these errors, and will facilitate their cor-
rection.  
References 
Olga Babko-Malaya. 2005. PropBank Annotation 
Guidelines. http://www.cis.upenn.edu/~mpalmer/ 
project_pages/PBguidelines.pdf 
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre. 1995. Bracketing Guidelines for Treebank 
II Style. Penn Treebank Project, University of 
Pennsylvania, Department of Computer and Infor-
mation Science Technical Report MS-CIS-95-06. 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambigua-
tion of English Verbs Using Rich Linguistic Fea-
tures. In Proceedings of the 2nd International Joint 
Conference on Natural Language Processing, 
IJCNLP2005, pp. 933-944. Oct. 11-13, Jeju Island, 
Republic of Korea. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz & B. Schas-
berger, 1994. The Penn Treebank: Annotating 
predicate argument structure. Proceedings of the 
Human Language Technology Workshop, San 
Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics, 
31(1). 
R. Quirk, S. Greenbaum, G. Leech and J. Svartvik. 
1985. A Comprehensive Grammar of the English 
Language. Longman, London. 
B. Santorini. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Penn-
sylvania, Department of Computer and Information 
Science Technical Report MS-CIS-90-47. 
77
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 91?96,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Interpretation of Unrealized Syntactic Material in LTAG 
 
Olga Babko-Malaya 
University of Pennsylvania 
malayao@ldc.upenn.edu 
 
 
  
 
Abstract 
This paper presents a LTAG-based 
analysis of gapping and VP ellipsis, 
which proposes that resolution of the 
elided material is part of a general dis-
ambiguation procedure, which is also re-
sponsible for resolution of underspecified 
representations of scope.  
1  Introduction 
The problem of ellipsis resolution is to recover 
the interpretation of the elided material. For ex-
ample, in (1), the elided VP is interpreted as be-
ing identical to the verb in the preceding sen-
tence. Likewise, in the gapping structures, as 
shown in (2), the interpretation of a gap is being 
identified with the interpretation of the preceding 
verb.   
  
(1) Mary likes Bill.  Jane does too.  
(2) Mary ate beans and others -- rice.  
 
Whereas some approaches assume syntactic 
identity between the antecedent and the elided 
material (e.g. Fiengo and May 1994),  others 
suggest that VP ellipsises are proforms, semanti-
cally identified with their antecedents (see Dal-
rymple et al1991, Shieber et al1996, Hardt 1993, 
1999).   
This paper follows semantic approaches to el-
lipsis resolution. It adopts the LTAG semantics 
of Kallmeyer and Romero 2004 and proposes 
that resolution of ellipsises and gaps is part of a 
general disambiguation procedure, which is also 
responsible for resolution of underspecified rep-
resentations of scope. 
2 LTAG Semantics with Semantic Uni-
fication 
In LTAG framework (Joshi and Schabes 1997), 
the basic units are (elementary) trees, which can 
be combined into bigger trees by substitution or 
adjunction. LTAG derivations are represented by 
derivation trees that record the history of how the 
elementary trees are put together. Given that 
derivation steps in LTAG correspond to predi-
cate-argument applications, it is usually assumed 
that LTAG semantics is based on the derivation 
tree, rather than the derived tree (Kallmeyer and 
Joshi 2003).  
Semantic composition which we adopt is 
based on LTAG semantics with semantic unifica-
tion (Kallmeyer and Romero 2004). In the deri-
vation tree, elementary trees are replaced by their 
semantic representations and corresponding fea-
ture structures.  Semantic representations are as 
defined in Kallmeyer and Joshi 2003, except that 
they do not have argument variables. These rep-
resentations consist of a set of formulas (typed ?-
expressions with labels) and a set of scope con-
straints.  
Each semantic representation is linked to a 
feature structure. Feature structures, as illustrated 
by different examples below, include a feature i 
whose values are individual variables and fea-
tures p and MaxS, whose values are proposi-
tional labels. Semantic composition consists of 
feature unification. After having performed all 
unifications, the union of all semantic representa-
tions is built.  
Consider, for example, the semantic represen-
tations and feature structures associated with the 
elementary trees of the sentence shown in (3).    
 
(3)  Mary dates Bill 
            S                         
                            l1: date(v1, v2 ) 
    NP      VP             
[i:v1]             
       date             NP    [i: v2]          
 
   NP                            NP           
          Mary(x)                     Bill (y) 
 
  Mary                       Bill            
  [i: x]                        [i: y]  
 
 
91
Derivation tree:      date 
                            1       2 
                       mary            bill 
 
Semantic composition proceeds on the derivation 
tree and consists of feature unification:  
 
(4)      l1: date(v1, v2 )                                                               
              1 [i: v1]                                                                      
              2 [i: v2 ]                                               
              1             2                                                                                 
                                                                  
    mary(x)               bill(y) 
     [i: x]                      [i: y]    
                   
Performing two unifications,  v1=x, v2=y, we ar-
rive at the final interpretation of this sentence: 
l1: date(x, y), bill(y), mary(x). This representa-
tion is interpreted conjunctively, with free vari-
ables being existentially bound.    
Quantificational NPs are analyzed as multi-
component TAGs, where the scope part of the 
quantifier introduces the proposition containing 
the quantifier, and the predicate-argument part 
introduces the restrictive clause (see Kallmeyer 
and Joshi 2003).  
 
(5) Every student likes some course 
.       S*                                                      S 
                                                                                                  
                                       
         NP [i: x, p: P1]                       NP           VP                          
                                        [p: l1, i: v1]                                 
every  student                                      likes      NP                     
                                              l1: like(v1, v2)   [p:l1, i:v2]                                  
                                                   
                                       
 
.       S*                                               
                                                                                                  
                                       
         NP [i: y, p: P2]                  
                                         
    Some course 
 
      
 
 
Final representation   
 
 
 
 
 
 
 
The final representation of this sentence is un-
derspecified for scope, given that there are no 
constraints which restrict the relative scope of 
every and some. In order to obtain one of the 
readings, a disambiguation mapping is needed: 
Disambiguations:                                                           
1. R2 ->l4, R3 ->l5, N2 ->l1, N3 -> l2:   
some(y,course(y), every(x,student(x), like(x, y))) 
2. R2->l4, R3->l5, N3->l1, N2->l3:                                                     
every(x, student(x), some(y, course(y), like(x, y)) 
 
Disambiguations are functions from proposi-
tional variables to propositional labels that re-
spect the scope constraints, such that after having 
applied this mapping, the transitive closure of the 
resulting scope is a partial order.  
3 The Problem of Ellipsis Resolution in 
LTAG semantics 
Given LTAG semantics, there are two possible 
approaches to resolution of the elided material: 
reconstruction can be done as part of the unifica-
tion process or as part of the disambiguation pro-
cedure. If reconstruction was done as unification, 
the semantic representation of the elided material 
would be disambiguated in the final representa-
tion. On the other hand, it is well known that 
resolution of ellipsises and gaps can be ambigu-
ous. For example, the sentence in (6), discussed 
in Siegel 1987 and Johnson 2003 among others, 
has 2 interpretations:1 
 
(6) Ward can?t eat caviar and his guests -- dried 
beans
Can?t (eat (ward, caviar)) & eat (his guests, dried 
beans)) 
Can?t (eat(ward, caviar)) & can?t (eat(his guests, 
dried beans))                                                                              
 
As this example shows, the gap in (6) can be re-
constructed by selecting either the verb or the 
negated modal as its antecedent. The two inter-
pretations represent different scope readings be-
tween the conjunction and negation, which 
should be analyzed as underspecified in LTAG 
semantics. Resolution of gaps, therefore, cannot 
be done as part of unification, since it depends on 
the disambiguated interpretation. The question is 
whether it is possible to define an underspecified 
representation of these two readings, and what 
kind of resolution mechanism can be used to dis-
ambiguate these interpretations? 
                                                
1
 Other cases of ambiguous interpretations of the elided 
material are discussed in section 7. 
l2: every(x, R2, N2) 
l3: some(y, R3, N3) 
l4: student(x) 
l4 ? R2, P1 ? N2 
l5: course(y) 
l5 ? R3, P2 ? N3 
  l2: every(x, R2, N2) 
l4: student(x)  l4 ?R2 
l3: some(y, R3, N3) 
l5: course(y)  l5 ? R3 
l1: like(x, y)   l1 ?N2   l1 ?N3 
 
92
4 LTAG Semantics of Gapping  
In LTAG semantics, semantic representations are 
introduced by lexicalized trees. In order to ac-
count for the analysis of gapping and VP ellipsis, 
this paper proposes that semantics should be de-
fined on both lexicalized and non-lexicalized 
trees. Specifically, we propose that  
Interpretation of a gap (or elided VP) is the se-
mantic interpretation of a non-lexicalized S tree. 
The semantic representations of lexicalized S 
trees under this new approach are derived com-
positionally, given the meaning of a nonlexical-
ized S tree and the meaning of a verb.  
(7)      S                                       
  
 NP[i:v1]  VP          V [Ag: v3, Pat: v4, MaxS: C1]             
                                      
                               date        l0: date(v3, v4), l0?C1                                                                             
        V            NP [i:v2] 
[Ag: v, Pat: u, MaxS: C]     
l2: ?u?v.C (v2)(v1)  
 
Non-lexicalized trees introduce a propositional 
label and a propositional variable, illustrated by 
l2 and C above.  If a tree is a transitive S-tree, 
there are two lambda bound variables, which cor-
respond to the Agent and Patient features of the 
verb. Performing feature unifications (v3=v, 
v4=u,C1=C) and scope constraint disambigua-
tions (C->l0), the proposition l2 will be reduced to: 
?u.?v.date(v, u)(v2)(v1)= date(v1, v2).  
Given this proposal, we suggest that the se-
mantics of gaps, VPE and other types of elided 
material is introduced by non-lexicalized trees. 
For example, the analysis of the sentence in (2) is 
shown in (7). Performing feature unifications 
(l2=P1, l3=P2, v=v1=v2, u=u1=u2, C=C1=C2) yields 
the final representation, where l2 and l3 are un-
derspecified. There is only one disambiguation 
of the variable C in this sentence: C -> l0, which 
gives us the desired interpretation of the sen-
tence: 
 
l2: ?u?v.eat(v, u) (y)(x) = eat(x, y) 
l3: ?u?v.eat(v, u) (w)(z) = eat(z, w) 
 
Resolution of the gap in this sentence is en-
forced by the feature structure of ?and?, which 
unifies MaxS as well as Agent and Patient fea-
tures. This analysis therefore accounts for the 
fact that gapping ?is intimately entangled with 
the syntax of coordination (as opposed to VP 
ellipsis)? (Johnson 2003). On the other hand, as 
the next example illustrates, it is crucial that pro-
positional variables introduced by non-
lexicalized trees are not unified during semantic 
composition, but rather are identified with their 
antecedents as part of the disambiguation proce-
dure.  
 
(7) Mary ate beans and others -- rice.  
 
          S   [p: l2, Ag: v1, Pat: u1, MaxS:C1]      
                
  Mary    VP 
                                                                                                              
        V         beans 
      eat           
          S 
 
 
  S   [p: P1, Ag: v, Pat: u, MaxS:C]             
 
      and          S   [p: P2, Ag: v, Pat: u, MaxS:C]      
  l1: P1 ? P2 
 
          S    [p:l3, Ag:v2, Pat:u2, MaxS:C2]          
                   
others      VP 
 
                          rice 
 
Final Representation: 
 
 
 
 
 
The sentence in (8), shown below, differs from 
the previous one in the presence of a negated 
modal. The interpretation of this modal intro-
duces a proposition l9: can?t(N9) and a constraint 
P3 ? N9 . After P3 is unified with the proposition 
l0, the final representation has two constraints on 
the variable l0: l0? C and l0? N9, and therefore 
two possible disambiguations. In the disambigua-
tion 1, C is mapped to l0, introduced by the verb 
?eat?, and propositions l2 and l3 are reduced to 
eat(x, y) and eat(z, w). In the disambiguation 2, 
the variable C is mapped to l9, introduced by the 
modal, and l2 and l3 are reduced to can?t(eat(x, 
y)) and can?t(eat(z, w)). These disambiguations 
yield the desired interpretations of this sentence.  
 
 (8) Ward can?t eat caviar and his guests -- dried 
beans  
l3: ?u2?v2.C2 (w)(z) 
others(z), rice(w) 
l2: ?u1 ?v1.C1 (y)(x)  
l0:  eat(v1, u1), l0? C1 
Mary(x), beans(y)  
 
l1: l2 ? l3                     
l2: ?u?v.C (y)(x)            l3: ?u?v.C (w)(z) 
l0: eat(v, u)   l0 ? C  
mary(x), beans(y), others(z), rice(w) 
 
93
          S   [p: l2, Ag: v1, Pat: u1, MaxS:C1]      
                
  Ward    VP [p: l0] 
                                                                                                                            
        V          caviar 
      eat                                       
 
                                VP [p: P3] 
                                           
        S             Can?t      VP   l9: can?t(N9)  P3 ? N9     
 
  S   [p: P1, Ag: v, Pat: u, MaxS:C]             
 
      and          S   [p: P2, Ag: v, Pat: u, MaxS:C]      
l1: P1 ? P2 
 
          S    [p:l3, Ag:v2, Pat:u2, MaxS:C2]          
                   
guests      VP 
 
                          beans 
Final Representation 
 
 
 
 
Disambiguation 1:  C->l0, N9 ->l1:     
can?t (eat(x, y) ? eat(z, w)) 
             l2                  l3 
 
Disambiguation 2:  C->l9, N9->l0:     
can?t(eat(x, y))  ?  can?t (eat(z, w))  
        l2                               l3 
 
Resolution of gaps under this analysis is done as 
part of the scope resolution procedure on under-
specified representations. A crucial feature of 
this analysis is that the propositions l2 and l3 are 
?underspecified? in the final representation and 
the variable C is computed during the 
disambiguation, i.e. when all scope ambiguities 
are being resolved.  In this respect this analysis 
differs from previous approaches, where the final 
representation did not include any variables, ex-
cept for the arguments of quantifiers or other 
scopal elements.2 
                                                
2
 However, see Babko-Malaya 2004, where a similar 
analysis is proposed to account for the semantics of coor-
dinated structures with quantified NPs.  
5 LTAG Analysis of VP Ellipsis 
The analysis of gapping presented above can be 
easily extended to the analysis of VP ellipsis. 
VPE differs from gapping in that it is not re-
stricted to coordinated structures. Whereas in the 
examples above resolution of gaps was enforced 
by the feature structure of ?and?, in the case of 
VPE, a similar unification, forced by pragmatic 
constraints, results in recovering the elided mate-
rial.  
As the example in (9) illustrates, our analysis 
of VPE assumes the following modification of 
the semantics of non-lexicalized trees: proposi-
tions introduced by non-lexicalized trees have 
one lambda-bound variable, so that each argu-
ment is introduced by a separate proposition. For 
example, the interpretation of a transitive tree 
below has two propositions l1 and l2, and two 
propositional variables C1 and C2. The proposi-
tion l2 corresponds to the meaning of a VP, 
which is missing in the standard TAG-based 
analyses
.
 This decomposition of the meaning of a 
nonlexicalized tree, therefore, can be independ-
ently motivated by the existence of modifiers 
which predicate of VPs. We further assume that 
the MaxS feature of the S tree corresponds to the 
variable introduced by the agent (or the highest-
ranked argument). 
 
(9)    Mary likes Bill. Jane does too.                                                     
 
                S  [Ag: v, Pat: u, MaxS: C1]                                             
 
[i:v3] NP       VP                                                                              
                               
NP [i:x]   V           NP  [i: v4]                                                
mary(x)      [Ag: v, Pat: u, MaxS: C2]                                   
   
                                   NP[i: y]  bill(y           
               V [Ag: v3, Pat: v4, MaxS: C]             
                                                                                        
               like      l0: like(v3, v4) l0?C                      
   
Final 
Representation: 
 
 
 
Applying disambiguations C2 -> l0, C1 -> l2 , we 
derive the following propositions: 
 
l2: ?u.like(v, u) (y)=like(v, y) 
l1: ?v.like(v, y) (x)=like(x, y) 
 
l3: ?u2?v2.C2 (w)(z) 
guests(z), beans(w) 
l2: ?u1 ?v1.C1 (y)(x)  
l0:  eat(v1, u1), l0? C1 
ward(x), caviar(y)  
l1: l2 ? l3           l0:  eat(v, u)   
l9: can?t(N9)       l0? N9     l0? C  
l2: ?u?v C (y)(x)     l3: ?u?v C (w)(z)       
guests(z), beans(w), caviar(y), ward(x)  
 
l1: ?v C1 (v3)  
l2: ?u C2 (v4)    l2 ? C1 
 
l1: ?v C1 (x)  
l2: ?u C2 (y)    l2 ? C1 
l0:  like(v, u),  l0? C2 
Mary(x), Bill(y)  
 
94
Now consider the second sentence: Jane does 
too: 
          S   [Ag: v3,  MaxS: C3]                                                            
 
NP[i:v5]  VP        l3: ?v3.C3 (v5)                                
                
 
NP [i: r]     V                                                                                           
  jane(r) 
 
Final 
Representation: 
 
This sentence introduces an intransitive tree and 
one propositional variable C3. This variable is 
not constrained within the sentence, and parallel 
to other pro-forms, it gets its interpretation from 
the previous discourse. Specifically, the interpre-
tation of the second sentence is derived by unifi-
cation of the S features of the second and the first 
S-trees in (9):  C3=C1, v3=v. Given that C1 is 
mapped to l2 above, it corresponds to the propo-
sition being reconstructed:  C3(=C1) -> l2 
l3: ?v.like(v, u) (r) = like(r, u) 
6 Scope Parallelism 
Many previous approaches impose parallelism 
constraints on the interpretation of the elided ma-
terial (e.g. Fox 2000, Asher et al2001 among 
others). Under the present analysis, scope paral-
lelism comes for free. Consider, for example, the 
following sentence discussed in Dalrymple et al
1991, among others, where ambiguity is resolved 
in the same way in both the antecedent and at the 
ellipsis site: John gave every student a test, and 
Bill did too. The final interpretation of the first 
sentence is given in (10) and has 2 possible dis-
ambiguations.  
 
 (10) John gave every student a test. 
 
 
 
 
 
 
 
 
The surface reading (every >> some) is derived 
by the following mapping:  C3->l0, C2->l3, R7->l8, 
N5 -> l2,  C1-> l7,  R5->l9, N7 -> l5  
l2: give(v, y, z) 
l5: some(x, test(x), give(v, y, z)) 
l7:every(y,student(y),some(x,test(x),give(v,y, z))) 
l1:every(y,student(y),some(x,test(x),give(x, y, z))) 
The interpretation of the second sentence is 
derived by unifying the S-features of the S-trees 
(as shown in the previous section). As the result, 
the variables C3 and v3 are unified with the vari-
ables C1 and v. Given that C1 is being mapped to 
the proposition l7 above, C3 is being recon-
structed as the proposition every(y, student(y), 
some(x, test(x), give(v, y, z)) and l3 corresponds 
to the desired reading of this sentence:   
  
(11) Bill did too.  
 
C3 (=C1) -> l7  
                                        v3=v 
l3: ?v. every(y, student(y),some(x, test(x), give(v, 
y, z))) (r) = every(y, student(y), some(x, test(x), 
give(r, y, z)))    
The inverse reading (where some>>every) can 
be obtained by the following mapping C3->l0,  
C2->l3, R7->l8, N7 -> l2,  C1-> l5,  R5->l9, N5 -> l7  
l2: give(v, y, z) 
l7: every(y, student(y), give(v, y, z)) 
l5:some(x,test(x),every(y,student(y),give(v,y, z))) 
l1: some(x,test(x),every(y,student(y),give(x, y, z))) 
 Now, when the second sentence is interpreted, 
C3 is unified with C1, which is being mapped to 
l5: C3(=C1) -> l5. The proposition l3, then, is re-
duced to: ?v.some(x, test(x), every(y, student(y), 
give(v, y, z))) (r) = some(x, test(x), every(y, stu-
dent(y), give(r, y, z))) 
As this example illustrates, scope parallelism 
follows from the present analysis, given that C3 
is unified with a disambiguated interpretation of 
a VP. It can also be shown that the wide scope 
puzzle (Sag 1980), shown in (12) is not unex-
pected under this approach, however, the analy-
sis of this phenomenon is beyond the scope of 
this paper. 3  
 
(12) A nurse saw every patient. Dr.Smith did too. 
some(x, nurse(x), every(y, patient(y), see(x, y))) 
*every(y, patient(y), some(x, nurse(x), see(x, y))) 
 
                                                
3
 As Hirschbuhler 1982, Fox 2000 among others noted,  
there are constructions where subjects of VPE  can have 
narrow scope relative to nonsubjects.  For example, the 
sentence A Canadian flag was hanging in front of every 
building. An American flag was too  has a reading in which 
each building has both an American and a Canadian flag 
standing in front of it.  The existence of such readings does 
not present a problem for the present analysis, if we adopt 
an analysis of quantificational NPs  proposed in Babko-
Malaya 2004.   
 
l3: ?v3 C3 (r)  
Jane(r) 
 
l4:  Bill(r) 
l3:  ?v3.C3 (r) 
 
l0:  give(v, u, w) 
l1: ?v.C1 (x)      l2: ?u.C2 (y)    l2 ? C1 
l3: ?w.C3 (z)           l3 ? C2 
l7:  every(y, R7, N7)     l5: some(z, R5, N5) 
l8:  student(y)     l9: test(z)    john(x) 
l0? C3 l0? N5 l0? N7 l8? R7 l9? R5  
95
7 Antecedent Contained Deletion(ACD) 
Further evidence for the proposed analysis comes 
from sentences with ACD, discussed in Sag 
1980, Egg and Erk 2001, Asher et al2001, Ja-
cobson (to appear), and illustrated in (13): 
 
(13) John wants Mary to read every book Bill 
does.  
 
The elided material in this sentence is under-
stood as either ?Bill reads? or ?Bill wants Mary 
to read?. Given that ?want? and ?every? can take 
different scope, four possible readings are ex-
pected. However, puzzling in this case is the un-
availability of one of these readings: *John wants 
that for every book that Bill wants Mary to read, 
she reads it. Let us consider the final interpreta-
tion of this sentence: 
 
 
 
 
 
 
 
 
 
The non-lexicalized S tree introduces a proposi-
tion l3 and variables C and v3. These variables 
can be unified with either S features of the 
?read?-tree (i.e. C1 and v), or S features of the 
?want?-tree (i.e. C2 and v0).  In the first case, the 
small ellipsis interpretation is derived, and both 
scope readings are available: C = C1, v3 = v   
C/C1 -> l6, C4-> l1  
 
l2: read(x, y),   l3: read(z, y) 
every >> want:   
N5 -> lo, C2 -> l4, N4 -> l2, R5 ->l8 
l5:every(y, book(y)&read(z, y),want(r, read(x, y)) 
                                      l3                                            l2 
want >> every: 
C2 -> l4,  N4 -> l5, N5 -> l2, R5 ->l8 
l0:want(r,every(y,book(y)&read(z,y), read(x, y))) 
                                                      l3                      l2 
If C and v3 are unified with S features of the 
?want?-tree, then the large ellipsis interpretation 
is derived: C = C2, v3 = v0, C/C2 -> l4, N4 -> l2, C4 
-> l1, C1 -> l6 
l0: want(r, read(x, y)),     l3: want(z, read(x, y)) 
 
The reading where every >> want is derived by 
the following constraints: N5-> l0, C1-> l1, R5 ->l8  
l5: every(y, book(y) & want(z, read(x,y)),  
want(r, read(x, y))                     l3 
           l0 
The fourth possible reading, where want >> 
every, however, is predicted to be unavailable 
under the present assumptions. This reading, 
want(r, every(y, book(y) & want(z, read(x, y)), 
read(x, y)),  cannot be derived, since it requires 
the proposition l3 to be ?inserted? within the 
proposition l0. 
References 
Asher N., D. Hardt and J.Busquets. 2001. Discourse 
Parallelism, Scope, and Ellipsis?, Journal of Se-
mantics, 18(1), pp. 1-25.   
Babko-Malaya O. 2004 ?LTAG Semantics of NP-
Coordination? in Proc. of TAG+7, Vancouver  
Dalrymple M., S.Shieber and F.Pereira 1991. Ellipsis 
and Higher-Order Unification. In Linguistics and 
Philosophy 14. 
Egg, M. and K.Erk 2001. A compositional approach 
to VP ellipsis 8th International Conference on 
HPSG,Trondheim, Norway 
Fiengo R. and R. May 1994. Indices and Identity. 
MIT Press, Cambridge 
Fox, D. 2000 Economy and Semantic Intepretation, 
MIT Press  
Hardt, D. 1993. VP Ellipsis: Form, Meaning and 
Processing. PhD Diss. Univ. of PA  
Hardt, D. 1999. Dynamic Interpretation of VP Ellip-
sis. Linguistics and Philosophy. 22.2. Heim, I. 
2001 
Jacobson, P. (to appear) Direct Compositionality and 
Variable-Free Semantics: The Case of Antecedent 
Contained Deletion?, in K. Johnson (ed.), Topics in 
Ellipsis, Oxford University Press. 
Johnson, K. 2003. In search of the Middle Field. Ms. 
Univ. of Mass.  
Joshi A. and Y. Schabes 1997. Tree-Adjoining 
Grammars, in G. Rozenberg and A. Salomaa (eds.) 
Handbook of Formal Languages, Springer, Berlin  
Kallmeyer, L. and A.K Joshi 2003. Factoring Predi-
cate Argument and Scope Semantics:  Underspeci-
fied Semantics with LTAG. Research on Language 
and Computation 1(1-2), 358. 
Kallmeyer, L. and M. Romero. 2004. LTAG Seman-
tics with Semantic Unification. In Proceedings of  
TAG+7, Vancouver, Canada  
Sag, Ivan 1980 Deletion and Logical Form. Garland 
Press: New York 
Shieber, S., F.Pereira, and M.Dalrymple 1996. Inter-
actions of Scope and Ellipsis. In Linguistics and 
Philosophy 19, pp.527-552. 
l4: want(v0, N4)    l0: ?v0.C2 (want) (r) 
l1: read(v, u)   l2: ?v.C1 (read) (x)  
l6: ?u.C4 (read) (y)     l6 ? C1    l8: book(y) ? l3 
l5: every(y, R5, N5)     l3: ?v3.C (z) 
mary(x),  john(r), bill(z) 
l1? C1, l4? C2, l1? N5, l8? R5, l1?N4 
 
96
