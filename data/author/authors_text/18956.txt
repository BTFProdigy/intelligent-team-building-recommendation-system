Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633?644,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
A Neural Network for Factoid Question Answering over
Paragraphs
Mohit Iyyer
1
, Jordan Boyd-Graber
2
, Leonardo Claudino
1
,
Richard Socher
3
, Hal Daume? III
1
1
University of Maryland, Department of Computer Science and umiacs
2
University of Colorado, Department of Computer Science
3
Stanford University, Department of Computer Science
{miyyer,claudino,hal}@umiacs.umd.edu,
Jordan.Boyd.Graber@colorado.edu, richard@socher.org
Abstract
Text classification methods for tasks
like factoid question answering typi-
cally use manually defined string match-
ing rules or bag of words representa-
tions. These methods are ineffective
when question text contains very few
individual words (e.g., named entities)
that are indicative of the answer. We
introduce a recursive neural network
(rnn) model that can reason over such
input by modeling textual composition-
ality. We apply our model, qanta, to
a dataset of questions from a trivia
competition called quiz bowl. Unlike
previous rnn models, qanta learns
word and phrase-level representations
that combine across sentences to reason
about entities. The model outperforms
multiple baselines and, when combined
with information retrieval methods, ri-
vals the best human players.
1 Introduction
Deep neural networks have seen widespread
use in natural language processing tasks such
as parsing, language modeling, and sentiment
analysis (Bengio et al., 2003; Socher et al.,
2013a; Socher et al., 2013c). The vector spaces
learned by these models cluster words and
phrases together based on similarity. For exam-
ple, a neural network trained for a sentiment
analysis task such as restaurant review classifi-
cation might learn that ?tasty? and ?delicious?
should have similar representations since they
are synonymous adjectives.
These models have so far only seen success in
a limited range of text-based prediction tasks,
Later in its existence, this polity?s leader was chosen
by a group that included three bishops and six laymen,
up from the seven who traditionally made the decision.
Free imperial cities in this polity included Basel and
Speyer. Dissolved in 1806, its key events included the
Investiture Controversy and the Golden Bull of 1356.
Led by Charles V, Frederick Barbarossa, and Otto I,
for 10 points, name this polity, which ruled most of
what is now Germany through the Middle Ages and
rarely ruled its titular city.
Figure 1: An example quiz bowl question about
the Holy Roman Empire. The first sentence
contains no words or named entities that by
themselves are indicative of the answer, while
subsequent sentences contain more and more
obvious clues.
where inputs are typically a single sentence and
outputs are either continuous or a limited dis-
crete set. Neural networks have not yet shown
to be useful for tasks that require mapping
paragraph-length inputs to rich output spaces.
Consider factoid question answering: given
a description of an entity, identify the per-
son, place, or thing discussed. We describe a
task with high-quality mappings from natural
language text to entities in Section 2. This
task?quiz bowl?is a challenging natural lan-
guage problem with large amounts of diverse
and compositional data.
To answer quiz bowl questions, we develop
a dependency tree recursive neural network
in Section 3 and extend it to combine predic-
tions across sentences to produce a question
answering neural network with trans-sentential
averaging (qanta). We evaluate our model
against strong computer and human baselines
in Section 4 and conclude by examining the
latent space and model mistakes.
633
2 Matching Text to Entities: Quiz
Bowl
Every weekend, hundreds of high school and
college students play a game where they map
raw text to well-known entities. This is a trivia
competition called quiz bowl. Quiz bowl ques-
tions consist of four to six sentences and are
associated with factoid answers (e.g., history
questions ask players to identify specific battles,
presidents, or events). Every sentence in a quiz
bowl question is guaranteed to contain clues
that uniquely identify its answer, even without
the context of previous sentences. Players an-
swer at any time?ideally more quickly than
the opponent?and are rewarded for correct
answers.
Automatic approaches to quiz bowl based on
existing nlp techniques are doomed to failure.
Quiz bowl questions have a property called
pyramidality, which means that sentences early
in a question contain harder, more obscure
clues, while later sentences are ?giveaways?.
This design rewards players with deep knowl-
edge of a particular subject and thwarts bag
of words methods. Sometimes the first sen-
tence contains no named entities?answering
the question correctly requires an actual un-
derstanding of the sentence (Figure 1). Later
sentences, however, progressively reveal more
well-known and uniquely identifying terms.
Previous work answers quiz bowl ques-
tions using a bag of words (na??ve Bayes) ap-
proach (Boyd-Graber et al., 2012). These mod-
els fail on sentences like the first one in Figure 1,
a typical hard, initial clue. Recursive neural
networks (rnns), in contrast to simpler models,
can capture the compositional aspect of such
sentences (Hermann et al., 2013).
rnns require many redundant training exam-
ples to learn meaningful representations, which
in the quiz bowl setting means we need multiple
questions about the same answer. Fortunately,
hundreds of questions are produced during the
school year for quiz bowl competitions, yield-
ing many different examples of questions ask-
ing about any entity of note (see Section 4.1
for more details). Thus, we have built-in re-
dundancy (the number of ?askable? entities is
limited), but also built-in diversity, as difficult
clues cannot appear in every question without
becoming well-known.
3 Dependency-Tree Recursive
Neural Networks
To compute distributed representations for the
individual sentences within quiz bowl ques-
tions, we use a dependency-tree rnn (dt-rnn).
These representations are then aggregated and
fed into a multinomial logistic regression clas-
sifier, where class labels are the answers asso-
ciated with each question instance.
In previous work, Socher et al. (2014) use
dt-rnns to map text descriptions to images.
dt-rnns are robust to similar sentences with
slightly different syntax, which is ideal for our
problem since answers are often described by
many sentences that are similar in meaning
but different in structure. Our model improves
upon the existing dt-rnn model by jointly
learning answer and question representations
in the same vector space rather than learning
them separately.
3.1 Model Description
As in other rnn models, we begin by associ-
ating each word w in our vocabulary with a
vector representation x
w
? R
d
. These vectors
are stored as the columns of a d ? V dimen-
sional word embedding matrix W
e
, where V is
the size of the vocabulary. Our model takes
dependency parse trees of question sentences
(De Marneffe et al., 2006) and their correspond-
ing answers as input.
Each node n in the parse tree for a partic-
ular sentence is associated with a word w, a
word vector x
w
, and a hidden vector h
n
? R
d
of the same dimension as the word vectors. For
internal nodes, this vector is a phrase-level rep-
resentation, while at leaf nodes it is the word
vector x
w
mapped into the hidden space. Un-
like in constituency trees where all words reside
at the leaf level, internal nodes of dependency
trees are associated with words. Thus, the dt-
rnn has to combine the current node?s word
vector with its children?s hidden vectors to form
h
n
. This process continues recursively up to
the root, which represents the entire sentence.
We associate a separate d?d matrix W
r
with
each dependency relation r in our dataset and
learn these matrices during training.
1
Syntac-
tically untying these matrices improves com-
1
We had 46 unique dependency relations in our quiz
bowl dataset.
634
This city ?s economy depended on subjugated peasants called helots
ROOT
DET
POSSESSIVE
POSS
NSUBJ
PREP
POBJ
AMOD
VMOD
DOBJ
Figure 2: Dependency parse of a sentence from a question about Sparta.
positionality over the standard rnn model by
taking into account relation identity along with
tree structure. We include an additional d? d
matrix, W
v
, to incorporate the word vector x
w
at a node into the node vector h
n
.
Given a parse tree (Figure 2), we first com-
pute leaf representations. For example, the
hidden representation h
helots
is
h
helots
= f(W
v
? x
helots
+ b), (1)
where f is a non-linear activation function such
as tanh and b is a bias term. Once all leaves
are finished, we move to interior nodes with
already processed children. Continuing from
?helots? to its parent, ?called?, we compute
h
called
=f(W
DOBJ
? h
helots
+W
v
? x
called
+ b). (2)
We repeat this process up to the root, which is
h
depended
=f(W
NSUBJ
? h
economy
+W
PREP
? h
on
+W
v
? x
depended
+ b). (3)
The composition equation for any node n with
children K(n) and word vector x
w
is h
n
=
f(W
v
? x
w
+ b+
?
k?K(n)
W
R(n,k)
? h
k
), (4)
where R(n, k) is the dependency relation be-
tween node n and child node k.
3.2 Training
Our goal is to map questions to their corre-
sponding answer entities. Because there are
a limited number of possible answers, we can
view this as a multi-class classification task.
While a softmax layer over every node in the
tree could predict answers (Socher et al., 2011;
Iyyer et al., 2014), this method overlooks that
most answers are themselves words (features)
in other questions (e.g., a question on World
War II might mention the Battle of the Bulge
and vice versa). Thus, word vectors associated
with such answers can be trained in the same
vector space as question text,
2
enabling us to
model relationships between answers instead
of assuming incorrectly that all answers are
independent.
To take advantage of this observation, we
depart from Socher et al. (2014) by training
both the answers and questions jointly in a
single model, rather than training each sep-
arately and holding embeddings fixed during
dt-rnn training. This method cannot be ap-
plied to the multimodal text-to-image mapping
problem because text captions by definition are
made up of words and thus cannot include im-
ages; in our case, however, question text can
and frequently does include answer text.
Intuitively, we want to encourage the vectors
of question sentences to be near their correct
answers and far away from incorrect answers.
We accomplish this goal by using a contrastive
max-margin objective function described be-
low. While we are not interested in obtaining a
ranked list of answers,
3
we observe better per-
formance by adding the weighted approximate-
rank pairwise (warp) loss proposed in Weston
et al. (2011) to our objective function.
Given a sentence paired with its correct an-
swer c, we randomly select j incorrect answers
from the set of all incorrect answers and denote
this subset as Z. Since c is part of the vocab-
ulary, it has a vector x
c
? W
e
. An incorrect
answer z ? Z is also associated with a vector
x
z
?W
e
. We define S to be the set of all nodes
in the sentence?s dependency tree, where an
individual node s ? S is associated with the
2
Of course, questions never contain their own answer
as part of the text.
3
In quiz bowl, all wrong guesses are equally detri-
mental to a team?s score, no matter how ?close? a guess
is to the correct answer.
635
hidden vector h
s
. The error for the sentence is
C(S, ?) =
?
s?S
?
z?Z
L(rank(c, s, Z))max(0,
1? x
c
? h
s
+ x
z
? h
s
), (5)
where the function rank(c, s, Z) provides the
rank of correct answer c with respect to the
incorrect answers Z. We transform this rank
into a loss function
4
shown by Usunier et al.
(2009) to optimize the top of the ranked list,
L(r) =
r
?
i=1
1/i.
Since rank(c, s, Z) is expensive to compute,
we approximate it by randomly sampling K
incorrect answers until a violation is observed
(x
c
? h
s
< 1 + x
z
? h
s
) and set rank(c, s, Z) =
(|Z|?1)/K, as in previous work (Weston et al.,
2011; Hermann et al., 2014). The model mini-
mizes the sum of the error over all sentences T
normalized by the number of nodes N in the
training set,
J(?) =
1
N
?
t?T
C(t, ?). (6)
The parameters ? = (W
r?R
,W
v
,W
e
, b), where
R represents all dependency relations in the
data, are optimized using AdaGrad(Duchi et
al., 2011).
5
In Section 4 we compare perfor-
mance to an identical model (fixed-qanta)
that excludes answer vectors from W
e
and show
that training them as part of ? produces signif-
icantly better results.
The gradient of the objective function,
?C
??
=
1
N
?
t?T
?J(t)
??
, (7)
is computed using backpropagation through
structure (Goller and Kuchler, 1996).
3.3 From Sentences to Questions
The model we have just described considers
each sentence in a quiz bowl question indepen-
dently. However, previously-heard sentences
within the same question contain useful infor-
mation that we do not want our model to ignore.
4
Our experiments show that adding this loss term to
the objective function not only increases performance
but also speeds up convergence
5
We set the initial learning rate ? = 0.05 and reset
the squared gradient sum to zero every five epochs.
While past work on rnn models have been re-
stricted to the sentential and sub-sentential
levels, we show that sentence-level representa-
tions can be easily combined to generate useful
representations at the larger paragraph level.
The simplest and best
6
aggregation method
is just to average the representations of each
sentence seen so far in a particular question.
As we show in Section 4, this method is very
powerful and performs better than most of our
baselines. We call this averaged dt-rnn model
qanta: a question answering neural network
with trans-sentential averaging.
4 Experiments
We compare the performance of qanta against
multiple strong baselines on two datasets.
qanta outperforms all baselines trained only
on question text and improves an information
retrieval model trained on all of Wikipedia.
qanta requires that an input sentence de-
scribes an entity without mentioning that
entity, a constraint that is not followed by
Wikipedia sentences.
7
While ir methods can
operate over Wikipedia text with no issues,
we show that the representations learned by
qanta over just a dataset of question-answer
pairs can significantly improve the performance
of ir systems.
4.1 Datasets
We evaluate our algorithms on a corpus of over
100,000 question/answer pairs from two differ-
ent sources. First, we expand the dataset used
in Boyd-Graber et al. (2012) with publically-
available questions from quiz bowl tournaments
held after that work was published. This gives
us 46,842 questions in fourteen different cate-
gories. To this dataset we add 65,212 questions
from naqt, an organization that runs quiz
bowl tournaments and generously shared with
us all of their questions from 1998?2013.
6
We experimented with weighting earlier sentences
less than later ones in the average as well as learning an
additional RNN on top of the sentence-level representa-
tions. In the former case, we observed no improvements
over a uniform average, while in the latter case the
model overfit even with strong regularization.
7
We tried transforming Wikipedia sentences into
quiz bowl sentences by replacing answer mentions with
appropriate descriptors (e.g., ?Joseph Heller? with ?this
author?), but the resulting sentences suffered from a
variety of grammatical issues and did not help the final
result.
636
Because some categories contain substan-
tially fewer questions than others (e.g., astron-
omy has only 331 questions), we consider only
literature and history questions, as these two
categories account for more than 40% of the
corpus. This leaves us with 21,041 history ques-
tions and 22,956 literature questions.
4.1.1 Data Preparation
To make this problem feasible, we only consider
a limited set of the most popular quiz bowl an-
swers. Before we filter out uncommon answers,
we first need to map all raw answer strings to
a canonical set to get around formatting and
redundancy issues. Most quiz bowl answers are
written to provide as much information about
the entity as possible. For example, the follow-
ing is the raw answer text of a question on the
Chinese leader Sun Yat-sen: Sun Yat-sen; or
Sun Yixian; or Sun Wen; or Sun Deming; or
Nakayama Sho; or Nagao Takano. Quiz bowl
writers vary in how many alternate acceptable
answers they provide, which makes it tricky to
strip superfluous information from the answers
using rule-based approaches.
Instead, we use Whoosh,
8
an information re-
trieval library, to generate features in an active
learning classifier that matches existing answer
strings to Wikipedia titles. If we are unable
to find a match with a high enough confidence
score, we throw the question out of our dataset.
After this standardization process and manual
vetting of the resulting output, we can use the
Wikipedia page titles as training labels for the
dt-rnn and baseline models.
9
65.6% of answers only occur once or twice
in the corpus. We filter out all answers that
do not occur at least six times, which leaves
us with 451 history answers and 595 literature
answers that occur on average twelve times
in the corpus. These pruning steps result in
4,460 usable history questions and 5,685 liter-
ature questions. While ideally we would have
used all answers, our model benefits from many
training examples per answer to learn mean-
ingful representations; this issue can possibly
be addressed with techniques from zero shot
learning (Palatucci et al., 2009; Pasupat and
Liang, 2014), which we leave to future work.
8
https://pypi.python.org/pypi/Whoosh/
9
Code and non-naqt data available at http://cs.
umd.edu/
~
miyyer/qblearn.
We apply basic named entity recogni-
tion (ner) by replacing all occurrences of
answers in the question text with single
entities (e.g., Ernest Hemingway becomes
Ernest Hemingway). While we experimented
with more advanced ner systems to detect
non-answer entities, they could not handle
multi-word named entities like the book Love
in the Time of Cholera (title case) or battle
names (e.g., Battle of Midway). A simple
search/replace on all answers in our corpus
works better for multi-word entities.
The preprocessed data are split into folds
by tournament. We choose the past two na-
tional tournaments
10
as our test set as well
as questions previously answered by players in
Boyd-Graber et al. (2012) and assign all other
questions to train and dev sets. History results
are reported on a training set of 3,761 ques-
tions with 14,217 sentences and a test set of
699 questions with 2,768 sentences. Literature
results are reported on a training set of 4,777
questions with 17,972 sentences and a test set
of 908 questions with 3,577 sentences.
Finally, we initialize the word embedding
matrix W
e
with word2vec (Mikolov et al., 2013)
trained on the preprocessed question text in
our training set.
11
We use the hierarchical skip-
gram model setting with a window size of five
words.
4.2 Baselines
We pit qanta against two types of baselines:
bag of words models, which enable comparison
to a standard NLP baseline, and information
retrieval models, which allow us to compare
against traditional question answering tech-
niques.
BOW The bow baseline is a logistic regres-
sion classifier trained on binary unigram indi-
cators.
12
This simple discriminative model is
an improvement over the generative quiz bowl
answering model of Boyd-Graber et al. (2012).
10
The tournaments were selected because naqt does
not reuse any questions or clues within these tourna-
ments.
11
Out-of-vocabulary words from the test set are ini-
tialized randomly.
12
Raw word counts, frequencies, and TF-IDF
weighted features did not increase performance, nor
did adding bigrams to the feature set (possibly because
multi-word named entities are already collapsed into
single words).
637
BOW-DT The bow-dt baseline is identical
to bow except we augment the feature set with
dependency relation indicators. We include
this baseline to isolate the effects of the depen-
dency tree structure from our compositional
model.
IR-QB The ir-qb baseline maps questions to
answers using the state-of-the-art Whoosh ir
engine. The knowledge base for ir-qb consists
of ?pages? associated with each answer, where
each page is the union of training question text
for that answer. Given a partial question, the
text is first preprocessed using a query lan-
guage similar to that of Apache Lucene. This
processed query is then matched to pages uses
bm-25 term weighting, and the top-ranked page
is considered to be the model?s guess. We also
incorporate fuzzy queries to catch misspellings
and plurals and use Whoosh?s built-in query ex-
pansion functionality to add related keywords
to our queries. IR-WIKI The ir-wiki model
is identical to the ir-qb model except that each
?page? in its knowledge base also includes all
text from the associated answer?s Wikipedia
article. Since all other baselines and dt-rnn
models operate only on the question text, this
is not a valid comparison, but we offer it to
show that we can improve even this strong
model using qanta.
4.3 DT-RNN Configurations
For all dt-rnn models the vector dimension d
and the number of wrong answers per node j
is set to 100. All model parameters other than
W
e
are randomly initialized. The non-linearity
f is the normalized tanh function,
13
f(v) =
tanh(v)
?tanh(v)?
. (8)
qanta is our dt-rnn model with feature
averaging across previously-seen sentences in a
question. To obtain the final answer prediction
given a partial question, we first generate a
feature representation for each sentence within
that partial question. This representation is
computed by concatenating together the word
embeddings and hidden representations aver-
aged over all nodes in the tree as well as the
13
The standard tanh function produced heavy sat-
uration at higher levels of the trees, and corrective
weighting as in Socher et al. (2014) hurt our model
because named entities that occur as leaves are often
more important than non-terminal phrases.
root node?s hidden vector. Finally, we send
the average of all of the individual sentence fea-
tures
14
as input to a logistic regression classifier
for answer prediction.
fixed-qanta uses the same dt-rnn configu-
ration as qanta except the answer vectors are
kept constant as in the text-to-image model.
4.4 Human Comparison
Previous work provides human answers (Boyd-
Graber et al., 2012) for quiz bowl questions.
We use human records for 1,201 history guesses
and 1,715 literature guesses from twenty-two of
the quiz bowl players who answered the most
questions.
15
The standard scoring system for quiz bowl is
10 points for a correct guess and -5 points for
an incorrect guess. We use this metric to com-
pute a total score for each human. To obtain
the corresponding score for our model, we force
it to imitate each human?s guessing policy. For
example, Figure 3 shows a human answering
in the middle of the second sentence. Since our
model only considers sentence-level increments,
we compare the model?s prediction after the
first sentence to the human prediction, which
means our model is privy to less information
than humans.
The resulting distributions are shown in Fig-
ure 4?our model does better than the average
player on history questions, tying or defeat-
ing sixteen of the twenty-two players, but it
does worse on literature questions, where it
only ties or defeats eight players. The figure
indicates that literature questions are harder
than history questions for our model, which is
corroborated by the experimental results dis-
cussed in the next section.
5 Discussion
In this section, we examine why qanta im-
proves over our baselines by giving examples
of questions that are incorrectly classified by
all baselines but correctly classified by qanta.
We also take a close look at some sentences that
all models fail to answer correctly. Finally, we
visualize the answer space learned by qanta.
14
Initial experiments with L
2
regularization hurt per-
formance on a validation set.
15
Participants were skilled quiz bowl players and are
not representative of the general population.
638
History Literature
Model Pos 1 Pos 2 Full Pos 1 Pos 2 Full
bow 27.5 51.3 53.1 19.3 43.4 46.7
bow-dt 35.4 57.7 60.2 24.4 51.8 55.7
ir-qb 37.5 65.9 71.4 27.4 54.0 61.9
fixed-qanta 38.3 64.4 66.2 28.9 57.7 62.3
qanta 47.1 72.1 73.7 36.4 68.2 69.1
ir-wiki 53.7 76.6 77.5 41.8 74.0 73.3
qanta+ir-wiki 59.8 81.8 82.3 44.7 78.7 76.6
Table 1: Accuracy for history and literature at the first two sentence positions of each question
and the full question. The top half of the table compares models trained on questions only, while
the IR models in the bottom half have access to Wikipedia. qanta outperforms all baselines
that are restricted to just the question data, and it substantially improves an IR model with
access to Wikipedia despite being trained on much less data.
200
150
100
50
0
50
100
150
200
Sco
re D
iffe
ren
ce
History: Model vs. Human
Model losesModel wins 400300
200
100
0
100
200
Sco
re D
iffe
ren
ce
Literature: Model vs. Human
Model losesModel wins
Figure 4: Comparisons of qanta+ir-wiki to human quiz bowl players. Each bar represents an
individual human, and the bar height corresponds to the difference between the model score and
the human score. Bars are ordered by human skill. Red bars indicate that the human is winning,
while blue bars indicate that the model is winning. qanta+ir-wiki outperforms most humans
on history questions but fails to defeat the ?average? human on literature questions.
A minor character in this play can be summoned
by a bell that does not always work; that character
also doesn?t have eyelids. Near the end, a woman
who drowned her illegitimate child attempts to stab
another woman in the Second Empire-style
3
room
in which the entire play takes place. For 10 points,
Estelle and Ines are characters in which existentialist
play in which Garcin claims ?Hell is other people?,
written by Jean-Paul Sartre?
Figure 3: A question on the play ?No Exit?
with human buzz position marked as
3
. Since
the buzz occurs in the middle of the second
sentence, our model is only allowed to see the
first sentence.
5.1 Experimental Results
Table 1 shows that when bag of words and
information retrieval methods are restricted to
question data, they perform significantly worse
than qanta on early sentence positions. The
performance of bow-dt indicates that while
the dependency tree structure helps by itself,
the compositional distributed representations
learned by qanta are more useful. The signif-
icant improvement when we train answers as
part of our vocabulary (see Section 3.2) indi-
cates that our model uses answer occurrences
within question text to learn a more informa-
tive vector space.
The disparity between ir-qb and ir-wiki
indicates that the information retrieval models
need lots of external data to work well at all
sentence positions. ir-wiki performs better
than other models because Wikipedia contains
many more sentences that partially match spe-
cific words or phrases found in early clues than
the question training set. In particular, it is
impossible for all other models to answer clues
in the test set that have no semantically similar
639
or equivalent analogues in the training ques-
tion data. With that said, ir methods can
also operate over data that does not follow the
special constraints of quiz bowl questions (e.g.,
every sentence uniquely identifies the answer,
answers don?t appear in their corresponding
questions), which qanta cannot handle. By
combining qanta and ir-wiki, we are able to
leverage access to huge knowledge bases along
with deep compositional representations, giv-
ing us the best of both worlds.
5.2 Where the Attribute Space Helps
Answer Questions
We look closely at the first sentence from a
literature question about the author Thomas
Mann: ?He left unfinished a novel whose title
character forges his father?s signature to get
out of school and avoids the draft by feigning
desire to join?.
All baselines, including ir-wiki, are unable
to predict the correct answer given only this
sentence. However, qanta makes the correct
prediction. The sentence contains no named
entities, which makes it almost impossible for
bag of words or string matching algorithms to
predict correctly. Figure 6 shows that the plot
description associated with the ?novel? node
is strongly indicative of the answer. The five
highest-scored answers are all male authors,
16
which shows that our model is able to learn the
answer type without any hand-crafted rules.
Our next example, the first sentence in Ta-
ble 2, is from the first position of a question
on John Quincy Adams, which is correctly an-
swered by only qanta. The bag of words
model guesses Henry Clay, who was also a Sec-
retary of State in the nineteenth century and
helped John Quincy Adams get elected to the
presidency in a ?corrupt bargain?. However,
the model can reason that while Henry Clay
was active at the same time and involved in
the same political problems of the era, he did
not represent the Amistad slaves, nor did he
negotiate the Treaty of Ghent.
5.3 Where all Models Struggle
Quiz bowl questions are intentionally written to
make players work to get the answer, especially
at early sentence positions. Our model fails to
16
three of whom who also have well-known unfinished
novels
answer correctly more than half the time after
hearing only the first sentence. We examine
some examples to see if there are any patterns
to what makes a question ?hard? for machine
learning models.
Consider this question about the Italian ex-
plorer John Cabot: ?As a young man, this
native of Genoa disguised himself as a Muslim
to make a pilgrimage to Mecca?.
While it is obvious to human readers that
the man described in this sentence is not actu-
ally a Muslim, qanta has to accurately model
the verb disguised to make that inference. We
show the score plot of this sentence in Figure 7.
The model, after presumably seeing many in-
stances of muslim and mecca associated with
Mughal emperors, is unable to prevent this
information from propagating up to the root
node. On the bright side, our model is able to
learn that the question is expecting a human
answer rather than non-human entities like the
Umayyad Caliphate.
More examples of impressive answers by
qanta as well as incorrect guesses by all sys-
tems are shown in Table 2.
5.4 Examining the Attribute Space
Figure 5 shows a t-SNE visualization (Van der
Maaten and Hinton, 2008) of the 451 answers
in our history dataset. The vector space is
divided into six general clusters, and we focus
in particular on the us presidents. Zooming
in on this section reveals temporal clustering:
presidents who were in office during the same
timeframe occur closer together. This observa-
tion shows that qanta is capable of learning
attributes of entities during training.
6 Related Work
There are two threads of related work relevant
to this paper. First, we discuss previous ap-
plications of compositional vector models to
related NLP tasks. Then, we examine existing
work on factoid question-answering and review
the similarities and differences between these
tasks and the game of quiz bowl.
6.1 Recursive Neural Networks for
NLP
The principle of semantic composition states
that the meaning of a phrase can be derived
640
TSNE-1TSNE-2 Wars, rebellions, and battlesU.S. presidentsPrime ministersExplorers & emperorsPoliciesOthertammany_hall calvin_coolidgelollardy fourth_crusadesonghai_empire peace_of_westphaliainca_empireatahualpa charles_sumnerjohn_paul_jones wounded_knee_massacrehuldrych_zwingli darius_ibattle_of_ayacuchojohn_cabotghana ulysses_s._grant hartford_conventioncivilian_conservation_corpsroger_williams_(theologian)george_h._pendleton william_mckinleyvictoria_woodhullcredit_mobilier_of_america_scandal henry_cabot_lodge,_jr. mughal_empire john_marshallcultural_revolutionguadalcanallouisiana_purchasenight_of_the_long_kniveschandragupta_mauryasamuel_de_champlainthirty_years'_war compromise_of_1850battle_of_hastingsbattle_of_salamisakbar lewis_cassdawes_plan hernando_de_soto carthage joseph_mccarthymainesalvador_allende battle_of_gettysburgmikhail_gorbachev aaron_burrequal_rights_amendmentwar_of_the_spanish_successioncoxey's_army george_meadefourteen_pointsmapp_v._ohio sam_houston ming_ ynastyboxer_rebellionanti-masonic_partyporfirio_diaz treaty_of_portsmouththebes,_greece golden_hordefrancisco_i._madero hittitesjames_g._blaineschenck_v._united_states caligulawilliam_walker_(filibuster)henry_vii_of_ nglandkonrad_adenauerkellogg-briand_pact battle_of_cullodentreaty_of_brest-litovsk william_p nna._philip_randolphh nry_l._stimsonwhig_party_(united_states)caroline_affair clarence_darrowwhiskey_rebellionbattle_of_midwaybattle_of_lepantoadolf_eichmanngeorges_clemenceau battle_of_the_little_bighornpontiac_(person) black_hawk_warbattle_of_tannenbergclayton_antitrust_actprovisions_of_oxford battle_of_actiumsuez_crisis spartacusdorr_rebellion jay_treatytriangle_shirtwaist_factory_fire kamakura_shogunatejulius_nyerere frederick_douglasspierre_trudeaunagasaki suleiman_the_magnificentfalklands_war war_of_devolutioncharlemagnedaniel_boone edict_of_nantesharry_s._trumanshakapedro_alvares_cabralthomas_hart_benton_(politician)battle_of_the_coral_sea peterloo_massacrebattle_of_bosworth_fieldroger_b._taneybernardo_o'higginsneville_chamberlainhenry_hudson cyrus_the_great jane_addamsrough_ridersjames_a._garfieldnapoleon_iii missouri_compromisebattle_of_leyte_gulfambrose_burnsidetrent_affairmaria_theresawilliam_ewart_gladstone walter_mondalebarry_goldw terlouis_rielhideki_tojo marco_polobrian_mulroney truman_doctrineroald_amundsen tokugawa_shogunateeleanor_of_aquitaine louis_brandeisbattle_of_trentonkhmer_empirebenito_juarez battle_of_antietamwhiskey_ring otto_von_bismarckbook r_t._washingtonbattle_of_bannockburneugene_v._debs erie_canaljameson_raid green_mountain_boyshaymarket_affairfinlandfashoda_incident battle_of_shilohhannibal john_jayeaster_rising jamaicabrook_farm umayyad_caliph temuhammadfrancis_drakeclara_barton shays'_rebellion verdunhadrianvyacheslav_molotov oda_nobunagacanossasamuel_gompers battle_of_bunker_hi lbattle_of_plasseydavid_livingstonesol npericles tang_dynastyteutonic_knightssecond_vatican_councilalfred_dreyfushenry_the_navigatornelson_mandelapeasants'_revolt gaius_mariusgetulio_vargas horatio_gatesjohn_t._scopes league_of_nationsfirst_battle_of_bull_runalfred_the_greatleonid_brezhn cherokee long_marchemiliano_zapata james_monroewoodrow_wilsonvandals william_henry_harrisonbattle_of_puebla battle_of_zamajustinian_i thaddeus_stevenscecil_rhodeskwame_nkrumah diet_of_wormsgeorge_armstrong_custerbattle_of_agincourtseminole_wars shah_jahanamerigo_vespucci john_foster_dulleslester_b._pearson oregon_trail claudiuslateran_treatychester_a._arthuropium_wars treaty_of_utrechtknights_of_labor alexander_hamiltonplessy_v._ferguson hora e_greeleymary baker_eddyalex nder_kerensky jacquerie treaty_of_ghentb y_of_pigs_invasionantonio_lopez_de_santa_anna great_northern_warhenry_i_of_england council_of_trentchiang_kai-sheksamuel_j._tildenfidel_castro wilmot_proviso yu n_dynastybastille b njamin_harrisonwar_of_the_austrian_successioncrimean_warjohn_brown_(abolitionist)teapot_dome_scandal albert_b._fallmarcus_licinius_crassus earl_warrenwarren_g._harding gunpowder_plothomestead_strike samuel_adamsjohn_peter_zenger thomas_painefree_soil_partyst._barth lomew's_day_massacrearthur_wellesley,_1st_duke_of_wellingtoncharles_de_gaulleleon_trotsky hugh_capetal xander_h._stephens haile_selassieilliam_h._sewardrutherford_b._hayes safavid_dynastymuhammad_ali_jinnah kulturkampfmaximilien_de_robespierre hub rt_humphreyluddite hull_housephilip_ii_of_macedon guelphs_a d_ghibellines byzantine_empirealbigensian_crusade diocletianfort_ticonderoga parthian_empirecharles_martel william_jennings_bryanalexan er_ii_of_russiaferdinand_magellanstate_of_franklin ivan_the_terriblemartin_luther_(1953_film) millard_fillmorefrancisco_francoaethelred_th _unready ronald_reaganbenito_mussolini henry_claykitchen_cabinetblack_hole_of_calcuttaancient_corinth john_wilkes_b th john_tylerrobert_walpolehuey_long tokugawa_ieyasuthomas_nastnikita_khrushchev andrew_jacksonportug llabour_party_(uk) monroe_doctrine john_quincy_adamscongress_of_berlintecumsehjacques_cartier battle_ f_the_thamesspanis _civil_warethiopia fugitiv _slave_lawsjoh _a._macdonald council_of_chalcedonpancho_villa war_of_the_pacific george_ allacesusan_b._anthonymarcus_garvey grover_clevelandjohn_haygeorge_b._mcclellanoctober_manifestovitus_bering john_hanc ckwilliam_lloyd_garrisonplatt_amendmentmary,_queen_of_scotsfirst_triumviratefra cisc _vasquez_de_coronadomargaret_thatcher sherma _antitrust_acthanseatic_leaguehenry_morton_stanley ju y_revolutionstephen_a._douglas xyz_affair jimmy_ca terfrancisco_pizarrokublai_khanvasco_da_gama spartabattle_of_caporetto ostend_manifestomustafa_kemal_ataturk peter_the_greatgang_of_fourbattle_of_chance lorsvilledavid_lloyd_georgecardinal_mazarin embargo_act_of_1807 brigham_youngcharles_lindberghhudson's_bay_company attilaparis_commu e jefferson_davisamelia_earhart mali_empireadolf_hitler benedict_arnoldcamillo_benso,_count_of_cavour meiji_restorationblack_panther_party mark_antony f anklin_p ercemolly_maguires zachary_taylorhan_dynastyadlai_stevenson_iijames_k._polk douglas_macarthurboston_massacretoyotomi_hidey shigreenback_partysecond_boer_warthird_crusade j es_buchananjoh _she mangeorge_washingtonwars_of_the_rosesatlantic_charter eleanor_rooseveltcongress_ f_viennajohn_wycliffewinston_churchillmilio_aguinaldom guel_hidalgo_ costill second_bank_of_the_united_statescouncil_of constanceseneca_falls_convention first_crusade spiro_agnewtaiping_rebellionmao_zedong paul_von_hindenburgalbany_congressjawaharlal_nehru battle_of_blenheimethan_allenantonio_de_oliveira_salazar herbert_hooverpepin_the_shortindira_gandhi william_howard_taftthomas_jeffersonga a _abdel_nasser oliver_cromw llsalmon_p._chase battle_of_austerlitzbenjamin_disraeli gadsden purchasegirolamo_savonarola treaty_of_tordesillasbattle_of_marathon elizabeth_cady_stantonbattle_of_kings_mountainchristopher_colum uswilliam_the_conquerorbattle_of_trafalgar charles_evans_hughescleistheneswilliam_tecumseh_shermanmobutu_sese ek prague_spring babur peloponn sian_w rjacques_marquette neroparaguay hyksos artin_van_burenbonus rmycha les_stew rt_parnell edward_the_confessorbartolome _d ssalem_witch_trials battle of_ he_bulge john_ damsaginot_lin henry_cabot_logiuseppe_garib ldi daniel_websterjohn_c._calhoun treaty_of_waitangizebulon_pike genghis_khan
calvin_coolidgewilliam_mckinley
james_monroe
woodrow_wilson
william_henry_harrisonbenjamin_harrisonmillard_fillmoreronald_reagan
john_tyler andrew_jacksonjohn_quincy_adamsgrover_clevelandjimmy_carter
franklin_piercezachary_taylorjames_buchanangeorge_washington
herbert_hooverwilliam_howard_taft
thomas_jefferson
martin_van_buren
john_adams
Figure 5: t-SNE 2-D projections of 451 answer
vectors divided into six major clusters. The
blue cluster is predominantly populated by U.S.
presidents. The zoomed plot reveals temporal
clustering among the presidents based on the
years they spent in office.
from the meaning of the words that it con-
tains as well as the syntax that glues those
words together. Many computational models
of compositionality focus on learning vector
spaces (Zanzotto et al., 2010; Erk, 2012; Grefen-
stette et al., 2013; Yessenalina and Cardie,
2011). Recent approaches towards modeling
compositional vector spaces with neural net-
works have been successful, although simpler
functions have been proposed for short phrases
(Mitchell and Lapata, 2008).
Recursive neural networks have achieved
state-of-the-art performance in sentiment anal-
ysis and parsing (Socher et al., 2013c; Hermann
and Blunsom, 2013; Socher et al., 2013a). rnns
have not been previously used for learning at-
tribute spaces as we do here, although recursive
tensor networks were unsuccessfully applied to
a knowledge base completion task (Socher et
al., 2013b). More relevant to this work are the
dialogue analysis model proposed by Kalchbren-
ner & Blunsom (2013) and the paragraph vec-
tor model described in Le and Mikolov (2014),
both of which are able to generate distributed
representations of paragraphs. Here we present
a simpler approach where a single model is able
to learn complex sentence representations and
average them across paragraphs.
6.2 Factoid Question-Answering
Factoid question answering is often functionally
equivalent to information retrieval. Given a
knowledge base and a query, the goal is to
Thomas Mann
Joseph Conrad
Henrik Ibsen
Franz Kafka
Henry James
Figure 6: A question on the German novelist
Thomas Mann that contains no named entities,
along with the five top answers as scored by
qanta. Each cell in the heatmap corresponds
to the score (inner product) between a node
in the parse tree and the given answer, and
the dependency parse of the sentence is shown
on the left. All of our baselines, including ir-
wiki, are wrong, while qanta uses the plot
description to make a correct guess.
return the answer. Many approaches to this
problem rely on hand-crafted pattern matching
and answer-type classification to narrow down
the search space (Shen, 2007; Bilotti et al.,
2010; Wang, 2006). More recent factoid qa
systems incorporate the web and social media
into their retrieval systems (Bian et al., 2008).
In contrast to these approaches, we place the
burden of learning answer types and patterns
on the model.
7 Future Work
While we have shown that dt-rnns are effec-
tive models for quiz bowl question answering,
other factoid qa tasks are more challenging.
Questions like what does the aarp stand for?
from trec qa data require additional infras-
tructure. A more apt comparison would be to
IBM?s proprietary Watson system (Lally et al.,
2012) for Jeopardy, which is limited to single
sentences, or to models trained on Yago (Hof-
fart et al., 2013).
We would also like to fairly compare qanta
641
Akbar
Shah Jahan
Muhammad
Babur
Ghana
Figure 7: An extremely misleading question
about John Cabot, at least to computer models.
The words muslim and mecca lead to three
Mughal emperors in the top five guesses from
qanta; other models are similarly led awry.
with ir-wiki. A promising avenue for future
work would be to incorporate Wikipedia data
into qanta by transforming sentences to look
like quiz bowl questions (Wang et al., 2007) and
to select relevant sentences, as not every sen-
tence in a Wikipedia article directly describes
its subject. Syntax-specific annotation (Sayeed
et al., 2012) may help in this regard.
Finally, we could adapt the attribute space
learned by the dt-rnn to use information from
knowledge bases and to aid in knowledge base
completion. Having learned many facts about
entities that occur in question text, a dt-rnn
could add new facts to a knowledge base or
check existing relationships.
8 Conclusion
We present qanta, a dependency-tree recursive
neural network for factoid question answering
that outperforms bag of words and informa-
tion retrieval baselines. Our model improves
upon a contrastive max-margin objective func-
tion from previous work to dynamically update
answer vectors during training with a single
model. Finally, we show that sentence-level
representations can be easily and effectively
combined to generate paragraph-level represen-
Q he also successfully represented the amistad
slaves and negotiated the treaty of ghent and
the annexation of florida from spain during his
stint as secretary of state under james monroe
A john quincy adams, henry clay, andrew jack-
son
Q this work refers to people who fell on their
knees in hopeless cathedrals and who jumped
off the brooklyn bridge
A howl, the tempest, paradise lost
Q despite the fact that twenty six martyrs were
crucified here in the late sixteenth century it
remained the center of christianity in its coun-
try
A nagasaki, guadalcanal, ethiopia
Q this novel parodies freudianism in a chapter
about the protagonist ?s dream of holding a
live fish in his hands
A
billy budd, the ambassadors, all my sons
Q a contemporary of elizabeth i he came to power
two years before her and died two years later
A
grover cleveland, benjamin harrison, henry
cabot lodge
Table 2: Five example sentences occuring at
the first sentence position along with their top
three answers as scored by qanta; correct an-
swers are marked with blue and wrong answers
are marked with red. qanta gets the first
three correct, unlike all other baselines. The
last two questions are too difficult for all of
our models, requiring external knowledge (e.g.,
Freudianism) and temporal reasoning.
tations with more predictive power than those
of the individual sentences.
Acknowledgments
We thank the anonymous reviewers, Stephanie
Hwa, Bert Huang, and He He for their insight-
ful comments. We thank Sharad Vikram, R.
Hentzel, and the members of naqt for pro-
viding our data. This work was supported by
nsf Grant IIS-1320538. Boyd-Graber is also
supported by nsf Grant CCF-1018625. Any
opinions, findings, conclusions, or recommen-
dations expressed here are those of the authors
and do not necessarily reflect the view of the
sponsor.
642
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR.
Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the right facts in
the crowd: factoid question answering over social
media. In WWW.
Matthew W. Bilotti, Jonathan Elsas, Jaime Carbonell,
and Eric Nyberg. 2010. Rank learning for factoid
question answering with linguistic and semantic con-
straints. In CIKM.
Jordan Boyd-Graber, Brianna Satinoff, He He, and
Hal Daume III. 2012. Besting the quiz master:
Crowdsourcing incremental classification games. In
EMNLP.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 999999:2121?
2159.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on, volume 1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. CoRR.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In ACL.
Karl Moritz Hermann, Edward Grefenstette, and Phil
Blunsom. 2013. ?not not bad? is not ?bad?: A
distributional account of negation. Proceedings of the
ACL Workshop on Continuous Vector Space Models
and their Compositionality.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. Yago2: A spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, 194:28?61.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and
Philip Resnik. 2014. Political ideology detection
using recursive neural networks.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Composi-
tionality.
Adam Lally, John M Prager, Michael C McCord,
BK Boguraev, Siddharth Patwardhan, James Fan,
Paul Fodor, and Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How watson reads a clue. IBM Journal
of Research and Development.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL.
Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning with
semantic output codes. In NIPS.
P. Pasupat and P. Liang. 2014. Zero-shot entity extrac-
tion from web pages. In ACL.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In NAACL.
Dan Shen. 2007. Using semantic role to improve ques-
tion answering. In EMNLP.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Predict-
ing Sentiment Distributions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing With Composi-
tional Vector Grammars. In ACL.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013b. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In NIPS.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013c. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP.
Richard Socher, Quoc V Le, Christopher D Manning,
and Andrew Y Ng. 2014. Grounded compositional
semantics for finding and describing images with
sentences. TACL.
Nicolas Usunier, David Buffoni, and Patrick Gallinari.
2009. Ranking with ordered weighted pairwise clas-
sification. In ICML.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP.
643
Mengqiu Wang. 2006. A survey of answer extraction
techniques in factoid question answering. Computa-
tional Linguistics, 1(1).
Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
Wsabie: Scaling up to large vocabulary image anno-
tation. In IJCAI.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In EMNLP.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In COLT.
644
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113?1122,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Political Ideology Detection Using Recursive Neural Networks
Mohit Iyyer
1
, Peter Enns
2
, Jordan Boyd-Graber
3,4
, Philip Resnik
2,4
1
Computer Science,
2
Linguistics,
3
iSchool, and
4
UMIACS
University of Maryland
{miyyer,peter,jbg}@umiacs.umd.edu, resnik@umd.edu
Abstract
An individual?s words often reveal their po-
litical ideology. Existing automated tech-
niques to identify ideology from text focus
on bags of words or wordlists, ignoring syn-
tax. Taking inspiration from recent work in
sentiment analysis that successfully models
the compositional aspect of language, we
apply a recursive neural network (RNN)
framework to the task of identifying the po-
litical position evinced by a sentence. To
show the importance of modeling subsen-
tential elements, we crowdsource political
annotations at a phrase and sentence level.
Our model outperforms existing models on
our newly annotated dataset and an existing
dataset.
1 Introduction
Many of the issues discussed by politicians and
the media are so nuanced that even word choice
entails choosing an ideological position. For ex-
ample, what liberals call the ?estate tax? conser-
vatives call the ?death tax?; there are no ideolog-
ically neutral alternatives (Lakoff, 2002). While
objectivity remains an important principle of jour-
nalistic professionalism, scholars and watchdog
groups claim that the media are biased (Groseclose
and Milyo, 2005; Gentzkow and Shapiro, 2010;
Niven, 2003), backing up their assertions by pub-
lishing examples of obviously biased articles on
their websites. Whether or not it reflects an under-
lying lack of objectivity, quantitative changes in the
popular framing of an issue over time?favoring
one ideologically-based position over another?can
have a substantial effect on the evolution of policy
(Dardis et al, 2008).
Manually identifying ideological bias in polit-
ical text, especially in the age of big data, is an
impractical and expensive process. Moreover, bias
They 
dubbed it 
the
death tax? ?
and created a 
big lie about
its adverse effects
on small 
businesses
Figure 1: An example of compositionality in ideo-
logical bias detection (red? conservative, blue?
liberal, gray? neutral) in which modifier phrases
and punctuation cause polarity switches at higher
levels of the parse tree.
may be localized to a small portion of a document,
undetectable by coarse-grained methods. In this pa-
per, we examine the problem of detecting ideologi-
cal bias on the sentence level. We say a sentence
contains ideological bias if its author?s political
position (here liberal or conservative, in the sense
of U.S. politics) is evident from the text.
Ideological bias is difficult to detect, even for
humans?the task relies not only on political
knowledge but also on the annotator?s ability to
pick up on subtle elements of language use. For
example, the sentence in Figure 1 includes phrases
typically associated with conservatives, such as
?small businesses? and ?death tax?. When we take
more of the structure into account, however, we
find that scare quotes and a negative propositional
attitude (a lie about X) yield an evident liberal bias.
Existing approaches toward bias detection have
not gone far beyond ?bag of words? classifiers, thus
ignoring richer linguistic context of this kind and
often operating at the level of whole documents.
In contrast, recent work in sentiment analysis has
used deep learning to discover compositional ef-
fects (Socher et al, 2011b; Socher et al, 2013b).
Building from those insights, we introduce a re-
cursive neural network (RNN) to detect ideological
bias on the sentence level. This model requires
1113
wb
 = changew
a
 = climate
w
d
 = so-called
p
c
 = climate change
p
e
 = so-called climate change
x
d
= x
c
=
x
e
=
x
a
= x
b
=
W
L
W
R
W
R
W
L
Figure 2: An example RNN for the phrase ?so-
called climate change?. Two d-dimensional word
vectors (here, d = 6) are composed to generate a
phrase vector of the same dimensionality, which
can then be recursively used to generate vectors at
higher-level nodes.
richer data than currently available, so we develop
a new political ideology dataset annotated at the
phrase level. With this new dataset we show that
RNNs not only label sentences well but also im-
prove further when given additional phrase-level
annotations. RNNs are quantitatively more effec-
tive than existing methods that use syntactic and
semantic features separately, and we also illustrate
how our model correctly identifies ideological bias
in complex syntactic constructions.
2 Recursive Neural Networks
Recursive neural networks (RNNs) are machine
learning models that capture syntactic and semantic
composition. They have achieved state-of-the-art
performance on a variety of sentence-level NLP
tasks, including sentiment analysis, paraphrase de-
tection, and parsing (Socher et al, 2011a; Hermann
and Blunsom, 2013). RNN models represent a shift
from previous research on ideological bias detec-
tion in that they do not rely on hand-made lexicons,
dictionaries, or rule sets. In this section, we de-
scribe a supervised RNN model for bias detection
and highlight differences from previous work in
training procedure and initialization.
2.1 Model Description
By taking into account the hierarchical nature of
language, RNNs can model semantic composition,
which is the principle that a phrase?s meaning is a
combination of the meaning of the words within
that phrase and the syntax that combines those
words. While semantic composition does not ap-
ply universally (e.g., sarcasm and idioms), most
language follows this principle. Since most ide-
ological bias becomes identifiable only at higher
levels of sentence trees (as verified by our annota-
tion, Figure 4), models relying primarily on word-
level distributional statistics are not desirable for
our problem.
The basic idea behind the standard RNN model
is that each word w in a sentence is associated
with a vector representation x
w
? R
d
. Based on a
parse tree, these words form phrases p (Figure 2).
Each of these phrases also has an associated vector
x
p
? R
d
of the same dimension as the word vectors.
These phrase vectors should represent the meaning
of the phrases composed of individual words. As
phrases themselves merge into complete sentences,
the underlying vector representation is trained to
retain the sentence?s whole meaning.
The challenge is to describe how vectors com-
bine to form complete representations. If two
words w
a
and w
b
merge to form phrase p, we posit
that the phrase-level vector is
x
p
= f(W
L
? x
a
+W
R
? x
b
+ b
1
), (1)
where W
L
and W
R
are d ? d left and right com-
position matrices shared across all nodes in the
tree, b
1
is a bias term, and f is a nonlinear activa-
tion function such as tanh. The word-level vectors
x
a
and x
b
come from a d ? V dimensional word
embedding matrix W
e
, where V is the size of the
vocabulary.
We are interested in learning representations that
can distinguish political polarities given labeled
data. If an element of this vector space, x
d
, repre-
sents a sentence with liberal bias, its vector should
be distinct from the vector x
r
of a conservative-
leaning sentence.
Supervised RNNs achieve this distinction by ap-
plying a regression that takes the node?s vector x
p
as input and produces a prediction y?
p
. This is a
softmax layer
y?
d
= softmax(W
cat
? x
p
+ b
2
), (2)
where the softmax function is
softmax(q) =
exp q
?
k
j=1
exp q
j
(3)
and W
cat
is a k ? d matrix for a dataset with k-
dimensional labels.
We want the predictions of the softmax layer to
match our annotated data; the discrepancy between
categorical predictions and annotations is measured
1114
through the cross-entropy loss. We optimize the
model parameters to minimize the cross-entropy
loss over all sentences in the corpus. The cross-
entropy loss of a single sentence is the sum over
the true labels y
i
in the sentence,
`(y?
s
) =
k
?
p=1
y
p
? log(y?
p
). (4)
This induces a supervised objective function
over all sentences: a regularized sum over all node
losses normalized by the number of nodes N in the
training set,
C =
1
N
N
?
i
`(pred
i
) +
?
2
???
2
. (5)
We use L-BFGS with parameter averag-
ing (Hashimoto et al, 2013) to optimize the model
parameters ? = (W
L
,W
R
,W
cat
,W
e
, b
1
, b
2
). The
gradient of the objective, shown in Eq. (6), is
computed using backpropagation through struc-
ture (Goller and Kuchler, 1996),
?C
??
=
1
N
N
?
i
?`(y?
i
)
??
+ ??. (6)
2.2 Initialization
When initializing our model, we have two choices:
we can initialize all of our parameters randomly or
provide the model some prior knowledge. As we
see in Section 4, these choices have a significant
effect on final performance.
Random The most straightforward choice is to
initialize the word embedding matrix W
e
and com-
position matrices W
L
and W
R
randomly such that
without any training, representations for words and
phrases are arbitrarily projected into the vector
space.
word2vec The other alternative is to initialize the
word embedding matrix W
e
with values that reflect
the meanings of the associated word types. This
improves the performance of RNN models over ran-
dom initializations (Collobert and Weston, 2008;
Socher et al, 2011a). We initialize our model with
300-dimensional word2vec toolkit vectors gener-
ated by a continuous skip-gram model trained on
around 100 billion words from the Google News
corpus (Mikolov et al, 2013).
The word2vec embeddings have linear relation-
ships (e.g., the closest vectors to the average of
?green? and ?energy? include phrases such as ?re-
newable energy?, ?eco-friendly?, and ?efficient
lightbulbs?). To preserve these relationships as
phrases are formed in our sentences, we initialize
our left and right composition matrices such that
parent vector p is computed by taking the average
of children a and b (W
L
= W
R
= 0.5I
d?d
). This
initialization of the composition matrices has pre-
viously been effective for parsing (Socher et al,
2013a).
3 Datasets
We performed initial experiments on a dataset of
Congressional debates that has annotations on the
author level for partisanship, not ideology. While
the two terms are highly correlated (e.g., a member
of the Republican party likely agrees with conserva-
tive stances on most issues), they are not identical.
For example, a moderate Republican might agree
with the liberal position on increased gun control
but take conservative positions on other issues. To
avoid conflating partisanship and ideology we cre-
ate a new dataset annotated for ideological bias on
the sentence and phrase level. In this section we
describe our initial dataset (Convote) and explain
the procedure we followed for creating our new
dataset (IBC).
1
3.1 Convote
The Convote dataset (Thomas et al, 2006) con-
sists of US Congressional floor debate transcripts
from 2005 in which all speakers have been labeled
with their political party (Democrat, Republican,
or independent). We propagate party labels down
from the speaker to all of their individual sentences
and map from party label to ideology label (Demo-
crat? liberal, Republican? conservative). This
is an expedient choice; in future work we plan to
make use of work in political science characteriz-
ing candidates? ideological positions empirically
based on their behavior (Carroll et al, 2009).
While the Convote dataset has seen widespread
use for document-level political classification, we
are unaware of similar efforts at the sentence level.
3.1.1 Biased Sentence Selection
The strong correlation between US political parties
and political ideologies (Democrats with liberal,
Republicans with conservative) lends confidence
that this dataset contains a rich mix of ideological
1
Available at http://cs.umd.edu/
?
miyyer/ibc
1115
statements. However, the raw Convote dataset con-
tains a low percentage of sentences with explicit
ideological bias.
2
We therefore use the features
in Yano et al (2010), which correlate with politi-
cal bias, to select sentences to annotate that have
a higher likelihood of containing bias. Their fea-
tures come from the Linguistic Inquiry and Word
Count lexicon (LIWC) (Pennebaker et al, 2001),
as well as from lists of ?sticky bigrams? (Brown
et al, 1992) strongly associated with one party or
another (e.g., ?illegal aliens? implies conservative,
?universal healthcare? implies liberal).
We first extract the subset of sentences that con-
tains any words in the LIWC categories of Negative
Emotion, Positive Emotion, Causation, Anger, and
Kill verbs.
3
After computing a list of the top 100
sticky bigrams for each category, ranked by log-
likelihood ratio, and selecting another subset from
the original data that included only sentences con-
taining at least one sticky bigram, we take the union
of the two subsets. Finally, we balance the resulting
dataset so that it contains an equal number of sen-
tences from Democrats and Republicans, leaving
us with a total of 7,816 sentences.
3.2 Ideological Books
In addition to Convote, we use the Ideologi-
cal Books Corpus (IBC) developed by Gross et
al. (2013). This is a collection of books and maga-
zine articles written between 2008 and 2012 by au-
thors with well-known political leanings. Each doc-
ument in the IBC has been manually labeled with
coarse-grained ideologies (right, left, and center) as
well as fine-grained ideologies (e.g., religious-right,
libertarian-right) by political science experts.
There are over a million sentences in the IBC,
most of which have no noticeable political bias.
Therefore we use the filtering procedure outlined
in Section 3.1.1 to obtain a subset of 55,932 sen-
tences. Compared to our final Convote dataset, an
even larger percentage of the IBC sentences exhibit
no noticeable political bias.
4
Because our goal
is to distinguish between liberal and conservative
2
Many sentences in Convote are variations on ?I think this
is a good/bad bill?, and there is also substantial parliamentary
boilerplate language.
3
While Kill verbs are not a category in LIWC, Yano et
al. (2010) adopted it from Greene and Resnik (2009) and
showed it to be a useful predictor of political bias. It includes
words such as ?slaughter? and ?starve?.
4
This difference can be mainly attributed to a historical
topics in the IBC (e.g., the Crusades, American Civil War).
In Convote, every sentence is part of a debate about 2005
political policy.
bias, instead of the more general task of classify-
ing sentences as ?neutral? or ?biased?, we filter
the dataset further using DUALIST (Settles, 2011),
an active learning tool, to reduce the proportion
of neutral sentences in our dataset. To train the
DUALIST classifier, we manually assigned class la-
bels of ?neutral? or ?biased? to 200 sentences, and
selected typical partisan unigrams to represent the
?biased? class. DUALIST labels 11,555 sentences as
politically biased, 5,434 of which come from con-
servative authors and 6,121 of which come from
liberal authors.
3.2.1 Annotating the IBC
For purposes of annotation, we define the task of
political ideology detection as identifying, if pos-
sible, the political position of a given sentence?s
author, where position is either liberal or conser-
vative.
5
We used the Crowdflower crowdsourcing
platform (crowdflower.com), which has previously
been used for subsentential sentiment annotation
(Sayeed et al, 2012), to obtain human annotations
of the filtered IBC dataset for political bias on both
the sentence and phrase level. While members of
the Crowdflower workforce are certainly not ex-
perts in political science, our simple task and the
ubiquity of political bias allows us to acquire useful
annotations.
Crowdflower Task First, we parse the filtered
IBC sentences using the Stanford constituency
parser (Socher et al, 2013a). Because of the ex-
pense of labeling every node in a sentence, we only
label one path in each sentence. The process for
selecting paths is as follows: first, if any paths
contain one of the top-ten partisan unigrams,
6
we
select the longest such path; otherwise, we select
the path with the most open class constituencies
(NP, VP, ADJP). The root node of a sentence is
always included in a path.
Our task is shown in Figure 3. Open class con-
stituencies are revealed to the worker incrementally,
starting with the NP, VP, or ADJP furthest from
the root and progressing up the tree. We choose
this design to prevent workers from changing their
lower-level phrase annotations after reading the full
sentence.
5
This is a simplification, as the ideological hierarchy in
IBC makes clear.
6
The words that the multinomial na??ve Bayes classifier
in DUALIST marked as highest probability given a polarity:
market, abortion, economy, rich, liberal, tea, economic, taxes,
gun, abortion
1116
Filtering the Workforce To ensure our anno-
tators have a basic understanding of US politics,
we restrict workers to US IP addresses and require
workers manually annotate one node from 60 dif-
ferent ?gold ? paths annotated by the authors. We
select these nodes such that the associated phrase is
either obviously biased or obviously neutral. Work-
ers must correctly annotate at least six of eight
gold paths before they are granted access to the full
task. In addition, workers must maintain 75% accu-
racy on gold paths that randomly appear alongside
normal paths. Gold paths dramatically improve
the quality of our workforce: 60% of contributors
passed the initial quiz (the 40% that failed were
barred from working on the task), while only 10%
of workers who passed the quiz were kicked out
for mislabeling subsequent gold paths.
Annotation Results Workers receive the
following instructions:
Each task on this page contains a set of
phrases from a single sentence. For each
phrase, decide whether or not the author fa-
vors a political position to the left (Liberal) or
right (Conservative) of center.
? If the phrase is indicative of a position to
the left of center, please choose Liberal.
? If the phrase is indicative of a position to
the right of center, please choose Conser-
vative.
? If you feel like the phrase indicates some
position to the left or right of the political
center, but you?re not sure which direc-
tion, please mark Not neutral, but I?m
unsure of which direction.
? If the phrase is not indicative of a posi-
tion to the left or right of center, please
mark Neutral.
We had workers annotate 7,000 randomly se-
lected paths from the filtered IBC dataset, with half
of the paths coming from conservative authors and
the other half from liberal authors, as annotated
by Gross et al (2013). Three workers annotated
each path in the dataset, and we paid $0.03 per
sentence. Since identifying political bias is a rela-
tively difficult and subjective task, we include all
sentences where at least two workers agree on a
label for the root node in our final dataset, except
when that label is ?Not neutral, but I?m unsure of
Figure 3: Example political ideology annotation
task showing incremental reveal of progressively
longer phrases.
which direction?. We only keep phrase-level an-
notations where at least two workers agree on the
label: 70.4% of all annotated nodes fit this defini-
tion of agreement. All unannotated nodes receive
the label of their closest annotated ancestor. Since
the root of each sentence is always annotated, this
strategy ensures that every node in the tree has a
label. Our final balanced IBC dataset consists of
3,412 sentences (4,062 before balancing and re-
moving neutral sentences) with a total of 13,640
annotated nodes. Of these sentences, 543 switch
polarity (liberal? conservative or vice versa) on
an annotated path.
While we initially wanted to incorporate neutral
labels into our model, we observed that lower-level
phrases are almost always neutral while full sen-
tences are much more likely to be biased (Figure 4).
Due to this discrepancy, the objective function in
Eq. (5) was minimized by making neutral predic-
tions for almost every node in the dataset.
4 Experiments
In this section we describe our experimental frame-
work. We discuss strong baselines that use lexi-
cal and syntactic information (including framing-
specific features from previous work) as well as
multiple RNN configurations. Each of these mod-
els have the same task: to predict sentence-level
ideology labels for sentences in a test set. To ac-
count for label imbalance, we subsample the data
so that there are an equal number of labels and
report accuracy over this balanced dataset.
1117
0 1 2 3 4 5 6 7 8 9 10Node Depth
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Lab
el P
rob
abil
ity
Label Probability vs. Node Depth
ConservativeLiberalNeutral / No Agreement
Figure 4: Proportion of liberal, conservative, and
neutral annotations with respect to node depth (dis-
tance from root). As we get farther from the root
of the tree, nodes are more likely to be neutral.
4.1 Baselines
? The RANDOM baseline chooses a label at ran-
dom from {liberal, conservative}.
? LR1, our most basic logistic regression base-
line, uses only bag of words (BoW) features.
? LR2 uses only BoW features. However, LR2
also includes phrase-level annotations as sep-
arate training instances.
7
? LR3 uses BoW features as well as syntac-
tic pseudo-word features from Greene &
Resnik (2009). These features from depen-
dency relations specify properties of verbs
(e.g., transitivity or nominalization).
8
? LR-(W2V) is a logistic regression model
trained on the average of the pretrained word
embeddings for each sentence (Section 2.2).
The LR-(W2V) baseline allows us to compare
against a strong lexical representation that encodes
syntactic and semantic information without the
RNN tree structure. (LR1, LR2) offer a compari-
son to simple bag of words models, while the LR3
baseline contrasts traditional syntactic features with
those learned by RNN models.
4.2 RNN Models
For RNN models, we generate a feature vector for
every node in the tree. Equation 1 allows us to
7
The Convote dataset was not annotated on the phrase
level, so we only provide a result for the IBC dataset.
8
We do not include phrase-level annotations in the LR3
feature set because the pseudo-word features can only be
computed from full sentence parses.
Model Convote IBC
RANDOM 50% 50%
LR1 64.7% 62.1%
LR2 ? 61.9%
LR3 66.9% 62.6%
LR-(W2V) 66.6% 63.7%
RNN1 69.4% 66.2%
RNN1-(W2V) 70.2% 67.1%
RNN2-(W2V) ? 69.3%
Table 1: Sentence-level bias detection accuracy.
The RNN framework, adding phrase-level data, and
initializing with word2vec all improve performance
over logistic regression baselines. The LR2 and
RNN2-(W2V) models were not trained on Convote
since it lacks phrase annotations.
percolate the representations to the root of the tree.
We generate the final instance representation by
concatenating the root vector and the average of
all other vectors (Socher et al, 2011b). We train
an L
2
-regularized logistic regression model over
these concatenated vectors to obtain final accuracy
numbers on the sentence level.
To analyze the effects of initialization and
phrase-level annotations, we report results for three
different RNN settings. All three models were im-
plemented as described in Section 2 with the non-
linearity f set to the normalized tanh function,
f(v) =
tanh(v)
?tanh(v)?
. (7)
We perform 10-fold cross-validation on the training
data to find the best RNN hyperparameters.
9
We report results for RNN models with the fol-
lowing configurations:
? RNN1 initializes all parameters randomly and
uses only sentence-level labels for training.
? RNN1-(W2V) uses the word2vec initialization
described in Section 2.2 but is also trained on
only sentence-level labels.
? RNN2-(W2V) is initialized using word2vec
embeddings and also includes annotated
phrase labels in its training. For this model,
we also introduce a hyperparameter ? that
weights the error at annotated nodes (1? ?)
higher than the error at unannotated nodes (?);
since we have more confidence in the anno-
tated labels, we want them to contribute more
towards the objective function.
9
[?
W
e
=1e-6, ?
W
=1e-4, ?
W
cat
=1e-3, ? = 0.3]
1118
For all RNN models, we set the word vector
dimension d to 300 to facilitate direct comparison
against the LR-(W2V) baseline.
10
5 Where Compositionality Helps Detect
Ideological Bias
In this section, we examine the RNN models to see
why they improve over our baselines. We also give
examples of sentences that are correctly classified
by our best RNN model but incorrectly classified by
all of the baselines. Finally, we investigate sentence
constructions that our model cannot handle and
offer possible explanations for these errors.
Experimental Results Table 1 shows the RNN
models outperforming the bag-of-words base-
lines as well as the word2vec baseline on both
datasets. The increased accuracy suggests that the
trained RNNs are capable of detecting bias polar-
ity switches at higher levels in parse trees. While
phrase-level annotations do not improve baseline
performance, the RNN model significantly bene-
fits from these annotations because the phrases are
themselves derived from nodes in the network struc-
ture. In particular, the phrase annotations allow our
best model to detect bias accurately in complex
sentences that the baseline models cannot handle.
Initializing the RNN W
e
matrix with word2vec
embeddings improves accuracy over randomly ini-
tialization by 1%. This is similar to improvements
from pretrained vectors from neural language mod-
els (Socher et al, 2011b).
We obtain better results on Convote than on IBC
with both bag-of-words and RNN models. This
result was unexpected since the Convote labels
are noisier than the annotated IBC labels; however,
there are three possible explanations for the discrep-
ancy. First, Convote has twice as many sentences
as IBC, and the extra training data might help the
model more than IBC?s better-quality labels. Sec-
ond, since the sentences in Convote were originally
spoken, they are almost half as short (21.3 words
per sentence) as those in the IBC (42.2 words per
sentence). Finally, some information is lost at ev-
ery propagation step, so RNNs are able to model
the shorter sentences in Convote more effectively
than the longer IBC sentences.
Qualitative Analysis As in previous work
(Socher et al, 2011b), we visualize the learned
10
Using smaller vector sizes (d ? {50, 100}, as in previous
work) does not significantly change accuracy.
vector space by listing the most probable n-grams
for each political affiliation in Table 2. As expected,
conservatives emphasize values such as freedom
and religion while disparaging excess government
spending and their liberal opposition. Meanwhile,
liberals inveigh against the gap between the rich
and the poor while expressing concern for minority
groups and the working class.
Our best model is able to accurately model the
compositional effects of bias in sentences with com-
plex syntactic structures. The first three sentences
in Figure 5 were correctly classified by our best
model (RNN2-(W2V)) and incorrectly classified by
all of the baselines. Figures 5A and C show tradi-
tional conservative phrases, ?free market ideology?
and ?huge amounts of taxpayer money?, that switch
polarities higher up in the tree when combined with
phrases such as ?made worse by? and ?saved by?.
Figure 5B shows an example of a bias polarity
switch in the opposite direction: the sentence neg-
atively portrays supporters of nationalized health
care, which our model picks up on.
Our model often makes errors when polarity
switches occur at nodes that are high up in the
tree. In Figure 5D, ?be used as an instrument to
achieve charitable or social ends? reflects a lib-
eral ideology, which the model predicts correctly.
However, our model is unable to detect the polarity
switch when this phrase is negated with ?should
not?. Since many different issues are discussed
in the IBC, it is likely that our dataset has too few
examples of some of these issues for the model to
adequately learn the appropriate ideological posi-
tions, and more training data would resolve many
of these errors.
6 Related Work
A growing NLP subfield detects private states such
as opinions, sentiment, and beliefs (Wilson et al,
2005; Pang and Lee, 2008) from text. In general,
work in this category tends to combine traditional
surface lexical modeling (e.g., bag-of-words) with
hand-designed syntactic features or lexicons. Here
we review the most salient literature related to the
present paper.
6.1 Automatic Ideology Detection
Most previous work on ideology detection ignores
the syntactic structure of the language in use in
favor of familiar bag-of-words representations for
1119
be used as an instrument to 
achieve charitable or social ends
should notthe law
X
X
X
nationalized health care
An entertainer once
said a sucker is born
every minute , and
surely this is the
case with
those who
support
made worse by
the implementing
Thus , the harsh
conditions for
farmers caused
by a number of
factors ,
, have created a 
continuing stream of 
people leaving the 
countryside and going 
to live in cities that do 
not have jobs for them .
of free-market
ideology
huge 
amounts of 
taxpayer 
money
saved byBut taxpayers 
do know
already that 
TARP was
designed in a 
way that
allowed
to continue to 
show the same 
arrogant traits 
that should have
destroyed their
companies .
the same 
corporations
who were
A B
C D
Figure 5: Predictions by RNN2-(W2V) on four sentences from the IBC. Node color is the true label (red
for conservative, blue for liberal), and an ?X? next to a node means the model?s prediction was wrong. In
A and C, the model accurately detects conservative-to-liberal polarity switches, while in B it correctly
predicts the liberal-to-conservative switch. In D, negation confuses our model.
the sake of simplicity. For example, Gentzkow
and Shapiro (2010) derive a ?slant index? to rate
the ideological leaning of newspapers. A newspa-
per?s slant index is governed by the frequency of
use of partisan collocations of 2-3 tokens. Simi-
larly, authors have relied on simple models of lan-
guage when leveraging inferred ideological posi-
tions. E.g., Gerrish and Blei (2011) predict the
voting patterns of Congress members based on bag-
of-words representations of bills and inferred polit-
ical leanings of those members.
Recently, Sim et al (2013) have proposed a
model to infer mixtures of ideological positions
in documents, applied to understanding the evolu-
tion of ideological rhetoric used by political can-
didates during the campaign cycle. They use an
HMM-based model, defining the states as a set
of fine-grained political ideologies, and rely on
a closed set of lexical bigram features associated
with each ideology, inferred from a manually la-
beled ideological books corpus. Although it takes
elements of discourse structure into account (cap-
turing the?burstiness? of ideological terminology
usage), their model explicitly ignores intrasenten-
tial contextual influences of the kind seen in Fig-
ure 1. Other approaches on the document level use
topic models to analyze bias in news articles, blogs,
and political speeches (Ahmed and Xing, 2010; Lin
et al, 2008; Nguyen et al, 2013).
6.2 Subjectivity Detection
Detecting subjective language, which conveys opin-
ion or speculation, is a related NLP problem. While
sentences lacking subjective language may con-
tain ideological bias (e.g., the topic of the sen-
tence), highly-opinionated sentences likely have
obvious ideological leanings. In addition, senti-
ment and subjectivity analysis offers methodolog-
ical approaches that can be applied to automatic
bias detection.
Wiebe et al (2004) show that low-frequency
words and some collocations are a good indica-
tors of subjectivity. More recently, Recasens et al
(2013) detect biased words in sentences using indi-
cator features for bias cues such as hedges and fac-
tive verbs in addition to standard bag-of-words and
part-of-speech features. They show that this type of
linguistic information dramatically improves per-
formance over several standard baselines.
Greene and Resnik (2009) also emphasize the
connection between syntactic and semantic rela-
tionships in their work on ?implicit sentiment?,
1120
n Most conservative n-grams Most liberal n-grams
1 Salt, Mexico, housework, speculated, consensus, lawyer,
pharmaceuticals, ruthless, deadly, Clinton, redistribution
rich, antipsychotic, malaria, biodiversity, richest, gene,
pesticides, desertification, Net, wealthiest, labor, fertil-
izer, nuclear, HIV
3 prize individual liberty, original liberal idiots, stock mar-
ket crash, God gives freedom, federal government inter-
ference, federal oppression nullification, respect individ-
ual liberty, Tea Party patriots, radical Sunni Islamists,
Obama stimulus programs
rich and poor,?corporate greed?, super rich pay, carrying
the rich, corporate interest groups, young women work-
ers, the very rich, for the rich, by the rich, soaking the
rich, getting rich often, great and rich, the working poor,
corporate income tax, the poor migrants
5 spending on popular government programs, bailouts and
unfunded government promises, North America from
external threats, government regulations place on busi-
nesses, strong Church of Christ convictions, radical Is-
lamism and other threats
the rich are really rich, effective forms of worker partic-
ipation, the pensions of the poor, tax cuts for the rich,
the ecological services of biodiversity, poor children and
pregnant women, vacation time for overtime pay
7 government intervention helped make the Depression
Great, by God in His image and likeness, producing
wealth instead of stunting capital creation, the tradi-
tional American values of limited government, trillions
of dollars to overseas oil producers, its troubled assets to
federal sugar daddies, Obama and his party as racialist
fanatics
African Americans and other disproportionately poor
groups; the growing gap between rich and poor; the
Bush tax cuts for the rich; public outrage at corporate
and societal greed; sexually transmitted diseases , most
notably AIDS; organize unions or fight for better condi-
tions, the biggest hope for health care reform
Table 2: Highest probability n-grams for conservative and liberal ideologies, as predicted by the RNN2-
(W2V) model.
which refers to sentiment carried by sentence struc-
ture and not word choice. They use syntactic depen-
dency relation features combined with lexical infor-
mation to achieve then state-of-the-art performance
on standard sentiment analysis datasets. However,
these syntactic features are only computed for a
thresholded list of domain-specific verbs. This
work extends their insight of modeling sentiment
as an interaction between syntax and semantics to
ideological bias.
Future Work There are a few obvious directions
in which this work can be expanded. First, we can
consider more nuanced political ideologies beyond
liberal and conservative. We show that it is pos-
sible to detect ideological bias given this binary
problem; however, a finer-grained study that also
includes neutral annotations may reveal more sub-
tle distinctions between ideologies. While acquir-
ing data with obscure political biases from the IBC
or Convote is unfeasible, we can apply a similar
analysis to social media (e.g., Twitter or Facebook
updates) to discover how many different ideologies
propagate in these networks.
Another direction is to implement more sophis-
ticated RNN models (along with more training
data) for bias detection. We attempted to apply
syntactically-untied RNNs (Socher et al, 2013a)
to our data with the idea that associating separate
matrices for phrasal categories would improve rep-
resentations at high-level nodes. While there were
too many parameters for this model to work well
here, other variations might prove successful, espe-
cially with more data. Finally, combining sentence-
level and document-level models might improve
bias detection at both levels.
7 Conclusion
In this paper we apply recursive neural networks
to political ideology detection, a problem where
previous work relies heavily on bag-of-words mod-
els and hand-designed lexica. We show that our
approach detects bias more accurately than existing
methods on two different datasets. In addition, we
describe an approach to crowdsourcing ideological
bias annotations. We use this approach to create a
new dataset from the IBC, which is labeled at both
the sentence and phrase level.
Acknowledgments
We thank the anonymous reviewers, Hal Daum?e,
Yuening Hu, Yasuhiro Takayama, and Jyothi Vinju-
mur for their insightful comments. We also want to
thank Justin Gross for providing the IBC and Asad
Sayeed for help with the Crowdflower task design,
as well as Richard Socher and Karl Moritz Her-
mann for assisting us with our model implemen-
tations. This work was supported by NSF Grant
CCF-1018625. Boyd-Graber is also supported by
NSF Grant IIS-1320538. Any opinions, findings,
conclusions, or recommendations expressed here
are those of the authors and do not necessarily re-
flect the view of the sponsor.
1121
References
Amr Ahmed and Eric P Xing. 2010. Staying informed: super-
vised and semi-supervised multi-view topical analysis of
ideological perspective. In EMNLP.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent
J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram
models of natural language. Comp. Ling., 18(4):467?479.
Royce Carroll, Jeffrey B Lewis, James Lo, Keith T Poole, and
Howard Rosenthal. 2009. Measuring bias and uncertainty
in dw-nominate ideal point estimates via the parametric
bootstrap. Political Analysis, 17(3):261?275.
Ronan Collobert and Jason Weston. 2008. A unified architec-
ture for natural language processing: Deep neural networks
with multitask learning. In ICML.
Frank E Dardis, Frank R Baumgartner, Amber E Boydstun,
Suzanna De Boef, and Fuyuan Shen. 2008. Media framing
of capital punishment and its impact on individuals? cogni-
tive responses. Mass Communication & Society, 11(2):115?
140.
Matthew Gentzkow and Jesse M Shapiro. 2010. What drives
media slant? evidence from us daily newspapers. Econo-
metrica, 78(1):35?71.
Sean Gerrish and David M Blei. 2011. Predicting legislative
roll calls from text. In ICML.
Christoph Goller and Andreas Kuchler. 1996. Learning task-
dependent distributed representations by backpropagation
through structure. In Neural Networks, 1996., IEEE Inter-
national Conference on, volume 1.
Stephan Greene and Philip Resnik. 2009. More than words:
Syntactic packaging and implicit sentiment. In NAACL.
Tim Groseclose and Jeffrey Milyo. 2005. A measure of media
bias. The Quarterly Journal of Economics, 120(4):1191?
1237.
Justin Gross, Brice Acree, Yanchuan Sim, and Noah A Smith.
2013. Testing the etch-a-sketch hypothesis: A compu-
tational analysis of mitt romney?s ideological makeover
during the 2012 primary vs. general elections. In APSA
2013 Annual Meeting Paper.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and
Takashi Chikayama. 2013. Simple customization of recur-
sive neural networks for semantic relation classification. In
EMNLP.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role of
Syntax in Vector Space Models of Compositional Seman-
tics. In ACL.
George Lakoff. 2002. Moral Politics: How Liberals and Con-
servatives Think, Second Edition. University of Chicago
Press.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008.
A joint topic and perspective model for ideological dis-
course. In Machine Learning and Knowledge Discovery in
Databases, pages 17?32. Springer.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In NIPS,
pages 1106?1114.
David Niven. 2003. Objective evidence on media bias: News-
paper coverage of congressional party switchers. Journal-
ism & Mass Communication Quarterly, 80(2):311?326.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment
analysis. Foundations and trends in information retrieval,
2(1-2).
James W. Pennebaker, Martha E. Francis, and Roger J. Booth.
2001. Linguistic inquiry and word count [computer soft-
ware]. Mahwah, NJ: Erlbaum Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan
Jurafsky. 2013. Linguistic models for analyzing and de-
tecting biased language.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy
Weinberg. 2012. Grammatical structures for word-level
sentiment detection. In NAACL.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features and
instances. In EMNLP.
Yanchuan Sim, Brice Acree, Justin H Gross, and Noah A
Smith. 2013. Measuring ideological proportions in politi-
cal speeches. In EMNLP.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.
Ng, and Christopher D. Manning. 2011a. Dynamic Pool-
ing and Unfolding Recursive Autoencoders for Paraphrase
Detection. In NIPS.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.
Ng, and Christopher D. Manning. 2011b. Semi-Supervised
Recursive Autoencoders for Predicting Sentiment Distribu-
tions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing With Compositional Vector
Grammars. In ACL.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. 2013b. Recursive deep models for semantic compo-
sitionality over a sentiment treebank. In EMNLP.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the
vote: Determining support or opposition from Congres-
sional floor-debate transcripts. In EMNLP.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell,
and Melanie Martin. 2004. Learning subjective language.
Comp. Ling., 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP.
Tae Yano, Philip Resnik, and Noah A Smith. 2010. Shedding
(a thousand points of) light on biased language. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechanical
Turk, pages 152?158.
1122
