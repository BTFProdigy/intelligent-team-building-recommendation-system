Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842?851,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Alternative to Head-Driven Approaches for
Parsing a (Relatively) Free Word-Order Language
Reut Tsarfaty Khalil Sima?an Remko Scha
Institute for Logic Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan,r.scha}@uva.nl
Abstract
Applying statistical parsers developed for
English to languages with freer word-
order has turned out to be harder than
expected. This paper investigates the
adequacy of different statistical parsing
models for dealing with a (relatively)
free word-order language. We show
that the recently proposed Relational-
Realizational (RR) model consistently
outperforms state-of-the-art Head-Driven
(HD) models on the Hebrew Treebank.
Our analysis reveals a weakness of HD
models: their intrinsic focus on configu-
rational information. We conclude that the
form-function separation ingrained in RR
models makes them better suited for pars-
ing nonconfigurational phenomena.
1 Introduction
Parsing technology has come a long way since
Charniak (1996) demonstrated that a simple tree-
bank PCFG performs better than any other parser
(with F
1
75 accuracy) on parsing the WSJ Penn
treebank (Marcus et al, 1993). Treebank Gram-
mars (Scha, 1990; Charniak, 1996) trained on
large corpora nowadays present the best available
means to parse natural language text.
The performance curve for parsing the WSJ was
a steep one at first, as the incorporation of no-
tions such as head, distance, subcategorization
(Charniak, 1997; Collins, 1999) brought about
a dramatic increase in parsing accuracy to the
level of F
1
88. Discriminative approaches, Data-
Oriented Parsing (?all-subtrees?) approaches, and
self-training techniques brought further improve-
ments, and recent results are starting to level off at
around F
1
92.1 (McClosky et al, 2008).
As the interest of the NLP community grows
to encompass more languages, we observe efforts
towards adapting an English parser for parsing
other languages (e.g., (Collins et al, 1999)), or
towards designing a language-independent frame-
work based on principles underlying the mod-
els for parsing English (Bikel, 2002). The per-
formance curve for parsing other languages with
these models looks rather different. A case in point
is Modern Standard Arabic. Since the initial ef-
fort of (Bikel, 2002) to parse the Arabic treebank
(Maamouri et al, 2004), which yielded F
1
75 ac-
curacy, four years and successive revisions have
led to no more than F
1
79 (Maamouri et al, 2008).
This pattern from Arabic is not peculiar. The
level of state-of-the-art results for other languages
still lags behind those for English, even after
putting considerable effort into the adaptation.1
Given that these languages are inherently differ-
ent from English and from one another, it appears
that we cannot avoid a question concerning the ad-
equacy of the models used to parse them. That is,
given the properties of a language, which model-
ing strategy would be appropriate for parsing it?
Until recently, there has been practically
no computationally affordable alternative to the
Head-Driven (HD) approach in the development
of phrase-structure based statistical parsing mod-
els. Recently, we proposed the Relational-
Realizational (RR) approach that rests upon differ-
ent premises (Tsarfaty and Sima?an, 2008). The
question of how the RR model fares against the
HD models that have so far been predominantly
used has never been tackled. Yet, it is precisely
such a comparison that can shed new light on the
question of adequacy we posed above.
Empirically quantifying the effects of differ-
ent modeling choices has been addressed for En-
glish by, e.g., (Johnson, 1998; Klein and Manning,
2003), and for German by, e.g., (Dubey, 2004;
1Consider, e.g., ?The PaGe shared task on parsing Ger-
man? (Kubler, 2008), reporting F
1
75, F
1
79, F
1
83 for the
participating parsers.
842
Rafferty and Manning, 2008). This paper provides
an empirical systematic comparison of conceptu-
ally different modeling strategies with respect to
parsing Hebrew. This comparison is intended to
provide a first answer to the question of parser ad-
equacy in the face of word-order freedom.
Our two empirical results are unequivocal.
Firstly, RR models significantly outperform HD
models (about 2 points absolute improvement in
F
1
) in parsing the Modern Hebrew treebank. In
particular, RR models show better performance
in identifying the constituents for which syntactic
positions are relatively free. Secondly, we show
a novel variation of the HD model, incorporating
the Relational notions of the RR model, on the hy-
pothesis that this might bridge the gap. The RR
model remains superior.
Our post-experimental analysis shows that HD
modeling is inherently problematic for parsing a
language with freer word-order because of the
hard-wiring of notions such as left, right and dis-
tance from the head. RR models, taking a prin-
cipled approach towards capturing variable form-
function correspondence patterns, are better suited
for parsing nonconfigurational phenomena.
2 The Data
This section describes some properties of Modern
Hebrew (henceforth, Hebrew) that make it signifi-
cantly different from English. These properties af-
fect the syntactic representations found in the He-
brew Treebank and the kind of syntactic phenom-
ena a parser for Hebrew has to cope with.
Modern Hebrew is a Semitic language with a
canonical SVO word-order pattern,2 yet it allows
considerable freedom in the placement of syntac-
tic constituents in a clause. For example, linguistic
elements of any kind may be fronted, triggering
an inversion familiar from Germanic languages
as in (1b) (Triggered Inversion (TI) in (Shlonsky,
1997)). Under some information structuring con-
ditions, Verb Initial (VI) constructions are also al-
lowed, as in (1c) (Melnik, 2002). All sentences
in (1) thus mean ?Dani gave the present to Dina?,
despite their different word-ordering.
(1) a. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
b. et hamatana natan dani ledina
ACC the-present gave Dani to-Dina
2SVO is an abbreviation for the Subject-Verb-Object type
in the basic word-order typology of (Greenberg, 1963).
Word Order Frequency Relative Frequency
SV 1612 41%
VS 1144 29%
No S 624 16%
No V 550 14%
Table 1: Modern Hebrew Predicative Clause-
Types in 3930 Predicative Matrix Clauses in the
Training Set of the Modern Hebrew Treebank.
c. natan dani et hamatana ledina
gave Dani ACC the-present to-Dina
A corpus study we conducted on a fragment of
the Modern Hebrew treebank reveals that although
there is a significant number of subjects preceding
verbs in simple (matrix) clauses (41%), there are
also a fair number of sentences for which this or-
der is reversed (29%), and there is evidence for
other configurations, such as empty realization of
subjects (16%) and non-verbal realization of pred-
icates (14%).
In the face of such lack of consistency in its
configurational position, the grammatical function
Object in Hebrew is indicated by Differential Ob-
ject Marking (DOM) (Aissen, 2003). NP objects
in Hebrew are marked for accusativity (using the
marker et) if they are also marked for definiteness
(indicated by the prefix ha). So, in contrast with
(2a)-(2b), the indefinite object renders (2c) un-
grammatical, and the missing accusativity renders
(2d) awkward. The fact that marking NP objects
involves the joint contribution of multiple surface
elements (et, ha) contributing features to the NP
constituent is referred to as extended exponence
(Matthews, 1993, p. 182).
(2) a. dani natan matana ledina
Dani gave present to-Dina
?Dani gave a present to Dina?
b. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
?Dani gave the present to Dina?
c. *dani natan et matana ledina
Dani gave ACC present to-Dina
d. ??dani natan hamatana ledina
Dani gave the-present to-Dina
These data pose a challenge to generative pars-
ing models, as they would be required to gener-
ate alternative word-order patterns while maintain-
ing a coherent pattern of object marking, encom-
843
passing the contribution of multiple surface expo-
nents. The question this paper addresses is there-
fore what kind of modeling approach would be ad-
equate for modeling the interplay between syntax
and morphology in marking grammatical relations
in Hebrew, as reflected by the sentence-pair (3).
They both mean, roughly, ?Dani gave the present
to Dina yesterday; their word-order vary, but the
pattern of object marking is retained.
(3) a. dani natan etmol et hamatana ledina
Dani gave yesterday ACC the-present
to-Dina
b. et hamatana natan etmol dani ledina
ACC the-present gave yesterday dani
to-dina
3 The Models
The different models we experiment with are all
trained on syntactic structures annotated in the
Modern Hebrew Treebank (Sima?an et al, 2001).
The native representation of clause-level cate-
gories in the Treebank employs flat structures.
This choice was made due to the lack of empirical
evidence in Hebrew for grouping freely positioned
syntactic elements to form a constituent.3 In order
to compensate for the ambiguity in the interpreta-
tion of flat structures, additional information such
as morphological marking and grammatical func-
tion labels is added to the phrase-structure trees.
3.1 The State-Splits Approach
The simplest way to encode grammatical func-
tions information on top of the phrase-structure
representation in the treebank is by decorating
non-terminal nodes with morphological or func-
tional features, similarly to the rich representation
format of syntactic categories in GPSG. This is
the approach taken by the annotators of the He-
brew treebank in which information about mor-
phological marking appears at multiple levels of
constituency (Guthmann et al, 2009), and func-
tional features (such as subject, object, etc.) deco-
rate phrase-level constituent labels (Sima?an et al,
2001). The S-level representation of our example
sentences (3a)?(3b) then would be as we depict
in figure 1, which can be read off as feature-rich
3Such clauses are defined formally as exocentric in for-
mal theories of syntax, and are used to describe syntactic
structures in, e.g., Tagalog, Hungarian and Warlpiri (Bres-
nan, 2001, page 110). This flat representation format is char-
acteristic of treebanks for other languages with relatively-free
word-order as well, such as German (cf. (Kubler, 2008)).
PCFG productions. We refer to this approach as
the State-Splits (SP) approach, which serves as the
baseline for the rest of our investigation.
3.2 The Head-Driven Approach
Following the linguistic wisdom that the inter-
nal organization of syntactic constituents revolves
around their heads, Head-Driven (HD) models
have been proposed by (Magerman, 1995; Char-
niak, 1997; Collins, 1999). In a generative HD
model, the head daughter is generated first, con-
ditioned on properties of the mother node. Then,
sisters of the head daughter are generated condi-
tioned on the head, typically by left and right gen-
eration processes. Overall, HD processes have the
modeling advantage that they capture structurally-
marked positions that characterize the argument
structure of the sentence. The simplest possible
process uses unigram probabilities, but (Klein and
Manning, 2003) show that using vertical and hori-
zontal Markovization improves parsing accuracy.4
An unlexicalized generative HD model will
generate our two example sentences as we illus-
trate in figure 2. The generation of the context-
free events in figure 1 is then broken down to
seven different context-free parameters each, en-
coding head-parent and head-sister structural rela-
tionships ? the latter mediated with a structurally-
marked delta function (?
i
). The rich morpho-
logical representation of phrase-level NP objects
(+def/acc), for instance, is conditioned on the
head sister, its direction, and the distance from the
head (check, e.g., nodes ?
L
1
,?
R
2
).
3.3 The Relational-Realizational Approach
The Relational-Realizational (RR) parsing model
of (Tsarfaty and Sima?an, 2008) similarly decom-
poses the generation of the context-free events in
figure 1 into multiple independent parameters, but
does so in a conceptually different way. Instead of
decomposing a context-free event to head and sis-
ters, the RR model is best viewed as a generative
grammar that decomposes it to form and function.
The RR grammar first generates a set of gram-
matical functions depicting the Relational Net-
work (RN) (Perlmutter, 1982) of the clause. This
4The success of Head-Driven models (Charniak, 1997;
Collins, 2003) was initially attributed to the fact that they
were fully lexicalized, but (Klein and Manning, 2003) show
that an unlexicalized model combining Head-Driven Marko-
vian processes with linguistically motivated state-splits can
approach the performance of fully lexicalized models.
844
(3a) S
NP-SBJ
Dani
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP
+D+ACC
-OBJ
et-hamatana
the-present
PP-COM
le-dina
to-Dina
(3b) S
NP
+D+ACC
-OBJ
et-ha-matana
the-present
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP-SBJ
Dani
Dani
PP-COM
le-dina
to-Dina
Figure 1: The State-Splits Approach for Ex. (3)
(3a) S
V P@S
L,?
L
1
, V P@S
NP
Dani
Dani
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,?
R
3
, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,?
L1
, V P@S
NP
+D+ACC
et-ha-matana
the-present
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
Dani
Dani
R,?
R
3
, V P@S
PP
le-dina
to-Dina
Figure 2: The Head-Driven Approach for Ex. (3)
(3a) S
{SBJ,PRD,OBJ,COM}@S
SBJ@S
NP
Dani
Dani
PRD@S
VP
natan
gave
PRD : OBJ@S
ADVP
etmol
yesterday
OBJ@S
NP
+D+ACC
et-hamatana
the-present
COM@S
PP
le-dina
to-Dina
(3b) S
{SBJ,PRD,OBJ,COM}@S
OBJ@S
NP
+D+ACC
et-ha-matana
the-Present
PRD@S
VP
natan
gave
PRD : SBJ@S
ADVP@S
etmol
yesterday
SBJ@S
NP
Dani
Dani
COM@S
PP
le-dina
to-Dina
Figure 3: The Relational-Realizational Approach
RN provides an abstract set-theoretic representa-
tion of the argument structure of the clause.5 This
is called the projection phase. Then an ordering
of the grammatical relations is generated, includ-
ing reserved contextual slots for adjunction and/or
punctuation marks. This is called the configura-
tion phase. Finally, each of the grammatical func-
tion labels and adjunction slots gets realized as a
morphosyntactic representation (a category label
plus dominated morphological features) of the re-
spective daughter constituent. This is called the
realization phase.6
Figure 3 shows the generation of sentences
(3a)?(3b) following the projection, configuration
and realization phases corresponding to the top-
down context-free layers of the tree. In both
cases, the same relational network is generated,
capturing the fact that they have the same argu-
ment structure. Then the different orderings of
the grammatical elements are generated, reserving
an adjunction slot for sentential modification (la-
beled by short context). Interestingly, the HD/RR
models for our sentences are of comparable size
(seven parameters) but the parameter types en-
code radically different notions. Illustrative of the
difference is the realization of a morphologically
marked NP object. In the RR model this is con-
ditioned on a grammatical relation (check, for in-
stance, node OBJ@S) and in the HD model it is
conditioned on linear ordering or configurational
notions such as left, right and distance.
4 Experiments
Goal We set out to compare the performance
of the different modeling approaches for pars-
ing Modern Hebrew. Considerable effort was de-
voted to making the models strictly comparable,
in terms of preparing the data, defining statistical
events, and unifying the rules determining cross-
cutting linguistic notions (e.g., heads and predi-
cates, grammatical functions and subcat sets). We
spell out some of the setup considerations below.
Data We use the Modern Hebrew treebank
(MHTB) (Sima?an et al, 2001) consisting of 6501
sentences from news-wire texts, morphologically
analyzed and syntactically annotated as phrase-
5Unlike in HD models or dependency grammars, the head
predicative element has no distinguished status here.
6Realization of adjunction slots (but not of function la-
bels) may generate multiple sisters adjoining at a single
position.
845
GF Description Applicable to. . .
PRD Predicative Elements VP, PREDP
SBJ Grammatical Subjects NP, SBAR
OBJ Direct Objects NP
COM Indirect Objects NP, PP
FInite Complements SBAR
IC Infinitival Complements VP
CNJ A Conjunct within
a Conjunction Structure All
Table 2: Grammatical Functions in the MHTB
SP-PCFG Expansion P(C
l
n
, . . . , C
h
, . . . , C
r
n
|P )
HD-PCFG Head P(C
h
|P )
Left Branch? P(L:?
l
1
, H:?
h
|C
h
, P )
Right Branch? P(C
h
, R:?
r
1
|?
h
, C
h
, P )
Left Arg/Mod P(C
l
i
,?
l
i+1
| L ,?
l
i
, C
h
, P )
Right Arg/Mod P(C
r
i
,?
r
i+1
| R ,?
r
i
, C
h
, P )
Left Final? P(C
1
| L ,?
l
n?1
, C
h
, P )
Right Final? P(C
n
| R ,?
r
n?1
, C
h
, P )
RR-PCFG Projection P({gr
1
, . . . , gr
m
}|P )
Configuration P(?gr
1
, . . . , gr
m
?|{gr
1
, . . . , gr
m
}P )
Realization P(C
j
|gr
j
, P )
Adjunction P(C
j
1
, . . . , C
j
n
|gr
j
: gr
j+1
, P )
Table 3: PCFG Parameter Classes for All Models
structure trees. In our version of the MHTB, def-
initeness and accusativity features are percolated
from the PoS-tags level to phrase-level categories,
extending the procedure of (Guthmann et al,
2009). For all models, we applied non-terminal
state-splits distinguishing finite from non-finite
verb forms and possessive from non-possessive
noun phrases. We head-annotated the treebank,
and based on the ?subject?, ?object?, ?complement?
and ?conjunction? labels in the MHTB we devised
an automatic procedure to annotate all the gram-
matical functions indicated in table 2.7
Procedure For all models, we learn a PCFG by
reading off the parameters described in table 3,
in accordance with the trees depicted in figures
1?3.8 For all models, we use relative frequency
estimates. For lexical parameters, we use a sim-
ple smoothing procedure assigning probability to
unknown words using the per-tag distribution of
rare words (?rare? threshold set to < 2). The in-
put to our parser consists of morphologically seg-
mented surface forms, and the parser has to as-
7The enhanced corpus will be available at www.
science.uva.nl/
?
rtsarfat/resources.htm.
8Our training procedure is strictly equivalent to the
transform-detransform methodology of (Johnson, 1998), but
we implement a tree-traverse procedure as in (Bikel, 2002)
collecting all parameters per event at once.
sign the syntactic as well as morphological anal-
ysis to the surface segments.9 We use the stan-
dard development/training/test split as in (Tsarfaty
and Sima?an, 2008). Since our goal is a detailed
comparison and fine-grained analysis of the results
we concentrate on the development set. We use
a general-purpose CKY parser (Schmid, 2004) to
exhaustively parse the sentences, and we strip off
all model-specific information prior to evaluation.
Evaluation We use standard Parseval measures
calculated for the original, flat, canonical repre-
sentation of the parse trees.10 We report Pre-
cision/Recall for the coarse-grained non-terminal
categories. In addition to overall Parseval scores
we report the accuracy results Per Syntactic Cate-
gory. We further report model size in terms of the
number of parameters. As is well known in Ma-
chine Learning, models with more parameters re-
quire more data to learn, and are more vulnerable
to sparseness. In our evaluation we thus follow the
rule of thumb that (all else being equal) for mod-
els of equal size the better performing model is
preferred, and for models with equal performance,
the smaller one is preferred.
5 Results and Analysis
5.1 Overall Results
Table 4 shows the parsing results for the State-
Split (SP) PCFG, the Head-Driven (HD) PCFG
and the Relational-Realizational (RR) PCFG
models on parsing the Modern Hebrew Treebank,
with definiteness and accusativity marked on PoS-
tags as well as phrase-level categories. For all
models, we experiment with grandparent encod-
ing. For non-HD models, we also examine the
utility of a head-category split.11
9This setup is more difficult than, e.g., the Arabic parsing
setup of (Bikel, 2002), as they assume gold-standard pos-tags
as input. Yet it is easier than the setup of (Tsarfaty, 2006;
Goldberg and Tsarfaty, 2008) which uses unsegmented sur-
face forms as input. The decision to use segmented and un-
tagged forms was made to retain a realistic scenario. Mor-
phological analysis is known to be ambiguous, and we do
not assume that morphological features are known up front.
Morphological segmentation is also ambiguous, but for our
purposes it is unavoidable. When comparing different mod-
els on an individual sentence they may propose segmenta-
tion to sequences of different lengths, for which accuracy re-
sults cannot be faithfully compared. See (Tsarfaty, 2006) for
discussion.
10The flat canonical representation also allows for a fair
comparison that is not biased by the differing branching fac-
tors of the different models.
11In HD models, a head-tag is already assumed in the con-
ditioning context for sister nodes (Klein and Manning, 2003).
846
SP-PCFG
Grand-Parent ? ? + +
Head-Tag ? + ? +
Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17
(#Params) (4995) (8366) (7385) (11633)
HD-PCFG
Grand-Parent ? ? + +
Markov 0 1 0 1
Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84
(#Params) (6678) (10015) (19066) (21399)
RR-PCFG
Grand-Parent ? ? + +
Head Tag ? + ? +
Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51
(#Params) (3791) (7546) (7611) (13618)
Table 4: The Performance of Different Models
in Parsing Hebrew: Parsing Results Prec/Recall
for Sentences of Length ? 40.
For all models, grandparent encoding is help-
ful. For HD models, a higher Markovian order im-
proves performance. This shows that even in He-
brew there are linear-precedence tendencies that
help steer the disambiguation in the right direc-
tion, which is in line with our observation that
word-order patterns in Modern Hebrew are not
completely free (cf. table 1).
The best SP model performs equally or better
than all HD models. This might be due to the
smaller size of SP grammars, resulting in more ro-
bust estimates. But it is remarkable that, given the
feature-rich representation, such a simple treebank
grammar provides better disambiguation capacity
than linguistically articulated HD models. We at-
tribute this to the fact that parent-daughter rela-
tions have a stronger association with grammati-
cal functions than relations between neighbouring
nodes. For Hebrew, such adjacency relations may
be arbitrary due to word-order variability.
Overall, RR models show the best performance
for the set of all models with parent encoding, and
for the set of all models without. Our best RR
model shows 6.6%/8.4% Prec/Rec error reduction
from the best SP model. The Recall improvement
shows that the RR model is much better in gener-
alizing, recovering successfully more of the con-
stituents found in the gold representation. The
best RR model also outperforms HD models with
8.7%/6.7% Prec/Rec error reduction from the best
In our SP or RR models, head-information is used as yet an-
other feature-value pair rather than an object with a distin-
guished status during generation.
Model / SP-PCFG HD-PCFG RR-PCFG
Category
NP 77.39 / 74.32 77.94 / 73.75 78.96 / 76.11
PP 71.78 / 71.14 71.83 / 69.24 74.4 / 72.02
SBAR 55.73 / 59.71 53.79 / 57.49 57.97 / 61.67
ADVP 71.37 / 77.01 72.52 / 73.56 73.57 / 77.59
ADJP 79.37 / 78.96 78.47 / 77.14 78.69 / 78.18
S 73.25 / 79.07 71.07 / 76.49 72.37 / 78.33
SQ 36.00 / 32.14 30.77 / 14.29 55.56 / 17.86
PREDP 36.31 / 39.63 44.74 / 39.63 44.51 / 46.95
VP 76.34 / 80.81 77.33 / 82.51 78.59 / 81.18
Table 5: Per-Category Evaluation of Parsing
Performance for Different Models: Prec/Rec
Per Category Calculated for All Sentences.
HD model. The resulting precision improvement
of the RR relative to HD is larger than the im-
provement relative to SP, and the Recall improve-
ment pattern is reversed. So it seems that the HD
model generalizes better than the SP model, but
also gets generalizations wrong more often than
the SP model.
The RR model combines the generalization
advantage of breaking down context-free events
while it maintains the coherence advantage of
learning flat trees (cf. (Johnson, 1998)). The best
RR model obtains the best performance among
all models: F
1
76.41. To put this result in con-
text, for the setting in which the Arabic parser of
(Maamouri et al, 2008) obtains F
1
78.1, ? i.e.,
with gold standard feature-rich tags ? the best
RR model obtains F
1
83.3 accuracy which is the
best parsing result reported for a Semitic language
so far. RR models also have the advantage of re-
sulting in more compact grammars, which makes
learning and parsing with them much more com-
putationally efficient.
5.2 Per-Category Break-Down Analysis
To understand better the merits of the different
models we conducted a break-down analysis of
performance-per-category for the best performing
models of each kind. The break-down results are
shown in table 5. We divided the table into three
sets of categories: those for which the RR model
gave the best performance, those for which the SP
model gave the best performance, and those for
which there is no clear trend.
The most striking outcome is that the RR model
identifies at higher accuracy precisely those syn-
tactic elements that are freely positioned with re-
847
spect to the head: NPs, PPs, ADVPs and SBARs.
Adjectives, in contrast, have clear ordering con-
straints ? they always appear after the noun. S
level elements, when embedded, always appear
immediately after a conjunction or a relativizer.
In particular, NPs and PPs realize arguments and
adjuncts that may occupy different positions rela-
tive to the head. The RR model is better than the
other models in identifying those elements partly
because morphological information helps to dis-
ambiguate syntactically relevant chunks and make
correct attachment decisions about them.
Remarkably, predicative (verb-less) phrases
(PREDP), which are characteristic of Semitic lan-
guages, are hard to parse, but here too the RR does
slightly better than the other two, as it allows for
variability in the means to realize a (verbal or verb-
less) predicate. Both RR and HD models outper-
form SP for VPs, which is due to the specific na-
ture of VPs in the MHTB ? they exist only for
complement phrases with strict linear ordering.
6 Distances, Functions and
Subcategorization Frames
Markovian processes to the left and to the right of
the head provide a first approximation of the pred-
icate?s argument structure, as they capture trends
in the co-occurrences of constituents reflected in
their pattern of positioning and adjacency. But
as our results so far show, such an approxima-
tion is empirically less rewarding for a language
in which grammatical relations are not tightly cor-
related with structural notions.12
Collins (2003) attempted a more abstract for-
mulation of argument-structure by articulating left
and right subcat-sets. Each set represents those
arguments that are expected to occur at each side
of the head. Argument sisters (?complements?)
are generated if and only if they are required, and
their generation ?cancels? the requirement in the
set. Adjuncts (?modifiers?) may be freely gener-
ated at any position.
At first glance, such a dissociation of configura-
tional positions and subcategorization sets seems
to be more adequate for parsing Hebrew, because
it allows for some variability in the order of gen-
eration. But here too, since the model uses sets of
12Conditioning based on adjacency and distance is also
common inside dependency parsing models, and we conjec-
ture that this is one of the reasons for their difficulty in coping
with freer word-order languages, a difficulty pointed out in
(Nivre et al, 2007).
(3a) S
V P@S
L,{SBJ}, V P@S
NP
Dani
Dani
H,V P@S
VP
natan
gave
R,{OBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{OBJ,COM}, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,{COM}, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,{OBJ}, V P@S
NP
+D+ACC
et-ha-matana
the-present
H,V P@S
VP
natan
gave
R,{SBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{SBJ,COM}, V P@S
NP
Dani
Dani
R,{COM}, V P@S
PP
le-dina
to-Dina
Figure 4: The Relational Head-Driven Approach
constituent labels, it disambiguates the grammati-
cal functions of an NP solely based on the direc-
tion of the head, which is adequate for English but
not for Hebrew. In order to relax this association
further, we propose to replace constituent labels
in the subcat-sets with grammatical relations iden-
tical to the functional elements in the relational
network of the RR. This provides means to medi-
ate the cancellation of constituents in the sets with
their functions and correlate it with morphology.
To get an idea of the implications of such a
modeling strategy, let us consider our example
sentences in such a Relational-HD model as de-
picted in figure 4. Both representations share
the event of generating the verbal head. Sisters
are generated conditioned on the head and the
functional elements remaining to be ?cancelled?.
Each of the two trees consists of an event real-
izing an ?object?, one for an NP to the right of
the head, and the other for an NP to its left. In
both cases, an object constituent will be generated
jointly with the morphological features associated
with it. Evidently, when using sets of grammatical
relations instead of constituent-labels, correlation
of morphology and grammatical functions is more
straight-forward to maintain.
848
Model SP-PCFG HD-PCFG HD-PCFG HD-PCFG HD-PCFG RR-PCFG
Type of Distance ? Phrase-Level Intervening Left and Right Left and Right Left and Right Subcat Sets
or Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels Configuration
Precision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51
(#Params) (13884) (11650) (18058) (16334) (16460) (13618)
Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing Models
Reporting Precison/Recall (#Parameters) for Sentences Length < 40.
6.1 Results and Analysis
Table 6 reports the results of experimenting with
HD models with different instantiations of a dis-
tance function, starting from the standard notion
of (Collins, 2003) and ending with our proposed,
relational, function sets. For all HD models, we
retain the head, left and right generation cycle and
only change the conditioning context (?
i
) for sis-
ter generation.
As a baseline, we show the results of adding
grammatical function information as state-splits
on top of an SP-PCFG.13 This SP model presents
much lower performance than the RR model al-
though they are almost of the same size and they
start off with the same information. This result
shows that sophisticated modeling can blunt the
claws of the sparseness problem. One may ob-
tain the same number of parameters for two dif-
ferent models, but correlate them with more pro-
found linguistic notions in one model than in the
other. In our case, there is more statistical evi-
dence in the data for, e.g., case marking patterns,
than for association of grammatical relations with
structurally-marked positions.
For all HD variations, the RR model contin-
ues to outperform HD models. The function-set
variation performs slightly (but not significantly)
better than the category-set. What seems to be
still standing in the way of getting useful dis-
ambiguation cues for HD models is the fact that
the left and right direction of realization is hard-
wired in their representation. This breaks down a
coherent distribution over morphosyntactic repre-
sentations realizing grammatical relations to arbi-
trary position-dependent fragments, which results
in larger grammars and inferior performance.14
13The startegy of adding grammatical functions as state-
splits is used in, e.g., German (Rafferty and Manning, 2008).
14Due to the difference in the size of the grammars, one
could argue that smoothing will bridge the gap between
the HD and RR modeling strategies. However, the better
size/accuracy trade-off shown here for RR models suggests
that they provide a good bias/variance balancing point, es-
pecially for feature-rich models characterizing morphologi-
7 A Typological Detour
Hebrew, Arabic and other Semitic Languages are
known to be substantially different from English
in that English is strongly configurational. In
configurational languages word-order is fixed, and
information about the grammatical functions of
constituents (e.g., subject or object) is often cor-
related with structurally-marked positions inside
highly-nested constituency structures. Nonconfig-
urational languages (Hale, 1983), in contrast, al-
low for freedom in their word-ordering and infor-
mation about grammatical relations between con-
stituents is often marked by means of morphology.
Configurationality is hardly a clear-cut notion.
The difference in the configurationality level of
different languages is often conceived as depicted
in figure 7. In linguistic typology, the branch
of linguistics that studies the differences between
languages (Song, 2001), the division of labor be-
tween linear ordering and morphological marking
in the realization of grammatical relations is of-
ten viewed as a continuum. Common wisdom has
it that the lower a language is on the configura-
tionality scale, the more morphological marking
we expect to be used (Bresnan, 2001, page 6).
For a statistical parser to cope with nonconfig-
urational phenomena as observed in, for instance,
Hebrew or German, it should allow for flexibil-
ity in the form of realization of the grammati-
cal functions within the phrase-structure represen-
tation of trees. Recent morphological theories
employ Form-Function separation as a widely-
accepted practice for enhancing the adequacy of
models describing variability in the realization of
grammatical properties. Our results suggest that
the adequacy of syntactic processing models is re-
lated to such typological insights as well, and is
enhanced by adopting a similar form-function sep-
aration for expressing grammatical relations.
cally rich languages. A promising strategy then would be to
smooth or split-and-merge (Petrov et al, 2006)) RR-based
models rather than to add an elaborate smoothing component
to configurationally-based HD models.
849
configurational ?????? nonconfigurational
Chinese>English>{German,Hebrew}>Warlpiri
Figure 5: The Configurationality Scale
The HD assumptions take the function of a con-
stituent to be transparently related to its formal
position, which entails word-order rigidity. Such
transparent relations between configurational po-
sitions and grammatical functions are assumed by
other kinds of parsing frameworks such as the ?all-
subtrees? approach of Data-Oriented Parsing, and
the distinction between left and right application
in CCG-based parsers.
The RR modeling strategy stipulates a strict
separation between form ? parametrizing explic-
itly basic word-order (Greenberg, 1963) and mor-
phological realization (Greenberg, 1954) ? and
function ? parametrizing relational networks bor-
rowed from (Perlmutter, 1982) ? which makes
it possible to statistically learn complex form-
function mapping reflected in the data. This is
an adequate means to capture, e.g., morphosyn-
tactic interactions, which characterize the less-
configurational languages on the scale.
8 Conclusion
In our comparison of the HD and RR modeling
approaches, the RR approach is shown to be em-
pirically superior and typologically more adequate
for parsing a language exhibiting word-order vari-
ation interleaved with extended morphology. HD
models are less accurate and more vulnerable to
sparseness as they assume transparent mappings
between form and function, based on left and right
decompositions hard-wired in the HD representa-
tion. RR models, in contrast, employ form and
function separation which allows the statistical
model to learn complex correspondance patterns
reflected in the data. In the future we plan to in-
vestigate how the different models fare against one
another in parsing different languages. In particu-
lar we wish to examine whether parsing different
languages should be pursued by different models,
or whether the RR strategy can effectively cope
with different languages types. Finally, we wish
to explore the implications of RR modeling for
applications that consider the form of expression
in multiple languages, for instance Statistical Ma-
chine Translation (SMT).
9 Acknowledgements
We thank Jelle Zuidema, Inbal Tsarfati, David
McCloskey and Yoav Golderg for excellent com-
ments on earlier versions. We also thank Miles
Osborne and Tikitu de Jager for comments on the
camera-ready draft. All errors are our own. The
work of the first author is funded by the Dutch Sci-
ence Foundation (NWO) grant 017.001.271.
References
J. Aissen. 2003. Differential Object Marking: Iconic-
ity vs. Economy. Natural Language and Linguistic
Theory, 21.
D. M. Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In Proceed-
ings of HLT.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Textbooks in Linguistics. Blackwell.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
M. Collins, J. Hajic?, E. Brill, L. Ramshaw, and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics.
A. Dubey. 2004. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Y. Goldberg and R. Tsarfaty. 2008. A Single Frame-
work for Joint Morphological Segmentation and
Syntactic Parsing. In Proceedings of ACL.
J.H. Greenberg. 1954. A Quantitative Approach to
the Morphological Typology of Language. In R. F.
Spencer, editor, Method and Perspective in Anthro-
pology. University of Minessota Press.
J. H. Greenberg. 1963. Some Universals of Grammar
with Particular Reference to the Order of Meaning-
ful Elements. In Joseph H. Greenberg, editor, Uni-
versals of Language. MIT Press.
N. Guthmann, Y. Krymolowski, A. Milea, and Y. Win-
ter. 2009. Automatic Annotation of Morpho-
Syntactic Dependencies in a Modern Hebrew Tree-
bank. In Proceedings of TLT.
850
K. L. Hale. 1983. Warlpiri and the Grammar of Non-
Configurational Languages. Natural Language and
Linguistic Theory, 1(1).
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL.
S. Kubler. 2008. The PaGe Shared task on Parsing
German. In ACL Workshop on Parsing German.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a Large-
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanced
Annotation and Parsing of the Arabic treebank. In
Proceedings of INFOS.
D. M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics.
P. H. Matthews. 1993. Morphology. Cambridge.
D. McClosky, E. Charniak, and M. Johnson. 2008.
When is self-training effective for parsing? In Pro-
ceedings of CoLing.
N. Melnik. 2002. Verb-Initial Constructions in Mod-
ern Hebrew. Ph.D. thesis, Berkeley, California.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task.
D. M. Perlmutter. 1982. Syntactic Representation,
Syntactic Levels, and the Notion of a Subject. In
Pauline Jacobson and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL.
A. Rafferty and C. D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In ACL WorkShop on Parsing German.
R. Scha. 1990. Language Theory and Language Tech-
nology; Competence and Performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Computer-
toepassingen in de Neerlandistiek. Almere: LVVN.
H. Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit vectors.
In Proceedings of COLING.
U. Shlonsky. 1997. Clause Structure and Word Order
in Hebrew and Arabic. Oxford University Press.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern He-
brew Text. In Traitement Automatique des Langues.
J. J. Song. 2001. Linguistic Typology: Morphology
and Syntax. Pearson Education Limited, Edinbrugh.
R. Tsarfaty and K. Sima?an. 2008. Relational-
Realizational Parsing. In Proceedings of CoLing.
R. Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
851
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 11?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Learning from errors: Using vector-based compositional semantics for
parse reranking
Phong Le, Willem Zuidema, Remko Scha
Institute for Logic, Language, and Computation
University of Amsterdam, the Netherlands
{p.le,zuidema,scha}@uva.nl
Abstract
In this paper, we address the problem of
how to use semantics to improve syntac-
tic parsing, by using a hybrid reranking
method: a k-best list generated by a sym-
bolic parser is reranked based on parse-
correctness scores given by a composi-
tional, connectionist classifier. This classi-
fier uses a recursive neural network to con-
struct vector representations for phrases in
a candidate parse tree in order to classify
it as syntactically correct or not. Tested on
the WSJ23, our method achieved a statisti-
cally significant improvement of 0.20% on
F-score (2% error reduction) and 0.95% on
exact match, compared with the state-of-
the-art Berkeley parser. This result shows
that vector-based compositional semantics
can be usefully applied in syntactic pars-
ing, and demonstrates the benefits of com-
bining the symbolic and connectionist ap-
proaches.
1 Introduction
Following the idea of compositionality in formal
semantics, compositionality in vector-based se-
mantics is also based on the principle of composi-
tionality, which says that ?The meaning of a whole
is a function of the meanings of the parts and of
the way they are syntactically combined? (Partee,
1995). According to this principle, composing the
meaning of a phrase or sentence requires a syntac-
tic parse tree, which is, in most current systems,
given by a statistical parser. This parser, in turn, is
trained on syntactically annotated corpora.
However, there are good reasons to also con-
sider information flowing in the opposite direc-
tion: from semantics to syntactic parsing. Per-
formance of parsers trained and evaluated on the
Penn WSJ treebank has reached a plateau, as many
ambiguities cannot be resolved by syntactic infor-
mation alone. Further improvements in parsing
may depend on the use of additional sources of in-
formation, including semantics. In this paper, we
study the use of semantics for syntactic parsing.
The currently dominant approach to syntactic
parsing is based on extracting symbolic grammars
from a treebank and defining appropriate proba-
bility distributions over the parse trees that they
license (Charniak, 2000; Collins, 2003; Klein
and Manning, 2003; Petrov et al, 2006; Bod et
al., 2003; Sangati and Zuidema, 2011; van Cra-
nenburgh et al, 2011). An alternative approach,
with promising recent developments (Socher et
al., 2010; Collobert, 2011), is based on us-
ing neural networks. In the present paper, we
combine the ?symbolic? and ?connectionist? ap-
proaches through reranking: a symbolic parser
is used to generate a k-best list which is then
reranked based on parse-correctness scores given
by a connectionist compositional-semantics-based
classifier.
The idea of reranking is motivated by anal-
yses of the results of state-of-the-art symbolic
parsers such as the Brown and Berkeley parsers,
which have shown that there is still considerable
room for improvement: oracle results on 50-best
lists display a dramatic improvement in accuracy
(96.08% vs. 90.12% on F-score and 65.56% vs.
37.22% on exact match with the Berkeley parser).
This suggests that parsers that rely on syntactic
corpus-statistics, though not sufficient by them-
selves, may very well serve as a basis for sys-
tems that integrate other sources of information by
means of reranking.
One important complementary source of infor-
mation is the semantic plausibility of the con-
stituents of the syntactically viable parses. The ex-
ploitation of that kind of information is the topic
of the research we report here. In this work,
we follow up on a proposal by Mark Steedman
11
(1999), who suggested that the realm of seman-
tics lacks the clearcut hierarchical structures that
characterise syntax, and that semantic information
may therefore be profitably treated by the clas-
sificatory mechanisms of neural nets?while the
treatment of syntactic structures is best left to sym-
bolic parsers. We thus developed a hybrid system,
which parses its input sentences on the basis of a
symbolic probabilistic grammar, and reranks the
candidate parses based on scores given by a neural
network.
Our work is inspired by the work of Socher and
colleagues (2010; 2011). They proposed a parser
using a recursive neural network (RNN) for en-
coding parse trees, representing phrases in a vec-
tor space, and scoring them. Their experimental
result (only 1.92% lower than the Stanford parser
on unlabelled bracket F-score for sentences up to a
length of 15 words) shows that an RNN is expres-
sive enough for syntactic parsing. Additionally,
their qualitative analysis indicates that the learnt
phrase features capture some aspects of phrasal se-
mantics, which could be useful to resolve semantic
ambiguity that syntactical information alone can
not. Our work in this paper differs from their work
in that we replace the parsing task by a reranking
task, and thus reduce the object space significantly
to a set of parses generated by a symbolic parser
rather than the space of all parse trees. As a result,
we can apply our method to sentences which are
much longer than 15 words.
Reranking a k-best list is not a new idea.
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) have built reranking sys-
tems with performances that are state-of-the-art.
In order to achieve such high F-scores, those
rerankers rely on a very large number of features
selected on the basis of expert knowledge. Unlike
them, our feature set is selected automatically, yet
the reranker achieved a statistically significant im-
provement on both F-score and exact match.
Closest to our work is Menchetti et al (2005)
and Socher et al (2013): both also rely on sym-
bolic parsers to reduce the search space and use
RNNs to score candidate parses. However, our
work differs in the way the feature set for rerank-
ing is selected. In their methods, only the score at
the tree root is considered whereas in our method
the scores at all internal nodes are taken into ac-
count. Selecting the feature set like that gives us a
flexible way to deal with errors accumulated from
the leaves to the root.
Figure 1 shows a diagram of our method. First,
a parser (in this paper: the Berkeley parser) is used
to generate k-best lists of the Wall Street Jour-
nal (WSJ) sections 02-21. Then, all parse trees in
these lists and the WSJ02-21 are preprocessed by
marking head words, binarising, and performing
error-annotation (Section 2). After that, we use
the annotated trees to train our parse-correctness
classifier (Section 3). Finally, those trees and the
classifier are used to train the reranker (Section 4).
2 Experimental Setup
The experiments presented in this paper have the
following setting. We use the WSJ corpus with
the standard splits: sections 2-21 for training, sec-
tion 22 for development, and section 23 for test-
ing. The latest implementation (version 1.7) of the
Berkeley parser1 (Petrov et al, 2006) is used for
generating 50-best lists. We mark head words and
binarise all trees in the WSJ and the 50-best lists
as in Subsection 2.1, and annotate them as in Sub-
section 2.2 (see Figure 2).
2.1 Preprocessing Trees
We preprocess trees by marking head words and
binarising the trees. For head word marking,
we used the head finding rules of Collins (1999)
which are implemented in the Stanford parser.
To binarise a k-ary branching, e.g. P ?
C1 ... H ... Ck where H is the top label of the
head constituent, we use the following method. If
H is not the left-most child, then
P ? C1 @P ; @P ? C2 ... H ... Ck
otherwise,
P ? @P Ck ; @P ? H ... Ck?1
where @P , which is called extra-P , now is the
head of P . We then apply this transformation
again on the children until we reach terminal
nodes. In this way, we ensure that every internal
node has one head word.
2.2 Error Annotation
We annotate nodes (as correct or incorrect) as fol-
lows. Given a parse tree T in a 50-best list and
a corresponding gold-standard tree G in the WSJ,
1https://code.google.com/p/berkeleyparser
12
Figure 1: An overview of our method.
Figure 2: Example for preprocessing trees. Nodes marked with (*) are labelled incorrect whereas the
other nodes are labelled correct.
we first attempt to align their terminal nodes ac-
cording to the following criterion: a terminal node
t is aligned to a terminal node g if they are at
the same position counting from-left-to-right and
they have the same label. Then, a non-terminal
node P [wh] with children C1, ..., Ck is aligned to
a gold-standard non-terminal node P ?[w?h] with
children C?1 , ..., C
?
l (1 ? k, l ? 2 in our case)
if they have the same word head, the same syn-
tactical category, and their children are all aligned
in the right order. In other words, the following
conditions have to be satisfied
P = P ? ; wh = w?h ; k = l
Ci is aligned to C?i , for all i = 1..k
Aligned nodes are annotated as correct whereas
the other nodes are annotated as incorrect.
3 Parse-Correctness Classification
This section describes how a neural network
is used to construct vector representations for
phrases given parse trees and to identify if those
trees are syntactically correct or not. In order to
encode tree structures, we use an RNN2 (see Fig-
ure 3 and Figure 4) which is similar to the one
proposed by Socher and colleagues (2010). How-
ever, unlike their RNN, our RNN can handle unary
branchings, and also takes head words and syntac-
tic tags as input. It is worth noting that, although
we can use some transformation to remove unary
branchings, handling them is helpful in our case
because the system avoids dealing with so many
syntactic tags that would result from the transfor-
2The first neural-network approach attempting to operate
and represent compositional, recursive structure is the Recur-
sive Auto-Associative Memory network (RAAM), which was
proposed by Pollack (1988). In order to encode a binary tree,
the RAAM network contains three layers: an input layer for
two daughter nodes, a hidden layer for their parent node, and
an output layer for their reconstruction. Training the network
is to minimise the reconstruction error such that we can de-
code the information captured in the hidden layer to the orig-
inal tree form. Our RNN differs from the RAAM network in
that its output layer is not for reconstruction but for classifi-
cation.
13
mation. In addition, using a new set of weight ma-
trices for unary branchings makes our RNN more
expressive without facing the problem of sparsity
thanks to a large number of unary branchings in
the treebank.
Figure 3: An RNN attached to the parse tree
shown in the top-right of Figure 2. All unary
branchings share a set of weight matrices, and all
binary branchings share another set of weight ma-
trices (see Figure 4).
An RNN processes a tree structure by repeat-
edly applying itself at each internal node. Thus,
walking bottom-up from the leaves of the tree to
the root, we compute for every node a vector based
on the vectors of its children. Because of this
process, those vectors have to have the same di-
mension. It is worth noting that, because informa-
tion at leaves, i.e. lexical semantics, is composed
according to a given syntactic parse, what a vec-
tor at each internal node captures is some aspects
of compositional semantics of the corresponding
phrase. In the remainder of this subsection, we
describe in more detail how to construct composi-
tional vector-based semantics geared towards the
parse-correctness classification task.
Similar to Socher et al (2010), and Col-
lobert (2011), given a string of words (w1, ..., wl),
we first compute a string of vectors (x1, ..., xl)
representing those words by using a look-up table
(i.e., word embeddings) L ? Rn?|V |, where |V | is
the size of the vocabulary and n is the dimension-
ality of the vectors. This look-up table L could
be seen as a storage of lexical semantics where
each column is a vector representation of a word.
Hence, let bi be the binary representation of word
wi (i.e., all of the entries of bi are zero except the
one corresponding to the index of the word in the
dictionary), then
xi = Lbi ? Rn (1)
We also encode syntactic tags by binary vectors
but put an extra bit at the end of each vector to
mark if the corresponding tag is extra or not (i.e.,
@P or P ).
Figure 4: Details about our RNN for a unary
branching (top) and a binary branching (bottom).
The bias is not shown for the simplicity.
Then, given a unary branching P [wh]? C, we
can compute the vector at the node P by (see Fig-
ure 4-top)
p = f
(
Wuc+Whxh +W?1x?1 +
W+1x+1 +Wttp + bu
)
where c, xh are vectors representing the child C
and the head word, x?1, x+1 are the left and right
neighbouring words of P , tp encodes the syn-
tactic tag of P , Wu,Wh,W?1,W+1 ? Rn?n,
Wt ? Rn?(|T |+1), |T | is the size of the set of
syntactic tags, bu ? Rn, and f can be any acti-
vation function (tanh is used in this case). With
a binary branching P [wh] ? C1 C2, we simply
change the way the children?s vectors added (see
Figure 4-bottom)
p = f
(
Wb1c1 +Wb2c2 +Whxh +W?1x?1 +
W+1x+1 +Wttp + bb
)
Finally, we put a sigmoid neural unit on the
top of each internal node (except pre-terminal
nodes because we are not concerned with POS-
tagging) to detect the correctness of the subparse
tree rooted at that node
y = sigmoid(Wcatp+ bcat) (2)
where Wcat ? R1?n, bcat ? R.
14
3.1 Learning
The error on a parse tree is computed as the sum
of classification errors of all subparses. Hence, the
learning is to minimise the objective
J(?) =
1
N
?
T
?
(y(?),t)?T
1
2
(t? y(?))2 + ????2
(3)
where ? are the model parameters, N is the num-
ber of trees, ? is a regularisation hyperparameter,
T is a parse tree, y(?) is given by Equation 2, and
t is the class of the corresponding subparse (t = 1
means correct). The gradient ?J?? is computed ef-
ficiently thanks to backpropagation through the
structure (Goller and Kuchler, 1996). L-BFGS
(Liu and Nocedal, 1989) is used to minimise the
objective function.
3.2 Experiments
We implemented our classifier in Torch73 (Col-
lobert et al, 2011a), which is a powerful Matlab-
like environment for machine learning. In order to
save time, we only trained the classifier on 10-best
parses of WSJ02-21. The training phase took six
days on a computer with 16 800MHz CPU-cores
and 256GB RAM. The word embeddings given by
Collobert et al (2011b)4 were used as L in Equa-
tion 1. Note that these embeddings, which are the
result of training a language model neural network
on the English Wikipedia and Reuters, have been
shown to capture many interesting semantic simi-
larities between words.
We tested the classifier on the development
set WSJ22, which contains 1, 700 sentences, and
measured the performance in positive rate and
negative rate
pos-rate =
#true pos
#true pos +#false neg
neg-rate =
#true neg
#true neg +#false pos
The positive/negative rate tells us the rate at which
positive/negative examples are correctly labelled
positive/negative. In order to achieve high per-
formance in the reranking task, the classifier must
have a high positive rate as well as a high nega-
tive rate. In addition, percentage of positive exam-
ples is also interesting because it shows the unbal-
ancedness of the data. Because the accuracy is not
3http://www.torch.ch/
4http://ronan.collobert.com/senna/
a reliable measurement when the dataset is highly
unbalanced, we do not show it here. Table 1, Fig-
ure 5, and Figure 6 show the classification results.
pos-rate (%) neg-rate (%) %-Pos
gold-std 75.31 - 1
1-best 90.58 64.05 71.61
10-best 93.68 71.24 61.32
50-best 95.00 73.76 56.43
Table 1: Classification results on the WSJ22 and
the k-best lists.
Figure 5: Positive rate, negative rate, and percent-
age of positive examples w.r.t. subtree depth.
3.3 Discussion
Table 1 shows the classification results on the
gold-standard, 1-best, 10-best, and 50-best lists.
The positive rate on the gold-standard parses,
75.31%, gives us the upper bound of %-pos when
this classifier is used to yield 1-best lists. On the 1-
best data, the classifier missed less than one tenth
positive subtrees and correctly found nearly two
third of the negative ones. That is, our classi-
fier might be useful for avoiding many of the mis-
takes made by the Berkeley parser, whilst not in-
troducing too many new mistakes of its own. This
fact gave us hope to improve parsing performance
when using this classifier for reranking.
Figure 5 shows positive rate, negative rate, and
percentage of positive examples w.r.t. subtree
depth on the 50-best data. We can see that the pos-
itive rate is inversely proportional to the subtree
depth, unlike the negative rate. That is because the
15
Figure 6: Positive rate, negative rate, and percentage of positive samples w.r.t. syntactic categories
(excluding POS tags).
deeper a subtree is, the lower the a priori likeli-
hood that the subtree is positive (we can see this
in the percentage-of-positive-example curve). In
addition, deep subtrees are difficult to classify be-
cause uncertainty is accumulated when propagat-
ing from bottom to top.
4 Reranking
In this section, we describe how we use the above
classifier for the reranking task. First, we need to
represent trees in one vector space, i.e., ?(T ) =
(
?1(T ), ..., ?v(T )
)
for an arbitrary parse tree T .
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) set the first entry to the
model score and the other entries to the number of
occurrences of specific discrete hand-chosen prop-
erties (e.g., how many times the word pizza comes
after the word eat) of trees. We here do the same
with a trick to discretize results from the classifier:
we use a 2D histogram to store predicted scores
w.r.t. subtree depth. This gives us a flexible way to
penalise low score subtrees and reward high score
subtrees w.r.t. the performance of the classifier at
different depths (see Subsection 3.3). However,
unlike the approaches just mentioned, we do not
use any expert knowledge for feature selection; in-
stead, this process is fully automatic.
Formally speaking, a vector feature ?(T ) is
computed as following. ?1(T ) is the model score
(i.e., max-rule-sum score) given by the parser,
(
?2(T ), ..., ?v(T )
)
is the histogram of a set of
(y, h) where y is given by Equation 2 and h is the
depth of the corresponding subtree. The domain
of y (i.e., [0, 1]) is split into ?y equal bins whereas
the domain of h (i.e., {1, 2, 3, ...}) is split into ?h
bins such that the i-th (i < ?h) bin corresponds to
subtrees of depth i and the ?h-th bin corresponds
to subtrees of depth equal or greater than ?h. The
parameters ?y and ?h are then estimated on the de-
velopment set.
After extracting feature vectors for parse trees,
we then find a linear ranking function
f(T ) = w>?(T )
such that
f(T1) > f(T2) iff fscore(T1) > fscore(T2)
where fscore(.) is the function giving F-score, and
w ? Rv is a weight vector, which is efficiently
estimated by SVM ranking (Yu and Kim, 2012).
SVM was initially used for binary classification.
Its goal is to find the hyperplane which has the
largest margin to best separate two example sets. It
was then proved to be efficient in solving the rank-
ing task in information retrieval, and in syntactic
parsing (Shen and Joshi, 2003; Titov and Hender-
son, 2006). In our experiments, we used SVM-
16
Rank5 (Joachims, 2006), which runs extremely
fast (less than two minutes with about 38, 000 10-
best lists).
4.1 Experiments
Using the classifier in Section 3, we implemented
the reranker in Torch7, trained it on WSJ02-21.
We used WSJ22 to estimate the parameters ?y and
?h by the grid search and found that ?y = 9 and
?h = 4 yielded the best F-score.
Table 2 shows the results of our reranker on
50-best WSJ23 given by the Berkeley parser, us-
ing the standard evalb. Our method improves
0.20% on F-score for sentences with all length,
and 0.22% for sentences with ? 40 words.
These differences are statistically significant6 with
p-value < 0.003. Our method also improves ex-
act match (0.95% for all sentences as well as for
sentences with ? 40 words).
Parser LR LP LF EX
all
Berkeley parser 89.98 90.25 90.12 37.22
This paper 90.10 90.54 90.32 38.17
Oracle 95.94 96.21 96.08 65.56
? 40 words
Berkeley parser 90.43 90.70 90.56 39.65
This paper 90.57 91.01 90.78 40.50
Oracle 96.47 96.73 96.60 68.51
Table 2: Reranking results on 50-best lists on
WSJ23 (LR is labelled recall, LP is labelled pre-
cision, LF is labelled F-score, and EX is exact
match.)
Table 3 shows the comparison of the three
parsers that use the same hybrid reranking ap-
proach. On F-score, our method performed 0.1%
lower than Socher et al (2013), and 1.5% better
than Menchetti et al (2005). However, our method
achieved the least improvement on F-score over its
corresponding baseline. That could be because our
baseline parser (i.e., the Berkeley parser) performs
much better than the other two baseline parsers;
and hence, detecting errors it makes on candidate
parse trees is more difficult.
5www.cs.cornell.edu/people/tj/svm light/svm rank.html
6We used the ?Significance testing for evalua-
tion statistics? software (http://www.nlpado.de/ sebas-
tian/software/sigf.shtml) given by Pado? (2006).
Parser LF (all) K-best
parser
Menchetti et
al. (2005)
88.8 (0.6) Collins
(1999)
Socher et
al. (2013)
90.4 (3.8) PCFG Stan-
ford parser
This paper 90.3 (0.2) Berkeley
parser
Table 3: Comparison of parsers using the same hy-
brid reranking approach. The numbers in the blan-
kets indicate the improvements on F-score over the
corresponding baselines (i.e., the k-best parsers).
5 Conclusions
This paper described a new reranking method
which uses semantics in syntactic parsing: a sym-
bolic parser is used to generate a k-best list which
is later reranked thanks to parse-correctness scores
given by a connectionist compositional-semantics-
based classifier. Our classifier uses a recursive
neural network, like Socher et al, (2010; 2011), to
not only represent phrases in a vector space given
parse trees, but also identify if these parse trees are
grammatically correct or not.
Tested on WSJ23, our method achieved a
statistically significant improvement on F-score
(0.20%) as well as on exact match (0.95%).
This result, although not comparable to the re-
sults reported by Collins (2000), Charniak and
Johnson (2005), and Johnson and Ural (2010),
shows an advantage of using vector-based com-
positional semantics to support available state-of-
the-art parsers.
One of the limitations of the current paper is the
lack of a qualitative analysis of how learnt vector-
based semantics has affected the reranking results.
Therefore, the need for ?compositional seman-
tics? in syntactical parsing may still be doubted.
In future work, we will use vector-based seman-
tics together with non-semantic features (e.g., the
ones of Charniak and Johnson (2005)) to find out
whether the semantic features are truly helpful or
they just resemble non-semantic features.
Acknowledgments
We thank two anonymous reviewers for helpful
comments.
17
References
Rens Bod, Remko Scha, and Khalil Sima?an. 2003.
Data-Oriented Parsing. CSLI Publications, Stan-
ford, CA.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139. Association for
Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the In-
ternational Workshop on Machine Learning (then
Conference), pages 175?182.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Ronan Collobert, Koray Kavukcuoglu, and Cle?ment
Farabet. 2011a. Torch7: A matlab-like environment
for machine learning. In BigLearn, NIPS Workshop.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352. IEEE.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 217?226. ACM.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 665?668. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natu-
ral language processing using kernel methods and
neural networks for structured data. Pattern Recogn.
Lett., 26(12):1896?1906, September.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Barbara Partee. 1995. Lexical semantics and compo-
sitionality. In L. R. Gleitman and M. Liberman, ed-
itors, Language. An Invitation to Cognitive Science,
volume 1, pages 311?360. MIT Press, Cambridge,
MA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Jordan B Pollack. 1988. Recursive auto-associative
memory. Neural Networks, 1:122.
Federico Sangati and Willem Zuidema. 2011. Ac-
curate parsing with compact tree-substitution gram-
mars: Double-DOP. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 84?95. Association for Computa-
tional Linguistics.
Libin Shen and Aravind K Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 9?16. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neu-
ral networks. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
volume 2.
18
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Proceedings of the ACL
conference (to appear).
Mark Steedman. 1999. Connectionist sentence
processing in perspective. Cognitive Science,
23(4):615?634.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 560?567. Association
for Computational Linguistics.
Andreas van Cranenburgh, Remko Scha, and Federico
Sangati. 2011. Discontinuous Data-Oriented Pars-
ing: A mildly context-sensitive all-fragments gram-
mar. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
pages 34?44. Association for Computational Lin-
guistics.
Hwanjo Yu and Sungchul Kim. 2012. SVM tutorial:
Classification, regression, and ranking. In Grzegorz
Rozenberg, Thomas Ba?ck, and Joost N. Kok, ed-
itors, Handbook of Natural Computing, volume 1,
pages 479?506. Springer.
19
