Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165?168,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Hierarchical Multi-Class Text Categorization
with Global Margin Maximization
Xipeng Qiu
School of Computer Science
Fudan University
xpqiu@fudan.edu.cn
Wenjun Gao
School of Computer Science
Fudan University
wjgao616@gmail.com
Xuanjing Huang
School of Computer Science
Fudan University
xjhuang@fudan.edu.cn
Abstract
Text categorization is a crucial and well-
proven method for organizing the collec-
tion of large scale documents. In this pa-
per, we propose a hierarchical multi-class
text categorization method with global
margin maximization. We not only max-
imize the margins among leaf categories,
but also maximize the margins among
their ancestors. Experiments show that the
performance of our algorithm is competi-
tive with the recently proposed hierarchi-
cal multi-class classification algorithms.
1 Introduction
In the past serval years, hierarchical text catego-
rization has become an active research topic in
database area (Koller and Sahami, 1997; Weigend
et al, 1999) and machine learning area (Rousu et
al., 2006; Cai and Hofmann, 2007).
Hierarchical categorization methods can be di-
vided in two types: local and global approaches
(Wang et al, 1999; Sun and Lim, 2001). A lo-
cal approach usually proceeds in a top-down fash-
ion, which firstly picks the most relevant cate-
gories of the top level and then recursively making
the choice among the low-level categories. The
global approach builds only one classifier to dis-
criminate all categories in a hierarchy. Due that the
global hierarchical categorization can avoid the
drawbacks about those high-level irrecoverable er-
ror, it is more popular in the machine learning do-
main.
The essential idea behind global approach is
that the close classes(nodes) have some common
underlying factors. Especially, the descendant
classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997). A key problem for global hi-
erarchical categorization is how to combine these
underlying factors.
In this paper, we propose an method for hierar-
chical multi-class text categorization with global
margin maximization. We emphasize that it is im-
portant to separate all the nodes of the correct path
in the class hierarchy from their sibling node, then
we incorporate such information into the formula-
tion of hierarchical support vector machine.
The rest of the paper is organized as follows.
Section 2 describes the basic model of multi-class
hierarchical categorization with maximizing mar-
gin. Then we propose our improved versions in
section 3. Section 4 gives the experimental analy-
sis. Section 5 concludes the paper.
2 Hierarchical Multi-Class Text
Categorization
Multiclass SVM can be generalized to the problem
of hierarchical categorization (Cai and Hofmann,
2007), which has more than two categories in most
of the case. Denote Y
i
as the multilabels of x
i
and
?
Y
i
the multilabels set not in Y
i
. The separation
margin of w, with respect to x
i
, can be approxi-
mated as:
?
i
(w) = min
y?Y
i
,?y?
?
Y
i
??(x
i
,y)? ?(x
i
,
?
y),w? (1)
The loss function can be accommodated to
multi-class SVM to scale the penalties for margin
violations proportional to the loss. This is moti-
vated by the fact that margin violations involving
an incorrect class with high loss should be penal-
ized more severely. So the cost-sensitive hierar-
chical multiclass formulation takes takes the fol-
lowing form:
min
w,?
1
2
||w||
2
+ C
n
?
i=1
?
i
(2)
s.t.?w,??
i
(y,
?
y)??1?
?
i
l(y,?y)
, (?i,y?Y
i
,
?
y?
?
Y
i
)
?
i
? 0(?i)
165
where ??
i
(y,
?
y) = ?(x
i
,y) ? ?(x
i
,
?
y),
l(y,
?
y) > 0 and ?(x,y) is the joint feature of in-
put x and output y, which can be represented as:
?(x,y) = ?(y)? ?(x) (3)
where ? is the tensor product. ?(y) is the feature
representation of y.
Thus, we can classify a document x to label y
?
:
y
?
= arg max
y
F (w,?(x,y)) (4)
where F (?) is a map function.
There are different kinds of loss functions
l(y,
?
y).
One is thezero-one loss, l
0/1
(y,u) = [y 6= u].
Another is specially designed for the hierarchy
is tree loss(Dekel et al, 2004). Tree loss is defined
as the length of the path between two multilabels
with positive microlabels,
l
tr
= |path(i : y
i
= 1, j : u
j
= 1)| (5)
(Rousu et al, 2006) proposed a simplified ver-
sion of l
H
, namely l
?
H
:
l
?
H
=
?
j
c
j
[y
j
6= u
j
&y
pa
(j) = u
pa(j)
], (6)
that penalizes a mistake in a child only if the label
of the parent was correct. There are some different
choices for setting c
j
. One naive idea is to use
a uniform weighting (c
j
= 1). Another possible
choice is to divide the loss among the sibling:
c
root
= 1, c
j
= c
Parent(j)
/(|Sib(j)|+ 1) (7)
Another possible choice is to scale the loss by the
proportion of the hierarchy that is in the subtree
T (j) rooted by j:
c
j
= |T (j)|/|T (root)| (8)
Using these scaling weights, the derived losses are
referred as l
?
uni
,l
?
sib
and l
?
sub
respectively.
3 Hierarchical Multi-Class Text
Categorization with Global Margin
Maximization
In previous literature (Cai and Hofmann, 2004;
Tsochantaridis et al, 2005), they focused on sep-
arating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize
it is also important to separate the ancestor node in
the correct path from their sibling node.
The vector w can be decomposed in to the set
of w
i
for each node (category) in the hierarchy. In
Figure 1, the example hierarchy has 7 nodes and 4
of them are leaf nodes. The category is encode
as an integer, 1, . . . , 7. Suppose that the train-
ing pattern x belongs to category 4. Both w in
the Figure 1a and Figure 1b can successfully clas-
sify x into category 4, since F (w,?(x,y
4
)) =
?
1,2,4
?w
i
,x? is the maximal among all the possi-
ble discriminate functions. So both learned param-
eter w is acceptable in current hierarchical support
vector machine.
Here we claim the w in Figure 1b is better than the
w in Figure 1a. Since we notice in Figure 1a, the
discriminate function ?w
2
,x? is smaller than the
discriminate function ?w
3
,x?. The discriminate
function ?w
i
,x? measures the similarity of x to
category i. The larger the discriminate function is,
the more similar x is to category i. Since category
2 is in the path from the root to the correct cate-
gory and category 3 is not, intuitively, x should be
closer to category 2 than category 3. But the dis-
criminate function in Figure 1a is contradictive to
this assumption. But such information is reflected
correctly in Figure 1b. So we conclude w in Fig.
1b is superior to w in 1a.
Here we propose a novel formulation to incor-
porate such information. Denote A
i
as the mul-
tilabel in Y
i
that corresponds to the nonleaf cate-
gories and Sib(z) denotes the sibling nodes of z,
that is the set of nodes that have the same parent
with z, except z itself. Implementing the above
idea, we can get the following formulation:
min
w,?,?
1
2
?w?
2
+ C
1
?
i
?
i
+ C
2
?
i
?
i
(9)
s.t.?w, ??
i
(y,
?
y)? ? 1?
?
i
l(y,
?
y)
, (?i,
y ? Y
i
?
y ?
?
Y
i
)
?w, ??
i
(z,
?
z)? ? 1?
?
i
l(z,
?
z)
, (?i,
z ? A(i)
?
z ? Sib(z)
)
?
i
? 0(?i)
?
i
? 0(?i)
It arrives at the following Lagrangian:
L(w, ?
1
, ..., ?
n
, ?
1
, ..., ?
n
)
=
1
2
?w?
2
+ C
1
X
i
?
i
+ C
2
X
i
?
i
?
X
i
X
y?Y
i
?y?
?
Y
i
?
iy?y
(?w, ??
i
(y,
?
y)? ? 1 +
?
i
l(y,
?
y)
)
166
1
2 3
4 5 6 7
10,1  xw
3,2  xw 5,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
1
2 3
4 5 6 7
10,1  xw
7,2  xw 3,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
a) b)
Figure 1: Two different discriminant function in a hierarchy
?
X
i
X
z?A
i
?z?Sib(z)
?
iz?z
(?w, ??
i
(z,
?
z)? ? 1 +
?
i
l(z,
?
z)
)
?
X
i
c
i
?
i
?
X
i
d
i
?
i
(10)
The dual QP becomes
max
?
?(?) =
?
i
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
+
?
i
?
z?A
i
?z?Sib(z)
?
iz
?
z
?
1
2
?
i,j
?
y?Y
i
?y?
?
Y
i
?
r?Y
j
?r?
?
Y
j
?
1
i,j,y,
?
y,r,
?
r
(11)
?
1
2
?
i,j
?
z?A
i
?z?Sib(z)
?
k?A
j
?
k?Sib(k)
?
2
i,j,z,
?
z,k,
?
k
,
s.t.?
iy
?
y
? 0, (12)
?
jz
?
z
? 0, (13)
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
l(y,
?
y)
? C
1
, (14)
?
z?A
i
?z?Sib(z)
?
iz
?
z
l(z,
?
z)
? C
2
, (15)
where ?
1
i,j,y,
?
y,r,
?
r
=
?
iy
?
y
?
jr
?
r
???
i
(y,
?
y), ??
j
(r,
?
r)? and ?
2
i,j,z,
?
z,k,
?
k
=
?
iz
?
z
?
jk
?
k
???
i
(z,
?
z), ??
j
(k,
?
k)?.
3.1 Optimization Algorithm
The derived QP can be very large, since the num-
ber of ? and ? variables is up to O(n?2
N
), where
n is number of training pattern and N is the num-
ber of nodes in the hierarchy. But two properties
of the dual problem can be exploited to design a
much more efficient optimization.
First, the constraints in the dual problem Eq. 11
- Eq. 15 factorize over the instance index for both
?-variables and ?-variables. The constraints in
Eq. 14 do not couple ?-variables and ?-variables
together. Further, dual variables ?
iy
?
y
and ?
jy
?
?
y
?
belonging to different training instances i and j do
not join in a same constraints. This inspired an
optimization procedure which iteratively performs
subspace optimization over all dual variables ?
iy
?
y
belonging to the same training instance. This will
in general reduced to a much smaller QP, since
it freezes all ?
jy
?
y
with j 6= i and ?-variables at
their current values. This strategy can be applied
in solving ?-variables.
Secondly, the number of active constraints at the
solution is expected to be relatively small, since
only a small fraction of categories
?
y ?
?
Y
i
( or
?
y ? Sib(y) when y ? A
i
) will typically fail to
achieve the required margin. The expected sparse-
ness of the variable for the dual problem can be
exploited by employing a variable selection strat-
egy. Equivalently, this corresponds to a cutting
plane algorithm for the primal QP. Intuitively, we
will identify the most violated margin constraint
with index (i,y,
?
y) and then add the correspond-
ing variable to the optimization problem. This
means that we start with extremely sparse prob-
lems and only successively increase the number of
variables in the active set. This general approach
to deal with large linear or quadratic optimization
problems is also known as column selection. In
practice, it is often not necessary to optimize until
final convergence, which adds to the attractiveness
of this approach.
We have used the LOQO optimization package
(Vanderbei, 1999) in our experiments.
4 Experiment
We evaluate our proposed model on the section D
in the WIPO-alpha collection
1
, which consists of
the 1372 training and 358 testing document. The
1
World Intellectual Property Organization (WIPO)
167
Table 1: Prediction losses (%) obtained on WIPO.
The values per column is calculated with the dif-
ferent loss function.
X
X
X
X
X
X
X
Train
Test
l
0/1
l
?
l
tr
l
uni
l
sib
l
sub
HSVM 48.6 188.8 94.4 97.2 5.4 7.5
l
0/1
HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4
HSVM 49.7 187.7 93.9 99.4 5.0 7.1
l
?
HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9
HM3 70.9 167.0 - 89.1 5.0 7.0
HSVM 49.4 186.0 93.0 98.9 5.0 7.5
l
tr
HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
HSVM 47.2 181.0 90.5 94.4 5.0 7.0
l
?
uni
HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
HSVM 49.4 184.9 92.5 98.9 4.8 7.4
l
?
sib
HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
HSVM 50.6 189.9 95.0 101.1 5.2 7.5
l
?
sub
HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2
number of nodes in the hierarchy is 188, with max-
imum depth 3.
We compared the performance of our proposed
method HSVM-S with two algorithms: HSVM(Cai
and Hofmann, 2007) and HM3(Rousu et al, 2006).
4.1 Effect of Different Loss Function
We compare the methods based on different loss
functions, l
0/1
, l
?
, l
tr
, l
u?ni
, l
s?ib
and l
s?ub
. The per-
formances for three algorithms can be seen in Ta-
ble 1. Those empty cells, denoted by ?-?, are not
available in (Rousu et al, 2006).
As expected, l
0/1
is inferior to other hierarchi-
cal losses by getting poorest performance in all the
testing losses, since it can not take into account the
hierarchical information between categories. The
results suggests that training with a hierarchical
losses function, like l
s?ib
or l
u?ni
, would lead to a
better reduced l
0/1
on the test set as well as in
terms of the hierarchical loss. In Table 1, we can
also point out that when training with the same
hierarchical loss, the performance of HSVM-S is
better than HSVM under the measure of most hier-
archical losses, since HSVM-S includes more hier-
archical information,the relationship between the
sibling categories, than HSVM which only separate
the leave categories.
5 Conclusion
In this paper we present a hierarchical multi-class
document categorization, which focus on maxi-
mize the margin of the classes at the different
levels in the class hierarchy. In future work, we
plan to extend the proposed hierarchical learning
method to the case where the hierarchy is a DAG
instead of tree and scale up the method further.
Acknowledgments
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302.
References
L. Cai and T Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine
Learning, 28(1):41?75.
Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004.
Large margin hierarchical classification. In Pro-
ceedings of the 21 st International Conference on
Machine Learning.
D. Koller and M Sahami. 1997. Hierarchically classi-
fying documents using very few words. In Proceed-
ings of the International Conference on Machine
Learning (ICML).
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning
of hierarchical multilabel classification models. In
Journal of Machine Learning Research.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning.
R. J. Vanderbei. 1999. Loqo: An interior point code
for quadratic programming. In Optimization Meth-
ods and Software.
K. Wang, S. Zhou, and S Liew. 1999. Building hier-
archical classifiers using class proximities. In Pro-
ceedings of the International Conference on Very
Large Data Bases (VLDB).
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
168
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1154?1164, Dublin, Ireland, August 23-29 2014.
Automatic Corpus Expansion for Chinese Word Segmentation
by Exploiting the Redundancy of Web Information
Xipeng Qiu, ChaoChao Huang and Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University, Shanghai, China
xpqiu@fudan.edu.cn, superhuang007@gmail.com, xjhuang@fudan.edu.cn
Abstract
Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are
based on supervised learning, which depend on large scale annotated corpus. However,
these supervised methods do not work well when we deal with a new different domain
without enough annotated corpus. In this paper, we propose a method to automatically
expand the training corpus for the out-of-domain texts by exploiting the redundant in-
formation on Web. We break up a complex and uncertain segmentation by resorting to
Web for an ample supply of relevant easy-to-segment sentences. Then we can pick out
some reliable segmented sentences and add them to corpus. With the augmented corpus,
we can re-train a better segmenter to resolve the original complex segmentation. The
experimental results show that our approach can more effectively and stably improve the
performance of CWS. Our method also provides a new viewpoint to enhance the perfor-
mance of CWS by automatically expanding corpus rather than developing complicated
algorithms or features.
1 Introduction
Word segmentation is a fundamental task for Chinese language processing. In recent years,
Chinese word segmentation (CWS) has undergone great development. The popular method is
to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004).
The goal of sequence labeling is to assign labels to all elements in a sequence, which can be
handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al.,
1996), Conditional Random Fields (CRF)(Lafferty et al., 2001).
After years of intensive researches, Chinese word segmentation achieves a quite high precision.
However, the performance of segmentation is not so satisfying for the practical demands to
analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news
texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain
texts.
Since these supervised approaches often has a high requirement on the quality and quantity of
annotated corpus, which is always not easy to create. As a result, many methods were proposed
to utilize the information of unlabeled data.
There are three kinds of methods for domain adaptation problem in CWS.
The first is to use unsupervised learning algorithm to segment texts, like branching entropy
(BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry
and Sagot, 2012).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
1154
The second is to use unsupervised or domain-independent features in supervised learning for
Chinese word segmentation, such as punctuation and mutual information(MI), word accessory
variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011)
The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the
difference in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and
Isozaki, 2008).
Although these methods improve the performance of out-of-domain texts, the performance is
still worse than that of in-domain texts obviously.
We firstly investigate the reasons of lower performance in new domain for state-of-the-art
CWS systems and find that most of error segmentation were caused by out-of-vocabulary (OOV)
words, also called new words or unknown words (see details in Section 3). It is difficult to devote
efforts to building a corpus for out-of-domain texts, since new words are produced frequently as
the development of the society, especially the Internet society. It is also impractical to manually
maintain an up-to-date corpus to include all geographical names, person names, organization
names, technical terms, etc.
In this paper, we propose a method to automatically expand the training corpus for the out-
of-domain texts by exploiting the redundant information on Web. When we meet a complex
and potentially difficult-to-segment sentence, we do not expect to solve it with more complicated
learning algorithm or elaborate features. We assume that there are some relevant sentences that
are relatively easy to process. These simple sentences can help to solve the complex one.
For example, the sentence ??????? (L?Oreal, Maybelline)?is difficult to segment if
both ???? (L?Oreal)?and ???? (Maybelline)?are unknown words. However, we can
always find some easy-to-segment sentences, such as??????? (I use Maybelline)?,???
???? (production of L?Oreal)?, and so on. When we use these simple sentences to re-train
the segmenter, we can solve the previous complex sentence.
Our method relies on breaking up the complex problems into relevant smaller, simpler prob-
lems that can be solved easily. Fortunately, we can resort to the scale and redundancy of the
web for an ample supply of simple sentences that are relatively easy to process.
Our method is very easy to implement upon a trainable base segmenter. Given the out-of-
domain texts, we firstly choose some uncertain segmentations and select the candidate expansion
seeds. Secondly, we use these seeds to get the relevant texts from Web search engine. Then we
segment these texts and add the texts with high confidence to training corpus. Finally, we can
get a better segmenter with the new corpus.
The rest of the paper is organized as follows: we review the related works in section 2. In
section 3, we analyze the influence factor for CWS. Then we describe our method in section 4.
Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we
conclude our work in section 7.
2 Related Works
The idea of exploring information redundancy on Web was introduced in question answering
system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information
extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned
in Chinese word segmentation.
Nonetheless, there are three kinds of related methods on Chinese word segmentation.
One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning
method to expand annotated corpus, but they still need to manually label some new raw texts
in order to enlarge the training corpus. Different with these methods, our method do not require
any manual oracle labeling at all.
Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training
is a general semi-supervised learning approach. In self-training, a classifier is first trained with
the small amount of labeled data. The classifier is then used to classify the unlabeled data.
1155
0.7
0.75
0.8
0.85
0.9
0.95
1
0 1 2 3 4 5
F1
Number of Continuous OOV Words
(a) Number of continuous OOV
words
0.100.20
0.300.40
0.500.60
0.700.80
0.901.00
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.6 0.7 1
F1
OOV Rate
(b) OOV rate
0.100.20
0.300.40
0.500.60
0.700.80
0.901.00
1 2 3 4 5 6 7 8 9 10
F1
Word Length 
(c) Word Length
The blue horizontal line is the overall F1 score, and the red line is the F1 scores with different values of the
factor.
Figure 1: Analysis of Influence Factors
Typically the most confident unlabeled points, together with their predicted labels, are added
to the training set. The classifier is re-trained and the procedure repeated. Note that the
classifier uses its own predictions to teach itself. Self-training has been applied to several natural
language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POS-
tagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman
et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information
extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve
system performance on the target domain by simultaneously modeling annotated source-domain
data and unannotated target domain data in the training process. However, the data on target
domain cannot always help itself (Steedman et al., 2003).
The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the
massive manual natural annotations or punctuation information on the Internet to improve the
performance of CWS. However, these natural annotations are just partial annotations and their
roles depend on the qualities of the selected resource, such as Wikipedia.
In this paper, we wish to propose a method to obtain new fully-annotated data in more
aggressive way, which can combine the advantages of the above works.
3 Analysis of Influence Factors for CWS
Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV)
words for segmentation. We first conduct experiments on the Chinese Treebank (CTB6.0)
dataset (Xue et al., 2005) (The detailed information of dataset is shown in Section 6).
Table 1 shows the performance of base segmenter. The F1 score of OOV words is significantly
lower than that of in-vocabulary (INV) words.
Precision Recall F1
INV 95.86 96.58 96.21
OOV 74.12 66.77 70.25
Total 94.64 94.73 94.69
Table 1: Performances of INV and OOV words
We also investigate the impacts of three different factors: number of continuous OOV words,
OOV rate and word length. Figure 1 shows the F1 scores with the changes of the different
factors. We find that OOV words significantly improve the difficulty of segmentation, while the
word length does not always harm the accuracy.
These findings also indicate that we can improve the performance of CWS if we have a
dictionary or annotated corpus including these OOV words. With the redundancy of the Web
information, it is not difficult to automatically obtain the expected dictionary or corpus.
1156
4 Our Method
In this section, we describe our method to automatically expand the training corpus.
4.1 Framework of Automatic Corpus Expansion
Our framework of automatic corpus expansion is similar to standard process self-training or
active learning for domain adaptation. Given a trainable base segmenter, the texts in out-
of-domain, we firstly choose some uncertain segmentations and select the candidate expansion
seeds. Secondly, we use these seeds to get the relevant texts from Web search engine. Then we
segment these texts and add the texts with high confidence to training corpus. Finally, we can
get a better segmenter with the new corpus.
Algorithm 1 illustrates the framework of automatic corpus expansion.
Algorithm 1 Framework of Automatic Corpus Expansion
Input:
Annotated Corpus C
A
Unannotated Corpus in Target domain C
T
Uncertainty Threshold T
u
Seed Extraction Threshold T
se
Acceptation Threshold T
a
Maximum Iteration Number: M
Output: Expanded Annotated Corpus C
A
1: for i = 1 to M do
2: Train a basic segmenter using current C
A
with base learner
3: Use the basic segmenter to do segmentation for each sentence in C
T
and calculate its
confidence.
4: Choose out the sentences collection C
TS
, in which the segmentation confidence of each
sentence is less than T
u
.
5: Extract the expansion seeds collection C
seeds
from C
TS
and use search engine to acquire
relevant raw texts C
RRT
.
6: Segment and calculate the confidence for each sentence in C
RRT
.
7: Pick the reliable segmentations C
new
with confidence more than T
a
from C
RRT
.
8: Add C
new
into C
A
.
9: end for
10: return C
A
;
4.2 Uncertainty Sampling
The first key step in our method is to find the uncertain segmentations. There are many proposed
uncertainty measures in the literature of active learning (Settles, 2010), such as entropy and
query-by-committee (QBC) algorithm.
In our works, we investigate four following uncertainty measures for each sentence x. We use
S
1
(x), S
2
(x), ? ? ? , S
N
(x) to represent the top N scores given by the segmenter.
Normalized Score U
NS
The first measures is normalized score by the length of x, the normalized score U
NS
is calcu-
lated by
U
NS
=
S
1
(x)
L
(1)
where L is the length of x.
Standard Deviation U
SD
1157
The standard deviation is calculate with the top N scores.
U
SD
=
?
?
?
?
1
N
N
?
i=1
(S
i
(x) ? ?)
2 (2)
where ? = 1
N
?
N
i=1
S
i
(x) is the average or expected value of S
i
(x).
Entropy U
Entropy
Entropy is a measure of unpredictability or information content. Since we use character-based
method for word segmentation, each character is labeled as one of {B, M, E, S} to indicate
the segmentation. {B, M, E} represent Begin, Middle, End of a multi-character segmentation
respectively, and S represents a Single character segmentation.
Given the top N labeled results for a sentence, each labeled sequence consists of the labels
{B, M, E, S}. We define l ? {B,M,E, S} to represent the label variable, and count
j
(l) to be
the number of occurrences of l on position j among the top N results. Thus, we can calculate
the entropy for the labeling uncertainty of each character.
The entropy H
j
(l) for the character on position j is calculated by
H
j
(l) = ?
?
l
count
j
(l)
N
log countj(l)
N
, (3)
where ?
l
count
j
(l) = N .
The entropy of sentence U
Entropy
is the sum of the entropies of all the characters in the
sentence.
U
Entropy
=
L
?
j=1
H
j
(l). (4)
Margin U
Margin
Margin is the deviation of top 2 scores, which is often used in machine learning algorithms,
such as support vector machine (Cristianini and Shawe-Taylor, 2000) and passive-aggressive
algorithm (Crammer et al., 2006).
U
Margin
= S
1
(x) ? S
2
(x) (5)
Among the above four measures, the larger the entropy is, the more uncertain the result is.
For the rest three factors, the less the score is, the more uncertain the result is.
We test these four uncertainty measures on the development set in order to choose the best
one as our confidence measure.
In figure 2, we illustrate the relationship between each uncertainty measure and the OOV
count. We assume that the more OOV words are, the more uncertainty is. Meanwhile, a steep
learning curve imply a good ability to distinguish whether the result is uncertain.
Obviously, the entropy is not helpful according to our assumption. The normalized score is
okay but not good, and both the standard deviation and margin seem to be useful because they
can give a better threshold to distinguish uncertain segmentation. Finally, we choose margin as
our uncertainty measure.
4.3 Expansion Seeds Extraction
For the uncertain segmentation, not every word is unreliable. We just pick the suspicious
fragments. Therefore, we need to extract some seed phrases to get the relevant texts. It is
notable that these seed phrases do not need to be words. They can be the combinations of
several words or only parts of words.
Take the following sentence for example.
1158
0.3450.35
0.3550.36
0.3650.37
0.3750.38
0.385
0 2 4 6 8 10
U NS
OOV count
(a) NScore
00.001
0.0020.003
0.0040.005
0.0060.007
0 2 4 6 8 10
U SD
OOV count
(b) STDEV
00.5
11.5
22.5
33.5
44.5
5
0 2 4 6 8 10
U Entropy
OOV count
(c) Entropy
0
0.002
0.004
0.006
0.008
0.01
0.012
0 2 4 6 8 10
U Margin
OOV count
(d) Margin
Figure 2: Different Uncertainty Measures
??????????????
(L?Oreal, Maybelline, Lancome are good brands)
The first fragment??????????is difficult to segment if these words does not appear in
training corpus. Conversely, the second fragment is easy to segment since the containing words
are very common.
We use base segmenter to get the top five results as follows:
? ? ? ? ? ? ? ? ? ? ? ? ? ?
1 B M M M E B M E S B E S B E
2 B M M E B E B E S B E S B E
3 B M E B E B M E S B E S B E
4 S B M M E B E S S B E S B E
5 S B M M M E B E S B E S B E
(Li et al., 2012) proposed a good way to select the candidate words for active learning with
diversity measurement to avoid duplicate annotation. However, their method is not suitable for
our work. The reason is that they regarded CWS as a binary classification problem, while our
base segmenter uses 1st-order sequence labeling.
In our work, we choose the expansion seeds by calculating the entropy of each character. If
the entropy of the character is larger than threshold T
se
, we say that this character may be in
an uncertain context. Thus, we extract the consecutive uncertain characters and their contexts
as the expansion seeds.
For the above example, we select the ????????? (L?Oreal, Maybelline, Lancome)?
and its context ?? (is)?as a seed ???????????.
4.4 Collect relevance texts by using Web Search Engines
After obtaining the expansion seeds, we collect the relevant texts on multiple search engines
including Google, Baidu and Bing.
For the seed ???????????, we can get the following relevance sentence, which is
easy to segment.
??????????????????? 500 ????
(L?Oreal owns more than 500 brands, including Lancome, L?Oreal, Maybelline, Vichy, etc.)
In our work, we just get the top 100 relevant texts returned by each search engine without
manual intervention. We do not use any search API and directly use the returned webpages by
search engine, then extract the snippets and titles. Therefore, we just write a simple program
to collect the webpages and clean them.
4.5 Expand Training Corpus
Since the qualities of these relevant texts are spotty, we just pick the reliable texts with high
confidence scores. In contrast to uncertainty sampling, we find the certain segmentations from
the collecting raw texts and add them to training corpus. Here, we also use a margin to find
the reliable ones as new training data.
In our experiments, the number of selected sentence is 1 ? 5 for each seed.
Thus, we can re-train a new segmenter on the expanded corpus. After several iteration, we
will get a segmenter with the best performance.
1159
5 Base Segmenter
We use discriminative character-based sequence labeling for base word segmentation. Each
character is labeled as one of {B, M, E, S} to indicate the segmentation.
We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et
al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used
to avoid the overfitting problem.
6 Experiment
To evaluate our algorithm, we use both CTB6.0 and CTB7.0 datasets in our experiments. CTB
is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism.
It is also a popular data set to evaluate word segmentation methods, such as (Sun and Xu, 2011).
Since CTB dataset is collected from different sources, such as newswire, magazine, broadcast
news and web blogs, it is suitable to evaluate the performance of CWS systems on different
domains.
We conduct two experiments on different divisions of datasets.
1. The first experiment is performed on CTB6.0 for comparison with state-of-the-art systems
which also utilize the unlabeled data for word segmentation.
2. The second experiment is performed on CTB7.0 for better evaluation on out-of-domain
texts. CTB7.0 contains some newer news texts and web blogs texts, which is more suitable
to evaluate our method for out-of-domain data.
In our experiments, we set C = 0.01 for PA algorithm. We also try to use the different values
of C, and found that larger values of C imply a more aggressive update step and result to fast
convergence, but it has little influence on the final accuracy. The maximum iteration number
M
? of PA algorithm is set to 50.
The feature templates are C
i
T
0
, (i = ?1, 0, 1),C
?1,0
T
0
, C
0,1
T
0
, C
?1,1
T
0
, T
?1,0
. C represents a
Chinese character, and the subscript of C indicates its position relative to the current character,
whose subscript is 0. T represents the character-based tag.
The evaluation measure are reported are precision, recall, and an evenly-weighted F
1
.
6.1 Experiments on CTB6.0
Train Dev Test
81-325, 400-454, 500-554, 590-596,
600-885, 900, 1001-1017, 1019,
1021-1035, 1037-1043, 1045-1059,
1062-1071, 1073-1078, 1100-1117,
1130-1131 1133-1140, 1143-1147,
1149-1151,2000-2139, 2160-2164,
2181-2279,2311-2549, 2603-2774,
2820-3079
41-80,
1120-1129,
2140-2159,
2280-2294,
2550-2569,
2775-2799,
3080-3109
(1-40,901-931 newswire)
(1018, 1020, 1036,
1044,1060-1061, 1072,
1118-1119, 1132,1141-1142,
1148 magazine) (2165-2180,
2295-2310, 2570-2602, 2800-
2819, 3110-3145 broadcast
news)
Table 2: CTB6.0 Dataset Division
On CTB 6.0, we divide the training, development and test sets according to (Yang and Xue,
2012). , which are shown in Table 2 The detailed statistical information is shown in Table 3.
Firstly, We use the development set to determine the parameters in Algorithm 1. For T
u
, T
se
and T
a
, we have three rounds to determine the parameters. In first round, we find the best value
t1 in the range to 0 ? 1 with the interval of 0.1. In second round, we find the best value t2 in
range t1? 0.1 ? t1+0.1 with the interval of 0.01. In third round, we find the final best value t3
1160
94.594.794.9
95.195.395.5
95.795.996.1
0 1 2 3 4 5
F1
Iterations
(a) F1 score
7072
7476
7880
8284
0 1 2 3 4 5
Roov
Iterations
(b) OOV Recall
Figure 3: Iterative Learning Curve on
CTB6.0
92.592.792.9
93.193.393.5
93.793.994.1
94.394.5
0 1 2 3 4 5
F1
Iterations
(a) F1 score
60.562.564.5
66.568.570.5
72.574.576.5
78.5
0 1 2 3 4 5
Roov
Iterations
(b) OOV Recall
Figure 4: Iterative Learning Curve on
CTB7.0
in the range to t2? 0.01 ? t1+ 0.01 with the interval of 0.001. The maximum iteration number
M is just determined based on convergence with the range 1 ? 10.
Finally, we set these parameters as following: uncertainty threshold T
u
= 0.003, seed extrac-
tion threshold T
se
= 0.65, acceptation threshold T
a
= 0.004 and maximum iteration number
M = 5.
Figure 3 shows the changing curve of F1 and OOV recall in the process of corpus expansion.
The performance of the baseline segmenter is shown at iteration 0. The curve shows that the
F1 score and OOV recall have continuous improvement with the increasing of train corpus. The
maximum performance is achieved at the 5th iteration. The detailed results are shown in Table
4. Compared with the baseline, the expanded corpus leads to a segmenter with significantly
higher accuracy. The relative error reductions are 26.37% and 43.63% in terms of the balanced
F-score and the recall of OOV words respectively.
Dataset Sents Words Chars OOV Rate
Train. 22757 639506 1053426 -
Dev. 2003 59764 100038 5.45%
Test 2694 81304 133798 5.58%
Table 3: Corpus Information of CTB 6.0
Test P R F1 R
oov
Baseline 94.64 94.73 94.69 70.25
Final 95.66 96.51 96.09 83.23
(Sun and Xu, 2011) 95.86 95.62 95.74 79.28
Table 4: Performance on CTB6.0
6.2 Experiments on CTB7.0
CTB7.0 includes documents from newswire, magazine articles, broadcast news, broadcast con-
versations, newsgroups and weblogs. The newly added documents contains texts from web
blogs, which is very different with news texts. Therefore, we use the documents (No. 4198 4411,
weblogs) as test dataset, and the rest as training dataset. The detailed statistical information
is shown in Table 5. We can see that the OOV rate is higher than the dataset in the first
experiment.
Dataset Sents Words Chars OOV Rate
Train. 40425 987307 1601142 -
Test 10177 209827 342061 7.09%
Table 5: Corpus Information of CTB 7.0
Test P R F1 R
oov
Baseline 93.58 92.40 92.98 60.72
Final 94.47 94.40 94.43 79.24
Table 6: Performance on CTB7.0
Figure 4 shows the changing curve of F1 and OOV recall in the process of corpus expansion.
The performance of the baseline segmenter is shown at iteration 0. The curve shows that the
F1 score and OOV recall have continuous improvement with the increasing of train corpus. The
maximum performance is achieved at iteration 5. The detailed results are shown in Table 6.
Compared with the baseline, the expanded corpus leads to a segmenter with significantly higher
accuracy. The relative error reductions are 20.66% and 47.15% in terms of the balanced F-score
and the recall of OOV words respectively.
1161
6.3 Analysis
The experimental results show that our method is very effective to improve the performance of
Chinese word segmentation. Especially, our method gives a significant boost on OOV words.
For the words such as ???????? (Borussia Moenchengladbach)?, ??????
(catalase)?,???? (Yi ZhongTian, a Chinese person name)?and???? (prime time)?,
it is still difficult to segment them correctly even if we can obtain useful features from unlabeled
data. When we take advantage of the redundant information from Web, we can easily collect
the relevant easy-to-segment sentences to expand the training corpus.
Our method can result to a segmenter significantly better than the systems which finds the
informative features derived from unlabeled data, such as (Sun and Xu, 2011). This also suggests
that expanding corpus is more effective than developing complicated algorithm or well-design
features. Of course, our method is compatible with these technologies, which can further improve
the performance of CWS by combining the Web redundancy.
7 Conclusion
In this paper, we propose a method to automatically expand the training corpus for the out-
of-domain texts. Given the out-of-domain texts, we first choose some uncertain segmentations
as candidate expansion seeds, and use these seeds to get the relevant texts from search engine.
Then we segment the texts and add the texts with high confidence to training corpus. We can
always obtain some easily-segmented texts due to the large amount of redundancy texts on Web,
especially for new words. Our experimental results show that our proposed method can more
effectively and stably utilize the unlabeled examples to improve the performance. Our method
also provides a new viewpoint to enhance the performance of CWS by expanding corpus rather
than developing complicated algorithms or features.
The long term goal of our method is to build an online and constant learning system, which
can identify the difficult tasks and seek help from crowdsourcing. Search engines are special
cases of crowdsourcing. In the future, we wish to investigate our method for other NLP tasks,
such as POS tagging, Named Entity Recognition, and so on.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments. This work was
funded by NSFC (No.61003091), Science and Technology Commission of Shanghai Municipality
(14ZR1403200) and Shanghai Leading Academic Discipline Project (B114).
References
Y. Altun, D. McAllester, and M. Belkin. 2006. Maximum margin semi-supervised learning for structured
variables. Advances in neural information processing systems, 18:33.
Michele Banko, Eric Brill, Susan Dumais, and Jimmy Lin. 2002. AskMSR: Question answering using the
worldwide web. In Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and
Knowledge Bases, pages 7?9.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1):39?71.
Stephen Clark, James R Curran, and Miles Osborne. 2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-
Volume 4, pages 49?55. Association for Computational Linguistics.
C.L.A. Clarke, G.V. Cormack, and T.R. Lynam. 2001. Exploiting redundancy in question answering.
Proceedings of the 24th annual international ACM SIGIR conference on Research and development in
information retrieval, pages 358?365.
1162
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing.
K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine Learning Research, 7:551?585.
N. Cristianini and J. Shawe-Taylor. 2000. An introduction to support Vector Machines: and other
kernel-based learning methods. Cambridge Univ Pr.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
H. Feng, K. Chen, X. Deng, and W. Zheng. 2004. Accessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL,
volume 2007, page 22.
Wenbin Jiang, Meng Sun, Yajuan L?, Yating Yang, and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case study. In ACL, pages 761?769.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semi-supervised
conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th annual meeting of the Association
for Computational Linguistics, pages 209?216. Association for Computational Linguistics.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branch-
ing entropy. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 428?435.
Association for Computational Linguistics.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling question answering to the web. Proceedings of
the 10th international conference on World Wide Web, pages 150?161.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation.
Computational Linguistics, 35(4):505?512.
Shoushan Li, Guodong Zhou, and Chu-Ren Huang. 2012. Active learning for Chinese word segmentation.
In COLING (Posters), pages 683?692.
Yang Liu and Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and pos-tagging.
In COLING (Posters), pages 745?754.
Pierre Magistry and Beno?t Sagot. 2012. Unsupervized word segmentation: the case for mandarin
chinese. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 383?387. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In
Proceedings of the main conference on human language technology conference of the North American
Chapter of the Association of Computational Linguistics, pages 152?159. Association for Computational
Linguistics.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th international conference on Computational Linguistics.
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statis-
tical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 616?623, Prague, Czech Republic, June. Association for Compu-
tational Linguistics.
1163
Kenji Sagae. 2010. Self-training without reranking for parser domain adaptation and its impact on
semantic role labeling. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural
Language Processing, pages 37?44. Association for Computational Linguistics.
Manabu Sassano. 2002. An empirical study of active learning with support vector machines for japanese
word segmentation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, pages 505?512. Association for Computational Linguistics.
Burr Settles. 2010. Active learning literature survey. University of Wisconsin, Madison.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul
Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets.
In Proceedings of the tenth conference on European chapter of the Association for Computational
Linguistics-Volume 1, pages 331?338. Association for Computational Linguistics.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970?979.
Association for Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665?673. Citeseer.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural language engineering, 11(2):207?238.
Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Yaqin Yang and Nianwen Xue. 2012. Chinese comma disambiguation for discourse analysis. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume
1, pages 786?794. Association for Computational Linguistics.
D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Computational Linguistics, pages 189?196. Association
for Computational Linguistics.
H. Zhao and C. Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging
for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
1164
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1379?1388, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Part-of-Speech Tagging for Chinese-English Mixed Texts
with Dynamic Features
Jiayi Zhao? Xipeng Qiu? Shu Zhang? Feng Ji? Xuanjing Huang?
School of Computer Science, Fudan University, Shanghai, China ? ?
Fujitsu Research and Development Center, Beijing, China?
zjy.fudan@gmail.com?
{xpqiu,fengji,xjhuang}@fudan.edu.cn?
zhangshu@cn.fujitsu.com ?
Abstract
In modern Chinese articles or conversations,
it is very popular to involve a few English
words, especially in emails and Internet liter-
ature. Therefore, it becomes an important and
challenging topic to analyze Chinese-English
mixed texts. The underlying problem is how
to tag part-of-speech (POS) for the English
words involved. Due to the lack of specially
annotated corpus, most of the English words
are tagged as the oversimplified type, ?foreign
words?. In this paper, we present a method
using dynamic features to tag POS of mixed
texts. Experiments show that our method
achieves higher performance than traditional
sequence labeling methods. Meanwhile, our
method also boosts the performance of POS
tagging for pure Chinese texts.
1 Introduction
Nowadays, Chinese-English mixed texts are
prevalent in modern articles or emails. More and
more English words are used in Chinese texts as
names of organizations, products, terms and abbre-
viations, such as ?eBay?, ?iPhone?, ?GDP?, ?An-
droid? etc. On the other hand, it is also a common
phenomenon to use Chinese-English mixed texts
in daily conversation, especially in communication
among employers in large international corporations.
There are some challenges for analyzing Chinese-
English mixed texts:
1. How to define the POS tags for English words
in these mixed texts. Since the standard of
POS tags for English and Chinese are different,
we cannot use English POS to tag the English
words in mixed texts.
2. Due to lack of annotated corpus for mixed texts,
most of the English words are tagged as ?for-
eign words?, which is oversimplified. So we
cannot use them in further processing for the
syntactic and semantic analysis.
3. Most English words used in mixed texts are of-
ten out-of-vocabulary (OOV), which thus in-
creases the difficulties to tag them.
Currently, the mainstream method of Chinese
POS tagging is joint segmentation & tagging with
cross-labels, which can avoid the problem of error
propagation and achieve higher performance on both
subtasks(Ng and Low, 2004). Each label is the cross-
product of a segmentation label and a tagging la-
bel, e.g. {B-NN, I-NN, E-NN, S-NN, ...}. The fea-
tures are generated by position-based templates on
character-level.
Since the main part of mixed texts is in Chinese
and the role of English word is more like Chinese,
we use Chinese POS tags (Xia, 2000) to tag English
words. Since the categories of the most commonly
used English words are nouns, verbs and adjectives,
we can use ?NN?, ?NR?, ?VV?, ?VA?, ?JJ? to label
their POS tags.
For the English proper nouns and verbs, there
are no significant differences in Chinese and En-
glish POS tags except that English features plural
and tense forms.
For the English nouns, these are some English
nouns used as verbs, such as ??? [fan/VV]??(I
adore him very much.)? where ?fan? means ?adore?
and is used as a verb.
For the English adjectives, there are two corre-
sponding Chinese POS tags ?VA? and ?JJ?. For ex-
ample, the roles of some English words in Table 1,
1379
Table 1: The POS tags of English Adjectives in Mixed
Texts
Chinese English
? ? ? [profes-
sional/VA]?
I am very profes-
sional.
??? [high/VA]? Feel very high.
?? [super/JJ] [star/NN] He is a super star.
such as ?professional? and ?high?, are different with
their original ones.
Therefore, the POS tagging for mixed texts cannot
be settled with simple methods, such as looking up
in a dictionary.
One of the main differences between Chinese and
English in POS tagging is that the two languages
have character-based features and word-based fea-
tures respectively. To ensure the consistency of tag-
ging models, we prefer to use word-level informa-
tion in Chinese, which is both useful for Chinese-
English mixed texts and Chinese-only texts. For in-
stance, in a sentence ?X ?? Y ... (X or Y ...)?,
the word Y ought to have the same POS tag as the
word X . Another example is that the word follow-
ing a pronoun is usually a verb, and adjectives of-
ten describe nouns. Some related works show that
word-level features can improve the performance of
Chinese POS tagging (Jiang et al 2008; Sun, 2011).
In this paper, we propose a method to tag mixed
texts with dynamic features. Our method combines
these dynamic features, which are dynamically gen-
erated at the decoding stage, with traditional static
features. For Chinese-English mixed texts, the tra-
ditional features cannot yield a satisfied result due to
lack of training data. The proposed dynamic features
can improve the performance by using the informa-
tion of a word, such as POS tag or length of the whole
word, which is proven effective by experiments.
The rest of the paper is organized as follows: In
section 2, we introduce the sequence labeling mod-
els, thenwe describe our method of dynamic features
in section 3 and analyze its complexity in section 4.
Section 5 describes the training method. The exper-
imental results are manifested in section 6. Finally,
We review the relevant research works in section 7
and conclude our work in section 8.
2 Sequence Labeling Models
Sequence labeling is the task of assigning labels
y = y1, . . . , yn to an input sequence x = x1, . . . , xn.
Given a sample x, we define the feature ?(x, y).
Thus, we can label x with a score function,
y? = argmax
y
F (w,?(x, y)), (1)
where w is the parameter of function F (?).
For sequence labeling, the feature can be denoted
as?k(yi, yi?1, x, i), where i stands for the position in
the sequence and k stands for the number of feature
templates.
we use online Passive-Aggressive (PA) algorithm
(Crammer and Singer, 2003; Crammer et al 2006)
to train the model parameters. Following (Collins,
2002), the average strategy is used to avoid the over-
fitting problem.
3 Dynamic Features
The form of traditional features is shown in Table
2, where C represents a Chinese character, and T
represents the character-based tag. The subscript i
indicates its position related to the current character.
Table 2: Traditional Feature Templates
Ci, T0(i = ?2,?1, 0, 1, 2)
Ci, Cj , T0(i, j = ?2,?1, 0, 1, 2 and i ?= j)
T?1, T0
Traditional features are generated by position-
fixed templates. Since the length of Chinese word
is unfixed, their meanings are incomplete. We cat-
egorize them as ?static? features since they can be
calculated before tagging (except ?T?1, T0?).
The form of dynamic features is shown in Table
3, where WORD represents a Chinese word, and
POS (LEN ) is the POS tag (length) of the word.
The subscript of dynamic feature template indicates
its position related to the current word.
Table 4 shows an example. If the current posi-
tion is ? Apple?, then {POS?1=CC, POS?2=NR,
WORD?1=???, LEN?2=2}. Since these features
are unavailable before tagging, we call them ?dy-
namic? features.
1380
Table 3: Examples of Dynamic Feature Templates
POSi, POSj , T0(i, j = ?2,?1, 0 and i ?= j)
POSi,WORDj , T0(i, j = ?2,?1, 0)
WORDi, LENj , POSk, T0(i, j, k = ?2,?1, 0)
?
Dynamic features are more flexible because the
number of involved characters is dependent on the
length of previous words. Unlike static features, dy-
namic features do not merely rely on the input se-
quence C1:n, so the weights of dynamic features, in
which POS/LEN are involved, can be trained by
Chinese-only texts and used by mixed texts, which
resolve the problem of the lack of training data.
4 Tagging with Dynamic Features
In the tagging stage, we use the current best result
to approximately calculate the unknown tag infor-
mation. For an input sequence C1:n, the current best
tags from index 0 to i?1 can be calculated by Viterbi
algorithm and they can be used to generate dynamic
features for index i. The specific algorithm is shown
in Algorithm 1.
Here is an example to explain the time com-
plexity of the dynamic features. Normal template
xi?2xi?1yi requires to look for the positions of
i ? 2 and i ? 1 related to the current character
xi, but dynamic template posi?2posi?1yi needs to
know the pos tags of two words. If the length of
wordi?1/wordi?2 is 2, then the positions of i?4, i?
3, i?2, i?1 are needed to generate the dynamic fea-
tures.
For all dynamic features, it is unnecessary to
repetitively calculate the POS/WORD/LEN ar-
ray. Apart from that one time calculation of the ar-
ray, no distinction can be found between the time
complexity of the dynamic features and the tradi-
tional features. For input C1:n, the time complexity
isO(n?[O(op.2)+(Ts.num+Td.num)?O(op.1)+
O(op.4)]), n.b. O(op.1) = O(op.3). Universally
the dynamic features only require the information of
position i ? 2 and i ? 1, so the time complexity of
calculating the POS/WORD/LEN array can be
ignored as compared with the complexity of Viterbi
algorithm and feature extraction. The approximate
algorithm is thus faster than the Brute-Force way by
input : character sequence C1:n
static templates Ts
dynamic templates Td
number of labelsm
trans matrixM
output: resultsMax & Vp
Initialize: weight matrixW (n?m)
viterbi score matrix Vs (n?m)
viterbi path matrix Vp (n?m)
the index of current best labelMax
for i = 1 ? ? ?n do
for ts in Ts do
// create feature string Fs (Op.1)
Fs = createFeature(C1:n, ts);
W [i] += getWeightVector(Fs);
end
// create a list of <posk,wordk,lenk>
// (k = 0,?1,?2 . . .) (Op.2)
dList = getCurrentBestPath(Max, Vp);
for td in Td do
// create dynamic features string Fd
// (Op.3)
Fd = createFeature(C1:n, td, dList);
W [i] += getWeightVector(Fd);
end
// Update Vs[i], Vp[i] (Op.4)
viterbi_OneStep(Vs[i? 1],W [i],M );
Max = argmaxi(Vs[i]) ;
end
Algorithm 1: Tagging Algorithm with Dynamic
Features
using word-level information.
5 Training
Given an example (x, y), y? are denoted as the in-
correct labels with the highest score
y? = argmax
z ?=y
wT?(x, z). (2)
The margin ?(w; (x, y)) is defined as
?(w; (x, y)) = wT?(x, y)? wT?(x, y?). (3)
Thus, we calculate the hinge loss ?(w; (x, y), (ab-
breviated as ?w) by
1381
Table 4: Example for Chinese-English Mixed POS Tagging
? ? ? Apple ? OS ? ? ? ? ?
B-NR E-NR S-CC S-NR S-DEG S-NN B-NN E-NN B-VA E-VA S-PU
?w =
{
0, ?(w; (x, y)) > 1
1? ?(w; (x, y)), otherwise (4)
In round k, the new weight vector wk+1 is calcu-
lated by
wk+1 = argminw
1
2
||w? wk||2 + C ? ?,
s.t. ?(w; (xk, yk)) <= ? and ? >= 0 (5)
where ? is a non-negative slack variable, and C is
a positive parameter which controls the influence of
the slack term on the objective function.
Following the derivation in PA (Crammer et al
2006), we can get the update rule,
wk+1 = wk + ?k(?(xk, yk)? ?(xk, y?k)), (6)
where
?k = min(C,
?wk
??(xk, yk)? ?(xk, y?k)?2
) (7)
Our algorithm based on PA algorithm is shown in
Algorithm 2.
6 Experiments
We implement our system based on FudanNLP1.
We employ the commonly used label set {B, I, E,
S} for the segmentation part of cross-labels. {B,
I, E} represent Begin, Inside, End of a multi-node
segmentation respectively, and S represents a Single
node segmentation.
The F1 score is used for evaluation, which is the
harmonic mean of precision P (percentage of pre-
dict phrases that exactlymatch the reference phrases)
and recallR (percentage of reference phrases that re-
turned by system).
The feature templates, which are used to extract
features, are listed in Table 5. We set traditional
method (static features) as the baseline. The detailed
experimental settings and results are reported in the
following subsections.
1Available at http://code.google.com/p/fudannlp/
input : training data sets:
(xi, yi), i = 1, ? ? ? , N , and parameters:
C,K
output: wK
Initialize: wTemp? 0,w? 0;
for k = 0 ? ? ?K ? 1 do
for i = 1 ? ? ?N do
receive an example (xi, yi);
predict: y?i = argmax
y
?wk,?(xi, y)?;
if y?i ?= yi then
update wk+1 with Eq. 6;
end
end
wTemp = wTemp+ wk+1 ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
Table 5: Feature Templates
Static
xi?2yi, xi?1yi, xiyi, xi+1yi, xi+2yi
xi?1xiyi, xi+1xiyi, xi?1xi+1yi,
yi?1yi
Dynamic
posi?2posi?1yi, posi?1posiyi
posi?2wordi?1yi, posi?1wordiyi
posi?1wordi?1yi, posiwordiyi
wordi?2wordi?1yi, wordi?1wordiyi
wordileniyi
6.1 POS Tagging for Chinese-only Texts
Before the experiments onChinese-Englishmixed
texts, we evaluate the performance of our method on
Chinese-only texts. We use the CTB dataset from
the POS tagging task of the Fourth International Chi-
nese Language Processing Bakeoff (SIGHAN Bake-
off 2008)(Jin and Chen, 2008). The details are
shown in Table 6.
The performance comparison on joint segmenta-
tion & POS tagging is shown in Table 7. Our method
obtains an error reduction of 6.7% over the baseline.
The reason is that our dynamic features can utilize
1382
Table 6: POS Tagging Dataset in SIGHAN Bakeoff 2008
Train Set Test Set
(number) (number)
Sentence 23444 2079
Word
Total 642246 59955
NN 168896 16793
NR 42906 3970
VV 92887 8641
VA 9106 649
JJ 15640 1581
word-level information effectively and the feature
templates are more flexible.
Table 7: Performances of POS Tagging on Chinese-only
Texts with Static and Dynamic Features
Method P R F1
Baseline 89.68 89.60 89.64
Our 90.35 90.31 90.33
6.2 POS Tagging for Chinese-English Mixed
Texts
Without annotated corpus for Chinese-English
mixed texts, we use synthetic data as the alternative.
In Chinese-English mixed texts, English words of
noun(NN/NR), verb(VV/VA) and adjective(JJ) cat-
egories are the most commonly used, so we ran-
domly transform a certain percentage of Chinese
words with these POS tags in the SIGHAN Bakeoff
2008 dataset(Jin and Chen, 2008) into their English
counterparts.
6.2.1 Synthetic Data
Before trying out an experiment, we first study
how to generate the data of mixed texts.
We use two ways to produce the synthetic data:
?Respective Replacement? and ?Unified Replace-
ment?.
Respective Replacement We replace the selected
Chinese words into their corresponding English
counterparts.
Unified Replacement We replace the selected Chi-
nese words with a unified labelENG. The rea-
son we use the labelENG instead of real words
is that we want to consider the context of these
words but not the words themselves and over-
come the problem of out-of-vocabulary (OOV)
English words.
For our experiments, we just select 5% of the Chi-
nese nouns and verbs from SIGHAN dataset, and re-
place them in the above two ways. After replace-
ment, the training and test data have 12780 and 1254
English words, respectively. 5189 words are gener-
ated by way of ?Respective Replacement?. In the
test data, 326words are OOV, which comprises 25%
of the whole vocabulary. The information of gener-
ated data is shown in Table 8.
Table 8: The Synthetic Chinese-English Mixed Dataset
H
Dataset Numbers of ENGNN VV
H Train Set 8191 4589Test Set 842 412
We use H1 to represent the dataset generated by
way of ?Respective Replacement?, and H2 for the
dataset by way of with ?Unified Replacement?. The
experimental results on these two datasets are shown
in Table 9.
Table 9: Performances of POS Tagging on Dataset H1
and H2
Method Dataset ENG OOV TotalF1 F1oov F1
Baseline H1 73.60 54.91 88.93H2 77.59 73.93 89.11
Our H1 75.60 54.60 89.79H2 79.82 77.61 89.81
From Table 9, we can see that the ?Unified
Replacement? way is better than the ?Respective
Replacement? way for both the baseline and our
method. The main reason is that the ?Unified Re-
placement? way can greatly improve the tagging per-
formance of OOV words.
6.2.2 Detail Comparisons
For detail comparisons of all situations of
mixed texts, we design six synthetic datasets,
A/B/C/D1/D2/E by randomly selecting 10% or
15% of Chinese words (?NN/NR/VV/VA/JJ?) in the
1383
above SIGHANBakeoff 2008 dataset, and replacing
them with English label ENG.
The differences of these datasets are as following:
? Dataset A only contains English words with
tags ?NN/VV?.
? Dataset B contains English words with tags
?NN/VV/VA?.
? Dataset C contains one more tag ?NR? than
Dataset B.
? Datasets D1 and D2 contain one more tag ?JJ?
than Dataset B. The difference between D1
andD2 is thatD2 has about 50%more English
words than D1 in training set.
? Dataset E contains English words with all the
tags ?NN/NR/VV/VA/JJ?.
The detailed information of datasets
A/B/C/D1/D2/E is shown in Table 10.
Table 10: The Synthetic Chinese-English Mixed Dataset
Dataset Numbers of ENGNN NR VV VA JJ
A Train 16302 0 9007 0 0Test 1675 0 841 0 0
B Train 16116 0 8882 906 0Test 1573 0 830 58 0
C Train 16312 4057 9067 899 0Test 1549 400 795 61 0
D1
Train 16042 0 8957 855 1539
Test 1588 0 845 58 150
D2
Train 23705 0 13154 1300 2211
Test 1588 0 845 58 150
E Train 16066 4162 9156 886 1547Test 1647 415 809 57 141
The results are shown in Table 11. On dataset E,
our method achieves 6.78% higher performance on
tagging ENG labels than traditional static features.
This result is reasonable because our model can use
more flexible feature templates to extract features
and reduce the problem of being dependent on spe-
cific English words.
Tables 12/13/14/15/16/17 show the detailed re-
sults on datasets A/B/C/D1/D2/E.
Table 11: Performances of POS Tagging on Datasets
A/B/C/D1/D2/E
Dataset Method ENG labels TotalF1 F1
A Baseline 80.25 88.74Our 83.03 89.72
B Baseline 76.72 88.51Our 80.54 89.55
C Baseline 68.16 88.13Our 70.34 88.99
D1
Baseline 71.30 88.33
Our 74.02 89.15
D2
Baseline 69.59 88.09
Our 74.10 89.15
E Baseline 61.58 87.71Our 68.36 88.83
Experiment on dataset A gets the best result be-
cause ?NN? and ?VV? can be easily distinguished by
its context. Sometimes, ?VA? has the similar context
with ?VV?, experiment on datasetB shows its influ-
ence. The performances on datasetsB/C/E descend
in turn. The reason is that words with tag ?NN? or
?NR/JJ? have the similar usage/contexts in Chinese.
Since we use the same form ENG instead of real
words, there are no differences between these words,
which leads to some errors. Though the datasets is
generated randomly, we can see our method perform
better on every dataset than the baseline.
Table 12: Performances on Dataset A
POS tag Method P R F1
NN Baseline 84.36 86.33 85.33Our 85.37 89.91 87.58
VV Baseline 71.45 68.13 69.75Our 77.53 69.32 73.20
Table 13: Performances on Dataset B
POS tag Method P R F1
NN Baseline 84.89 80.36 82.56Our 83.51 88.87 86.11
VV Baseline 65.90 72.65 69.11Our 75.75 67.35 71.30
VA Baseline 36.84 36.21 36.52Our 51.02 43.10 46.73
1384
Table 14: Performances on Dataset C
POS tag Method P R F1
NN Baseline 73.77 78.24 75.94Our 76.84 77.99 77.41
VV Baseline 61.67 66.79 64.13Our 64.94 67.80 66.34
NR Baseline 55.22 37.00 44.31Our 55.65 50.50 52.95
VA Baseline 63.64 34.43 44.68Our 60.00 39.34 47.52
Table 15: Performances on DatasetD1
POS tag Method P R F1
NN Baseline 77.15 81.42 79.23Our 76.70 88.54 82.20
VV Baseline 67.53 64.50 65.98Our 79.65 59.76 68.29
JJ Baseline 25.00 18.00 20.93Our 22.92 14.67 17.89
VA Baseline 36.00 31.03 33.33Our 28.57 37.93 32.59
Table 16: Performances on DatasetD2
POS tag Method P R F1
NN Baseline 79.11 74.87 76.93Our 79.29 82.68 80.95
VV Baseline 55.77 72.78 65.64Our 69.17 70.89 70.02
JJ Baseline 27.27 12.00 16.67Our 34.38 22.00 26.83
VA Baseline 37.21 27.59 31.68Our 52.17 20.69 29.63
6.3 POS Tagging for Mixed Texts with a Real
Dataset
To investigate the actual performance, we collect
a real dataset from Web, which consists of 142 rep-
resentative Chinese-English mixed sentences. This
dataset contains 4, 238 Chinese characters and 275
English words. Since we focus on the performance
for English words, we only label the POS tags of the
English words. Table 18 shows some examples in
the real dataset of mixed texts.
Table 17: Performances on Dataset E
POS tag Method P R F1
NN Baseline 72.41 68.85 70.59Our 71.18 84.88 77.43
VV Baseline 63.65 59.09 61.28Our 76.19 55.38 64.14
JJ Baseline 28.57 25.53 26.97Our 30.21 20.57 24.47
VA Baseline 44.83 45.61 45.22Our 60.42 50.88 55.24
NR Baseline 38.03 52.05 43.95Our 52.01 46.75 49.24
Table 18: Examples in Real Dataset of Mixed Texts
?? [Ninja Cloud/NR] ????? [Ninja
Blocks/NR] ? ? [Facebook/NR]? [Twit-
ter/NR]?[Dropbox/NR]??????
By using [Ninja Cloud/NR], [Ninja
Blocks/NR] can connect to [Facebook/NR],
[Twitter/NR], [Dropbox/NR].
?? [follow/VV]?????????
You should [follow/VV] this man?s work.
?????????? [COOL/VA]?
... very [COOL/VA]!
The information of the real dataset is shown in Ta-
ble 19. If all involved English words are tagging as
?NN?, the precision is just 56%.
Table 19: The Numbers of English Words with Different
Tags in Dataset R
Dataset NN VV VA NR
R 154 58 28 35
Since there is no noun-modifier ?JJ? in our col-
lected data. We use the models trained on dataset
B and C to tag the real data. The results are shown
in Table 20. The difference between model B and
C is that model B regards all words with tag ?NR?
as ?NN?. Since it is difficult to distinguish between
?NR? and ?NN? merely according to the context,
model B performs better than model C.
The detail results of model B and C are shown in
Table 21 and 22.
1385
Table 20: Performances of POS Tagging on R
Model Method ENGF1
B Baseline 74.91Our 82.55
C Baseline 70.91Our 74.91
Table 21: Performances of Model B on Dataset R
POS tag Method P R F1
NN Baseline 88.62 78.31 83.15Our 91.67 87.30 89.43
VV Baseline 48.31 74.14 58.50Our 60.53 79.31 68.66
VA Baseline 78.95 53.57 63.83Our 84.21 57.14 68.09
Table 22: Performances of Model C on Dataset R
POS tag Method P R F1
NN Baseline 80.25 81.82 81.03Our 84.56 81.82 83.17
VV Baseline 54.88 77.59 64.29Our 61.25 84.48 71.01
VA Baseline 84.62 39.29 53.66Our 88.24 53.57 66.67
NR Baseline 56.52 37.14 44.83Our 55.17 45.71 50.00
7 Related Works
In recent years, POS tagging has undergone great
development. The mainstream method is to regard
POS tagging as sequence labeling problems (Ra-
biner, 1990; Xue, 2003; Peng et al 2004; Ng and
Low, 2004).
However, the analysis of Chinese-English mixed
texts is rarely involved in previous literature. In
the aspect of the general multilingual POS tagging,
most works focus on modeling cross-lingual corre-
lations and tagging multilingual POS on respective
monolingual texts, not on mixed texts (Cucerzan and
Yarowsky, 2002; Yarowsky et al 2001; Naseem et
al., 2009).
Since we choose to use dynamic word-level fea-
tures to improve the performance of POS tagging,
we also review some works on word-level features.
Semi-Markov Conditional Random Fields (semi-
CRF) (Sarawagi and Cohen, 2004) is a model in
which segmentation task is implicitly included into
the decoding algorithm. In this model, feature rep-
resentation would be more flexible than traditional
CRFs, since features can be extracted from the previ-
ous/the next segmentation within a window of vari-
able size. The problem of this approach lies in that
the decoding algorithm depends on the predefined
window size to exploit the boundaries of segmenta-
tions but not the real length of words.
Bunescu (2008) presents an improved pipeline
model in which the output of the previous subtasks
are considered as hidden variables, and the hidden
variables together with their probabilities denoting
the confidence are used as probabilistic features in
the next subtasks. One shortcoming of this method
is inefficiency caused by the calculation of marginal
probabilities of features. The other disadvantages
of the pipeline method are error propagation and the
need of separate training of different subtasks in the
pipeline. Another disadvantage of pipeline method
is error propagation.
Jiang et al(2008) proposes a cascaded linear
model for joint Chinese word segmentation and POS
tagging. With a character-based perceptron as the
core, combinedwith real-valued features such as lan-
guage models, the cascaded model can efficiently
utilize knowledge sources that are inconvenient to
incorporate into the perceptron directly. However,
they use POS tags or word information in a Brute-
Force way, which may suffer from the problem of
time complexity.
Sun (2011) presents a stacked sub-word model for
joint Chinese word segmentation and POS tagging.
By merging the outputs of the three predictors (in-
cluding one word-based segmenter) into sub-word
sequences, rich contextual features can be approx-
imately derived. The experiments are conducted to
show the effectiveness of using word-based informa-
tion.
The difference between the above methods and
ours is that our word-level features are dynamically
generated in the decoding stage without exhaustive
or preprocessed word segmentation.
1386
8 Conclusion
In this paper, we focus on Chinese-English mixed
texts and use dynamic features for POS tagging.
To overcome the problem of the lack of annotated
corpus on mixed texts, our features use both lo-
cal and non-local information and take advantage of
the characteristics of Chinese-English mixed texts.
The experiments demonstrate the effectiveness of
our method. It should be noted that our method is
also effective for the mixed texts of Chinese and any
foreign languages since we use ?Unified Replace-
ment?.
For future works, we plan to improve our approx-
imate tagging algorithm to reduce error propagation.
In addition, we will refer to an English dictionary
to generate some useful features to distinguish be-
tween ?NR? and ?NN? in Chinese-English mixed
texts and add some statistical features derived from
English resources, such as the most common tag of
each English word. We would also like to investi-
gate these features in more applications of natural
language processing, such as name entity recogni-
tion, information extraction, etc.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable comments. We also thanks Amy
Zhou for her help in spell and grammar checking.
This work was funded by NSFC (No.61003091 and
No.61073069), 863 Program (No.2011AA010604)
and 973 Program (No.2010CB327900).
References
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In EMNLP,
pages 670?679. ACL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
1?8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. J. Mach. Learn. Res.,
7:551?585, December.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In proceedings of the 6th conference on
Natural language learning - Volume 20, COLING-
02, pages 1?7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Kath-
leen McKeown, Johanna D. Moore, Simone Teufel,
James Allan, and Sadaoki Furui, editors, ACL, pages
897?904. The Association for Computer Linguistics.
C. Jin and X. Chen. 2008. The fourth international chi-
nese language processing bakeoff: Chinese word seg-
mentation, named entity recognition and chinese pos
tagging. In Sixth SIGHAN Workshop on Chinese Lan-
guage Processing, page 69.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two unsu-
pervised approaches. Journal of Artificial Intelligence
Research, 36(1):341?385.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based. In Proceedings of EMNLP, volume
2004, page 277.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Lawrence R. Rabiner. 1990. Readings in speech recog-
nition. chapter A tutorial on hidden Markov mod-
els and selected applications in speech recognition,
pages 267?296. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Dekang Lin, Yuji Matsumoto, and Rada
Mihalcea, editors, ACL, pages 1385?1394. The Asso-
ciation for Computer Linguistics.
F. Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0).
N. Xue. 2003. Chinese word segmentation as character
tagging. Computational Linguistics and Chinese Lan-
guage Processing, 8(1):29?48.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of
1387
the first international conference on Human language
technology research, pages 1?8. Association for Com-
putational Linguistics.
1388
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658?668,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Chinese Word Segmentation and POS Tagging on
Heterogeneous Annotated Corpora with Multiple Task Learning
Xipeng Qiu, Jiayi Zhao, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, zjy.fudan@gmail.com, xjhuang@fudan.edu.cn
Abstract
Chinese word segmentation and part-of-
speech tagging (S&T) are fundamental
steps for more advanced Chinese language
processing tasks. Recently, it has at-
tracted more and more research interests
to exploit heterogeneous annotation cor-
pora for Chinese S&T. In this paper, we
propose a unified model for Chinese S&T
with heterogeneous annotation corpora.
We first automatically construct a loose
and uncertain mapping between two rep-
resentative heterogeneous corpora, Penn
Chinese Treebank (CTB) and PKU?s Peo-
ple?s Daily (PPD). Then we regard the
Chinese S&T with heterogeneous corpora
as two ?related? tasks and train our model
on two heterogeneous corpora simultane-
ously. Experiments show that our method
can boost the performances of both of the
heterogeneous corpora by using the shared
information, and achieves significant im-
provements over the state-of-the-art meth-
ods.
1 Introduction
Currently, most of statistical natural language
processing (NLP) systems rely heavily on manu-
ally annotated resources to train their statistical
models. The more of the data scale, the better
the performance will be. However, the costs are
extremely expensive to build the large scale re-
sources for some NLP tasks. Even worse, the ex-
isting resources are often incompatible even for a
same task and the annotation guidelines are usu-
ally different for different projects, since there
are many underlying linguistic theories which
explain the same language with different per-
spectives. As a result, there often exist multi-
ple heterogeneous annotated corpora for a same
task with vastly different and incompatible an-
notation philosophies. These heterogeneous re-
sources are waste on some level if we cannot fully
exploit them.
However, though most of statistical NLP
methods are not bound to specific annota-
tion standards, almost all of them cannot deal
simultaneously with the training data with
different and incompatible annotation. The
co-existence of heterogeneous annotation data
therefore presents a new challenge to utilize
these resources.
The problem of incompatible annotation stan-
dards is very serious for many tasks in NLP,
especially for Chinese word segmentation and
part-of-speech (POS) tagging (Chinese S&T). In
Chinese S&T, the annotation standards are of-
ten incompatible for two main reasons. One is
that there is no widely accepted segmentation
standard due to the lack of a clear definition
of Chinese words. Another is that there are no
morphology for Chinese word so that there are
many ambiguities to tag the parts-of-speech for
Chinese word. For example, the two commonly-
used corpora, PKU?s People?s Daily (PPD) (Yu
et al, 2001) and Penn Chinese Treebank (CTB)
(Xia, 2000), use very different segmentation and
POS tagging standards.
For example, in Table 1, it is very different
to annotate the sentence ?????????
?? (Liu Xiang reaches the national final in
China)? with guidelines of CTB and PDD. PDD
breaks some phrases, which are single words in
658
Liu Xiang reachs China final
CTB ??/NR ??/VV ???/NN ???/NN
PDD ?/nrf ?/nrg ??/v ??/ns ?/n ?/b ??/vn
Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD
CTB, into two words. The POS tagsets are also
significantly different. For example, PDD gives
diverse tags ?n? and ?vn? for the noun, while
CTB just gives ?NN?. For proper names, they
may be tagged as ?nr?, ?ns?, etc in PDD, while
they are just tagged as ?NR? in CTB.
Recently, it has attracted more and more re-
search interests to exploit heterogeneous anno-
tation data for Chinese word segmentation and
POS tagging. (Jiang et al, 2009) presented a
preliminary study for the annotation adapta-
tion topic. (Sun and Wan, 2012) proposed a
structure-based stacking model to fully utilize
heterogeneous word structures. They also re-
ported that there is no one-to-one mapping be-
tween the heterogeneous word classification and
the mapping between heterogeneous tags is very
uncertain.
These methods usually have a two-step pro-
cess. The first step is to train the preliminary
taggers on heterogeneous annotations. The sec-
ond step is to train the final taggers by using
the outputs of the preliminary taggers as fea-
tures. We call these methods as ?pipeline-
based? methods.
In this paper, we propose a method for joint
Chinese word segmentation and POS tagging
with heterogeneous annotation corpora. We re-
gard the Chinese S&T with heterogeneous cor-
pora as two ?related? tasks which can improve
the performance of each other. Since it is impos-
sible to establish an exact mapping between two
annotations, we first automatically construct a
loose and uncertain mapping the heterogeneous
tagsets of CTB and PPD. Thus we can tag a sen-
tence in one style with the help of the ?related?
information in another heterogeneous style. The
proposed method can improve the performances
of joint Chinese S&T on both corpora by using
the shared information of each other, which is
proven effective by experiments.
There are three main contributions of our
model:
? First, we regard these two joint S&T tasks
on different corpora as two related tasks
which have interdependent and peer rela-
tionship.
? Second, different to the pipeline-based
methods, our model can be trained simul-
taneously on the heterogeneous corpora.
Thus, it can also produce two different
styles of POS tags.
? Third, our model do not depend on the
exactly correct mappings between the two
heterogeneous tagsets. The correct map-
ping relations can be automatically built in
training phase.
The rest of the paper is organized as follows:
We first introduce the related works in section 2
and describe the background of character-based
method for joint Chinese S&T in section 3. Sec-
tion 4 presents an automatic method to build
the loose mapping function. Then we propose
our method on heterogeneous corpora in 5 and
6. The experimental results are given in section
7. Finally, we conclude our work in section 8.
2 Related Works
There are some works to exploit heteroge-
neous annotation data for Chinese S&T.
(Gao et al, 2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
However, this converter need human designed
transformation templates, and is hard to be gen-
eralized to POS tagging.
(Jiang et al, 2009) proposed an automatic
adaptation method of heterogeneous annotation
standards, which depicts a general pipeline to in-
tegrate the knowledge of corpora with different
659
TaggerPPD
TaggerCTB
Input: x
Output: f(x)
Output: CTB-style Tags
z=f(x)
y=h(x,f(x))
Figure 1: Traditional Pipeline-based Strategy for
Heterogeneous POS Tagging
underling annotation guidelines. They further
proposed two optimization strategies, iterative
training and predict-self re-estimation, to fur-
ther improve the accuracy of annotation guide-
line transformation (Jiang et al, 2012).
(Sun and Wan, 2012) proposed a structure-
based stacking model to fully utilize heteroge-
neous word structures.
These methods regard one annotation as the
main target and another annotation as the com-
plementary/auxiliary purposes. For example, in
their solution, an auxiliary tagger TaggerPPD
is trained on a complementary corpus PPD, to
assist the target CTB-style TaggerCTB. To re-
fine the character-based tagger, PPD-style char-
acter labels are directly incorporated as new
features. The brief sketch of these methods is
shown in Figure 1.
The related work in machine learning liter-
ature is multiple task learning (Ben-David and
Schuller, 2003), which learns a problem together
with other related problems at the same time,
using a shared representation. This often leads
to a better model for the main task, because
it allows the learner to use the commonality
among the tasks. Multiple task learning has
been proven quite successful in practice and has
been also applied to NLP (Ando and Zhang,
2005). We also preliminarily verified that mul-
tiple task learning can improve the performance
on this problem in our previous work (Zhao et
al., 2013), which is a simplified case of the work
in this paper and has a relative low complexity.
Different with the multiple task learning,
whose tasks are actually different labels in the
same classification task, our model utilizes the
shared information between the real different
tasks and can produce the corresponding differ-
ent styles of outputs.
3 Joint Chinese Word Segmentation
and POS Tagging
Currently, the mainstream method of Chi-
nese POS tagging is joint segmentation & tag-
ging with character-based sequence labeling
models(Lafferty et al, 2001), which can avoid
the problem of segmentation error propagation
and achieve higher performance on both sub-
tasks(Ng and Low, 2004; Jiang et al, 2008; Sun,
2011; Qiu et al, 2012).
The label of each character is the cross-
product of a segmentation label and a tagging
label. If we employ the commonly used label set
{B, I, E, S} for the segmentation part of cross-
labels ({B, I, E} represent Begin, Inside, End of
a multi-node segmentation respectively, and S
represents a Single node segmentation), the la-
bel of character can be in the form of {B-T}(T
represents POS tag). For example, B-NN indi-
cates that the character is the begin of a noun.
4 Automatically Establishing the
Loose Mapping Function for the
Labels of Characters
To combine two human-annotated corpora,
the relationship of their guidelines should be
found. A mapping function should be estab-
lished to represent the relationship between two
different annotation guidelines. However, the
exact mapping relations are hard to establish.
As reported in (Sun and Wan, 2012), there is
no one-to-one mapping between their heteroge-
neous word classification, and the mapping be-
tween heterogeneous tags is very uncertain.
Fortunately, there is a loose mapping
can be found in CTB annotation guide-
line1 (Xia, 2000). Table 2 shows some
1Available at http://www.cis.upenn.edu/ ?chi-
660
CTB?s Tag PDD? Tag1
Total tags 33 26
verbal noun NN v[+nom]
proper noun NR n
? (shi4) VC v
? (you3) VE, VV v
conjunctions CC, CS c
other verb VV, VA v, a, z
number CD, OD m
1 The tag set of PDD just includes the 26 broad
categories in the mapping table. The whole tag set
of PDD has 103 sub categories.
Table 2: Examples of mapping between CTB and
PDD?s tagset
mapping relations in CTB annotation guide-
line. These loose mapping relations are
many-to-many mapping. For example, the
mapping may be ?NN/CTB?{n,nt,nz}/PDD?,
?NR/CTB?{nr,ns}/PDD?, ?v/PDD?{VV,
VA}/CTB? and so on.
We define T1 and T2 as the tag sets for two
different annotations, and t1 ? T1 and t2 ? T2
are the corresponding tags in two tag sets re-
spectively.
We first establish a loose mapping function
m : T1 ? T2 ? {0, 1} between the tags of CTB
and PDD.
m(t1, t2) =
{
1 if t1 and t2 have mapping relation
0 else
(1)
The mapping relations are automatically
build from the CTB guideline (Xia, 2000). Due
to the fact that the tag set of PPD used in
the CTB guideline is just broad categories, we
expand the mapping relations to include the
sub categories. If a PPD?s tag is involved
in the mapping, all its sub categories should
be involved. For example, for the mapping
?NR/CTB?nr/PDD?, the relation of NR and
nrf/nrg should be added in the mapping rela-
tions too (nrf/nrg belong to nr).
Since we use the character-based joint S&T
model, we also need to find the mapping func-
tion between the labels of characters.
nese/posguide.3rd.ch.pdf
In this paper, we employ the commonly used
label set {B, I, E, S} for the segmentation part
of cross-labels and the label of character can be
in the form of {B-T}(T represents POS tag).
Thus, each mapping relation t1 ? t2 can be
automatically transformed to four forms: B-
t1 ?B-t2, I-t1 ?I-t2, E-t1 ?E-t2 and S-t1 ?S-
t2. (?B-NR/CTB?{B-nr,B-ns}/PPD? for ex-
ample).
Beside the above transformation, we also
give a slight modification to adapt the dif-
ferent segmentation guidelines. For in-
stance, the person name ??? (Mo Yan)?
is tagged as ?B-NR, E-NR? in CTB but
?S-nrf, S-nrg? in PPD. So, some spe-
cial mappings may need to be added like
?B-NR/CTB?S-nrf/PPD?, ?E-NR/CTB?{S-
nrg, E-nrg}/PPD?, ?M-NR/CTB?{B-nrg, M-
nrg}/PPD? and so on. Although these spe-
cial mappings are also established automatically
with an exhaustive solution. In fact, we give seg-
mentation alignment only to proper names due
to the limitation of computing ability.
Thus, we can easily build the loose bidirec-
tional mapping function m? for the labels of
characters. An illustration of our construction
flowchart is shown in Figure 2.
Finally, total 524 mappings relationships are
established.
5 Joint Chinese S&T with
Heterogeneous Data with Multiple
Task Learning
Inspired by the multiple task learning (Ben-
David and Schuller, 2003), we can regard the
joint Chinese S&T with heterogeneous data as
two ?related? tasks, which can improve the
performance of each other simultaneously with
shared information.
5.1 Sequence Labeling Model
We first introduce the commonly used se-
quence labeling model in character-based joint
Chinese S&T.
Sequence labeling is the task of assigning la-
bels y = y1, . . . , yn(yi ? Y) to an input sequence
x = x1, . . . , xn. Y is the set of labels.
661
PPD-style
CTB-style NR
nr
NR
nrf nrg
B-NR S-NR...
B-nrf B-nrg S-nrgS-nrg...
mapping function m() 
between tags
mapping function m() 
between labels
~
Figure 2: An Illustration of Automatically Establishing the Loose Mapping Function
Given a sample x, we define the feature
?(x,y). Thus, we can label x with a score func-
tion,
y? = arg max
y
S(w,?(x,y)), (2)
where w is the parameter of score function S(?).
The feature vector ?(x,y) consists of lots of
overlapping features, which is the chief benefit of
discriminative model. Different algorithms vary
in the definition of S(?) and the corresponding
objective function. S(?) is usually defined as lin-
ear or exponential family function.
For first-order sequence labeling, the feature
can be denoted as ?k(x, yi?1:i), where i stands
for the position in the sequence and k stands for
the number of feature templates. For the linear
classifier, the score function can be rewritten in
detail as
y? = arg max
y
L
?
i=1
(?u, f(x, yi)?+ ?v,g(x, yi?1:i)?) ,
(3)
where yi:j denotes label subsequence
yiyi+1 ? ? ? yj ; f and g denote the state and
transition feature vectors respectively, u and v
are their corresponding weight vectors; L is the
length of x.
5.2 The Proposed Model
Different to the single task learning, the het-
erogeneous data have two sets of labels Y and
Z.
The heterogeneous datasets Ds and Ds con-
sist of {xi,yi}(i = 0, ? ? ? ,m) and {xi, zi}(i =
0, ? ? ? , n) respectively.
For a sequence x = x1, . . . , xL with length
L. , there may have two output sequence labels
y = y1, . . . , yL and z = z1, . . . , zL, where yi ? Y
and zi ? Z.
We rewrite the loose mapping function m? be-
tween two label sets into the following forms,
?(y) = {z|m?(y, z) = 1}, (4)
?(z) = {y|m?(y, z) = 1}, (5)
where ?(z) ? Y and ?(y) ? Z are the subsets
of Y and Z. Give a label y(or z) in an annota-
tion, the loose mapping function ? returns the
corresponding mapping label set in another het-
erogeneous annotation.
Our model for heterogeneous sequence label-
ing can be write as
y? = arg max
y,yi?Y
L
?
i=1
(
?u, f(x, yi)?
+ ?s,
?
z??(yi)
h(x, z)?
+ ?v1,g1(x, yi?1:i)?
+ ?v2,
?
zi?1??(yi?1)
zi??(yi)
g2(x, zi?1:i)?
)
, (6)
and
z? = arg max
z,zi?Z
L
?
i=1
(
?u,
?
y??(zi)
f(x, y)?+
?s,h(x, zi)?
+ ?v1,
?
yi?1??(zi?1)
yi??(zi)
g1(x, yi?1:i)?
+ ?v2,g2(x, zi?1:i)?
)
, (7)
where f and h represent the state feature vectors
on two label sets Y and Z respectively.
In Eq.(6) and (7), the score of the label of
every character is decided by the weights of the
corresponding mapping labels and itself.
662
Input sequence: x
Output: PPD-style Tags
TaggerPPD TaggerCTBSharedInformation
Output: CTB-style Tags
Figure 3: Our model for Heterogeneous POS Tagging
The main challenge of our model is the effi-
ciency of decoding algorithm, which is similar to
structured learning with latent variables(Liang
et al, 2006) (Yu and Joachims, 2009). Most
methods for structured learning with latent vari-
ables have not expand all possible mappings.
In this paper, we also only expand the map-
ping that with highest according to the current
model.
Our model is shown in Figure 3 and the
flowchart is shown in Algorithm 1. If given the
output type of label T , we only consider the la-
bels in T to initialize the Viterbi matrix, and
the score of each node is determined by all the
involved heterogeneous labels according to the
loose mapping function.
input : character sequence x1:L
loose mapping function ?
output type: T (T ? {Ty, Tz})
output: label sequence ls
if T == Ty then
calculate ls using Eq. (6);
else if T == Tz then
calculate ls using Eq. (7) ;
else
return null;
end
return ls
Algorithm 1: Flowchart of the Tagging pro-
cess of the proposed model
6 Training
We use online Passive-Aggressive (PA) algo-
rithm (Crammer and Singer, 2003; Crammer et
al., 2006) to train the model parameters. Fol-
lowing (Collins, 2002), the average strategy is
used to avoid the overfitting problem.
For the sake of simplicity, we merge the Eq.(6)
and (7) into a unified formula.
Given a sequence x and the expect type of
tags T , the merged model is
y? = arg max
y
t(y)=T
?w,
?
z??(y)
?(x, z)?, (8)
where t(y) is a function to judge the type of
output tags; ?(y) represents the set {?(y1) ?
?(y2) ? ? ? ? ? ?(yL)} ? {y}, where ? means
Cartesian product; w = (uT , sT ,vT1 ,vT2 )T and
? = (fT ,hT ,gT1 ,gT2 )T .
We redefine the score function as
S(w,x,y) = ?w,
?
z??(y)
?(x, z)?. (9)
Thus, we rewrite the model into a unified for-
mula
y? = arg max
y
t(y)=T
S(w,x,y). (10)
Given an example (x,y), y? is denoted as the
incorrect label sequence with the highest score
y? = arg max
y? ?=y
t(y?)=t(y)
S(w,x, y?). (11)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = S(w,x,y)? S(w,x, y?). (12)
Thus, we calculate the hinge loss
?(w; (x,y)), (abbreviated as ?w) by
?w =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(13)
In round k, the new weight vector wk+1 is
calculated by
wk+1 = arg min
w
1
2 ||w?wk||
2 + C ? ?,
s.t. ?(w; (xk,yk)) <= ? and ? >= 0 (14)
663
where ? is a non-negative slack variable, and C
is a positive parameter which controls the influ-
ence of the slack term on the objective function.
Following the derivation in PA (Crammer et
al., 2006), we can get the update rule,
wk+1 = wk + ?kek, (15)
where
ek =
?
z??(yk)
?(xk, z)?
?
z??(y?k)
?(xk, z),
?k = min(C,
?wk
?ek?2
).
As we can see from the Eq. (15), when we up-
date the weight vector, the update information
includes not only the features extracted from
current input, but also that extracted from the
loose mapping sequence of input. For each fea-
ture, the weights of its corresponding related
features derived from the loose mapping func-
tion will be updated with the same magnitude
as well as itself.
Our method regards two annotations to be in-
terdependence and peer relationship. Therefore,
the two heterogeneous annotated corpora can be
simultaneously used as the input of our training
algorithm. Because of the tagging and training
algorithm, the weights and tags of two corpora
can be used separately with the only dependent
part built by the loose mapping function.
Our training algorithm based on PA is shown
in Algorithm 2.
6.1 Analysis
Although our mapping function between two
heterogeneous annotations is loose and uncer-
tain, our online training method can automat-
ically increase the relative weights of features
from the beneficial mapping relations and de-
crease the relative weights of features from the
unprofitable mapping relations.
Consider an illustrative loose mapping re-
lation ?NN/CTB?n,nt,nz/PDD?. For an in-
put sequence x and PDD-style output is ex-
pected. If the algorithm tagging a charac-
ter as ?n/PDD?(with help of the weight of
?NN/CTB?) and the right tag isn?t one of
input : mixed heterogeneous datasets:
(xi,yi), i = 1, ? ? ? , N ;
parameters: C,K;
loose mapping function: ? ;
output: wK
Initialize: wTemp? 0,w? 0;
for k = 0 ? ? ?K ? 1 do
for i = 1 ? ? ?N do
receive an example (xi,yi);
predict: y?i with Eq.(11);
if hinge loss ?w > 0 then
update w with Eq. (15);
end
end
wTemp = wTemp + w ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
?n,nt,nz/PDD?, the weight of ?NN/CTB? will
also be decreased, which is reasonable since
it is beneficial to distinguish the right tag.
And if the right tag is one of ?n,nt,nz/PDD?
but not ?n/PDD? (for example, ?nt/PDD?),
which means it is a ?NN/CTB?, the weight of
?NN/CTB? will remain unchanged according to
the algorithm (updating ?n/PDD? changes the
?NN/CTB?, but updating ?nt/PDD? changes it
back).
Therefore, after multiple iterations, useful fea-
tures derived from the mapping function are
typically receive more updates, which take rela-
tively more responsibility for correct prediction.
The final model has good parameter estimates
for the shared information.
We implement our method based on Fu-
danNLP(Qiu et al, 2013).
7 Experiments
7.1 Datasets
We use the two representative corpora men-
tioned above, Penn Chinese Treebank (CTB)
and PKU?s People?s Daily (PPD) in our ex-
periments.
664
Dataset Partition Sections Words
CTB-5
Training 1?270 0.47M
400?931
1001?1151
Develop 301?325 6.66K
Test 271?300 7.82K
CTB-S Training 0.64MTest - 59.96K
PPD Training - 1.11MTest - 0.16M
Table 3: Data partitioning for CTB and PD
7.1.1 CTB Dataset
To better comparison with the previous
works, we use two commonly used criterions to
partition CTB dataset into the train and test
sets.
? One is the partition criterion used in (Jin
and Chen, 2008; Jiang et al, 2009; Sun and
Wan, 2012) for CTB 5.0.
? Another is the CTB dataset from the
POS tagging task of the Fourth Interna-
tional Chinese Language Processing Bake-
off (SIGHAN Bakeoff 2008)(Jin and Chen,
2008).
7.1.2 PPD Dataset
For the PPD dataset, we use the PKU dataset
from SIGHAN Bakeoff 2008.
The details of all datasets are shown in Table
3. Our experiment on these datasets may lead to
a fair comparison of our system and the related
works.
7.2 Setting
We conduct two experiments on CTB-5 +
PPD and CTB-S + PPD respectively.
The form of feature templates we used is
shown in Table 7.2, where C represents a Chi-
nese character, and T represents the character-
based tag. The subscript i indicates its position
related to the current character.
Our method can be easily combined with
some other complicated models, but we only use
the simple one for the purpose of observing the
Ci, T0(i = ?2,?1, 0, 1, 2)
Ci, Ci+1, T0(i = ?1, 0)
T?1, T0
Table 4: Feature Templates
sole influence of our unified model. The parame-
ter C is tested on develop dataset, and we found
that it just impact the speed of convergence and
have no effect on the accuracy. Moreover, since
we use the averaged strategy, we wish more iter-
ations to avoid overfitting and set a small value
0.01 to it. The maximum number of iterations
K is 50.
The F1 score is used for evaluation, which is
the harmonic mean of precision P (percentage of
predict phrases that exactly match the reference
phrases) and recall R (percentage of reference
phrases that returned by system).
7.3 Evaluation on CTB-5 + PPD
The experiment results on the heterogeneous
corpora CTB-5 + PPD are shown in Table
5. Our method obtains an error reductions of
24.08% and 90.8% over the baseline on CTB-5
and PDD respectively.
Our method also gives better performance
than the pipeline-based methods on heteroge-
neous corpora, such as (Jiang et al, 2009) and
(Sun and Wan, 2012).
The reason is that our model can utilize the
information of both corpora effectively, which
can boost the performance of each other.
Although the loose mapping function are bidi-
rectional between two annotation tagsets, we
may also use unidirectional mapping. Therefore,
we also evaluate the performance when we use
unidirectional mapping. We just use the map-
ping function ?PDD?CTB, which means we ob-
tain the PDD-style output without the informa-
tion from CTB in tagging stage. Thus, in train-
ing stage, there are no updates for the weights of
CTB-features for the instances from PDD cor-
pus, while instances from CTB corpus can result
to updates for PDD-features.
Surprisedly, we find that the one-way map-
ping can also improve the performances of both
corpora. The results are shown in Table 7. The
665
Method Training Dataset Test Dataset P R F1
(Jiang et al, 2009) CTB-5, PDD CTB-5 - - 94.02
(Sun and Wan, 2012) CTB-5, PDD CTB-5 94.42 94.93 94.68
Our Model CTB-5 CTB-5 93.28 93.35 93.31
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-5, PDD CTB-5 94.74 95.11 94.92
Our Model CTB-5, PDD PDD 90.25 89.73 89.99
Table 5: Performances of different systems on CTB-5 and PPD.
Method Training Dataset Test Dataset P R F1
Our Model CTB-S CTB-S 89.11 89.16 89.13
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-S, PDD CTB-S 89.86 90.02 89.94
Our Model CTB-S, PDD PDD 90.5 89.82 90.16
Table 6: Performances of different systems on CTB-S and PPD.
modelPPD?CTB obtains an error reductions of
14.63% and 6.12% over the baseline on CTB-5
and PDD respectively.
Method P R F1
ModelS on CTB-5 93.86 94.73 94.29
ModelS on PDD 90.05 89.28 89.66
?ModelS? is the model which is trained on both CTB-
5 and PDD training datasets with just just using the
unidirectional mapping function ?PDD?CTB.
Table 7: Performances of unidirectional PPD?CTB
mapping on CTB-5 and PPD.
7.4 Evaluation on CTB-S + PPD
Table 6 shows the experiment results on the
heterogeneous corpora CTB-S + PPD. Our
method obtains an error reductions of 7.41% and
10.59% over the baseline on CTB-S and PDD re-
spectively.
7.5 Analysis
As we can see from the above experiments,
our proposed unified model can improve the
performances of the two heterogeneous corpora
with unidirectional or bidirectional loose map-
ping functions. Different to the pipeline-based
methods, our model can use the shared infor-
mation between two heterogeneous POS tag-
gers. Although the mapping function is loose
and uncertain, it is still can boost the perfor-
mances. The features derived from the wrong
mapping function take relatively less responsi-
bility for prediction after multiple updates of
their weights in training stage. The final model
has good parameter estimates for the shared in-
formation.
Another phenomenon is that the performance
of one corpus can gains when the data size of an-
other corpus increases. In our two experiments,
the training set?s size of CTB-S is larger than
CTB-5, so the performance of PDD is higher in
latter experiment.
8 Conclusion
We proposed a method for joint Chinese word
segmentation and POS tagging with heteroge-
neous annotation data. Different to the previous
pipeline-based works, our model is learned on
heterogeneous annotation data simultaneously.
Our method also does not require the exact
corresponding relation between the standards
of heterogeneous annotations. The experimen-
tal results show our method leads to a signif-
icant improvement with heterogeneous annota-
tions over the best performance for this task.
Although our work is for a specific task on joint
Chinese word segmentation and POS, the key
idea to leverage heterogeneous annotations is
very general and applicable to other NLP tasks.
666
In the future, we will continue to refine the
proposed model in two ways: (1) We wish to use
the unsupervised method to extract the loose
mapping relation between the different annota-
tion standards, which is useful to the corpora
without loose mapping guideline. (2) We will
analyze the shared information (weights of the
features derived from the tags which have the
mapping relation) in detail and propose a more
effective model. Besides, we would also like to
investigate for other NLP tasks which have dif-
ferent annotation-style corpora.
Acknowledgments
We would like to thank the anonymous re-
viewers for their valuable comments. This
work was funded by NSFC (No.61003091), Key
Projects in the National Science & Technol-
ogy Pillar Program (2012BAH18B01), Shang-
hai Municipal Science and Technology Com-
mission (12511504500), Shanghai Leading Aca-
demic Discipline Project (B114) and 973 Pro-
gram (No.2010CB327900).
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from mul-
tiple tasks and unlabeled data. J. Mach. Learn.
Res., 6:1817?1853, December.
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. Learning
Theory and Kernel Machines, pages 567?580.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
J. Gao, A. Wu, M. Li, C.N. Huang, H. Li, X. Xia,
and H. Qin. 2004. Adaptive chinese word segmen-
tation. In Proceedings of ACL-2004.
W. Jiang, L. Huang, Q. Liu, and Y. Lu. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics. Citeseer.
W. Jiang, L. Huang, and Q. Liu. 2009. Automatic
adaptation of annotation standards: Chinese word
segmentation and POS tagging: a case study. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language
Processing, pages 522?530.
Wenbin Jiang, Fandong Meng, Qun Liu, and Ya-
juan L?. 2012. Iterative annotation transfor-
mation with predict-self reestimation for Chinese
word segmentation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 412?420, Jeju Is-
land, Korea, July. Association for Computational
Linguistics.
C. Jin and X. Chen. 2008. The fourth interna-
tional Chinese language processing bakeoff: Chi-
nese word segmentation, named entity recognition
and Chinese pos tagging. In Sixth SIGHAN Work-
shop on Chinese Language Processing, page 69.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-C?t?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 761?768. Association for Computa-
tional Linguistics.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based
or character-based. In Proceedings of EMNLP,
volume 4.
Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing
Huang. 2012. Joint segmentation and tagging
with coupled sequences labeling. In Proceedings
of COLING 2012, pages 951?964, Mumbai, India,
December. The COLING 2012 Organizing Com-
mittee.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
FudanNLP: A toolkit for Chinese natural language
processing. In Proceedings of ACL.
Weiwei Sun and Xiaojun Wan. 2012. Reducing
approximation and estimation errors for Chinese
lexical processing with heterogeneous annotations.
In Proceedings of the 50th Annual Meeting of the
667
Association for Computational Linguistics, pages
232?241.
W. Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1385?1394.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn Chinese treebank (3.0).
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169?1176.
ACM.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern Chinese corpus. Technical
report, Technical report.
Jiayi Zhao, Xipeng Qiu, and Xuanjing Huang. 2013.
A unified model for joint chinese word segmen-
tation and pos tagging with heterogeneous anno-
tation corpora. In International Conference on
Asian Language Processing, IALP.
668
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598?602,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Hierarchical Text Classification with Latent Concepts
Xipeng Qiu, Xuanjing Huang, Zhao Liu and Jinlong Zhou
School of Computer Science, Fudan University
{xpqiu,xjhuang}@fudan.edu.cn, {zliu.fd,abc9703}@gmail.com
Abstract
Recently, hierarchical text classification has
become an active research topic. The essential
idea is that the descendant classes can share
the information of the ancestor classes in a
predefined taxonomy. In this paper, we claim
that each class has several latent concepts and
its subclasses share information with these d-
ifferent concepts respectively. Then, we pro-
pose a variant Passive-Aggressive (PA) algo-
rithm for hierarchical text classification with
latent concepts. Experimental results show
that the performance of our algorithm is com-
petitive with the recently proposed hierarchi-
cal classification algorithms.
1 Introduction
Text classification is a crucial and well-proven
method for organizing the collection of large scale
documents. The predefined categories are formed
by different criterions, e.g. ?Entertainment?, ?Sport-
s? and ?Education? in news classification, ?Junk E-
mail? and ?Ordinary Email? in email classification.
In the literature, many algorithms (Sebastiani, 2002;
Yang and Liu, 1999; Yang and Pedersen, 1997) have
been proposed, such as Support Vector Machines
(SVM), k-Nearest Neighbor (kNN), Na??ve Bayes
(NB) and so on. Empirical evaluations have shown
that most of these methods are quite effective in tra-
ditional text classification applications.
In past serval years, hierarchical text classification
has become an active research topic in database area
(Koller and Sahami, 1997; Weigend et al, 1999)
and machine learning area (Rousu et al, 2006; Cai
and Hofmann, 2007). Different with traditional clas-
sification, the document collections are organized
as hierarchical class structure in many application
fields: web taxonomies (i.e. the Yahoo! Directory
http://dir.yahoo.com/ and the Open Direc-
tory Project (ODP) http://dmoz.org/), email
folders and product catalogs.
The approaches of hierarchical text classification
can be divided in three ways: flat, local and global
approaches.
The flat approach is traditional multi-class classi-
fication in flat fashion without hierarchical class in-
formation, which only uses the classes in leaf nodes
in taxonomy(Yang and Liu, 1999; Yang and Peder-
sen, 1997; Qiu et al, 2011).
The local approach proceeds in a top-down fash-
ion, which firstly picks the most relevant categories
of the top level and then recursively making the
choice among the low-level categories(Sun and Lim,
2001; Liu et al, 2005).
The global approach builds only one classifier to
discriminate all categories in a hierarchy(Cai and
Hofmann, 2004; Rousu et al, 2006; Miao and Qiu,
2009; Qiu et al, 2009). The essential idea of global
approach is that the close classes have some com-
mon underlying factors. Especially, the descendan-
t classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997; Xue et al, 2007).
Because the global hierarchical categorization can
avoid the drawbacks about those high-level irrecov-
erable error, it is more popular in the machine learn-
ing domain.
However, the taxonomy is defined artificially and
is usually very difficult to organize for large scale
taxonomy. The subclasses of the same parent class
may be dissimilar and can be grouped in differen-
t concepts, so it bring great challenge to hierarchi-
598
Sports
Football
Basketball
Swimming
Surfing
Sports
Water
Football
Basketball
Swimming
Surfing
Ball
(a) (b)
College
High 
School
College
High 
SchoolAcade
my
Figure 1: Example of latent nodes in taxonomy
cal classification. For example, the ?Sports? node
in a taxonomy have six subclasses (Fig. 1a), but
these subclass can be grouped into three unobserv-
able concepts (Fig. 1b). These concepts can show
the underlying factors more clearly.
In this paper, we claim that each class may have
several latent concepts and its subclasses share in-
formation with these different concepts respectively.
Then we propose a variant Passive-Aggressive (PA)
algorithm to maximizes the margins between latent
paths.
The rest of the paper is organized as follows. Sec-
tion 2 describes the basic model of hierarchical clas-
sification. Then we propose our algorithm in section
3. Section 4 gives experimental analysis. Section 5
concludes the paper.
2 Hierarchical Text Classification
In text classification, the documents are often rep-
resented with vector space model (VSM) (Salton et
al., 1975). Following (Cai and Hofmann, 2007),
we incorporate the hierarchical information in fea-
ture representation. The basic idea is that the notion
of class attributes will allow generalization to take
place across (similar) categories and not just across
training examples belonging to the same category.
Assuming that the categories is ? =
[?1, ? ? ? , ?m], where m is the number of the
categories, which are organized in hierarchical
structure, such as tree or DAG.
Give a sample x with its class path in the taxono-
my y, we define the feature is
?(x,y) = ?(y)? x, (1)
where ?(y) = (?1(y), ? ? ? , ?m(y))T ? Rm and ?
is the Kronecker product.
We can define
?i(y) =
{
ti if ?i ? y
0 otherwise , (2)
where ti >= 0 is the attribute value for node v. In
the simplest case, ti can be set to a constant, like 1.
Thus, we can classify x with a score function,
y? = argmax
y
F (w,?(x,y)), (3)
where w is the parameter of F (?).
3 Hierarchical Text Classification with
Latent Concepts
In this section, we first extent the Passive-
Aggressive (PA) algorithm to the hierarchical clas-
sification (HPA), then we modify it to incorporate
latent concepts (LHPA).
3.1 Hierarchical Passive-Aggressive Algorithm
The PA algorithm is an online learning algorithm,
which aims to find the new weight vectorwt+1 to be
the solution to the following constrained optimiza-
tion problem in round t.
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C?
s.t. ?(w; (xt, yt)) <= ? and ? >= 0. (4)
where ?(w; (xt, yt)) is the hinge-loss function and ?
is slack variable.
Since the hierarchical text classification is loss-
sensitive based on the hierarchical structure. We
need discriminate the misclassification from ?near-
ly correct? to ?clearly incorrect?. Here we use tree
induced error ?(y,y?), which is the shortest path
connecting the nodes yleaf and y?leaf . yleaf repre-
sents the leaf node in path y.
Given a example (x,y), we look for the w to
maximize the separation margin ?(w; (x,y)) be-
tween the score of the correct path y and the closest
error path y?.
?(w; (x,y)) = wT?(x,y)?wT?(x, y?), (5)
599
where y? = argmaxz ?=y wT?(x, z) and ? is a fea-
ture function.
Unlike the standard PA algorithm, which achieve
a margin of at least 1 as often as possible, we wish
the margin is related to tree induced error ?(y, y?).
This loss is defined by the following function,
?(w; (x,y)) =
{
0, ?(w; (x,y)) > ?(y, y?)
?(y, y?)? ?(w; (x,y)), otherwise (6)
We abbreviate ?(w; (x,y)) to ?. If ? = 0 then wt
itself satisfies the constraint in Eq. (4) and is clearly
the optimal solution. We therefore concentrate on
the case where ? > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (4) to be,
L(w, ?, ?, ?) = 1
2
||w?wt||2+C?+?(???)???
s.t. ?, ? >= 0. (7)
where ?, ? is a Lagrange multiplier.
We set the gradient of Eq. (7) respect to ? to zero.
? + ? = C. (8)
The gradient of w should be zero.
w ?wt ? ?(?(x,y)? ?(x, y?)) = 0 (9)
Then we get,
w = wt + ?(?(x,y)? ?(x, y?)). (10)
Substitute Eq. (8) and Eq. (10) to objective func-
tion Eq. (7), we get
L(?) = ?1
2
?2||?(x,y)? ?(x, y?)||2
+ ?wt(?(x,y)? ?(x, y?)))? ??(y, y?) (11)
Differentiate Eq. (11 with ?, and set it to zero, we
get
?? = ?(y, y?)?wt(?(x,y)? ?(x, y?)))
||?(x,y)? ?(x, y?)||2 (12)
From ? + ? = C, we know that ? < C, so
?? = min(C, ?(y, y?)?wt(?(x,y)? ?(x, y?)))
||?(x,y)? ?(x, y?)||2
).
(13)
3.2 Hierarchical Passive-Aggressive Algorithm
with Latent Concepts
For the hierarchical taxonomy ? = (?1, ? ? ? , ?c),
we define that each class ?i has a set H?i =
h1?i , ? ? ? , h
m
?i with m latent concepts, which are un-
observable.
Given a label path y, it has a set of several latent
paths Hy. For a latent path z ? Hy, a function
Proj(z) .= y is the projection from a latent path z
to its corresponding path y.
Then we can define the predict latent path h? and
the most correct latent path h?:
h? = arg max
proj(z)?=y
wT?(x, z), (14)
h? = arg max
proj(z)=y
wT?(x, z). (15)
Similar to the above analysis of HPA, we re-define
the margin
?(w; (x,y) = wT?(x,h?)? wT?(x, h?), (16)
then we get the optimal update step
??L = min(C,
?(wt; (x,y))
||?(x,h?)? ?(x, h?)||2
). (17)
Finally, we get update strategy,
w = wt + ??L(?(x,h?)? ?(x, h?)). (18)
Our hierarchical passive-aggressive algorithm
with latent concepts (LHPA) is shown in Algorith-
m 1. In this paper, we use two latent concepts for
each class.
4 Experiment
4.1 Datasets
We evaluate our proposed algorithm on two datasets
with hierarchical category structure.
WIPO-alpha dataset The dataset1 consisted of the
1372 training and 358 testing document com-
prising the D section of the hierarchy. The
number of nodes in the hierarchy was 188, with
maximum depth 3. The dataset was processed
into bag-of-words representation with TF?IDF
1World Intellectual Property Organization, http://www.
wipo.int/classifications/en
600
input : training data set: (xn,yn), n = 1, ? ? ? , N ,
and parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
get (xt,yt) from data set;
predict h?,h?;
calculate ?(w; (x,y)) and?(yt, y?t);
if ?(w; (x,y)) ? ?(yt, y?t) then
calculate ??L by Eq. (17);
update wt+1 by Eq. (18). ;
end
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1:Hierarchical PA algorithmwith la-
tent concepts
weighting. No word stemming or stop-word
removal was performed. This dataset is used
in (Rousu et al, 2006).
LSHTC dataset The dataset2 has been constructed
by crawling web pages that are found in the
Open Directory Project (ODP) and translating
them into feature vectors (content vectors) and
splitting the set of Web pages into a training,
a validation and a test set, per ODP category.
Here, we use the dry-run dataset(task 1).
4.2 Performance Measurement
Macro Precision, Macro Recall and Macro F1 are
the most widely used performance measurements
for text classification problems nowadays. The
macro strategy computes macro precision and re-
call scores by averaging the precision/recall of each
category, which is preferred because the categories
are usually unbalanced and give more challenges to
classifiers. The Macro F1 score is computed using
the standard formula applied to the macro-level pre-
cision and recall scores.
MacroF1 = P ?R
P +R
, (19)
2Large Scale Hierarchical Text classification Pascal Chal-
lenge, http://lshtc.iit.demokritos.gr
Table 1: Results on WIPO-alpha Dataset.?-? means that
the result is not available in the author?s paper.
Accuracy F1 Precision Recall TIE
PA 49.16 40.71 43.27 38.44 2.06
HPA 50.84 40.26 43.23 37.67 1.92
LHPA 51.96 41.84 45.56 38.69 1.87
HSVM 23.8 - - - -
HM3 35.0 - - - -
Table 2: Results on LSHTC dry-run Dataset
Accuracy F1 Precision Recall TIE
PA 47.36 44.63 52.64 38.73 3.68
HPA 46.88 43.78 51.26 38.2 3.73
LHPA 48.39 46.26 53.82 40.56 3.43
where P is the Macro Precision and R is the Macro
Recall. We also use tree induced error (TIE) in the
experiments.
4.3 Results
We implement three algorithms3: PA(Flat PA), H-
PA(Hierarchical PA) and LHPA(Hierarchical PA
with latent concepts). The results are shown in Table
1 and 2. For WIPO-alpha dataset, we also compared
LHPA with two algorithms used in (Rousu et al,
2006): HSVM and HM3.
We can see that LHPA has better performances
than the other methods. From Table 2, we can see
that it is not always useful to incorporate the hierar-
chical information. Though the subclasses can share
information with their parent class, the shared infor-
mation may be different for each subclass. So we
should decompose the underlying factors into dif-
ferent latent concepts.
5 Conclusion
In this paper, we propose a variant Passive-
Aggressive algorithm for hierarchical text classifi-
cation with latent concepts. In the future, we will
investigate our method in the larger and more noisy
data.
Acknowledgments
This work was (partially) funded by NSFC (No.
61003091 and No. 61073069), 973 Program (No.
3Source codes are available in FudanNLP toolkit, http:
//code.google.com/p/fudannlp/
601
2010CB327906) and Shanghai Committee of Sci-
ence and Technology(No. 10511500703).
References
L. Cai and T. Hofmann. 2004. Hierarchical document
categorization with support vector machines. In Pro-
ceedings of CIKM.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine Learn-
ing, 28(1):41?75.
D. Koller and M Sahami. 1997. Hierarchically classify-
ing documents using very few words. In Proceedings
of the International Conference on Machine Learning
(ICML).
T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y.
Ma. 2005. Support vector machines classification
with a very large-scale taxonomy. ACM SIGKDD Ex-
plorations Newsletter, 7(1):43.
Youdong Miao and Xipeng Qiu. 2009. Hierarchical
centroid-based classifier for large scale text classifica-
tion. In Large Scale Hierarchical Text classification
(LSHTC) Pascal Challenge.
Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009.
Hierarchical multi-class text categorization with glob-
al margin maximization. In Proceedings of the ACL-
IJCNLP 2009 Conference, pages 165?168, Suntec,
Singapore, August. Association for Computational
Linguistics.
Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011.
An effective feature selection method for text catego-
rization. In Proceedings of the 15th Pacific-Asia Con-
ference on Knowledge Discovery and Data Mining.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. In Journal of
Machine Learning Research.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM computing surveys, 34(1):1?47.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining.
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007.
Multi-task learning for classification with dirichlet
process priors. The Journal of Machine Learning Re-
search, 8:63.
Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proc. of SIGIR. ACMPress
New York, NY, USA.
Y. Yang and J.O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
Int. Conf. on Mach. Learn. (ICML), volume 97.
602
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?439,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Latent Semantic Tensor Indexing
for Community-based Question Answering
Xipeng Qiu, Le Tian, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, tianlefdu@gmail.com, xjhuang@fudan.edu.cn
Abstract
Retrieving similar questions is very
important in community-based ques-
tion answering(CQA). In this paper,
we propose a unified question retrieval
model based on latent semantic index-
ing with tensor analysis, which can cap-
ture word associations among different
parts of CQA triples simultaneously.
Thus, our method can reduce lexical
chasm of question retrieval with the
help of the information of question con-
tent and answer parts. The experimen-
tal result shows that our method out-
performs the traditional methods.
1 Introduction
Community-based (or collaborative) ques-
tion answering(CQA) such as Yahoo! An-
swers1 and Baidu Zhidao2 has become a pop-
ular online service in recent years. Unlike tra-
ditional question answering (QA), information
seekers can post their questions on a CQA
website which are later answered by other
users. However, with the increase of the CQA
archive, there accumulate massive duplicate
questions on CQA websites. One of the pri-
mary reasons is that information seekers can-
not retrieve answers they need and thus post
another new question consequently. There-
fore, it becomes more and more important to
find semantically similar questions.
The major challenge for CQA retrieval is the
lexical gap (or lexical chasm) among the ques-
tions (Jeon et al, 2005b; Xue et al, 2008),
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
Query:
Q: Why is my laptop screen blinking?
Expected:
Q1: How to troubleshoot a flashing
screen on an LCD monitor?
Not Expected:
Q2: How to blinking text on screen
with PowerPoint?
Table 1: An example on question retrieval
as shown in Table 1. Since question-answer
pairs are usually short, the word mismatch-
ing problem is especially important. However,
due to the lexical gap between questions and
answers as well as spam typically existing in
user-generated content, filtering and ranking
answers is very challenging.
The earlier studies mainly focus on generat-
ing redundant features, or finding textual clues
using machine learning techniques; none of
them ever consider questions and their answers
as relational data but instead model them as
independent information. Moreover, they only
consider the answers of the current question,
and ignore any previous knowledge that would
be helpful to bridge the lexical and se mantic
gap.
In recent years, many methods have been
proposed to solve the word mismatching prob-
lem between user questions and the questions
in a QA archive(Blooma and Kurian, 2011),
among which the translation-based (Riezler et
al., 2007; Xue et al, 2008; Zhou et al, 2011)
or syntactic-based approaches (Wang et al,
2009) methods have been proven to improve
the performance of CQA retrieval.
However, most of these approaches used
434
pipeline methods: (1) modeling word asso-
ciation; (2) question retrieval combined with
other models, such as vector space model
(VSM), Okapi model (Robertson et al, 1994)
or language model (LM). The pipeline meth-
ods often have many non-trivial experimental
setting and result to be very hard to repro-
duce.
In this paper, we propose a novel unified
retrieval model for CQA, latent semantic
tensor indexing (LSTI), which is an exten-
sion of the conventional latent semantic index-
ing (LSI) (Deerwester et al, 1990). Similar
to LSI, LSTI can integrate the two detached
parts (modeling word association and question
retrieval) into a single model.
In traditional document retrieval, LSI is an
effective method to overcome two of the most
severe constraints on Boolean keyword queries:
synonymy, that is, multiple words with similar
meanings, and polysemy, or words with more
than one meanings.
Usually in a CQA archive, each en-
try (or question) is in the following triple
form:?question title, question content,
answer?. Because the performance based
solely on the content or the answer part is
less than satisfactory, many works proposed
that additional relevant information should be
provided to help question retrieval(Xue et al,
2008). For example, if a question title contains
the keyword ?why?, the CQA triple, which
contains ?because? or ?reason? in its answer
part, is more likely to be what the user looks
for.
Since each triple in CQA has three parts, the
natural representation of the CQA collection
is a three-dimensional array, or 3rd-order ten-
sor, rather than a matrix. Based on the tensor
decomposition, we can model the word associ-
ation simultaneously in the pairs: question-
question, question-body and question-answer.
The rest of the paper is organized as fol-
lows: Section 3 introduces the concept of LSI.
Section 4 presents our method. Section 5 de-
scribes the experimental analysis. Section 6
concludes the paper.
2 Related Works
There are some related works on question re-
trieval in CQA. Various query expansion tech-
niques have been studied to solve word mis-
match problems between queries and docu-
ments. The early works on question retrieval
can be traced back to finding similar ques-
tions in Frequently Asked Questions (FAQ)
archives, such as the FAQ finder (Burke et al,
1997), which usually used statistical and se-
mantic similarity measures to rank FAQs.
Jeon et al (2005a; 2005b) compared four
different retrieval methods, i.e., the vector
space model(Jijkoun and de Rijke, 2005),
the Okapi BM25 model (Robertson et al,
1994), the language model, and the trans-
lation model, for question retrieval on CQA
data, and the experimental results showed
that the translation model outperforms the
others. However, they focused only on similar-
ity measures between queries (questions) and
question titles.
In subsequent work (Xue et al, 2008), a
translation-based language model combining
the translation model and the language model
for question retrieval was proposed. The
results showed that translation models help
question retrieval since they could effectively
address the word mismatch problem of ques-
tions. Additionally, they also explored an-
swers in question retrieval.
Duan et al (2008) proposed a solution that
made use of question structures for retrieval
by building a structure tree for questions in
a category of Yahoo! Answers, which gave
more weight to important phrases in question
matching.
Wang et al (2009) employed a parser to
build syntactic trees for questions, and ques-
tions were ranked based on the similarity be-
tween their syntactic trees and that of the
query question.
It is worth noting that our method is to-
tally different to the work (Cai et al, 2006)
of the same name. They regard documents
as matrices, or the second order tensors to
generate a low rank approximations of ma-
trices (Ye, 2005). For example, they convert
a 1, 000, 000-dimensional vector of word space
into a 1000 ? 1000 matrix. However in our
model, a document is still represented by a
vector. We just project a higher-dimensional
vector to a lower-dimensional vector, but not
a matrix in Cai?s model. A 3rd-order tensor is
435
also introduced in our model for better repre-
sentation for CQA corpus.
3 Latent Semantic Indexing
Latent Semantic Indexing (LSI) (Deer-
wester et al, 1990), also called Latent Seman-
tic Analysis (LSA), is an approach to auto-
matic indexing and information retrieval that
attempts to overcome these problems by map-
ping documents as well as terms to a represen-
tation in the so-called latent semantic space.
The key idea of LSI is to map documents
(and by symmetry terms) to a low dimen-
sional vector space, the latent semantic space.
This mapping is computed by decomposing
the term-document matrix N with SVD, N =
U?V t, where U and V are orthogonal matri-
ces U tU = V tV = I and the diagonal matrix
? contains the singular values of N . The LSA
approximation of N is computed by just keep
the largest K singular values in ?, which is
rank K optimal in the sense of the L2-norm.
LSI has proven to result in more robust word
processing in many applications.
4 Tensor Analysis for CQA
4.1 Tensor Algebra
We first introduce the notation and basic
definitions of multilinear algebra. Scalars are
denoted by lower case letters (a, b, . . . ), vectors
by bold lower case letters (a,b, . . . ), matri-
ces by bold upper-case letters (A,B, . . . ), and
higher-order tensors by calligraphic upper-case
letters (A,B, . . . ).
A tensor, also known as n-way array, is a
higher order generalization of a vector (first
order tensor) and a matrix (second order ten-
sor). The order of tensor D ? RI1?I2?????IN is
N . An element of D is denoted as di1,...,N .
An Nth-order tensor can be flattened into
a matrix by N ways. We denote the matrix
D(n) as the mode-n flattening of D (Kolda,
2002).
Similar with a matrix, an Nth-order tensor
can be decomposed through ?N -mode singu-
lar value decomposition (SVD)?, which is a an
extension of SVD that expresses the tensor as
the mode-n product of N -orthogonal spaces.
D = Z?1 U1?2 U2 ? ? ??n Un ? ? ??N UN . (1)
Tensor Z, known as the core tensor, is analo-
gous to the diagonal singular value matrix in
conventional matrix SVD. Z is in general a
full tensor. The core tensor governs the in-
teraction between the mode matrices Un, for
n = 1, . . . , N . Mode matrix Un contains the
orthogonal left singular vectors of the mode-n
flattened matrix D(n).
The N -mode SVD algorithm for decompos-
ing D is as follows:
1. For n = 1, . . . , N , compute matrix Un in
Eq.(1) by computing the SVD of the flat-
tened matrix D(n) and setting Un to be
the left matrix of the SVD.
2. Solve for the core tensor as follows Z =
D ?1 UT1 ?2 UT2 ? ? ? ?n UTn ? ? ? ?N UTN .
4.2 CQA Tensor
Given a collection of CQA triples, ?qi, ci, ai?
(i = 1, . . . ,K), where qi is the question and
ci and ai are the content and answer of qi
respectively. We can use a 3-order tensor
D ? RK?3?T to represent the collection, where
T is the number of terms. The first dimension
corresponds to entries, the second dimension,
to parts and the third dimension, to the terms.
For example, the flattened matrix of CQA
tensor with ?terms? direction is composed
by three sub-matrices MTitle, MContent and
MAnswer, as was illustrated in Figure 1. Each
sub-matrix is equivalent to the traditional
document-term matrix.
Figure 1: Flattening CQA tensor with ?terms?
(right matrix)and ?entries? (bottom matrix)
Denote pi,j to be part j of entry i. Then we
436
have the term frequency, defined as follows.
tfi,j,k =
ni,j,k?
i ni,j,k
, (2)
where ni,j,k is the number of occurrences of the
considered term (tk) in pi,j , and the denomi-
nator is the sum of number of occurrences of
all terms in pi,j .
The inverse document frequency is a mea-
sure of the general importance of the term.
idfj,k = log
|K|
1 +?i I(tk ? pi,j)
, (3)
where |K| is the total number of entries and
I(?) is the indicator function.
Then the element di,j,k of tensor D is
di,j,k = tfi,j,k ? idfj,k. (4)
4.3 Latent Semantic Tensor Indexing
For the CQA tensor, we can decompose it
as illustrated in Figure 2.
D = Z ?1 UEntry ?2 UPart ?3 UTerm, (5)
where UEntry, UPart and UTerm are left sin-
gular matrices of corresponding flattened ma-
trices. UTerm spans the term space, and we
just use the vectors corresponding to the 1, 000
largest singular values in this paper, denoted
as U?Term.
Figure 2: 3-mode SVD of CQA tensor
To deal with such a huge sparse data set, we
use singular value decomposition (SVD) im-
plemented in Apache Mahout3 machine learn-
ing library, which is implemented on top
of Apache Hadoop4 using the map/reduce
paradigm and scalable to reasonably large
data sets.
3http://mahout.apache.org/
4http://hadoop.apache.org
4.4 Question Retrieval
In order to retrieve similar question effec-
tively, we project each CQA triple Dq ?
R1?3?T to the term space by
D?i = Di ?3 U?TTerm. (6)
Given a new question only with title part,
we can represent it by tensor Dq ? R1?3?T ,
and its MContent and MAnswer are zero ma-
trices. Then we project Dq to the term space
and get D?q.
Here, D?q and D?i are degraded tensors and
can be regarded as matrices. Thus, we can cal-
culate the similarity between D?q and D?i with
normalized Frobenius inner product.
For two matrices A and B, the Frobenius
inner product, indicated as A : B, is the
component-wise inner product of two matrices
as though they are vectors.
A : B =
?
i,j
Ai,jBi,j (7)
To reduce the affect of length, we use the
normalized Frobenius inner product.
A : B = A : B?
A : A?
?
B : B
(8)
While given a new question both with title
and content parts, MContent is not a zero ma-
trix and could be also employed in the question
retrieval process. A simple strategy is to sum
up the scores of two parts.
5 Experiments
5.1 Datasets
We collected the resolved CQA triples from
the ?computer? category of Yahoo! Answers
and Baidu Zhidao websites. We just selected
the resolved questions that already have been
given their best answers. The CQA triples are
preprocessed with stopwords removal (Chinese
sentences are segmented into words in advance
by FudanNLP toolkit(Qiu et al, 2013)).
In order to evaluate our retrieval system, we
divide our dataset into two parts. The first
part is used as training dataset; the rest is used
as test dataset for evaluation. The datasets are
shown in Table 2.
437
DataSet training
data size
test data
size
Baidu Zhidao 423k 1000
Yahoo! Answers 300k 1000
Table 2: Statistics of Collected Datasets
Methods MAP
Okapi 0.359
LSI 0.387
(Jeon et al, 2005b) 0.372
(Xue et al, 2008) 0.381
LSTI 0.415
Table 3: Retrieval Performance on Dataset
from Yahoo! Answers
5.2 Evaluation
We compare our method with two baseline
methods: Okapi BM25 and LSI and two state-
of-the-art methods: (Jeon et al, 2005b)(Xue
et al, 2008). In LSI, we regard each triple
as a single document. Three annotators are
involved in the evaluation process. Given a
returned result, two annotators are asked to
label it with ?relevant? or ?irrelevant?. If an
annotator considers the returned result seman-
tically equivalent to the queried question, he
labels it as ?relevant?; otherwise, it is labeled
as ?irrelevant?. If a conflict happens, the third
annotator will make the final judgement.
We use mean average precision (MAP)
to evaluate the effectiveness of each method.
The experiment results are illustrated in Ta-
ble 3 and 4, which show that our method out-
performs the others on both datasets.
The primary reason is that we incorpo-
rate the content of the question body and
the answer parts into the process of ques-
tion retrieval, which should provide addi-
tional relevance information. Different to
Methods MAP
Okapi 0.423
LSI 0.490
(Jeon et al, 2005b) 0.498
(Xue et al, 2008) 0.512
LSTI 0.523
Table 4: Retrieval Performance on Dataset
from Baidu Zhidao
the translation-based methods, our method
can capture the mapping relations in three
parts (question, content and answer) simulta-
neously.
It is worth noting that the problem of data
sparsity is more crucial for LSTI since the size
of a tensor in LSTI is larger than a term-
document matrix in LSI. When the size of data
is small, LSTI tends to just align the common
words and thus cannot find the correspond-
ing relations among the focus words in CQA
triples. Therefore, more CQA triples may re-
sult in better performance for our method.
6 Conclusion
In this paper, we proposed a novel re-
trieval approach for community-based QA,
called LSTI, which analyzes the CQA triples
with naturally tensor representation. LSTI
is a unified model and effectively resolves the
problem of lexical chasm for question retrieval.
For future research, we will extend LSTI to
a probabilistic form (Hofmann, 1999) for bet-
ter scalability and investigate its performance
with a larger corpus.
Acknowledgments
We would like to thank the anony-
mous reviewers for their valuable com-
ments. This work was funded by NSFC
(No.61003091 and No.61073069) and 973 Pro-
gram (No.2010CB327900).
References
M.J. Blooma and J.C. Kurian. 2011. Research
issues in community based question answering.
In PACIS 2011 Proceedings.
R. Burke, K. Hammond, V. Kulyukin, S. Lytinen,
N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question
files: Experiences with the faq finder system.
AI Magazine, 18(2):57?66.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Ten-
sor space model for document analysis. In SI-
GIR ?06: Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research
and development in information retrieval.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Index-
ing by latent semantic analysis. Journal of
the American society for information science,
41(6):391?407.
438
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and
Yong Yu. 2008. Searching questions by iden-
tifying question topic and question focus. In
Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, June. Association for Compu-
tational Linguistics.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
50?57. ACM Press New York, NY, USA.
J. Jeon, W.B. Croft, and J.H. Lee. 2005a. Find-
ing semantically similar questions based on their
answers. In Proceedings of the 28th annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
617?618. ACM.
J. Jeon, W.B. Croft, and J.H. Lee. 2005b. Finding
similar questions in large question and answer
archives. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 84?90.
V. Jijkoun and M. de Rijke. 2005. Retrieving an-
swers from frequently asked questions pages on
the web. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 76?83.
T.G. Kolda. 2002. Orthogonal tensor decompo-
sitions. SIAM Journal on Matrix Analysis and
Applications, 23(1):243?255.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
Fudannlp: A toolkit for chinese natural lan-
guage processing. In Proceedings of ACL.
S. Riezler, A. Vasserman, I. Tsochantaridis,
V. Mittal, and Y. Liu. 2007. Statistical ma-
chine translation for query expansion in answer
retrieval. In Proceedings of the Annual Meeting
of the Association for Computational Linguis-
tics.
S.E. Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1994.
Okapi at trec-3. In TREC, pages 109?126.
K. Wang, Z. Ming, and T.S. Chua. 2009. A syn-
tactic tree matching approach to finding similar
questions in community-based QA services. In
Proceedings of the 32nd international ACM SI-
GIR conference on Research and development in
information retrieval, pages 187?194. ACM.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval
models for question and answer archives. In Pro-
ceedings of the 31st annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 475?482. ACM.
J.M. Ye. 2005. Generalized low rank approxima-
tions of matrices. Mach. Learn., 61(1):167?191.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011.
Phrase-based translation model for question re-
trieval in community question answer archives.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages
653?662. Association for Computational Lin-
guistics.
439
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 49?54,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
FudanNLP: A Toolkit for Chinese Natural Language Processing
Xipeng Qiu, Qi Zhang, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, qz@fudan.edu.cn, xjhuang@fudan.edu.cn
Abstract
The growing need for Chinese natural
language processing (NLP) is largely
in a range of research and commer-
cial applications. However, most of
the currently Chinese NLP tools or
components still have a wide range
of issues need to be further improved
and developed. FudanNLP is an open
source toolkit for Chinese natural lan-
guage processing (NLP), which uses
statistics-based and rule-based meth-
ods to deal with Chinese NLP tasks,
such as word segmentation, part-of-
speech tagging, named entity recogni-
tion, dependency parsing, time phrase
recognition, anaphora resolution and so
on.
1 Introduction
Chinese is one of the most widely used lan-
guages in this world, and the proportion that
Chinese language holds on the Internet is also
quite high. Under the current circumstances,
there are greater and greater demands for in-
telligent processing and analyzing of the Chi-
nese texts.
Similar to English, the main tasks in Chi-
nese NLP include word segmentation (CWS),
part-of-speech (POS) tagging, named en-
tity recognition (NER), syntactic parsing,
anaphora resolution (AR), and so on. Al-
though the general ways are essentially the
same for English and Chinese, the implemen-
tation details are different. It is also non-
trivial to optimize these methods for Chinese
NLP tasks.
There are also some toolkits to be used
for NLP, such as Stanford CoreNLP1, Apache
OpenNLP2, Curator3 and NLTK4. But these
toolkits are developed mainly for English and
not optimized for Chinese.
In order to customize an optimized system
for Chinese language process, we implement
an open source toolkit, FudanNLP5, which is
written in Java. Since most of the state-of-the-
art methods for NLP are based on statistical
learning, the whole framework of our toolkit
is established around statistics-based meth-
ods, supplemented by some rule-based meth-
ods. Therefore, the quality of training data
is crucial for our toolkit. However, we find
that there are some drawbacks in currently
most commonly used corpora, such as CTB
(Xia, 2000) and CoNLL (Haji? et al, 2009)
corpora. For example, in CTB corpus, the set
of POS tags is relative small and some cate-
gories are derived from the perspective of En-
glish grammar. And in CoNLL corpus, the
head words are often interrogative particles
and punctuations, which are unidiomatic in
Chinese. These drawbacks bring more chal-
lenges to further analyses, such as informa-
tion extraction and semantic understanding.
Therefore, we first construct a corpus with
a modified guideline, which is more in ac-
cordance with the common understanding for
Chinese grammar.
In addition to the basic Chinese NLP tasks
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://incubator.apache.org/opennlp/
3http://cogcomp.cs.illinois.edu/page/
software_view/Curator
4http://www.nltk.org/
5http://fudannlp.googlecode.com
49
Figure 1: System Structure of FudanNLP
mentioned above, the toolkit also provides
many minor functions, such as text classifi-
cation, dependency tree kernel, tree pattern-
based information extraction, keywords ex-
traction, translation between simplified and
traditional Chinese, and so on.
Currently, our toolkit has been used by
many universities and companies for various
applications, such as the dialogue system, so-
cial computing, recommendation system and
vertical search.
The rest of the demonstration is organized
as follows. We first briefly describe our system
and its main components in section 2. Then we
show system performances in section 3. Sec-
tion 4 introduces three ways to use our toolkit.
In section 5, we summarize the paper and give
some directions for our future efforts.
2 System Overview
The components of our system have three
layers of structure: data preprocessing, ma-
chine learning and natural language process-
ing, which is shown in Figure 1. We will in-
troduce these components in detail in the fol-
lowing subsections.
2.1 Data Preprocessing Component
In the natural language processing system,
the original input is always text. However,
the statistical machine learning methods often
deal with data with vector-based representa-
tion. So we firstly need to preprocess the input
texts and transform them to the required for-
mat. Due to the fact that text data is usually
discrete and sparse, the sparse vector struc-
ture is largely used. Similar to Mallet (Mc-
Callum, 2002), we use the pipeline structure
for a flexible transformation of various data.
The pipeline consists of several serial or par-
allel modules. Each module, called ?pipe?, is
aimed at a single and simple function.
For example, when we transform a sentence
into a vector with ?bag-of-words?, the trans-
formation process would involve the following
serial pipes:
1. String2Token Pipe: to transform a string
into word tokens.
2. Token2Index Pipe: to look up the word
alphabet to get the indices of the words.
3. WeightByFrequency Pipe: to calculate
the vector weight for each word accord-
ing to its frequency of occurrence.
With the pipeline structure, the data pre-
processing component has good flexibility, ex-
tensibility and reusability.
2.2 Machine Learning Component
The outputs of NLP are often structured,
so the structured learning is our core module.
Structured learning is the task of assigning a
structured label y to an input x. The label y
can be a discrete variable, a sequence, a tree
or a more complex structure.
To illustrate by a sample x, we define the
feature as ?(x,y). Thus, we can label x with
a score function,
y? = arg max
y
F (w,?(x,y)), (1)
where w is the parameter of function F (?).
The feature vector ?(x,y) consists of lots of
overlapping features, which is the chief benefit
of a discriminative model.
For example, in sequence labeling, both x =
x1, . . . , xL and y = y1, . . . , yL are sequences.
For first-order Markov sequence labeling, the
feature can be denoted as ?k(yi?1, yi,x, i),
where i is the position in the sequence. Then
the score function can be rewritten as
y? = arg max
y
F (
L?
i=1
?
k
wk?k(yi?1, yi,x, i)), (2)
where L is the length of x.
Different algorithms vary in the definition of
F (?) and the corresponding objective function.
50
F (?) is usually defined as a linear or exponen-
tial family function. For example, in condi-
tional random fields (CRFs) (Lafferty et al,
2001), F (?) is defined as:
Pw(y|x) =
1
Zw
exp(wT?(x,y)), (3)
where Zw is the normalization constant such
that it makes the sum of all the terms one.
In FudanNLP, the linear function is univer-
sally used as the objective function. Eq. (1) is
written as:
y? = arg max
y
< w,?(x,y) > . (4)
2.2.1 Training
In the training stage, we use the passive-
aggressive algorithm to learn the model pa-
rameters. Passive-aggressive (PA) algorithm
(Crammer et al, 2006) was proposed for nor-
mal multi-class classification and can be easily
extended to structure learning (Crammer et
al., 2005). Like Perceptron, PA is an online
learning algorithm.
2.2.2 Inference
For consistency with statistical machine
learning, we call the process to calculate the
Eq.(1) as ?inference?. In structured learning,
the number of possible solutions is very huge,
so dynamic programming or approximate ap-
proaches are often used for efficiency. For NLP
tasks, the most popular structure is sequence.
To label the sequence, we use Viterbi dynamic
programming to solve the inference problem in
Eq. (4).
Our system can support any order of Viterbi
decoding. In addition, we also implement a
constrained Viterbi algorithm to reduce the
number of possible solutions by pre-defined
rules. For example, when we know the prob-
able labels, we delete the unreachable states
from state transition matrix. It is very useful
for CWS and POS tagging with sequence la-
beling. When we have a word dictionary or
know the POS for some words, we can get
more accurate results.
2.2.3 Other Algorithms
Apart from the core modules of structured
learning, our system also includes several tra-
ditional machine learning algorithms, such as
Perceptron, Adaboost, kNN, k-means, and so
on.
2.3 Natural Language Processing
Components
Our toolkit provides the basic NLP func-
tions, such as word segmentation, part-of-
speech tagging, named entity recognition, syn-
tactic parsing, temporal phrase recognition,
anaphora resolution, and so on. These func-
tions are trained on our developed corpus. We
also develop a visualization module to display-
ing the output. Table 1 shows the output rep-
resentation of our toolkit.
2.3.1 Chinese Word Segmentation
Different from English, Chinese sentences
are written in a continuous sequence of char-
acters without explicit delimiters such as the
blank space. Since the meanings of most Chi-
nese characters are not complete, words are
the basic syntactic and semantic units. There-
fore, it is indispensable step to segment the
sentence into words in Chinese language pro-
cessing.
We use character-based sequence labeling
(Peng et al, 2004) to find the boundaries of
words. Besides the carefully chosen features,
we also use the meaning of character drawn
from HowNet(Dong and Dong, 2006), which
improves the performance greatly. Since un-
known words detection is still one of main chal-
lenges of Chinese word segmentation. We im-
plement a constrained Viterbi algorithm to al-
low users to add their own word dictionary.
2.3.2 POS tagging
Chinese POS tagging is very different from
that in English. There are no morphological
changes for a word among its different POS
tags. Therefore, most of Chinese words may
have multiple POS tags. For example, there
are different morphologies in English for the
word ??? (destroy)?, such as ?destroyed?,
?destroying? and ?destruction?. But in Chi-
nese, there is just one same form(Xia, 2000).
There are two popular guidelines to tag the
word?s POS: CTB (Xia, 2000) and PKU (Yu
et al, 2001). We take into account both
the weaknesses and the strengths of these two
guidelines, and propose our guideline for bet-
ter subsequent analyses, such as parser and
named entity recognition. For example, the
proper name is labeled as ?NR? in CTB, while
we label it with one of four categories: person,
51
Input:
??????????? 1980 ??
John is from Washington, and he was born in 1980.
Output:
.
.?? .?? .??? .? .? .?? .1980 ? .?
.John .is from .Washington ., .he .was born in .1980 ..
.PER .VV .LOC .PU .PRN .NN .PU
.1 .2 .3 .4 .5 .6 .7 .8
Root
SUB
CS:COO1
OBJ
PUN
SUB OBJ
PUN
NER:
1 ? PER
3 ? LOC
AR:
5 ? 1
TIME:
7 ? 1980
1 CS:COO means the coordinate complex sentence.
Table 1: Example of the output representation of our toolkit
location, organization and other proper name.
Conversely, we merge the ?VC? and ?VE? into
?VV? since there is no link verb in Chinese.
Finally, we use a tag set with 39 categories in
total.
Since a POS tag is assigned to each word,
not to each character, Chinese POS tag-
ging has two ways: pipeline method or joint
method. Currently, the joint method is more
popular and effective because it uses more flex-
ible features and can reduce the error propa-
gation (Ng and Low, 2004). In our system,
we implement both methods for POS tagging.
Besides, we also use some knowledge to im-
prove the performance, such as Chinese sur-
name and the common suffixes of the names
of locations and organizations.
2.3.3 Named Entity Recognition
In Chinese named entity recognition (NER),
there are usually three kinds of named enti-
ties (NEs) to be dealt with: names of per-
sons (PER) , locations (LOC) and organiza-
tions (ORG). Unlike English, there is no obvi-
ous identification for NEs, such as initial capi-
tals. The internal structures are also different
for different kinds of NEs, so it is difficult to
build a unified model for named entity recog-
nition.
Our NER is based on the results of POS
tagging and uses some customize features to
detect NEs. First, the number of NEs is very
large and the new NEs are endlessly emerg-
ing, so it is impossible to store them in dic-
tionary. Since the internal structures are rela-
tively more important, we use language mod-
els to capture the internal structures. Second,
we merge the continuous NEs with some rule-
based strategies. For example, we combine the
continuous words ???/NN???/NN? into
? ?????/LOC?.
2.3.4 Dependency parsing
Our syntactic parser is currently a depen-
dency parser, which is implemented with the
shift-reduce deterministic algorithm based on
the work in (Yamada and Matsumoto, 2003).
The syntactic structure of Chinese is more
complex than that of English, and semantic
meaning is more dominant than syntax in Chi-
nese sentences. So we select the dependency
parser to avoid the minutiae in syntactic con-
stituents and wish to pay more attention to
the subsequent semantic analysis. Since the
structure of the Chinese language is quite dif-
ferent from that of English, we use more effec-
tive features according to the characteristics of
Chinese sentences.
The common used corpus for Chinese de-
pendency parsing is CoNLL corpus (Haji? et
al., 2009). However, there are some illogical
cases in CoNLL corpus. For example, the
head words are often interrogative particles
and punctuations. Our guideline is based on
common understanding for Chinese grammar.
The Chinese syntactic components usually in-
clude subject, predicate, object, attribute, ad-
verbial modifier and complement. Figure 2
and 3 show the differences between the trees of
CoNLL and our Corpus. Table 2 shows some
52
primary dependency relations in our guideline.
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.VV .VV .NR .VV .NN .SP .PU
Root
COMP
ADV
COMP
COMP
COMP UNK
Figure 2: Dependency Tree in CoNLL Corpus
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.MD .VV .LOC .VV .NN .SP .PU
Root
ADV
OBJ
OBJ
OBJ
VOC
PUN
Figure 3: Dependency Tree in Our Corpus
Relations Chinese Definitions
SUB ?? Subject
PRED ?? Predicate
OBJ ?? Object
ATT ?? Attribute
ADV ?? Adverbial Modifier
COMP ?? Complement
SVP ?? Serial Verb Phrases
SUB-OBJ ?? Pivotal Construction
VOC ?? Voice
TEN ?? Tense
PUN ?? Punctuation
Table 2: Some primary dependency relations
2.3.5 Temporal Phrase Recognition
and Normalization
Chinese temporal phrases is more flexible
than English. Firstly, there are two calendars:
Gregorian and lunar calendars. Both of them
are frequently used. Secondly, the forms of
same temporal phrase are various, which often
consists of Chinese characters, Arabic numer-
als and English letters, such as ??? 10 ??
and ?10:00 PM?.
Different from the general process based
on machine learning, we implement the time
phrase recognizer with a rule-based method.
These rules include 376 regular expressions
and nearly a hundred logical judgments.
After recognizing the temporal phrases, we
normalize them with a standard time format.
For a phrase indicating a relative time , such
as ????? and ? ?????, we first find the
base time in the context. If no base time is
found, or there is also no temporal phrase to
indicate the base time (such as ????), we
set the base time to the current system time.
Table 3 gives examples for our temporal phrase
recognition module.
Input:
08 ?????????8 ? 8 ??????????
????????????
The Beijing Olympic Games took place from Au-
gust 8, 2008. Four years later, the London Olympic
Games took place from July 21.
???????? 9 ?????????????
I?m busy today, and have to come off duty after 9:00
PM. And I also have to work this Sunday.
Output:
08 ? (2008) 2008
8 ? 8 ? (August 8) 2008-8-8
?????? (July 21) 2012-7-27
?? (today) 2012-2-221
?? 9 ? (9:00 PM) 2012-2-22 21:00
?? (this Sunday) 2012-2-26
1 The base time is 2012-02-22 10:00AM.
Table 3: Examples for Temporal Phrase
Recognition
2.3.6 Anaphora Resolution
Anaphora resolution is to detect the pro-
nouns and find what they are referring to.
We first find all pronouns and entity names,
then use a classifier to predict whether there
is a relation between each pair of pronoun and
entity name. Table 4 gives examples for our
anaphora resolution module.
Input:
??????? 1167 ?????????????
?????????????
Oxford University is founded in 1167. It is located
in Oxford, UK. The university has nurtured a lot
of good students.
Output:
? (It) ????
???? (The
university)
???? (Oxford University)
Table 4: Examples for Anaphora Resolution
3 System Performances
In this section, we investigate the per-
formances for the six tasks: Chinese word
segmentation (CWS), POS tagging (POS),
53
named entity recognition (NER) and de-
pendency parser(DePar), Temporal Phrase
Recognition (TPR) and Anaphora Resolution
(AR). We use 5-fold cross validation on our
developed corpus. The corpus includes 65, 745
sentences and 959, 846 words. The perfor-
mances are shown in Table 5.
Task Accuracy Speed1 Memory
CWS 97.5% 98.9K 66M
POS 93.4% 44.5K 110M
NER 98.40% 38K 30M
DePar 85.3% 21.1 80M
TPR 95.16% 22.9k 237K
AR 70.3% 35.7K 52K
1 characters per second. Test environment:
CPU 2.67GHz, JRE 7.
Table 5: System Performances
4 Usages
We provide three ways to use our toolkit.
Firstly, our toolkit can be used as library.
Users can call application programming inter-
faces (API) in their own applications.
Secondly, users can also invoke the main
NLP modules to process the inputs (strings
or files) from the command line directly.
Thirdly, the web services are provided
for platform-independent and language-
independent use. We use a REST (Represen-
tational State Transfer) architecture, in which
the web services are viewed as resources and
can be identified by their URLs.
5 Conclusions
In this demonstration, we have described
the system, FudanNLP, which is a Java-based
open source toolkit for Chinese natural lan-
guage processing. In the future, we will add
more functions, such as semantic parsing. Be-
sides, we will also optimize the algorithms and
codes to improve the system performances.
Acknowledgments
We would like to thank all the people6
involved with our FudanNLP project. This
work was funded by NSFC (No.61003091
6https://code.google.com/p/fudannlp/wiki/
People
and No.61073069) and 973 Program
(No.2010CB327900).
References
K. Crammer, R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for struc-
tured classification. In NIPS Workshop on
Learning With Structured Outputs. Citeseer.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of
Machine Learning Research, 7:551?585.
Z. Dong and Q. Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Pub-
lishing Co., Inc. River Edge, NJ, USA.
J. Haji?, M. Ciaramita, R. Johansson, D. Kawa-
hara, M.A. Mart?, L. M?rquez, A. Meyers,
J. Nivre, S. Pad?, J. ?t?p?nek, et al 2009. The
CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, pages 1?18. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
H.T. Ng and J.K. Low. 2004. Chinese part-
of-speech tagging: one-at-a-time or all-at-once?
word-based or character-based. In Proceedings
of EMNLP, volume 4.
F. Peng, F. Feng, and A. McCallum. 2004. Chi-
nese segmentation and new word detection us-
ing conditional random fields. Proceedings of the
20th international conference on Computational
Linguistics.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
H. Yamada and Y. Matsumoto. 2003. Statis-
tical dependency analysis with support vector
machines. In Proceedings of the International
Workshop on Parsing Technologies (IWPT),
volume 3.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern chinese corpus. Technical
report, Technical report.
54
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32?39,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Hedge Cues and their Scopes with Average Perceptron
Feng Ji, Xipeng Qiu, Xuanjing Huang
Fudan University
{fengji,xpqiu,xjhuang}@fudan.edu.cn
Abstract
In this paper, we proposed a hedge de-
tection method with average perceptron,
which was used in the closed challenge
in CoNLL-2010 Shared Task. There are
two subtasks: (1) detecting uncertain sen-
tences and (2) identifying the in-sentence
scopes of hedge cues. We use the unified
learning algorithm for both subtasks since
that the hedge score of sentence can be de-
composed into scores of the words, espe-
cially the hedge words. On the biomedical
corpus, our methods achieved F-measure
with 77.86% in detecting in-domain un-
certain sentences, 77.44% in recognizing
hedge cues, and 19.27% in identifying the
scopes.
1 Introduction
Detecting hedged information in biomedical lit-
eratures has received considerable interest in the
biomedical natural language processing (NLP)
community recently. Hedge information indicates
that authors do not or cannot back up their opin-
ions or statements with facts (Szarvas et al, 2008),
which exists in many natural language texts, such
as webpages or blogs, as well as biomedical liter-
atures.
For many NLP applications, such as question
answering and information extraction, the infor-
mation extracted from hedge sentences would be
harmful to their final performances. Therefore,
the hedge or speculative information should be
detected in advance, and dealt with different ap-
proaches or discarded directly.
In CoNLL-2010 Shared Task (Farkas et al,
2010), there are two different level subtasks: de-
tecting sentences containing uncertainty and iden-
tifying the in-sentence scopes of hedge cues.
For example, in the following sentence:
These results suggest that the IRE motif
in the ALAS mRNA is functional and
imply that translation of the mRNA is
controlled by cellular iron availability
during erythropoiesis.
The words suggest and imply indicate that the
statements are not supported with facts.
In the first subtask, the sentence is considered
as uncertainty.
In the second subtask, suggest and imply are
identified as hedge cues, while the consecutive
blocks suggest that the IRE motif in the ALAS
mRNA is functional and imply that translation of
the mRNA is controlled by cellular iron availabil-
ity during erythropoiesis are recognized as their
corresponding scopes.
In this paper, we proposed a hedge detec-
tion method with average perceptron (Collins,
2002), which was used in the closed challenges in
CoNLL-2010 Shared Task (Farkas et al, 2010).
Our motivation is to use a unified model to de-
tect two level hedge information (word-level and
sentence-level) and the model is easily expanded
to joint learning of two subtasks. Since that the
hedge score of sentence can be decomposed into
scores of the words, especially the hedge words,
we chosen linear classifier in our method and used
average perceptron as the training algorithm.
The rest of the paper is organized as follows. In
Section 2, a brief review of related works is pre-
sented. Then, we describe our method in Section
3. Experiments and results are presented in the
section 4. Finally, the conclusion will be presented
in Section 5.
2 Related works
Although the concept of hedge information has
been introduced in linguistic community for a
long time, researches on automatic hedge detec-
tion emerged from machine learning or compu-
32
tational linguistic perspective in recent years. In
this section, we give a brief review on the related
works.
For speculative sentences detection, Medlock
and Briscoe (2007) report their approach based
on weakly supervised learning. In their method,
a statistical model is initially derived from a seed
corpus, and then iteratively modified by augment-
ing the training dataset with unlabeled samples
according the posterior probability. They only
employ bag-of-words features. On the public
biomedical dataset1, their experiments achieve the
performance of 0.76 in BEP (break even point).
Although they also introduced more linguistic fea-
tures, such as part-of-speech (POS), lemma and
bigram (Medlock, 2008), there are no significant
improvements.
In Ganter and Strube (2009), the same task on
Wikipedia is presented. In their system, score of a
sentence is defined as a normalized tangent value
of the sum of scores over all words in the sentence.
Shallow linguistic features are introduced in their
experiments.
Morante and Daelemans (2009) present their re-
search on identifying hedge cues and their scopes.
Their system consists of several classifiers and
works in two phases, first identifying the hedge
cues in a sentence and secondly finding the full
scope for each hedge cue. In the first phase, they
use IGTREE algorithm to train a classifier with
3 categories. In the second phase, three different
classifiers are trained to find the first token and last
token of in-sentence scope and finally combined
into a meta classifier. The experiments shown
that their system achieves an F1 of nearly 0.85
of identifying hedge cues in the abstracts sub cor-
pus, while nearly 0.79 of finding the scopes with
predicted hedge cues. More experiments could
be found in their paper (Morante and Daelemans,
2009). They also provide a detail statistics on
hedge cues in BioScope corpus2.
3 Hedge detection with average
perceptron
3.1 Detecting uncertain sentences
The first subtask is to identify sentences con-
taining uncertainty information. In particular,
1http://www.benmedlock.co.uk/
hedgeclassif.html
2http://www.inf.u-szeged.hu/rgai/
bioscope
this subtask is a binary classification problem at
sentence-level.
We define the score of sentence as the confi-
dence that the sentence contains uncertainty infor-
mation.
The score can be decomposed as the sum of the
scores of all words in the sentence,
S(x, y) = ?
xi?x
s(xi, y) =
?
xi?x
wT?(xi, y)
where, x denotes a sentence and xi is the i-
th word in the sentence x, ?(xi, y) is a sparse
high-dimensional binary feature vector of word xi.
y ? {uncertain, certain} is the category of the
sentence. For instance, in the example sentence,
if current word is suggest while the category of
this sentence is uncertain, the following feature is
hired,
?n(xi, y) =
{
1, if xi=??suggest??y=??uncertain?? ,
0, otherwise
where n is feature index.
This representation is commonly used in struc-
tured learning algorithms. We can combine the
features into a sparse feature vector ?(x, y) =?
i ?(xi, y).
S(x, y) = wT?(x, y) = ?
xi?x
wT?(xi, y)
In the predicting phase, we assign x to the cate-
gory with the highest score,
y? = argmax
y
wT?(x, y)
We learn the parameters w with online learning
framework. The most common online learner is
the perceptron (Duda et al, 2001). It adjusts pa-
rameters w when a misclassification occurs. Al-
though this framework is very simple, it has been
shown that the algorithm converges in a finite
number of iterations if the data is linearly separa-
ble. Moreover, much less training time is required
in practice than the batch learning methods, such
as support vector machine (SVM) or conditional
maximum entropy (CME).
Here we employ a variant perceptron algorithm
to train the model, which is commonly named
average perceptron since it averages parameters
w across iterations. This algorithm is first pro-
posed in Collins (2002). Many experiments of
33
NLP problems demonstrate better generalization
performance than non averaged parameters. More
theoretical proofs can be found in Collins (2002).
Different from the standard average perceptron al-
gorithm, we slightly modify the average strategy.
The reason to this modification is that the origi-
nal algorithm is slow since parameters accumulate
across all iterations. In order to keep fast training
speed and avoid overfitting at the same time, we
make a slight change of the parameters accumu-
lation strategy, which occurs only after each iter-
ation over the training data finished. Our training
algorithm is shown in Algorithm 1.
input : training data set:
(xn, yn), n = 1, ? ? ? , N ,
parameters: average number: K,
maximum iteration number: T .
output: average weight: cw
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt, yt);
predict: y?t = argmaxy wTt ?(xt, y) ;
if y?t 6= yt then
wt+1 = wt+?(xt, yt)??(xt, y?t)
end
end
cw = cw +wT ;
end
cw = cw/K ;
Algorithm 1: Average Perceptron algorithm
Binary context features are extracted from 6
predefined patterns, which are shown in Figure 1.
By using these patterns, we can easily obtain the
complicate features. As in the previous example,
if the current word is suggest, then a new com-
pound feature could be extracted in the form of
w?1 =results//w0 =suggest by employing the pat-
tern w?1w0. // is the separate symbol.
3.2 Identifying hedge cues and their scopes
Our approach for the second subtask consists of
two phases: (1) identifying hedge cues in a sen-
tence, then (2) recognizing their corresponding
scopes.
3.2.1 Identifying hedge cues
Hedge cues are the most important clues for de-
termining whether a sentence contains uncertain
? unigram: w0,p0
? bigram: w0w1, w0p0, p0p1
? trigram: w?1w0w1
Figure 1: Patterns employed in the sentence-level
hedge detection. Here w denotes single word, p is
part of speech, and the subscript denotes the rela-
tive offset compared with current position.
? unigram: w?2, w?1, w0, w1, w2, p0
? bigram: w?1w0, w0w1, w0p0, p?1p0, p0p1
? trigram: w?1w0w1
Figure 2: Patterns employed in the word-level
hedge detection.
information. Therefore in this phase, we treat the
problem of identifying hedge cues as a classifica-
tion problem. Each word in a sentence would be
predicted a category indicating whether this word
is a hedge cue word or not. In the previous ex-
ample, there are two different hedge cues in the
sentence (show in bold manner). Words suggest
and imply are assigned with the category CUE de-
noting hedge cue word, while other words are as-
signed with label O denoting non hedge cue word.
In our system, this module is much similar to
the module of detecting uncertain sentences. The
only difference is that this phase is word level. So
that each training sample in this phase is a word,
while in detecting speculative sentences training
sample is a sentence. The training algorithm is the
same as the algorithm shown in Algorithm 1. 12
predefined patterns of context features are shown
in Figure 2.
3.2.2 Recognizing in-sentence scopes
After identifying the hedge cues in the first phase,
we need to recognize their corresponding in-
sentence scopes, which means the boundary of
scope should be found within the same sentence.
We consider this problem as a word-cue pair
classification problem, where word is any word
in a sentence and cue is the identified hedge cue
word. Similar to the previous phase, a word-level
linear classifier is trained to predict whether each
34
word-cue pair in a sentence is in the scope of the
hedge cue.
Besides base context features used in the pre-
vious phase, we introduce additional syntactic de-
pendency features. These features are generated
by a first-order projective dependency parser (Mc-
Donald et al, 2005), and listed in Figure 3.
The scopes of hedge cues are always covering
a consecutive block of words including the hedge
cue itself. The ideal method should recognize only
one consecutive block for each hedge cue. How-
ever, our classifier cannot work so well. Therefore,
we apply a simple strategy to process the output
of the classifier. The simple strategy is to find a
maximum consecutive sequence which covers the
hedge cue. If a sentence is considered to contain
several hedge cues, we simply combine the con-
secutive sequences, which have at least one com-
mon word, to a large block and assign it to the
relative hedge cues.
4 Experiments
In this section, we report our experiments on
datasets of CoNLL-2010 shared tasks, including
the official results and our experimental results
when developing the system.
Our system architecture is shown in Figure 4,
which consists of the following modules.
1. corpus preprocess module, which employs a
tokenizer to normalize the corpus;
2. sentence detection module, which uses a bi-
nary sentence-level classifier to determine
whether a sentence contains uncertainty in-
formation;
3. hedge cues detection module, which identi-
fies which words in a sentence are the hedge
cues, we train a binary word-level classifier;
4. cue scope recognition module, which recog-
nizes the corresponding scope for each hedge
cue by another word-level classifier.
Our experimental results are obtained on the
training datasets by 10-fold cross validation. The
maximum iteration number for training the aver-
age perceptron is set to 20. Our system is imple-
mented with Java3.
3http://code.google.com/p/fudannlp/
biomedical Wikipedia
#sentences 14541 11111
#words 382274 247328
#hedge sentences 2620 2484
%hedge sentences 0.18 0.22
#hedge cues 3378 3133
average number 1.29 1.26
average cue length 1.14 2.45
av. scope length 15.42 -
Table 1: Statistical information on annotated cor-
pus.
4.1 Datasets
In CoNLL-2010 Shared Task, two different
datasets are provided to develop the system: (1)
biological abstracts and full articles from the Bio-
Scope corpus, (2) paragraphs from Wikipedia. Be-
sides manually annotated datasets, three corre-
sponding unlabeled datasets are also allowed for
the closed challenges. But we have not employed
any unlabeled datasets in our system.
A preliminary statistics can be found in Ta-
ble 1. We make no distinction between sen-
tences from abstracts or full articles in biomedi-
cal dataset. From Table 1, most sentences are cer-
tainty while about 18% sentences in biomedical
dataset and 22% in Wikipedia dataset are spec-
ulative. On the average, there exists nearly 1.29
hedge cues per sentence in biomedical dataset and
1.26 in Wikipedia. The average length of hedge
cues varies in these two corpus. In biomedical
dataset, hedge cues are nearly one word, but more
than two words in Wikipedia. On average, the
scope of hedge cue covers 15.42 words.
4.2 Corpus preprocess
The sentence are processed with a maximum-
entropy part-of-speech tagger4 (Toutanova et al,
2003), in which a rule-based tokenzier is used to
separate punctuations or other symbols from reg-
ular words. Moreover, we train a first-order pro-
jective dependency parser with MSTParser5 (Mc-
Donald et al, 2005) on the standard WSJ training
corpus, which is converted from constituent trees
to dependency trees by several heuristic rules6.
4http://nlp.stanford.edu/software/
tagger.shtml
5http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
6http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
35
? word-cue pair: current word and the hedge cue word pair,
? word-cue POS pair: POS pair of current word and the hedge cue word,
? path of POS: path of POS from current word to the hedge cue word along dependency
tree,
? path of dependency: relation path of dependency from current word to the hedge cue
word along dependency tree,
? POS of hedge cue word+direction: POS of hedge cue word with the direction to the
current word. Here direction can be ?LEFT? if the hedge cue is on the left to the current
word, or ?RIGHT? on the right,
? tree depth: depth of current in the corresponding dependency tree,
? surface distance: surface distance between current word and the hedge cue word. The
value of this feature is always 10 in the case of surface distance greater than 10,
? surface distance+tree depth: combination of surface distance and tree depth
? number of punctuations: number of punctuations between current word and the hedge
cue word,
? number of punctuations + tree depth: combination of number of punctuations and tree
depth
Figure 3: Additional features used in recognizing in-sentence scope
4.3 Uncertain sentences detection
In the first subtask, we carry out the experiments
within domain and cross domains. As previously
mentioned, we do not use the unlabeled datasets
and make no distinction between abstracts and full
articles in biomedical dataset. This means we
train the models only with the official annotated
datasets. The model for cross-domain is trained
on the combination of annotated biomedical and
Wikipedia datasets.
In this subtask, evaluation is carried out on the
sentence level and F-measure of uncertainty sen-
tences is employed as the chief metric.
Table 2 shows the results within domain. Af-
ter 10-fold cross validation over training dataset,
we achieve 84.39% of F1-measure on biomedical
while 56.06% on Wikipedia.
We analyzed the low performance of our sub-
mission result on Wikipedia. The possible rea-
son is our careless work when dealing with the
trained model file. Therefore we retrain a model
for Wikipedia and the performance is listed on the
bottom line (Wikipedia?) in Table 2.
Dataset Precision Recall F1
10-fold cross validation
biomedical 91.03 78.66 84.39
Wikipedia 66.54 48.43 56.06
official evaluation
biomedical 79.45 76.33 77.86
Wikipedia 94.23 6.58 1.23
Wikipedia? 82.19 32.86 46.95
Table 2: Results for in-domain uncertain sentences
detection
Table 3 shows the results across domains. We
split each annotated dataset into 10 folds. Then
training dataset is combined by individually draw-
ing 9 folds out from the split datasets and the
rests are used as the test data. On biomedical
dataset, F1-measure gets to 79.24% while 56.16%
on Wikipedia dataset. Compared with the results
within domain, over 5% performance decreases
from 84.39% to 79.24% on biomedical, but a
slightly increase on Wikipedia.
36
Figure 4: System architecture of our system
Dataset Precision Recall F1
10-fold cross validation
biomedical 87.86 72.16 79.24
Wikipedia 67.78 47.95 56.16
official evaluation
biomedical 62.81 79.11 70.03
Wikipedia 62.66 55.28 58.74
Table 3: Results for across-domain uncertain sen-
tences detection
4.3.1 Results analysis
We investigate the weights of internal features and
found that the words, which have no uncertainty
information, also play the significant roles to pre-
dict the uncertainty of the sentence.
Intuitively, the words without uncertainty infor-
mation should just have negligible effect and the
corresponding features should have low weights.
However, this ideal case is difficult to reached by
learning algorithm due to the sparsity of data.
In Table 4, we list the top 10 words involved
in features with the largest weights for each cate-
gory. These words are ranked by the accumulative
scores of their related features.
In Table 5, we list the top 10 POS involved in
features with the largest weight for each category.
4.4 Hedge cue identification
Hedge cues identification is one module for the
second subtask, we also analyze the performance
on this module.
Since we treat this problem as a binary classi-
fication problem, we evaluate F-measure of hedge
cue words. The results are listed in Table 6.
We have to point out that our evaluation is
Dataset Precision Recall F1
10-fold cross validation(word-level)
biomedical 90.15 84.43 87.19
Wikipedia 57.74 39.81 47.13
official evaluation(phrase-level)
biomedical 78.7 76.22 77.44
Table 6: Results for in-domain hedge cue identifi-
cation
based on word while official evaluation is based
on phrase. That means our results would seem
to be higher than the official results, especially on
Wikipedia dataset because average length of hedge
cues in Wikipedia dataset is more than 2 words.
4.4.1 Result Analysis
We classify the results into four categories: false
negative, false positive, true positive and true neg-
ative. We found that most mistakes are made be-
cause of polysemy and collocation.
In Table 7, we list top 10 words for each cate-
gory. For the false results, the words are difficult to
distinguish without its context in the correspond-
ing sentence.
4.5 Scopes recognition
For recognizing the in-sentence scopes, F-measure
is also used to evaluate the performance of the
word-cue pair classifier. The results using gold
hedge cues are shown in Table 8. From the re-
sults, F-measure achieves respectively 70.44% and
75.94% when individually using the base context
features extracted by 12 predefined patterns (see
Figure 1) and syntactic dependency features (see
Figure 3), while 79.55% when using all features.
The results imply that syntactic dependency
37
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
whether show probably the other suggest show
may demonstrate some often whether used to
suggest will many patients probably was
likely can one of another indicate CFS
indicate role believed days appear demonstrate
possible found possibly CFS putative the other
putative human considered are some of all
appear known to such as any other thought ?:?
thought report several western possibly people
potential evidence said to pop likely could not
Table 4: Top 10 significant words in detecting uncertain sentences
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
MD SYM RB VBZ JJS SYM
VBG PRP JJS CD RBS ?:?
VB NN RBS ?:? RB JJR
VBZ CD FW WRB EX WDT
IN WDT VBP PRP CC CD
Table 5: Top 5 significant POS in detecting uncertain sentences
Dataset Precision Recall F1
base context features
biomedical 66.04 75.48 70.44
syntactic dependency features
biomedical 93.77 63.05 75.94
all features
biomedical 78.72 80.41 79.55
Table 8: Results for scopes recognizing with gold
hedge cues (word-level)
features contribute more benefits to recognize
scopes than surface context features.
Official results evaluated at block level are also
listed in Table 9.
dataset Precision Recall F1
biomedical 21.87 17.23 19.27
Table 9: Official results for scopes recognizing
(block level)
From Table 9 and the official result on hedge
cue identification in Table 6, we believe that our
post-processing strategy would be responsible for
the low performance on recognizing scopes. Our
strategy is to find a maximum consecutive block
covering the corresponding hedge cue. This strat-
egy cannot do well with the complex scope struc-
ture. For example, a scope is covered by another
scope. Therefore, our system would generate a
block covering all hedge cues if there exists more
than one hedge cues in a sentence.
5 Conclusion
We present our implemented system for CoNLL-
2010 Shared Task in this paper. We introduce
syntactic dependency features when recognizing
hedge scopes and employ average perceptron al-
gorithm to train the models. On the biomedi-
cal corpus, our system achieves F-measure with
77.86% in detecting uncertain sentences, 77.44%
in recognizing hedge cues, and 19.27% in identi-
fying the scopes.
Although some results are low and beyond our
expectations, we believe that our system can be at
least improved within the following fields. Firstly,
we would experiment on other kinds of features,
such as chunk or named entities in biomedical.
Secondly, the unlabeled datasets would be ex-
plored in the future.
38
False Negative False Positive True Positive True Negative
support considered suggesting chemiluminescence
of potential may rhinitis
demonstrate or proposal leukemogenic
a hope might ribosomal
postulate indicates indicating bp
supports expected likely nc82
good can appear intronic/exonic
advocates should possible large
implicated either speculate allele
putative idea whether end
Table 7: Top 10 words with the largest scores for each category in hedge cue identification
Acknowledgments
This work was funded by Chinese NSF (Grant
No. 60673038), 973 Program (Grant No.
2010CB327906, and Shanghai Committee of Sci-
ence and Technology(Grant No. 09511501404).
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, pages 1?8. Associa-
tion for Computational Linguistics.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley, New York.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the ACL, pages
91?98. Association for Computational Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999. Association for Computational Lin-
guistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
39
Adaptive Chinese Word Segmentation with
Online Passive-Aggressive Algorithm
Wenjun Gao
School of Computer Science
Fudan University
Shanghai, China
wjgao616@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
In this paper, we describe our system1
for CIPS-SIGHAN-2010 bake-off task of
Chinese word segmentation, which fo-
cused on the cross-domain performance
of Chinese word segmentation algorithms.
We use the online passive-aggressive al-
gorithm with domain invariant informa-
tion for cross-domain Chinese word seg-
mentation.
1 Introduction
In recent years, Chinese word segmentation
(CWS) has undergone great development (Xue,
2003; Peng et al, 2004). The popular method is
to regard word segmentation as a sequence label-
ing problems. The goal of sequence labeling is to
assign labels to all elements of a sequence.
Due to the exponential size of the output
space, sequence labeling problems tend to be
more challenging than the conventional classifi-
cation problems. Many algorithms have been
proposed and the progress has been encourag-
ing, such as SVMstruct (Tsochantaridis et al,
2004), conditional random fields (CRF) (Lafferty
et al, 2001), maximum margin Markov networks
(M3N) (Taskar et al, 2003) and so on. After years
of intensive researches, Chinese word segmenta-
tion achieves a quite high precision. However, the
performance of segmentation is not so satisfying
for out-of-domain text.
There are two domains in domain adaption
problem, a source domain and a target domain.
When we use the machine learning methods for
1Available at http://code.google.com/p/
fudannlp/
Chinese word segmentation, we assume that train-
ing and test data are drawn from the same distri-
bution. This assumption underlies both theoreti-
cal analysis and experimental evaluations of learn-
ing algorithms. However, the assumption does
not hold for domain adaptation(Ben-David et al,
2007; Blitzer et al, 2006). The challenge is the
difference of distribution between the source and
target domains.
In this paper, we use online margin max-
imization algorithm and domain invariant fea-
tures for domain adaptive CWS. The online learn-
ing algorithm is Passive-Aggressive (PA) algo-
rithm(Crammer et al, 2006), which passively ac-
cepts a solution whose loss is zero, while it ag-
gressively forces the new prototype vector to stay
as close as possible to the one previously learned.
The rest of the paper is organized as follows.
Section 2 introduces the related works. Then we
describe our algorithm in section 3 and 4. The
feature templates are described in section 5. Sec-
tion 6 gives the experimental analysis. Section 7
concludes the paper.
2 Related Works
There are several approaches to deal with the do-
main adaption problem.
The first approach is to use semi-supervised
learning (Zhu, 2005).
The second approach is to incorporate super-
vised learning with domain invariant information.
The third approach is to improve the present
model with a few labeled domain data.
Altun et al (2006) investigated structured clas-
sification in a semi-supervised setting. They pre-
sented a discriminative approach that utilizes the
intrinsic geometry of inputs revealed by unlabeled
data points and we derive a maximum-margin for-
mulation of semi-supervised learning for struc-
tured variables.
Self-training (Zhu, 2005) is also a popular tech-
nology. In self-training a classifier is first trained
with the small amount of labeled data. The clas-
sifier is then used to classify the unlabeled data.
Typically the most confident unlabeled points, to-
gether with their predicted labels, are added to the
training set. The classifier is re-trained and the
procedure repeated. Note the classifier uses its
own predictions to teach itself. Yarowsky (1995)
uses self-training for word sense disambiguation,
e.g. deciding whether the word plant means a liv-
ing organism or a factory in a given context.
Zhao and Kit (2008) integrated unsupervised
segmentation and CRF learning for Chinese word
segmentation and named entity recognition. They
found word accessory variance (Feng et al, 2004)
is useful to CWS.
3 Online Passive-Aggressive Algorithm
Sequence labeling, the task of assigning labels
y = y1, . . . , yL to an input sequence x =
x1, . . . , xL.
Give a sample (x,y), we define the feature is
?(x,y). Thus, we can label x with a score func-
tion,
y? = argmax
z
F (w,?(x, z)), (1)
where w is the parameter of function F (?).
The score function of our algorithm is linear
function.
Given an example (x,y), y? is denoted as the
incorrect label with the highest score,
y? = argmax
z 6=y
wT?(x, z). (2)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = wT?(x,y)?wT?(x, y?). (3)
Thus, we calculate the hinge loss.
`(w; (x,y) =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(4)
We use the online PA learning algorithm to
learn the weights of features. In round t, we find
new weight vector wt+1 by
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C ? ?,
s.t. `(w; (xt,yt)) <= ? and ? >= 0 (5)
where C is a positive parameter which controls
the influence of the slack term on the objective
function.
The algorithms goal is to achieve a margin at
least 1 as often as possible, thus the Hamming loss
is also reduced indirectly. On rounds where the
algorithm attains a margin less than 1 it suffers an
instantaneous loss.
We abbreviate `(wt; (x, y)) to `t. If `t = 0
then wt itself satisfies the constraint in Eq. (5)
and is clearly the optimal solution. We therefore
concentrate on the case where `t > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (5) to be
L(w, ?, ?, ?) = 1
2
||w ?wt||2 + C ? ?
+ ?(`t ? ?)? ??
s.t. ? >= 0, ? >= 0. (6)
where ?, ? is a Lagrange multiplier.
Setting the partial derivatives of L with respect
to the elements of ? to zero gives
? + ? = C. (7)
The gradient of w should be zero,
w ? wt ? ?(?(x,y) ? ?(x, y?)) = 0, (8)
we get
w = wt + ?(?(x,y) ? ?(x, y?)). (9)
Substitute Eq. (7) and Eq. (9) to dual objective
function Eq. (6), we get
L(?) = ?1
2
||?(?(x,y)? ?(x, y?))||2
? ?(wtT (?(x,y)? ?(x, y?)) + ? (10)
Differentiate with ?, and set it to zero, we get
?||?(x,y)? ?(x, y?)||2
+wtT (?(x,y)? ?(x, y?))? 1 = 0. (11)
So,
?? = 1?wt
T (?(x,y)? ?(x, y?))
||?(x,y)? ?(x, y?)||2
. (12)
From ? + ? = C, we know that ? < C, so
??? = min(C, ??). (13)
Finally, we get update strategy,
wt+1 = wt + ???(?(x,y)? ?(x, y?)). (14)
Our final algorithm is shown in Algorithm 1. In
order to avoiding overfitting, the averaging tech-
nology is employed.
input : training data set:
(xn,yn), n = 1, ? ? ? , N , and
parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt,yt);
predict:
y?t = argmax
z 6=yt
?wt,?(xt, z)?;
calculate `(w; (x,y));
update wt+1 with Eq.(14);
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1: Labelwise Margin Maxi-
mization Algorithm
4 Inference
The PA algorithm is used to learn the weights of
features in training procedure. In inference pro-
cedure, we use Viterbi algorithm to calculate the
maximum score label.
Let ?(n) be the best score of the partial label
sequence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(n):
?(n) = max
n?1
(
?(n? 1) +wT?(x, yn, yn?1)
)
(15)
+wt?(x, yn)
Using this recursive definition, we can evalu-
ate ?(N) for all yN , where N is the input length.
This results in the identification of the best label
sequence.
The computational cost of the Viterbi algorithm
is O(NL2), where L is the number of labels.
5 Feature Templates
All feature templates used in this paper are shown
in Table 1. C represents a Chinese character while
the subscript of C indicates its position in the sen-
tence relative to the current character, whose sub-
script is 0. T represents the character-based tag:
?B?, ?B2?, ?B3?, ?M?, ?E? and ?S?, which repre-
sent the beginning, second, third, middle, end or
single character of a word respectively.
The type of character includes: digital, letter,
punctuation and other.
We also use the word accessor variance for do-
main adaption. Word accessor variance (AV) was
proposed by (Feng et al, 2004) and was used to
evaluate how independently a string is used, and
thus how likely it is that the string can be a word.
The accessor variety of a string s of more than one
character is defined as
AV (s) = min{Lav(s), Rav(s)} (16)
Lav(s) is called the left accessor variety and is
defined as the number of distinct characters (pre-
decessors) except ?S? that precede s plus the num-
ber of distinct sentences of which s appears at
the beginning. Similarly, the right accessor va-
riety Rav(s) is defined as the number of distinct
characters (successors) except ?E? that succeed s
plus the number of distinct sentences in which s
appears at the end. The characters ?S? and ?E?
are defined as the begin and end of a sentence.
The word accessor variance was found effective
for CWS with unsegmented text (Zhao and Kit,
2008).
Table 1: Feature templates
Ci, T0, (i = ?1, 0, 1, 2)
Ci, Ci+1, T0, (i = ?2,?1, 0, 1)
T?1,0
Tc: Type of Character
AV : word accessor variance
6 CIPS-SIGHAN-2010 Bakeoff
CIPS-SIGHAN-2010 bake-off task of Chinese
word segmentation focused on the cross-domain
performance of Chinese word segmentation algo-
rithms. There are two subtasks for this evaluation:
(1) Word Segmentation for Simplified Chinese
Text;
(2) Word Segmentation for Traditional Chinese
Text.
The test corpus of each subtask covers four do-
mains: literature, computer science, medicine and
finance.
We participate in closed training evaluation of
both subtasks.
Firstly, we calculate the word accessor variance
AVL(s)of the continuous string s from labeled
corpus. Here, we set the largest length of string
s to be 4.
Secondly, we train our model with feature tem-
ples and AVL(s).
Thirdly, when we process the different domain
unlabeled corpus, we recalculate the word ac-
cessory variance AVU (s) from the corresponding
corpus.
Fourthly, we segment the domain corpus with
new word accessory variance AVU (s) instead of
AVL(s).
The results are shown in Table 2 and 3. The
results show our method has a poor performance
in OOV ( Out-Of-Vocabulary) word.
The running environment is shown in Table 4.
Table 4: Experimental environment
OS Win 2003
CPU Intel Xeon 2.0G
Memory 4G
We set the max iterative number is 20. Our run-
ning time is shown in Table 5. ?s? represents sec-
ond, ?chars? is the number of Chinese character,
and ?MB? is the megabyte. In practice, we found
the system can achieve the same performance af-
ter 7 loops. Therefore, we just need less half the
time in Table 5 actually.
7 Conclusion
In this paper, we describe our system in CIPS-
SIGHAN-2010 bake-off task of Chinese word
segmentation. Although our method just achieve
a consequence of being average and not outstand-
ing, it has an advantage of faster training than
other batch learning algorithm, such as CRF and
M3N.
In the future, we wish to improve our method
in the following aspects. Firstly, we will investi-
gate more effective domain invariant feature rep-
resentation. Secondly, we will integrate our algo-
rithm with self-training and other semi-supervised
learning methods.
Acknowledgments
This work was (partially) funded by 863 Pro-
gram (No. 2009AA01A346), 973 Program (No.
2010CB327906), and Shanghai Science and Tech-
nology Development Funds (No. 08511500302).
References
Altun, Y., D. McAllester, and M. Belkin. 2006. Max-
imum margin semi-supervised learning for struc-
tured variables. Advances in neural information
processing systems, 18:33.
Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.
2007. Analysis of representations for domain adap-
tation. Advances in Neural Information Processing
Systems, 19:137.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128. Association for Computational
Linguistics.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Table 2: Evaluation results on simplified corpus
R P F1 OOV RR IV RR
Best 0.945 0.946 0.946 0.816 0.954
Literature Our 0.915 0.925 0.92 0.577 0.94
Best 0.953 0.95 0.951 0.827 0.975
Computer Our 0.934 0.919 0.926 0.739 0.969
Best 0.942 0.936 0.939 0.75 0.965
Medicine Our 0.927 0.924 0.925 0.714 0.953
Best 0.959 0.96 0.959 0.827 0.972
Finance Our 0.94 0.942 0.941 0.719 0.961
Table 3: Evaluation results on traditional corpus
R P F1 OOV RR IV RR
Best 0.942 0.942 0.942 0.788 0.958
Literature Our 0.869 0.91 0.889 0.698 0.887
Best 0.948 0.957 0.952 0.666 0.977
Computer Our 0.933 0.949 0.941 0.791 0.948
Best 0.953 0.957 0.955 0.798 0.966
Medicine Our 0.908 0.932 0.92 0.771 0.919
Best 0.964 0.962 0.963 0.812 0.975
Finance Our 00.925 0.939 0.932 0.793 0.935
Table 5: Execution time of training and test phase.
Task A B C D
Training Simp 817.2s 795.6s 774.0s 792.0s
Trad 903.6s 889.2s 885.6s 874.8s
Test 20327 chars/s, or 17.97 s/MB
Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Ac-
cessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of the
Eighteenth International Conference on Machine
Learning.
Peng, F., F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceed-
ings of Neural Information Processing Systems.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Pro-
ceedings of the International Conference on Ma-
chine Learning(ICML).
Xue, N. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196. Associ-
ation for Computational Linguistics.
Zhao, H. and C. Kit. 2008. Unsupervised segmenta-
tion helps supervised learning of character tagging
for word segmentation and named entity recogni-
tion. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Zhu, Xiaojin. 2005. Semi-supervised learning
literature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
Triplet-Based Chinese Word Sense Induction
Zhao Liu
School of Computer Science
Fudan University
Shanghai, China
ZLiu.fd@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
This paper describes the implementa-
tion of our system at CLP 2010 bake-
off of Chinese word sense induction.
We first extract the triplets for the tar-
get word in each sentence, then use
the intersection of all related words of
these triplets from the Internet. We
use the related word to construct fea-
ture vectors for the sentence. At last
we discriminate the word senses by
clustering the sentences. Our system
achieved 77.88% F-score under the of-
ficial evaluation.
1 Introduction
The goal of the CLP 2010 bake-off of Chi-
nese word sense induction is to automati-
cally discriminate the senses of Chinese target
words by the use of only un-annotated data.
The use of word senses instead of word
forms has been shown to improve perfor-
mance in information retrieval, information
extraction and machine translation. Word
Sense Disambiguation generally requires the
use of large-scale manually annotated lexical
resources. Word Sense Induction can over-
come this limitation, and it has become one
of the most important topics in current com-
putational linguistics research.
In this paper we introduce a method to
solve the problem of Chinese word sense in-
duction.For this task, Firstly we constructed
triplets containing the target word in every
instance, then searched the intersection of all
the three words from the Internet with web
searching engine and constructed feature vec-
tors.Then we clustered the vectors with the
sIB clustering algorithm and at last discrimi-
nated the word senses.
This paper is organized as following: firstly
we introduce the related works. Then we talk
about the methods in features selection and
clustering. The method of evaluation and the
result of our system is following. At last we
discuss the improvement and the weakness of
our system.
2 Related Works
Sense induction is typically treated as a
clustering problem, by considering their co-
occurring contexts, the instances of a target
word are partitioned into classes. Previous
methods have used the first or second or-
der co-occurrence (Pedersen and Bruce, 1997;
Sch?tze, 1998), parts of speech, and local col-
locations (Niu et al, 2007). The size of con-
text window is also various, it can be as small
as only two words before and after the target
words. It may be the sentence where the tar-
get word is in. Or it will be 20 surrounding
words on either side of the target words and
even more words.
After every instance of the target word is
represented as a feature vector, it will be the
input of the clustering methods. Many clus-
tering methods have been used in the task
of word sense induction. For example, k-
means and agglomerative clustering (Sch?tze,
1998). sIB (Sequential Information Bottle-
neck) a variation of Information Bottleneck is
applied in (Niu et al, 2007). In (Dorow and
Widdows, 2003) Graph-based clustering algo-
rithm is employed that in a graph a node rep-
resents a noun and two nodes have an edge be-
tween them if they co-occur in list more than
a given number of times. A generative model
based on LDA is proposed in (Brody and La-
pata, 2009).
In our method, we use the triplets (Bordag,
2006) and their intersections from the Internet
to construct the feature vectors then sIB is
used as the clustering method.
3 Feature Selection
Our method select the features of the words
similar to (Bordag, 2006) is also using the
triplets. In Chinese there are no natural sep-
arators between the words as English, so the
first step in Chinese language processing is of-
ten the Chinese word segmentation. In our
system we use the FudanNLP toolkit1 to split
the words.
At the first stage, we split the instance of
the target word and filter out the numbers,
English words and stop words from it. So we
get a sequence of the words. Then we select
two words before the target and another two
words after it. If there are no words before or
after then leave it empty. After that we enu-
merate two words from the selected four words
to construct a triplets together with the target
words. So we can get several triplets for every
instance of the target. Because the faulty of
Chinese word segmentation and some special
target word for example a single Chinese char-
acter as a word, there are some errors finding
the position of the target words. If the word
is a single Chinese character and the toolkit
combine it with other Chinese characters to
be a word, we will use that word as the tar-
get instead of the character to construct the
triplets.
The second stage is obtaining corpus from
the Internet. For every triplet we search the
three words sequence in it with a pair of dou-
ble quotation marks in Baidu web searching
engine2. It gives the snippets of the webs
1http://code.google.com/p/fudannlp/
2http://www.baidu.com
which have all the three words in it. We se-
lect the first 50 snippets of each triplets. If
the number of the snippets is less than 50,
we will ignore that triplet. For some rare
words the snippets searched from the Internet
for all the triplets of the instance is less than
50. In that situation we will search the target
word and another one co-occurring word in
the searching engine to achieve enough snip-
pets as features. After searching the triplets
we select the first three triplets (or doublets)
with largest amount of the webs searched by
the searching engine. For every instance there
are three or less triplets (or doublets) and we
have obtained many snippets for them. After
segmenting and filtering these snippets we use
the bag of words from them as the feature for
this instance.
The last stage of feature selection is to con-
struct the feature vector for every instances
containing the target word. In the previous
stage we get a bag of words for each instance.
For all the instances of one target word we
make a statistic of the frequence of each word
in the bags. In our system we select the words
whose frequence is more than 50 as the dimen-
sions for the feature vectors. From the tests
we find that when this thread varies from 50 to
120 the result of our system is nearly the same,
but outside that bound the result will become
rather bad. So we use 50 as the thread. Af-
ter constructing the dimension of that target
word, we can get a feature vector for each in-
stance that at each dimension the number is
the frequence of that word occurs in that po-
sition.
We obtain the feature vectors for the target
words by employing these three stage. The
following work is clustering these vector to get
the classes of the word senses.
4 The Clustering Algorithm
There are many classical clustering methods
such as k-means, EM and so on. In (Niu et
al., 2007) they applied the sIB (Slonim et al,
2002) clustering algorithm at SemEval-2007
for task 2 and it achieved a quite good result.
And at first this algorithm is also introduced
for the unsupervised document classification
problem. So we use the sIB algorithm for clus-
tering the feature vectors in our system.
Unlike the situation in (Niu et al, 2007),
the number of the sense classes is provided in
CLP2010 task 4. So we can apply the sIB
algorithm directly without the sense number
estimation procedure in that paper. sIB algo-
rithm is a variant of the information bottle-
neck method.
Let d represent a document, and w repre-
sent a feature word, d ? D,w ? F . Given
the joint distribution p(d,w), the document
clustering problem is formulated as looking for
a compact representation T for D, which re-
serves as much information as possible about
F . T is the document clustering solution. For
solving this optimization problem, sIB algo-
rithm was proposed in (Slonim et al, 2002),
which found a local maximum of I(T, F ) by:
given a initial partition T, iteratively draw-
ing a d ? D out of its cluster t(d), t ? T ,
and merging it into tnew such that tnew =
argmaxt?Td(d, t). d(d, t) is the change of
I(T, F ) due to merging d into cluster tnew,
which is given by
d(d, t) = (p(d)+p(t))JS(p(w|d), p(w|t)). (1)
JS(p, q) is the Jensen-Shannon divergence,
which is defined as
JS(p, q) = pipDKL(p||p) + piqDKL(q||p), (2)
DKL(p||p) =
?
y
p log p
p
, (3)
DKL(q||p) =
?
y
q log q
p
, (4)
{p, q} ? {p(w|d), p(w|t)}, (5)
{pip, piq} ? {
p(d)
p(d) + p(t)
, p(t)
p(d) + p(t)
}, (6)
p = pipp(w|d) + piqp(w|t). (7)
In our system we use the sIB algorithm in
the Weka 3.5.8 cluster package to cluster the
feature vectors obtained in the previous sec-
tion. The detailed description of the sIB algo-
rithm in weka can refer to the website 3. And
the parameters for this Weka class is that: the
number of clusters is the number of senses pro-
vided by the task, the random number seed is
zero and the other parameters like maximum
number of iteration and so on is set as default.
5 CLP 2010 Bake-Off of Chinese
Word Sense Induction
5.1 Evaluation Measure
The evaluation measure is described as fol-
lowing:
We consider the gold standard as a solu-
tion to the clustering problem. All examples
tagged with a given sense in the gold standard
form a class. For the system output, the clus-
ters are formed by instances assigned to the
same sense tag (the sense tag with the highest
weight for that instance). We will compare
clusters output by the system with the classes
in the gold standard and compute F-score as
usual. F-score is computed with the formula
below.
Suppose Cr is a class of the gold standard,
and Si is a cluster of the system generated,
then
1. F ? score(Cr, Si) = 2?P?RP+R
2. P =the number of correctly labeled ex-
amples for a cluster/total cluster size
3. R =the number of correctly labeled ex-
amples for a cluster/total class size
Then for a given class Cr,
FScore(Cr) = max
Si
(F ? score(Cr, Si)) (8)
Then
FScore =
c
?
r=1
nr
n
FScore(Cr) (9)
3http://pacific.mpi-cbg.de/javadoc/Weka/
clusterers/sIB.html
Where c is total number of classes, nr is the
size of class Cr , and n is the total size.
5.2 DataSet
The data set includes 100 ambiguous Chi-
nese words and for every word it provided 50
instances. Besides that they also provided a
sample test set of 2500 examples of 50 target
words with the answers to illustrate the data
format.
Besides the sIB algorithm we also apply the
k-means and EM algorithm to cluster the fea-
ture vectors. These algorithms are separately
using the simpleKMeans class and the EM
class in the Weka 3.5.8 cluster package. Ex-
cept the number of clusters set as the given
number of senses and number of seeds set as
zero, all other parameters are set as default.
For the given sample test set with answers the
result is illustrated in the Table 1 below.
algorithm F-score
k-means 0.7025
EM 0.7286
sIB 0.8132
Table 1: Results of three clustering algorithms
From Table 1 we can see the sIB clustering
algorithm improves the result of the Chinese
word sense induction evidently.
In the real test data test containing 100 am-
biguous Chinese words, our system achieves a
F-score 0.7788 ranking 6th among the 18 sys-
tems submitted. The best F-score of these 18
systems is 0.7933 and the average of them is
0.7128.
5.3 Discussion
In our system we only use the local collo-
cations and the co-occurrences of the target
words. But the words distance for the target
word in the same sentence and the parts of
speech of the neighboring word together with
the target word is also important in this task.
In our experiment we used the parts of
speech for the target word and each word be-
fore and after it achieved by the Chinese word
segmentation system as part of the features
vectors for clustering. With a proper weight
on each POS dimension in the feature vectors,
the F score for some word in the given sample
test set with answers improved evidently. For
example the Chinese word ????, the F score
of it was developed from 0.5983 to 0.7573. But
because of the fault of the segmentation sys-
tem and other reasons F score of other words
fell and the speed of the system was rather
slower than before, we gave up this improve-
ment finally.
Without the words distance for the target
word in the same sentence the feature vectors
maybe lack some information useful. So if we
can calculate the correlation between the tar-
get word and other words, we will use these
word sufficiently. However because of quan-
tity of the Internet corpus is unknown, we
didn?t find the proper method to weigh the
correlation.
From the previous section we find that the
F score for the real test data test is lower than
that for the sample test set. It is mainly be-
cause there are more single Chinese charac-
ters (as words) in the real test data set. Our
system does not process these characters spe-
cially. For most of the Chinese characters we
can?t judge their correct senses only from the
context where they appear. Their meaning
always depends on the collocations with the
other Chinese characters with which they be-
come a Chinese word. However our system
discriminates the senses of them only refer-
ring to the context of them, it can?t judge the
meaning of these Chinese characters properly.
Maybe the best way is to search them in the
dictionary.
However our system does not always have a
very poor performance for any single Chinese
character (as a word). The result is quite good
for some Chinese characters. For example the
Chinese character ??? which has three mean-
ing: valley, millet and a family name, the pre-
cision (P) of our system is 0.760. But for most
of single Chinese characters such as ??? and
???, it is so bad that the result in the sample
test worked rather better than the real test.
In Chinese the former character ??? tends
to express a complete meaning and the other
characters in the word which they combine of-
ten modify it such as the characters ??? and
??? in the word ???? and ????. So this
character can have a relatively high correla-
tion with the words around and our system
can deal with such characters like it. Unfor-
tunately most characters need other charac-
ters to represent a complete meaning as the
the latter ??? and ??? so they almost have
no correlation with the words around but with
those characters in the word in which they oc-
cur. But our system only uses the context fea-
tures and even doesn?t do any special process
about these single Chinese characters. There-
fore our system can not address those char-
acters appropriately and we need to find a
proper method to solve it, using a dictionary
may be a choice.
This method works better for nouns and ad-
jectives (in the sample test data set there are
only 4 adjectives), but for verbs F score falls
a little, illustrated in the Table 2 below.
POS F-score
nouns 0.8473
adjectives 0.8543
verbs 0.7921
Table 2: Results of each POS in the sample
test data set
Only using the local collocations in our sys-
tem the F score is achieve above 80% (in the
sample test), it demonstrates to some extent
the information of collocations is so important
that we should pay more attention to it.
6 Conclusion
The triplet-based Chinese word sense induc-
tion method is fitted to the task of Chinese
word sense induction and obtain rather good
result. But for some single characters word
and some verbs, this method is not appropri-
ate enough. In the future work, we will im-
prove the method with more reasonable triplet
selection strategies.
Acknowledgments
This work was (partially) funded by 863
Program (No. 2009AA01A346), 973 Pro-
gram (No. 2010CB327906), and Shanghai Sci-
ence and Technology Development Funds (No.
08511500302).
References
Bordag, S. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. Pro-
ceedings of EACL-06. Trento.
Brody, S. and M. Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 103?
111. Association for Computational Linguistics.
Dorow, B. and D. Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of
the tenth conference on European chapter of
the Association for Computational Linguistics-
Volume 2, pages 79?82. Association for Compu-
tational Linguistics.
Niu, Z.Y., D.H. Ji, and C.L. Tan. 2007. I2r: Three
systems for word sense discrimination, chinese
word sense disambiguation, and english word
sense disambiguation. In Proceedings of the 4th
International Workshop on Semantic Evalua-
tions, pages 177?182. Association for Compu-
tational Linguistics.
Pedersen, T. and R. Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, volume 2, pages
197?207.
Sch?tze, H. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?
123.
Slonim, N., N. Friedman, and N. Tishby. 2002.
Unsupervised document classification using se-
quential information maximization. In Proceed-
ings of the 25th annual international ACM SI-
GIR conference on Research and development
in information retrieval, pages 129?136. ACM.
