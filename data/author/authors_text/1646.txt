Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 213?216,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with a Multi-Phase Model 
Zhou Junsheng 
State Key Laboratory for Novel Software Tech-
nology, Nanjing University,  China 
Deptartment of Computer Science, Nanjing 
Normal University, China 
zhoujs@nlp.nju.edu.cn 
He Liang 
State Key Laboratory for Novel Software 
Technology, Nanjing University, China 
 hel@nlp.nju.edu.cn 
Dai Xinyu 
State Key Laboratory for Novel Software Tech-
nology, Nanjing University,  China 
 dxy@nlp.nju.edu.cn 
Chen Jiajun 
State Key Laboratory for Novel Software 
Technology, Nanjing University, China 
 chenjj@nlp.nju.edu.cn 
 
Abstract 
Chinese named entity recognition is one 
of the difficult and challenging tasks of 
NLP. In this paper, we present a Chinese 
named entity recognition system using a 
multi-phase model. First, we segment the 
text with a character-level CRF model. 
Then we apply three word-level CRF 
models to the labeling person names, lo-
cation names and organization names in 
the segmentation results, respectively. 
Our systems participated in the NER tests 
on open and closed tracks of  Microsoft 
Research (MSRA). The actual evaluation 
results show that our system performs 
well on both the open tracks and closed 
tracks. 
1 Introduction 
Named entity recognition (NER) is a fundamen-
tal component for many NLP applications, such 
as Information extraction, text Summarization, 
machine translation and so forth. In recent years, 
much attention has been focused on the problem 
of recognition of Chinese named entities. The 
problem of Chinese named entity recognition is 
difficult and challenging, In addition to the chal-
lenging difficulties existing in the counterpart 
problem in English, this problem also exhibits 
the following more difficulties: (1) In a Chinese 
document, the names do not have ?boundary to-
kens? such as the capitalized initial letters for a 
person name in an English document. (2) There 
is no space between words in Chinese text, so we 
have to segment the text before NER is per-
formed. 
In this paper, we report a Chinese named en-
tity recognition system using a multi-phase 
model which includes a basic segmentation 
phase and three named entity recognition phases. 
In our system, the implementations of basic seg-
mentation components and named entity recogni-
tion component are both based on conditional 
random fields (CRFs) (Lafferty et al, 2001). At 
last, we apply the rule method to recognize some 
simple and short location names and organization 
names in the text. We will describe each of these 
phases in more details below. 
2 Chinese NER with multi-level models 
2.1 Recognition Process 
The input to the recognition algorithm is Chinese 
character sequence that is not segmented and the 
output is recognized entity names. The process of 
recognition of Chinese NER is illustrated in fig-
ure 1. First, we segment the text with a character-
level CRF model. After basic segmentation, a 
small number of named entities in the text, such 
as ?????, ??????????? and so on, 
which are segmented as a single word. These 
simple single-word entities will be labeled with 
some rules in the last phase. However, a great 
number of named entities in the text, such as ??
???????????, ????????, are 
not yet segmented as a single word. Then, differ-
ent from (Andrew et al 2003), we apply three 
trained CRFs models with carefully designed and 
selected features to label person names, location 
names and organization names in the segmenta-
tion results, respectively. At last phase, we apply 
some rules to tag some names not recognized by 
CRFs models, and adjust part of the organization 
names recognized by CRFs models. 
213
 2.2 Word segmentation 
We implemented the basic segmentation compo-
nent with linear chain structure CRFs. CRFs are  
undirected graphical models that encode a condi-
tional probability distribution using a given set of 
features. In the special case in which the desig-
nated output nodes of the graphical model are 
linked by edges in a linear chain, CRFs make a 
first-order Markov independence assumption 
among output nodes, and thus correspond to fi-
nite state machines (FSMs). CRFs define the 
conditional probability of a state sequence given 
an input sequence as 
??
???
?= ??
= =
??
T
t
K
k
ttkk
o
tossf
Z
osP
1 1
1 ),,,(exp
1
)|( ?  
Where ),,,( 1 tossf ttk ?  is an arbitrary feature 
function over its arguments, and ?k is a learned 
weight for each feature function.  
Based on CRFs model, we cast the segmenta-
tion problem as a sequence tagging problem. Dif-
ferent from (Peng et al, 2004), we represent the 
positions of a hanzi (Chinese character) with four 
different tags: B for a hanzi that starts a word, I 
for a hanzi that continues the word, F for a hanzi 
that ends the word, S for a hanzi that occurs as a 
single-character word. The basic segmentation is 
a process of labeling each hanzi with a tag given 
the features derived from its surrounding context. 
The features used in our experiment can be bro-
ken into two categories: character features and 
word features. The character features are instan-
tiations of the following templates, similar to 
those described in (Ng and Jin, 2004), C refers to 
a Chinese hanzi. 
(a) Cn (n = ?2,?1,0,1,2 ) 
(b) CnCn+1( n = ?2,?1,0,1)  
(c) C?1C1 
(d) Pu(C0 ) 
In addition to the character features, we came 
up with another type word context feature which 
was found very useful in our experiments. The 
feature captures the relationship between the 
hanzi and the word which contains the hanzi. For 
a two-hanzi word, for example, the first hanzi 
??? within the word ???? will have the fea-
ture WC0=TWO_F set to 1, the second hanzi 
??? within the same word ???? will have the 
feature WC0=TWO_L set to 1. For the three-
hanzi word, for example, the first hanzi ??? 
within a word ????? will have the feature 
WC0=TRI_F set to 1, the second hanzi ??? 
within the same word ????? will have the 
feature WC0=TRI_M set to 1, and the last hanzi 
??? within the same word ????? will have 
the feature WC0=TRI_L set to 1. Similarly, the 
feature can be extended to a four-hanzi word. 
2.3 Named entity tagging with CRFs 
After basic segmentation, we use three word-
level CRFs models to label person names, loca-
tion names and organization names, respectively. 
The important factor in applying CRFs model to 
name entity recognition is how to select the 
proper features set. Most of entity names do not 
have any common structural characteristics ex-
cept for containing some feature words,  such as 
????, ????, ??? , ??? and so on. In addi-
tion, for person names, most names include a 
common surname, e.g. ???, ???. But as a 
proper noun, the occurrence of an entity name 
has the specific context. In this section, we only 
present our approach to organization name rec-
ognition. For example, the context information of 
organization name mainly includes the boundary 
words and some title words (e.g. ??????). 
By analyzing a large amount of entity name cor-
pora, we find that the indicative intensity of dif-
ferent boundary words vary greatly. So we divide 
the left and right boundary words into two 
classes according to the indicative intensity. Ac-
cordingly we construct the four boundary words 
lexicons. To solve the problem of the selection 
and classification of boundary words, we make 
use of mutual Information I(x, y). If there is a 
genuine association between x and y, then I(x, y) 
>>0. If there is no interesting relationship be-
Unsegmented text 
Word Segmentation 
Person Names Recognition 
Recognized Named Entities 
Processing with Rules 
Location Names Recognition 
Organization Names Recognition  
Fig1. Chinese NER process 
214
tween x and y, then I(x, y)?0. If x and y are in 
complementary distribution, then I(x, y) << 0. 
By using mutual information, we compute the 
association between boundary word and the type 
of organization name, then select and classify the 
boundary words. Some example boundary words 
for organization names are listed in table 1. 
Table 1. The classified boundary words for ORG names 
Based on the consideration given in preceding 
section, we constructed a set of atomic feature 
patterns, listed in table 2. Additionally, we de-
fined a set of conjunctive feature patterns, which 
could form effective feature conjunctions to ex-
press complicated contextual information. 
Table 2.  Atomic feature patterns for ORG names 
Atomic pattern Meaning of pattern 
CurWord Current word 
LocationName Check if current word is a location 
name 
PersonName Check if current word is a person 
name 
KnownORG Check if current word is a known 
organization name 
ORGFeature 
 
ScanFeatureWord_8 
Check if current word is a feature 
word of ORG name 
Check if there exist a feature word 
among eight words behind the 
current word 
LeftBoundary1_-2 
LeftBoundary2_-2 
Check if there exist a first-class or 
second-class left boundary word 
among two words before the cur-
rent word 
RightBoundary1_+2 
RightBoundary2_+2 
Check if there exist a first-class or 
second-class right boundary word 
among two words behind the cur-
rent word 
2.4 Processing with rules 
There exists some single-word named entities 
that aren?t tagged by CRFs models. We recog-
nize these single-word named entities with some 
rules. We first construct two known location 
names and organization names dictionaries and 
two feature words lists for location names and 
organization names. In closed track, we collect 
known location names and organization names 
only from training corpus. The recognition proc-
ess is described below. For each word in the text, 
we first check whether it is a known location or 
organization names according to the known loca-
tion names and organization names dictionaries. 
If it isn?t a known name, then we further check 
whether it is a known word. If it is not a known 
word also, we next check whether the word ends 
with a feature word of location or organization 
names. If it is, we label it as a location or organi-
zation name. 
In addition, we introduce some rules to adjust 
organization names recognized by CRF model 
based on the labeling specification of MRSA 
corpus. For example, the string ???????
???? ? is recognized as an organization 
name, but the string should be divided into two 
names: a location name (?????) and a or-
ganization name (?????????), according 
to label specification, so we add some rules to 
adjust it. 
3 Experimental results 
We participated in the three GB tracks in the 
third international Chinese language processing 
bakeoff: NER msra-closed, NER msra-open and 
WS msra-open. In the closed track, we con-
structed all dictionaries only with the words ap-
pearing in the training corpus. In the closed track, 
we didn?t use the same feature characters lists for 
location names and organization names as in the 
open tracks and we collected the feature charac-
ters from the training data in the closed track. We 
constructed feature characters lists for location 
names and organization names by the following 
approach. First, we extract all suffix string for all 
location names and organization names in the 
training data and count the occurrence of these 
suffix strings in all location names and organiza-
tion names. Second, we check every suffix string 
to judge whether it is a known word. If a suffix 
string is not a known word, we discard it. Finally, 
in the remaining suffix words, we select the fre-
quently used suffix words as the feature charac-
ters whose counts are greater than the threshold. 
We set different thresholds for single-character 
feature words and multi-character feature words. 
Similar approaches were taken to the collection 
of common Chinese surnames in the closed track.  
While making training data for segmentation 
model, we adopted different tagging methods for 
organization names in the closed track and in the 
open track. In the closed track, we regard every 
organization name, such as ????????
??, as a single word. But, in the open track, we 
segment a long organization name into several 
words. For example, the organization name ??
Type Class Examples
First-class ???6.0006?Left boundary 
word Second-class ???3.1161?
First -class ???5.4531?Right boundary 
word Second-class  ???2.0135?
215
???????? would be divided into three 
words: ?????, ???? and ?????. The 
different tagging methods at segmentation phase 
would bring different effect to organization 
names recognition. The size of training data used 
in the open tracks is same as the closed tracks. 
We have not employed any additional training 
data in the open tracks. Table 3 shows the per-
formance of our systems for NER in the bakeoff.  
Table 3: Named entity recognition outcome 
Track P  R F  Per-F Loc-F Org-F
NER msra  
closed      
88.94  84.20 86.51 90.09 85.45 83.10
NER msra 
open 
90.76 89.22 89.99 92.61 90.99 83.97
For the separate word segmentation task(WS), 
the above NER task is performed first. Then we 
added several additional processing steps on the 
result of named entity recognition. As we all 
know, disambiguation problem is one of the key 
issue in Chinese words segmentation. In this task, 
some ambiguities were resolved through a rule-
set which was automatically constructed based 
on error driven learning theory. The pre-
constructed rule-set stored many pseudo-
ambiguity strings and gave their correct segmen-
tations. After analyzing the result of our NER 
based on CRFs model, we noticed that it presents 
a high recall on out-of-vocabulary. But at the 
same time, some characters and words were 
wrongly combined as new words which caused 
the losing of the precision of OOV and the recall 
of IV. To this phenomenon, we adopted an un-
conditional rule, that if a word, except recog-
nized name entity, was detected as a new word 
and its length was more than 6 (Chinese Charac-
ters), and it should be segmented as several in-
vocabulary words based on the combination of  
FMM and BMM methods. Table 4 shows the 
result of our systems for word segmentation in 
the bakeoff. 
Table 4: Word segmentation outcome 
Track P R F OOV-R IV-R
WS msra 
open 0.975 0.976 0.975 0.811 0.981
4 Conclusion 
We have presented our Chinese named entity 
recognition system with a multi-phase model and 
its result for Msra_open and mrsa_closed tracks. 
Our open and closed GB track experiments show 
that its performance is competitive. We will try 
to select more useful feature functions into the 
existing segmentation model and named entity 
recognition model in future work. 
Reference 
Aitao Chen. 2003. Chinese Word Segmentation Using 
Minimal Linguistic Knowledge. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing.  
Andrew McCallum, Wei Li. 2003. Early Results for 
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced 
Lexicons. Proceedings of the Seventh CoNLL con-
ference, Edmonton,  
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Proc. 
ICML 01. 
 Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Taging: One-at-a-Time or All at Once? 
Word-based or Character based? In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Spain. 
Peng, Fuchun, Fangfang Feng, and Andrew 
McCallum. 2004. Chinese Segmentation and New 
Word Detection using Conditional Random Fields . 
In Proceedings of the Twentith International Con-
ference on Computaional Linguistics.  
 
216
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285?290,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Enhancing Statistical Machine Translation with Character Alignment 
 
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, Jiajun Chen 
State Key Laboratory for Novel Software Technology, 
Department of Computer Science and Technology, 
Nanjing University, Nanjing, 210046, China 
{xin,tanggc,dxy,huangsj,chenjj}@nlp.nju.edu.cn 
 
  
Abstract 
The dominant practice of statistical machine 
translation (SMT) uses the same Chinese word 
segmentation specification in both alignment 
and translation rule induction steps in building 
Chinese-English SMT system, which may suf-
fer from a suboptimal problem that word seg-
mentation better for alignment is not necessarily 
better for translation. To tackle this, we propose 
a framework that uses two different segmenta-
tion specifications for alignment and translation 
respectively: we use Chinese character as the 
basic unit for alignment, and then convert this 
alignment to conventional word alignment for 
translation rule induction. Experimentally, our 
approach outperformed two baselines: fully 
word-based system (using word for both 
alignment and translation) and fully charac-
ter-based system, in terms of alignment quality 
and translation performance. 
1 Introduction 
Chinese Word segmentation is a necessary step in 
Chinese-English statistical machine translation 
(SMT) because Chinese sentences do not delimit 
words by spaces. The key characteristic of a Chi-
nese word segmenter is the segmentation specifi-
cation1. As depicted in Figure 1(a), the dominant 
practice of SMT uses the same word segmentation 
for both word alignment and translation rule induc-
tion. For brevity, we will refer to the word seg-
mentation of the bilingual corpus as word segmen-
tation for alignment (WSA for short), because it 
determines the basic tokens for alignment; and refer 
to the word segmentation of the aligned corpus as 
word segmentation for rules (WSR for short), be-
cause it determines the basic tokens of translation 
                                                          
1 We hereafter use ?word segmentation? for short. 
rules2, which also determines how the translation 
rules would be matched by the source sentences. 
It is widely accepted that word segmentation with 
a higher F-score will not necessarily yield better 
translation performance (Chang et al, 2008; Zhang 
et al, 2008; Xiao et al, 2010). Therefore, many 
approaches have been proposed to learn word 
segmentation suitable for SMT. These approaches 
were either complicated (Ma et al, 2007; Chang et 
al., 2008; Ma and Way, 2009; Paul et al, 2010), or 
of high computational complexity (Chung and 
Gildea 2009; Duan et al, 2010). Moreover, they 
implicitly assumed that WSA and WSR should be 
equal. This requirement may lead to a suboptimal 
problem that word segmentation better for align-
ment is not necessarily better for translation. 
To tackle this, we propose a framework that uses 
different word segmentation specifications as WSA 
and WSR respectively, as shown Figure 1(b). We 
investigate a solution in this framework: first, we 
use Chinese character as the basic unit for align-
ment, viz. character alignment; second, we use a 
simple method (Elming and Habash, 2007) to 
convert the character alignment to conventional 
word alignment for translation rule induction. In the 
                                                          
2 Interestingly, word is also a basic token in syntax-based rules. 
Word alignment 
Bilingual Corpus 
Aligned Corpus 
WSA
Translation Rules
WSA 
WSR 
Rule induction 
Decoding 
Translation Results WSR
Word alignment 
Bilingual Corpus 
Aligned Corpus 
WSA
Translation Rules 
WSA
WSR
Rule induction 
Decoding 
Translation Results WSR
Aligned Corpus 
WSR
Conversion 
(b) WSA?WSR 
Figure 1. WSA and WSR in SMT pipeline
(a)  WSA=WSR 
285
experiment, our approach consistently outper-
formed two baselines with three different word 
segmenters: fully word-based system (using word 
for both alignment and translation) and fully char-
acter-based system, in terms of alignment quality 
and translation performance. 
The remainder of this paper is structured as fol-
lows: Section 2 analyzes the influences of WSA and 
WSR on SMT respectively; Section 3 discusses 
how to convert character alignment to word align-
ment; Section 4 presents experimental results, fol-
lowed by conclusions and future work in section 5. 
2 Understanding WSA and WSR 
We propose a solution to tackle the suboptimal 
problem: using Chinese character for alignment 
while using Chinese word for translation. Character 
alignment differs from conventional word align-
ment in the basic tokens of the Chinese side of the 
training corpus3. Table 1 compares the token dis-
tributions of character-based corpus (CCorpus) and 
word-based corpus (WCorpus). We see that the 
WCorpus has a longer-tailed distribution than the 
CCorpus. More than 70% of the unique tokens ap-
pear less than 5 times in WCorpus. However, over 
half of the tokens appear more than or equal to 5 
times in the CCorpus.  This indicates that modeling 
word alignment could suffer more from data 
sparsity than modeling character alignment.  
Table 2 shows the numbers of the unique tokens 
(#UT) and unique bilingual token pairs (#UTP) of 
the two corpora. Consider two extensively features, 
fertility and translation features, which are exten-
sively used by many state-of-the-art word aligners. 
The number of parameters w.r.t. fertility features 
grows linearly with #UT while the number of pa-
rameters w.r.t. translation features grows linearly 
with #UTP. We compare #UT and #UTP of both 
corpora in Table 2. As can be seen, CCorpus has 
less UT and UTP than WCorpus, i.e. character 
alignment model has a compact parameterization 
than word alignment model, where the compactness 
of parameterization is shown very important in sta-
tistical modeling (Collins, 1999). 
Another advantage of character alignment is the 
reduction in alignment errors caused by word seg- 
                                                          
3 Several works have proposed to use character (letter) on both 
sides of the parallel corpus for SMT between similar (European) 
languages (Vilar et al, 2007; Tiedemann, 2009), however, 
Chinese is not similar to English. 
Frequency Characters (%) Words (%) 
1 27.22 45.39 
2 11.13 14.61 
3 6.18 6.47 
4 4.26 4.32 
5(+) 50.21 29.21 
Table 1 Token distribution of CCorpus and WCorpus 
 
Stats. Characters Words 
#UT 9.7K 88.1K 
#UTP 15.8M 24.2M 
Table 2 #UT and #UTP in CCorpus and WCorpus 
 
mentation errors. For example, ??? (Cheney)? 
and ?? (will)? are wrongly merged into one word 
???  by the word segmenter, and ??? 
wrongly aligns to a comma in English sentence in 
the word alignment; However, both ? and ? align 
to ?Cheney? correctly in the character alignment. 
However, this kind of errors cannot be fixed by 
methods which learn new words by packing already 
segmented words, such as word packing (Ma et al, 
2007) and Pseudo-word (Duan et al, 2010). 
As character could preserve more meanings than 
word in Chinese, it seems that a character can be 
wrongly aligned to many English words by the 
aligner. However, we found this can be avoided to a 
great extent by the basic features (co-occurrence 
and distortion) used by many alignment models. For 
example, we observed that the four characters of the 
non-compositional word ????? (Arafat)? align 
to Arafat correctly, although these characters pre-
serve different meanings from that of Arafat. This 
can be attributed to the frequent co-occurrence (192 
times) of these characters and Arafat in CCorpus. 
Moreover,?  usually means France in Chinese, 
thus it may co-occur very often with France in 
CCorpus. If both France and Arafat appear in the 
English sentence, ? may wrongly align to France. 
However, if ? aligns to Arafat, ? will probably 
align to Arafat, because aligning ? to Arafat could 
result in a lower distortion cost than aligning it to 
France. 
Different from alignment, translation is a pattern 
matching procedure (Lopez, 2008). WSR deter-
mines how the translation rules would be matched 
by the source sentences. For example, if we use 
translation rules with character as WSR to translate 
name entities such as the non-compositional word 
????, i.e. translating literally, we may get a 
wrong translation. That?s because the linguistic 
286
knowledge that the four characters convey a spe-
cific meaning different from the characters has been 
lost, which cannot always be totally recovered even 
by using phrase in phrase-based SMT systems (see 
Chang et al (2008) for detail). Duan et al (2010) 
and Paul et al, (2010) further pointed out that 
coarser-grained segmentation of the source sen-
tence do help capture more contexts in translation. 
Therefore, rather than using character, using 
coarser-grained, at least as coarser as the conven-
tional word, as WSR is quite necessary. 
3 Converting Character Alignment to Word 
Alignment 
In order to use word as WSR, we employ the same 
method as Elming and Habash (2007)4 to convert 
the character alignment (CA) to its word-based 
version (CA?) for translation rule induction. The 
conversion is very intuitive: for every Eng-
lish-Chinese word pair ??, ?? in the sentence pair, 
we align ? to ? as a link in CA?, if and only if there 
is at least one Chinese character of ? aligns to ? in 
CA.  
Given two different segmentations A and B of the 
same sentence, it is easy to prove that if every word 
in A is finer-grained than the word of B at the cor-
responding position, the conversion is unambiguity 
(we omit the proof due to space limitation). As 
character is a finer-grained than its original word, 
character alignment can always be converted to 
alignment based on any word segmentation. 
Therefore, our approach can be naturally scaled to 
syntax-based system by converting character 
alignment to word alignment where the word seg-
mentation is consistent with the parsers. 
We compare CA with the conventional word 
alignment (WA) as follows: We hand-align some 
sentence pairs as the evaluation set based on char-
acters (ESChar), and converted it to the evaluation 
set based on word (ESWord) using the above con-
version method. It is worth noting that comparing 
CA and WA by evaluating CA on ESChar and 
evaluating WA on ESWord is meaningless, because 
the basic tokens in CA and WA are different. 
However, based on the conversion method, com-
paring CA with WA can be accomplished by evalu-
ating both CA? and WA on ESWord. 
                                                          
4 They used this conversion for word alignment combination 
only, no translation results were reported. 
4 Experiments 
4.1 Setup 
FBIS corpus (LDC2003E14) (210K sentence pairs) 
was used for small-scale task. A large bilingual 
corpus of our lab (1.9M sentence pairs) was used for 
large-scale task. The NIST?06 and NIST?08 test sets 
were used as the development set and test set re-
spectively. The Chinese portions of all these data 
were preprocessed by character segmenter (CHAR), 
ICTCLAS word segmenter 5  (ICT) and Stanford 
word segmenters with CTB  and PKU specifica-
tions6 respectively. The first 100 sentence pairs of 
the hand-aligned set in Haghighi et al (2009) were 
hand-aligned as ESChar, which is converted to 
three ESWords based on three segmentations re-
spectively. These ESWords were appended to 
training corpus with the corresponding word seg-
mentation for evaluation purpose. 
Both character and word alignment were per-
formed by GIZA++ (Och and Ney, 2003) enhanced 
with gdf heuristics to combine bidirectional align-
ments (Koehn et al, 2003). A 5-gram language 
model was trained from the Xinhua portion of 
Gigaword corpus. A phrase-based MT decoder 
similar to (Koehn et al, 2007) was used with the 
decoding weights optimized by MERT (Och, 2003). 
4.2 Evaluation 
We first evaluate the alignment quality. The method 
discussed in section 3 was used to compare char-
acter and word alignment. As can be seen from 
Table 3, the systems using character as WSA out-
performed the ones using word as WSA in both 
small-scale (row 3-5) and large-scale task (row 6-8) 
with all segmentations. This gain can be attributed 
to the small vocabulary size (sparsity) for character 
alignment. The observation is consistent with 
Koehn (2005) which claimed that there is a negative 
correlation between the vocabulary size and trans-
lation performance without explicitly distinguish-
ing WSA and WSR. 
We then evaluated the translation performance. 
The baselines are fully word-based MT systems 
(WordSys), i.e. using word as both WSA and WSR, 
and fully character-based systems (CharSys). Table  
 
                                                          
5 http://www.ictclas.org/ 
6 http://nlp.stanford.edu/software/segmenter.shtml 
287
  Word alignment Character alignment 
  P R F P R F 
S 
CTB 76.0 81.9 78.9 78.2 85.2 81.8 
PKU 76.1 82.0 79.0 78.0 86.1 81.9 
ICT 75.2 80.8 78.0 78.7 86.3 82.3 
L 
CTB 79.6 85.6 82.5 82.2 90.6 86.2 
PKU 80.0 85.4 82.6 81.3 89.5 85.2 
ICT 80.0 85.0 82.4 81.3 89.7 85.3 
Table 3 Alignment evaluation. Precision (P), recall (R), 
and F-score (F) with ? ? 0.5 (Fraser and Marcu, 2007) 
 
 WSA WSR CTB PKU ICT 
S word word 21.52 20.99 20.95char word 22.04 21.98 22.04
L word word 22.07 22.86 22.23 char word 23.41 23.51 23.05 
Table 4 Translation evaluation of WordSys and pro-
posed system using BLEU-SBP (Chiang et al, 2008) 
 
4 compares WordSys to our proposed system. Sig-
nificant testing was carried out using bootstrap 
re-sampling method proposed by Koehn (2004) 
with a 95% confidence level. We see that our pro-
posed systems outperformed WordSys in all seg-
mentation specifications settings. Table 5 lists the 
results of CharSys in small-scale task. In this setting, 
we gradually set the phrase length and the distortion 
limits of the phrase-based decoder (context size) to 
7, 9, 11 and 13, in order to remove the disadvantage 
of shorter context size of using character as WSR 
for fair comparison with WordSys as suggested by 
Duan et al (2010). Comparing Table 4 and 5, we 
see that all CharSys underperformed WordSys. This 
observation is consistent with Chang et al (2008) 
which claimed that using characters, even with 
large phrase length (up to 13 in our experiment) 
cannot always capture everything a Chinese word 
segmenter can do, and using word for translation is 
quite necessary. We also see that CharSys under-
performed our proposed systems, that?s because the 
harm of using character as WSR outweighed the 
benefit of using character as WSA, which indicated 
that word segmentation better for alignment is not 
necessarily better for translation, and vice versa. 
We finally compared our approaches to Ma et al 
(2007) and Ma and Way (2009), which proposed 
?packed word (PW)? and ?bilingual motivated 
word (BS)? respectively. Both methods iteratively 
learn word segmentation and alignment alterna-
tively, with the former starting from word-based 
corpus and the latter starting from characters-based 
corpus. Therefore, PW can be experimented on all 
segmentations. Table 6 lists their results in small- 
Context Size 7 9 11 13 
BLEU 20.90 21.19 20.89 21.09 
Table 5 Translation evaluation of CharSys. 
 
System WSA WSR CTB PKU ICT 
WordSys word word 21.52 20.99 20.95
Proposed char word 22.04 21.98 22.04
PW PW PW 21.24 21.24 21.19 
Char+PW char PW 22.46 21.87 21.97 
BS BS BS 19.76 
Char+BS char BS 20.19 
Table 6 Comparison with other works 
 
scale task, we see that both PW and BS underper-
formed our approach. This may be attributed to the 
low recall of the learned BS or PW in their ap-
proaches. BS underperformed both two baselines, 
one reason is that Ma and Way (2009) also em-
ployed word lattice decoding techniques (Dyer et al, 
2008) to tackle the low recall of BS, which was 
removed from our experiments for fair comparison. 
Interestingly, we found that using character as 
WSA and BS as WSR (Char+BS), a moderate gain 
(+0.43 point) was achieved compared with fully 
BS-based system; and using character as WSA and 
PW as WSR (Char+PW), significant gains were 
achieved compared with fully PW-based system, 
the result of CTB segmentation in this setting even 
outperformed our proposed approach (+0.42 point). 
This observation indicated that in our framework, 
better combinations of WSA and WSR can be found 
to achieve better translation performance. 
5 Conclusions and Future Work 
We proposed a SMT framework that uses character 
for alignment and word for translation, which im-
proved both alignment quality and translation per-
formance. We believe that in this framework, using 
other finer-grained segmentation, with fewer am-
biguities than character, would better parameterize 
the alignment models, while using other coars-
er-grained segmentation as WSR can help capture 
more linguistic knowledge than word to get better 
translation. We also believe that our approach, if 
integrated with combination techniques (Dyer et al, 
2008; Xi et al, 2011), can yield better results. 
 
Acknowledgments 
We thank ACL reviewers. This work is supported 
by the National Natural Science Foundation of 
China (No. 61003112), the National Fundamental 
Research Program of China (2010CB327903). 
288
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della 
Peitra, and Robert L. Mercer. 1993. The mathematics 
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2), pages 
263-311. 
Pi-Chuan Chang, Michel Galley, and Christopher D. 
Manning. 2008. Optimizing Chinese word segmenta-
tion for machine translation performance.  In Pro-
ceedings of third workshop on SMT, pages 224-232. 
David Chiang, Steve DeNeefe, Yee Seng Chan and 
Hwee Tou Ng. 2008. Decomposability of Translation 
Metrics for Improved Evaluation and Efficient Algo-
rithms. In Proceedings of Conference on Empirical 
Methods in Natural Language Processing, pages 
610-619. 
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of Conference on Empirical Methods in 
Natural Language Processing, pages 718-726. 
Michael Collins. 1999. Head-driven statistical models 
for natural language parsing. Ph.D. thesis, University 
of Pennsylvania. 
Xiangyu  Duan, Min Zhang,  and  Haizhou Li. 2010. 
Pseudo-word for phrase-based machine translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 148-156. 
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 
2008. Generalizing word lattice translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 1012-1020. 
Jakob Elming and Nizar Habash. 2007. Combination of 
statistical word alignments based on multiple pre-
processing schemes. In Proceedings of the Associa-
tion for Computational Linguistics, pages 25-28. 
Alexander Fraser and Daniel Marcu. 2007. Squibs and 
Discussions: Measuring Word Alignment Quality for 
Statistical Machine Translation. In Computational 
Linguistics, 33(3), pages 293-303. 
Aria Haghighi, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better word alignments with supervised 
ITG models. In Proceedings of the Association for 
Computational Linguistics, pages 923-931. 
Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch, 
M. Federico, N. Bertoldi, B. Cowan,W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. 
Herbst. 2007. Moses: Open source toolkit for statis-
tical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 177-180.  
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 388-395. 
Philipp Koehn. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Proceedings of the 
MT Summit. 
Adam David Lopez. 2008. Machine translation by pat-
tern matching. Ph.D. thesis, University of Maryland. 
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. 
Bootstrapping word alignment via word packing. In 
Proceedings of the Association for Computational 
Linguistics, pages 304-311. 
Yanjun Ma and Andy Way. 2009. Bilingually motivated 
domain-adapted word segmentation for statistical 
machine translation. In Proceedings of the Conference 
of the European Chapter of the ACL, pages 549-557. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
440-447. 
Franz Josef Och and Hermann  Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1), pages 19-51. 
Michael Paul, Andrew Finch and Eiichiro Sumita. 2010. 
Integration of multiple bilingually-learned segmenta-
tion schemes into statistical machine translation. In 
Proceedings of the Joint Fifth Workshop on Statistical 
Machine Translation and MetricsMATR, pages 
400-408. 
J?rg Tiedemann. 2009. Character-based PSMT for 
closely related languages. In Proceedings of the An-
nual Conference of the European Association for 
machine Translation, pages 12-19. 
David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can 
we translate letters? In Proceedings of the Second 
Workshop on Statistical Machine Translation, pages 
33-39. 
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu 
and Shouxun Lin. 2010.  Joint tokenization and 
translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pages 
1200-1208. 
Ning  Xi, Guangchao  Tang,  Boyuan  Li, and  Yinggong 
Zhao. 2011. Word alignment combination over mul-
tiple word segmentation. In Proceedings of the ACL 
2011 Student Session, pages 1-5. 
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 
2008. Improved statistical machine translation by 
multiple Chinese word segmentation. In Proceedings 
289
of the Third Workshop on Statistical Machine Trans-
lation, pages 216-223. 
 
290
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 425?429,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
MIXCD: System Description for Evaluating Chinese Word Similarity at 
SemEval-2012 
Yingjie Zhang 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
jillzhyj@139.c
om 
Bin Li 
Nanjing University 
Nanjing Normal University 
122 Ninghai Road 
Jiangsu P. R. China 
gothere@126.com 
Xinyu Dai 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
dxy@nju.edu.cn 
Jiajun Chen 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
cjj@nju.eud.cn 
 
 
Abstract 
This document describes three systems calcu-
lating semantic similarity between two Chi-
nese words. One is based on Machine 
Readable Dictionaries and the others utilize 
both MRDs and Corpus. These systems are 
performed on SemEval-2012 Task 4: Evaluat-
ing Chinese Word Similarity. 
1 Introduction 
The characteristics of polysemy and synonymy that 
exist in words of natural language have always 
been a challenge in the fields of Natural Language 
Processing (NLP) and Information Retrieval (IR). 
In many cases, humans have little difficulty in de-
termining the intended meaning of an ambiguous 
word, while it is extremely difficult to replicate 
this process computationally. For many tasks in 
psycholinguistics and NLP, a job is often decom-
posed to the requirement of resolving the semantic 
similarity between words or concepts. 
There are two ways to get the similarity between 
two words. One is to utilize the machine readable 
dictionary (MRD). The other is to use the corpus. 
For the 4th task in SemEval-2012 we are re-
quired to evaluate the semantic similarity of Chi-
nese word pairs. We consider 3 methods in this 
study. One uses MRDs only and the other two use 
both MRD and corpus. A post processing will be 
done on the results of these methods to treat syno-
nyms. 
In chapter 2 we introduce the previous works on 
the evaluation of Semantic Similarity. Chapter 3 
shows three methods used in this task. Chapter 4 
reveals the results of these methods. And conclu-
sion is stated in chapter 5. 
2 Related Work 
For words may have more than one sense, similari-
ty between two words can be determined by the 
best score among all the concept pairs which their 
various senses belong to. 
Before constructed dictionary is built, Lesk 
similarity (Lesk, 1986) which is proposed as a so-
lution for word sense disambiguation is often used 
to evaluating the similarity between two concepts. 
This method calculates the overlap between the 
corresponding definitions as provided by a diction-
ary. 
       (     )  |     (  )       (  )| 
Since the availability of computational lexicons 
such as WordNet, the taxonomy can be represented 
as a hierarchical structure. Then we use the struc-
ture information to evaluate the semantic similarity. 
In these methods, the hierarchical structure is often 
seen as a tree and concepts as the nodes of the tree 
while relations between two concepts as the edges. 
(Resnik, 1995) determines the conceptual simi-
larity of two concepts by calculating the infor-
mation content (IC) of the least common subsumer 
(LCS) of them. 
      (     )    (   (     )) 
where the IC of a concept can be quantified as 
follow: 
  ( )        ( ) 
425
This method do not consider the distance of two 
concepts. Any two concepts have the same LCS 
will have the same similarity even if the distances 
between them are different. It is called node-based 
method. 
(Leacock and Chodorow, 1998) develops a simi-
larity measure based on the distance of two senses 
   and   . They focus on hypernymy links and 
scaled the path length by the overall depth   of the 
tree. 
      (     )      
      (     )
   
 
(Wu and Palmer, 1994) combines the depth of 
the LCS of two concepts into a similarity score. 
      (     )  
       (   (     ))
     (  )       (  )
 
These approaches are regarded as edge-based 
methods. They are more natural and direct to eval-
uating semantic similarity in taxonomy. But they 
treat all nodes as the same and do not consider the 
different information of different nodes. 
(Jiang and Conrath, 1998) uses the information 
content of concept instead of its depth. So both 
node and edge information can be considered to 
evaluate the similarity. It performs well in evaluat-
ing semantic similarity between two texts (Zhang 
et al, 2008; Corley and Mihalcea, 2005; Pedersen, 
2010). 
      (     )  
 
  (  )   (  )     (   (     ))
  
SemCor is used in Jiang's work to get the fre-
quency of a word with a specific sense treated by 
the Lagrange Smoothing. 
3 Approaches 
For SemEval-2012 task 4, we use two MRDs and 
one corpus as our knowledge resources. One MRD 
is HIT IR-Lab Tongyici Cilin (Extended) (Cilin) 
and the other is Chinese Concept Dictionary 
(CCD). The corpus we used in our system is Peo-
ple's Daily. Three systems are proposed to evaluate 
the semantic similarity between two Chinese words. 
The first one utilizes both the MRDs called 
MIXCC (Mixture of Cilin and CCD) and other two 
named MIXCD1 (Mixture of Corpus and Diction-
ary) and MIXCD2 respectively combine the infor-
mation derived from both corpus and dictionary 
into the similarity score. A post processing is done 
to trim the similarity of words with the same mean-
ing. 
3.1 Knowledge Resources 
HIT IR-Lab Tongyici Cilin (Extended) is built by 
Harbin Institute of Technology which contained 
77343 word items. Cilin is constructed as a tree 
with five levels. With the increasing of the level, 
word senses are more fine-grained. All word items 
in Cilin are located at the fifth level. The larger 
level the LCS of an item pair has, the closer their 
concepts are. 
Chinese Concept Dictionary (CCD) is a Chinese 
WordNet produced by Peking University. Word 
concepts in it are represented as Synsets and one-
one corresponding to WordNet 1.6. There are 4 
types of hierarchical semantic relations in CCD as 
follows: 
? Synonym: the meanings of two words are 
equivalence 
? Antonym: two synsets contain the words 
with opposite meaning 
? Hypernym and Hyponym: two synsets 
with the IS-A relation 
? Holonym and Meronym: two synsets with 
the IS-PART-OF relation 
Additionally there is another type of semantic 
relation such as Attribute in CCD This relation 
type often happens between two words with differ-
ent part-of-speech. Even though it is not the hierar-
chical relation, this relation type can make two 
words with different POS have a path between 
them. In WordNet it is often shown as a Morpho-
logical transform between two words, while it may 
happen on two different words with closed mean-
ing in CCD. 
The corpus we use in our system is People's 
Daily 2000 from January to June which has been 
manually segmented. 
3.2 MIXCC 
MIXCC utilizes both Cilin and CCD to evaluate 
the semantic similarity of word pair. In this method 
we get the rank in three steps. 
First, we use Cilin to separate the list of word 
pairs into five parts and sort them in descending 
order of LCS's level. The word pairs having the 
same level of LCS will be put in the same part. 
426
Second, for each part we compute the similarity 
almost by Jiang and Conrath's method mentioned 
in Section 2 above. Only Synonym and Hypernym-
Hyponym relations of CCD concepts are consid-
ered in this method. So CCD could be constructed 
as a forest. We add a root node which combined 
the forest into a tree to make sure that there is a 
path between any two concepts. 
      (     )     
          (  )
           (  )
      (     ) 
   and   compose a word pair needed to cal-
culate semantic similarity between them.    (  ) is 
the Synset in CCD which contains   (  ).  
Because there is no sense-tagged corpus for 
CCD, the frequency of every word in each concept 
is always 1. 
After       (     )  of all word pairs in the 
same part are calculated, we sort the scores in a 
decreasing order again. Then we get five groups of 
ranked word pairs. 
At last the five groups are combined together as 
the result shown in table 1. 
3.3 MIXCD 
MIXCD combines the information of corpus and 
MRDs to evaluate semantic similarity. 
In this system we use trial data to learn a multiple 
linear regression function. There are two classes of 
features for this study which are derived from CCD 
and People's Daily respectively. One class of fea-
ture is the mutual information of a word pair and 
the other is the shortest path between two concepts 
containing the words of which the similarity need-
ed to be evaluated. 
We consider CCD as a large directed graph. The 
nodes of the graph are Synsets and edges are the 
semantic relations between two Synsets. All five 
types of semantic relation showed in Section 3.1 
will be used to build the graph. 
For each word pair, the shortest path between 
two Synsets which contain the words respectively 
is found. Then the path is represented in two forms. 
In one form we record the vector consisting of 
the counts of every relation type in the path. The 
system using this path's form is called MIXCD0. 
For example the path between "??? (psy-
chology)" and "???? (psychiatry)" is repre-
sented as (0, 0, 3, 2, 0). It means that "???" and 
"????" are not synonym and the shortest path 
between them contained 3 IS-A relations and 2 IS-
PART-OF relations. 
We suppose that the path's length is a significant 
feature to measure the semantic similarity of a 
word pair. So in the other form the length is added 
into the vector as the first component. And the 
counts of each relation are recorded in proportion 
to the length. This form of path representation is 
used in the submitted system called MIXCD. Then 
the path between "???" and "????" is rep-
resented as (5, 0, 0, 0.6, 0.4, 0). 
In both forms, the Synonym feature will be 1 if 
the length of the path is 0. 
The mutual information of all word pairs is cal-
culated via the segmented People's Daily. 
Last we use the result of multiple linear regres-
sion to forecast the similarity of other word pairs 
and get the rank. 
3.4 Post Processing 
The word pair with the same meaning may be con-
sisted of two same words or two different words 
belong to the same concept. It is difficult for both 
systems to separate one from the other. Therefore 
we display a post processing on our systems to 
make sure that the similarity between the same 
words has a larger rank than two different words of 
the same meaning. 
4 Experiments and Results 
We perform our systems on trial data and then use 
Kendall tau Rank Correlation (Kendall, 1995; 
Wessa, 2012) to evaluate the results shown in Ta-
ble 1. The trial data contains 50 word pairs. The 
similarity of each pair is scored by several experts 
and the mean value is regarded as the standard an-
swer to get the manual ranking. 
 
Method Kendall tau 2-sided p value 
MIXCC 0.273469 0.005208 
MIXCD0 0.152653 0.119741 
MIXCD 0.260408 0.007813 
Manual(upper) 0.441633 6.27E-06 
Table 1: Kendall tau Rank Correlation of systems on trial 
 
From Table 1, we can see the tau value of MIX-
CD0 is 0.1526 and MIXCD is 0.2604. MIXCD 
performed notably better than MIXCD0. It shows 
427
that path's length between two words is on an im-
portant position of measuring semantic similarity. 
This feature does improve the similarity result. The 
2-sided p value of MIXCD0 is 0.1197. It is much 
larger than the value of MIXCD which is 0.0078. 
So the ranking result of MIXCD0 is much more 
occasional than result of MIXCD. 
The tau value of MIXCC is 0.2735 and it is 
much smaller than the manual ranking result which 
is 0.4416 seen as the upper bound. It shows that the 
similarity between two words in human's minds 
dose not only depend on their hierarchical relation 
represented in Dictionary. But the value is larger 
than that of MIXCD. It seems that the mutual in-
formation derived from corpus which is expected 
to improve the result reduces the correction of rank 
result contrarily. There may be two reasons on it. 
First, because of the use of trial data in MIXCD, 
the result of similarity ranking strongly depended 
on this data. The reliability of trial data's ranking 
may influent the performance of our system. We 
calculate the tau value between every manual and 
the correct ranking. The least tau value is 0.4416 
and the largest one is 0.8220 with a large disparity. 
We use the Fleiss' kappa value (Fleiss, 1971) to 
evaluate the agreement of manual ranking and the 
result is 0.1526 which showed the significant disa-
greement. This disagreement may make the regres-
sion result cannot show the relation between 
features and score correctly. To reduce the disa-
greement's influence we calculate the mean of 
manual similarity score omitting the maximum and 
minimum ones and get a new standard rank (trial2). 
Then we perform MIXCD on trail2 and show the 
new result as MIXCD-2 in Form 2. MIXCC's re-
sult is also compared with trail2 shown as MIXCC-
2. 
 
 MIXCC-2 MIXCD-2 MIXCC MIXCD 
Kendall tau 0.297959 0.265306 0.273469 0.260408 
Table 2: tau value on new standard (omit max/min manual 
scores) 
 
From Table 2 we can see the tau values of 
MIXCC rose to 0.2980 and MIXCD to 0.2653. It 
shows that omitting the maximum and minimum 
manual scores can reduce some influence of the 
disagreement of artificial scoring.  
Second, the combination method of mutual in-
formation and semantic path in MRD may also 
influent the performance of our system. The ranks 
between MIXCD and MIXCC are also compared 
and the tau value is 0.2065. It shows a low agree-
ment of semantic similarity measurements between 
MRD and Corpus. The mutual information exerts a 
large influence on the measure of similarity and 
sometimes may bring the noise to the result mak-
ing it worse. 
We also perform our systems on test data con-
taining 297 words pairs in the same form of trial 
data and got the follow result: 
 
Method Kendall tau 
MIXCC 0.050 
MIXCD0 -0.064 
MIXCD 0.040 
Table 3 tau values of the result of test data 
 
The ranking on test data of our systems shows 
an even worse result. Because of the low confi-
dence of trial data ranking, multiple linear regres-
sion function learning from the trial data performs 
bad on other word pairs. 
5 Conclusion 
In this paper we propose three methods to evaluate 
the semantic similarity of Chinese word pairs. The 
first one uses MRDs and the second one adds the 
information derived from corpus. The third one 
uses the same knowledge resources as the second 
one but highlights the path length of the word pair. 
The results of the systems show a large difference 
and all have a low score. From the results we can 
see the similarity showed in corpus is much differ-
ent from the one expressed in MRD. One reason of 
the low score is that the manual rank given by the 
task has a low agreement among them. We get a 
new manual rank which reduces some influence of 
disagreement by calculating the mean value of 
scores omitting the maximum and minimum ones. 
Comparing the result of our systems with the new 
ranking, all of them get a higher tau value. 
Acknowledgement 
This paper is supported in part by National Natural 
Science Fund of China under contract 61170181, 
National Social Science Fund of China under con-
tract 10CYY021, State Key Lab. for Novel Soft-
ware Technology under contract KFKT2011B03, 
Jiangsu PostDoc Fund under contract 1101065C. 
428
References 
Mike E. Lesk, 1986. Automatic sense disambiguation 
using machine readable dictionaries: How to tell a 
pine cone from an ice cream cone. In Proceedings of 
the SIGDOC Conference 1986, Toronto, June. 
Philip Resnik, 1995. Using information content to eval-
uate semantic similarity. In Proceedings of the 14th 
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada. 
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing local context and WordNet sense similiarity for 
word sense disambiguation. In WordNet, An Elec-
tronic Lexical Database. The MIT Press. 
Zhibiao Wu and Martha Palmer, 1994. Verb semantics 
and lexical selection. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational 
Linguistics, Las Cruces, New Mexico. 
Jay J. Jiang and David W. Conrath, 1998. Semantic sim-
ilarity based on corpus statistics and lexical taxono-
my. In Proceedings of the International Conference 
on Research in Computational Linguistics. 
Ce Zhang , Yu-Jing Wang , Bin Cui , Gao Cong, 2008. 
Semantic similarity based on compact concept ontol-
ogy. In Proceeding of the 17th international confer-
ence on World Wide Web, April 21-25, 2008, Beijing, 
China 
Courtney Corley , Rada Mihalcea, 2005. Measuring the 
semantic similarity of texts. In Proceedings of the 
ACL Workshop on Empirical Modeling of Semantic 
Equivalence and Entailment, p.13-18, June 30-30, 
2005, Ann Arbor, Michigan. 
Ted Pedersen, 2010. Information content measures of 
semantic similarity perform better without sense-
tagged text. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational Lin-
guistics, p.329-332, June 02-04, 2010, Los Angeles, 
California. 
M. G. Kendall, 1955. Rank Correlation Methods. New 
York: Hafner Publishing Co. 
P. Wessa, 2012. Free Statistics Software, Office for Re-
search Development and Education, version 1.1.23-
r7, URL http://www.wessa.net/ 
Jordan L. Fleiss, 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin, Vol. 
76, No. 5 pp. 378?382. 
429
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 519?523,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
NJU-Parser: Achievements on Semantic Dependency Parsing 
Guangchao Tang1 Bin Li1,2 Shuaishuai Xu1 Xinyu Dai1 Jiajun Chen1 
1 State Key Lab for Novel Software Technology, Nanjing University 
2 Research Center of Language and Informatics, Nanjing Normal University 
Nanjing, Jiangsu, China 
{tanggc, lib, xuss, dxy, chenjj}@nlp.nju.edu.cn 
 
Abstract 
In this paper, we introduce our work on 
SemEval-2012 task 5: Chinese Semantic De-
pendency Parsing. Our system is based on 
MSTParser and two effective methods are 
proposed: splitting sentence by punctuations 
and extracting last character of word as lemma. 
The experiments show that, with a combina-
tion of the two proposed methods, our system 
can improve LAS about one percent and final-
ly get the second prize out of nine participat-
ing systems. We also try to handle the multi-
level labels, but with no improvement. 
1 Introduction 
Task 5 of SemEval-2012 tries to find approaches to 
improve Chinese sematic dependency parsing 
(SDP). SDP is a kind of dependency parsing. Cur-
rently, there are many dependency parsers availa-
ble, such as Eisner?s probabilistic dependency 
parser (Eisner, 1996), McDonald?s MSTParser 
(McDonald et al 2005a; McDonald et al 2005b) 
and Nivre?s MaltParser (Nivre, 2006). 
Despite of elaborate models, lots of problems 
still exist in dependency parsing. For example, sen-
tence length has been proved to show great impact 
on the parsing performance. (Li et al, 2010) used a 
two-stage approach based on sentence fragment for 
high-order graph-based dependency parsing. Lack-
ing of linguistic knowledge is also blamed. 
Three methods are promoted in this paper try-
ing to improve the performance: splitting sentence 
by commas and semicolons, extracting last charac-
ter of word as lemma and handling multi-level la-
bels. Improvements could be achieved through the 
first two methods while not for the third. 
2 Overview of Our System 
Our system is based on MSTParser which is one of 
the state-of-the-art parsers. MSTParser tries to ob-
tain the maximum spanning tree of a sentence. For 
projective parsing task, it takes Eisner?s algorithm 
(Eisner, 1996) to get the dependency tree in O(n3) 
time. Meanwhile, Chu-Liu-Edmond?s algorithm 
(Chu and Liu, 1965) is applied for non-projective 
task, which takes O(n2) time. 
Three methods are adopted to MSTParser in our 
system: 
1) Sentences are split into sub-sentences by 
commas and semicolons, for which there 
are two ways. Splitting sentences by all 
commas and semicolons is used in our 
primary system. In our contrast system, we 
use a classifier to determine whether a 
comma or semicolon can be used to split 
the sentence. In the primary and contrast 
system, the proto sentences and the sub-
sentences are trained and tested separately 
and the outputs are merged in the end. 
2) In a Chinese word, the last character usual-
ly contains main sense or semantic class. 
We treat the last character of the word as 
word lemma and find it gets a slightly im-
provement in the experiment. 
3) An experiment trying to solve the problem 
of multi-level labels was conducted by 
parsing different levels separately and con-
sequently merging the outputs together. 
The experiment results have shown that the first 
two methods could enhance the system perfor-
mance while further improvements could be ob-
tained through a combination of them in our sub-
submitted systems. 
519
 
a) The proto sentence from train data 
                       
b) The first sub sentence of a)                         c) The second sub sentence of a) 
Figure 1. An example of the split procedure. 
 
3 Experiments 
3.1 Split sentences by commas and semicolons 
It is observed that the performance decreases as 
the length of the sentences increases. Table 1 
shows the statistical analysis on the data including 
SemEval-2012, Conll-07?s Chinese corpus and a 
subset extracted from CTB using Penn2Malt. Long 
sentence can be split into sub-sentences to get bet-
ter parsing result.  
 
Items 
SemEval
-2012 
Conll-
07 CN 
CTB 
Postages count 35 13 33 
Dependency 
labels count 
122 69 12 
Average sentence 
length 
30.15 5.92 25.89 
Average 
dependency length 
4.80 1.71 4.36 
LAS 61.37 82.89 67.35 
UAS 80.18 87.64 79.90 
Table 1. Statistical analysis on the data. The CTB data is 
a subset extracted from CTB using Penn2Malt. 
 
Our work can be described as following steps: 
Step 1: Use MSTParser to parse the data. We 
name the result as ?normal output?. 
Step 2: Split train and test data by all commas 
and semicolons. The delimiters are removed in the 
sub sentences. For train data, a word?s dependency 
relation is kept if the word?s head is under the cov-
er of the sub sentence. Otherwise, its head will be 
set to root and its label will be set to ROOT (ROOT 
is the default label of dependency arcs whose head 
is root). We define the word as ?sentence head? if 
its head is root. ?Sub-sentence head? indicates the 
sentence head of a sub-sentence. After splitting, 
there may be more than one sub-sentence heads in 
a sub-sentence. Figure 1 shows an example of the 
split procedure. 
Step 3: Use MSTParser to parse the data gener-
ated in step 2. We name the parsing result ?split 
output?. In split output, there may be more than 
one sub-sentences corresponding to a single sen-
tence in normal output. 
Step 4: Merge the split output and the normal 
output. The outputs of sub-sentences are merged 
with delimiters restored. Dependency relations are 
recovered for all punctuations and sub-sentence 
heads in split output with relations in normal out-
put. The sentence head of normal output is kept in 
final output. The result is called ?merged split out-
put?. This step need to be consummated because it 
may result in a dependency tree not well formed 
with several sentence heads or even circles. 
The results of experiments on develop data and 
test data are showed in table 2. For develop data, 
an improvement of 0.85 could be obtained while 
0.93 for test data, both on LAS. 
In step 2, there is an alternative to split the sen-
tences, i.e., using a classifier to determine which 
comma and semicolon can be split. This method is 
taken in the contrast system. When applying the 
classifier, all commas and semicolons in train data 
520
are labeled with S-IN or S-STOP while other 
words with NULL. If the sub sentence before the 
comma or semicolon has only one sub-sentence 
head, it is labeled with S-STOP, otherwise with S-
IN. A model is built from train data with CRF++ 
and test data is evaluated with it. Features used are 
listed in table 3. Only commas and semicolons 
with label S-STOP can be used to split the sen-
tence in step 2. Other steps are the same as above. 
The result is also shown in table 2 as ?merged split 
output with CRF++?. 
 
Data Methods LAS UAS 
Develop 
data 
normal output 61.37 80.18 
merged split output 62.22 80.56 
merged split output 
with CRF++ 
61.97 80.73 
lemma output 61.64 80.47 
primary system output 62.41 80.96 
contrast system output 62.05 80.90 
Test 
 data 
normal output 60.63 79.37 
merged split output 61.56 80.17 
merged split output 
with CRF++ 
61.42 80.20 
lemma output 60.88 79.42 
primary system output 61.63 80.35 
contrast system output 61.64 80.29 
Table 2. Results of the experiments. 
 
w-4,w-3,w-2,w-1,w,w+1,w+2,w+3,w+4 
p-4,p-3,p-2,p-1,p,p+1,p+2,p+3,p+4 
wp-4,wp-3,wp-2,wp-1,wp wp+1,wp+2,wp+3,wp+4 
w-4|w-3,w-3|w-2,w-2|w-1,w-1|w, 
w|w+1,w+1|w+2,w+2|w+3,w+3|w+4 
p-4|p-3,p-3|p-2,p-2|p-1,p-1|p, 
p|p+1,p+1|p+2,p+2|p+3,p+3|p+4 
first word of sub-sentence before the delimiter 
Table 3. Features used in CRF++. w represents for word 
and p for PosTag. +1 means the index after current 
while -1 means before. 
3.2 Extract last character of word as lemma 
In Chinese, the last character of a word usually 
contains main sense or semantic class, which indi-
cates that it may represent the whole word. For 
example, ? ? ?(country) can represent ? ?
? ?(China) and ?? ?(love) can represent ??
??(crazy love).  
The last character is used as lemma in the ex-
periment, with an improvement of 0.27 for LAS on 
develop data and 0.24 on test data. Details of the 
scores are listed in table 2 as ?lemma output?. 
3.3 Multi-level labels experiment 
A notable characteristic of SemEval-2012?s da-
ta is multi-level labels. It introduces four kinds of 
multi-level labels which are s-X, d-X, j-X and r-X. 
The first level represents the basic semantic rela-
tion of the dependency while the second level 
shows the second import, except that s-X repre-
sents sub-sentence relation.  
The r-X label means that a verb modifies a 
noun and the relation between them is reverse. For 
example, in phrase ???(poor) ??(born) ? ?
?(star)?, ???? is headed to ???? with label r-
agent. It means that ???? is the agent of ????. 
When a verbal noun is the head word and its 
child has indirect relation to it, the dependency is 
labeled with j-X. In phrase ???(school) ??
(construction)?, ???? is the head of ???? with 
label j-content. ???? is the content of ????. 
The d-X label means that the child modifies the 
head with an additional relation. For example, in 
phrase ???(technology) ??(enterprise)?, ??
?? modifies ???? and the domain of ???? is 
????. 
A heuristic method is tried in the experiment. 
The multi-level labels of d-X, j-X and r-X are sep-
arated into two parts for each level. For example, 
?d-content? will be separated to ?d? and ?content?. 
For each part, MSTParser is used to train and test. 
We call the outputs ?first-level output? and ?se-
cond-level output?. The outputs of each level and 
normal output are merged then. 
In our experiments, only the word satisfies the 
following conditions need to be merged: 
a) The dependency label in normal output is 
started with d-, j- or r-. 
b) The dependency label in first-level output is 
d, j or r. 
c) The heads in first-level output and second-
level output are of the same. 
Otherwise, the dependency relation in normal 
output will be kept. There are also three ways in 
merging outputs: 
a) Label in first-level output and label in se-
cond-level output are merged. 
b) First level label in normal output and label 
in second-level output are merged. 
c) Label in first-level output and second level 
label in normal output are merged. 
521
Experiment has been done on develop data. In 
the experiment, 24% of the labels are merged and 
92% of the new merged labels are the same as 
original. The results of three ways are listed in ta-
ble 4. All of them get decline compared to normal 
output. 
 
outputs LAS UAS 
normal output 61.37 80.18 
way a) 61.18 80.18 
way b) 61.25 80.18 
way c) 61.25 80.18 
Table 4. Results of multi-level labels experiment on 
develop data. 
3.4 Combined experiment on split and lemma 
Improvements are achieved by first two meth-
ods in the experiment while a further enhancement 
is made with a combination of them in the submit-
ted systems. The split method and lemma method 
are combined as primary system. The split method 
with CRF++ and lemma method are combined as 
contrast system. When combining the two methods, 
last character of the word is firstly extracted as 
lemma for train data and test data. Then the split or 
split with CRF++ method is used. 
The outputs of the primary system and contrast 
system are listed in table 2.  
4 Analysis and Discussion 
The contrast system presented in this paper finally 
got the second prize among nine systems. The pri-
mary system gets the third. There is an improve-
ment of about one percent for both primary and 
contrast system. The following conclusions can be 
made from the experiments: 
1) Parsing is more effective and accurate on 
short sentences. A word prefers to depend 
on another near to it. A sentence can be 
split to several sub sentences by commas 
and semicolons to get better parsing output. 
Result may be improved with a classifier to 
determine whether a comma or semicolon 
can be used to split the sentence. 
2) Last character of word is a useful feature. 
In the experiment, the last character is 
coarsely used as lemma and a minor im-
provement is achieved. Much more lan-
guage knowledge can be used in parsing. 
3) The label set of the data is worthy to be re-
viewed. The meanings of the labels are not 
given in the task. Some of them are confus-
ing especially the multi-level labels. The 
trying of training and testing multi-level la-
bels separately by levels fails with a slight-
ly decline of the score. Multi-level also 
causes too many labels: any single-level la-
bel can be prefixed to form a new multi-
level label. It?s a great problem for current 
parsers. Whether the label set is suitable to 
Chinese semantic dependency parsing 
should be discussed. 
5 Conclusion and Future Work 
Three methods applied in NJU-Parser are de-
scribed in this paper: splitting sentences by com-
mas and semicolons, taking last character of word 
as lemma and handling multi-level labels. The first 
two get improvements in the experiments. Our 
primary system is a combination of the first two 
methods. The contrast system is the same as prima-
ry system except that it has a classifier implement-
ed in CRF++ to determine whether a comma or a 
semicolon should be used to split the sentence. 
Both of the systems get improvements for about 
one percent on LAS. 
In the future, a better classifier should be devel-
oped to split the sentence. New method should be 
applied in merging split outputs to get a well 
formed dependency tree. And we hope there will 
be a better label set which are more capable of de-
scribing semantic dependency relations for Chi-
nese. 
Acknowledgments 
This paper is supported in part by National Natural 
Science Fund of China under contract 61170181, 
Natural Science Fund of Jiangsu under contract 
BK2011192, and National Social Science Fund of 
China under contract 10CYY021. 
References 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
MSTParser: 
http://www.seas.upenn.edu/~strctlrn/MSTParser/MS
TParser.html 
522
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING. 
J. Nivre. 2006. Inductive Dependency Parsing. Springer. 
R. McDonald, K. Crammer, and F. Pereira. 2005. 
Online Large-Margin Training of Dependency 
Parsers. 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005). 
R. McDonald, F. Pereira, K. Ribarov, and J. Haji?. 2005. 
Non-projective Dependency Parsing using Spanning 
Tree Algorithms. Proceedings of HLT/EMNLP 2005. 
Zhenghua Li, Wanxiang Che, Ting Liu. 2010. Improv-
ing Dependency Parsing Using Punctuation. Interna-
tional Conference on Asian Language 
Processing(IALP) 2010. 
523
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135?143,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improving Word Alignment by Semi-supervised Ensemble
Shujian Huang1, Kangxi Li2, Xinyu Dai1, Jiajun Chen1
1State Key Laboratory for Novel Software Technology at Nanjing University
Nanjing 210093, P.R.China
{huangsj,daixy,chenjj}@nlp.nju.edu.cn
2School of Foreign Studies, Nanjing University
Nanjing 210093, P.R.China
richardlkx@126.com
Abstract
Supervised learning has been recently
used to improve the performance of word
alignment. However, due to the limited
amount of labeled data, the performance
of ?pure? supervised learning, which only
used labeled data, is limited. As a re-
sult, many existing methods employ fea-
tures learnt from a large amount of unla-
beled data to assist the task. In this pa-
per, we propose a semi-supervised ensem-
ble method to better incorporate both la-
beled and unlabeled data during learning.
Firstly, we employ an ensemble learning
framework, which effectively uses align-
ment results from different unsupervised
alignment models. We then propose to
use a semi-supervised learning method,
namely Tri-training, to train classifiers us-
ing both labeled and unlabeled data col-
laboratively and further improve the result.
Experimental results show that our meth-
ods can substantially improve the quality
of word alignment. The final translation
quality of a phrase-based translation sys-
tem is slightly improved, as well.
1 Introduction
Word alignment is the process of learning bilin-
gual word correspondences. Conventional word
alignment process is treated as an unsupervised
learning task, which automatically learns the cor-
respondences between bilingual words using an
EM style algorithm (Brown et al, 1993; Vogel
et al, 1996; Och and Ney, 2003). Recently, su-
pervised learning methods have been used to im-
prove the performance. They firstly re-formalize
word alignment as some kind of classification
task. Then the labeled data is used to train the
classification model, which is finally used to clas-
sify unseen test data (Liu et al, 2005; Taskar et
al., 2005; Moore, 2005; Cherry and Lin, 2006;
Haghighi et al, 2009).
It is well understood that the performance of
supervised learning relies heavily on the fea-
ture set. As more and more features are added
into the model, more data is needed for train-
ing. However, due to the expensive cost of la-
beling, we usually cannot get as much labeled
word alignment data as we want. This may limit
the performance of supervised methods (Wu et
al., 2006). One possible alternative is to use
features learnt in some unsupervised manner to
help the task. For example, Moore (2005) uses
statistics like log-likelihood-ratio and conditional-
likelihood-probability to measure word associa-
tions; Liu et al (2005) and Taskar et al (2005)
use results from IBM Model 3 and Model 4, re-
spectively.
Ayan and Dorr (2006) propose another way of
incorporating unlabeled data. They first train some
existing alignment models, e.g. IBM Model4 and
Hidden Markov Model, using unlabeled data. The
results of these models are then combined using a
maximum entropy classifier, which is trained us-
ing labeled data. This method is highly efficient
in training because it only makes decisions on
alignment links from existing models and avoids
searching the entire alignment space.
In this paper, we follow Ayan and Dorr (2006)?s
idea of combining multiple alignment results. And
we use more features, such as bi-lexical features,
which help capture more information from unla-
beled data. To further improve the decision mak-
ing during combination, we propose to use a semi-
supervised strategy, namely Tri-training (Zhou
and Li, 2005), which ensembles three classifiers
using both labeled and unlabeled data. More
specifically, Tri-training iteratively trains three
classifiers and labels all the unlabeled instances.
It then uses some instances among the unlabeled
ones to expand the labeled training set of each in-
135
dividual classifier. As word alignment task usually
faces a huge parallel corpus, which contains mil-
lions of unlabeled instances, we develop specific
algorithms to adapt Tri-training for this large scale
task.
The next section introduces the supervised
alignment combination framework; Section 3
presents our semi-supervised learning algorithm.
We show the experiments and results in Section
4; briefly overview related work in Section 5 and
conclude in the last section.
2 Word Alignment as a Classification
Task
2.1 Modeling
Given a sentence pair (e, f), where e =
e1, e2, . . . , eI and f = f1, f2, . . . , fJ , an align-
ment link ai,j indicates the translation correspon-
dence between words ei and fj . Word alignment
is to learn the correct alignment A between e and
f , which is a set of such alignment links.
As the number of possible alignment links
grows exponentially with the length of e and f , we
restrict the candidate set using results from several
existing alignment models. Note that, all the mod-
els we employ are unsupervised models. We will
refer to them as sub-models in the rest of this pa-
per.
Let A = {A1, A2, . . . , An} be a set of align-
ment results from sub-models; AI and AU be the
intersection and union of these results, respec-
tively. We define our learning task as: for each
alignment link ai,j in the candidate set AC =
AU?AI , deciding whether ai,j should be included
in the alignment result. We use a random variable
yi,j (or simply y) to indicate whether an alignment
link ai,j ? AC is correct. A Maximum Entropy
model is employed to directly model the distribu-
tion of y. The probability of y is defined in For-
mula 1, where hm(y, e, f ,A, i, j) is the mth fea-
ture function, and ?m is the corresponding weight.
p(y|e, f ,A, i, j)
=
exp
?M
m=1 ?mhm(y, e, f ,A, i, j)
?
y??{0,1} exp
?M
m=1 ?mhm(y?, e, f ,A, i, j)
(1)
While Ayan and Dorr (Ayan and Dorr, 2006)
make decisions on each alignment link in AU , we
take a different strategy by assuming that all the
alignment links in AI are correct, which means
alignment links in AI are always included in the
combination result. One reason for using this
strategy is that it makes no sense to exclude an
alignment link, which all the sub-models vote for
including. Also, links in AI usually have a good
quality (In our experiment, AI can always achieve
an accuracy higher than 96%). On the other hand,
because AI is decided before the supervised learn-
ing starts, it will be able to provide evidence for
making decisions on candidate links.
Also note that, Formula 1 is based on the as-
sumption that given AI , the decision on each y is
independent of each other. This is the crucial point
that saves us from searching the whole alignment
space. We take this assumption so that the Tri-
training strategy can be easily applied.
2.2 Features
For ensemble, the most important features are the
decisions of sub-models. We also use some other
features, such as POS tags, neighborhood infor-
mation, etc. Details of the features for a given link
ai,j are listed below.
Decision of sub-models: Whether ai,j exists in
the result of kth sub-model Ak. Besides in-
dividual features for each model, we also in-
clude features describing the combination of
sub-models? decisions. For example, if we
have 3 sub-models, there will be 8 features
indicating the decisions of all the sub-models
as 000, 001, 010, . . . , 111.
Part of speech tags: POS tags of previous, cur-
rent and next words in both languages. We
also include features describing the POS tag
pairs of previous, current and next word pairs
in the two languages.
Neighborhood: Whether each neighbor link ex-
ists in the intersection AI . Neighbor links re-
fer to links in a 3*3 window with (i, j) in the
center.
Fertilities: The number of words that ei (or fj) is
aligned to in AI .
Relative distance: The relative distance between
ei and fj , which is calculated as abs(i/I ?
j/J).
Conditional Link Probability (CLP) : The con-
ditional link probability (Moore, 2005) of ei
136
and fj . CLP of word e and f is estimated on
an aligned parallel corpus using Formula 2,
CLPd(e, f) =
link(e, f)? d
cooc(e, f)
(2)
where link(e, f) is the number of times e and
f are linked in the aligned corpus; cooc(e, f)
is the number of times e and f appear in
the same sentence pair; d is a discount-
ing constant which is set to 0.4 following
Moore (2005). We estimate these counts on
our set of unlabeled data, with the union of
all sub-model results AU as the alignment.
Union is used in order to get a better link cov-
erage. Probabilities are computed only for
those words that occur at least twice in the
parallel corpus.
bi-lexical features: The lexical word pair ei-fj .
Lexical features have been proved to be useful in
tasks such as parsing and name entity recognition.
Taskar et al (2005) also employ similar bi-lexical
features of the top 5 non-punctuation words for
word alignment. Using bi-lexicons for arbitrary
word pairs will capture more evidence from the
data; although it results in a huge feature set which
may suffer from data sparseness. In the next sec-
tion, we introduce a semi-supervised strategy will
may alleviate this problem and further improve the
learning procedure.
3 Semi-supervised methods
Semi-supervised methods aim at using unlabeled
instances to assist the supervised learning. One
of the prominent achievements in this area is
the Co-training paradigm proposed by Blum and
Mitchell (1998). Co-training applies when the fea-
tures of an instance can be naturally divided into
two sufficient and redundant subsets. Two weak
classifiers can be trained using each subset of fea-
tures and strengthened using unlabeled data. Blum
and Mitchell (1998) prove the effectiveness of this
algorithm, under the assumption that features in
one set is conditionally independent of features in
the other set. Intuitively speaking, if this condi-
tional independence assumption holds, the most
confident instance of one classifier will act as a
random instance for the other classifier. Thus it
can be safely used to expand the training set of the
other classifier.
The standard Co-training algorithm requires a
naturally splitting in the feature set, which is hard
to meet in most scenarios, including the task of
word alignment. Variations include using random
split feature sets or two different classification al-
gorithms. In this paper, we use the other Co-
training style algorithm called Tri-training, which
requires neither sufficient and redundant views nor
different classification algorithms.
3.1 Tri-training
Similar with Co-training, the basic idea of Tri-
training (Zhou and Li, 2005) is to iteratively ex-
pand the labeled training set for the next-round
training based on the decisions of the current clas-
sifiers. However, Tri-training employs three clas-
sifiers instead of two. To get diverse initial classi-
fiers, the training set of each classifier is initially
generated via bootstrap sampling from the origi-
nal labeled training set and updated separately. In
each round, these three classifiers are used to clas-
sify all the unlabeled instances. An unlabeled in-
stance is added to the training set of any classifier
if the other two classifiers agree on the labeling
of this example. So there is no need to explicitly
measure the confidence of any individual classi-
fier, which might be a problem for some learning
algorithms. Zhou and Li (2005) also give a termi-
nate criterion derived from PAC analysis. As the
algorithm goes, the number of labeled instances
increases, which may bring in more bi-lexical fea-
tures and alleviate the problem of data sparseness.
3.2 Tri-training for Word Alignment
One crucial problem for word alignment is the
huge amount of unlabeled instances. Typical par-
allel corpus for word alignment contains at least
hundreds of thousands of sentence pairs, with each
sentence pair containing tens of instances. That
makes a large set of millions of instances. There-
fore, we develop a modified version of Tri-training
algorithm using sampling techniques, which can
work well with such large scale data. A sketch of
our algorithm is shown in Figure 1.
The algorithm takes original labeled instance
set L, unlabeled sentence set SU , sub-model re-
sults As for each s in SU and a sampling ratio r as
input. Fk represents the kth classifier. Variables
with superscript i represent their values during the
ith iteration.
Line 2 initializes candidate instance set AC,s of
each sentence s to be the difference set between
137
Input: L, SU , As for each s and sampling ratio r.
1: for all sentence s in SU do
2: A0C,s ? AU,s ?AI,s //initializing candidate set
3: end for
4: for all l ? {1, 2, 3} do
5: L0l ? Subsample(L, 0.33)
6: F 0l ? Train(L0l )
7: end for
8: repeat
9: for all l ? {1, 2, 3} do
10: Let m,n ? {1, 2, 3} and m ?= n ?= l; Lil = ?
11: for all sentence s in SU do
12: for all instance a in Ai?1C,s do
13: if F i?1m (a) = F i?1n (a) then
14: Ai?1C,s ? A
i?1
C,s ? {(a, F
i?1
m (a))}
15: Lil ? Lil ? {(a, F i?1m (a))}
16: end if
17: end for
18: end for
19: end for
20: for all l ? {1, 2, 3} do
21: Lil ? Subsampling(Lil, r) ? Li?1l
22: F il ? Train(Lil)
23: AiC,s ? Ai?1C,s
24: end for
25: until all AiC,s are unchanged or empty
Output: F (x)? argmaxy?{0,1}
?
l:Fl(x)=y
1
Figure 1: Modified Tri-training Algorithm
AU,s and AI,s. In line 5-6, sub-samplings are per-
formed on the original labeled set L and the ini-
tial classifier F 0l is trained using the sampling re-
sults. In each iteration, the algorithm labels each
instance in the candidate set AiC,s for each clas-
sifier with the other two classifiers trained in last
iteration. Instances are removed from the candi-
date set and added to the labeled training set (Lil)
of classifier l, if they are given the same label by
the other two classifiers (line 13-16).
A sub-sampling is performed before the labeled
training set is used for training (line 21), which
means all the instances in Lil are accepted as cor-
rect, but only part of them are added into the train-
ing set. The sampling rate is controlled by a pa-
rameter r, which we empirically set to 0.01 in all
our experiments. The classifier is then re-trained
using the augmented training set Lil (line 22). The
algorithm iterates until all instances in the candi-
date sets get labeled or the candidate sets do not
change since the last iteration (line 25). The result-
ing classifiers can be used to label new instances
via majority voting.
Our algorithm differs from Zhou and Li (2005)
in the following three aspects. First of all, com-
paring to the original bootstrap sampling initial-
ization, we use a more aggressive strategy, which
Source Usage Sent. Pairs Cand. Links
LDC Train 288111 8.8M
NIST?02 Train 200 5,849
NIST?02 Eval 291 7,797
Table 1: Data used in the experiment
actually divides the original labeled set into three
parts. This strategy ensures that initial classifiers
are trained using different sets of instances and
maximizes the diversity between classifiers. We
will compare these two initializations in the ex-
periments section. Secondly, we introduce sam-
pling techniques for the huge number of unlabeled
instances. Sampling is essential for maintain-
ing a reasonable growing speed of training data
and keeping the computation physically feasible.
Thirdly, because the original terminate criterion
requires an error estimation process in each iter-
ation, we adapt the much simpler terminate cri-
terion of standard Co-training into our algorithm,
which iterates until all the unlabeled data are fi-
nally labeled or the candidate sets do not change
since the last iteration. In other words, our algo-
rithm inherits both the benefits of using three clas-
sifiers and the simplicity of using Co-training style
termination criterion. Parallel computing tech-
niques are also used during the processing of un-
labeled data to speed up the computation.
4 Experiments and Results
4.1 Data and Evaluation Methodology
All our experiments are conducted on the lan-
guage pair of Chinese and English. For training
alignment systems, a parallel corpus coming from
LDC2005T10 and LDC2005T14 is used as un-
labeled training data. Labeled data comes from
NIST Open MT Eval?02, which has 491 labeled
sentence pairs. The first 200 labeled sentence pairs
are used as labeled training data and the rest are
used for evaluation (Table 1). The number of can-
didate alignment links in each data set is also listed
in Table 1. These candidate alignment links are
generated using the three sub-models described in
Section 4.2.
The quality of word alignment is evaluated in
terms of alignment error rate (AER) (Och and Ney,
2003), classifier?s accuracy and recall of correct
decisions. Formula 3 shows the definition of AER,
where P and S refer to the set of possible and sure
alignment links, respectively. In our experiments,
138
ModelName AER Dev AER Test Accuracy Recall F1
Model4C2E 0.4269 0.4196 0.4898 0.3114 0.3808
Model4E2C 0.3715 0.3592 0.5642 0.5368 0.5502
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Model4GDF 0.3328 0.3336 0.6059 0.6184 0.6121
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
Table 2: Experiments of Sub-models
ModelName AER Dev AER Test Accuracy Recall F1
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Tri-Bootstrap0 0.2301 0.2488 0.8030 0.6858 0.7398
Tri-Divide0 0.2458 0.2525 0.8002 0.6630 0.7251
Tri-Bootstrap 0.2264 0.2468 0.7934 0.7449 0.7684
Tri-Divide 0.2416 0.2494 0.7832 0.7605 0.7717
Table 3: Experiments of Semi-supervised Models
we treat all alignment links as sure links.
AER = 1? |A ? P |+ |A ? S|
|A|+ |S|
(3)
We also define a F1 score to be the harmonic mean
of classifier?s accuracy and recall of correct deci-
sions (Formula 4).
F1 =
2 ? accuracy ? recall
accuracy + recall
(4)
We also evaluate the machine translation quality
using unlabeled data (in Table 1) and these align-
ment results as aligned training data. We use
multi-references data sets from NIST Open MT
Evaluation as development and test data. The En-
glish side of the parallel corpus is trained into
a language model using SRILM (Stolcke, 2002).
Moses (Koehn et al, 2003) is used for decoding.
Translation quality is measured by BLEU4 score
ignoring the case.
4.2 Experiments of Sub-models
We use the following three sub-models: bidi-
rectional results of Giza++ (Och and Ney,
2003) Model4, namely Model4C2E and
Model4E2C, and the joint training result of
BerkeleyAligner (Liang et al, 2006) (Berke-
leyAl.). To evaluate AER, all three data sets
listed in Table 1 are combined and used for the
unsupervised training of each sub-model.
Table 2 presents the alignment quality of those
sub-models, as well as a supervised ensemble of
them, as described in Section 2.1. We use the sym-
metrized IBM Model4 results by the grow-diag-
final-and heuristic as our baseline (Model4GDF).
Scores in Table 2 show the great improvement
of supervised learning, which reduce the align-
ment error rate significantly (more than 5% AER
points from the best sub-model, i.e. Berke-
leyAligner). This result is consistent with Ayan
and Dorr (2006)?s experiments. It is quite reason-
able that supervised model achieves a much higher
classification accuracy of 0.8124 than any unsu-
pervised sub-model. Besides, it also achieves the
highest recall of correct alignment links (0.7027).
4.3 Experiments of Semi-supervised Models
We present our experiment results on semi-
supervised models in Table 3. The two strategies
of generating initial classifiers are compared. Tri-
Bootstrap is the model using the original boot-
strap sampling initialization; and Tri-Divide is
the model using the dividing initialization as de-
scribed in Section 3.2. Items with superscripts 0
indicate models before the first iteration, i.e. ini-
tial models. The scores of BerkeleyAligner and
the supervised model are also included for com-
parison.
In general, all supervised and semi-supervised
models achieve better results than the best sub-
model, which proves the effectiveness of labeled
training data. It is also reasonable that initial mod-
els are not as good as the supervised model, be-
cause they only use part of the labeled data for
training. After the iterative training, both the two
139
0 1000 2000 3000 4000 5000 60000.4
0.5
0.6
0.7
0.8
0.9
Training Instances Number
Score
s(F?1, 
Accura
cy, Rec
all)
 
 
F?1RecallAccuracy
(a)
0 0.5 1 1.5 2 2.5 3x 1050.73
0.74
0.75
0.76
0.77
0.78
0.79
Number of sentences
F?1 s
cores
 
 
Tri?DivideSupervisedTri?Bootstrap
(b)
Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b) Experiments
on the Size of Unlabeled Data in Tri-training
Tri-training models get a significant increase in
recall. We attribute this to the use of bi-lexical
features described in Section 2.2. Analysis of
the resulting model shows that the number of
bi-lexical features increases from around 300 to
nearly 7,800 after Tri-training. It demonstrates
that semi-supervised algorithms are able to learn
more bi-lexical features automatically from the
unlabeled data, which may help recognize more
translation equivalences. However, we also notice
that the accuracy drops a little after Tri-training.
This might also be caused by the large set of bi-
lexical features, which may contain some noises.
In the comparison of initialization strategies,
the dividing strategy achieves a much higher re-
call of 0.7605, which is also the highest among
all models. It also achieves the best F1 score of
0.7717, higher than the bootstrap sampling strat-
egy (0.7684). This result confirms that diversity of
initial classifiers is important for Co-training style
algorithms.
4.4 Experiments on the Size of Data
4.4.1 Size of Labeled Data
We design this experiment to see how the size of
labeled data affects the supervised training proce-
dure. Our labeled training set contains 5,800 train-
ing instances. We randomly sample different sets
of instances from the whole set and perform the
supervised training.
The alignment results are plotted in Figure 2a.
Basically, both accuracy and recall increase with
the size of labeled data. However, we also find that
the increase of all the scores gets slower when the
number of training instances exceeds 3,000. One
possible explanation for this is that the training
set itself is too small and contains redundant in-
stances, which may prevent further improvement.
We can see in the Section 4.4.2 that the scores can
be largely improved when more data is added.
4.4.2 Size of Unlabeled Data
For better understanding the effect of unlabeled
data, we run the Tri-training algorithm on unla-
beled corpus of different sizes. The original un-
labeled corpus contains about 288 thousand sen-
tence pairs. We create 12 sub-corpus of it with
different sizes by selecting certain amounts of sen-
tences from the beginning. Our smallest sub-
corpus consists of the first 5,000 sentence pairs of
the original corpus; while the largest sub-corpus
contains the first 275 thousand sentence pairs. The
alignment results on these different sub-corpus are
evaluated (See Figure 2b).
The result shows that as the size of unlabeled
data grows, the F1 score of Tri-Divide increases
from around 0.74 to 0.772. The F1 score of Tri-
Bootstrap also gets a similar increase. This proves
that adding unlabeled data does help the learning
process. The result also suggests that when the
size of unlabeled data is small, both Tri-Bootstrap
and Tri-Divide get lower scores than the super-
vised model. This is because the Tri-training mod-
els only use part of the labeled data for the training
of each individual classifier, while the supervised
model use the whole set. We can see that when
there are more than 50 thousand unlabeled sen-
tence pairs, both Tri-training models outperform
the supervised model significantly.
140
ModelName Dev04 Test05 Test06 Test08
Model4C2E 24.54 17.10 17.52 14.59
Model4E2C 26.54 19.00 20.18 16.56
BerkeleyAl. 26.19 20.08 19.65 16.70
Model4GDF 26.75 20.67 20.58 17.05
Supervised 27.07 20.00 19.47 16.13
Tri-Bootstrap 26.88 20.49 20.76 17.31
Tri-Divide 27.04 20.96 20.79 17.18
Table 4: Experiments on machine translation (BLEU4 scores in percentage)
Note that, both experiments on data size show
some unsteadiness during the learning process.
We attribute this mainly to the random sampling
we use in the algorithm. As there are, in all, about
8.8 million instances , it is highly possible that
some of these instances are redundant or noisy.
And because our random sampling does not dis-
tinguish different instances, the quality of result-
ing model may get affected if these redundant or
noisy instances are selected and added to the train-
ing set.
4.5 Experiments on Machine Translation
We compare the machine translation results of
each sub-models, supervised models and semi-
supervised models in Table 4. Among sub-models,
BerkeleyAligner gets better BLEU4 scores in al-
most all the data sets except TEST06, which
agrees with its highest F1 score among all sub-
models. The supervised method gets the highest
BLEU score of 27.07 on the dev set. However, its
performance on the test sets is a bit lower than that
of BerkeleyAligner.
As we expect, our two semi-supervised mod-
els achieve highest scores on almost all the data
sets, which are also higher than the commonly
used grow-diag-final-and symmetrization of IBM
Model 4. More specifically, Tri-Divide is the
best of all systems. It gets a dev score of 27.04,
which is comparable with the highest one (27.07).
Tri-Divide also gets the highest BLEU scores
on Test05 and Test06 (20.96 and 20.79, respec-
tively), which are nearly 1 point higher than all
sub-models. The other Tri-training model, Tri-
Bootstrap, gets the highest score on Test08, which
is also significantly better than those sub-models.
Despite the large improvement in F1 score, our
two Tri-training models only get slightly better
score than the well-known Model4GDF. This kind
of inconsistence between AER or F1 scores and
BLEU scores is a known issue in machine trans-
lation community (Fraser and Marcu, 2007). One
possible explanation is that both AER or F1 are
0-1 loss functions, which means missing one link
and adding one redundant link will get the same
penalty. And more importantly, every wrong link
receives the same penalty under these metrics.
However, these different errors may have different
effects on the machine translation quality. Thus,
improving alignment quality according to AER or
F1 may not directly lead to an increase of BLEU
scores. The relationship among these metrics are
still under investigation.
5 Related work
Previous work mainly focuses on supervised
learning of word alignment. Liu et al (2005)
propose a log-linear model for the alignment be-
tween two sentences, in which different features
can be used to describe the alignment quality.
Moore (2005) proposes a similar framework, but
with more features and a different search method.
Other models such as SVM and CRF are also
used (Taskar et al, 2005; Cherry and Lin, 2006;
Haghighi et al, 2009). For alignment ensemble,
Wu and Wang (2005) introduce a boosting ap-
proach, in which the labeled data is used to cal-
culate the weight of each sub-model.
These researches all focus on the modeling of
alignment structure and employ some strategy to
search for the optimal alignment. Our main con-
tribution here is the use Co-training style semi-
supervised methods to assist the ensemble learn-
ing framework of Ayan and Dorr (2006). Although
we use a maximum entropy model in our experi-
ment, other models like SVM and CRF can also
be incorporated into our learning framework.
In the area of semi-supervised learning of word
alignment, Callison-Burch et al (2004) compare
the results of interpolating statistical machine
141
translation models learnt from labeled and unla-
beled data, respectively. Wu et al (2006) propose
a modified boosting algorithm, where two differ-
ent models are also trained using labeled and un-
labeled data respectively and interpolated. Fraser
and Marcu (2006) propose an EMD algorithm,
where labeled data is used for discriminative re-
ranking. It should be pointed out that these pieces
of work all use two separate processes for learn-
ing with labeled and unlabeled data. They either
train and interpolate two separate models or re-
rank previously learnt models with labeled data
only. Our proposed semi-supervised strategy is
able to incorporate both labeled and unlabeled data
in the same process, which is in a different line of
thinking.
6 Conclusions and Future Work
Semi-supervised techniques are useful when there
is a large amount of unlabeled data. In this
paper, we introduce a semi-supervised learning
method, called Tri-training, to improve the word
alignment combination task. Although experi-
ments have proved the effectiveness of our meth-
ods, there is one defect that should be mentioned.
As we previously assume that all the decisions
on alignment links are independent of each other
(in Section 2.1), our model are only able to cap-
ture link level evidence like bi-lexical features.
Some global features, such as final word fertil-
ity, cannot be integrated into the current frame-
work. In the future, we plan to apply our semi-
supervised strategy in more complicated learning
frameworks, which are able to capture those global
features.
Currently we use a random sampling to handle
the 8.8 million instances. We will also explore
better and more aggressive sampling techniques,
which may lead to more stable training results and
also enable us to process larger corpus.
Acknowledgments
The authors would like to thank Dr. Ming Li,
Mr. Junming Xu and the anonymous reviewers for
their valuable comments. This work is supported
by the National Fundamental Research Program
of China(2010CB327903) and the Scientific Re-
search Foundation of Graduate School of Nanjing
University(2008CL08).
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
imum entropy approach to combining word align-
ments. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 96?103, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92?100. Morgan
Kaufmann Publishers.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In ACL
?04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 175,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 105?112,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, pages 769?776, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293?303.
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Association for Computational Linguistics, Sin-
gapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In Robert C. Moore,
Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association
for Computational Linguistics.
142
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 459?
466, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 81?88, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, page
901 904.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
73?80, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836?841.
Hua Wu and Haifeng Wang. 2005. Boosting statistical
word alignment. In Proceedings of MT SUMMIT X,
pages 364?371, Phuket Island, Thailand, September.
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 913?
920, Sydney, Australia, July. Association for Com-
putational Linguistics.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classifiers. vol-
ume 17, pages 1529?1541, Piscataway, NJ, USA.
IEEE Educational Activities Department.
143
Proceedings of the 2012 Student Research Workshop, pages 13?18,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Active Learning with Transfer Learning 
 
 
Chunyong Luo, Yangsheng Ji, Xinyu Dai, Jiajun Chen 
State Key Laboratory for Novel Software Technology, 
Department of Computer Science and Technology, 
Nanjing University, Nanjing, 210046, China 
{luocy,jiys,daixy,chenjj}@nlp.nju.edu.cn 
 
 
 
 
 
 
Abstract 
In sentiment classification, unlabeled user 
reviews are often free to collect for new 
products, while sentiment labels are rare. In this 
case, active learning is often applied to build a 
high-quality classifier with as small amount of 
labeled instances as possible.  However, when 
the labeled instances are insufficient, the 
performance of active learning is limited. In 
this paper, we aim at enhancing active learning 
by employing the labeled reviews from a 
different but related (source) domain. We 
propose a framework Active Vector Rotation 
(AVR), which adaptively utilizes the source 
domain data in the active learning procedure. 
Thus, AVR gets benefits from source domain 
when it is helpful, and avoids the negative 
affects when it is harmful. Extensive 
experiments on toy data and review texts show 
our success, compared with other state-of-the-
art active learning approaches, as well as 
approaches with domain adaptation. 
1 Introduction 
To get a good generalization in traditional 
supervised learning, we need sufficient labeled 
instances in training, which are drawn from the 
same distribution as testing instances. When there 
are plenty of unlabeled instances but labels are 
insufficient and expensive to obtain, active 
learning (Settles, 2009) selects a small set of 
critical instances from target domain to be labeled, 
but costs are incurred for each label. On the other 
hand, transfer learning (Ji et al, 2011), also known 
as domain adaptation (Blitzer et al, 2006), aims at 
leveraging instances from other related source 
domains to construct high-quality models in the 
target domain. For example, we may employ 
labeled user reviews of similar products, to predict 
sentiment labels of new product reviews. When the 
distributions of source and target domain are 
similar, transfer learning would work well. But 
significant distribution divergence might cause 
negative transfer (Rosenstein et al, 2005). 
To further reduce the labeling cost and avoid 
negative transfer, we propose a framework, namely 
Active Vector Rotation (AVR), which takes 
advantage of both active learning and transfer 
learning techniques. Basically, AVR makes 
model?s parameter vector ? actively rotate towards 
its optimal direction with as few labeled instances 
in target domain as possible. Specifically, AVR 
first applies certain unsupervised learning 
techniques to make source and target domain?s 
distributions more ?similar?, and then leverages 
source domain information to query the most 
informative instances of target domain. Most 
importantly, it carefully reweights instances to 
mitigate the risk of negative transfer. AVR is 
general enough to incorporate various active 
learning and transfer learning modules, as well as 
varied basic learners such as LR and SVM.  
2 Related Work 
Shi et al (2008) proposed an approach AcTraK, 
using labeled source and target domain instances to 
build a so-called ?transfer classifier? to help label 
actively selected target domain instances. AcTraK 
initially requires labeled target domain instances, 
13
and relies too much on the transfer classifier. Thus 
it might be degenerated by negative transfer. 
An ALDA framework was proposed in (Saha et 
al., 2011). ALDA employs source domain 
classifier ????? to help label actively selected target domain instances. When conditional distributions 
???|?? are a bit different (Chattopadhyay et al, 
2011) or marginal distributions ????  are 
significantly different between source and target 
domain, ALDA would perform poorly. ALDA 
doesn?t discuss the negative transfer problem and 
gets hurts when it happens, while AVR actively 
avoids it by its projection and reweighting strategy. 
Liao et al (2005) proposed a method M-Logit, 
utilizing auxiliary data to help train LR. They also 
proposed actively sampling target domain 
instances using Fisher Information Matrix 
(Fedorov, 1972; Mackay, 1992). Besides, instance 
weighting was used to mitigate distribution 
difference between source and target domain in 
(Huang et al, 2006; Jiang and Zhai, 2007; 
Sugiyama et al, 2008). These can work as a 
module in our framework. 
3 AVR: Active Vector Rotation  
Without loss of generalization, we will constrain 
the discussion of AVR to binary classification 
tasks. But in fact, AVR can also be applied to 
multi-class classification and regression. 
Given training set ??? ? ????, ???|? ? 1,? ,??, ?? ? ?? , ?? ? ??1,?1? , traditional supervised learning tries to optimize (Fan et al, 2008; Lin et 
al., 2008): 
min??? ||?|| ? ? ? ???; ??, ??????? ,        (1) where the penalty parameter ? ? 0 , controls the 
importance ratio between loss function ???; ??, ??? and regularization parameter ||?||. Loss function?s 
definition is diverse for different basic learners, e.g. 
LR uses log?1 ? ????????? , while L2-SVM uses  
max??1 ? ??????, 0??. In the paper, we have the following assumptions: 
1) Target domain ???? ? ????? , ????|? ?
1,? , ?????, ??? ? ??? , ??? ? ??1,?1?, ???? 
is the size of ????; 
2) Source domain ???? ? ?????, ????|? ?1, ? , ?????, ??? ? ??? , ??? ? ??1,?1?, ???? is the size of ????; 3) ????? ? ?????; 
4) ???? and ???? are large enough; 
5) Testing set ????? and ???? are i.i.d.. 
Under maximum labeling budget ??, our goal is to employ source and target domain instances to 
maximize model accuracy: 
max? ????????? ? ? ?????????????????,????????? ,  (2) where the hypothesis is: 
????? ? ??1, ?
?? ? 0
?1, ??? ? 0.              (3) 
So, we design the machine learning framework, 
Active Vector Rotation, to optimize ?: 
min??? ||?|| ? ? c????; ??, ??????? ,         (4) where the weight variables c? ? 0 ,  control the importance of each instance in training. Larger c?  means more necessity of ?  to fit ???, ??? . Intuitively, ?  of ???  should try harder to fit the instances from ????  than the instances from ???? , 
so that the corresponding c?  of instances from ???? 
should be larger. The algorithm of AVR is 
described in Table 1, which is discussed in detail in 
the following subsections. 
Input: ????, ????, ?????, ??; Output: ?, ????????? 
1. Project ??  and ??  to a common latent semantic 
space, where ???, ??? ? ??. 
2. Actively select the least source domain instances, 
which can characterize source domain classifier 
???? , into training set ??? ? ??????, ?????|? ?
1,? , ????? ?. 3. Initialize ? using ???. 4. For ?? ? ????? ? 1 ? ????? ? ?? 1) Actively select the most informative 
instance ?????, ????? from ????. 
2) Insert the new labeled instance into 
training set, ??? ? ??? ? ?????, ?????. 
3) Update c? for ? ? 1: ??. 
4) Retrain ? using ??? and (4). end 
5. Compute ????????? . 
Table 1: AVR algorithm 
3.1 Projection of Source and Target Domain 
?? and ??  might be in different vector spaces. To 
employ ????  in the training of ???? ?s optimal ? , 
we?d better project ??  and ??  into a common n-
dimensional latent semantic space, where the 
distributions of the projected ???, ??? ? ?? would 
be more similar. Varied projection approaches 
could be employed in different tasks. For example, 
Hardoon et al (2004) used CCA to project text and 
14
image to a latent semantic space, where image 
could be retrieved by text. Blitzer et al (2007) and 
Ji et al (2011) utilized SCL and VMVPCA 
respectively in sentiment classification. Huang et 
al. (2006) applied RKHS and KMM in breast 
cancer prediction. 
Regarding the case where ??  and ??  are in the 
same vector space but certain approach is applied 
to make their distributions more similar, we also 
consider it as a kind of projection of ???? and ????. 
3.2 Initialization of Training set 
To reduce training cost and risk of negative 
transfer, AVR actively selects a relatively small set 
of instances from ????  into ??? . Transfer learning mainly leverages ???? ?s separating hyperplane information, i.e. ???? , while only a small set of critical instances from ????  can characterize the statistics of ???? . AVR initializes ???  by these critical instances. Different tasks may employ 
different selection strategy. E.g. in our experiments, 
the text classification task employs uncertainty 
sampling (Settles, 2009), while sentiment 
classification task selects the least ?????  instances which can accurately characterize ?????, such that: 
min????????? ? ????? ????
????????? .                 (5) 
3.3 Query Strategy in Target Domain 
After initialization of ???, AVR uses certain basic learner, such as LR and SVM, to get ? ? ?????? . As the labeling budget ??  is limited, we need iteratively query the most informative instance and 
add the new labeled instance into ??? to retrain ?. AVR revises the query strategy of traditional 
active learning. After a few new labeled instances 
added to ??? , the retrained ?  would be different from ?????? and closer to the optimum. Traditional active learning queries the instance in ???? w.r.t. ?, 
e.g. uncertainty sampling queries the instance 
closest to separating hyperplane, such that: 
min?????????? ??
??????.                    (6) 
However, AVR queries the most informative 
instance from which are identically classified by ?  
and ?????? , e.g. for uncertainty sampling, AVR queries the instance such that: 
min?????????,??????????????? ???????? ??
?????? .       (7) 
The instance queried by AVR makes ? more 
quickly approach to its optimum, as to some extent, 
part of the statistics of the instances which are 
differently classified by ? and ????? , can be characterized by the new queried instances. But 
when ? is very close to the optimum, AVR will 
query by traditional active learning strategy. 
3.4 Reweighting ?? 
Appropriate reweighting can help accelerate ? 
rotating to the optimum and avoid negative transfer. 
Intuitively, the instances from ????  and the 
instances which have similar distribution with ???? 
should be given higher weight. Varied reweighting 
strategy, e.g. TrAdaBoost (Dai et al, 2007), could 
be applied in AVR framework. In our experiments, 
AVR employs a simple but efficient reweighting 
strategy, without iteration: 
?? ? ??
1, ? ? ????? , ???????????? ???? ? 0
??0, ? ? ????? , ???????????? ???? ? 0??
?, ?????????.
    (8) 
4 Experiments 
We perform AVR on a set of toy data and two real 
world datasets, 20 Newsgroups Dataset 1  and 
Multi-Domain Sentiment Dataset 2 , comparing it 
with several baseline methods. In this paper, we 
use model accuracy ????????? under fixed labeling budget ?? as the evaluation. We used LR and L2-SVM as basic learner respectively, but due to 
space limit, we only report the results of LR. 
4.1 Toy Data 
We generate four bivariate Gaussian distributions 
as the positive and negative instances of ???? and ???? respectively as illustrated in Figure 1.  
 Figure 1: Distribution of toy data and AVR process 
                                                          
1 http://people.csail.mit.edu/jrennie/20Newsgroups/. 
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/. 
15
As shown in Figure 1, ????  and ????  randomly 
sample 1000 instances respectively, then ????? randomly samples 200 instances from ????. Circle 
and diamond, big plus and cross, small plus and 
cross, represent positive and negative instances of 
????, ???? and ????? respectively. 
To this toy data, AVR?s configuration is: 
1) ??? ? ??, ??? ? ??. 
2) AVR uses uncertainty sampling to select the 
least 5 instances which can characterize 
?????, to initialize ??? and ?????. In Figure 1, the 5 instances are marked by big filled 
circles or diamonds, the dash line draws the 
separating hyperplane ?????? ? ? 0. 3) Then AVR queries instances as described in 
Section 3.3, the first 10 queried instances are 
marked by large numerals, with the first 3 
are queried w.r.t. (7). The small numerals 
mark the first 3 instances which would be 
queried w.r.t. (6). 
4) AVR reweights ?? by (8), where ? ? 4. The black filled circles mark the instances whose 
corresponding c? ? 0. The solid line draws the current hyperplane ??? ? 0. 
Baseline methods are briefly described in Table 
2. Details about AcTraK and ALDA can be found 
in (Shi et al, 2008) and (Saha et al, 2011) 
respectively. 
Method Note 
Random 
 
Active 
Randomly sample instances from ????, 
without use of ???? Uncertainty sampling, without use of ???? AcTraK 
 
 
 
O-ALDA 
Initiated by one positive and one negative 
instances from ????, followed by uncertainty 
sampling from ???? 
Stream-based sampling, without instance 
reweighting 
B-ALDA 
 
 
Source-A 
Pool-based sampling, without instance 
reweighting 
Initialize ??? by ????, following uncertainty sampling without instance reweighting 
AVR-U 
 
 
AVR-W?
Uncertainty sampling with instance 
reweighting 
Give all instances from ???? the same weight, regardless prediction difference 
between ??and??????.?Table 2: Brief description of baseline methods 
 
The first 4 methods referring randomness are run 
1000 times to average results as shown in Table 3. 
Method Target Domain Labeling Budget ??  1 2 3 4 5 6 7 8 9 10 
Random 
Active 
50.05
49.90
69.35
75.65
79.88
90.41
86.04
95.92
90.26 
96.30 
93.01 
97.23 
94.41 
97.41 
95.30 
97.59 
96.03
97.64
96.41
97.72
AcTraK 
O-ALDA
93.15
77 
95.23
77 
96.10
77.01
96.69
77.07
97.03 
77.15 
97.30 
77.24 
97.53 
77.33 
97.68 
77.37 
97.78
77.42
97.82
77.48
B-ALDA
Source-A
AVR-U 
AVR-W 
77 
77 
80.50
80.50
77 
77 
95 
94 
77 
77 
85 
94.50
77 
77 
96 
97 
77 
77 
98.50 
98.50 
77 
77 
96 
97 
77 
77 
98 
98.50 
77.50 
77.50 
98 
97.50 
77.50
77.50
97 
98.50
77.50
77.50
96.50
97 
AVR 80.50 94 94.50 97 98.50 97 98.50 97.50 98.50 98.50
Table 3: Performance of different methods on toy 
data, where AcTraK unfairly uses two more labels. 
4.2 20 Newsgroups Dataset 
20 Newsgroups Dataset is commonly used in 
machine learning and NLP tasks. It contains about 
20000 newsgroup documents which are 
categorized into 6 top categories and 20 
subcategories. We split it into 6 pair of ????  and ???? , with each pair includes only two top 
categories documents, such as ?comp? and ?rec?, 
but ????  and ????  are drawn from different 
subcategories, e.g. ????  has ?comp.graphics? and ?comp.graphics?, but ????  has ?comp.windows.x? 
and ?sci.autos?. The task is to leverage ????  to distinguish the top categories of documents in ????. 
Our settings of 20 Newsgroups Dataset is identical 
with Dai et al (2007), details can be found there. 
On this dataset, AVR?s configuration is similar 
with that on toy data, with ?????  varies from 500 to 800 on different pairs. 
Due to space limit, we only report results on the 
pair of ?comp vs. rec? in Figure 2, with all 
methods are averaged over 30 runs. The results on 
other pairs are similar. Since AVR-U and AVR-W 
are variants of AVR, with similar performance, we 
only report the results of AVR.  
 Figure 2: AVR outperforms others on the ?comp 
vs. rec? pair. 
16
4.3 Multi-Domain Sentiment Dataset 
The sentiment dataset consists of user reviews 
about several products (Book, DVD, Electronic, 
Kitchen) from Amazon.com, the task is to classify 
a review?s sentiment label as positive or negative. 
We have 12 pairs with each pair has two products 
as ????  and ????  respectively. On this dataset, 
AVR employs VMVPCA (Ji et al, 2011) to project 
????  and ???? , and initializes ???  with ????? ?
1000  instances from ????  w.r.t. (5), while the other configuration is the same as that described in 
Section 4.1. To be comparable, the baseline 
methods which leverage ????  are preprocessed by VMVPCA. We also add another baseline method 
Source-A? here, which is identical with Source-A, 
except that it is not projected by VMVPCA. Given 
space limit, we only report the results on the pair 
?DVD?Kitchen?, with other pairs have similar 
performance. 
 
 Figure 3: AVR does better than previous work on 
the ?DVD?Kitchen? dataset for all budget sizes. 
4.4 Discussion 
From inspection of experimental results, we get the 
following remarks. 
Why to combine active learning and transfer 
learning? 
? Active learning such as uncertainty sampling 
can significantly reduce the labeling cost. But 
when ? is far from the optimum, uncertainty 
sampling may oversample instances near a 
direction. For example, in Figure 2, Active 
method is worse than Random method when 
??? ? 50. 
? ????  could help ????  in learning accurate ? , 
e.g. in Figure 2, when ??? ? 200, Source-A method with the help of ????  outperforms Random and Active methods which never use 
????. But inappropriate use of ???? may cause negative transfer, e.g. in Figure 2, when 
??? ? 200 , Source-A, ALDA and AcTraK methods, which overuse ???? , underperform Active method. 
? Thus, we realize that appropriate combination 
of transfer learning and active learning could 
advance and complement each other. 
Especially when ????  has scarce labels, ???? 
could help avoid oversample instances near a 
direction. But with the increase of labels in 
???? , ????  should decrease its weight in 
training to avoid negative transfer. 
Does each component of AVR work? 
? Appropriate Projection of ???? and ???? could 
mitigate distribution divergence, e.g. in our 
sentiment classification task, Source-A and 
AVR which applied VMVPCA significantly 
and consistently outperforms Source-A?. 
? Initialize ???  by a small set of critical instances from ????  can significantly reduce training cost without loss of accuracy. E.g. in 
our experiments, when ??? ? 1 , AVR has better or comparable performance w.r.t. 
Source-A which initializes ??? by whole ????. More importantly, AVR trims initial ???  size from 1000 to 5 in toy data, from 4000 to 500 
in Newsgroups dataset, and from 2000 to 
1000 in Sentiment dataset. 
? The query strategy of AVR described in 
Section 3.3 advances traditional active 
learning, which is supported by the 
performance of AVR over AVR-U. 
? Appropriately reweighting instances from 
????  and ????  could result in accurate ?  and 
avoid negative transfer meanwhile. For 
example, in our experiments, the reweighting 
strategy of (8) makes AVR outperform all 
baseline methods, while some of which suffer 
from negative transfer. 
How about AcTraK?s performance? 
? AcTraK works well on our toy data, just 
because it unfairly uses too much more labels 
of ???? , even though, it underperforms AVR 
when ? ??? ? 3 . Besides, AcTraK performs poorly on high dimensional data like text in 
our experiments. 
17
5 Conclusion and Future Work 
Our proposed machine learning framework AVR 
actively and carefully leverages information of 
source domain to query the most informative 
instances in target domain, as well as to train the 
best possible model of target domain. The four 
essential components of AVR, which establish its 
efficacy and help it avoid negative transfer, are 
validated in experiments.  
In the future, we are planning to apply AVR in 
more tasks with appropriate specification of 
projection, query and reweighting strategy. 
Especially for sentiment classification, we will 
combine prior domain knowledge, such as domain 
sentiment lexicon, with AVR framework to further 
reduce labeling cost. 
Acknowledgements 
This work is supported by the National 
Fundamental Research Program of China 
(2010CB327903) and the Doctoral Fund of 
Ministry of Education of China (20110091110003). 
We also thank Shujian Huang, Ning Xi, Yinggong 
Zhao, and anonymous reviewers for their greatly 
helpful comments. 
References 
John Biltzer, Ryan Mcdonald, Fernando Pereira. 2006. 
Domain adaptation with structural correspondence 
learning. In Proc.EMNLP, pp.120-128. 
John Biltzer, Mark Dredze, Fernando Pereira. 2007. 
Biographies, bollywood, boom-boxes and blenders: 
domain adaptation for sentiment classification. In 
Proc. ACL, pp.432-439. 
Rita Chattopadhyay, Jieping Ye, Sethuraman 
Panchanathan, Wei Fan, Ian Davidson. 2011. Multi-
source domain adaptation and its application to early 
detection of fatigue. In Proc. KDD, pp.717-725. 
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu. 
2007. Boosting for transfer learning. In Proc. ICML, 
pp.93-200. 
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, Chih-Jen Lin. 2008. Liblinear: a library 
for large linear classification. JMLR, 9:1871-1874. 
Valerii? Vadimovich Fedorov. 1972. Theory of optimal 
experiments. Academic Press. 
David R. Hardoon, Sandor Szedmak, John Shaew-
Taylor.  2004. Canonical correlation analysis: An 
overview with application to learning methods. 
Neural Computation, 16(12): 2639-2664. 
Jiayuan Huang, Alexander J. Smola, Arthur Gretton, 
Karsten M. Borgwardt, Bernhard Scho? lkpf. 2006. 
Correcting sample selection bias by unlabeled data. 
In Proc. NIPS, pp.601-608. 
Yangsheng Ji, Jiajun Chen, Gang Niu, Lin Shang, 
Xinyu Dai. 2011. Transfer learning via multi-view 
principal component analysis. JCST, 26(1):81-98. 
Jing Jiang, ChengXiang Zhai. 2007. Instance weighting 
for domain adaptation in NLP. In proc. ACL, pp.264-
271. 
Xuejun Liao, Ya Xue, Lawrence Cain. 2005. Logistic 
regression with an auxiliary data source. In Proc. 
ICML,  pp.505-512. 
Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi. 2008. 
Trust region newton method for large-scale logistic 
regression. JMLR, 9:627-650. 
David J. C. Mackay. 1992. Information-based objective 
functions for active data selection. Neural 
Computation, 5:590-604. 
Michael T. Rosenstein, Zvika Marx, Leslie Pack 
Kaelbling, Thomas G. Dietterich. 2005. To transfer 
or not to transfer. In Proc. NIPS, December 9-10. 
Avishek Saha, Piyush Rai, Hal Daum e?  III, Suresh 
Venkatasubramanian, Scott L. DuVall. 2011. Active 
supervised domain adaptation. In Proc. ECML-
PKDD, pp.97-112. 
Burr Settles. 2009. Active learning Literature Survey. In 
Computer Sciences Technology Report 1648, 
University of Wisconsin-Madison. 
Xiaoxiao Shi, Wei Fan, Jiangtao Ren. 2008. Actively 
transfer domain knowledge. In Proc. ECML-PKDD 
pp.342-357. 
Masashi Sugiyama, Shinichi Nakajima, Hisashi 
Kashima, Paul von B??nau, Motoaki Kawanabe. 2008. 
Direct importance estimation with model selection 
and its application to covariate shift adaptation. 
NIPS, pp.1433-1440. 
18
