Automat ic  Senmnt ic  Class i f icat ion for Ch inese Unknown Compound Nouns  
Keh-Jiann Chert & Chao-jan Chen 
Institute of Information Science, Acadeinia Sinica, Taipei 
Abstract 
Tim paper describes a similarity-based model to 
present he morphological rules for Chinese com- 
pound nouns. This representation model serves 
functions of 1) as the morphological rules of the 
compounds, 2) as a mean to evaluate the proper- 
ness of a compound construction, and 3) as a mean 
to disambiguate the semantic ambiguity of the 
nlorphological head of a compound noun. An 
automatic semantic lassil'ication system for Chine- 
se unknown compounds i thus implemented based 
on the model. Experiments and on'or analyses arc 
also presented. 
1. Introduction 
The occurrences of unknown words cause difficul- 
ties in natural language processing. Tile word set of 
a natural anguage is open-ended. There is no way 
of collecting every words of a language, since new 
words will be created for expressing new concepts, 
new inventions. Therefore how to identify new 
words in a text will bc tile most challenging task for 
natural language processing. It is especially true for 
Chinese. Each Chinese morpheme (usually a single 
character) carries meanings and most are polysc- 
incus. New words are easily constructed by com- 
bining lnorphelnes and their meanings are tile se- 
mantic composition of morpheme components. 
Of course there are exceptions of semantically non- 
compositional compounds. In Chinese text, there is 
no blank to mark word boundaries and no inlqec- 
tional markers nor capitalization markers to denote 
the syntactic or selnantic types of new words. 
Hence the unknown word identification for Chinese 
became one of the most difficult and demanding 
research topic. 
The syntactic and semantic categories of 
unknown words in principle can be determined by 
their content and contextual information. However 
many difficult problems have to be solved. First of 
all it is not possible to find a uniforln representa- 
tional schema nd categorization algorithm to han- 
dle different ypes of unknown words, since each 
type of unknown words has very much differeut 
morpho-syntactic structures. Second, the clues for 
identifying different ype of unknown words are 
also different. For instance, identification of 
names of Chinese is very much relied on the 
surnames, which is a limited set of characters. 
The statistical methods are commonly used for 
identifying proper names (Chang et al 1994, 
Sun et al 1994). The identification of general 
compounds is more relied on the morphemes 
and tile semantic relations between morphemes. 
There are co-occurrence restrictions between 
morphemes of compounds, but their relations are 
irregular and mostly due to common sense 
knowledge. The third difficulty is the problems 
of ambiguities, such as structure ambiguities, 
syntactic alnbiguitics and semantic ambiguities. 
For instances, usually a morpheme charac- 
tedword has multiple lneaning and syntactic 
categories. Therefore the ambiguity resolution 
became one of the major tasks. 
Compound nouus are ttle most frequently 
occurred unknown words in Chinese text. 
According to an inspection on tile Sinica corpus 
(Chen etc. 1996), 3.51% of lhe word tokens in 
the corpus are unknown, i.e. they are not listed 
in the CKIP lexicon, which contains about 
80,000 entries. Alnong them, about 51% of the 
word types are compound nouns, 34% are 
compound verbs and 15% are proper names. In 
this paper we locus our attention on the 
identification of the compound nouns. We 
propose a representation model, which will be 
facilitated to identify, to disambiguate and to 
evaluate the structure of a compound noun. In 
fact this model can be extended to handle 
compound verbs also. 
1.1 General properties of compounds and 
their identification strategy 
The semantic ategory and syntactic category 
are closely related. For coarse-grained analysis, 
syntactic ategorization and semantic ategori- 
zation are close related. For instances, nouns 
denote entities; active verbs denote events and 
stative verbs denote states. For fine-grained 
analysis, syntactic and semantic lassifications 
take difl'erent classification criterion, in our 
model the coarse-grained analysis is processed 
first. The syntactic categories of an unknown 
173 
word are predicted first and the possible semantic 
categories will be identified according to its top 
ranked syntactic categories. Different syntactic 
categories require different representational models 
and different fine-grained semantic lassification 
methods. 
The presupposition of automatic se- 
mantic classification for compounds i that the 
meaning of a compound is the semantic om- 
position of its morphemic omponents and the 
head morpheme determines the major semantic 
class of this compound. There are many poly- 
syllabic words of which the property of se- 
mantic composition does not hold, for in- 
stances the transliteration words, those words 
should be listed in the lexicon. Since for the 
majority of compounds the presupposition hold, 
the design of our semantic lassification algo- 
rithm will be based upon this presupposition. 
Therefore the process of identifying semantic 
class of a compound boils down to find and to 
determine the semantic lass of its head mor- 
phen-~e. However ambiguous morphological 
structures cause the difficulties in finding head 
morpheme. For instances, the compound in la) 
has two possible morphological structures, but 
only lb) is the right interpretation. 
1 a) ~2\]..l..'American' 
b) ~ /~.'Amcrica' 'people', 
c) ~ \["~l)\ 'beautiful' 'country-man' 
Once the morphological head is deterlnined, the 
semantic resolution for the head morpheme is the 
next difficulty to be solved. About 51.5% of the 
200 most productive morphemes are polysemous 
and according to the Collocation Dictionary of 
Noun and Measure Words (CDNM), in average 
each ambiguous morpheme carries 3.5 different 
senses (Huang et al 1997). 
2. Representation Models 
Compounds are very productive types of unknown 
words. Nominal and verbal compounds are easily 
coined by combiniug two/many words/characters. 
Since there are more than 5000 commonly used 
Chinese characters and each with idiosyncratic 
syntactic behaviors, it is hm,t to derive a set of 
morphological rules to generate the set of 
Chinese colnpounds without over-generation r 
under-generation. The set of general compounds 
is an open-class. The strategy for automatic 
identification will be relied not only on the 
morpho-syntactic structures but also morpho- 
semantic relations. In general, certain 
interpretable semantic relationships between 
morphemic must be held. However there is no 
absolute lneans to judge whether the semantic 
relations between morphemic omponents are 
acceptable, i,e. the acceptability of such type of 
compounds is not simply 'yes' or 'no'. The 
degree of properness of a compound should 
depend on the logical relation between 
morphemic omponents and their logicalness 
should be judged by cominou sense knowledge. 
It is ahnost ilnpossible to in\]plement a system 
with common sense knowledge. Chen & Chen 
(1998) proposed an example-based measurement 
to evaluate the properness of a newly coined 
compound instead. They postulate that for a 
newly coined compound, if the semantic relation 
of its morphemic omponents i  similar to the 
existing compounds, then it is inore likely that 
this newly coined compound is proper. 
2.1 Example-based similarity nleasure 
Supposed that a compound has the structure of 
XY where X and Y are morphemes and sup- 
posed without loss of generality Y is the head. 
For instance, ~ i<~-  'learn-word-machine' is a
noun compound and the head morphemeY is 
~'machine'  and the modifier X is ~ ' :  'learn- 
word'. In fact the morpheme f~{~ has four differ- 
ent meanings. They are 'machine', 'airplane', 
'secret' and 'opportunity'. How do computers 
judge which one is the right meaning and how is 
the compound construction well-formed or logi- 
cally lneaningful? First of all, the exalnples with 
the head morpheme ~ are extracted from cor- 
pora and dictionaries. The examples are classi- 
fied according to their meaning as shown in the 
Table 1. 
Senses semantic category examples 
( l )  machine <~> Bo0ll 
(2) a i rp lane <~{~> Bo223 
(3) opportunity <{N~> Ca042 
(4) secret <~> Da011 
Table 1. Four senses of tile morpheme ')~' and their respective samples 
174 
Tile meaning of ~l-\]i:i':I~,~- is then determined by 
comparing the similarity between it and each class 
of exalllples. Tile nleauing of the input ul\]kuown 
word will be assigned with the moaning of tile class 
with the most simihu morpho-semantic structures 
with this unknown word. Tile similarity measure is 
based on tile following formula. 
Supl)osed that each class of examples forlllS lhe 
following SOlUalltic relatioll rules. The rules silow 
the possible semantic relations between prel'ix and 
suffix Y and their weight in term ol' ihe frequency 
distribution of each semantic category of the pro- 
fixes in tile class. 
Rules: Semi +Y Freql 
Sere2 + Y Freq2 
: 
Semk + Y Freqk 
( freqi: tile number of the words of the l'orm Semi + 
Y)  
Take sulTix ~- with moaning of 'machine' as 
oxaulple. Igor tile nlorphonle I{} 'ulachine', tile 
extracted COlllpotlllds of the fornl X+~j~-'machine' 
and tile semantic categories of the n3odifiors al'e 
shown il1 Table 2 and the n3orpl~ological rule de- 
rived froill them ix in Table 3. The scnlai/tic types 
alld their hierarchical structure are adopted fro111 
tile Chilin (Moi el al. 1984). The similarity is 
measured between the semantic class of tile prefix 
X o1' tile unknown conlpound and tile prefix se- 
mantic types shown in the rule. ()no ot' the meas- 
uroulonts proposed is: 
SIMILAR (Sere ,Rule)  = {} ; i l , k  lnforl l lati()n- 
Load(Sem(hSemi) * lq-eqi } / Max-vahle 
Where Sere is tile semantic class of X. Max-value 
is tile maxinlal vahle o1' {~\] \[nfornlation Load(S 
~Selni)  * Freqi } for all semantic lasses S. The 
iriax-wllue normalizes tile SIMILAR value to 
0-1. S(hSemi denotes the least common ances- 
tor of S and Semi. For instance, (Hh03(' lHb06) 
= H. Tile Information-Load(S) of a senmntic 
class S ix defined as Entropy(sonlantic sys- 
tem) - Entropy(S). Simply speaking it is the 
anlount of reduced entropy after S is seen. En- 
tropy(S) =~\]i=l,k -p(SemilS) * Log P(SemiJs), 
where {Semi, Sem2 ..... Semk} is lhe set of the 
bottoln level selnantic lasses contained in S. 
J r ,  5 % t,n ~,~-(f~) AoI73 llh031 ,(7J~#~(t~) Bo171 
,~;'.te(7"(,l~) AoI73 I11~032 SII;~(}~)I f083 Ih063 
I~JTl{(\]~) 11c013 11e032 ll~(L, tt~(4~) Ili141 tlj345 
LI~7':~ @~) I 1c()32 (7 P J'-?! ('~{~)Fa221 
I~1 ~)j(4~) Eb342 4~Jl~(4~) Iic071 
'i:\[{'~)2@~) Eb342 /JJ(JC(lJ~) Ih031 
~1~ ,,,:(~) AoI62 117162 ?,3~(4,~) P>e(>51 
gg~'l.;@}) I)k162 1fg191 ~J~!',lZ({~J~) 1h042 
5',J{(,).@{) Be041 ,~':~\[;41@~) 117192 
. - -  I l l  I u ". { :  ILT-)I':(IN) Ca039 Ca041 ~.~pl) (4~) FhOI2 
Os~!l\]Ja(t~) 11c122 IJl:\];';::(45~) 1\[c231 
~t&i I(q~b \]~,12i 4,,i~1;~(.4',2~) Fa212 .hl102 
Table 2. The senlanlic categories ol' modifiers of 
tile COlllpouuds el' X-"f~ ~ math inc" 
Take lhe word l~'~ i :J': I~ 'learning-word- 
inachine' as example. In tile table 3, the results 
show tile estiinated similarity between tile 
X-~J,~ Serui freqi 
Hh031 
Ao 173 
He032 
Eb342 
Ae 162 
Hgl91 
3 
I 
2 
2 
I 
1 
Sem(~.~i!j ":)= Ilg111 
lnl'orrnation-Load( Hgl I 1 f-I Hh031 )=hlfornlation-Load(H)=2.231 
lnfornlation-Load( Hgl 11 N I1e032 )=hlformation-Load(Ii)=2.231 
Inforuultion-l~oad( Itg I 11 I'3 fig 191 )=lnformation-Load(Hg)=5.91 
? i= l,k hfformation-Load(Hgl I 1 f-I Semi) * Freqi 
=2.231"3+2.231"2+5.912"  1 + ...... 
= 104.632 
Max-Vahlel = Z i hffornlation-Load(Hg031 f-I Semi) * Freqi 
= 155.164 
S1MILAP,= (104.632 / 155.164) = 0.6743 
Table 3. The derived morphological rule for tile ulorphenle 'machine' and tile simihu'ity measure of ,~.~l~j?: 
t~"  aS ;_i I1OUn conlpound which denotes a kind of nlachille. 
175 
compound ~-~ and the extracted examples. The 
similarity value is also considered as the logical 
properness value of this compound. In this case is 
0.67, which can be interpreted as that we have 67% 
of confidence to say that ~z~ 'learning-word- 
machine' is a well-formed compound. 
The above representation model serves many func- 
tions. First of all it serves as the morl3hological 
rules of the colnpounds. Second it serves as a mean 
to implement the evaluation function. Third it 
serves as a mean to disambiguate he semantic am- 
biguity of the morphological head of a compound 
noun. For instance, them are four different @. 
Each denotes 'machine', 'airplane', 'opportunity' 
and 'secret' and they are considered as four differ- 
ent morphemes. The example shows that '~  
~}~'denotes a machine not other senses, since the 
evaluation score for matching the rules of '~-'ma- 
chine' has the highest evaluation score among 
theln. 
The above discussion shows the basic concept 
and the base-line model of the example-based 
model. The above similarity measure is called 
over-all-similarity measure, since it takes the equal 
weight on the similarity values of the input com- 
pound with every member in the class. Another 
similarity measum is called maximal-similarity, 
which is defined as follows. It takes the maximal 
value of the similarity between input compound 
and every member in the class as the output. 
SlM2(Word,Rule) = Maxi=l,k{ (Information 
Load(Sem~Semi)) / Max-value2 } 
Both similarity measures are reasonable and have 
their own advantages. The experiment results 
showed that the combination of these two measures 
achieved the best performance on the weights of 
w 1=0.3 and w2=0.7 (Chen & Chen 1998), i.e. SIM 
= SIMI * wl + SIM2 * w2, where wl+w2 = 1. We 
adopt his measure in our experiments. 
It also showed a strong co-relation between the 
similarity scores and the human evaluation scores 
on the properness of testing compounds. The hu- 
man considemd bad compounds howed also low 
similarity scores by computers. 
3. System Implementation 
3.1 Knowledge sources 
To categorize unknown words, the computer sys- 
tem has to equip with the linguistic and semantic 
knowledge about words, morphemes, and word 
formation rules. The knowledge is facilitated to 
identify words, to categorize their semantic and 
syntactic lasses, and to evaluate the properness of 
word formation and the confidence level of 
categorization. In our experiments, the available 
knowledge sources include: 
1) CKIP lexicon: an 80,000 entry Chinese lexi- 
con with syntactic categories for each entry 
(CKIP 1993). 
2) Chilin: a thesaurus of synonym classes, which 
contains about 70,000 words distributed un- 
der 1428 semantic lasses (Mei 1984). 
3) Sinica Corpus: a 5 million word balanced 
Chinese corpus with word segmented and 
part-of-speech tagged (Chen 1996). 
4) the Collocation Dictionary of Noun and 
Measure Words (CDNM) : The CDNM lists 
collocating measure words for nouns. The 
nouns in this dictionary are arranged by their 
ending morpheme, i.e. head morpheme. There 
are 1910 noun ending morphemes and 12,352 
example nouns grouped according to their 
different senses. 
Each knowledge source provides partial data for 
representing morphological rules, which in- 
clndes lists of sample compounds, high frequen- 
cy morphemes and their syntactic and semantic 
information. Unknown words and their frequen- 
cies can be extracted from the Sinica corpus. 
The extracted unknown words produce the 
testing data and the morpheme-category asso- 
ciation-strength which are used in the algorithm 
for the syntactic category prediction for un- 
known words (Chen et al 1997). The CKIP dic- 
tionary provides the syntactic categories for 
morphemes and words. The Chilin provides the 
semantic categories for morpheme and words. 
The CDNM provides the set of high frequency 
noun morphemes and the example compounds 
grouped according to each difference sense. The 
semantic categories for each sense is extracted 
from the Chilin and disambiguated manually. 
The sample compounds for each sense- 
difl'emntiated morpheme xtracted from CDNM 
form the base samples for the morphological 
rules. Additional samples are supplemented from 
the Chilin. 
3.2 Tile algorithm for morphological 
analysis 
The process of morphological analysis for com- 
pound words is very similar to Chinese word 
segmentation process. It requires dictionary 
look-up for matching nlorphemes and resolution 
methods for the inherent ambiguous egmenta- 
176 
tions, such as the exalnples in 1). However con- 
ventional word segmentation algorithms cannot 
apply for the morphological analysis without modi- 
fication, since the nlorpho-syntactic behavior is 
different froth syntactic behavior. Since ihc struc- 
ture of the Chinese COlllpound nOtlllS is head final 
and the most productive morphemes arc monosyl- 
labic, there is a simple and effective algorithm, 
which agrees with these facts. This algorithm seg- 
ments input compounds flom left to right by the 
longest matching criterion (Chcn& Liu 1992). It is 
clear that the loft to right longest lllaiching algo- 
rithm prel'ers shorler head and longer modifier 
structtlres. 
3.3 Senlant ic  categories of morphemes  
The semantic categories of morphemes arc lot- 
lowed from the thesaurus Chilin. This thesaurus is 
a lattice structure of concept taxonomy. Mor- 
phemes/words may have multiple classification due 
to either ambiguous classification or inherent so- 
illantic mnbiguitios. For lhe ambiguous scn'lantic 
categories o1' a morl)hcmo, lhc lower ranking se- 
nmntic categories will be eliminated and leave the 
higher-ranking scnlantic categories to conlpotc 
during the identification process. For instances, in 
the table 2 only the re;tier categories of each exam- 
ple are shown. Since the majority of nlorphemcs 
are unanlbiguous, they will compensate the uncer- 
tainty caused by die semantically ambiguous roof 
phemes. The rank of a semantic category of a mot'- 
phonic depends on the Occurrillg order o1: lhis lilO1- 
plionlo in ils synonyln group, since lhc arrangcincnt 
of the Chilin cilirics is by this natural, hi addition, 
dtlo to limit coverage o1: Chilin, nlally of the ll\]Ol'- 
phemes arc not listed. For the unlisted morphemes, 
we recursivcly apply the currellt algorithm to pre- 
dict their semantic categories. 
4. Semantic Chlssification and Ambiguity 
Resolut ion for  Compound Nouns 
The demand o1" a semantic hlssification system for 
COlllpound nouns was first raised while the task of 
selnantic tagging for Chinese corpus was lriod. The 
Sin|ca corpus is a 5 in|Ilion-word Chinese corpus 
with part-of speech lagging, lit lhis corpus there are 
47,777 word typos tagged with conllllOn nOl.lllS and 
Ollly 12,536 Of tholll are listed ill the Chilin. They 
count only 26.23%. In oilier words the scmandc 
categories for most of the common nouns arc tin- 
known. They will be the target for automatic se- 
mantic classification. 
4.1 Derivation of morphological rules 
A list of' most productive lriorphoinos arc first 
generated from the unknown words extracted 
fl'om the Sinica corpus. The morphological rules 
o1' the sot of the lllOSl productive head mor- 
phonies {llO derived flonl their examples. Both 
the CI)MN all(\] Chilin provide SOlilO oxanlplcs. 
So lhr there are 1910 head morphemes for com- 
pound nouns with examples in the system and 
increasing. They are all monosyllabic mor- 
phemes. For the top 200 most productive mor- 
phenlcs, among them 51.5% are polysemous and 
in average each has 3.5 different meanings. \]'tie 
coverage of ihe ctlrrollt 1910 illorphonlos is 
aboul 71% of ihc uilkiiown noun conlpounds of 
the iesling dala. The rosl 29% tincovorod noun 
nlorphonlos are cilher polysyllabic i-llorpholiies 
or/lie low frequency nlorl~hemes. 
4.2 Semantic classification algorithm 
The unknown compound nouns extracted from 
the Sinica corpus w'cre classified according to 
Ihc morphological representation by the simihtr- 
ity-bascd algoriltnn. The problcms of semantic 
ambiguitics and out-of-coverage morphcmcs 
were two major dilTicultics to be solved during 
the classification stage. The complete scmanlic 
classification algorilhm is as follows: 
I) For each inpu! noun compound, apply mor- 
phological analysis algorilhm lo derive die 
morphemic components of the input com- 
pound. 
2) I)clcrminc the head nlorphenlc and modifiers. 
'flit: dcfaull head illorphclllo is lhc last liior- 
phonic of a conlpound. 
3) Got die synlactic and semantic categories of 
the modifiers. If a modil\]or is also an tin- 
known word, lhen apply this algorilhm rocur- 
sively to idendfy its son-ialltic category. 
4) For lhe head morpholne with the representa- 
tional rules, apply siinilarity illeastlro for each 
possible sornantic chtss and outptlt the so- 
manlic class with lhe highest siinilariiy wthic. 
5) If the head illorphonlo is not covered by tile 
nlorphological rules, search its semantic lass 
from the Chilin. If its semantic lass is not list 
in the Chilin, then no ariswcr can be found, if 
it is polysemous, then the top ranked classes 
will be the output. 
In lhc step I, thc algorithm rcsolvcs the possible 
ambiguities o1' the morphological slrtlcttlrcs of 
the input COlllpound. In the step 3, the selllantic 
categories of the modil'ier arc determined. There 
arc some complications. The firsl complication 
is lhat lhe modifier has nmltiple semantic are- 
177 
gories. In our current process, tile categories of 
lower ranking order will be eliminated. The re- 
maining categories will be processed independently. 
One of the semantic categories of the modifier 
pairing with one of the rule of the head morpheme 
with the category will achieve the maximal simi- 
larity value. The step 4 thus achieves the resolution 
of both semantic ambiguities of the head and tile 
modifier. However only the category of the head is 
our target of resolution. The second complication is
that the modifier is also unknown. If it is a not list- 
ed in the Chilin, there is no way of knowing its 
semantic categories by tile era'rent available re- 
sources. At the step 4, the prediction of semantic 
category of the input compound will depend solely 
on the information about its head morpheme. If the 
head morpheme is unambiguous then output the 
category of the head morpheme as the prediction. 
Otherwise, output he semantic ategory of the top 
rank sense of the head morpheme. The step 5 han- 
dles the cases of exceptions, i.e. no representational 
rule for head morphemes. 
4.3 Experimental  results 
The system classifes the set of unknown common 
nouns extracted from tile Sinica corpus. We ran- 
domly picked two hundred samples from tile output 
for the performance valuation by examining the 
semantic classification manually. The correction 
rate for semantic lassil'ication is 84% and 81% 
for tile frst hundred samples and the second 
hundred samples respectively. We further classi- 
fy tim errors into different ypes. The first type is 
caused by the selection error while disam- 
biguating the polysemous head lnorphemes. The 
second type is caused by the fact that the mean- 
ings of some compounds are not semantic om- 
position of tile meanings of their morphological 
components. Tile third type errors are caused by 
the fact that a few compounds are conjunctive 
structures not assumed head-modifier structure 
by the system. Tile forth type errors are caused 
by the head-initial constructions. Other than tile 
classification errors, there exist 10 unidentifiable 
colnpounds, 4 and 6 in each set, for their head 
morphemes are not listed in tile system nor in 
the Chilin. Among tile 190 identifiable head 
morphemes, 142 of them are covered by the 
morphological rules encoded in the system and 
80 of theln have multiple semantic categories. 
Tile semantic categories of remaining 48 head 
morphemes were found fiom the Chilin. If the 
type 1 selection errors are all caused by the 80 
morphemes with multiple semantic categories, 
then the correction rate of semantic disambigua- 
tion by our similarity-based measure is (80- 
15)/80 = 81%. 
Testing data 1 Testing data 2 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Total : 100 
error: 12 
type l(semantic selection error): 8 
type2(non-compositional): 2 
type3(conjunction): I 
typre4(head-initial): 1 
unidentified: 4
Total: 100 
error: 13 
type l(semantic selection error): 7 
type3(non-compositional): 5 
type3(con.junction): 0 
typre4(head-initial): 1 
unidentified: 6
Table 5. The performance valuations of 
5. Further Remarks and Conclusions 
In general if an unknown word was extracted from 
corpora, both of its syntactic and semantic atego- 
ries are not known. The syntactic ategories will 
be predicted first according to its prefix-category 
and suffix-category associations as mentioned in 
(Chen et al 1997). According to the top ranked 
syntactic predictions, each respective semantic 
representational rules or models will be applied to 
produce the morpho-semantic plausibility of the 
unknown word of its respective syntactic atego- 
rization. For instance if the predicted syntactic 
the semantic lassification algorithm 
categories are either a common noun or a verb, the 
algorithm present in this paper will be carried out 
to classify its semantic ategory and produce its 
plausibility value for the noun category. Similar 
process should deal with tile case of verbs and 
produce tile plausibility of being a verb. The final 
syntactic and semantic prediction will be based oil 
their plausibility values and its contextual envi- 
ronments (Bai et al 1998, Ide 1998). 
The advantages of tile current representational 
model are: 
1) it is declarative. New examples and new mor- 
178 
phemes can be added into the system withoul 
changing the processing algorilhm, but 111e per- 
formance o1' the system might be increased ue 
to the increlnent of the knowledge. 
2) The representational model not only provides 
the semantic classification of the unknown 
words but also gives the wdue of the phmsibil- 
ity o1' a compound construction. This value 
could be utilized to resolve the alnbiguous 
matching between compeling compound rules. 
3) The representational model can be extended for 
presenting compound verbs. 
4) It acts as one of the major building block of a 
self-learning systeln for linguistic and world 
knowledge acquisition on the lnternel environ- 
l l lel l t .  
Tile classification errors are caused by a) some of 
the testing examples have no semantic omposi- 
tion property, b) some semantic lassifications are 
too much fine-grained. There is no clear cut dif- 
ference between some classes, even Imman judge 
cannot lnake a right classification, c) there are not 
enough samples that causes the simihuity-based 
model does not work on the suffixes with few or 
no sample data. The above classification errors 
can be resolved by collecting the new words, 
which are Selnantically nol>compositional, into 
tile lexicon and by adding new examples for each 
naorphenle. 
Current Selnantic ategorization system only 
roughly classifies the unknown compound nouns 
according to their semantic heads. In the future 
deeper analysis on the semantic relations between 
modifier and head should also be carried otll. 
6. Rel'erences 
Bai, M.H., C.J. Chert & K..I. Chert, 1998, "POS- 
lagging for Chinese Unknown Words by 
Contextual P, ules" Proceedings of 
ROCLING, pp.47-62. 
Chang, .1. S.,S.D. Chert, S. J. Ker, Y. Chert, & J. 
Liu,1994 "A Multiple-Corpus Approach to 
Recognition of Proper Nmnes in Chinese 
Texts", Computer Processing o/" Chinese 
and Oriental Languages, Vo\[. 8, No. 1, pp. 
75-85. 
Chert, C. J., M. It. Bai, K. J. Chert, 1997, "Catego- 
ry Guessing for Chinese Unknown Words." 
Proceedings of the Natural Langttage 
Processing Pac(fi'c Rim Symposimn 1997, 
pp. 35-40. NLPRS '97 Thailand. 
Chert, K. J., C.J. Chert, 1998, "A Corpus-based 
Study on Computational Morphology for 
Mandarin Chinese", in Qlmnlitative and 
Comlmtationa\[ Studies on the Chinese 
lxmgttage Eds by l?,cnjanfin K. Tsou, City 
Univ. of Hong Kong, pp283-306. 
Chert, l<eh-.liann, Ming-Hong Bai, 1997, "Un- 
known Wolzl l)etection for Chinese by a 
Corpus-based Learning Method." Pro- 
ceedings of the lOth Research on Comlm- 
lalional Linguistics International Confer- 
ence, pp 159-174. 
Chert, K.J. & S.II. Liu, 1992,"Word ktentification 
for Mandarin Chinese Sentences," lbv- 
ceedings o.f 14th Colili,g, pp. 101 - 107. 
Chien, Lee-feng, 1999," lWF-tree-based Adaptive 
Keyphrase Extraction for Intelligent Chine- 
se Information Retrieval," Information 
Processing and Management, Vol. 35, pp. 
501-521. 
l;ung P., 1998," Extracting Key Terms from Chi- 
nese and Japanese Texts," Computer Proc- 
essing of Oriental Languages, Vol. 12, #1, 
pp 99-122. 
Huang, C. R. E1 al.,1995,"The lnlroduction of 
Sinica Corpus," Proceedings of ROCIJNG 
VIII, pp. 81-89. 
lhtang, Chu-P, en, Keh-Jiann Chert, and Ching- 
hsiung Lai, 1997, Mandarin 1)ally 
Classification l)ictionary, Taipci: Mandarin 
l)aily Press. 
Ide, Nancy & Jean Veronis, 1998, " Special Issue 
on Word Sense l)isambiguation", 
Computational Linguistics, Vol. 24, # I. 
Lee, J.C. , Y.S. Lee and H.H. Chert, 1994, 
"Identification of Personal Names in 
Chinese Texts." Proceedings of 7th ROC 
Computational Linguistics Conference. 
Lin, M. Y., T. H. Chiang, & K.Y. Su, t993," A 
Preliminary Study on Unknown Word 
Problem in Chinese Word Segmentation" 
Proceedings of Reeling V1, pp 119-137. 
Mei, Gia-Chu etc., 1984Iq * - -  4~ ~q ~q g.(Chil in - 
thesaurus of Chinese words). Hong Kong, 
McDonald 1)., 1996, " Internal and External 
Evidence in the Identification and Semantic 
Categorization of Proper Names", in 
Corpus Processing Jot Lexical Acquisition, 
J. Pustejovsky and B. Boguraev Eds, MIT 
Press 1996. 
Sun, M. S., C.N. Huang, H.Y. Gao, & Jie Fang, 
1994, "Identifying Chinese Names in Unre- 
stricted Texts", Communication of COLIPS, 
Vol.4 No. 2. 113-122. 
179 
Knowledge Extraction for Identification of Chinese Organization Names 
Keh-Jiann Chen & Chao-jan Chen 
kchen@iis.siniea.edu.tw fichard@iis.siniea.edu.tw 
Institute of  Information Science, Academia Siniea, Taipei 
ABSTRACT 
In this paper, a knowledge extraction process 
was proposed to extract the knowledge for 
identifying Chinese organization ames. The 
knowledge extraction process utilizes the 
structure property, statistical property as well as 
partial inguistic knowledge of the organization 
names to extract new organizations from domain 
texts. The knowledge xtraction processes were 
experimented on large amount of texts retrieved 
from WWW. With high standard of threshold 
values, new organization names can be 
identified with very high precision. Therefore 
the knowledge extraction processes can be 
carried out automatically to self improve the 
performance in the future. 
1. INTRODUCTION 
The occurrences of unknown words cause 
difficulties in natural language processing. The 
word set of a natural language is open-ended. 
There is no way of collecting every words of a 
language, since new words will be created for 
expressing new concepts, new inventions, 
newborn babies, new organizations. Therefore 
how to identify new words in a text will be the 
most challenging task for natural language 
processing. It is especially true for Chinese. 
Each Chinese morpheme (usually a single 
character) carries meanings and most arc 
polyscmous. New words are easily constructed 
by combining morphemes and their meanings 
are the semantic composition of morpheme 
components. However there are also 
semantically non-compositional compounds, 
such as proper names. In Chinese text, there is 
no blank to mark word boundaries and no 
inflectional markers nor capitalization markers 
to denote the syntactic or semantic types of new 
words. Hence the unknown word identification 
for Chinese became one of the most difficult and 
demanding research topic. 
There are many different types of 
unknown words and each has different morph- 
syntactic and morph-scmantic structures. In 
principle their syntactic and semantic ategories 
can be determined by their content and 
contextual information, but there arc many 
difficult problems have to be solved. First of 
all it is not possible to find a uniform 
representational schema and categorization 
algorithm to handle different types of 
unknown words due to their different morph- 
syntactic structures. Second, the clues for 
identifying different type of unknown words 
are also different. For instance, identification 
of names of Chinese people is very much 
relied on the surnames, which is a limited set 
of characters. The statistical methods are 
commonly used for identifying proper names 
(Chen & Lee 1996, Chang ct al. 1994, Sun et 
al. 1994). The identification of general 
compounds is more relied on the morphemes 
and the semantic relations between 
morphemes (Chcn & Chcn 2000). The third 
difficulty is the problems of ambiguities, 
such as structure ambiguities, syntactic 
ambiguities and semantic ambiguities. For 
instances, usually a morpheme 
character/word has multiple meaning and 
syntactic categories and may play the roles 
of common words or proper names. 
Therefore the ambiguity resolution became 
one of the major tasks. 
In this paper we focus our attention on 
the identification of the organization names. 
It is considered to be a hard task to identify 
organization names in comparing with the 
identification of other types of unknown 
words, because there are not much morph- 
syntactic and morph-sernantic clues to 
indicate an organization name. There is no 
significant preference on the selection of 
morphemes/characters and the semantic of 
the morphemes, which gives no clue leading 
toward the identification. For instance, '~ ,  
micro-soW (Microsoft) has the character by 
character (morpheme by morpheme) 
translation of 'slightly soR" and there is no 
marker, such as capitalization, to indicate 
that it is a proper name. The only reliable 
clue is its context information. However an 
organization's full names usually occur at its 
first mention, unless it is a well-known 
organization. A full name contains its proper 
15 
name and organization type, such as '~  
"~ , Acer Computer-Company'. The 
organization types became the major clue of 
identifying a new organization name. However 
abbreviated shorter names usually will be used, 
such as a) omit part of the organization type, for 
instances '~  ~,  Acer Computer', '~  
~..~J, Acer Company', b) omit the organization 
type totally, for instance '~ .~,  Acer', or c) the 
abbreviation, for instance '~ ,  global-electric 
(Acer-computer)'. Therefore the task became 
not only the identification of organization names 
in different forms but also finding their meaning 
equivalence classes. To achieve the above goal, 
the knowledge of 1) proper names of 
organizations, 2) different lines of the businesses, 
and 3) different organization types, should be 
equipped. Unfortunately there is no well- 
prepared knowledge sources containing the 
above information. Therefore a knowledge 
extraction model is proposed to extract the 
above mentioned knowledge from the dictionary 
and domain texts. 
2. STRUCTURES OF ORGANIZATION 
NAMES 
There is no rigid structure for an organization 
name as mentioned in the previous section. 
Roughly speaking an organization ame is 
composed by two major components. The first 
part is the proper name and the second part is the 
organization type. The second part contains the 
major key words lead toward the identification 
of an organization, since the organization types, 
such as '~ J ,  company', ~ '~"  foundation', 
"/J',~J~ group', '~ \ [ \ ]  enterprise' etc, tells what 
kind of organizations they are. If it is a company, 
to be more informative the line of business 
usually goes with the key word '~\ ]  company', 
for instances '~ff~.t..~J food company', ~  
~ computer company', ' ~ M  ~='~ ~ 
investment consultant company', but in most 
cases the keyword '~B\ ]  company' will be 
ignored, such as ~- -~(  President food). 
Sometimes the line of business and the 
organization type go together to become a single 
word, such as ,qa~ middle school',  ' f , l~  
bank' , '~ :  ~ hospital'. By observing the 
structure of the organization name, it seems that 
once a complete list of the organization types is 
well prepared, then it is not hard to identify the 
organizations by their Rill names. The only 
complication is that abbreviated names occur 
more frequently than fidl names. The identifier 
'.~..~\] company' is usually ignored in real text. 
The lines of business became the major 
identifier for a company and many business lines 
are common words, such as '~1~ food', 
'~JJ~ computer', 'TJ~0~ cement'. Therefore 
it is necessary to make the distinction 
between a common compounds and a 
company name, for examples, '~ J~J~n 
health food" vs. ,~m~ President food', 
'~ fg j~ personal computer' vs. ' ~  
~ Acer computer'. Although they are two- 
way ambiguous, usually they have only one 
preference r ading. 
In conclusion, the types and the proper 
names of organizations will be the major 
clues lead toward the identification of the 
organizations. In addition, it is also better to 
have a list of well known organization 
names, such that the well known company 
names, like '~.~i \ [  MicrosoR', can be 
identified immediately. Most of the 
knowledge preparation works should be 
done by oflline approaches. The prepared 
knowledge would be utilized to online 
identification of newly coined organiT~ations. 
The equivalent classes of the well-known 
organizations are also classified by a 
similarity-based approach. 
3. KNOWLEDGE EXTRACTION 
There are two knowledge sources. One is the 
CKIP Chinese lexicon and another is the 
Chinese text from WWW. The lexicon 
provides a partial list of important 
organizations and the information extracted 
from them will be the initial knowledge of 
the identification system. The texts from 
WWW provide ample of new organization 
names implicitly. The problem is how to 
extract some, if not all, of them from the 
texts. Once we have a list of organization 
names. The proper names for organizations 
and the organization types will be extracted 
by analyzing the morphological structures of 
the organization names. However an 
effective morphological analyzer depends 
upon the availability of the knowledge of the 
organization types, but the lists of the 
organization types are not available yet. 
As we mentioned before the complete 
organization names have two parts. The first 
part is the proper name and the second part 
is the organization type. The number of 
different proper names is unlimited and on 
the other hand the number of different 
organization types is limited. Th/s property 
will be utilized to separate the variable parts, 
i.e. the proper name, and the constant parts, 
i.e. the organization type, from the 
organization names. 
16 
The numbers of organization names in the 
lexicon is very limited, since only the important 
organizations in the common domain will be 
collected. Therefore the initial knowledge 
extracted from lexicon is also very limited. To 
make the sources of knowledge more adequate, 
vast amount of new organization names hould 
be extracted from each different domain corpus. 
Unfortunately none of the existing corpora had 
tagged the organization names. Therefore we are 
going to design a semi-automatic method to 
extract the high frequency organization names 
from text corpora. 
The locality of occurrences of keywords in a 
text will be utilized for keyword extraction. 
Once an organization name occun-ing in a text it 
is very probably reoccurred in the same text. The 
recurrence property had been utilized to extract 
keywords or key-phrases from text (Chien 1999, 
Fung 1998, Smadja 1993). However not all 
keywords arc organization names. The 
knowledge extracted from the lexicon, i.e. the 
list of the organization types will be the initial 
knowledge for identifying organization names. 
In addition to the initial knowledge, the structure 
property of the organization names will be also 
utilized in classifying extracted keywords into 
organization names and non-organization names. 
The extraction processes will be repeated for 
extracting new organizations and therefore 
extracting new organization types. The more 
knowledge would have been extracted the more 
accurate of the organization identification will 
achieve. 
3.1 Morphological Analysis for Organization 
Names 
There are 1391 number of words in the CKIP  
lexicon classified as organizations. Table I 
shows some of the examples. 
Table I. The samples of organizations from the 
CKIP  dictionary 
As we observed, the morphological structure of 
an organization name usually is a compounding 
of a proper name and a organization type. The 
organization type might be a compounding of a 
line of business and a type, for instances i.~JJ~ 
~..~J (computer company), ~\]~,~(bank), qb~ 
(middle school), or simply a line of business, 
for instances AM(food), ~-~J~(computer), 
7~ ~ (cement). The proper names are 
variables, since each organization type may 
have many different institutions with 
different names. The types are constants. 
There is a limited number of constants 
attached with many different proper names 
to form different organization names. 
Therefore to extract the organization types is 
equivalent o extract the high frequency 
ending morphemes. Table 2 shows the top 20 
high frequency ending morphemes xtracted 
from the 1391 organization ames and in 
deed they are organization types. 
52 ~" 38 ~ 36 ~ 30 ~d: 29 ;/q~t 
21 ~ 21 I~ 20 ~ 17 Y~ 16 ~z 
16 ~ 16 ~ 15 ~ 15 ~ 15 "l-\]g 
13 I~ 12 ~ 12 ~\] 12 ~JJ 11 
Table 2. The top 20 organization types 
ranked with their occurrence frequencies 
extracted from the 1391 organization names 
3.2 Automatic Extraction of Organization 
Names 
A Web spider can extract ext from each 
different domain through WWW. Then 
keyword extraction technique is applied on 
domain texts to retrieve possible keywords. 
The kcyword set includes organization 
names, personal names, general compounds, 
and also error extraction. Most of which are 
not organization names. It is supposed that 
the available list of the organization types 
will be the source of knowledge to identify 
candidates oforganizations. However such a 
method only identifies the organizations of 
the known organization types and provides 
new proper names only. It will not identify 
new types of organizations. Therefore we 
use a new method to extract the organization 
names by using the structure property of 
organization names. 
Extraction Algorithm for Organization 
Types: 
Step 1. Using a Web spider to collect 
Chinese texts of a fixed domain, such as 
domain of finance and business, from 
.WWW. 
Step 2. Extract high frequency keywords in 
the text (Smadja 93, Chang & Su 97, Chien 
99). 
Step 3. For the keywords of length 3,4, and 5, 
each keyword is divided into two parts X 
and Y. X is a candidate of proper name and 
17 
Y is a candidate of organization type. The X is 
the initial two-characters of the keyword and Y 
is the remained characters. (Since most proper 
names of organizations have two characters, we 
can extract he organization types of the lengths 
1, 2 and 3 from three different groups of 
keywords with lengths 3, 4 and 5 respectively.) 
Extract the organization type Y, if for some 
keywords X+Y, the following conditions hold. 
a) X satisfies one of the following cases. 
1. X is not in the lexicon, i.e. X is an 
unknown word. 
2. X has the categories of Nb or No, i.e. it is 
a known proper name (bib) or a location 
name (No). 
b) For each Y, assumed to be the organization 
type, there must have more than n number of 
different X, such that X+Y in the extracted 
keyword list. In practice, the threshold value 
n was set to 2. 
In general, Chinese company names like most 
proper names are non-common word (unknown 
words). However sometimes they are place 
names (No), but rarely they are common ouns, 
adjectives, or verbs. Therefore in order to avoid 
too many false alarms, such as "~. ,~, /~ super 
computer", to be considered as a company name, 
the condition a) of step 3 is set. The reason to 
setup the condition b) is that each organization 
type Y should have many different organizations 
which have the same organization type Y, such 
as ' ~ ~  Acer computer',  '~,."~_,~ 
Leo computer', ' ~ ~ ~ ~ Blue-slcy 
computer', ...etc.. The real implementation 
shows the different hreshold value n gives the 
different precision and recall for identification. 
For the first iteration of knowledge 
extraction, we suggest to have higher ecall rate. 
Set the threshold value low and manually select 
the final list of the organization types. For the 
future automatic knowledge xtraction, in order 
to increase the precision of the information 
extraction higher threshold values are suggested. 
4. E~ERIME~ ~S~TS 
The knowledge xtraction processes for Chinese 
org~t ion  names are carried out by different 
stages. At the first stage, the words marked with 
semantic category of organization were accessed 
from the CKIP dictionary. There are 1391 word 
organization types. As mentioned in section 3.1, 
a pseudo morphological analysis process was 
carried out, which try to find the high frequency 
ending morphemes. Since the structure of an 
organization ame is a composition of X+Y, 
where X is a proper name and Y is a 
organization type. There are 546 different 
ending morphemes. The high frequency 
ending morphemes are exactly to be the 
morphemes for common organization types. 
Many of them are monosyllabic words and 
they are polysemous, as shown in Table 1. 
For the future identification, the 
disambiguation process has to be carried out 
for those polysemous ending morphemes 
(Chen & Chen 2000). The extracted 
morphemes and list of organizations will be 
the first collection of the organization types. 
At the second stage, we try to extract 
new organizations names from different 
domain text. Each different domain has 
many new organization types. For instance 
in the domain of finance and business, there 
are many company names, which have 
completely different word strings for the 
organization types as in the extracted list by 
the first stage. 
The algorithm shown in the section 3.2 
was carried out. At the step 1, 31787 texts of 
news of the finance and business domain 
were extracted from http://www.cnyes.com. 
At step 2, 40675 keywords were extracted 
from the news corpus. At step 3, 
organization ames were identified and the 
organization types were extracted. If the 
threshold value n -- 2, 92 types were 
extracted and among them 83 are correct 
organization types. The precision is 90%. If 
the threshold was set to 3, only 56 types 
were extracted and all of them happen to be 
correct. The precision increased to 100%, 
but of course the recall rate dropped. We 
don't know the exact recall rate, since there 
are too many keywords in the training set. 
However the recall rate is not important, 
since the whole knowledge extraction 
process is a recurrent process. The 
knowledge xtraction procedures should be 
repeatedly applied on the different set of text 
and at each iteration more information will 
be extracted. Hence the precision is much 
more important than the recall. The 
knowledge sources for future identification 
of organizations are the accumulated lists of 
the organization names, the proper names of 
organizations and the organization types. 
Table 3 contains the extracted 
organization types while the threshold value 
n=3. The organization types are classified by 
their lengths and sorted by their frequencies 
of uses. Table 4 contains the extracted 
organization types which associated with 
exactly two different names and the last line 
shows the error extractions. Among newly 
extracted organization types only 23 of them 
18 
are already in the old list. 
_~t~ 81 ~ 74 ~:~ 46 ~1~'~ 41 ~---~. 40 {...~j 36 
19 ~-~-~ 18 ~-~ 17 ~b~ 16 ~ 15 ~q~. 15 
~-~ 15 ~ 15 ~ 14 ~?~ 13 ~ II B~ 10 
~ 9 ~ 9 ~ 8 ~ 7 ~=r.~ 6 ~fl~l~ 6 
~f~6 ~I/,~6 )L~5 ~5 ~5 ~5 
Ig-~4 ~32 4 ~_T..4 .T.:~3 'fJfiI 3 ~ i :3  
~/~3 ,~3 ~3 ~3 ~3 ~3 
~? 3 
~\ [ \ [~ 7 ~\ ]~ 6 
Table 3. The extracted organization types 
associated with the number of different names 
>=3 
Table 4. The extracted organization types 
associated with two different names and the last 
lines show the error exWactions. 
4.1 Strategies for On-line Identification of 
Organization Names 
The knowledge about organizations extracted 
from the dictionaries and domain texts will be 
used to identify organization ames at on-line 
sentence processing. During the word 
segmentation process, an organization name is 
either identified immediately (if it is a known 
organization name), or it will be segmented into 
two segments of X+Y or several segments of 
(xl+x2+...+xn)+Y, where X is a proper names, 
Y is the organization type. When the proper 
name X is a new word, it will be segmented into 
shorter segments (xl+x2+...+xn). To simplify 
the experiment process, we  assume the proper 
names X are either the words of categories Nb  
(i.e. proper names) or Nc (i.e. the place names) 
or a two-character unknown word. For the 
identification experiment, a corpus extract from 
a T.V. news (http://www.ttv.com.tw) 
The patterns of X+Y in the testing corpus 
were searched. 117 different organizations were 
identified. Among thern 56 are known 
organizations, i.e. they are in the organization 
name list. 61 of them arc identified by the 
composition of X+Y and 52 of them are correct. 
It counts the precision of 52/61=85% for 
identifying new names. The total performance is 
the precision of 108/I 17=92%. 
The knowledge-based approach for 
identifying organization ames seems very 
promising. It outperforms the reports of the 
precision of 61.79% and the recall of 
54.50% in (Chen & Lee 1996) and the 
experiment was carried out under the 
condition that the knowledge extraction 
process is in its initial stage. We expect hat 
performance of the algorithm will become 
better and better while the knowledge 
extraction process continuously performs. 
4.2 Automatic Extraction of Name 
Equivalent Classes 
The abbreviated names are very frequently 
occurred in the real text especially in the 
domain of the stock market. By observing 
the abbreviation ames, the heuristic rules 
for abbreviating a company name can be 
concluded as follows. 
Abbreviation rule: If the proper name of a 
company is unique, then take the proper 
name as its abbreviation name, such as '~ ,  
Microsoft'. Otherwise the abbreviation will 
be a compound of key-characters from part 
of its proper name and part of its line of 
business, such as ~ is the abbreviation 
o f '~  m~l~,  China petroleum'. 
An  experiment was carried out to find 
the full names of the abbreviations of 
company names shown in the price table of 
the Taiwan stock market. The purposes of 
this experiment are a) to fred the equivalent 
classes of company names and b) to have 
some idea about the recall rate of the current 
knowledge extraction process. 
The matching process between the 
abbreviations and the extracted organization 
name lists is as follows. 
I. For each abbreviation name matches the 
organization names in the organization 
name list. Find all the organization names 
containing the abbreviation name. 
2. Rank the matched organization names 
according to the following criterion. 
The first rank: The proper name of  the 
organization ame is  exactly matched 
with the abbreviation name. 
The second rank: The abbreviation is 
compounding of key-characters f om part 
of the proper name and part of the line of 
business of the matched organization 
names. 
If there are many candidates with the 
same rank, then rank them according to 
their frequencies occurring in the training 
corpus. 
19 
There are 471 abbreviated company names in 
the price list of the stock market. 302 of them 
have matched candidates. Each abbreviation 
name may match many different organization 
names. The recall rate for the top ranked 
candidate is 282/471=60%. The precision of the 
first rank candidate is 282/302=93%. Table 5 
shows some of the results. 
Abbr. Candidates arranged in the order of their anks 
~ ~ 8 ~l.IJ~l~l 3 ~I.L~-T-" 2 
TW~ttt 
~m~4.tt 
11 
I01 ~..~-~; 2
48 m ~  2 
I0 ~rd~;H'f~ 2 ~, f~ I 
5 ~. . J~ l l  22 ~K~....~ 22 
21 
6 
15 ~:~\]~J~ 2 ~\ [~ 2 
11 
67 ~. JW 29 ~,~, J~ 17 
6 ~.~J~gg 2 ~.,~ "~r~ 2 
:~,,~?~ 2 
Table 5. Some examples of the abbreviations 
and the matched candidates (the correct answer 
is highlighted by the boldface characters) 
5. CONCLUSIONS 
The knowledge extraction process will be 
continuously carried on in the future. The 
accumulated knowledge will be utilized for the 
on-line unknown word identification as well as 
for the off-line knowledge extraction. The 
proposed knowledge extraction processing 
model can be generalized to extract other types 
of linguistics or morphological knowledge, for 
instances, to extract the transliterate foreign 
names, to extract the titles of people. 
Some of the errors are caused by that the 
titles of the people are wrongly identified as 
organization types, since the patterns of people's 
name followed by their title are commonly 
occurred in real text. These patterns are similar 
to the sU'uctures of organization ames. Such 
kind of errors can be avoid, if the titles of people 
are known and in fact the titles of people can be 
extracted by the same extraction model except 
that most of people's names have three 
characters instead of two. 
In the future, the knowledge xtraction 
processes will be automatically carried out. We 
expect hat it will be one of the major building 
blocks for automatic learning systems for 
Chinese morphology and sentence processing. 
REFERENCES 
\[1\] Bai, M.H., C.J. Chert & KJ. Chert, 1998, 
"POS tagging for Chinese Unknown 
Words by Contextual Rules" 
Proceedings ofROCLING, pp.47-62. 
\[2\] Chang, J. S.,S.D. Chen, S. J. Ker, Y. 
Chen,& J. Liu,1994 "A Multiple-Corpus 
Approach to Recognition of Proper 
Names in Chinese Texts", Computer 
Processing of Chinese and Oriental 
Languages, Vol. 8, No. 1, pp. 75-85. 
\[3\] Chang, Jing Shin and Keh-Yih Su, 
1997," An Unsupervised Iterative 
Method for Chinese New Lexicon 
Extraction," Computational Linguistics 
and Chinese Language Processing, Vol. 
2 #2, pp97-147. 
\[4\] Chert, Keh-Jiann, Ming-Hong Bai, 1997, 
"Unknown Word Detection for Chinese 
by a Corpus-based Learning Method." 
Proceedings of the l Oth Research on 
Computational Linguistics International 
Conference, pp159-174. 
\[5\] Chen, K.J. & Chao-jan Chen, 2000," 
Automati Semantic Classification for 
Chinese Unknown Compound Nouns," 
Coling 2000. 
\[6\] Chert, K.J. & S.H. Liu, 1992,"Word 
Identification for Mandarin Chinese 
Sentences," Proceedings of14th Coling, 
pp. 101-107. 
\[7\] Chen, Hsin-His & Jen-Chang Lee, 
1996," Identification and Classification 
of Proper Nouns in Chinese Texts," 
Proceedings of Coling-96, Vol. 1., pp. 
222-229. 
\[8\] Chien, Lee-feng, 1999," PAT-tree-based 
Adaptive Keyphrase Extraction for 
Intelligent Chinese Information 
Retrieval," Information Processing and 
Management, Vol. 35, pp. 501-521. 
\[9\] Fung P., 1998," Extracting Key Terms 
from Chinese and Japanese Texts," 
Computer Processing of Oriental 
Languages, Vol. 12, #1, pp 99-122. 
\[10\] Lee, J.C. , Y.S. Lee and H.H. Chen, 
1994, "'Identification of Personal Names 
in Chinese Texts." Proceedings of 7th 
ROC Computational Linguistics 
Conference. 
\[11\] Lin, M. Y., T. H. CMang, & K.Y. Su, 
1993," A Preliminary Study on 
Unknown Word Problem in Chinese 
Word Segmentation" Proceedings of 
Rocling VI, pp 119-137. 
\[12\] McDonald D., 1996, '" Internal and 
20 
External Evidence in the Identification and 
Semantic Categorization f Proper Names", 
in Corpus Processing for Lexical 
Acquisition, J. Pustejovsky and B. Boguraev 
Eds, MIT Press 1996. 
\[13\] Smadja, Frank, 1993,'Retrieving 
Collocations from Text: Xtract," 
Computational Linguistics, vil. 19, #1, pp. 
143-177. 
\[14\] Sun, M. S., C.N. Huang, H.Y. Gao, & Jie 
Fang, 1994, "Identifying Chinese Names in 
Unrestricted Texts", Communication of 
COUPS, Vol.4 No. 2. 113-122. 
21 
Character-Sense Association and Compounding Template Similarity: 
Automatic Semantic Classification of Chinese Compounds 
Chao-Jan Chen 
LATTICE, University Paris VII, Paris, France 
chen_chaojan@yahoo.com.tw 
 
Abstract 
This paper presents a character-based model of 
automatic sense determination for Chinese 
compounds. The model adopts a sense 
approximation approach using synonymous 
compounds retrieved by measuring similarity 
of semantic template in compounding. The 
similarity measure is derived from an 
association network among characters and 
senses, which is built from a formatted MRD. 
Adopting the taxonomy of CILIN, a system of 
deep semantic classification (at least to the 
small classes) for V-V compounds is 
implemented and evaluated to test the model. 
The experiment reports a high precision rate 
(about 38% in outside test and 61% in inside 
test) against the baseline one (about 18%). 
 
1. Introduction 
Sense tagging is an important task in NLP. It is 
supposed to provide semantic information useful to 
the application tasks like IR and MT.  As generally 
acknowledged, sense tagging is to assign a certain 
sense to a word in a certain context by using a 
semantic lexicon (Yarowsky, 1992, Wilks and 
Stevenson, 1997). In addition to word sense 
disambiguation (WSD) for known words, sense 
determination for words unknown to the lexicon 
poses another challenge in sense tagging. This is 
especially the case in NLP of Chinese, a language 
rich in compound words. According to the data in 
(Chen and Lin, 2000), about 5.51% of unknown 
words is encountered in their sense-tagging task of 
Chinese corpus. Instead of proper names, the 
cross-linguistically most common type of unknown 
words, compound words constitute the majority of 
unknown words in Chinese text. According to Chen 
and Chen (2000), the three most dominant types of 
Chinese unknown words are: compound nouns 
(about 51%), compound verbs (about 34%), and 
proper names (about 15%). While the identification 
and classification of proper names is an issue already 
well discussed in Chinese NLP researches, the sense 
determination of unknown compounds remains a 
subject relatively less tackled. 
 
1.1 Shallow vs. Deep Classification 
While word sense might be conceptually vague and 
controversial in linguistics and difficult to define 
(Manning and Sch?tze, 1999), sense tag is more 
concrete and can be defined according to the specific 
need of the NLP tasks in question. For example, in a 
task of semantic tagging or classification, sense tag 
can be the semantic class from a thesaurus. Or 
otherwise, in a task of machine translation, the 
equivalent foreign word from a bilingual dictionary 
can be chosen as sense tag. In this paper, it is the 
sense tag so defined that is meant by the term sense. 
The notion sense determination then refers to the 
assignment of sense tag to a word without using 
contextual information. It is so called to be 
distinguished from sense tagging, which requires 
contextual information. Under such a definition, 
semantic classification can be regarded as a case of 
sense determination using the taxonomy of a certain 
thesaurus, in which a semantic class is a sense tag. 
According to Wilks and Stevenson (1997), a 
task assigning broad sense tags like HUMAN, 
ANIMATE in WordNet is referred to as semantic 
tagging, different from sense tagging, which assigns 
more particular sense tags. In fact, a similar 
distinction can also be made for semantic 
classification according to the target level of the 
semantic classes in the taxonomy tree: a task aiming 
at the top-level classes can be called shallow 
semantic classification (like Lua, 1997), while a task 
aiming at the bottom-level classes can be called a 
deep semantic classification1 (like Chen and Chen, 
2000). Since many top-level semantic classes, like 
TIME, SPACE, QUALITY, ACTION, etc., are often 
already reflected in the syntactic information, a 
shallow semantic classification does not actually 
provide much semantic information independent of 
syntactic tagging. It is therefore the deep semantic 
classification that the paper is concerned about. 
                                                 
1
 Take the word ?(?attack?) for example. According to 
CILIN (a thesaurus widely used in Chinese semantic 
classification, see 3.1), it can be classified to shallow-levels as 
major class H (ACTIVITY) or as medium class Hb (MILITARY 
ACTIVITY). It can also be classified to deep-levels as small 
class Hb03 (specific military operations: ATTACK, RESIST, 
and COUNTERATTACK) or as subclass Hb031 (ATTACK). 
1.2 Previous Researches 
In the previous researches of automatic semantic 
classification of Chinese compounds, compounds 
are generally presupposed to be endocentric, 
composed of a head and a modifier. Determining the 
class of the head is therefore determining the class of 
the target compound (Lua, 1997, Chen and Chen, 
2000). This head-determination approach has two 
advantages: (1) it is simple and easy to implement (2) 
it works effectively for compound nouns, the 
dominant type of compounds, since most of them are 
head-final endocentric words.2 However, there exist 
considerable exocentric compounds, for which such 
a simple algorithm does not work successfully. It is 
especially the case for compound verbs like V-Vs3. 
For example, q is a V-V compound meaning ?to 
kill by beating?. Obviously, neither the sense of q
(?beat?) nor that of (?die?) is appropriate to be 
assigned to the compound q as the sense of B
(?car?) can be assigned to ?B (?tram?, literally 
?electricity-car?) as a general meaning. 
A second problem encountered in compound 
semantic classification is that there are considerable 
out-of-coverage morphemes, which are not listed in 
the lexicon, as remarked in (Chen and Chen, 2000). 
Moreover, even a morpheme is listed, the given 
senses are not necessarily appropriate to the task. 
For example, in the search of compound 
morphological rules in (Chen and Chen, 1998), some 
appropriate senses of morphemes have to be added 
manually to facilitate the task. Obviously this causes 
a great difficulty to an automatic task, especially to 
the example-based models which rely on the 
similarity measurement of the modifier morphemes 
to disambiguate the head senses (Chen and Chen, 
1998, 2000). An alternative approach is thus needed 
to solve the problems of exocentric compounds and 
lexicon incompleteness. 
Therefore in this paper I will present a non 
head-oriented model of Chinese compound sense 
determination, in which lexicon incompleteness will 
be overcome by exploring the association between 
                                                 
2
 Though a compound noun and its head are strictly speaking in 
a hyponym relation, they are usually categorized as members of 
the same class. For example, in CILIN,B(?car?, ?vehicle?) and 
most of the compounds X-B are put under the same class Bo21 
(VEHICLES), where X can be a morpheme designating the 
energy source (like horse, cow, electricity) or the load content 
(like passenger, merchandise). 
3
 An introspection on the two-character verbs in CILIN shows 
that about 48% of them are semantically exocentric, which 
means the semantic class of a compound X-Y in CILIN is equal 
neither to that of X nor to that of Y. As to the endocentric V1-V2, 
V1 and V2 are about equally likely to be the head of a compound 
verb according to the introspection. 
characters and senses in a MRD. The sense of an 
unknown compound can be approximated by 
retrieved synonyms. Its sense tag can be assigned 
according to a certain MRD. This model facilitates 
an automatic system of deep semantic classification 
for unknown compounds. In this paper, a system for 
V-V compounds is implemented and evaluated. The 
model can however be extended to handle general 
Chinese compounds, like V-N and N-N, as well. 
 
2. Compound Sense Determination 
2.1 Compounding Semantic Templates 
Most of the Chinese compounds are composed of 
two constituents, which can be bound morphemes of 
one character or free words of one or more 
characters. The two-character compound is a most 
representative type because its components can be 
bound morphemes as well as free words. The 
handling of two-character compounds becomes 
therefore the focus in this paper. 
As in general Chinese compounding, a 
two-character compound is usually semantically 
compositional, with each character conveying a 
certain sense. The principle of semantic composition 
implies that under each compound lies a semantic 
pattern, which can be represented as the combination 
of the sense tags of the two component characters. 
The combination pattern is referred to as 
compounding semantic template (denoted by 
S-template) in this paper; compounds of the same 
S-template are then referred to as template-similar 
(denoted by T-similar). Since T-similar compounds 
are alike in their semantic compositions, they are 
supposed to possess roughly the same meaning and 
to be put under a considerably fine-grained semantic 
class. Take the compound verbq~ for example. 
This compound suggests the existence of a 
S-template of HIT-BROKEN, as the senses of the 
two component characters q and ~  are 
respectively ?hit? and ?broken?. The S-template 
HIT-BROKEN refers to a complex event schema [to 
make something BROKEN by HITting]. This 
S-template can also be found in many other 
compounds with a similar meaning:q?,?,~,
??etc. Obviously such T-similar words can 
make a good set of examples for the example-based 
approach to the sense determination, if an effective 
measure of word similarity is available for their 
retrieval. 
 
2.2 Compound Similarity 
As a critical technique, word similarity is generally 
used in the example-based models of semantic 
classification. The measure of word similarity can be 
divided into two major approaches: taxonomy-based 
lexical approach (Resnik 1995, Lin 1998a, Chen and 
Chen 1998) and context-based syntactic approach 
(Lin 1998b,Chen and You 2002), which is not the 
concern in this context-free model. However, two 
problems arise here for the taxonomy-based lexical 
approach. First, such similarity measures risk the 
failure to capture the similarity among some 
semantically highly related words, if they happen to 
be put under classes distant from each other 
according to a specific ontology 4 . Second, as 
mentioned, the appropriate senses of some 
characters just cannot be found in the thesaurus. One 
major reason why dictionaries do not include certain 
character senses is that many of such characters are 
used in contemporary Chinese only as bound 
morphemes not as free words, when the senses in 
question are involved. However, such senses could 
be kept in the compounds in the lexicon, so they 
might be covert but not inextricable. 
To remedy the effects of such lexicon 
incompleteness, I propose an approach to retrieve 
the latent senses 5  of characters and the latent 
synonymy among characters by exploring 
association among characters and senses. The idea is 
that if a character C appears in a compound W, then 
according to semantic composition, the sense of C 
must somehow contributes to S, the sense of W. 
Therefore the association strength between character 
C and sense S in a MRD is supposed to reflect the 
potentiality of S to be a sense of C. By transitivity, 
such association between characters and senses 
allows to capture association among characters. A 
new way to measure word similarity of two 
compounds can be thus derived based on the 
association strength of the corresponding component 
characters. This measure actually reflects the 
S-template similarity between two compounds and 
can be used to retrieve for a compound its T-similar 
words, which are potentially synonymous. 
                                                 
4
 Take an example in CILIN (a Chinese thesaurus, see 3.1). 
KILL(), BUTCHER(??), and EXCUTE(?) are three 
concepts all meaning ?cause to die?. However, the words 
expressing these three ideas are respectively put under small 
classes Hn05, Hd28, and Hm10, respectively under medium 
class Hn: Criminal Activities(u@ ), class Hd: Economical 
Production Activities(?Y), and class Hm: Security and Justice 
Activities(??-P[). We wonder if any measurement based on 
that hierarchy can capture the similarity among the words 
situated in these three small classes in CILIN, for those words 
share only a common major class H, denoting vaguely Activities, 
which includes 296 small classes and 836 subclasses. 
5
 Here the term latent is used only to mean ?hidden, potential, 
and waiting to be discovered?. It has nothing to do with the LSI 
techniques, though they both evoke the same meaning of latent. 
 
2.3 Synonyms and Sense Approximation 
The acquisition of synonyms plays an important role 
in the sense determination of a word. When a 
native-speaker is capable of giving synonyms to a 
word, he is considered to understand the meaning of 
that word. In fact, such a way of sense capturing is 
also reflected in how the senses of words can be 
explained in many dictionaries6. Moreover, as some 
researches propose, synonyms can be used to 
construct the semantic space for a given word (Ploux 
and Victorri, 1998, Ploux and Ji, 2003). In such a 
semantic space, each synonym with different nuance 
occupies a certain area. As visually reflected in this 
approach, retrieving a proper set of its synonyms 
means the ability to well capture the senses of a 
word. In fact, my model of automatic sense 
determination for a compound is exactly built upon 
the retrieval of its near synonyms, the T-similar 
compounds as previously described. 
 
2.4 Model Representation  
With a S-template similarity measure, one can 
retrieve, for a given compound, its potential 
synonymous T-similar compounds. Then the sense 
tags of the retrieved compounds can be used to 
determine the sense tag of the target compound. The 
model of compound sense determination can be thus 
composed of two modules, as illustrated in Fig.1. 
 
W(X-Y) 
 
{dico1,dico2,?} 
 
Module-A       < T-similar Word Retriever > 
 
 
   { SW-set(X-Y) }       
       Filter-C  
        dicox 
 
Module-B       < S-tag Determiner > 
 
 
 
Fig.1 Model of Compound 
Sense Determination 
{S-tag(X-Y)} 
 
Module-A (<T-similar Word Retriever>) is to find 
the potential synonyms ({SW-set(X-Y)}) of a given 
compound (X-Y) by using association information 
provided from dicos {dico1, dico2,?}. Module-B 
(<S-tag Determiner>) is to obtain the most likely 
                                                 
6
 Especially in Chinese dictionaries, it is often the case that 
several synonymous words are given as explanation to the 
meaning of a word, especially when it is a compound verb. 
sense tags ({S-tag(X-Y)}) according to dicox for the 
target word by using the output of Module-A. The 
component filter-C is optional, which passes only 
the T-similar words with the same syntactic category 
as the target compound, if it is already known. In 
fact, a system of semantic classification can be so 
created by choosing dico2 as dicox and the S-tag is 
then the semantic class in CILIN (as in section 4). 
 
3 Character-Sense Association Network 
Before exploring the critical measurement of 
association among characters and senses needed in 
the model, I have to briefly present the lexical 
sources in use and to define the idealized dictionary 
format adopted in this task. 
 
3.1 Lexical Sources 
The lexical sources used to implement my system 
include: 
(1) Sinica Corpus: a balanced Chinese corpus     
with 5 million words segmented and tagged with 
syntactic categories. (Huang et al, 1995) 
(2) HowNet: an on-line Chinese-English bilingual 
lexical resource created by Dong. It is used in 
this paper as a Chinese-English dictionary 
registering about 51,600 Chinese words, each 
assigned with its equivalent English words and 
its POS. (http://www.keenage.com/) 
(3) CILIN: a Chinese thesaurus collecting about 
53,200 words. CILIN classifies its lexicon in a 
four-level hierarchy according to different 
semantic granularities: 12 major classes (level-1), 
95 medium classes (level-2), 1428 small classes 
(level-3), and 3924 subclasses (level-4). The 
words in the same small class can be regarded as 
semantically similar, but only the words in the 
same subclasses can be surely regarded as 
synonyms7.(Mei et al, 1984) 
 
3.2 Idealized Dictionary Format (dico) 
The idealized dictionary, denoted as dico, is actually 
a formatted MRD defined as follows:  
 
A dico is a set of <W-S> correspondence pairs, 
where W is a word, and S is a sense tag.   (1) 
 
                                                 
7
 Take two verbs?(?to buy?) and [(?to sell?) as examples to 
demonstrate the taxonomy of CILIN. Both of the two verbs are 
grouped in the small class He03 (commercial trade), which is 
under the major class H (activities) and the medium class He 
(economic activities). However, the two antonyms are put under 
two different subclasses, respectively He031 (buying) and 
He032 (selling). 
In the system implementation in this paper, two 
dicos are converted respectively from HowNet and 
CILIN for the calculation of the association 
measures among characters and sense tags with 
different types of sense tags adopted. For HowNet, 
the English equivalent words are used as sense tags 
to form dico1. For CILIN, the subclasses are used as 
sense tags to form dico2. 
 
3.3 Character-Sense Association 
All the semantic information provided by a dico, as 
defined in (1), can be in fact represented as a 
network with links between two domains: W domain 
(words) and S domain (sense tags). In such a 
viewpoint, polysemy is then a one-to-many mapping 
from W to S, while synonymy a one-to-many 
mapping from S to W. If we further link a 
component character C of a word W to one of the S 
linked to W, such a C-S link might intuitively reflect 
a potential sense S for the character C, probably a 
latent sense of C, as previously described in section 
2.2. We can use a statistical association measure, 
like MI or ?2, to extract such C-S links. The 
statistically extracted C-S association can then lead 
to the finding of latent senses for a character. The 
revelation of a latent character-sense association will 
further lead to the retrieval of new synonymy 
relation between characters. Symmetrically, the 
revelation of a latent character-sense association will 
also lead to the retrieval of the potential polysemy of 
a character. As illustrated in the Z-diagram below, 
supposed that C1 is already associated to S1 and C2 
to S2, the retrieval of latent sense S1 to C2 will, 
meanwhile, lead to the finding of an association 
between C1 and C2 (latent synonymy), and an 
association between S1 and S2 (latent polysemy). 
 
C1          S1 
                                      
latent synonymy      latent sense       latent polysemy 
                                
           C2           S2 
                    
 
Fig. 2  Z-diagram of C-S links 
 
The directed association measure from a character to 
a sense, denoted as CS-asso(Ci,Sj), can be defined as 
follows: 
 
?(Ci, Sj) = [ freq(Ci,Sj)E/ ( freq(Ci)+freq(Sj) ) ] ^ 0.5   
CS-asso (Ci, Sj) =  ? (Ci,Sj) / Max k { ? (Ci,Sk) }      (2) 
 
where freq(Ci,Sj) is the number of the words in the 
MRD that contain character Ci and is tagged with 
sense Sj, while freq(Ci) is the number of words 
containing character Ci, and freq(Sj) the number of 
words tagged with sense Sj.8Likewise, the directed 
association measure from a sense to a character, 
denoted as SC-asso(Si,Cj), can be defined as 
follows9: 
 
? (Si,Cj)= [ freq(Si,Cj)E/( freq(Si)+freq(Cj) ) ] ^0.5    
SC-asso (Si,Cj) =  ? (Si,Cj) / Max k { ? (Si , Ck) },     (3) 
 
Consequently, by link of a Ci-Sj-Ck chain (a latent 
synonymy), the directed association measure for a 
character Ci to another character Ck is defined as a 
combination of two types of directed association 
measures, the maximal association measure 
CC-asso1(Ci ,Ck) and the over-all association 
measure CC-asso2(Ci ,Ck), with respective weights 
of 1-? and ? (the value ? is by default set at 0.5). 
 
asso-chain(Ci,Sj,Ck) = ?asso (Ci,Sj) * asso (Sj,Ck) ) ^ 0.5  
f1 (Ci,Ck) = Max j {asso-chain (Ci,Sj,Ck) } 
CC-asso1(Ci,Ck) = f1 (Ci,Ck) / Max m { f1 (Ci,Cm) }  
f2 (Ci,Ck) = ?j asso-chain(Ci,Sj,Ck) 
CC-asso2(Ci,Ck) = f2 (Ci,Ck) / Max m { f2 (Ci,Cm) }   
CC-asso = (1-?) * CC-asso1 + ? * CC-asso2           (4) 
 
3.4 S-Template Similarity Measure 
Supposed that Wi(Ci1-Ci2) and Wj(Cj1-Cj2) are both 
two-character compounds, a measure of word-word 
directed association (denoted as WW-asso) from Wi 
to Wj can be defined based on the CC-asso between 
their corresponding component characters: 
 
? (Wi,Wj) = { CC-asso(Ci1,Cj1) * CC-asso(Ci2,Cj2) } ^ 0.5 
WW-asso(Wi,Wj) = ? (Wi,Wj) / Max k { ? (Wi,Wk) }       (5) 
 
Since the corresponding characters of two T-similar 
compounds must share the same sense tags and thus 
have strong CC-asso, the measure WW-asso(Wi,Wj) 
indicates, in fact, how T-similar for a compound Wj 
to a target Wi, compared with other compounds. 
WW-asso(Wi,Wj) is therefore taken as the measure 
of S-template similarity (denoted as T-similarity). 
Applying the S-template similarity measure in 
(5), now the T-similar Word Retriever (<TWR>) can 
                                                 
8
 The formula ? in (2) is actually a simplified approximation to 
the ?2 -test measure by supposing that freq(C,S) is much smaller 
than freq(C) and freq(S). In fact, MI (mutual information) is 
another association measure frequently used in Chinese NLP. 
For example, it is successfully used for the character-POS 
association measure in the task of syntactical classification for 
Chinese unknown words (Chen et al, 1997). However, a 
heuristic evaluation on some randomly picked examples shows 
that it seems to be outperformed by the ?2 measure in this task. 
9
 It must be noted that the measures of directed association (2) 
and (3) are asymmetric in that they give different values for the 
association from Ci to Sj and for the one from Sj to Ci because 
their normalization factors are not the same. That is why the 
notion directed is added here to point out the asymmetry. 
give for a compound X-Y the list of its most 
T-similar compounds from the corpus and their 
T-similarity scores. As to the <S-tag Determiner>, it 
receives as input the output T-similar words from 
<TWR>. Among the input T-similar words, the ones 
known to dicox, are picked out and their sense tags 
(S-tag) with the T-similarity scores (WW-asso) are 
used, as in the formula (6), to calculate the 
likelihood score ? for a compound V-Vi to possess a 
certain S-tagj. Therefore a set of ranked possible 
semantic classes for the compound X-Y can be given 
({S-tag(X-Y)}). 
 
?(V-Vi, S-tagj) = ? j WW-asso (V-Vi, SWk)            (6) 
,where SWk is a known word in dicox                   
and S-tagj is one of the S-tages to SWk 
?(V-Vi,S-tagj)=?(V-Vi,S-tagj)/Max n { ?(V-Vi, S-tagn) } 
 
4. System Implementation  
4.1 Classification for V-V Compounds 
Based on the model proposed, a system of semantic 
classification can be implemented for two-character 
V-V compound verbs by using dico2 as the dicox in 
the Module-B (the S-tag now is the semantic class in 
CILIN). The V-V compounds are chosen as subjects 
in this system because the choice can best 
distinguish the present model from the previous 
head-orientated approaches. As the involvement of 
only V characters make training data homogeneous, 
it simplifies the association network and reduces 
largely the computational complexity. However, the 
partial system for V-V compounds can be easily 
extended to handle V-N compounds and N-N 
compounds as well when the character-sense 
association network for N characters is established. 
Since only the V characters are involved, a 
subset of <W-S> pairs of dico1 (HowNet) and dico2 
(CILIN) is extracted to calculate the association 
measures and then the T-similarity measure. The 
subset contains only the <W-S> pairs whose W are 
one-character or two-character verbs. In CILIN the 
verbs are put under the major classes from E to J, 
designating the concepts of attributes (E), actions (F), 
mental activities (G), activities (H), physical states 
(I), and relations (J). By choosing only the words in 
the above 6 major classes, the nominal senses of 
characters (A: human, B: concrete object, C: time 
and space, D: abstract object) are supposed to be 
excluded. Besides, the occurrence frequency of a 
character in a mono-character word will be double 
weighted, since in this case the word sense is surely 
contributed by that character alone. 
Let us take the V-V compound ?> (?to catch 
by hunting?, literally ?hunt-catch?) for example to 
see how the model operates. Based on the 
association network created from HowNet, the 
characters associated to ?  and > are listed in 
List 1 and List 2 (only the 10 top ranked are listed 
here), the 20 top ranked T-similar compounds of ?
> are listed in List 3 with their similarity scores, 
syntactic categories and semantic classes, if they are 
known in CILIN. Among the 20 T-similar 
compounds retrieved, 10 of them (the grayed ones) 
can be found in CILIN; 9 of them (the framed ones) 
can be considered as good synonyms of ?>, while 
other 7 (the starred ones) considered semantically 
really close. In this particular example, 80% (16/20) 
of the T-similar compounds can be considered as at 
least near synonymous, while 50%(8/16) of them 
can be actually found in CILIN to serve the 
automatic semantic classification. 
 
???????? ? ???????? ? ?? ????????? ?  Z???????  
???????? ? ???????? ? ?? S???????    ???????? ?
???????? ? ???????? ? ?? ???????? ?   ???????? ?
Z??????? ? ???????? ? ?? ?? ???????   ???????? ?
???????? ? ???????? ? ?? ????????    ???????? ?
???????? List 1                  List 2                  
?
??????????????  ??? ??????????????? ????
??????????????  ???? ?S?????????? ?
?S?????????? ? ? ?S??????????????????
Z??????????? ? ? ???????????? ?
???????? ?????? ???? ??????????????
??????????????? ???? ??????????????????  
???????????? ? ? ????????????  
?????????????? ? Z?????????????? ????
?)?????????? ? ? ?S???????????????? ?
??????????????? ???? ?S 0.7790 VC 
List 3 
Applying the formula for the likelihood score of 
semantic class determination in (6), we have the 4 
top ranked semantic classes for ?> predicted by 
the system as follows: 
(1) Hm051 (Z? ?arrest? )  
(2) Je121 (7S ?acquire? ) 
(3) Hb121  (?? ?attack and occupy? ) 
(4) Hb141 (?? ?capture as war prisoner?)  
In this case, the standard answer of class Hm051 for 
the compound ?> is ranked as the first candidate, 
while the second ranked candidate class Je121 
(?acquire?) is also reasonable, which can be 
considered rather correct in a certain way by human 
judgment. In fact, according to the native speaker?s 
instinct, the 4th ranked candidate class Hb141 
(?capture?) is also quite suitable to the meaning of 
the verb ?>, though that is not what it is classified 
in CILIN. However, to avoid the subjective 
interference of human judgment and particularly to 
make the evaluation task automatic, the evaluation in 
the following sections will be made by machine only 
according to the standard classification in CILIN. 
 
4.2 Experiment Results 
For evaluating the performance of the system, 500 
V-V compounds are randomly picked out from 
CILIN to form the test set. Two modes of evaluation 
experiments are carried out: both modes adopt dico2 
(CILIN) in Module-B (dicox=dioc2) to determine 
semantic classes, while the inside-test mode uses 
dico2 (CILIN) in Module-A and the outside-test 
mode uses dico1 (HowNet) in Module-A, to obtain 
association network and retrieve the T-similar words. 
To make the test compounds unknown to the model, 
the semantic classes of the test compounds have to 
be invisible to CILIN, while the invisibility should 
not undermine the training of the association 
network in Module-A. The effect is done by 
dynamically withdrawing a word from dico2 in 
Module-B each time when it is in test. Two ways of 
evaluation can be made: by verifying the answer to 
the level of small class (level-3) and to the level of 
subclasses (level-4). The accuracy is calculated by 
verifying if the correct answer or one of the correct 
answers (if V-V is polysemous) according to CILIN 
can be found in the first n ranked semantic classes 
predicted by the system. The performance of a 
random head-picking model is offered as the 
baseline. In this baseline model, one of the semantic 
classes of X and Y is randomly chosen as the 
semantic class of the compound X-Y. 
 
Level-3(Small Class)     Level-4(Subclass )  
n outside inside Baseline outside inside Baseline 
1 39.80% 61.60% 18.83% 36.60% 60.40% 17.34% 
2 56.80% 76.00% 31.40% 52.80% 74.40% 29.12% 
3 64.40% 83.80% 40.21% 59.80% 80.80% 37.54% 
Table 1. Performance for 500 V-V compounds 
 
The results in Table 1 show that the system achieves 
a precision rate of 60.40% for inside test and 36.60% 
for outside test in level-4 classification against the 
baseline one of 17.34%. Not to our surprise, the 
performance of classification to level-3, a slightly 
shallower level, is slightly better: 61.60% for inside 
test and 39.80% for outside test. Table 1 also shows 
that the system can achieve a correction rate of 
59.8% (outside) and 80.80% (inside) for including 
the correct answer in the first 3 ranked candidate 
classes in level-4, 64.40% (outside) and 83.80% 
(inside) in level-3, all much better than the baseline 
ones, 37.54% and 40.21%. 
 
4.3 A Pseudo-WSD Problem 
If the correct semantic class can be found in a 
limited number of candidates, context information 
can be used to help determine which candidate is 
more likely to be the proper one, just as a WSD task 
does. Take again the example of the compound ?
> in section 4.1, which the system classifies most 
likely as: ?arrest?, ?acquire? and ?attack-occupy?. 
Obviously the verbs in the three classes should take 
different stereotypes of objects: respectively person, 
thing, and place. Therefore it is not difficult to 
determine the correct semantic class of the verb in 
question by using context information, in this case 
the type of the object. Through this example, we can 
see that the high inclusion rate of the correct answer 
in the top ranked classes has in fact a great 
significance: the ranking of the top candidates can be 
further adjusted and eventually ameliorated by 
context information, and thus the task of class 
determination can become a pseudo-WSD problem, 
in which domain various techniques are well 
available (Manning and Schutze, 1999). The 
performance of the present non-contextual system of 
automatic semantic classification is then expected to 
be improvable with the eventual help of a good 
context-sensitive WSD system, though it is out of 
the scope in this paper. Therefore the correct 
inclusion rate of top n ranked classes is also the 
concern of this paper. 
 
4.4 Endocentric vs. Exocentric Compounds  
Table 2 shows the performance of the system on the 
endocentric compounds (with heads) and on the 
exocentric ones (without heads) in level-3. Among 
the 500 V-V compounds, the endocentric V-V 
compounds have much higher precision rates than 
the exocentric ones. But even for the exocentric 
compounds, the precision rate of the system is 
49.28% in inside test and 27.05% in outside test, 
while the correct inclusion rate of top 3 ranked 
classes achieves 74.64% in inside test and 51.69% in 
outside test. Such a performance is in fact rather 
encouraging since it shows that this model has 
overcome the inherent difficulty met by a 
head-oriented approach. 
 
    Outside inside  
n +Head -Head +Head -Head 
1 48.81% 27.05% 70.69% 49.28% 
2 68.26% 40.58% 84.83% 64.11% 
3 73.38% 51.69% 90.69% 74.64% 
Table 2. Level-3 performance for [+/- Head] V-V 
 
4.5 Syntactic Category Filter 
To test the function of the Filter-C in the model, two 
sets of 500 V-V compounds are randomly picked out 
from verbs of category VC corpus, and from verbs 
of category VA in Sinica Corpus.10. Table 3 and 4 
show the performance of the system on the two 
kinds of verbs when evaluated to level-3. The results 
show that the system using the syntactic category 
filter (+SCF) performs slightly better than that 
without using the filter (-SCF) only in the precision 
of first ranked class in the outside test. Beside that, 
the use of the syntactic category filter generally 
undermines the performance of the system. Such a 
result might be explained by the fact that 
synonymous words in CILIN are not necessarily of 
the same syntactic category; it also suggests that for 
the entire model recall is perhaps more important 
than precision in Module-A. 
 
outside Inside  
n +SCF -SCF +SCF -SCF 
Baseline 
1 49.60% 47.60% 64.40% 67.20% 22.90% 
2 63.20% 64.00% 76.40% 78.40% 39.74% 
3 70.00% 73.60% 84.40% 84.80% 50.27% 
Table 3. Level-3 performance for V-V of category VC 
 
outside inside  
n +SCF -SCF +SCF -SCF 
Baseline 
1 41.00% 38.60% 52.00% 58.60% 15.90% 
2 52.20% 49.60% 67.40% 73.80% 26.84% 
3 55.80% 55.00% 73.00% 80.20% 34.61% 
Table 4. Level-3 performance for V-V of category VA 
 
4.6 Classification Errors 
An examination of the bad performing cases 
suggests that there are three major sources of 
erroneous classification in the experiments. (1) 
Some test compounds are just idiomatic or non 
semantic compositional. Naturally, it is highly 
difficult, if not impossible, to correctly predict their 
semantic classes. (2) Some compounds are from 
unproductive S-templates, which causes the example 
sparseness of the T-similar compounds. The scarcity 
of examples will easily lead to a poor determination 
result caused by a low noise tolerance of occasional 
bad examples. (3) Some classifications predicted by 
the system are reasonable to native speakers, but 
happen not to be the case in CILIN as the standard 
answers. 
 
5. Conclusions and Further Remarks 
In this paper I have proposed a character-based 
model of sense determination for Chinese 
                                                 
10
 VC (transitive action/activity) and VA (intransitive 
action/activity) are the two most dominant types of 
two-character verbs in the corpus, occupying respectively 44% 
and 27% around. Here the statistics does not include the VH 
(intransitive state) verbs, because they generally correspond to 
the adjectives in English, and in deed they are categorized as 
adjective in HowNet. 
compounds using compounding template similarity. 
Based on this model, a system of deep semantic 
classification for V-V compounds is implemented, 
which classifies compounds according to the 
taxonomy of CILIN to its deep-level (level-3 and 
level-4) classes. The evaluation experiment reports a 
fairly satisfactory precision rate of the first ranked 
predicted semantic class (about 38% in outside test 
and 61% in inside test) against the baseline one 
(about 18%). The results also show a high inclusion 
rate of correct answer in the top3 ranked classes, 
which suggests that in the future the present 
non-contextual system can cooperate with a WSD 
module using context information. Though the 
model is only tested on a partial system for V-V 
compounds, it can be extended to work for general 
compounds, like V-N and N-N, with the association 
network further established for N characters. 
The model proposed in this paper has the 
following advantages: (1) It proposes a similarity 
measure of compounding template to retrieve 
potential synonyms for sense approximation, which 
avoids the inherent difficulty of head determination 
in a head-oriented approach and is thus capable of 
handling exocentric compounds. (2) It establishes a 
network of character-sense association, which allows 
the discovery of latent senses of characters, latent 
synonymy, and latent polysemy, thus remedying the 
incompleteness effect of the MRD in use. (3) It can 
carry out deep semantic classification, not just 
shallow classification assigning general and vague 
categories. (4) It requires only a simple format of 
idealized dictionary, which facilitates the conversion 
from a general MRD and allows an easy 
enhancement of the system by adding a new MRD. 
However, as can be remarked in the discussion 
of classification errors, the performance of the model 
relies much on the productivity of compounding 
semantic templates of the target compounds. To 
correctly predict the semantic class of a compound 
with an unproductive semantic template is no doubt 
very difficult due to a sparse existence of the 
T-similar compounds. How to remedy such an effect 
is thus a challenging task in the future. In addition, 
how to generalize the present character-based model 
to make it applicable to compounds with 
multi-character component morphemes will be 
another essential task to undertake. Besides, a task of 
automatic lexical translation for Chinese unknown 
compounds will also be carried out in the future. The 
task can be executed under the very same structure 
of the present model, since the only difference will 
be the change of working dicox (from dico2 to dico1) 
in the Module-B. A pilot experiment has already 
shown encouraging results. 
References 
Chao-Jan Chen, Ming-hong Bai, and Keh-Jiann Chen. 
1997. Category Guessing for Chinese Unknown Words. 
In Proceedings of the Natural Language Processing 
Pacific Rim Symposium 1997, pages 35-40. 
Hsin-Hsi Chen and Chi-Ching Lin. 2000. Sense-tagging 
Chinese corpus. In Proceedings of ACL-2000 workshop 
on Chinese Language Processing, pages 7-14. 
Keh-Jiann Chen and Chao-Jan Chen. 1998. A Corpus 
Based Study on Computational Morphology for 
Mandarin Chinese. In Quantitative and Computational 
Studies on the Chinese Language, pages 283-306. 
Keh-Jiann Chen and Chao-Jan Chen. 2000. Automatic 
Semantic Classification for Chinese Unknown 
Compound Nouns, In Proceedings of Coling-2000, 
pages 173-179. 
Keh-Jiann Chen and Jia-Ming You. 2002. A Study on 
Word Similarity Using Context Vector Models, 
Computational Linguistics & Chinese Language  
Processing, 8(2):37-58. 
Chu-Ren Huang et al 1995. The Introduction of Sinica 
Corpus. In Proceedings of ROCLING VIII, pages 
81-99. 
Dekang Lin. 1998a. An Information-Theoretic Definition 
of Similarity, In Proceedings of International 
Conference on Machine Learning, pages 296-304 
Dekang Lin. 1998b. Automatic retrieval and clustering of 
similar words. In Proceedings of COLING/ACL-98, 
pages 768?774. 
Kim-Teng Lua. 1997. Prediction of Meaning of 
Bi-syllabic Chinese Compound Words Using Back 
Propagation Neural Network. In Computer Processing 
of Oriental Languages, 11(2):133-144. 
Christopher Manning and Hinrich Sch?tze. 1999. 
Fondations of Statistical Natural Language Processing, 
MIT Press. 
Jia-Ju Mei et al 1984. TonYiCi CiLin ? thesaurus of 
Chinese words (????<), Shangwu Yinshuguan 
(OD??`???), Hong Kong.  
Sabine Ploux and Bernard Victorri. 1998. Construction 
d?espace s?mantique ? l?aide de dictionnaires de 
synonymes. Traitement Automatique des Langues, 
39(1):161-182. 
Sabine Ploux and Hyungsuk Ji. 2003. A Model for 
Matching Semantic Maps Between Languages 
(French/English, English/French). Computational 
Linguistics, 29(2):155-178. 
Philip Resnik. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. In 
Proceedings of the 14th International Joint Conference 
on Artificial Intelligence (IJCAI), pages 448-453. 
Yorick Wilks and Mark Stevenson. 1997. Sense Tagging: 
Semantic Tagging with a Lexicon. In Proceedings of 
the SIGLEX Workshop on Tagging Text with Lexical 
Semantics, pages 47-51. 
David Yarowsky. 1992. Word-Sense Disambiguation 
Using Statistical Models of Rogets Categories Trained 
on Large Corpora?, In Proceedings of COLING-92, 
pages 454-460. 
