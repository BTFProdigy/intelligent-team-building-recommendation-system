Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281?1290,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Dependency Parsing without Gold Part-of-Speech Tags
Valentin I. Spitkovsky??
valentin@cs.stanford.edu
Hiyan Alshawi?
hiyan@google.com
Angel X. Chang??
angelx@cs.stanford.edu
Daniel Jurafsky??
jurafsky@stanford.edu
?Computer Science Department
Stanford University
Stanford, CA, 94305
?Google Research
Google Inc.
Mountain View, CA, 94043
?Department of Linguistics
Stanford University
Stanford, CA, 94305
Abstract
We show that categories induced by unsuper-
vised word clustering can surpass the perfor-
mance of gold part-of-speech tags in depen-
dency grammar induction. Unlike classic clus-
tering algorithms, our method allows a word
to have different tags in different contexts.
In an ablative analysis, we first demonstrate
that this context-dependence is crucial to the
superior performance of gold tags ? requir-
ing a word to always have the same part-of-
speech significantly degrades the performance
of manual tags in grammar induction, elim-
inating the advantage that human annotation
has over unsupervised tags. We then introduce
a sequence modeling technique that combines
the output of a word clustering algorithm with
context-colored noise, to allow words to be
tagged differently in different contexts. With
these new induced tags as input, our state-of-
the-art dependency grammar inducer achieves
59.1% directed accuracy on Section 23 (all
sentences) of the Wall Street Journal (WSJ)
corpus ? 0.7% higher than using gold tags.
1 Introduction
Unsupervised learning ? machine learning without
manually-labeled training examples ? is an active
area of scientific research. In natural language pro-
cessing, unsupervised techniques have been success-
fully applied to tasks such as word alignment for ma-
chine translation. And since the advent of the web,
algorithms that induce structure from unlabeled data
have continued to steadily gain importance. In this
paper we focus on unsupervised part-of-speech tag-
ging and dependency parsing ? two related prob-
lems of syntax discovery. Our methods are applica-
ble to vast quantities of unlabeled monolingual text.
Not all research on these problems has been fully
unsupervised. For example, to the best of our knowl-
edge, every new state-of-the-art dependency gram-
mar inducer since Klein and Manning (2004) relied
on gold part-of-speech tags. For some time, multi-
point performance degradations caused by switching
to automatically induced word categories have been
interpreted as indications that ?good enough? parts-
of-speech induction methods exist, justifying the fo-
cus on grammar induction with supervised part-of-
speech tags (Bod, 2006), pace (Cramer, 2007). One
of several drawbacks of this practice is that it weak-
ens any conclusions that could be drawn about how
computers (and possibly humans) learn in the ab-
sence of explicit feedback (McDonald et al, 2011).
In turn, not all unsupervised taggers actually in-
duce word categories: Many systems ? known as
part-of-speech disambiguators (Merialdo, 1994) ?
rely on external dictionaries of possible tags. Our
work builds on two older part-of-speech inducers
? word clustering algorithms of Clark (2000) and
Brown et al (1992) ? that were recently shown to
be more robust than other well-known fully unsuper-
vised techniques (Christodoulopoulos et al, 2010).
We investigate which properties of gold part-of-
speech tags are useful in grammar induction and
parsing, and how these properties could be intro-
duced into induced tags. We also explore the number
of word classes that is good for grammar induction:
in particular, whether categorization is needed at all.
By removing the ?unrealistic simplification? of us-
ing gold tags (Petrov et al, 2011, ?3.2, Footnote 4),
we will go on to demonstrate why grammar induc-
tion from plain text is no longer ?still too difficult.?
1281
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0? ?? ?
PSTOP(, L, T)) ? PATTACH(, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(, L, F)? ?? ?
1
? PSTOP(, R, T)? ?? ?
1
.
Figure 1: A dependency structure for a short WSJ sen-
tence and its probability, factored by the DMV, using gold
tags, after summing out PORDER (Spitkovsky et al, 2009).
2 Methodology
In all experiments, we model the English grammar
via Klein and Manning?s (2004) Dependency Model
with Valence (DMV), induced from subsets of not-
too-long sentences of the Wall Street Journal (WSJ).
2.1 The Model
The original DMV is a single-state head automata
model (Alshawi, 1996) over lexical word classes
{cw} ? gold part-of-speech tags. Its generative story
for a sub-tree rooted at a head (of class ch) rests on
three types of independent decisions: (i) initial di-
rection dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e., ad-
jacent, child); and (iii) attachments (of class ca), ac-
cording to PATTACH(ch, dir, ca). This recursive process
produces only projective trees. A root token ? gen-
erates the head of the sentence as its left (and only)
child (see Figure 1 for a simple, concrete example).
2.2 Learning Algorithms
The DMV lends itself to unsupervised learning via
inside-outside re-estimation (Baker, 1979). Klein
and Manning (2004) initialized their system using an
?ad-hoc harmonic? completion, followed by training
using 40 steps of EM (Klein, 2005). We reproduce
this set-up, iterating without actually verifying con-
vergence, in most of our experiments (#1?4, ?3?4).
Experiments #5?6 (?5) employ our new state-of-
the-art grammar inducer (Spitkovsky et al, 2011),
which uses constrained Viterbi EM (details in ?5).
2.3 Training Data
The DMV is usually trained on a customized sub-
set of Penn English Treebank?s Wall Street Jour-
nal portion (Marcus et al, 1993). Following Klein
and Manning (2004), we begin with reference con-
stituent parses, prune out all empty sub-trees and
remove punctuation and terminals (tagged # and $)
that are not pronounced where they appear. We then
train only on the remaining sentence yields consist-
ing of no more than fifteen tokens (WSJ15), in most
of our experiments (#1?4, ?3?4); by contrast, Klein
and Manning?s (2004) original system was trained
using less data: sentences up to length ten (WSJ10).1
Our final experiments (#5?6, ?5) employ a simple
scaffolding strategy (Spitkovsky et al, 2010a) that
follows up initial training at WSJ15 (?less is more?)
with an additional training run (?leapfrog?) that in-
corporates most sentences of the data set, at WSJ45.
2.4 Evaluation Methods
Evaluation is against the training set, as is standard
practice in unsupervised learning, in part because
Klein and Manning (2004, ?3) did not smooth the
DMV (Klein, 2005, ?6.2). For most of our experi-
ments (#1?4, ?3?4), this entails starting with the ref-
erence trees from WSJ15 (as modified in ?2.3), au-
tomatically converting their labeled constituents into
unlabeled dependencies using deterministic ?head-
percolation? rules (Collins, 1999), and then com-
puting (directed) dependency accuracy scores of the
corresponding induced trees. We report overall per-
centages of correctly guessed arcs, including the
arcs from sentence root symbols, as is standard prac-
tice (Paskin, 2001; Klein and Manning, 2004).
For a meaningful comparison with previous work,
we also test some of the models from our earlier ex-
periments (#1,3) ? and both models from final ex-
periments (#5,6) ? against Section 23 of WSJ?, af-
ter applying Laplace (a.k.a. ?add one?) smoothing.
1WSJ15 contains 15,922 sentences up to length fifteen (a to-
tal of 163,715 tokens, not counting punctuation) ? versus 7,422
sentences of at most ten words (only 52,248 tokens) comprising
WSJ10 ? and is a better trade-off between the quantity and
complexity of training data in WSJ (Spitkovsky et al, 2009).
1282
Accuracy Viable
1. manual tags Unsupervised Sky Groups
gold 50.7 78.0 36
mfc 47.2 74.5 34
mfp 40.4 76.4 160
ua 44.3 78.4 328
2. tagless lexicalized models
full 25.8 97.3 49,180
partial 29.3 60.5 176
none 30.7 24.5 1
3. tags from a flat (Clark, 2000) clustering
47.8 83.8 197
4. prefixes of a hierarchical (Brown et al, 1992) clustering
first 7 bits 46.4 73.9 96
8 bits 48.0 77.8 165
9 bits 46.8 82.3 262
Table 1: Directed accuracies for the ?less is more? DMV,
trained on WSJ15 (after 40 steps of EM) and evaluated
also against WSJ15, using various lexical categories in
place of gold part-of-speech tags. For each tag-set, we
include its effective number of (non-empty) categories in
WSJ15 and the oracle skylines (supervised performance).
3 Motivation and Ablative Analyses
The concepts of polysemy and synonymy are of fun-
damental importance in linguistics. For words that
can take on multiple parts of speech, knowing the
gold tag can reduce ambiguity, improving parsing by
limiting the search space. Furthermore, pooling the
statistics of words that play similar syntactic roles,
as signaled by shared gold part-of-speech tags, can
simplify the learning task, improving generalization
by reducing sparsity. We begin with two sets of ex-
periments that explore the impact that each of these
factors has on grammar induction with the DMV.
3.1 Experiment #1: Human-Annotated Tags
Our first set of experiments attempts to isolate the
effect that replacing gold part-of-speech tags with
deterministic one class per word mappings has on
performance, quantifying the cost of switching to a
monosemous clustering (see Table 1: manual; and
Table 4). Grammar induction with gold tags scores
50.7%, while the oracle skyline (an ideal, supervised
instance of the DMV) could attain 78.0% accuracy.
It may be worth noting that only 6,620 (13.5%) of
49,180 unique tokens in WSJ appear with multiple
part-of-speech tags. Most words, like it, are always
tagged the same way (5,768 times PRP). Some words,
token mfc mfp ua
it {PRP} {PRP} {PRP}
gains {NNS} {VBZ, NNS} {VBZ, NNS}
the {DT} {JJ, DT} {VBP, NNP, NN, JJ, DT, CD}
Table 2: Example most frequent class, most frequent pair
and union all reassignments for tokens it, the and gains.
like gains, usually serve as one part of speech (227
times NNS, as in the gains) but are occasionally used
differently (5 times VBZ, as in he gains). Only 1,322
tokens (2.7%) appear with three or more different
gold tags. However, this minority includes the most
frequent word ? the (50,959 times DT, 7 times JJ,
6 times NNP and once as each of CD, NN and VBP).2
We experimented with three natural reassign-
ments of part-of-speech categories (see Table 2).
The first, most frequent class (mfc), simply maps
each token to its most common gold tag in the entire
WSJ (with ties resolved lexicographically). This ap-
proach discards two gold tags (types PDT and RBR are
not most common for any of the tokens in WSJ15)
and costs about three-and-a-half points of accuracy,
in both supervised and unsupervised regimes.
Another reassignment, union all (ua), maps each
token to the set of all of its observed gold tags, again
in the entire WSJ. This inflates the number of group-
ings by nearly a factor of ten (effectively lexicaliz-
ing the most ambiguous words),3 yet improves the
oracle skyline by half-a-point over actual gold tags;
however, learning is harder with this tag-set, losing
more than six points in unsupervised training.
Our last reassignment, most frequent pair (mfp),
allows up to two of the most common tags into
a token?s label set (with ties, once again, resolved
lexicographically). This intermediate approach per-
forms strictly worse than union all, in both regimes.
3.2 Experiment #2: Lexicalization Baselines
Our next set of experiments assesses the benefits of
categorization, turning to lexicalized baselines that
avoid grouping words altogether. All three models
discussed below estimated the DMV without using
the gold tags in any way (see Table 1: lexicalized).
2Some of these are annotation errors in the treebank (Banko
and Moore, 2004, Figure 2): such (mis)taggings can severely
degrade the accuracy of part-of-speech disambiguators, without
additional supervision (Banko and Moore, 2004, ?5, Table 1).
3Kupiec (1992) found that the 50,000-word vocabulary of
the Brown corpus similarly reduces to ?400 ambiguity classes.
1283
First, not surprisingly, a fully-lexicalized model
over nearly 50,000 unique words is able to essen-
tially memorize the training set, supervised. (With-
out smoothing, it is possible to deterministically at-
tach most rare words in a dependency tree correctly,
etc.) Of course, local search is unlikely to find good
instantiations for so many parameters, causing unsu-
pervised accuracy for this model to drop in half.
For our next experiment, we tried an intermediate,
partially-lexicalized approach. We mapped frequent
words ? those seen at least 100 times in the training
corpus (Headden et al, 2009) ? to their own indi-
vidual categories, lumping the rest into a single ?un-
known? cluster, for a total of under 200 groups. This
model is significantly worse for supervised learn-
ing, compared even with the monosemous clusters
derived from gold tags; yet it is only slightly more
learnable than the broken fully-lexicalized variant.
Finally, for completeness, we trained a model that
maps every token to the same one ?unknown? cat-
egory. As expected, such a trivial ?clustering? is
ineffective in supervised training; however, it out-
performs both lexicalized variants unsupervised,4
strongly suggesting that lexicalization alone may be
insufficient for the DMV and hinting that some de-
gree of categorization is essential to its learnability.
Cluster #173 Cluster #188
1. open 1. get
2. free 2. make
3. further 3. take
4. higher 4. find
5. lower 5. give
6. similar 6. keep
7. leading 7. pay
8. present 8. buy
9. growing 9. win
10. increased 10. sell
.
.
.
.
.
.
37. cool 42. improve
.
.
.
.
.
.
1,688. up-wind 2,105. zero-out
Table 3: Representative members for two of the flat word
groupings: cluster #173 (left) contains adjectives, espe-
cially ones that take comparative (or other) complements;
cluster #188 comprises bare-stem verbs (infinitive stems).
(Of course, many of the words have other syntactic uses.)
4Note that it also beats supervised training. That isn?t a bug:
Spitkovsky et al (2010b, ?7.2) explain this paradox in the DMV.
4 Grammars over Induced Word Clusters
We have demonstrated the need for grouping simi-
lar words, estimated a bound on performance losses
due to monosemous clusterings and are now ready
to experiment with induced part-of-speech tags. We
use two sets of established, publicly-available hard
clustering assignments, each computed from a much
larger data set than WSJ (approximately a million
words). The first is a flat mapping (200 clusters)
constructed by training Clark?s (2000) distributional
similarity model over several hundred million words
from the British National and the English Gigaword
corpora.5 The second is a hierarchical clustering ?
binary strings up to eighteen bits long ? constructed
by running Brown et al?s (1992) algorithm over 43
million words from the BLLIP corpus, minus WSJ.6
4.1 Experiment #3: A Flat Word Clustering
Our main purely unsupervised results are with a flat
clustering (Clark, 2000) that groups words having
similar context distributions, according to Kullback-
Leibler divergence. (A word?s context is an ordered
pair: its left- and right-adjacent neighboring words.)
To avoid overfitting, we employed an implemen-
tation from previous literature (Finkel and Manning,
2009). The number of clusters (200) and the suf-
ficient amount of training data (several hundred-
million words) were tuned to a task (NER) that is
not directly related to dependency parsing. (Table 3
shows representative entries for two of the clusters.)
We added one more category (#0) for unknown
words. Now every token in WSJ could again be re-
placed by a coarse identifier (one of at most 201,
instead of just 36), in both supervised and unsuper-
vised training. (Our training code did not change.)
The resulting supervised model, though not as
good as the fully-lexicalized DMV, was more than
five points more accurate than with gold part-of-
speech tags (see Table 1: flat). Unsupervised accu-
racy was lower than with gold tags (see also Table 4)
but higher than with all three derived hard assign-
ments. This suggests that polysemy (i.e., ability to
5http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
6http://people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
1284
1 4 16 64 256 1,024 (# of clusters) 49,180
20
40
60
80
%
gold
mfc mfp ua
full
partial
none
flat
gold
mfc
mfp
ua
full
partial
none
flat
k = 1 2 3 4 5 6 7 8 9 10 11 12 ? 18 bits
Figure 2: Parsing performance (accuracy on WSJ15) as a ?function? of the number of syntactic categories, for all prefix
lengths ? k ? {1, . . . , 18} ? of a hierarchical (Brown et al, 1992) clustering, connected by solid lines (dependency
grammar induction in blue; supervised oracle skylines in red, above). Tagless lexicalized models (full, partial and
none) connected by dashed lines. Models based on gold part-of-speech tags, and derived monosemous clusters (mfc,
mfp and ua), shown as vertices of gold polygons. Models based on a flat (Clark, 2000) clustering indicated by squares.
tag a word differently in context) may be the primary
advantage of manually constructed categorizations.
4.2 Experiment #4: A Hierarchical Clustering
The purpose of this batch of experiments is to show
that Clark?s (2000) algorithm isn?t unique in its suit-
ability for grammar induction. We found that Brown
et al?s (1992) older information-theoretic approach,
which does not explicitly address the problems of
rare and ambiguous words (Clark, 2000) and was de-
signed to induce large numbers of plausible syntac-
tic and semantic clusters, can perform just as well.
Once again, the sufficient amount of data (43 mil-
lion words) was tuned in earlier work (Koo, 2010).
His task of interest was, in fact, dependency parsing.
But since this algorithm is hierarchical (i.e., there
isn?t a parameter for the number of categories), we
doubt that there was a strong enough risk of overfit-
ting to question the clustering?s unsupervised nature.
As there isn?t a set number of categories, we used
binary prefixes of length k from each word?s address
in the computed hierarchy as cluster labels. Results
for 7 ? k ? 9 bits (approximately 100?250 non-
empty clusters, close to the 200 we used before) are
similar to those of flat clusters (see Table 1: hierar-
chical). Outside of this range, however, performance
can be substantially worse (see Figure 2), consistent
with earlier findings: Headden et al (2008) demon-
strated that (constituent) grammar induction, using
the singular-value decomposition (SVD-based) tag-
ger of Schu?tze (1995), also works best with 100?200
clusters. Important future research directions may
include learning to automatically select a good num-
ber of word categories (in the case of flat clusterings)
and ways of using multiple clustering assignments,
perhaps of different granularities/resolutions, in tan-
dem (e.g., in the case of a hierarchical clustering).
4.3 Further Evaluation
It is important to enable easy comparison with pre-
vious and future work. Since WSJ15 is not a stan-
dard test set, we evaluated two key experiments ?
?less is more? with gold part-of-speech tags (#1, Ta-
ble 1: gold) and with Clark?s (2000) clusters (#3, Ta-
ble 1: flat) ? on all sentences (not just length fifteen
and shorter), in Section 23 of WSJ (see Table 4).
This required smoothing both final models (?2.4).
We showed that two classic unsupervised word
1285
System Description Accuracy
#1 (?3.1) ?less is more? (Spitkovsky et al, 2009) 44.0
#3 (?4.1) ?less is more? with monosemous induced tags 41.4 (-2.6)
Table 4: Directed accuracies on Section 23 of WSJ (all sentences) for two experiments with the base system.
clusterings ? one flat and one hierarchical ? can
be better for dependency grammar induction than
monosemous syntactic categories derived from gold
part-of-speech tags. And we confirmed that the un-
supervised tags are worse than the actual gold tags,
in a simple dependency grammar induction system.
5 State-of-the-Art without Gold Tags
Until now, we have deliberately kept our experimen-
tal methods simple and nearly identical to Klein and
Manning?s (2004), for clarity. Next, we will explore
how our main findings generalize beyond this toy
setting. A preliminary test will simply quantify the
effect of replacing gold part-of-speech tags with the
monosemous flat clustering (as in experiment #3,
?4.1) on a modern grammar inducer. And our last
experiment will gauge the impact of using a polyse-
mous (but still unsupervised) clustering instead, ob-
tained by executing standard sequence labeling tech-
niques to introduce context-sensitivity into the origi-
nal (independent) assignment or words to categories.
These final experiments are with our latest state-
of-the-art system (Spitkovsky et al, 2011) ? a par-
tially lexicalized extension of the DMV that uses
constrained Viterbi EM to train on nearly all of the
data available in WSJ, at WSJ45 (48,418 sentences;
986,830 non-punctuation tokens). The key contribu-
tion that differentiates this model from its predeces-
sors is that it incorporates punctuation into grammar
induction (by turning it into parsing constraints, in-
stead of ignoring punctuation marks altogether). In
training, the model makes a simplifying assumption
? that sentences can be split at punctuation and that
the resulting fragments of text could be parsed inde-
pendently of one another (these parsed fragments are
then reassembled into full sentence trees, by pars-
ing the sequence of their own head words). Fur-
thermore, the model continues to take punctuation
marks into account in inference (using weaker, more
accurate constraints, than in training). This system
scores 58.4% on Section 23 of WSJ? (see Table 5).
5.1 Experiment #5: A Monosemous Clustering
As in experiment #3 (?4.1), we modified the base
system in exactly one way: we swapped out gold
part-of-speech tags and replaced them with a flat dis-
tributional similarity clustering. In contrast to sim-
pler models, which suffer multi-point drops in ac-
curacy from switching to unsupervised tags (e.g.,
2.6%), our new system?s performance degrades only
slightly, by 0.2% (see Tables 4 and 5). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).7
One risk that arises from using gold tags is that
newer systems could be finding cleverer ways to ex-
ploit manual labels (i.e., developing an over-reliance
on gold tags) instead of actually learning to acquire
language. Part-of-speech tags are known to contain
significant amounts of information for unlabeled de-
pendency parsing (McDonald et al, 2011, ?3.1), so
we find it reassuring that our latest grammar inducer
is less dependent on gold tags than its predecessors.
5.2 Experiment #6: A Polysemous Clustering
Results of experiments #1 and 3 (?3.1, 4.1) suggest
that grammar induction stands to gain from relaxing
the one class per word assumption. We next test this
conjecture by inducing a polysemous unsupervised
word clustering, then using it to induce a grammar.
Previous work (Headden et al, 2008, ?4) found
that simple bitag hidden Markov models, classically
trained using the Baum-Welch (Baum, 1972) variant
of EM (HMM-EM), perform quite well,8 on aver-
age, across different grammar induction tasks. Such
sequence models incorporate a sensitivity to context
via state transition probabilities PTRAN(ti | ti?1), cap-
turing the likelihood that a tag ti immediately fol-
lows the tag ti?1; emission probabilities PEMIT(wi | ti)
capture the likelihood that a word of type ti is wi.
7We also briefly comment on this result in the ?punctuation?
paper (Spitkovsky et al, 2011, ?7), published concurrently.
8They are also competitive with Bayesian estimators, on
larger data sets, with cross-validation (Gao and Johnson, 2008).
1286
System Description Accuracy
(?5) ?punctuation? (Spitkovsky et al, 2011) 58.4
#5 (?5.1) ?punctuation? with monosemous induced tags 58.2 (-0.2)
#6 (?5.2) ?punctuation? with context-sensitive induced tags 59.1 (+0.7)
Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system.
We need a context-sensitive tagger, and HMM
models are good ? relative to other tag-inducers.
However, they are not better than gold tags, at least
when trained using a modest amount of data.9 For
this reason, we decided to relax the monosemous
flat clustering, plugging it in as an initializer for the
HMM. The main problem with this approach is that,
at least without smoothing, every monosemous la-
beling is trivially at a local optimum, since P(ti | wi)
is deterministic. To escape the initial assignment,
we used a ?noise injection? technique (Selman et
al., 1994), inspired by the contexts of Clark (2000).
First, we collected the MLE statistics for PR(ti+1 | ti)
and PL(ti | ti+1) in WSJ, using the flat monosemous
tags. Next, we replicated the text of WSJ 100-fold.
Finally, we retagged this larger data set, as follows:
with probability 80%, a word kept its monosemous
tag; with probability 10%, we sampled a new tag
from the left context (PL) associated with the origi-
nal (monosemous) tag of its rightmost neighbor; and
with probability 10%, we drew a tag from the right
context (PR) of its leftmost neighbor.10 Given that
our initializer ? and later the input to the grammar
inducer ? are hard assignments of tags to words, we
opted for (the faster and simpler) Viterbi training.
In the spirit of reproducibility, we again used an
off-the-shelf component for tagging-related work.11
Viterbi training converged after just 17 steps, re-
placing the original monosemous tags for 22,280 (of
1,028,348 non-punctuation) tokens in WSJ. For ex-
9All of Headden et al?s (2008) grammar induction experi-
ments with induced parts-of-speech were worse than their best
results using gold part-of-speech tags, most likely because they
used a very small corpus (half of WSJ10) to cluster words.
10We chose the sampling split (80:10:10) and replication pa-
rameter (100) somewhat arbitrarily, so better results could likely
be obtained with tuning. However, we suspect that the real gains
would come from using soft clustering techniques (Hinton and
Roweis, 2003; Pereira et al, 1993, inter alia) and propagating
(joint) estimates of tag distributions into a parser. Our ad-hoc
approach is intended to serve solely as a proof of concept.
11David Elworthy?s C+ tagger, with options -i t -G -l,
available from http://friendly-moose.appspot.com/
code/NewCpTag.zip.
ample, the first changed sentence is #3 (of 49,208):
Some ?circuit breakers? installed after
the October 1987 crash failed their first
test, traders say, unable to cool the selling
panic in both stocks and futures.
Above, the word cool gets relabeled as #188 (from
#173 ? see Table 3), since its context is more
suggestive of an infinitive verb than of its usual
grouping with adjectives. (A proper analysis of all
changes, however, is beyond the scope of this work.)
Using this new context-sensitive hard assignment
of tokens to unsupervised categories our gram-
mar inducer attained a directed accuracy of 59.1%,
nearly a full point better than with the monosemous
hard assignment (see Table 5). To the best of our
knowledge it is also the first state-of-the-art unsuper-
vised dependency parser to perform better with in-
duced categories than with gold part-of-speech tags.
6 Related Work
Early work in dependency grammar induction al-
ready relied on gold part-of-speech tags (Carroll and
Charniak, 1992). Some later models (Yuret, 1998;
Paskin, 2001, inter alia) attempted full lexicaliza-
tion. However, Klein and Manning (2004) demon-
strated that effort to be worse at recovering depen-
dency arcs than choosing parse structures at random,
leading them to incorporate gold tags into the DMV.
Klein and Manning (2004, ?5, Figure 6) had also
tested their own models with induced word classes,
constructed using a distributional similarity cluster-
ing method (Schu?tze, 1995). Without gold part-of-
speech tags, their combined DMV+CCM model was
about five points worse, both in (directed) unlabeled
dependency accuracy (42.3% vs. 47.5%)12 and unla-
beled bracketing F1 (72.9% vs. 77.6%), on WSJ10.
In constituent parsing, earlier Seginer (2007a, ?6,
Table 1) built a fully-lexicalized grammar inducer
12On the same evaluation set (WSJ10), our context-sensitive
system without gold tags (Experiment #6, ?5.2) scores 66.8%.
1287
that was competitive with DMV+CCM despite not
using gold tags. His CCL parser has since been
improved via a ?zoomed learning? technique (Re-
ichart and Rappoport, 2010). Moreover, Abend et
al. (2010) reused CCL?s internal distributional rep-
resentation of words in a cognitively-motivated part-
of-speech inducer. Unfortunately their tagger did
not make it into Christodoulopoulos et al?s (2010)
excellent and otherwise comprehensive evaluation.
Outside monolingual grammar induction, fully-
lexicalized statistical dependency transduction mod-
els have been trained from unannotated parallel bi-
texts for machine translation (Alshawi et al, 2000).
More recently, McDonald et al (2011) demonstrated
an impressive alternative to grammar induction by
projecting reference parse trees from languages that
have annotations to ones that are resource-poor.13 It
uses graph-based label propagation over a bilingual
similarity graph for a sentence-aligned parallel cor-
pus (Das and Petrov, 2011), inducing part-of-speech
tags from a universal tag-set (Petrov et al, 2011).
Even in supervised parsing we are starting to see
a shift away from using gold tags. For example,
Alshawi et al (2011) demonstrated good results for
mapping text to underspecified semantics via depen-
dencies without resorting to gold tags. And Petrov et
al. (2010, ?4.4, Table 4) observed only a small per-
formance loss ?going POS-less? in question parsing.
We are not aware of any systems that induce both
syntactic trees and their part-of-speech categories.
However, aside from the many systems that induce
trees from gold tags, there are also unsupervised
methods for inducing syntactic categories from gold
trees (Finkel et al, 2007; Pereira et al, 1993), as
well as for inducing dependencies from gold con-
stituent annotations (Sangati and Zuidema, 2009;
Chiang and Bikel, 2002). Considering that Headden
et al?s (2008) study of part-of-speech taggers found
no correlation between standard tagging metrics and
the quality of induced grammars, it may be time for
a unified treatment of these very related syntax tasks.
13When the target language is English, however, their best ac-
curacy (projected from Greek) is low: 45.7% (McDonald et al,
2011, ?4, Table 2); tested on the same CoNLL 2007 evaluation
set (Nivre et al, 2007), our ?punctuation? system with context-
sensitive induced tags (trained on WSJ45, without gold tags)
performs substantially better, scoring 51.6%. Note that this is
also an improvement over our system trained on the CoNLL set
using gold tags: 50.3% (Spitkovsky et al, 2011, ?8, Table 6).
7 Discussion and Conclusions
Unsupervised word clustering techniques of Brown
et al (1992) and Clark (2000) are well-suited to de-
pendency parsing with the DMV. Both methods out-
perform gold parts-of-speech in supervised modes.
And both can do better than monosemous clusters
derived from gold tags in unsupervised training. We
showed how Clark?s (2000) flat tags can be relaxed,
using context, with the resulting polysemous cluster-
ing outperforming gold part-of-speech tags for the
English dependency grammar induction task.
Monolingual evaluation is a significant flaw in our
methodology, however. One (of many) take-home
points made in Christodoulopoulos et al?s (2010)
study is that results on one language do not neces-
sarily correlate with other languages.14 Assuming
that our results do generalize, it will still remain to
remove the present reliance on gold tokenization and
sentence boundary labels. Nevertheless, we feel that
eliminating gold tags is an important step towards
the goal of fully-unsupervised dependency parsing.
We have cast the utility of a categorization scheme
as a combination of two effects on parsing accuracy:
a synonymy effect and a polysemy effect. Results
of our experiments with both full and partial lexi-
calization suggest that grouping similar words (i.e.,
synonymy) is vital to grammar induction with the
DMV. This is consistent with an established view-
point, that simple tabulation of frequencies of words
participating in certain configurations cannot be reli-
ably used for comparing their likelihoods (Pereira et
al., 1993, ?4.2): ?The statistics of natural languages
is inherently ill defined. Because of Zipf?s law, there
is never enough data for a reasonable estimation of
joint object distributions.? Seginer?s (2007b, ?1.4.4)
argument, however, is that the Zipfian distribution
? a property of words, not parts-of-speech ?
should allow frequent words to successfully guide
14Furthermore, it would be interesting to know how sensitive
different head-percolation schemes (Yamada and Matsumoto,
2003; Johansson and Nugues, 2007) would be to gold versus
unsupervised tags, since the Magerman-Collins rules (Mager-
man, 1995; Collins, 1999) agree with gold dependency annota-
tions only 85% of the time, even for WSJ (Sangati and Zuidema,
2009). Proper intrinsic evaluation of dependency grammar in-
ducers is not yet a solved problem (Schwartz et al, 2011).
1288
parsing and learning: ?A relatively small number of
frequent words appears almost everywhere and most
words are never too far from such a frequent word
(this is also the principle behind successful part-of-
speech induction).? We believe that it is important to
thoroughly understand how to reconcile these only
seemingly conflicting insights, balancing them both
in theory and in practice. A useful starting point may
be to incorporate frequency information in the pars-
ing models directly ? in particular, capturing the
relationships between words of various frequencies.
The polysemy effect appears smaller but is less
controversial: Our experiments suggest that the pri-
mary drawback of the classic clustering schemes
stems from their one class per word nature ? and
not a lack of supervision, as may be widely believed.
Monosemous groupings, even if they are themselves
derived from human-annotated syntactic categories,
simply cannot disambiguate words the way gold tags
can. By relaxing Clark?s (2000) flat clustering, us-
ing contextual cues, we improved dependency gram-
mar induction: directed accuracy on Section 23 (all
sentences) of the WSJ benchmark increased from
58.2% to 59.1% ? from slightly worse to better than
with gold tags (58.4%, previous state-of-the-art).
Since Clark?s (2000) word clustering algorithm is
already context-sensitive in training, we suspect that
one could do better simply by preserving the polyse-
mous nature of its internal representation. Importing
the relevant distributions into a sequence tagger di-
rectly would make more sense than going through an
intermediate monosemous summary. And exploring
other uses of soft clustering algorithms ? perhaps as
inputs to part-of-speech disambiguators ? may be
another fruitful research direction. We believe that
a joint treatment of grammar and parts-of-speech in-
duction could fuel major advances in both tasks.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Spence Green,
David McClosky and the anonymous reviewers for many help-
ful comments on draft versions of this paper.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In ACL.
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26.
H. Alshawi, P.-C. Chang, and M. Ringgaard. 2011. De-
terministic statistical mapping of sentences to under-
specied semantics. In IWCS.
H. Alshawi. 1996. Head automata for speech translation.
In ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M. Banko and R. C. Moore. 2004. Part of speech tagging
in context. In COLING.
L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. In Inequalities.
R. Bod. 2006. An all-subtrees approach to unsupervised
parsing. In COLING-ACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
D. Chiang and D. M. Bikel. 2002. Recovering latent
information in treebanks. In COLING.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL: Student Research.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In ACL.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In EMNLP.
1289
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor
embedding. In NIPS.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
T. Koo. 2010. Advances in Discriminative Dependency
Parsing. Ph.D. thesis, MIT.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden Markov model. Computer Speech and Lan-
guage, 6.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In ACL.
S. Petrov, P.-C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv.
R. Reichart and A. Rappoport. 2010. Improved fully un-
supervised parsing with zoomed learning. In EMNLP.
F. Sangati and W. Zuidema. 2009. Unsupervised meth-
ods for head assignments. In EACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
Y. Seginer. 2007a. Fast unsupervised incremental pars-
ing. In ACL.
Y. Seginer. 2007b. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise
strategies for improving local search. In AAAI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More? in
unsupervised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011.
Punctuation: Making a point in unsupervised depen-
dency parsing. In CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
D. Yuret. 1998. Discovery of Linguistic Relations Using
Lexical Attraction. Ph.D. thesis, MIT.
1290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 489?500, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Entity and Event Coreference Resolution across Documents
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan Jurafsky
Stanford University, Stanford, CA 94305
{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.edu
Abstract
We introduce a novel coreference resolution
system that models entities and events jointly.
Our iterative method cautiously constructs
clusters of entity and event mentions using lin-
ear regression to model cluster merge opera-
tions. As clusters are built, information flows
between entity and event clusters through fea-
tures that model semantic role dependencies.
Our system handles nominal and verbal events
as well as entities, and our joint formulation
allows information from event coreference to
help entity coreference, and vice versa. In a
cross-document domain with comparable doc-
uments, joint coreference resolution performs
significantly better (over 3 CoNLL F1 points)
than two strong baselines that resolve entities
and events separately.
1 Introduction
Most coreference resolution systems focus on enti-
ties and tacitly assume a correspondence between
entities and noun phrases (NPs). Focusing on NPs
is a way to restrict the challenging problem of coref-
erence resolution, but misses coreference relations
like the one between hanged and his suicide in (1),
and between placed and put in (2).
1. (a) One of the key suspected Mafia bosses ar-
rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.
2. (a) The New Orleans Saints placed Reggie Bush
on the injured list on Wednesday.
(b) Saints put Bush on I.R.
As (1c) shows, NPs can also refer to events, and
so corefer with phrases other than NPs (Webber,
1988). By being anchored in spatio-temporal dimen-
sions, events represent the most frequent referent of
verbal elements. In addition to time and location,
events are characterized by their participants or ar-
guments, which often correspond with discourse en-
tities. This two-way feedback between events and
their arguments (or entities) is the core of our ap-
proach. Since arguments play a key role in describ-
ing an event, knowing that two arguments corefer
is useful for finding coreference relations between
events, and knowing that two events corefer is use-
ful for finding coreference relations between enti-
ties. In (1), the coreference relation between One
of the key suspected Mafia bosses arrested yesterday
and Lo Presti can be found by knowing that their
predicates (i.e., has hanged and had hanged) core-
fer. On the other hand, the coreference relations be-
tween the arguments Saints and Bush in (2) helps
to determine the coreference relation between their
predicates placed and put.
In this paper, we take a holistic approach to coref-
erence. We annotate a corpus with cross-document
coreference relations for nominal and verbal men-
tions. We focus on both intra and inter-document
coreference because this scenario is at the same time
more challenging and more relevant to real-world
applications such as news aggregation. We use this
corpus to train a model that jointly addresses refer-
ences to both entities and events across documents.
The contributions of this work are the following:
? We introduce a novel approach for entity and
event coreference resolution. At the core of
489
our approach is an iterative algorithm that cau-
tiously constructs clusters of entity and event
mentions using linear regression to model clus-
ter merge operations. Importantly, our model
allows information to flow between clusters of
both types through features that model context
using semantic role dependencies.
? We annotate and release a new corpus with
coreference relations between both entities and
events across documents. The relations anno-
tated are both intra and inter-document, which
more accurately models real-world scenarios.
? We evaluate our cross-document coreference
resolution system on this corpus and show that
our joint approach significantly outperforms
two strong baselines that resolve entities and
events separately.
2 Related Work
Entity coreference resolution is a well studied prob-
lem with many successful techniques for identify-
ing mention clusters (Ponzetto and Strube, 2006;
Haghighi and Klein, 2009; Stoyanov et al2009;
Haghighi and Klein, 2010; Raghunathan et al2010;
Rahman and Ng, 2011, inter alia). Most of these
techniques focus on matching compatible noun pairs
using various syntactic and semantic features, with
efforts targeted toward improving features and clus-
tering models.
Prior work showed that models that jointly resolve
mentions across multiple entities result in better per-
formance than simply resolving mentions in a pair-
wise fashion (Denis and Baldridge, 2007; Poon and
Domingos, 2008; Wick et al2008; Lee et al2011,
inter alia). A natural extension is to perform coref-
erence jointly across both entities and events. Yet
there has been little attempt in this direction.
We know of only limited work that incorporates
event-related information in entity coreference, typ-
ically by incorporating the verbs in context as fea-
tures. For instance, Haghighi and Klein (2010) in-
clude the governor of the head of nominal mentions
as features in their model. Rahman and Ng (2011)
also used event-related information by looking at
which semantic role the entity mentions can have
and the verb pairs of their predicates. We confirm
that such features are useful but also show that the
complementary features for verbal mentions lead to
even better performance, especially when event and
entity clusters are jointly modeled.
Compared to the extensive work on entity coref-
erence, the related problem of event coreference re-
mains relatively under-explored, with minimal work
on how entity and event coreference can be con-
sidered jointly on an open domain. Early work on
event coreference for MUC (Humphreys et al1997;
Bagga and Baldwin, 1999) focused on scenario-
specific events. More recently, there have been
approaches that looked at event coreference for
wider domains. Chen and Ji (2009) proposed us-
ing spectral graph clustering to cluster events. Be-
jan and Harabagiu (2010) proposed a nonparamet-
ric Bayesian model for open-domain event resolu-
tion. However, most of this prior work focused only
on event coreference, whereas we address both en-
tities and events with a single model. Humphreys
et al1997) considered entities as well as events,
but due to the lack of a corpus annotated with event
coreference, their approach was only evaluated im-
plicitly in the MUC-6 template filling task. To our
knowledge, the only previous work that considered
entity and event coreference resolution jointly is
He (2007), but limited to the medical domain and
focused on just five semantic categories.
3 Architecture
Following the intuition introduced in Section 1, our
approach iteratively builds clusters of event and en-
tity mentions jointly. As more information becomes
available (e.g., finding out that two verbal mentions
have arguments that belong to the same entity clus-
ter), the features of both entity and event mentions
are re-generated, which prompts future clustering
operations. Our model follows a cautious (or ?baby
steps?) approach, which we previously showed to be
successful for entity coreference resolution (Raghu-
nathan et al2010; Lee et al2011). However,
unlike our previous work, which used deterministic
rules, in this paper we learn a coreference resolution
model using linear regression. Algorithm 1 summa-
rizes the flow of the proposed algorithm. We detail
its steps next. We describe the training procedure in
Section 4 and the features used in Section 5.
490
Algorithm 1: Joint Coreference Resolution
input : set of documents D
input : coreference model ?
// clusters of mentions:
E= {}1
// clusters of documents:
C = clusterDocuments(D)2
foreach document cluster c in C do3
// all mentions in one doc cluster:
M = extractMentions(c)4
// singleton mention clusters:
E ? = buildSingletonClusters(M)5
// high-precision deterministic sieves:
E ? = applyHighPrecisionSieves(E ?)6
// iterative event/entity coreference:
while ? e1, e2 ? E ?s.t. score(e1, e2,?) > 0.5 do7
(e1, e2) = arg max e1,e2?E? score(e1, e2,?)8
E ? = merge(e1, e2, E ?)9
// pronoun sieve:
E ? = applyPronounSieve(E ?)10
// append to global output:
E = E + E ?11
output : E
3.1 Document Clustering
Our approach starts with several steps that reduce
the search space for the actual coreference resolution
task. The first is document clustering, which clusters
the set of input documents (D) into a set of docu-
ment clusters (C). In the subsequent steps we only
cluster mentions that appear in the same document
cluster. We found this to be very useful in practice
because, in addition to reducing the search space, it
provides a word sense disambiguation mechanism
based on corpus-wide topics. For example, with-
out document clustering, our algorithm may decide
to cluster two mentions of the verb hit, but know-
ing that one belongs to a cluster containing earth-
quake reports and the other to a cluster with reports
on criminal activities, this decision can be avoided.1
Any non-parametric clustering algorithm can be
used in this step. In this paper, we used the algo-
rithm proposed by Surdeanu et al2005). This algo-
rithm is an Expectation Maximization (EM) variant
where the initial points (and the number of clusters)
are selected from the clusters generated by a hierar-
chical agglomerative clustering algorithm using ge-
1Since different mentions of the verb say in the same topic
might refer to different events, they are only merged if they have
coreferent arguments.
ometric heuristics. This algorithm performs well on
our data. For example, in the training dataset, only
two topics (handling different earthquake events) are
incorrectly merged into the same cluster.
3.2 Mention Extraction
In this step (4 in Algorithm 1) we extract nominal,
pronominal, and verbal mentions. We extract nom-
inal and pronominal mentions using the mention
identification component in the publicly download-
able Stanford coreference resolution system (Raghu-
nathan et al2010; Lee et al2011). We consider
as verbal mentions all words whose part of speech
starts with VB, with the exception of some auxil-
iary/copulative verbs (have, be and seem). For each
of the identified mentions we build a singleton clus-
ter (step 5 in Algorithm 1).
Crucially, we do not make a formal distinction be-
tween entity and event mentions. This distinction is
not trivial to implement (e.g., is the noun earthquake
an entity or an event mention?) and an imperfect
classification would negatively affect the following
coreference resolution. Instead, we simply classify
mentions into verbal or nominal, and use this dis-
tinction later during feature generation (Section 5).
To compare event nouns (e.g., development) with
verbal mentions, the ?derivationally related form?
relation in WordNet is used.
3.3 High-precision Entity Resolution Sieves
To further reduce the problem?s search space, in
step 6 of Algorithm 1 we apply a set of high-
precision filters from the Stanford coreference res-
olution system. This system is a collection of deter-
ministic models (or ?sieves?) for entity coreference
resolution that incorporate lexical, syntactic, seman-
tic, and discourse information. These sieves are ap-
plied from higher to lower precision. As clusters are
built, information such as mention gender and num-
ber is propagated across mentions in the same clus-
ter, which helps subsequent decisions. The Stanford
system obtained the highest score at the CoNLL-
2011 shared task on English coreference resolution.
For this step, we selected all the sieves from the
Stanford system with the exception of the pronoun
resolution sieve. All the remaining sieves (listed
in Table 1) have high precision because they em-
ploy linguistic heuristics with little ambiguity, e.g.,
491
High-precision sieves
Discourse processing sieve
Exact string match sieve
Relaxed string match sieve
Precise constructs sieve (e.g., appositives)
Strict head match sieves
Proper head noun match sieve
Relaxed head matching sieve
Table 1: Deterministic sieves in step 6 of Algorithm 1.
one sieve clusters together two entity mentions only
when they have the same head word. Note that all
these heuristics were designed for within-document
coreference. They work well in our context be-
cause we apply them in individual document clus-
ters, where the one-sense-per-discourse principle
still holds (Yarowsky, 1995).
Importantly, these sieves do not address verbal
mentions. That is, all verbal mentions are still in sin-
gleton clusters after this step. Furthermore, none of
these sieves use features that facilitate the joint reso-
lution of nominal and verbal mentions (e.g., features
from semantic role frames). All these limitations are
addressed next.
3.4 Iterative Entity/Event Resolution
In this stage (steps 7 ? 9 in Algorithm 1), we con-
struct entity and event clusters using a cautious or
?baby steps? approach. We use a single linear re-
gressor (?) to model cluster merge operations be-
tween both verbal and nominal clusters. Intuitively,
the linear regressor models the quality of the merge
operation, i.e., a score larger than 0.5 indicates that
more than half of the mention pairs introduced by
this merge are correct. We discuss the training pro-
cedure that yields this scoring function in Section 4.
In each iteration, we perform the merge operation
that has the highest score. Once two clusters are
merged (step 9) we regenerate all the mention fea-
tures to reflect the current clusters. We stop when no
merging operation with an overall benefit is found.
This iterative procedure is the core of our joint
coreference resolution approach. This algorithm
transparently merges both entity and event men-
tions and, importantly, allows information to flow
between clusters of both types as merge operations
take place. For example, assume that during iter-
ation i we merge the two hanged verbs in the first
example in Section 1 (because they have the same
lemma). Because of this merge, in iteration i+ 1 the
nominal mentions Lo Presti and One of the key sus-
pected Mafia bosses have the same semantic role for
verbs assigned to the same cluster. This is a strong
hint that these two nominal mentions belong to the
same cluster. Indeed, the feature that models this
structure received one of the highest weights in our
linear regression model (see Section 7).
3.5 Pronoun Sieve
Our approach concludes with the pronominal coref-
erence resolution sieve from the Stanford system.
This sieve is necessary because our current reso-
lution algorithm ignores mention ordering and dis-
tance (i.e., in step 7 we compare all clusters regard-
less of where their mentions appear in the text). As
previous work has proved, the structure of the text is
crucial for pronominal coreference (Hobbs, 1978).
For this reason, we handle pronouns outside of the
main algorithm block.
4 Training the Cluster Merging Model
Two observations drove our choice of model and
training algorithm. First, modeling the merge op-
eration as a classification task is not ideal, because
only a few of the resulting clusters are entirely cor-
rect or incorrect. In practice, most of the clusters
will contain some mention pairs that are correct and
some that are not. Second, generating training data
for the merging model is not trivial: a brute force
approach that looks at all the possible combinations
is exponential in the number of mentions. This is
both impractical and unnecessary, as some of these
combinations are unlikely to be seen in practice.
We address these observations with Algorithm 2.
The algorithm uses gold coreference labels to train a
linear regressor that models the quality of the clus-
ters produced by merge operations. We define the
quality score q of a new cluster as the percentage of
new mention pairs (i.e., not present in either one of
the clusters to be merged) that are correct:
q =
linkscorrect
linkscorrect + linksincorrect
(1)
where links(in)correct is the number of newly intro-
duced (in)correct pairwise mention links when two
clusters are merged.
492
Algorithm 2: Training Procedure
input : set of documents D
input : correct mention clusters G
C = clusterDocuments(D)1
// linear regression coreference model:
? = assignInitialWeights(C,G)2
// repeat for T epochs:
for t = 1 to T do3
// training data for linear regressor:
? = {}4
foreach document cluster c in C do5
M = extractMentions(c)6
E = buildSingletonClusters(M)7
E = applyHighPrecisionSieves(E)8
// gather training examples
// as clusters are built:
while ? e1, e2 ? Es.t. sco(e1, e2,?) > 0.5 do9
forall e?1, e
?
2 ? E do10
q = qualityOfMerge(e?1, e
?
2,G)11
? = append(e?1, e
?
2, q,?)12
(e1, e2) = arg max e1,e2?E sco(e1, e2,?)13
E = merge(e1, e2, E)14
// train using data from last epoch:
?? = trainLinearRegressor(?)15
// interpolate with older model:
? = ?? + (1? ?)??16
output : ?
We address the potential explosion in training data
size by considering only merge operations that are
likely to be inspected by the algorithm as it runs.
To achieve this, Algorithm 2 repeatedly runs the ac-
tual clustering algorithm (as given by the current
model ?) over the training dataset (steps 5 ? 14).2
When the algorithm iteratively constructs its clus-
ters (steps 9 ? 14), we generate training data from
all possible cluster pairs available during a particular
iteration (steps 10 ? 12). For each pair, we compute
its score using Equation 1 (step 11) and add it to the
training corpus ? (step 12). Note that this avoids in-
specting many of the possible cluster combinations:
once a cluster is built (e.g., during the previous iter-
ations or by the deterministic sieves in step 8), we
do not generate training data from its members, but
rather treat it as an atomic unit. On the other hand,
our approach generates more training data than on-
line learning, which trains using only the actual de-
cisions taken during inference in each iteration (i.e.,
2We skip the pronoun sieve here because it does not affect
the decisions taken during the iterative resolution steps.
the pair (e1, e2) in step 13).
After each epoch we have a new training cor-
pus ?, which we use to train the new linear regres-
sion model ?? (step 15), which is then interpolated
with the old one (step 16).
Our training procedure is similar in spirit to trans-
formation based learning (TBL) (Brill, 1995). Sim-
ilarly to TBL, our approach repeatedly applies the
model over the training data and attempts to mini-
mize the error rate of the current model. However,
while TBL learns rules that directly minimize the
current error rate, our approach achieves this indi-
rectly, by incorporating the reduction in error rate in
the score of the generated datums. This allows us
to fit a linear regression to this task, which, as dis-
cussed before, is a better model for this task.
Just like any hill-climbing algorithm, our ap-
proach has the risk of converging to a local max-
imum. To mitigate this risk, we do not initialize
our model ? with random weights, but rather use
hints from the deterministic sieves. This procedure
(listed in step 2) runs the high-precision sieves in-
troduced in Section 3.3 and, just like the data gen-
eration loop in Algorithm 2, creates training exam-
ples from the clusters available after every merge
operation. Since these deterministic models address
only nominal clusters, at the end we generate train-
ing data for events by inspecting all the pairs of sin-
gleton verbal clusters. Using this data, we train the
initial linear regression model.
We trained our model using L2 regularized linear
regression with a regularization coefficient of 1.0.
We did not tune the regularization coefficient. We
ran the training algorithm for 10 epochs, although
we observed minimal changes after three epochs.
We tuned the interpolation weight (?) to a value
of 0.7 using our development corpus.
5 Features
We list in Table 2 the features used by the lin-
ear regression model. As the table indicates, our
feature set relies heavily on semantic roles, which
were extracted using the SwiRL semantic role la-
beling (SRL) system (Surdeanu et al2007).3 Be-
cause SwiRL addresses only verbal predicates, we
extended it to handle nominal predicates. In this
3http://www.surdeanu.name/mihai/swirl/
493
Feature Name
Applies to
Entities (E)
or Events (V)
Description and Example
Entity Heads E
Cosine similarity of the head-word vectors of two clusters. The head-word vector
stores the head words of all mentions in a cluster and their frequencies. For example,
the vector for the three-mention cluster {Barack Obama, President Obama, US
president}, is {Obama:2, president:1}.
Event Lemmas V
Cosine similarity of the lemma vectors of two clusters. For example, the lemma
vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.
Links between
Synonyms
E, V
The percentage of newly-introduced mention links after the merge that are WordNet
synonyms (Fellbaum, 1998). For example, when merging the following two clus-
ters, {hit, strike} and {strike, join, say}, two out of the six new links are between
words that belong to the same WordNet synset: (hit ? strike) and (strike ? strike).
Number of Coreferent
Arguments or
Predicates
E, V
The total number of shared arguments and predicates between mentions in the
two clusters. We use the cluster IDs of the corresponding arguments/predicates
to check for identity. For example, when comparing the event clusters {bought}
and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and
[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-
tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,
AMD} and {ATI, ATI} were previously created). For entity clusters, this feature
counts the number of coreferent predicates. In addition to PropBank-style roles, for
event mentions we also include the closest left and right entity mentions in order to
capture any arguments missed by the SRL system.
Coreferent Arguments
in a Specific Role?
E, V
Indicator feature set to 1 if the two clusters have at least one coreferent argument in
a given role. We generate one variant of this feature for each argument label, e.g.,
Arg0, Arg1, etc. For example, the value of this feature for Arg0 for the clusters
{bought} and {acquired} in the above example is 1.
Coreferent Predicate in
a Specific Role?
E
Indicator feature set to 1 if the two clusters have at least one coreferent predicate for
a given role. For example, for the clusters {the man} and {the person}, extracted
from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of
this feature is 1 if the two helped verbs were previously clustered together.
2nd Order Similarity of
Mention Words
E
Cosine similarity of vectors containing words that are distributionally similar to
words in the cluster mentions. We built these vectors by extracting the top-ten
most-similar words in Dekang Lin?s similarity thesaurus (Lin, 1998) for all the
nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new
home}, we construct this vector by expanding new and home to: {new:1, original:1,
old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,
small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,
mansion:1, school:1, restaurant:1, hospital:1 }.
Number; Animacy;
Gender; NE Label
E
Cosine similarity of number, gender, animacy, and NE label vectors. For example,
the number and gender vectors for the two-mention cluster {systems, a pen} are
Number = {singular:1, plural:1}, Gender = {neutral:2}.
Table 2: List of features used when comparing two clusters. If any of the two clusters contains a verbal mention we
consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We
append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in
each of the two clusters. We use the suffix Proper only if both head words are proper nouns.
paper we used a single heuristic: the possessor of
a nominal event?s predicate is marked as its Arg0,
e.g., Logan is the Arg0 to run in Logan?s run.4
4A principled solution to this problem is to use an SRL sys-
tem for nominal predicates trained using NomBank (Meyers et
al., 2004). We will address this in future work.
494
We extracted named entity labels using the named
entity recognizer from the Stanford CoreNLP suite.
6 Evaluation
6.1 Corpus
The training and test data sets were derived from
the EventCorefBank (ECB) corpus5 created by Be-
jan and Harabagiu (2010) to study event coreference
since standard corpora such as OntoNotes (Pradhan
et al2007) contain a small number of annotated
event clusters. The ECB corpus consists of 482 doc-
uments from Google News clustered into 43 topics,
where a topic is described as a seminal event. The
reason for including comparable documents was to
increase the number of cross-document coreference
relations. Bejan and Harabagiu (2010) only anno-
tated a selection of events.
For the purpose of our study, we extended the
original corpus in two directions: (i) fully anno-
tated sentences, and (ii) entity coreference relations.
In addition, we removed relations other than coref-
erence (e.g., subevent, purpose, related, etc.) that
had been originally annotated. We revised and com-
pleted the original annotation by annotating every
entity and event in the sentences that were (partially)
annotated. The annotation was performed by four
experts, using the Callisto annotation tool.6 The
annotation guidelines and the generated corpus are
available here.7
Our annotation of the ECB corpus followed the
OntoNotes (Pradhan et al2007) standard for coref-
erence annotation, with a few extensions to handle
events. For nouns, we annotated full NPs (with all
modifiers), excluding appositive phrases and nomi-
nal predicates. Only premodifiers that were proper
nouns or possessive phrases were annotated. For
events, we annotated the semantic head of the verb
phrase. We extended the OntoNotes guidelines by
also annotating singletons (but we do not score
them; see below), and by including all events men-
tions (not only those mentioned at least once with an
NP). This required us to be specific with respect to:
5http://faculty.washington.edu/bejan/
data/ECB1.0.tar.gz
6http://callisto.mitre.org
7http://nlp.stanford.edu/pubs/
jcoref-corpus.zip
Training Dev Test Total
# Topics 12 3 28 43
# Documents 112 39 331 482
# Entities 459 46 563 1068
# Entity Mentions 1723 259 3465 5447
# Events 300 30 444 774
# Event Mentions 751 140 1642 2533
Table 3: Corpus statistics.
?ENTITY COREFID=?26?? A publicist ?/ENTITY? ?EVENT
COREFID=?4?? says ?/EVENT? ?ENTITY COREFID=?23??
Tara Reid ?/ENTITY? has ?EVENT COREFID=?3?? checked
?/EVENT? ?ENTITY COREFID=?23?? herself ?/ENTITY? ?EVENT
COREFID=?3*?? into ?/EVENT? ?ENTITY COREFID=?28?? rehab
?/ENTITY?.
Figure 1: Annotation example.
Light verbs Verbs such as give and make followed
by a noun (e.g., make an offer) were not anno-
tated, but the noun was.
Phrasal verbs We annotated the verb together with
the preposition or adverb (e.g., check in).
Idioms They were annotated with all their elements
(e.g., booze it up).
The first topic was annotated by all four anno-
tators as burn-in. Afterwards, annotation disagree-
ments were resolved between all annotators and the
next three topics were annotated again by all four an-
notators to measure agreement. Following Passon-
neau (2004), we computed an inter-annotator agree-
ment of ? = 0.55 (Krippendorff, 2004) on these
three topics, indicating moderate agreement among
the annotators. Given the complexity of the task, we
consider this to be a good score. For example, the
average of the CoNLL F1 between any two annota-
tors is 73.58, which is much higher than the system
scores reported in the literature.
After annotating the four topics, disagreements
were resolved again and all the documents in the
four topics were corrected to match the consensus.
The rest of the corpus was split between the four an-
notators, and each document was annotated by a sin-
gle annotator. Figure 1 shows an example. Table 3
shows the corpus statistics, including the training,
development (dev) and test set splits. The dev topics
were used for tuning the interpolation parameter ?
from Section 4.
495
MUC B3 CEAF-?4 BLANC
System R P F1 R P F1 R P F1 R P F1 CoNLL F1
Baseline 1
Wo/ SRL
Entity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9
Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2
Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8
Baseline 2
With SRL
Entity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8
Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2
Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6
This paper
Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2
Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8
Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9
Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the
complete task using five metrics.
6.2 Evaluation
We use five coreference evaluation metrics widely
used in the literature:
MUC (Vilain et al1995) Link-based metric which
measures how many predicted and gold clus-
ters need to be merged to cover the gold and
predicted clusters, respectively.
B3 (Bagga and Baldwin, 1998) Mention-based
metric which measures the proportion of over-
lap between predicted and gold clusters for a
given mention.
CEAF (Luo, 2005) Entity-based metric that, unlike
B3, enforces a one-to-one alignment between
gold and predicted clusters. We employ the
entity-based version of CEAF.
BLANC (Recasens and Hovy, 2011) Metric based
on the Rand index (Rand, 1971) that consid-
ers both coreference and non-coreference links
to address the imbalance between singleton and
coreferent mentions.
CoNLL F1 Average of MUC, B3, and CEAF-?4.
This was the official metric in the CoNLL-2011
shared task (Pradhan et al2011).
We followed the CoNLL-2011 evaluation methodol-
ogy, that is, we removed all singleton clusters, and
apposition/copular relations before scoring.
We evaluated the systems on three different set-
tings: only on entity clusters, only on event clus-
ters, and on the complete task, i.e., both entities and
events. Note that the gold corpus separates clusters
into entity and event clusters (see Table 3), but our
system does not make this distinction at runtime.
In order to compute the entity-only and event-only
scores in Table 4, we implemented the following
procedure: (a) when scoring entity clusters, we re-
moved all mentions that were found to be coreferent
with at least one gold event mention and not coref-
erent with any gold entity mentions; and (b) we per-
formed the opposite action when scoring event clus-
ters. This procedure is necessary because our men-
tion identification component is not perfect, i.e., it
generates mentions that do not exist in the gold an-
notation. Furthermore, this procedure is conserva-
tive with respect to the clustering errors of our sys-
tem, e.g., all spurious mentions that our system in-
cludes in a cluster with a gold entity mention are
considered for the entity score, regardless of their
gold type (event or entity).
6.3 Results
Table 4 compares the performance of our system
against two strong baselines that resolve entities and
events separately. Baseline 1 uses a modified Stan-
ford coreference resolution system after our doc-
ument clustering and mention identification steps.
Because the original Stanford system implements
only entity coreference, we extended it with an extra
sieve that implements lemma matching for events.
This additional sieve merges two verbal clusters
(i.e., clusters that contain at least one verbal men-
tion) or a verbal and a nominal cluster when at least
two lemmas of mention head words are the same be-
tween clusters, e.g., helped and the help.
The second baseline adds two more sieves to
Baseline 1. Both these sieves model entity and event
496
contextual information using semantic roles. The
first sieve merges two nominal clusters when two
mentions in the respective clusters have the same
head words and two mentions (possibly with dif-
ferent heads) modify with the same role label two
predicates that have the same lemma. For exam-
ple, this sieve merges the clusters {Obama, the pres-
ident} (seen in the text [Obama]Arg0 attended and
[the president]Arg1 was elected) and {Obama} (seen
in the text [Obama]Arg1 was elected), because they
share a mention with the same head word (Obama)
and two mentions modify with the same role (Arg1)
predicates with the same lemma (elect). The sec-
ond sieve implements the complementary action for
event clusters. That is, it merges two verbal clusters
when at least two mentions have the same lemma
and at least two mentions have semantic arguments
with the same role label and the same lemma.
7 Discussion
The first block in Table 4 indicates that lemma
matching is a strong baseline for event resolution.
Most of the event scores for Baseline 1 are actually
higher than the corresponding entity scores, which
were obtained using the highest ranked system at the
CoNLL-2011 shared task (Lee et al2011). Adding
contextual information using semantic roles (Base-
line 2) helps both entities and events. The CoNLL
F1 for Baseline 2 increases almost 3 points for enti-
ties and 1 point for events. This demonstrates that
local syntactico-semantic context is important for
coreference resolution even in a cross-document set-
ting and that the current state-of-the-art in SRL can
model this context accurately.
The best scores (almost unanimously) are ob-
tained by the model proposed in this paper, which
scores 3.4 CoNLL F1 points higher than Baseline 2
for entities, and 2.6 points higher for events. For the
complete task, our approach scores 3.3 CoNLL F1
points higher than Baseline 2, and 6.1 points higher
than Baseline 1. This demonstrates that a holistic
approach to coreference resolution improves the res-
olution of both entities and events more than models
that address aspects of the task separately. To fur-
ther understand our experiments, we listed the top
five entity/event features with the highest weights in
our model in Table 5. The table indicates that six out
of the ten features serve the purpose of passing infor-
Entity Feature Weight
Entity Heads ? Proper 1.10
Coreferent Predicate for ArgM-LOC ? Common 0.45
Entity Heads ? Common 0.36
Coreferent Predicate for Arg0 ? Proper 0.29
Coreferent Predicate for Arg2 ? Common 0.28
Event Feature Weight
Event Lemmas 0.45
Coreferent Argument for Arg1 0.19
Links between Synonym 0.16
Coreferent Argument for Arg2 0.13
Number of Coreferent Arguments 0.07
Table 5: Top five features with the highest weights.
mation between entity and event clusters. For exam-
ple, the ?Coreferent Argument for Arg1? feature is
triggered when two event clusters have Arg1 argu-
ments that already belong to the same entity cluster.
This allows information from previous entity coref-
erence operations to impact future merges of event
clusters. This is the crux of our iterative approach to
joint coreference resolution.
Finally, we performed an error analysis by man-
ually evaluating 100 errors. We distinguished nine
major types of errors. Their ratios together with a
description and an example are given in Table 6.
This work demonstrates that an approach that
jointly models entities and events is better for cross-
document coreference resolution. However, our
model can be improved. For example, document
clustering and coreference resolution can be solved
jointly, which we expect would improve both tasks.
Furthermore, our iterative coreference resolution
procedure (Algorithm 1) could be modified to ac-
count for mention ordering and distance, which
would allow us to include pronominal resolution in
our joint model, rather than addressing it with a sep-
arate deterministic sieve.
8 Conclusion
We have presented a holistic model for cross-
document coreference resolution that jointly solves
references to events and entities by handling both
nominal and verbal mentions. Our joint resolution
algorithm allows event coreference to help improve
entity coreference, and vice versa. In addition, our
iterative procedure, based on a linear regressor that
models the quality of cluster merges, allows each
497
Error Type (Ratio)
Description
Example
Pronoun resolution
(36%)
The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity
system. These errors include (only a small number of) event pronouns.
He said Timmons aimed and missed his target.
Semantics beyond
role frames
(20%)
The semantics of the coreference relation cannot be captured by role frames or WordNet.
Israeli forces on Tuesday killed at least 40 people . . . The Israeli army said the UN school in the
Jabaliya refugee camp was hit . . . and that the dead included a number of Hamas militants.
Arguments of
nominal events
(17%)
The arguments of two nominal events are not detected and thus not coreferred.
The attack on the school has caused widespread shock across Israel . . . while Israeli forces on
Tuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.
Cascaded errors
(7%)
Entities or events are not coreferred due to errors in a previous merge iteration in the same
semantic frame. In the example below, we failed to link the two die verbs, which leads to the
listed entity error.
An Australian climber who survived two nights stuck on Mount Cook after seeing his brother
die . . . Dr Mark Vinar, 43, is presumed dead . . .
Initial high-precision
sieves
(6%)
An error made by the initial high-precision entity resolution sieves is propagated to our model.
Timmons told police he fired when he thought he saw someone in the other group reach for
a gun . . . 15-year-old Timmons was at the scene of the shooting and had a gun.
Phrasal verbs
(6%)
The meaning of a phrasal verb is not captured.
A relative unknown will take over the title role of Doctor Who . . . But the casting of Smith is
a stroke of genius.
Linear regression
(4%)
Recall error made by the regression model when the features are otherwise correct.
The Interior Department on Thursday issued ?revised? regulations . . . Interior Secretary Dirk
Kempthorne announced major changes . . .
Mention detection
(3%)
The mention detection module detects a spurious mention.
Police have arrested a man . . . in the parking lot crosswalk at Sam?s Club in Bloomington.
SRL
(1%)
The SRL system fails to label the semantic role. In this example, jail is detected as the ArgM-
MNR of hanged instead of ArgM-LOC.
A Mafia boss in Palermo hanged himself in jail.
Table 6: Error analysis. Mentions to be resolved are in bold face, correct antecedents are in italics, and our system?s
predictions are underlined.
merging state to benefit from the previous merged
entity and event mentions. This approach allows us
to start with a set of high-precision coreference rela-
tions and gradually add new ones to increase recall.
The experimental evaluation shows that our coref-
erence algorithm gives markedly better F1 for both
entities and events, outperforming two strong base-
lines that handle entities and events separately, mea-
sured by all the standard measures: MUC, B3,
CEAF-?4, BLANC and the official CoNLL-2011
metric. This is noteworthy since each measure has
been shown to place primary emphasis in evaluating
a different aspect of the coreference resolution task.
Our system is tailored for cross-document coref-
erence resolution on a corpus that contains news ar-
ticles that repeatedly report on a smaller number of
topics. This makes it particularly suitable for real-
world applications such as multi-document summa-
rization and cross-document information extraction.
We also release our labeled corpus to facilitate ex-
tensions and comparisons to our work.
Acknowledgements
We acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of the DARPA, AFRL, or the US
government. MR is supported by a Beatriu de Pino?s post-
doctoral scholarship (2010 BP-A 00149) from Generali-
tat de Catalunya. AC is supported by a SAP Stanford
Graduate Fellowship. We also gratefully thank Cosmin
Bejan for sharing his code and the useful discussions.
498
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the ACL 1999 Workshop
on Coreference and Its Applications, pages 1?8.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the ACL-
IJCNLP 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP 2009, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL 2010, pages 385?393.
Tian He. 2007. Coreference Resolution on Entities and
Events for Hospital Discharge Summaries. Thesis,
Massachusetts Institute of Technology.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311?338.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop On Operational
Factors In Practical Robust Anaphora Resolution For
Unrestricted Texts, pages 75?81.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to its Methodology. Sage, Thousand Oaks,
CA, second edition.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of CoNLL 2011: Shared Task, pages 28?34.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
pages 768?774.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: an interim report. In Proceedings of the
HLT-NAACL 2004 Workshop on Frontiers in Corpus
Annotation, pages 24?31.
Rebecca Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of LREC
2004, pages 1503?1506.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL 2006, pages 192?199.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of EMNLP 2008, pages 650?659.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in OntoNotes. In Proceedings of ICSC 2007, pages
446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011: Shared Task, pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Chris Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
EMNLP 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings of
ACL 2011, pages 814?824.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP 2009, pages 656?664.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005.
A hybrid unsupervised approach for document cluster-
ing. In Proceedings of KDD 2005, pages 685?690.
499
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Bonnie Lynn Webber. 1988. Discourse deixis: reference
to discourse segments. In Proceedings of ACL 1988,
pages 113?122.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of KDD 2008, pages 722?
730.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL 1995, pages 189?196.
500
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028?2038,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Learning Spatial Knowledge for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Stanford University
{angelx,msavva,manning}@cs.stanford.edu
Abstract
We address the grounding of natural lan-
guage to concrete spatial constraints, and
inference of implicit pragmatics in 3D en-
vironments. We apply our approach to the
task of text-to-3D scene generation. We
present a representation for common sense
spatial knowledge and an approach to ex-
tract it from 3D scene data. In text-to-
3D scene generation, a user provides as in-
put natural language text from which we
extract explicit constraints on the objects
that should appear in the scene. The main
innovation of this work is to show how
to augment these explicit constraints with
learned spatial knowledge to infer missing
objects and likely layouts for the objects
in the scene. We demonstrate that spatial
knowledge is useful for interpreting natu-
ral language and show examples of learned
knowledge and generated 3D scenes.
1 Introduction
To understand language, we need an understanding
of the world around us. Language describes the
world and provides symbols with which we rep-
resent meaning. Still, much knowledge about the
world is so obvious that it is rarely explicitly stated.
It is uncommon for people to state that chairs are
usually on the floor and upright, and that you usu-
ally eat a cake from a plate on a table. Knowledge
of such common facts provides the context within
which people communicate with language. There-
fore, to create practical systems that can interact
with the world and communicate with people, we
need to leverage such knowledge to interpret lan-
guage in context.
Spatial knowledge is an important aspect of the
world and is often not expressed explicitly in nat-
ural language. This is one of the biggest chal-
Figure 1: Generated scene for ?There is a room
with a chair and a computer.? Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
lenges in grounding language and enabling natu-
ral communication between people and intelligent
systems. For instance, if we want a robot that can
follow commands such as ?bring me a piece of
cake?, it needs to be imparted with an understand-
ing of likely locations for the cake in the kitchen
and that the cake should be placed on a plate.
The pioneering WordsEye system (Coyne and
Sproat, 2001) addressed the text-to-3D task and is
an inspiration for our work. However, there are
many remaining gaps in this broad area. Among
them, there is a need for research into learning spa-
tial knowledge representations from data, and for
connecting them to language. Representing un-
stated facts is a challenging problem unaddressed
by prior work and the focus of our contribution.
This problem is a counterpart to the image descrip-
tion problem (Kulkarni et al., 2011; Mitchell et al.,
2012; Elliott and Keller, 2013), which has so far
remained largely unexplored by the community.
We present a representation for this form of spa-
tial knowledge that we learn from 3D scene data
and connect to natural language. We will show
how this representation is useful for grounding
language and for inferring unstated facts, i.e., the
pragmatics of language describing physical envi-
ronments. We demonstrate the use of this repre-
sentation in the task of text-to-3D scene genera-
2028
Room
Table
Plate
Cake
color(red)?There is a room with a table and a cake. There is a red chair to the right of the table.?
a) Scene TemplateInput Text
supports(o0,o1) supports(o0,o2)
right(o2,o1)
o3cake
c) 3D Scene
o0room
o1table o2chair
supports(o1,o4)
supports(o4,o3)o4plate
Parse
Infer
Ground
Layout
b) Geometric Scene
Render
ViewChair
Figure 2: Overview of our spatial knowledge representation for text-to-3D scene generation. We parse
input text into a scene template and infer implicit spatial constraints from learned priors. We then ground
the template to a geometric scene, choose 3Dmodels to instantiate and arrange them into a final 3D scene.
tion, where the input is natural language and the
desired output is a 3D scene.
We focus on the text-to-3D task to demonstrate
that extracting spatial knowledge is possible and
beneficial in a challenging scenario: one requiring
the grounding of natural language and inference of
rarelymentioned implicit pragmatics based on spa-
tial facts. Figure 1 illustrates some of the inference
challenges in generating 3D scenes from natural
language: the desk was not explicitly mentioned
in the input, but we need to infer that the computer
is likely to be supported by a desk rather than di-
rectly placed on the floor. Without this inference,
the user would need to be much more verbose with
text such as ?There is a room with a chair, a com-
puter, and a desk. The computer is on the desk, and
the desk is on the floor. The chair is on the floor.?
Contributions We present a spatial knowledge
representation that can be learned from 3D scenes
and captures the statistics of what objects occur
in different scene types, and their spatial posi-
tions relative to each other. In addition, we model
spatial relations (left, on top of, etc.) and learn a
mapping between language and the geometric con-
straints that spatial terms imply. We show that
using our learned spatial knowledge representa-
tion, we can infer implicit constraints, and generate
plausible scenes from concise natural text input.
2 Task Definition and Overview
We define text-to-scene generation as the task of
taking text that describes a scene as input, and gen-
erating a plausible 3D scene described by that text
as output. More concretely, based on the input
text, we select objects from a dataset of 3D models
and arrange them to generate output scenes.
The main challenge we address is in transform-
ing a scene template into a physically realizable 3D
scene. For this to be possible, the system must be
able to automatically specify the objects present
and their position and orientation with respect to
each other as constraints in 3D space. To do so, we
need to have a representation of scenes (?3). We
need good priors over the arrangements of objects
in scenes (?4) and we need to be able to ground
textual relations into spatial constraints (?5). We
break down our task as follows (see Figure 2):
Template Parsing (?6.1): Parse the textual de-
scription of a scene into a set of constraints on the
objects present and spatial relations between them.
Inference (?6.2): Expand this set of constraints by
accounting for implicit constraints not specified in
the text using learned spatial priors.
Grounding (?6.3): Given the constraints and pri-
ors on the spatial relations of objects, transform the
scene template into a geometric 3D scenewith a set
of objects to be instantiated.
Scene Layout (?6.4): Arrange the objects and op-
timize their placement based on priors on the rel-
ative positions of objects and explicitly provided
spatial constraints.
3 Scene Representation
To capture the objects present and their arrange-
ment, we represent scenes as graphs where nodes
are objects in the scene, and edges are semantic re-
lationships between the objects.
We represent the semantics of a scene using a
scene template and the geometric properties using
a geometric scene. One critical property which is
captured by our scene graph representation is that
of a static support hierarchy, i.e., the order in which
bigger objects physically support smaller ones: the
floor supports tables, which support plates, which
can support cakes. Static support and other con-
straints on relationships between objects are rep-
resented as edges in the scene graph.
2029
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p(Scene|Knife+Table)
Kitchen Counter Dining TableLiving Room Kitchen
Figure 3: Probabilities of different scene types
given the presence of ?knife? and ?table?.
Figure 4: Probabilities of support for some most
likely child object categories given four different
parent object categories, from top left clockwise:
dining table, bookcase, room, desk.
3.1 Scene Template
A scene template T = (O, C, C
s
) consists of a
set of object descriptions O = {o
1
, . . . , o
n
} and
constraints C = {c
1
, . . . , c
k
} on the relationships
between the objects. A scene template also has a
scene type C
s
.
Each object o
i
, has properties associated with
it such as category label, basic attributes such as
color and material, and number of occurrences in
the scene. For constraints, we focus on spatial re-
lations between objects, expressed as predicates of
the form supported_by(o
i
, o
j
) or left(o
i
, o
j
)where
o
i
and o
j
are recognized objects.1 Figure 2a shows
an example scene template. From the scene tem-
plate we instantiate concrete geometric 3D scenes.
To infer implicit constraints on objects and spa-
tial support we learn priors on object occurrences
in 3D scenes (?4.1) and their support hierarchies
(?4.2).
3.2 Geometric Scene
We refer to the concrete geometric representation
of a scene as a ?geometric scene?. It consists of
a set of 3D model instances ? one for each ob-
ject ? that capture the appearance of the object. A
transformation matrix that represents the position,
orientation, and scaling of the object in a scene is
also necessary to exactly position the object. We
generate a geometric scene from a scene template
by selecting appropriate models from a 3D model
database and determining transformations that op-
1Our representation can also support other relationships
such as larger(o
i
, o
j
).
timize their layout to satisfy spatial constraints. To
inform geometric arrangement we learn priors on
the types of support surfaces (?4.2) and the relative
positions of objects (?4.4).
4 Spatial Knowledge
Our model of spatial knowledge relies on the idea
of abstract scene types describing the occurrence
and arrangement of different categories of objects
within scenes of that type. For example, kitchens
typically contain kitchen counters on which plates
and cups are likely to be found. The type of scene
and category of objects condition the spatial rela-
tionships that can exist in a scene.
We learn spatial knowledge from 3D scene data,
basing our approach on that of Fisher et al. (2012)
and using their dataset of 133 small indoor scenes
created with 1723 Trimble 3D Warehouse mod-
els (Fisher et al., 2012).
4.1 Object Occurrence Priors
We learn priors for object occurrence in different
scene types (such as kitchens, offices, bedrooms).
P
occ
(C
o
|C
s
) =
count(C
o
in C
s
)
count(C
s
)
This allows us to evaluate the probability of dif-
ferent scene types given lists of object occurring
in them (see Figure 3). For example given input of
the form ?there is a knife on the table? then we are
likely to generate a scene with a dining table and
other related objects.
4.2 Support Hierarchy Priors
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category C
p
sup-
porting a given child category C
c
as a simple con-
ditional probability based on normalized observa-
tion counts.2
P
support
(C
p
|C
c
) =
count(C
c
on C
p
)
count(C
c
)
We show a few of the priors we learn in Figure 4
as likelihoods of categories of child objects being
statically supported by a parent category object.
2The support hierarchy is explicitly modeled in the scene
dataset we use.
2030
Figure 5: Predicted positions using learned rela-
tive position priors for chair given desk (top left),
poster-room (top right), mouse-desk (bottom left),
keyboard-desk (bottom right).
4.3 Support Surface Priors
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector, limited to the six canonical
directions: up, down, left, right, front, back. We
learn a probability of supporting surface normal di-
rection S
n
given child object category C
c
. For ex-
ample, posters are typically found on walls so their
support normal vectors are in the horizontal di-
rections. Any unobserved child categories are as-
sumed to haveP
surf
(S
n
= up|C
c
) = 1 sincemost
things rest on a horizontal surface (e.g., floor).
P
surf
(S
n
|C
c
) =
count(C
c
on surface with S
n
)
count(C
c
)
4.4 Relative Position Priors
We model the relative positions of objects based
on their object categories and current scene type:
i.e., the relative position of an object of category
C
obj
is with respect to another object of category
C
ref
and for a scene type C
s
. We condition on the
relationship R between the two objects, whether
they are siblings (R = Sibling) or child-parent
(R = ChildParent).
P
relpos
(x, y, ?|C
obj
, C
ref
, C
s
, R)
When positioning objects, we restrict the search
space to points on the selected support surface.
The position x, y is the centroid of the target ob-
ject projected onto the support surface in the se-
mantic frame of the reference object. The ? is the
angle between the front of the two objects. We rep-
resent these relative position and orientation pri-
ors by performing kernel density estimation on the
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
outside(A,B) 1 - V ol(A?B)
V ol(A)
left_of(A,B) V ol(A? left_of (B))
V ol(A)
right_of(A,B) V ol(A? right_of (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
faces(A,B) cos(front(A), c(B)? c(A))
Table 1: Definitions of spatial relation using
bounding boxes. Note: dist(A,B) is normalized
against the maximum extent of the bounding box
ofB. front(A) is the direction of the front vector
of A and c(A) is the centroid of A.
Keyword Top Relations and Scores
behind (back_of, 0.46), (back_side, 0.33)
adjacent (front_side, 0.27), (outside, 0.26)
below (below, 0.59), (lower_side, 0.38)
front (front_of, 0.41), (front_side, 0.40)
left (left_side, 0.44), (left_of, 0.43)
above (above, 0.37), (near, 0.30)
opposite (outside, 0.31), (next_to, 0.30)
on (supported_by, 0.86), (on_top_of, 0.76)
near (outside, 0.66), (near, 0.66)
next (outside, 0.49), (near, 0.48)
under (supports, 0.62), (below, 0.53)
top (supported_by, 0.65), (above, 0.61)
inside (inside, 0.48), (supported_by, 0.35)
right (right_of, 0.50), (lower_side, 0.38)
beside (outside, 0.45), (right_of, 0.45)
Table 2: Map of top keywords to spatial relations
(appropriate mappings in bold).
observed samples. Figure 5 shows predicted posi-
tions of objects using the learned priors.
5 Spatial Relations
We define a set of formal spatial relations that we
map to natural language terms (?5.1). In addi-
tion, we collect annotations of spatial relation de-
scriptions from people, learn a mapping of spatial
keywords to our formal spatial relations, and train
a classifier that given two objects can predict the
likelihood of a spatial relation holding (?5.2).
5.1 Predefined spatial relations
For spatial relations we use a set of predefined rela-
tions: left_of, right_of, above, below, front, back,
supported_by, supports, next_to, near, inside, out-
side, faces, left_side, right_side.3 These are mea-
sured using axis-aligned bounding boxes from the
viewer?s perspective; the involved bounding boxes
are compared to determine volume overlap or clos-
est distance (for proximity relations; see Table 1).
3Wedistinguish left_of(A,B) asA being left of the left edge
of the bounding box of B vs left_side(A,B) as A being left of
the centroid of B.
2031
Feature # Description
delta(A,B) 3 Delta position (x, y, z) between the centroids of A and B
dist(A,B) 1 Normalized distance (wrt B) between the centroids of A and B
overlap(A, f(B)) 6 Fraction of A inside left/right/front/back/top/bottom regions wrt B: V ol(A?f(B))
V ol(A)
overlap(A,B) 2 V ol(A?B)
V ol(A)
and V ol(A?B)
V ol(B)
support(A,B) 2 supported_by(A,B) and supports(A,B)
Table 3: Features for trained spatial relations predictor.
Figure 6: Our data collection task.
Since these spatial relations are resolvedwith re-
spect to the view of the scene, they correspond to
view-centric definitions of spatial concepts.
5.2 Learning Spatial Relations
We collect a set of text descriptions of spatial rela-
tionships between two objects in 3D scenes by run-
ning an experiment on Amazon Mechanical Turk.
We present a set of screenshots of scenes in our
dataset that highlight particular pairs of objects and
we ask people to fill in a spatial relationship of the
form ?The __ is __ the __? (see Fig 6). We col-
lected a total of 609 annotations over 131 object
pairs in 17 scenes. We use this data to learn pri-
ors on view-centric spatial relation terms and their
concrete geometric interpretation.
For each response, we select one keyword from
the text based on length. We learn a mapping of
the top 15 keywords to our predefined set of spa-
tial relations. We use our predefined relations on
annotated spatial pairs of objects to create a binary
indicator vector that is set to 1 if the spatial relation
holds, or zero otherwise. We then create a simi-
lar vector for whether the keyword appeared in the
annotation for that spatial pair, and then compute
the cosine similarity of the two vectors to obtain
a score for mapping keywords to spatial relations.
Table 2 shows the obtained mapping. Using just
the top mapping, we are able to map 10 of the 15
Above
Above On
Left Right
Front Behind
Figure 7: High probability regions where the cen-
ter of another object would occur for some spatial
relations with respect to a table: above (top left),
on (top right), left (mid left), right (mid right), in
front (bottom left), behind (bottom right).
keywords to an appropriate spatial relation. The 5
keywords that are not well mapped are proximity
relations that are not well captured by our prede-
fined spatial relations.
Using the 15 keywords as our spatial relations,
we train a log linear binary classifier for each key-
word over features of the objects involved in that
spatial relation (see Table 3). We then use this
model to predict the likelihood of that spatial re-
lation in new scenes.
Figure 7 shows examples of predicted likeli-
hoods for different spatial relations with respect to
an anchor object in a scene. Note that the learned
spatial relations are much stricter than our prede-
fined relations. For instance, ?above? is only used
to referred to the area directly above the table, not
to the region above and to the left or above and in
front (which our predefined classifier will all con-
sider to be above). In our results, we showwe have
more accurate scenes using the trained spatial re-
lations than the predefined ones.
2032
Dependency Pattern Example Text
{tag:VBN}=verb >nsubjpass {}=nsubj >prep ({}=prep >pobj {}=pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj].
attribute(verb,pobj)(nsubj,pobj) material(chair,wood)
{}=dobj >cop {} >nsubj {}=nsubj The chair[nsubj] is red[dobj].
attribute(dobj)(nsubj,dobj) color(chair,red)
{}=dobj >cop {} >nsubj {}=nsubj >prep ({}=prep >pobj {}=pobj) The table[nsubj] is next[dobj] to[prep] the chair[pobj].
spatial(dobj)(nsubj, pobj) next_to(table,chair)
{}=nsubj >advmod ({}=advmod >prep ({}=prep >pobj {}=pobj)) There is a table[nsubj] next[advmod] to[prep] a chair[pobj].
spatial(advmod)(nsubj, pobj) next_to(table,chair)
Table 4: Example dependency patterns for extracting attributes and spatial relations.
6 Text to Scene generation
We generate 3D scenes from brief scene descrip-
tions using our learned priors.
6.1 Scene Template Parsing
During scene template parsing we identify the
scene type, the objects present in the scene, their
attributes, and the relations between them. The
input text is first processed using the Stanford
CoreNLP pipeline (Manning et al., 2014). The
scene type is determined by matching the words
in the utterance against a list of known scene types
from the scene dataset.
To identify objects, we look for noun phrases
and use the head word as the category, filtering
with WordNet (Miller, 1995) to determine which
objects are visualizable (under the physical object
synset, excluding locations). We use the Stanford
coreference system to determine when the same
object is being referred to.
To identify properties of the objects, we extract
other adjectives and nouns in the noun phrase. We
alsomatch dependency patterns such as ?X ismade
of Y? to extract additional attributes. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we identify
a set of associated keywords to be used later for
querying the 3D model database.
Dependency patterns are also used to extract
spatial relations between objects (see Table 4 for
some example patterns). We use Semgrex patterns
to match the input text to dependencies (Cham-
bers et al., 2007). The attribute types are deter-
mined from a dictionary using the text express-
ing the attribute (e.g., attribute(red)=color, at-
tribute(round)=shape). Likewise, spatial relations
are looked up using the learned map of keywords
to spatial relations.
As an example, given the input ?There is a room
with a desk and a red chair. The chair is to the left
of the desk.? we extract the following objects and
spatial relations:
Objects category attributes keywords
o
0
room room
o
1
desk desk
o
2
chair color:red chair, red
Relations: left(o
2
, o
1
)
6.2 Inferring Implicits
From the parsed scene template, we infer the pres-
ence of additional objects and support constraints.
We can optionally infer the presence of addi-
tional objects from object occurrences based on the
scene type. If the scene type is unknown, we use
the presence of known object categories to pre-
dict the most likely scene type by using Bayes?
rule on our object occurrence priors P
occ
to get
P (C
s
|{C
o
}) ? P
occ
({C
o
}|C
s
)P (C
s
). Once we
have a scene type C
s
, we sample P
occ
to find ob-
jects that are likely to occur in the scene. We re-
strict sampling to the top n = 4 object categories.
We can also use the support hierarchy priors
P
support
to infer implicit objects. For instance, for
each object o
i
we find the most likely supporting
object category and add it to our scene if not al-
ready present.
After inferring implicit objects, we infer the sup-
port constraints. Using the learned text to prede-
fined relation mapping from ?5.2, we can map the
keywords ?on? and ?top? to the supported_by re-
lation. We infer the rest of the support hierarchy
by selecting for each object o
i
the parent object o
j
that maximizes P
support
(C
o
j
|C
o
i
).
6.3 Grounding Objects
Once we determine from the input text what ob-
jects exist and their spatial relations, we select 3D
models matching the objects and their associated
properties. Each object in the scene template is
grounded by querying a 3D models database with
2033
?There is a desk and a keyboard and a monitor .?
Input Text Basic + Support Hierarchy + Relative Positions
?There is a coffee table and there is a lamp behind the coffee table. There is a chair in front of the coffee table.?
UPDATE UPDATE
No Relations Predefined Relations Learned Relations
Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support
Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of
spatial relations (No Relations), scoring using Predefined Relations and Learned Relations.
the appropriate category and keywords.
We use a 3D model dataset collected from
Google 3DWarehouse by prior work in scene syn-
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). We then indexed all
models in a database that we query at run-time for
retrieval based on category and tag labels.
6.4 Scene Layout
Once we have instantiated the objects in the scene
by selecting models, we aim to optimize an over-
all layout score L = ?
obj
L
obj
+ ?
rel
L
rel
that is
a weighted sum of object arrangement L
obj
score
and constraint satisfaction L
rel
score:
L
obj
=
?
o
i
P
surf
(S
n
|C
o
i
)
?
o
j
?F (o
i
)
P
relpos
(?)
L
rel
=
?
c
i
P
rel
(c
i
)
where F (o
i
) are the sibling objects and parent ob-
ject of o
i
. We use ?
obj
= 0.25 and ?
rel
= 0.75 for
the results we present.
We use a simple hill climbing strategy to find a
reasonable layout. We first initialize the positions
Figure 9: Generated scene for ?There is a room
with a desk and a lamp. There is a chair to the
right of the desk.? The inferred scene hierarchy is
overlayed in the center.
of objects within the scene by traversing the sup-
port hierarchy in depth-first order, positioning the
children from largest to first and recursing. Child
nodes are positioned by first selecting a supporting
surface on a candidate parent object through sam-
pling of P
surf
. After selecting a surface, we sam-
ple a position on the surface based on P
relpos
. Fi-
nally, we check whether collisions exist with other
objects, rejecting layouts where collisions occur.
We iterate by randomly jittering and repositioning
objects. If there are any spatial constraints that are
not satisfied, we also remove and randomly repo-
sition the objects violating the constraints, and it-
erate to improve the layout. The resulting scene is
rendered and presented to the user.
7 Results and Discussion
We show examples of generated scenes, and com-
pare against naive baselines to demonstrate learned
priors are essential for scene generation. We
2034
Figure 10: Generated scene for ?There is a room
with a poster bed and a poster.?
Figure 11: Generated scene for ?living room?.
also discuss interesting aspects of using spatial
knowledge in view-based object referent resolu-
tion (?7.2) and in disambiguating geometric inter-
pretations of ?on? (?7.3).
Model Comparison Figure 8 shows a compari-
son of scenes generated by our model versus sev-
eral simpler baselines. The top row shows the im-
pact of modeling the support hierarchy and the rel-
ative positions in the layout of the scene. The bot-
tom row shows that the learned spatial relations
can give a more accurate layout than the naive
predefined spatial relations, since it captures prag-
matic implicatures of language, e.g., left is only
used for directly left and not top left or bottom
left (Vogel et al., 2013).
Figure 12: Left: chair is selected using ?the chair
to the right of the table? or ?the object to the right of
the table?. Chair is not selected for ?the cup to the
right of the table?. Right: Different view results
in different chair being selected for the input ?the
chair to the right of the table?.
7.1 Generated Scenes
Support Hierarchy Figure 9 shows a generated
scene along with the input text and support hier-
archy. Even though the spatial relation between
lamp and desk was not mentioned, we infer that the
lamp is supported by the top surface of the desk.
Disambiguation Figure 10 shows a generated
scene for the input ?There is a room with a poster
bed and a poster?. Note that the system differen-
tiates between a ?poster? and a ?poster bed? ? it
correctly selects and places the bed on the floor,
while the poster is placed on the wall.
Inferring objects for a scene type Figure 11
shows an example of inferring all the objects
present in a scene from the input ?living room?.
Some of the placements are good, while others can
clearly be improved.
7.2 View-centric object referent resolution
After a scene is generated, the user can refer to ob-
jects with their categories andwith spatial relations
between them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In Fig-
ure 12 (left), the user can select a chair to the right
of the table using the phrase ?chair to the right of
the table? or ?object to the right of the table?. The
user can then change their viewpoint by rotating
and moving around. Since spatial relations are re-
solved with respect to the current viewpoint, a dif-
ferent chair is selected for the same phrase from a
different viewpoint in the right screenshot.
7.3 Disambiguating ?on?
As shown in ?5.2, the English preposition ?on?,
when used as a spatial relation, corresponds
strongly to the supported_by relation. In our
trained model, the supported_by feature also has
a high positive weight for ?on?.
Our model for supporting surfaces and hierar-
chy allows interpreting the placement of ?A on
B? based on the categories of A and B. Fig-
ure 13 demonstrates four different interpretations
for ?on?. Given the input ?There is a cup on the
table? the system correctly places the cup on the
top surface of the table. In contrast, given ?There
is a cup on the bookshelf?, the cup is placed on a
supporting surface of the bookshelf, but not nec-
essarily the top one which would be fairly high.
2035
Figure 13: From top left clockwise: ?There is a
cup on the table?, ?There is a cup on the book-
shelf?, ?There is a poster on the wall?, ?There is
a hat on the chair?. Note the different geometric
interpretations of ?on?.
Given the input ?There is a poster on the wall?, a
poster is pasted on the wall, while with the input
?There is a hat on the chair? the hat is placed on
the seat of the chair.
7.4 Limitations
While the system shows promise, there are still
many challenges in text-to-scene generation. For
one, we did not address the difficulties of resolving
objects. A failure case of our system stems from
using a fixed set of categories to identify visualiz-
able objects. For example, the sense of ?top? refer-
ring to a spinning top, and other uncommon object
types, are not handled by our system as concrete
objects. Furthermore, complex phrases including
object parts such as ?there?s a coat on the seat of
the chair? are not handled. Figure 14 shows some
Figure 14: Left: A water bottle instead of wine
bottle is selected for ?There is a bottle of wine on
the table in the kitchen?. In addition, the selected
table is inappropriate for a kitchen. Right: A floor
lamp is incorrectly selected for the input ?There is
a lamp on the table?.
example cases where the context is important in
selecting an appropriate object and the difficulties
of interpreting noun phrases.
In addition, we rely on a few dependency pat-
terns for extracting spatial relations so robustness
to variations in spatial language is lacking. We
only handle binary spatial relations (e.g., ?left?,
?behind?) ignoringmore complex relations such as
?around the table? or ?in the middle of the room?.
Though simple binary relations are some of the
most fundamental spatial expressions and a good
first step, handling more complex expressions will
do much to improve the system.
Another issue is that the interpretation of sen-
tences such as ?the desk is covered with paper?,
which entails many pieces of paper placed on the
desk, is hard to resolve. With a more data-driven
approach we can hope to link such expressions to
concrete facts.
Finally, we use a traditional pipeline approach
for text processing, so errors in initial stages
can propagate downstream. Failures in depen-
dency parsing, part of speech tagging, or coref-
erence resolution can result in incorrect interpre-
tations of the input language. For example, in
the sentence ?there is a desk with a chair in front
of it?, ?it? is not identified as coreferent with
?desk? so we fail to extract the spatial relation
front_of(chair, desk).
8 Related Work
There is related prior work in the topics of mod-
eling spatial relations, generating 3D scenes from
text, and automatically laying out 3D scenes.
8.1 Spatial knowledge and relations
Prior work that required modeling spatial knowl-
edge has defined representations specific to the
task addressed. Typically, such knowledge is man-
ually provided or crowdsourced ? not learned from
data. For instance, WordsEye (Coyne et al., 2010)
uses a set of manually specified relations. The
NLP community has explored grounding text to
physical attributes and relations (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013), gener-
ating text for referring to objects (FitzGerald et
al., 2013) and connecting language to spatial re-
lationships (Vogel and Jurafsky, 2010; Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Most
of this work focuses on learning a mapping from
text to formal representations, and does not model
2036
implicit spatial knowledge. Many priors on real
world spatial facts are typically unstated in text and
remain largely unaddressed.
8.2 Text to Scene Systems
Early work on the SHRDLU system (Winograd,
1972) gives a good formalization of the linguis-
tic manipulation of objects in 3D scenes. By re-
stricting the discourse domain to a micro-world
with simple geometric shapes, the SHRDLU sys-
tem demonstrated parsing of natural language in-
put for manipulating scenes. However, generaliza-
tion to more complex objects and spatial relations
is still very hard to attain.
More recently, a pioneering text-to-3D scene
generation prototype system has been presented by
WordsEye (Coyne and Sproat, 2001). The authors
demonstrated the promise of text to scene genera-
tion systems but also pointed out some fundamen-
tal issues which restrict the success of their system:
much spatial knowledge is required which is hard
to obtain. As a result, users have to use unnatural
language (e.g., ?the stool is 1 feet to the south of
the table?) to express their intent. Follow up work
has attempted to collect spatial knowledge through
crowd-sourcing (Coyne et al., 2012), but does not
address the learning of spatial priors.
We address the challenge of handling natural
language for scene generation, by learning spatial
knowledge from 3D scene data, and using it to in-
fer unstated implicit constraints. Our work is simi-
lar in spirit to recent work on generating 2D clipart
for sentences using probabilistic models learned
from data (Zitnick et al., 2013).
8.3 Automatic Scene Layout
Work on scene layout has focused on determining
good furniture layouts by optimizing energy func-
tions that capture the quality of a proposed layout.
These energy functions are encoded from design
guidelines (Merrell et al., 2011) or learned from
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
We leverage ideas from this work, but they do not
focus on linking spatial knowledge to language.
9 Conclusion and Future Work
We have demonstrated a representation of spatial
knowledge that can be learned from 3D scene data
and how it corresponds to natural language. We
also showed that spatial inference and grounding is
critical for achieving plausible results in the text-
to-3D scene generation task. Spatial knowledge is
critically useful not only in this task, but also in
other domains which require an understanding of
the pragmatics of physical environments.
We only presented a deterministic approach for
mapping input text to the parsed scene template.
An interesting avenue for future research is to
automatically learn how to parse text describing
scenes into formal representations by using more
advanced semantic parsing methods.
We can also improve the representation used for
spatial priors of objects in scenes. For instance, in
this paper we represented support surfaces by their
orientation. We can improve the representation by
modeling whether a surface is an interior or exte-
rior surface.
Another interesting line of future work would
be to explore the influence of object identity in de-
termining when people use ego-centric or object-
centric spatial reference models, and to improve
resolution of spatial terms that have different in-
terpretations (e.g., ?the chair to the left of John? vs
?the chair to the left of the table?).
Finally, a promising line of research is to explore
using spatial priors for resolving ambiguities dur-
ing parsing. For example, the attachment of ?next
to? in ?Put a lamp on the table next to the book? can
be readily disambiguated with spatial priors such
as the ones we presented.
Acknowledgments
We thank the anonymous reviewers for their
thoughtful comments. We gratefully acknowl-
edge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program under
Air Force Research Laboratory (AFRL) contract
no. FA8750-13-2-0040. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
2037
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Bob Coyne, Richard Sproat, and Julia Hirschberg.
2010. Spatial relations in text-to-scene conversion.
InComputational Models of Spatial Language Inter-
pretation, Workshop at Spatial Cognition.
BobCoyne, Alexander Klapheke, MasoudRouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP).
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).
Alan D. Kalvin and Russell H. Taylor. 1996. Super-
faces: Polygonal mesh simplification with bounded
error. Computer Graphics and Applications, IEEE.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Con-
ference on.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference onMa-
chine Learning (ICML).
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
George A. Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, andHal Daum? III. 2012.
Midge: Generating image descriptions from com-
puter vision detections. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approxi-
mate Decentralized-POMDPs. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
2038
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 78?82, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SUTIME: Evaluation in TempEval-3
Angel X. Chang
Stanford University
angelx@cs.stanford.edu
Christopher D. Manning
Stanford University
manning@cs.stanford.edu
Abstract
We analyze the performance of SUTIME, a
temporal tagger for recognizing and normal-
izing temporal expressions, on TempEval-3
Task A for English. SUTIME is available as
part of the Stanford CoreNLP pipeline and can
be used to annotate documents with temporal
information. Testing on the TempEval-3 eval-
uation corpus showed that this system is com-
petitive with state-of-the-art techniques.
1 Introduction
The importance of modeling temporal information
is increasingly apparent in natural language appli-
cations, such as information extraction and ques-
tion answering. Extracting temporal information re-
quires the ability to recognize temporal expressions,
and to convert them from text to a normalized form
that is easy to process. Temporal tagging systems
are designed to address this problem. In this paper,
we evaluate the performance of the SUTIME (Chang
and Manning, 2012) rule-based temporal tagging
system.
We evaluate the performance of SUTIME on ex-
tracting temporal information in TempEval-3 (Uz-
Zaman et al, 2013), which requires systems to auto-
matically annotate documents with temporal infor-
mation using TimeML (Pustejovsky et al, 2003).
The TempEval-3 training data contains gold human
annotated data from TimeBank, AQUAINT, and a
new dataset of silver data automatically annotated
using a combination of TipSem (Llorens et al, 2010)
and TRIOS (UzZaman and Allen, 2010), two of the
best performing systems from TempEval-2 (Verha-
gen et al, 2010).
2 System Description
We use the Stanford CoreNLP1 pipeline with SU-
TIME to identify and normalize TIMEX32 ex-
pressions. SUTIME is incorporated into Stanford
CoreNLP as part of the Named Entity Recognition
annotator. For TempEval-3, we use the standard set
of rules provided with SUTIME. Since SUTIME can
also recognize temporal expressions whose values
are not specified by TIMEX3, we ran SUTIME in
a TIMEX3 compatible mode.3
2.1 SUTime
SUTIME is a rule-based temporal tagger built on
regular expression patterns over tokens. Tempo-
ral expressions are bounded in their complexity, so
many of them can be captured using finite automata.
As shown by systems such as FASTUS (Hobbs et
al., 1997), a cascade of finite automata can be very
effective at extracting information from text. With
SUTIME, we follow a similar staged strategy of
(i) building up patterns over individual words to
find numerical expressions; then (ii) using patterns
over words and numerical expressions to find sim-
ple temporal expressions; and finally (iii) forming
composite patterns over the discovered temporal ex-
pressions.
SUTIME recognizes Time, Duration, Interval,
and Set according to the TIMEX3 specification. In
1nlp.stanford.edu/software/corenlp.shtml
2www.timeml.org
3sutime.restrictToTimex3 = true
78
addition, it recognizes nested time expressions and
duration ranges. To achieve this it uses a temporal
pattern language defined over tokens (a regular ex-
pression language for expressing how tokenized text
should be mapped to temporal objects). SUTIME is
built on top of TOKENSREGEX,4 a generic frame-
work included in Stanford CoreNLP for definining
patterns over text and mapping to semantic objects.
With TOKENSREGEX we have access to any anno-
tations provided by the Stanford CoreNLP system,
such as the part-of-speech tag or the lemma. The full
specification of the pattern language is available at
nlp.stanford.edu/software/sutime.shtml.
To recognize temporal expressions, SUTIME ap-
plies three types of rules, in the following order: 1)
text regex rules: mappings from simple regular ex-
pressions over characters or tokens to temporal rep-
resentations; 2) compositional rules: mappings from
regular expressions over chunks (both tokens and
temporal objects) to temporal representations and
3) filtering rules: in which ambiguous expressions
that are likely to not be temporal expressions are re-
moved from the list of candidates (such as fall and
spring by themselves). The compositional rules are
applied repeatedly until the final list of time expres-
sions stablizes.
After all the temporal expressions have been rec-
ognized, each temporal expression is associated with
a temporal object. Each temporal object is resolved
with respect to the reference date using heuristic
rules. In this step, relative times are converted to
an absolute time, and composite time objects are
simplified as much as possible. The final resolution
of relative temporal expressions is currently limited
due to the usage of simple hard-coded rules (e.g. rel-
ative to document date with local context inform-
ing before and after heuristics). Finally, SUTIME
will take the internal time representation and pro-
duce a TIMEX3 annotation for each temporal ex-
pression. SUTIME currently only handles English.
It can however, be extended to other languages by
creating sets of rules for additional languages.
3 Evaluation
We evaluated SUTIME?s performance on the
TempEval-3 Task A for English. Task A consists
4nlp.stanford.edu/software/tokensregex.shtml
of determining the extent of time expressions as de-
fined by the TimeML TIMEX3 tag, as well as pro-
viding normalized attributes for type and value. Ex-
tracted temporal expressions from the system and
the gold are matched, and precision, recall, and F1
are computed. For the evaluation of extents, there
are two metrics: a relaxed match score for identi-
fying a matching temporal expression, and a strict
match that requires the text to be matched exactly.
For example, identifying the twentieth century when
the gold is twentieth centry will give a relaxed match
but not a strict match. For the type and value at-
tributes, an accuracy and a measure of the F1 with
respect to the relaxed match is given.
We compare SUTIME?s performance with several
other top systems on the English TempEval-3 Task
A. We also include TIPSem which was used to cre-
ate the silver data for TempEval-3 as a baseline. Of
the systems that prepared multiple runs, we selected
the best performing run to report. Table 1 gives the
results for these systems on the TempEval-3 evalu-
ation set. Interestingly, NavyTime which uses SU-
TIME for Task A, actually did better than SUTIME
in the value normalization and is effectively the 2nd
best system in Task A. The performance of Navy-
Time is otherwise identical to SUTIME. In Navy-
Time the normalization was tuned to the TimeBank
annotation whereas the SUTIME submission was
untuned. SUTIME has the highest recall in discov-
ering temporal expressions. It also has the high-
est overall relaxed F1, slightly higher than Heidel-
Time (Stro?tgen and Gertz, 2010) (cleartk had the
highest strict F1 of 82.71). Not surprisingly, the sys-
tem used to generate the silver data, TIPSem, had
the highest precision when extracting temporal ex-
pressions. For normalization, HeidelTime had the
overall best performance on value and type. Both
SUTIME and HeidelTime are rule-based, indicating
the effectiveness of using rules for this domain. An-
other top performing system, ManTime used condi-
tional random fields, a machine learning approach,
for identifying temporal expressions and rules for
normalization.
79
Identification Normalization
Relaxed Strict Value Type
System F1 P R F1 P R F1 Accuracy F1 Accuracy
SUTime 90.32 89.36 91.30 79.57 78.72 80.43 67.38 74.60 80.29 88.90
NavyTime 90.32 89.36 91.30 79.57 78.72 80.43 70.97 78.58 80.29 88.90
HeidelTime 90.30 93.08 87.68 81.34 83.85 78.99 77.61 85.95 82.09 90.91
ManTime 89.66 95.12 84.78 74.33 78.86 70.29 68.97 76.92 77.39 86.31
TIPSem 84.90 97.20 75.36 81.63 93.46 72.46 65.31 76.93 75.92 89.42
Table 1: TempEval-3; English Platinum Test set.
4 Error Analysis
Given the small size of the platinum data set, we
were able to perform thorough error analysis of the
errors made by SUTIME on the data set.
Table 2 shows the number of temporal expres-
sions marked by the evaluation script as being in-
correct. The errors can be grouped into three broad
categories: i) those proposed by the system but not
in the gold (relaxed precision errors), ii) those in the
gold but not identified by the system (relaxed recall
errors), and iii) temporal expressions with the wrong
value (and sometimes type) normalization.
Of the 14 precision errors, many of the temporal
expressions suggested by the system are reasonable.
For instance, current is identified by the system. A
few of the errors are not actual temporal expres-
sions. For example, in the phrase British Summer
Time, Summer was identified as a temporal expres-
sion which is not correct.
Given SUTime?s high recall, only a few temporal
expressions in the gold are not found by the system.
In most cases, the temporal expressions missed by
SUTIME do not have a well defined value associated
with them (e.g. ?digital age?, ?each season?).
Performance using the strict match metric is not
as good as some other systems. SUTIME was
derived from GUTime (Mani, 2004) and focuses
on matching longer time expressions as per ear-
lier guidelines. Thus it is less conformant to the
more current TimeML guidelines of having minimal
blocks. For instance, SUTIME treats 2009-2010 as
a range, whereas the gold standard treats it as two
separate dates. This results in an incorrect value nor-
malization and a recall error.
We now examine the cases where the SUTIME
normalization differed from the gold. Table 3 shows
a further breakdown of these errors.
Error type Count
System not in gold (precision) 14
Gold not in system (recall) 12
Wrong value 32
Table 2: Summary of errors made by SUTIME on the
platinum data set
Error type Count
Value incorrectly resolved wrt to DCT 7
Value should not be resolved wrt to DCT 5
DURATION resolved to DATE 6
DATE misidentified as DURATION 3
Wrong granularity 4
Wrong normalization for set 2
Different normalization 3
Other 2
Table 3: Break down of value errors made by SUTime on
the platinum data set
One weakness of SUTIME is that temporal ex-
pressions are always resolved with respect to the
document creation time (DCT). While this heuris-
tic works fairly well in most cases, and SUTime can
achieve reasonable performance, there are obvious
limitations with this approach. For instance, some-
times it is more appropriate to resolve the tempo-
ral expression with respect to nearby dates or events
in the text. As an example, in the test document
CNN 20130322 1003 there is the sentence Call me
Sunday night at 8 PM at the resort that is part of
an email of an unknown date. In this case, SUTIME
still attempts to resolve the temporal expression Sun-
day night at 8 PM using the document creation time
which is incorrect.
There can be inherent ambiguity as to which time
point a time expression refers to. For instance, given
a reference date of 2011-09-19, a Monday, it is un-
80
clear whether Friday refers to 2011-09-16 or 2011-
09-23. SUTIME will normally resolve to the closest
date/time with respect to the reference date. SU-
TIME also has some rules that will use the verb tense
of the surrounding words to attempt to resolve the
ambiguity. For instance, if a verb close to the tem-
poral expression has a POS tag of VBD (past tense
verb) then the expression will be resolved so that it
occurs before the document date.
Most of the type errors are due to confusions be-
tween DATE and DURATION. Often SUTIME will
attempt to resolve a DURATION as a DATE. For
instance, given the phrase ?the following decade?,
SUTIME will attempt to resolve that as a DATE with
value 202X (using a DCT of 2013-03-22). While
this can be desirable in some cases, this is not what
the gold annotation contains: type of DURATION
and value of P1DE. In some other cases, SUTIME
misidentifies DURATION as a DATE. For instance,
it lacks rules to parse the 3:07:35 in finishing in
3:07:35 as a duration.
Another problem faced by SUTIME is in figuring
out the correct granularity to use. Given a document
date of 2013-03-22, it will identify two years ago as
being 2011-03-22. However, since these expressions
indicate a less precise date, the gold annotation is a
simple 2011.
SUTIME also provided the wrong normalization
for SET in several cases. For the expression every
morning, SUTIME reported a value of TMO when
the gold annotation was XXXX-XX-XXTMO. In
other cases, SUTIME offered an alternative normal-
ization, for instance, a value of 19XX for the 20th
century instead of just 19. And PTXM instead of
PXM for minutes. In this case, the PTXM is more
correct as the T is required by ISO-8601 to differ-
entiate between M for month, and M for minutes.
The remaining errors are due to lacking rules such
as SUTIME?s inability to handle time zones in cer-
tain cases.
5 Discussion
As a rule-based system, SUTIME is limited by the
coverage of its rule set for the different types of
temporal expressions it can recognize. Many of the
errors in SUTIME can be resolved by adding more
rules to the system.
One key to improving the normalization of the
value is to have better resolution of ambiguous tem-
poral expressions. Identifying when temporal ex-
pressions should not be resolved using the document
creation time, and how the temporal expression re-
lates to other temporal expressions or events within
the document is also critical. This suggests that nor-
malization can benefit from being able to perform
TempEval-3 Task C well.
Another approach to improving the system would
be to provide different modes of use: a mode for end
users that would like complex temporal expressions
to be identified, or a mode for more basic temporal
expressions that can be used as input for other tem-
poral systems. Allowing for nested TIMEXes would
also benefit the system?s performance. For example,
2009-2010 should be a range, with a nested timex
for 2009 and 2010.
Another interesting direction to explore would
be to evaluate the performance of SUTIME on do-
mains other than current news. Since SUTIME also
supports temporal expressions such as holidays and
more distant dates such as 400 B.C., it would be in-
teresting to see how well SUTIME can extract these
different types of temporal expressions.
6 Conclusion
We have evaluated SUTIME by participating in
TempEval-3 Task A and have shown that it is a
competitive system for extracting time expressions.
By providing it as part of the Stanford CoreNLP
pipeline, we hope that it can be easily used as a basic
component for building temporally aware systems.
Acknowledgements
We would like to acknowledge Justin Heermann,
Jonathan Potter, Garrett Schlesinger and John Bauer
for helping to implement parts of the system for
TempEval-3.
References
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In 8th International Conference on
Language Resources and Evaluation (LREC 2012).
Jerry R. Hobbs, Douglas E. Appelt, John Bear, David Is-
rael, Megumi Kameyama, Mark Stickel, and Mabry
81
Tyson. 1997. FASTUS: A cascaded finite-state
transducer for extracting information from natural-
language text. Finite State Devices for Natural Lan-
guage Processing, pages 383?406.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. TIPSem (English and Spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 284?291. Association for Compu-
tational Linguistics.
Inderjeet Mani. 2004. Recent developments in temporal
information extraction. In Proceedings of RANLP03,
pages 45?60.
James Pustejovsky, Jos Castao, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust specification of event
and temporal expressions in text. In in Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5.
Jannik Stro?tgen and Michael Gertz. 2010. HeidelTime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
321?324.
Naushad UzZaman and James F Allen. 2010. TRIPS and
TRIOS system for TempEval-2: Extracting temporal
information from text. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
276?283. Association for Computational Linguistics.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
Marc Verhagen, James Allen, and James Pustejovsky.
2013. SemEval-2013 Task 1: TempEval-3: Evaluat-
ing time expressions, events, and temporal relations.
In Proceedings of the 7th International Workshop on
Semantic Evaluation, SemEval ?13.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
?10, pages 57?62. Association for Computational Lin-
guistics.
82
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Stanford?s Multi-Pass Sieve Coreference Resolution System at the
CoNLL-2011 Shared Task
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky
Stanford NLP Group
Stanford University, Stanford, CA 94305
{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu
Abstract
This paper details the coreference resolution
system submitted by Stanford at the CoNLL-
2011 shared task. Our system is a collection
of deterministic coreference resolution mod-
els that incorporate lexical, syntactic, seman-
tic, and discourse information. All these mod-
els use global document-level information by
sharing mention attributes, such as gender and
number, across mentions in the same cluster.
We participated in both the open and closed
tracks and submitted results using both pre-
dicted and gold mentions. Our system was
ranked first in both tracks, with a score of 57.8
in the closed track and 58.3 in the open track.
1 Introduction
This paper describes the coreference resolution sys-
tem used by Stanford at the CoNLL-2011 shared
task (Pradhan et al, 2011). Our system extends
the multi-pass sieve system of Raghunathan et
al. (2010), which applies tiers of deterministic coref-
erence models one at a time from highest to lowest
precision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, this model propagates
global information by sharing attributes (e.g., gender
and number) across mentions in the same cluster.
We made three considerable extensions to the
Raghunathan et al (2010) model. First, we added
five additional sieves, the majority of which address
the semantic similarity between mentions, e.g., us-
ing WordNet distance, and shallow discourse under-
standing, e.g., linking speakers to compatible pro-
nouns. Second, we incorporated a mention detection
sieve at the beginning of the processing flow. This
sieve filters our syntactic constituents unlikely to be
mentions using a simple set of rules on top of the
syntactic analysis of text. And lastly, we added a
post-processing step, which guarantees that the out-
put of our system is compatible with the shared task
and OntoNotes specifications (Hovy et al, 2006;
Pradhan et al, 2007).
Using this system, we participated in both the
closed1 and open2 tracks, using both predicted and
gold mentions. Using predicted mentions, our sys-
tem had an overall score of 57.8 in the closed track
and 58.3 in the open track. These were the top scores
in both tracks. Using gold mentions, our system
scored 60.7 in the closed track in 61.4 in the open
track.
We describe the architecture of our entire system
in Section 2. In Section 3 we show the results of sev-
eral experiments, which compare the impact of the
various features in our system, and analyze the per-
formance drop as we switch from gold mentions and
annotations (named entity mentions and parse trees)
to predicted information. We also report in this sec-
tion our official results in the testing partition.
1Only the provided data can be used, i.e., WordNet and gen-
der gazetteer.
2Any external knowledge source can be used. We used
additional animacy, gender, demonym, and country and states
gazetteers.
28
2 System Architecture
Our system consists of three main stages: mention
detection, followed by coreference resolution, and
finally, post-processing. In the first stage, mentions
are extracted and relevant information about men-
tions, e.g., gender and number, is prepared for the
next step. The second stage implements the ac-
tual coreference resolution of the identified men-
tions. Sieves in this stage are sorted from highest
to lowest precision. For example, the first sieve (i.e.,
highest precision) requires an exact string match be-
tween a mention and its antecedent, whereas the
last one (i.e., lowest precision) implements pronom-
inal coreference resolution. Post-processing is per-
formed to adjust our output to the task specific con-
straints, e.g., removing singletons.
It is important to note that the first system stage,
i.e., the mention detection sieve, favors recall heav-
ily, whereas the second stage, which includes the ac-
tual coreference resolution sieves, is precision ori-
ented. Our results show that this design lead to
state-of-the-art performance despite the simplicity
of the individual components. This strategy has
been successfully used before for information ex-
traction, e.g., in the BioNLP 2009 event extraction
shared task (Kim et al, 2009), several of the top sys-
tems had a first high-recall component to identify
event anchors, followed by high-precision classi-
fiers, which identified event arguments and removed
unlikely event candidates (Bjo?rne et al, 2009). In
the coreference resolution space, several works have
shown that applying a list of rules from highest to
lowest precision is beneficial for coreference reso-
lution (Baldwin, 1997; Raghunathan el al., 2010).
However, we believe we are the first to show that this
high-recall/high-precision strategy yields competi-
tive results for the complete task of coreference res-
olution, i.e., including mention detection and both
nominal and pronominal coreference.
2.1 Mention Detection Sieve
In our particular setup, the recall of the mention de-
tection component is more important than its preci-
sion, because any missed mentions are guaranteed
to affect the final score, but spurious mentions may
not impact the overall score if they are left as sin-
gletons, which are discarded by our post-processing
step. Therefore, our mention detection algorithm fo-
cuses on attaining high recall rather than high preci-
sion. We achieve our goal based on the list of sieves
sorted by recall (from highest to lowest). Each sieve
uses syntactic parse trees, identified named entity
mentions, and a few manually written patterns based
on heuristics and OntoNotes specifications (Hovy et
al., 2006; Pradhan et al, 2007). In the first and
highest recall sieve, we mark all noun phrase (NP),
possessive pronoun, and named entity mentions in
each sentence as candidate mentions. In the follow-
ing sieves, we remove from this set al mentions that
match any of the exclusion rules below:
1. We remove a mention if a larger mention with
the same head word exists, e.g., we remove The
five insurance companies in The five insurance
companies approved to be established this time.
2. We discard numeric entities such as percents,
money, cardinals, and quantities, e.g., 9%,
$10, 000, Tens of thousands, 100 miles.
3. We remove mentions with partitive or quanti-
fier expressions, e.g., a total of 177 projects.
4. We remove pleonastic it pronouns, detected us-
ing a set of known expressions, e.g., It is possi-
ble that.
5. We discard adjectival forms of nations, e.g.,
American.
6. We remove stop words in a predetermined list
of 8 words, e.g., there, ltd., hmm.
Note that the above rules extract both mentions in
appositive and copulative relations, e.g., [[Yongkang
Zhou] , the general manager] or [Mr. Savoca] had
been [a consultant. . . ]. These relations are not an-
notated in the OntoNotes corpus, e.g., in the text
[[Yongkang Zhou] , the general manager], only the
larger mention is annotated. However, appositive
and copulative relations provide useful (and highly
precise) information to our coreference sieves. For
this reason, we keep these mentions as candidates,
and remove them later during post-processing.
2.2 Mention Processing
Once mentions are extracted, we sort them by sen-
tence number, and left-to-right breadth-first traversal
29
order in syntactic trees in the same sentence (Hobbs,
1977). We select for resolution only the first men-
tions in each cluster,3 for two reasons: (a) the first
mention tends to be better defined (Fox, 1993),
which provides a richer environment for feature ex-
traction; and (b) it has fewer antecedent candidates,
which means fewer opportunities to make a mis-
take. For example, given the following ordered list
of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6}, where
the subscript indicates textual order and the super-
script indicates cluster id, our model will attempt
to resolve only m22 and m
3
4. Furthermore, we dis-
card first mentions that start with indefinite pronouns
(e.g., some, other) or indefinite articles (e.g., a, an)
if they have no antecedents that have the exact same
string extents.
For each selected mention mi, all previous men-
tions mi?1, . . . , m1 become antecedent candidates.
All sieves traverse the candidate list until they find
a coreferent antecedent according to their criteria
or reach the end of the list. Crucially, when com-
paring two mentions, our approach uses informa-
tion from the entire clusters that contain these men-
tions instead of using just information local to the
corresponding mentions. Specifically, mentions in
a cluster share their attributes (e.g., number, gen-
der, animacy) between them so coreference decision
are better informed. For example, if a cluster con-
tains two mentions: a group of students, which is
singular, and five students, which is plural,
the number attribute of the entire cluster becomes
singular or plural, which allows it to match
other mentions that are both singular and plural.
Please see (Raghunathan et al, 2010) for more de-
tails.
2.3 Coreference Resolution Sieves
2.3.1 Core System
The core of our coreference resolution system is
an incremental extension of the system described in
Raghunathan et al (2010). Our core model includes
two new sieves that address nominal mentions and
are inserted based on their precision in a held-out
corpus (see Table 1 for the complete list of sieves
deployed in our system). Since these two sieves use
3We initialize the clusters as singletons and grow them pro-
gressively in each sieve.
Ordered sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve (e.g., appositives)
6-8. Strict Head Matching Sieves A-C
9. Proper Head Word Match Sieve
10. Alias Sieve
11. Relaxed Head Matching Sieve
12. Lexical Chain Sieve
13. Pronouns Sieve
Table 1: The sieves in our system; sieves new to this pa-
per are in bold.
simple lexical constraints without semantic informa-
tion, we consider them part of the baseline model.
Relaxed String Match: This sieve considers two
nominal mentions as coreferent if the strings ob-
tained by dropping the text following their head
words are identical, e.g., [Clinton] and [Clinton,
whose term ends in January].
Proper Head Word Match: This sieve marks two
mentions headed by proper nouns as coreferent if
they have the same head word and satisfy the fol-
lowing constraints:
Not i-within-i - same as Raghunathan et al (2010).
No location mismatches - the modifiers of two men-
tions cannot contain different location named entities,
other proper nouns, or spatial modifiers. For example,
[Lebanon] and [southern Lebanon] are not coreferent.
No numeric mismatches - the second mention cannot
have a number that does not appear in the antecedent, e.g.,
[people] and [around 200 people] are not coreferent.
In addition to the above, a few more rules are
added to get better performance for predicted men-
tions.
Pronoun distance - sentence distance between a pronoun
and its antecedent cannot be larger than 3.
Bare plurals - bare plurals are generic and cannot have a
coreferent antecedent.
2.3.2 Semantic-Similarity Sieves
We first extend the above system with two
new sieves that exploit semantics from WordNet,
Wikipedia infoboxes, and Freebase records, drawing
on previous coreference work using these databases
(Ng & Cardie, 2002; Daume? & Marcu, 2005;
Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,
30
2007; Bengston & Roth, 2008; Huang et al, 2009;
inter alia). Since the input to a sieve is a collection of
mention clusters built by the previous (more precise)
sieves, we need to link mention clusters (rather than
individual mentions) to records in these three knowl-
edge bases. The following steps generate a query for
these resources from a mention cluster.
First, we select the most representative mention
in a cluster by preferring mentions headed by proper
nouns to mentions headed by common nouns, and
nominal mentions to pronominal ones. In case of
ties, we select the longer string. For example, the
mention selected from the cluster {President George
W. Bush, president, he} is President George W.
Bush. Second, if this mention returns nothing from
the knowledge bases, we implement the following
query relaxation algorithm: (a) remove the text fol-
lowing the mention head word; (b) select the lowest
noun phrase (NP) in the parse tree that includes the
mention head word; (c) use the longest proper noun
(NNP*) sequence that ends with the head word; (d)
select the head word. For example, the query pres-
ident Bill Clinton, whose term ends in January is
successively changed to president Bill Clinton, then
Bill Clinton, and finally Clinton. If multiple records
are returned, we keep the top two for Wikipedia and
Freebase, and all synsets for WordNet.
Alias Sieve
This sieve addresses name aliases, which are de-
tected as follows. Two mentions headed by proper
nouns are marked as aliases (and stored in the same
entity cluster) if they appear in the same Wikipedia
infobox or Freebase record in either the ?name? or
?alias? field, or they appear in the same synset in
WordNet. As an example, this sieve correctly de-
tects America Online and AOL as aliases. We also
tested the utility of Wikipedia categories, but found
little gain over morpho-syntactic features.
Lexical Chain Sieve
This sieve marks two nominal mentions as coref-
erent if they are linked by a WordNet lexical chain
that traverses hypernymy or synonymy relations. We
use all synsets for each mention, but restrict it to
mentions that are at most three sentences apart, and
lexical chains of length at most four. This sieve cor-
rectly links Britain with country, and plane with air-
craft.
To increase the precision of the above two sieves,
we use additional constraints before two mentions
can match: attribute agreement (number, gender, an-
imacy, named entity labels), no i-within-i, no loca-
tion or numeric mismatches (as in Section 2.3.1),
and we do not use the abstract entity synset in Word-
Net, except in chains that include ?organization?.
2.3.3 Discourse Processing Sieve
This sieve matches speakers to compatible pro-
nouns, using shallow discourse understanding to
handle quotations and conversation transcripts. Al-
though more complex discourse constraints have
been proposed, it has been difficult to show improve-
ments (Tetreault & Allen, 2003; 2004).
We begin by identifying speakers within text. In
non-conversational text, we use a simple heuristic
that searches for the subjects of reporting verbs (e.g.,
say) in the same sentence or neighboring sentences
to a quotation. In conversational text, speaker infor-
mation is provided in the dataset.
The extracted speakers then allow us to imple-
ment the following sieve heuristics:
? ?I?s4 assigned to the same speaker are coreferent.
? ?you?s with the same speaker are coreferent.
? The speaker and ?I?s in her text are coreferent.
For example, I, my, and she in the following sen-
tence are coreferent: ?[I] voted for [Nader] because
[he] was most aligned with [my] values,? [she] said.
In addition to the above sieve, we impose speaker
constraints on decisions made by subsequent sieves:
? The speaker and a mention which is not ?I? in the
speaker?s utterance cannot be coreferent.
? Two ?I?s (or two ?you?s, or two ?we?s) assigned to
different speakers cannot be coreferent.
? Two different person pronouns by the same speaker
cannot be coreferent.
? Nominal mentions cannot be coreferent with ?I?,
?you?, or ?we? in the same turn or quotation.
? In conversations, ?you? can corefer only with the
previous speaker.
For example, [my] and [he] are not coreferent in the
above example (third constraint).
4We define ?I? as ?I?, ?my?, ?me?, or ?mine?, ?we? as first
person plural pronouns, and ?you? as second person pronouns.
31
Annotations Coref R P F1
Gold Before 92.8 37.7 53.6
Gold After 75.1 70.1 72.6
Not gold Before 87.9 35.6 50.7
Not gold After 71.7 68.4 70.0
Table 2: Performance of the mention detection compo-
nent, before and after coreference resolution, with both
gold and actual linguistic annotations.
2.4 Post Processing
To guarantee that the output of our system matches
the shared task requirements and the OntoNotes
annotation specification, we implement two post-
processing steps:
? We discard singleton clusters.
? We discard the mention that appears later in
text in appositive and copulative relations. For
example, in the text [[Yongkang Zhou] , the
general manager] or [Mr. Savoca] had been
[a consultant. . . ], the mentions Yongkang Zhou
and a consultant. . . are removed in this stage.
3 Results and Discussion
Table 2 shows the performance of our mention de-
tection algorithm. We show results before and after
coreference resolution and post-processing (when
singleton mentions are removed). We also list re-
sults with gold and predicted linguistic annotations
(i.e., syntactic parses and named entity recognition).
The table shows that the recall of our approach is
92.8% (if gold annotations are used) or 87.9% (with
predicted annotations). In both cases, precision is
low because our algorithm generates many spurious
mentions due to its local nature. However, as the ta-
ble indicates, many of these mentions are removed
during post-processing, because they are assigned
to singleton clusters during coreference resolution.
The two main causes for our recall errors are lack
of recognition of event mentions (e.g., verbal men-
tions such as growing) and parsing errors. Parsing
errors often introduce incorrect mention boundaries,
which yield both recall and precision errors. For
example, our system generates the predicted men-
tion, the working meeting of the ?863 Program? to-
day, for the gold mention the working meeting of the
?863 Program?. Due to this boundary mismatch,
all mentions found to be coreferent with this pre-
dicted mention are counted as precision errors, and
all mentions in the same coreference cluster with the
gold mention are counted as recall errors.
Table 3 lists the results of our end-to-end system
on the development partition. ?External Resources?,
which were used only in the open track, includes: (a)
a hand-built list of genders of first names that we cre-
ated, incorporating frequent names from census lists
and other sources, (b) an animacy list (Ji and Lin,
2009), (c) a country and state gazetteer, and (d) a de-
monym list. ?Discourse? stands for the sieve intro-
duced in Section 2.3.3. ?Semantics? stands for the
sieves presented in Section 2.3.2. The table shows
that the discourse sieve yields an improvement of
almost 2 points to the overall score (row 1 versus
3), and external resources contribute 0.5 points. On
the other hand, the semantic sieves do not help (row
3 versus 4). The latter result contradicts our initial
experiments, where we measured a minor improve-
ment when these sieves were enabled and gold men-
tions were used. Our hypothesis is that, when pre-
dicted mentions are used, the semantic sieves are
more likely to link spurious mentions to existing
clusters, thus introducing precision errors. This sug-
gests that a different tuning of the sieve parameters
is required for the predicted mention scenario. For
this reason, we did not use the semantic sieves for
our submission. Hence, rows 2 and 3 in the table
show the performance of our official submission in
the development set, in the closed and open tracks
respectively.
The last three rows in Table 3 give insight on the
impact of gold information. This analysis indicates
that using gold linguistic annotation yields an im-
provement of only 2 points. This implies that the
quality of current linguistic processors is sufficient
for the task of coreference resolution. On the other
hand, using gold mentions raises the overall score by
15 points. This clearly indicates that pipeline archi-
tectures where mentions are identified first are inad-
equate for this task, and that coreference resolution
might benefit from the joint modeling of mentions
and coreference chains.
Finally, Table 4 lists our results on the held-out
testing partition. Note that in this dataset, the gold
mentions included singletons and generic mentions
32
Components MUC B3 CEAFE BLANC
ER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?
58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?
59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6? ?
60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1? ? ?
60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8? ? ?
63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0? ? ?
73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2? ? ? ?
74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5
Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,
and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows
results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold
information. Avg F1 is the arithmetic mean of MUC, B3, and CEAFE. We used the development partition for these
experiments.
MUC B3 CEAFE BLANC
Track Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1
Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4
Table 4: Results on the official test set.
as well, whereas in development (lines 6 and 7 in Ta-
ble 3), gold mentions included only mentions part of
an actual coreference chain. This explains the large
difference between, say, line 6 in Table 3 and line 4
in Table 4.
Our scores are comparable to previously reported
state-of-the-art results for coreference resolution
with predicted mentions. For example, Haghighi
and Klein (2010) compare four state-of-the-art sys-
tems on three different corpora and report B3 scores
between 63 and 77 points. While the corpora used
in (Haghighi and Klein, 2010) are different from the
one in this shared task, our result of 68 B3 suggests
that our system?s performance is competitive. In this
task, our submissions in both the open and the closed
track obtained the highest scores.
4 Conclusion
In this work we showed how a competitive end-to-
end coreference resolution system can be built using
only deterministic models (or sieves). Our approach
starts with a high-recall mention detection compo-
nent, which identifies mentions using only syntactic
information and named entity boundaries, followed
by a battery of high-precision deterministic corefer-
ence sieves, applied one at a time from highest to
lowest precision. These models incorporate lexical,
syntactic, semantic, and discourse information, and
have access to document-level information (i.e., we
share mention attributes across clusters as they are
built). For this shared task, we extended our ex-
isting system with new sieves that model shallow
discourse (i.e., speaker identification) and seman-
tics (lexical chains and alias detection). Our results
demonstrate that, despite their simplicity, determin-
istic models for coreference resolution obtain com-
petitive results, e.g., we obtained the highest scores
in both the closed and open tracks (57.8 and 58.3
respectively). The code used for this shared task is
publicly released.5
Acknowledgments
We thank the shared task organizers for their effort.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
5See http://nlp.stanford.edu/software/
dcoref.shtml for the standalone coreference resolution
system and http://nlp.stanford.edu/software/
corenlp.shtml for Stanford?s suite of natural language
processing tools, which includes this coreference resolution
system.
33
References
B. Baldwin. 1997. CogNIAC: high precision corefer-
ence with limited knowledge and linguistic resources.
In Proceedings of a Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts.
E. Bengston & D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In EMNLP-HLT.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In Proc. of HLT-
NAACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.
Weischedel 2006. OntoNotes: The 90% Solution. In
HLT/NAACL.
Z. Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.
Accurate semantic class classifier for coreference res-
olution. In EMNLP.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
V. Ng 2007. Semantic Class Induction and Coreference
Resolution. In ACL.
V. Ng and C. Cardie. 2002. Improving Machine Learn-
ing Approaches to Coreference Resolution. in ACL
2002
S. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, Wordnet and Wikipedia for coreference
resolution. Proceedings of NAACL.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Indentifying Entities and Events
in OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011).
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning 2010.
A Multi-Pass Sieve for Coreference Resolution. In
EMNLP.
J. Tetreault and J. Allen. 2003. An Empirical Evalua-
tion of Pronoun Resolution and Clausal Structure. In
Proceedings of the 2003 International Symposium on
Reference Resolution.
J. Tetreault and J. Allen. 2004. Dialogue Structure and
Pronoun Resolution. In DAARC.
X. Yang and J. Su. 2007. Coreference Resolution Us-
ing Semantic Relatedness Information from Automat-
ically Discovered Patterns. In ACL.
34
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 17?21,
Baltimore, Maryland USA, June 26 2014. c?2014 Association for Computational Linguistics
Semantic Parsing for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Computer Science Department, Stanford University
angelx,msavva,manning@cs.stanford.edu
Figure 1: Generated scene for ?There is a room
with a chair and a computer.? Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
1 Introduction
We propose text-to-scene generation as an appli-
cation for semantic parsing. This is an applica-
tion that grounds semantics in a virtual world that
requires understanding of common, everyday lan-
guage. In text to scene generation, the user pro-
vides a textual description and the system gener-
ates a 3D scene. For example, Figure 1 shows the
generated scene for the input text ?there is a room
with a chair and a computer?. This is a challeng-
ing, open-ended problem that prior work has only
addressed in a limited way.
Most of the technical challenges in text to
scene generation stem from the difficulty of map-
ping language to formal representations of vi-
sual scenes, as well as an overall absence of real
world spatial knowledge from current NLP sys-
tems. These issues are partly due to the omis-
sion in natural language of many facts about the
world. When people describe scenes in text, they
typically specify only important, relevant informa-
tion. Many common sense facts are unstated (e.g.,
chairs and desks are typically on the floor). There-
fore, we focus on inferring implicit relations that
are likely to hold even if they are not explicitly
stated by the input text.
Text to scene generation offers a rich, interactive
environment for grounded language that is famil-
iar to everyone. The entities are common, every-
day objects, and the knowledge necessary to ad-
dress this problem is of general use across many
domains. We present a system that leverages user
interactionwith 3D scenes to generate training data
for semantic parsing approaches.
Previous semantic parsing work has dealt with
grounding text to physical attributes and rela-
tions (Matuszek et al., 2012; Krishnamurthy and
Kollar, 2013), generating text for referring to ob-
jects (FitzGerald et al., 2013) and with connect-
ing language to spatial relationships (Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Seman-
tic parsing methods can also be applied to many
aspects of text to scene generation. Furthermore,
work on parsing instructions to robots (Matuszek
et al., 2013; Tellex et al., 2014) has analogues in
the context of discourse about physical scenes.
In this extended abstract, we formalize the text
to scene generation problem and describe it as a
task for semantic parsing methods. To motivate
this problem, we present a prototype system that
incorporates simple spatial knowledge, and parses
natural text to a semantic representation. By learn-
ing priors on spatial knowledge (e.g., typical posi-
tions of objects, and common spatial relations) our
system addresses inference of implicit spatial con-
straints. The user can interactively manipulate the
generated scene with textual commands, enabling
us to refine and expand learned priors.
Our current system uses deterministic rules to
map text to a scene representation but we plan to
explore training a semantic parser from data. We
can leverage our system to collect user interactions
for training data. Crowdsourcing is a promising
avenue for obtaining a large scale dataset.
17
Objects:PLATE, FORK
ON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)
?There is a piece of cake on a table.?
Scene Generation
3D ModelsSpatial KB
Objects:CAKE, TABLE
ON(CAKE, TABLE)SemanticParsing
INTERACTION
Scene Inference
ObjectSelection
Figure 2: Illustration of our system architecture.
2 Task Definition
We define text to scene generation as the task of
taking text describing a scene as input, and gen-
erating a plausible 3D scene described by that
text as output. More concretely, we parse the
input text into a scene template, which places
constraints on what objects must be present and
relationships between them. Next, using priors
from a spatial knowledge base, the system expands
the scene template by inferring additional implicit
constraints. Based on the scene template, we select
objects from a dataset of 3D models and arrange
them to generate an output scene.
After a scene is generated, the user can interact
with the scene using both textual commands and
mouse interactions. During interaction, semantic
parsing can be used to parse the input text into
a sequence of scene interaction commands. See
Figure 2 for an illustration of the system archi-
tecture. Throughout the process, we need to ad-
dress grounding of language to: 1) actions to be
performed, 2) objects to be instantiated or manip-
ulated, and 3) constraints on the objects.
2.1 Scene Template
A scene template T = (O, C) consists of a set
of object descriptions O = {o
1
, . . . , o
n
} and con-
straints C = {c
1
, . . . , c
k
} on the relationships be-
tween the objects. For each object o
i
, we identify
properties associated with it such as category la-
bel, basic attributes such as color and material, and
number of occurrences in the scene. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial rela-
tions between objects are extracted as predicates of
the form on(o
i
, o
j
) or left(o
i
, o
j
) where o
i
and o
j
are recognized objects.
As an example, given the input ?There is a room
with a desk and a red chair. The chair is to the left
of the desk.? we extract the following objects and
spatial relations:
Objects category attributes keywords
o
0
room room
o
1
desk desk
o
2
chair color:red chair, red
Relations: left(o
2
, o
1
)
2.2 Scene Interaction Commands
During interaction, we parse textual input provided
by the user into a sequence of commands with rele-
vant parts of the scene as arguments. For example,
given a scene S, we use the input text to identify a
subset of relevant objects matchingX = {O
s
, C
s
}
where O
s
is the set of object descriptions and C
s
is the set of object constraints. Commands can
then be resolved against this argument to manip-
ulate the scene state: Select(X), Remove(X),
Insert(X), Replace(X,Y ), Move(X,?X),
Scale(X,?X), and Orient(X,?X). X and Y
are semantic representations of objects, while?X
is a change to be applied to X , expressed as either
a target condition (?put the lamp on the table?) or
a relative change (?move the lamp to the right?).
These basic operations demonstrate possible
scene manipulations through text. This set of op-
erations can be enlarged to cover manipulation of
parts of objects (?make the seat of the chair red?),
and of the viewpoint (?zoom in on the chair?).
2.3 Spatial Knowledge
One of the richest sources of spatial knowledge
is 3D scene data. Prior work by (Fisher et al.,
2012) collected 133 small indoor scenes created
with 1723 3D Warehouse models. Based on their
approach, we create a spatial knowledge base with
priors on the static support hierarchy of objects in
scenes1, their relative positions and orientations.
We also define a set of spatial relations such as left,
right, above, below, front, back, on top of, next to,
near, inside, and outside. Table 1 gives examples
of the definitions of these spatial relations.
We use a 3D model dataset collected from
Google 3DWarehouse by prior work in scene syn-
1A static support hierarchy represents which objects are
likely to support which other objects on their surface (e.g.,
the floor supports tables, tables support plates).
18
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
right(A,B) V ol(A? right (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
Table 1: Definitions of spatial relation using object
bounding box computations.
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags, and category labels. In addition, we assume
the models have been scaled to physically plausi-
ble sizes and oriented with consistent up and front
direction (Savva et al., 2014). All models are in-
dexed in a database so they can be queried at run-
time for retrieval.
3 System Description
We present how the parsed representations are
used by our system to demonstrate the key issues
that have to be addressed during text to scene gen-
eration. Our current implementation uses a sim-
ple deterministic approach to map text to the scene
template and user actions on the scene. We use the
Stanford CoreNLP pipeline2 to process the input
text and use rules to match dependency patterns.
3.1 Scene generation
During scene generation, we want to construct the
most likely scene given the input text. We first
parse the text into a scene template and use it to
select appropriate models from the database. We
then perform object layout and arrangement given
the priors on spatial knowledge.
Scene Template Parsing We use the Stanford
coreference system to determine when the same
object is being referred to. To identify objects,
we look for noun phrases and use the head word
as the category, filtering with WordNet (Miller,
1995) to determine which objects are visualizable
(under the physical object synset, excluding loca-
tions). To identify properties of the objects, we ex-
tract other adjectives and nouns in the noun phrase.
We also match syntactic dependency patterns such
as ?X is made of Y? to extract more attributes and
keywords. Finally, we use dependency patterns to
extract spatial relations between objects.
2http://nlp.stanford.edu/software/corenlp.shtml
Figure 3: Select ?a blue office chair? and ?a
wooden desk? from the models database
Object Selection Once we have the scene tem-
plate, we use the keywords associated with each
object to query the model database. We select ran-
domly from the top 10 results for variety and to
allow the user to regenerate the scene with differ-
ent models. This step can be enhanced to take into
account correlations between objects (e.g., a lamp
on a table should not be a floor lamp model). See
Figure 3 for an example of object selection.
Object Layout Given the selected models, the
source scene template, and priors on spatial rela-
tions, we find an arrangement of the objects within
the scene that maximizes the probability of the lay-
out under the given scene template.
3.2 Scene Interaction
Here we address parsing of text after a scene has
been generated and during interaction sessions.
Command Parsing We deterministically map
verbs to possible actions as shown in Table 2.
Multiple actions are possible for some verbs (e.g.,
?place? and ?put? can refer to either Move or
Insert). To differentiate between these, we as-
sume new objects are introduced with the indefi-
nite article ?a? whereas old ones are modified with
the definite article ?the?.
Object Resolution To allow interaction with the
scene, wemust resolve references to objects within
a scene. Objects are disambiguated by category
and view-centric spatial relations. In addition to
matching objects by their categories, we use the
WordNet hierarchy to handle hyponym or hyper-
nym referents. Depending on the current view,
spatial relations such as ?left? or ?right? can refer
to different objects (see Figure 4).
Scene Modification Based on the action we
need to appropriately modify the current scene.
19
verb Action Example Text Example Parse
generate Generate generate a room with a desk and a lamp Generate( {room,desk,lamp} , {}) )
select Select select the chair on the right of the table Select({lamp},{right(lamp,table)})
add, insert Insert add a lamp to the table Insert({lamp},{on(lamp,table)})
delete, remove Remove remove the lamp Remove({lamp})
move Move move the chair to the left Move({chair},{left(chair)})
place, put Move, Insert put the lamp on the table Move({lamp},{on(lamp,table)})
replace Replace replace the lamp with a vase Replace({lamp},{vase})
Table 2: Mapping of verbs to possible actions.
Figure 4: Left: chair is selected by ?chair to the
right of the table? or ?object to the right of the ta-
ble?, but not selected by ?cup to the right of the
table?. Right: Different view results in a different
chair selection for ?chair to the right of the table?.
Figure 5: Left: initial scene. Right: after input
?Put a lamp on the table?.
We do this by maximizing the probability of a new
scene template given the requested action and pre-
vious scene template (see Figure 5 for an example).
4 Future Directions
We described a system prototype to motivate ap-
proaching text to scene generation as a semantic
parsing application. While this prototype illus-
trates inference of implicit constraints using prior
knowledge, it still relies on hand coded rules for
mapping text to the scene representation. This is
similar to most previous work on text to scene gen-
eration (Winograd, 1972; Coyne and Sproat, 2001)
and limits handling of natural language. More re-
cently, (Zitnick et al., 2013) used data to learn how
to ground sentences to a CRF representing 2D cli-
part scenes. Similarly, we plan to investigate using
data to learn how to ground sentences to 3D scenes.
Spatial knowledge can be helpful for resolving
ambiguities during parsing. For instance, from
spatial priors of object positions and reasoning
with physical constraints we can disambiguate the
attachment of ?next to? in ?there is a book on the
table next to the lamp?. The book and lamp are
likely on the table and thus next_to(book, lamp)
should be more likely.
User interaction is a natural part of text to scene
generation. We can leverage such interaction to
obtain data for training a semantic parser. Every
time the user issues a command, the user can indi-
cate whether the result of the interaction was cor-
rect or not, and optionally provide a rating. By
keeping track of these scene interactions and the
user ratings we can construct a corpus of tuples
containing: user action, parsed scene interaction,
scene operation, scene state before and after the
operation, and rating by the user. By building up
such a corpus over multiple interactions and users,
we obtain data for training semantic parsers.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on EMNLP.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
EMNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
20
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference onMa-
chine Learning.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2013. Learning to parse natural
language commands to a robot control system. In
Experimental Robotics.
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy. 2014. Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
21
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 14?21,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
Interactive Learning of Spatial Knowledge
for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Computer Science Department, Stanford University
{angelx,msavva,manning}@cs.stanford.edu
Abstract
We present an interactive text to 3D scene
generation system that learns the expected
spatial layout of objects from data. A user
provides input natural language text from
which we extract explicit constraints on
the objects that should appear in the scene.
Given these explicit constraints, the sys-
tem then uses prior observations of spa-
tial arrangements in a database of scenes
to infer the most likely layout of the ob-
jects in the scene. Through further user
interaction, the system gradually adjusts
and improves its estimates of where ob-
jects should be placed. We present exam-
ple generated scenes and user interaction
scenarios.
1 Introduction
People possess the power of visual imagination
that allows them to turn descriptions of scenes into
imagery. The conceptual simplicity of generating
pictures from descriptions has spurred the desire
to make systems capable of this task. However, re-
search into computational systems for creating im-
agery from textual descriptions has seen only lim-
ited success.
Most current 3D scene design systems require
the user to learn complex manipulation interfaces
through which objects are constructed and pre-
cisely positioned within scenes. However, arrang-
ing objects in scenes can much more easily be
achieved using natural language. For instance, it
is much easier to say ?Put a cup on the table?,
rather than having to search for a 3D model of a
cup, insert it into the scene, scale it to the correct
size, orient it, and position it on a table ensuring
it maintains contact with the table. By making
3D scene design more accessible to novice users
we empower a broader demographic to create 3D
scenes for use cases such as interior design, virtual
storyboarding and personalized augmented reality.
Unfortunately, several key technical challenges
restrict our ability to create text to 3D scene sys-
tems. Natural language is difficult to map to for-
mal representations of spatial knowledge and con-
straints. Furthermore, language rarely mentions
common sense facts about the world, that contain
critically important spatial knowledge. For exam-
ple, people do not usually mention the presence of
the ground or that most objects are supported by it.
As a consequence, spatial knowledge is severely
lacking in current computational systems.
Pioneering work in mapping text to 3D scene
representations has taken two approaches to ad-
dress these challenges. First, by restricting the dis-
course domain to a micro-world with simple geo-
metric shapes, the SHRDLU system demonstrated
parsing of natural language input for manipulating
the scene, and learning of procedural knowledge
through interaction (Winograd, 1972). However,
generalization to scenes with more complex ob-
jects and spatial relations is very hard to attain.
More recently, the WordsEye system has fo-
cused on the general text to 3D scene generation
task (Coyne and Sproat, 2001), allowing a user
to generate a 3D scene directly from a textual de-
scription of the objects present, their properties and
their spatial arrangement. The authors of Words-
Eye demonstrated the promise of text to scene gen-
eration systems but also pointed out some funda-
mental issues which restrict the success of their
system: a lot of spatial knowledge is required
which is hard to obtain. As a result, the user has to
use unnatural language (e.g. ?the stool is 1 feet to
the south of the table?) to express their intent.
For a text to scene system to understand more
natural text, it must be able to infer implicit in-
formation not explicitly stated in the text. For in-
stance, given the sentence ?there is an office with
a red chair?, the system should be able to infer
14
that the office also has a desk in front of the chair.
This sort of inference requires a source of prior
spatial knowledge. We propose learning this spa-
tial knowledge from existing 3D scene data. How-
ever, since the number of available scenes is small,
it is difficult to have broad coverage. Therefore,
we also rely on user interaction to augment and
grow the spatial knowledge. Luckily, user inter-
action is also natural for scene design since it is an
inherently interactive process where user input is
needed for refinement.
Our contributions address the fundamental chal-
lenges of establishing and interactively expanding
a spatial knowledge base. We build on prior work
in data-driven scene synthesis (Fisher et al., 2012)
to automatically extract general spatial knowledge
from data: knowledge of what objects occur in
scenes, and their expected spatial relations. Our
system then uses this knowledge to generate scenes
from natural text inferring implicit constraints. It
then leverages user interaction to allow refinement
of the scene, and improve the spatial knowledge
base. We demonstrate that user interaction is criti-
cal in expanding and improving spatial knowledge
learned from data.
2 Background
A key insight for enabling text to scene generation
is that linguistic and non-linguistic spatial knowl-
edge is critical for this task and can be learned di-
rectly from data representing the physical world
and from interactions of people with such data.
User feedback allows us to interactively update
spatial knowledge, an idea that we illustrate here
in the domain of spatial relations. Early work on
the PUT system (Clay andWilhelms, 1996) and the
SHRDLU system (Winograd, 1972) gives a good
formalization of the interactive linguistic manipu-
lation of objects in 3D scenes. Recently, there has
been promising work on generating 2D clipart for
sentences using probabilistic models with place-
ment priors learned from data (Zitnick et al., 2013).
2.1 Text to Scene Systems
Prior work on text to 3D scene generation has re-
sulted in systems such as WordsEye (Coyne and
Sproat, 2001) and other similar approaches (Sev-
ersky and Yin, 2006). These systems are typi-
cally not designed to be fully interactive and do not
leverage user interaction to improve their results.
Furthermore, they mostly rely on manual annota-
tion of 3Dmodels and on hand crafted rules to map
text to object placement decisions, which makes
them hard to extend and generalize. More re-
cent work has used crowdsourcing platforms, such
as Amazon Mechanical Turk, to collect necessary
annotations (Coyne et al., 2012). However, this
data collection is treated as a separate pre-process
and the user still has no influence on the system?s
knowledge base. We address one part of this is-
sue: learning simple spatial knowledge from data
and interactively updating it through user feed-
back. We also infer unstated implicit constraints
thus allowing for more natural text input.
2.2 Automatic Scene Layout
Prior work on scene layout has focused largely on
room interiors and determining good furniture lay-
outs by optimizing energy functions that capture
the quality of a proposed layout. These energy
functions are encoded from interior design guide-
lines (Merrell et al., 2011) or learned from input
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
Methods to learn scene structure have been demon-
strated using various data sources including sim-
ulation of human agents in 3D scenes (Jiang et
al., 2012; Jiang and Saxena, 2013), and analysis
of supporting contact points in scanned environ-
ments (Rosman and Ramamoorthy, 2011).
However, prior work has not explored methods
for enabling users of scene generation algorithms
to interactively refine and improve an underlying
spatial knowledge model ? a capability which is
critically important. Our work focuses on demon-
strating an interactive system which allows a user
to manipulate and refine such spatial knowledge.
Such a system is useful regardless of the algorithm
used to get the input spatial knowledge.
2.3 Interactive Learning
In many tasks, user interaction can provide feed-
back to an automated system and guide it towards
a desired goal. There is much prior work in various
domains including interactive systems for refin-
ing image search algorithms (Fogarty et al., 2008)
and for manipulating social network group cre-
ation (Amershi et al., 2012). We focus on the do-
main of text to 3D scene generation where despite
the success of data-driven methods there has been
little work on interactive learning systems.
15
3 Approach Overview
What should an interactive text to scene system
look like from the perspective of a user? The user
should be able to provide a brief scene description
in natural language as input. The system parses
this text to a set of explicitly provided constraints
on what objects should be present, and how they
are arranged. This set of constraints should be au-
tomatically expanded by using prior knowledge so
that ?common sense? facts are reflected in the gen-
eral scene ? an example is the static support hier-
archy for objects in the scene (i.e. plate goes on
table, table goes on ground). The system gener-
ates a candidate scene and then the user is free to
interact with it by direct control or through textual
commands. The system can then leverage user in-
teraction to update its spatial knowledge and inte-
grate newly learned constraints or relations. The
final output is a 3D scene that can be viewed from
any position and rendered by a graphics engine. In
this paper we select an initial viewpoint such that
objects are in the frame and view-based spatial re-
lations are satisfied.
How might we create such a system? Spatial
knowledge is critical for this task. We need it to
understand spatial language, to plausibly position
objects within scenes and to allow users to manip-
ulate them. We learn spatial knowledge from ex-
ample scene data to ensure that our approach can
be generalized to different scenarios. We also learn
from user interaction to refine and expand existing
spatial knowledge. In ?5 we describe the spatial
knowledge used by our system.
We define our problem as the task of taking text
describing a scene as input, and generating a plau-
sible 3D scene described by that text as output.
More concretely, based on the input text, we se-
lect objects from a dataset of 3D models (?4) and
arrange them to generate output scenes. See Fig-
ure 1 for an illustration of the system architecture.
We break the system down into several subtasks:
Constraint Parsing (?6): Parse the input textual
description of a concrete scene into a set of con-
straints on the objects present and spatial relations
between them. Automatically expand this set of
constraints to account for implicit constraints not
specified in the text.
SceneGeneration (?7): Using above constraints
and prior knowledge on the spatial arrangement of
objects, construct a scene template. Next, sample
Objects:PLATE, FORK
ON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)
?There is a piece of cake on a table.?
Create SceneIdentify missing objects
3D ModelsSpatial KB
Objects:CAKE, TABLE
ON(CAKE, TABLE)
Identify objects and relationships
INTERACTION
CONSTRAINTPARSING
Figure 1: Diagram illustrating the architecture of
our system.
the template and select a set of objects to be in-
stantiated. Finally, optimize the placement of the
objects to finalize the arrangement of the scene.
Interaction and Learning (?8): Provide means
for a user to interactively adjust the scene through
direct manipulation and textual commands. Use
any such interaction to update the system?s spatial
knowledge so it better captures the user?s intent.
4 Object Knowledge from 3D Models
To generate scenes we need to have a collection
of 3D models for representing physical objects.
We use a 3D model dataset collected from Google
3D Warehouse by prior work in scene synthe-
sis and containing about 12490 mostly indoor ob-
jects (Fisher et al., 2012). These models have text
associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). Due to the num-
ber of models in the database, not all models were
rescaled and re-oriented. We then indexed all mod-
els in a database that we query at run-time for re-
trieval based on category and tag labels.
5 Spatial Knowledge
Here we describe how we learn spatial knowledge
from existing scene data. We base our approach
on that of (Fisher et al., 2012) and use their dataset
16
of 133 small indoor scenes created with 1723 3D
Warehouse models. Relative object-to-object po-
sition and orientation priors can also be learned
from the scene data but we have not yet incorpo-
rated them in the results for this paper.
5.1 Support Hierarchy
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category C
p
sup-
porting a given child category C
c
as a simple con-
ditional probability based on normalized observa-
tion counts.
P
support
(C
p
|C
c
) =
count(C
c
on C
p
)
count(C
c
)
5.2 Supporting surfaces
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector limited to the six canonical di-
rections: up, down, left, right, front, back. We then
learn a probability of supporting surface normal
direction S
n
given child object category C
c
. For
example, posters are typically found on walls so
their support normal vectors are in the horizontal
directions. Any unobserved child categories are
assumed to have P
surf
(S
n
= up|C
c
) = 1 since
most things rest on a horizontal surface (e.g. floor).
P
surf
(S
n
|C
c
) =
count(C
c
on surface with S
n
)
count(C
c
)
5.3 Spatial Relations
For spatial relations we use a set of predefined re-
lations: left, right, above, below, front, back, on
top of, next to, near, inside, and outside. These
are measured using axis-aligned bounding boxes
from the viewer?s perspective. More concretely,
the bounding boxes of the two objects involved in
a spatial relation are compared to determine vol-
ume overlap or closest distance (for proximity re-
lations). Table 1 gives a few examples of the defi-
nitions of these spatial relations.
Since these spatial relations are resolvedwith re-
spect to the current view of the scene, they corre-
spond to view-centric definitions of these spatial
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
outside(A,B) 1 - V ol(A?B)
V ol(A)
left(A,B) V ol(A? left (B))
V ol(A)
right(A,B) V ol(A? right (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
Table 1: Definitions of spatial relation using object
bounding box computations. Note that dist(A,B)
is normalized with respect to the maximum extent
of the bounding box of B.
concepts. An interesting line of future work would
be to explore when ego-centric and object-centric
spatial reference models are more likely in a given
utterance, and resolve the spatial term accordingly.
6 Constraint Parsing
During constraint parsing we take the input text
and identify the objects and the relations between
them. For each object, we also identify proper-
ties associated with it such as category label, ba-
sic attributes such as color and material, and num-
ber of occurrences in the scene. Based on the ob-
ject category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial re-
lations between objects are extracted as predicates
of the form on(A,B) or left(A,B) where A and B are
recognized objects.
As an example, given the input ?There is a
room with a desk and a red chair. The chair is
to the left of the desk.? we extract the following
objects and spatial relations:
Objects:
index category attributes keywords
0 room room
1 desk desk
2 chair color:red chair, red
Relations: left(chair, desk)
The input text is processed using the Stanford
CoreNLP pipeline1. We use the Stanford corefer-
ence system to determine when the same object is
being referred to. To identify objects, we look for
noun phrases and use the head word as the cate-
gory, filtering with WordNet (Miller, 1995) to de-
termine which objects are visualizable (under the
1http://nlp.stanford.edu/software/corenlp.shtml
17
Dependency Pattern Example Text
tag:VBN=verb >nsubjpass =nsubj >prep (=prep >pobj =pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj]
tag:VB=verb >dobj =dobj >prep (=prep >pobj =pobj) Put[verb] the cup[dobj] on[prep] the table[pobj]
Table 2: Example dependency patterns for extracting spatial relations.
Figure 2: Generated scene for ?There is a room
with a desk and a lamp. There is a chair to the
right of the desk.? The inferred scene hierarchy is
overlayed in the center.
physical object synset, excluding locations). To
identify properties of the objects, we extract other
adjectives and nouns in the noun phrase. We also
match dependency patterns such as ?X is made of
Y? to extract more attributes and keywords. Fi-
nally, we use dependency patterns to extract spa-
tial relations between objects (see Table 2 for some
example patterns).
We used a fairly simple deterministic approach
to map text to the scene template and user actions
on the scene. An interesting avenue for future re-
search is to automatically learn how to map text
using more advanced semantic parsing methods.
7 Scene Generation
During scene generation we aim to find the most
likely scene given the input utterance, and prior
knowledge. Once we have determined from the
input text what objects exist and their spatial re-
Figure 3: Generated scene for ?There is a room
with a poster bed and a poster.?
Figure 4: Generated scene for ?There is a room
with a table and a sandwich.? Note that the plate is
not explicitly stated, but is inferred by the system.
lations in the scene, we select 3D models match-
ing the objects and their associated properties. We
sample the support hierarchy prior P
support
to ob-
tain the support hierarchy for the scene.
We then initialize the positions of objects within
the scene by traversing the support hierarchy in
depth-first order, positioning the largest available
child node and recursing. Child nodes are posi-
tioned by selecting a supporting surface on a can-
didate parent object through sampling ofP
surf
and
ensuring no collisions exist with other objects. If
there are any spatial constraints that are not satis-
fied, we remove and randomly reposition the ob-
jects violating the constraints, and iterate to im-
prove the layout. The resulting scene is rendered
and presented to the user.
Figure 2 shows a rendering of a generated scene
along with the support hierarchy and input text.
Even though the spatial relation between lamp and
desk was not mentioned explicitly, we infer that
the lamp is supported by the top surface of the
desk. In Figure 3 we show another example of
a generated scene for the input ?There is a room
with a poster bed and a poster?. Note that the sys-
tem differentiates between a ?poster? and a ?poster
bed? ? it correctly selects and places the bed on the
floor, while the poster is placed on the wall.
Figure 4 shows an example of inferring missing
objects. Even though the plate was not explicitly
mentioned in the input, we infer that the sandwich
is more likely to be supported by a plate rather than
directly placed on the table. Without this infer-
18
Figure 5: Left: chair is selected using ?the chair to
the right of the table? or ?the object to the right of
the table?. Chair is not selected for ?the cup to the
right of the table?. Right: Different view results
in different chair being selected for the input ?the
chair to the right of the table?.
ence, the user would need to bemuchmore verbose
with text such as ?There is a room with a table, a
plate and a sandwich. The sandwich is on the plate,
and the plate is on the table.?
8 Interactive System
Once a scene is generated, the user can view the
scene and manipulate it using both simple action
phrases and mouse interaction. The system sup-
ports traditional 3D scene interaction mechanisms
such as navigating the viewpoint with mouse and
keyboard, selection and movement of object mod-
els by clicking. In addition, a user can give simple
textual commands to select and modify objects, or
to refine the scene. For example, a user can re-
quest to ?remove the chair? or ?put a pot on the
table? which requires the system to resolve refer-
ents to objects in the scene (see ?8.1). The system
tracks user interactions throughout this process and
can adjust its spatial knowledge accordingly. In
the following sections, we give some examples of
how the user can interact with the system and how
the system learns from this interaction.
8.1 View centric spatial relations
During interaction, the user can refer to objects
with their categories and with spatial relations be-
tween them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In the left
screenshot in Figure 5, the user can select a chair
to the right of the table using the phrase ?chair to
the right of the table? or ?object to the right of the
table?. The user can then change their viewpoint
by rotating and moving around. Since spatial rela-
tions are resolved with respect to the current view-
point, we see that a different chair is selected for
Figure 6: Left: initial scene. Right: after input
?Put a lamp on the table?.
the same phrase from the different viewpoint in the
right screenshot.
8.2 Scene Editing with Text
By using simple textual commands the user can
edit the scene. For example, given the initial scene
on the left in Figure 6, the user can then issue the
command ?put a lamp on the table? which results
in the scene on the right. The system currently al-
lows for adding objects to new positions and re-
moving existing objects. Currently, repositioning
of objects is performed only with direct control,
but in the future we also plan to support reposi-
tioning of objects by using textual commands.
8.3 Learning Support Hierarchy
After a user requests that a lamp be placed on a ta-
ble, the system updates its prior on the likelihood
of a lamp being supported by a table. Based on
prior observations the likelihood of lamps being
placed on tables was very low (4%) since very few
lamps were observed on tables in the scene dataset.
However, after the user interaction, we recompute
the prior including the scene that the user has cre-
ated and the probability of lamp on table increases
to 12% (see Figure 7).
8.4 Learning Object Names
Often, objects or parts may not have associated la-
bels that the user would use to refer to the objects.
In those cases, the system can inform the user that
it cannot resolve a given name, and the user can
then select the object or part of the object they were
referring to and annotate it with a label. For in-
stance, in Figure 8, the user annotated the differ-
ent parts of the room as ?floor?, ?wall?, ?window?,
and ?door?. Before annotation, the system did not
know any labels for these parts of the room. After
annotation, the user can select these parts using the
associated names. In addition, the system updates
its spatial knowledge base and can now predict that
the probability of a poster being placed on a wall
19
0% 25% 50% 75% 100%
Before
After
Nightstand
Nightstand
Room
Room
Table
Table
Desk
Desk
Figure 7: Probability of supporting parent categories for lamps before and after the user explicitly requests
a lamp on a table.
Figure 8: The user clicks and selects parts of the scene, annotating them as ?floor?, ?wall?, ?window?,
?door?. After annotation, the user can also refer to these parts with the associated names. The system
spatial knowledge base is updated accordingly.
is 40%, and that the probability of a table being
placed on the floor is 23%. Note that these prob-
abilities are based on multiple observations of the
annotated room. Accumulating annotations such
as these and propagating labels to new models is
an effective way to expand spatial knowledge.
9 Future Work
We described a preliminary interactive text to 3D
scene generation system that can learn from prior
data and user interaction. We hope to improve
the system by incorporating more feedback mech-
anisms for the user, and the learning algorithm.
If the user requests a particular object be se-
lected but the system gets the referent wrong, the
user could then indicate the error and provide a cor-
rection. We can then use this feedback as a source
of training data to improve the interpretation of text
to the desired user action. For example, if the user
asks to ?select the red bowl? and the system could
not resolve ?red bowl? to the correct object, the
user could intervene by clicking on the correct ref-
erent object. Simple interactions such as this are
incredibly powerful for providing additional data
for learning. Though we did not focus on this as-
pect, a dialogue-based interaction pattern is natural
for our system. The user can converse with the sys-
tem to iteratively refine the scene and the system
can ask for clarifications at any point ? when and
how the system should inquire for more informa-
tion is interesting future research.
To evaluate whether the generated scenes are
satisfactory, we can ask people to rate them against
input text descriptions. We can also study usage
of the system in concrete tasks to see how often
users need to provide corrections and manually
manipulate the scene. A useful baseline to com-
pare against would be a traditional scenemanipula-
tion system. By doing these studies at a large scale,
for instance by making the interface available on
20
the web, we can crowdsource the accumulation of
user interactions and gathering of spatial knowl-
edge. Simultaneously, running formal user stud-
ies to better understand preference for text-based
versus direct interactions during different actions
would be very beneficial for more informed design
of text-to-scene generation systems.
10 Conclusion
We have demonstrated the usefulness of an inter-
active text to 3D scene generation system. Spatial
knowledge is essential for text to 3D scene gener-
ation. While it is possible to learn spatial knowl-
edge purely from data, it is hard to have complete
coverage of all possible scenarios. Interaction and
user feedback is a good way to improve coverage
and to refine spatial knowledge. In addition, in-
teraction is a natural mode of user involvement in
scene generation and creative tasks.
Little prior work has addressed the need for in-
teraction or the need for recovering implicit spatial
constraints. We propose that the resolution of un-
mentioned spatial constraints, and leveraging user
interaction to acquire spatial knowledge are criti-
cal for enabling natural text to scene generation.
User interaction is essential for text to scene
generation since the process is fundamentally
under-constrained. Most natural textual descrip-
tions of scenes will not mention many visual as-
pects of a physical scene. However, it is still pos-
sible to automatically generate a plausible starting
scene for refinement.
Our work focused on showing that user interac-
tion is both natural and useful for a text to scene
generation system. Furthermore, refining spatial
knowledge through interaction is a promising way
of acquiring more implicit knowledge. Finally,
any practically useful text to scene generation will
by necessity involve interaction with users who
have particular goals and tasks in mind.
References
Saleema Amershi, James Fogarty, and Daniel Weld.
2012. Regroup: interactive machine learning for on-
demand group creation in social networks. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems.
Sharon Rose Clay and Jane Wilhelms. 1996. Put:
Language-based interactive manipulation of objects.
Computer Graphics and Applications, IEEE.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
BobCoyne, Alexander Klapheke,MasoudRouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
James Fogarty, Desney Tan, Ashish Kapoor, and Simon
Winder. 2008. CueFlik: interactive concept learn-
ing in image search. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems.
Yun Jiang and Ashutosh Saxena. 2013. Infinite la-
tent conditional random fields for modeling environ-
ments through humans.
Yun Jiang, Marcus Lim, and Ashutosh Saxena. 2012.
Learning object arrangements in 3D scenes using hu-
man context. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12).
AlanDKalvin andRussell HTaylor. 1996. Superfaces:
Polygonal mesh simplification with bounded error.
Computer Graphics and Applications, IEEE.
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Benjamin Rosman and Subramanian Ramamoorthy.
2011. Learning spatial relationships between ob-
jects. The International Journal of Robotics Re-
search.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Lee M Seversky and Lijun Yin. 2006. Real-time au-
tomatic 3D scene generation from natural language
voice and text descriptions. In Proceedings of the
14th annual ACM international conference on Mul-
timedia.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
21
