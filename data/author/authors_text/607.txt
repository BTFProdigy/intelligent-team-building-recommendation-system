Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 537?544
Manchester, August 2008
Hindi Urdu Machine Transliteration using Finite-state Transducers 
M G Abbas Malik   Christian Boitet 
GTALP, Laboratoire d?Informatique Grenoble 
Universit? Joseph Fourier, France 
abbas.malik@imag.fr, 
Christian.Boitet@imag.fr 
Pushpak Bhattacharyya 
Dept. of Computer Science and Engineering, 
IIT Bombay, India 
pb@cse.iitb.ac.in 
 
Abstract 
Finite-state Transducers (FST) can be 
very efficient to implement inter-dialectal 
transliteration. We illustrate this on the 
Hindi and Urdu language pair. FSTs can 
also be used for translation between sur-
face-close languages. We introduce UIT 
(universal intermediate transcription) for 
the same pair on the basis of their com-
mon phonetic repository in such a way 
that it can be extended to other languages 
like Arabic, Chinese, English, French, etc. 
We describe a transliteration model based 
on FST and UIT, and evaluate it on Hindi 
and Urdu corpora. 
1 Introduction 
Transliteration is mainly used to transcribe a 
word written in one language in the writing sys-
tem of the other language, thereby keeping an 
approximate phonetic equivalence. It is useful for 
MT (to create possible equivalents of unknown 
words) (Knight and Stall, 1998; Paola and San-
jeev, 2003), cross-lingual information retrieval 
(Pirkola et al 2003), the development of multi-
lingual resources (Yan et al 2003) and multilin-
gual text and speech processing. Inter-dialectal 
translation without lexical changes is quite useful 
and sometimes even necessary when the dialects 
in question use different scripts; it can be 
achieved by transliteration alone. That is the case 
of HUMT (Hindi-Urdu Machine Transliteration) 
where each word has to be transliterated from 
Hindi to Urdu and vice versa, irrespective of its 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
type (noun, verb, etc. and not only proper noun 
or unknown word). 
?One man?s Hindi is another man?s Urdu? 
(Rai, 2000). The major difference between Hindi 
and Urdu is that the former is written in Devana-
gari script with a more Sanskritized vocabulary 
and the latter is written in Urdu script (derivation 
of Persio-Arabic script) with more vocabulary 
borrowed from Persian and Arabic. In contrast to 
the transcriptional difference, Hindi and Urdu 
share grammar, morphology, a huge vocabulary, 
history, classical literature, cultural heritage, etc. 
Hindi is the National language of India with 366 
million native speakers. Urdu is the National and 
one of the state languages of Pakistan and India 
respectively with 60 million native speakers 
(Rahman, 2004). Table 1 gives an idea about the 
size of Hindi and Urdu. 
 Native Speakers 
2nd Language 
Speakers Total 
Hindi 366,000,000 487,000,000 853,000,000 
Urdu 60,290,000 104,000,000 164,290,000 
Total 426,290,000 591,000,000 1,017,000,000 
Table 1: Hindi and Urdu speakers 
Hindi and Urdu, being varieties of the same 
language, cover a huge proportion of world?s 
population. People from Hindi and Urdu com-
munities can understand the verbal expressions 
of each other but not the written expressions. 
HUMT is an effort to bridge this scriptural divide 
between India and Pakistan. 
Hindi and Urdu scripts are briefly introduced 
in section 2. Universal Intermediate Transcrip-
tion (UIT) is described in section 3, and UIT 
mappings for Hindi and Urdu are given in sec-
tion 4. Contextual HUMT rules are presented and 
discussed in section 5. An HUMT system im-
plementation and its evaluation are provided in 
section 6 and 7. Section 8 is on future work and 
conclusion. 
537
2 HUMT 
There exist three languages at the border between 
India and Pakistan: Kashmiri, Punjabi and Sindhi. 
All of them are mainly written in two scripts, one 
being a derivation of the Persio-Arabic script and 
the other being Devanagari script. A person us-
ing the Persio-Arabic script cannot understand 
the Devanagari script and vice versa. The same is 
true for Hindi and Urdu which are varieties or 
dialects of the same language, called Hindustani 
by Platts (1909). 
PMT (Punjabi Machine Transliteration) (Ma-
lik, 2006) was a first effort to bridge this scrip-
tural divide between the two scripts of Punjabi 
namely Shahmukhi (a derivation of Perio-Arabic 
script) and Gurmukhi (a derivation of Landa, 
Shardha and Takri, old Indian scripts). HUMT is 
a logical extension of PMT. Our HUMT system 
is generic and flexible such that it will be extend-
able to handle similar cases like Kashmiri, Pun-
jabi, Sindhi, etc. HUMT is also a special type of 
machine transliteration like PMT. 
A brief account of Hindi and Urdu is first giv-
en for unacquainted readers. 
2.1 Hindi 
The Devanagari (literally ?godly urban?) script, a 
simplified version of the alphabet used for San-
skrit, is a left-to-right script. Each consonant 
symbol inherits by default the vowel sound [?]. 
Two or more consonants may be combined to-
gether to form a cluster called Conjunct that 
marks the absence of the inherited vowel [?] be-
tween two consonants (Kellogg, 1872; Montaut, 
2004). A sentence illustrating Devanagari is giv-
en below: 
?????? ?????????? ?? ???? ????? ??. 
[h?n?i h?n?ust?n ki q?mi zub?n h?] 
(Hindi is the national language of India) 
2.2 Urdu 
Urdu is written in an alphabet derived from the 
Persio-Arabic alphabet. It is a right-to-left script 
and the shape assumed by a character in a word 
is context-sensitive, i.e. the shape of a character 
is different depending on whether its position is 
at the beginning, in the middle or at the end of a 
word (Zia, 1999). A sentence illustrating Urdu is 
given below: 
?? y6?36 G?6[  zEegEZ F? ?X? y6[ Ei ??? 
[?r?u p?k?st?n ki q?mi zub?n h?] 
(Urdu is the National Language of Pakistan.) 
3 Universal Intermediate Transcription 
UIT (Universal Intermediate Transcription) is a 
scheme to transcribe texts in Hindi, Urdu, Punja-
bi, etc. in an unambiguous way encoded in AS-
CII range 32 ? 126, since a text in this range is 
portable across computers and operating systems 
(James 1993; Wells, 1995). SAMPA (Speech 
Assessment Methods Phonetic Alphabet) is a 
widely accepted scheme for encoding the IPA 
(International Phonetic Alphabet) into ASCII. It 
was first developed for Danish, Dutch, French, 
German and Italian, and since then it has been 
extended to many languages like Arabic, Czech, 
English, Greek, Hebrew, Portuguese, Russian, 
Spanish, Swedish, Thai, Turkish, etc. 
We define UIT as a logical extension of 
SAMPA. The UIT encoding for Hindi and Urdu 
is developed on the basis of rules and principles 
of SAMPA and X-SAMPA (Wells, 1995), that 
cover all symbols on the IPA chart. Phonemes 
are the most appropriate invariants to mediate 
between the scripts of Hindi, Punjabi, Urdu, etc., 
so that the encoding choice is logical and suitable. 
4 Analysis of Scripts and UIT Mappings 
For the analysis and comparison, scripts of Hindi 
and Urdu are divided into different groups on the 
basis of character types. 
4.1 Consonants 
These are grouped into two categories: 
Aspirated Consonants: Hindi and Urdu both 
have 15 aspirated consonants. In Hindi, 11 aspi-
rated consonants are represented by separate cha-
racters e.g. ? [k?], ? [b?], etc. The remaining 4 
consonants are represented by combining a sim-
ple consonant to be aspirated and the conjunct 
form of HA ?[h], e.g. ? [l] + ?? + ? [h] = ??? [l?]. 
In Urdu, all aspirated consonants are 
represented by a combination of a simple conso-
nant to be aspirated and Heh Doachashmee (?) 
[h], e.g. ? [k] + ? [h] = ?? [k?], ? [b] + ? [h] = ?? 
[b?],  ? [l] + ? [h] = ?? [l?], etc.  
The UIT mapping for aspirated consonants is 
given in Table 2. 
Hindi Urdu UIT Hindi Urdu UIT 
? ?? [b?] b_h ??? ?? [r?] r_h 
? ?? [p?] p_h ? ?? [??] r`_h 
? ?? [??] t_d_h ? ?? [k?] k_h 
? ?? [??] t`_h ? ?? [g?] g_h 
? ?? [??] d_Z_h ??? ?? [l?] l_h 
538
? ?? [??] t_S_h ??? ?? [m?] m_h 
? ?? [??] d_d_h ??? ?? [n?] n_h 
? ?? [??] d`_h    
Table 2: Hindi Urdu aspirated consonants 
Non-aspirated Consonants: Hindi has 29 
non-aspirated consonant symbols representing 28 
consonant sounds as both SHA (?) and SSA (?) 
represent the same sound [?]. Similarly Urdu has 
35 consonant symbols representing 27 sounds as 
multiple characters are used to represent the 
same sound e.g. Heh (?) and Heh-Goal (?) 
represent the sound [h] and Theh (?), Seen (?) 
and Sad (?) represent the sound [s], etc. 
UIT mapping for non-aspirated consonants is 
given in Table 3. 
Hindi Urdu UIT Hindi Urdu UIT 
? ? [b] b ? ? [s] s2 
? ? [p] p ? ? [z] z2 
? ? [?] t_d ? ? [?] t_d1 
? ? [?] t` ? ? [z] z3 
? ? [s] s1 - ? [?] ? 
? ? [?] d_Z ? ? [?] X 
? ? [?] t_S ? ? [f] f 
? ? [h] h1 ? ? [q] q 
? ? [x] x ? ? [k] k 
? ? [?] d_d ? ? [g] g 
? ? [?] d` ? ? [l] l 
? ? [z] z1 ? ? [m] m 
? ? [r] r ? ? [n] n 
? ? [?] r` ? ? [v] v 
? ? [z] z ? ? [h] h 
? ? [?] Z ? ? [j] j 
? ? [s] s ? ? [?] t_d2 
? ? [?] S ? - [?] n` 
? ? [?] S1 ? ? ? [?] ~ 
Table 3: Hindi Urdu non-aspirated consonants 
4.2 Vowels 
Hindi has 11 vowels and 10 of them have nasa-
lized forms. They are represented by 11 indepen-
dent vowel symbols e.g. ? [?], ? [u], ? [?], 
etc. and 10 dependent vowel symbols e.g. ?? 
[?], ?? [u], ?? [?], etc. called maatraas. When a 
vowel comes at the start of a word or a syllable, 
the independent form is used; otherwise the de-
pendent form is used (Kellogg, 1872; Montaut, 
2004). 
Urdu contains 10 vowels and 7 of them have 
nasalized forms (Hussain, 2004; Khan, 1997). 
Urdu vowels are represented using four long vo-
wels (Alef Madda (?), Alef (?), Vav (?) and Choti 
Yeh (?)) and three short vowels (Arabic Fatha ? 
Zabar -?, Arabic Damma ? Pesh -? and Arabic Ka-
sra ? Zer -?). Vowel representation is context-
sensitive in Urdu. Vav (?) and Choti Yeh (?) are 
also used as consonants. 
Hamza (?) is a place holder between two suc-
cessive vowel sounds, e.g. in ?????  [k?m?i] 
(earning), Hamza (?) separates the two vowel 
sounds Alef (?) [?] and Choti Yeh (?) [i]. Noon-
ghunna (?) is used as nasalization marker. Anal-
ysis and mapping of Hindi Urdu vowels is given 
in Table 5. 
4.3 Diacritical Marks 
Urdu contains 15 diacritical marks. They 
represent vowel sounds, except Hamza-e-Izafat  -? 
and Kasr-e-Izafat -? that are used to build com-
pound words, e.g. ??????? ????? [???r?h?s??ns] (In-
stitute of Science), ??????? ?????? [t?rix?ped???] 
(date of birth), etc. Shadda -? is used to geminate 
a consonant e.g. ??? [r?bb] (God), ????? [?????] 
(good), etc. Jazm  -? is used to mark the absence of 
a vowel after the base consonant (Platts, 1909). 
In Hindi, the conjunct form is used to geminate a 
consonant. Urdu diacritical marks mapping is 
given in Table 4. 
Hindi Urdu UIT Hindi Urdu UIT 
- F? [?] @ ?? G? [?] A 
?? G? [?] I ? F? [?n] @n 
?? E? [?] U ??? E? [?n] Un 
?? E? [u] u ??? F? [?n] In 
?? G? [i] i    
Table 4: Diacritical Marks of Urdu 
Diacritical marks are present in Urdu but spa-
ringly used by people. They are very important 
for the correct pronunciation and understanding 
the meanings of a word. For example, 
 ??? ?????? ??? ???  
[je s???k b?h?? ???i h?] (This is a wide road.) 
 ??? ??? ???????? 
[meri ?u?i s?r?x h?] (My bangle is red.) 
In the first sentence, the word ???? is pro-
nounced as [???i] (wide) and in the second, it is 
539
pronounced as [?u?i] (bangle). There should be 
Zabar (??) and Pesh (??) after Cheh (?) in above 
words and correct transcriptions are ????? (wide) 
and ????? (bangle). Thus diacritical marks are 
essential for removing ambiguities, natural lan-
guage processing and speech synthesis. 
 
Vowel Urdu Hindi (UIT) 
? It is represented by Alef (?) + Zabar -? at the start of a word e.g. ??? [?b] (now) and by Zabar -? in the middle of a word respectively e.g. ???? [r?bb] (God). It never comes at the end of a word. ? (@) 
? 
It is represented by Alef Madda (?) at the start of a word e.g. ???? [?d?mi] (man) and by Alef (?) or Alef 
Madda (?) in the middle of a word e.g. ???? [??n?] (go), ?????? [b?l?x?r] (at last). At the end of a word, it is 
represented by Alef (?). In some Arabic loan words, it is represented by Choti Yeh (?) + Khari Zabar ?- at 
the end of a word e.g. ????? [??l?] (Superior) and by Khari Zabar ?- in the middle of a word e.g. ????? [?l?hi] 
(God). 
? or ?? (A) 
e 
It is represented by Alef (?) + Choti Yeh (?) at the start of a word e.g. ????? [es?r] (sacrifice), ??? [ek] (one), 
etc. and by Choti Yeh (?) or Baree Yeh (?) in the middle of a word e.g. ???? [mer?] (mine), ??????? 
[?nd??er?] (darkness), ????? [beg??r] (homeless) etc. At the end of a word, It is represented by Baree Yeh 
(?) e.g. ???? [s?re] (all). 
? or ?? (e) 
? 
It is represented by Alef (?) + Zabar -? + Choti Yeh (?) at the start of a word e.g. ????? [?h] (this) and by Zabar 
-? + Choti Yeh (?) in the middle of a word e.g. ???? [m?l] (dirt). At the end of a word, it is represented by 
Zabar -? + Baree Yeh (?) e.g. ??? [h?] (is). 
? or ?? ({) 
? 
It is represented by Alef (?) + Zer -? at the start of a word e.g. ??? [?s] (this) and by Zer -? in the middle of a 
word e.g. ????? [b?r??] (rain). It never comes at the end of a word. At the end of a word, it is used as Kasr-e-
Izafat to build compound words. 
? or ?? (I) 
i 
It is represented by Alef (?) + Zer -? + Choti Yeh (?) at the start of a word e.g. ?????? [im?n] (belief) and by 
Zer -? + Choti Yeh (?) in the middle or at the end of a word e.g. ?????? [?miri] (richness), ????? [q?rib] (near), 
etc. 
? or ?? (i) 
? 
It is represented by Alef (?) + Pesh -? at the start of a word e.g. ?????? [?d?d???r] (there) and by Pesh -? in the 
middle of a word e.g. ???? [m?ll] (price). It never comes at the end of a word. 
? or ?? (U) 
u 
It is represented by Alef (?) + Pesh -? + Vav (?) at the start of a word e.g. ???????? [?g??t??] (dozzing) and by 
Pesh -? + Vav (?) in the middle or at the end of a word e.g. ????? [sur?t ?] (face), ?????? [t??r?zu] (physical bal-
ance), etc. 
? or ?? (u) 
o It is represented by Alef (?) + Vav (?) at the start of a word e.g. ????? [o???] (nasty) and by Vav (?) in the 
middle or at the end of a word e.g. ???? [holi] (slowly), ??? [k?ho] (say), etc. 
? or ?? (o) 
? 
It is represented by Alef (?) + Zabar -? + Vav (?) at the start of a word e.g. ???? [??] (hindrance) and by Zabar -? 
+ Vav (?) in the middle or at the end of a word e.g. ???? [m?t ?] (death). 
? or ?? (O) 
r ? 
It is represented by a consonant symbol Reh (?) [r] as this vowel is only present in Sanskrit loan words. It is 
almost not used in modern standard Hindi. It is not present in Urdu as it is used only in Sanskrit loan words. ? or ?? (r1) 
Note: In Hindi, Nasalization of a vowel is done by adding Anunasik (??) or Anusavar (??) after the vowel. Anusavar (??) is used when 
the vowel graph goes over the upper line; otherwise Anunasik (??) is used (Kellogg, 1872; Montaut, 2004). In UIT, ~ is added at end of 
UIT encoding for nasalization of all above vowels except the last one that do not have a nasalized form. 
Table 5: Analysis and Mapping of Hindi Urdu Vowels 
5 HUMT Rules 
In this section, UIT mappings of Hindi Urdu al-
phabets and contextual rules that are necessary 
for Hindi-Urdu transliteration are discussed. 
5.1 UIT Mappings 
UIT mappings for Hindi and Urdu alphabets and 
their vowels are given in Table 2 ? 5. In Hindi, 
SHA (?) and SSA (?) both represent the sound 
[?] and have one equivalent symbol in Urdu, i.e. 
Sheen (?). To make distinction between SHA 
(?) and SSA (?) in UIT, they are mapped on S 
and S1 respectively. Similarly in Urdu, Seh (?), 
Seen (?) and Sad (?) represent the sound [s] 
and have one equivalent symbol in Hindi, i.e. SA 
(?). To make distinction among them in UIT, 
they are mapped on s1, s and s2 respectively. All 
similar cases are shown in Table 6. 
IPA Urdu (UIT) Hindi (UIT) 
? ? (t_d), ? (t_d1), ? (t_d2) ? (t_d) 
s ? (s1), ? (s), ? (s2) ? (s) 
H ? (h1), ? (h) ? (h) 
540
z ? (z1), ? (z), ? (Z), ? (z2), ? (z3) ? (z) 
? ? (S) ? (S), ? (S1) 
r ? (r) ? (r), ? (r1) 
Table 6: Multiple Characters for one IPA 
Multi-equivalences are problematic for Hindi-
Urdu transliteration. 
UIT is extendable to other languages like Eng-
lish, French, Kashmiri, Punjabi, Sindhi, etc. For 
example, Punjabi has one extra character than 
Urdu i.e. Rnoon [?] (?), it is mapped on ?n`? in 
UIT. Similarly, UIT, a phonetic encoding 
scheme, can be extended to other languages. 
All these mappings can be implemented by 
simple finite-state transducers using XEROX?s 
XFST (Beesley and Karttunen, 2003) language. 
A sample XFST code is given in Figure 1. 
read regex [? -> b, ? -> p, ? -> [d ?_? Z] ]; 
read regex [[? ?] -> [d ?_? Z ?_? h]]; 
read regex [? -> v, ? -> j || .#. _ ]; 
read regex [? -> v, ? -> j || _ [? | ?]]; 
read regex [? -> e || CONSONANTS _ ]; 
read regex [ ? -> i || _ [ ?| .#.]]; 
? 
read regex [? -> b, ? -> p, ? -> z, ? -> [d ?_? Z ?_? h]]; 
read regex [? -> ?@?, ? -> A, ? -> i || .#. _ ] 
? 
Figure 1: Sample XFST code 
Finite-state transducers are robust and time 
and space efficient (Mohri, 1997). They are a 
logical choice for Hindi-Urdu transliteration via 
UIT as this problem could also be seen as string 
matching and producing an analysis string as an 
output like finite-state morphological analysis. 
5.2 Contextual HUMT Rules 
UIT mappings need to be accompanied by neces-
sary contextual HUMT rules for correct Hindi to 
Urdu transliteration and vice versa. 
For example, Vav (?) and Choti Yeh (?) are 
used to represent vowels like [o], [?], [i], [e], etc. 
but they are also used as consonants. Vav (?) and 
Choti Yeh (?) are consonants when they come at 
the beginning of a word or when they are fol-
lowed by Alef mada (?) or Alef (?). Also, Choti 
Yeh (?) represents the vowel [e] when it is pre-
ceded by a consonant but when it comes at the 
end of a word and is preceded by a consonant 
then it represents the vowel [i]. These rules are 
shown in red colour in Figure 1. 
Thus HUMT contextual rules are necessary for 
Hindi-Urdu transliteration and they can also be 
implemented as finite-state transducer using 
XFST. All these rules can?t be given here due to 
shortage of space. 
6 HUMT System 
The HUMT system exploits the simplicity, ro-
bustness, power and time and space efficiency of 
finite-state transducers. Exactly the same trans-
ducer that encodes a Hindi or Urdu text into UIT 
can be used in the reverse direction to generate 
Hindi or Urdu text from the UIT encoded text. 
This two-way power of the finite-state transducer 
(Mohri, 1997) has significantly reduced the 
amount of efforts to build the HUMT system. 
Another very important and powerful strength of 
finite-state transducers, they can be composed 
together to build a single transducer that can per-
form the same task that could be done with help 
of two or more transducers when applied sequen-
tially (Mohri, 1997), not only allows us to build a 
direct Hindi ? Urdu transducer, but also helps to 
divide difficult and complex problems into sim-
ple ones, and has indeed simplified the process of 
building the HUMT system. A direct Hindi ? 
Urdu transducer can be used in applications 
where UIT encoding is not necessary like Hindi-
Urdu MT system. 
The HUMT system can be extended to per-
form transliteration between two or more differ-
ent scripts used for the same languages like 
Kashmiri, Kazakh, Malay, Punjabi, Sindhi, etc. 
or between language pairs like English?Hindi, 
English?Urdu, English?French, etc. by just in-
troducing the respective transducers in the Fi-
nite-state Transducer Manager of 
the HUMT system to build a multilingual ma-
chine transliteration system. 
 
Figure 2: HUMT System 
In the HUMT system, Text Tokenizer 
takes the input Hindi or Urdu Unicode text, toke-
nizes it into Hindi or Urdu words and passes 
541
them to UIT Enconverter. The enconverter 
enconverts Hindi or Urdu words into UIT words 
using the appropriate transducer from Finite-
state Transducers Manager, e.g. for 
Hindi words, it uses the Hindi ? UIT transducer. 
It passes these UIT encoded words to UIT De-
converter, which deconverts them into Hindi 
or Urdu words using the appropriate transducer 
from Finite-state Transducers Man-
ager in reverse and generates the target Hindi 
or Urdu text. 
6.1 Enconversion of Hindi-Urdu to UIT 
Hindi ? UIT transducer is a composition of the 
mapping rules transducers and the contextual 
rules transducers. This is clearly shown in figure 
3 with a sample XFST code. 
clear stack 
set char-encoding UTF-8 
define CONSONANTS [? | ? | ? | ? | ? | ? | ?]; 
read regex [?? -> J, ?? -> h, ?? -> 0]; 
read regex [? -> k, ? -> [k ?_? h],  ? -> g, ? -> [g ?_? 
h],  ? -> [n ?@? g], ? -> [t ?_? S], ? -> [t ?_? S ?_? h]]; 
read regex [[? ?? ?] -> [k k]?, [? ?? ?] -> [k k ?_? h],  
[? ?? ?] -> [g g]?, [? ?? ?] -> [g g ?_? h]]; 
? 
read regex [[? ??] -> [k h], [?] -> [n A], [? ??] -> [j h], 
[? ??] -> [v h] || .#. _ .#.]; 
compose net 
Figure 3: Sample code for Hindi ? UIT Transducer 
How the HUMT system works is shown with 
the help of an example. Take the Hindi sentence: 
????? ????? ?? ??? ?? ????? ?? 
[f?x??? m?h?b?? ?r ?m?n k? n???n h?] 
(Dove is symbol of love and peace) 
This sentence is received by the Text To-
kenizer and is tokenized into Hindi words, 
which are enconverted into UIT words using the 
mapping and the contextual rules of Hindi ? 
UIT transducer by the UIT Enconverter. 
The Hindi Words and the UIT enconversions are 
given in Table 7. 
Hindi Words UIT 
????? [f?x???] fAx@t_dA 
????? [m?h?b??] mUh@b@t_d 
?? [?r] Or 
??? [?m?n] @m@n 
?? [k?] kA 
????? [n???n] nISAn 
?? [h?] H{ 
Table 7: Hindi Words with UIT 
6.2 Deconversion of UIT to Hindi-Urdu 
For the deconversion, Hindi ? UIT or Urdu ? 
UIT transducer is applied in reverse on the UIT 
enconverted words to generate Hindi or Urdu 
words. To continue with the example in the pre-
vious section, the UIT words are deconverted 
into the Urdu words by the UIT Deconver-
ter using Urdu ? UIT transducer in reverse. 
The Urdu words are given in table 8 with the 
Hindi and the UIT words. 
Hindi UIT Urdu 
????? [f?x???] fAx@t_dA ????? 
????? [m?h?b??] mUh@b@t_d ????? 
?? [?r] Or ???? 
??? [?m?n] @m@n ??? 
?? [k?] kA ?? 
????? [n???n] nISAn ????? 
?? [h?] H{ ??? 
Table 8: Hindi, UIT and Urdu Words 
Finally, the following Urdu sentence is gener-
ated from Urdu words. 
????? ????? ???? ??? ?? ????? ???  
Here the word ????? [f?x???] (Dove) is 
transliterated wrongly into ??????? because the 
vowel [?] at the end of some Urdu words (bor-
rowed from Persian language) is transcribed with 
help of Heh-gol [h] (?). This phenomenon is a 
problem for Hindi to Urdu transliteration but not 
for Urdu to Hindi transliteration. 
7 Evaluation Experiments and Results 
For evaluation purpose, we used a Hindi corpus, 
containing 374,150 words, and an Urdu corpus 
with 38,099 words. The Hindi corpus is extracted 
from the Hindi WordNet2 developed by the Re-
source Center for Indian Language Technology 
Solutions, CSE Department, Indian Institute of 
Technology (IIT) Bombay, India and from the 
project CIFLI (GETALP-LIG 3 , University Jo-
seph Fourier), a project for building resources 
and tools for network-based ?linguistic survival? 
communication between French, English and 
Indian languages like Hindi, Tamil, etc. The Ur-
du corpus was developed manually from a book 
titled ?????? ???? [z?lm?? k?d?]. The Hindi-Urdu 
corpus contains in total 412,249 words. 
The HUMT system is an initial step to build 
Urdu resources and add Urdu to the languages of 
                                                 
2 http://www.cfilt.iitb.ac.in 
3 http://www.liglab.fr 
542
SurviTra-CIFLI (Survival Translation) (Boitet et 
al, 2007), a multilingual digital phrase-book to 
help tourists for communication and enquiries 
like restaurant, hotel reservation, flight enquiry, 
etc. 
To reduce evaluation and testing efforts, 
unique words are extracted from the Hindi-Urdu 
corpus and are transliterated using the HUMT 
system. These unique words and their translitera-
tions are checked for accuracy with the help of 
dictionaries (Platts, 1911; Feroz). 
7.1 Urdu ? Hindi Transliteration Results 
While transliterating Urdu into Hindi, multiple 
problems occur like multi-equivalences, no equi-
valence, missing diacritical marks in Urdu text. 
For example, Sheen [?] (?) can be transliterated 
in Hindi into SHA [?] (?) or SSA [?] (?) that are 
present in 7,917 and 6,399 corpus words respec-
tively. Sheen [?] (?) is transliterated into SHA 
[?] (?) by default. Thus, 6,399 words containing 
SSA [?] (?) are wrongly transliterated into Hindi 
using HUMT. Urdu to Hindi multi-equivalences 
cases are given in Table 9 with their frequencies. 
Urdu Hindi (corpus Frequency) 
? [?] ? (7917), ? (6399) 
? [r] ? (79,345), ? (199) 
Table 9: Urdu ? Hindi Multi-equivalences 
Some Hindi characters do not have equivalent 
characters in Urdu, e.g. NNA [?] (?), retroflexed 
version of [n], has approximately mapped onto 
Noon [n] (?). This creates a problem when a 
word actually containing NNA [?] (?) is transli-
terated from Urdu to Hindi. No-equivalence cas-
es are given in Table 10. 
Urdu Hindi (corpus Frequency) 
- ? (4744) 
- ? (0) 
- ? (532) 
Table 10: Urdu ? Hindi No-equivalences 
Missing diacritical marks is the major problem 
when transliterating Urdu into Hindi. The impor-
tance of diacritical marks has already been ex-
plained in section 4.3. This work assumed that all 
necessary diacritical marks are present in Urdu 
text because they play a vital role in Urdu to 
Hindi transliterations. Results of Urdu to Hindi 
transliteration are given in Table 11. 
 Error Words Accuracy 
Corpus 11,874 97.12% 
Unique Words 123 98.54% 
Table 11: Urdu ? Hindi Transliteration Results 
7.2 Hindi ? Urdu Transliteration Results 
Hindi ? Urdu transliteration also have multi-
equivalences and no-equivalence problems that 
are given in Table 12. 
 
Hindi Urdu (corpus Frequency) 
? 1312) ? ,(41,751) ?) 
? 86) ? ,(751) ? ,(53,289) ?) 
? 1800) ? ,(72,850) ?) 
? 2) ? ,(215) ? ,(228) ? ,(1489) ? ,(2551) ?) 
- 2857) ?) 
Table 12: Hindi ? Urdu Multi & No equivalences 
Results of Hindi to Urdu transliteration are 
given in Table 13. 
 Error Words Accuracy 
Corpus 8,740 97.88% 
Unique Words 1400 83.41% 
Table 13: Hindi ? Urdu Transliteration Results 
Interestingly, Hindi to Urdu conversion is 
14.47% less accurate on the unique words as 
compared to its result on the corpus data that is a 
contrasting fact for the reverse conversion. 
The HUMT system gives 97.12% accuracy for 
Urdu to Hindi and 97.88% accuracy for Hindi to 
Urdu. Thus, the HUMT system works with 
97.50% accuracy. 
8 Future Implications 
Hindi-Urdu transliteration is one of the cases 
where one language is written in two or more 
mutually incomprehensible scripts like Kazakh, 
Kashmiri, Malay, Punjabi, Sindhi, etc. The 
HUMT system can be enhanced by extending 
UIT and introducing the respective finite-state 
transducers. It can similarly be enhanced to 
transliterate between language pairs, e.g. Eng-
lish-Arabic, English-Hindi, English-Urdu, 
French-Hindi, etc. Thus, it can be enhanced to 
build a multilingual machine transliteration sys-
tem that can be used for cross-scriptural transli-
teration and MT. 
We are intended to resolve the problems of 
multi-equivalences, no-equivalences and the 
most importantly the restoration of diacritical 
marks in Urdu text that are observed but left un-
attended in the current work. Restoration of dia-
critical marks in Urdu, Sindhi, Punjabi, Kashmi-
ri, etc. texts is essential for word sense disambig-
uation, natural language processing and speech 
synthesis of the said languages. 
The HUMT system will also provide a basis 
for the development of Inter-dialectal translation 
system and MT system for surface-close lan-
guages like Indonesian-Malay, Japanese-Korean, 
543
Hindi-Marathi, Hindi-Urdu, etc. Translation of 
the surface-close languages or inter-dialectal 
translation can be performed by using mainly 
transliteration and some lexical translations. 
Thus HUMT will also provide basis for Cross-
Scriptural Transliteration, Cross-scriptural In-
formation Retrieval, Cross-scriptural Applica-
tion Development, inter-dialectal translation and 
translation of surface-close languages. 
9 Conclusion 
Finite-state transducers are very efficient, robust, 
and simple to use. Their simplicity and powerful 
features are exploited in the HUMT model to 
perform Hindi-Urdu transliteration using UIT 
that is a generic and flexible encoding scheme to 
uniquely encode natural languages into ASCII. 
The HUMT system gives 97.50% accuracy when 
it is applied on the Hindi-Urdu corpora contain-
ing 412,249 words in total. It is an endeavor to 
bridge the scriptural, ethnical, cultural and geo-
graphical division between 1,017 millions people 
around the globe. 
Acknowledgement 
This study is partially supported by the project CIFLI 
funded under ARCUS-INDIA program by Ministry of 
Foreign Affairs and Rh?ne-Alpes region. 
References 
Beesley, Kenneth R. and Karttunen, Lauri. 2003. Fi-
nite State Morphology. CSLI Publications, USA. 
Boitet, Christian. Bhattacharayya, Pushpak. Blanc, 
Etienne. Meena, Sanjay. Boudhh, Sangharsh. Fafiotte, 
Georges. Falaise, Achille. Vacchani, Vishal. 2007. 
Building Hindi-French-English-UNL Resources for 
SurviTra-CIFLI, a linguistic survival system under 
construction. Proceedings of the Seventh Symposium 
on NLP, 13 ? 15 December, Chonburi, Thailand. 
Feroz ul Din. ????????????? ????? Feroz Sons Publishers, 
Lahore, Pakistan. 
Hussain, Sarmad. 2004. Letter to Sound Rules for 
Urdu Text to Speech System. Proceedings of Work-
shop on Computational Approaches to Arabic Script-
based Languages, COLING 2004, Geneva, Switzer-
land. 
James, L. Hieronymus. 1993. ASCII Phonetic Symbols 
for the World?s Languages: Worldbet. AT&T Bell 
Laboratories, Murray Hill, NJ 07974, USA. 
Kellogg, Rev. S. H. 1872. A Grammar of Hindi Lan-
guage. Delhi, Oriental Book Reprints. 
Khan, Mehboob Alam. 1997. ????? ?? ???? ???? (Sound 
System in Urdu) National Language Authority, Pakis-
tan. 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion. Computational Linguistics, 24(4). 
Knight, K. and Stall, B G. 1998. Translating Names 
and Technical Terms in Arabic Tex. Proceedings of 
the COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Malik, M. G. Abbas. 2006. Punjabi Machine Transli-
teration. Proceedings of the 21st International Confe-
rence on Computational Linguistics and 44th Annual 
Meeting of the ACL, July 2006, Sydney.  
Mohri, Mehryar. 1997. Finite-state Transducers in 
Language and Speech Processing. Computational 
Linguistics, 23(2). 
Montaut A. 2004. A Linguistic Grammar of Hindi. 
Studies in Indo-European Linguistics Series, M?n-
chen, Lincom Europa. 
Paola, V. and Sanjeev, K. 2003. Transliteration of 
proper names in cross-language applications. Pro-
ceedings of the 26th annual International ACM SIGIR 
conference on research and development in informa-
tion retrieval. 
Pirkola, A. Toivonen, J. Keskustalo, H. Visala, K. and 
J?rvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of the 26th Annual 
international ACM SIGIR Conference on Research 
and Development in informaion Retrieval, Toronto, 
Canada. 
Platts, John T. 1909. A Grammar of the Hindustani or 
Urdu Language. Crosby Lockwood and Son, 7 Sta-
tioners Hall Court, Ludgate hill, London. E.C. 
Platts, John T. 1911. A Dictionary of Urdu, Classical 
Hindi and English. Crosby Lockwood and Son, 7 Sta-
tioners Hall Court, Ludgate hill, London, E.C. 
Rahman, Tariq. 2004. Language Policy and Localiza-
tion in Pakistan: Proposal for a Paradigmatic Shift. 
Crossing the Digital Divide, SCALLA Conference on 
Computational Linguistics. 
Rai, Alok. 2000. Hindi Nationalism. Orient Longman 
Private Limited, New Delhi. 
Wells, J C. 1995. Computer-coding the IPA: A Pro-
posed Extension of SAMPA. University College Lon-
don. http://www.phon.ucl.ac.uk/home/sampa/ipasam-
x.pdf. 
Yan Qu, Gregory Grefenstette, David A. Evans. 2003. 
Automatic transliteration for Japanese-to-English text 
retrieval. Proceedings of the 26th annual interntional 
ACM SIGIR conference on Research and develop-
ment in information retrieval. 
Zia, Khaver. 1999a. Standard Code Table for Urdu. 
Proceedings of 4th Symposium on Multilingual In-
formation Processing (MLIT-4), Yangon, Myanmar, 
CICC, Japan. 
544
Coling 2008: Companion volume ? Posters and Demonstrations, pages 27?30
Manchester, August 2008
Hindi Compound Verbs and their Automatic Extraction 
Debasri Chakrabarti 
Humanities and Social 
Sciences Department 
IIT Bombay 
debasri@iitb.ac.in 
Hemang Mandalia 
Computer Science and En-
gineering Department 
IIT Bombay 
hemang.rm@gmail.com 
Ritwik Priya 
Computer Science and En-
gineering Department 
IIT Bombay 
ritwik@cse.iitb.ac.in 
 
Vaijayanthi Sarma 
Humanities and Social Sci-
ences Department 
IIT Bombay 
vsarma@iitb.ac.in 
Pushpak Bhattacharyya 
Computer Science and En-
gineering Department 
IIT Bombay 
pb@cse.iitb.ac.in 
Abstract 
We analyse Hindi complex predicates 
and propose linguistic tests for their de-
tection. This analysis enables us to iden-
tify a category of V+V complex predi-
cates called lexical compound verbs 
(LCpdVs) which need to be stored in the 
dictionary. Based on the linguistic analy-
sis, a simple automatic method has been 
devised for extracting LCpdVs from cor-
pora. We achieve an accuracy of around 
98% in this task. The LCpdVs thus ex-
tracted may be used to automatically 
augment lexical resources like wordnets, 
an otherwise time consuming and labour-
intensive process 
1 Introduction 
Complex predicates (CPs) abound in South 
Asian languages [Butt, 1995; Hook, 1974] pri-
marily as either, noun+verb combinations (con-
junct verbs) or verb+verb (V+V) combinations 
(compound verbs). This paper discusses the lat-
ter. 
Of the many V+V sequences in Hindi, only a 
subset constitutes true CPs. Thus, we first need 
diagnostic tests to differentiate between CP and 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
non-CP V+V sequences. Of the CPs thus iso-
lated, we need to distinguish between those CPs 
that are formed in the syntax (derivationally) and 
those that are formed in the lexicon (LCpdVs) in 
order to include only the latter  in lexical knowl-
edge bases. Further, automatic extraction of 
LCpdVs from electronic corpora and their inclu-
sion in lexical knowledge bases is a desirable 
goal for languages like Hindi, which liberally use 
CPs. 
This paper discusses Hindi Verb+Verb (V+V) 
CPs and their automatic extraction from a corpus.  
1.1 Related work 
Alsina (1996) discusses the general theory of 
complex predicates. Early work on conjunct and 
compound verbs in Hindi appears in Burton-Page 
(1957) and Arora (1979). Our work on diagnostic 
tests for CPs, as reported here, has been inspired 
by Butt (1993, 1995 for Urdu) and Paul (2004, 
for Bengali). The analysis of lexical derivation of 
LCpdVs derives from the work on compound 
verbs by Abbi (1991, 1992) and Gopalkrishnan 
and Abbi (1992).  
This work is motivated primarily by the need 
to automatically augment lexical networks such 
as the Princeton Wordnet (Miller et. al., 1990) 
and the Hindi Wordnet (Narayan et. al., 2002). 
Pasca (2005) and Snow et. al. (2006) report work 
on such augmentations by processing web docu-
ments.   
To the best of our knowledge ours is the first 
attempt at automatic extraction of LCpdVs from 
Hindi corpora.  
27
 1.2 Organization of the paper 
Section 2 discusses CPs in Hindi and the ways to 
distinguish them from other, similar looking, 
constructions. Section 3 discusses the automatic 
extraction of CPs from corpora. Section 4 con-
cludes the paper. 
2 V+V Complex Predicates in Hindi 
We have identified five different types of V+V 
sequences in Hindi. These are:  
 
1. V1 stem+V2: maar Daalnaa (kill-put) ?kill?.  
2. V1 inf-e+lagnaa: rone lagnaa (cry-feel) ?start 
crying?.  
3. V1 inf+paRnaa: bolnaa paRaa (say-lie) ?say?.  
4. V1 inf-e+V2: likhne ko/ke lie kahaa ?asked to 
write?.  
5. V1?kar+V2: lekar gayaa ?took and went?. 
2.1 Identification of CPs] 
Following Butt (1993) and Paul (2004), we use 
the following diagnostic tests to identify CPs in 
Hindi:  
 
1. Scope of adverbs  
2. Scope of negation  
3. Nominalization  
4. Passivization  
5. Causativization  
6. Movement 
(see Appendix A for an example of these tests) 
 
The tests above have been exhaustively applied 
to varied data. The results of these tests show 
that some V+V sequences function as single se-
mantic units and others do not. They also show 
that the V1stem+V2, V1inf-e+lagnaa and 
V1inf+paRnaa sequences show similar proper-
ties and the V1 inf-e+V2 stem and the V1?
kar+V2 behave similarly. We call these Group 1 
and Group 2 respectively. 
Group 1 sequences are true CPs in Hindi. The 
V+V sequences are simple predicates (mono-
clausal) with one subject. Group 2 constructions 
are not CPs. They show clausal embedding and 
each verb behaves as if it were an independent 
syntactic entity. In the next section we summa-
rize the semantic properties of CPs (Group 1). 
2.2 Semantic Properties of V2 in Group 1 
After identifying the CPs from among different 
V+V sequences, the next step was to determine 
how they are formed. To accomplish this we ex-
amined the semantic properties of the second 
verbs (V2) in Group 1: 
 
(1) V1inf+paRnaa: 
Examples include karnaa paRaa ?do-lie (had to 
do)?, bolnaa paRaa ?say-lie (had to say)? etc. The 
second verb is always paRnaa ?to lie (lay)?. It 
appears in its stem form and bears all the inflec-
tions. As V2, paRnaa has the meaning of com-
pulsion/force. paRnaa ?lie? as a V2 can be com-
bined with any V1 irrespective of the latter?s se-
mantic properties. Since there are no syntactic or 
semantic restrictions on the selection of V1, this 
construction should be treated in the syntax as a 
combination of a V1 and a modal auxiliary. 
 
(2) V1 inf-e+lagnaa: 
Examples include karne lagaa ?do-feel (start to 
do)?, bolne lagaa ?say-feel (start to say)? etc. The 
V2 in this sequence is always lagnaa ?feel? in the 
bare form and carries all the inflections. The core 
meaning of lagnaa ?feel? is lost when it is com-
bined with a V1. As a V2 it always has the mean-
ing of beginning, happening of an event. lagnaa 
?feel? as a V2 can be combined with any V1 irre-
spective of the latter?s semantic properties. Thus, 
this is also an instance of a modal auxiliary and 
should be derived in the syntax. 
 
(3) V1stem+V2 
In the formation of V1 stem+V2, the V2 may be 
any one of ten verbs, as shown in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: The 10 vector verbs 
All these V2s also occur as main verbs. As V2, 
the core meaning of these verbs is lost 
(bleached), but they acquire some new semantic 
properties which are otherwise not seen (Abbi, 
1991, 1992; Gopalkrishnan and Abbi, 1992). The 
semantic properties of V2s include finality, defi-
niteness, negative value, manner of the action, 
attitude of the speaker etc.                          
The combination of V1 and V2 is subject to 
the semantic compatibility between the two verbs. 
1. Daalnaa ?put? 
2. lenaa ?take? 
3. denaa ?give? 
4. uThnaa ?wake? 
5. jaanaa ?go? 
6. paRnaa ?lie? 
7. baiThnaa ?sit? 
8. maarnaa ?kill? 
9. dhamaknaa ?throb? 
10. girnaa ?fall? 
28
The argument structure of the CP is determined 
by V1 as is the case-marking on the internal ar-
guments, but the case-marking on the external 
argument (subject) is determined by both verbs. 
From this analysis we conclude that V+V 
CPs are formed both lexically and syntactically 
in Hindi. Detailed investigation shows us that the 
V2 in the V1inf-e+lagnaa and the 
V1inf+paRnaa constructions is a type of modal 
auxiliary and its semantic features are predictable 
and unvarying. We propose to deal with these 
verbs in the syntax and call these verbs syntactic 
compound verbs (SCpdVs). The V2 choice in the 
V1stem+V2 is not predictable and the CPs func-
tion as a single complex of syntactic and seman-
tic features. We call these verbs lexical com-
pound verbs (LCpdVs) and we propose to in-
clude them in the lexical knowledge base. In the 
next section we provide a heuristic for automatic 
extraction of LCpdVs for storage in the lexicon. 
2.3 The Extraction Process 
By scanning the corpus, V1stem+V2 sequences 
were found given the heuristic H* specified in 
Figure 2.  
 
 
 
 
 
 
 
 
 
 
Figure 2: Main heuristic for identifying LCpdVs 
 
Ten native speakers of Hindi were consulted. 
They were asked to construct sentences with the 
extracted sequences. If they were able to do so, 
that sequence was registered as a true LCpdV.  
The precision of the heuristic is calculated as 
the ratio of the actual LCpdVs arrived at through 
manual validation to the total number of antici-
pated LCpdVs identified by the heuristic. 
The results of these calculations are shown in 
Table 1, with a precision rate of 70% for the 
BBC corpus and 79%  for the CIIL one.  
 
Cor-
pus 
To-
tal 
de-
tec-
tio
ns 
POS 
ambi
gui-
ties 
Pas-
sive 
forms 
LCpdVs 
(manu-
ally 
de-
tected) 
Preci
sion 
BBC 40 8 4 28 0.7 
(28/4
0) 
CIIL 174 32 7 135 0.79 
(135/ 
174) 
Table 1: Precision of LCpdV extraction 
The loss in precision was caused by (i) part of 
speech ambiguity, (ii) passivisation and (iii) 
idiomatic usages. For lack of space, we do not 
discuss this here.  
When measures were taken to remedy these 
errors, we reached an accuracy of close to 98%  
(see table 2). 
 
BBC CIIL  
Confirmed LCpdVs 
(A) 
423 953 
Not LCpdVs (B) 13 12 
Different POS (C) 65 179 
Possible LCpdVs but 
contexts insufficient 
(D) 
44 36 
Minimum Precision 
(A/(A+B+D)) 
0.88 
(423/480) 
0.95 
(953/1001) 
Maximum Precision 
((A+B)/(A+B+D)) 
0.97 
(467/480) 
0.99 
(989/1001) 
Total V1stem+V2 
constructions in the 
corpus 
10,145 36,115 
Table 2: Final results of LCpdV extraction 
 
A partial list of LCpdVs extracted from a test run 
on the CIIL corpus is presented in Table 3. 
 
baandh 
denaa 
?tie? 
Kar 
lenaa 
?do? 
Bhar 
denaa 
?fill? 
le jaanaa 
?take? 
Banaa 
denaa 
?make? 
jaan 
lenaa 
?know? 
kaaT 
denaa 
?cut? 
Kar de-
naa ?do? 
Badal 
jaanaa 
?change? 
Bhuul 
jaanaa 
?forget? 
jalaa 
denaa 
?burn? 
Gir 
jaanaa 
?fall? 
Samajh 
lenaa 
?under-
stand? 
Samjhaa 
denaa 
?make 
under-
stand? 
Khod 
lenaa 
?dig?  
lauTaa 
denaa 
?return? 
Rah 
jaanaa 
?stay? 
Le lenaa 
?take? 
De denaa 
?give? 
ghusaa 
denaa 
?enter? 
Table 3: Examples of LCpdV extraction 
3 Conclusions and Future Work 
In this paper, we have presented a study of Hindi 
compound verbs, proposed diagnostic tests for 
their detection and given automatic methods for 
their extraction from a corpus. Native speakers 
(Heuristic H*) 
If a verb V1 is in the stem form and 
is followed by a verb V2 from a  pre-
stored list of verbs that can form the  
second component of the CP (section 
2.2, Figure 3), i.e., the ?vector?, then 
this verb along with the V2 is taken 
to be an instance of an LCpdV. 
29
verify that the accuracy of our method is close to 
98% on representative corpora. 
Future work will consist in inserting the ex-
tracted LCpdVs into lexical resources such as the 
Hindi wordnet2 at the right places with the right 
links. 
References 
Abbi, Anvita. 1991. Semantics of explicator com-
pound verbs. In South Asian Languages, Language 
Sciences, 13:2, 161-180 
.Abbi, Anvita. 1992. The explicator compound verb: 
some definitional issues and criteria for identifica-
tion. Indian Linguistics, 53, 27-46. 
Alsina, Alex. 1996. Complex Predicates:Structure 
and Theory. CSLI Publications,Stanford, CA. 
Arora, H. 1979. Aspects of Compound Verbs in Hindi. 
M.Litt. dissertation, Delhi University. 
Burton-Page, J. 1957. Compound and conjunct verbs 
in Hindi. BSOAS 19 469-78. 
Butt, M. 1993. Conscious choice and some light verbs 
in Urdu. In M. K. Verma ed. (1993) Complex 
Predicates in South Asian Languages. Manohar 
Publishers and Distributors, New Delhi. 
Butt, M. 1995. The Structure of Complex Predicates 
in Urdu. Doctoral Dissertation, Stanford Univer-
sity.  
Cruys Time De and B. V. Moiron. 2007. Semantics-
based multiword expression extraction. ACL-2007 
Workshop on Multiword Expressions. 
Gopalkrishnan, D. and Abbi, A. 1992. The explicator 
compound verb: some definitional issues and crite-
ria for identification. Indian Linguistics, 53, 27-46. 
Miller,G., R. Beckwith, C. Fellbaum,, D. Gross, and 
K. Miller, Five Papers on WordNet. CSL Report 
43, Cognitive Science Laboratory, Princeton Uni-
versity, Princeton, 1990. 
http://www.cogsci.princeton.edu/~wn 
Narayan, D., D. Chakrabarty, P. Pande, and P. Bhat-
tacharyya. 2002. An experience in building the 
Indo WordNet - a WordNet for Hindi, International 
Conference on Global WordNet (GWC 02), My-
sore, India, January. 
Pasca, Marius, 2005. finding instance names and al-
ternative glosses on the web: WordNet reloaded. 
Proceedings of CICLing, Mexico City. 
Snow, Rion, Dan Jurafsky, and Andrew Y. Ng. 2006. 
Semantic taxonomy induction from heterogenous 
evidence. Proceedings of COLING/ACL, Sydney. 
                                                 
2 Developed by the wordnet team at IIT Bombay, 
www.cfilt.iitb.ac.in/webhwn 
Appendix A. Example of a diagnostic Test for 
LCpdVs: scope of adverbs 
 
Verb 
Type 
Example Comment CP? 
V1 
stem+ 
V2 
us-ne jaldii 
jaldii    
khaa  li-
aa?(S)he 
ate 
quickly.? 
Scope over 
the whole 
sequence 
Yes 
V1inf-
e+ lag-
naa 
vah jaldii 
se khaan-e  
lag-aa ?He 
started eat-
ing imme-
diately.? 
Scope over 
the whole 
sequence 
Yes 
V1 
inf+ 
paRnaa 
mujhe yah  
kaam jaldii 
karnaa       
paR-aa  ?I 
had to do 
the work 
quickly.? 
Scope over 
the whole 
sequence 
Yes 
V1inf-
e+V2  
us-ne mu-
jhe khat  
jaldii se   
likhn-e         
kah-aa  ?He 
asked me 
to write the 
letter 
quickly.? 
Either over 
V1 or V2 de-
pends upon 
the syntactic 
position of 
the adverb 
No 
V1?
kar+ 
V2 
vah jaldii 
se nahaa-
kar   aa-
yeg-aa 
 ?He will 
take bath 
quickly and 
come.? 
Either over 
V1 or V2 de-
pends upon 
the syntactic 
position of 
the adverb 
No 
 
30
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459?467,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Projecting Parameters for Multilingual Word Sense Disambiguation 
 
Mitesh M. Khapra Sapan Shah Piyush Kedia Pushpak Bhattacharyya 
Department of Computer Science and Engineering 
Indian Institute of Technology, Bombay 
Powai, Mumbai ? 400076, 
Maharashtra, India. 
{miteshk,sapan,charasi,pb}@cse.iitb.ac.in 
 
Abstract 
We report in this paper a way of doing Word 
Sense Disambiguation (WSD) that has its ori-
gin in multilingual MT and that is cognizant 
of the fact that parallel corpora, wordnets and 
sense annotated corpora are scarce re-
sources. With respect to these resources, lan-
guages show different levels of readiness; 
however a more resource fortunate language 
can help a less resource fortunate language. 
Our WSD method can be applied to a lan-
guage even when no sense tagged corpora for 
that language is available. This is achieved by 
projecting wordnet and corpus parameters 
from another language to the language in 
question. The approach is centered around a 
novel synset based multilingual dictionary and 
the empirical observation that within a domain 
the distribution of senses remains more or less 
invariant across languages. The effectiveness 
of our approach is verified by doing parameter 
projection and then running two different 
WSD algorithms. The accuracy values of ap-
proximately 75% (F1-score) for three lan-
guages in two different domains establish the 
fact that within a domain it is possible to cir-
cumvent the problem of scarcity of resources 
by projecting parameters like sense distribu-
tions, corpus-co-occurrences, conceptual dis-
tance, etc. from one language to another. 
1 Introduction 
Currently efforts are on in India to build large scale 
Machine Translation and Cross Lingual Search 
systems in consortia mode. These efforts are large, 
in the sense that 10-11 institutes and 6-7 languages 
spanning the length and breadth of the country are 
involved.  The approach taken for translation is 
transfer based which needs to tackle the problem of 
word sense disambiguation (WSD) (Sergei et. al., 
2003).  Since 90s machine learning based ap-
proaches to WSD using sense marked corpora have 
gained ground (Eneko Agirre & Philip Edmonds, 
2007). However, the creation of sense marked cor-
pora has always remained a costly proposition. 
Statistical MT has obviated the need for elaborate 
resources for WSD, because WSD in SMT hap-
pens implicitly through parallel corpora (Brown et. 
al., 1993). But parallel corpora too are a very cost-
ly resource.  
The above situation brings out the challenges 
involved in Indian language MT and CLIR. Lack 
of resources coupled with the multiplicity of Indian 
languages severely affects the performance of sev-
eral NLP tasks. In the light of this, we focus on the 
problem of developing methodologies that reuse 
resources. The idea is to do the annotation work 
for one language and find ways of using them for 
another language. 
Our work on WSD takes place in a multilingual 
setting involving Hindi (national language of India; 
500 million speaker base), Marathi (20 million 
speaker base), Bengali (185 million speaker base) 
and Tamil (74 million speaker base). The wordnet 
of Hindi and sense marked corpora of Hindi are 
used for all these languages. Our methodology 
rests on a novel multilingual dictionary organiza-
tion and on the idea of ?parameter projection? from 
Hindi to the other languages. Also the domains of 
interest are tourism and health. 
The roadmap of the paper is as follows. Section 
2 describes related work. In section 3 we introduce 
the parameters essential for domain-specific WSD. 
Section 4 builds the case for parameter projection. 
Section 5 introduces the Multilingual Dictionary 
Framework which plays a key role in parameter 
projection. Section 6 is the core of the work, where 
we present parameter projection from one language 
to another. Section 7 describes two WSD algo-
rithms which combine various parameters for do-
459
main-specific WSD. Experiments and results are 
presented in sections 8 and 9. Section 10 concludes 
the paper. 
2 Related work 
Knowledge based approaches to WSD such as 
Lesk?s algorithm (Michael Lesk, 1986), Walker?s 
algorithm (Walker D. & Amsler R., 1986), concep-
tual density (Agirre Eneko & German Rigau, 1996) 
and random walk algorithm (Mihalcea Rada, 2005) 
essentially do Machine Readable Dictionary loo-
kup. However, these are fundamentally overlap 
based algorithms which suffer from overlap sparsi-
ty, dictionary definitions being generally small in 
length.  
Supervised learning algorithms for WSD are 
mostly word specific classifiers, e.g., WSD using 
SVM (Lee et. al., 2004), Exemplar based WSD 
(Ng Hwee T. & Hian B. Lee, 1996) and decision 
list based algorithm (Yarowsky, 1994). The re-
quirement of a large training corpus renders these 
algorithms unsuitable for resource scarce languag-
es. 
Semi-supervised and unsupervised algorithms 
do not need large amount of annotated corpora, but 
are again word specific classifiers, e.g., semi-
supervised decision list algorithm (Yarowsky, 
1995) and Hyperlex (V?ronis Jean, 2004)). Hybrid 
approaches like WSD using Structural Semantic 
Interconnections (Roberto Navigli & Paolo Velar-
di, 2005) use combinations of more than one 
knowledge sources (wordnet as well as a small 
amount of tagged corpora). This allows them to 
capture important information encoded in wordnet 
(Fellbaum, 1998) as well as draw syntactic genera-
lizations from minimally tagged corpora.  
At this point we state that no single existing so-
lution to WSD completely meets our requirements 
of multilinguality, high domain accuracy and 
good performance in the face of not-so-large 
annotated corpora. 
3 Parameters for WSD  
We discuss a number of parameters that play a 
crucial role in WSD. To appreciate this, consider 
the following example: 
 
The river flows through this region to meet the sea. 
 
The word sea is ambiguous and has three senses as 
given in the Princeton Wordnet (PWN): 
S1: (n) sea (a division of an ocean or a large body 
of salt water partially enclosed by land) 
S2: (n) ocean, sea (anything apparently limitless in 
quantity or volume) 
S3: (n) sea (turbulent water with swells of consi-
derable size) "heavy seas" 
Our first parameter is obtained from Domain 
specific sense distributions. In the above example, 
the first sense is more frequent in the tourism do-
main (verified from manually sense marked tour-
ism corpora). Domain specific sense distribution 
information should be harnessed in the WSD task. 
The second parameter arises from the domin-
ance of senses in the domain. Senses are ex-
pressed by synsets, and we define a dominant 
sense as follows: 
 
A few dominant senses in the Tourism domain are 
{place, country, city, area}, {body of water}, {flo-
ra, fauna}, {mode of transport} and {fine arts}. In 
disambiguating a word, that sense which belongs 
to the sub-tree of a domain-specific dominant 
sense should be given a higher score than other 
senses. The value of this parameter (?) is decided 
as follows: 
? = 1; if the candidate synset is a dominant synset 
? = 0.5; if the candidate synset belongs to the sub-
tree of a dominant synset 
? = 0.001; if the candidate synset is neither a do-
minant synset nor belongs to the sub-tree of a do-
minant synset. 
Our third parameter comes from Corpus co-
occurrence. Co-occurring monosemous words as 
well as already disambiguated words in the con-
text help in disambiguation. For example, the word 
river appearing in the context of sea is a mono-
semous word. The frequency of co-occurrence of 
river with the ?water body? sense of sea is high in 
the tourism domain. Corpus co-occurrence is cal-
A synset node in the wordnet hypernymy 
hierarchy is called Dominant if the syn-
sets in the sub-tree below the synset are 
frequently occurring in the domain cor-
pora. 
460
culated by considering the senses which occur in a 
window of 10 words around a sense. 
Our fourth parameter is based on the semantic 
distance between any pair of synsets in terms of 
the shortest path length between two synsets in the 
wordnet graph. An edge in the shortest path can be 
any semantic relation from the wordnet relation 
repository (e.g., hypernymy, hyponymy, meronymy, 
holonymy, troponymy etc.). 
For nouns we do something additional over and 
above the semantic distance. We take advantage of 
the deeper hierarchy of noun senses in the wordnet 
structure. This gives rise to our fifth and final pa-
rameter which arises out of the conceptual dis-
tance between a pair of senses. Conceptual 
distance between two synsets S1 and S2 is calcu-
lated using Equation (1), motivated by Agirre Ene-
ko & German Rigau (1996). 
 
Concep-
tual 
Distance    
(S1, S2) 
 
 
 
= 
Length of the path between (S1, 
S2) in terms of hypernymy hie-
rarchy 
Height of the lowest common 
ancestor of S1 and S2 in the word-
net hierarchy 
 
 
 (1) 
The conceptual distance is proportional to the 
path length between the synsets, as it should be. 
The distance is also inversely proportional to the 
height of the common ancestor of two sense nodes, 
because as the common ancestor becomes more 
and more general the conceptual relatedness tends 
to get vacuous (e.g., two nodes being related 
through entity which is the common ancestor of 
EVERYTHING, does not really say anything 
about the relatedness). 
To summarize, our various parameters used for 
domain-specific WSD are: 
Wordnet-dependent parameters  
? belongingness-to-dominant-concept 
? conceptual-distance 
? semantic-distance 
Corpus-dependent parameters 
? sense distributions 
? corpus co-occurrence. 
In section 7 we show how these parameters are 
used to come up with a scoring function for WSD. 
4 Building a case for Parameter Projec-
tion   
Wordnet-dependent parameters depend on the 
graph based structure of Wordnet whereas the 
Corpus-dependent parameters depend on various 
statistics learnt from a sense marked corpora. Both 
the tasks of (a) constructing a wordnet from scratch 
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. An im-
portant question being addressed in this paper is: 
whether the effort required in constructing seman-
tic graphs for multiple wordnets and collecting 
sense marked corpora can be avoided? Our find-
ings seem to suggest that by projecting relations 
from the wordnet of a language and by projecting 
corpus statistics from the sense marked corpora of 
the language we can achieve this end. Before we 
proceed to discuss the way to realize parameter 
projection, we present a novel dictionary which 
facilitates this task. 
5 Synset based multilingual dictionary  
Parameter projection as described in section 4 rests 
on a novel and effective method of storage and use 
of dictionary in a multilingual setting proposed by 
Mohanty et. al. (2008). For the purpose of current 
discussion, we will call this multilingual dictionary 
framework MultiDict. One important departure 
from traditional dictionary is that synsets are 
linked, and after that the words inside the syn-
sets are linked. The basic mapping is thus be-
tween synsets and thereafter between the words.  
 
Concepts L1 
(Eng-
lish) 
L2 (Hindi) L3 (Mara-
thi) 
04321: a 
youthful 
male per-
son 
{male
child, 
boy} 
{????? ladkaa, 
????  baalak,  
????? 
bachchaa}  
{?????  mulgaa , 
?????  porgaa , 
???  por } 
Table 1: Multilingual Dictionary Framework 
Table 1 shows the structure of MultiDict, with one 
example row standing for the concept of boy. The 
first column is the pivot describing a concept with 
a unique ID. The subsequent columns show the 
words expressing the concept in respective lan-
guages (in the example table above, English, Hindi 
and Marathi). Thus to express the concept ?04321: 
a youthful male person?, there are two lexical ele-
ments in English, which constitute a synset. Cor-
respondingly, the Hindi and Marathi synsets 
contain 3 words each. 
461
It may be noted that the central language whose 
synsets the synsets of other languages link to is 
Hindi. This way of linking synsets- more popularly 
known as the expansion approach- has several ad-
vantages as discussed in (Mohanty et. al., 2008). 
One advantage germane to the point of this paper 
is that the synsets in a particular column automati-
cally inherit the various semantic relations of the 
Hindi wordnet (Dipak Narayan et. al., 2000), 
which saves the effort involved in reconstructing 
these relations for multiple languages. 
After the synsets are linked, cross linkages are 
set up manually from the words of a synset to the 
words of a linked synset of the central language. 
The average number of such links per synset per 
language pair is approximately 3. These cross-
linkages actually solve the problem of lexical 
choice in translating from text of one language to 
another. 
Thus for the Marathi word ?????  {mulagaa} de-
noting ?a youthful male person?, the correct lexi-
cal substitute from the corresponding Hindi synset 
is ????? {ladakaa} (Figure 1). One might argue that 
any word within the synset could serve the purpose 
of translation. However, the exact lexical substitu-
tion has to respect native speaker acceptability.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Cross linked synset members for the 
concept: a youthful male person 
We put these cross linkages to another use, as 
described later. 
Since it is the MultiDict which is at the heart of 
parameter projection, we would like to summarize 
the main points of this section. (1) By linking with 
the synsets of Hindi, the cost of building wordnets 
of other languages is partly reduced (semantic rela-
tions are inherited). The wordnet parameters of 
Hindi wordnet now become projectable to other 
languages. (2) By using the cross linked words in 
the synsets, corpus parameters become projectable 
(vide next section).  
6 Parameter projection using MultDict  
6.1 P(Sense|Word) parameter 
Suppose a word (say, W) in language L1 (say, Ma-
rathi) has k senses. For each of these k senses we 
are interested in finding the parameter P(Si|W)- 
which is the probability of sense Si given the word 
W expressed as: 
? ??  ?) =  
#(??  ,?)
 #(??  ,?)?  
 
where ?#? indicates ?count-of?. Consider the exam-
ple of two senses of the Marathi word ???? 
{saagar}, viz., sea and abundance and the corres-
ponding cross-linked words in Hindi (Figure 2 be-
low): 
     Marathi            Hindi 
 
 
 
 
 
 
Figure 2: Two senses of the Marathi word ???? 
(saagar), viz., {water body} and {abundance}, and 
the corresponding cross-linked words in Hindi1. 
The probability P({water body}|saagar) for Mara-
thi is  
#({????? ????}, ??????)
#({????? ????}, ??????) + #({?????????}, ??????)
 
 
We propose that this can be approximated by the 
counts from Hindi sense marked corpora by replac-
ing saagar with the cross linked Hindi words sa-
mudra and saagar, as per Figure 2: 
 
#({water body}, samudra)
#({water body}, samudra) + #({abundance}, saagar)
 
                                                          
1 Sense_8231 shows the same word saagar for both Marathi 
and Hindi. This is not uncommon, since Marathi and Hindi are 
sister languages. 
????? 
/MW1 
mulagaa,  
????? 
/MW2 
poragaa, 
??? /MW3 
pora  
 
  ????? 
/HW1 
ladakaa,  
???? 
/HW2 
baalak, 
????? /HW3 
bachcha, 
???? /HW4 
choraa  
 
 
 
male-child 
/HW1, 
 
boy 
/HW2  
 
 
 
 
Marathi Synset Hindi Synset   English Synset 
Sense_2650 
Sense_8231 
 
saagar (sea) 
{water body} 
saagar (sea) 
{abundance} 
samudra (sea) 
{water body} 
saagar (sea) 
{abundance} 
462
Thus, the following formula is used for calculat-
ing the sense distributions of Marathi words using 
the sense marked Hindi corpus from the same do-
main: 
? ?? ?) =  
#(??  , ?????_??????_?????_????)
 #(??  , ?????_??????_?????_????)?  
           (2) 
Note that we are not interested in the exact sense 
distribution of the words, but only in the relative 
sense distribution.  
To prove that the projected relative distribution 
is faithful to the actual relative distribution of 
senses, we obtained the sense distribution statistics 
of a set of Marathi words from a sense tagged Ma-
rathi corpus (we call the sense marked corpora of a 
language its self corpora). These sense distribu-
tion statistics were compared with the statistics for 
these same words obtained by projecting from a 
sense tagged Hindi corpus using Equation (2).  The 
results are summarized in Table 2. 
Sr. 
No 
Marathi 
Word 
Synset P(S|word) 
as learnt 
from 
sense 
tagged 
Marathi 
corpus 
P(S|word) as 
projected 
from sense 
tagged 
Hindi cor-
pus 
1 ???? ?? 
(kimat)  
{ worth } 0.684 0.714 
{ price }  0.315 0.285 
2 ????? 
(rasta)  
 
{ roadway } 0.164 0.209 
{road, 
route} 
0.835 0.770 
3 ????? 
(thikan) 
{ land site, 
place} 
0.962 0.878 
{ home } 0.037 0.12 
4 ???? 
(saagar) 
{water 
body} 
1.00 1.00 
{abun-
dance} 
0 0 
Table 2: Comparison of the sense distributions of 
some Marathi words learnt from Marathi sense 
tagged corpus with those projected from Hindi 
sense tagged corpus. 
The fourth row of Table 2 shows that whenever 
???? (saagar) (sea) appears in the Marathi tourism 
corpus there is a 100% chance that it will appear in 
the ?water body? sense and 0% chance that it will 
appear in the sense of ?abundance?. Column 5 
shows that the same probability values are ob-
tained using projections from Hindi tourism cor-
pus. Taking another example, the third row shows 
that whenever ????? (thikaan) (place, home) ap-
pears in the Marathi tourism corpus there is a much 
higher chance of it appearing in the sense of 
?place? (96.2%) then in the sense of ?home? 
(3.7%). Column 5 shows that the relative proba-
bilities of the two senses remain the same even 
when using projections from Hindi tourism corpus 
(i.e. by using the corresponding cross-linked words 
in Hindi). To quantify these observations, we cal-
culated the average KL divergence and Spearman?s 
correlation co-efficient between the two distribu-
tions. The KL divergence is 0.766 and Spearman?s 
correlation co-efficient is 0.299. Both these values 
indicate that there is a high degree of similarity 
between the distributions learnt using projection 
and those learnt from the self corpus. 
6.2 Co-occurrence parameter 
Similarly, within a domain, the statistics of co-
occurrence of senses remain the same across lan-
guages. For example, the co-occurrence of the Ma-
rathi synsets {???? (akash) (sky), ????? (ambar) 
(sky)} and {??? (megh) (cloud), ???? (abhra) 
(cloud)} in the Marathi corpus remains more or 
less same as (or proportional to) the co-occurrence 
between the corresponding Hindi synsets in the 
Hindi corpus.   
Sr. No Synset Co-
occurring 
Synset 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Marathi 
corpus 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Hindi 
corpus 
1 {???, ?????} 
{small bush} 
{???, ?????, 
?????, ?????, 
???, ????}  
{tree} 
0.125 0.125 
2 {???, ????} 
{cloud} 
{????, 
????, 
?????}  
{sky} 
0.167 0.154 
3 {?????, ?^???, 
?^???, 
??????}  
{geographical 
area} 
{????, 
???}  
{travel} 
0.0019 0.0017 
Table 3: Comparison of the corpus co-occurrence 
statistics learnt from Marathi and Hindi Tourism 
corpus. 
463
Table 3 shows a few examples depicting similarity 
between co-occurrence statistics learnt from Mara-
thi tourism corpus and Hindi tourism corpus. Note 
that we are talking about co-occurrence of synsets 
and not words. For example, the second row shows 
that the probability of co-occurrence of the synsets 
{cloud} and {sky} is almost same in the Marathi 
and Hindi corpus. 
7 Our algorithms for WSD 
We describe two algorithms to establish the use-
fulness of the idea of parameter projection. The 
first algorithm- called iterative WSD (IWSD-) is 
greedy, and the second based on PageRank algo-
rithm is exhaustive. Both use scoring functions that 
make use of the parameters detailed in the previous 
sections.  
7.1 Iterative WSD (IWSD) 
We have been motivated by the Energy expression 
in Hopfield network (Hopfield, 1982) in formulat-
ing a scoring function for ranking the senses. Hop-
field Network is a fully connected bidirectional 
symmetric network of bi-polar (0/1 or +1/-1) neu-
rons. We consider the asynchronous Hopfield 
Network. At any instant, a randomly chosen neu-
ron (a) examines the weighted sum of the input, (b) 
compares this value with a threshold and (c) gets to 
the state of 1 or 0, depending on whether the input 
is greater than or less than or equal to the thre-
shold. The assembly of 0/1 states of individual 
neurons defines a state of the whole network. Each 
state has associated with it an energy, E, given by 
the following expression 
 
? = ????? +  ???
?
?>?
?
?=1
????  
 
(3) 
 
where, N is the total number of neurons in the net-
work, ??   and ??  are the activations of neurons i and 
j respectively and ???  is the weight of the connec-
tion between neurons i and j.  Energy is a funda-
mental property of Hopfield networks, providing 
the necessary machinery for discussing conver-
gence, stability and such other considerations. 
The energy expression as given above cleanly 
separates the influence of self-activations of neu-
rons and that of interactions amongst neurons to 
the global macroscopic property of energy of the 
network.  This fact has been the primary insight for 
equation (4) which was proposed to score the most 
appropriate synset in the given context. The cor-
respondences are as follows:   
 
Neuron ? Synset 
Self-activation ? Corpus Sense Distribu-
tion 
Weight of connec-
tion between two 
neurons 
 
? 
Weight as a function of 
corpus co-occurrence 
and Wordnet distance 
measures between syn-
sets 
 
?? = argmax
?
  ?? ? ?? +  ??? ? ?? ? ??
?  ? J
   4  
?????, 
  J = ??? ?? ????????????? ?????            
         ?? = ?????????????????????????????? (??)
   ??  = ? ??  | ????                                                   
 
 ??? =  ??????????????????  ?? , ??                    
                 ?  1 ????????????????????(?? , ?? )           
                   ?  1 ???????????????????????(?? , ?? )    
 
The component ?? ? ??  of the energy due to the self 
activation of a neuron can be compared to the cor-
pus specific sense of a word in a domain. The other 
component ??? ?  ?? ? ??  coming from the interaction 
of activations can be compared to the score of a 
sense due to its interaction in the form of corpus 
co-occurrence, conceptual distance, and wordnet-
based semantic distance with the senses of other 
words in the sentence. The first component thus 
captures the rather static corpus sense, whereas the 
second expression brings in the sentential context.  
Algorithm 1: performIterativeWSD(sentence) 
1. Tag all monosemous words in the sentence. 
2. Iteratively disambiguate the remaining words in the 
sentence in increasing order of their degree of polyse-
my. 
3. At each stage select that sense for a word which max-
imizes the score given by Equation (4) 
Algorithm1: Iterative WSD  
IWSD is clearly a greedy algorithm. It bases its 
decisions on already disambiguated words, and 
ignores words with higher degree of polysemy. For 
example, while disambiguating bisemous words, 
the algorithm uses only the monosemous words. 
464
7.2 Modified PageRank algorithm 
Rada Mihalcea (2005) proposed the idea of using 
PageRank algorithm to find the best combination 
of senses in a sense graph. The nodes in a sense 
graph correspond to the senses of all the words in a 
sentence and the edges depict the strength of inte-
raction between senses. The score of each node in 
the graph is then calculated using the following 
recursive formula: 
????? ?? =                                                                 
 1? d + d ?  
Wij
 WjkSk?Out  Si 
? Score Sj 
S j?In S i 
 
Instead of calculating Wij  based on the overlap 
between the definition of senses Si and S  as pro-
posed by Rada Mihalcea (2005), we calculate the 
edge weights using the following formula: 
 ??? =  ??????????????????  ?? , ??                    
                   ?  1 ???????????????????? ?? , ??             
                   
?  1 ??????????????????????? ?? , ??    
?  ? ??  | ?????                                                   
?  ? ??  | ?????                                                   
  
? = ??????? ??????  ????????? 0.85     
 
This formula helps capture the edge weights in 
terms of the corpus bias as well as the interaction 
between the senses in the corpus and wordnet. It 
should be noted that this algorithm is not greedy. 
Unlike IWSD, this algorithm allows all the senses 
of all words to play a role in the disambiguation 
process.  
8 Experimental Setup: 
We tested our algorithm on tourism corpora in 3 
languages (viz., Marathi, Bengali and Tamil) and 
health corpora in 1 language (Marathi) using pro-
jections from Hindi. The corpora for both the do-
mains were manually sense tagged. A 4-fold cross 
validation was done for all the languages in both 
the domains. The size of the corpus for each lan-
guage is described in Table 4. 
Language # of polysemous words 
(tokens) 
Tourism 
Domain 
Health 
Domain 
Hindi 50890 29631 
Marathi 32694 8540 
Bengali 9435  - 
Tamil 17868 - 
Table 4: Size of manually sense tagged corpora for 
different languages. 
 
Table 5 shows the number of synsets in MultiDict 
for each language. 
Language # of synsets in 
MultiDict 
Hindi 29833 
Marathi 16600 
Bengali 10732 
Tamil 5727 
Table 5: Number of synsets for each language 
 
Algorithm Language 
Marathi Bengali 
P  % R % F % P  % R % F % 
IWSD (training on self corpora; no parameter pro-
jection) 81.29 80.42 80.85 81.62 78.75 79.94 
IWSD (training on Hindi and reusing parameters  
for another language) 73.45 70.33 71.86 79.83 79.65 79.79 
PageRank (training on self corpora; no parameter 
projection) 79.61 79.61 79.61 76.41 76.41 76.41 
PageRank (training on Hindi and reusing parame-
ters  for another language) 71.11 71.11 71.11 75.05 75.05 75.05 
Wordnet Baseline 58.07 58.07 58.07 52.25 52.25 52.25 
Table 6: Precision, Recall and F-scores of IWSD, PageRank and Wordnet Baseline. Values are re-
ported with and without parameter projection. 
 
465
9 Results and Discussions 
Table 6 shows the results of disambiguation (preci-
sion, recall and F-score). We give values for two 
algorithms in the tourism domain: IWSD and Pa-
geRank. In each case figures are given for both 
with and without parameter projection. The word-
net baseline figures too are presented for the sake 
of grounding the results.  
Note the lines of numbers in bold, and compare 
them with the numbers in the preceding line. This 
shows the fall in accuracy value when one tries the 
parameter projection approach in place of self cor-
pora. For example, consider the F-score as given 
by IWSD for Marathi. It degrades from about 81% 
to 72% in using parameter projection in place of 
self corpora.  Still, the value is much more than the 
baseline, viz., the wordnet first sense (a typically 
reported baseline). 
Coming to PageRank for Marathi, the fall in ac-
curacy is about 8%. Appendix A shows the corres-
ponding figure for Tamil with IWSD as 10%. 
Appendix B reports the fall to be 11% for a differ-
ent domain- Health- for Marathi (using IWSD).  
In all these cases, even after degradation the per-
formance is far above the wordnet baseline. This 
shows that one could trade accuracy with the cost 
of creating sense annotated corpora.  
10 Conclusion and Future Work: 
Based on our study for 3 languages and 2 domains, 
we conclude the following: 
(i) Domain specific sense distributions- if 
obtainable- can be exploited to advantage. 
(ii) Since sense distributions remain same across 
languages, it is possible to create a disambiguation 
engine that will work even in the absence of sense 
tagged corpus for some resource deprived 
language, provided (a) there are aligned and cross 
linked sense dictionaries for the language in 
question and another resource rich language, (b) 
the domain in which disambiguation needs to be 
performed for the resource deprived language is 
the same as the domain for which sense tagged 
corpora is available for the resource rich language.  
(iii) Provided the accuracy reduction is not drastic, 
it may make sense to trade high accuracy for the 
effort in collecting sense marked corpora.  
It would be interesting to test our algorithm on 
other domains and other languages to conclusively 
establish the effectiveness of parameter projection 
for multilingual WSD.  
It would also be interesting to analyze the con-
tribution of corpus and wordnet parameters inde-
pendently. 
References  
Agirre Eneko & German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In Proceed-
ings of the 16th International Conference on 
Computational Linguistics (COLING), Copenhagen, 
Denmark. 
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande 
and P. Bhattacharyya. 2002. An Experience in Build-
ing the Indo WordNet - a WordNet for Hindi. First 
International Conference on Global WordNet, My-
sore, India. 
Eneko Agirre & Philip Edmonds. 2007. Word Sense 
Disambiguation Algorithms and Applications. Sprin-
ger Publications. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press.  
Hindi Wordnet. 
http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
J. J. Hopfield. April 1982. "Neural networks and physi-
cal systems with emergent collective computational 
abilities", Proceedings of the National Academy of 
Sciences of the USA, vol. 79 no. 8 pp. 2554-2558. 
Lee Yoong K., Hwee T. Ng & Tee K. Chia. 2004. Su-
pervised word sense disambiguation with support 
vector machines and multiple knowledge sources. 
Proceedings of Senseval-3: Third International 
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain, 137-140. 
Lin Dekang. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics (ACL), Madrid, 64-71. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. In Proceedings of 
the 5th annual international conference on Systems 
documentation, Toronto, Ontario, Canada. 
Mihalcea Rada. 2005. Large vocabulary unsupervised 
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of 
the Joint Human Language Technology and Empiri-
466
cal Methods in Natural Language Processing Confe-
rence (HLT/EMNLP), Vancouver, Canada, 411-418. 
Ng Hwee T. & Hian B. Lee. 1996. Integrating multiple 
knowledge sources to disambiguate word senses: An 
exemplar-based approach. In Proceedings of the 34th 
Annual Meeting of the Association for Computation-
al Linguistics (ACL), Santa Cruz, U.S.A., 40-47. 
Peter F. Brown and Vincent J.Della Pietra and Stephen 
A. Della Pietra and Robert. L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics Vol 
19, 263-311. 
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar 
Pande, Shraddha Kalele, Mitesh Khapra and Aditya 
Sharma. 2008. Synset Based Multilingual Dictionary: 
Insights, Applications and Challenges. Global Word-
net Conference, Szeged, Hungary, January 22-25. 
Resnik Philip. 1997. Selectional preference and sense 
disambiguation. In Proceedings of ACL Workshop 
on Tagging Text with Lexical Semantics, Why, What 
and How? Washington, U.S.A., 52-57. 
Roberto Navigli, Paolo Velardi. 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation. IEEE 
Transactions On Pattern Analysis and Machine Intel-
ligence. 
Sergei Nirenburg, Harold Somers, and Yorick Wilks. 
2003. Readings in Machine Translation. Cambridge, 
MA: MIT Press. 
V?ronis Jean. 2004. HyperLex: Lexical cartography for 
information retrieval. Computer Speech & Language, 
18(3):223-252. 
Walker D. and Amsler R. 1986. The Use of Machine 
Readable Dictionaries in Sublanguage Analysis. In 
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pp. 69-83. 
Yarowsky David. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in 
Spanish and French. In Proceedings of the 32nd An-
nual Meeting of the association for Computational 
Linguistics (ACL), Las Cruces, U.S.A., 88-95. 
Yarowsky David. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL), 
Cambridge, MA, 189-196. 
 
 
Appendix A: Results for Tamil (Tourism 
Domain) 
Algorithm P  % R  % F % 
IWSD (training on 
Tamil) 89.50 88.18 88.83 
IWSD (training on 
Hindi and reusing  for 
Tamil) 84.60 73.79 78.82 
Wordnet Baseline 65.62 65.62 65.62 
Table 7: Tamil Tourism corpus using parameters 
projected from Hindi 
Appendix B: Results for Marathi (Health 
Domain) 
Algorithm 
Words 
P  % R  % F % 
IWSD (training on Mara-
thi) 84.28 81.25 82.74 
IWSD (training on Hindi 
and reusing  for Marathi) 75.96 67.75 71.62 
Wordnet Baseline 60.32 60.32 60.32 
Table 8: Marathi Health corpus parameters pro-
jected from Hindi 
 
467
Simple Syntactic and Morphological Processing Can Help English-Hindi
Statistical Machine Translation
Ananthakrishnan Ramanathan,
Pushpak Bhattacharyya
Department of Computer Science
and Engineering
Indian Institute of Technology
Powai, Mumbai-400076
India
{anand,pb}@cse.iitb.ac.in
Jayprasad Hegde, Ritesh M. Shah,
Sasikumar M
CDAC Mumbai (formerly NCST)
Gulmohar Cross Road No. 9
Juhu, Mumbai-400049
India
{jjhegde,ritesh,sasi}
@cdacmumbai.in
Abstract
In this paper, we report our work on incor-
porating syntactic and morphological infor-
mation for English to Hindi statistical ma-
chine translation. Two simple and compu-
tationally inexpensive ideas have proven to
be surprisingly effective: (i) reordering the
English source sentence as per Hindi syntax,
and (ii) using the suffixes of Hindi words.
The former is done by applying simple trans-
formation rules on the English parse tree.
The latter, by using a simple suffix separa-
tion program. With only a small amount of
bilingual training data and limited tools for
Hindi, we achieve reasonable performance
and substantial improvements over the base-
line phrase-based system. Our approach es-
chews the use of parsing or other sophisti-
cated linguistic tools for the target language
(Hindi) making it a useful framework for
statistical machine translation from English
to Indian languages in general, since such
tools are not widely available for Indian lan-
guages currently.
1 Introduction
Techniques for leveraging syntactic and morpholog-
ical information for statistical machine translation
(SMT) are receiving a fair amount of attention nowa-
days. For SMT from English to Indian languages,
these techniques are especially important for the fol-
lowing three reasons: (i) Indian languages differ
widely from English in terms of word-order; (ii) In-
dian languages are morphologically quite rich; and
(iii) large amounts of parallel corpora are not avail-
able for these languages, though smaller amounts of
text in specific domains (such as health, tourism, and
agriculture) are now becoming accessible. It might
therefore be expected that using syntactic and mor-
phological information for English to Indian lan-
guage SMT will prove highly beneficial in terms
of achieving reasonable performance out of limited
parallel corpora. However, the difficulty in this is
that crucial tools, such as parsers and morphological
analyzers, are not widely available for Indian lan-
guages yet.
In this paper, we present our work on incorporat-
ing syntactic and morphological information for En-
glish to Hindi SMT. Our approach, which eschews
the use of parsing and other tools for Hindi, is two-
pronged:
1. Incorporating syntactic information by com-
bining phrase-based models with a set of struc-
tural preprocessing rules on English
2. Incorporating morphological information by
using a simple suffix separation program for
Hindi, the likes of which can be created with
limited effort for other Indian languages as well
Significant improvements over the baseline
phrase-based SMT system are obtained using our
approach. Table 1 illustrates this with an example 1.
Since only limited linguistic effort and tools are
required for the target language, we believe that the
framework we propose is suitable for SMT from En-
glish to other Indian languages as well.
1This example is discussed further in section 4
513
input For a celestial trip of the scientific kind, visit the planetarium.
reference
	 ta	 	 e	   	 e , ta ae?
vaigyaanika tariike ke eka divya saira ke lie, taaraamandala aaem
scientific kind of a celestial trip for, planetarium visit (come)
baseline
	   	 	 pr	 , 	 ta

 ?
ke svargiiya yaatraa ke vaigyaanika prakaara, kaa taaraagruha hai
of celestial trip of scientific kind, of planetarium is
baseline+syn
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha hai
scientific kind of celestial trip for, planetarium is
baseline+syn+morph
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha dekhem
scientific kind of celestial trip for, planetarium visit (see)
Table 1: Effects of Syntactic and Morphological Processing (reference: human reference translation;
baseline: phrase-based system; syn: with syntactic information; morph: with morphological information)
The rest of this paper is organized as follows: Sec-
tion 2 outlines related work. Section 3 describes our
approach ? first, the phrase-based baseline system is
sketched briefly, leading up to the techniques used
for incorporating syntactic and morphological infor-
mation within this system. Experimental results are
discussed in section 4. Section 5 concludes the pa-
per with some directions for future work.
2 Related Work
Statistical translation models have evolved from the
word-based models originally proposed by Brown
et al (1990) to syntax-based and phrase-based tech-
niques.
The beginnings of phrase-based translation can
be seen in the alignment template model introduced
by Och et al (1999). A joint probability model
for phrase translation was proposed by Marcu and
Wong (2002). Koehn et al (2003) propose certain
heuristics to extract phrases that are consistent with
bidirectional word-alignments generated by the IBM
models (Brown et al, 1990). Phrases extracted us-
ing these heuristics are also shown to perform bet-
ter than syntactically motivated phrases, the joint
model, and IBM model 4 (Koehn et al, 2003).
Syntax-based models use parse-tree representa-
tions of the sentences in the training data to learn,
among other things, tree transformation probabili-
ties. These methods require a parser for the target
language and, in some cases, the source language
too. Yamada and Knight (2001) propose a model
that transforms target language parse trees to source
language strings by applying reordering, insertion,
and translation operations at each node of the tree.
Graehl and Knight (2004) and Melamed (2004), pro-
pose methods based on tree-to-tree mappings. Ima-
mura et al (2005) present a similar method that
achieves significant improvements over a phrase-
based baseline model for Japanese-English transla-
tion.
Recently, various preprocessing approaches have
been proposed for handling syntax within SMT.
These algorithms attempt to reconcile the word-
order differences between the source and target lan-
guage sentences by reordering the source language
data prior to the SMT training and decoding cy-
cles. Nie?en and Ney (2004) propose some restruc-
turing steps for German-English SMT. Popovic and
Ney (2006) report the use of simple local trans-
formation rules for Spanish-English and Serbian-
English translation. Collins et al (2006) propose
German clause restructuring to improve German-
English SMT.
The use of morphological information for SMT
has been reported in (Nie?en and Ney, 2004) and
(Popovic and Ney, 2006). The detailed experi-
ments by Nie?en and Ney (2004) show that the use
of morpho-syntactic information drastically reduces
the need for bilingual training data.
Recent work by Koehn and Hoang (2007) pro-
514
poses factored translation models that combine fea-
ture functions to handle syntactic, morphological,
and other linguistic information in a log-linear
model.
Our work uses a preprocessing approach for in-
corporating syntactic information within a phrase-
based SMT system. For incorporating morphology,
we use a simple suffix removal program for Hindi
and a morphological analyzer for English. These as-
pects are described in detail in the next section.
3 Syntactic & Morphological Information
for English-Hindi SMT
3.1 Phrase-Based SMT: the Baseline
Given a source sentence f , SMT chooses as its trans-
lation e?, which is the sentence with the highest prob-
ability:
e? = arg max
e
p(e|f)
According to Bayes? decision rule, this is written
as:
e? = arg max
e
p(e)p(f |e)
The phrase-based model that we use as our base-
line system (defined by Koehn et al (2003)) com-
putes the translation model p(f |e) by using a phrase
translation probability distribution. The decoding
process works by segmenting the input sentence f
into a sequence of I phrases f
I
1
. A uniform proba-
bility distribution over all possible segmentations is
assumed. Each phrase f
i
is translated into a target
language phrase e
i
with probability ?(f
i
|e
i
). Re-
ordering is penalized according to a simple exponen-
tial distortion model.
The phrase translation table is learnt in the fol-
lowing manner: The parallel corpus is word-aligned
bidirectionally, and using various heuristics (see
(Koehn et al, 2003) for details) phrase correspon-
dences are established. Given the set of collected
phrase pairs, the phrase translation probability is cal-
culated by relative frequency:
?(f |e) = count(f, e)?
f
count(f, e)
Lexical weighting, which measures how well
words within phrase pairs translate to each other,
validates the phrase translation, and addresses the
problem of data sparsity.
The language model p(e) used in our baseline sys-
tem is a trigram model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998).
The weights for the various components of the
model (phrase translation model, language model,
distortion model etc.) are set by minimum error rate
training (Och, 2003).
3.2 Syntactic Information
As mentioned in section 2, phrase-based models
have emerged as the most successful method for
SMT. These models, however, do not handle syntax
in a natural way. Reordering of phrases during trans-
lation is typically managed by distortion models,
which have proved not entirely satisfactory (Collins
et al, 2006), especially for language pairs that differ
a lot in terms of word-order. We use a preprocess-
ing approach to get over this problem, by reordering
the English sentences in the training and test corpora
before the SMT system kicks in. This reduces, and
often eliminates, the ?distortion load? on the phrase-
based system.
The reordering rules that we use for prepro-
cessing can be broadly described by the following
transformation rule going from English to Hindi
word order (Rao et al 2000):
SS
m
V V
m
OO
m
Cm ? C
?
m
S
?
m
S
?
O
?
m
O
?
V
?
m
V
?
where,
S: Subject
O: Object
V : Verb
C
m
: Clause modifier
X
?: Corresponding constituent in Hindi,
where X is S, O, or V
X
m
: modifier of X
Essentially, the SVO order of English is changed
to SOV order, and post-modifiers are converted to
pre-modifiers. Our preprocessing module effects
this by parsing the input English sentence 2 and ap-
2Dan Bikel?s parser was used for parsing
(http://www.cis.upenn.edu/d?bikel/license.html).
515
structural transformation
morph analysis (English) Giza++
alignment correction
phrase extraction
suffix separation 
(Hindi)
decoder
	



	



Designing a Common POS-Tagset Framework for Indian Languages 
Sankaran Baskaran, Microsoft Research India. Bangalore. baskaran@microsoft.com 
Kalika Bali, Microsoft Research India. Bangalore. kalikab@microsoft.com 
Tanmoy Bhattacharya, Delhi University, Delhi. tanmoy1@gmail.com 
Pushpak Bhattacharyya, IIT-Bombay, Mumbai. pb@cse.iitb.ac.in 
Girish Nath Jha, Jawaharlal Nehru University, Delhi. girishj@mail.jnu.ac.in 
Rajendran S, Tamil University, Thanjavur. raj_ushush@yahoo.com 
Saravanan K, Microsoft Research India, Bangalore. v-sarak@microsoft.com 
Sobha L, AU-KBC Research Centre, Chennai. sobha@au-kbc.org 
Subbarao K V. Delhi. kvs2811@yahoo.com
 
 
Abstract 
Research in Parts-of-Speech (POS) tagset 
design for European and East Asian lan-
guages started with a mere listing of impor-
tant morphosyntactic features in one lan-
guage and has matured in later years to-
wards hierarchical tagsets, decomposable 
tags, common framework for multiple lan-
guages (EAGLES) etc. Several tagsets 
have been developed in these languages 
along with large amount of annotated data 
for furthering research. Indian Languages 
(ILs) present a contrasting picture with 
very little research in tagset design issues. 
We present our work in designing a com-
mon POS-tagset framework for ILs, which 
is the result of in-depth analysis of eight 
languages from two major families, viz. 
Indo-Aryan and Dravidian. Our framework 
follows hierarchical tagset layout similar to 
the EAGLES guidelines, but with signifi-
cant changes as needed for the ILs. 
1 Introduction 
A POS tagset design should take into consideration 
all possible morphosyntactic categories that can 
occur in a particular language or group of languag-
es (Hardie, 2004). Some effort has been made in 
the past, including the EAGLES guidelines for 
morphosyntactic annotation (Leech and Wilson, 
1996) to define guidelines for a common tagset 
across multiple languages with an aim to capture 
more detailed morphosyntactic features of these 
languages.  
However, most of the tagsets for ILs are lan-
guage specific and cannot be used for tagging data 
in other language. This disparity in tagsets hinders 
interoperability and reusability of annotated corpo-
ra. This further affects NLP research in resource 
poor ILs where non-availability of data, especially 
tagged data, remains a critical issue for researchers. 
Moreover, these tagsets capture the morphosyntac-
tic features only at a shallow level and miss out the 
richer information that is characteristic of these 
languages. 
The work presented in this paper focuses on de-
signing a common tagset framework for Indian 
languages using the EAGLES guidelines as a mod-
el. Though Indian languages belong to (mainly) 
four distinct families, the two largest being Indo-
Aryan and Dravidian, as languages that have been 
in contact for a long period of time, they share sig-
nificant similarities in morphology and syntax. 
This makes it desirable to design a common tagset 
framework that can exploit this similarity to facili-
tate the mapping of different tagsets to each other. 
This would not only allow corpora tagged with 
different tagsets for the same language to be reused 
but also achieve cross-linguistic compatibility be-
tween different language corpora. Most important-
ly, it will ensure that common categories of differ-
ent languages are annotated in the same way. 
In the next section we will discuss the impor-
tance of a common standard vis-?-vis the currently 
available tagsets for Indian languages. Section 3 
will provide the details of the design principles 
The 6th Workshop on Asian Languae Resources, 2008
89
behind the framework presented in this paper. Ex-
amples of tag categories in the common framework 
will be presented in Section 4. Section 5 will dis-
cuss the current status of the paper and future steps 
envisaged.  
2 Common Standard for POS Tagsets 
Some of the earlier POS tagsets were designed 
for English (Greene and Rubin, 1981; Garside, 
1987; Santorini, 1990) in the broader context of 
automatic parsing of English text. These tagsets 
popular even today, though designed for the same 
language differ significantly from each other mak-
ing the corpora tagged by one incompatible with 
the other. Moreover, as these are highly language 
specific tagsets they cannot be reused for any other 
language without substantial changes this requires 
standardization of POS tagsets (Hardie 2004).  
Leech and Wilson (1999) put forth a strong argu-
ment for the need to standardize POS tagset for 
reusability of annotated corpora and interopera-
bility across corpora in different languages. 
EAGLES guidelines (Leech and Wilson 1996) 
were a result of such an initiative to create stan-
dards that are common across languages that share 
morphosyntactic features. 
Several POS tagsets have been designed by a 
number of research groups working on Indian 
Languages though very few are available publicly 
(IIIT-tagset, Tamil tagset). However, as each of 
these tagsets have been motivated by specific re-
search agenda, they differ considerably in terms of 
morphosyntactic categories and features, tag defi-
nitions, level of granularity, annotation guidelines 
etc. Moreover, some of the tagsets (Tamil tagset) 
are language specific and do not scale across other 
Indian languages. This has led to a situation where 
despite strong commonalities between the lan-
guages addressed resources cannot be shared due 
to incompatibility of tasgets. This is detrimental to 
the development of language technology for Indian 
languages which already suffer from a lack of ade-
quate resources in terms of data and tools. 
In this paper, we present a common framework 
for all Indian languages where an attempt is made 
to treat equivalent morphosyntactic phenomena 
consistently across all languages. The hierarchical 
design, discussed in detail in the next section, also 
allows for a systematic method to annotate lan-
guage particular categories without disregarding 
the shared traits of the Indian languages.  
3 Design Principles 
Whilst several large projects have been concerned 
with tagset development very few have touched 
upon the design principles behind them. Leech 
(1997), Cloeren (1999) and Hardie (2004) are 
some important examples presenting universal 
principles for tagset design. 
In this section we restrict the discussion to the 
principles behind our tagset framework. Important-
ly, we diverge from some of the universal prin-
ciples but broadly follow them in a consistent way.  
Tagset structure: Flat tagsets just list down the 
categories applicable for a particular language 
without any provision for modularity or feature 
reusability. Hierarchical tagsets on the other hand 
are structured relative to one another and offer a 
well-defined mechanism for creating a common 
tagset framework for multiple languages while 
providing flexibility for customization according to 
the language and/ or application. 
Decomposability in a tagset alows different fea-
tures to be encoded in a tag by separate sub-stings. 
Decomposable tags help in better corpus analysis 
(Leech 1997) by allowing to search with an un-
derspecified search string. 
In our present framework, we have adopted the 
hierarchical layout as well as decomposable tags 
for designing the tagset. The framework will have 
three levels in the hierarchy with categories, types 
(subcategories) and features occupying the top, 
medium and the bottom layers. 
What to encode? One thumb rule for the POS 
tagging is to consider only the aspects of morpho-
syntax for annotation and not that of syntax, se-
mantics or discourse. We follow this throughout 
and focus only on the morphosyntactic aspects of 
the ILs for encoding in the framework. 
Morphology and Granularity: Indian languag-
es have complex morphology with varying degree 
of richness. Some of the languages such as those of 
the Dravidian family also display agglutination as 
an important characteristic. This entails that mor-
phological analysis is a desirable pre-process for 
the POS tagging to achieve better results in auto-
matic tagging. We encode all possible morphosyn-
tactic features in our framework assuming the exis-
The 6th Workshop on Asian Languae Resources, 2008
90
tence of morphological analysers and leave the 
choice of granularity to users. 
As pointed out by Leech (1997) some of the 
linguistically desirable distinctions may not be 
feasible computationally. Therefore, we ignore 
certain features that may not be computationally 
feasible at POS tagging level. 
Multi-words: We treat the constituents of Mul-
ti-word expressions (MWEs) like Indian Space 
Research Organization as individual words and tag 
them separately rather than giving a single tag to 
the entire word sequence. This is done because: 
Firstly, this is in accordance with the standard 
practice followed in earlier tagsets. Secondly, 
grouping MWEs into a single unit should ideally 
be handled in chunking. 
Form vs. function: We try to adopt a balance 
between form and function in a systematic and 
consistent way through deep analysis. Based on 
our analysis we propose to consider the form in 
normal circumstances and the function for words 
that are derived from other words. More details on 
this will be provided in the framework document 
(Baskaran et al2007) 
Theoretical neutrality: As Leech (1997) points 
out the annotation scheme should be theoretically 
neutral to make it clearly understandable to a larger 
group and for wider applicability. 
Diverse Language families: As mentioned ear-
lier, we consider eight languages coming from two 
major language families of India, viz. Indo-Aryan 
and Dravidian. Despite the distinct characteristics 
of these two families, it is however striking to note 
the typological parallels between them, especially 
in syntax. For example, both families follow SOV 
pattern. Also, several Indo-Aryan languages such 
as Marathi, Bangla etc. exhibit some agglutination, 
though not to the same extent of Dravidian. Given 
the strong commonalities between the two families 
we decided to use a single framework for them 
4 POS Tagset Framework for Indian lan-
guages 
The tagset framework is laid out at the following 
four levels similar to EAGLES. 
I. Obligatory attributes or values are generally 
universal for all languages and hence must be 
included in any morphosyntactic tagset. The 
major POS categories are included here. 
II. Recommended attributes or values are recog-
nised to be important sub-categories and fea-
tures common to a majority of languages.  
III. Special extensions1 
a. Generic attributes or values 
b. Language-specific attributes or values are 
the attributes that are relevant only for few lan-
guages and do not apply to most languages. 
All the tags were discussed and debated in detail 
by a group of linguists and computer scien-
tists/NLP experts for eight Indian languages viz. 
Bengali, Hindi, Kannada, Malayalam, Marathi, 
Sanskrit, Tamil and Telugu.  
Now, because of space constraints we present 
only the partial tagset framework. This is just to 
illustrate the nature of the framework and the com-
plete version as well as the rationale for different 
categories/features in the framework can be found 
in Baskaran et al (2007).2 
In the top level the following 12 categories are 
identified as universal categories for all ILs and 
hence these are obligatory for any tagset. 
 
1. [N] Nouns 7.   [PP] Postpositions  
2. [V] Verbs  8.   [DM] Demonstratives 
3. [PR] Pronouns  9.   [QT] Quantifiers 
4. [JJ] Adjectives  10. [RP] Particles  
5. [RB] Adverbs  11. [PU] Punctuations  
6. [PL] Participles  12. [RD] Residual3 
 
The partial tagset illustrated in Figure 1 high-
lights entries in recommended and optional catego-
ries for verbs and participles marked for three le-
vels.4 The features take the form of attribute-value 
pairs with values in italics and in some cases (such 
as case-markers for participles) not all the values 
are fully listed in the figure. 
5 Current Status and Future Work 
In the preceding sections we presented a common 
framework being designed for POS tagsets for In-
dian Languages. This hierarchical framework has 
                                                 
1
 We do not have many features defined under the special 
extensions and this is mainly retained for any future needs. 
2 Currently this is just the draft version and the final version 
will be made available soon 
3 For words or segments in the text occurring outside the gam-
bit of grammatical categories like foreign words, symbols,etc.   
4  These are not finalised as yet and there might be some 
changes in the final version of the framework. 
The 6th Workshop on Asian Languae Resources, 2008
91
three levels to permit flexibility and interoperabili-
ty between languages. We are currently involved in 
a thorough review of the present framework by 
using it to design the tagset for specific Indian lan-
guages. The issues that come up during this 
process will help refine and consolidate the 
framework further.  In the future, annotation guide-
lines with some recommendations for handling 
ambiguous categories will also be defined.  With 
the common framework in place, it is hoped that 
researchers working with Indian Languages would 
be able to not only reuse data annotated by each 
other but also share tools across projects and lan-
guages. 
References 
Baskaran S. et al 2007. Framework for a Common 
     Parts-of-Speech Tagset for Indic Languages. (Draft) 
    http://research.microsoft.com/~baskaran/POSTagset/ 
  
Cloeren, J. 1999. Tagsets. In Syntactic Wordclass Tagging, 
ed. Hans van Halteren, Dordrecht.: Kluwer Academic. 
Hardie, A . 2004. The Computational Analysis of Morpho-
syntactic Categories in Urdu. PhD thesis submitted to 
Lancaster University. 
Greene, B.B. and Rubin, G.M. 1981. Automatic grammati-
cal tagging of English. Providence, R.I.: Department of 
Linguistics, Brown University 
Garside, R. 1987 The CLAWS word-tagging system. In 
The Computational Analysis of English, ed. Garside, 
Leech and Sampson, London: Longman. 
Leech, G and Wilson, A. 1996. Recommendations for the 
Morphosyntactic Annotation of Corpora. EAGLES Re-
port EAG-TCWG-MAC/R. 
Leech, G. 1997. Grammatical Tagging. In Corpus Annota-
tion: Linguistic Information from Computer Text Cor-
pora, ed: Garside, Leech and McEnery, London: Long-
man  
Leech, G and Wilson, A. 1999. Standards for Tag-sets. In 
Syntactic Wordclass Tagging, ed. Hans van Halteren, 
Dordrecht: Kluwer Academic. 
Santorini, B. 1990. Part-of-speech tagging guidelines for 
the Penn Treebank Project. Technical report MS-CIS-
90-47, Department of Computer and Information 
Science, University of Pennsylvania 
IIIT-tagset. A Parts-of-Speech tagset for Indian languages. 
http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.
pdf 
Tamil tagset. AU-KBC Parts-of-Speech tagset for Tamil. 
http://nrcfosshelpline.in/smedia/images/downloads/Tam
il_Tagset-opensource.odt 
Aspect 
 Perfect 
 Imperfect 
 Progressive 
Mood 
 Declarative 
 Subjunctative/   
        Hortative 
 Conditional 
 Imperative 
 Presumptive 
Level - 3 
Nouns 
Verbs 
Pronouns 
Adjectives 
Adverbs 
Postpositions 
Demonstratives 
Quantifiers 
Particles 
Punctuations 
Residual Participles 
Level - 1 
Type 
 Finite 
 Auxiliary 
 Infinitive 
 Non-finite 
 Nominal 
Gender 
 Masculine 
 Feminine 
 Neuter 
Number 
 Singular 
 Plural/Hon. 
 Dual 
 Honourific 
Person 
 First 
 Second 
 Third 
Tense 
 Past 
 Present 
 Future 
Negative 
Type 
 General 
 Adjectival 
 Verbal 
 Nominal 
Gender 
 As in verbs 
Number 
 Singular 
 Plural 
 Dual 
Case 
 Direct 
 Oblique 
Case-markers 
 Ergative 
 Accusative 
 etc. 
Tense 
 As in verbs 
Negative 
Level - 2 
Fig-1. Tagset framework - partial representation 
The 6th Workshop on Asian Languae Resources, 2008
92
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 779?786,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Morphological Richness Offsets Resource Demand- Experiences in
Constructing a POS Tagger for Hindi
Smriti Singh Kuhoo Gupta
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
Powai, Mumbai
400076 Maharashtra, India
{smriti,kuhoo,manshri,pb}@cse.iitb.ac.in
Manish Shrivastava Pushpak Bhattacharyya
Abstract
In this paper we report our work on
building a POS tagger for a morpholog-
ically rich language- Hindi. The theme
of the research is to vindicate the stand
that- if morphology is strong and har-
nessable, then lack of training corpora is
not debilitating. We establish a method-
ology of POS tagging which the re-
source disadvantaged (lacking annotated
corpora) languages can make use of. The
methodology makes use of locally an-
notated modestly-sized corpora (15,562
words), exhaustive morpohological anal-
ysis backed by high-coverage lexicon
and a decision tree based learning algo-
rithm (CN2). The evaluation of the sys-
tem was done with 4-fold cross valida-
tion of the corpora in the news domain
(www.bbc.co.uk/hindi). The current ac-
curacy of POS tagging is 93.45% and can
be further improved.
1 Motivation and Problem Definition
Part-Of-Speech (POS) tagging is a complex
task fraught with challenges like ambiguity of
parts of speech and handling of ?lexical ab-
sence? (proper nouns, foreign words, deriva-
tionally morphed words, spelling variations and
other unknown words) (Manning and Schutze,
2002). For English there are many POS tag-
gers, employing machine learning techniques
like transformation-based error-driven learning
(Brill, 1995), decision trees (Black et al, 1992),
markov model (Cutting et al 1992), maxi-
mum entropy methods (Ratnaparkhi, 1996) etc.
There are also taggers which are hybrid using
both stochastic and rule-based approaches, such
as CLAWS (Garside and Smith, 1997). The
accuracy of these taggers ranges from 93-98%
approximately. English has annotated corpora
in abundance, enabling usage of powerful data
driven machine learning methods. But, very few
languages in the world have the resource advan-
tage that English enjoys.
In this scenario, POS tagging of highly in-
flectional languages presents an interesting case
study. Morphologically rich languages are char-
acterized by a large number of morphemes in
a single word, where morpheme boundaries are
difficult to detect because they are fused to-
gether. They are typically free-word ordered,
which causes fixed-context systems to be hardly
adequate for statistical approaches (Samuelsson
and Voutilainen, 1997). Morphology-based POS
tagging of some languages like Turkish (Oflazer
and Kuruoz, 1994), Arabic (Guiassa, 2006),
Czech (Hajic et al, 2001), Modern Greek (Or-
phanos et al, 1999) and Hungarian (Megyesi,
1999) has been tried out using a combination of
hand-crafted rules and statistical learning. These
systems use large amount of corpora along with
morphological analysis to POS tag the texts. It
may be noted that a purely rule-based or a purely
stochastic approach will not be effective for such
779
languages, since the former demands subtle lin-
guistic expertise and the latter variously per-
muted corpora.
1.1 Previous Work on Hindi POS Tagging
There is some amount of work done on
morphology-based disambiguation in Hindi POS
tagging. Bharati et al (1995) in their work
on computational Paninian parser, describe a
technique where POS tagging is implicit and is
merged with the parsing phase. Ray et al (2003)
proposed an algorithm that identifies Hindi word
groups on the basis of the lexical tags of the indi-
vidual words. Their partial POS tagger (as they
call it) reduces the number of possible tags for a
given sentence by imposing some constraints on
the sequence of lexical categories that are pos-
sible in a Hindi sentence. UPENN also has an
online Hindi morphological tagger1 but there ex-
ists no literature discussing the performance of
the tagger.
1.2 Our Approach
We present in this paper a POS tagger for
Hindi- the national language of India, spoken
by 500 million people and ranking 4th in the
world. We establish a methodology of POS tag-
ging which the resource disadvantaged (lack-
ing annotated corpora) languages can make
use of. This methodology uses locally anno-
tated modestly sized corpora (15,562 words), ex-
haustive morphological analysis backed by high-
coverage lexicon and a decision tree based learn-
ing algorithm- CN2 (Clark and Niblett, 1989).
To the best of our knowledge, such an approach
has never been tried out for Hindi. The heart of
the system is the detailed linguistic analysis of
morphosyntactic phenomena, adroit handling of
suffixes, accurate verb group identification and
learning of disambiguation rules.
The approach can be used for other inflec-
tional languages by providing the language spe-
cific resources in the form of suffix replacement
rules (SRRs), lexicon, group identification and
morpheme analysis rules etc. and keeping the
1http://ccat.sas.upenn.edu/plc/tamilweb/hindi.html
processes the same as shown in Figure 1. The
similar kind of work exploiting morphological
information to assign POS tags is under progress
for Marathi which is also an Indian language.
In what follows, we discuss in section 2 the
challenges in Hindi POS tagging followed by
a section on morphological structure of Hindi.
Section 4 presents the design of Hindi POS tag-
ger. The experimental setup and results are given
in sections 5 and 6. Section 7 concludes the pa-
per.
2 Challenges of POS Tagging in Hindi
The inter-POS ambiguity surfaces when a word
or a morpheme displays an ambiguity across
POS categories. Such a word has multiple en-
tries in the lexicon (one for each category). After
stemming, the word would be assigned all pos-
sible POS tags based on the number of entries it
has in the lexicon. The complexity of the task
can be understood looking at the following En-
glish sentence where the word ?back? falls into
three different POS categories-
?I get back to the back seat to give rest to my
back.?
The complexity further increases when it
comes to tagging a free-word order language like
Hindi where almost all the permutations of words
in a clause are possible (Shrivastava et al, 2005).
This phenomenon in the language, makes the
task of a stochastic tagger difficult.
Intra-POS ambiguity arises when a word has
one POS with different feature values, e.g., the
word ?   ? {laDke} (boys/boy) in Hindi is a
noun but can be analyzed in two ways in terms
of its feature values:
1. POS: Noun, Number: Sg, Case: Oblique
 	
   Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 800?808,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Case markers and Morphology: Addressing the crux of the fluency
problem in English-Hindi SMT
Ananthakrishnan Ramanathan, Hansraj Choudhary
Avishek Ghosh, Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology Bombay
Powai, Mumbai-400076
India
{anand, hansraj, avis, pb}@cse.iitb.ac.in
Abstract
We report in this paper our work on
accurately generating case markers and
suffixes in English-to-Hindi SMT. Hindi
is a relatively free word-order language,
and makes use of a comparatively richer
set of case markers and morphological
suffixes for correct meaning representa-
tion. From our experience of large-scale
English-Hindi MT, we are convinced that
fluency and fidelity in the Hindi output get
an order of magnitude facelift if accurate
case markers and suffixes are produced.
Now, the moot question is: what entity on
the English side encodes the information
contained in case markers and suffixes on
the Hindi side? Our studies of correspon-
dences in the two languages show that case
markers and suffixes in Hindi are predom-
inantly determined by the combination of
suffixes and semantic relations on the En-
glish side. We, therefore, augment the
aligned corpus of the two languages, with
the correspondence of English suffixes and
semantic relations with Hindi suffixes and
case markers. Our results on 400 test
sentences, translated using an SMT sys-
tem trained on around 13000 parallel sen-
tences, show that suffix + semantic rela-
tion? case marker/suffix is a very useful
translation factor, in the sense of making a
significant difference to output quality as
indicated by subjective evaluation as well
as BLEU scores.
1 Introduction
Two fundamental problems in applying statistical
machine translation (SMT) techniques to English-
Hindi (and generally to Indian language) MT are:
i) the wide syntactic divergence between the lan-
guage pairs, and ii) the richer morphology and
case marking of Hindi compared to English. The
first problem manifests itself in poor word-order in
the output translations, while the second one leads
to incorrect inflections (word-endings) and case
marking. Being a free word-order language, Hindi
suffers badly when morphology and case markers
are incorrect.
To solve the former, word-order related, prob-
lem, we use a preprocessing technique, which we
have discussed in (Ananthakrishnan et al, 2008).
This procedure is similar to what is suggested in
(Collins et al, 2005) and (Wang, 2007), and re-
sults in the input sentence being reordered to fol-
low Hindi structure.
The focus of this paper, however, is on the
thorny problem of generating case markers and
morphology. It is recognized that translating from
poor to rich morphology is a challenge (Avramidis
and Koehn, 2008) that calls for deeper linguistic
analysis to be part of the translation process. Such
analysis is facilitated by factored models (Koehn
et al, 2007), which provide a framework for incor-
porating lemmas, suffixes, POS tags, and any other
linguistic factors in a log-linear model for phrase-
based SMT. In this paper, we motivate a factoriza-
tion well-suited to English-Hindi translation. The
factorization uses semantic relations and suffixes
to generate inflections and case markers. Our ex-
periments include two different kinds of semantic
relations, namely, dependency relations provided
by the Stanford parser, and the deeper semantic
roles (agent, patient, etc.) provided by the univer-
sal networking language (UNL). Our experiments
show that the use of semantic relations and syntac-
tic reordering leads to substantially better quality
translation. The use of even moderately accurate
semantic relations has an especially salubrious ef-
fect on fluency.
800
2 Related Work
There have been quite a few attempts at includ-
ing morphological information within statistical
MT. Nie?en and Ney (2004) show that the use of
morpho-syntactic information drastically reduces
the need for bilingual training data. Popovic and
Ney (2006) report the use of morphological and
syntactic restructuring information for Spanish-
English and Serbian-English translation.
Koehn and Hoang (2007) propose factored
translation models that combine feature functions
to handle syntactic, morphological, and other lin-
guistic information in a log-linear model. This
work also describes experiments in translating
from English to German, Spanish, and Czech, in-
cluding the use of morphological factors.
Avramidis and Koehn (2008) report work on
translating from poor to rich morphology, namely,
English to Greek and Czech translation. They use
factored models with case and verb conjugation
related factors determined by heuristics on parse
trees. The factors are used only on the source side,
and not on the target side.
To handle syntactic differences,
Melamed (2004) proposes methods based on
tree-to-tree mappings. Imamura et al (2005)
present a similar method that achieves significant
improvements over a phrase-based baseline model
for Japanese-English translation.
Another method for handling syntactic differ-
ences is preprocessing, which is especially perti-
nent when the target language does not have pars-
ing tools. These algorithms attempt to recon-
cile the word-order differences between the source
and target language sentences by reordering the
source language data prior to the SMT training
and decoding cycles. Nie?en and Ney (2004) pro-
pose some restructuring steps for German-English
SMT. Popovic and Ney (2006) report the use
of simple local transformation rules for Spanish-
English and Serbian-English translation. Collins
et al (2005) propose German clause restructur-
ing to improve German-English SMT, while Wang
et al (2007) present similar work for Chinese-
English SMT. Our earlier work (Ananthakrishnan
et al, 2008) describes syntactic reordering and
morphological suffix separation for English-Hindi
SMT.
3 Motivation
The fundamental differences between English and
Hindi are:
? English follows SVO order, whereas Hindi
follows SOV order
? English uses post-modifiers, whereas Hindi
uses pre-modifiers
? Hindi allows greater freedom in word-order,
identifying constituents through case mark-
ing
? Hindi has a relatively richer system of mor-
phology
We resolve the first two syntactic differences
by reordering the English sentence to conform to
Hindi word-order in a preprocessing step as de-
scribed in (Ananthakrishnan et al, 2008).
The focus of this paper, however, is on the last
two of these differences, and here we dwell a bit
on why this focus on case markers and morphol-
ogy is crucial to the quality of translation.
3.1 Case markers
While in English, the major constituents of a sen-
tence (subject, object, etc.) can usually be iden-
tified by their position in the sentence, Hindi is a
relatively free word-order language. Constituents
can be moved around in the sentence without im-
pacting the core meaning. For example, the fol-
lowing sentence pair conveys the same meaning
(John saw Mary), albeit with different emphases.
jAn n mrF ko dKA
John ne Mary ko dekhaa
John-nom Mary-acc saw
mrF ko jAn n dKA
Mary ko John ne dekhaa
Mary-acc John-nom saw
The identity of John as the subject and Mary
as the object in both sentences comes from the
case markers n (ne ? nominative) and ko (ko ?
accusative). Therefore, even though Hindi is pre-
dominantly SOV in its word-order, correct case
marking is a crucial part of making translations
convey the right meaning.
801
3.2 Morphology
The following examples illustrate the richer mor-
phology of Hindi compared to English:
Oblique case: The plural-marker in the word
?boys? in English is translated as e (e ? plural di-
rect) or ao\ (on ? plural oblique):
The boys went to school.
lXk pAWfAlA gy
ladake paathashaalaa gaye
The boys ate apples.
lXko\ n sb KAy
ladokon ne seba khaaye
Future tense: Future tense in Hindi is marked
on the verb. In the following example, ?will go? is
translated as jAy\g (jaaenge), with e\g (enge) as
the future tense marker:
The boys will go to school.
lXk pAWfAlA jAy\g
ladake paathashaalaa jayenge
Causative constructions: The aAyA (aayaa)
suffix indicates causativity:
The boys made them cry.
lXko\ n uh zlAyA
ladakon ne unhe rulaayaa
3.3 Sparsity
Using a standard SMT system for English-Hindi
translation will cause severe data sparsity with re-
spect to case marking and morphology.
For example, the fact that the word boys in
oblique case (say, when followed by n (ne))
should take the form lXko\ (ladakon) will be
learnt only if the correspondence between boys
and lXko\ n (ladakon ne) exists in the training
corpus. The more general rule that n (ne) should
be preceded by the oblique case ending ao\ (on)
cannot be learnt. Similarly, the plural form of boys
will be produced only if that form exists in the
training corpus.
Essentially, all morphological forms of a word
and its translations have to exist in the training cor-
pus, and every word has to appear with every pos-
sible case marker, which will require an impossi-
ble amount of training data. Therefore, it is im-
perative to make it possible for the system to learn
general rules for morphology and case marking.
The next section describes our approach to facili-
tating the learning of such rules.
4 Approach
While translating from a language of moderate
case marking and morphology (English) to one
with relatively richer case marking and morphol-
ogy (Hindi), we are faced with the problem of ex-
tracting information from the source language sen-
tence, transferring the information onto the target
side, and translating this information into the ap-
propriate case markers and morphological affixes.
The key bits of information for us are suffixes
and semantic relations, and the vehicle that trans-
fers and translates the information is the factored
model for phrase based SMT (Koehn 2007).
4.1 Factored Model
Factored models allow the translation to be broken
down into various components, which are com-
bined using a log-linear model:
p(e|f) =
1
Z
exp
n?
i=1
?ihi(e, f) (1)
Each hi is a feature function for a component of
the translation (such as the language model), and
the ? values are weights for the feature functions.
4.2 Our Factorization
Our factorization, which is illustrated in figure 1,
consists of:
1. a lemma to lemma translation factor (boy?
lXk^ (ladak))
2. a suffix + semantic relation to suffix/case
marker factor (-s + subj? e (e))
3. a lemma + suffix to surface form genera-
tion factor (lXk^ + e (ladak + e) ? lXk
(ladake))
The above factorization is motivated by the fol-
lowing:
? Case markers are decided by semantic re-
lations and tense-aspect information in suf-
fixes.
For example, if a clause has an object, and
has a perfective form, the subject usually re-
quires the case marker n (ne).
John ate an apple.
John|empty|subj eat|ed|empty an|empty|det
apple|empty|obj
802
Figure 1: Semantic and Suffix Factors: the combination of English suffixes and semantic relations is
aligned with Hindi suffixes and case markers
jAn n sb KAyA
john ne seba khaayaa
Thus, the combination of the suffix and
semantic relation generates the right case
marker (ed|empty + empty|obj? n (ne)).
? Target language suffixes are largely deter-
mined by source language suffixes and case
markers (which in turn are determined by the
semantic relations)
The boys ate apples.
The|empty|det boy|s|subj eat|ed|empty
apple|s|obj
lXko\ n sb KAy
ladakon ne seba khaaye
Here, the plural suffix on boys leads to two
possibilities ? lXk (ladake ? plural direct)
and lXko\ (ladakon ? plural oblique). The
case marker n (ne) requires the oblique case.
? Our factorization provides the system with
two sources to determine the case markers
and suffixes. While the translation steps dis-
cussed above are one source, the language
model over the suffix/case marker factor re-
inforces the decisions made.
For example, the combination lXkA n
(ladakaa ne) is impossible, while lXko\ n
(ladakon ne) is very likely. The separation of
the lemma and suffix helps in tiding over the
data sparsity problem by allowing the system
to reason about the suffix-case marker com-
bination rather than the combination of the
specific word and the case marker.
5 Semantic Relations
The experiments have been conducted with two
kinds of semantic relations. One of them is the re-
lations from the Universal Networking Language
(UNL), and the other is the grammatical relations
produced by the Stanford parser.
The relations in both UNL and the Stanford de-
pendency parser are strictly binary and form a di-
rected graph. These relations express the semantic
dependencies among the various words in the sen-
tence.
Stanford: The Stanford dependency
parser (Marie-Catherine and Manning, 2008)
uses 55 relations to express the dependencies
among the various words in a sentence. These
relations form a hierarchical structure with the
most general relation at the root. There are
various argument relations like subject, object,
objects of prepositions, and clausal complements,
modifier relations like adjectival, adverbial,
participial, and infinitival modifiers, and other
relations like coordination, conjunct, expletive,
and punctuation.
UNL: The 44 UNL relations1 include relations
such as agent, object, co-agent, and partner, tem-
poral relations, locative relations, conjunctive and
disjunctive relations, comparative relations and
also hierarchical relationships like part-of and an-
instance-of.
Comparison: Unlike the Stanford parser which
expresses the semantic relationships through
grammatical relations, UNL uses attributes and
universal words, in addition to the semantic roles,
to express the same. Universal words are used to
disambiguate words, while attributes are used to
express the speaker?s point of view in the sentence.
UNL relations, compared to the relations in the
Stanford parser, are more semantic than grammat-
ical. For instance, in the Stanford parser, the agent
relation is the complement of a passive verb intro-
duced by the preposition by, whereas in UNL it
1http://www.undl.org/unlsys/unl/unl2005/
803
Figure 2: UNL and Stanford semantic relation graphs for the sentence ?John said that he was hit
by Jack?
#sentences #words
Training 12868 316508
Tuning 600 15279
Test 400 8557
Table 1: Corpus Statistics
signifies the doer of an action. Consider the fol-
lowing sentence:
John said that he was hit by Jack.
In this sentence, the Stanford parser produces
the relation agent(hit, Jack) and nsubj(said, John)
as shown in figure 2. In UNL, however, both the
cases use the agent relation. The other distinguish-
ing aspect of UNL is the hyper-node that repre-
sents scope. In the example sentence, the whole
clause ?that he was hit by Jack? forms the ob-
ject of the verb said, and hence is represented in
a scope. The Stanford dependency parser on the
other hand represents these dependencies with the
help of the clausal complement relation, which
links said with hit, and uses the complementizer
relation to introduce the subordinating conjunc-
tion.
The pre-dependency accuracy of the Stan-
ford dependency parser is around 80% (Marie-
Catherine et al, 2006), while the accuracy
achieved by the UNL generating system is
64.89%.
6 Experiments
6.1 Setup
The corpus described in table 1 was used for the
experiments.
The SRILM toolkit 2 was used to create Hindi
language models using the target side of the train-
ing corpus.
Training, tuning, and decoding were performed
using the Moses toolkit 3. Tuning (learning the
? values discussed in section 4.1) was done using
minimum error rate training (Och, 2003).
The Stanford parser 4 was used for parsing the
English text for syntactic reordering and to gener-
ate ?stanford? semantic relations.
The program for syntactic reordering used the
parse trees generated by the Stanford parser,
and was written in perl using the module
Parse::RecDescent.
English morphological analysis was performed
using morpha (Minnen et al, 2001), while Hindi
suffix separation was done using the stemmer de-
scribed in (Ananthakrishnan and Rao, 2003).
Syntactic and morphological transformations,
in the models where they were employed, were ap-
plied at every phase: training, tuning, and testing.
Evaluation Criteria: Automatic evaluation
was performed using BLEU and NIST on the en-
tire test set of 400 sentences. Subjective evaluation
was performed on 125 sentences from the test set.
? BLEU (Papineni et al, 2001): measures the
precision of n-grams with respect to the ref-
erence translations, with a brevity penalty. A
higher BLEU score indicates better transla-
tion.
? NIST 5: measures the precision of n-grams.
This metric is a variant of BLEU, which was
2http://www.speech.sri.com/projects/srilm/
3http://www.statmt.org/moses/
4http://nlp.stanford.edu/software/lex-parser.shtml
5www.nist.gov/speech/tests/mt/doc/ngram-study.pdf
804
shown to correlate better with human judg-
ments. Again, a higher score indicates better
translation.
? Subjective: Human evaluators judged the
fluency and adequacy, and counted the num-
ber of errors in case markers and morphology.
6.2 Results
Table 2 shows the impact of suffix and semantic
factors. The models experimented with are de-
scribed below:
baseline: The default settings of Moses were
used for this model.
lemma + suffix: This uses the lemma and suf-
fix factors on the source side, and the lemma and
suffix/case marker on the target side. The trans-
lation steps are i) lemma to lemma and ii) suffix
to suffix/case marker, and the generation step is
lemma+suffix/case marker to surface form.
lemma + suffix + unl: This model uses, in ad-
dition to the factors in the lemma+suffix model,
a semantic relation factor (UNL relations). The
translation steps are i) lemma to lemma and ii)
suffix+semantic relation to suffix/case marker, and
the generation step again is lemma+suffix/case
marker to surface form.
lemma + suffix + stanford: This is identical
to the previous model, except that stanford depen-
dency relations are used instead of UNL relations.
We can see a substantial improvement in scores
when semantic relations are used.
Table 5 shows the impact of syntactic reorder-
ing. The surface form with distortion-based, lex-
icalized, and syntactic reordering were experi-
mented with. The model with the suffix and se-
mantic factors was used with syntactic reordering.
For subjective evaluation, sentences were
judged on fluency, adequacy and the number of er-
rors in case marking/morphology.
To judge fluency, the judges were asked to look
at how well-formed the output sentence is accord-
ing to Hindi grammar, without considering what
the translation is supposed to convey. The five-
point scale in table 3 was used for evaluation.
To judge adequacy, the judges were asked to
compare each output sentence to the reference
translation and judge how well the meaning con-
veyed by the reference was also conveyed by the
output sentence. The five-point scale in table 4
was used.
Table 6 shows the average fluency and adequacy
scores, and the average number of errors per sen-
tence.
All differences are significant at the 99%
level, except the difference in adequacy be-
tween the surface-syntactic model and the
lemma+suffix+stanford syntactic model, which is
significant at the 95% level.
7 Discussion
We can see from the results that better fluency and
adequacy are achieved with the use of semantic re-
lations. The improvement in fluency is especially
noteworthy. Figure 3 shows the distribution of flu-
ency and adequacy scores. What is worth noting
is that the number of sentences at levels 4 and 5
in terms of fluency and adequacy are much higher
in case of the model that uses semantic relations.
That is, the use of semantic relations, in combi-
nation with syntactic reordering, produces many
more sentences that are reasonably or even per-
fectly fluent and convey most or all of the mean-
ing.
Table 7 shows the impact of sentence length on
translation quality. We can see that with smaller
sentences the improvements using syntactic re-
ordering and semantic relations are much more
pronounced. All models find long sentences dif-
ficult to handle, which contributes to bringing the
mean performances closer. However, it is clear
that many more useful translations are being pro-
duced due to syntactic reordering and semantic re-
lations.
The following is an example of the kind of im-
provements achieved:
Input: Inland waterway is one of the most pop-
ular picnic spots in Alappuzha.
Baseline: m\ ek ata,-TlFy jlmAg k
sbs ?Es? EpkEnk -Tl m\ jlo\ m\ dOXtaF
h{
men eka antahsthaliiya jalamaarga ke sabase
prasiddha pikanika sthala men jalon men daudatii
hai
gloss: in a waterway of most popular picnic spot
in waters runs.
Reorder: ata,-TlFy jlmAg aAlp  yA k
sbs ?Es? EpkEnk -Tl m\ s ek h{
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthala men se eka hai
805
Model BLEU NIST
Baseline (surface) 24.32 5.85
lemma + suffix 25.16 5.87
lemma + suffix + unl 27.79 6.05
lemma + suffix + stanford 28.21 5.99
Table 2: Results: The impact of suffix and semantic factors
Level Interpretation
5 Flawless Hindi, with no grammatical errors whatsoever
4 Good Hindi, with a few minor errors in morphology
3 Non-native Hindi, with possibly a few minor grammatical errors
2 Disfluent Hindi, with most phrases correct, but ungrammatical overall
1 Incomprehensible
Table 3: Subjective Evaluation: Fluency Scale
Level Interpretation
5 All meaning is conveyed
4 Most of the meaning is conveyed
3 Much of the meaning is conveyed
2 Little meaning is conveyed
1 None of the meaning is conveyed
Table 4: Subjective Evaluation: Adequacy Scale
Model Reordering BLEU NIST
surface distortion 24.42 5.85
surface lexicalized 28.75 6.19
surface syntactic 31.57 6.40
lemma + suffix + stanford syntactic 31.49 6.34
Table 5: Results: The impact of reordering and semantic relations
Model Reordering Fluency Adequacy #errors
surface lexicalized 2.14 2.26 2.16
surface syntactic 2.6 2.71 1.79
lemma + suffix + stanford syntactic 2.88 2.82 1.44
Table 6: Subjective Evaluation: The impact of reordering and semantic relations
Baseline Reorder Stanford
F A E F A E F A E
Small (<19 words) 2.63 2.84 1.30 3.30 3.52 0.74 3.66 3.75 0.62
Medium (20-34 words) 1.92 2.00 2.23 2.32 2.43 2.05 2.62 2.46 1.74
Large (>34 words) 1.62 1.69 4.00 1.86 1.73 3.36 1.86 1.86 2.82
Table 7: Impact of sentence length (F: Fluency; A:Adequacy; E:# Errors)
806
Figure 3: Subjective evaluation: analysis
gloss: waterway Alappuzha of most popular
picnic spot of one is
Semantic: ata,-TlFy jlmAg aAlp  yA k
sbs ?Es? EpkEnk -Tlo\ m\ s ek h{
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthalon men se eka hai
gloss: waterway Alappuzha of most popular
picnic spots of one is
We can see that poor word-order makes the
baseline output almost incomprehensible, while
syntactic reordering solves the problem correctly.
The morphology improvement using semantic
relations can be seen in the correct inflection
achieved in the word -Tlo\ (sthalon ? plural
oblique ? spots), whereas the output without using
semantic relations generates -Tl (sthala ? singu-
lar ? spot).
The next couple of examples illustrate how case
marking improves through the use of semantic re-
lations.
Input: Gandhi Darshan and Gandhi National
Museum is across Rajghat.
Reorder: gA\DF dfn v gA\DF rA?~ Fy s\g}hAly
rAjGAV m\ h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata men hai
Semantic: gA\DF dfn v gA\DF rA?~ Fy
s\g}hAly rAjGAV k pAr h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata ke paara hai
Here, the use of semantic relations produces the
correct meaning that the locations mentioned are
across (k pAr (ke paara)) Rajghat, and not in (m\
(men)) Rajghat as suggested by the translation pro-
duced without using semantic relations.
Another common error in case marking is that
two case markers are produced in successive po-
sitions in the translation, which is not possible in
Hindi. The following example (a fragment) shows
this error (kF (kii) repeated) being correctly han-
dled by using semantic relations:
Input: For varieties of migratory birds
Reorder: ?vAsF pE"yo\ kF kF ?kAr k Ely
pravaasii pakshiyon kii kii prakaara ke liye
Semantic: ?vAsF pE"yo\ kF ?kAr k Ely
pravaasii pakshiyon kii prakaara ke liye
It is important to note that the gains made us-
ing syntactic reordering and semantic relations are
limited by the accuracy of the parsers (see section
5). We observe that even the use of moderate qual-
ity semantic relations goes a long way in increas-
ing the quality of translation.
8 Conclusion
We have reported in this paper the marked im-
provement in the output quality of Hindi transla-
tions ? especially fluency ? when the correspon-
dence of English semantic relations and suffixes
with Hindi case markers and inflections is used as
a translation factor in English-Hindi SMT. The im-
provement is statistically significant. Subjective
evaluation too lends ample credence to this claim.
Future work consists of investigations into (i) how
the internal structure of constituents can be strictly
preserved and (ii) how to glue together correctly
the syntactically well-formed bits and pieces of
the sentences. This course of future action is sug-
gested by the fact that smaller sentences are much
more fluent in translation compared to medium
length and long sentences.
807
References
Ananthakrishnan, R., and Rao, D., A Lightweight
Stemmer for Hindi, Workshop on Com-
putational Linguistics for South-Asian Lan-
guages, EACL, 2003.
Ananthakrishnan, R., Bhattacharyya, P., Hegde, J.
J., Shah, R. M., and Sasikumar, M., Sim-
ple Syntactic and Morphological Processing
Can Help English-Hindi Statistical Machine
Translation, Proceedings of IJCNLP, 2008.
Avramidis, E., and Koehn, P., Enriching Morpho-
logically Poor Languages for Statistical Ma-
chine Translation, Proceedings of ACL-08:
HLT, 2008.
Collins, M., Koehn, P., and I. Kucerova, Clause
Restructuring for Statistical Machine Trans-
lation, Proceedings of ACL, 2005.
Imamura, K., Okuma, H., Sumita, E., Prac-
tical Approach to Syntax-based Statistical
Machine Translation, Proceedings of MT-
SUMMIT X, 2005.
Koehn, P., and Hoang, H., Factored Translation
Models, Proceedings of EMNLP, 2007.
Marie-Catherine de Marneffe, MacCartney, B.,
and Manning, C., Generating Typed Depen-
dency Parses from Phrase Structure Parses,
Proceedings of LREC, 2006.
Marie-Catherine de Marneffe and Manning, C.,
Stanford Typed Dependency Manual, 2008.
Melamed, D., Statistical Machine Translation by
Parsing, Proceedings of ACL, 2004.
Minnen, G., Carroll, J., and Pearce, D., Applied
Morphological Processing of English, Natu-
ral Language Engineering, 7(3), pages 207?
223, 2001.
Nie?en, S., and Ney, H., Statistical Machine
Translation with Scarce Resources Using
Morpho-syntactic Information, Computa-
tional Linguistics, 30(2), pages 181?204,
2004.
Och, F., Minimum Error Rate Training in Sta-
tistical Machine Translation, Proceedings of
ACL, 2003.
Papineni, K., Roukos, S., Ward, T., and Zhu,
W., BLEU: a Method for Automatic Evalu-
ation of Machine Translation, IBM Research
Report, Thomas J. Watson Research Center,
2001.
Popovic, M., and Ney, H., Statistical Machine
Translation with a Small Amount of Bilin-
gual Training Data, 5th LREC SALTMIL
Workshop on Minority Languages, 2006.
Wang, C., Collins, M., and Koehn, P., Chinese
Syntactic Reordering for Statistical Machine
Translation, Proceedings of the EMNLP-
CoNLL, 2007.
808
Question answering via Bayesian inference on lexical relations
Ganesh Ramakrishnan, Apurva Jadhav, Ashutosh Joshi, Soumen Chakrabarti, Pushpak Bhattacharyya
 
hare,apurvaj,ashuj,soumen,pb  @cse.iitb.ac.in
Dept. of Computer Science and Engg.,
Indian Institute of Technology, Mumbai, India
Abstract
Many researchers have used lexical networks
and ontologies to mitigate synonymy and polysemy
problems in Question Answering (QA), systems
coupled with taggers, query classifiers, and answer
extractors in complex and ad-hoc ways. We seek
to make QA systems reproducible with shared and
modest human effort, carefully separating knowl-
edge from algorithms. To this end, we propose
an aesthetically ?clean? Bayesian inference scheme
for exploiting lexical relations for passage-scoring
for QA . The factors which contribute to the effi-
cacy of Bayesian Inferencing on lexical relations are
soft word sense disambiguation, parameter smooth-
ing which ameliorates the data sparsity problem and
estimation of joint probability over words which
overcomes the deficiency of naive-bayes-like ap-
proaches. Our system is superior to vector-space
ranking techniques from IR, and its accuracy ap-
proaches that of the top contenders at the TREC QA
tasks in recent years.
1 Introduction
This paper describes an approach to probabilistic in-
ference using lexical relations, such as expressed by
a WordNet, an ontology, or a combination, with ap-
plications to passage-scoring for open-domain ques-
tion answering (QA).
The use of lexical resources in Information Re-
trieval (IR) is not new; for almost a decade, the
IR community has considered the use of natural
language processing techniques (Lewis and Jones,
1996) to circumvent synonymy, polysemy, and other
barriers to purely string-matching search engines. In
particular, a number of researchers have attempted
to use the English WordNet to ?bridge the gap? be-
tween query and response. Interestingly, the results
have mostly been inconclusive or negative (Fell-
baum, 1998a). A number of explanations have been
offered for this lack of success, some of which are
 presence of unnecessary links and absence of
necessary links in the WordNet (Fellbaum,
1998b),
 hurdle of Word Sense Disambiguation (WSD)
(Sanderson, 1994)
 ad-hocness in the distance and scoring func-
tions (Abe et al, 1996).
1.1 Question answering (QA)
Unlike IR systems which return a list of documents
in response to a query, from which the user must
extract the answer manually, the goal of QA is to
extract from the corpus direct answers to questions
posed in a natural language.
An important step before answer extraction is
to identify and rate candidate passages from docu-
ments which might contain the answer. The notion
of a passage is somewhat arbitrary: various notions
of a passage have emerged (Vorhees, 2000); For our
purposes, a passage comprises  consecutive sen-
tences, or  consecutive words.
In contrast to IR, where linguistic resources have
not been found very useful, QA has always de-
pended on a mixture of stock lexical networks and
custom ontologies (language-independent concep-
tual hierarchies) crafted through human understand-
ing of the task at hand (Harabagiu et al, 2000;
Clarke et al, 2001). Ontologies, hand-crafted and
customized, sometimes from the WordNet itself, are
employed for question type classification, relation-
ships between places, measures, etc.
The scoring (and thereby, ranking) of passages
through lexical networks or ontologies is more suc-
cessful in QA than in classic IR because of the na-
ture of the QA task. Passage-scoring in QA benefits
from indirect matches through an ontology.
By separating the passage-scoring algorithm from
the knowledge base, we can keep improving our sys-
tem by continually upgrading the lexical relations in
the knowledge base and retraining our inference al-
gorithm.
Map:  2 describes the related work.  3 gives the
motivation behind our approach and the background
information (WordNet and Bayesian inferencing).
 4 describes our QA system. Results are presented
in  5, and concluding remarks made in  6.
1
2 Related work
Information Retrieval (IR) systems such as
SMART (Buckley, 1985) rank documents for
relevance w.r.t. to a user query, based on keyword
match between the query and a document, each rep-
resented in the well-known ?vector space model?.
The degree of match is measured as the cosine of
the angle between query and document vectors.
In QA, an IR subsystem is typically used to short-
list passages which are likely to embed the answer.
Usually, several enhancements are made to stock IR
systems to meet this task.
First, the cosine measure used in stock vector-
space systems will be biased against long docu-
ments even if they embed the answer in a narrow
zone. This problem can be ameliorated by repre-
senting suitably-sized passage windows (rather than
whole documents) as vectors. While scoring pas-
sages using the cosine measure, we can also ignore
passage terms which do not occur in the query.
The second issue is one of proximity. A passage
is likely to be promising if query words occur close
to one another. Commercial search engines reward
proximity of matched query terms, but in undocu-
mented ways. Clarke et al (Clarke et al, 2001) ex-
ploit term proximity within documents for passage
scoring.
The third and most important limitation of stock
IR systems is the inability to bridge the lexical
chasm between question and potential answer via
lexical networks. One query from TREC (Vorhees,
2000) asks, ?Who painted Olympia?? The answer
is in the passage: ?Manet, who, after all, created
Olympia, gets no credit.?
QA systems use a gamut of techniques to deal
with this problem. FALCON (Harabagiu et al,
2000) (one of the best QA systems in recent TREC
competitions) integrates syntactic, semantic and
pragmatic knowledge for QA. It uses WordNet-
based query expansion to try to bridge the lexical
chasm. WordNet is customized into a answer-type
taxonomy to infer the expected answer type for a
question. Named-entity recognition techniques are
also employed to improve quality of passages re-
trieved. The answers are finally filtered by justifying
them using abductive reasoning. Mulder (Kwok et
al., 2001) uses a similar approach to perform QA on
Web scale. The well-known START system (Katz, )
goes even further in this direction.
Discussion: In general, the TREC QA systems di-
vide QA into two tasks: identifying relevant doc-
uments and extracting answer passages from them.
For the former task, most systems use traditional IR
engines coupled with ad-hoc query expansion based
on WordNet. Handcrafted knowledge bases, ques-
tion/answer type classifiers and a variety of heuris-
tics are used for the latter task. Success in QA
comes at the cost of great effort in custom-designed
wordnets and ontologies, and expansion, matching
and scoring heuristics which need to be upgraded
as the knowledge bases are enhanced. Ideally, we
should use a knowledge base which can be readily
extended, and a core scoring algorithm which is ele-
gant and ?universal?.
3 Proposed approach
3.1 An inferencing approach to QA
Given a question and a passage that contains the an-
swer, how do we correlate the two ? Take for exam-
ple, the following question
What type of animal is Winnie the Pooh?
and the answer passage is
A Canadian town that claims to be the birthplace
of Winnie the Pooh wants to erect a giant statue of
the famous bear; but Walt Disney Studios will not
permit it.
It is clear that there is a linkage between the ques-
tion word animal and the answer word bear. That
the word bear occurred in the answer, in the context
of Winnie, means that there was a hidden ?cause?
for the occurrence of bear, and that was the concept
of  animal  .
In general, there could be multiple words in the
question and answer that are connected by many hid-
den causes. This scenario is depicted in figure  1.
The causes themselves may have hidden causes as-
sociated with them.
QUESTION ANSWER
NODESNODES
Hidden Causes that are switched on
Observed nodes(WORDS) 
Hidden Causes that are switched off(CONCEPTS)
(CONCEPTS)
Figure 1: Motivation
2
These causal relationships are represented in on-
tologies and WordNets. The familiar English Word-
Net, in particular, encodes relations between words
and concepts. For instance WordNet gives the hy-
pernymy relation between the concepts  animal 
and  bear  .
3.2 WordNet
WordNet (Fellbaum, 1998b) is an online lexical ref-
erence system in which English nouns, verbs, ad-
jectives and adverbs are organized into synonym
sets or synsets, each representing one underly-
ing lexical concept. Noun synsets are related to
each other through hypernymy (generalization), hy-
ponymy (specialization), holonymy (whole of) and
meronymy (part of) relations. Of these, (hypernymy,
hyponymy) and (meronymy,holonymy) are comple-
mentary pairs.
The verb and adjective synsets are very sparsely
connected with each other. No relation is available
between noun and verb synsets. However, 4500 ad-
jective synsets are related to noun synsets with per-
tainyms (pertaining to) and attra (attributed with) re-
lations.
DOG, DOMESTIC_DOG, CANIS_FAMILIARIS 
CORGI, WELSH_CORGIFLAG
meronymy
(from CANIS, GENUS_CANIS)
hyponymy
Figure 2: Illustration of WordNet relations.
Figure  2 shows that the synset  dog, domes-
tic dog, canis familiaris  has a hyponymy link to
 corgi, welshcorgi  and meronymy link to  flag 
(?a conspicuously marked or shaped tail?). While
the hyponymy link helps us answer the question
(TREC#371) ?A corgi is a kind of what??, the
meronymy connection here is perhaps more confus-
ing than useful: this sense of flag is rare.
3.3 Inferencing on lexical relations
It is surprisingly difficult to make the simple idea
of bridging passage to query through lexical net-
works perform well in practice. Continuing the ex-
ample of Winnie the bear (section  3.1), the En-
glish WordNet has five synsets on the path from bear
to animal:  carnivore...  ,  placental mammal...  ,
 mammal...  ,  vertebrate..  ,  chordate...  .
Some of these intervening synsets would be ex-
tremely unlikely to be associated with a corpus that
is not about zoology; a common person would more
naturally think of a bear as a kind of animal, skip-
ping through the intervening nodes.
It is, however, dangerous to design an algorithm
which is generally eager to skip across links in a lex-
ical network. E.g., few QA applications are expected
to need an expansion of ?bottle? beyond ?vessel?
and ?container? to ?instrumentality? and beyond.
Another example would be the shallow verb hierar-
chy in the English WordNet, with completely dis-
similar verbs within very few links of each other.
There is also the problem of missing links.
Another important issue is which ?hidden causes?
(synsets) should be inferred to have caused words
in the text. This is a classical problem called
word sense disambiguation (WSD). For instance,
the word dog belongs to 6 noun synsets in Word-
Net. Which of the  synsets should be treated as the
?hidden cause? that generated the word dog in the
passage could be inferred from the fact that collie is
related to dog only through one of the latter?s senses
- it?s sense as  dog, domestic dog, Canis familiaris  .
But this problem of finding the ?appropriate? hidden
causes, in general, in non-trivial. Given that state-of-
the-art WSD systems perform not better than 74%
(Sanderson, 1994) (Lewis and Jones, 1996) (Fell-
baum, 1998b), in this paper, we use a probabilistic
approach to WSD - called ?soft WSD? (Pushpak, )
; hidden nodes are considered to have probabilisti-
cally ?caused? words in the question and answer or in
other words, causes are probabilistically ?switched
on?.
Clearly, any scoring algorithm that seeks to uti-
lize WordNet link information must also discrimi-
nate between them based (at least) on usage statis-
tics of the connected synsets. Also required is an
estimate of the likelihood of instantiating a synset
into a token because it was ?activated? by a closely
related synset. We find a Bayesian belief network
(BBN) a natural structure to encode such combined
knowledge from WordNet and corpus.
3.4 Bayesian Belief Network
A Bayesian Network (Heckerman, 1995) for a set of
random variables 	
			A Gloss-centered Algorithm for Disambiguation
Ganesh Ramakrishnan
Dept. of C.S.E
IIT Bombay
India - 400076
B. Prithviraj
Dept. of C.S.E
IIT Bombay
India - 400076
 
hare,prithvir,pb  @cse.iitb.ac.in
Pushpak Bhattacharyya
Dept. of C.S.E
IIT Bombay
India - 400076
Abstract
The task of word sense disambiguation is to assign
a sense label to a word in a passage. We report our
algorithms and experiments for the two tasks that
we participated in viz. the task of WSD of Word-
Net glosses and the task of WSD of English lexical
sample. For both the tasks, we explore a method of
sense disambiguation through a process of ?compar-
ing? the current context for a word against a reposi-
tory of contextual clues or glosses for each sense of
each word. We compile these glosses in two differ-
ent ways for the two tasks. For the first task, these
glosses are all compiled using WordNet and are of
various types viz. hypernymy glosses, holonymy
mixture, descriptive glosses and some hybrid mix-
tures of these glosses. The ?comparison? could be
done in a variety of ways that could include/exclude
stemming, expansion of one gloss type with another
gloss type, etc. The results show that the system
does best when stemming is used and glosses are
expanded. However, it appears that the evidence for
word-senses ,accumulated through WordNet, in the
form of glosses, are quite sparse. Generating dense
glosses for all WordNet senses requires a massive
sense tagged corpus - which is currently unavail-
able. Hence, as part of the English lexical sample
task, we try the same approach on densely popu-
lated glosses accumulated from the training data for
this task.
1 Introduction
The main idea behind our approach for both the
WSD tasks is to use the context of a word along
with the gloss or description of each of its senses
to find its correct sense. The similarity between the
context and each sense of the word is measured and
the word-sense with the highest similarity measure
is picked as most appropriate, that with second high-
est similarity is ranked second and so on.
Glosses have been used by authors in the past for
WSD (Lesk, 1986). The novelty in our approach,
for the task of disambiguation of extended Word-
Net is in the way we generate our descriptions or
glosses. Also, an additional novelty in the sec-
ond task, is in our use of textual proximity between
words in the neighborhood of the word to be disam-
biguated and the words in the glosses of each of its
senses.
2 Glosses
2.1 Descriptive glosses
A word, with its associated part of speech and an
associated sense number, has a description.
Description for fifth noun sense of ?tape?
memory device consisting of a long thin plastic
strip coated with iron oxide; used to record audio
or video signals or to store computer information
Figure 1: An example descriptive-gloss for ?tape?
from WordNet
We call these descriptions - descriptive glosses.
For word-senses picked up from WordNet, the
WordNet glosses are the descriptive glosses. Word-
Net glosses also contain example usages of the
word. We have excluded the examples from de-
scriptive glosses
For other word-senses, the descriptions could
come from glossaries (like glossaries of software
terms), encyclopedias (for names of people, places,
events, pacts etc), world fact books, abbreviation
lists etc. Examples glosses picked up from above
sources are listed below.
descriptive-gloss for ?piccolo? an instrument
of the woodwind family. Most of these instru-
ments were once of made of wood, and because
they are played by blowing with air or wind, they
are called woodwind.
Figure 2: Examples of descriptive glosses for non-
WordNet words picked from glossaries
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.2 Hypernymy glosses
The gloss for a particular sense of a word could also
describe what hierarchical categories it belongs to.
For instance, the hierarchical categorization of
the   noun sense of the word ?Vesuvius? is:
Vesuvius#n#1
=> volcano
=> mountain, mount
=> natural elevation, elevation
=> geological formation
=> natural object
=> object, physical object
=> entity
Based on this hierarchical categorization of the
first noun sense of ?Vesuvius?, we describe its
hypernymy-gloss as the collection of all nodes in its
hypernymy-path to the root - viz. ?entity?.
Hypernymy gloss for first noun sense of ?Vesu-
vius?  volcano  ,  mountain, mount  ,  natural
elevation, elevation  ,  geological formation, for-
mation  ,  natural object  ,  object, physical ob-
ject  ,  entity 
Figure 3: Hypernymy-based gloss for the first sense
of Vesuvius(noun)
Whereas descriptive-glosses can be derived
even for word-senses not present in WordNet,
hypernymy-glosses require classification of word-
senses into nodes into an ontological structure - like
the hypernymy hierarchy of WordNet. This is not
that easy to procure for words not present in Word-
Net.
2.3 Hyper-Desc(  ) glosses
This category of gloss was developed for each word-
sense by concatenating the descriptive glosses of a
word-sense with the glosses of its hypernyms, all
the way upto height  . Hyper-Desc(  ) glosses de-
notes concatenating descriptive glosses all the way
upto the root.
2.4 Holo-Desc(  ) glosses
The specification of these glosses is same as of
Hyper-Desc(  ) glosses, except that holonyms are
considered here instead of hypernyms.
Handling Named Entities
One possible solution, and the one that we actu-
ally resort to, is to find the named entity tag for
a token (if one exists) and then map the tag to a
node in WordNet. For example, the token ?Pres-
ident Musharraf? is not present in WordNet. But
this token can be tagged as a PERSON and PER-
SON could be mapped to a node in WordNet - viz.
the first noun sense of ?person? (person#n#1). Sim-
ilarly, the token ? 	
 December 2003? has a DATE
named-entity tag. DATE could be translated to the

 sense of the word ?date? (date#n#7) in Word-
Net.
Thus, the glosses of named entites, which dont
find their entries into WordNet could be evolved
from their named-entity tags. This information is
valuable for disambiguating the surrounding words.
For the seneval task, we have built our own Named
Entity tagger that uses gazetteers and context-
sensitive grammar rules for tagging named entities.
Context of a word
The context of the word to be disambiguated (target
word) can be evolved in several possible ways.
1. The passage in which the target word lies can
be tokenized and the set of tokens are consid-
ered the context for that word.
2. In addition to tokenizing the passage as de-
scribed above, each token is also subjected
to stemming using the porter stemming algo-
rithm (Porter, 1980). The corresponding set of
stemmed tokens form the context. This option
is abbreviated as ST in table ??.
3. The passage can be part of speech tagged. In
the case of SemCor and Extended WordNet,
the part of speech tags have already been as-
signed manually. In the absence of a manual
POS tags, we use the QTag part of speech tag-
ger (Manson, 1980). And each part of speech
tagged word is expanded to the concatenation
of the glosses of all its word-senses. The col-
lection of all tokens in the expansions of all
words in the passage put together forms the
context for the target word. In table ??, this
option is abbreviated as FG.
3 Similarity metrics
Another parameter for measuring the similarity be-
tween the context of a word and the gloss of each of
its senses is the similarity metric.
The similarity between two sets of tokens is
found by constructing vectors of counts from the
two vectors and finding similarity between the vec-
tors.
3.1 Cosine similarity
One standard metric of similarity, as used in infor-
mation retrieval, is the cosine-similarity. We find
the cosine similarity between the term frequency-
inverse gloss frequency (tfigf) vectors of the two
sets. The inverse gloss frequency (igf) of a token
is the inverse of the number of glosses which con-
tain that token and it captures the ?commonness? of
that particular token.
There have been fancier definitions of similar-
ity in literature (Lin, 1998) which involve informa-
tion theoretic measures of similarity between word-
senses, based on the hypernymy path and DAG
structure of WordNet. These methods are heav-
ily dependent on frequencies of synsets in a sense-
tagged corpus. The idea is that two word-senses are
highly related if their subsuming synsets are highly
information bearing - or in other words, have high
information content. Information content is com-
puted from a sense tagged corpus - word-senses
with high frequencies of occurrence have low in-
formation content. This brings in the the problem
of data sparsity - because sense-tagged corpora are
very scarce and of short size. Their coverage of
synsets is poor as well. Hence there is the danger
of making the similarity measure biased toward the
sense-tagged corpus.
Also, these methods are very slow and CPU in-
tensive, since finding similarity between two word-
senses at run time involves traversing the WordNet
graph, in the direction of hypernymy links, up to the
least common ancestor.
On the other hand, a cosine similarity on tfigf
vectors built from hypernymy-glosses, gives a
low similarity value between word-senses whose
hypernymy-glosses overlap in very frequently oc-
curring synsets relative to the synsets which are not
common to their glosses. This is because igf implic-
itly captures the information content of a synset - the
higher the igf - higher is the information content of
a synset. The purpose served by a sense-tagged cor-
pus is cumulatively served by the collection of hy-
pernymy glosses of all the WordNet synsets. This
method is also more reliable since the igf values
come from WordNet which is very exhaustive, un-
like sense tagged corpora (like SemCor) which will
have bias and data-sparsity in terms of which words
occur in the corpus and which sense is picked for a
word. (The reader might want to note some work
which has been done to illustrate that words can in-
herently have multiple senses in a given context).
The cosine similarity on tfigf vectors built from
descriptive glosses is very much like the similarity
found between document and query vectors, since
the tokens in descriptive glosses are regular words.
Cosine similarity is intuitively the most useful sim-
ilarity measure on descriptive glosses since cosine
similarity of tfigf vectors takes care of stop words
and very non-informative words like ?the? etc.
3.2 Jaccard similarity
Another metric of similarity is the jaccard similar-
ity. Jaccard similarity between two sets of tokens
(glosses) is computed as  

 
 

 
. Here  and  are
the two glosses.
Jaccard similarity is appealing only if the glosses
used are hypernymy-glosses.
3.3 Asymmetric measures of similarity
The above two were symmetric measures of sim-
ilarity. A third asymmetric similarity measure is
one that takes a value of 	 if the intersection of the
glosses of two word-senses is not equal to the gloss
of one of the word-senses. Else, the similarity is
equal to one of cosine or jaccard similarity mea-
sures. This means that there are actually two asym-
metric similarity measures - one due to jaccard and
the other due to cosine.
4 Main Algorithm
For each word, a set of content words in its sur-
rounding was found and the similarity of this set
with with the gloss of each sense of the word was
measured. Cosine similarity measure was used for
all the experiments. The senses were then ordered
in decreasing value of scores. The word-sense with
highest similarity measure was picked as its most
appropriate sense. Following were the parameters
used in the sense-ranking algorithm.
4.1 Parameters
1. GlossType : The type of gloss being used in
the algorithm. It can be any one of the four
outlined in section 2.
2. Similarity measure: The cosine similarity
measure was used in all the experiments.
3. Stemming : Sometimes the words in the con-
text are related semantically with the gloss of
the ambiguous word but they may not be in the
same morphological form. For example, sup-
pose that the context contains the word Chris-
tian but the gloss of the word contains the
word Christ. The base form of both the words
is Christ but since they are not in the same
morphological form they will not be treated as
common words during intersection. Stemming
of words may prove useful in this case, because
after stemming both will give the same base
form.
4. FullContextExpansion : This parameter de-
termines whether or not the words in the con-
text should be expanded to their glosses. This
feature expands the context massively. If set
true the gloss of each sense of each context
word will be included in the context.
5. Context size : The context size can be 1 or 2
sentences etc. or 1 or 2 paragraphs etc.
5 Experimental Results
The algorithms were evaluated against Semcor and
was also used in Senseval-3 competition. We
present results in this section.
5.1 Results for Semcor
For preliminary experiments, we chose the Sem-
cor 1.7 corpus. It has been manually tagged us-
ing WordNet 1.7 glosses. The baseline algorithm
for sense-tagging of Semcor was of picking a sense
for a word, as its correct sense, uniformly at ran-
dom. This gave us a precision measure of 42.5%
for nouns and 23.2% for verbs. Tables 2, 3, 4 and 5
report precision for WSD on Semcor, using our al-
gorithm, with different parameter settings. We see
that the algorithm certainly makes a difference over
the baseline algorithm.
PrRank1 and PrRank2 (precision at rank 1 and 2
respectively) denote the percentage of cases where
the highest scoring sense is the correct sense or one
of first two highest scoring senses is the correct
sense, respectively. Our recall measures were the
same as precision because every word was assigned
a sense tag. In the event of lack of any evidence
for any sense tag, the first WordNet sense(the most
frequent sense) was picked.
Also note that acronyms in table 1 have been em-
ployed for parameters in the subsequent tables.
Stemming ST
ContextSize (in number of sentences) WS
FullContextExpansion FG
POS P
PrRank1 (%) R1
PrRank2 (%) R2
Table 1: List of acronyms used
5.2 Results for Senseval-3 task
For the Senseval task, we employed hypernym
glosses. The remaining parameters and the results
are tabulated in table 6.
We find results quite poor. We performed addi-
tional experiments with modified paramater set and
find great improvement in numbers. Moreover, we
ST WS FG P R1 R2
No 1 T n 50.3 69.2
No 1 T v 29.1 50.1
No 1 F n 71.4 83.9
No 1 F v 41.5 64.7
No 2 T n 47.7 66.8
No 2 T v 26.4 44.8
No 2 F n 49.1 67.7
No 2 F v 24.9 41.4
No 3 F n 47.3 66.5
No 3 F v 25.5 41.6
Table 2: Results for Hypernymy glosses
ST WS FG P R1 R2
Yes 1 T n 62.2 80.32
Yes 1 T v 36.6 59.5
No 2 T n 57.04 77.21
No 2 T v 34.2 56
Yes 2 T n 45.8 65.8
Yes 2 T v 22.8 40
Yes 2 F n 58.13 78.04
Yes 2 F v 34.03 56
Yes 3 F n 54.7 76.3
Yes 3 F v 31.4 51
Yes 3 T n 47.7 66.1
Yes 3 T v 24.4 42.5
Table 3: Results for Hyper-Desc( 	 ) glosses
pick the first WordNet sense in event of lack of any
evidence for disambiguation. Hence, in the next re-
ported experiment, the recall values are all same as
precision. Based on our experience with the Sem-
Cor experiments, we used Hyper-Desc( 	 ) glosses
and a context size of 1 sentence. The results are
presented in the table 7. The baseline precisions we
obtained were by sampling word-senses uniformly
at random. The baseline precision was 45.7% for
nouns and 25.4% for verbs.
6 English Lexical Sample Task
The results of our gloss based disambiguation sys-
tem show that an optimal configuration of the pa-
rameters is essential to get good results. Hyper-
Desc( 	 ) glosses together with stemming seem to al-
most always give better results than other. But it
may be worthwhile finding out the weight-age for
different types of glosses and use all of them to-
gether. However - the algorithm performs better
than the baseline algorithm, it still falls short of a
decent precision that is generally a pre-requisite for
the use of WSD in Machine Translation -   	 %. One
obvious reason for this is that no matter how we try
ST WS FG P R1 R2
No 1 T n 43 61.5
No 1 T v 21.4 35.8
Yes 1 T n 41.3 59.3
Yes 1 T v 21.1 36
No 2 F n 53.6 74.9
No 2 F v 29.7 50.6
No 3 F n 50.9 73.1
No 3 F v 29 47.8
Table 4: Results for Hyper-Desc(  ) glosses
ST WS FG P R1 R2
No 1 T n 49.18 71.5
No 1 T v 26.37 43.8
No 2 F n 62.75 79.7
No 2 F v 37.5 58.6
No 2 T n 48.2 73.2
No 2 T v 26 43.3
No 3 T n 48.5 74.3
No 3 T v 25 43.5
No 3 F n 61.08 77.75
No 3 F v 35.6 54.7
Table 5: Results for Holo-Desc(  ) glosses
to use WordNet, the descriptive glosses of Word-
Net are very sparse and contain very few contex-
tual clues for sense disambiguation. In the task of
English Lexical Sample, we further develop the al-
gorithm describe for the previous task and use rela-
tively dense glosses from the training set. The large
size of the glosses require us to modify the architec-
ture for ranking glosses. We use and inverted index
for indexing the glosses and treat the context of the
word to be disambiguated as a query. The senses of
the word are ranked using the same set of parame-
ters as described for the earlier task.
6.1 Experiments
For this task, the gloss for a word-sense is gener-
ated by concatenating the contexts of all training in-
stances for that word-sense. An inverted index is
generated for the glosses. The context for a test in-
stance is fired as a query and the senses for the word
are ranked using the tf-igf based cosine similarity
metric described in section 3.1. The top sense is
picked.
The baseline precision obtained for this task was
53.5%
The precision obtained using fine-grained scoring
was 66.1% and the recall was 65.7%. The precision
obtained using coarse-grained scoring was 74.3%
and the recall was 73.9%.
Gloss ST WS FG P Precision Recall
Hyper No 1 T n and v 34.0 29.1
Table 6: Senseval-3 report
ST WS FG P R1 R2
Yes 1 F n 72.9 88.5
Yes 1 F v 43.5 62
Yes 1 T n 65.1 83
Yes 1 T v 26.2 44.07
Table 7: Report of Senseval-3 Extended WordNet
task with modified parameters
6.2 Conclusion
We see that densely populated glosses do help in
getting a better precision score. One possible course
of action that this finding suggests is some kind of
interactive WSD where the user is allowed to cor-
rect machine generated tags for some dataset. The
contexts for words in the correctly tagged data could
next get appended to existing gloss of the corre-
sponding word-sense.
References
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how
to tell a pine code from an ice cream cone. In
Proceedings of the 5th annual international con-
ference on Systems documentation, pages 24?26.
ACM Press.
D Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf.
on Machine Learning, pages 296?304, San Fran-
cisco, CA. Morgan Kaufmann.
Christopher D Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, MA.
Oliver Manson. 1980. Qtag?a portable probabilis-
tic tagger. In Corpus Research, The University of
Birmingham, U.K.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. J. Miller. 1990. Introduction to wordnet:
an on-line lexical database. International Journal
of Lexicography 3 (4), pages 235 ? 244.
M. F. Porter. 1980. An algorithm for suffix strip-
ping. In Proceedings of SIGIR.
Ganesh Ramakrishnan, Soumen Chakrabarthi,
Deepa Paranjpe, and Pushpak Bhattacharyya.
2004. Is question answering an acquired skill ?
In Proceedings of the 13th World Wide Web Con-
ference (WWW13).
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 84?87,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Improving transliteration accuracy using word-origin detection and
lexicon lookup
Mitesh M. Khapra
IIT Bombay
miteshk@cse.iitb.ac.in
Pushpak Bhattacharyya
IIT Bombay
pb@cse.iitb.ac.in
Abstract
We propose a framework for translit-
eration which uses (i) a word-origin
detection engine (pre-processing) (ii) a
CRF based transliteration engine and (iii)
a re-ranking model based on lexicon-
lookup (post-processing). The results
obtained for English-Hindi and English-
Kannada transliteration show that the pre-
processing and post-processing modules
improve the top-1 accuracy by 7.1%.
1 Introduction
Machine transliteration is the method of automati-
cally converting Out-Of-Vocabulary (OOV) words
in one language to their phonetic equivalents in
another language. An attempt is made to retain
the original pronunciation of the source word to
as great an extent as allowed by the orthographic
and phonological rules of the target language. This
is not a great challenge for language pairs like
Hindi-Marathi which have very similar alphabetic
and phonetic sets. However, the problem becomes
non-trivial for language pairs like English-Hindi
and English-Kannada which have reasonably dif-
ferent alphabet sets and sound systems.
Machine transliteration find its application in
Cross-Lingual Information Retrieval (CLIR) and
Machine Translation (MT). In CLIR, machine
transliteration can help in translating the OOV
terms like proper names and technical terms which
frequently appear in the source language queries
(e.g. Jaipur in ?Jaipur palace?). Similarly it can
help improve the performance of MT by translat-
ing proper names and technical terms which are
not present in the translation dictionary.
Current models for transliteration can be clas-
sified as grapheme-based models, phoneme-based
models and hybrid models. Grapheme-based mod-
els like source channel model (Lee and Choi,
1998), Maximum Entropy Model (Goto et al,
2003), Conditional Random Fields (Veeravalli et
al., 2008) and Decision Trees (Kang and Choi,
2000) treat transliteration as an orthographic pro-
cess and try to map the source graphemes di-
rectly to the target graphemes. Phoneme based
models like the ones based on Weighted Finite
State Transducers (WFST) (Knight and Graehl,
1997) and extended Markov window (Jung et al,
2000) treat transliteration as a phonetic process
rather than an orthographic process. Under this
framework, transliteration is treated as a conver-
sion from source grapheme to source phoneme
followed by a conversion from source phoneme
to target grapheme. Hybrid models either use a
combination of a grapheme based model and a
phoneme based model (Stalls and Knight, 1998)
or capture the correspondence between source
graphemes and source phonemes to produce target
language graphemes (Oh and Choi, 2002).
Combining any of the above transliteration en-
gines with pre-processing modules like word-
origin detection (Oh and Choi, 2002) and/or
post-processing modules like re-ranking using
clues from monolingual resources (Al-Onaizan
and Knight, 2002) can enhance the performance of
the system. We propose such a framework which
uses (i) language model based word-origin detec-
tion (ii) CRF based transliteration engine and (iii)
a re-ranking model based on lexicon lookup on the
target language (Hindi and Kannada in our case).
The roadmap of the paper is as follows. In
section 2 we describe the 3 components of the
proposed framework. In section 3 we present
the results for English-Hindi and English-Kannada
transliteration on the datasets (Kumaran and
Kellner, 2007) released for NEWS 2009 Ma-
chine Transliteration Shared Task1(Haizhou et al,
2009). Section 4 concludes the paper.
1https://translit.i2r.a-star.edu.sg/news2009/
84
2 Proposed framework for
Transliteration
Figure 1: Proposed framework for transliteration.
2.1 Word Origin Detection
To emphasize the importance of Word Origin De-
tection we consider the example of letter ?d?.
When ?d? appears in a name of Western origin (e.g.
Daniel, Durban) and is not followed by the letter
?h?, it invariably gets transliterated as Hindi letter
X, whereas, if it appears in a name of Indic origin
(e.g. Indore, Jharkhand) then it is equally likely to
be transliterated as d or X. This shows that the de-
cision is influenced by the origin of the word. The
Indic dataset (Hindi, Kannada, and Tamil) for the
Shared Task consisted of a mix of Indic and West-
ern names. We therefore felt the need of train-
ing separate models for words of Indic origin and
words of Western origin.
For this we needed to separate the words in
the training data based on their origin. We first
manually classified 3000 words from the training
set into words of Indic origin and Western origin.
These words were used as seed input for the boot-
strapping algorithm described below:
1. Build two n-gram language models: one for
the already classified names of Indic origin
and another for the names of Western origin.
Here, by n-gram we mean n-character ob-
tained by splitting the words into a sequence
of characters.
2. Split each of the remaining words into a se-
quence of characters and find the probability
of this sequence using the two language mod-
els constructed in step 1.
3. If the probability of a word (i.e. a sequence
of characters) is higher in the Indic language
model than in the Western language model
then classify it as Indic word else classify it
as Western word.
4. Repeat steps 1-3 till all words have been clas-
sified.
Thus, we classified the entire training set into
words of Indic origin and words of Western origin.
The two language models (one for words of Indic
origin and another for words of Western origin)
thus obtained were then used to classify the test
data using steps 2 and 3 of the above algorithm.
Manual verification showed that this method was
able to determine the origin of the words in the test
data with an accuracy of 97%.
2.2 CRF based transliteration engine
Conditional Random Fields (Lafferty et al, 2001)
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given
the source word is given by,
P (Y |X;?) = 1Z(X) ? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word (English)
Y = target word (Hindi,Kannada)
T = length of source word (English)
K = number of features
?k = feature weight
Z(X) = normalization constant
CRF++2 which is an open source implemen-
tation of CRF was used for training and decod-
ing. GIZA++ (Och and Ney, 2000), which is a
freely available implementation of the IBM align-
ment models (Brown et al, 1993) was used to get
character level alignments for English-Hindi word
pairs in the training data. Under this alignment,
each character in the English word is aligned to
zero or more characters in the corresponding Hindi
word. The following features are then generated
using this character-aligned data (here ei and hi
are the characters at position i of the source word
and target word respectively):
? hi and ej such that i ? 2 ? j ? i + 2
? hi and source character bigrams ( {ei?1, ei}
or {ei, ei+1})
? hi and source character trigrams ( {ei?2,
ei?1, ei} or {ei?1, ei, ei+1} or {ei, ei+1,
ei+2})
2http://crfpp.sourceforge.net/
85
? hi, hi?1 and ej such that i ? 2 ? j ? i + 2
? hi, hi?1 and source character bigrams
? hi, hi?1 and source character trigrams
Two separate models were trained: one for the
words of Indic origin and another for the words
of Western origin. At the time of testing, the
words were first classified as Indic origin words
and Western origin words using the classifier de-
scribed in section 2.1. The top-10 transliterations
for each word were then generated using the cor-
rect CRF model depending on the origin of the
word.
2.3 Re-ranking using lexicon lookup
Since the dataset for the Shared Task contains
words of Indic origin there is a possibility that the
correct transliteration of some of these words may
be found in a Hindi lexicon. Such a lexicon con-
taining 90677 unique words was constructed by
extracting words from the Hindi Wordnet3. If a
candidate transliteration generated by the CRF en-
gine is found in this lexicon then its rank is in-
creased and it is moved towards the top of the list.
If multiple outputs are found in the lexicon then all
such outputs are moved towards the top of the list
and the relative ranking of these outputs remains
the same as that assigned by the CRF engine. For
example, if the 4th and 6th candidate generated by
the CRF engine are found in the lexicon then these
two candidates will be moved to positions 1 and 2
respectively. We admit that this way of moving
candidates to the top of the list is adhoc. Ideally, if
the lexicon also stored the frequency of each word
then the candidates could be re-ranked using these
frequencies. But unfortunately the lexicon does
not store such frequency counts.
3 Results
The system was tested for English-Hindi and
English-Kannada transliteration using the dataset
(Kumaran and Kellner, 2007) released for NEWS
2009 Machine Transliteration Shared Task. We
submitted one standard run and one non-standard
run for the English-Hindi task and one standard
run for the English-Kannada task. The re-ranking
module was used only for the non-standard run as
it uses resources (lexicon) other than those pro-
vided for the task. We did not have a lexicon
3http://www.cfilt.iitb.ac.in/wordnet/webhwn
for Kannada so were not able to apply the re-
ranking module for English-Kannada task. The
performance of the system was evaluated us-
ing 6 measures, viz., Word Accuracy in Top-
1 (ACC), Fuzziness in Top-1 (Mean F-score),
Mean Reciprocal Rank (MRR), MAPref , MAP10
and MAPsys. Please refer to the white paper of
NEWS 2009 Machine Transliteration Shared Task
(Haizhou et al, 2009) for more details of these
measures.
Table 1 and Table 2 report the results4 for
English-Hindi and English-Kannada translitera-
tion respectively. For English-Hindi we report
3 results: (i) without any pre-processing (word-
origin detection) or post-processing (re-ranking)
(ii) with pre-processing but no post-processing and
(iii) with both pre-processing and post-processing.
The results clearly show that the addition of these
modules boosts the performance. The use of
word-origin detection boosts the top-1 accuracy by
around 0.9% and the use of lexicon lookup based
re-ranking boosts the accuracy by another 6.2%.
Thus, together these two modules give an incre-
ment of 7.1% in the accuracy. Corresponding im-
provements are also seen in the other 5 metrics.
4 Conclusion
We presented a framework for transliteration
which uses (i) a word-origin detection engine
(pre-processing) (ii) a CRF based transliteration
engine and (iii) a re-ranking model based on
lexicon-lookup (post-processing). The results
show that this kind of pre-processing and post-
processing helps to boost the performance of
the transliteration engine. The re-ranking using
lexicon lookup is slightly adhoc as ideally the
re-ranking should take into account the frequency
of the words in the lexicon. Since such frequency
counts are not available it would be useful to find
the web counts for each transliteration candidate
using a search engine and use these web counts to
re-rank the candidates.
4Please note that the results reported in this paper are bet-
ter than the results we submitted to the shared task. This im-
provement was due to the correction of an error in the tem-
plate file given as input to CRF++.
86
Method ACC Mean
F-score
MRR MAPref MAP10 MAPsys
CRF Engine
(no word origin detection, no re-
ranking)
0.408 0.878 0.534 0.403 0.188 0.188
CRF Engine +
Word-Origin detection
(no re-ranking)
Standard run
0.417 0.877 0.546 0.409 0.192 0.192
CRF Engine +
Word-Origin detection +
Re-ranking
Non-Standard run
0.479 0.884 0.588 0.475 0.208 0.208
Table 1: Results for English-Kannada transliteration.
Method Accuracy
(top1)
Mean
F-score
MRR MAPref MAP10 MAPsys
CRF Engine +
Word-Origin detection
(no re-ranking)
Standard run
0.335 0.859 0.453 0.327 0.154 0.154
Table 2: Results for English-Kannada transliteration.
References
B. J. Kang and K. S. Choi 2000. Automatic translitera-
tion and back-transliteration by decision tree learn-
ing. Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation, 1135-
1411.
Bonnie Glover Stalls and Kevin Knight 1998. Trans-
lating Names and Technical Terms in Arabic Text.
Proceedings of COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages, 34-41.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009).
I. Goto and N. Kato and N. Uratani and T. Ehara
2003. Transliteration considering context informa-
tion based on the maximum entropy method. Pro-
ceedings of MT-Summit IX, 125132.
J. S. Lee and K. S. Choi. 1998. English to Korean
statistical transliteration for information retrieval.
Computer Processing of Oriental Languages, 17-37.
John Lafferty, Andrew McCallum, Fernando Pereira
2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence
Data. In Proceedings of the Eighteenth International
Conference on Machine Learning.
Jong-hoon Oh and Key-sun Choi 2002. An English-
Korean Transliteration Model Using Pronunciation
and Contextual Rules. Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING), 758-764.
Kevin Knight and Jonathan Graehl 1997. Machine
transliteration. Computational Linguistics, 128-
135.
Kumaran, A. and Kellner, Tobias 2007. A generic
framework for machine transliteration. SIGIR ?07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, 721-722.
Och Franz Josef and Hermann Ney 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447
P. F. Brown, S. A. Della Pietra, and R. L. Mercer 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263-311.
Sung Young Jung and SungLim Hong and Eunok Paek
2000. An English to Korean transliteration model of
extended Markov window. Proceedings of the 18th
conference on Computational linguistics, 383-389.
Suryaganesh Veeravalli and Sreeharsha Yella and
Prasad Pingali and Vasudeva Varma 2008. Statisti-
cal Transliteration for Cross Language Information
Retrieval using HMM alignment model and CRF.
Proceedings of the 2nd workshop on Cross Lingual
Information Access (CLIA) Addressing the Infor-
mation Need of Multilingual Societies.
Yaser Al-Onaizan and Kevin Knight 2001. Translating
named entities using monolingual and bilingual re-
sources. ACL ?02: Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, 400-408.
87
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 177?185,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Hybrid Model for Urdu Hindi Transliteration 
 
 
Abbas Malik Laurent Besacier Christian Boitet 
GETALP, Laboratoire d?Informatique Grenoble (LIG) 
Universit? Joseph Fourier 
Abbas.Malik, Laurent.Besacier, 
Christian.Boitet@imag.fr 
Pushpak Bhattacharyya 
IIT Bombay 
 
pb@cse.iitb.ac.in 
 
  
 
Abstract 
We report in this paper a novel hybrid ap-
proach for Urdu to Hindi transliteration that 
combines finite-state machine (FSM) based 
techniques with statistical word language 
model based approach. The output from the 
FSM is filtered with the word language model 
to produce the correct Hindi output. The main 
problem handled is the case of omission of di-
acritical marks from the input Urdu text. Our 
system produces the correct Hindi output even 
when the crucial information in the form of di-
acritic marks is absent. The approach improves 
the accuracy of the transducer-only approach 
from 50.7% to 79.1%. The results reported 
show that performance can be improved using 
a word language model to disambiguate the 
output produced by the transducer-only ap-
proach, especially when diacritic marks are not 
present in the Urdu input. 
1 Introduction 
Transliteration is a process to transcribe a word 
written in one language, in another language by 
preserving its articulation. It is crucial for han-
dling out-of-vocabulary (OOV) words in differ-
ent domains of Natural Language Processing 
(NLP), especially in Machine Translation 
(Knight and Graehl, 1998; Knight and Stall, 
1998; Paola and Sanjeev, 2003), Cross-Lingual 
Information Retrieval (Pirkola et al, 2003), the 
development of multi-lingual resources (Yan et 
al., 2003) and multi-lingual text and speech 
processing. It is also useful for Inter-dialectal 
translation without lexical changes and some-
times it is mandatory when the dialects in ques-
tion use mutually incomprehensible writing sys-
tems. Such cases exists in Malay (written in 2 
different scripts), Turkish (2 scripts), Kurdish (3 
scripts), Hindi/Urdu (2 scripts), Punjabi (2 
scripts), etc., where words are transliterated from 
one script to the other, irrespective of their type 
(noun, verb, etc., and not only proper nouns and 
unknown words). In this study, we will focus on 
Hindi/Urdu example. 
Hindi and Urdu are written in two mutually 
incomprehensible scripts, Devanagari and Urdu 
script ? a derivative of Persio-Arabic script re-
spectively. Hindi and Urdu are the official lan-
guages of India and the later is also the National 
language of Pakistan (Rahman, 2004). Table 1 
gives an idea about the number of speakers of 
Hindi and Urdu. 
 
 Native Speaker
2nd Lang. 
Speaker Total 
Hindi 366 487 853 
Urdu 60.29 104 164.29 
Total 426.29 591 1,017.29 
Source: (Grimes, 2000) all numbers are in millions 
Table 1: Hindi and Urdu Speakers 
Notwithstanding the transcriptional differences, 
Hindi and Urdu share phonology, grammar, 
morphology, literature, cultural heritage, etc. 
People from Hindi and Urdu communities can 
understand the verbal expressions of each other 
but the written expression of one community is 
alien to the other community. 
A finite-state transliteration model for Hindi 
and Urdu transliteration using the Universal In-
termediate Transcription (UIT ? a pivot between 
the two scripts) was proposed by Malik et al 
(2008). The non-probabilistic finite-state model 
is not powerful enough to solve all problems of 
Hindi ? Urdu transliteration. We visit and ana-
lyze Hindi ? Urdu transliteration problems in 
the next section and show that the solution of 
these problems is beyond the scope of a non-
probabilistic finite-state transliteration model. 
177
Following this, we show how a statistical model 
can be used to solve some of these problems, 
thereby enhancing the capabilities of the finite-
state model. 
Thus, we propose a hybrid transliteration 
model by combining the finite-state model and 
the statistical word language model for solving 
Hindi ? Urdu transliteration problems, dis-
cussed in section 2. Section 3 will throw light on 
the proposed model, its different components and 
various steps involved in its construction. In sec-
tion 4, we will report and various aspects of dif-
ferent experiments and their results. Finally, we 
will conclude this study in section 5. 
2 Hindi Urdu Transliteration 
In this section, we will analyze Hindi ? Urdu 
transliteration problems and will concentrate on 
Urdu to Hindi transliteration only due to shortage 
of space and will discuss the reverse translitera-
tion later. Thus, the remainder of the section ana-
lyzes the problems from Urdu to Hindi translite-
ration. 
2.1 Vowel, Yeh (?) and Waw (?) 
Urdu is written in a derivation of Persio-Arabic 
script. Urdu vowels are represented with the help 
of four long vowels Alef-madda (?), Alef (?), 
Waw (?), Yeh (?) and diacritical marks. One 
vowel can be represented in many ways depend-
ing upon its context or on the origin of the word, 
e.g. the vowel [?] is represented by Alef-madda 
(?) at the beginning of a word, by Alef (?) in the 
middle of a word and in some Persio-Arabic loan 
word, it is represented by the diacritical mark 
Khari Zabar (G?). Thus Urdu has very complex 
vowel system, for more details see Malik et al 
(2008). Urdu contains 10 vowels, and 7 of them 
also have their nasalization forms (Hussain, 
2004; Khan, 1997) and 15 diacritical marks. 
Thou diacritical marks form the cornerstone of 
the Urdu vowel system, but are sparingly used 
(Zia, 1999). They are vital for the correct Urdu to 
Hindi transliteration using the finite-state transli-
teration model. The accuracy of the finite-state 
transliteration model decreases from above 80% 
to 50% in the absence of diacritical marks. Fig-
ure 1 shows two example Urdu phrases (i) with 
and (ii) without the diacritical marks and their 
Hindi transliteration using the finite-state transli-
teration model. Due to the absence of Zabar (F?) 
in the first and the last words in (1)(ii) and in the 
5th word in (2)(ii), vowels ? ? [?] and ? [?] are 
transliterated into vowels ?? [e] and ? [o] re-
spectively. Similarly, due to the absence of Pesh 
( E?) and Zer (G?) in 3rd and 4th words respectively 
in (1)(ii), both vowels ? ? [?] and ?? [?] are con-
verted into the vowel [?]. All wrongly converted 
words are underlined. 
 
(1)  (i) ??? ???? ?? ???? ????? ??? ????? ???? 
 (ii) ?? ??? ???? ??? ??? ?? ??? ???? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ??? (ii) 
I have not done a lot of work 
(2) (i) ????? ?? ?? ?????? ???? ??? ?? ???? ???????????  
  (ii) ????? ??? ?? ??? ??????? ??? ?? ??? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
?? ?? ??? ???? ?? ?? ?? ??? ??? ??? (ii) 
Both at the central level and at the state level 
Figure 1: Example Urdu Phrases 
In Hindi, each vowel is represented by a cha-
racter and a vowel sign except the vowel [?], 
which is only represented by the character ? and 
do not have a vowel sign (Malik et al, 2008). 
Table 2 gives all vowel conversion problems. 
 
Sr. IPA 
Vowel 
Conversion 
Problems 
Hindi 
1 ? ? ? ? ? or ?? ? ? or 0* 
2 ? ? ? ? ? or ?? ? ? or 0* 
3 i i ? e ? or ?? ? ? or ?? 
4 ? ? ? e ? or ?? ? ? or ?? 
5 u u ? o ? or ?? ? ? or ?? 
6 ? ? ? o ? or ?? ? ? or ??
7 j j ? e ? ? ?? 
8 v v ? o ? ? ?? 
* Zero (0) means deleted. 
Table 2: Vowel Problems from Urdu to Hindi 
Long vowels Yeh (?) [j] and Waw (?) [v] are 
also used as consonants and certain contextual 
rules help us to decide whether they are used as a 
consonant or as a vowel, e.g., Yeh (?) and Waw 
(?) are used as consonants at the start of a word 
and after the long vowel Alef-madda (?), etc. Fi-
178
nite-state transliteration model can exploit such 
contextual rules but it is not possible to decide 
Yeh (?) and Waw (?) as consonants in the ab-
sence of diacritics. Thus a finite-state translitera-
tion model wrongly converts consonant Yeh (?) 
and Waw (?) into vowels ?? [e] and ?? [o], also 
given in Table 2, instead of consonants Ya (?) 
and Wa (?) respectively, e.g., in the word ????? 
(prince) [k??vr], Waw is wrongly converted into 
the vowel [o] due to the absence of Zabar ( F?) 
after it and the word becomes [k?nor], which is 
not a valid word of Hindi/Urdu. 
2.2 Native Sounds 
The Hindi writing system contains some native 
sounds/characters, e.g., vocalic R (?) [r?], retrof-
lex form of Na (?) [?], etc. On the other hand 
Urdu does not have their equivalents. Thus 
words containing such sounds are transcribed in 
Urdu with their approximate phonetic equiva-
lents. All such cases are problematic for Urdu to 
Hindi transliteration and are given in Table 3. 
 
Sr. IPA Hindi Urdu 
1 r? ? or ?? ? [r] 
2 ? ? ? [n] 
3 ? ? ? [?] 
4 Half h ?? ? [h] 
Table 3: Sounds of Sanskrit Origin 
2.3 Conjunct Form 
The Hindi alphabet is partly syllabic because 
each consonant inherits the vowel [?]. Two or 
more consonants may be combined together to 
form a cluster called Conjunct that marks the 
absence of the inherited vowel [?] between con-
sonants (Kellogg, 1872; Montaut, 2004). Con-
junction is also used to represent the gemination 
of a consonant, e.g., ?[k]+??+?[k]=???[kk] 
where ?? is the conjunct marker and aspiration of 
some consonants like ? [n], ? [m], ? [r] and ? 
[l] when used as conjunction with ? [h], e.g., 
?[n] + ?? + ?[h] = ???[nh]. Conjunction has a spe-
cial meaning but native speakers use conjunct 
forms without any explicit rule (Montaut, 2004). 
On the other hand, Urdu uses Jazam ( H? ? a 
diacritic) and Shadda (H?) to mark the absence of 
a vowel between two consonants and gemination 
of a consonant respectively. In the absence of 
these diacritics in the input Urdu text, it is not 
possible to decide on the conjunct form of con-
sonants except in the case of aspiration. In Urdu, 
aspiration of a consonant is marked with the spe-
cial character Heh-Doachashmee (?) (Malik et 
al., 2008), thus a finite-state transducer can easi-
ly decide about the conjunction for aspiration 
with a simple contextual rule, e.g. the word ????? 
(bride) [d??lhn] is correctly transliterated by our 
finite-state transliteration model into ??????. 
2.4 Native Hindi Spellings and Sanskritized 
Vocabulary 
Sanskrit highly influences Hindi and especially 
its vocabulary. In some words of Sanskrit origin, 
the vowel ?? [i] and ?? [u] are transcribed as ?? 
[?] and ?? [?] respectively at the end of a word. 
Javaid and Ahmed (2009) have pointed to this 
issue in these words ?Hindi language can have 
words that end on short vowel??. Table 4 gives 
some examples of such native words. On the 
other hand in Urdu, short vowels can never come 
at the end of a word (Javaid and Ahmed, 2009; 
Malik et al, 2008). 
 
Vowel Examples 
?? [i] 
??????? ? ????? (person) [vj?kti] 
??????? ? ??????? (culture) [s??skr?t?i] 
???????? ? ??????? (high) [???ko?i] 
?? [u] 
???? ? ????? (for) [het?u] 
????? ?? ????? (but) [k?nt?u] 
???? ? ?????? (metal) [d??t?u] 
Table 4: Hindi Word with Short vowel at End 
It is clear from above examples that short vowels 
at the end of a Hindi word can easily be translite-
rated in Urdu using a contextual rule of a finite-
state transducer, but it is not possible to do so for 
Urdu to Hindi transliteration using a non-
probabilistic finite-state transliteration model. 
Thus Urdu to Hindi transliteration can also be 
179
considered as a special case of Back Translitera-
tion. 
In some words, the vowel ?? [u] is written as 
the vowel ?? [?], e.g., ??? ? ????? or ??? ? ???? (to be) 
[hue], ??????? (name of a city) [r???npur]. 
Some of these cases are regular and can be im-
plemented as contextual rules in a finite-state 
transducer but it is not possible in every case. 
2.5 Ain (?) 
Ain (? ? glottal stop) exists in the Arabic alpha-
bet and native Arabic speakers pronounce it 
properly. Urdu also has adopted Ain (?) in its 
alphabet as well as Arabic loan words but native 
speakers of the sub-continent cannot produce its 
sound properly, rather they produce a vowel 
sound by replacing Ain (?) with Alef (?). The 
Hindi alphabet follows one character for one 
sound rule and it does not have any equivalent of 
Ain (?). Then, Ain (?) in Urdu words is tran-
scribed in Hindi by some vowel representing the 
pronunciation of the word by native sub-
continent speakers. Thus it is always translite-
rated in some vowel in Hindi. For example, Ain 
(?) gives the sound of the vowel [?] in ????  ? 
???? (strange) [??ib] and the vowel [?] with 
and without Alef (?) in words ???  ? ?? (com-
mon) [?m] and ???  ? ??? (after) [b?d?] respective-
ly. In some words, Ain (?) is not pronounced at 
all and should be deleted while transliterating 
from Urdu to Hindi, e.g., ??????  ? ???? (to start) 
[??ru], etc. Conversion of Ain (?) is a big prob-
lem for transliteration. 
2.6 Nasalization 
Noonghunna (?) [?] is the nasalization marker of 
vowels in Urdu. Interestingly, it is only used to 
nasalize a vowel at the end of a word. In the 
middle of a word, Noon (?) [n] is used to mark 
the nasalization of a vowel and it is also used as 
a consonant. It is difficult to differentiate be-
tween nasalized and consonant Noon (?). There 
are certain contextual rules that help to decide 
that Noon (?) is used as a consonant or a nasali-
zation marker, but it not possible in all cases. 
2.7 Persio-Arabic Vocabulary 
Urdu borrows a considerable portion of it voca-
bulary from Persian and Arabic and translitera-
tion of these words in Hindi is not regular. Table 
5 explains it with few examples. 
 
Urdu 
Hindi 
FST Conversion Correct 
??????  
?????? 
(surely) 
?????? 
[b?lk?l] 
???????? 
???????? 
(with reference of) 
???????? 
[b?lv?st??] 
?? ????????
????????? 
(in fact) 
???????? 
[f?lh?qiq?t]
Table 5: Persio-Arabic Vocabulary in Urdu 
3 Hybrid Transliteration Model 
The analysis of the previous section clearly 
shows that solution of these problems is beyond 
the scope of the non-probabilistic Hindi Urdu 
Finite-state transliteration model (Malik et al, 
2008). We propose a hybrid transliteration model 
that takes the input Urdu text and converts it in 
Hindi using the Finite-state Transliteration Mod-
el (Malik et al 2008). After that, it tries to cor-
rect the orthographic errors in the transducer-
only Hindi output string using a statistical word 
language model for Hindi with the help of a 
Hindi Word Map described later. The approach 
used is rather similar to what is done in text re-
capitalization (Stolcke et al 1998) for instance. 
 
Figure 2: Hybrid Transliteration Model for Urdu 
Hindi 
Normally, the Urdu text does not contain neces-
sary diacritical marks that are mandatory for the 
correct transliteration by the finite-state compo-
nent Urdu Hindi Transliteration 
180
Finite-state Machine (UHT-FSM), 
described by Malik et al (2008). The proposed 
hybrid model focuses on the correct translitera-
tion of Urdu texts without diacritical marks. Fig-
ure 2 gives the proposed Model architecture. 
3.1 Preprocessing UHT-FSM Output 
The goal of this pre-processing is to generate a 
more ?normalized? (and consequently more am-
biguous) form of Hindi, e.g. pre-processing 
transforms both corpus words ?? (this) [?s] and 
?? (that) [?s] (if encountered in the UHT-FSM 
Hindi output) into the default input Hindi word 
??* [?s] (not a valid Hindi word but is a finite-
state transliteration of the input Urdu word ??, a 
word without diacritical marks). Thus pre-
processing is vital for establishing connections 
between the UHT-FSM Hindi output words 
(from the Urdu input without diacritical marks) 
and the Hindi corpus words. In the example 
above, the word ??* [?s] is aligned to two Hin-
di corpus words. All such alignments are record-
ed in the Hindi Word Map. This ambiguity will 
be solved by the Hindi word language 
model, trained on a large amount of Hindi data. 
Thus pre-processing is a process that establishes 
connections between the most likely expected 
input Hindi word forms (UHT-FSM Hindi output 
from the Urdu input without diacritical marks) 
and the correct Hindi word forms (words that are 
present in the Hindi corpus). 
The Preprocessing component is a finite-
state transducer that normalizes the Hindi output 
of UHT-FSM component for the Hindi word 
language model. The transducer converts all 
cases of gemination of consonants into a simple 
consonant. For example, the UHT-FSM converts 
the Urdu word ??? (God) [r?bb] into ???? and the 
Preprocessing converts it into ?? [rb]. The 
transducer also removes the conjunct marker (??) 
from the output of the UHT-FSM except when it 
is preceded by one of the consonant from the set 
{? [r], ? [l], ? [m], ? [n]} and also followed by 
the consonant ? [h] (first 3 lines of Figure 3), 
e.g., UHT-FSM converts the Urdu words ?????? 
(Hindi) [h?ndi] and ????? (bride) [d??lhn] into ?????? 
and ?????? respectively and the Preprocess-
ing component converts them into ????? (re-
moves ??) and ?????? (no change). Actually, Pre-
processing deteriorates the accuracy of the output 
of the UHT-FSM component. We will come back 
to this point with exact figures in the next sec-
tion. 
The code of the finite-state transducer is given 
in XFST (Beesley and Karttunen, 2003) style in 
Figure 3. In XFST, the rules are applied in re-
verse order due to XFST?s transducer stack, i.e. a 
rule written at the end of the XFST script file 
will apply first and so on. 
 
read regex [? ?-> 0 || [? - [? | ? | ? | ?]] _ [? - 
?]]; 
read regex [?? -> 0 || [? | ? | ? | ?] _ [? - ?]]; 
read regex [?? -> 0 || [? - [? | ? | ? | ?]] _ [?]]; 
read regex [[? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ?
?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] 
-> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> 
?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?]; 
Figure 3: Preprocessing Transducer 
3.2 Hindi Word Language Model 
The Hindi Word Language Model is an 
important component of the hybrid transliteration 
model. For the development of our statistical 
word language model, we have used the Hindi 
Corpus freely available from the Center for In-
dian Language Technology1, Indian Institute of 
Technology Bombay (IITB), India. 
First, we extracted all Hindi sentences from 
the Hindi corpus. Then we removed all punctua-
tion marks from each sentence. Finally, we add-
ed ?<s>? and ?</s>? tags at the start and at the 
end of each sentence. We trained a tri-gram 
Hindi Word Language Model with the 
SRILM (Stolcke, 2002) tool. The processed Hin-
di corpus data contains total 173,087 unique sen-
                                                 
1 http://www.cfilt.iitb.ac.in/ 
181
tences and more than 3.5 million words. The 
SRILM toolkit command ?disambig? is used to 
generate the final Hindi output using the statis-
tical word language model for Hindi and the 
Hindi Word Map described in the next section.  
3.3 Hindi Word Map 
The Hindi Word Map is another very important 
component of the proposed hybrid transliteration 
model. It describes how each ?normalized? Hindi 
word that can be seen after the Preprocess-
ing step and can be converted to one or several 
correct Hindi words, the final decision being 
made by the statistical word language model for 
Hindi. We have developed it from the same 
processed Hindi corpus data that was used to 
build the Hindi Word Language Model. 
We extracted all unique Hindi words (120,538 
unique words in total). 
The hybrid transliteration model is an effort to 
correctly transliterate the input Urdu text without 
diacritical marks in Hindi. Thus we take each 
unique Hindi word and try to generate all possi-
ble Hindi word options that can be given as input 
to the Hindi Word Language Model 
component for the said word. Consider the Urdu 
word ??? (God) [r?bb]; its correct Hindi spel-
lings are ????. If we remove the diacritical mark 
Shadda (H?) after the last character of the word, 
then the word becomes ?? and UHT-FSM trans-
literates it in ??*. Thus the Hindi Word 
Language Model will encounter either ???? or 
??* for the Hindi word ???? (two possible word 
options). In other words, the Hindi Word Map is 
a computational model that records all possible 
alignments between the ?normalized? or pre-
processed words (most likely input word forms) 
and the correct Hindi words from the corpus. 
We have applied a finite-state transducer that 
generates all possible word options for each 
unique Hindi word. We cannot give the full 
XFST code of the ?Default Input Creator? due to 
space shortage, but a sample XFST code is given 
in Figure 4. If the Urdu input contains all neces-
sary diacritical marks, then pre-processing of the 
output of the UHT-FSM tries to remove the effect 
of some of these diacritical marks from the Hindi 
output. In the next section, we will show that 
actually it increases the accuracy at the end. 
 
 
define CONSONANTS [? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ?]; 
? 
read regex [?? (->) ?,? ?? (->) ?,? ?? (->) ??, ?? 
(->) ??, ?? (->) 0, ? ?(->) 0 || [CONSONANTS] 
_ ]; 
read regex [?? (->) ? ?|| [CONSONANTS] _ [? -
 .#.]]; 
read regex [?? -> ??, ? ? -> ??, ? ? -> ? ? || 
[CONSONANTS] _ .#.]; 
? 
Figure 4: Default Input Creator Transducer 
Practically, the Hindi Word Map is a file in 
which each line contains a possible input word to 
Hindi Word Language Model, followed 
by a list of one (see line 3 of Figure 5) or more 
(see line 1 of Figure 5) words from the corpus 
that are associated with this possible input word. 
The ?Default Input Creator? transducer has 
generated in total 961,802 possible input words 
for 120,538 unique Hindi words. For implemen-
tation reasons, we also added non-ambiguous 
pair entries in the word map (see line 2 of Figure 
5), thus the initial word map contains in total 
1,082,340 entries. We extract unique option 
words and finally, Hindi Word Map contains in 
total 962,893 entries. Some examples from Hindi 
Word Map file are given in Table 5. 
 
(1) ???? ???? ???? 
(2) ???? ???? 
(3) ?? ???? 
(4) ???????? ????????? ?????????? 
(5) ?? ?? ?? 
Figure 5: Sample Hindi Word Map 
4 Test and Results 
For testing purposes, we extracted 200 Hindi 
sentences from the Hindi corpus before removing 
punctuation marks. These sentences were of 
course removed from the training corpus used to 
build the statistical word language model for 
Hindi. First we converted these 200 Hindi sen-
tences in Urdu using Hindi Urdu Finite-state 
transliteration model (Malik et al, 2008). Trans-
182
literated Urdu sentences were post edited ma-
nually for any error and we also made sure that 
the Urdu text contained all diacritical marks. 200 
original Hindi sentences served as Hindi refer-
ence for evaluation purposes. 
From the post-edited Urdu sentences, we de-
veloped two test corpora. The first test corpus 
was the Urdu test with all diacritical marks. In 
the second test corpus, all diacritical marks were 
removed. We calculated both word level and 
character level accuracy and error rates using the 
SCLITE 2  tool. Our 200 sentence test contains 
4,250 words and 16,677 characters in total. 
4.1 Test: UHT-FSM 
First we converted both Urdu test data using 
UHT-FSM only and compared the transliterated 
Hindi texts with the Hindi reference. UHT-FSM 
shows a word error rate of 21.5% and 51.5% for 
the Urdu test data with and without diacritics 
respectively. Results are given in Table 6, row 1. 
 
Urdu Test Data With diacritics 
Without 
diacritics 
UHT-FSM 
Accuracy/Error 
80.7% / 
21.5% 
50.7% / 
51.5% 
UHT-FSM + 
HLM 
82.6% / 
19.6% 
79.1% / 
23.1% 
UHT-FSM + 
PrePro 
67.5% / 
32.4% 
50.7% / 
51.5% 
UHT-FSM + 
PrePro + HLM 
85.8% / 
16.4% 
79.1% / 
23.1% 
Table 6: Word Level Results 
These results support our claims that the absence 
of diacritical marks considerably increases the 
error rate. 
4.2 Test: UHT-FSM + Hindi Language 
Model 
Both outputs of UHT-FSM are first passed direct-
ly to Hindi Word Language Model with-
out preprocessing. The Hindi Word Lan-
guage Model converts UHT-FSM Hindi out-
put in the final Hindi output with the help of 
Hindi Word Map. 
Two final outputs were again compared with 
the Hindi reference and results are given in Table 
6, row 2. For Urdu test data without diacritics, 
error rate decreased by 28.4% due to the Hindi 
Word Language Model and Hindi Word 
                                                 
2 http://www.itl.nist.gov/iad/mig//tools/ 
Map as compared to the UHT-FSM error rate. 
The Hindi Word Language Model also decreases 
the error rate by 1.9% for the Urdu test data with 
diacritics. 
4.3 Test: UHT-FSM + Preprocessing 
In this test, both outputs of UHT-FSM were pre-
processed and the intermediate Hindi outputs 
were compared with the Hindi reference. Results 
are given in Table 6, row 3. After the comparison 
of results of row 1 and row 3, it is clear that pre-
processing deteriorates the accuracy of Urdu test 
data with diacritics and does not have any effect 
on Urdu test data without diacritics. 
4.4 Test: UHT-FSM + Preprocessing + 
Hindi Language Model 
Preprocessed UHT-FSM Hindi outputs of the test 
of Section 4.3 were passed to the Hindi Word 
Language Model that produced final Hindi 
outputs with the help of the Hindi Word Map. 
Results are given in Table 6, row 4. They show 
that the Hindi Word Language Model 
increases the accuracy by 5.1% and 18.3% when 
compared with the accuracy of UHT-FSM and 
UHT-FSM + Preprocessing tests respectively, for 
the Urdu test data with diacritical marks. 
For the Urdu test data without diacritical 
marks, the Hindi Word Language Model 
increases the accuracy rate by 28.3% in compari-
son to the accuracy of the UHT-FSM output 
(whether pre-processed or not). 
4.5 Character Level Results 
All outputs of tests of Sections 4.1, 4.2, 4.3 and 
4.4 and the Hindi reference are processed to cal-
culate the character level accuracy and error 
rates. Results are given in Table 7. 
 
Urdu Test 
Data 
With 
diacritics 
Without 
diacritics 
UHT-FSM 94.1% / 6.5% 77.5% / 22.6%
UHT-FSM + 
HLM 94.6% / 6.1% 89.8% / 10.7 
UHT-FSM + 
PreP 87.5% / 13.0% 77.5% / 22.6 
UHT-FSM + 
PreP + HLM 94.5% / 6.1% 89.8% / 10.7 
Table 7: Character Level Results 
183
4.6 Results and Examples 
The Hindi Word Language Model in-
creases the accuracy of Urdu Hindi translitera-
tion, especially for the Urdu input without dia-
critical marks. 
Consider the examples of Figure 7. Figure 1 is 
reproduced here by adding the Hindi translitera-
tion of example sentences using the proposed 
hybrid transliteration model and Hindi reference. 
 
(1)  (i) ?? ???? ????? ??? ????? ???? ??? ???? 
 (ii) ??? ?? ??? ???? ??? ???? ??? ?? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ???(ii)  
I have not done a lot of work
Output of Hybrid Transliteration Model 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
(ii) ??? ?? ???? ???? ??? ???? ???? ?? 
Hindi Reference 
????? ???? ???? ??? ???? ???? ?? 
(2) (i) ????? ?? ?? ?????? ???? ????? ?? ?? ???????????  
  (ii) ??????? ??? ?? ??? ??? ????? ??? ?? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
 ?? ??? ???? ?? ?? ?? ??? ??? ???(ii) 
?? 
Both at the central level and at the state level
Output of Hybrid Transliteration Model 
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
(ii) ??? ??? ??? ?? ?? ?? ????? ??? ?? ?? 
Hindi Reference 
??????? ??? ?? ?? ?? ????? ??? ?? ?? 
Figure 7: Examples 
By comparing Hindi outputs of Hindi Word 
Language Model with the Hindi reference, 
only the first word of (2)(ii) is wrong and other 
errors due to the absence of diacritical marks in 
the source Urdu sentences are corrected properly. 
5 Conclusion 
From the test results of the previous section we 
can conclude that the statistical word language 
model increases the accuracy of Urdu to Hindi 
transliteration, especially for Urdu input text 
without diacritical marks. The proposed Hybrid 
Transliteration Model improves the accuracy and 
produces the correct Hindi output even when the 
crucial information in the form of diacritical 
marks is absent. It increases the accuracy by 
28.3% in comparison to our previous Finite-state 
Transliteration Model. This study also shows that 
diacritical marks are crucial and necessary for 
Hindi Urdu transliteration. 
References  
Beesley, Kenneth R. and Karttunen, Lauri. 2003. Fi-
nite State Morphology, CSLI Publication, USA. 
Grimes, Barbara F. (ed). 2000. Pakistan, in Ethnolo-
gue: Languages of the World, 14th Edition Dallas, 
Texas; Summer Institute of Linguistics, pp: 588-
598. 
Hussain, Sarmad. 2004. Letter to Sound Rules for 
Urdu Text to Speech System, proceedings of Work-
shop on Computational Aproaches to Arabic 
Script-based Languages, COLING 2004, Geneva, 
Switzerland. 
Jawaid, Bushra and Tafseer Ahmed. 2009. Hindi to 
Urdu Conversion: Beyond Simple Transliteration, 
in proceedings of Conference on Language & 
Technology, Lahore, Pakistan. 
Kellogg, Rev. S. H. 1872. A Grammar of Hindi Lan-
guage, Delhi, Oriental Book reprints. 
Khan, Mehboob Alam. 1997. ????? ?? ???? ???? (Sound 
System in Urdu), National Language Authority, 
Pakistan 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion, Computational Linguistics, 24(4). 
Knight, K. and Stall, B. G. 1998. Transliterating 
Names and Technical Terms in Arabic Text, pro-
ceedings of COLING/ACL Workshop on Compu-
tational Approaches to Semitic Languages. 
Malik, M. G. Abbas. Boitet, Christian. Bhattcharyya, 
Pushpak. 2008. Hindi Urdu Machine Translitera-
tion using Finite-state Transducers, proceedings of 
COLING 2008, Manchester, UK. 
Montaut, A. 2004. A Linguistic Grammar of Hindi, 
Studies in Indo-European Linguistics Series, Mun-
chen, Lincom Europe. 
Paola, V. and Sanjeev, K. 2003. Transliteration of 
Proper Names in Cross-language Application, pro-
ceedings of 26th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval, Toronto, Canada. 
Pirkola, A. Toivonen, J. Keshustalo, H. Visala, K. and 
Jarvelin, K. 2003. Fuzzy Translation of Cross-
lingual Spelling Variants, proceedings of 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
Toronto, Canada. 
Rahman, Tariq. 2004. Language Policy and Localiza-
tion in Pakistan: Proposal for a Paradigmatic 
184
Shift, Crossing the Digital Divide, SCALLA Con-
ference on Computational Linguistics. 
Stolcke, A. 2002. SRILM ? An Extensible Language 
Modeling Toolkit, in proceedings of International 
Conference on Spoken Language Processing. 
Stolcke, A. Shriberg, E. Bates, R. Ostendorf, M. Hak-
kani, D. Plauche, M. Tur, G. and Lu, Y. 1998. Au-
tomatic Detection of Sentence Boundaries and Dis-
fluencies based on Recognized Words. Proceedings 
of International Conference on Spoken Language 
Processing (ICSLP), Sydney, Australia. 
Yan, Qu. Gregory, Grefenstette. and David A. Evans. 
2003. Automatic Transliteration for Japanese-to-
English Text Retrieval. In proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and Development in Information Retriev-
al, pp: 353 ? 360. 
Zia, Khaver. 1999. Standard Code Table for Urdu. 
Proceedings of 4th Symposium on Multilingual In-
formation Processing (MILIT-4), Yangon, Myan-
mar, CICC, Japan. 
 
185
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 555?563,
Beijing, August 2010
Value for Money: Balancing Annotation Effort, Lexicon Building and
Accuracy for Multilingual WSD
Mitesh M. Khapra Saurabh Sohoney Anup Kulkarni Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{miteshk,saurabhsohoney,anup,pb}@cse.iitb.ac.in
Abstract
Sense annotation and lexicon building are
costly affairs demanding prudent invest-
ment of resources. Recent work on mul-
tilingual WSD has shown that it is possi-
ble to leverage the annotation work done
for WSD of one language (SL) for another
(TL), by projecting Wordnet and sense
marked corpus parameters of SL to TL.
However, this work does not take into ac-
count the cost of manually cross-linking
the words within aligned synsets. Further,
it does not answer the question of ?Can
better accuracy be achieved if a user is
willing to pay additional money?? We
propose a measure for cost-benefit analy-
sis which measures the ?value for money?
earned in terms of accuracy by invest-
ing in annotation effort and lexicon build-
ing. Two key ideas explored in this pa-
per are (i) the use of probabilistic cross-
linking model to reduce manual cross-
linking effort and (ii) the use of selective
sampling to inject a few training examples
for hard-to-disambiguate words from the
target language to boost the accuracy.
1 Introduction
Word Sense Disambiguation (WSD) is one of
the most widely investigated problems of Natural
Language Processing (NLP). Previous works have
shown that supervised approaches to Word Sense
Disambiguation which rely on sense annotated
corpora (Ng and Lee, 1996; Lee et al, 2004) out-
perform unsupervised (Veronis, 2004) and knowl-
edge based approaches (Mihalcea, 2005). How-
ever, creation of sense marked corpora has always
remained a costly proposition, especially for some
of the resource deprived languages.
To circumvent this problem, Khapra et al
(2009) proposed a WSD method that can be ap-
plied to a language even when no sense tagged
corpus for that language is available. This is
achieved by projecting Wordnet and corpus pa-
rameters from another language to the language
in question. The approach is centered on a novel
synset based multilingual dictionary (Mohanty et
al., 2008) where the synsets of different languages
are aligned and thereafter the words within the
synsets are manually cross-linked. For example,
the word WL1 belonging to synset S of language
L1 will be manually cross-linked to the word WL2
of the corresponding synset in language L2 to in-
dicate that WL2 is the best substitute for WL1 ac-
cording to an experienced bilingual speaker?s in-
tuition.
We extend their work by addressing the follow-
ing question on the economics of annotation, lex-
icon building and performance:
? Is there an optimal point of balance between
the annotation effort and the lexicon build-
ing (i.e. manual cross-linking) effort at which
one can be assured of best value for money in
terms of accuracy?
To address the above question we first propose
a probabilistic cross linking model to eliminate
the effort of manually cross linking words within
the source and target language synsets and cali-
brate the resultant trade-off in accuracy. Next, we
show that by injecting examples for most frequent
hard-to-disambiguate words from the target do-
main one can achieve higher accuracies at optimal
555
cost of annotation. Finally, we propose a measure
for cost-benefit analysis which identifies the op-
timal point of balance between these three related
entities, viz., cross-linking, sense annotation and
accuracy of disambiguation.
The remainder of this paper is organized as fol-
lows. In section 2 we present related work. In sec-
tion 3 we describe the Synset based multilingual
dictionary which enables parameter projection. In
section 4 we discuss the work of Khapra et al
(2009) on parameter projection for multilingual
WSD. Section 5 is on the economics of multilin-
gual WSD. In section 6 we propose a probabilistic
model for representing the cross-linkage of words
within synsets. In section 7 we present a strat-
egy for injecting hard-to-disambiguate cases from
the target language using selective sampling. In
section 8 we introduce a measure for cost-benefit
analysis for calculating the value for money in
terms of accuracy, annotation effort and lexicon
building effort. In section 9 we describe the exper-
imental setup. In section 10 we present the results
followed by discussion in section 11. Section 12
concludes the paper.
2 Related Work
Knowledge based approaches to WSD such as
Lesk?s algorithm (Lesk, 1986), Walker?s algo-
rithm (Walker and Amsler, 1986), Conceptual
Density (Agirre and Rigau, 1996) and PageRank
(Mihalcea, 2005) are less demanding in terms of
resources but fail to deliver good results. Super-
vised approaches like SVM (Lee et al, 2004) and
k-NN (Ng and Lee, 1996), on the other hand, give
better accuracies, but the requirement of large an-
notated corpora renders them unsuitable for re-
source scarce languages.
Recent work by Khapra et al (2009) has shown
that it is possible to project the parameters learnt
from the annotation work of one language to an-
other language provided aligned Wordnets for two
languages are available. However, their work does
not address the question of further improving the
accuracy of WSD by using a small amount of
training data from the target language. Some sim-
ilar work has been done in the area of domain
adaptation where Chan et al (2007) showed that
adding just 30% of the target data to the source
data achieved the same performance as that ob-
tained by taking the entire source and target data.
Similarly, Agirre and de Lacalle (2009) reported a
22% error reduction when source and target data
were combined for training a classifier, compared
to the case when only the target data was used for
training the classifier. However, such combining
of training statistics has not been tried in cases
where the source data is in one language and the
target data is in another language.
To the best of our knowledge, no previous work
has attempted to perform resource conscious all-
words multilingual Word Sense Disambigua-
tion by finding a trade-off between the cost (in
terms of annotation effort and lexicon creation ef-
fort) and the quality in terms of F-score.
3 Synset based multilingual dictionary
A novel and effective method of storage and use
of dictionary in a multilingual setting was pro-
posed by Mohanty et al (2008). For the purpose
of current discussion, we will refer to this multi-
lingual dictionary framework as MultiDict. One
important departure in this framework from the
traditional dictionary is that synsets are linked,
and after that the words inside the synsets
are linked. The basic mapping is thus between
synsets and thereafter between the words.
Concepts L1 (English) L2 (Hindi) L3 (Marathi)
04321: a
youthful
male
person
{malechild,
boy}
{lw?A
(ladkaa),
bAl?
(baalak),
bQcA
(bachchaa)}
{m  lgA
(mulgaa),
porgA
(por-
gaa), por
(por)}
Table 1: Multilingual Dictionary Framework
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with
a unique ID. The subsequent columns show the
words expressing the concept in respective lan-
guages (in the example table, English, Hindi and
Marathi). After the synsets are linked, cross link-
ages are set up manually from the words of a
synset to the words of a linked synset of the pivot
language. For example, for the Marathi word
m  lgA (mulgaa), ?a youthful male person?, the
556
correct lexical substitute from the corresponding
Hindi synset is lw?A (ladkaa). The average num-
ber of such links per synset per language pair is
approximately 3.
4 Parameter Projection
Khapra et al (2009) proposed that the various
parameters essential for domain-specific Word
Sense Disambiguation can be broadly classified
into two categories:
Wordnet-dependent parameters:
? belongingness-to-dominant-concept
? conceptual distance
? semantic distance
Corpus-dependent parameters:
? sense distributions
? corpus co-occurrence
They proposed a scoring function (Equation (1))
which combines these parameters to identify the
correct sense of a word in a context:
S? = argmax
i
(?iVi +
?
j?J
Wij ? Vi ? Vj) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
The first component ?iVi of Equation (1) captures
influence of the corpus specific sense of a word in
a domain. The other component Wij ?Vi ?Vj cap-
tures the influence of interaction of the candidate
sense with the senses of context words weighted
by factors of co-occurrence, conceptual distance
and semantic distance.
Wordnet-dependent parameters depend on the
structure of the Wordnet whereas the Corpus-
dependent parameters depend on various statis-
tics learnt from a sense marked corpora. Both the
tasks of (a) constructing a Wordnet from scratch
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. Khapra
et al (2009) observed that by projecting relations
from the Wordnet of a language and by project-
ing corpus statistics from the sense marked cor-
pora of the language to those of the target lan-
guage, the effort required in constructing seman-
tic graphs for multiple Wordnets and collecting
sense marked corpora for multiple languages can
be avoided or reduced. At the heart of their work
lies the MultiDict described in previous section
which facilitates parameter projection in the fol-
lowing manner:
1. By linking with the synsets of a pivot re-
source rich language (Hindi, in our case), the cost
of building Wordnets of other languages is partly
reduced (semantic relations are inherited). The
Wordnet parameters of Hindi Wordnet now be-
come projectable to other languages.
2. For calculating corpus specific sense distri-
butions, P (Sense Si|Word W ), we need the
counts, #(Si,W ). By using cross linked words
in the synsets, these counts become projectable to
the target language (Marathi, in our case) as they
can be approximated by the counts of the cross
linked Hindi words calculated from the Hindi
sense marked corpus as follows:
P (Si|W ) = #(Si,marathi word)P
j #(Sj ,marathi word)
P (Si|W ) ? #(Si, cross linked hindi word)P
j #(Sj , cross linked hindi word)
The rationale behind the above approximation
is the observation that within a domain sense dis-
tributions remain the same across languages.
5 The Economics of Multilingual WSD
The problem of multilingual WSD using parame-
ter projection can be viewed as an economic sys-
tem consisting of three factors. The first factor is
the cost of manually cross-linking the words in a
synsets of the target language to the words in the
corresponding synset in the pivot language. The
second factor is the cost of sense annotated data
from the target language. The third factor is the
accuracy of WSD The first two factors in some
557
sense relate to the cost of purchasing a commod-
ity and the third factor relates to the commodity
itself.
The work of Khapra et al (2009) as described
above does not attempt to reach an optimal cost-
benefit point in this economic system. They place
their bets on manual cross-linking only and set-
tle for the accuracy achieved thereof. Specifi-
cally, they do not explore the inclusion of small
amount of annotated data from the target language
to boost the accuracy (as mentioned earlier, su-
pervised systems which use annotated data from
the target language are known to perform bet-
ter). Further, it is conceivable that with respect
to accuracy-cost trade-off, there obtains a case
for balancing one cost against the other, viz., the
cost of cross-linking and the cost of annotation.
In some cases bilingual lexicographers (needed
for manual cross-linking) may be more expensive
compared to monolingual annotators. There it
makes sense to place fewer bets on manual cross-
linking and more on collecting annotated corpora.
On the other hand if manual cross-linking is cheap
then a very small amount of annotated corpora
can be used in conjunction with full manual cross-
linking to boost the accuracy. Based on the above
discussion, if ka is the cost of sense annotating
one word, kc is the cost of manually cross-linking
a word and A is the accuracy desired then the
problem of multilingual WSD can be cast as an
optimization problem:
minimize wa ? ka +wc ? kc
s.t.
Accuracy ? A
where, wc and wa are the number of words to be
manually cross linked and annotated respectively.
Ours is thus a 3-factor economic model (cross-
linking, annotation and accuracy) as opposed to
the 2-factor model (cross-linking, accuracy) pro-
posed by Khapra et al (2009).
6 Optimal cross-linking
As mentioned earlier, in some cases where bilin-
gual lexicographers are expensive we might be in-
terested in reducing the effort of manual cross-
linking. For such situations, we propose that
only a small number of words, comprising of the
most frequently appearing ones should be manu-
ally cross linked and the rest of the words should
be cross-linked using a probabilistic model. The
rationale here is simple: invest money in words
which are bound to occur frequently in the test
data and achieve maximum impact on the accu-
racy. In the following paragraphs, we explain our
probabilistic cross linking model.
The model proposed by Khapra et al (2009) is
a deterministic model where the expected count
for (Sense S, Marathi Word W ), i.e., the num-
ber of times the word W appears in sense S is
approximated by the count for the correspond-
ing cross linked Hindi word. Such a model as-
sumes that each Marathi word links to appropri-
ate Hindi word(s) as identified manually by a lex-
icographer. Instead, we propose a probabilistic
model where a Marathi word can link to every
word in the corresponding Hindi synset with
some probability. The expected count for (S,W )
can then be estimated as:
E[#(S,W )] =
X
hi?cross links
P (hi|W,S) ? #(S, hi) (2)
where, P (hi|W,S) is the probability that the word
hi from the corresponding Hindi synset is the
correct cross-linked word for the given Marathi
word. For example, one of the senses of the
Marathi word maan is {neck} i.e. ?the body
part which connects the head to the rest of the
body?. The corresponding Hindi synset has 10
words {gardan, gala, greeva, halak, kandhar and
so on}. Thus, using Equation (2), the expected
count, E[C({neck},maan)], is calculated as:
E[#({neck}, maan)] =
P (gardan|maan,{neck}) ? #({neck}, gardan)
+ P (gala|maan, {neck}) ?#({neck}, gala)
+ P (greeva|maan,{neck}) ? #({neck}, greeva)
+ . . . so on for all words in the Hindi synset
Instead of using a uniform probability distribution
over the Hindi words we go by the empirical ob-
servation that some words in a synset are more
representative of that sense than other words, i.e.
some words are more preferred while expressing
that sense. For example, out of the 10 words in
558
the Hindi synset only 2 words {gardan, gala} ap-
peared in the corpus. We thus estimate the value
of P (hi|W,S) empirically from the Hindi sense
marked corpus by making the following indepen-
dence assumption:
P (hi|W,S) = P (hi|S)
The rationale behind the above independence as-
sumption becomes clear if we represent words and
synsets using the Bayesian network of Figure 1.
Here, the Hindi word hi and the Marathi word W
Figure 1: Bayesian network formed by a synset S
and the constituent Hindi and Marathi words
are considered to be derived from the same par-
ent concept S. In other words, they represent two
different manifestations- one in Hindi and one in
Marathi- of the same synset S. Given the above
representation, it is easy to see that given the par-
ent synset S, the Hindi word hi is independent of
the Marathi word W .
7 Optimal annotation using Selective
Sampling
In the previous section we dealt with the ques-
tion of optimal cross-linking. Now we take up
the other dimension of this economic system, viz.,
optimal use of annotated corpora for better accu-
racy. In other words, if an application demands
higher accuracy for WSD and is willing to pay for
some annotation then there should be a way of en-
suring best possible accuracy at lowest possible
cost. This can be done by including small amount
of sense annotated data from the target language.
The simplest strategy is to randomly annotate text
from the target language and use it as training
data. However, this strategy of random sampling
may not be the most optimum in terms of cost.
Instead, we propose a selective sampling strategy
where the aim is to identify hard-to-disambiguate
words from the target language and use them for
training.
The algorithm proceeds as follows:
1. First, using the probabilistic cross linking
model and aligned Wordnets we learn the param-
eters described in Section 4.
2. We then apply this scoring function on un-
tagged examples (development set) from the tar-
get language and identify hard-to-disambiguate
words i.e., the words which were disambiguated
with a very low confidence.
3. Training instances of these words are then in-
jected into the training data and the parameters
learnt from them are used instead of the projected
parameters learnt from the source language cor-
pus.
Thus, the selective sampling strategy ensures
that we get maximum value for money by spend-
ing it on annotating only those words which would
otherwise not have been disambiguated correctly.
A random selection strategy, in contrast, might
bring in words which were disambiguated cor-
rectly using only the projected parameters.
8 A measure for cost-benefit analysis
We need a measure for cost-benefit analysis based
on the three dimensions of our economic system,
viz., annotation effort, lexicon creation effort and
performance in terms of F-score. The first two di-
mensions can be fused into a single dimension by
expressing the annotation effort and lexicon cre-
ation effort in terms of cost incurred. For example,
we assume that the cost of annotating one word is
ka and the cost of cross-linking one word is kc ru-
pees. Further, we define a baseline and an upper
bound for the F-score. In this case, the baseline
would be the accuracy that can be obtained with-
out spending any money on cross-linking and an-
notation in the target language. An upper bound
could be the best F-score obtained using a large
amount of annotated corpus in the target domain.
Based on the above description, an ideal measure
for cost-benefit analysis would assign a
1. reward depending on the improvement over the
baseline performance.
2. penalty depending on the difference from the
upper bound on performance.
3. reward inversely proportional to the cost in-
559
curred in terms of annotation effort and/or manual
cross-linking.
Based on the above wish-list we propose a mea-
sure for cost-benefit analysis. Let,
MGB = Marginal Gain over Baseline (MGB)
= Performance(P )?Baseline(B)Cost(C)
MDU = Marginal Drop from Upperbound (MDU)
= UpperBound(U)? Performance(P )Cost(C)
then
CostBenefit(CB) = MGB ?MDU
9 Experimental Setup
We used Hindi as the source language (SL) and
trained a WSD engine using Hindi sense tagged
corpus. The parameters thus learnt were then pro-
jected using the MultiDict (refer section 3 and
4) to build a resource conscious Marathi (TL)
WSD engine. We used the same dataset as de-
scribed in Khapra et al (2009) for all our ex-
periments. The data was collected from two do-
mains, viz., Tourism and Health. The data for
Tourism domain was collected by manually trans-
lating English documents downloaded from In-
dian Tourism websites into Hindi and Marathi.
Similarly, English documents for Health domain
were obtained from two doctors and were manu-
ally translated into Hindi and Marathi. The Hindi
and Marathi documents thus created were manu-
ally sense annotated by two lexicographers adept
in Hindi and Marathi using the respective Word-
nets as sense repositories. Table 2 summarizes
some statistics about the corpora.
As for cross-linking, Hindi is used as the pivot
language and words in Marathi synset are linked
to the words in the corresponding Hindi synset.
The total number of cross-links that were man-
ually setup were 3600 for Tourism and 1800 for
Health. The cost of cross-linking as well as
sense annotating one word was taken to be 10 ru-
pees. These costs were estimated based on quo-
tations from lexicographers. However, these costs
need to be taken as representative values only and
may vary greatly depending on the availability of
skilled bilingual lexicographers and skilled mono-
lingual annotators.
Language #of polysemous
words
average degree of
polysemy
Tourism Health Tourism Health
Hindi 56845 30594 3.69 3.59
Marathi 34156 10337 3.41 3.60
Table 2: Number of polysemous words and aver-
age degree of polysemy.
10 Results
Tables 3 and 4 report the average 4-fold perfor-
mance on Marathi Tourism and Health data using
different proportions of available resources, i.e.,
annotated corpora and manual cross-links. In each
of these tables, along the rows, we increase the
amount of Marathi sense annotated corpora from
0K to 6K. Similarly, along the columns we show
the increase in the number of manual cross links
(MCL) used. For example, the second column of
Tables 3 and 4 reports the F-scores when proba-
bilistic cross-linking (PCL) was used for all words
(i.e., no manual cross-links) and varying amounts
of sense annotated corpora from Marathi were
used. Similarly, the first row represents the case
in which no sense annotated corpus from Marathi
was used and varying amounts of manual cross-
links were used.
We report three values in the tables, viz., F-
score (F), cost in terms of money (C) and the cost-
benefit (CB) obtained by using x amount of anno-
tated corpus and y amount of manual cross-links.
The cost was estimated using the values given in
section 9 (i.e., 10 rupees for cross-linking or sense
annotating one word). For calculating, the cost-
benefit baseline was taken as the F-score obtained
by using no cross-links and no annotated corpora
i.e. 68.21% for Tourism and 67.28% for Health
(see first F-score cell of Tables 3 and 4). Similarly
the upper bound (F-scores obtained by training on
entire Marathi sense marked corpus) for Tourism
and Health were 83.16% and 80.67% respectively
(see last row of Table 5).
Due to unavailability of large amount of tagged
Health corpus, the injection size was varied from
0-to-4K only. In the other dimension, we varied
the cross-links from 0 to 1/3rd to 2/3rd to full only
560
Selective Only PCL 1/3 MCL 2/3 MCL Full MCL
Sampling F C CB F C CB F C CB F C CB
0K 68.21 0 - 72.08 12 -0.601 73.31 24 -0.198 73.34 36 -0.130
1K 71.18 10 -0.901 74.96 22 -0.066 77.58 34 0.111 77.73 46 0.089
2K 74.35 20 -0.134 76.96 32 0.080 78.57 44 0.131 79.23 56 0.127
3K 75.21 30 -0.032 77.78 42 0.100 78.68 54 0.111 79.8 66 0.125
4K 76.40 40 0.036 78.66 52 0.114 79.18 64 0.110 80.36 76 0.123
5K 77.04 50 0.054 78.51 62 0.091 79.60 74 0.106 80.46 86 0.111
6K 78.58 60 0.097 79.75 72 0.113 80.8 84 0.122 80.44 96 0.099
Table 3: F-Score (F) in %, Cost (C) in thousand rupees and Cost Benefit (CB) values using different
amounts of sense annotated corpora and manual cross links in Tourism domain.
Selective Only PCL 1/3 MCL 2/3 MCL Full MCL
Sampling F C CB F C CB F C CB F C CB
0K 67.28 0 - 71.39 6 -0.862 73.06 12 -0.153 73.34 18 -0.071
1K 72.51 10 -0.293 75.57 16 0.199 77.41 22 0.312 78.16 28 0.299
2K 75.64 20 0.167 77.29 26 0.255 78.13 32 0.260 78.63 38 0.245
3K 76.78 30 0.187 79.35 36 0.299 79.79 42 0.277 79.88 48 0.246
4K 77.42 40 0.172 79.59 46 0.244 80.54 52 0.253 80.15 58 0.213
Table 4: F-Score (F) in %, Cost (C) in thousand rupees and Cost Benefit (CB) values using different
amounts of sense annotated corpora and manual cross links in Health domain.
Strategy Tourism Health
WFS 57.86 52.77
Only PCL 68.21 67.28
1/6 MCL 69.95 69.57
2/6 MCL 72.08 71.39
3/6 MCL 72.97 72.61
4/6 MCL 73.39 73.06
5/6 MCL 73.55 73.27
Full MCL 73.62 73.34
Upper Bound 83.16 80.67
Table 5: F-score (in %) obtained by using different amounts of manually cross linked words
Strategy Size of target side annotated corpus
0K 1K 2K 3K 4K 5K 6K
Random + PCL 68.21 70.62 71.79 73.03 73.61 76.42 77.52
Random + MCL 73.34 75.32 75.89 76.79 76.83 78.91 80.87
Selective Sampling + PCL 68.21 71.18 74.35 75.21 76.40 77.04 78.58
Selective Sampling + MCL 73.34 77.73 79.23 79.8 79.8 80.46 80.44
Table 6: Comparing F-scores obtained using random sampling and selective sampling (Tourism)
Strategy Size of target side annotated corpus
0K 1K 2K 3K 4K 5K 6K
Annotation + PCL 68.21 71.20 74.35 75.21 76.40 77.04 78.58
Only Annotation 57.86 62.32 64.84 66.86 68.89 69.64 71.82
Table 7: Comparing F-scores obtained using Only Annotation and Annotation + PCL(Tourism)
561
(refer to Tables 3 and 4). However, to give an
idea about the soundness of probabilistic cross-
linking we performed a separate set of experi-
ments by varying the number of cross-links and
using no sense annotated corpora. Table 5 sum-
marizes these results and compares them with the
baseline (Wordnet first sense) and skyline.
In Table 6 we compare our selective sampling
strategy with random sampling when fully proba-
bilistic cross-linking (PCL) is used and when fully
manual cross-linking (MCL) is used. Here again,
due to lack of space we report results only on
Tourism domain. However, we would like to men-
tion that similar experiments on Health domain
showed that the results were indeed consistent.
Finally, in Table 7 we compare the accuracies
obtained when certain amount of annotated corpus
from Marathi is used alone, with the case when the
same amount of annotated corpus is used in con-
junction with probabilistic cross-linking. While
calculating the results for the second row in Table
7, we found that the recall was very low due to the
small size of injections. Hence, to ensure a fair
comparison with our strategy (first row) we used
the Wordnet first sense (WFS) for these recall er-
rors (a typical practice in WSD literature).
11 Discussions
We make the following observations:
1. PCL v/s MCL: Table 5 shows that the proba-
bilistic cross-linking model performs much better
than the WFS (a typically reported baseline) and
it comes very close to the performance of manual
cross-linking. This establishes the soundness of
the probabilistic model and suggests that with a
little compromise in the accuracy, the model can
be used as an approximation to save the cost of
manual cross-linking. Further, in Table 7 we see
that when PCL is used in conjunction with cer-
tain amount of annotated corpus we get up to 9%
improvement in F-score as compared to the case
when the same amount of annotated corpus is used
alone. Thus, in the absence of skilled bilingual
lexicographers, PCL can still be used to boost the
accuracy obtained using annotated corpora.
2. Selective Sampling v/s Random Annotation:
Table 6 shows the benefit of selective sampling
over random annotation. This benefit is felt more
when the amount of training data injected from
Marathi is small. For example, when an annotated
corpus of size 2K is used, selective sampling gives
an advantage of 3% to 4% over random selection.
Thus the marginal gain (i.e., value for money) ob-
tained by using selective sampling is more than
that obtained by using random annotation.
3. Optimal cost-benefit: Finally, we address the
main message of our work, i.e., finding the best
cost benefit. By referring to Tables 3 and 4, we
see that the best value for money in Tourism do-
main is obtained by manually cross-linking 2/3rd
of all corpus words and sense annotating 2K tar-
get words and in the Health domain it is obtained
by manually cross-linking 2/3rd of all corpus
words but sense annotating only 1K words. This
suggests that striking a balance between cross-
linking and annotation gives the best value for
money. Further, we would like to highlight that
our 3-factor economic model is able to capture
these relations better than the 2-factor model of
Khapra et al (2010). As per their model the best
F-score achieved using manual cross-linking for
ALL words was 73.34% for both Tourism and
Health domain at a cost of 36K and 18K respec-
tively. On the other hand, using our model we ob-
tain higher accuracies of 76.96% in the Tourism
domain (using 1/3rd manual cross-links and 2K
injection) at a lower total cost (32K rupees) and
75.57% in the Health domain (using only 1/3rd
cross-linking and 1K injection) at a lower cost
(16K rupees).
12 Conclusion
We reported experiments on multilingual WSD
using different amounts of annotated corpora and
manual cross-links. We showed that there exists
some trade-off between the accuracy and balanc-
ing the cost of annotation and lexicon creation.
In the absence of skilled bilingual lexicographers
one can use a probabilistic cross-linking model
and still obtain good accuracies. Also, while sense
annotating a corpus, careful selection of words us-
ing selective sampling can give better marginal
gain as compared to random sampling.
562
References
Agirre, Eneko and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for wsd. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 42?50, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Agirre, Eneko and German Rigau. 1996. Word sense
disambiguation using conceptual density. In In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING).
Chan, Y.S., H. T. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical machine
translation. In In Proc. of ACL.
Khapra, Mitesh M., Sapan Shah, Piyush Kedia, and
Pushpak Bhattacharyya. 2009. Projecting param-
eters for multilingual word sense disambiguation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
459?467, Singapore, August. Association for Com-
putational Linguistics.
Khapra, Mitesh, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Lee, Yoong Keok, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of Senseval-3: Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, pages 137?140.
Lesk, Michael. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In In Proceed-
ings of the 5th annual international conference on
Systems documentation.
Mihalcea, Rada. 2005. Large vocabulary unsuper-
vised word sense disambiguation with graph-based
algorithms for sequence data labeling. In In Pro-
ceedings of the Joint Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing Conference (HLT/EMNLP), pages 411?418.
Mohanty, Rajat, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dic-
tionary: Insights, applications and challenges. In
Global Wordnet Conference.
Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Veronis, Jean. 2004. Hyperlex: Lexical cartography
for information retrieval. In Computer Speech and
Language, pages 18(3):223?252.
Walker, D. and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
563
Coling 2010: Poster Volume, pages 347?355,
Beijing, August 2010
Verbs are where all the action lies: Experiences of Shallow Parsing of a
Morphologically Rich Language
Harshada Gune Mugdha Bapat Mitesh M. Khapra Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{harshadag,mbapat,miteshk,pb}@cse.iitb.ac.in
Abstract
Verb suffixes and verb complexes of mor-
phologically rich languages carry a lot of
information. We show that this infor-
mation if harnessed for the task of shal-
low parsing can lead to dramatic improve-
ments in accuracy for a morphologically
rich language- Marathi1. The crux of the
approach is to use a powerful morpholog-
ical analyzer backed by a high coverage
lexicon to generate rich features for a CRF
based sequence classifier. Accuracy fig-
ures of 94% for Part of Speech Tagging
and 97% for Chunking using a modestly
sized corpus (20K words) vindicate our
claim that for morphologically rich lan-
guages linguistic insight can obviate the
need for large amount of annotated cor-
pora.
1 Introduction
Shallow parsing which involves Part-of-Speech
(POS) tagging and Chunking is a fundamental
task of Natural Language Processing (NLP). It is
natural to view each of these sub-tasks as a se-
quence labeling task of assigning POS/chunk la-
bels to a given word sequence. For languages like
English where annotated corpora are available in
abundance these tasks can be performed with very
high accuracy using data-driven machine learning
techniques. Languages of the world show differ-
ent levels of readiness with respect to such anno-
tated resources and hence not all languages may
1Marathi is the official language of Maharashtra, a state in
Western India. The language has close to 20 million speakers
in the world.
provide a conducive platform for machine learn-
ing techniques.
In this scenario, morphologically rich lan-
guages from the Indian subcontinent present a
very interesting case. While these languages do
not enjoy the resource abundance of English, their
linguistic richness can be used to offset this re-
source deficit. Specifically, in such languages, the
suffixes carry a lot of information about the cate-
gory of a word which can be harnessed for shal-
low parsing. This is especially true in the case of
verbs where suffixes like Z {ne}, ZAr {naare} 2
clearly indicate the category of the word. Further,
the structure of verb groups in such languages is
relatively rigid and can be used to reduce the am-
biguity between main verbs and auxiliary verbs.
In the current work, we aim to reduce the data
requirement of machine learning techniques by
appropriate feature engineering based on the char-
acteristics of the language. Specifically, we tar-
get Marathi- a morphologically rich language-
and show that a powerful morphological analyzer
backed by a high coverage lexicon and a simple
but accurate Verb Group Identifier (VGI) can go a
long way in improving the accuracy of a state of
the art sequence classifier. Further, we show that
harnessing such features is the only way by which
one can hope to build a high-accuracy classifier
for such languages, and that simply throwing in a
large amount of annotated corpora does not serve
the purpose. Hence it makes more sense to invest
time and money in developing good morphologi-
cal analyzers for such languages than investing in
annotation. Accuracy figures of 94% for Part of
2These are the suffixes which derive infinitive and gerund
verb forms respectively.
347
Speech Tagging and 97% for Chunking using a
modestly sized corpus (20K words) vindicate our
claim that for morphologically rich languages lin-
guistic knowledge plays a very important role in
shallow parsing of these languages.
2 Related Work
Many POS taggers have been built for English
employing machine learning techniques ranging
from Decision Trees (Black et al, 1992) to Graph-
ical Models (Brants, 2000; Brill, 1995; Ratna-
parkhi, 1996; Lafferty et al, 2001). Even hy-
brid taggers such as CLAWS (Garside and Smith,
1997) which combine stochastic and rule based
approaches have been developed. However, most
of these techniques do not focus on harnessing the
morphology; instead they rely on the abundance
of data which is not a very suitable proposition
for some of the resource deprived languages of the
Indian sub-continent.
Morphological processing based taggers using
a combination of hand-crafted rules and anno-
tated corpora have been tried for Turkish (Oflazer
and Kuruo?z, 1994), Arabic (Tlili-Guiassa, 2006),
Hungarian (Megyesi, 1999) and Modern Greek
(Giorgos et al, 1999). The work on Hindi POS
tagging (Singh et al, 2006) comes closest to our
approach which showed that using a detailed lin-
guistic analysis of morphosyntactic phenomena,
followed by leveraging suffix information and ac-
curate verb group identification can help to build
a high-accuracy (93-94%) part of speech tagger
for Hindi. However, to the best of our knowledge,
there is no POS tagger and Chunker available for
Marathi and ours is the first attempt at building
one.
3 Motivating Examples
To explain the importance of suffix information
for shallow parsing we present two motivating ex-
amples. First, consider the following Marathi sen-
tence,
hA r-taA don gAvA\nA joXZArA aAh.
haa rasta don gavaannaa jodaNaaraa VM aahe.
this road two villages connecting is
this is the road connecting VM two villages.
The word joXZArA {jodaNaaraa} (connecting)
in the above sentence is a verb and can be cat-
egorized as such by simply looking at the suffix
ZArA {Naaraa} as this suffix does not appear with
any other POS category. When suffix informa-
tion is used as a feature a statistical POS tagger
is able to identify the correct POS tag of joXZArA
{jodaNaaraa} even when it does not appear in the
training data. Hence, using suffix information en-
sures that a classifier is able to learn meaningful
patterns even in the absence of large training data.
Next, we consider two examples for chunking.
? VGNN (Gerund Verb Chunk)
mAZsAn uXyAcA ?y? klA.
maaNasaane uDaNyaachaa B-VGNN3
prayatna kelaa.
man fly try do
man tried flying B-VGNN.
? VGINF (Infinitival Verb Chunk)
(yAn cAlAylA s  zvAta klF.
tyaane chaalaayalaa B-VGNF suruvaata
kelii.
he walk start did
he started to walk B-VGINF.
Here, we are dealing with the case of two specific
verb chunks, viz., VGNN (gerund verb chunk) and
VGINF (infinitival verb chunk). A chunk having
a gerund always gets annotated as VGNN and a
chunk having an infinitival verb always gets anno-
tated as VGINF. Thus, the correct identification of
these verb chunks boils down to the correct iden-
tification of gerunds and infinitival verb forms in
the sentence which in turn depend on the careful
analysis of suffix information. For example, in
Marathi, the attachment of the verbal suffix ?y-
AcA? {Nyaachaa} to a verb root always results in
a gerund. Similarly, the attachment of the verbal
suffix ?ylA? {yalaa} to a verb root always results
in an infinitival verb form. The use of such suffix
information as features can thus lead to better gen-
eralization for handling unseen words and thereby
reduce the need for additional training data. For
instance, in the first sentence, even when the word
?uXyAcA? {uDaNyaachaa} does not appear in
3Note that for all our experiments we used BI scheme for
chunking as opposed to the BIO scheme
348
the training data, a classifier which uses suffix in-
formation is able to label it correctly based on its
experience of previous words having suffix ?y-
AcA? {Nyaachaa} whereas a classifier which does
not use suffix information fails to classify it cor-
rectly.
4 Morphological Structure of Marathi
Marathi nouns inflect for number and case. They
may undergo derivation on the attachment of post-
positions. In the oblique case, first a stem is ob-
tained from the root by applying the rules of in-
flection. Then a postposition is attached to the
stem. Postpositions (including case markers and
the derivational suffixes) play a very important
role in Marathi morphology due to the complex
morphotactics.
Marathi adjectives can be classified into two
categories: ones that do not inflect and others that
inflect for gender, number and case where such an
inflection agrees with the gender and number of
the noun modified by them.
The verbs inflect for gender, number and
person of the subject and the direct object in a
sentence. They also inflect for tense and aspect
of the action as well as mood of the speaker in
an illocutionary act. They may even undergo
derivation to derive the nouns, adjectives or
postpositions. Verbal morphology in Marathi
is based on Aakhyaata theory for inflection and
Krudanta theory for derivation which are two
types of verb suffixes (Damale, 1970).
Aakhyaata Theory: Aakhyaata refers to tense,
aspect and mood. Aakhyaata form is realized
through an aakhyaata suffix which is a closing
suffix attached to verb root. For example, bslA
{basalaa} (sat) comes from basa + laa. There are
8 types of aakhyaatas named after the phonemic
shape of the aakhyaata suffix. Associated with ev-
ery aakhyaata are various aakhyaata-arthas which
indicate the features: tense, aspect and mood. An
aakhyaata may or may not agree with gender.
Krudanta Theory: Krudanta suffixes are at-
tached to the end of verbs to form non-infinitive
verb forms. For example, DAvAylA (DAv +
aAylA) {dhaavaayalaa} (to run). There are 8
types of krudantas defined in Marathi.
5 Design of Marathi Shallow Parser
Figure 1 and 2 show the overall architectures of
Marathi POS tagger and chunker. The proposed
system contains 3 important components. First,
a morphological analyzer which provides ambi-
guity schemes and suffix information for gener-
ating a rich set of features. Ambiguity Scheme
refers to the list of possible POS categories a word
can take. This can add valuable information to a
sequence classifier by restricting the set of pos-
sible POS categories for a word. For example,
the word jAta {jaat} meaning caste or go(caste-
noun, go- VM/VAUX) can appear as a noun or a
main verb or an auxiliary verb. Hence it falls in
the ambiguity scheme <NN-VM-VAUX>. This
information is stored in a lexicon. These features
are then fed to a CRF based engine which cou-
ples them with other elementary features (previ-
ous/next words and bigram tags) for training a se-
quence labeler. Finally, in the case of POS tagger,
we use a Verb Group Identifier (VGI) which acts
as an error correcting module for correcting the
output of the CRF based sequence labeler. Each
of these components is described in detail in the
following sub-sections.
5.1 Morphological Analyzer
The formation of polymorphemic words leads
to complexities which need to be handled dur-
ing the analysis process. For example, consider
the steps involved in the formation of the word
dvAsmorQyAn {devasamorchyane} (the one in
front of the God + ERGATIVE).
devaasamora = (deva ? devaa)
+ samora
devaasamorachaa = ( devaasamora ? devaasamora)
+ chaa
devaasamorachyaane = (devaasamorachaa ? devaasamorachyaa)
+ ne
In theory, the process can continue recursively for
the attachment of any number of suffixes. How-
ever, in practice, we have observed that a word in
Marathi contains at most 4 suffixes.
FSMs prove to be elegant and computationally
efficient tools for analyzing polymorphemic
349
Figure 1: Architecture of POS Tagger
words. However, the recursive process of word
formation in Marathi involves inflection at the
time of attachment of every new suffix. The FSM
needs to be modified to handle this. However,
during the i-th recursion only (i-1)-th morpheme
changes its form which can be handled by suit-
ably modifying the FSM. The formation of word
dvAsmorQyAn {devaasamorachyaane} can be
viewed as:
devaasamora = (deva ? devaa)
+ samora
devaasamorachaa = ( deva ? devaa)
+ ( samora ? samora)
+ chaa
devaasamorachyaane = (deva ? devaa)
+ (samora ? samora)
+ (chaa ? chyaa)
+ ne
In general,
Polymorphemic word = (inflected morpheme1)
+ (inflected morpheme2) + ...
Now, we can create an FSM which is aware of
these inflected forms of morphemes in addition to
the actual morphemes to handle the above recur-
sive process of word formation. These inflected
forms are generated using the paradigm-based4
system written in Java and then fed to the FSM
implemented using SFST5.
4A paradigm identifies the uninflected form of words
which share similar inflectional patterns.
5http://www.ims.uni-stuttgart.de/projekte/gramotron
Our lexicon contains 16448 nouns categorized
into 76 paradigms, 8516 adjectives classified
as inflecting and non-inflecting adjectives, 1160
verbs classified into 22 classes. It contains 142
postpositions, 80 aakhyaata and 8 krudanta suf-
fixes.
5.2 CRF
Conditional Random Fields (Lafferty et al, 2001)
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of a tag given the observed
word sequence is given by,
P (Y |X;?) = 1Z(X) ? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of sentence
K = number of features
?k = feature weight
Z(X) = normalization constant
We used CRF++6, an open source implementa-
tion of CRF, for training and further decoding the
tag sequence. We used the following features for
training the sequence labeler (here, wi is the i-th
word, ti is the i-th pos tag and ci is the i-th chunk
tag).
/SOFTWARE/SFST.html
6http://crfpp.sourceforge.net/
350
Figure 2: Architecture of Chunker
Features used for POS tagger training
Consider position of interest = i
? ti ti?1 and wj such that i? 3 < j < i+ 3
? ti ti?1 and suffix information of wi
? ti ti?1 and ambiguity scheme of wi
Here, the first features are weak features which
depend only on the previous/next words and bi-
gram tags. The next two are rich morphological
features which make use of the output of the
morphological analyzer.
Features used for Chunker training
Consider position of interest = i
? ci ci?1 and tj , wj such that i?3 < j < i+3
? ci ci?1 and suffix information of wi
where ci, ci?1 ? {B, I}. Here again, the first set
of features are weak features and the second set
of features are rich morphological features.
5.3 Verb Group Identification (VGI)
In Marathi, certain auxiliaries like asta {asate}
(be), aAh {aahe} (is) etc.. can also act as main
verbs in certain contexts. This ambiguity between
VM (main verbs) and VAUX (auxiliary verbs) can
lead to a large number of errors in POS tagging
if not handled correctly. However, the relatively
rigid structure of Marathi VG coupled with dis-
tinct suffix-affinity of auxiliary verbs allows us to
capture this ambiguity well using the following
simple regular expression:
MainVerbRoot (KrudantaSuffix AuxVerbRoot)*
AakhyaataSuffix
The above regular expression imposes some re-
striction on the occurrence of certain auxiliary
verbs after specific krudanta suffixes. This restric-
tion is captured with the help of a rule file contain-
ing krudanta suffix-auxiliary verb pairs. A sample
entry from this file is
Un , kAY [oon, kaaDh]
which suggests that the auxiliary verb kAY
{kaaDh} can appear after the suffix Un {oon}.
We created a rule file containing around 350 such
valid krudanta suffix-auxiliary verb pairs.
An important point which needs to be high-
lighted here is that a simple left to right scan ig-
noring suffix information and marking the first
verb constituent as main verb and every other
constituent as auxiliary verb does not work for
Marathi. For example, consider the following
verb sequence,
(yAlA ucl n aAZAv lAgl.
tyaalaa uchaluun aaNaave laagale
He carry bring need
It was needed to carry and bring him.
Here, a simple left to right scan of the verb se-
quence ignoring the suffix information would im-
ply that ucl n is a VM whereas aAZAv and
lAgl are VAUX. However, this is not the case
and can be identified correctly by considering the
suffix affinity of auxiliary verbs. Specifically, in
this case, the verb root aAZ cannot take the role
of an auxiliary verb when it appears after the kru-
danta suffix Un. This suggests that the verb
351
aAZAv does not belong to the same verb group
as ucl n and hence is not a VAUX. This shows
suffix and regular expression help in disambiguat-
ing VM-VAUX which is a challenge in all POS
taggers.
6 Experimental Setup
We used documents from the TOURISM and
NEWS domain for all our experiments 7. These
documents were hand annotated by two Marathi
lexicographers. The total size of the corpus was
kept large (106273 POS tagged words and 63033
chunks) to study the impact of the size of training
data versus the amount of linguistic information
used. The statistics about each POS tag and chunk
tag are summarized in Table 1 and Table 2.
POS
Tag
Frequency
in Corpus
POS
Tag
Frequency
in Corpus
NN 51047 RP 359
NST 578 CC 3735
PRP 8770 QW 630
DEM 3241 QF 1928
VM 17716 QC 2787
VAUX 6295 QO 277
JJ 7311 INTF 158
RB 1060 INJ 22
UT 97 RDP 39
PSP 69 NEG 154
Table 1: POS Tags in Training Data
Chunk
Tag
Frequency
in Corpus
Chunk
Tag
Frequency
in Corpus
NP 40254 JJP 2680
VGF 7425 VGNF 3553
VGNN 1105 VGINF 58
RBP 782 BLK 2337
CCP 4796 NEGP 43
Table 2: Chunk Tags in Training Data
7 Results
We report results in four different settings:
Weak Features (WF): Here we use the basic
7The data can be found at www.cfilt.iitb.ac.in/
CRF classifier with elementary word features (i.e.,
words appearing in a context window of 3) and bi-
gram tag features and POS tags in case of chunker.
Weak Morphological Features (Weak-MF): In
addition to the elementary features we use sub-
strings of length 1 to 7 appearing at the end of the
word as feature. The idea here is that such sub-
strings taken from the end of the word can provide
a good approximation of the actual suffix of the
word. Such substrings thus provide a statistical
approximation of the suffixes in the absence of a
full fledged morphological analyzer. This should
not be confused with weak features which mean
tags and word.
Rich Morphological Features (Rich-MF): In
addition to the elementary features we use the am-
biguity schemes and suffix information provided
by the morphological analyzer.
Reach Morphological Features + Verb Group
Identification (Rich-MF+VGI): This setting is
applicable only for POS tagging where we apply
an error correcting VGI module to correct the out-
put of the feature rich CRF tagger.
In each case we first divided the data into four
folds (75% for training and 25% for testing).
Next, we varied the training data in increments of
10K and calculated the accuracy of each of the
above models. The x-axis represents the size of
the training data and the y-axis represents the pre-
cision of the tagger/chunker. Figure 3 plots the
average precision of the POS tagger across all cat-
egories using WF, Weak-MF, Rich-MF and Rich-
MF VGI for varying sizes of the training data.
Figure 6 plots the average precision of the chun-
ker across all categories using WF, Weak-MF and
Rich-MF. Next, to show that the impact of mor-
phological analysis is felt more for verbs than
other POS categories we plot the accuracies of
verb pos tags (Figure 4) and verb chunk tags (Fig-
ure 7) using WF, Weak-MF, Rich-MF and Rich-
MF VGI for varying sizes of the training data.
8 Discussions
We made the following interesting observations
from the above graphs and tables.
1. Importance of linguistic knowledge: Fig-
ure 3 shows that using a large amount of anno-
tated corpus (91k), the best accuracy one can hope
352
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Rich-M
F + V
GI
 
86
 
88
 
90
 
92
 
94
 
96
 
98
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 3: Average Accuracy of all POS Tags Figure 6: Average Accuracy of all Chunk Tags
(Note: The graphs for Rich-MF and Rich-MF+VGI coincide)
 
30
 
40
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Rich-M
F + V
GI
 
75
 
80
 
85
 
90
 
95
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 4: Average Accuracy of Verb POS Tags Figure 7: Average Accuracy of Verb Chunks
(Note: The graphs for Rich-MF and Rich-MF+VGI almost coincide)
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
 
75
 
80
 
85
 
90
 
95
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 5: Average Accuracy of Non Verb POS Tags Figure 8: Average Accuracy of Non Verb Chunks
(Note: All the graphs coincide.)
353
for is around 85% if morphological information is
not harnessed i.e., if only weak features are used.
Adding more data will definitely not be of much
use as the curve is already close to saturation. On
the other hand, if morphological information is
completely harnessed using a rich morphological
analyzer then an accuracy as high as 94% can be
obtained by using data as small as 20k words. Fig-
ure 6 tells a similar story. In the absence of mor-
phological features a large amount of annotated
corpus (62k words) is needed to reach an accu-
racy of 96%, whereas if suffix information is used
then the same accuracy can be reached using a
much smaller training corpus (20k words). This
clearly shows that while dealing with morpholog-
ically rich languages, time and effort should be
invested in building powerful morphological ana-
lyzer.
2. Weak morphological features vs rich mor-
phological analyzer: Figure 3 shows that in the
case of POS tagging using just weak morpholog-
ical features gives much better results than the
baseline (i.e. using only weak features). How-
ever, it does not do as well as the rich features
especially when the training size is small, thereby
suggesting that an approximation of the morpho-
logical suffixes may not work for a language hav-
ing rich and diverse morphology. On the other
hand, in the case of chunking, the weak morpho-
logical features do marginally better than the rich
morphological features suggesting that for a rela-
tively easier task (chunking as compared to POS
tagging) even a simple approximation of the ac-
tual suffixes may deliver the goods.
3. Specific case of verbs: Figure 4 shows that in
case of POS tagging using suffixes as features re-
sults in a significant increase in accuracy of verbs.
Specifically accuracy increases from 62% to 95%
using a very small amount of annotated corpus
(20K words). Comparing this with figure 5 we see
that while using morphological information defi-
nitely helps other POS categories, the impact is
not as high as that felt for verbs. Figures 7 and
8 for chunking show a similar pattern i.e., the ac-
curacy of verb chunks is affected more by mor-
phology as compared to other chunk tags. These
figures support our claim that ?verbs is where all
the action lies? and they indeed need special treat-
VM VAUX
VM 17078 347
VAUX 257 6025
Table 3: Confusion matrix for VM-VAUX using
Rich-MF
ment in terms of morphological analysis.
4. Effect of VGI: Figures 3 and 4 show that
the VGI module does not lead to any improve-
ment in the overall accuracy. A detailed analysis
showed that this is mainly because there was not
much VM-VAUX ambiguity left after applying
CRF model containing rich morphological fea-
tures. To further illustrate our point we present the
confusion matrix (see Table 3 ) for verb tags for
a POS tagger using Rich-MF. Table 3 shows that
there were only 347 VM tags which got wrongly
tagged as VAUX and 257 VAUX tags which got
wrongly tagged as VM. Thus the rich morpholog-
ical features were able to take care of most VM-
VAUX ambiguities in the data. However we feel
that if the data contains several VM-VAUX ambi-
guities such as the one illustrated in the example
in Section 5.3 then the VGI module would come
in play and help to boost the performance by re-
solving such ambiguities.
9 Conclusion
We presented here our work on shallow parsing of
a morphologically rich language- Marathi. Our re-
sults show that while dealing with such languages
one cannot ignore the importance of harnessing
morphological features. This is especially true for
verbs where improvements upto 50% in accuracy
can be obtained by adroit handling of suffixes and
accurate verb group identification. An important
conclusion that can be drawn from our work is
that while dealing with morphologically rich lan-
guages it makes sense to invest time and money
in developing powerful morphological analyzers
than placing all the bets on annotating data.
References
Black, Ezra, Fred Jelinek, John Lafferty, Robert Mer-
cer, and Salim Roukos. 1992. Decision tree mod-
els applied to the labeling of text with parts-of-
354
speech. In HLT ?91: Proceedings of the workshop
on Speech and Natural Language, pages 117?121,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Brants, Thorsten. 2000. TnT - A Statistical Part-
of-Speech Tagger. In 6th Applied Natural Lan-
guage Processing (ANLP ?00), April 29 - May 4,
pages 224?231. Association for Computational Lin-
gusitics.
Brill, Eric. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational
Linguistics, 21(4):543?565.
Damale, M K. 1970. Shastriya Marathi Vyaakarana.
Pune Deshmukh and Company.
Garside, Roger and Nicholas Smith. 1997. A Hybrid
Grammatical Tagger: CLAWS. In Garside, Roger,
Geoffrey Leech, and Tony McEnery, editors, Cor-
pus Annotation, pages 102?121. Longman, London.
Giorgos, Orphanos, Kalles Dimitris, Papagelis Thana-
sis, and Christodoulakis Dimitris. 1999. Decision
Trees and NLP: A case study in POS Tagging.
Lafferty, John, Andrew McCallum, and F. Pereira.
2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence
Data. In Proc. 18th International Conf. on Machine
Learning, pages 282?289. Morgan Kaufmann, San
Francisco, CA.
Megyesi, Beta. 1999. Improving Brill?s POS Tagger
For An Agglutinative Language, 02.
Oflazer, Kemal and Ilker Kuruo?z. 1994. Tagging and
Morphological Disambiguation of Turkish Text. In
ANLP, pages 144?149.
Ratnaparkhi, Adwait. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Brill, Eric
and Kenneth Church, editors, Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 133?142. Association for
Computational Linguistics, Somerset, New Jersey.
Singh, Smriti, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological
Richness Offsets Resource Demand - Experiences
in Constructing a POS Tagger for Hindi. In Pro-
ceedings of ACL-2006.
Tlili-Guiassa, Yamina. 2006. Hybrid Method for Tag-
ging Arabic Text. Journal of Computer Science 2,
3:245?248.
355
Coling 2010: Poster Volume, pages 791?800,
Beijing, August 2010
Finite-state Scriptural Translation 
M. G. Abbas Malik Christian Boitet Pushpak Bhattacharyya 
GETALP ? LIG (Grenoble Informatics Lab.) 
University of Grenoble 
 
IIT Bombay 
Abbas.Malik Christian.Boitet@imag.fr pb@iitb.ac.in 
  
 
Abstract 
We use robust and fast Finite-State Machines 
(FSMs) to solve scriptural translation prob-
lems. We describe a phonetico-morphotactic 
pivot UIT (universal intermediate transcrip-
tion), based on the common phonetic reposito-
ry of Indo-Pak languages. It is also extendable 
to other language groups. We describe a finite-
state scriptural translation model based on fi-
nite-state transducers and UIT. We report its 
performance on Hindi, Urdu, Punjabi and Se-
raiki corpora. For evaluation, we design two 
classification scales based on the word and 
sentence accuracies for translation system 
classifications. We also show that subjective 
evaluations are vital for real life usage of a 
translation system in addition to objective 
evaluations. 
1 Introduction 
Transliteration refers to phonetic translation 
across two languages with different writing sys-
tems, such as Arabic to English (Arbabi et al, 
1994; Stall and Knight, 1998; Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003). 
Most prior work on transliteration has been done 
for MT of English, Arabic, Japanese, Chinese, 
Korean, etc., for CLIR (Lee and Choi., 1998; 
Jeong et al, 1999; Fujii and Ishikawa, 2001; 
Sakai et al, 2002; Pirkola et al, 2003; Virga and 
Khudanpur, 2003; Yan et al, 2003), and for the 
development of multilingual resources (Kang 
and Choi, 2000; Yan, Gregory et al, 2003). 
The terms transliteration and transcription are 
often used as generic terms for various processes 
like transliteration, transcription, romanization, 
transcribing and technography (Halpern, 2002). 
In general, the speech processing community 
uses the term transcription to denote a process of 
conversion from the script or writing system to 
the sound (phonetic representation). For exam-
ple, the transcription of the word ?love? in the 
International Phonetic Alphabet (IPA) is [l?v]. 
While the text processing community uses the 
term transliteration and defines it as a process of 
converting a word written in one writing system 
into another writing system while preserving the 
sound of the original word (Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003). 
More precisely, the text processing community 
defines the term transliteration as two transcrip-
tion processes ?source script to sound transcrip-
tion? and ?sound to target script transcription? 
and sometimes as one transcription process 
?source script to target script transcription?. 
We propose a new term Scriptural Translation 
for this combined process. Scriptural translation 
is a process of transcribing a word written in the 
source language script into the target language 
script by preserving its articulation in the original 
language in such a way that the native speaker of 
the target language can produce the original pro-
nunciation. 
FSMs have been successfully used in various 
domains of Computational Linguistics and Natu-
ral Language Processing (NLP). The successful 
use of FSMs have already been shown in various 
fields of computational linguistics (Mohri, 1997; 
Roche and Schabes, 1997; Knight and Al-
Onaizan, 1998). Their practical and advanta-
geous features make them very strong candidates 
to be used for solving scriptural translation 
problems. 
First, we describe scriptural translation and 
identify its problems that fall under weak transla-
tion problems. Then, we analyze various chal-
lenges for solving weak scriptural translation 
problems. We describe our finite-state scriptural 
translation model and report our results on Indo-
Pak languages. 
791
2 Scriptural Translation ? a weak 
translation problem 
A weak translation problem is a translation prob-
lem in which the number of possible valid trans-
lations, say N, is either very small, less than 5, or 
almost always 1. 
Scriptural Translation is a sub-problem of 
general translation and almost always a weak 
translation problem. For example, French-IPA 
and Hindi-Urdu scriptural translation problems 
are weak translation problems due to their small 
number of valid translations. On the other hand, 
Japanese-English and French-Chinese scriptural 
translation problems are not weak. 
Scriptural translation is not only vital for 
translation between different languages, but also 
becomes inevitable when the same language is 
written in two or more mutually incomprehensi-
ble scripts. For example, Punjabi is written in 
three different scripts: Shahmukhi (a derivation 
of the Perso-Arabic script), Gurmukhi and Deva-
nagari. Kazakh and Kurdish are also written in 
three different scripts, Arabic, Latin and Cyrillic. 
Malay has two writing systems, Latin and Jawi 
(a derivation of the Arabic script), etc. Figure 1 
shows an example of scriptural divide between 
Hindi and Urdu. 
6 ?3[ 3e ?? ??Z ?? ]gzu[? X? 
?????? ?? ??? ?? ???? ??? 
[??n?j? ko ?m?n ki z?rur?? h?.] 
The world needs peace. 
Figure 1: Example of scriptural divide 
Thus, solving the scriptural translation prob-
lem is vital to bridge the scriptural divide be-
tween the speakers of different languages as well 
as of the same language. 
Punjabi, Sindhi, Seraiki and Kashmiri exist on 
both sides of the common border between India 
and Pakistan and all of them are written in two or 
more mutually incomprehensible scripts. The 
Hindi?Urdu pair exists both in India and Pakis-
tan. We call all these languages the Indo-Pak 
languages. 
3 Challenges of Scriptural Translation 
In this section, we describe the main challenges 
of scriptural translation. 
3.1 Scriptural divide 
There exists a written communication gap be-
tween people who can understand each other 
verbally but cannot read each other. They are 
virtually divided and become scriptural aliens. 
Examples are the Hindi & Urdu communities, 
the Punjabi/Shahmukhi & Punjabi/Gurmukhi 
communities, etc. An example of scriptural di-
vide is shown in Figure 1. Such a gap also ap-
pears when people want to read some foreign 
language or access a bilingual dictionary and are 
not familiar with the writing system. For exam-
ple, Japanese?French or French?Urdu dictiona-
ries are useless for French learners because of the 
scriptural divide. Table 1 gives some figures on 
how this scriptural divide affects a large popula-
tion of the world. 
Sr. Language Number of Speakers 
1 Hindi 853,000,000 
2 Urdu 164,290,000 
3 Punjabi 120,000,000 
4 Sindhi 21,382,120 
5 Seraiki 13,820,000 
6 Kashmir 5,640,940 
Total 1178,133,060 
Table 1: Number of Speakers of Indo-Pak languages 
3.2 Under-resourced languages 
Under-resourced and under-written features of 
the source or target language are the second big 
challenge for scriptural translation. The lack of 
standard writing practices or even the absence of 
a standard code page for a language makes trans-
literation or transcription very hard. The exis-
tence of various writing styles and systems for a 
language leads towards a large number of va-
riants and it becomes difficult and complex to 
handle them. 
In the case of Indo-Pak languages, Punjabi is 
the largest language of Pakistan (more than 70 
million) and is more a spoken language than a 
written one. There existed only two magazines 
(one weekly and one monthly) in 1992 (Rahman, 
1997). In the words of (Rahman, 2004), ?? 
there is little development in Punjabi, Pashto, 
Balochi and other languages??. (Malik, 2005) 
reports the first effort towards establishing a 
standard code page for Punjabi-Shahmukhi and 
till date, a standard code page for Shahmukhi 
does not exist. Similar problems also exist for the 
Kashmiri and Seraiki languages. 
792
3.3 Absence of necessary information 
There are cases where the necessary and indis-
pensable information for scriptural translation 
are missing in the source text. For example, the 
first word ???? [??n?j?] (world) of the example sen-
tence of Figure 1 misses crucial diacritical in-
formation, mandatory to perform Urdu to Hindi 
scriptural translation. Like in Arabic, diacritical 
marks are part of the Urdu writing system but are 
sparingly used in writings (Zia, 1999; Malik et 
al., 2008; Malik et al, 2009). 
Figure 2(a) shows the example word without 
diacritical marks and its wrong Hindi conversion 
according to conversion rules (explained later). 
The Urdu community can understand the word in 
its context or without the context because people 
are tuned to understand the Urdu text or word 
without diacritical marks, but the Hindi conver-
sion of Figure 2(a) is not at all acceptable or 
readable in the Hindi community. 
Figure 2(b) shows the example word with dia-
critical marks and its correct Hindi conversion 
according to conversion rules. Similar problems 
also arise for the other Indo-Pak languages. 
Therefore, missing information in the source text 
makes the scriptural translation problem compu-
tationally complex and difficult. 
 ? = ?? ????]?]  ?[? [ ?]n]  ?[? [ ?]j [ ?]?[ 
??????  = ?  ]? [ ??  ]? [ ?  ]n [ ??  ]? [ ?  ]j [ ??  ]?[  
(b) with necessary information 
 ? = ????]? [ ?]n [ ?]j [ ?]?[ 
????   = ?  ]? [ ?  ]n [ ?  ]j [ ??  ]?[  
(a) without necessary information 
Figure 2: Example of missing information 
3.4 Different spelling conventions 
Different spelling conventions exist across dif-
ferent scripts used for the same language or for 
different languages because users of a script are 
tuned to write certain words in a traditional way. 
For example, the words ?? [je] (this) = ? [j] + ? [h] 
and ?? [vo] (that) = ? [v] + ? [h] are used in Urdu 
and Punjabi/Shahmukhi. The character ? [h] pro-
duces the vowel sounds [e] and [o] in the exam-
ple words respectively. On the other hand, the 
example words are written as ?? [je] & ?? [vo] and 
? ? [je] & ?? [vo] in Devanagari and Gurmukhi, 
respectively. There exist a large number of such 
conventions between Punjabi/Shahmukhi?
Punjabi Gurmukhi, Hindi?Urdu, etc. 
Different spelling conventions are also driven 
by different religious influences on different 
communities. In the Indian sub-continent, Hindi 
is a part of the Hindu identity, while Urdu is a 
part of the Muslim identity1 (Rahman, 1997; Rai, 
2000). Hindi derives its vocabulary from San-
skrit, while Urdu borrows its literary and scien-
tific vocabulary from Persian and Arabic. Hindi 
and Urdu not only borrow from Sanskrit and Per-
sian/Arabic, but also adopt the original spellings 
of the borrowed word due the sacredness of the 
original language. These differences make scrip-
tural translation across scripts, dialects or lan-
guages more challenging and complex. 
3.5 Transcriptional ambiguities 
Character level scriptural translation across dif-
ferent scripts is ambiguous. For example, the 
Sindhi word ?????? [??s?n] (human being) can be 
converted into Devanagari either as ????? [??s?n] or 
????* [?ns?n] (* means wrong spellings). The trans-
literation process of the example word from 
Sindhi to Devanagari is shown in Figure 3(a). 
The transliteration of the third character from the 
left, Noon (?) [n], is ambiguous because in the 
middle of a word, Noon may represent a conso-
nant [n] or the nasalization [?] of a vowel. 
 
Figure 3: Sindhi transliteration example 
In the reverse direction, the Sindhi Devanagari 
word ????? [??s?n] can be converted into a set of 
possible transliterations [?????? ,*?????? ,??????*]. All 
these possible transliterations have the same pro-
nunciation [??s?n] but have different spellings in 
                                                 
1 The Hindi movement of the late 19th century played 
a central role in the ideologization of Hindi. The 
movement started in reaction to the British Act 29 of 
1837 by which Persian was replaced by Hindusta-
ni/Urdu, written in Persian script, as the official ver-
nacular of the courts of law in North India. It is the 
moment in history, when Hindi and Urdu started to 
emerge as Hindu and Muslim identities. 
793
the Perso-Arabic script, as shown in Figure 3(b). 
Similar kinds of ambiguities also arise for other 
pairs of scripts, dialects or languages. Thus these 
ambiguities increase the complexity and difficul-
ty of scriptural translation. 
3.6 Distinctive sound inventories 
Sound inventories across dialects or languages 
can be different. Consider the English?Japanese 
pair. Japanese make no distinction between the 
?L? [l] and ?R? [r] sounds so that these two Eng-
lish sounds collapse onto the same Japanese 
sound (Knight and Al-Onaizan, 1998). 
For Indo-Pak languages, Punjabi/Gurmukhi (a 
dialect of Punjabi spoken in India) possesses two 
additional sounds than Punjabi/Shahmukhi (a 
dialect of Punjabi spoken in Pakistan). Similarly, 
Hindi, Punjabi, Sindhi and Seraiki have the re-
troflex form [?], but Urdu and Kashmiri do not. 
Marathi has 14 vowels in contrast to Hindi?s 11 
vowels, shown in Table 2. 
Hindi Vowels 
? [?] ? [?] ? [?] ? [i] ? [?] ? [u] ? [r]? ? [e] ? [?] 
? [o] ? [?] 
Marathi Vowels 
? [?] ? [?] ? [?] ? [i] ? [?] ? [u] ? [r]? ? [e] ? [?] 
? [o] ? [?] ?? [??] ?? [?h] ? [l]? 
Table 2: Hindi and Marathi vowel comparison 
Scriptural translation approximates the pro-
nunciation of the source language or dialect in 
the target due to different sound inventories. 
Thus a distinctive sound inventory across scripts, 
dialects or languages increases ambiguities and 
adds to the complexity of the scriptural transla-
tion problem. 
4 Universal Intermediate Transcription 
UIT (Universal Intermediate Transcription) is a 
multipurpose pivot. In the current study, it is 
used as a phonetico-morphotactic pivot for the 
surface morphotactic translation or scriptural 
translation. 
Although we have not used IPA as encoding 
scheme, we have used the IPA coding associated 
with each character as the encoding principle for 
our ASCII encoding scheme. We selected the 
printable ASCII characters to base the UIT en-
coding scheme because it is universally portable 
to all computer systems and operating systems 
without any problem (Boitet and Tch?ou, 1990; 
Hieronymus, 1993; Wells, 1995). UIT is a de-
terministic and unambiguous scheme of tran-
scription for Indo-Pak languages in ASCII range 
32?126, since a text in this rage is portable 
across computers and operating systems 
(Hieronymus, 1993; Wells, 1995). 
Speech Assessment Methods Phonetic Alpha-
bet (SAMPA)2 is a widely accepted scheme for 
encoding IPA into ASCII. The purpose of SAM-
PA was to form the basis of an international 
standard machine-readable phonetic alphabet for 
the purpose of international collaboration in 
speech research (Wells, 1995). The UIT encod-
ing of Indo-Pak languages is developed as an 
extension of the SAMPA and X-SAMPA that 
covers all symbols on the IPA chart (Wells, 
1995). 
4.1 UIT encodings 
All characters of the Indo-Pak languages are 
subdivided into three categories, consonants, 
vowels and other symbols (punctuations and di-
gits). 
Consonants are further divided into aspirated 
consonants and non-aspirated consonants. For 
aspiration, in phonetic transcription a simple ?h? 
following the base consonant symbol is consi-
dered adequate (Wells, 1995). In the Indo-Pak 
languages, we have two characters with IPA [h]. 
Thus to distinguish between the ?h? consonants 
and the aspiration, we use underscore ?_? to 
mark the aspirate and we encode an aspiration as 
?_h?. For example, the aspirated consonants J[J 
[??], J ?J [p?] and J ?Y [??] of the Indo-Pak languages 
are encoded as ?t`_h?, ?p_h? and ?t_S_h? respec-
tively. Similarly for the dental consonants, we 
use the ?_d? marker. For example, the characters 
? [?] and ? [?] are encoded as ?d_d? and ?t_d? in 
UIT. Table 3 shows the UIT encodings of Hindi 
and Urdu aspirated consonants. 
Hindi Urdu UIT Hindi Urdu UIT 
? J [J [b?] b_h ??? |g [r?] r_h 
? J ?J [p?] p_h ? |g [??] r`_h 
? J[J [??] t_d_h ? J? [k?] k_h 
? J[J [??] t`_h ? J? [g?] g_h 
? J [Y [??] d_Z_h ?? J? [l?] l_h 
? J ?Y [??] t_S_h ?? Jb [m?] m_h 
                                                 
2 http://www.phon.ucl.ac.uk/home/sampa/ 
794
? |e [??] d_d_h ?? J [J [n?] n_h 
? |e [??] d`_h    
Table 3: UIT encodings of Urdu aspirated consonants 
Similarly, we can encode all characters of In-
do-Pak languages. Table 4 gives UIT encodings 
of Hindi and Urdu non-aspirated consonants. We 
cannot give all encoding tables here due to short-
age of space. 
Hindi Urdu UIT Hindi Urdu UIT 
? ? [b] b ? ? [s] s2 
? ? [p] p ? ? [z] z2 
? ? [?] t_d ? ? [?] t_d1 
? ? [?] t` ? ? [z] z3 
? ? [s] s1 - ? [?] ? 
? ? [?] d_Z ? ? [?] X 
? ? [?] t_S ? ? [f] f 
? ? [h] h1 ? ? [q] q 
? ? [x] x ? ? [k] k 
? ? [?] d_d ? ? [g] g 
? ? [?] d` ? ? [l] l 
? ? [z] z1 ? ? [m] m 
? ? [r] r ? ? [n] n 
? ? [?] r` ? ? [v] v 
? ? [z] z ? ? [h] h 
? ? [?] Z ? ? [j] j 
? ? [s] s ? ? [?] t_d2 
? ? [?] S ? - [?] n` 
? ? [?] S1 ?? ? [?] ~ 
Table 4: UIT encodings of Urdu non-aspirated conso-
nants 
5 Finite-state Scriptural Translation 
Model 
Figure 4 shows the system architecture of our 
finite-state scriptural translation system. 
Text Tokenizer receives and converts the 
input source language text into constituent words 
or tokens. This list of the source language tokens 
is then passed to the UIT Encoder that en-
codes these tokens into a list of UIT tokens using 
the source language to UIT conversion transduc-
er from the repertoire of Finite-State Transduc-
ers. These UIT tokens are given to the UIT De-
coder that decodes them into target language 
tokens using the UIT to target language conver-
sion transducer from the repertoire of Transduc-
ers. Finally, Text Generator generates the 
target language text from the translated target 
language tokens. 
 
Figure 4: System Architecture of fintie-state scriptural 
translation 
5.1 Finite-state Transducers 
Both conversions of the source language text into 
the UIT encoded text and from the UIT encoded 
text into the target language text are regular rela-
tions on strings. Moreover, regular relations are 
closed under serial composition and a finite set 
of conversion relations when applied to each 
other?s output in a specific order, also defines a 
regular expression (Kaplan and Kay, 1994). Thus 
we model the conversions from the source lan-
guage to UIT and from UIT to the target lan-
guage as finite-state transducers. These transla-
tional transducers can be deterministic and non-
deterministic. 
Character Mappings: Table 5 shows regular 
relations for converting Hindi and Urdu aspirated 
consonants into UIT. 
IPA Hindi to UIT Urdu to UIT 
b? ? ? b_h J [J ? b_h 
p? ? ? p_h J ?J ? p_h 
?? ? ? t_d_h J[J ? t_d_h 
?? ? ? t`_h J[J ? t`_h 
?? ? ? d_Z_h J [Y ? d_Z_h 
?? ? ? t_S_h J ?Y ? t_S_h 
795
?? ? ? d_d_h |e ? d_d_h 
?? ? ? d`_h |e ? d`_h 
r? ??? ? r_h |g ? r_h 
?? ? ? r`_h |g ? r`_h 
k? ? ? k_h J? ? k_h 
g? ? ? g_h J? ? g_h 
l? ?? ? l_h J? ? l_h 
m? ?? ? m_h Jb ? m_h 
n? ?? ? n_h J [J ? n_h 
Table 5: Regular rules for aspirated consonants of 
Hindi and Urdu 
By interchanging the UIT encodings before 
the arrow sign and the respective characters of 
Hindi and Urdu after the arrow, we can construct 
regular conversion relations from UIT to Hindi 
and Urdu. We have used XFST (Xerox finite-
state engine) to build finite-state transducers. 
Table 6 shows a sample XFST code. 
Contextual Mappings: A contextual mapping 
is a contextual rule that determines a desired out-
put when a character appears in a certain context. 
The third command of Table 6 models another 
contextual mapping saying that ????? is translated 
by ?_h? when it is preceded by any of the charac-
ters ?, ?, ?, and ?. The second last rule of Table 6 
models the contextual mapping rule that ?A1? is 
translated into ??? when it is at the end of a word 
and preceded by a consonant. 
clear stack 
set char-encoding UTF-8 
read regex [?? -> I]; 
read regex [? -> [k "_" h], ? -> [g 
"_" h], ? -> [t "_" S "_" h], ? -
> [d "_" Z "_" h], ? -> [t "`" "_" 
h], ? -> [d "`" "_" h], ? -> [t 
"_" d "_" h], ? -> [d "_" d "_" 
h], ? -> [p "_" h], ? -> [b "_" 
h], ? -> [r "`" "_" h], ? -> s, ?
-> [t "_" d], ? -> r, ? -> l, ? -> 
m, ? -> n, ? -> v, ? -> h]; 
read regex [[?? ?] -> ["_" h] || [? | 
? | ? | ?] _ ]; 
compose net 
Table 6: Sample XFST code 
Vowel representations in Urdu, Punja-
bi/Shahmukhi, Sindhi, Seraiki/Shahmukhi and 
Kashmiri are highly context-sensitive (Malik et 
al., 2010). 
6 Experiments and Results 
A sample run of our finite-state scriptural trans-
lation system on the Hindi to Urdu example sen-
tence of Figure 1 is shown in Table 7. 
Text 
Tokenizer 
UIT 
Encoder 
UIT Decoder 
Unique 
output 
Ambiguous 
outputs 
?????? dUnIjA1 ?????? , ?????? ] ?????? ]  
?? ko ?? , ?? ] ?? ] 
??? @mn ??? ] ??? ] 
?? ki ?? , ?? ] ?? ] 
???? zrurt_d ?????  [ ????? , 
?????? , 
?????? , 
?????? , 
?????? , 
?] 
 
?? 
 
h{  
??? , ?? ] ??? ] 
Table 7: Sample run of finite-state scriptural transla-
tion model on Hindi to Urdu example 
Text Generator converts the unique out-
put of the UIT Decoder into an Urdu sentence 
with one error in the fifth word (highlighted), 
shown in Figure 5. 
? ]gzEgi ?? ??Z ?? 6 ?3G[ 3 Ee 
Figure 5: Unique output of the sample run by deter-
ministic FSTs 
On the other hand, from the ambiguous output 
of the UIT Decoder, we can generate 240 output 
sentences, but only one is the correct scriptural 
translation of the source Hindi sentence in Urdu. 
The correct sentence is shown in Figure 6. The 
sole difference between the output of the deter-
ministic FST and the correct scriptural transla-
tion is highlighted in both sentences shown in 
Figure 5 and 6. 
? ]gzEu [? ?? ??Z ?? 6 ?3G[ 3 Ee 
Figure 6: Correct scriptural translation of the example 
6.1 Test Data 
Table 8 shows test sets for the evaluation of our 
finite-state scriptural translation system. 
796
Data 
set Language pair 
No. of 
words 
No. of 
sentences Source 
HU 
1 Hindi?Urdu 52,753 - 
Platts 
dictionary
HU 
2 Hindi?Urdu 4,281 200 
Hindi 
corpus 
HU 
3 Hindi?Urdu 4,632 226 
Urdu 
corpus 
PU Punjabi/Shahmukhi?Punjabi/Gurmukhi 5,069 500 
Classical 
poetry 
SE Seraiki/Shahmukhi?Seraiki/Devanagari 2,087 509 
Seraiki 
poetry 
Table 8: Test Sets of Hindi, Urdu, Punjabi and Seraiki 
HU 1 is a word list obtained from the Platts 
dictionary3 (Platts, 1884). 
6.2 Results 
For Hindi to Urdu scriptural translation, we have 
applied the finite-state model to all Hindi inputs 
of HU Test sets 1, 2 and 3. In general, it gives us 
an Urdu output with the necessary diacritical 
marks. To evaluate the performance of Hindi to 
Urdu scriptural translation of our finite-state sys-
tem against the Urdu without diacritics, we have 
created a second Urdu output by removing all 
diacritical marks from the default Urdu output of 
the finite-state system. We have calculated the 
Word Accuracy Rate (WAR) and Sentence Accu-
racy Rate (SAR) for the default and the 
processed Urdu outputs by comparing them with 
the Urdu references with and without diacritics 
respectively. To compute WAR and SAR, we 
have used the SCLITE utility from the Speech 
Recognition Scoring Toolkit (SCTK)4 of NIST. 
The results of Hindi to Urdu scriptural transla-
tion are given in Table 24. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
HU 1 32.5% - 78.9% - 
HU 2 90.8% 26.5% 91.0% 27% 
HU 3 81.2% 8.8% 82.8% 9.7% 
Table 9:Hindi to Urdu scriptural translation restuls 
The finite-state scriptural translation system 
for Hindi to Urdu produces an Urdu output with 
diacritics. However, we know that the Urdu 
community is used to see the Urdu text without 
diacritics. Thus, we removed all diacritical marks 
from the Urdu output text that is more acceptable 
to the Urdu community. By this post-processing, 
                                                 
3 Shared by University of Chicago for research pur-
poses. 
4 http://www.itl.nist.gov/iad/mig//tools/ 
we gain more than 40% accuracy in case of HU 
Test Set 1. We also gain in accuracy for the other 
test sets. 
For the classification of our scriptural transla-
tion systems, we have devised two scales. One 
corresponds to the word accuracy rate and the 
other corresponds to the sentence level accuracy. 
They are shown in Figure 7 and 8. 
 
Figure 7: Classification scale based on the word 
accuracy rate for scriptural transaltion 
 
Figure 8: Classification scale based on the sentence 
accucary rate for scriptural translation 
According to the scale of Figure 7 and 8, the 
Hindi to Urdu scriptural translation system is 
classified as ?Good? and ?Good Enough?, respec-
tively. 
The subjective evaluations like usability, ef-
fectiveness and adequacy depend on several fac-
tors. A user with a good knowledge of Hindi and 
Urdu languages would rate our Hindi to Urdu 
system quite high and would also rate the Urdu 
output very usable. Another user who wants to 
read a Hindi text, but does not know Hindi, 
would also rate this system and the Urdu output 
quite high and very usable respectively, because 
it serves its purpose. 
On the other hand, a user who wants to pub-
lish a Hindi book in Urdu, would rate this system 
not very good. This is because he has to localize 
the Hindi vocabulary of Sanskrit origin as the 
acceptance of the Hindi vocabulary in the Urdu 
797
community, target of his published book, is very 
low. Thus the subjective evaluation depends on 
various factors and it is not easy to compute such 
measures for the evaluation of a scriptural trans-
lation system, but they are vital in real life. 
For Urdu to Hindi scriptural translation, we 
have two inputs for each HU Test Set. One input 
contains all diacritical marks and the other does 
not contain any. On Hindi side, we have a single 
Hindi reference with which we will compare 
both Hindi outputs. We already know that it will 
give us less accuracy rates for the Urdu input 
without diacritical marks that are mandatory for 
correct Urdu to Hindi scriptural translation. The 
results for Urdu to Hindi scriptural translation 
are given in Table 10. 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
HU 1 68.0% - 31.2% - 
HU 2 83.9% 10% 53.0% 1% 
HU 3 98.4% 73.9% 58.9% 0.4% 
Table 10: Urdu to Hindi scriptural translation results 
For the Urdu input with diacritics, the accura-
cy of the Urdu to Hindi finite-state scriptural 
translation system is 83.9% at word level for HU 
Test Set 2 and it is classified as ?GOOD? the 
classification scale of Figure 7. On the other 
hand, it shows a sentence-level accuracy of 10% 
for the same test set and is classified as ?AVER-
AGE? by the classification scale of Figure 8. 
For the Urdu input without diacritics, the Urdu 
to Hindi scriptural translation system is classified 
as ?OK? by the scale of Figure 7 for HU Test set 
2 and 3. It is classifies as ?NULL? for HU Test 
Set 1. According to the scale of Figure 8, it is 
classified as ?NULL? for all three test sets. 
For Punjabi scriptural translation, we also de-
veloped two types of output default and 
processed for Gurmukhi to Shahmukhi transla-
tion. In the reverse direction, it has two types of 
inputs, one with diacritics and the other without 
diacritics. Table 11 and 12 shows results of Pun-
jabi scriptural translation. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
PU 84.2% 27.8% 85.2% 29.9% 
Table 11: Gurmukhi to Shahmukhi scriptural transla-
tion results 
 
 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
PU 98.8% 90.3% 67.3% 6.4% 
Table 12: Shahmukhi to Gurmukhi scriptural 
translation results 
Compared to the Hindi?Urdu pair, the Punja-
bi/Shahmukhi?Punjabi/Gurmukhi pair is compu-
tationally less hard. The post-processing to the 
default out of the finite-state scriptural transla-
tion systems for Punjabi/Gurmukhi to Punja-
bi/Shahmukhi also helps to gain an increase of 
approximately 1% and 2% at word and sentence 
levels respectively. The Shahmukhi to Gurmukhi 
scriptural translation system is classified as 
?GOOD? by both scales of Figure 7 and 8. Thus 
the usability of the Punjabi finite-state scriptural 
translation system is higher than the Hindi?Urdu 
finite-state scriptural translation system. 
In the reverse direction, the Shahmukhi to 
Gurmukhi scriptural translation system gives an 
accuracy of 98.8% and 67.3% for the Shahmukhi 
input text with and without diacritics respective-
ly. For the Shahmukhi input text with diacritics, 
the scriptural translation system is classified as 
?EXCELLENT? by both scales. On the other 
hand, it is classified as ?NULL? according to the 
scale of Figure 8 for the Shahmukhi input text 
without diacritical marks. 
Similar to Hindi?Urdu and Punjabi finite-state 
scriptural translation, we have applied our finite-
state system to the Seraiki test set. Here again, 
we have developed a processed Serai-
ki/Shahmukhi output from the default output of 
our finite-state system by removing the diacrit-
ics. The results are given in Table 13 and 14. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
SE 81.3% 19.4% 83.7% 20.3% 
Table 13: Seraiki/Devanagari to Seraiki/Shahmukhi 
scriptural translation results 
 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
SE 95.2% 76.4% 58.6% 8.6% 
Table 14: Seraiki/Shahmukhi to Seraiki/Devanagari 
scriptural translation results 
In the case of the Seraiki/Devanagari to Serai-
ki/Shahmukhi scriptural translation system, the 
post-processing also helps to gain an increase in 
word accuracy of approximately 1 to 2 percent 
798
both at the word and the sentence levels. The 
accuracy for both the default and the processed 
Seraiki/Shahmukhi outputs is also more than 
80% at word level. The system is classified as 
?GOOD? and ?GOOD ENOUGH? according to 
the scale of Figure 7 and 8 respectively. 
The absence of diacritical marks in the Serai-
ki/Shahmukhi has a very bad effect on the accu-
racy of the finite-state scriptural translation sys-
tem. The scriptural translation system is classi-
fied as ?NULL? for the Seraiki/Shahmukhi input 
text without diacritics. 
7 Conclusion 
Finite-state methods are robust and efficient to 
implement scriptural translation rules in a very 
precise and compact manner. 
The missing information and the diacritical 
marks in the source text proved to be very criti-
cal, crucial and important for achieving high and 
accurate results. The above results support our 
hypothesis that lack of important information in 
the source texts considerably lowers the quality 
of scriptural translation. They are crucial and 
their absence in the input texts decreases the per-
formance considerably, from more than 80% to 
less than 60% at word level. Thus restoration of 
the missing information and the diacritical marks 
or reducing the effect of their absence on the 
scriptural translation is one of the major ques-
tions for further study and work. 
In general, only word accuracy rates are re-
ported. We have observed that only word accura-
cy rates may depict a good performance, but the 
performance of the same system at sentence-
level may be not very good. Therefore, subjec-
tive evaluations and usage of translation results 
in real life should also be considered while eva-
luating the translation quality. 
Acknowledgments 
This study is supported by Higher Education Com-
mission (HEC), Government of Pakistan under its 
overseas PhD scholarship scheme. We are also thank-
ful to Digital South Asian Library, University of Chi-
cago for sharing Platts dictionary data (Platts, 1884). 
References  
AbdulJaleel, N. and L. S. Larkey. 2003. Statistical 
Transliteration for English-Arabic Cross Language 
Information Retrieval. 12th international 
Conference on information and Knowledge 
Management (CIKM 03), New Orleans. 139-146. 
Al-Onaizan, Y. and K. Knight. 2002. Machine 
Transliteration of Names in Arabic Text. 
Workshop on Computational Approaches To 
Semitic Languages, the 40th Annual Meeting of 
the ACL, Philadelphia, Pennsylvania, 1-13. 
Arbabi, M., S. M. Fischthal, V. C. Cheng and E. Bart. 
1994. Algorithms for Arabic Name 
Transliteration. IBM J. Res. Dev. 38(2): 183-193. 
Boitet, C. and F. X. Tch?ou. 1990. On a Phonetic and 
Structural Encoding of Chinese Characters in 
Chinese texts. ROCLING III, Taipeh. 73-80. 
Fujii, A. and T. Ishikawa. 2001. Japanese/English 
Cross-Language Information Retrieval: 
exploration of query translation and transliteration. 
Computers and the Humanities 35(4): 389-420. 
Halpern, J. 2002. Lexicon-based Orthographic 
Disambiguation in CJK Intelligent Information 
Retrieval. 3rd workshop on Asian language 
resources and international standardization, the 
19th International Conference on Computational 
Linguistics (COLING), Taipei, Taiwan. 1-7. 
Hieronymus, J. 1993. ASCII Phonetic Symbols for the 
World's Languages: Worldbet. AT&T Bell 
Laboratories. 
Jeong, K. S., S. H. Myaeng, J. S. Lee and K.-S. Choi. 
1999. Automatic Identification and Back-
transliteration of Foreign Words for Information 
Retrieval. Information Processing and 
Management 35: 523-540. 
Kang, B. and K. Choi. 2000. Automatic 
Transliteration and Back Transliteration by 
Decision Tree Learning. 2nd International 
Conference on Evaluation and Language 
Resources (ELRC), Athens. 
Kaplan, R. M. and M. Kay. 1994. Regular Models of 
Phonological Rule Systems.  20(3). 
Knight, K. and Y. Al-Onaizan. 1998. Translation with 
Finite-State Devices 3rd Conference of the 
Association for Machine Translation in the 
Americas on Machine Translation and the 
Information Soup (AMTA-98), Pennsylvania. 
421-437. 
Lee, J. S. and K. S. Choi. 1998. English to Korean 
Statistical Transliteration for Information 
Retrieval. Computer Processing of Oriental 
languages 12(1): 17-37. 
Malik, M. G. A. 2005. Towards a Unicode 
Compatible Punjabi Character Set. 27th 
Internationalization and Unicode Conference, 
Berlin. 
Malik, M. G. A., L. Besacier, C. Boitet and P. 
Bhattacharyya. 2009. A Hybrid Model for Urdu 
Hindi Transliteration. Joint conference of the 47th 
Annual Meeting of the Association of 
Computational Linguistics and the 4th 
799
International Joint Conference on Natural 
Language Processing of the Asian Federation of 
NLP ACL/IJCNLP Workshop on Named Entities 
(NEWS-09), Singapore, 177?185. 
Malik, M. G. A., C. Boitet and P. Bhattacharyya. 
2008. Hindi Urdu Machine Transliteration using 
Finite-state Transducers. 22nd International 
Conference on Computational Linguistics 
(COLING), Manchester, 537-544. 
Malik, M. G. A., C. Boitet and P. Bhattacharyya. 
2010. Analysis of Noori Nast'aleeq for Major 
Pakistani Languages. 2nd Workshop on Spoken 
Language Technologies for Under-resourced 
Languages SLTU-2010, Penang, Malaysia. 
Mohri, M. 1997. Finite-state Transducers in Language 
and Speech Processing.  23(2). 
Pirkola, A., J. Toivonen, H. Keskustalo, K. Visala and 
K. J?rvelin. 2003. Fuzzy Translation of Cross-
lingual Spelling Variants. 26th Annual 
international ACM SIGIR Conference on 
Research and Development in informaion 
Retrieval, Toronto. 
Platts, J. T. 1884. A Dictionary of Urdu, Classical 
Hindi and Englsih. W. H. Allen & Co. 
Rahman, T. 1997. Language and Politics in Pakistan. 
Oxford University Press, Lahore.  
Rahman, T. 2004. Language Policy and Localization 
in Pakistan: Proposal for a Paradigmatic Shift. 
Crossing the Digital Divide, SCALLA Conference 
on Computational Linguistics, Katmandu. 
Rai, A. 2000. Hindi Nationalism. Orient Longman 
Private Limited, New Delhi.  
Roche, E. and Y. Schabes, Eds. 1997. Finite-state 
Language Processing. MIT Press,  Cambridge. 
Sakai, T., A. Kumano and T. Manabe. 2002. 
Generating Transliteration Rules for Cross-
language Information Retrieval from Machine 
Translation Dictionaries. IEEE Conference on 
Systems, Man and Cybernatics. 
Stall, B. and K. Knight. 1998. Translating Names and 
Technical Terms in Arabic Text. Workshop on 
Computational Approaches to Semitic Languages, 
COLING/ACL, Montreal, 34-41. 
Virga, P. and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-language Applications. 
26th Annual international ACM SIGIR 
Conference on Research and Development in 
informaion Retrieval, Toronto. 
Wells, J. C. 1995. Computer-coding the IPA: a 
proposed extension of SAMPA. University 
College London. 
Yan, Q., G. Gregory and A. E. David. 2003. 
Automatic Transliteration for Japanese-to-English 
Text Retrieval. 26th annual international ACM 
SIGIR conference on Research and development 
in information retrieval, 353-360. 
Zia, K. 1999. Standard Code Table for Urdu. 4th 
Symposium on Multilingual Information 
Processing (MLIT-4), Yangon. 
800
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081?1091,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing WordNet Senses for Supervised Sentiment Classification
Balamurali A R1,2 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
Traditional approaches to sentiment classifica-
tion rely on lexical features, syntax-based fea-
tures or a combination of the two. We pro-
pose semantic features using word senses for
a supervised document-level sentiment classi-
fier. To highlight the benefit of sense-based
features, we compare word-based representa-
tion of documents with a sense-based repre-
sentation where WordNet senses of the words
are used as features. In addition, we highlight
the benefit of senses by presenting a part-of-
speech-wise effect on sentiment classification.
Finally, we show that even if a WSD engine
disambiguates between a limited set of words
in a document, a sentiment classifier still per-
forms better than what it does in absence of
sense annotation. Since word senses used as
features show promise, we also examine the
possibility of using similarity metrics defined
on WordNet to address the problem of not
finding a sense in the training corpus. We per-
form experiments using three popular similar-
ity metrics to mitigate the effect of unknown
synsets in a test corpus by replacing them with
similar synsets from the training corpus. The
results show promising improvement with re-
spect to the baseline.
1 Introduction
Sentiment Analysis (SA) is the task of prediction of
opinion in text. Sentiment classification deals with
tagging text as positive, negative or neutral from the
perspective of the speaker/writer with respect to a
topic. In this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Traditional supervised approaches for SA have
explored lexeme and syntax-level units as features.
Approaches using lexeme-based features use bag-
of-words (Pang and Lee, 2008) or identify the
roles of different parts-of-speech (POS) like adjec-
tives (Pang et al, 2002; Whitelaw et al, 2005).
Approaches using syntax-based features construct
parse trees (Matsumoto et al, 2005) or use text
parsers to model valence shifters (Kennedy and
Inkpen, 2006).
Our work explores incorporation of semantics
in a supervised sentiment classifier. We use the
synsets in Wordnet as the feature space to represent
word senses. Thus, a document consisting of
words gets mapped to a document consisting of
corresponding word senses. Harnessing WordNet
senses as features helps us address two issues:
1. Impact of WordNet sense-based features on the
performance of supervised SA
2. Use of WordNet similarity metrics to solve the
problem of features unseen in the training cor-
pus
The first points deals with evaluating sense-based
features against word-based features. The second is-
sue that we address is in fact an opportunity to im-
prove the performance of SA that opens up because
of the choice of sense space. Since sense-based
features prove to generate superior sentiment clas-
sifiers, we get an opportunity to mitigate unknown
1081
synsets in the test corpus by replacing them with
known synsets in the training corpus. Note that such
replacement is not possible if word-based represen-
tation were used as it is not feasible to make such a
large number of similarity comparisons.
We use the corpus by Ye et al (2009) that con-
sists of travel domain reviews marked as positive or
negative at the document level. Our experiments on
studying the impact of Wordnet sense-based features
deal with variants of this corpus manually or auto-
matically annotated with senses. Besides showing
the overall impact, we perform a POS-wise analysis
of the benefit to SA. In addition, we compare the ef-
fect of varying training samples on a sentiment clas-
sifier developed using word based features and sense
based features. Through empirical evidence, we also
show that disambiguating some words in a docu-
ment also provides a better accuracy as compared
to not disambiguating any words. These four sets of
experiments highlight our hypothesis that WordNet
senses are better features as compared to words.
Wordnet sense-based space allows us to mitigate
unknown features in the test corpus. Our synset re-
placement algorithm uses Wordnet similarity-based
metrics which replace an unknown synset in the test
corpus with the closest approximation in the training
corpus. Our results show that such a replacement
benefits the performance of SA.
The roadmap for the rest of the paper is as fol-
lows: Existing related work in SA and the differ-
entiating aspects of our work are explained in sec-
tion 2 Section 3 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
in section 4. Our experiments have been described
in section 5. In section 6, we present our results
and related discussions. Section 7 analyzes some of
the causes for erroneous classification. Finally, sec-
tion 8 concludes the paper and points to future work.
2 Related Work
This work studies the benefit of a word sense-based
feature space to supervised sentiment classification.
However, a word sense-based feature space is feasi-
ble subject to verification of the hypothesis that sen-
timent and word senses are related. Towards this,
Wiebe and Mihalcea (2006) conduct a study on hu-
man annotation of 354 words senses with polarity
and report a high inter-annotator agreement. The
work in sentiment analysis using sense-based fea-
tures, including ours, assumes this hypothesis that
sense decides the sentiment.
The novelty of our work lies in the following.
Firstly our approach is distinctly. Akkaya et al
(2009) and Martn-Wanton et al (2010) report per-
formance of rule-based sentiment classification us-
ing word senses. Instead of a rule-based implemen-
tation, We used supervised learning. The supervised
nature of our approach renders lexical resources un-
necessary as used in Martn-Wanton et al (2010).
Rentoumi et al (2009) suggest using word senses
to detect sentence level polarity of news headlines.
The authors use graph similarity to detect polarity of
senses. To predict sentence level polarity, a HMM
is trained on word sense and POS as the observa-
tion. The authors report that word senses partic-
ularly help understanding metaphors in these sen-
tences. Our work differs in terms of the corpus and
document sizes in addition to generating a general
purpose classifier.
Another supervised approach of creating an emo-
tional intensity classifier using concepts as features
has been reported by Carrillo de Albornoz et al
(2010). This work is different based on the feature
space used. The concepts used for the purpose are
limited to affective classes. This restricts the size of
the feature space to a limited set of labels. As op-
posed to this, we construct feature vectors that map
to a larger sense-based space. In order to do so, we
use synset offsets as representation of sense-based
features.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) perform sen-
timent classification of individual sentences. How-
ever, we consider a document as a unit of sentiment
classification i.e. our goal is to predict a document
on the whole as positive or negative. This is different
from Pang and Lee (2004) which suggests that sen-
timent is associated only with subjective content. A
document in its entirety is represented using sense-
based features in our experiments. Carrillo de Al-
bornoz et al (2010) suggests expansion using Word-
Net relations which we also follow. This is a benefit
that can be achieved only in a sense-based space.
1082
3 Features based on WordNet Senses
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This pa-
per refers to synset offset as synset identifiers or sim-
ply, senses.
This section first gives the motivation for using
word senses and then, describes the approaches that
we use for our experiments.
3.1 Motivation
Consider the following sentences as the first sce-
nario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity of
the first sentence from the negative sense of ?fell? in
it while the user will state that the second sentence
does not carry any sentiment. This implies that there
is at least one sense of the word ?fell? that carries
sentiment and at least one that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
3.2 Sense versus Lexeme-based Feature
Representation
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on two
subsets of the corpus, the details of which are given
in Section 5.1. This is done to determine the ideal
case scenario- the skyline performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. Word senses that have been manually annotated
(M)
2. Word senses that have been annotated by an au-
tomatic WSD (I)
1083
3. Manually annotated word senses and words
(both separately as features) (Words +
Sense(M))
4. Automatically annotated word senses and
words (both separately as features) (Words +
Sense(I))
Our first set of experiments compares the four
feature representations to find the feature represen-
tation with which sentiment classification gives the
best performance. W+S(M) and W+S(I) are used to
overcome non-coverage of WordNet for some noun
synsets. In addition to this, we also present a part-
of-speech-wise analysis of benefit to SA as well as
effect of varying the training samples on sentiment
classification accuracy.
3.3 Partial disambiguation as opposed to no
disambiguation
The state-of-the-art automatic WSD engine that we
use performs (approximately) with 70% accuracy on
tourism domain (Khapra et al, 2010). This means
that the performance of SA depends on the perfor-
mance of WSD which is not very high in case of the
engine we use.
A partially disambiguated document is a docu-
ment which does not contain senses of all words.
Our hypothesis is that disambiguation of even few
words in a document can give better results than
no disambiguation. To verify this, we create differ-
ent variants of the corpus by disambiguating words
which have candidate senses within a threshold. For
example, a partially disambiguated variant of the
corpus with threshold 3 for candidate senses is cre-
ated by disambiguating words which have a maxi-
mum of three candidate senses. These synsets are
then used as features for classification along with
lexeme based features. We conduct multiple experi-
ments using this approach by varying the number of
candidate senses.
4 Advantage of senses: Similarity Metrics
and Unknown Synsets
4.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in Algo-
rithm 1. The algorithm follows from the fact that the
similarity value for a synset with itself is maximum.
4.2 Similarity metrics used
We conduct different runs of the replacement
algorithm using three similarity metrics, namely
LIN?s similarity metric, Lesk similarity metric and
Leacock and Chodorow (LCH) similarity metric.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B) (1)
1084
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
C = temp concept ;
replace synset corpus(C,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
( len(A,B)
2D
)
(2)
5 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
5.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset of the corpus showed 91% sense
overlap. The manually annotated corpus consists of
34508 words with 6004 synsets.
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjective
POS based synsets can be detrimental to classifica-
tion since adjectives are known to express direct sen-
timent (Pang et al, 2002). Hence, in the context of
sentiment classification, disambiguation of adjective
synsets is more critical as compared to disambigua-
tion of noun synsets.
5.2 Experimental setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are not
stemmed since stemming is known to be detrimen-
tal to sentiment classification (Leopold and Kinder-
mann, 2002). To conduct the experiments based on
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
1085
Feature Representations Accuracy(%) PF NF PP NP PR NR
Words (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-
Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
the synset representation, words in the corpus are an-
notated with synset identifiers along with POS cat-
egory identifiers. For automatic sense disambigua-
tion, we used the trained IWSD engine from Khapra
et al (2010). These synset identifiers along with
POS category identifiers are then used as features.
For replacement using semantic similarity measures,
we used WordNet::Similarity 2.05 package by Ped-
ersen et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
score.
6 Results and Discussions
6.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Baseline).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
While reported results suggest that it is more diffi-
cult to detect negative sentiment than positive senti-
ment (Gindl and Liegl, 2008), our results show that
negative recall increases by around 8% in case of
sense-based representation of documents.
The combined model of words and manually an-
notated senses (Words + Senses (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
1086
Sens
e
81.2
4
78.3
0
66.1
4
73 .
70.0
0
80.0
0
90.0
0
50.0
0
60.0
0
racy(%)
20.0
0
30.0
0
40.0
0
Accu
0.0010.0
0
20.0
0
Adve
rb?
Verb
? PO
Wor
ds 74.99
66.8
3
.78
71.8
1
80.0
3
Nou
n?
Adje
ctive
OS?c
ateg
ory
Figure 1: POS-wise statistics of manually annotated se-
mantic space
6.2 POS-wise analysis
For each POS, we compare the performance of two
models:
? Model trained on words of only that POS
? Model trained on word senses of only that POS
Figure 1 shows the parts-of-speech-wise classifica-
tion accuracy of sentiment classification for senses
(manual) and words. In the lexeme space, adjectives
directly impact the classification performance. But it
can be seen that disambiguation of adverb and verb
synsets impact the performance of SA higher than
disambiguation of nouns and adjectives.
While it is believed that adjectives carry direct
sentiments, our results suggest that using adjectives
alone as features may not improve the accuracy. The
results prove that sentiment may be subtle at times
and not expressed directly through adjectives.
As manual sense annotation is an effort and cost
intensive process, the parts-of-speech-wise results
suggest improvements expected from an automatic
WSD engine so that it can aid sentiment classifica-
tion. Table 1 suggests that the WSD engine works
better for noun synsets compared to adjective and
adverb synsets. While this is expected in a typical
WSD setup, it is the adverbs and verbs that are more
important for detecting sentiment in semantics space
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
than nouns. The future WSD systems will have to
show an improvement in their accuracy with respect
to adverb and verb synsets.
Sense Words
POS Category PF NF PF NF
Adverb 79.65 80.45 70.25 73.68
Verb 75.50 79.28 62.23 63.12
Noun 73.39 75.40 69.77 72.55
Adjective 63.11 65.03 78.29 79.20
Table 3: POS-wise F-score for sense (M) and Words;PF-
Positive F-score(%), NF- Negative F-score (%)
Table 3 shows the positive and negative F-score
statistics with respect to different POS. Detection
of negative reviews using lexeme space is difficult.
POS-wise statistics also suggest the same. It should
be noted that adverb and verb synsets play an im-
portant role in negative class detection. Thus, an au-
tomatic WSD engine should give importance to the
correct disambiguation of these POS categories.
6.3 Effect of size of training corpus
#Training
Documents
W M I W+S(M) W+S(I)
100 76.5 87 79.5 82.5 79.5
200 81.5 88.5 82 90 84
300 79.5 92 81 89.5 82
400 82 90.5 81 94 85.5
500 83.5 91 85 96 82.5
Table 4: Accuracy (%) with respect to number of training
documents; W: Words, M: Manual Annotation, I: IWSD-
based sense annotation, W+S(M): Word+Senses (Manual
annotation), W+S(I): Word+Senses(IWSD-based sense
annotation)
From table 2, the benefit of sense disambigua-
tion to sentiment prediction is evident. In addition,
Table 4 shows variation of classification accuracy
with respect to different number of training sam-
ples based on different approaches of annotation ex-
plained in previous sections. The results are based
on a blind set of 90 test samples from both the po-
larity labels 4.
4No cross validation is performed for this experiment
1087
Compared to lexeme-based features, manually an-
notated sense based features give better performance
with lower number of training samples. IWSD is
also better than lexeme-based features. A SA sys-
tem trained on 100 training samples using manually
annotated senses gives an accuracy of 87%. Word-
based features never achieve this accuracy. An
IWSD-based system requires lesser samples when
compared to lexeme space for an equivalent accu-
racy. Note that model based on words + senses(M)
features achieve an accuracy of 96% on this test set.
This implies that the synset space, in addition
to benefit to sentiment prediction in general, re-
quires lesser number of training samples in order to
achieve the accuracy that lexeme space can achieve
with a larger number of samples.
6.4 Effect of Partial disambiguation
Figure 2 shows the accuracy, positive F-score and
negative F-score with respect to different thresholds
of candidate senses for partially disambiguated doc-
uments as described in Section 3.3. We compare the
performance of these documents with word-based
features (B) and sense-based features based on man-
ually (M) or automatically obtained senses (I). Note
that Sense (I) and Sense (M) correspond to com-
pletely disambiguated documents.
In case of partial disambiguation using manual
annotation, disambiguating words with less than
three candidate senses performs better than others.
For partial disambiguation that relies on an auto-
matic WSD engine, a comparable performance to
full disambiguation can be obtained by disambiguat-
ing words which have a maximum of four candidate
senses.
As expected, completely disambiguated docu-
ments provide the best F-score and accuracy fig-
ures5. However, a performance comparable to com-
plete disambiguation can be attained by disam-
biguating selective words.
Our results show that even if highly ambiguous
(in terms of senses) words are not disambiguated by
a WSD engine, the performance of sentiment classi-
fication improves.
5All results are statistically significant with respect to base-
line
6se
nse
s(M
)
6?se
nse
s?(I
)Ne
gat
ive
?Fsc
ore
Pos
5?se
nse
s?(M
)
5?se
nse
s?(I
)
6?se
nse
s?(M
)
3?se
nse
s?(I
)
4?se
nse
s?(M
)
4?se
nse
s?(I
)
2?se
nse
s?(M
)
2?se
nse
s?(I
)
3?se
nse
s?(M
)
Wo
rds
?(B)
Sen
se(
M)
Sen
se(
I) 81
.00
82.
00
83.
00
84.
00
85.
00
Fsc
ore
/itiv
e?F
sco
re
Acc
ura
cy
86.
00
87.
00
88.
00
89.
00
90.
00
91.
00
/Ac
cur
acy
?(%
)
Figure 2: Partial disambiguation statistics: Accu-
racy,Positive F-score, Negative F-score variation with re-
spect to sense disambiguation difficult level is shown.
Words(B): baseline system
6.5 Synset replacement using similarity metrics
Table 5 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 4. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach6. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
6Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
1088
Feature Representation Similarity
Metric
Accuracy PF NF PP NP PR NR
Words (Baseline) NA 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Words + Sense(M) NA 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Words + Sense(I) NA 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Words + Sense (M) LCH 90.60 90.20 90.85 92.85 88.61 87.70 93.21
Words + Sense(M) LIN 90.70 90.26 90.97 93.17 88.50 87.53 93.57
Words + Sense (M) Lesk 91.12 90.70 91.38 93.55 88.97 88.03 93.92
Words + Sense (I) LCH 85.66 85.85 85.52 85.67 85.76 86.02 85.28
Words + Sense(I) LIN 86.16 86.37 86.00 86.06 86.40 86.69 85.61
Words + Sense (I) Lesk 86.25 86.41 86.10 86.31 86.26 86.52 85.95
Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset and
words;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-
Positive Recall (%), NR-Negative Recall (%)
Top information
content features
(in %)
IWSD
synset #
Manual
synsets #
Match
synset #
Match
Synsets (%)
Unmatched
Synset(%)
10 601 722 288 39.89 60.11
20 1199 1443 650 45.05 54.95
30 1795 2165 1005 46.42 53.58
40 2396 2889 1375 47.59 52.41
50 2997 3613 1730 47.88 52.12
Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno-
tated corpora
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated
as LIN or LCH.
Thus, we observe a marginal improvement by us-
ing similarity-based metrics for WordNet. A good
metric which covers all POS categories can provide
substantial improvement in the classification accu-
racy.
7 Error Analysis
For sentiment classification based on semantic
space, we classify the errors into four categories.
The examples quoted are from manual evaluation of
the results.
1. Effect of low disambiguation accuracy of IWSD
engine: SA using automatic sense annotation
depends on the annotation system used. To as-
sess the impact of IWSD system on sentiment
classification, we compare the feature set based
on manually annotated senses with the feature
set based on automatically annotated senses.
We compare the most informative features of
the two classifiers. Table 6 shows the number
of top informative features (synset) selected as
the percentage of total synset features present
when the semantic representation of documen-
tation is used. The matched synset column rep-
resents the number of IWSD synsets that match
1089
with manually annotated synsets.
The number of top performing features is more
in case of manually annotated synsets. This
can be attributed to the total number of synsets
tagged in the two variant of the corpus. The re-
duction in the performance of SA for automati-
cally annotated senses is because of the number
of unmatched synsets.
Thus, although the accuracy of IWSD is cur-
rently 70%, the table indicates that IWSD can
match the performance of manually annotated
senses for SA if IWSD is able to tag correctly
those top information content synsets. This as-
pect needs to be investigated further.
2. Negation Handling: For the purpose of this
work, we concentrate on words as units for sen-
timent determination. Syntax and its contri-
bution in understanding sentiment is neglected
and hence, positive documents which con-
tain negations are wrongly classified as nega-
tive. Negation may be direct as in the excerpt
?....what is there not to like about Vegas.? or
may be double as in the excerpt?...that aren?t
insecure?.
3. Interjections and WordNet coverage: Recent
informal words are not covered in WordNet and
hence, do not get disambiguated. The same
is the case for interjections like ?wow?,?duh?
which sometimes carry direct sentiment. Lex-
ical resources which include them can be used
to incorporate information about these lexical
units.
4. Document Specificity: The assumption under-
lying our analysis is that a document contains
description of only one topic. However, re-
views are generic in nature and tend to express
contrasting sentiment about sub-topics . For
example, a travel review about Paris can talk
about restaurants in Paris, traffic in Paris, pub-
lic behaviour, etc. with opposing sentiments.
Assigning an overall sentiment to a document
is subjective in such cases.
8 Conclusion & Future Work
This work presents an empirical benefit of WSD to
sentiment analysis. The study shows that supervised
sentiment classifier modeled on wordNet senses per-
form better than word-based features. We show how
the performance impact differs for different auto-
matic and manual techniques, parts-of-speech, dif-
ferent training sample size and different levels of
disambiguation. In addition, we also show the bene-
fit of using WordNet based similarity metrics for re-
placing unknown features in the test set. Our results
support the fact that not only does sense space im-
prove the performance of a sentiment classification
system, but also opens opportunities for improve-
ment using better similarity metrics.
Incorporation of syntactical information along
with semantics can be an interesting area of work.
More sophisticated features which include the two
need to be explored. Another line of work is in the
context of cross-lingual sentiment analysis. Current
solutions are based on machine translation which is
very resource-intensive. Using a bi-lingual dictio-
nary which maps WordNet across languages, such a
machine translation sub-system can be avoided.
Acknowledgment
We thank Jaya Saraswati and Rajita Shukla from
CFILT Lab, IIT Bombay for annotating the dataset
used for this work. We also thank Mitesh Khapra
and Salil Joshi, IIT Bombay for providing us with
the IWSD engine for the required experiments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
1090
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
1091
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 128?138, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Towards Efficient Named-Entity Rule Induction for Customizability
Ajay Nagesh1,2
1IITB-Monash Research Academy
ajaynagesh@cse.iitb.ac.in
Ganesh Ramakrishnan
2IIT Bombay
ganesh@cse.iitb.ac.in
Laura Chiticariu
IBM Research - Almaden
chiti@us.ibm.com
Rajasekar Krishnamurthy
IBM Research - Almaden
rajase@us.ibm.com
Ankush Dharkar
SASTRA University
ankushdharkar@cse.sastra.edu
Pushpak Bhattacharyya
IIT Bombay
pb@cse.iitb.ac.in
Abstract
Generic rule-based systems for Information
Extraction (IE) have been shown to work
reasonably well out-of-the-box, and achieve
state-of-the-art accuracy with further domain
customization. However, it is generally rec-
ognized that manually building and customiz-
ing rules is a complex and labor intensive pro-
cess. In this paper, we discuss an approach
that facilitates the process of building cus-
tomizable rules for Named-Entity Recognition
(NER) tasks via rule induction, in the Annota-
tion Query Language (AQL). Given a set of
basic features and an annotated document col-
lection, our goal is to generate an initial set
of rules with reasonable accuracy, that are in-
terpretable and thus can be easily refined by
a human developer. We present an efficient
rule induction process, modeled on a four-
stage manual rule development process and
present initial promising results with our sys-
tem. We also propose a simple notion of ex-
tractor complexity as a first step to quantify
the interpretability of an extractor, and study
the effect of induction bias and customization
of basic features on the accuracy and complex-
ity of induced rules. We demonstrate through
experiments that the induced rules have good
accuracy and low complexity according to our
complexity measure.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
Generic NER rules have been shown to work reason-
ably well-out-of-the-box, and with further domain
customization (Chiticariu et al2010b), achieve
quality surpassing state-of-the-art results. Table 1
System Dataset F?=1
Generic Customized
GATE ACE2002 57.8 82.2
ACE 2005 57.32 88.95
SystemT CoNLL 2003 64.15 91.77
Enron 76.53 85.29
Table 1: Quality of generic vs. customized rules.
summarizes the quality of NER rules out-of-the-box
and after domain customization in the GATE (Cun-
ningham et al2011) and SystemT (Chiticariu et
al., 2010a) systems, as reported in (Maynard et al
2003) and (Chiticariu et al2010b) respectively.
Rule-based systems are widely used in enterprise
settings due to their explainability. Rules are trans-
parent, which leads to better explainability of errors.
One can easily identify the cause of a false positive
or negative, and refine the rules without affecting
other correct results identified by the system. Fur-
thermore, rules are typically easier to understand by
an IE developer and can be customized for a new
domain without requiring additional labeled data.
Typically, a rule-based NER system consists of a
combination of four categories of rules (Chiticariu et
al., 2010b): (1) Basic Feature (BF) rules to identify
components of an entity such as first name and last
name. (2) Candidate definition (CD) rules to iden-
tify complete occurrences of an entity by combining
the output of multiple BF rules, e.g., first name fol-
lowed by last name is a person candidate. (3) Candi-
date refinement (CR) rules to refine candidates gen-
erated by CD rules. E.g., discard candidate persons
contained within organizations. (4) Consolidation
rules (CO) to resolve overlapping candidates gener-
ated by multiple CD and CR rules.
A well-known drawback that influences the
adoptability of rule-based NER systems is the man-
128
ual effort required to build the rules. A common ap-
proach to address this problem is to build a generic
NER extractor and then customize it for specific do-
mains. While this approach partially alleviates the
problem, substantial manual effort (in the order of
several person weeks) is still required for the two
stages as reported in (Maynard et al2003; Chiti-
cariu et al2010b). In this paper, we present initial
work towards facilitating the process of building a
generic NER extractor using induction techniques.
Specifically, given as input an annotated docu-
ment corpus, a set of BF rules, and a default CO
rule for each entity type, our goal is to generate a
set of CD and CR rules such that the resulting ex-
tractor constitutes a good starting point for further
refinement by a developer. Since the generic NER
extractor has to be manually customized, a major
challenge is to ensure that the generated rules have
good accuracy, and, at the same time, that they are
not too complex, and consequently interpretable.
The main contributions in this paper are
1. An efficient system for NER rule induction, us-
ing a highly expressive rule language (AQL) as
the target language. The first phase of rule in-
duction uses a combination of clustering and
relative least general generalization (RLGG)
techniques to learn CD rules. The second phase
identifies CR rules using a propositional rule
learner like JRIP to learn accurate composi-
tions of CD rules.
2. Usage of induction biases to enhance the inter-
pretability of rules. These biases capture the
expertise gleaned from manual rule develop-
ment and constrain the search space in our in-
duction system.
3. Definition of an initial notion of extractor com-
plexity to quantify the interpretability of an ex-
tractor and to guide the process of adding in-
duction biases to favor learning less complex
extractors. This is to ensure that the rules are
easily customizable by the developer.
4. Scalable induction process through usage of
SystemT, a state-of-the-art IE system which
serves as a highly efficient theorem prover for
AQL, and performance optimizations such as
clustering of examples and parallelizing vari-
ous modules (E.g.: propositional rule learning).
Roadmap We first describe preliminaries on Sys-
temT and AQL (Section 3) and define the target lan-
guage for our induction algorithm and the notion of
rule complexity (Section 4). We then present our
approach for inducing CD and CR rules, and dis-
cuss induction biases that would favor interpretabil-
ity (Section 5), and discuss the results of an empir-
ical evaluation (Section 6). We conclude with av-
enues for improvement in the future (Section 7).
2 Related Work
Existing approaches to rule induction for IE focus
on rule-based systems based on the cascading gram-
mar formalism exemplified by the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998), where rules are specified as
a sequence of basic features that describe an en-
tity, with limited predicates in the context of an
entity mention. Patel et al2009) and Soderland
(1999) elaborate on top-down techniques for induc-
tion of IE rules, whereas (Califf and Mooney, 1997;
Califf and Mooney, 1999) discuss a bottom-up IE
rule induction system that uses the relative least gen-
eral generalization (RLGG) of examples1. However,
in all these systems, the expressivity of the rule-
representation language is restricted to that of cap-
turing sequence information. As discussed in Sec-
tion 3, contextual clues and higher level rule inter-
actions such as filtering and join are very difficult,
if not impossible to express in such representations
without resorting to custom code. Learning higher
level interactions between rules has received little
attention. Our technique for learning higher level in-
teractions is similar to the induction of ripple down
rules (Gaines and Compton, 1995), which, to the
best of our knowledge, has not been previously ap-
plied to IE. A framework for refining AQL extractors
based on an annotated document corpus described
in (Liu et al2010). We present complementary
techniques for inducing an initial extractor that can
be automatically refined in this framework.
3 Preliminaries
SystemT is a declarative IE system based on an al-
gebraic framework. In SystemT, developers write
rules in AQL. To represent annotations in a docu-
1Our work also makes use of RLGGs but computes these
generalizations for clusters of examples, instead of pairs.
129
Figure 1: Example Person extractor in AQL
ment, AQL uses a simple relational data model with
three types: a span is a region of text within a docu-
ment identified by its ?begin? and ?end? positions; a
tuple is a fixed-size list of spans; a relation, or view,
is a multi-set of tuples, where every tuple in the view
must be of the same size.
Figure 1 shows a portion of a Person extractor
written in AQL. The basic building block of AQL
is a view. A view is a logical description of a set of
tuples in terms of (i) the document text (denoted as a
special view called Document), and (ii) the contents
of other views, as specified in the from clauses of
each statement. Figure 1 also illustrates five of the
basic constructs that can be used to define a view,
and which we explain next. The complete specifica-
tion can be found in the AQL manual (IBM, 2012).
In the paper, we will use ?rules? and ?views? inter-
changeably.
The extract statement specifies basic character-
level extraction primitives such as regular expression
and dictionary matching over text, creating a tuple
for each match. As an example, rule R1 uses the ex-
tract statement to identify matches (Caps spans) of a
regular expression for capitalized words.
The select statement is similar to the SQL select
statement but it contains an additional consolidate
on clause (explained further), along with an exten-
sive collection of text-specific predicates. Rule R5
illustrates a complex example: it selects First spans
immediately followed within zero tokens by a Last
span, where the latter is also a Caps span. The
two conditions are specified using two join predi-
cates: FollowsTok and Equals respectively. For each
triplet of First, Last and Caps spans satisfying the two
predicates, the CombineSpans built-in scalar func-
tion in the select clause constructs larger PersonFirst-
Last spans that begin at the begin position of the First
span, and end at the end position of the Last (also
Caps) span.
The union all statement merges the outputs of two
or more statements. For example, rule R6 unions
person candidates identified by rules R4 and R5.
The minus statement subtracts the output of one
statement from the output of another. For example,
rule R8 defines a view PersonAll by filtering out Per-
sonInvalid tuples from the set of PersonCandidate tu-
ples. Notice that rule R7 used to define the view Per-
sonInvalid illustrates another join predicate of AQL
called Overlaps, which returns true if its two argu-
ment spans overlap in the input text. Therefore, at
a high level, rule R8 removes person candidates that
overlap with an Organization span. (The Organization
extractor is not depicted in the figure.)
The consolidate clause of a select statement re-
moves selected overlapping spans from the indicated
column of the input tuples, according to the spec-
ified policy (for instance, ?ContainedWithin?). For
example, rule R9 retains PersonAll spans that are not
contained in other PersonAll spans.
Internally, SystemT compiles an AQL extractor
into an executable plan in the form of a graph of
operators. The formal definition of these operators
takes the form of an algebra (Reiss et al2008), sim-
ilar to relational algebra, but with extensions for text
processing. The decoupling between AQL and the
operator algebra allows for greater rule expressiv-
ity because the rule language is not constrained by
the need to compile to a finite state transducer, as in
grammar systems based on the CPSL standard. In
fact, join predicates such as Overlaps, as well as fil-
ter operations (minus) are extremely difficult to ex-
130
press in CPSL systems such as GATE without an
escape to custom code (Chiticariu et al2010b). In
addition, the decoupling between the AQL specifi-
cation of ?what? to extract from ?how? to extract
it, allows greater flexibility in choosing an efficient
execution strategy among the many possible opera-
tor graphs that may exist for the same AQL extrac-
tor. Therefore, extractors written in AQL achieve
orders of magnitude higher throughput (Chiticariu
et al2010a).
4 Induction Target Language
Our goal is to automatically generate NER extrac-
tors with good quality, and at the same time, man-
ageable complexity, so that the extractors can be fur-
ther refined and customized by the developer. To this
end, we focus on inducing extractors using the sub-
set of AQL constructs described in Section 3. We
note that we have chosen a small subset of AQL con-
structs that are sufficient to implement several com-
mon operations required for NER. However, AQL is
a much more expressive language, and incorporating
additional constructs is subject to our future work.
In this section we describe the building blocks of
our target language, and propose a simple definition
for measuring the complexity of an extractor.
Target Language. The components of the target
language are as follows, and summarized in Table 2.
Basic features (BF): BF views are specified using the
extract statement, such as rulesR1 toR3 in Figure 1.
In this paper, we assume as input a set of basic fea-
tures, consisting of dictionaries and regular expres-
sions.
Candidate definition (CD): CD views are expressed
using the select statement to combine BF views with
join predicates (e.g., Equals, FollowsTok or Over-
laps), and the CombineSpans scalar function to con-
struct larger candidate spans from input spans. Rules
R4 and R5 in Figure 1 are example CD rules. In
general, a CD view is defined as: ?Select all spans
constructed from view1, view2, . . ., viewn, such that all
join predicates are satisfied?.
Candidate refinement (CR): CR views are used to
discard spans output by the CD views that may be
incorrect. In general, a CR view is defined as: ?From
the list of spans of viewvalid subtract all those spans that
belong to viewinvalid?. viewvalid is obtained by join-
ing all the positive CD clues on the Equals predicate
and viewinvalid is obtained by joining all the nega-
tive overlapping clues with the Overlaps predicate
and subsequently ?union?ing all the negative clues.
(e.g., similar in spirit to rules R6, R7 and R8 in Fig-
ure 1, except that the subtraction is done from a sin-
gle view and not the union of multiple views).
Consolidation (CO): Finally, a select statement with
a fixed consolidate clause is used for each entity type
to remove overlapping spans from CR views. An
example CO view is defined by rule R9 in Figure 1.
Extractor Complexity. Since our goal is to gener-
ate extractors with manageable complexity, we must
introduce a quantitative measure of extractor com-
plexity, in order to (1) judge the complexity of the
extractors generated by our system, and (2) reduce
the search space considered by the induction system.
To this end, we define a simple complexity score
that is a function of the number of rules, and the
number of input views to each rule of the extrac-
tor. In particular, we define the length of rule R,
denoted as L(R), as the number of input views in
the from clause(s) of the view. For example, in Fig-
ure 1, we have L(R4) = 2 and L(R5) = 3, since
R4 and R5 have two, and respectively three views
in the from clause. Furthermore, L(R8) = 2 since
each of the two inner statements of R8 has one from
clause with a single input view. The complexity of
BF rules (e.g., R1 to R3) and CO rules (e.g., R9) is
always 1, since these types of rules have a single in-
put view. We define the complexity of extractor E,
denoted as C(E) as the sum of lengths of all rules of
E. For example, the complexity of the Person extrac-
tor from Figure 1 is 15, plus the length of all rules
involved in defining Organization, which are omitted
from the figure.
Our simple notion of rule length is motivated
by existing literature in the area of database sys-
tems (Abiteboul et al1995), where the size of a
conjunctive query is determined only by the number
of atoms in the body of the query (e.g., items in the
FROM clause), and it is independent on the number
of join variables (i.e., items in the WHERE clause),
or the size of the head of the query (e.g., items in the
SELECT clause). As such, our notion of complexity
is rather coarse, and we shall discuss its shortcom-
ings in detail in Section 6.2. However, we shall show
that the complexity score significantly reduces the
search space of our induction techniques leading to
131
Phase name AQL statements Prescription Rule Type
Basic Features extract Off-the-shelf, Learning using prior
work (Riloff, 1993; Li et al2008)
Basic Features Definition
Phase 1 (Clustering and
RLGG)
select Bottom-up learning (LGG), Top-down refine-
ment
Development of Candidate
Rules
Phase 2 (Propositional Rule
Learning)
select, union
all, minus
RIPPER, Lightweight Rule Induction Candidate Rules Filtering
Consolidation consolidate,
union all
Manually identified consolidation rules, based
on domain knowledge
Consolidation rules
Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the
phase and the corresponding type of rule in manual rule development.
simpler and smaller extractors, and therefore consti-
tutes a good basis for more comprehensive measures
of interpretability in the future.
5 Induction of Rules
Since the goal is to generate rules that can be cus-
tomized by humans, the overall structure of the in-
duced rules must be similar in spirit to what a devel-
oper following best practices would write. Hence,
the induction process is divided into multiple phases.
Figure 2 shows the correspondence between the
phases of induction and the types of rules. In Ta-
ble 2, we summarize the phases of our induction al-
gorithm, along with the subset of AQL constructs
that comprise the language of the rules learnt in that
phase, the possible methods prescribed for inducing
the rules and their correspondence with the stages in
the manual rule development.
Our induction system generates rules for two of
the four categories, namely CD and CR rules as
highlighted in Figure 2. We assume that we are
given the BFs in the form of dictionaries and reg-
ular expressions. Prior work on learning dictionar-
ies (Riloff, 1993) and regular expressions (Li et al
2008) could be leveraged to semi-automate the pro-
cess of defining the basic features.
We represent each example, in conjunction with
relevant background knowledge in the form first
order horn clauses. This background knowledge
will serve as input to our induction system. The
first phase of induction uses a combination of
clustering and relative least general generalization
(RLGG) (Nienhuys-Cheng andWolf, 1997; Muggle-
ton and Feng, 1992) techniques. At the end of this
phase, we have a number of CD rules. In the sec-
ond phase, we begin by forming a structure called
the span-view table. Broadly speaking, this is an
attribute-value table formed by all the views induced
in the first phase along with the textual spans gener-
ated by them. The attribute-value table is used as
input to a propositional rule learner such as JRIP
to learn accurate compositions of a useful (as deter-
mined by the learning algorithm) subset of the CD
rules. This forms the second phase of our system.
The rules learnt from this phase are the CR rules.
At various phases, several induction biases are intro-
duced to enhance the interpretability of rules. These
biases capture the expertise gleaned from manual
rule development and constrain the search space in
our induction system.
The hypothesis language of our induction sys-
tem is Annotation Query Language (AQL) and we
use SystemT as the theorem prover. SystemT pro-
vides a very fast rule execution engine and is cru-
cial in our induction system as we test multiple hy-
potheses in the search for the more promising ones.
AQL provides a very expressive rule representation
language that is proven to be capable of encoding
all the paradigms that any rule-based representa-
tion can encode. The dual advantages of rich rule-
representation and execution efficiency are the main
motivation behind our choice.
We discuss our induction procedure in detail next.
5.1 Basic Features and Background Knowledge
We assume that we are provided with a set of dictio-
naries and regular expressions for defining all our
basic create view statements. R1, R2 and R3 in
Figure 1 are such basic view definitions. The ba-
sic views are compiled and executed in SystemT
over the training document collection and the re-
sulting spans are represented by equivalent predi-
cates in first order logic. Essentially, each train-
ing example is represented as a definite clause,
132
Figure 2: Correspondence between Manual Rule devel-
opment and Rule Induction.
that includes in its body, the basic view-types en-
coded as background knowledge predicates. To es-
tablish relationships between different background
knowledge predicates for each training example, we
define some additional ?glue predicates? such as
contains and before.
5.2 Induction of Candidate Definition Rules
Clustering Module. We obtain non-overlapping
clusters of examples within each type, by comput-
ing similarities between their representations as def-
inite clauses. We present the intuition behind this
approach in Figure 3 which illustrates the process
of taking two examples and finding their generaliza-
tion. It is worthwhile to look at generalizations of
instances that are similar. For instance, two token
person names such as Mark Waugh and Mark Twain
are part of a single cluster. However, we would not
be able to generalize a two-token name (e.g., Mark
Waugh) with another name consisting of initials fol-
lowed by a token (e.g., M. Waugh). Using a wrap-
per around the hierarchical agglomerative cluster-
ing implemented in LingPipe2, we cluster examples
and look at generalizations only within each cluster.
Clustering also helps improve efficiency by reduc-
ing the computational overhead, since otherwise, we
would have to consider generalizations of all pairs of
examples (Muggleton and Feng, 1992).
RLGG computation. We compute our CD
rules as the relative least general generalization
(RLGG) (Nienhuys-Cheng and Wolf, 1997; Mug-
gleton and Feng, 1992) of examples in each clus-
ter. Given a set of clauses in first order logic,
their RLGG is the least generalized clause in the
2http://alias-i.com/lingpipe/demos/tutorial/cluster/read-
me.html
Figure 3: Relative Least General Generalization
subsumption lattice of the clauses relative to the
background knowledge (Nienhuys-Cheng and Wolf,
1997). RLGG is associative, and we use this fact
to compute RLGGs of sets of examples in a clus-
ter. The RLGG of two bottom clauses as computed
in our system and its translation to an AQL view is
illustrated in Figure 3. We filter out noisy RLGGs
and convert the selected RLGGs into the equivalent
AQL views. Each such AQL view is treated as a
CD rule. We next discuss the process of filtering-
out noisy RLGGs. We interchangeably refer to the
RLGGs and the clusters they represent .
Iterative Clustering and RLGG filtering. Since
clustering is sub-optimal, we expected some clusters
in a single run of clustering to have poor RLGGs, ei-
ther in terms of complexity or precision. We there-
fore use an iterative clustering approach, based on
the separate-and-conquer (Fu?rnkranz, 1999) strat-
egy. In each iteration, we pick the clusters with the
best RLGGs and remove all examples covered by
those RLGGs. The best RLGGs must have preci-
sion and number of examples covered above a pre-
specified threshold.
5.3 Induction of Candidate Refinement Rules
Span-View Table. The CD views from phase 1
along with the textual spans they generate, yield the
span-view table. The rows of the table correspond
to the set of spans returned by all the CD views. The
columns correspond to the set of CD view names.
Each span either belongs to one of the named en-
tity types (PER, ORG or LOC) or is none of them
(NONE); the type information constitutes its class
label (see Figure 4 for an illustrated example). The
cells in the table correspond to either a match (M) or
a no-match (N) or partial/overlapping match (O) of
a span generated by a CD view. This attribute-value
table is used as input to a propositional rule learner
133
Figure 4: Span-View Table
like JRIP to learn compositions of CD views.
Propositional Rule Learning. Based on our study
of different propositional rule learners, we decided
to use RIPPER (Fu?rnkranz and Widmer, 1994) im-
plemented as the JRIP classifier in weka (Witten et
al., 2011). Some considerations that favor JRIP are
(i) absence of rule ordering, (ii) ease of conversion
to AQL and (iii) amenability to add induction biases
in the implementation.
A number of syntactic biases were introduced in
JRIP to aid in the interpretability of the induced
rules. We observed in our manually developed rules
that CR rules for a type involve interaction between
CDs for the same type and negations (not-overlaps,
not matches) of CDs of the other types. This bias
was incorporated by constraining a JRIP rule to con-
tain only positive features (CDs) of the same type
(say PER) and negative features (CDs) of only other
types (ORG and LOC, in this case).
The output of the JRIP algorithm is a set of
rules, one set for each of PER, ORG and LOC.
Here is an example rule: PER-CR-Rule ? (PerCD
= m) AND (LocCD != o) which is read as : ?If a
span matches PerCD and does not overlap with LocCD,
then that span denotes a PER named entity?. Here
PerCD is {[FirstName ? CapsPerson][LastName
? CapsPerson]} 3 and LocCD is {[CapsPlace ?
CitiesDict]}. This rule filters out wrong person
annotations like ?Prince William? in Prince William
Sound. (This is the name of a location but has over-
lapped with a person named entity.) In AQL, this
effect can be achieved most elegantly by the minus
(filter) construct. Such an AQL rule will filter all
those occurrences of Prince William from the list of
3Two consecutive spans where the 1st is FirstName and
CapsPerson and the 2nd is LastName and CapsPerson.
persons that overlap with a city name.
Steps such as clustering, computation of RLGGs,
JRIP, and theorem proving using SystemT were par-
allelized. Once the CR views for each type of
named entity are learnt, many forms of consolida-
tions (COs) are possible, both within and across
types. A simple consolidation policy that we have
incorporated in the system is as follows: union all
the rules of a particular type, then perform a con-
tained within consolidation, resulting in the final set
of consolidated views for each named entity type.
6 Experiments
We evaluate our system on CoNLL03 (Tjong
Kim Sang and De Meulder, 2003), a collection
of Reuters news stories. We used the CoNLL03
training set for induction and report results on the
CoNLL03 test collection.
The basic features (BFs) form the primary input to
our induction system. We experimented with three
sets of BFs:
Initial set(E1): The goal in this setup is to induce
an initial set of rules based on a small set of reason-
able BFs. We use a conservative initial set consisting
of 15 BFs (5 regular expressions and 10 dictionar-
ies).
Enhanced set (E2): Based on the results of E1,
we identify a set of additional domain independent
BFs4. Five views were added to the existing set in
E1 (1 regular expression and 4 dictionaries). The
goal is to observe whether our approach yields rea-
sonable accuracies compared to generic rules devel-
oped manually.
Domain customized set (E3): Based on the
knowledge of the domain of the training dataset
(CoNLL03), we introduced a set of features specific
to this dataset. These included sports-related person,
organization and location dictionaries5. These views
were added to the existing set in E2. The intended
goal is to observe what are the best possible accura-
cies that could be achieved with BFs customized to
a particular domain.
The set of parameters for iterative clustering on
which the accuracies reported are : the precision
threshold for the RLGGs of the clusters was 70%
4E.g., the feature preposition dictionary was added in E2 to
help identify organization names such as Bank of England.
5Half of the documents in CoNLL03 are sports-related.
134
Train Test
Type P R F P R F C(E)
E1 (Initial set)
PER 88.5 41.4 56.4 92.5 39.4 55.3 144
ORG 89.1 7.3 13.4 85.9 5.2 9.7 22
LOC 91.6 54.5 68.3 87.3 55.3 67.8 105
Overall 90.2 35.3 50.7 89.2 33.3 48.5 234
E2 (Enhanced set)
PER 84.7 52.9 65.1 87.5 49.9 63.5 233
ORG 88.2 7.8 14.3 85.8 5.9 11.0 99
LOC 92.1 58.6 71.7 88.6 59.1 70.9 257
Overall 88.6 40.7 55.8 88.0 38.2 53.3 457
E3 (Domain customized set)
PER 89.9 57.3 70.0 91.7 56.0 69.5 430
ORG 86.9 50.9 64.2 86.9 47.5 61.4 348
LOC 90.8 67.0 77.1 84.3 67.3 74.8 356
Overall 89.4 58.7 70.9 87.3 57.0 68.9 844
Table 3: Results on CoNLL03 dataset with different basic
feature sets
and the number of examples covered by each RLGG
was 5. We selected the top 5 clusters from each iter-
ation whose RLGGs crossed this threshold. If there
were no such clusters then we would lower the preci-
sion threshold to 35% (half of the threshold). When
no new clusters were formed, we ended the itera-
tions.
6.1 Experiments and Results
Effect of Augmenting Basic Features. Table 3
shows the accuracy and complexity of rules induced
with the three basic feature sets E1, E2 and E3,
respectively 6. The overall F-measure on the test
dataset is 48.5% with E1, it increases to around
53.3% with E2 and is highest at 68.9% with E3.
As we increase the number of BFs, the accuracies
of the induced extractors increases, at the cost of
an increase in complexity. In particular, the re-
call increases significantly across the board, and is
more prominent between E2 and E3, where the ad-
ditional domain specific features result in recall in-
crease from 5.9% to 47.5% for ORG. The precision
increases slightly for PER, but decreases slightly for
LOC and ORG with the addition of domain specific
features.
Comparison with manually developed rules. We
compared the induced extractors with the manually
developed extractors of (Chiticariu et al2010b),
heretofore referred to as manual extractors. (For a
detailed analysis, we obtained the extractors from
6These are the results for the configuration with bias.
the authors). Table 4 shows the accuracy and com-
plexity of the induced rules with E2 and E3 and the
manual extractors for the generic domain and, re-
spectively, customized for the CoNLL03 domain.
(In the table, ignore the column Induced (without
bias), which is discussed later). Our technique
compares reasonably with the manually constructed
generic extractor for two of the three entity types;
and on precision for all entity types, especially since
our system generated the rules in 1 hour, whereas the
development of manual rules took much longer 7.
Additional work is required to match the manual
customized extractor?s performance, primarily due
to shortcomings in our current target language. Re-
call that our framework is limited to a small subset
of AQL constructs for expressing CD and CR rules,
and there is a single consolidation rule. In particu-
lar, advanced constructs such as dynamic dictionar-
ies are not supported, and the set of predicates to the
Filter construct supported in our system is restricted
to predicates over other concepts, which is only a
subset of those used in (Chiticariu et al2010b).
The manual extractors also contain a larger number
of rules covering many different cases, improving
the accuracy, but also leading to a higher complex-
ity score. To better analyze the complexity, we also
computed the average rule length for each extrac-
tor by dividing the complexity score by the number
of AQL views of the extractor. The average rule
length is 1.78 and 1.87 for the induced extractors
with E2 and E3, respectively, and 1.9 and 2.1 for the
generic and customized extractors of (Chiticariu et
al., 2010b), respectively. The average rule length in-
creases from the generic extractor to the customized
extractor in both cases. On average, however, an in-
dividual induced rule is slightly smaller than a man-
ually developed rule.
Effect of Bias. The goal of this experiment is to
demonstrate the importance of biases in the induc-
tion process. The biases added to the system are
broadly of two types: (i) Partition of basic features
based on types (ii) Restriction on the type of CD
views that can appear in a CR view. 8 Without
7 (Chiticariu et al2010b) mentions that customization for 3
domains required 8 person weeks. It is reasonable to infer that
developing the generic rules took comparable effort.
8For e.g., person CR view can contain only person CD views
as positive clues and CD views of other types as negative clues.
135
(i) many semantically similar basic features (espe-
cially, regular expressions) would match a given to-
ken, leading to an increase in the length of a CD
a rule. For example, in the CD rule [FirstName-
Dict][CapsPerson ? CapsOrg]} (?A FirstNameDict
span followed by a CapsPerson span that is also a Cap-
sOrg span?), CapsPerson and CapsOrg are two very
similar regular expressions identifying capitalized
phrases that look like person, and respectively, orga-
nization names, with small variations (e.g., the for-
mer may allow special characters such as ?-?). In-
cluding both BFs in a CD rule leads to a larger rule
that is unintuitive for a developer. The former bias
excludes such CD rules from consideration.
The latter type of bias prevents CD rules of one
type to appear as positive clues for a CR rule of
a different type. For instance, without this bias,
one of the CR rules obtained was Per ? (OrgCD
= m) AND (LocCD != o) (?If a span matches OrgCD
and does not overlap with LocCD, then that span
denotes a PER named entity?. Here OrgCD was
{[CapsOrg][CapsOrg]} and LocCD was {[CapsLoc
? CitiesDict]}. The inclusion of an Organization
CD rule as a positive clue for a Person CR rule is
unintuitive for a developer.
Table 4, shows the effect (for E2 and E3) on the
test dataset of disabling and enabling bias during
the induction of CR rules using JRIP. Adding bias
improves the precision of the induced rules. With-
out bias, however, the system is less constrained in
its search for high recall rules, leading to slightly
higher overall F measure. This comes at the cost
of an increase in extractor complexity and average
rule length. For example, for E2, the average rule
length decreases from 2.17 to 1.78 after adding the
bias. Overall, our results show that biases lead to
less complex extractors with only a very minor ef-
fect on accuracy, thus biases are important factors
contributing to inducing rules that are understand-
able and may be refined by humans.
Comparison with other induction systems. We
also experimented with two other induction systems,
Aleph9 and ALP10, a package that implements one
of the reportedly good information extraction algo-
rithms (Ciravegna, 2001). While induction in Aleph
9A system for inductive logic programming. See
http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html
10http://code.google.com/p/alpie/
was performed with the same target language as in
our approach, the target language of ALP is JAPE,
which has been shown (Chiticariu et al2010b) to
lack in some of the constructs (such as minus) that
AQL provides and which form a part of our tar-
get language (especially the rule refinement phase).
However, despite experimenting with all possible
parameter configurations for each of these (in each
of E1, E2 and E3 settings), the accuracies obtained
were substantially (30-50%) worse and the extrac-
tor complexity was much (around 60%) higher when
compared to our system (with or without bias). Ad-
ditionally, Aleph takes close to three days for induc-
tion, whereas both ALP and our system require less
than an hour.
6.2 Discussion
Weak and Strong CDs reflected in CRs. In
our experiments, we found that varying the pre-
cision and complexity thresholds while inducing
the CDs (c.f Section 5) affected the F1 of the fi-
nal extractor only minimally. But reducing the
precision threshold generally improved the preci-
sion of the final extractor, which seemed counter-
intuitive at first. We found that CR rules learned
by JRIP consist of a strong CD rule (high preci-
sion, typically involving a dictionary) and a weak
CD rule (low precision, typically involving only
regular expressions). The strong CD rule always
corresponded to a positive clue (match) and the
weak CD rule corresponded to the negative clue
(overlaps or not-matches). This is illustrated in
the following CR rule: PER ? (PerCD = m) AND
(OrgCD != o) where (PerCD is {[CapsPersonR]
[CapsPersonR ? LastNameDict]} and (OrgCD is
{[CapsOrgR][CapsOrgR][CapsOrgR]}. This is
posited to be the way the CR rule learner operates
? it tries to learn conjunctions of weak and strong
clues so as to filter one from the other. Therefore,
setting a precision threshold too high limited the
number of such weak clues and the ability of the CR
rule learner to find such rules.
Interpretability. Measuring interpretability of rules
is a difficult problem. In this work, we have taken
a first step towards measuring interpretability using
a coarse grain measure in the form of a simple no-
tion of complexity score. The complexity is very
helpful in comparing alternative rule sets based on
136
Chiticariu et al010b Induced (With Bias) Induced (Without Bias)
P R F C(E) P R F C(E) P R F C(E)
Generic (E2) PER 82.2 60.3 69.5 945 87.5 49.9 63.5 233 85.8 53.7 66.0 476
ORG 75.7 17.5 28.5 1015 85.8 5.9 11.0 99 74.1 15.7 25.9 327
LOC 72.2 86.1 78.6 921 88.6 59.1 70.9 257 85.9 61.5 71.7 303
Overall 75.9 54.6 63.5 1015 88.0 38.2 53.3 457 84.2 43.5 57.4 907
Customised (E3) PER 96.3 92.2 94.2 2154 91.7 56.0 69.5 430 90.7 60.3 72.4 359
ORG 91.1 85.1 88.0 2154 86.9 47.5 61.4 348 90.4 46.8 61.7 397
LOC 93.3 91.7 92.5 2154 84.3 67.3 74.8 356 83.9 69.1 75.8 486
Overall 93.5 89.6 91.5 2160 87.3 57.0 68.9 844 87.8 58.7 70.4 901
Table 4: Comparison of induced rules (with and without bias) and manually developed rules. (CoNLL03 test dataset)
the number of rules, and the size of each rule, but
exhibits a number of shortcomings described next.
First, it disregards other components of a rule be-
sides its from clause, for example, the number of
items in the select clause, or the where clause. Sec-
ond, rule developers use semantically meaningful
view names such as those shown in Figure 1 to help
them recall the semantics of a rule at a high-level, an
aspect that is not captured by the complexity mea-
sure. Automatic generation of meaningful names
for induced views is an interesting direction for fu-
ture work. Finally, the overall structure of an extrac-
tors is not considered. In simple terms, an extrac-
tor consisting of 5 rules of size 1 is indistinguish-
able from an extractor consisting of a single rule
of size 5, and it is arguable which of these extrac-
tors is more interpretable. More generally, the ex-
tent of this shortcoming is best explained using an
example. When informally examining the rules in-
duced by our system, we found that CD rules are
similar in spirit to those written by rule develop-
ers. On the other hand, the induced CR rules are
too fine-grained. In general, rule developers group
CD rules with similar semantics, then write refine-
ment rules at the higher level of the group, as op-
posed to the lower level of individual CD views. For
example, one may write multiple CD rules for can-
didate person names of the form ?First??Last?, and
multiple CD rules of the form ?Last?, ?First?. One
would then union together the candidates from each
of the two groups into two different views, e.g., Per-
FirstLast and PerLastCommaFirst, and write filter
rules at the higher level of these two views, e.g.,
?Remove PerLastCommaFirst spans that overlap with a
PerFirstLast span?. In contrast, our induction algo-
rithm considers CR rules consisting of combinations
of CD rules directly, leading to many semantically
similar CR rules, each operating over small parts of
a larger semantic group (see rule in Section 6.1).
This results in repetition, and qualitatively less in-
terpretable rules, since humans prefer higher levels
of abstraction and generalization. This nuance is not
captured by the complexity score which may deem
an extractor consisting of many rules, where many
of the rules operate at higher levels of groups of can-
didates to be more complex than a smaller extrac-
tor with many fine-grained rules. Indeed, as shown
before, the complexity of the induced extractors is
much smaller compared to that of manual extrac-
tors, although the latter follow the semantic group-
ing principle and are considered more interpretable.
7 Conclusion
We presented a system for efficiently inducing
named entity annotation rules in the AQL language.
The design of our approach is aimed at producing
accurate rules that can be understood and refined
by humans, by placing special emphasis on low
complexity and efficient computation of the induced
rules, while mimicking a four stage approach used
for manually constructing rules. The induced rules
have good accuracy and low complexity according
to our complexity measure. While our complexity
measure informs the biases in our system and leads
to simpler, smaller extractors, it captures extrac-
tor interpretability only to a certain extent. There-
fore, we believe more work is required to devise a
more comprehensive quantitative measure for inter-
pretability, and refine our techniques in order to in-
crease the interpretability of induced rules. Other
interesting directions for future work are introduc-
ing more constructs in our framework, and applying
our techniques to other languages.
137
References
S. Abiteboul, R. Hull, and V. Vianu. 1995. Foundations
of Databases. Addison Wesley Publishing Co.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
workshop.
Mary Elaine Califf and Raymond J. Mooney. 1997. Ap-
plying ilp-based techniques to natural language infor-
mation extraction: An experiment in relational learn-
ing. In IJCAI Workshop on Frontiers of Inductive
Logic Programming.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010a. Systemt: an algebraic ap-
proach to declarative information extraction. In ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010b. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP.
Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm for
information extraction from web-related texts. In In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
J. Fu?rnkranz and G. Widmer. 1994. Incremental reduced
error pruning. pages 70?77.
Johannes Fu?rnkranz. 1999. Separate-and-conquer rule
learning. Artif. Intell. Rev., 13(1):3?54, February.
B. R. Gaines and P. Compton. 1995. Induction of ripple-
down rules applied to modeling large databases. J. In-
tell. Inf. Syst., 5:211?228, November.
IBM, 2012. IBM InfoSphere BigInsights - An-
notation Query Language (AQL) reference.
http://publib.boulder.ibm.com/
infocenter/bigins/v1r3/topic/com.
ibm.swg.im.infosphere.biginsights.
doc/doc/biginsights_aqlref_con_
aql-overview.html.
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan,
Shivakumar Vaithyanathan, and H. V. Jagadish. 2008.
Regular expression learning for information extrac-
tion. In EMNLP.
Bin Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish,
and Frederick R. Reiss. 2010. Automatic rule refine-
ment for information extraction. Proc. VLDB Endow.,
3:588?597.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In In Recent Advances in Natural Lan-
guage Processing.
Stephen Muggleton and C. Feng. 1992. Efficient induc-
tion in logic programs. In ILP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30:3?26.
Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. 1997.
Foundations of Inductive Logic Programming.
Anup Patel, Ganesh Ramakrishnan, and Pushpak Bhat-
tacharyya. 2009. Incorporating linguistic expertise
using ilp for named entity recognition in data hungry
indian languages. In ILP.
Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-
murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.
2008. An algebraic approach to rule-based informa-
tion extraction. In ICDE.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. In AAAI.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Mach.
Learn., 34:233?272.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL.
Ian H.Witten, Eibe Frank, andMark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann, Amsterdam, 3rd edition.
138
Hindi and Marathi to English Cross Language Information 
 
Manoj Kumar Chinnakotla, Sagar Ranadive, Om P. Damani and Pushpak 
Bhattacharyya 
 
Abstract 
 
In this paper, we present our Hindi ->English and Marathi ->English CLIR 
systems developed as part of our participation in the CLEF 2007 Ad-Hoc 
Bilingual task. We take a query translation based approach using bi-lingual 
dictionaries. Query words not found in the dictionary are transliterated using 
a simple lookup table based transliteration approach. The resultant 
transliteration is then compared with the index items of the corpus to return 
the `k' closest English index words of the given Hindi/Marathi word. The 
resulting multiple translation/transliteration choices for each query word are 
disambiguated using an iterative page-rank style algorithm, proposed in the 
literature, which makes use of term-term co-occurrence statistics to produce 
the final translated query. Using the above approach, for Hindi, we achieve a 
Mean Average Precision (MAP) of 0.2366 in title which is 61.36% of 
monolingual performance and a MAP of 0.2952 in title and description 
which is 67.06% of monolingual performance. For Marathi, we achieve a 
MAP of 0.2163 in title which is 56.09% of monolingual performance. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 420?428,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Everybody loves a rich cousin: An empirical study of transliteration through
bridge languages
Mitesh M. Khapra
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
miteshk@cse.iitb.ac.in
A Kumaran
Microsoft Research India,
Bangalore,
India
a.kumaran@microsoft.com
Pushpak Bhattacharyya
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
pb@cse.iitb.ac.in
Abstract
Most state of the art approaches for machine
transliteration are data driven and require sig-
nificant parallel names corpora between lan-
guages. As a result, developing translitera-
tion functionality among n languages could
be a resource intensive task requiring paral-
lel names corpora in the order of nC2. In this
paper, we explore ways of reducing this high
resource requirement by leveraging the avail-
able parallel data between subsets of the n lan-
guages, transitively. We propose, and show
empirically, that reasonable quality transliter-
ation engines may be developed between two
languages, X and Y , even when no direct par-
allel names data exists between them, but only
transitively through language Z . Such sys-
tems alleviate the need for O(nC2) corpora,
significantly. In addition we show that the per-
formance of such transitive transliteration sys-
tems is in par with direct transliteration sys-
tems, in practical applications, such as CLIR
systems.
1 Introduction
Names and Out Of Vocabulary (OOV) terms appear
very frequently in written and spoken text and hence
play a very important role in several Natural Lan-
guage Processing applications. Several studies have
shown that handling names correctly across lan-
guages can significantly improve the performance of
CLIR Systems (Mandl and Womser-Hacker, 2004)
and the utility of machine translation systems. The
fact that translation lexicons or even statistical dic-
tionaries derived from parallel data do not provide a
good coverage of name and OOV translations, un-
derscores the need for good transliteration engines
to transform them between the language.
The importance of machine transliteration, in the
above context, is well realized by the research com-
munity and several approaches have been proposed
to solve the problem. However, most state of the art
approaches are data driven and require significant
parallel names corpora between languages. Such
data may not always be available between every pair
of languages, thereby limiting our ability to support
transliteration functionality between many language
pairs, and subsequently information access between
languages. For example, let us consider a practi-
cal scenario where we have six languages from four
different language families as shown in Figure 1.
The nodes in the graph represent languages and the
edges indicate the availability of data between that
language pair and thus the availability of a Machine
Transliteration system for that pair. It is easy to see
the underlying characteristics of the graph. Data is
available between a language pair due to one of the
following three reasons:
Politically related languages: Due to the political
dominance of English it is easy to obtain parallel
names data between English and most languages.
Genealogically related languages: Arabic and He-
brew share a common origin and there is a signifi-
cant overlap between their phoneme and grapheme
inventory. It is easy to obtain parallel names data
between these two languages.
Demographically related languages: Hindi and
Kannada are languages spoken in the Indian sub-
continent, though they are from different language
families. However, due to the shared culture and de-
mographics, it is easy to create parallel names data
between these two languages.
420
Figure 1: A connected graph of languages
On the other hand, for politically, demographi-
cally and genealogically unrelated languages such
as, say, Hindi and Hebrew, parallel data is not readily
available, either due to the unavailability of skilled
bilingual speakers. Even the argument of using
Wikipedia resources for such creation of such par-
allel data does not hold good, as the amount of inter-
linking may be very small to yield data. For exam-
ple, only 800 name pairs between Hindi and Hebrew
were mined using a state of the art mining algorithm
(Udupa et al, 2009), from Wikipedia interwiki links.
We propose a methodology to develop a practi-
cal Machine Transliteration system between any two
nodes of the above graph, provided a two-step path
exists between them. That is, even when no parallel
data exists between X & Y but sufficient data exists
between X & Z and Z & Y it is still possible to de-
velop transliteration functionality between X & Y
by combining a X ? Z system with a Z ? Y
system. For example, given the graph of Figure 1,
we explore the possibility of developing translitera-
tion functionality between Hindi and Russian even
though no direct data is available between these two
languages. Further, we show that in many cases the
bridge language can be suitably selected to ensure
optimal MT accuracy.
To establish the practicality and utility of our ap-
proach we integrated such a bridge transliteration
system with a standard CLIR system and compared
its performance with that of a direct transliteration
system. We observed that such a bridge system
performs well in practice and in specific instances
results in improvement in CLIR performance over
a baseline system further strengthening our claims
that such bridge systems are good practical solutions
for alleviating the resource scarcity problem.
To summarize, our main contributions in this pa-
per are:
1. Constructing bridge transliteration systems and
establishing empirically their quality.
2. Demonstrating their utility in providing prac-
tical transliteration functionality between two
languages X & Y with no direct parallel data
between them.
3. Demonstrating that in specific cases it is pos-
sible to select the bridge language so that op-
timal Machine Transliteration accuracy is en-
sured while stepping through the bridge lan-
guage.
1.1 Organization of the Paper
This paper is organized in the following manner. In
section 2 we present the related work and highlight
the lack of work on transliteration in resource scarce
scenarios. In section 3 we discuss the methodology
of bridge transliteration. Section 4 discusses the ex-
periments and datasets used. Section 4.3 discusses
the results and error analysis. Section 5 discusses or-
thographic characteristics to be considered while se-
lecting the bridge language. Section 6 demonstrates
the effectiveness of such bridge systems in a practi-
cal scenario, viz., Cross Language Information Re-
trieval. Section 7 concludes the paper, highlighting
future research issues.
2 Related Work
Current models for transliteration can be classi-
fied as grapheme-based, phoneme-based and hy-
brid models. Grapheme-based models, such as,
Source Channel Model (Lee and Choi, 1998), Max-
imum Entropy Model (Goto et al, 2003), Condi-
tional Random Fields (Veeravalli et al, 2008) and
Decision Trees (Kang and Choi, 2000) treat translit-
eration as an orthographic process and try to map
the source language graphemes directly to the tar-
get language graphemes. Phoneme based models,
such as, the ones based on Weighted Finite State
421
Transducers (WFST) (Knight and Graehl, 1997)
and extended Markov window (Jung et al, 2000)
treat transliteration as a phonetic process rather than
an orthographic process. Under such frameworks,
transliteration is treated as a conversion from source
grapheme to source phoneme followed by a conver-
sion from source phoneme to target grapheme. Hy-
brid models either use a combination of a grapheme
based model and a phoneme based model (Stalls
and Knight, 1998) or capture the correspondence be-
tween source graphemes and source phonemes to
produce target language graphemes (Oh and Choi,
2002).
A significant shortcoming of all the previous
works was that none of them addressed the issue of
performing transliteration in a resource scarce sce-
nario, as there was always an implicit assumption
of availability of data between a pair of languages.
In particular, none of the above approaches address
the problem of developing transliteration functional-
ity between a pair of languages when no direct data
exists between them but sufficient data is available
between each of these languages and an intermedi-
ate language. Some work on similar lines has been
done in Machine Translation (Wu and Wang, 2007)
wherein an intermediate bridge language (say, Z) is
used to fill the data void that exists between a given
language pair (say, X and Y ). In fact, recently it has
been shown that the accuracy of a X ? Z Machine
Translation system can be improved by using addi-
tional X ? Y data provided Z and Y share some
common vocabulary and cognates (Nakov and Ng,
2009). However, no such effort has been made in the
area of Machine Transliteration. To the best of our
knowledge, this work is the first attempt at providing
a practical solution to the problem of transliteration
in the face of resource scarcity.
3 Bridge Transliteration Systems
In this section, we explore the salient question ?Is
it possible to develop a practical machine transliter-
ation system between X and Y , by composing two
intermediate X ? Z and Z ? Y transliteration
systems?? We use a standard transliteration method-
ology based on orthography for all experiments (as
outlined in section 3.1), to ensure the applicability
of the methodology to a variety of languages.
3.1 CRF based transliteration engine
Conditional Random Fields ((Lafferty et al, 2001))
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given the
source word is given by,
P (Y |X;?) = 1
N(X)
? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of source word
K = number of features
?k = feature weight
N(X) = normalization constant
CRF++ 1, an open source implementation of CRF
was used for training and decoding (i.e. transliter-
ating the names). GIZA++ (Och and Ney, 2003),
a freely available implementation of the IBM align-
ment models (Brown et al, 1993) was used to get
character level alignments for the name pairs in the
parallel names training corpora. Under this align-
ment, each character in the source word is aligned to
zero or more characters in the corresponding target
word. The following features are then generated us-
ing this character-aligned data (here ei and hi form
the i-th pair of aligned characters in the source word
and target word respectively):
? hi and ej such that i? 2 ? j ? i + 2
? hi and source character bigrams ( {ei?1, ei} or
{ei, ei+1})
? hi and source character trigrams ( {ei?2, ei?1,
ei} or {ei?1, ei, ei+1} or {ei, ei+1, ei+2})
? hi, hi?1 and ej such that i? 2 ? j ? i + 2
? hi, hi?1 and source character bigrams
? hi, hi?1 and source character trigrams
1http://crfpp.sourceforge.net/
422
3.2 Bridge Transliteration Methodology
In this section, we outline our methodology for com-
posing transitive transliteration systems between X
and Y , using a bridge language Z , by chaining indi-
vidual direct transliteration systems. Our approach
of using bridge transliteration for finding the best
target string (Y ?), given the input string X can be
represented by the following probabilistic expres-
sion:
Y ? = arg max
Y
P (Y |X)
=
?
Z
P (Y,Z|X)
=
?
Z
P (Y |Z,X) ? P (Z|X) (2)
We simplify the above expression, by assuming that
Y is independent of X given Z; the linguistic intu-
ition behind this assumption is that the top-k outputs
of the X ? Z system corresponding to a string in
X, capture all the transliteration information neces-
sary for transliterating to Y . Subsequently, in sec-
tion 5 we discuss the characteristics of the effective
bridge languages to maximize the capture of neces-
sary information for the second stage of the translit-
eration, namely for generating correct strings of Z .
Thus,
Y ? =
?
Z
P (Y |Z) ? P (Z|X) (3)
The probabilities P (Y |Z) and P (Z|X) in Equation
(3) are derived from the two stages of the bridge sys-
tem. Specifically, we assume that the parallel names
corpora are available between the language pair, X
and Z , and the language pair, Z and Y . We train two
baseline CRF based transliteration systems (as out-
lined in Section 3.1), between the language X and
Z , and Z and Y . Each name in language X was
provided as an input into X ? Z transliteration sys-
tem, and the top-10 candidate strings in language Z
produced by this first stage system were given as an
input into the second stage system Z ? Y . The re-
sults were merged using Equation (2). Finally, the
top-10 outputs of this system were selected as the
output of the bridge system.
4 Experiments
It is a well known fact that transliteration is lossy,
and hence the transitive systems may be expected to
suffer from the accumulation of errors in each stage,
resulting in a system that is of much poorer quality
than a direct transliteration system. In this section,
we set out to quantify this expected loss in accuracy,
by a series of experiments in a set of languages us-
ing bridge transliteration systems and a baseline di-
rect systems. We conducted a comprehensive set of
experiments in a diverse set of languages, as shown
in Figure 1, that include English, Indic (Hindi and
Kannada), Slavic (Russian) and Semitic (Arabic and
Hebrew) languages. The datasets and results are de-
scribed in the following subsections.
4.1 Datasets
To be consistent, for training each of these systems,
we used approximately 15K name pairs corpora (as
this was the maximum data available for some lan-
guage pairs). While we used the NEWS 2009 train-
ing corpus (Li et al, 2009) as a part of our train-
ing data, we enhanced the data set to about 15K by
adding more data of similar characteristics (such as,
name origin, domain, length of the name strings,
etc.), taken from the same source as the original
NEWS 2009 data. For languages such as Arabic
and Hebrew which were not part of the NEWS 2009
shared task, the data was created along the same
lines. All results are reported on the standard NEWS
2009 test set, wherever applicable. The test set con-
sists of about 1,000 name pairs in languages X and
Y ; to avoid any bias, it was made sure that there is
no overlap between the test set with the training sets
of both the X ? Z and Z ? Y systems. To estab-
lish a baseline, the same CRF based transliteration
system (outlined in Section 3.1) was trained with a
15K name pairs corpora between the languages X
? Y . The same test set used for testing the transi-
tive systems was used for testing the direct system
as well. As before, to avoid any bias, we made sure
that there is no overlap between the test set and the
training set for the direct system as well.
4.2 Results
We produce top-10 outputs from the bridge system
as well from the direct system and compare their
performance. The performance is measured using
the following standard measures, viz., top-1 accu-
racy (ACC-1) and Mean F-score. These measures
are described in detail in (Li et al, 2009). Table 1
423
Language
Pair
ACC-1 Relative change in
ACC-1 Mean F-score
Relative change in
Mean F-score
Hin-Rus 0.507 0.903
Hin-Eng-Rus 0.466 -8.08% 0.886 -1.88%
Hin-Ara 0.458 0.897
Hin-Eng-Ara 0.420 -8.29% 0.876 -2.34%
Eng-Heb 0.544 0.917
Eng-Ara-Heb 0.544 0% 0.917 0%
Hin-Eng 0.422 0.884
Hin-Kan-Eng 0.382 -9.51% 0.871 -1.47%
Table 1: Stepping through an intermediate language
presents the performance measures, both for a di-
rect system (say, Hin-Rus), and a transitional sys-
tem (say, Hin-Eng-Rus), in 4 different transitional
systems, between English, Indic, Semitic and Slavic
languages. In each case, we observe that the transi-
tional systems have a slightly lower quality, with an
absolute drop in accuracy (ACC-1) of less than 0.05
(relative drop under 10%), and an absolute drop in
Mean F-Score of 0.02 (relative drop under 3%).
4.3 Analysis of Results
Intuitively, one would expect that the errors of the
two stages of the transitive transliteration system
(i.e., X ? Z , and Z ? Y ) to compound, leading
to a considerable loss in the overall performance of
the system. Given that the accuracies of the direct
transliteration systems are as given in Table 2, the
transitive systems are expected to have accuracies
close to the product of the accuracies of the individ-
ual stages, for independent systems.
Language Pair ACC-1 Mean F-Score
Hin-Eng 0.422 0.884
Eng-Rus 0.672 0.935
Eng-Ara 0.514 0.905
Ara-Heb 1.000 1.000
Hin-Kan 0.433 0.879
Kan-Eng 0.434 0.886
Table 2: Performance of Direct Transliteration Systems
However, as we observe in Table 1, the relative
drop in the accuracy (ACC-1) is less than 10% from
that of the direct system, which goes against our in-
tuition. To identify the reasons for the better than
expected performance, we performed a detailed er-
ror analysis of each stage of the bridge translitera-
tion systems, and the results are reported in Tables 3
? 5. We draw attention to two interesting facts which
account for the better than expected performance of
the bridge system:
Improved 2nd stage performance on correct
inputs: In each one of the cases, as expected, the
ACC-1 of the first stage is same as the ACC-1 of the
X ? Z system. However, we notice that the ACC-1
of the second stage on the correct strings output
in the first stage, is significantly better than the the
ACC-1 of the Z ? Y system! For example, the
ACC-1 of the Eng-Rus system is 67.2% (see Table
2), but, that of the 2nd stage Eng-Rus system is
77.8%, namely, on the strings that are transliterated
correctly by the first stage. Our analysis indicate
that there are two reasons for such improvement:
First, the strings that get transliterated correctly in
the first stage are typically shorter or less ambigu-
ous and hence have a better probability of correct
transliterations in the both stages. This phenomenon
could be verified empirically: Names like gopAl
{Gopal}, rm? {Ramesh}, rAm {Ram} are
shorter and in general have less ambiguity on target
orthography. Second, also significantly, the use of
top-10 outputs from the first stage as input to the
second stage provides a better opportunity for the
second stage to produce correct string in Z . Again,
this phenomenon is verified by providing increasing
number of top-n results to the 2nd stage.
424
Hi?En?Ru En ? Ru(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 263 75 77.81%
(Stage-1) Error 119 362 24.74%
Table 3: Error Analysis for Hi?En?Ru
Hi?En?Ar En ? Ar(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 221 127 63.50%
(Stage-1) Error 119 340 25.70%
Table 4: Error Analysis for Hi?En?Ar
2nd stage error correction on incorrect inputs:
The last rows in each of the above tables 3 ? 5 re-
port the performance of the second stage system on
strings that were transliterated incorrectly by the first
stage. While we expected the second row to pro-
duce incorrect transliterations nearly for all inputs
(as the input themselves were incorrect in Z), we
find to our surprise that upto 25% of the erroneous
strings in Z were getting transliterated correctly in
Y ! This provides credence to our hypothesis that
sufficient transliteration information is captured in
the 1st stage output (even when incorrect) that may
be exploited in the 2nd stage. Empirically, we veri-
fied that in most cases (nearly 60%) the errors were
due to the incorrectly transliterated vowels, and in
many cases, they get corrected in the second stage,
and re-ranked higher in the output. Figure 2 shows a
few examples of such error corrections in the second
stage.
Figure 2: Examples of error corrections
Hi?Ka?En Ka ? En(Stage-2)
Stage-2
Acc.
Correct Error
Hi?Ka Correct 225 196 53.44%
(Stage-1) Error 151 400 27.40%
Table 5: Error Analysis for Hi?Ka?En
5 Characteristics of the bridge language
An interesting question that we explore in this sec-
tion is ?how the choice of bridge language influence
the performance of the bridge system??. The under-
lying assumption in transitive transliteration systems
(as expressed in Equation 3), is that ?Y is indepen-
dent of X given Z?. In other words, we assume that
the representations in the language will Z ?capture
sufficient transliteration information from X to pro-
duce correct strings in Y ?. We hypothesize that two
parameters of the bridge language, namely, the or-
thography inventory and the phoneme-to-grapheme
entropy, that has most influence on the quality of the
transitional systems, and provide empirical evidence
for this hypothesis.
5.1 Richer Orthographic Inventory
In each of the successful bridge systems (that is,
those with a relative performance drop of less than
10%), presented in Table 1, namely, Hin-Eng-Ara,
Eng-Ara-Heb and Hin-Kan-Eng, the bridge lan-
guage has, in general, richer orthographic inven-
tory than the target language. Arabic has a reduced
set of vowels, and hence poorer orthographic inven-
tory compared with English. Similarly, between the
closely related Semitic languages Arabic-Hebrew,
there is a many-to-one mapping from Arabic to He-
brew, and between Kannada-English, Kannada has
nearly a superset of vowels and consonants as com-
pared to English or Hindi.
As an example for a poor choice of Z , we present
a transitional system, Hindi ? Arabic ? English, in
Table 6, in which the transitional language z (Ara-
bic) has smaller orthographic inventory than Y (En-
glish).
Arabic has a reduced set of vowels and, unlike En-
glish, in most contexts short vowels are optional. As
a result, when Arabic is used as the bridge language
the loss of information (in terms of vowels) is large
425
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Ara-Eng 0.155 -64.28%
Table 6: Incorrect choice of bridge language
and the second stage system has no possibility of re-
covering from such a loss. The performance of the
bridge system confirms such a drastic drop in ACC-
1 of nearly 64% compared with the direct system.
5.2 Higher Phoneme-Grapheme Entropy
We also find that the entropy in phoneme - grapheme
mapping of a language indicate a good correlation
with a good choice for a transition language. In
a good transitional system (say, Hin-Eng-Rus), En-
glish has a more ambiguous phoneme-to-grapheme
mapping than Russian; for example, in English the
phoneme ?s? as in Sam or Cecilia can be repre-
sented by the graphemes ?c? and ?s?, whereas Rus-
sian uses only a single character to represent this
phoneme. In such cases, the ambiguity introduced
by the bridge language helps in recovering from er-
rors in the X ? Z system. The relative loss of
ACC-1 for this transitional system is only about 8%.
The Table 7 shows another transitional system, in
which a poor choice was for the transitional lan-
guage was made.
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Tam-Eng 0.231 -45.26%
Table 7: Incorrect choice of bridge language
Tamil has a reduced set of consonants compared
with Hindi or English. For example, the Hindi con-
sonants (k, kh, g, gh) are represented by a sin-
gle character in Tamil. As a result, when Tamil is
used as the bridge language it looses information (in
terms of consonants) and results in a significant drop
in performance (nearly a 45% drop in ACC-1) for
the bridge system.
6 Effectiveness of Bridge Transliteration
on CLIR System
In this section, we demonstrate the effectiveness of
our bridge transliteration system on a downstream
application, namely, a Crosslingual Information Re-
trieval system. We used the standard document col-
lections from CLEF 2006 (Nardi and Peters, 2006),
CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008
(FIRE, 2008). We used Hindi as the query language.
All the three fields (title, description and narration)
of the topics were used for the retrieval. Since the
collection and topics are from the previous years,
their relevance judgments were also available as a
reference for automatic evaluation.
6.1 Experimental Setup
We used primarily the statistical dictionaries gen-
erated by training statistical word alignment mod-
els on an existing Hindi-English parallel corpora.
As with any CLIR system that uses translation lex-
icon, we faced the problem of out-of-vocabulary
(OOV) query terms that need to be transliterated,
as they are typically proper names in the target lan-
guage. First, for comparison, we used the above
mentioned CLIR system with no transliteration en-
gine (Basic), and measured the crosslingual retrieval
performance. Clearly, the OOV terms would not be
converted into target language, and hence contribute
nothing to the retrieval performance. Second, we in-
tegrated a direct machine transliteration system be-
tween Hindi and English (D-HiEn), and calibrated
the improvement in performance. Third, we inte-
grate, instead of a direct system, a bridge transliter-
ation system between Hindi and English, transition-
ing through Kannada (B-HiKaEn). For both, direct
as well as bridge transliteration, we retained the top-
5 transliterations generated by the appropriate sys-
tem, for retrieval.
6.2 Results and Discussion
The results of the above experiments are given in
Table 7. The current focus of these experiments is
to answer the question of whether the bridge ma-
chine transliteration systems used to transliterate
the OOV words in Hindi queries to English (by step-
ping through Kannada) performs at par with a di-
rect transliteration system. As expected, enhancing
the CLIR system with a machine transliteration sys-
426
Collection CLIR System MAP Relative MAP change
from Basic
Recall Relative Recall change
from Basic
Basic 0.1463 - 0.4952 -
CLEF 2006 D-HiEn 0.1536 +4.98% 0.5151 +4.01%
B-HiKaEn 0.1529 +4.51% 0.5302 +7.06%
Basic 0.2521 - 0.7156 -
CLEF 2007 D-HiEn 0.2556 +1.38% 0.7170 + 0.19%
B-HiKaEn 0.2748 +9.00% 0.7174 + 0.25%
Basic 0.4361 - 0.8457 -
FIRE 2008 D-HiEn 0.4505 +3.30% 0.8506 +0.57%
B-HiKaEn 0.4573 +4.86% 0.8621 +1.93%
Table 8: CLIR Experiments with bridge transliteration systems
tem (D-HiEn) gives better results over a CLIR sys-
tem with no transliteration functionality (Basic). On
the standard test collections, the bridge translitera-
tion system performs in par or better than the di-
rect transliteration system in terms of MAP as well
as recall. Even though, the bridged system is of
slightly lesser quality in ACC-1 in Hi-Ka-En, com-
pared to Hi-En (see Table 1), the top-5 results had
captured the correct transliteration, as shown in our
analysis. A detailed analysis of the query transla-
tions produced by the above systems showed that in
some cases the bridge systems does produce a bet-
ter transliteration thereby leading to a better MAP.
As an illustration, consider the OOV terms vEV?n
{Vatican} and n-l {Nestle} and the corre-
sponding transliterations generated by the different
systems. The Direct-HiEn system was unable to
OOV term D-HiEn B-HiKaEn
vetican vetican
veticon vettican
vEV?n vettican vatican
(vatican) vetticon watican
wetican wetican
nesle nestle
nesly nesle
n-l nesley nesley
(nestle) nessle nestley
nesey nesly
Table 9: Sample output in direct and bridge systems
generate the correct transliteration in the top-5 re-
sults whereas the B-HiKaEn was able to produce the
correct transliteration in the top-5 results thereby re-
sulting in an improvement in MAP for these queries.
7 Conclusions
In this paper, we introduced the idea of bridge
transliteration systems that were developed employ-
ing well-studied orthographic approaches between
constituent languages. We empirically established
the quality of such bridge transliteration systems
and showed that quite contrary to our expectations,
the quality of such systems does not degrade dras-
tically as compared to the direct systems. Our er-
ror analysis showed that these better-than-expected
results can be attributed to (i) Better performance
(?10-12%) of the second stage system on the strings
transliterated correctly by the first stage system and
(ii) Significant (?25%) error correction in the sec-
ond stage. Next, we highlighted that the perfor-
mance of such bridge systems will be satisfactory as
long as the orthographic inventory of the bridge lan-
guage is either richer or more ambiguous as com-
pared to the target language. We showed that our
results are consistent with this hypothesis and pro-
vided two examples where there is a significant drop
in the accuracy when the bridge language violates
the above constraints. Finally, we showed that a
state of the art CLIR system integrated with a bridge
transliteration system performs in par with the same
CLIR system integrated with a direct translitera-
tion system, vindicating our claim that such bridge
transliteration systems can be use in real-world ap-
plications to alleviate the resource requirement of
nC2 parallel names corpora.
427
References
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263?311.
FIRE. 2008. Forum for information retrieval evaluation.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context in-
formation based on the maximum entropy method. In
Proceedings of MT-Summit IX, pages 125?132.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics, pages 383?
389.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
transliteration and back-transliteration by decision tree
learning. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation, pages
1135?1411.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Computational Linguistics, pages
128?135.
John D. Lafferty, Andrew Mccallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pages 282?
289, San Francisco, CA, USA.
Jae Sung Lee and Key-Sun Choi. 1998. English to ko-
rean statistical transliteration for information retrieval.
In Computer Processing of Oriental Languages, pages
17?37.
Haizhou Li, A Kumaran, , Min Zhang, and Vladimir Per-
vouvhine. 2009. Whitepaper of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation (NEWS 2009), pages 19?26, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Mandl and Christa Womser-Hacker. 2004. How
do named entities contribute to retrieval effectiveness?
In CLEF, pages 833?842.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1358?1367, Singa-
pore, August. Association for Computational Linguis-
tics.
A Nardi and C Peters. 2006. Working notes for the clef
2006 workshop.
A Nardi and C Peters. 2007. Working notes for the clef
2007 workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Jong-hoon Oh and Key-Sun Choi. 2002. An english-
korean transliteration model using pronunciation and
contextual rules. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING), pages 758?764.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text. In
Proceedings of COLING/ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 34?41.
Raghavendra Udupa, K Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009. ?they are out there, if you know
where to look: Mining transliterations of oov query
terms for cross language information retrieval?. In
ECIR?09: Proceedings of the 31st European Confer-
ence on IR research on Advances in Information Re-
trieval, pages 437?448, Toulouse, France.
Suryaganesh Veeravalli, Sreeharsha Yella, Prasad Pin-
gali, and Vasudeva Varma. 2008. Statistical translit-
eration for cross language information retrieval using
hmm alignment model and crf. In Proceedings of the
2nd workshop on Cross Lingual Information Access
(CLIA) Addressing the Information Need of Multilin-
gual Societies.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
428
Proceedings of NAACL-HLT 2013, pages 733?738,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
More than meets the eye: Study of Human Cognition in Sense Annotation
Salil Joshi
IBM Research India
Bangalore, India
saljoshi@in.ibm.com
Diptesh Kanojia
Gautam Buddha Technical University
Lucknow, India
dipteshkanojia@gmail.com
Pushpak Bhattacharyya
Computer Science and Engineering Department
Indian Institute of Technology, Bombay
Mumbai, India
pb@cse.iitb.ac.in
Abstract
Word Sense Disambiguation (WSD) ap-
proaches have reported good accuracies in
recent years. However, these approaches can
be classified as weak AI systems. According
to the classical definition, a strong AI based
WSD system should perform the task of sense
disambiguation in the same manner and with
similar accuracy as human beings. In order
to accomplish this, a detailed understanding
of the human techniques employed for sense
disambiguation is necessary. Instead of
building yet another WSD system that uses
contextual evidence for sense disambiguation,
as has been done before, we have taken a step
back - we have endeavored to discover the
cognitive faculties that lie at the very core of
the human sense disambiguation technique.
In this paper, we present a hypothesis regard-
ing the cognitive sub-processes involved in the
task ofWSD.We support our hypothesis using
the experiments conducted through the means
of an eye-tracking device. We also strive to
find the levels of difficulties in annotating vari-
ous classes of words, with senses. We believe,
once such an in-depth analysis is performed,
numerous insights can be gained to develop a
robust WSD system that conforms to the prin-
ciple of strong AI.
1 Introduction
Word Sense Disambiguation (WSD) is formally
defined as the task of computationally identifying
senses of a word in a context. The phrase ?in a
context? is not defined explicitly in the literature.
NLP researchers define it according to their conve-
nience. In our current work, we strive to unravel
the appropriate meaning of contextual evidence
used for the human annotation process. Chatterjee
et al (2012) showed that the contextual evidence
is the predominant parameter for the human sense
annotation process. They also state that WSD is
successful as a weak AI system, and further analysis
into human cognitive activities lying at the heart of
sense annotation can aid in development of a WSD
system built upon the principles of strong AI.
Knowledge based approaches, which can be con-
sidered to be closest form of WSD conforming to
the principles of strong AI, typically achieve low
accuracy. Recent developments in domain-specific
knowledge based approaches have reported higher
accuracies. A domain-specific approach due to
Agirre et al (2009) beats supervised WSD done
in generic domains. Ponzetto and Navigli (2010)
present a knowledge based approach which rivals
the supervised approaches by using the semantic
relations automatically extracted from Wikipedia.
They reported approximately 7% gain over the
closet supervised approach.
In this paper, we delve deep into the cognitive roles
associated with sense disambiguation through the
means of an eye-tracking device capturing the gaze
patterns of lexicographers, during the annotation
process. In-depth discussions with trained lexicog-
raphers indicate that there are multiple cognitive
sub-processes driving the sense disambiguation
task. The eye movement paths available from the
screen recordings done during sense annotation
conform to this theory.
Khapra et al (2011) points out that the accuracy
of various WSD algorithms is poor on certain
733
Part-of-speech (POS) categories, particularly, verbs.
It is also a general observation for lexicographers
involved in sense annotation that there are different
levels of difficulties associated with various classes
of words. This fact is also reflected in our analysis
on sense annotation. The data available after the
eye-tracking experiments gave us the fixation times
and saccades pertaining to different classes of
words. From the analysis of this data we draw
conclusive remarks regarding the reasons behind
this phenomenon. In our case, we classified words
based on their POS categories.
In this paper, we establish that contextual evidence is
the prime parameter for the human annotation. Fur-
ther, we probe into the implication of context used
as a clue for sense disambiguation, and the manner
of its usage. In this work, we address the following
questions:
? What are the cognitive sub-processes associ-
ated with the human sense annotation task?
? Which classes of words are more difficult to dis-
ambiguate and why?
By providing relevant answers to these questions we
intend to present a comprehensive understanding of
sense annotation as a complex cognitive process and
the factors involved in it. The remainder of this pa-
per is organized as follows. Section 2 contains re-
lated work. In section 3 we present the experimental
setup. Section 4 displays the results. We summarize
our findings in section 5. Finally, we conclude the
paper in section 6 presenting the future work.
2 Related Work
As mentioned earlier, we used the eye-tracking
device to ascertain the fact that contextual evidence
is the prime parameter for human sense annotation
as quoted by Chatterjee et al (2012) who used dif-
ferent annotation scenarios to compare human and
machine annotation processes. An eye movement
experiment was conducted by Vainio et al (2009)
to examine effects of local lexical predictability
on fixation durations and fixation locations during
sentence reading. Their study indicates that local
lexical predictability influences in decisions but not
where the initial fixation lands in a word. In another
work based on word grouping hypothesis and eye
movements during reading by Drieghe et al (2008),
the distribution of landing positions and durations of
first fixations in a region containing a noun preceded
by either an article or a high-frequency three-letter
word were compared.
Recently, some work is done on the study of sense
annotation. A study of sense annotations done on 10
polysemous words was conducted by Passonneau
et al (2010). They opined that the word meanings,
contexts of use, and individual differences among
annotators gives rise to inter-annotation variations.
De Melo et al (2012) present a study with a
focus on MASC (Manually-Annotated SubCorpus)
project, involving annotations done using WordNet
sense identifiers as well as FrameNet lexical units.
In our current work we use eye-tracking as a tool
to make findings regarding the cognitive processes
connected to the human sense disambiguation
procedure, and to gain a better understanding
of ?contextual evidence? which is of paramount
importance for human annotation. Unfortunately,
our work seems to be a first of its kind, and to the
best of our knowledge we do not know of any such
work done before in the literature.
3 Experimental Setup
We used a generic domain (viz., News) corpus in
Hindi language for experimental purposes. To iden-
tify the levels of difficulties associated with human
annotation, across various POS categories, we con-
ducted experiments on around 2000 words (includ-
ing function words and stop words). The analysis
was done only for open class words. The statistics
pertaining to the our experiment are illustrated in ta-
ble 1. For statistical significance of our experiments,
we collected the data with the help of 3 skilled lexi-
cographers and 3 unskilled lexicographers.
POS Noun Verb Adjective Adverb
#(senses) 2.423 3.814 2.602 3.723
#(tokens) 452 206 96 177
Table 1: Number of words (tokens) and average degree
of corpus polysemy (senses) of words per POS category
(taken from Hindi News domain) used for experiments
For our experiments we used a Sense Annotation
734
Figure 1: Sense marker tool showing an example Hindi sentence in the Context Window and the wordnet synsets of
the highlighted word in the Synset Window with the black dots and lines indicating the scan path
Tool, designed at IIT Bombay and an eye-tracking
device. The details of the tools and their purposes
are explained below:
3.1 The Sense Marker Tool
A word may have a number of senses, and the task
of identifying and marking which particular sense
has been used in the given context, is known as
sense marking.
The Sense Marker tool1 is a Graphical User Inter-
face based tool developed using Java, which facil-
itates the task of manual sense marking. This tool
displays the senses of the word as available in the
Marathi, Hindi and Princeton (English) WordNets
and allows the user to select the correct sense of the
word from the candidate senses.
3.2 Eye-Tracking device
An eye tracker is a device for measuring eye posi-
tions and eye movement. A saccade denotes move-
1http://www.cse.iitb.ac.in/s?alilj/resources
/SenseMarker/SenseMarkerTool.zip
ment to another position. The resulting series of fix-
ations and saccades is called a scan path. Figure 1
shows a sample scan path. In our experiments, we
have used an eye tracking device manufactured by
SensoMotoric Instruments2. We recorded saccades,
fixations, length of each fixation and scan paths on
the stimulus monitor during the annotation process.
A remote eye-tracking device (RED) measures gaze
hotspots on a stimulus monitor.
4 Results
In our experiments, each lexicographer performed
sense annotation on the stimulus monitor of the
eye tracking device. Fixation times, saccades
and scan paths were recorded during the sense
annotation process. We analyzed this data and the
corresponding observations are enumerated below.
Figure 2 shows the annotation time taken by differ-
ent lexicographers across POS categories. It can be
observed that the time taken for disambiguating the
verbs is significantly higher than the remaining POS
2http://www.smivision.com/
735
Unskilled Lexicographer (Seconds) Skilled Lexicographer (Seconds)
Word Degree of
polysemy
Thypo Tclue Tgloss Ttotal Thypo Tclue Tgloss Ttotal
lAnA (laanaa - to bring) 4 0.63 0.80 5.20 6.63 0.31 1.20 1.82 3.30
krnA (karanaa - to do) 22 0.90 1.42 2.20 4.53 0.50 0.64 1.14 2.24
jtAnA (jataanaa - to express) 4 0.70 2.45 5.93 9.09 0.25 0.39 0.62 1.19
Table 2: Comparison of time taken across different cognitive stages of sense annotation by lexicographers for verbs
Figure 2: Histogram showing time taken (in seconds) by
each lexicographer across POS categories for sense anno-
tation
categories. This behavior can be consistently seen
in the timings recorded for all the six lexicographers.
Table 2 presents the comparison of time taken
across different cognitive stages of sense annotation
by lexicographers for some of the most frequently
occurring verbs.
To know if the results gathered from all the lexicog-
raphers are consistent, we present the correlation be-
tween each pair of lexicographers in table 3. The
table also shows the value of the t-test statistic gen-
erated for each pair of lexicographers.
5 Discussion
The data obtained from the eye-tracking device and
corresponding analysis of the fixation times, sac-
cades and scan paths of the lexicographers? eyes re-
veal that sense annotation is a complex cognitive
process. From the videos of the scan paths obtained
from the eye-tracking device and from detailed dis-
cussion with lexicographers it can be inferred that
this cognitive process can be broken down into 3
stages:
1. When a lexicographer sees a word, he/she
makes a hypothesis about the domain and con-
sequently about the correct sense of the word,
mentally. In cases of highly polysemous words,
the hypothesis may narrow down to multiple
senses. We denote the time required for this
phase as Thypo.
2. Next the lexicographer searches for clues to
support this hypothesis and in some cases to
eliminate false hypotheses, when the word is
polysemous. These clues are available in the
form of neighboring words around the target
word. We denote the time required for this ac-
tivity as Tclue.
3. The clue words aid the lexicographer to decide
which one of the initial hypotheses was true.
To narrow down the candidate synsets, the lex-
icographers use synonyms of the words in a
synset to check if the sentence retains its mean-
ing.
From the scan paths and fixation times obtained
from the eye-tracking experiment, it is evident that
stages 1, 2 and 3 are chronological stages in the hu-
man cognitive process associated with sense disam-
biguation. In cases of highly polysemous words and
instances where senses are fine-grained, stages 2 and
3 get interleaved. It is also clear that each stage takes
up separate proportions of the sense disambiguation
time for humans. Hence time taken to disambiguate
a word using the Sense Marker Tool (as explained in
Section 3.1) can be factored as follows:
Ttotal = Thypo + Tclue + Tgloss
Where:
Ttotal = Total time for sense disambiguation
736
Correlation value T-test statistic
Lexicographer B C D E F B C D E F
A 0.933 0.976 0.996 0.996 0.769 0.007 0.123 0.185 0.036 0.006
B 0.987 0.960 0.915 0.945 0.009 0.028 0.084 0.026
C 0.989 0.968 0.879 0.483 0.088 0.067
D 0.988 0.820 0.367 0.709
E 0.734 0.418
Table 3: Pairwise correlation between annotation time taken by lexicographers
Thypo = Time for hypothesis building
Tclue = Clue word searching time
Tgloss = Gloss Matching time and winner sense
selection time.
The results in table 2 reveal the different ratios of
time invested during each of the above stages. Thypo
takes the minimum amount of time among the dif-
ferent sub-processes. Tgloss > Tclue in all cases.
? For unskilled lexicographers: Tgloss >> Tclue
because of errors in the initial hypothesis.
? For skilled lexicographers: Tgloss ? Tclue, as
they can identify the POS category of the word
and their hypothesis thus formed is pruned.
Hence during selection of the winner sense,
they do not browse through other POS cate-
gories, which unskilled lexicographers do.
The results shown in figure 2 reveal that verbs take
the maximum disambiguation time. In fact the
average time taken by verbs is around 75% more
than the time taken by other POS categories. This
supports the fact that verbs are the most difficult to
disambiguate.
The analysis of the scan paths and fixation times
available from the eye-tracking experiments in case
of verbs show that the Tgloss covers around 66%
of Ttotal, as shown in table 2. This means that the
lexicographer takes more time in selecting a winner
sense from the list of wordnet senses. This happens
chiefly because of following reasons:
1. Higher degree of polysemy of verbs compared
to other POS categories (as shown in tables 1
and 2).
2. In several cases the senses are fine-grained.
3. Sometimes the hypothesis of the lexicogra-
phers may not match any of the wordnet senses.
The lexicographer then selects the wordnet
sense closest to their hypothesis.
Adverbs and adjectives show higher degree of pol-
ysemy than nouns (as shown in table 1), but take
similar disambiguation time as nouns (as shown in
figure 2). In case of adverbs and adjectives, the lex-
icographer is helped by their position around a verb
or noun respectively. So, Tclue only involves search-
ing for the nearby verbs or nouns, as the case may
be, hence reducing total disambiguation time Ttotal.
6 Conclusion and Future Work
In this paper we examined the cognitive process that
enables the human sense disambiguation task. We
have also laid down our findings regarding the vary-
ing levels of difficulty in sense annotation across
different POS categories. These experiments are
just a stepping stone for going deeper into finding
the meaning and manner of usage of contextual
evidence which is fundamental to the human sense
annotation process.
In the future we aim to perform an in-depth analy-
sis of clue words that aid humans in sense disam-
biguation. The distance of clue words from the tar-
get word and their and pattern of occurrence could
give us significant insights into building a ?Discrim-
ination Net?.
References
E. Agirre, O.L. De Lacalle, A. Soroa, and I. Fakultatea.
2009. Knowledge-based wsd on specific domains:
performing better than generic supervised wsd. Pro-
ceedigns of IJCAI, pages 1501?1506.
Arindam Chatterjee, Salil Joshi, Pushpak Bhattacharyya,
Diptesh Kanojia, and Akhlesh Meena. 2012. A
737
study of the sense annotation process: Man v/s ma-
chine. In Proceedings of 6th International Conference
on Global Wordnets, January.
G. De Melo, C.F. Baker, N. Ide, R.J. Passonneau, and
C. Fellbaum. 2012. Empirical comparisons of masc
word sense annotations. In Proceedings of the 8th
international conference on language resources and
evaluation (LREC12). Istanbul: European Language
Resources Association (ELRA).
D. Drieghe, A. Pollatsek, A. Staub, and K. Rayner. 2008.
The word grouping hypothesis and eye movements
during reading. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 34(6):1552.
Mitesh M. Khapra, Salil Joshi, and Pushpak Bhat-
tacharyya. 2011. It takes two to tango: A bilingual
unsupervised approach for estimating sense distribu-
tions using expectation maximization. In Proceedings
of 5th International Joint Conference on Natural Lan-
guage Processing, pages 695?704, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
R.J. Passonneau, A. Salleb-Aouissi, V. Bhardwaj, and
N. Ide. 2010. Word sense annotation of polysemous
words by multiple annotators. Proceedings of LREC-
7, Valleta, Malta.
S.P. Ponzetto and R. Navigli. 2010. Knowledge-rich
word sense disambiguation rivaling supervised sys-
tems. In Proceedings of the 48th annual meeting of the
association for computational linguistics, pages 1522?
1531. Association for Computational Linguistics.
S. Vainio, J. Hyo?na?, and A. Pajunen. 2009. Lexical pre-
dictability exerts robust effects on fixation duration,
but not on initial landing position during reading. Ex-
perimental psychology, 56(1):66.
738
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1346?1356,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Multilingual Pseudo-Relevance Feedback: Performance Study of
Assisting Languages
Manoj K. Chinnakotla Karthik Raman Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay,
Mumbai, India
{manoj,karthikr,pb}@cse.iitb.ac.in
Abstract
In a previous work of ours Chinnakotla
et al (2010) we introduced a novel
framework for Pseudo-Relevance Feed-
back (PRF) called MultiPRF. Given a
query in one language called Source, we
used English as the Assisting Language to
improve the performance of PRF for the
source language. MulitiPRF showed re-
markable improvement over plain Model
Based Feedback (MBF) uniformly for 4
languages, viz., French, German, Hungar-
ian and Finnish with English as the as-
sisting language. This fact inspired us
to study the effect of any source-assistant
pair on MultiPRF performance from out
of a set of languages with widely differ-
ent characteristics, viz., Dutch, English,
Finnish, French, German and Spanish.
Carrying this further, we looked into the
effect of using two assisting languages to-
gether on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions
drawn therefrom. While performance im-
provement on MultiPRF is observed what-
ever the assisting language and whatever
the source, observations are mixed when
two assisting languages are used simul-
taneously. Interestingly, the performance
improvement is more pronounced when
the source and assisting languages are
closely related, e.g., French and Spanish.
1 Introduction
The central problem of Information Retrieval (IR)
is to satisfy the user?s information need, which is
typically expressed through a short (typically 2-3
words) and often ambiguous query. The problem
of matching the user?s query to the documents is
rendered difficult by natural language phenomena
like morphological variations, polysemy and syn-
onymy. Relevance Feedback (RF) tries to over-
come these problems by eliciting user feedback
on the relevance of documents obtained from the
initial ranking and then uses it to automatically
refine the query. Since user input is hard to ob-
tain, Pseudo-Relevance Feedback (PRF) (Buckley
et al, 1994; Xu and Croft, 2000; Mitra et al, 1998)
is used as an alternative, wherein RF is performed
by assuming the top k documents from the initial
retrieval as being relevant to the query. Based on
the above assumption, the terms in the feedback
document set are analyzed to choose the most dis-
tinguishing set of terms that characterize the feed-
back documents and as a result the relevance of
a document. Query refinement is done by adding
the terms obtained through PRF, along with their
weights, to the actual query.
Although PRF has been shown to improve re-
trieval, it suffers from the following drawbacks:
(a) the type of term associations obtained for query
expansion is restricted to co-occurrence based re-
lationships in the feedback documents, and thus
other types of term associations such as lexical and
semantic relations (morphological variants, syn-
onyms) are not explicitly captured, and (b) due to
the inherent assumption in PRF, i.e., relevance of
top k documents, performance is sensitive to that
of the initial retrieval algorithm and as a result is
not robust.
Multilingual Pseudo-Relevance Feedback
(MultiPRF) (Chinnakotla et al, 2010) is a novel
framework for PRF to overcome both the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
In MultiPRF, given a query in source language
L1, the query is automatically translated into
the assisting language L2 and PRF performed
in the assisting language. The resultant terms
are translated back into L1 using a probabilistic
bi-lingual dictionary. The translated feedback
1346
model, is then combined with the original feed-
back model of L1 to obtain the final model which
is used to re-rank the corpus. MulitiPRF showed
remarkable improvement on standard CLEF
collections over plain Model Based Feedback
(MBF) uniformly for 4 languages, viz., French,
German, Hungarian and Finnish with English as
the assisting language. This fact inspired us to
study the effect of any source-assistant pair on
PRF performance from out of a set of languages
with widely different characteristics, viz., Dutch,
English, Finnish, French, German and Spanish.
Carrying this further, we looked into the effect of
using two assisting languages together on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions drawn
therefrom. While performance improvement on
PRF is observed whatever the assisting language
and whatever the source, observations are mixed
when two assisting languages are used simulta-
neously. Interestingly, the performance improve-
ment is more pronounced when the source and as-
sisting languages are closely related, e.g., French
and Spanish.
The paper is organized as follows: Section 2,
discusses the related work. Section 3, explains the
Language Modeling (LM) based PRF approach.
Section 4, describes the MultiPRF approach. Sec-
tion 5 discusses the experimental set up. Section 6
presents the results, and studies the effect of vary-
ing the assisting language and incorporates mul-
tiple assisting languages. Finally, Section 7 con-
cludes the paper by summarizing and outlining fu-
ture work.
2 Related Work
PRF has been successfully applied in various IR
frameworks like vector space models, probabilis-
tic IR and language modeling (Buckley et al,
1994; Jones et al, 2000; Lavrenko and Croft,
2001; Zhai and Lafferty, 2001). Several ap-
proaches have been proposed to improve the per-
formance and robustness of PRF. Some of the rep-
resentative techniques are (i) Refining the feed-
back document set (Mitra et al, 1998; Sakai et
al., 2005), (ii) Refining the terms obtained through
PRF by selecting good expansion terms (Cao et
al., 2008) and (iii) Using selective query expan-
sion (Amati et al, 2004; Cronen-Townsend et al,
2004) and (iv) Varying the importance of docu-
ments in the feedback set (Tao and Zhai, 2006).
Another direction of work, often reported in the
TREC Robust Track, is to use a large external col-
lection like Wikipedia or the Web as a source of
expansion terms (Xu et al, 2009; Voorhees, 2006).
The intuition behind the above approach is that
if the query does not have many relevant docu-
ments in the collection then any improvements in
the modeling of PRF is bound to perform poorly
due to query drift.
Several approaches have been proposed for
including different types of lexically and se-
mantically related terms during query expansion.
Voorhees (1994) use Wordnet for query expan-
sion and report negative results. Recently, random
walk models (Lafferty and Zhai, 2001; Collins-
Thompson and Callan, 2005) have been used to
learn a rich set of term level associations by com-
bining evidence from various kinds of information
sources like WordNet, Web etc. Metzler and Croft
(2007) propose a feature based approach called la-
tent concept expansion to model term dependen-
cies.
All the above mentioned approaches use the re-
sources available within the language to improve
the performance of PRF. However, we make use of
a second language to improve the performance of
PRF. Our proposed approach is especially attrac-
tive in the case of resource-constrained languages
where the original retrieval is bad due to poor cov-
erage of the collection and/or inherent complexity
of query processing (for example term conflation)
in those languages.
Jourlin et al (1999) use parallel blind relevance
feedback, i.e. they use blind relevance feedback on
a larger, more reliable parallel corpus, to improve
retrieval performance on imperfect transcriptions
of speech. Another related idea is by Xu et al
(2002), where a statistical thesaurus is learned us-
ing the probabilistic bilingual dictionaries of Ara-
bic to English and English to Arabic. Meij et
al. (2009) tries to expand a query in a differ-
ent language using language models for domain-
specific retrieval, but in a very different setting.
Since our method uses a corpus in the assisting
language from a similar time period, it can be
likened to the work by Talvensaari et al (2007)
who used comparable corpora for Cross-Lingual
Information Retrieval (CLIR). Other work pertain-
ing to document alignment in comparable corpora,
such as Braschler and Scha?uble (1998), Lavrenko
et al (2002), also share certain common themes
with our approach. Recent work by Gao et al
1347
(2008) uses English to improve the performance
over a subset of Chinese queries whose transla-
tions in English are unambiguous. They use inter-
document similarities across languages to improve
the ranking performance. However, cross lan-
guage document similarity measurement is in it-
self known to be an hard problem and the scale of
their experimentation is quite small.
3 PRF in the LM Framework
The Language Modeling (LM) Framework allows
PRF to be modelled in a principled manner. In the
LM approach, documents and queries are modeled
using multinomial distribution over words called
document language model P (w|D) and query lan-
guage model P (w|?Q) respectively. For a given
query, the document language models are ranked
based on their proximity to the query language
model, measured using KL-Divergence.
KL(?Q||D) =
X
w
P (w|?Q) ? log
P (w|?Q)
P (w|D)
Since the query length is short, it is difficult to es-
timate ?Q accurately using the query alone. In
PRF, the top k documents obtained through the ini-
tial ranking algorithm are assumed to be relevant
and used as feedback for improving the estima-
tion of ?Q. The feedback documents contain both
relevant and noisy terms from which the feedback
language model is inferred based on a Generative
Mixture Model (Zhai and Lafferty, 2001).
Let DF = {d1, d2, . . . , dk} be the top k docu-
ments retrieved using the initial ranking algorithm.
Zhai and Lafferty (Zhai and Lafferty, 2001) model
the feedback document setDF as a mixture of two
distributions: (a) the feedback language model and
(b) the collection model P (w|C). The feedback
language model is inferred using the EM Algo-
rithm (Dempster et al, 1977), which iteratively
accumulates probability mass on the most distin-
guishing terms, i.e. terms which are more fre-
quent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, ?F is interpolated
with the initial query model ?Q to obtain the final
query model ?Final.
?Final = (1? ?) ??Q + ? ??F
?Final is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
Initial Retrieval Algorithm(LM Based Query Likelihood)
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results Top ?k? Results
PRF(Model Based Feedback)
PRF(Model Based Feedback)
L1 Index L2  Index
Final Ranked List Of Documents in L1
FeedbackModel Interpolation Relevance ModelTranslation 
KL-Divergence Ranking Function
Feedback Model ?L2Feedback Model ?L1
Query in L1 Translated Query to L2
ProbabilisticDictionaryL2? L1
TranslatedFeedback Model
Query Model ?Q
Figure 1: Schematic of the Multilingual PRF Approach
Symbol Description
?Q Query Language Model
?FL1 Feedback Language Model obtained from PRF in L1
?FL2 Feedback Language Model obtained from PRF in L2
?TransL1 Feedback Model Translated from L2 to L1
t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1
?, ? Interpolation coefficients coefficients used in MultiPRF
Table 2: Glossary of Symbols used in explaining MultiPRF
to the above technique as Model Based Feedback
(MBF).
4 Multilingual PRF (MultiPRF)
The schematic of the MultiPRF approach is shown
in Figure 1. Given a query Q in the source lan-
guage L1, we automatically translate the query
into the assisting language L2. We then rank the
documents in the L2 collection using the query
likelihood ranking function (John Lafferty and
Chengxiang Zhai, 2003). Using the top k doc-
uments, we estimate the feedback model using
MBF as described in the previous section. Simi-
larly, we also estimate a feedback model using the
original query and the top k documents retrieved
from the initial ranking in L1. Let the resultant
feedback models be ?FL2 and ?
F
L1 respectively.
The feedback model estimated in the assisting lan-
guage ?FL2 is translated back into language L1
using a probabilistic bi-lingual dictionary t(f |e)
from L2 ? L1 as follows:
P (f |?TransL1 ) =
X
? e in L2
t(f |e) ? P (e|?FL2 ) (1)
The probabilistic bi-lingual dictionary t(f |e) is
1348
Language CLEF Collection Identifier Description
No. of 
Documents
No. of Unique 
Terms
CLEF Topics 
(No. of Topics)
English
EN-00+01+02 LA Times 94 113005 174669 -
EN-03+05+06 LA Times 94, Glasgow Herald 95 169477 234083 -
EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French
FR-00 Le Monde 94 44013 127065 1-40 (29) 
FR-01+02 Le Monde 94, French SDA 94  87191 159809 41-140 (88) 
FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
FR-03+05 Le Monde 94, French SDA 94-95 129806 182214 141-200,251-300 (99) 
FR-06 Le Monde 94-95, French SDA 94-95 177452 231429 301-350 (48) 
German
DE-00 Frankfurter Rundschau 94, Der Spiegel 94-95 153694 791093 1-40 (33) 
DE-01+02 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94 225371 782304 41-140 (85) 
DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)
DE-03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 141-200 (51) 
Finnish FI-02+03+04 Aamulehti 94-95 55344 531160 91-250 (119) FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
Table 1: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
Source Term Top Aligned Terms in Target
French English
ame?ricain american, us, united, state, america
nation nation, un, united, state, country
et?ude study, research, assess, investigate, survey
German English
flugzeug aircraft, plane, aeroplane, air, flight
spiele play, game, stake, role, player
verha?ltnis relationship, relate, balance, proportion
Table 3: Top Translation Alternatives for some sample words
in Probabilistic Bi-Lingual Dictionary
learned from a parallel sentence-aligned corpora
in L1?L2 based on word level alignments. Tiede-
mann (Tiedemann, 2001) has shown that the trans-
lation alternatives found using word alignments
could be used to infer various morphological and
semantic relations between terms. In Table 3,
we show the top translation alternatives for some
sample words. For example, the French word
ame?ricain (american) brings different variants of
the translation like american, america, us, united,
state, america which are lexically and semanti-
cally related. Hence, the probabilistic bi-lingual
dictionary acts as a rich source of morphologically
and semantically related feedback terms. Thus,
during this step, of translating the feedback model
as given in Equation 1, the translation model adds
related terms in L1 which have their source as the
term from feedback model ?FL2 . The final Multi-
PRF model is obtained by interpolating the above
translated feedback model with the original query
model and the feedback model of language L1 as
given below:
?MultiL1 = (1? ? ? ?) ??Q + ? ??
F
L1
+ ? ??TransL1 (2)
Since we want to retain the query focus during
back translation the feedback model in L2 is inter-
polated with the translated query before transla-
tion of the L2 feedback model. The parameters ?
and ? control the relative importance of the orig-
inal query model, feedback model of L1 and the
translated feedback model obtained from L1 and
are tuned based on the choice of L1 and L2.
5 Experimental Setup
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Span-
ish and Finnish using more than 600 topics. The
details of the collections and their corresponding
topics used for MultiPRF are given in Table 1.
Note that, in each experiment, we choose assist-
ing collections such that the topics in the source
language are covered in the assisting collection so
as to get meaningful feedback terms. In all the top-
ics, we only use the title field. We ignore the top-
ics which have no relevant documents as the true
performance on those topics cannot be evaluated.
We demonstrate the performance of MultiPRF
approach with French, German and Finnish as
source languages and Dutch, English and Span-
ish as the assisting language. We later vary the
assisting language, for each source language and
study the effects. We use the Terrier IR platform
(Ounis et al, 2005) for indexing the documents.
We perform standard tokenization, stop word re-
moval and stemming. We use the Porter Stemmer
for English and the stemmers available through the
Snowball package for other languages. Other than
these, we do not perform any language-specific
processing on the languages. In case of French,
1349
Collection Assist. Lang P@5 P@10 M AP GMAPM BF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr.
FR - 00 EN 0.4690 0.5241 11.76
?
0.4000 0.4000 0.00 0.4220 0.4393 4.10 0.2961 0.3413 15.27ES 0.5034 7.35 ? 0.4103 2.59 0.4418 4.69 0.3382 14.22NL 0.5034 7.35 0.4103 2.59 0.4451 5.47 0.3445 16.34
FR - 01+02 EN 0.4636 0.4818 3.92 0.4068 0.4386 7.82
?
0.4342 0.4535 4.43
?
0.2395 0.2721 13.61ES 0.4977 7.35 ? 0.4363 7.26 ? 0.4416 1.70 0.2349 -1.92NL 0.4818 3.92 0.4409 8.38 ? 0.4375 0.76 0.2534 5.80
FR - 03+05 EN 0.4545 0.4768 4.89
?
0.4040 0.4202 4
?
0.3529 0.3694 4.67
?
0.1324 0.1411 6.57ES 0.4727 4.00 0.4080 1.00 0.3582 1.50 0.1325 0.07NL 0.4525 -0.44 0.4010 -0.75 0.3513 0.45 0.1319 -0.38
FR - 06 EN 0.4917 0.5083 3.39 0.4625 0.4729 2.25 0.3837 0.4104 6.97 0.2174 0.2810 29.25ES 0.5083 3.39 0.4687 1.35 0.3918 2.12 0.2617 20.38NL 0.5083 3.39 0.4646 0.45 0.3864 0.71 0.2266 4.23
DE- 00 EN 0.2303 0.3212 39.47
?
0.2394 0.2939 22.78
?
0.2158 0.2273 5.31 0.0023 0.0191 730.43ES 0.3212 39.47 ? 0.2818 17.71 ? 0.2376 10.09 0.0123 434.78NL 0.3151 36.82 ? 0.2818 17.71 ? 0.2331 8.00 0.0122 430.43
DE- 01+02 EN 0.5341 0.6000 12.34
?
0.4864 0.5318 9.35
?
0.4229 0.4576 8.2
?
0.1765 0.2721 9.19ES 0.5682 6.39 ? 0.5091 4.67 ? 0.4459 5.43 0.2309 30.82NL 0.5773 8.09 ? 0.5114 5.15 ? 0.4498 6.35 ? 0.2355 33.43
DE- 03 EN 0.5098 0.5412 6.15 0.4784 0.4980 4.10 0.4274 0.4355 1.91 0.1243 0.1771 42.48ES 0.5647 10.77 ? 0.4980 4.10 0.4568 6.89 ? 0.1645 32.34NL 0.5529 8.45 ? 0.4941 3.27 0.4347 1.72 0.1490 19.87
FI- 02+03+04 EN 0.3782 0.4034 6.67
?
0.3059 0.3319 8.52
?
0.3966 0.4246 7.06
?
0.1344 0.2272 69.05ES 0.3879 2.58 0.3267 6.81 0.3881 -2.15 0.1755 30.58NL 0.3948 4.40 0.3301 7.92 0.4077 2.79 0.1839 36.83
Table 4: Results comparing the performance of MultiPRF over baseline MBF on CLEF collections with English (EN), Spanish
(ES) and Dutch (NL) as assisting languages. Results marked as ? indicate that the improvement was found to be statistically
significant over the baseline at 90% confidence level (? = 0.01) when tested using a paired two-tailed t-test.
since some function words like l?, d? etc., occur as
prefixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@5 and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary used
in MultiPRF was learnt automatically by running
GIZA++: a word alignment tool (Och and Ney,
2003) on a parallel sentence aligned corpora. For
all the above language pairs we used the Europarl
Corpus (Philipp, 2005). We use Google Trans-
late as the query translation system as it has been
shown to perform well for the task (Wu et al,
2008). We use the MBF approach explained in
Section 3 as a baseline for comparison. We use
two-stage Dirichlet smoothing with the optimal
parameters tuned based on the collection (Zhai and
Lafferty, 2004). We tune the parameters of MBF,
specifically ? and ?, and choose the values which
give the optimal performance on a given collec-
tion. We uniformly choose the top ten documents
for feedback. Table 4 gives the overall results.
6 Results and Discussion
In Table 4, we see the performance of the Multi-
PRF approach for three assisting languages, and
how it compares with the baseline MBF meth-
ods. We find MultiPRF to consistently outperform
the baseline value on all metrics, namely MAP
(where significant improvements range from 4.4%
to 7.1%); P@5 (significant improvements range
from 4.9% to 39.5% and P@10 (where MultiPRF
has significant gains varying from 4% to 22.8%).
Additionally we also find MultiPRF to be more ro-
bust than the baseline, as indicated by the GMAP
score, where improvements vary from 4.2% to
730%. Furthermore we notice these trends hold
across different assisting languages, with Span-
ish and Dutch outperforming English as the as-
sisting language on some of the French and Ger-
man collections. On performing a more detailed
study of the results we identify the main reason
for improvements in our approach is the ability to
obtain good feedback terms in the assisting lan-
guage coupled with the introduction of lexically
and semantically related terms during the back-
translation step.
In Table 5, we see some examples, which illus-
trates the feedback terms brought by the MultiPRF
method. As can be seen by these example, the
gains achieved by MultiPRF are primarily due to
one of three reasons: (a) Good Feedback in As-
sisting Language: If the feedback model in the
assisting language contains good terms, then the
back-translation process will introduce the corre-
sponding feedback terms in the source language,
thus leading to improved performance. As an
example of this phenomena, consider the French
Query ?Maladie de Creutzfeldt-Jakob?. In this
case the original feedback model also performs
1350
TOPIC NO
ASSIST 
LANG.
SOURCE LANGUAGE 
QUERY
TRANSLATED 
QUERY
QUERY 
MEANING
M BF 
M AP
M P RF 
M AP
M BF - Top Representative Terms 
(With Meaning) Excl. Query 
Terms
MultiPRF - Top Representative 
Terms (With Meaning) Excl. Query 
Terms
GERMAN '01: 
TOPIC 61 EN
?lkatastrophe in 
Sibirien Oil Spill in Siberia
Siberian Oil 
Catastrophe 0.618 0.812
exxon , million,  ol (oil), tonn, 
russisch (russian), olp (oil), 
moskau (moscow), us
olverschmutz (oil pollution), ol, 
russisch, erdol (petroleum), russland
(russia), olunfall(oil spill), olp
GERMAN '02: 
TOPIC 105 ES Bronchialasthma El asma bronquial
Bronchial 
Asthma 0.062 0.636
chronisch (chronic), pet, athlet 
(athlete), ekrank (ill), gesund 
(healthy),  tuberkulos 
(tuberculosis), patient, reis (rice), 
person
asthma, allergi, krankheit (disease), 
allerg (allergenic), chronisch, 
hauterkrank (illness of skin), arzt 
(doctor), erkrank (ill)
FRENCH '02: 
TOPIC 107 NL Ing?nierie g?n?tique
Genetische 
Manipulatie
Genetic 
Engineering 0.145 0.357
d?velopp (developed), ?volu 
(evolved), product, produit 
(product), mol?culair (molecular)
genetic, gen, engineering, d?velopp, 
product
FRENCH '06: 
TOPIC 256 EN
Maladie de 
Creutzfeldt -Jakob Creutzfeldt -Jakob
Creutzfeldt -
Jakob Disease 0.507 0.688
malad (illness), produit (product), 
animal (animal), hormon 
(hormone)
malad, humain (human), bovin 
(bovine), enc?phalopath (suffering 
from encephalitis), scientif, recherch 
(research)
GERMAN '03: 
TOPIC 157 EN
Siegerinnen von 
Wimbledon
Champions of 
Wimbledon
Wimbledon 
Lady Winners 0.074 0.146
telefonbuch (phone book), sieg 
(victory), titelseit (front page), 
telekom (telecommunication), 
graf
gross (large), verfecht (champion), 
sampra (sampras), 6, champion, 
steffi, verteidigt (defendending), 
martina, jovotna , navratilova
GERMAN '01: 
TOPIC 91 ES AI in Lateinamerika
La gripe aviar en 
Am?rica Latina
AI in Latin 
America 0.456 0.098
international, amnesty, 
strassenkind (street child),  
kolumbi (Columbian), land, brasili
(Brazil), menschenrecht (human 
rights), polizei (police)
karib (Caribbean), land, brasili, 
schuld (blame), amerika, kalt (cold), 
welt (world), forschung (research)
GERMAN '03: 
TOPIC 196 EN
Fusion japanischer 
Banken
Fusion of Japanese 
banks
Merger of 
Japanese Banks 0.572 0.264
daiwa, tokyo, filial (branch), 
zusammenschluss (merger)
kernfusion (nuclear fusion), 
zentralbank (central bank), daiwa, 
weltbank (world bank), 
investitionsbank (investment bank)
FRENCH '03: 
TOPIC 152 NL Les droits de l'enfant
De rechten van het 
kind Child Rights 0.479 0.284
convent (convention), franc, 
international, onun (united 
nations), r?serv (reserve)
per (father), convent, franc, jurid
(legal), homm (man), cour (court), 
biolog
Table 5: Qualitative comparison of feedback terms given by MultiPRF and MBF on representative queries where positive and
negative results were observed in French and German collections.
quite strongly with a MAP score of 0.507. Al-
though there is no significant topic drift in this
case, there are not many relevant terms apart from
the query terms. However the same query per-
forms very well in English with all the documents
in the feedback set of the English corpus being rel-
evant, thus resulting in informative feedback terms
such as {bovin, scientif, recherch}. (b) Finding
Synonyms/Morphological Variations: Another sit-
uation in which MultiPRF leads to large improve-
ments is when it finds semantically/lexically re-
lated terms to the query terms which the origi-
nal feedback model was unable to. For example,
consider the French query ?Inge?nierie g?n?tique?.
While the feedback model was unable to find
any of the synonyms of the query terms, due to
their lack of co-occurence with the query terms,
the MultiPRF model was able to get these terms,
which are introduced primarily during the back-
translation process. Thus terms like {genetic, gen,
engineering}, which are synonyms of the query
words, are found thus resulting in improved per-
formance. (c) Combination of Above Factors:
Sometimes a combination of the above two factors
causes improvements in the performance as in the
German query ?O?lkatastrophein Sibirien?. For
this query, MultiPRF finds good feedback terms
such as {russisch, russland} while also obtaining
semantically related terms such as {olverschmutz,
erdol, olunfall}.
Although all of the previously described exam-
ples had good quality translations of the query
in the assisting language, as mentioned in (Chin-
nakotla et al, 2010), the MultiPRF approach is
robust to suboptimal translation quality as well.
To see how MultiPRF leads to improvements even
with errors in query translation consider the Ger-
man Query ?Siegerinnen von Wimbledon?. When
this is translated to English, the term ?Lady? is
dropped, this causes only ?Wimbledon Champi-
ons? to remain. As can be observed, this causes
terms like sampras to come up in the MultiPRF
model. However, while the MultiPRF model has
some terms pertaining to Men?s Winners of Wim-
bledon as well, the original feedback model suf-
fers from severe topic drift, with irrelevant terms
such as {telefonbuch, telekom} also amongst the
top terms. Thus we notice that despite the er-
ror in query translation MultiPRF still manages to
correct the drift of the original feedback model,
while also introducing relevant terms such as
{verfecht, steffi, martina, novotna, navratilova}
as well. Thus as shown in (Chinnakotla et al,
2010), having a better query translation system
can only lead to better performance. We also
perform a detailed error analysis and found three
main reasons for MultiPRF failing: (i) Inaccura-
cies in query translation (including the presence of
out-of-vocabulary terms). This is seen in the Ger-
man Query AI in Lateinamerika, which wrongly
translates to Avian Flu in Latin America in Span-
ish thus affecting performance. (ii) Poor retrieval
in Assisting Language. Consider the French query
Les droits de l?enfant, for which due to topic drift
in English, MultiPRF performance reduces. (iii)
In a few rare cases inaccuracy in the back transla-
1351
(a) Source:French (FR-01+02) Assist:Spanish (b) Source:German (DE-01+02) Assist:Dutch
(c) Source:Finnish (FI-02+03+04) Assist:English
Figure 2: Results showing the sensitivity of MultiPRF performance to parameters ? and ? for French, German and Finnish.
tion affects performance as well.
6.1 Parameter Sensitivity Analysis
The MultiPRF parameters ? and ? in Equation
2 control the relative importance assigned to the
original feedback model in source language L1,
the translated feedback model obtained from as-
sisting language L2 and the original query terms.
We varied the ? and ? parameters for French, Ger-
man and Finnish collections with English, Dutch
and Spanish as assisting languages and studied its
effect on MAP of MultiPRF. The results are shown
in Figure 2. The results show that, in all the three
collections, the optimal value of the parameters
almost remains the same and lies in the range of
0.4-0.48. Due to the above reason, we arbitrarily
choose the parameters in the above range and do
not use any technique to learn these parameters.
6.2 Effect of Assisting Language Choice
In this section, we discuss the effect of varying
the assisting language. Besides, we also study
the inter and intra familial behaviour of source-
assisting language pairs. In order to ensure that
the results are comparable across languages, we
indexed the collections from the years 2002, 2003
and use common topics from the topic range 91-
200 that have relevant documents across all the six
languages. The number of such common topics
were 67. For each source language, we use the
other languages as assisting collections and study
the performance of MultiPRF. Since query trans-
lation quality varies across language pairs, we an-
alyze the behaviour of MultiPRF in the following
two scenarios: (a) Using ideal query translation
(b) Using Google Translate for query translation.
In ideal query translation setup, in order to elim-
inate its effect, we skip the query translation step
and use the corresponding original topics for each
target language instead. The results for both the
above scenarios are given in Tables 6 and 7.
From the results, we firstly observe that besides
English, other languages such as French, Spanish,
German and Dutch act as good assisting languages
and help in improving performance over mono-
lingual MBF. We also observe that the best as-
sisting language varies with the source language.
However, the crucial factors of the assisting lan-
guage which influence the performance of Multi-
PRF are: (a) Monolingual PRF Performance: The
main motivation for using a different language was
to get good feedback terms, especially in case of
queries which fail in the source language. Hence,
an assisting language in which the monolingual
feedback performance itself is poor, is unlikely
to give any performance gains. This observation
is evident in case of Finnish, which has the low-
est Monolingual MBF performance. The results
show that Finnish is the least helpful of assist-
ing languages, with performance similar to those
of the baselines. We also observe that the three
best performing assistant languages, i.e. English,
French and Spanish, have the highest monolingual
performances as well, thus further validating the
claim. One possible reason for this is the relative
1352
Source 
Lang.
Assisting Language Source 
Lang.MBF English German Dutch Spanish French Finnish 
English 
MAP 
-
0.4464 ( -0.7%) 0.4471 (-0.5%) 0.4566 (+1.6%) 0.4563 (+1.5%) 0.4545 (+1.1%) 0.4495
P@5 0.4925 ( -0.6%) 0.5045 (+1.8%) 0.5164 (+4.2%) 0.5075 (+2.4%) 0.5194 (+4.8%) 0.4955
P@10 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4537 (+4.8%) 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4328
German 
MAP 0.4229 (+4.9%)
-
0.4346 (+7.8%) 0.4314 (+7.0%) 0.411 (+1.9%) 0.3863 ( -4.2%) 0.4033
P@5 0.5851 (+14%) 0.5851 (+14%) 0.5791 (+12.8%) 0.594 (+15.7%) 0.5522 (+7.6%) 0.5134
P@10 0.5284 (+11.3%) 0.5209 (+9.8%) 0.5179 (+9.1%) 0.5149 (+8.5%) 0.5075 (+6.9%) 0.4746
Dutch 
MAP 0.4317 (+4%) 0.4453 (+7.2%)
-
0.4275 (+2.9%) 0.4241 (+2.1%) 0.3971 ( -4.4%) 0.4153
P@5 0.5642 (+11.8%) 0.5731 (+13.6%) 0.5343 (+5.9%) 0.5582 (+10.6%) 0.5045 (0%) 0.5045
P @10 0.5075 (+9%) 0.4925 (+5.8%) 0.4896 (+5.1%) 0.5015 (+7.7%) 0.4806 (+3.2%) 0.4657
Spanish MAP 0.4667 ( -2.9%) 0.4749 ( -1.2%) 0.4744 (-1.3%)
-
0.4609 ( -4.1%)
0.4311 ( -
10.3%) 0.4805
P@5 0.62 ( -2.9%) 0.6418 (+0.5%) 0.6299 (-1.4%) 0.6269 ( -1.6%) 0.6149 ( -3.7%) 0.6388
P@10 0.5625 ( -1.8%) 0.5806 (+1.3%) 0.5851 (+2.1%) 0.5627 ( -1.8%) 0.5478 ( -4.4%) 0.5731
French 
MAP 0.4658 (+6.9%) 0.4526 (+3.9%) 0.4374 (+0.4%) 0.4634 (+6.4%)
-
0.4451 (+2.2%) 0.4356
P@5 0.4925 (+3.1%) 0.4806  (+0.6%) 0.4567 (-4.4%) 0.4925 (+3.1%) 0.4836 (+1.3%) 0.4776
P@10 0.4358 (+3.9%) 0.4239 (+1%) 0.4224 (+0.7%) 0.4388 (+4.6%) 0.4209 (+0.4%) 0.4194
Finnish
MAP 0.3411 ( -4.7%) 0.3796 (+6.1%) 0.3722 (+4%) 0.369 (+3.1%) 0.3553 ( -0.7%)
-
0.3578
P@5 0.394 (+3.1%) 0.403 (+5.5%) 0.406 (+6.3%) 0.4119 (+7.8%) 0.397 (+3.9%) 0.3821
P@10 0.3463 (+11.5%) 0.3582 (+15.4%) 0.3478 (+12%) 0.3448 (+11%) 0.3433 (+10.6%) 0.3105
Table 6: Results showing the performance of MultiPRF with different source and assisting languages using Google Translate
for query translation step. The intra-familial affinity could be observed from the elements close to the diagonal.
ease of processing in these languages. (b) Familial
Similarity Between Languages: We observe that
the performance of MultiPRF is good if the as-
sisting language is from the same language fam-
ily. Birch et al (2008) show that the language
family is a strong predictor of machine transla-
tion performance. Hence, the query translation
and back translation quality improves if the source
and assisting languages belong to the same family.
For example, in the Germanic family, the source-
assisting language pairs German-English, Dutch-
English, Dutch-German and German-Dutch show
good performance. Similarly, in Romance family,
the performance of French-Spanish confirms this
behaviour. In some cases, we observe that Multi-
PRF scores decent improvements even when the
assisting language does not belong to the same
language family as witnessed in French-English
and English-French. This is primarily due to their
strong monolingual MBF performance.
6.3 Effect of Language Family on Back
Translation Performance
As already mentioned, the performance of Multi-
PRF is good if the source and assisting languages
belong to the same family. In this section, we ver-
ify the above intuition by studying the impact of
language family on back translation performance.
The experiment designed is as follows: Given a
query in source language L1, the ideal translation
in assisting language L2 is used to compute the
query model in L2 using only the query terms.
Then, without performing PRF the query model
Source 
Lang.
Assisting Language
M BF MPRFFR ES DE NL EN FI
French - 0.3686 0.3113 0.3366 0.4338 0.3011 0.4342 0.4535
Spanish 0.3647 - 0.3440 0.3476 0.3954 0.3036 0.5000 0.4892
German 0.2729 0.2736 - 0.2951 0.2107 0.2266 0.4229 0.4576
Dutch 0.2663 0.2836 0.2902 - 0.2757 0.2372 0.3968 0.3989
Table 8: Effect of Language Family on Back Translation
Performance measured through MultiPRF MAP. 100 Topics
from years 2001 and 2002 were used for all languages.
is directly back translated from L2 into L1 and
finally documents are re-ranked using this trans-
lated feedback model. Since the automatic query
translation and PRF steps have been eliminated,
the only factor which influences the MultiPRF per-
formance is the back-translation step. This means
that the source-assisting language pairs for which
the back-translation is good will score a higher
performance. The results of the above experiment
is shown in Table 8. For each source language,
the best performing assisting languages have been
highlighted.
The results show that the performance of
closely related languages like French-Spanish and
German-Dutch is more when compared to other
source-assistant language pairs. This shows that
in case of closely related languages, the back-
translation step succeeds in adding good terms
which are relevant like morphological variants,
synonyms and other semantically related terms.
Hence, familial closeness of the assisting language
helps in boosting the MultiPRF performance. An
exception to this trend is English as assisting lan-
1353
Source 
Lang.
Assisting Language Source 
Lang.MBF English German Dutch Spanish French Finnish 
English 
MAP 
-
0.4513 (+0.4%) 0.4475 ( -0.4%) 0.4695 (+4.5%) 0.4665 (+3.8%) 0.4416 ( -1.7%) 0.4495
P @5 0.5104 (+3.0%) 0.5104 (+3.0%) 0.5343 (+7.8%) 0.5403 (+9.0%) 0.4806 ( -3.0%) 0.4955
P@10 0.4373 (+1.0%) 0.4358 (+0.7%) 0.4597 (+6.2%) 0.4582 (+5.9%) 0.4164 ( -3.8%) 0.4328
German 
MAP 0.4427 (+9.8%)
-
0.4306 (+6.8%) 0.4404 (+9.2%) 0.4104 (+1.8%) 0.3993 ( -1.0%) 0.4033
P@5 0.606 (+18%) 0.5672 (+10.5%) 0.594 (+15.7%) 0.5761 (+12.2%) 0.5552 (+8.1%) 0.5134
P @10 0.5373 (+13.2%) 0.503 (+6.0%) 0.5299 (+11.7%) 0.494 (+4.1%) 0.5 (+5.4%) 0.4746
Dutch 
MAP 0.4361 (+5.0%) 0.4344 (+4.6%)
-
0.4227 (+1.8%) 0.4304 (+3.6%) 0.4134 ( -0.5%) 0.4153
P@5 0.5761 (+14.2%) 0.5552 (+10%) 0.5403 (+7.1%) 0.5463 (+8.3%) 0.5433 (+7.7%) 0.5045
P @10 0.5254 (+12.8%) 0.497 (+6.7%) 0.4776 (+2.6%) 0.5134 (+10.2%) 0.4925 (+5.8%) 0.4657
Spanish 
MAP 0.4665 ( -2.9%) 0.4773 ( -0.7%) 0.4733 ( -1.5%)
-
0.4839 (+0.7%) 0.4412 ( -8.2%) 0.4805
P@5 0.6507 (+1.8%) 0.6448 (+0.9%) 0.6507 (+1.8%) 0.6478 (+1.4%) 0.597 ( -6.5%) 0.6388
P@10 0.5791 (+1.0%) 0.5791 (+1.0%) 0.5761 (+0.5%) 0.5866 (+2.4%) 0.5567 ( -2.9%) 0.5731
French 
MAP 0.4591 (+5.4%) 0.4514 (+3.6%) 0.4409 (+1.2%) 0.4712 (+8.2%)
-
0.4354 (0%) 0.4356
P@5 0.4925 (+3.1%) 0.4776 (0%) 0.4776 (0%) 0.4995 (+4.6%) 0.4955 (+3.8%) 0.4776
P @10 0.4463 (+6.4%) 0.4313 (+2.8%) 0.4373 (+4.3%) 0.4448 (+6.1%) 0.4209 (+0.3%) 0.4194
Finnish
MAP 0.3733 (+4.3%) 0.3559 ( -0.5%) 0.3676 (+2.7%) 0.3594 (+0.4%) 0.371 (+3.7%)
-
0.3578
P@5 0.4149 (+8.6%) 0.385 (+0.7%) 0.388 (+1.6%) 0.388 (+1.6%) 0.3911 (+2.4%) 0.3821
P@10 0.3567 (+14.9%) 0.31 ( -0.2%) 0.3253 (+4.8%) 0.32 (+3.1%) 0.3239 (+4.3%) 0.3105
Table 7: Results showing the performance of MultiPRF without using automatic query translation i.e. by using corresponding
original queries in assisting collection. The results show the potential of MultiPRF by establishing a performance upper bound.
guage which shows good performance across both
families.
6.4 Multiple Assisting Languages
So far, we have only considered a single assist-
ing language. However, a natural extension to
the method which comes to mind, is using mul-
tiple assisting languages. In other words, com-
bining the evidence from all the feedback mod-
els of more than one assisting language, to get a
feedback model which is better than that obtained
using a single assisting language. To check how
this simple extension works, we performed exper-
iments using a pair of assisting languages. In these
experiments for a given source language (from
amongst the 6 previously mentioned languages)
we tried using all pairs of assisting languages (for
each source language, we have 10 pairs possible).
To obtain the final model, we simply interpolate all
the feedback models with the initial query model,
in a similar manner as done in MultiPRF. The re-
sults for these experiments are given in Table 9.
As we see, out of the 60 possible combinations
of source language and assisting language pairs,
we obtain improvements of greater than 3% in 16
cases. Here the improvements are with respect to
the best model amongst the two MultiPRF mod-
els corresponding to each of the two assisting lan-
guages, with the same source language. Thus we
observe that a simple linear interpolation of mod-
els is not the best way of combining evidence from
multiple assisting languages. We also observe than
when German or Spanish are used as one of the
two assisting languages, they are most likely to
Source 
Language
Assisting Language Pairs with 
Improvement > 3%
English FR-DE (4.5%),  FR -ES (4.8%), DE-NL (+3.1%)
French EN-DE (4.1%), DE -ES (3.4%), NL-FI (4.8 %)
German None
Spanish None
Dutch
EN-DE (3.9%), DE -FR (4.1%), FR -ES (3.8%), DE-ES 
(3.9%)
Finnish
EN-ES (3.2%), FR -DE (4.6%), FR -ES (6.4%),  
DE-ES (11.2%), DE -NL (4.4%), ES -NL (5.9%)
Total - 16
EN ? 3 Pairs; FR ? 6 Pairs; DE ? 10 Pairs; 
ES - 8 Pairs; NL ? 4 Pairs; FI ? 1 Pair
Table 9: Summary of MultiPRF Results with Two Assisting
Languages. The improvements described above are with re-
spect to maximum MultiPRF MAP obtained using either L1
or L2 alone as assisting language.
lead to improvements. A more detailed study of
this observation needs to be done to explain this.
7 Conclusion and Future Work
We studied the effect of different source-assistant
pairs and multiple assisting languages on the per-
formance of MultiPRF. Experiments across a wide
range of language pairs with varied degree of fa-
milial relationships show that MultiPRF improves
performance in most cases with the performance
improvement being more pronounced when the
source and assisting languages are closely related.
We also notice that the results are mixed when two
assisting languages are used simultaneously. As
part of future work, we plan to vary the model
interpolation parameters dynamically to improve
the performance in case of multiple assisting lan-
guages.
Acknowledgements
The first author was supported by a fellowship
award from Infosys Technologies Ltd., India. We
would like to thank Mr. Vishal Vachhani for his
help in running the experiments.
1354
References
Giambattista Amati, Claudio Carpineto, and Giovanni Ro-
mano. 2004. Query Difficulty, Robustness, and Selec-
tive Application of Query Expansion. In ECIR ?04, pages
127?137.
Alexandra Birch, Miles Osborne and Philipp Koehn. 2008.
Predicting Success in Machine Translation. In EMNLP
?08, pages 745-754, ACL.
Martin Braschler and Carol Peters. 2004. Cross-Language
Evaluation Forum: Objectives, Results, Achievements.
Inf. Retr., 7(1-2):7?31.
Martin Braschler and Peter Scha?uble. 1998. Multilingual In-
formation Retrieval based on Document Alignment Tech-
niques. In ECDL ?98, pages 183?197, Springer-Verlag.
Chris Buckley, Gerald Salton, James Allan, and Amit Sing-
hal. 1994. Automatic Query Expansion using SMART :
TREC 3. In TREC-3, pages 69?80.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting Good Expansion Terms for
Pseudo-Relevance Feedback. In SIGIR ?08, pages 243?
250. ACM.
Manoj K. Chinnakotla, Karthik Raman, and Pushpak Bhat-
tacharyya. 2010. Multilingual PRF: English Lends a
Helping Hand. In SIGIR ?10, ACM.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query
Expansion Using Random Walk Models. In CIKM ?05,
pages 704?711. ACM.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2004. A Framework for Selective Query Expansion. In
CIKM ?04, pages 236?237. ACM.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two Lan-
guages Are More Informative Than One. In ACL ?91,
pages 130?137. ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum Like-
lihood from Incomplete Data via the EM Algorithm. Jour-
nal of the Royal Statistical Society, 39:1?38.
T. Susan Dumais, A. Todd Letsche, L. Michael Littman, and
K. Thomas Landauer. 1997. Automatic Cross-Language
Retrieval Using Latent Semantic Indexing. In AAAI ?97,
pages 18?24.
Wei Gao, John Blitzer, and Ming Zhou. 2008. Using English
Information in Non-English Web Search. In iNEWS ?08,
pages 17?24. ACM.
David Hawking, Paul Thistlewaite, and Donna Harman.
1999. Scaling Up the TREC Collection. Inf. Retr., 1(1-
2):115?137.
Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Marcello Fed-
erico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondej Bojar. 2007. Moses:
Open Source Toolkit for Statistical Machine Translation.
In ACL ?07, pages 177?180.
P. Jourlin, S. E. Johnson, K. Spa?rck Jones and P. C. Wood-
land. 1999. Improving Retrieval on Imperfect Speech
Transcriptions (Poster Abstract). In SIGIR ?99, pages
283?284. ACM.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic Rel-
evance Models Based on Document and Query Genera-
tion. Language Modeling for Information Retrieval, pages
1?10. Kluwer International Series on IR.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A
Probabilistic Model of Information Retrieval: Develop-
ment and Comparative Experiments. Inf. Process. Man-
age., 36(6):779?808.
John Lafferty and Chengxiang Zhai. 2001. Document Lan-
guage Models, Query Models, and Risk Minimization for
Information Retrieval. In SIGIR ?01, pages 111?119.
ACM.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based
Language Models. In SIGIR ?01, pages 120?127. ACM.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-Lingual Relevance Models. In SIGIR ?02,
pages 175?182, ACM.
Edgar Meij, Dolf Trieschnigg, Maarten Rijke de, and Wessel
Kraaij. 2009. Conceptual Language Models for Domain-
specific Retrieval. Information Processing & Manage-
ment, 2009.
Donald Metzler and W. Bruce Croft. 2007. Latent Concept
Expansion Using Markov Random Fields. In SIGIR ?07,
pages 311?318. ACM.
Mandar Mitra, Amit Singhal, and Chris Buckley. 1998. Im-
proving Automatic Query Expansion. In SIGIR ?98, pages
206?214. ACM.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
I. Ounis, G. Amati, Plachouras V., B. He, C. Macdonald, and
Johnson. 2005. Terrier Information Retrieval Platform.
In ECIR ?05, volume 3408 of Lecture Notes in Computer
Science, pages 517?519. Springer.
Koehn Philipp. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In MT Summit ?05.
Stephen Robertson. 2006. On GMAP: and Other Transfor-
mations. In CIKM ?06, pages 78?83. ACM.
Tetsuya Sakai, Toshihiko Manabe, and Makoto Koyama.
2005. Flexible Pseudo-Relevance Feedback Via Selective
Sampling. ACM TALIP, 4(2):111?135.
Tao Tao and ChengXiang Zhai. 2006. Regularized Esti-
mation of Mixture Models for Robust Pseudo-Relevance
Feedback. In SIGIR ?06, pages 162?169. ACM.
Tuomas Talvensaari, Jorma Laurikkala, Kalervo Ja?rvelin,
Martti Juhola, and Heikki Keskustalo. 2007. Creating and
Exploiting a Comparable Corpus in Cross-language Infor-
mation Retrieval. ACM Trans. Inf. Syst., 25(1):4, 2007.
Jrg Tiedemann. 2001. The Use of Parallel Corpora in Mono-
lingual Lexicography - How word alignment can identify
morphological and semantic relations. In COMPLEX ?01,
pages 143?151.
Ellen M. Voorhees. 1994. Query Expansion Using Lexical-
Semantic Relations. In SIGIR ?94, pages 61?69. Springer-
Verlag.
1355
Ellen Voorhees. 2006. Overview of the TREC 2005 Robust
Retrieval Track. In TREC 2005, Gaithersburg, MD. NIST.
Dan Wu, Daqing He, Heng Ji, and Ralph Grishman. 2008.
A Study of Using an Out-of-Box Commercial MT System
for Query Translation in CLIR. In iNEWS ?08, pages 71?
76. ACM.
Jinxi Xu and W. Bruce Croft. 2000. Improving the Effective-
ness of Information Retrieval with Local Context Analy-
sis. ACM Trans. Inf. Syst., 18(1):79?112.
Jinxi Xu, Alexander Fraser, and Ralph Weischedel. 2002.
Empirical Studies in Strategies for Arabic Retrieval. In
SIGIR ?02, pages 269?274. ACM.
Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query Dependent Pseudo-Relevance Feedback Based on
Wikipedia. In SIGIR ?09, pages 59?66. ACM.
Chengxiang Zhai and John Lafferty. 2001. Model-based
Feedback in the Language Modeling approach to Infor-
mation Retrieval. In CIKM ?01, pages 403?410. ACM.
Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied to In-
formation Retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
1356
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532?1541,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
All Words Domain Adapted WSD: Finding a Middle Ground between
Supervision and Unsupervision
Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya
Indian Institute of Technology Bombay,
Mumbai - 400076, India.
{miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in
Abstract
In spite of decades of research on word
sense disambiguation (WSD), all-words
general purpose WSD has remained a dis-
tant goal. Many supervised WSD systems
have been built, but the effort of creat-
ing the training corpus - annotated sense
marked corpora - has always been a matter
of concern. Therefore, attempts have been
made to develop unsupervised and knowl-
edge based techniques for WSD which do
not need sense marked corpora. However
such approaches have not proved effective,
since they typically do not better Word-
net first sense baseline accuracy. Our re-
search reported here proposes to stick to
the supervised approach, but with far less
demand on annotation. We show that if
we have ANY sense marked corpora, be it
from mixed domain or a specific domain, a
small amount of annotation in ANY other
domain can deliver the goods almost as
if exhaustive sense marking were avail-
able in that domain. We have tested our
approach across Tourism and Health do-
main corpora, using also the well known
mixed domain SemCor corpus. Accuracy
figures close to self domain training lend
credence to the viability of our approach.
Our contribution thus lies in finding a con-
venient middle ground between pure su-
pervised and pure unsupervised WSD. Fi-
nally, our approach is not restricted to any
specific set of target words, a departure
from a commonly observed practice in do-
main specific WSD.
1 Introduction
Amongst annotation tasks, sense marking surely
takes the cake, demanding as it does high level
of language competence, topic comprehension and
domain sensitivity. This makes supervised ap-
proaches to WSD a difficult proposition (Agirre
et al, 2009b; Agirre et al, 2009a; McCarthy et
al., 2007). Unsupervised and knowledge based ap-
proaches have been tried with the hope of creating
WSD systems with no need for sense marked cor-
pora (Koeling et al, 2005; McCarthy et al, 2007;
Agirre et al, 2009b). However, the accuracy fig-
ures of such systems are low.
Our work here is motivated by the desire to de-
velop annotation-lean all-words domain adapted
techniques for supervised WSD. It is a common
observation that domain specific WSD exhibits
high level of accuracy even for the all-words sce-
nario (Khapra et al, 2010) - provided training and
testing are on the same domain. Also domain
adaptation - in which training happens in one do-
main and testing in another - often is able to attain
good levels of performance, albeit on a specific set
of target words (Chan and Ng, 2007; Agirre and
de Lacalle, 2009). To the best of our knowledge
there does not exist a system that solves the com-
bined problem of all words domain adapted WSD.
We thus propose the following:
a. For any target domain, create a small amount
of sense annotated corpus.
b. Mix it with an existing sense annotated cor-
pus ? from a mixed domain or specific do-
main ? to train the WSD engine.
This procedure tested on four adaptation scenar-
ios, viz., (i) SemCor (Miller et al, 1993) to
Tourism, (ii) SemCor to Health, (iii) Tourism to
Health and (iv) Health to Tourism has consistently
yielded good performance (to be explained in sec-
tions 6 and 7).
The remainder of this paper is organized as fol-
lows. In section 2 we discuss previous work in the
area of domain adaptation for WSD. In section 3
1532
we discuss three state of art supervised, unsuper-
vised and knowledge based algorithms for WSD.
Section 4 discusses the injection strategy for do-
main adaptation. In section 5 we describe the
dataset used for our experiments. We then present
the results in section 6 followed by discussions in
section 7. Section 8 examines whether there is any
need for intelligent choice of injections. Section
9 concludes the paper highlighting possible future
directions.
2 Related Work
Domain specific WSD for selected target words
has been attempted by Ng and Lee (1996), Agirre
and de Lacalle (2009), Chan and Ng (2007), Koel-
ing et al (2005) and Agirre et al (2009b). They
report results on three publicly available lexical
sample datasets, viz., DSO corpus (Ng and Lee,
1996), MEDLINE corpus (Weeber et al, 2001)
and the corpus made available by Koeling et al
(2005). Each of these datasets contains a handful
of target words (41-191 words) which are sense
marked in the corpus.
Our main inspiration comes from the target-
word specific results reported by Chan and Ng
(2007) and Agirre and de Lacalle (2009). The
former showed that adding just 30% of the target
data to the source data achieved the same perfor-
mance as that obtained by taking the entire source
and target data. Agirre and de Lacalle (2009) re-
ported a 22% error reduction when source and
target data were combined for training a classi-
fier, as compared to the case when only the target
data was used for training the classifier. However,
both these works focused on target word specific
WSD and do not address all-words domain spe-
cific WSD.
In the unsupervised setting, McCarthy et al
(2007) showed that their predominant sense acqui-
sition method gives good results on the corpus of
Koeling et al (2005). In particular, they showed
that the performance of their method is compa-
rable to the most frequent sense obtained from a
tagged corpus, thereby making a strong case for
unsupervised methods for domain-specific WSD.
More recently, Agirre et al (2009b) showed that
knowledge based approaches which rely only on
the semantic relations captured by the Wordnet
graph outperform supervised approaches when ap-
plied to specific domains. The good results ob-
tained by McCarthy et al (2007) and Agirre et
al. (2009b) for unsupervised and knowledge based
approaches respectively have cast a doubt on the
viability of supervised approaches which rely on
sense tagged corpora. However, these conclusions
were drawn only from the performance on certain
target words, leaving open the question of their
utility in all words WSD.
We believe our work contributes to the WSD
research in the following way: (i) it shows that
there is promise in supervised approach to all-
word WSD, through the instrument of domain
adaptation; (ii) it places in perspective some very
recently reported unsupervised and knowledge
based techniques of WSD; (ii) it answers some
questions arising out of the debate between super-
vision and unsupervision in WSD; and finally (iv)
it explores a convenient middle ground between
unsupervised and supervised WSD ? the territory
of ?annotate-little and inject? paradigm.
3 WSD algorithms employed by us
In this section we describe the knowledge based,
unsupervised and supervised approaches used for
our experiments.
3.1 Knowledge Based Approach
Agirre et al (2009b) showed that a graph based
algorithm which uses only the relations between
concepts in a Lexical Knowledge Base (LKB) can
outperform supervised approaches when tested on
specific domains (for a set of chosen target words).
We employ their method which involves the fol-
lowing steps:
1. Represent Wordnet as a graph where the con-
cepts (i.e., synsets) act as nodes and the re-
lations between concepts define edges in the
graph.
2. Apply a context-dependent Personalized
PageRank algorithm on this graph by intro-
ducing the context words as nodes into the
graph and linking them with their respective
synsets.
3. These nodes corresponding to the context
words then inject probability mass into the
synsets they are linked to, thereby influencing
the final relevance of all nodes in the graph.
We used the publicly available implementation
of this algorithm1 for our experiments.
1http://ixa2.si.ehu.es/ukb/
1533
3.2 Unsupervised Approach
McCarthy et al (2007) used an untagged corpus to
construct a thesaurus of related words. They then
found the predominant sense (i.e., the most fre-
quent sense) of each target word using pair-wise
Wordnet based similarity measures by pairing the
target word with its top-k neighbors in the the-
saurus. Each target word is then disambiguated
by assigning it its predominant sense ? the moti-
vation being that the predominant sense is a pow-
erful, hard-to-beat baseline. We implemented their
method using the following steps:
1. Obtain a domain-specific untagged corpus (we
crawled a corpus of approximately 9M words
from the web).
2. Extract grammatical relations from this text us-
ing a dependency parser2 (Klein and Manning,
2003).
3. Use the grammatical relations thus extracted to
construct features for identifying the k nearest
neighbors for each word using the distributional
similarity score described in (Lin, 1998).
4. Rank the senses of each target word in the test
set using a weighted sum of the distributional
similarity scores of the neighbors. The weights
in the sum are based on Wordnet Similarity
scores (Patwardhan and Pedersen, 2003).
5. Each target word in the test set is then disam-
biguated by simply assigning it its predominant
sense obtained using the above method.
3.3 Supervised approach
Khapra et al (2010) proposed a supervised algo-
rithm for domain-specific WSD and showed that it
beats the most frequent corpus sense and performs
on par with other state of the art algorithms like
PageRank. We implemented their iterative algo-
rithm which involves the following steps:
1. Tag all monosemous words in the sentence.
2. Iteratively disambiguate the remaining words in
the sentence in increasing order of their degree
of polysemy.
3. At each stage rank the candidate senses of
a word using the scoring function of Equa-
tion (1) which combines corpus based param-
eters (such as, sense distributions and corpus
co-occurrence) and Wordnet based parameters
2We used the Stanford parser - http://nlp.
stanford.edu/software/lex-parser.shtml
(such as, semantic similarity, conceptual dis-
tance, etc.)
S? = arg max
i
(?iVi +
?
j?J
Wij ? Vi ? Vj)
(1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
4. Select the candidate synset with maximizes the
above score as the winner sense.
4 Injections for Supervised Adaptation
This section describes the main interest of our
work i.e. adaptation using injections. For su-
pervised adaptation, we use the supervised algo-
rithm described above (Khapra et al, 2010) in the
following 3 settings as proposed by Agirre et al
(2009a):
a. Source setting: We train the algorithm on a
mixed-domain corpus (SemCor) or a domain-
specific corpus (say, Tourism) and test it on a
different domain (say, Health). A good perfor-
mance in this setting would indicate robustness
to domain-shifts.
b. Target setting: We train and test the algorithm
using data from the same domain. This gives the
skyline performance, i.e., the best performance
that can be achieved if sense marked data from
the target domain were available.
c. Adaptation setting: This setting is the main fo-
cus of interest in the paper. We augment the
training data which could be from one domain
or mixed domain with a small amount of data
from the target domain. This combined data is
then used for training. The aim here is to reach
as close to the skyline performance using as lit-
tle data as possible. For injecting data from the
target domain we randomly select some sense
marked words from the target domain and add
1534
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 53133 15437 23665 6979
Verb 15528 7348 1027 356
Adjective 19732 5877 10569 2378
Adverb 6091 1977 4323 1694
All 94484 30639 39611 11407
Avg. no. of instances perpolysemous word
Category Health Tourism SemCor
Noun 7.06 12.56 10.98
Verb 7.47 9.76 11.95
Adjective 5.74 12.07 8.67
Adverb 9.11 19.78 25.44
All 6.94 12.17 11.25
Table 1: Polysemous and Monosemous words per
category in each domain
Table 2: Average number of instances per polyse-
mous word per category in the 3 domains
Avg. degree of Wordnet polysemy
for polysemous words
Category Health Tourism SemCor
Noun 5.24 4.95 5.60
Verb 10.60 10.10 9.89
Adjective 5.52 5.08 5.40
Adverb 3.64 4.16 3.90
All 6.49 5.77 6.43
Avg. degree of Corpus polysemy
for polysemous words
Category Health Tourism SemCor
Noun 1.92 2.60 3.41
Verb 3.41 4.55 4.73
Adjective 2.04 2.57 2.65
Adverb 2.16 2.82 3.09
All 2.31 2.93 3.56
Table 3: Average degree of Wordnet polysemy of
polysemous words per category in the 3 domains
Table 4: Average degree of Corpus polysemy of
polysemous words per category in the 3 domains
them to the training data. An obvious ques-
tion which arises at this point is ?Why were the
words selected at random?? or ?Can selection
of words using some active learning strategy
yield better results than a random selection??
We discuss this question in detail in Section 7
and show that a random set of injections per-
forms no worse than a craftily selected set of
injections.
5 DataSet Preparation
Due to the lack of any publicly available all-words
domain specific sense marked corpora we set upon
the task of collecting data from two domains, viz.,
Tourism and Health. The data for Tourism do-
main was downloaded from Indian Tourism web-
sites whereas the data for Health domain was ob-
tained from two doctors. This data was manu-
ally sense annotated by two lexicographers adept
in English. Princeton Wordnet 2.13 (Fellbaum,
1998) was used as the sense inventory. A total
of 1,34,095 words from the Tourism domain and
42,046 words from the Health domain were man-
ually sense marked. Some files were sense marked
by both the lexicographers and the Inter Tagger
Agreement (ITA) calculated from these files was
83% which is comparable to the 78% ITA reported
on the SemCor corpus considering the domain-
specific nature of the corpus.
We now present different statistics about the
corpora. Table 1 summarizes the number of poly-
semous and monosemous words in each category.
3http://wordnetweb.princeton.edu/perl/webwn
Note that we do not use the monosemous words
while calculating precision and recall of our algo-
rithms.
Table 2 shows the average number of instances
per polysemous word in the 3 corpora. We note
that the number of instances per word in the
Tourism domain is comparable to that in the Sem-
Cor corpus whereas the number of instances per
word in the Health corpus is smaller due to the
overall smaller size of the Health corpus.
Tables 3 and 4 summarize the average degree
of Wordnet polysemy and corpus polysemy of the
polysemous words in the corpus. Wordnet poly-
semy is the number of senses of a word as listed
in the Wordnet, whereas corpus polysemy is the
number of senses of a word actually appearing in
the corpus. As expected, the average degree of
corpus polysemy (Table 4) is much less than the
average degree of Wordnet polysemy (Table 3).
Further, the average degree of corpus polysemy
(Table 4) in the two domains is less than that in the
mixed-domain SemCor corpus, which is expected
due to the domain specific nature of the corpora.
Finally, Table 5 summarizes the number of unique
polysemous words per category in each domain.
No. of unique polysemous words
Category Health Tourism SemCor
Noun 2188 4229 5871
Verb 984 1591 2565
Adjective 1024 1635 2640
Adverb 217 308 463
All 4413 7763 11539
Table 5: Number of unique polysemous words per category
in each domain.
1535
The data is currently being enhanced by manu-
ally sense marking more words from each domain
and will be soon freely available4 for research pur-
poses.
6 Results
We tested the 3 algorithms described in section 4
using SemCor, Tourism and Health domain cor-
pora. We did a 2-fold cross validation for su-
pervised adaptation and report the average perfor-
mance over the two folds. Since the knowledge
based and unsupervised methods do not need any
training data we simply test it on the entire corpus
from the two domains.
6.1 Knowledge Based approach
The results obtained by applying the Personalized
PageRank (PPR) method to Tourism and Health
data are summarized in Table 6. We also report
the Wordnet first sense baseline (WFS).
Domain Algorithm P(%) R(%) F(%)
Tourism PPR 53.1 53.1 53.1
WFS 62.5 62.5 62.5
Health PPR 51.1 51.1 51.1
WFS 65.5 65.5 65.5
Table 6: Comparing the performance of Person-
alized PageRank (PPR) with Wordnet First Sense
Baseline (WFS)
6.2 Unsupervised approach
The predominant sense for each word in the two
domains was calculated using the method de-
scribed in section 4.2. McCarthy et al (2004)
reported that the best results were obtained us-
ing k = 50 neighbors and the Wordnet Similar-
ity jcn measure (Jiang and Conrath, 1997). Fol-
lowing them, we used k = 50 and observed that
the best results for nouns and verbs were obtained
using the jcn measure and the best results for ad-
jectives and adverbs were obtained using the lesk
measure (Banerjee and Pedersen, 2002). Accord-
ingly, we used jcn for nouns and verbs and lesk
for adjectives and adverbs. Each target word in
the test set is then disambiguated by simply as-
signing it its predominant sense obtained using
the above method. We tested this approach only
on Tourism domain due to unavailability of large
4http://www.cfilt.iitb.ac.in/wsd/annotated corpus
untagged Health corpus which is needed for con-
structing the thesaurus. The results are summa-
rized in Table 7.
Domain Algorithm P(%) R(%) F(%)
Tourism McCarthy 51.85 49.32 50.55
WFS 62.50 62.50 62.50
Table 7: Comparing the performance of unsuper-
vised approach with Wordnet First Sense Baseline
(WFS)
6.3 Supervised adaptation
We report results in the source setting, target set-
ting and adaptation setting as described earlier
using the following four combinations for source
and target data:
1. SemCor to Tourism (SC?T) where SemCor is
used as the source domain and Tourism as the
target (test) domain.
2. SemCor to Health (SC?H) where SemCor is
used as the source domain and Health as the tar-
get (test) domain.
3. Tourism to Health (T?H) where Tourism is
used as the source domain and Health as the tar-
get (test) domain.
4. Health to Tourism (H?T) where Health is
used as the source domain and Tourism as the
target (test) domain.
In each case, the target domain data was divided
into two folds. One fold was set aside for testing
and the other for injecting data in the adaptation
setting. We increased the size of the injected target
examples from 1000 to 14000 words in increments
of 1000. We then repeated the same experiment by
reversing the role of the two folds.
Figures 1, 2, 3 and 4 show the graphs of the av-
erage F-score over the 2-folds for SC?T, SC?H,
T?H and H?T respectively. The x-axis repre-
sents the amount of training data (in words) in-
jected from the target domain and the y-axis rep-
resents the F-score. The different curves in each
graph are as follows:
a. only random : This curve plots the perfor-
mance obtained using x randomly selected
sense tagged words from the target domain and
zero sense tagged words from the source do-
main (x was varied from 1000 to 14000 words
in increments of 1000).
1536
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+semcor
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+semcor
Figure 1: Supervised adaptation from
SemCor to Tourism using injections
Figure 2: Supervised adaptation from
SemCor to Health using injections
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+tourism
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+health
Figure 3: Supervised adaptation from
Tourism to Health using injections
Figure 4: Supervised adaptation from
Health to Tourism using injections
b. random+source : This curve plots the perfor-
mance obtained by mixing x randomly selected
sense tagged words from the target domain with
the entire training data from the source domain
(again x was varied from 1000 to 14000 words
in increments of 1000).
c. source baseline (srcb) : This represents the F-
score obtained by training on the source data
alone without mixing any examples from the
target domain.
d. wordnet first sense (wfs) : This represents the
F-score obtained by selecting the first sense
from Wordnet, a typically reported baseline.
e. target skyline (tsky) : This represents the av-
erage 2-fold F-score obtained by training on
one entire fold of the target data itself (Health:
15320 polysemous words; Tourism: 47242 pol-
ysemous words) and testing on the other fold.
These graphs along with other results are dis-
cussed in the next section.
7 Discussions
We discuss the performance of the three ap-
proaches.
7.1 Knowledge Based and Unsupervised
approaches
It is apparent from Tables 6 and 7 that knowl-
edge based and unsupervised approaches do not
perform well when compared to the Wordnet first
sense (which is freely available and hence can be
used for disambiguation). Further, we observe that
the performance of these approaches is even less
than the source baseline (i.e., the case when train-
ing data from a source domain is applied as it is
to a target domain - without using any injections).
These observations bring out the weaknesses of
these approaches when used in an all-words set-
ting and clearly indicate that they come nowhere
close to replacing a supervised system.
1537
7.2 Supervised adaptation
1. The F-score obtained by training on SemCor
(mixed-domain corpus) and testing on the two
target domains without using any injections
(srcb) ? F-score of 61.7% on Tourism and F-
score of 65.5% on Health ? is comparable to the
best result reported on the SEMEVAL datasets
(65.02%, where both training and testing hap-
pens on a mixed-domain corpus (Snyder and
Palmer, 2004)). This is in contrast to previ-
ous studies (Escudero et al, 2000; Agirre and
Martinez, 2004) which suggest that instead of
adapting from a generic/mixed domain to a spe-
cific domain, it is better to completely ignore
the generic examples and use hand-tagged data
from the target domain itself. The main rea-
son for the contrasting results is that the ear-
lier work focused only on a handful of target
words whereas we focus on all words appearing
in the corpus. So, while the behavior of a few
target words would change drastically when the
domain changes, a majority of the words will
exhibit the same behavior (i.e., same predomi-
nant sense) even when the domain changes. We
agree that the overall performance is still lower
than that obtained by training on the domain-
specific corpora. However, it is still better than
the performance of unsupervised and knowl-
edge based approaches which tilts the scale in
favor of supervised approaches even when only
mixed domain sense marked corpora is avail-
able.
2. Adding injections from the target domain im-
proves the performance. As the amount of in-
jection increases the performance approaches
the skyline, and in the case of SC?H and T?H
it even crosses the skyline performance showing
that combining the source and target data can
give better performance than using the target
data alone. This is consistent with the domain
adaptation results reported by Agirre and de La-
calle (2009) on a specific set of target words.
3. The performance of random+source is always
better than only random indicating that the data
from the source domain does help to improve
performance. A detailed analysis showed that
the gain obtained by using the source data is at-
tributable to reducing recall errors by increasing
the coverage of seen words.
4. Adapting from one specific domain (Tourism or
Health) to another specific domain (Health or
Tourism) gives the same performance as that ob-
tained by adapting from a mixed-domain (Sem-
Cor) to a specific domain (Tourism, Health).
This is an interesting observation as it suggests
that as long as data from one domain is avail-
able it is easy to build a WSD engine that works
for other domains by injecting a small amount
of data from these domains.
To verify that the results are consistent, we ran-
domly selected 5 different sets of injections from
fold-1 and tested the performance on fold-2. We
then repeated the same experiment by reversing
the roles of the two folds. The results were in-
deed consistent irrespective of the set of injections
used. Due to lack of space we have not included
the results for these 5 different sets of injections.
7.3 Quantifying the trade-off between
performance and corpus size
To correctly quantify the benefit of adding injec-
tions from the target domain, we calculated the
amount of target data (peak size) that is needed
to reach the skyline F-score (peak F) in the ab-
sence of any data from the source domain. The
peak size was found to be 35000 (Tourism) and
14000 (Health) corresponding to peak F values of
74.2% (Tourism) and 73.4% (Health). We then
plotted a graph (Figure 5) to capture the rela-
tion between the size of injections (expressed as
a percentage of the peak size) and the F-score (ex-
pressed as a percentage of the peak F).
 80
 85
 90
 95
 100
 105
 0  20  40  60  80  100
%
 p
ea
k_
F
% peak_size
Size v/s Performance
SC --> H
T --> H
SC --> T
H --> T
Figure 5: Trade-off between performance
and corpus size
We observe that by mixing only 20-40% of the
peak size with the source domain we can obtain up
to 95% of the performance obtained by using the
1538
entire target data (peak size). In absolute terms,
the size of the injections is only 7000-9000 poly-
semous words which is a very small price to pay
considering the performance benefits.
8 Does the choice of injections matter?
An obvious question which arises at this point is
?Why were the words selected at random?? or
?Can selection of words using some active learn-
ing strategy yield better results than a random
selection?? An answer to this question requires
a more thorough understanding of the sense-
behavior exhibited by words across domains. In
any scenario involving a shift from domain D1 to
domain D2, we will always encounter words be-
longing to the following 4 categories:
a. WD1 : This class includes words which are en-
countered only in the source domain D1 and do
not appear in the target domain D2. Since we
are interested in adapting to the target domain
and since these words do not appear in the tar-
get domain, it is quite obvious that they are not
important for the problem of domain adapta-
tion.
b. WD2 : This class includes words which are en-
countered only in the target domain D2 and do
not appear in the source domain D1. Again, it
is quite obvious that these words are important
for the problem of domain adaptation. They fall
in the category of unseen words and need han-
dling from that point of view.
c. WD1D2conformists : This class includes words
which are encountered in both the domains and
exhibit the same predominant sense in both the
domains. Correct identification of these words
is important so that we can use the predomi-
nant sense learned from D1 for disambiguating
instances of these words appearing in D2.
d. WD1D2non?conformists : This class includes
words which are encountered in both the do-
mains but their predominant sense in the tar-
get domain D2 does not conform to the pre-
dominant sense learned from the source domain
D1. Correct identification of these words is im-
portant so that we can ignore the predominant
senses learned from D1 while disambiguating
instances of these words appearing in D2.
Table 8 summarizes the percentage of words that
fall in each category in each of the three adapta-
tion scenarios. The fact that nearly 50-60% of the
words fall in the ?conformist? category once again
makes a strong case for reusing sense tagged data
from one domain to another domain.
Category SC?T SC?H T?H
WD2 7.14% 5.45% 13.61%
Conformists 49.54% 60.43% 54.31%
Non-Conformists 43.30% 34.11% 32.06%
Table 8: Percentage of Words belonging to each
category in the three settings.
The above characterization suggests that an ideal
domain adaptation strategy should focus on in-
jecting WD2 and WD1D2non?conformists as these
would yield maximum benefits if injected into the
training data. While it is easy to identify the
WD2 words, ?identifying non-conformists? is a
hard problem which itself requires some type of
WSD5. However, just to prove that a random in-
jection strategy does as good as an ideal strategy
we assume the presence of an oracle which iden-
tifies the WD1D2non?conformists. We then augment
the training data with 5-8 instances for WD2 and
WD1D2non?conformists words thus identified. We
observed that adding more than 5-8 instances per
word does not improve the performance. This is
due to the ?one sense per domain? phenomenon ?
seeing only a few instances of a word is sufficient
to identify the predominant sense of the word. Fur-
ther, to ensure a better overall performance, the
instances of the most frequent words are injected
first followed by less frequent words till we ex-
haust the total size of the injections (1000, 2000
and so on). We observed that there was a 75-
80% overlap between the words selected by ran-
dom strategy and oracle strategy. This is because
oracle selects the most frequent words which also
have a high chance of getting selected when a ran-
dom sampling is done.
Figures 6, 7, 8 and 9 compare the performance
of the two strategies. We see that the random strat-
egy does as well as the oracle strategy thereby sup-
porting our claim that if we have sense marked
corpus from one domain then simply injecting ANY
small amount of data from the target domain will
5Note that the unsupervised predominant sense acquisi-
tion method of McCarthy et al (2007) implicitly identifies
conformists and non-conformists
1539
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+semcor
oracle+semcor
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+semcor
oracle+semcor
Figure 6: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Tourism adaptation
Figure 7: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Health adaptation
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+tourism
oracle+tourism
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+health
oracle+health
Figure 8: Comparing random strat-
egy with oracle based ideal strategy for
Tourism to Health adaptation
Figure 9: Comparing random strat-
egy with oracle based ideal strategy for
Health to Tourism adaptation
do the job.
9 Conclusion and Future Work
Based on our study of WSD in 4 domain adap-
tation scenarios, we make the following conclu-
sions:
1. Supervised adaptation by mixing small amount
of data (7000-9000 words) from the target do-
main with the source domain gives nearly the
same performance (F-score of around 70% in
all the 4 adaptation scenarios) as that obtained
by training on the entire target domain data.
2. Unsupervised and knowledge based approaches
which use distributional similarity and Word-
net based similarity measures do not compare
well with the Wordnet first sense baseline per-
formance and do not come anywhere close to
the performance of supervised adaptation.
3. Supervised adaptation from a mixed domain to
a specific domain gives the same performance
as that from one specific domain (Tourism) to
another specific domain (Health).
4. Supervised adaptation is not sensitive to the
type of data being injected. This is an interest-
ing finding with the following implication: as
long as one has sense marked corpus - be it from
a mixed or specific domain - simply injecting
ANY small amount of data from the target do-
main suffices to beget good accuracy.
As future work, we would like to test our work on
the Environment domain data which was released
as part of the SEMEVAL 2010 shared task on ?All-
words Word Sense Disambiguation on a Specific
Domain?.
1540
References
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for wsd. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 42?50, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Eneko Agirre and David Martinez. 2004. The effect of
bias on an automatically-built word sense corpus. In
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2009a. Semeval-2010 task 17: all-words
word sense disambiguation on a specific domain. In
DEW ?09: Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions, pages 123?128, Morristown, NJ, USA.
Association for Computational Linguistics.
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009b. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd. In
In Proceedings of IJCAI.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49?56, Prague, Czech Republic,
June. Association for Computational Linguistics.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau.
2000. An empirical study of the domain depen-
dence of supervised word sense disambiguation sys-
tems. In Proceedings of the 2000 Joint SIGDAT con-
ference on Empirical methods in natural language
processing and very large corpora, pages 172?180,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int?l. Conf. on Research in Computa-
tional Linguistics, pages 19?33.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In IN PROCEEDINGS
OF THE 41ST ANNUAL MEETING OF THE ASSO-
CIATION FOR COMPUTATIONAL LINGUISTICS,
pages 423?430.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553?
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 303?308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40?47, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Siddharth Patwardhan and Ted Pedersen. 2003.
The cpan wordnet::similarity package. http://search
.cpan.org/ sid/wordnet-similarity/.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings of
the AMAI Symposium, pages 746?750.
1541
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 561?569,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Together We Can: Bilingual Bootstrapping for WSD
Mitesh M. Khapra Salil Joshi Arindam Chatterjee Pushpak Bhattacharyya
Department Of Computer Science and Engineering,
IIT Bombay,
Powai,
Mumbai, 400076.
{miteshk,salilj,arindam,pb}@cse.iitb.ac.in
Abstract
Recent work on bilingual Word Sense Disam-
biguation (WSD) has shown that a resource
deprived language (L1) can benefit from the
annotation work done in a resource rich lan-
guage (L2) via parameter projection. How-
ever, this method assumes the presence of suf-
ficient annotated data in one resource rich lan-
guage which may not always be possible. In-
stead, we focus on the situation where there
are two resource deprived languages, both
having a very small amount of seed annotated
data and a large amount of untagged data. We
then use bilingual bootstrapping, wherein, a
model trained using the seed annotated data
of L1 is used to annotate the untagged data of
L2 and vice versa using parameter projection.
The untagged instances of L1 and L2 which
get annotated with high confidence are then
added to the seed data of the respective lan-
guages and the above process is repeated. Our
experiments show that such a bilingual boot-
strapping algorithm when evaluated on two
different domains with small seed sizes using
Hindi (L1) and Marathi (L2) as the language
pair performs better than monolingual boot-
strapping and significantly reduces annotation
cost.
1 Introduction
The high cost of collecting sense annotated data for
supervised approaches (Ng and Lee, 1996; Lee et
al., 2004) has always remained a matter of concern
for some of the resource deprived languages of the
world. The problem is even more hard-hitting for
multilingual regions (e.g., India which has more than
20 constitutionally recognized languages). To cir-
cumvent this problem, unsupervised and knowledge
based approaches (Lesk, 1986; Walker and Amsler,
1986; Agirre and Rigau, 1996; McCarthy et al,
2004; Mihalcea, 2005) have been proposed as an al-
ternative but they have failed to deliver good accura-
cies. Semi-supervised approaches (Yarowsky, 1995)
which use a small amount of annotated data and a
large amount of untagged data have shown promise
albeit for a limited set of target words. The above
situation highlights the need for high accuracy re-
source conscious approaches to all-words multilin-
gual WSD.
Recent work by Khapra et al (2010) in this di-
rection has shown that it is possible to perform cost
effective WSD in a target language (L2) without
compromising much on accuracy by leveraging on
the annotation work done in another language (L1).
This is achieved with the help of a novel synset-
aligned multilingual dictionary which facilitates the
projection of parameters learned from the Wordnet
and annotated corpus of L1 to L2. This approach
thus obviates the need for collecting large amounts
of annotated corpora in multiple languages by rely-
ing on sufficient annotated corpus in one resource
rich language. However, in many situations such a
pivot resource rich language itself may not be avail-
able. Instead, we might have two or more languages
having a small amount of annotated corpus and a
large amount of untagged corpus. Addressing such
situations is the main focus of this work. Specifi-
cally, we address the following question:
In the absence of a pivot resource rich lan-
guage is it possible for two resource de-
prived languages to mutually benefit from
each other?s annotated data?
While addressing the above question we assume that
561
even though it is hard to obtain large amounts of
annotated data in multiple languages, it should be
fairly easy to obtain a large amount of untagged data
in these languages. We leverage on such untagged
data by employing a bootstrapping strategy. The
idea is to train an initial model using a small amount
of annotated data in both the languages and itera-
tively expand this seed data by including untagged
instances which get tagged with a high confidence
in successive iterations. Instead of using monolin-
gual bootstrapping, we use bilingual bootstrapping
via parameter projection. In other words, the pa-
rameters learned from the annotated data of L1 (and
L2 respectively) are projected to L2 (and L1 respec-
tively) and the projected model is used to tag the un-
tagged instances of L2 (and L1 respectively).
Such a bilingual bootstrapping strategy when
tested on two domains, viz., Tourism and Health us-
ing Hindi (L1) and Marathi (L2) as the language
pair, consistently does better than a baseline strat-
egy which uses only seed data for training without
performing any bootstrapping. Further, it consis-
tently performs better than monolingual bootstrap-
ping. A simple and intuitive explanation for this is
as follows. In monolingual bootstrapping a language
can benefit only from its own seed data and hence
can tag only those instances with high confidence
which it has already seen. On the other hand, in
bilingual bootstrapping a language can benefit from
the seed data available in the other language which
was not previously seen in its self corpus. This is
very similar to the process of co-training (Blum and
Mitchell, 1998) wherein the annotated data in the
two languages can be seen as two different views of
the same data. Hence, the classifier trained on one
view can be improved by adding those untagged in-
stances which are tagged with a high confidence by
the classifier trained on the other view.
The remainder of this paper is organized as fol-
lows. In section 2 we present related work. Section
3 describes the Synset algned multilingual dictio-
nary which facilitates parameter projection. Section
4 discusses the work of Khapra et al (2009) on pa-
rameter projection. In section 5 we discuss bilin-
gual bootstrapping which is the main focus of our
work followed by a brief discussion on monolingual
bootstrapping. Section 6 describes the experimental
setup. In section 7 we present the results followed
by discussion in section 8. Section 9 concludes the
paper.
2 Related Work
Bootstrapping for Word Sense Disambiguation was
first discussed in (Yarowsky, 1995). Starting with a
very small number of seed collocations an initial de-
cision list is created. This decisions list is then ap-
plied to untagged data and the instances which get
tagged with a high confidence are added to the seed
data. This algorithm thus proceeds iteratively in-
creasing the seed size in successive iterations. This
monolingual bootstrapping method showed promise
when tested on a limited set of target words but was
not tried for all-words WSD.
The failure of monolingual approaches (Ng and
Lee, 1996; Lee et al, 2004; Lesk, 1986; Walker and
Amsler, 1986; Agirre and Rigau, 1996; McCarthy
et al, 2004; Mihalcea, 2005) to deliver high accura-
cies for all-words WSD at low costs created interest
in bilingual approaches which aim at reducing the
annotation effort. Recent work in this direction by
Khapra et al (2009) aims at reducing the annotation
effort in multiple languages by leveraging on exist-
ing resources in a pivot language. They showed that
it is possible to project the parameters learned from
the annotation work of one language to another lan-
guage provided aligned Wordnets for the two lan-
guages are available. However, they do not address
situations where two resource deprived languages
have aligned Wordnets but neither has sufficient an-
notated data. In such cases bilingual bootstrapping
can be used so that the two languages can mutually
benefit from each other?s small annotated data.
Li and Li (2004) proposed a bilingual bootstrap-
ping approach for the more specific task of Word
Translation Disambiguation (WTD) as opposed to
the more general task of WSD. This approach does
not need parallel corpora (just like our approach)
and relies only on in-domain corpora from two lan-
guages. However, their work was evaluated only on
a handful of target words (9 nouns) for WTD as op-
posed to the broader task of WSD. Our work instead
focuses on improving the performance of all words
WSD for two resource deprived languages using
bilingual bootstrapping. At the heart of our work lies
parameter projection facilitated by a synset algned
562
multilingual dictionary described in the next section.
3 Synset Aligned Multilingual Dictionary
A novel and effective method of storage and use of
dictionary in a multilingual setting was proposed by
Mohanty et al (2008). For the purpose of current
discussion, we will refer to this multilingual dictio-
nary framework as MultiDict. One important de-
parture in this framework from the traditional dic-
tionary is that synsets are linked, and after that
the words inside the synsets are linked. The ba-
sic mapping is thus between synsets and thereafter
between the words.
Concepts L1
(English)
L2
(Hindi)
L3
(Marathi)
04321:
a youth-
ful male
person
{male
child,
boy}
{lwkA
(ladkaa),
bAlk
(baalak),
bQcA
(bachchaa)}
{m  lgA
(mulgaa),
porgA
(porgaa),
por (por)}
Table 1: Multilingual Dictionary Framework
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with a
unique ID. The subsequent columns show the words
expressing the concept in respective languages (in
the example table, English, Hindi and Marathi). Af-
ter the synsets are linked, cross linkages are set up
manually from the words of a synset to the words
of a linked synset of the pivot language. For exam-
ple, for the Marathi word m  lgA (mulgaa), ?a youth-
ful male person?, the correct lexical substitute from
the corresponding Hindi synset is lwkA (ladkaa).
The average number of such links per synset per lan-
guage pair is approximately 3. However, since our
work takes place in a semi-supervised setting, we
do not assume the presence of these manual cross
linkages between synset members. Instead, in the
above example, we assume that all the words in
the Hindi synset are equally probable translations
of every word in the corresponding Marathi synset.
Such cross-linkages between synset members facil-
itate parameter projection as explained in the next
section.
4 Parameter Projection
Khapra et al (2009) proposed that the various
parameters essential for domain-specific Word
Sense Disambiguation can be broadly classified into
two categories:
Wordnet-dependent parameters:
? belongingness-to-dominant-concept
? conceptual distance
? semantic distance
Corpus-dependent parameters:
? sense distributions
? corpus co-occurrence
They proposed a scoring function (Equation (1))
which combines these parameters to identify the cor-
rect sense of a word in a context:
S? = argmax
i
(?iVi +
?
j?J
Wij ? Vi ? Vj) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
The first component ?iVi of Equation (1) captures
influence of the corpus specific sense of a word in a
domain. The other component Wij ?Vi ?Vj captures
the influence of interaction of the candidate sense
with the senses of context words weighted by factors
of co-occurrence, conceptual distance and semantic
distance.
Wordnet-dependent parameters depend on the
structure of the Wordnet whereas the Corpus-
dependent parameters depend on various statistics
learned from a sense marked corpora. Both the
tasks of (a) constructing a Wordnet from scratch and
(b) collecting sense marked corpora for multiple
languages are tedious and expensive. Khapra et
563
al. (2009) observed that by projecting relations
from the Wordnet of a language and by projecting
corpus statistics from the sense marked corpora
of the language to those of the target language,
the effort required in constructing semantic graphs
for multiple Wordnets and collecting sense marked
corpora for multiple languages can be avoided
or reduced. At the heart of their work lies the
MultiDict described in previous section which
facilitates parameter projection in the following
manner:
1. By linking with the synsets of a pivot resource
rich language (Hindi, in our case), the cost of build-
ing Wordnets of other languages is partly reduced
(semantic relations are inherited). The Wordnet pa-
rameters of Hindi Wordnet now become projectable
to other languages.
2. For calculating corpus specific sense distribu-
tions, P (Sense Si|Word W ), we need the counts,
#(Si,W ). By using cross linked words in the
synsets, these counts become projectable to the tar-
get language (Marathi, in our case) as they can be
approximated by the counts of the cross linked Hindi
words calculated from the Hindi sense marked cor-
pus as follows:
P (Si|W ) =
#(Si,marathi word)
?
j #(Sj ,marathi word)
P (Si|W ) ?
#(Si, cross linked hindi word)
?
j #(Sj , cross linked hindi word)
The rationale behind the above approximation is the
observation that within a domain the counts of cross-
linked words will remain the same across languages.
This parameter projection strategy as explained
above lies at the heart of our work and allows us
to perform bilingual bootstrapping by projecting the
models learned from one language to another.
5 Bilingual Bootstrapping
We now come to the main contribution of our work,
i.e., bilingual bootstrapping. As shown in Algorithm
1, we start with a small amount of seed data (LD1
and LD2) in the two languages. Using this data we
learn the parameters described in the previous sec-
tion. We collectively refer to the parameters learned
Algorithm 1 Bilingual Bootstrapping
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
?1 := model trained using LD1
?2 := model trained using LD2
{Project models from L1/L2 to L2/L1}
??2 := project(?1, L2)
??1 := project(?2, L1)
for all u1 ? UD1 do
s := sense assigned by ??1 to u1
if confidence(s) >  then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 ? UD2 do
s := sense assigned by ??2 to u2
if confidence(s) >  then
LD2 := LD2 + u2
UD2 := UD2 - u2
end if
end for
until convergence
from the seed data as models ?1 and ?2 for L1 and L2
respectively. The parameter projection strategy de-
scribed in the previous section is then applied to ?1
and ?2 to obtain the projected models ??2 and ??1 re-
spectively. These projected models are then applied
to the untagged data of L1 and L2 and the instances
which get labeled with a high confidence are added
to the labeled data of the respective languages. This
process is repeated till we reach convergence, i.e.,
till it is no longer possible to move any data from
UD1 (and UD2) to LD1 (and LD2 respectively).
We compare our algorithm with monolingual
bootstrapping where the self models ?1 and ?2 are
directly used to annotate the unlabeled instances in
L1 and L2 respectively instead of using the projected
models ??1 and ??2. The process of monolingual boot-
564
Algorithm 2 Monolingual Bootstrapping
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
?1 := model trained using LD1
?2 := model trained using LD2
for all u1 ? UD1 do
s := sense assigned by ?1 to u1
if confidence(s) >  then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 ? UD2 do
s := sense assigned by ?2 to u2
if confidence(s) >  then
LD2 := LD2 + u2
UD2 := UD2 - u2
end if
end for
until convergence
strapping is shown in Algorithm 2.
6 Experimental Setup
We used the publicly available dataset1 described
in Khapra et al (2010) for all our experiments.
The data was collected from two domains, viz.,
Tourism and Health. The data for Tourism domain
was collected by manually translating English doc-
uments downloaded from Indian Tourism websites
into Hindi and Marathi. Similarly, English docu-
ments for Health domain were obtained from two
doctors and were manually translated into Hindi and
Marathi. The entire data was then manually an-
notated by three lexicographers adept in Hindi and
Marathi. The various statistics pertaining to the total
number of words, number of words per POS cate-
gory and average degree of polysemy are described
in Tables 2 to 5.
Although Tables 2 and 3 also report the num-
1http://www.cfilt.iitb.ac.in/wsd/annotated corpus
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 62336 24089 35811 18923
Verb 6386 1401 3667 5109
Adjective 18949 8773 28998 12138
Adverb 4860 2527 13699 7152
All 92531 36790 82175 43322
Table 2: Polysemous and Monosemous words per cate-
gory in each domain for Hindi
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 45589 17482 27386 11383
Verb 7879 3120 2672 1500
Adjective 13107 4788 16725 6032
Adverb 4036 1727 5023 1874
All 70611 27117 51806 20789
Table 3: Polysemous and Monosemous words per cate-
gory in each domain for Marathi
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.02 3.17
Verb 5.05 6.58
Adjective 2.66 2.75
Adverb 2.52 2.57
All 3.09 3.23
Table 4: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Hindi
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.06 3.18
Verb 4.96 5.18
Adjective 2.60 2.72
Adverb 2.44 2.45
All 3.14 3.29
Table 5: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Marathi
565
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
Figure 1: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi Health
data
Figure 2: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi
Tourism data
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
Figure 3: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Health data
Figure 4: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Tourism data
ber of monosemous words, we would like to clearly
state that we do not consider monosemous words
while evaluating the performance of our algorithms
(as monosemous words do not need any disambigua-
tion).
We did a 4-fold cross validation of our algorithm
using the above described corpora. Note that even
though the corpora were parallel we did not use this
property in any way in our experiments or algorithm.
In fact, the documents in the two languages were
randomly split into 4 folds without ensuring that the
parallel documents remain in the same folds for the
two languages. We experimented with different seed
sizes varying from 0 to 5000 in steps of 250. The
seed annotated data and untagged instances for boot-
strapping are extracted from 3 folds of the data and
the final evaluation is done on the held-out data in
the 4th fold.
We ran both the bootstrapping algorithms (i.e.,
monolingual bootstrapping and bilingual boot-
strapping) for 10 iterations but, we observed
that after 1-2 iterations the algorithms converge.
In each iteration only those words for which
P (assigned sense|word) > 0.6 get moved to the
labeled data. Ideally, this threshold (0.6) should
have been selected using a development set. How-
ever, since our work focuses on resource scarce lan-
guages we did not want to incur the additional cost
of using a development set. Hence, we used a fixed
threshold of 0.6 so that in each iteration only those
words get moved to the labeled data for which the
assigned sense is clearly a majority sense (P > 0.6).
566
Language-
Domain Algorithm F-score(%)
No. of tagged
words needed to
achieve this
F-score
% Reduction in annotation
cost
Hindi-Health Biboot 57.70 1250
(2250+2250)?(1250+1750)
(2250+2250) ? 100 = 33.33%
OnlySeed 57.99 2250
Marathi-Health Biboot 64.97 1750
OnlySeed 64.51 2250
Hindi-Tourism Biboot 60.67 1000
(2000+2000)?(1000+1250)
(2000+2000) ? 100 = 43.75%
OnlySeed 59.83 2000
Marathi-Tourism Biboot 61.90 1250
OnlySeed 61.68 2000
Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping
7 Results
The results of our experiments are summarized in
Figures 1 to 4. The x-axis represents the amount of
seed data used and the y-axis represents the F-scores
obtained. The different curves in each graph are as
follows:
a. BiBoot: This curve represents the F-score ob-
tained after 10 iterations by using bilingual boot-
strapping with different amounts of seed data.
b. MonoBoot: This curve represents the F-score ob-
tained after 10 iterations by using monolingual
bootstrapping with different amounts of seed data.
c. OnlySeed: This curve represents the F-score ob-
tained by training on the seed data alone without
using any bootstrapping.
d. WFS: This curve represents the F-score obtained
by simply selecting the first sense from Wordnet,
a typically reported baseline.
8 Discussions
In this section we discuss the important observations
made from Figures 1 to 4.
8.1 Performance of Bilingual bootstrapping
For small seed sizes, the F-score of bilingual boot-
strapping is consistently better than the F-score ob-
tained by training only on the seed data without us-
ing any bootstrapping. This is true for both the lan-
guages in both the domains. Further, bilingual boot-
strapping also does better than monolingual boot-
strapping for small seed sizes. As explained earlier,
this better performance can be attributed to the fact
that in monolingual bootstrapping the algorithm can
tag only those instances with high confidence which
it has already seen in the training data. Hence, in
successive iterations, very little new information be-
comes available to the algorithm. This is clearly
evident from the fact that the curve of monolin-
gual bootstrapping (MonoBoot) is always close to
the curve of OnlySeed.
8.2 Effect of seed size
The benefit of bilingual bootstrapping is clearly felt
for small seed sizes. However, as the seed size in-
creases the performance of the 3 algorithms, viz.,
MonoBoot, BiBoot and OnlySeed is more or less the
same. This is intuitive, because, as the seed size in-
creases the algorithm is able to see more and more
tagged instances in its self corpora and hence does
not need any assistance from the other language. In
other words, the annotated data in L1 is not able to
add any new information to the training process of
L2 and vice versa.
8.3 Bilingual bootstrapping reduces annotation
cost
The performance boost obtained at small seed sizes
suggests that bilingual bootstrapping helps to reduce
the overall annotation costs for both the languages.
To further illustrate this, we take some sample points
from the graph and compare the number of tagged
words needed by BiBoot and OnlySeed to reach the
same (or nearly the same) F-score. We present this
comparison in Table 6.
567
The rows for Hindi-Health and Marathi-Health in
Table 6 show that when BiBoot is employed we
need 1250 tagged words in Hindi and 1750 tagged
words in Marathi to attain F-scores of 57.70% and
64.97% respectively. On the other hand, in the ab-
sence of bilingual bootstrapping, (i.e., using Only-
Seed) we need 2250 tagged words each in Hindi and
Marathi to achieve similar F-scores. BiBoot thus
gives a reduction of 33.33% in the overall annota-
tion cost ( {1250 + 1750} v/s {2250 + 2250}) while
achieving similar F-scores. Similarly, the results for
Hindi-Tourism and Marathi-Tourism show that Bi-
Boot gives a reduction of 43.75% in the overall an-
notation cost while achieving similar F-scores. Fur-
ther, since the results of MonoBoot are almost the
same as OnlySeed, the above numbers indicate that
BiBoot provides a reduction in cost when compared
to MonoBoot also.
8.4 Contribution of monosemous words in the
performance of BiBoot
As mentioned earlier, monosemous words in the test
set are not considered while evaluating the perfor-
mance of our algorithm but, we add monosemous
words to the seed data. However, we do not count
monosemous words while calculating the seed size
as there is no manual annotation cost associated with
monosemous words (they can be tagged automati-
cally by fetching their singleton sense id from the
wordnet). We observed that the monosemous words
of L1 help in boosting the performance of L2 and
vice versa. This is because for a given monose-
mous word in L2 (or L1 respectively) the corre-
sponding cross-linked word in L1 (or L2 respec-
tively) need not necessarily be monosemous. In such
cases, the cross-linked polysemous word in L2 (or
L1 respectively) benefits from the projected statis-
tics of a monosemous word in L1 (or L2 respec-
tively). This explains why BiBoot gives an F-score
of 35-52% even at zero seed size even though the
F-score of OnlySeed is only 2-5% (see Figures 1 to
4).
9 Conclusion
We presented a bilingual bootstrapping algorithm
for Word Sense Disambiguation which allows two
resource deprived languages to mutually benefit
from each other?s data via parameter projection. The
algorithm consistently performs better than mono-
lingual bootstrapping. It also performs better than
using only monolingual seed data without using any
bootstrapping. The benefit of bilingual bootstrap-
ping is felt prominently when the seed size in the two
languages is very small thus highlighting the useful-
ness of this algorithm in highly resource constrained
scenarios.
Acknowledgments
We acknowledge the support of Microsoft Re-
search India in the form of an International Travel
Grant, which enabled one of the authors (Mitesh M.
Khapra) to attend this conference.
References
Eneko Agirre and German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING).
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. pages 92?
100. Morgan Kaufmann Publishers.
Mitesh M. Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2009. Projecting parameters for
multilingual word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 459?467, Singa-
pore, August. Association for Computational Linguis-
tics.
Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and
Pushpak Bhattacharyya. 2010. Value for money: Bal-
ancing annotation effort, lexicon building and accu-
racy for multilingual wsd. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics.
Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 137?140.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In In Proceedings of the
5th annual international conference on Systems docu-
mentation.
Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrapping. Comput. Lin-
guist., 30:1?22, March.
568
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 279, Morristown, NJ, USA.
Association for Computational Linguistics.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference
(HLT/EMNLP), pages 411?418.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictionary:
Insights, applications and challenges. In Global Word-
net Conference.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Proceed-
ings of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 40?47.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
569
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127?132,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
C-Feel-It: A Sentiment Analyzer for Micro-blogs
Aditya Joshi1 Balamurali A R2 Pushpak Bhattacharyya1 Rajat Mohanty 3
1Dept. of Computer Science and Engineering, IIT Bombay, Mumbai
2 IITB-Monash Research Academy, IIT Bombay, Mumbai
3 AOL India (R&D), Bangalore
India
{adityaj,balamurali,pb}@cse.iitb.ac.in r.mohanty@teamaol.com
Abstract
Social networking and micro-blogging sites
are stores of opinion-bearing content created
by human users. We describe C-Feel-It, a sys-
tem which can tap opinion content in posts
(called tweets) from the micro-blogging web-
site, Twitter. This web-based system catego-
rizes tweets pertaining to a search string as
positive, negative or objective and gives an ag-
gregate sentiment score that represents a senti-
ment snapshot for a search string. We present
a qualitative evaluation of this system based
on a human-annotated tweet corpus.
1 Introduction
A major contribution of Web 2.0 is the explosive rise
of user-generated content. The content has been a
by-product of a class of Internet-based applications
that allow users to interact with each other on the
web. These applications which are highly accessible
and scalable represent a class of media called social
media. Some of the currently popular social media
sites are Facebook (www.facebook.com), Myspace
(www.myspace.com), Twitter (www.Twitter.com)
etc. User-generated content on the social media rep-
resents the views of the users and hence, may be
opinion-bearing. Sales and marketing arms of busi-
ness organizations can leverage on this information
to know more about their customer base. In addi-
tion, prospective customers of a product/service can
get to know what other users have to say about the
product/service and make an informed decision.
C-Feel-It is a web-based system which
predicts sentiment in micro-blogs on
Twitter (called tweets). (Screencast at:
http://www.youtube.com/user/cfeelit/ ) C-Feel-
It uses a rule-based system to classify tweets as
positive, negative or objective using inputs from
four sentiment-based knowledge repositories. A
weighted-majority voting principle is used to predict
sentiment of a tweet. An overall sentiment score for
the search string is assigned based on the results of
predictions for the tweets fetched. This score which
is represented as a percentage value gives a live
snapshot of the sentiment of users about the topic.
The rest of the paper is organized as follows: Sec-
tion 2 gives background study of Twitter and related
work in the context of sentiment analysis for Twitter.
The system architecture is explained in section 3. A
qualitative evaluation of our system based on anno-
tated data is described in section 4. Section 5 sum-
marizes the paper and points to future work.
2 Background study
Twitter is a micro-blogging website and ranks sec-
ond among the present social media websites (Prelo-
vac, 2010). A micro-blog allows users to exchange
small elements of content such as short sentences,
individual pages, or video links (Kaplan and Haen-
lein, 2010). More about Twitter can be found here 1.
In Twitter, a micro-blogging post is called a
tweet which can be upto 140 characters in length.
Since the length is constrained, the language used in
tweets is highly unstructured. Misspellings, slangs,
contractions and abbreviations are commonly used
in tweets. The following example highlights these
problems in a typical tweet:
?Big brother doing sian massey no favours.
Let her ref. She?s good at it you know#lifesapitch?
We choose Twitter as the data source because
of the sheer quantity of data generated and its fast
reachability across masses. Additionally, Twitter al-
lows information to flow freely and instantaneously
unlike FaceBook or MySpace. These aspects of
1http://support.twitter.com/groups/31-twitter-basics
127
Twitter makes it a source for getting a live snapshot
of the things happenings on the web.
In the context of sentiment classification of tweets
Alec et al (2009a) describes a distant supervision-
based approach for sentiment classification. The
training data for this purpose is created following a
semi-supervised approach that exploits emoticons in
tweets. In their successive work, Alec et al (2009b)
additionally use hashtags in tweets to create train-
ing data. Topic-dependent clustering is performed
on this data and classifiers corresponding to each are
modeled. This approach is found to perform better
than a single classifier alone.
We believe that the models trained on data cre-
ated using semi-supervised approaches cannot clas-
sify all variants of tweets. Hence, we follow a rule-
based approach for predicting sentiment of a tweet.
An approach like ours provides a generic way of
solving sentiment classification problems in micro-
blogs.
3 Architecture
keywor
d (s)
Tweet fetcher
Tweet Sentime
nt 
Predicto
r
C-Feel-I
t
Sentime
nt score
Tweet Sentime
nt 
Collabo
rator
score
Figure 1: Overall Architecture
The overall architecture of C-Feel-It is shown in
Figure 1. C-Feel-It is divided into three parts: Tweet
Fetcher, Tweet Sentiment Predictor and Tweet
Sentiment Collaborator. All predictions are pos-
itive, negative or objective/neutral. C-Feel-It offers
two implementations of a rule-based sentiment pre-
diction system. We refer to them as version 1 and
2. The two versions differ in the Tweet Sentiment
Predictor module. This section describes different
modules of C-Feel-It and is organized as follows. In
subsections 3.1, 3.2 & 3.3, we describe the three
functional blocks of C-FeeL-It. In subsection 3.4,
we explain how four lexical resources are mapped
to the desired output labels. Finally, subsection 3.5
gives implementation details of C-Feel-It.
Input to C-Feel-It is a search string and a version
number. The versions are described in detail in sub-
section 3.2.
Output given by C-Feel-It is two-level: tweet-wise
prediction and overall prediction. For tweet-wise
prediction, sentiment prediction by each of the re-
sources is returned. On the other hand, overall pre-
diction combines sentiment from all tweets to return
the percentage of positive, negative and objective
content retrieved for the search string.
3.1 Tweet Fetcher
Tweet fetcher obtains tweets pertaining to a search
string entered by a user. To do so, we use live feeds
from Twitter using an API 2. The parameters passed
to the API ensure that system receives the latest 50
tweets about the keyword in English. This API re-
turns results in XML format which we parse using a
Java SAX parser.
3.2 Tweet Sentiment Predictor
Tweet sentiment predictor predicts sentiment for
a single tweet. The architecture of Tweet Senti-
ment Predictor is shown in Figure 2 and can be di-
vided into three fundamental blocks: Preprocessor,
Emoticon-based Sentiment Predictor, Lexicon-based
Sentiment Predictor (refer Figure 3 & 4). The first
two blocks are same for both the versions of C-Feel-
It. The two versions differ in the working of the
Lexicon-based Sentiment Predictor.
Preprocessor
The noisy nature of tweets is a classical challenge
that any system working on tweets needs to en-
counter. Preprocessor deals with obtaining clean
tweets. We do not deploy any spelling correction
module. However, the preprocessor handles exten-
sions and contractions found in tweets as follows.
Handling extensions: Extensions like ?besssssst?
are common in tweets. However, to look up re-
sources, it is essential that these words are normal-
ized to their dictionary equivalent. We replace con-
secutive occurrences of the same letter (if more than
2http://search.Twitter.com/search.atom
128
Lexicon
-based sentime
nt 
predicto
r
Word e
xtensio
n
handler
Tweet
if no em
oticonS
entimen
t 
predicti
on
Chat lin
go 
normali
zation
Emotico
n-based
 
sentime
nt 
predicto
r
Tweet P
reproce
ssing
Sentime
nt 
predicti
on
Figure 2: Tweet Sentiment Predictor: Version 1 and 2
three occurrences of the same letter) with a single
letter and replace the word.
An important issue here is that extensions are in fact
strong indicators of sentiment. Hence, we replace an
extended word by two occurences of the contracted
word. This gives a higher weight to the extended
word and retains its contribution to the sentiment of
the tweet.
Chat lingo normalization: Words used in
chat/Internet language that are common in tweets are
not present in the lexical resources. We use a dictio-
nary downloaded from http://chat.reichards.net/ . A
chat word is replaced by its dictionary equivalent.
Emoticon-based Sentiment Predictor
Emoticons are visual representations of emo-
tions frequently used in the user-generated con-
tent on the Internet. We observe that in most
cases, emoticons pinpoint the sentiment of a
tweet. We use an emoticon mapping from
http://chat.reichards.net/smiley.shtml. An emoticon
is mapped to an output label: positive or negative. A
tweet containing one of these emoticons that can be
mapped to the desired output labels directly. While
we understand that this heuristic does not work in
case of sarcastic tweets, it does provide a benefit in
most cases.
Lexicon-based Sentiment Predictor
For a tweet, the Lexicon-based Sentiment Predic-
tor gives a prediction each for four resources. In
addition, it returns one prediction which combines
the four predictions by weighting them on the ba-
Tweet
Lexical Resourc
e
Get 
sentime
nt pred
iction
For all w
ords Return 
output 
label 
corresp
onding 
to 
majorit
y of wo
rds
Sentime
nt 
Predicti
on
Figure 3: Lexicon-based Sentiment Predictor: C-Feel-It
Version 1
sis of their accuracies. We remove stop words 3
from the tweet and stem the words using Lovins
stemmer (Lovins, 1968). Negation in tweets is han-
dled by inverting sentiment of words after a negat-
ing word. The words ?no?, ?never?, ?not? are consid-
ered negating words and a context window of three
words after a negative words is considered for in-
version. The two versions of C-Feel-It vary in their
Lexicon-based Sentiment Predictor. Figure 3 shows
the Lexicon-based Sentiment Predictor for version
1. For each word in the tweet, it gets the predic-
tion from a lexical resource. We use the intuition
that a positive tweet has positive words outnumber-
ing other words, a negative tweet has negative words
outnumbering other words and an objective tweet
has objective words outnumbering other words.
Figure 4 shows the Lexicon-based Sentiment Predic-
tor for version 2. As opposed to the earlier version,
version 2 gets prediction from the lexical resource
for some words in the tweet. This is because certain
parts-of-speech have been found to be better indi-
cators of sentiment (Pang and Lee, 2004). A tweet
is annotated with parts-of-speech tags and the POS
bi-tags (i.e. a pattern of two consecutive POS) are
marked. The words corresponding to a set of opti-
mal POS bi-tags are retained and only these words
used for lookup. The prediction for a tweet uses
majority vote-based approach as for version 1. The
optimal POS bi-tags have been derived experimen-
tally by using top 10% features on information gain-
based-pruning classifier on polarity dataset by (Pang
and Lee, 2005). We used Stanford POS tagger(Tou,
3http://www.ranks.nl/resources/stopwords.html
129
2000) for tagging the tweets.
Note: The dataset we use to find optimal POS
bi-tags consists of movie reviews. We understand
that POS bi-tags hence derived may not be universal
across domains.
Tweet
Lexical Resourc
e
Get sentime
nt 
predicti
on
For all w
ords
POS tag
 the tweet
Retain words corresp
ond
Return 
output label corresp
ondin
g to ma
jority of word
s
Sentime
nt 
Predicti
on
corresp
ond
ing to select P
OS 
bi-tags
Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It
Version 2
3.3 Tweet Sentiment Collaborator
Based on predictions of individual tweets, the Tweet
Sentiment Collaborator gives overall prediction
with respect to a keyword in form of percentage
of positive, negative and objective content. This
is on the basis of predictions by each resource by
weighting them according to their accuracies. These
weights have been assigned to each resource based
on experimental results. For each resource, the
following scores are determined.
posscore[r] =
m?
i=1
piwpi
negscore[r] =
m?
i=1
niwni
objscore[r] =
m?
i=1
oiwoi
where
posscore[r] = Positive score for search string r
negscore[r] = Negative score for search string r
objscore[r] = Objective score for search string r
m = Number of resources used for prediction
pi, ni, oi = Positive,negative & objective count of tweet
predicted respectively using resource i
wpi, wni, ooi = Weights for respective classes derived
for each resource i
We normalize these scores to get the final positive, neg-
ative and objective pertaining to search string r. These
scores are represented in form of percentage.
3.4 Resources
Sentiment-based lexical resources annotate
words/concepts with polarity. The completeness
of these resources individually remains a question.
To achieve greater coverage, we use four different
sentiment-based lexical resources for C-Feel-It. They are
described as follows.
1. SentiWordNet (Esuli and Sebastiani, 2006) assigns
three scores to synsets of WordNet: positive score,
negative score and objective score. When a word is
looked up, the label corresponding to maximum of
the three scores is returned. For multiple synsets of
a word, the output label returned by majority of the
synsets becomes the prediction of the resource.
2. Subjectivity lexicon (Wiebe et al, 2004) is a re-
source that annotates words with tags like parts-of-
speech, prior polarity, magnitude of prior polarity
(weak/strong), etc. The prior polarity can be posi-
tive, negative or neutral. For prediction using this
resource, we use this prior polarity.
3. Inquirer (Stone et al, 1966) is a list of words
marked as positive, negative and neutral. We use
these labels to use Inquirer resource for our predic-
tion.
4. Taboada (Taboada and Grieve, 2004) is a word-list
that gives a count of collocations with positive and
negative seed words. A word closer to a positive
seed word is predicted to be positive and vice versa.
3.5 Implementation Details
The system is implemented in JSP (JDK 1.6) using Net-
Beans IDE 6.9.1. For the purpose of tweet annotation,
an internal interface was written in PHP 5 with MySQL
5.0.51a-3ubuntu5.7 for storage.
4 System Analysis
4.1 Evaluation Data
For the purpose of evaluation, a total of 7000 tweets
were downloaded by using popular trending topics of
20 domains (like books, movies, electronic gadget, etc.)
as keywords for searching tweets. In order to download
the tweets, we used the API provided by Twitter 4 that
crawls latest tweets pertaining to keywords.
Human annotators assigned to a tweet one out of 4
classes: positive, negative, objective and objective-spam.
4http://search.twitter.com/search.atom?
130
A tweet is assigned to objective-spam category if it con-
tains promotional links or incoherent text which was pos-
sibly not created by a human user. Apart from these nom-
inal class labels, we also assigned the positive/negative
tweets scores ranging from +2 to -2 with +2 being the
most positive and -2 being the most negative score re-
spectively. If the tweet belongs to the objective category,
a value of zero is assigned as the score.
The spam category has been included in the annotation
as a future goal of modeling a spam detection layer prior
to the sentiment detection. However, the current version
of C-Feel-It does not have a spam detection module and
hence for evaluation purpose, we use only the data be-
longing to classes other than objective-spam.
4.2 Qualitative Analysis
In this section, we perform a qualitative evaluation of ac-
tual results returned by C-Feel-It. The errors described
in this section are in addition to the errors due to mis-
spellings and informal language. These erroneous results
have been obtained from both version 1 and 2. They
have been classified into eleven categories and explained
henceforth.
4.2.1 Sarcastic Tweets
Tweet: Hoge, Jaws, and Palantonio are brilliant to-
gether talking X?s and O?s on ESPN right now.
Label by C-Feel-It: Positive
Label by human annotator: Negative
The sarcasm in the above tweet lies in the use of a pos-
itive word ?brilliant? followed by a rather trivial action of
?talking Xs and Os?. The positive word leads to the pre-
diction by C-Feel-It where in fact, it is a negative tweet
for the human annotator.
4.2.2 Lack of Sense Understanding
Tweet: If your tooth hurts drink some pain killers and
place a warm/hot tea bag like chamomile on your tooth
and hold it. it will relieve the pain
Label by C-Feel-It: Negative
This tweet is objective in nature. The words ?pain?,
?killers?, etc. in the tweet give an indication to C-Feel-It
that the tweet is negative. This misguided implication is
because of multiple senses of these words (for example,
?pain? can also be used in the sentence ?symptoms of the
disease are body pain and irritation in the throat? where
it is non-sentiment-bearing). The lack of understanding
of word senses and being unable to distinguish between
them leads to this error.
4.2.3 Lack of Entity Specificity
Tweet: Casablanca and a lunch comprising of rice
and fish: a good sunday
Keyword: Casablanca
Label by C-Feel-It: Positive
Label by human annotator: Objective
In the above tweet, the human annotator understood
that though the tweet contains the keyword ?Casablanca?,
it is not Casablanca about which sentiment is expressed.
The system finds a positive word ?good? and marks the
tweet as positive. This error arises because the system
cannot find out which sentence/parts of sentence is ex-
pressing opinion about the target entity.
4.2.4 Coverage of Resources
Tweet: I?m done with this bullshit. You?re the psycho
not me.
Label by SentiWordNet: Negative
Label by Taboada/Inquirer: Objective
Label by human annotator: Negative
On manual verification, it was observed that an entry
for the emotion-bearing word ?bullshit? is present in Sen-
tiWordNet while Inquirer and Taboada resource do not
have them. This shows that the coverage of the lexical
resource affects the performance of a system and may in-
troduce errors.
4.2.5 Absence of Named Entity Recognition
Tweet: @user I don?t think I need to guess, but ok,
close encounters of the third kind? Lol
Entity: Close encounters of the third kind
Label by C-Feel-It: Positive
The words comprising the name of the film ?Close en-
counters of the third kind? are also looked up. Inability to
identify the named entity leads the system into this trap.
4.2.6 Requirement of World Knowledge
Tweet: The soccer world cup boasts an audience twice
that of the Summer Olympics.
Label by C-Feel-It: Negative
To judge the opinion of this tweet, one requires an un-
derstanding of the fact that larger the audience, more fa-
vorable it is for a sports tournament. This world knowl-
edge is important for a system that aims to handle tweets
like these.
4.2.7 Mixed Emotion Tweets
Tweet: oh but that last kiss tells me it?s goodbye, just
like nothing happened last night. but if i had one chance,
i?d do it all over again
Label by C-Feel-It: Positive
The tweet contains emotions of positive as well as neg-
ative variety and it would in fact be difficult for a human
as well to identify the polarity. The mixed nature of the
tweet leads to this error by the system.
4.2.8 Lack of Context
Tweet: I?ll have to say it?s a tie between Little Women
or To kill a Mockingbird
131
Label by C-Feel-It: Negative
Label by human user: Positive
The tweet has a sentiment which will possibly be clear
in the context of the conversation. Going by the tweet
alone, while one understands that an comparative opinion
is being expressed, it is not possible to tag it as positive
or negative.
4.2.9 Concatenated Words
Tweet: To Kill a Mockingbird is a #goodbook.
Label by C-Feel-It: Negative
The tweet has a hashtag containing concatenated
words ?goodbook? which get overlooked as out-of-
dictionary words and hence, are not used for sentiment
prediction. The sentiment of ?good? is not detected.
4.2.10 Interjections
Tweet: Oooh. Apocalypse Now is on bluray now.
Label by C-Feel-It: Objective
Label by human user: Positive
The extended interjection ?Oooh? is an indicator of
sentiment. Since it does not have a direct prior polar-
ity, it is not present in any of the resources. However, this
interjection is an important carrier of sentiment.
4.2.11 Comparatives
Tweet: The more years I spend at Colbert Heights..the
more disgusted I get by the people there. I?m soooo ready
to graduate.
Label by C-Feel-It: Positive
Label by human user: Negative
The comparatives in the sentence expressed by ?..more
disgusted I get..? have to be handled as a special case
because ?more? is an intensification of the negative senti-
ment expressed by the word ?disgusted?.
5 Summary & Future Work
In this paper, we described a system which categorizes
live tweets related to a keyword as positive, negative
and objective based on the predictions of four sentiment-
based resources. We also presented a qualitative evalua-
tion of our system pointing out the areas of improvement
for the current system.
A sentiment analyzer of this kind can be tuned to take in-
puts from different sources on the internet (for example,
wall posts on facebook). In order to improve the qual-
ity of sentiment prediction, we propose two additions.
Firstly, while we use simple heuristics to handle exten-
sions of words in tweets, a deeper study is required to
decipher the pragmatics involved. Secondly, a spam de-
tection module that eliminates promotional tweets before
performing sentiment detection may be added to the cur-
rent system. Our goal with respect to this system is to de-
ploy it for predicting share market values of firms based
on sentiment on social networks with respect to related
entitites.
Acknowledgement
We thank Akshat Malu and Subhabrata Mukherjee, IIT
Bombay for their assistance during generation of evalua-
tion data.
References
Go Alec, Huang Lei, and Bhayani Richa. 2009a. Twit-
ter sentiment classification using distant supervision.
Technical report, Standford University.
Go Alec, Bhayani Richa, Raghunathan Karthik, and
Huang Lei. 2009b. May.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of LREC-06, Genova, Italy.
Andreas M. Kaplan and Michael Haenlein. 2010. The
early bird catches the news: Nine things you should
know about micro-blogging. Business Horizons,
54(2):05 ? 113.
Julie B. Lovins. 1968. Development of a Stemming Al-
gorithm. June.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL-05.
Vladimir Prelovac. 2010. Top social media sites. Web,
May.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, pages 158?161, Stan-
ford, US.
2000. Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computional Linguistics, 30:277?308,
September.
132
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?422,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Haves and the Have-Nots: Leveraging Unlabelled Corpora for
Sentiment Analysis
Kashyap Popat2 Balamurali A R1,2,3 Pushpak Bhattacharyya2 Gholamreza Haffari3
1IITB-Monash Research Academy, IIT Bombay 3Monash University
2Dept. of Computer Science and Engineering, IIT Bombay Australia
{kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu
Abstract
Expensive feature engineering based on
WordNet senses has been shown to
be useful for document level sentiment
classification. A plausible reason for
such a performance improvement is the
reduction in data sparsity. However,
such a reduction could be achieved with
a lesser effort through the means of
syntagma based word clustering. In
this paper, the problem of data sparsity
in sentiment analysis, both monolingual
and cross-lingual, is addressed through
the means of clustering. Experiments
show that cluster based data sparsity
reduction leads to performance better than
sense based classification for sentiment
analysis at document level. Similar idea
is applied to Cross Lingual Sentiment
Analysis (CLSA), and it is shown that
reduction in data sparsity (after translation
or bilingual-mapping) produces accuracy
higher than Machine Translation based
CLSA and sense based CLSA.
1 Introduction
Data sparsity is the bane of Natural Language
Processing (NLP) (Xue et al, 2005; Minkov et al,
2007). Language units encountered in the test data
but absent in the training data severely degrade the
performance of an NLP task. NLP applications
innovatively handle data sparsity through various
means. A special, but very common kind of
data sparsity viz., word sparsity, can be addressed
in one of the two obvious ways: 1) sparsity
reduction through paradigmatically related words
or 2) sparsity reduction through syntagmatically
related words.
Paradigmatic analysis of text is the analysis
of concepts embedded in the text (Cruse, 1986;
Chandler, 2012). WordNet is a byproduct of such
an analysis. In WordNet, paradigms are manually
generated based on the principles of lexical and
semantic relationship among words (Fellbaum,
1998). WordNets are primarily used to address the
problem of word sense disambiguation. However,
at present there are many NLP applications
which use WordNet. One such application is
Sentiment Analysis (SA) (Pang and Lee, 2002).
Recent research has shown that word sense based
semantic features can improve the performance of
SA systems (Rentoumi et al, 2009; Tamara et al,
2010; Balamurali et al, 2011) compared to word
based features.
Syntagmatic analysis of text concentrates on
the surface properties of the text. Compared
to paradigmatic property extraction, syntagmatic
processing is relatively light weight. One of
the obvious syntagmas is words, and words are
grouped into equivalence classes or clusters, thus
reducing the model parameters of a statistical NLP
system (Brown et al, 1992). When used as
an additional feature with word based language
models, it has been shown to improve the system
performance viz., machine translation (Uszkoreit
and Brants, 2008; Stymne, 2012), speech
recognition (Martin et al, 1995; Samuelsson and
Reichl, 1999), dependency parsing (Koo et al,
2008; Haffari et al, 2011; Zhang and Nivre, 2011;
Tratz and Hovy, 2011) and NER (Miller et al,
2004; Faruqui and Pado?, 2010; Turian et al, 2010;
Ta?ckstro?m et al, 2012).
In this paper, the focus is on alleviating the
data sparsity faced by supervised approaches
for SA through the means of cluster based
features. As WordNets are essentially word
412
clusters wherein words with the same meaning
are clubbed together, they address the problem of
data sparsity at word level. The abstraction and
dimensionality reduction thus achieved attributes
to the superior performance for SA systems that
employs WordNet senses as features. However,
WordNets are manually created. Automatic
creation of the same is challenging and not much
successful because of the linguistic complexity
involved. In case of SA, manually creating the
features based on WordNet senses is a tedious and
an expensive process. Moreover, WordNets are
not present for many languages. All these factors
make the paradigmatic property based cluster
features like WordNet senses a less promising
pursuit for SA.
The syntagmatic analysis essentially makes use
of distributional similarity and may in many
circumstances subsume the paradigmatic analysis.
In the current work, this particular insight is
used to solve the data sparsity problem in
the sentiment analysis by leveraging unlabelled
monolingual corpora. Specifically, experiments
are performed to investigate whether features
developed from manually crafted clusterings
(coming from WordNet) can be replaced by those
generated from clustering based on syntagmatic
properties.
Further, cluster based features are used to
address the problem of scarcity of sentiment
annotated data in a language. Popular
approaches for Cross-Lingual Sentiment Analysis
(CLSA) (Wan, 2009; Duh et al, 2011) depend
on Machine Translation (MT) for converting
the labeled data from one language to the
other (Hiroshi et al, 2004; Banea et al, 2008;
Wan, 2009). However, many languages which
are truly resource scarce, do not have an MT
system or existing MT systems are not ripe to
be used for CLSA (Balamurali et al, 2013). To
perform CLSA, this study leverages unlabelled
parallel corpus to generate the word alignments.
These word alignments are then used to link
cluster based features to obliterate the language
gap for performing SA. No MT systems or
bilingual dictionaries are used for this study.
Instead, language gap for performing CLSA is
bridged using linked cluster or cross-lingual
clusters (explained in section 4) with the
help of unlabelled monolingual corpora. The
contributions of this paper are two fold:
1. Features created from manually built and
finer clusters can be replaced by inexpensive
cluster based features generated solely from
unlabelled corpora. Experiments performed
on four publicly available datasets in three
languages viz., English, Hindi and Marathi1
suggest that cluster based features can
considerably boost the performance of an SA
system. Moreover, state of the art result
is obtained for one of the publicly available
dataset.
2. An alternative and effective approach for
CLSA is demonstrated using clusters as
features. Word clustering is a powerful
mechanism to ?transfer? a sentiment
classifier from one language to another. Thus
can be used in truly resource scarce scenarios
like that of English-Marathi CLSA.
The rest of the paper is organized as follows:
section 2 presents related work. Section 3 explains
different word cluster based features employed
to reduce data sparsity for monolingual SA. In
section 4, alternative CLSA approaches based
on word clustering are elucidated. Experimental
details are explained in section 5. Results and
discussions are presented in section 6 and section
7 respectively. Finally, section 8 concludes
the paper pointing to some future research
possibilities.
2 Related Work
The problem of SA at document level is defined
as the classification of document into different
polarity classes (positive and negative) (Turney,
2002). Both supervised (Benamara et al, 2007;
Martineau and Finin, 2009) and unsupervised
approaches (Mei et al, 2007; Lin and He, 2009)
exist for this task.
Supervised approaches are popular because
of their superior classification accuracy (Mullen
and Collier, 2004; Pang and Lee, 2008).
Feature engineering plays an important role
in these systems. Apart from the commonly
used bag-of-words features based on
unigrams/bigrams/ngrams (Dave et al, 2003;
Ng et al, 2006; Martineau and Finin, 2009),
1Hindi and Marathi belong to the Indo-Aryan subgroup
of the Indo-European language family and are two widely
spoken Indian languages with a speaker population of 450
million and 72 million respectively.
413
syntax (Matsumoto et al, 2005; Nakagawa et
al., 2010), semantic (Balamurali et al, 2011)
and negation (Ikeda et al, 2008) have also been
explored for this task. There has been research
related to clustering and sentiment analysis. In
Rooney et al (2011), documents are clustered
based on the context of each document and
sentiment labels are attached at the cluster level.
Zhai et al (2011) attempts to cluster features of a
product to perform sentiment analysis on product
reviews. In this work, word clusters (syntagmatic
and paradigmatic) encoding a mixture of syntactic
and semantic information are used for feature
engineering.
In situations where labeled data is not present
in a language, approaches based on cross-lingual
sentiment analysis are used. Most often these
methods depend on an intermediary machine
translation system (Wan, 2009; Brooke et al,
2009) or a bilingual dictionary (Ghorbel and
Jacot, 2011; Lu et al, 2011) to bridge the
language gap. Given the subtle and different
ways the sentiment can be expressed which itself
manifested as a result of cultural diversity amongst
different languages, an MT system has to be of a
superior quality to capture them.
3 Clustering for Sentiment Analysis
The goal of this paper, to remind the reader, is to
investigate whether superior word cluster features
based on manually crafted and fine grained lexical
resource like WordNet can be replaced with the
syntagmatic property based word clusters created
from unlabelled monolingual corpora.
In this section, different clustering approaches
are presented for feature engineering in a
monolingual setting.
3.1 Approach 1: Clustering based on
WordNet Sense
A synonymous set of words in a WordNet is called
a synset. Each synset can be considered as a word
cluster comprising of semantically similar words.
Balamurali et al (2011) showed that WordNet
synsets can act as good features for document level
sentiment classification.
Motivation for their study stems from the fact
that different senses of a word can have different
polarities. To empirically prove the superiority
of sense based features, different variants of
a travel review domain corpus were generated
by using automatic/manual sense disambiguation
techniques. Thereafter, accuracies of classifiers
based on different sense-based and word-based
features were compared. The results suggested
that WordNet synset based features performed
better than word-based features.
In this study, synset identifiers are extracted
from manually/automatically sense annotated
corpora and used as features for creating sentiment
classifiers. The classifier thus build is used as
a baseline. Apart from this, another baseline
employing word based features are used for a
comprehensive comparison.
3.2 Approach 2: Syntagmatic Property based
Clustering
For this particular study, a co-occurrence based
algorithm is used to create word clusters. As
the algorithm is based on co-occurrence, one
can extract the classes that have the flavour of
syntagmatic grouping, depending on the nature
of underlying statistics. Agglomerative clustering
algorithm by Brown et al (1992) is used for this
purpose. It is a hard clustering algorithm i.e., each
word belongs to one cluster only.
Formally, as mentioned in Brown et al (1992),
let C be a hard clustering function which maps
vocabulary V to one of the K clusters. Then,
the likelihood (L()) of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|C(wj?1)))
(1)
Words are assigned to clusters such that the
above quantity is maximized. For the purpose
of sentiment classification, cluster identifiers
representing words in the document are used as
features for training.
4 Clustering for Cross Lingual
Sentiment Analysis
Existing approaches for CLSA depend on an
intermediary machine translation system to bridge
the language gap (Hiroshi et al, 2004; Banea et
al., 2008). Machine translation is very resource
intensive. If a language is truly resource scarce, it
is mostly unlikely to have an MT system. Given
that sentiment analysis is a less resource intensive
task compared to machine translation, the use of
an MT system is hard to justify for performing
414
CLSA. As a viable alternative, cluster linkages
could be learned from a bilingual parallel corpus
and these linkages can be used to bridge the
language gap for CLSA.
In this section, three approaches using clusters
as features for CLSA are compared. The language
whose annotated data is used for training is
called the source language (S), while the language
whose documents are to be sentiment classified is
referred to as the target language (T ).
4.1 Approach 1: Projection based on Sense
(PS)
In this approach, a Multidict is used to bridge the
language gap for SA. A Multidict is an instance
of WordNet where the same sense from different
languages are linked (Mohanty et al, 2008).
An entry in the multidict will have a WordNet
sense identifier from S and the corresponding
WordNet sense identifier from T . The approach
of projection based on sense is explained in
Algorithm 1. Note that after the Sense Mark
operation, each document will be represented as
a vector of WordNet sense identifiers.
Algorithm 1 Projection based on sense
Input: Polarity labeled data in source language
(S) and data in target language (T ) to be
labeled
Output: Classified documents
1: Sense mark the polarity labeled data from S
2: Project the sense marked corpora from S to T
using a Multidict
3: Model the sentiment classifier using the data
obtained in step-2
4: Sense mark the unlabelled data from T
5: Test the sentiment classifier on data obtained
in step-4 using model obtained in step-3
Sense identifiers are the features for the
classifier. For those sense identifiers which do not
have a corresponding entry in the Multidict, no
projection is performed.
4.2 Approach 2: Direct Cluster Linking
(DCL)
Given a parallel bilingual corpus, word clusters in
S can be aligned to clusters in T . Word alignments
are created using parallel corpora. Given two
aligned word sequences wS = [wSj ]mj=1 and
wT = [wTk ]nk=1, let ?T |S be a set of scored
alignments from the source language to the target
language. Here, an alignment from the akth source
word to the kth target word, with score sk,ak > ?
is represented as (wTk , wSak , sk,ak ) ? ?T |S . To
simplify, k ? ?T |S is used to denote those target
words wTk that are aligned to some source word
wSak .
The source and the target side clusters are linked
using the Equation (2).
LC(l) = argmax
t
?
k??T |S ? ?S|T
s.t.CT (wTk )=t
CS (wSak )=l
sk,ak (2)
Here, a target side cluster t ? CT is linked to
a source side cluster l ? CS such that the total
alignment score between words in l and words in
t is maximum. CS and CT stands for source and
target side cluster list respectively. LC(l) gives
the target side cluster t to which l is linked.
4.3 Approach 3: Cross-Lingual Clustering
(XC)
Direct cluster linking approach suffers from the
size of alignment dataset in the form of parallel
corpora. The size of the alignment dataset is
typically smaller than the monolingual dataset.
To circumvent this problem, Ta?ckstro?m et al
(2012) introduced cross-lingual clustering. In
cross-lingual clustering, the objective function
maximizes the joint likelihood of monolingual
and cross-lingual factors. Given a list of
words and clusters it belongs to, a clustering
algorithm tries to obtain word-cluster association
which maximizes the joint likelihood of words
and clusters. Whereas in case of cross-
lingual clustering, the same clustering can be
explained in terms of maximizing the likelihood
of monolingual word-cluster pairs of the source,
the target and alignments between them.
Formally, as stated in Ta?ckstro?m et al (2012),
Using the model of Uszkoreit and Brants (2008),
the likelihood of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|wj?1))
(3)
Note this is different from the likelihood
estimation of Brown et al (1992) (Equation (1)),
where C(wj) was conditioned on C(wj?1). This
415
makes the computation easier as suggested in the
original paper. The Equation (3) in a cross lingual
setting will be transformed as given below:
LS,T (wS , wT ;?T |S , ?S|T , CS , CT ) =
LS(...).LT (...).LT |S(...).LS|T (...) (4)
Here, LT |S(...) and LS|T (...) are factors based on
word alignments, which can be represented as:
LT |S(wT ;?T |S , CT , CS) =
?
k??T |S
p(wTk |CT (wTk ))p(CT (wTk )|CS(wSak)))
(5)
Based on the optimization objective in
Equation (4), a pseudo algorithm is defined in
Algorithm 2. For more information, readers are
requested to refer Ta?ckstro?m et al (2012).
Algorithm 2 Cross-lingual Clustering (XC)
Input: Source and target language corpus
Output: Cross-lingual clusters
1: ## CS , CT randomly initialized
2: for i? 1 to N do
3: Find CS? ? argmaxCS LS(wS ;CS)
4: Project CS? to CT
5: Find CT? ? argmaxCT LT (wT ;CT )
6: Project CT? to CS
7: end for
An MT based CLSA approach is used as the
baseline. Training data from S is translated to T
and classification model is learned using unigram
based features. Thereafter, the classifier is directly
tested on data from T .
5 Experimental Setup
Analysis was performed on three languages, viz.,
English (En), Hindi (Hi) and Marathi (Mar).
CLSA was performed on two language
pairs, English-Hindi and English-Marathi.
For clustering the words, monolingual data of
Indian Languages Corpora Initiative (ILCI)2 was
used. It should also be noted that sentiment
annotated data was also included in the data used
for the word clusterings process. For Brown
clustering, an implementation by Liang (2005)
was used. Cross-lingual clustering for CLSA
2http://sanskrit.jnu.ac.in/ilci/index.
jsp
was implemented as directed in Ta?ckstro?m et al
(2012).
Monolingual SA: For experiments in English,
two polarity datasets were used. The first
one (En-TD) by Ye et al (2009) contains user-
written reviews on travel destinations. The
dataset consists of approximately 600 positive
and 591 negative reviews. Reviews were also
manually sense annotated using WordNet 2.1.
The sense annotation was performed by two
annotators with an inter-annotation agreement of
93%. The second dataset (En-PD)3 on product
reviews (music instruments) from Amazon by
Blitzer et al (2007) contains 1000 positive and
1000 negative reviews. This dataset was sense
annotated using an automatic WSD engine which
was trained on tourism domain (Khapra et al,
2010). Experiments using this dataset were
done to study the effect of domain on CLSA.
For experiments in Hindi and Marathi, polarity
datasets by Balamurali et al (2012) were used.4
These are reviews collected from various Hindi
and Marathi blogs and Sunday editorials. Hindi
dataset consist of 98 positive and 100 negative
reviews. Whereas Marathi dataset contains 75
positive and 75 negative reviews. Apart from
being marked with polarity labels at document
level, they are also manually sense annotated using
Hindi and Marathi WordNet respectively.
CLSA: The same datasets used in SA are also
used for CLSA. Three approaches (as described
in section 4) were tested for English-Hindi
and English-Marathi language pairs. To create
alignments, English-Hindi and English-Marathi
parallel corpora from ILCI were used. English-
Hindi parallel corpus contains 45992 sentences
and English-Marathi parallel corpus contains
47881 sentences. To create alignments, GIZA++5
was used (Och and Ney, 2003).
As a preprocessing step, all stop words
were removed. Stemming was performed on
English and Hindi whereas for Marathi data,
Morphological Analyzer was used to reduce the
words to their respective lemmas.
All experiments were performed using C-SVM
3http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/
4http://www.cfilt.iitb.ac.
in/resources/senti/MPLC_tour_
downloaderInfo.php
5http://www-i6.informatik.rwth-aachen.
de/Colleagues/och/software/GIZA++.html
416
Features En-TD En-PD Hi Mar
Words 87.02 77.60 77.36 92.28
WordNet Sense (Paradigmatic) 89.13 74.50 85.80 96.88
Clusters (Syntagmatic) 97.45 87.80 83.50 z 98.66
Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on
two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
Features Words Clust-200 Clust-500 Clust-1000 Clust-1500 Clust-2000 Clust-2500 Clust-3000
En-TD 87.02 97.37 97.45 96.94 96.94 96.52 96.52 96.52
En-PD 77.60 73.20 82.30 84.30 86.35 86.45 87.80 87.40
Table 2: Classification accuracy (in %) versus cluster size (number of clusters to be used).
(linear kernel with parameter optimized over
training set using 5 fold cross validation) available
as a part of LibSVM package6. SVM was used
since it is known to perform well for sentiment
classification (Pang et al, 2002). Results reported
are based on the average of ten-fold cross-
validation accuracies. Standard text metrics are
used for reporting the experimental results.
6 Results
Monolingual classification results are shown in
Table71. Table shows accuracies of SA systems
developed on feature set based on words, senses
and clusters. It must be noted that accuracies
reported for cluster based features are with
respect to the best accuracy based on different
cluster sizes. The improvements in results of
cluster features based approach is found to be
statistically significant over the word features
based approach and sense features based approach
at 95% confidence level when tested using a paired
t-test (except for Hindi cluster features based
approach). But in general, their accuracies do not
significantly vary after cluster size crosses 1500.
Table 2 shows the classification accuracy
variation when cluster size is altered. For,
En-TD and En-PD experiments, the cluster size
was varied between 200-3000 with an interval
of 500 (after a size of 500). In the En-TD
experiment, the best accuracy is achieved for
cluster size 500, which is lesser than the number of
unique-words/unique-senses (6435/6004) present
in the data. Similarly, for the En-PD experiment,
6http://www.csie.ntu.edu.tw/
?
cjlin/
libsvm
7All results reported here are based on 10-fold except for
Marathi (2-fold-5-repeats), as it had comparatively lesser data
samples.
the optimal cluster size of 2500 is also lesser
than the number of unique-words/unique-senses
(30468/4735) present in the data.
To see the effect of training data size variation
for different SA approaches in the En-TD
experiment, the training data size is varied
between 50 to 500. For this, a test set consisting
of 100 positive and 100 negative documents is
fixed. The training data size is varied by selecting
different number of documents from rest of the
dataset (?500 negative and ?500 positive) as a
training set. For each training data set 10 repeats
are performed, e.g., for training data size of 50, 50
negative and 50 positive documents are randomly
selected from the training data pool of ?500
negative and ?500 positive. This was repeated
10 times (with replacement). The results of this
experiment are presented in Figure 1.
 70
 75
 80
 85
 90
 95
 100
 0  100  200  300  400  500
Ac
cu
ra
cy
(%
)
Training data size
Words
Senses (Paradigmatic)
Clusters (Syntagmatic)
Figure 1: Training data variation on En-TD
dataset.
Cross-lingual SA accuracies are presented in
Table 3. As in monolingual case, the reported
accuracies are for features based on the best
cluster size.
417
Target Language MT PS DCL XC
T=Hi 63.13 53.80 51.51 66.16
T=Mar NA 54.00 56.00 60.30
Table 3: Cross-Lingual SA accuracy (%) on T=Hi and T=Mar with S=En for different approaches
(MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross-
Lingual Clustering. There is no MT system available for (S=En, T=Mar).
7 Discussions
In this section, some important observations from
the results are discussed.
1. Syntagmatic analysis may be used in lieu
of paradigmatic analysis for SA: The results
suggest that word cluster based features using
syntagmatic analysis is comparatively better than
cluster (sense) based features using paradigmatic
analysis. For two datasets in English and for
the one in Marathi this holds true. For English,
the gap between classification accuracy based on
sense features and cluster features is around 10%.
A state-of-art accuracy is obtained for the public
dataset on travel domain (En-TD).
The difference in accuracy reduces as the
language gets morphologically rich. In a
morphologically rich language, morphology
encompasses syntactical information, limiting the
context it can provide for clustering. This can be
seen from the classification results on Marathi.
However for Hindi, classifier built on features
based on syntagmatic analysis trails the one based
on paradigmatic analysis.
Compared to Marathi, Hindi is a less
morphologically rich language, hence, a better
result was expected. However, a contrary result
was obtained.z In Hindi, the subject and the
object of the sentence are linked using a case
marker. Upon error analysis, it was found that
there was a lot of irregular compounding based
on case markers. Case markers were compounded
with the succeeding word. This is a deviation
from the real scenario which would have resulted
in incorrect clustering leading to an unexpected
result. However, the same would not have
occurred for a classifier developed on sense based
features as it was manually sense tagged.
Clustering induces a reduction in the data
sparsity. For example, on En-PD, percentage of
features present in the test set and not present in
the training set to those present in the test set
are 34.17%, 11.24%, 0.31% for words, synsets
and cluster based features respectively. The
improvement in the performance of classifiers
may be attributed to this feature size reduction.
However, it must be noted that clustering based
on unlabelled corpora is less taxing than manually
creating paradigmatic property based clusters like
WordNet synsets.
Barring one instance, both cluster based
features outperform word based features. The
reason for the drop in the accuracy of approach
based on sense features for En-PD dataset
is the domain specific nature of sentiment
analysis (Blitzer et al, 2007), which is explained
in the next point.
2. Domain issues are resolved while using
cluster based features: For En-PD, the classifier
developed using sense features based on
paradigmatic analysis performs inferior to
word based features. Compared to other datasets
used for analysis, this dataset was sense annotated
using an automatic WSD engine. This engine was
trained on a travel domain corpus and as WSD
is also domain specific, the final classification
performance suffered. Additionally, as the target
domain was on products, the automatic WSD
engine employed had an in-domain accuracy
of 78%. The sense disambiguation accuracy of
the same would have lowered in a cross-domain
setting. This might have had a degrading effect on
the SA accuracy.
However, it was seen that classifier developed
on cluster features based on syntagmatic analysis
do not suffer from this. Such clusters
obliterate domain relates issues. In addition, as
more unlabelled data is included for clustering,
the classification accuracy improves.8 Thus,
clustering may be employed to tackle other
specific domain related issues in SA.
8It was observed that adding 0.1 million unlabelled
documents, SA accuracy improved by 1%. This was observed
in the case of English for which there is abundant unlabelled
corpus.
418
3. Cluster based features using syntagmatic
analysis requires lesser training data: Cluster
based features drastically reduces the dimension
of the feature vector. For instance, the size
of sense based features for En-TD dataset was
1/6th of the size of word based features. This
reduces the perplexity of the classification model.
The reduction in the perplexity leads to the
reduction of training documents to attain the same
classification accuracy without any dimensionality
reduction. This is evident from Figure 1
where accuracy of the cluster features based on
unlabelled corpora are higher even with lesser
training data.
4. Effect of cluster size: The cluster size
(number of clusters employed) has an implication
on the purity of each cluster with respect to the
application. The system performance improved
upon increasing the cluster size and converged
after attaining a certain level of accuracy. In
general, it was found that the best classification
accuracy was obtained for a cluster size between
1000 and 2500. As evident from Table 2, once
the optimal accuracy is obtained, no significant
changes were observed by increasing the cluster
size.
5. Clustering based CLSA is effective:
For target language as Hindi, CLSA accuracy
based on cross-lingual clustering (syntagmatic)
outperforms the one based on MT (refer to
Table 3). This was true for the constraint
clustering approach based on cross-lingual
clustering. Whereas, sentiment classifier using
sense (PS) or direct cluster linking (DCL) is
not very effective. In case of PS approach, the
coverage of the multidict was a problem. The
number of a linkages between sense from English
to Hindi is only around 1/3rd the size of Princeton
WordNet (Fellbaum, 1998). Similarly in case
of DCL approach, monolingual likelihood is
different from the cross-lingual likelihood in
terms of the linkages.
6. A note on CLSA for truly resource scarce
languages: Note that there is no publicly available
MT system for English to Marathi. Moreover,
the digital content in Marathi language does not
have a standard encoding format. This impedes
the automatic crawling of the web for corpora
creation for SA. Much manual effort has to be put
to collect enough corpora for analysis. However,
even in these languages, unlabelled corpora is
easy to obtain. Marathi was chosen to depict
a truly resource scarce SA scenario. Cluster
features based classifier comparatively performed
well with 60% classification accuracy. An MT
based system would have suffered in this case as
Marathi, as stated earlier, is a morphologically
rich language and as compared to English, has a
different word ordering. This could degrade the
accuracy of the machine translation itself, limiting
the performance of an MT based CLSA system.
All this is obliterated by the use of a cluster based
CLSA approach. Moreover, as more monolingual
copora is added for clustering, the cross lingual
cluster linkages could be refined. This can further
boost the CLSA accuracy.
8 Conclusion and Future Work
This paper explored feasibility of using word
cluster based features in lieu of features based on
WordNet senses for sentiment analysis to alleviate
the problem of data sparsity. Abstractly, the
motivation was to see if highly effective features
based on paradigmatic property based clustering
could be replaced with the inexpensive ones based
on syntagmatic property for SA.
The study was performed for both monolingual
SA and cross-lingual SA. It was found that
cluster features based on syntagmatic analysis
are better than the WordNet sense features based
on paradigmatic analysis for SA. Invesitgation
revealed that a considerable decrease in the
training data could be achieved while using such
class based features. Moreover, as syntagma based
word clusters are homogenous, it was able to
address domain specific nature of SA as well.
For CLSA, clusters linked together using
unlabelled parallel corpora do away with the need
of translating labelled corpora from one language
to another using an intermediary MT system or
bilingual dictionary. Such a method outperforms
an MT based CLSA approach. Further, this
approach was found to be useful in cases where
there are no MT systems to perform CLSA and
the language of analysis is truly resource scarce.
Thus, wider implication of this study is that many
widely spoken yet resource scare languages like
Pashto, Sundanese, Hausa, Gujarati and Punjabi
which do not have an MT system could now be
analysed for sentiment. The approach presented
here for CLSA will still require a parallel corpora.
However, the size of the parallel corpora required
419
for CLSA can considerably be much lesser than
the size of the parallel corpora required to train an
MT system.
A naive cluster linkage algorithm based on word
alignments was used to perform CLSA. As a
result, there were many erroneous linkages which
lowered the final SA accuracy. Better cluster-
linking approaches could be explored to alleviate
this problem. There are many applications which
use WordNet like IR, IE etc. It would be
interesting to see if these could be replaced by
clusters based on the syntagmatic property.
References
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2011. Harnessing wordnet senses for su-
pervised sentiment classification. In Proceedings of
EMNLP 2011, pages 1081?1091, Stroudsburg, PA,
USA.
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked wordnets. In Pro-
ceedings of COLING 2012, pages 73?82, Mumbai,
India.
A. R. Balamurali, Mitesh M. Khapra, and Pushpak
Bhattacharyya. 2013. Lost in translation: viability
of machine translation for cross language sentiment
analysis. In Proceedings of CICLing 2013, pages
38?49, Berlin, Heidelberg.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of EMNLP 2008, pages 127?135, Honolulu, Hawaii.
Farah Benamara, Sabatier Irit, Carmine Cesarano,
Napoli Federico, and Diego Reforgiato. 2007. Sen-
timent analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of the Inter-
national Conference on Weblogs and Social Media.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007, pages 440?
447, Prague, Czech Republic.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of the International
Conference RANLP-2009, pages 50?54, Borovets,
Bulgaria.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, pages 467?479,
December.
D. Chandler. 2012. Semiotics for begin-
ners. http://users.aber.ac.uk/dgc/
Documents/S4B/sem01.html. Online, ac-
cessed 20-February-2013.
D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003, pages 519?528, New
York, NY, USA.
Kevin Duh, Akinori Fujino, and Masaaki Nagata.
2011. Is machine translation ripe for cross-lingual
sentiment classification? In Proceedings of ACL-
HLT 2011, pages 429?433, Stroudsburg, PA, USA.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and Evaluating a German Named Entity Recognizer
with Semantic Generalization. In Proceedings of
KONVENS 2010, Saarbru?cken, Germany.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Hatem Ghorbel and David Jacot. 2011. Further ex-
periments in sentiment analysis of french movie re-
views. In Proceedings of AWIC 2011, pages 19?28,
Fribourg, Switzerland.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL-HLT
2011, pages 710?714, Stroudsburg, PA, USA.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING 2004, Stroudsburg, PA, USA.
Daisuke Ikeda, Hiroya Takamura, Lev arie Ratinov, and
Manabu Okumura. 2008. Learning to shift the po-
larity of words for sentiment classification. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In Proceedings of
Global Wordnet Conference.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-HLT 2008, pages 595?603,
Columbus, Ohio.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. M. eng. thesis, Massachusetts Institute
of Technology.
420
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM 2009, pages 375?384, New York,
NY, USA.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of
ACL-HLT 2011, pages 320?330, Stroudsburg, PA,
USA.
Sven Martin, Jrg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253?1256.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis.
In Proceedings of ICWSM.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Advances in Knowledge Discovery and Data Min-
ing, Lecture Notes in Computer Science, pages 301?
311.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW 2007, pages 171?180, New
York, NY, USA.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT-NAACL
2004: Main Proceedings, pages 337?342, Boston,
Massachusetts, USA.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of ACL 2007, pages
128?135, Prague, Czech Republic.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictio-
nary: Insights, applications and challenges. In Pro-
ceedings of Global Wordnet Conference.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of EMNLP 2004,
pages 412?418, Barcelona, Spain.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of HLT-NAACL 2010, pages 786?794, Strouds-
burg, PA, USA.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classifi-
cation of reviews. In Proceedings of the COLING
2006, pages 611?618, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Bo Pang and Lillian Lee. 2002. Thumbs up? sen-
timent classification using machine learning tech-
niques. In Proceedings of EMNLP 2002, pages 79?
86, Stroudsburg, PA, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Senti-
ment analysis of figurative language using a word
sense disambiguation approach. In Proceedings of
RANLP 2009, pages 370?375, Borovets, Bulgaria,
September.
Niall Rooney, Hui Wang, Fiona Browne, Fergal Mon-
aghan, Jann Mller, Alan Sergeant, Zhiwei Lin,
Philip Taylor, and Vladimir Dobrynin. 2011. An ex-
ploration into the use of contextual document clus-
tering for cluster sentiment analysis. In Proceedings
of RANLP 2011, pages 140?145, Hissar, Bulgaria.
C. Samuelsson and W. Reichl. 1999. A class-based
language model for large-vocabulary speech recog-
nition extracted from part-of-speech statistics. In
Proceedings of ICASSP 1999, pages 537?540.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of the Joint Workshop on Unsupervised and
Semi-Supervised Learning in NLP, pages 28?34.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings
of NAACL-HLT 2012, pages 477?487, Montre?al,
Canada.
Martin Tamara, Balahur Alexandra, and Montoyo An-
dres. 2010. Word sense disambiguation in opinion
mining: Pros and cons. Journal Research in Com-
puting Science, 46:119?130.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP 2011, pages 1257?1268,
Stroudsburg, PA, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384?394, Stroudsburg, PA, USA.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424, Stroudsburg, PA, USA.
421
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-HLT 2008, pages 755?762, Colum-
bus, Ohio.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of ACL 2009,
pages 235?243, Stroudsburg, PA, USA.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi,
Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005.
Scalable collaborative filtering using cluster-based
smoothing. In Proceedings of SIGIR 2005, pages
114?121, New York, NY, USA.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009.
Sentiment classification of online reviews to travel
destinations by supervised machine learning ap-
proaches. Expert Systems with Applications, 36(3,
Part 2):6527?6535.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Clustering product features for opinion mining. In
Proceedings of WSDM 2011, pages 347?354, New
York, NY, USA.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of ACL-HLT 2011, pages 188?
193, Stroudsburg, PA, USA.
422
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 268?272,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
IndoNet: A Multilingual Lexical Knowledge Network for Indian
Languages
Brijesh Bhatt Lahari Poddar Pushpak Bhattacharyya
Center for Indian Language Technology
Indian Institute of Technology Bombay
Mumbai, India
{ brijesh, lahari, pb } @cse.iitb.ac.in
Abstract
We present IndoNet, a multilingual lexi-
cal knowledge base for Indian languages.
It is a linked structure of wordnets of
18 different Indian languages, Universal
Word dictionary and the Suggested Up-
per Merged Ontology (SUMO). We dis-
cuss various benefits of the network and
challenges involved in the development.
The system is encoded in Lexical Markup
Framework (LMF) and we propose mod-
ifications in LMF to accommodate Uni-
versal Word Dictionary and SUMO. This
standardized version of lexical knowledge
base of Indian Languages can now easily
be linked to similar global resources.
1 Introduction
Lexical resources play an important role in nat-
ural language processing tasks. Past couple of
decades have shown an immense growth in the de-
velopment of lexical resources such as wordnet,
Wikipedia, ontologies etc. These resources vary
significantly in structure and representation for-
malism.
In order to develop applications that can make
use of different resources, it is essential to link
these heterogeneous resources and develop a com-
mon representation framework. However, the dif-
ferences in encoding of knowledge and multilin-
guality are the major road blocks in development
of such a framework. Particularly, in a multilin-
gual country like India, information is available in
many different languages. In order to exchange in-
formation across cultures and languages, it is es-
sential to create an architecture to share various
lexical resources across languages.
In this paper we present IndoNet, a lexical re-
source created by merging wordnets of 18 dif-
ferent Indian languages1, Universal Word Dictio-
nary (Uchida et al, 1999) and an upper ontology,
SUMO (Niles and Pease, 2001).
Universal Word (UW), defined by a headword
and a set of restrictions which give an unambigu-
ous representation of the concept, forms the vo-
cabulary of Universal Networking Language. Sug-
gested Upper Merged Ontology (SUMO) is the
largest freely available ontology which is linked
to the entire English WordNet (Niles and Pease,
2003). Though UNL is a graph based repre-
sentation and SUMO is a formal ontology, both
provide language independent conceptualization.
This makes them suitable candidates for interlin-
gua.
IndoNet is encoded in Lexical Markup Frame-
work (LMF), an ISO standard (ISO-24613) for
encoding lexical resources (Francopoulo et al,
2009).
The contribution of this work is twofold,
1. We propose an architecture to link lexical re-
sources of Indian languages.
2. We propose modifications in Lexical Markup
Framework to create a linked structure of
multilingual lexical resources and ontology.
2 Related Work
Over the years wordnet has emerged as the most
widely used lexical resource. Though most of the
wordnets are built by following the standards laid
by English Wordnet (Fellbaum, 1998), their con-
ceptualizations differ because of the differences in
lexicalization of concepts across languages. ?Not
1Wordnets for Indian languages are developed in In-
doWordNet project. Wordnets are available in following
Indian languages: Assamese, Bodo, Bengali, English, Gu-
jarati, Hindi, Kashmiri, Konkani, Kannada, Malayalam, Ma-
nipuri, Marathi, Nepali, Punjabi, Sanskrit, Tamil, Telugu and
Urdu. These languages covers 3 different language families,
Indo Aryan, Sino-Tebetian and Dravidian.http://www.
cfilt.iitb.ac.in/indowordnet
268
only that, there exist lexical gaps where a word
in one language has no correspondence in another
language, but there are differences in the ways lan-
guages structure their words and concepts?. (Pease
and Fellbaum, 2010).
The challenge of constructing a unified multi-
lingual resource was first addressed in EuroWord-
Net (Vossen, 1998). EuroWordNet linked word-
nets of 8 different European languages through
a common interlingual index (ILI). ILI consists
of English synsets and serves as a pivot to link
other wordnets. While ILI allows each language
wordnet to preserve its semantic structure, it has
two basic drawbacks as described in Fellbaum and
Vossen (2012),
1. An ILI tied to one specific language clearly
reflects only the inventory of the language it
is based on, and gaps show up when lexicons
of different languages are mapped to it.
2. The semantic space covered by a word in one
language often overlaps only partially with a
similar word in another language, resulting in
less than perfect mappings.
Subsequently in KYOTO project2, ontologies are
preferred over ILI for linking of concepts of dif-
ferent languages. Ontologies provide language in-
dpendent conceptualization, hence the linking re-
mains unbiased to a particular language. Top level
ontology SUMO is used to link common base
concepts across languages. Because of the small
size of the top level ontology, only a few wordnet
synsets can be linked directly to the ontological
concept and most of the synsets get linked through
subsumption relation. This leads to a significant
amount of information loss.
KYOTO project used Lexical Markup Frame-
work (LMF) (Francopoulo et al, 2009) as a rep-
resentation language. ?LMF provides a com-
mon model for the creation and use of lexical re-
sources, to manage the exchange of data among
these resources, and to enable the merging of a
large number of individual electronic resources to
form extensive global electronic resources? (Fran-
copoulo et al, 2009). Soria et al (2009) proposed
WordNet-LMF to represent wordnets in LMF for-
mat. Henrich and Hinrichs (2010) have further
modified Wordnet-LMF to accommodate lexical
2http://kyoto-project.eu/xmlgroup.iit.
cnr.it/kyoto/index.html
relations. LMF also provides extensions for multi-
lingual lexicons and for linking external resources,
such as ontology. However, LMF does not explic-
itly define standards to share a common ontology
among multilingual lexicons.
Our work falls in line with EuroWordNet and
Kyoto except for the following key differences,
? Instead of using ILI, we use a ?common con-
cept hierarchy? as a backbone to link lexicons
of different languages.
? In addition to an upper ontology, a concept in
common concept hierarchy is also linked to
Universal Word Dictionary. Universal Word
dictionary provides additional semantic in-
formation regarding argument types of verbs,
that can be used to provide clues for selec-
tional preference of a verb.
? We refine LMF to link external resources
(e.g. ontologies) with multilingual lexicon
and to represent Universal Word Dictionary.
3 IndoNet
IndoNet uses a common concept hierarchy to
link various heterogeneous lexical resources. As
shown in figure 1, concepts of different wordnets,
Universal Word Dictionary and Upper Ontology
are merged to form the common concept hierar-
chy. Figure 1 shows how concepts of English
WordNet (EWN), Hindi Wordnet (HWN), upper
ontology (SUMO) and Universal Word Dictionary
(UWD) are linked through common concept hier-
archy (CCH).
This section provides details of Common Con-
cept Hierarcy and LMF encoding for different re-
sources.
Figure 1: An Example of Indonet Structure
269
Figure 2: LMF representation for Universal Word Dictionary
3.1 Common Concept Hierarchy (CCH)
The common concept hierarchy is an abstract pivot
index to link lexical resources of all languages. An
element of a common concept hierarchy is defined
as < sinid1, sinid2, ..., uwid, sumoid > where,
sinidi is synset id of ith wordnet, uw id is univer-
sal word id, and sumo id is SUMO term id of the
concept. Unlike ILI, the hypernymy-hyponymy
relations from different wordnets are merged to
construct the concept hierarchy. Each synset of
wordnet is directly linked to a concept in ?com-
mon concept hierarchy?.
3.2 LMF for Wordnet
We have adapted the Wordnet-LMF, as specified
in Soria et al (2009). However IndoWordnet
encodes more lexical relations compared to Eu-
roWordnet. We enhanced the Wordnet-LMF to ac-
commodate the following relations: antonym, gra-
dation, hypernymy, meronym, troponymy, entail-
ment and cross part of speech links for ability and
capability.
3.3 LMF for Universal Word Dictionary
A Universal Word is composed of a headword and
a list of restrictions, that provide unique meaning
of the UW. In our architecture we allow each sense
of a headword to have more than one set of restric-
tions (defined by different UW dictionaries) and
be linked to lemmas of multiple languages with a
confidence score. This allows us to merge multiple
UW dictionaries and represent it in LMF format.
We introduce four new LMF classes; Restrictions,
Restriction, Lemmas and Lemma and add new at-
tributes; headword and mapping score to existing
LMF classes.
Figure 2 shows an example of LMF represen-
tation of UW Dictionary. At present, the dic-
tionary is created by merging two dictionaries,
UW++ (Boguslavsky et al, 2007) and CFILT
Hin-UW3. Lemmas from different languages are
mapped to universal words and stored under the
Lemmas class.
3.4 LMF to link ontology with Common
Concept Hierarchy
Figure 3 shows an example LMF representation
of CCH. The interlingual pivot is represented
through SenseAxis. Concepts in different re-
sources are linked to the SenseAxis in such a way
that concepts linked to same SenseAxis convey the
same Sense.
Using LMF class MonolingualExternalRefs,
ontology can be integrated with a monolingual
lexicon. In order to share an ontology among mul-
tilingual resources, we modify the original core
package of LMF.
As shown in figure 3, a SUMO term is shared
across multiple lexicons via the SenseAxis. SUMO
is linked with concept hierarchy using the follow-
3http://www.cfilt.iitb.ac.in/?hdict/
webinterface_user/
270
Figure 3: LMF representation for Common Concept Hierarchy
ing relations: antonym, hypernym, instance and
equivalent. In order to support these relations,
Reltype attribute is added to the interlingual Sense
class.
4 Observation
Table 1 shows part of speech wise status of linked
concepts4. The concept hierarchy contains 53848
concepts which are shared among wordnets of In-
dian languages, SUMO and Universal Word Dic-
tionary. Out of the total 53848 concepts, 21984 are
linked to SUMO, 34114 are linked to HWN and
44119 are linked to UW. Among these, 12,254 are
common between UW and SUMO and 21984 are
common between wordnet and SUMO.
POS HWN UW SUMO CCH
adjective 5532 2865 3140 5193
adverb 380 2697 249 2813
noun 25721 32831 16889 39620
verb 2481 5726 1706 6222
total 34114 44119 21984 53848
Table 1: Details of the concepts linked
This creates a multilingual semantic lexicon
that captures semantic relations between concepts
of different languages. Figure 1 demonstrates
this with an example of ?kinship relation?. As
4Table 1 shows data for Hindi Wordnet. Statistics for
other wordnets can be found at http://www.cfilt.
iitb.ac.in/wordnet/webhwn/iwn_stats.php
shown in Figure 1, ?uncle? is an English lan-
guage concept defined as ?the brother of your fa-
ther or mother?. Hindi has no concept equivalent
to ?uncle? but there are two more specific concepts
?kaka?, ?brother of father.? and ?mama?, ?brother
of mother.?
The lexical gap is captured when these con-
cepts are linked to CCH. Through CCH, these con-
cepts are linked to SUMO term ?FamilyRelation?
which shows relation between these concepts.
Universal Word Dictionary captures exact rela-
tion between these concepts by applying restric-
tions [chacha] uncle(icl>brother (mod>father))
and [mama] uncle(icl>brother (mod>mother)).
This makes it possible to link concepts across lan-
guages.
5 Conclusion
We have presented a multilingual lexical resource
for Indian languages. The proposed architecture
handles the ?lexical gap? and ?structural diver-
gence? among languages, by building a common
concept hierarchy. In order to encode this resource
in LMF, we developed standards to represent UW
in LMF.
IndoNet is emerging as the largest multilingual
resource covering 18 languages of 3 different lan-
guage families and it is possible to link or merge
other standardized lexical resources with it.
Since Universal Word dictionary is an integral
part of the system, it can be used for UNL based
271
Machine Translation tasks. Ontological structure
of the system can be used for multilingual infor-
mation retrieval and extraction.
In future, we aim to address ontological issues
of the common concept hierarchy and integrate
domain ontologies with the system. We are also
aiming to develop standards to evaluate such mul-
tilingual resources and to validate axiomatic foun-
dation of the same. We plan to make this resource
freely available to researchers.
Acknowledgements
We acknowledge the support of the Department of
Information Technology (DIT), Ministry of Com-
munication and Information Technology, Gov-
ernment of India and also of Ministry of Hu-
man Resource Development. We are also grate-
ful to Study Group for Machine Translation and
Automated Processing of Languages and Speech
(GETALP) of the Laboratory of Informatics of
Grenoble (LIG) for assissting us in building the
Universal Word dictionary.
References
I. Boguslavsky, J. Bekios, J. Cardenosa, and C. Gal-
lardo. 2007. Using Wordnet for Building an In-
terlingual Dictionary. In Fifth International Con-
ference Information Research and Applications,
(TECH 2007).
Christiane Fellbaum and Piek Vossen. 2012. Chal-
lenges for a multilingual wordnet. Language Re-
sources and Evaluation, 46(2):313?326, june.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Gil Francopoulo, Nuria Bel, Monte George, Nicoletta
Calzolari, Monica Monachini, Mandy Pet, and Clau-
dia Soria. 2009. Multilingual resources for NLP
in the lexical markup framework (LMF). Language
Resources and Evaluation.
Verena Henrich and Erhard Hinrichs. 2010. Standard-
izing wordnets in the ISO standard LMF: Wordnet-
LMF for GermaNet. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ?10, pages 456?464, Stroudsburg, PA,
USA.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Proceedings of the international
conference on Formal Ontology in Information Sys-
tems - Volume 2001, FOIS ?01, pages 2?9, New York
NY USA. ACM.
Ian Niles and Adam Pease. 2003. Linking Lexicons
and Ontologies: Mapping WordNet to the Suggested
Upper Merged Ontology. In Proceedings Of The
2003 International Conference On Information And
Knowledge Engineering (Ike 03), Las Vegas, pages
412?416.
Adam Pease and Christiane Fellbaum. 2010. Formal
ontology as interlingua: The SUMO and WordNet
linking project and global wordnet. In Ontology and
Lexicon, A Natural Language Processing perspec-
tive, pages 25?35. Cambridge University Press.
Claudia Soria, Monica Monachini, and Piek Vossen.
2009. Wordnet-LMF: fleshing out a standardized
format for wordnet interoperability. In Proceedings
of the 2009 international workshop on Intercultural
collaboration, IWIC ?09, pages 139?146, New York,
NY, USA. ACM.
H. Uchida, M. Zhu, and T. Della Senta. 1999. The
UNL- a Gift for the Millenium. United Nations Uni-
versity Press, Tokyo.
Piek Vossen, editor. 1998. EuroWordNet: a mul-
tilingual database with lexical semantic networks.
Kluwer Academic Publishers, Norwell, MA, USA.
272
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 346?351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatically Predicting Sentence Translation Difficulty
Abhijit Mishra?, Pushpak Bhattacharyya?, Michael Carl?
? Department of Computer Science and Engineering, IIT Bombay, India
{abhijitmishra,pb}@cse.iitb.ac.in
? CRITT, IBC, Copenhagen Business School, Denmark,
mc.ibc@cbs.dk
Abstract
In this paper we introduce Translation Dif-
ficulty Index (TDI), a measure of diffi-
culty in text translation. We first de-
fine and quantify translation difficulty in
terms of TDI. We realize that any mea-
sure of TDI based on direct input by trans-
lators is fraught with subjectivity and ad-
hocism. We, rather, rely on cognitive ev-
idences from eye tracking. TDI is mea-
sured as the sum of fixation (gaze) and
saccade (rapid eye movement) times of
the eye. We then establish that TDI is
correlated with three properties of the in-
put sentence, viz. length (L), degree of
polysemy (DP) and structural complexity
(SC). We train a Support Vector Regres-
sion (SVR) system to predict TDIs for
new sentences using these features as in-
put. The prediction done by our frame-
work is well correlated with the empiri-
cal gold standard data, which is a repos-
itory of < L,DP, SC > and TDI pairs
for a set of sentences. The primary use of
our work is a way of ?binning? sentences
(to be translated) in ?easy?, ?medium? and
?hard? categories as per their predicted
TDI. This can decide pricing of any trans-
lation task, especially useful in a scenario
where parallel corpora for Machine Trans-
lation are built through translation crowd-
sourcing/outsourcing. This can also pro-
vide a way of monitoring progress of sec-
ond language learners.
1 Introduction
Difficulty in translation stems from the fact that
most words are polysemous and sentences can be
long and have complex structure. While length of
sentence is commonly used as a translation diffi-
culty indicator, lexical and structural properties of
a sentence also contribute to translation difficulty.
Consider the following example sentences.
1. The camera-man shot the policeman
with a gun. (length-8)
2. I was returning from my old office
yesterday. (length-8)
Clearly, sentence 1 is more difficult to process
and translate than sentence 2, since it has lexical
ambiguity (?Shoot? as an act of firing a shot or
taking a photograph?) and structural ambiguity
(Shot with a gun or policeman with a gun?). To
produce fluent and adequate translations, efforts
have to be put to analyze both the lexical and syn-
tactic properties of the sentences.
The most recent work on studying translation
difficulty is by Campbell and Hale (1999) who
identified several areas of difficulty in lexis and
grammar. ?Reading? researchers have focused on
developing readability formulae, since 1970. The
Flesch-Kincaid Readability test (Kincaid et al,
1975), the Fry Readability Formula (Fry, 1977)
and the Dale-Chall readability formula (Chall and
Dale, 1999) are popular and influential. These for-
mulae use factors such as vocabulary difficulty (or
semantic factors) and sentence length (or syntac-
tic factors). In a different setting, Malsburg et
al. (2012) correlate eye fixations and scanpaths
of readers with sentence processing. While these
approaches are successful in quantifying readabil-
ity, they may not be applicable to translation sce-
narios. The reason is that, translation is not
merely a reading activity. Translation requires
co-ordination between source text comprehension
and target text production (Dragsted, 2010). To
the best of our knowledge, our work on predicting
TDI is the first of its kind.
The motivation of the work is as follows. Cur-
rently, for domain specific Machine Translation
systems, parallel corpora are gathered through
translation crowdsourcing/outsourcing. In such
346
Figure 1: Inherent sentence complexity and per-
ceived difficulty during translation
a scenario, translators are paid on the basis of
sentence length, which ignores other factors con-
tributing to translation difficulty, as stated above.
Our proposed Translation Difficulty Index (TDI)
quantifies the translation difficulty of a sentence
considering both lexical and structural proper-
ties. This measure can, in turn, be used to clus-
ter sentences according to their difficulty levels
(viz. easy, medium, hard). Different payment and
schemes can be adopted for different such clusters.
TDI can also be useful for training and evalu-
ating second language learners. For example, ap-
propriate examples at particular levels of difficulty
can be chosen for giving assignments and monitor-
ing progress.
The rest of the paper is organized in the fol-
lowing way. Section 2 describes TDI as func-
tion of translation processing time. Section 3 is
on measuring translation processing time through
eye tracking. Section 4 gives the correlation of
linguistic complexity with observed TDI. In sec-
tion 5, we describe a technique for predicting TDIs
and ranking unseen sentences using Support Vec-
tor Machines. Section 6 concludes the paper with
pointers to future work.
2 Quantifying Translation Difficulty
As a first approximation, TDI of a sentence can
be the time taken to translate the sentence, which
can be measured through simple translation exper-
iments. This is based on the assumption that more
difficult sentences will require more time to trans-
late. However, ?time taken to translate? may not
be strongly related to the translation difficulty for
two reasons. First, it is difficult to know what
fraction of the total translation time is actually
spent on the translation-related-thinking. For ex-
ample, translators may spend considerable amount
of time typing/writing translations, which is ir-
relevant to the translation difficulty. Second, the
translation time is sensitive to distractions from
the environment. So, instead of the ?time taken
to translate?, we are more interested in the ?time
for which translation related processing is carried
out by the brain?. This can be termed as the Trans-
lation Processing Time (Tp). Mathematically,
Tp = Tp comp + Tp gen (1)
Where Tp comp and Tp gen are the processing times
for source text comprehension and target text gen-
eration respectively. The empirical TDI, is com-
puted by normalizing Tp with sentence length.
TDI = Tpsentencelength (2)
Measuring Tp is a difficult task as translators of-
ten switch between thinking and writing activities.
Here comes the role of eye tracking.
3 Measuring Tp by eye-tracking
We measure Tp by analyzing the gaze behavior
of translators through eye-tracking. The rationale
behind using eye-tracking is that, humans spend
time on what they see, and this ?time? is corre-
lated with the complexity of the information being
processed, as shown in Figure 1. Two fundamental
components of eye behavior are (a) Gaze-fixation
or simply, Fixation and (b) Saccade. The former
is a long stay of the visual gaze on a single loca-
tion. The latter is a very rapid movement of the
eyes between positions of rest. An intuitive feel
for these two concepts can be had by consider-
ing the example of translating the sentence The
camera-man shot the policeman with a gun men-
tioned in the introduction. It is conceivable that
the eye will linger long on the word ?shot? which
is ambiguous and will rapidly move across ?shot?,
?camera-man? and ?gun? to ascertain the clue for
disambiguation.
The terms Tp comp and Tp gen in (1) can now be
looked upon as the sum of fixation and saccadic
durations for both source and target sentences re-
spectively.
Modifying 1
Tp =
?
f?Fs
dur(f) +
?
s?Ss
dur(s)
+
?
f?Ft
dur(f) +
?
s?St
dur(s)
(3)
347
Figure 2: Screenshot of Translog. The circles rep-
resent fixations and arrow represent saccades.
Here, Fs and Ss correspond to sets of fixations and
saccades for source sentence and Ft and St corre-
spond to those for the target sentence respectively.
dur is a function returning the duration of fixations
and saccades.
3.1 Computing TDI using eye-tracking
database
We obtained TDIs for a set of sentences from
the Translation Process Research Database (TPR
1.0)(Carl, 2012). The database contains trans-
lation studies for which gaze data is recorded
through the Translog software1(Carl, 2012). Fig-
ure 2 presents a screendump of Translog. Out of
the 57 available sessions, we selected 40 transla-
tion sessions comprising 80 sentence translations2.
Each of these 80 sentences was translated from
English to three different languages, viz. Span-
ish, Danish and Hindi by at least 2 translators.
The translators were young professional linguists
or students pursuing PhD in linguistics.
The eye-tracking data is noisy and often ex-
hibits systematic errors (Hornof and Halverson,
2002). To correct this, we applied automatic er-
ror correction technique (Mishra et al, 2012) fol-
lowed by manually correcting incorrect gaze-to-
word mapping using Translog. Note that, gaze and
saccadic durations may also depend on the transla-
tor?s reading speed. We tried to rule out this effect
by sampling out translations for which the vari-
ance in participant?s reading speed is minimum.
Variance in reading speed was calculated after tak-
ing a samples of source text for each participant
and measuring the time taken to read the text.
After preprocessing the data, TDI was com-
puted for each sentence by using (2) and (3).The
observed unnormalized TDI score3 ranges from
0.12 to 0.86. We normalize this to a [0,1] scale
1http://www.translog.dk
220% of the translation sessions were discarded as it was
difficult to rectify the gaze logs for these sessions.
3Anything beyond the upper bound is hard to translate and
can be assigned with the maximum score.
Figure 3: Dependency graph used for computing
SC
using MinMax normalization.
If the ?time taken to translate? and Tp were
strongly correlated, we would have rather opted
?time taken to translate? for the measurement of
TDI. The reason is that ?time taken to translate?
is relatively easy to compute and does not require
expensive setup for conducting ?eye-tracking? ex-
periments. But our experiments show that there
is a weak correlation (coefficient = 0.12) between
?time taken to translate? and Tp. This makes us
believe that Tp is still the best option for TDI mea-
surement.
4 Relating TDI to sentence features
Our claim is that translation difficulty is mainly
caused by three features: Length, Degree of Poly-
semy and Structural Complexity.
4.1 Length
It is the total number of words occurring in a sen-
tence.
4.2 Degree of Polysemy (DP)
The degree of polysemy of a sentence is the sum of
senses possessed by each word in the Wordnet nor-
malized by the sentence length. Mathematically,
DPsentence =
?
w?W Senses(w)
length(sentence) (4)
Here, Senses(w) retrieves the total number senses
of a word P from the Wordnet. W is the set of
words appearing in the sentence.
4.3 Structural Complexity (SC)
Syntactically, words, phrases and clauses are at-
tached to each other in a sentence. If the attach-
ment units lie far from each other, the sentence
has higher structural complexity. Lin (1996) de-
fines it as the total length of dependency links in
the dependency structure of the sentence.
348
Figure 4: Prediction of TDI using linguistic prop-
erties such as Length(L), Degree of Polysemy
(DP) and Structural Complexity (SC)
Example: The man who the boy attacked
escaped.
Figure 3 shows the dependency graph for the
example sentence. The weights of the edges cor-
respond how far the two connected words lie from
each other in the sentence. Using Lin?s formula,
the SC score for the example sentence turns out to
be 15.
Lin?s way of computing SC is affected by sen-
tence length since the number of dependency links
for a sentence depends on its length. So we nor-
malize SC by the length of the sentence. After
normalization, the SC score for the example given
becomes 15/7 = 2.14
4.4 How are TDI and linguistic features
related
To validate that translation difficulty depends on
the above mentioned linguistic features, we tried
to find out the correlation coefficients between
each feature and empirical TDI. We extracted
three sets of sample sentences. For each sample,
sentence selection was done with a view to vary-
ing one feature, keeping the other two constant.
The Correlation Coefficients between L, DP and
SC and the empirical TDI turned out to be 0.72,
0.41 and 0.63 respectively. These positive correla-
tion coefficients indicate that all the features con-
tribute to the translation difficulty.
5 Predicting TDI
Our system predicts TDI from the linguistic prop-
erties of a sentence as shown in Figure 4.
The prediction happens in a supervised setting
through regression. Training such a system re-
quires a set sentences annotated with TDIs. In
our case, direct annotation of TDI is a difficult and
unintuitive task. So, we annotate TDI by observ-
Kernel(C=3.0) MSE (%) Correlation
Linear 20.64 0.69
Poly (Deg 2) 12.88 0.81
Poly (Deg 3) 13.35 0.78
Rbf (default) 13.32 0.73
Table 1: Relative MSE and Correlation with ob-
served data for different kernels used for SVR.
ing translator?s behavior (using equations (1) and
(2))instead of asking people to rate sentences with
TDI.
We are now prepared to give the regression sce-
nario for predicting TDI.
5.1 Preparing the dataset
Our dataset contains 80 sentences for which TDI
have been measured (Section 3.1). We divided this
data into 10 sets of training and testing datasets in
order to carry out a 10-fold evaluation. DP and SC
features were computed using Princeton Wordnet4
and Stanford Dependence Parser5.
5.2 Applying Support Vector Regression
To predict TDI, Support Vector Regression (SVR)
technique (Joachims et al, 1999) was preferred
since it facilitates multiple kernel-based methods
for regression. We tried using different kernels us-
ing default parameters. Error analysis was done
by means of Mean Squared Error estimate (MSE).
We also measured the Pearson correlation coeffi-
cient between the empirical and predicted TDI for
our test-sets.
Table 1 indicates Mean Square Error percent-
ages for different kernel methods used for SVR.
MSE (%) indicates by what percentage the pre-
dicted TDIs differ from the observed TDIs. In our
setting, quadratic polynomial kernel with c=3.0
outperforms other kernels. The predicted TDIs are
well correlated with the empirical TDIs. This tells
us that even if the predicted scores are not as ac-
curate as desired, the system is capable of ranking
sentences in correct order. Table 2 presents exam-
ples from the test dataset for which the observed
TDI (TDIO) and the TDI predicted by polynomial
kernel based SVR (TDIP ) are shown.
Our larger goal is to group unknown sentences
into different categories by the level of transla-
4http://www.wordnet.princeton.edu
5http://www.nlp.stanford.edu/software/
lex-parser.html
349
Example L DP SC TDIO TDIP Error
1. American Express recently
announced a second round
of job cuts. 10 10 1.8 0.24 0.23 4%
2. Sociology is a relatively
new academic discipline. 7 6 3.7 0.49 0.53 8%
Table 2: Example sentences from the test dataset.
tion difficulty. For that, we tried to manually as-
sign three different class labels to sentences viz.
easy, medium and hard based on the empirical
TDI scores. The ranges of scores chosen for easy,
medium and hard categories were [0-0.3], [0.3-
0.75] and [0.75-1.0] respectively (by trial and er-
ror). Then we trained a Support Vector Rank
(Joachims, 2006) with default parameters using
different kernel methods. The ranking framework
achieves a maximum 67.5% accuracy on the test
data. The accuracy should increase by adding
more data to the training dataset.
6 Conclusion
This paper introduces an approach to quantify-
ing translation difficulty and automatically assign-
ing difficulty levels to unseen sentences. It estab-
lishes a relationship between the intrinsic senten-
tial properties, viz., length (L), degree of polysemy
(DP) and structural complexity (SC), on one hand
and the Translation Difficulty Index (TDI), on the
other. Future work includes deeper investigation
into other linguistic factors such as presence of do-
main specific terms, target language properties etc.
and applying more sophisticated cognitive analy-
sis techniques for more reliable TDI score. We
would like to make use of inter-annotator agree-
ment to decide the boundaries for the translation
difficulty categories. Extending the study to differ-
ent language pairs and studying the applicability
of this technique for Machine Translation Quality
Estimation are also on the agenda.
Acknowledgments
We would like to thank the CRITT, CBS group for
their help in manual correction of TPR data. In
particular, thanks to Barto Mesa and Khristina for
helping with Spanish and Danish dataset correc-
tions.
References
Campbell, S., and Hale, S. 1999. What makes a text
difficult to translate? Refereed Proceedings of the
23rd Annual ALAA Congress.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation (ELRA)
Carl, M. 2012 The CRITT TPR-DB 1.0: A Database
for Empirical Human Translation Process Research.
AMTA 2012 Workshop on Post-Editing Technology
and Practice (WPTP-2012).
Chall, J. S., and Dale, E. 1995. Readability revisited:
the new Dale-Chall readability formula Cambridge,
Mass.: Brookline Books.
Dragsted, B. 2010. Co-ordination of reading andwrit-
ing processes in translation. Contribution to Trans-
lation and Cognition, Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Fry, E. 1977 Fry?s readability graph: Clarification,
validity, and extension to level 17 Journal of Read-
ing, 21(3), 242-252.
Hornof, A. J. and Halverson, T. 2002 Cleaning up sys-
tematic error in eye-tracking data by using required
fixation locations. Behavior Research Methods, In-
struments, and Computers, 34, 592604.
Joachims, T., Schlkopf, B. ,Burges, C and A. Smola
(ed.). 1999. Making large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support
Vector Learning. MIT-Press, 1999,
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Kincaid, J. P., Fishburne, R. P., Jr., Rogers, R. L., and
Chissom, B. S. 1975. Derivation of New Read-
ability Formulas (Automated Readability Index, Fog
Count and Flesch Reading Ease Formula) for Navy
Enlisted Personnel Millington, Tennessee: Naval
Air Station Memphis,pp. 8-75.
350
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mishra, A., Carl, M, Bhattacharyya, P. 2012 A
heuristic-based approach for systematic error cor-
rection of gaze datafor reading. In MichaelCarl, P.B.
and Choudhary, K.K., editors, Proceedings of the
First Workshop on Eye-tracking and Natural Lan-
guage Processing, Mumbai, India. The COLING
2012 Organizing Committee
von der Malsburg, T., Vasishth, S., and Kliegl, R. 2012
Scanpaths in reading are informative about sen-
tence processing. In MichaelCarl, P.B. and Choud-
hary, K.K., editors, Proceedings of the First Work-
shop on Eye-tracking and Natural Language Pro-
cessing, Mumbai, India. The COLING 2012 Orga-
nizing Committee
351
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 538?542,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Neighbors Help: Bilingual Unsupervised WSD Using Context
Sudha Bhingardive Samiulla Shaikh Pushpak Bhattacharyya
Department of Computer Science and Engineering,
IIT Bombay, Powai,
Mumbai, 400076.
{sudha,samiulla,pb}@cse.iitb.ac.in
Abstract
Word Sense Disambiguation (WSD) is one
of the toughest problems in NLP, and in
WSD, verb disambiguation has proved to
be extremely difficult, because of high de-
gree of polysemy, too fine grained senses,
absence of deep verb hierarchy and low in-
ter annotator agreement in verb sense an-
notation. Unsupervised WSD has received
widespread attention, but has performed
poorly, specially on verbs. Recently an
unsupervised bilingual EM based algo-
rithm has been proposed, which makes
use only of the raw counts of the transla-
tions in comparable corpora (Marathi and
Hindi). But the performance of this ap-
proach is poor on verbs with accuracy
level at 25-38%. We suggest a modifica-
tion to this mentioned formulation, using
context and semantic relatedness of neigh-
boring words. An improvement of 17% -
35% in the accuracy of verb WSD is ob-
tained compared to the existing EM based
approach. On a general note, the work
can be looked upon as contributing to the
framework of unsupervised WSD through
context aware expectation maximization.
1 Introduction
The importance of unsupervised approaches in
WSD is well known, because they do not need
sense tagged corpus. In multilingual unsuper-
vised scenario, either comparable or parallel cor-
pora have been used by past researchers for disam-
biguation (Dagan et al, 1991; Diab and Resnik,
2002; Kaji and Morimoto, 2002; Specia et al,
2005; Lefever and Hoste, 2010; Khapra et al,
2011). Recent work by Khapra et al, (2011) has
shown that, in comparable corpora, sense distribu-
tion of a word in one language can be estimated
using the raw counts of translations of the target
words in the other language; such sense distribu-
tions contribute to the ranking of senses. Since
translations can themselves be ambiguous, Expec-
tation Maximization based formulation is used to
determine the sense frequencies. Using this ap-
proach every instance of a word is tagged with the
most probable sense according to the algorithm.
In the above formulation, no importance is
given to the context. That would do, had the ac-
curacy of disambiguation on verbs not been poor
25-35%. This motivated us to propose and inves-
tigate use of context in the formulation by Khapra
et al (2011).
For example consider the sentence in chem-
istry domain, ?Keep the beaker on the flat table.?
In this sentence, the target word ?table? will be
tagged as ?the tabular array? sense since it is dom-
inant in the chemistry domain by their algorithm.
But its actual sense is ?a piece of furniture? which
can be captured only if context is taken into con-
sideration. In our approach we tackle this problem
by taking into account the words from the context
of the target word. We use semantic relatedness
between translations of the target word and those
of its context words to determine its sense.
Verb disambiguation has proved to be extremely
difficult (Jean, 2004), because of high degree of
polysemy (Khapra et al, 2010), too fine grained
senses, absence of deep verb hierarchy and low in-
ter annotator agreement in verb sense annotation.
On the other hand, verb disambiguation is very
important for NLP applications like MT and IR.
Our approach has shown significant improvement
in verb accuracy as compared to Khapra?s (2011)
approach.
The roadmap of the paper is as follows. Sec-
tion 2 presents related work. Section 3 covers the
background work. Section 4 explains the modified
EM formulation using context and semantic relat-
edness. Section 5 presents the experimental setup.
538
Results are presented in section 6. Section 7 cov-
ers phenomena study and error analysis. Conclu-
sions and future work are given in the last section,
section 8.
2 Related work
Word Sense Disambiguation is one of the hard-
est problems in NLP. Successful supervised WSD
approaches (Lee et al, 2004; Ng and Lee, 1996)
are restricted to resource rich languages and do-
mains. They are directly dependent on availabil-
ity of good amount of sense tagged data. Creat-
ing such a costly resource for all language-domain
pairs is impracticable looking at the amount of
time and money required. Hence, unsupervised
WSD approaches (Diab and Resnik, 2002; Kaji
and Morimoto, 2002; Mihalcea et al, 2004; Jean,
2004; Khapra et al, 2011) attract most of the re-
searchers.
3 Background
Khapra et al (2011) dealt with bilingual unsuper-
vised WSD. It uses EM algorithm for estimating
sense distributions in comparable corpora. Ev-
ery polysemous word is disambiguated using the
raw counts of its translations in different senses.
Synset algned multilingual dictionary (Mohanty
et al, 2008) is used for finding its translations.
In this dictionary, synsets are linked, and after
that the words inside the synsets are also linked.
For example, for the concept of ?boy?, the Hindi
synset {ladakaa, balak, bachhaa} is linked with
the Marathi synset {mulagaa, poragaa, por}. The
Marathi word ?mulagaa? is linked to the Hindi
word ?ladakaa? which is its exact lexical substi-
tution.
Suppose words u in language L1 and v in lan-
guage L2 are translations of each other and their
senses are required. The EM based formulation is
as follows:
E-Step:
P (SL1 |u) =
?
v
P (piL2 (S
L1)|v) ? #(v)
?
SL1i
?
x
P (piL2 (S
L1
i )|x) ? #(x)
where, SL1i ? synsetsL1 (u)
v ? crosslinksL2 (u, S
L1)
x ? crosslinksL2 (u, S
L1
i )
M-Step:
P (SL2 |v) =
?
u
P (piL1 (S
L2)|u) ? #(u)
?
SL2i
?
y
P (piL1 (S
L2
i )|y) ? #(y)
where, SL2i ? synsetsL2 (v)
u ? crosslinksL1 (v, S
L2)
y ? crosslinksL1 (v, S
L2
i )
Here,
? ?#? indicates the raw count.
? crosslinksL1 (a, S
L2) is the set of possible
translations of the word ?a? from language L1
to L2 in the sense SL2 .
? piL2 (S
L1) means the linked synset of the
sense SL1 in L2.
E and M steps are symmetric except for the
change in language. In both the steps, we esti-
mate sense distribution in one language using raw
counts of translations in another language. But
this approach has following limitations:
Poor performance on verbs: This approach gives
poor performance on verbs (25%-38%). See sec-
tion 6.
Same sense throughout the corpus: Every oc-
currence of a word is tagged with the single sense
found by the algorithm, throughout the corpus.
Closed loop of translations: This formulation
does not work for some common words which
have the same translations in all senses. For ex-
ample, the verb ?karna? in Hindi has two differ-
ent senses in the corpus viz., ?to do? (S1) and ?to
make? (S2). In both these senses, it gets trans-
lated as ?karne? in Marathi. The word ?karne? also
back translates to ?karna? in Hindi through both its
senses. In this case, the formulation works out as
follows:
The probabilities are initialized uniformly.
Hence, P (S1|karna) = P (S2|karna) = 0.5.
Now, in first iteration the sense of ?karne? will be
estimated as follows (E-step):
P (S1|karne) =
P (S1|karna) ? #(karna)
#(karna)
= 0.5,
539
P (S2|karne) =
P (S2|karna) ? #(karna)
#(karna)
= 0.5
Similarly, in M-step, we will get P (S1|karna) =
P (S2|karna) = 0.5. Eventually, it will end up
with initial probabilities and no strong decision
can be made.
To address these problems we have introduced
contextual clues in their formulation by using se-
mantic relatedness.
4 Modified Bilingual EM approach
We introduce context in the EM formulation stated
above and treat the context as a bag of words. We
assume that each word in the context influences
the sense of the target word independently. Hence,
p(S|w,C) =
?
ci?C
p(S|w, ci)
where, w is the target word, S is one of the candi-
date synsets of w, C is the set of words in context
(sentence in our case) and ci is one of the context
words.
Suppose we would have sense tagged data,
p(S|w, c) could have been computed as:
p(S|w, c) = #(S,w, c)#(w, c)
But since the sense tagged corpus is not avail-
able, we cannot find #(S,w, c) from the corpus
directly. However, we can estimate it using the
comparable corpus in other language. Here, we
assume that given a word and its context word
in language L1, the sense distribution in L1 will
be same as that in L2 given the translation of a
word and the translation of its context word in L2.
But these translations can be ambiguous, hence
we can use Expectation Maximization approach
similar to (Khapra et al, 2011) as follows:
E-Step:
P (SL1 |u, a) =
?
v,b
P (piL2 (S
L1)|v, b) ? ?(v, b)
?
SL1i
?
x,b
P (piL2 (S
L1
i )|x, b) ? ?(x, b)
where, SL1i ? synsetsL1(u)
a ? context(u)
v ? crosslinksL2 (u, S
L1)
b ? crosslinksL2 (a)
x ? crosslinksL2 (u, S
L1
i )
crosslinksL1(a) is the set of all possible transla-
tions of the word ?a? from L1 to L2 in all its senses.
?(v, b) is the semantic relatedness between the
senses of v and senses of b. Since, v and b go over
all possible translations of u and a respectively.
?(v, b) has the effect of indirectly capturing the
semantic similarity between the senses of u and
a. A symetric formulation in the M-step below
takes the computation back from language L2 to
language L1. The semantic relatedness comes as
an additional weighing factor, capturing context,
in the probablistic score.
M-Step:
P (SL2 |v, b) =
?
u,a
P (piL1 (S
L2)|u, a) ? ?(u, a)
?
SL2i
?
y,b
P (piL1 (S
L2
i )|y, a) ? ?(y, a)
where, SL2i ? synsetsL2 (v)
b ? context(v)
u ? crosslinksL1 (v, S
L2)
a ? crosslinksL1(b)
y ? crosslinksL1 (v, S
L2
i )
?(u, a) is the semantic relatedness between the
senses of u and senses of a and contributes to the
score like ?(v, b).
Note how the computation moves back and
forth between L1 and L2 considering translations
of both target words and their context words.
In the above formulation, we could have con-
sidered the term #(word, context word) (i.e.,
the co-occurrence count of the translations of
the word and the context word) instead of
?(word, context word). But it is very unlikely
that every translation of a word will co-occur with
540
Algorithm HIN-HEALTH MAR-HEALTH
NOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB Overall
EM-C 59.82 67.80 56.66 60.38 59.63 62.90 62.54 53.63 52.49 59.77
EM 60.68 67.48 55.54 25.29 58.16 63.88 58.88 55.71 35.60 58.03
WFS 53.49 73.24 55.16 38.64 54.46 59.35 67.32 38.12 34.91 52.57
RB 32.52 45.08 35.42 17.93 33.31 33.83 38.76 37.68 18.49 32.45
Table 1: Comparison(F-Score) of EM-C and EM for Health domain
Algorithm HIN-TOURISM MAR-TOURISM
NOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB Overall
EM-C 62.78 65.10 54.67 55.24 60.70 59.08 63.66 58.02 55.23 58.67
EM 61.16 62.31 56.02 31.85 57.92 59.66 62.15 58.42 38.33 56.90
WFS 63.98 75.94 52.72 36.29 60.22 61.95 62.39 48.29 46.56 57.47
RB 32.46 42.56 36.35 18.29 32.68 33.93 39.30 37.49 15.99 32.65
Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
every translation of its context word considerable
number of times. This term may make sense only
if we have arbitrarily large comparable corpus in
the other language.
4.1 Computation of semantic relatedness
The semantic relatedness is computed by taking
the inverse of the length of the shortest path among
two senses in the wordnet graph (Pedersen et al,
2005). All the semantic relations (including cross-
part-of-speech links) viz., hypernymy, hyponymy,
meronymy, entailment, attribute etc., are used for
computing the semantic relatedness.
Sense scores thus obtained are used to disam-
biguate all words in the corpus. We consider all
the content words from the context for disam-
biguation of a word. The winner sense is the one
with the highest probability.
5 Experimental setup
We have used freely available in-domain compa-
rable corpora1 in Hindi and Marathi languages.
These corpora are available for health and tourism
domains. The dataset is same as that used in
(Khapra et al, 2011) in order to compare the per-
formance.
6 Results
Table 1 and Table 2 compare the performance of
the following two approaches:
1. EM-C (EM with Context): Our modified ap-
proach explained in section 4.
2. EM: Basic EM based approach by Khapra et
al., (2011).
1http://www.cfilt.iitb.ac.in/wsd/annotated corpus/
3. WFS: Wordnet First Sense baseline.
4. RB: Random baseline.
Results clearly show that EM-C outperforms EM
especially in case of verbs in all language-domain
pairs. In health domain, verb accuracy is increased
by 35% for Hindi and 17% for Marathi, while in
tourism domain, it is increased by 23% for Hindi
and 17% for Marathi. The overall accuracy is in-
creased by (1.8-2.8%) for health domain and (1.5-
1.7%) for tourism domain. Since there are less
number of verbs, the improved accuracy is not di-
rectly reflected in the overall performance.
7 Error analysis and phenomena study
Our approach tags all the instances of a word de-
pending on its context as apposed to basic EM ap-
proach. For example, consider the following sen-
tence from the tourism domain:
vh p? ?l rh T?
(vaha patte khel rahe the)
(They were playing cards/leaves)
Here, the word p? (plural form of p?A) has two
senses viz., ?leaf? and ?playing card?. In tourism
domain, the ?leaf? sense is more dominant. Hence,
basic EM will tag p? with ?leaf? sense. But it?s
true sense is ?playing card?. The true sense is cap-
tured only if context is considered. Here, the word
?lnA (to play) (root form of ?l) endorses the
?playing card? sense of the word p?A. This phe-
nomenon is captured by our approach through se-
mantic relatedness.
But there are certain cases where our algorithm
fails. For example, consider the following sen-
tence:
541
vh pX ? Enc p? ?l rh T?
(vaha ped ke niche patte khel rahe the)
(They were playing cards/leaves below the tree)
Here, two strong context words pX (tree) and
?l (play) are influencing the sense of the word
p?. Semantic relatedness between pX (tree) and
p?A (leaf) is more than that of ?l (play) and p?A
(playing card). Hence, the ?leaf sense? is assigned
to p?A.
This problem occurred because we considered
the context as a bag of words. This problem can
be solved by considering the semantic structure
of the sentence. In this example, the word p?A
(leaf/playing card) is the subject of the verb ?lnA
(to play) while pX (tree) is not even in the same
clause with p?A (leaf/playing cards). Thus we
could consider ?lnA (to play) as the stronger clue
for its disambiguation.
8 Conclusion and Future Work
We have presented a context aware EM formula-
tion building on the framework of Khapra et al
(2011). Our formulation solves the problems of
?inhibited progress due to lack of translation diver-
sity? and ?uniform sense assignment, irrespective
of context? that the previous EM based formula-
tion of Khapra et al suffers from. More impor-
tantly our accuracy on verbs is much higher and
more than the state of the art, to the best of our
knowledge. Improving the performance on other
parts of speech is the primary future work. Fu-
ture directions also point to usage of semantic role
clues, investigation of familialy apart pair of lan-
guages and effect of variation of measures of se-
mantic relatedness.
References
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Dou-
glas E. Appelt, editor, ACL, pages 130?137. ACL.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 255?262, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Ve?ronis Jean. 2004. Hyperlex: Lexical cartography
for information retrieval. In Computer Speech and
Language, pages 18(3):223?252.
Hiroyuki Kaji and Yasutsugu Morimoto. 2002. Unsu-
pervised word sense disambiguation using bilingual
comparable corpora. In Proceedings of the 19th in-
ternational conference on Computational linguistics
- Volume 1, COLING ?02, pages 1?7, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney,
and Pushpak Bhattacharyya. 2010. All words do-
main adapted wsd: Finding a middle ground be-
tween supervision and unsupervision. In Jan Ha-
jic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 1532?1541. The Association for Com-
puter Linguistics.
Mitesh M Khapra, Salil Joshi, and Pushpak Bhat-
tacharyya. 2011. It takes two to tango: A bilingual
unsupervised approach for estimating sense distribu-
tions using expectation maximization. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 695?704, Chiang
Mai, Thailand, November. Asian Federation of Nat-
ural Language Processing.
K. Yoong Lee, Hwee T. Ng, and Tee K. Chia. 2004.
Supervised word sense disambiguation with support
vector machines and multiple knowledge sources.
In Proceedings of Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 137?140.
Els Lefever and Veronique Hoste. 2010. Semeval-
2010 task 3: cross-lingual word sense disambigua-
tion. In Katrin Erk and Carlo Strapparava, editors,
SemEval 2010 : 5th International workshop on Se-
mantic Evaluation : proceedings of the workshop,
pages 15?20. ACL.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application to
word sense disambiguation. In COLING.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dic-
tionary: Insights, applications and challenges. In
Global Wordnet Conference.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40?47, Morristown,
NJ, USA. ACL.
T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.
Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI
2005/25, University of Minnesota Supercomputing
Institute, March.
Lucia Specia, Maria Das Grac?as, Volpe Nunes, and
Mark Stevenson. 2005. Exploiting parallel texts to
produce a multilingual sense tagged corpus for word
sense disambiguation. In In Proceedings of RANLP-
05, Borovets, pages 525?531.
542
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 860?865,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
 
 
Detecting Turnarounds in Sentiment Analysis: Thwarting 
 
  
  
Abstract 
Thwarting and sarcasm are two uncharted 
territories in sentiment analysis, the for-
mer because of the lack of training corpo-
ra and the latter because of the enormous 
amount of world knowledge it demands. 
In this paper, we propose a working defi-
nition of thwarting amenable to machine 
learning and create a system that detects if 
the document is thwarted or not. We focus 
on identifying thwarting in product re-
views, especially in the camera domain. 
An ontology of the camera domain is cre-
ated. Thwarting is looked upon as the 
phenomenon of polarity reversal at a 
higher level of ontology compared to the 
polarity expressed at the lower level.   
This notion of thwarting defined with re-
spect to an ontology is novel, to the best 
of our knowledge. A rule based imple-
mentation building upon this idea forms 
our baseline. We show that machine learn-
ing with annotated corpora (thwarted/non-
thwarted) is more effective than the rule 
based system. Because of the skewed dis-
tribution of thwarting, we adopt the Area-
under-the-Curve measure of performance. 
To the best of our knowledge, this is the 
first attempt at the difficult problem of 
thwarting detection, which we hope will at 
least provide a baseline system to compare 
against. 
1 Credits 
The authors thank the lexicographers at Center 
for Indian Language Technology (CFILT) at IIT 
Bombay for their support for this work. 
2 Introduction 
Although much research has been done in the 
field of sentiment analysis (Liu et al, 2012), 
thwarting and sarcasm are not addressed, to the 
best of our knowledge. Thwarting has been iden-
tified as a common phenomenon in sentiment 
analysis (Pang et al, 2002, Ohana et al, 2009, 
Brooke, 2009) in various forms of texts but no 
previous work has proposed a solution to the 
problem of identifying thwarting. We focus on 
identifying thwarting in product reviews. 
The definition of an opinion as specified in 
Liu (2012) is  
?An opinion is a quintuple, (   ,     ,      , 
  ,   ), where    is the name of an entity,     is 
an aspect of   ,       is the sentiment on aspect 
    of entity   ,    is the opinion holder, and     
is the time when the opinion is expressed by   .? 
 
If the sentiment towards the entity or one of its 
important attribute contradicts the sentiment to-
wards all other attributes, we can say that the 
document is thwarted. 
Ankit Ramteke 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
ankitr@cse.iitb.ac.in 
 
Pushpak Bhattacharyya 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
pb@cse.iitb.ac.in 
 
Akshat Malu 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
akshatmalu@cse.iitb.ac.in 
 
J. Saketha Nath 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
saketh@cse.iitb.ac.in 
 
860
 
 
A domain ontology is an ontology of various 
features pertaining to a domain, arranged in a 
hierarchy. Subsumption in this hierarchy implies 
that the child is a part or feature of the parent. 
Domain ontology has been used by various 
works in NLP (Saggion et al, 2007 and Polpinij 
et al, 2008). In our work, we use domain ontol-
ogy of camera. We look upon thwarting as the 
phenomenon of reversal of polarity from the 
lower level of the ontology to the higher level. At 
the higher level of ontology the entities men-
tioned are the whole product or a large critical 
part of the product. So while statements about 
entities at the lower level of the ontology are on 
?details?, statements about entities at higher lev-
els are on the ?big picture?. Polarity reversal 
from details to the big picture is at the heart 
of thwarting. 
The motivation for our study on thwarting 
comes from the fact that: a) Thwarting is a chal-
lenging NLP problem and b) Special ML ma-
chinery is needed in view of the fact that the 
training data is so skewed. Additionally large 
amount of world and domain knowledge maybe 
called for to solve the problem. In spite of the 
relatively fewer occurrence of the thwarting phe-
nomenon the problem poses an intellectually 
stimulating exercise. We may also say that in the 
limit, thwarting approaches the very difficult 
problem of sarcasm detection (Tsur et al 2010). 
We start by defining and understanding the 
problem of thwarting in section 2. In section 3, 
we describe a method to create the domain on-
tology. In section 4, we propose a na?ve rule 
based approach to detect thwarting. In section 5 
we discuss a machine learning based approach 
which could be used to identify whether a docu-
ment is thwarted or not. This is followed by ex-
perimental results in section 6. Section 7 draws 
conclusions and points to future work. 
3 Definition 
Thwarting is defined by Pang et al, (2008) as 
follows:  
?Thwarted expectations basically refer to the 
phenomenon wherein the author of the text first 
builds up certain expectations for the topic, only 
to produce a deliberate contrast to the earlier 
discussion."       
 
For our computational purposes, we define 
thwarting as:  
?The phenomenon wherein the overall polarity of 
the document is in contrast with the polarity of 
majority of the document.? 
 
This definition emphasizes thwarting as piggy-
backing on sentiment analysis to improve the 
latter?s performance. The current work however 
only addresses the problem of whether a docu-
ment is thwarted or not and does not output the 
sentiment of the document. The basic block dia-
gram for our system is shown in figure 1. 
 
 
 
 
 
 
Figure 1: Basic Block Diagram 
 
An example of a thwarted document is: 
?I love the sleek design. The lens is impressive. 
The pictures look good but, somehow this cam-
era disappoints me. I do not recommend it.? 
 
While thwarting occurs in various forms of sen-
timent bearing texts, it is not a very frequent one. 
It accounts for hardly 1-2% of any given corpus. 
Thus, it becomes hard to find sufficient number 
of examples of thwarting to train a classifier.  
Since thwarting is a complex natural language 
phenomenon we require basic NLP tools and 
resources, whose accuracy in turn can affect the 
overall performance of a thwarting detection sys-
tem. 
4 Building domain ontology 
Domain ontology comprises of features and enti-
ties from the domain and the relationships be-
tween them. The process thus has two steps, viz. 
(a) identify the features and entities, and (b) con-
nect them in the form of a hierarchy. We decided 
to use a combination of review corpora mining 
and manual means for identifying key features. 
Our approach to building the domain ontology is 
as follows: 
Step 1: We use Latent Dirichlet Allocation 
(LDA) (Blei et al, 2003) on a corpus containing 
reviews of a particular product (camera, in our 
case) to identify key features from the domain. 
The output is then analyzed manually to finally 
select the key features. Some additional features 
get added by human annotator to increase the 
coverage of the ontology. For Example, in the 
camera domain, the corpus may include words 
Thwarting 
Detection 
System 
Input 
 Document 
Thwarted or 
 Not -Thwarted 
861
 
 
like memory, card, gb, etc. but, may not contain 
the word storage. The abstract concept of stor-
age is contributed by the human annotator 
through his/her world knowledge. 
Step 2: The features thus obtained are ar-
ranged in the form of a hierarchy by a human 
annotator. 
 
 
Figure 2: Ontology for the camera domain 
5 A rule based approach to thwarting 
recognition 
As per the definition of thwarting, most of the 
thwarted document carries a single sentiment; 
however, a small but critical portion of the text, 
carrying the contrary sentiment, actually decides 
the overall polarity. The critical statement, thus, 
should be strongly polar (either positive or nega-
tive), and it should be on some critical feature of 
the product. 
From the perspective of the domain ontology, the 
sentiment towards the overall product or towards 
some critical feature mentioned near the root of 
the ontology should be opposite to the sentiment 
towards features near the leaves. 
 
Based on these observations we propose the fol-
lowing na?ve approach to thwarting detection: 
 
For each sentence in a review to be tested 
   1. Get the dependency parse of the sentence. 
This step is essential. It makes explicit the adjec-
tive noun dependencies, which in turn uncovers 
the sentiment on a specific part or feature of the 
product. 
   2. Identify the polarities towards all nouns, us-
ing the dependency parse and sentiment lexicons.    
   3. If a domain feature, identified using the do-
main ontology, exists in the sentence, anno-
tate/update the ontology node, containing the 
feature, using the polarity obtained. 
Once the entire review is processed, we obtain 
the domain ontology, with polarity marking on 
nodes, for the corresponding review. 
The given review is thwarted if there is a con-
tradiction of sentiment among different levels of 
the domain ontology with polarity marking on 
nodes. 
The sentiment lexicons used are SentiWord-
Net (Esuli et al, 2006), Taboada (Taboada et al, 
2004), BL lexicon (Hu et al, 2004) and Inquirer 
(Stone et al, 1966). 
The procedure is illustrated by an example.  
?I love the sleek design. The lens is impressive. 
The pictures look good but, somehow this cam-
era disappoints me. I do not recommend it.? 
 
A part of the ontology, with polarity marking on 
nodes, for this example is shown in figure 3. 
 
Figure 3: ontology with polarity marking on nodes: 
example 
Based on this ontology we see that there is an 
opposition of sentiment between the root (?cam-
era?) and the lower nodes. We thus determine 
that this document is thwarted. 
However, since the nodes, within the same 
level, might have different weighting based upon 
the product under consideration, this method 
fails to perform well. For example, the body and 
video capability might be subjective whereas any 
fault in the lens or the battery will render the 
camera useless, hence they are more critical. We 
thus see a need for relative weighting among all 
features in the ontology. 
Camera - 
negative 
Lens  - 
positive 
Body 
Design - 
positive 
Display 
Picture - 
positive 
862
 
 
6 A Machine Learning based approach 
Manual fixing of relative weightages for the fea-
tures of the product is possible, but that would be 
ad hoc. We now propose a machine learning 
based approach to detect thwarting in documents. 
It uses the domain ontology to identify key fea-
tures related to the domain. The approach in-
volves two major steps namely learning the 
weights and building a model that classifies the 
reviews using the learnt weights. 
6.1  Learning Weights 
The weights are learnt using the loss-
regularization framework. The key idea is that 
the overall polarity of the document is deter-
mined by the polarities of individual words in the 
document. Since, we need to find the weights for 
the nodes in the domain ontology; we consider 
only the words belonging to the ontology for fur-
ther processing. Thus, if P is the polarity of the 
review and    is the polarity associated with 
word i then   ?        gives the linear model. 
The word i should belong to the ontology as well 
as the review. Similarly, the hinge loss is given 
by               where w is the weight 
vector and x is the feature vector consisting of   
    .  
Based on the intuition, that every word con-
tributes some polarity to its parent node in the 
domain ontology, we also learnt weights on the 
ontology by percolating polarities towards the 
root. We experimented with complete percola-
tion, wherein the polarity at a node is its polarity 
in the document summed with the polarities of 
all its descendants. We also define controlled 
percolation, wherein the value added for a par-
ticular descendant is a function of its distance 
from the node. We halved the polarity value per-
colated, for each edge between the two nodes. 
Thus, for the example in figure 2, the polarity 
value of camera would be 
                  
     
 
 
     
 
  
        
 
  
       
 
  
        
 
 
Where         is the final polarity for camera 
and       is the polarity of the word ? {camera, 
body, display, design, picture}.  
6.2 Classifier 
We use the SVM classifier with features generat-
ed using the following steps. We first create a 
vector of weighted polarity values for each re-
view. This is constructed by generating a value 
for each word in the domain ontology encoun-
tered while reading the review sequentially. The 
value is calculated by multiplying the weight, 
found in the previous step (5.1), with the polarity 
of the word as determined from the sentence. 
Since, these vectors will be of different dimen-
sionality for each review, we extract features 
from these reviews. These features are selected 
based on our understanding of the problem and 
the fact that thwarting is a function of the change 
of polarity values and also the position of 
change. 
The Features extracted are: 
Document polarity, number of flips of sign (i.e. 
change of polarity from positive to negative and 
vice versa), the maximum and minimum values 
in a sequence, the length of the longest contigu-
ous subsequence of positive values (LCSP), the 
length of the longest contiguous subsequence of 
negative values (LCSN), the mean of all values, 
total number of positive values in the sequence, 
total number of negative values in the sequence, 
the first and the last value in the sequence, the 
variance of the moving averages, the difference 
in the means of LCSP and LCSN. 
7 Results 
Experiments were performed on a dataset ob-
tained by crawling product reviews from Ama-
zon1 . We focused on the camera domain. We 
obtained 1196 reviews from this domain. The 
reviews were annotated for thwarting, i.e., 
thwarted or non-thwarted as well as polarity. The 
reviews crawled were given to three different 
annotators. The instructions given for annotation 
were as follows: 
1. Read the entire review and try to form a 
mental picture of how sentiment in the 
document is distributed. Ignore anything 
that is not the opinion of the writer. 
2. Try to determine the overall polarity of 
the document. The star rating of the doc-
ument can be used for this purpose. 
3. If the overall polarity of the document is 
negative but, most of the words in the 
document indicate positive sentiment, or 
vice versa, then consider the document 
as thwarted. 
Since, identifying thwarting is a difficult task 
even for humans, we calculated the Cohen?s 
kappa score (Cohen 1960) in order to determine 
the inter annotator agreement. It was found out to 
                                                 
1Reviews crawled from http://www.amazon.com/ 
863
 
 
be 0.7317. The annotators showed high agree-
ment (98%) in the non-thwarted class whereas 
they agreed on 70% of the thwarted documents. 
Out of the 1196 reviews, exactly 21 were 
thwarted documents, agreed upon by all annota-
tors. We used the Stanford Core NLP tools 2 
(Klein et al, 2003, Toutanova et al, 2003) for 
basic NL processing. The system was tested on 
the entire dataset.  
Since, the data is highly skewed; we used Area 
under the Curve (AUC) for the ROC curve as the 
measure of evaluation (Ling et al, 2003). The 
AUC for a random baseline is expected to be 
50%, and the rule based approach is close to the 
baseline (56.3%). 
Table 1 shows the results for the experiments 
with the machine learning model. We used the 
CVX3 library in Matlab to solve the optimization 
problem for learning weights and the LIBSVM4 
library to implement the svm classifier. In order 
to account for the data skew, we assign a class 
weight of 50 (determined empirically) to the 
thwarted instances and 1 for non-thwarted in-
stances in the classifier. All results were obtained 
using a 10 fold cross validation. The same da-
taset was used for this set of experiments. 
 
Loss type 
for 
weights 
Percolation 
type for 
weights 
AUC value for 
classification 
Linear Complete 73% 
 Controlled 81% 
Hinge Complete 70% 
 Controlled 76% 
 
Table 1: Results of the machine learning based  
approach to thwarting detection 
 
We see that the overall system for identification 
of thwarting performs well for the weights ob-
tained using the linear model with a controlled 
percolation of polarity values in the ontology. 
The system outperforms both the random base-
line as well as the rule based system. These re-
sults though great are to be taken with a pinch of 
salt. The basic objective for creating a thwarting 
detection system was to include such a module in 
the general sentiment analysis framework. Thus, 
using document polarity as a feature contradicts 
the objective of sentiment analysis, which is to 
find the document polarity. Without the docu-
                                                 
2http://nlp.stanford.edu/software/corenlp.shtml  
3http://cvxr.com/cvx 
4http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
ment polarity feature, the values drop by 10% 
which is not acceptable. 
8 Conclusions and Future Work 
We have described a system for detecting thwart-
ing, based on polarity reversal between opinion 
on most parts of the product and opinion on the 
overall product or a critical part of the product. 
The parts of the product are related to one anoth-
er through an ontology. This ontology guides a 
rule based approach to thwarting detection, and 
also provides features for an SVM based learning 
system.  The ML based system scores over the 
rule based system. Future work consists in trying 
out the approach across products and across do-
mains, doing better ontology harnessing from the 
reviews and investing and searching for distribu-
tions and learning algorithms more suitable for 
the problem. 
References  
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent   
Dirichlet alocation. In the Journal of machine 
Learning research, 3, pages 993-1022. 
Brooke, J. 2009. A Semantic Approach to Automated 
Text Sentiment Analysis. Ph.D. thesis, Simon Fra-
ser University. 
Chang, C. C., and Lin, C. J. 2011. LIBSVM: a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology (TIST),2(3), 
27. 
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales.  Educational and psychological meas-
urement 20, no. 1, pages 37-46. 
Esuli, A. and Sebastiani, F. 2006. Sentiwordnet: A 
publicly available lexical resource for opinion min-
ing. In Proceedings of LREC, Volume 6, pages 
417-422. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the tenth 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pages 168-
177. ACM. 
Klein, D. and Manning, C. D. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Meeting 
of the Association for Computational Linguistics, 
pages 423-430. 
Ling, C. X., Huang, J. and Zhang, H.2003. AUC: A 
better measure than accuracy in comparing learn-
ing algorithms. In Advances in Artificial Intelli-
gence, pages 329-341, Springer Berlin Heidelberg. 
864
 
 
Liu, B., and Zhang, L. 2012. A survey of opinion 
mining and sentiment analysis. In Mining Text Da-
ta (pp. 415-463).Springer US. 
Liu B., 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1), 1-167. 
Ohana, B. and Tierney, B. 2009.Sentiment classifica-
tion of reviews using SentiWordNet. In 9th. IT & T 
Conference, page 13. 
Pang, B., and Lee, L. 2008. Opinion mining and sen-
timent analysis. Foundations and trends in infor-
mation retrieval, 2(1-2), 1-135. 
Pang, B., Lee, L. and Vaithyanathan S. 2002. Thumbs 
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proceedings of EMNLP pages 
79-86). 
Polpinij, J. and Ghose, A. K. 2008.An ontology-based 
sentiment classification methodology for online 
consumer reviews. In Web Intelligence and Intelli-
gent Agent Technology. 
Taboada, M. and Grieve, J. 2004. Analyzing appraisal 
automatically. In Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS# 04# 07), Stanford 
University, CA, pages. 158-161. AAAI Press. 
Toutanova, K., Klein, D., Manning, C. D. and Singer 
Y. 2003. Feature-Rich Part-of-Speech Tagging 
with a Cyclic Dependency Network. 
In Proceedings of HLT-NAACL, pages 252-259. 
Tsur, O., Davidov, D., & Rappoport, A. 2010. IC-
WSM?A great catchy name: Semi-supervised 
recognition of sarcastic sentences in online product 
reviews. In Proceedings of the fourth international 
AAAI conference on weblogs and social me-
dia, pages. 162-169. 
Saggion, H., Funk, A., Maynard, D. and Bontcheva, 
K. 2007. Ontology-based information extraction 
for business intelligence. In The Semantic 
Web pages 843-856, Springer Berlin Heidelberg. 
Stone, P. J., Dunphy, D. C., Smith, M. S., Ogilvie, D. 
M. and Associates. 1966. The General Inquirer: A 
Computer Approach to Content Analysis. The MIT 
Press. 
865
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175?180,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TransDoop: A Map-Reduce based Crowdsourced Translation for
Complex Domains
Anoop Kunchukuttan?, Rajen Chatterjee?, Shourya Roy?, Abhijit Mishra?,
Pushpak Bhattacharyya?
? Department of Computer Science and Engineering, IIT Bombay,
{anoopk,abhijitmishra,pb}@cse.iitb.ac.in, rajen.k.chatterjee@gmail.com
? Xerox India Research Centre,
Shourya.Roy@xerox.com
Abstract
Large amount of parallel corpora is re-
quired for building Statistical Machine
Translation (SMT) systems. We describe
the TransDoop system for gathering trans-
lations to create parallel corpora from on-
line crowd workforce who have familiar-
ity with multiple languages but are not
expert translators. Our system uses a
Map-Reduce-like approach to translation
crowdsourcing where sentence translation
is decomposed into the following smaller
tasks: (a) translation of constituent phrases
of the sentence; (b) validation of qual-
ity of the phrase translations; and (c)
composition of complete sentence trans-
lations from phrase translations. Trans-
Doop incorporates quality control mech-
anisms and easy-to-use worker user in-
terfaces designed to address issues with
translation crowdsourcing. We have eval-
uated the crowd?s output using the ME-
TEOR metric. For a complex domain like
judicial proceedings, the higher scores ob-
tained by the map-reduce based approach
compared to complete sentence translation
establishes the efficacy of our work.
1 Introduction
Crowdsourcing is no longer a new term in the do-
main of Computational Linguistics and Machine
Translation research (Callison-Burch and Dredze,
2010; Snow et al, 2008; Callison-Burch, 2009).
Crowdsourcing - basically where task outsourcing
is delegated to a largely unknown Internet audi-
ence - is emerging as a new paradigm of human
in the loop approaches for developing sophisti-
cated techniques for understanding and generat-
ing natural language content. Amazon Mechanical
Turk(AMT) and CrowdFlower 1 are representative
general purpose crowdsourcing platforms where
as Lingotek and Gengo2 are companies targeted
at localization and translation of content typically
leveraging freelancers.
Our interest is towards developing a crowd-
sourcing based system to enable general, non-
expert crowd-workers generate natural language
content equivalent in quality to that of expert lin-
guists. Realization of the potential of attaining
great scalability and cost-benefit of crowdsourcing
for natural language tasks is limited by the abil-
ity of novice multi-lingual workers generate high
quality translations. We have specific interest in
Indian languages due to the large linguistic diver-
sity as well as the scarcity of linguistic resources in
these languages when compared to European lan-
guages. Crowdsourcing is a promising approach
as many Indian languages are spoken by hundreds
of Millions of people (approximately, Hindi-Urdu
by 500M, Bangla by 200M, Punjabi by over 100M
3) coupled with the fact that representation of In-
dian workers in online crowdsourcing platforms is
very high (close to 40% in Amazon Mechanical
Turk (AMT)).
However, this is a non-trivial task owing to lack
of expertise of novice crowd workers in transla-
tion of content. It is well understood that famil-
iarity with multiple languages might not be good
enough for people to generate high quality transla-
tions. This is compounded by lack of sincerity and
in certain cases, dishonest intention of earning re-
wards disproportionate to the effort and time spent
for online tasks. Common techniques for quality
control like gold data based validation and worker
reputation are not effective for a subjective task
1http://www.mturk.com,http://www.
crowdflower.com
2http://www.lingotek.com,http:///www.
gengo.com
3http://en.wikipedia.org/wiki/List_of_
languages_by_total_number_of_speakers
175
like translation which does not have any task spe-
cific measurements. Having expert linguists man-
ually validate crowd generated content defies the
purpose of deploying crowdsourcing on a large
scale.
In this work, we propose a technique, based
on the Divide-and-Conquer principle. The tech-
nique can be considered similar to a Map-Reduce
task run on crowd processors, where the transla-
tion task is split into simpler tasks distributed to
the crowd (the map stage) and the results are later
combined in a reduce stage to generate complete
translations. The attempt is to make translation
tasks easy and intuitive for novice crowd-workers
by providing translations aids to help them gen-
erate high quality of translations. Our contribu-
tion in this work is a end-to-end, crowdsourcing-
platform-independent, translation crowdsourcing
system that completely automates the translation
crowdsourcing task by (i) managing the transla-
tion pipeline through software components and the
crowd; (ii) performing quality control on work-
ers? output; and (iii) interfacing with crowdsourc-
ing service providers. The multi-stage, Map-
reduce approach simplifies the translation task for
crowd workers, while novel design of user inter-
face makes the task convenient for the worker and
discourages spamming. The system thus offers the
potential to generate high quality parallel corpora
on a large scale.
We discuss related work in Section 2 and the
multi-staged approach which is central to our sys-
tem in Section 3. Section 4 describes the sys-
tem architecture and workflow, while Section 5
presents important aspects of the user interfaces
in the system. We present our preliminary exper-
iments and observations in Section 6. Section 7
concludes the paper, pointing to future directions.
2 Related Work
Lately, crowdsourcing has been explored as a
source for generating data for NLP tasks (Snow
et al, 2008; Callison-Burch and Dredze, 2010).
Specifically, it has been explored as a channel for
collecting different resources for SMT - evalua-
tions of MT output (Callison-Burch, 2009), word
alignments in parallel sentences (Gao et al, 2010)
and post-edited versions of MT output (Aikawa et
al., 2012). Ambati and Vogel (2010), Kunchukut-
tan et al (2012) have shown the feasibility of
crowdsourcing for collecting parallel corpora and
pointed out that quality assurance is a major issue
for successful translation crowdsourcing.
The most popular methods for quality control
of crowdsourced tasks are based on sampling and
redundancy. For translation crowdsourcing, Am-
bati et al (2010) use inter-translator agreement for
selection of a good translation from multiple, re-
dundant worker translations. Zaidan and Callison-
Burch (2011) score translations using a feature
based model comprising sentence level, worker
level and crowd ranking based features. However,
automatic evaluation of translation quality is diffi-
cult, such automatic methods being either inaccu-
rate or expensive. Post et al (2012) have collected
Indic language corpora data utilizing the crowd for
collecting translations as well as validations. The
quality of the validations is ensured using gold-
standard sentence translations. Our approach to
quality control is similar to Post et al (2012), but
we work at the level of phrases.
While most crowdsourcing activities for data
gathering has been concerned with collecting sim-
ple annotations like relevance judgments, there has
been work to explore the use of crowdsourcing
for more complex tasks, of which translation is
a good example. Little et al (2010) propose that
many complex tasks can be modeled either as iter-
ative workflows (where workers iteratively build
on each other?s works) or as parallel workflows
(where workers solve the tasks in parallel, with the
best result voted upon later). Kittur et al (2011)
suggest a map-and-reduce approach to solve com-
plex problems, where a problem is decomposed
into smaller problems, which are solved in the map
stage and the results are combined in the reduce
stage. Our method can be seen as an instance
of the map-reduce approach applied to translation
crowdsourcing, with two map stages (phrase trans-
lation and translation validation) and one reduce
stage (sentence combination).
3 Multi-Stage Crowdsourcing Pipeline
Our system is based on a multi-stage pipeline,
whose central idea is to simplify the translation
task into smaller tasks. The high level block di-
agram of the system is shown in Figure 1. Source
language documents are sentencified using stan-
dard NLP tokenizers and sentence splitters. Ex-
tracted sentences are then split into phrases us-
ing a standard chunker and rule-based merging
of small chunks. This step creates small phrases
176
Figure 1: Multistage crowdsourced translation
from complex sentences which can be easily and
independently translated. This leads to a crowd-
sourcing pipeline, with three stages of tasks for the
crowd: Phrase Translation (PT), Phrase Transla-
tion Validation (PV), Sentence Composition (SC).
A group of crowd workers translate source lan-
guage phrases, the translations are validated by a
different group of workers and finally a third group
of workers put the phrase translation together to
create target language sentences. The validation
is done by workers by providing ratings on a k-
point scale. This kind of divide and conquer ap-
proach helps to tackle the complexity of crowd-
sourcing translations since: (1) the tasks are sim-
pler for workers; (2) uniformity of smaller tasks
brings about efficiency as in any industrial assem-
bly line; (3) pricing can be controlled for each
stage depending on the complexity; and (4) quality
control can be performed better for smaller tasks.
4 System Architecture
Figure 2 shows the architecture of TransDoop,
which implements the 3-stage pipeline. The major
design considerations were: (i) translation crowd-
sourcing pipeline should be independent of spe-
cific crowdsourcing platforms; (ii) support multi-
ple crowdsourcing platforms; (iii) customize job
parameters like pricing, quality control method
and task design; and (iv) support multiple lan-
guages and domains.
The core component in the system is the
Crowdsourcing Engine. The engine manages the
execution of the crowdsourcing pipeline, lifecycle
of jobs and quality control of submitted tasks. The
Engine exposes its capabilities through the Re-
quester API, which can be used by clients for
setting up, customizing and monitoring transla-
tion crowdsourcing jobs and controlling their exe-
cution. These capabilities are made available to
requesters via the Requester Portal. In order
to make the crowdsourcing engine independent
of any specific crowdsourcing platform, platform
specific Connectors are developed. The Crowd-
sourcing system makes the tasks to be crowd-
sourced available through the Connector API.
The connectors are responsible for polling the en-
gine for tasks to be crowdsourced, pushing the
tasks to crowdsourcing platforms, hosting worker
interfaces for the tasks and pushing the results
back to the engine after they have been completed
by workers on the crowdsourcing platform. Cur-
rently the system supports the AMT crowdsourc-
ing platform.
Figure 3 depicts the lifecycle of a translation
crowdsourcing job. The requester initiates a trans-
lation job for a document (a set of sentences). The
Crowdsourcing Engine schedules the job for exe-
cution. It first splits each sentence into phrases.
For the job, PT tasks are created and made avail-
able through the Connector API. The connector
for the specified platform periodically polls the
Crowdsourcing Engine via the Connector API.
Once the connector has new PT tasks for crowd-
sourcing, it interacts with the crowdsourcing plat-
form to request crowdsourcing services. The con-
nector monitors the progress of the tasks and on
completion provides the results and execution sta-
tus to the Crowdsourcing Engine. Once all the PT
tasks for the job are completed, the crowdsourcing
Engine initiates the PV task to obtain validations
for the translations. The Quality Control system
kicks in when all the PV tasks for the job have
been completed.
The quality control (QC) relies on a combina-
tion of sampling and redundancy. Each PV task
has a few gold-standard phrase translation pairs,
which is used to ensure that the validators are hon-
estly doing their tasks. The judgments from the
177
Figure 2: Architecture of TransDoop
Figure 3: Lifecycle of a Translation Job
good validators are used to determine the quality
of the phrase translation, based on majority voting,
average rating, etc. using multiple judgments col-
lected for each phrase translation. If any phrase
validations or translations are incorrect, then the
corresponding phrases/translations are again sent
to the PT/PV stage as the case may be. This will
continue until all phrase translations in the job are
correctly translated or a pre-configured number of
iterations are done.
Once phrase translations are obtained for all
phrases in a sentence, the Crowdsourcing Engine
creates SC tasks, where the workers are asked
to compose a single correct, coherent translation
from the phrase translation obtained in the previ-
ous stages.
5 User Interfaces
5.1 Worker User Interfaces
This section describes the worker user interfaces
for each stage in the pipeline. These are man-
aged by the Connector and have been designed to
make the task convenient for the worker and pre-
vent spam submissions. In the rest of the section,
we describe the salient features of the PT and SC
UI?s. PV UI is similar to k-scale voting tasks com-
monly found in crowdsourcing platforms.
? Translation UI: Figure 4a shows the trans-
lation UI for the PT stage. The user in-
terface discourages spamming by: (a) dis-
playing source text as images; and (b) alert-
ing workers if they don?t provide a transla-
tion or spend very little time on a task. The
UI also provides transliteration support for
non-Latin scripts (especially helpful for Indic
scripts). A Vocabulary Support, which shows
translation suggestions for word sequences
appearing in the source phrase, is also avail-
able. Suggested translations can be copied to
the input area with ease and speed.
? Sentence Translation Composition UI: The
sentence translation composition UI (shown
in Figure 4b) facilitates composition of sen-
tence translations from phrase translations.
First, the worker can drag and rearrange the
translated phrases into the right order, fol-
lowed by reordering of individual words.
This is important because many Indian lan-
guages have different constituent order ( S-O-
V) with respect to English (S-V-O). Finally,
the synthesized language sentence can be
post-edited to correct spelling, case marking,
inflectional errors, etc. The system also cap-
tures the reordering performed by the worker,
an important byproduct, which can be used
for training reordering models for SMT.
5.2 Requester UI
The system provides a Requester Portal through
which the requester can create, control and mon-
itor jobs and retrieve results. The portal allows
the requester to customize the job during creation
by configuring various parameters: (a) domain
and language pair (b) entire sentence vs multi-
stage translation (c) price for task at each stage
(d) task design (number of tasks in a task group,
etc.) (e) translation redundancy (f) validation qual-
ity parameters. Translation redundancy refers to
the number of translations requested for a source
phrase. Validation redundancy refers to the num-
ber of validations collected for each phrase trans-
lation pair and the redundancy based acceptance
criteria for phrase translations (majority, consen-
sus, threshold, etc.)
178
(a) Phrase Translation UI (b) Sentence Composition UI
Figure 4: Worker User Interfaces
6 Experiments and Observations
Using TransDoop, we conducted a set of small-
scale, preliminary translation experiments. We ob-
tained translations for English-Hindi and English-
Marathi language pairs for the Judicial and
Tourism domains. For each experiment, 15 sen-
tences were given as input to the pipeline. For
evaluation, we chose METEOR, a well-known
translation evaluation metric (Banerjee and Lavie,
2005). We compared the results obtained from the
crowdsourcing system with a expert human trans-
lation and the output of Google Translate. We also
compared two expert translations using METEOR
to establish a skyline for the translation accuracy.
Table 1 summarizes the results of our experiments.
The translations with Quality Control and mul-
tistage pipeline are better than Google translations
and translations obtained from the crowd without
any quality control, as evaluated by METEOR.
Multi-stage translation yields better than complete
sentence translation. Moreover, the translation
quality is comparable to that of expert human
translation. This behavior is observed across the
two language pairs and domains. This can be seen
in some examples of crowdsourced translations
obtained through the system which are shown in
Table 2.
Incorrect splitting of sentences can cause diffi-
culties in translation for the worker. For instance,
discontinuous phrases will not be available to the
worker as a single translation unit. In the English
interrogative sentence, the noun phrase splits the
verb phrase, therefore the auxiliary and main verb
could be in different translation units. e.g.
Why did you buy the book?
In addition, the phrase structures of the source
and target languages may not map, making trans-
lation difficult. For instance, the vaala modifier in
Hindi translates to a clause in English. It does not
contain any tense information, therefore the tense
of the English clause cannot be determined by the
worker. e.g.
Lucknow vaalaa ladkaa
could translate to any one of:
the boy who lives/lived/is living in Lucknow
We rely on the worker in sentence composition
stage to correct mistakes due to these inadequacies
and compose a good translation. In addition, the
worker in the PT stage could be provided with the
sentence context for translation. However, there
is a tradeoff between the cognitive load of context
processing versus uncertainty in translation. More
elaborately, to what extent can the cognitive load
be reduced before uncertainty of translation sets
in? Similarly, how much of context can be shown
before the cognitive load becomes pressing?
7 Conclusions
In this system demonstration, we present Trans-
Doop as a translation crowdsourcing system which
has the potential to harness the strength of the
crowd to collect high quality human translations
on a large scale. It simplifies the tedious trans-
lation tasks by decomposing them into several
?easy-to-solve? subtasks while ensuring quality.
Our evaluation on small scale data shows that
the multistage approach performs better than com-
plete sentence translation. We would like to exten-
sively use this platform for large scale experiments
on more language pairs and complex domains like
Health, Parliamentary Proceedings, Technical and
Scientific literature etc. to establish the utility of
179
Language Pair Domain Google No QC Translation with QC Reference
Translate single stage multi stage Human
en-mr Tourism 0.227? 0.30 0.368 0.372 0.48
en-hi Tourism 0.292 0.363 0.387 0.422 0.51
en-hi Judicial 0.252 0.30 0.388 0.436 0.49
Table 1: Experimental Results: Comparison of METEOR scores for different techniques, language pairs and domains
?Translated by an internal Moses-based SMT system
Accordingly the penalty imposed by AO is not justified and the same is cancelled.
isk an  sAr e aO ?ArA lgAy gy d\X uEcta nhF\ h{ aOr ek hF r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and one also cancel did
tadAn  sAr e ao ?ArA lgAyA gyA d\X jAy) nhF\ h{ aOr us r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and that cancel did
(a) English-Hindi Judicial Translation
A crowd of devotees engulf Haridwar during the time of daily prayer in the evening
fAm m\ d{Enk ?ATnA k smy k dOrAn B?o\ ko apnF cpV m\ l hEr?Ar kF BFX
evening in daily prayer of time during devotees its engulf in take Haridwar of crowd
??Al  ao\ kF BFX fAm m\ d{Enk ?ATnA k smy hEr?Ar ko apnF cpV m\ ltaF h{
devotees of crowd evening in daily prayer of time haridwar its engulf in take
(b) English-Hindi Tourism Translation
Table 2: Examples of translation from Google and three
staged pipeline for source sentence (2nd, 3rd and 1st rows
of each table respectively). Domains and languages are indi-
cated above.
the method for collection of parallel corpora on a
large scale.
References
Takako Aikawa, Kentaro Yamamoto, and Hitoshi Isa-
hara. 2012. The impact of crowdsourcing post-
editing with the collaborative translation frame-
work. In Advances in Natural Language Processing.
Springer Berlin Heidelberg.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion LREC.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.
Aniket Kittur, Boris Smus, Susheel Khamkar, and
Robert E Kraut. 2011. Crowdforge: Crowdsourc-
ing complex work. In Proceedings of the 24th an-
nual ACM symposium on User interface software
and technology.
Anoop Kunchukuttan, Shourya Roy, Pratik Patel,
Kushal Ladha, Somya Gupta, Mitesh Khapra, and
Pushpak Bhattacharyya. 2012. Experiences in re-
source generation for machine translation through
crowdsourcing. Language Resources and Evalua-
tion LREC.
Greg Little, Lydia B Chilton, Max Goldman, and
Robert C Miller. 2010. Exploring iterative and par-
allel human computation processes. In Proceedings
of the ACM SIGKDD workshop on human computa-
tion.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
180
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36?41,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Measuring Sentiment Annotation Complexity of Text
Aditya Joshi
1,2,3?
Abhijit Mishra
1
Nivvedan Senthamilselvan
1
Pushpak Bhattacharyya
1
1
IIT Bombay, India,
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{adityaj, abhijitmishra, nivvedan, pb}@cse.iitb.ac.in
Abstract
The effort required for a human annota-
tor to detect sentiment is not uniform for
all texts, irrespective of his/her expertise.
We aim to predict a score that quantifies
this effort, using linguistic properties of
the text. Our proposed metric is called
Sentiment Annotation Complexity (SAC).
As for training data, since any direct judg-
ment of complexity by a human annota-
tor is fraught with subjectivity, we rely on
cognitive evidence from eye-tracking. The
sentences in our dataset are labeled with
SAC scores derived from eye-fixation du-
ration. Using linguistic features and anno-
tated SACs, we train a regressor that pre-
dicts the SAC with a best mean error rate of
22.02% for five-fold cross-validation. We
also study the correlation between a hu-
man annotator?s perception of complexity
and a machine?s confidence in polarity de-
termination. The merit of our work lies in
(a) deciding the sentiment annotation cost
in, for example, a crowdsourcing setting,
(b) choosing the right classifier for senti-
ment prediction.
1 Introduction
The effort required by a human annotator to de-
tect sentiment is not uniform for all texts. Com-
pare the hypothetical tweet ?Just what I wanted: a
good pizza.? with ?Just what I wanted: a cold
pizza.?. The two are lexically and structurally
similar. However, because of the sarcasm in the
second tweet (in ?cold? pizza, an undesirable sit-
uation followed by a positive sentiment phrase
?just what I wanted?, as discussed in Riloff et al
(2013)), it is more complex than the first for senti-
ment annotation. Thus, independent of how good
?
- Aditya is funded by the TCS Research Fellowship Pro-
gram.
the annotator is, there are sentences which will be
perceived to be more complex than others. With
regard to this, we introduce a metric called senti-
ment annotation complexity (SAC). The SAC of a
given piece of text (sentences, in our case) can be
predicted using the linguistic properties of the text
as features.
The primary question is whether such complex-
ity measurement is necessary at all. Fort et al
(2012) describe the necessity of annotation com-
plexity measurement in manual annotation tasks.
Measuring annotation complexity is beneficial in
annotation crowdsourcing. If the complexity of
the text can be estimated even before the annota-
tion begins, the pricing model can be fine-tuned
(pay less for sentences that are easy to annotate,
for example). Also, in terms of an automatic SA
engine which has multiple classifiers in its ensem-
ble, a classifier may be chosen based on the com-
plexity of sentiment annotation (for example, use
a rule-based classifier for simple sentences and a
more complex classifier for other sentences). Our
metric adds value to sentiment annotation and sen-
timent analysis, in these two ways. The fact that
sentiment expression may be complex is evident
from a study of comparative sentences by Gana-
pathibhotla and Liu (2008), sarcasm by Riloff et
al. (2013), thwarting by Ramteke et al (2013) or
implicit sentiment by Balahur et al (2011). To
the best of our knowledge, there is no general ap-
proach to ?measure? how complex a piece of text
is, in terms of sentiment annotation.
The central challenge here is to annotate a data
set with SAC. To measure the ?actual? time spent
by an annotator on a piece of text, we use an eye-
tracker to record eye-fixation duration: the time
for which the annotator has actually focused on
the sentence during annotation. Eye-tracking an-
notations have been used to study the cognitive as-
pects of language processing tasks like translation
by Dragsted (2010) and sense disambiguation by
36
Joshi et al (2011). Mishra et al (2013) present a
technique to determine translation difficulty index.
The work closest to ours is by Scott et al (2011)
who use eye-tracking to study the role of emotion
words in reading.
The novelty of our work is three-fold: (a) The
proposition of a metric to measure complexity of
sentiment annotation, (b) The adaptation of past
work that uses eye-tracking for NLP in the con-
text of sentiment annotation, (c) The learning of
regressors that automatically predict SAC using
linguistic features.
2 Understanding Sentiment Annotation
Complexity
The process of sentiment annotation consists of
two sub-processes: comprehension (where the an-
notator understands the content) and sentiment
judgment (where the annotator identifies the sen-
timent). The complexity in sentiment annotation
stems from an interplay of the two and we expect
SAC to capture the combined complexity of both
the sub-processes. In this section, we describe
how complexity may be introduced in sentiment
annotation in different classical layers of NLP.
The simplest form of sentiment annotation com-
plexity is at the lexical level. Consider the sen-
tence ?It is messy, uncouth, incomprehensible, vi-
cious and absurd?. The sentiment words used
in this sentence are uncommon, resulting in com-
plexity.
The next level of sentiment annotation com-
plexity arises due to syntactic complexity. Con-
sider the review: ?A somewhat crudely con-
structed but gripping, questing look at a person so
racked with self-loathing, he becomes an enemy to
his own race.?. An annotator will face difficulty
in comprehension as well as sentiment judgment
due to the complicated phrasal structure in this re-
view. Implicit expression of sentiment introduces
complexity at the semantic and pragmatic level.
Sarcasm expressed in ?It?s like an all-star salute to
disney?s cheesy commercialism? leads to difficulty
in sentiment annotation because of positive words
like ?an all-star salute?.
Manual annotation of complexity scores may
not be intuitive and reliable. Hence, we use a cog-
nitive technique to create our annotated dataset.
The underlying idea is: if we monitor annotation
of two textual units of equal length, the more com-
plex unit will take longer to annotate, and hence,
should have a higher SAC. Using the idea of ?an-
notation time? linked with complexity, we devise a
technique to create a dataset annotated with SAC.
It may be thought that inter-annotator agree-
ment (IAA) provides implicit annotation: the
higher the agreement, the easier the piece of text
is for sentiment annotation. However, in case of
multiple expert annotators, this agreement is ex-
pected to be high for most sentences, due to the
expertise. For example, all five annotators agree
with the label for 60% sentences in our data set.
However, the duration for these sentences has a
mean of 0.38 seconds and a standard deviation of
0.27 seconds. This indicates that although IAA is
easy to compute, it does not determine sentiment
annotation complexity of text in itself.
3 Creation of dataset annotated with
SAC
We wish to predict sentiment annotation complex-
ity of the text using a supervised technique. As
stated above, the time-to-annotate is one good can-
didate. However, ?simple time measurement? is
not reliable because the annotator may spend time
not doing any annotation due to fatigue or distrac-
tion. To accurately record the time, we use an
eye-tracking device that measures the ?duration of
eye-fixations
1
?. Another attribute recorded by the
eye-tracker that may have been used is ?saccade
duration
2
?. However, saccade duration is not sig-
nificant for annotation of short text, as in our case.
Hence, the SAC labels of our dataset are fixation
durations with appropriate normalization.
It may be noted that the eye-tracking device is
used only to annotate training data. The actual
prediction of SAC is done using linguistic features
alone.
3.1 Eye-tracking Experimental Setup
We use a sentiment-annotated data set consisting
of movie reviews by (Pang and Lee, 2005) and
tweets from http://help.sentiment140.
com/for-students. A total of 1059 sen-
tences (566 from a movie corpus, 493 from a twit-
ter corpus) are selected.
We then obtain two kinds of annotation from
five paid annotators: (a) sentiment (positive, nega-
tive and objective), (b) eye-movement as recorded
1
A long stay of the visual gaze on a single location.
2
A rapid movement of the eyes between positions of rest
on the sentence.
37
Figure 1: Gaze-data recording using Translog-II
by an eye-tracker. They are given a set of instruc-
tions beforehand and can seek clarifications. This
experiment is conducted as follows:
1. A sentence is displayed to the annotator on
the screen. The annotator verbally states the
sentiment of this sentence, before (s)he can
proceed to the next.
2. While the annotator reads the sentence, a
remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to a Translog II soft-
ware (Carl, 2012) in order to record the data.
A snapshot of the software is shown in fig-
ure 1. The dots and circles represent position
of eyes and fixations of the annotator respec-
tively.
3. The experiment then continues in modules of
50 sentences at a time. This is to prevent fa-
tigue over a period of time. Thus, each an-
notator participates in this experiment over a
number of sittings.
We ensure the quality of our dataset in different
ways: (a) Our annotators are instructed to avoid
unnecessary head movements and eye-movements
outside the experiment environment. (b) To min-
imize noise due to head movements further, they
are also asked to state the annotation verbally,
which was then manually recorded, (c) Our an-
notators are students between the ages 20-24 with
English as the primary language of academic in-
struction and have secured a TOEFL iBT score of
110 or above.
We understand that sentiment is nuanced- to-
wards a target, through constructs like sarcasm and
presence of multiple entities. However, we want to
capture the most natural form of sentiment anno-
tation. So, the guidelines are kept to a bare mini-
mum of ?annotating a sentence as positive, nega-
tive and objective as per the speaker?. This exper-
iment results in a data set of 1059 sentences with
a fixation duration recorded for each sentence-
annotator pair
3
The multi-rater kappa IAA for sen-
timent annotation is 0.686.
3.2 Calculating SAC from eye-tracked data
We now need to annotate each sentence with a
SAC. We extract fixation durations of the five an-
notators for each of the annotated sentences. A
single SAC score for sentence s for N annotators
is computed as follows:
SAC(s) =
1
N
N
?
n=1
z(n,dur(s,n))
len(s)
where,
z(n, dur(s, n)) =
dur(s,n)??(dur(n))
?(dur(n))
(1)
In the above formula, N is the total number of an-
notators while n corresponds to a specific annota-
tor. dur(s, n) is the fixation duration of annotator
n on sentence s. len(s) is the number of words
in sentence s. This normalization over number
of words assumes that long sentences may have
high dur(s, n) but do not necessarily have high
SACs. ?(dur(n)), ?(dur(n)) is the mean and
standard deviation of fixation durations for anno-
tator n across all sentences. z(n, .) is a function
that z-normalizes the value for annotator n to stan-
dardize the deviation due to reading speeds. We
convert the SAC values to a scale of 1-10 using
min-max normalization. To understand how the
formula records sentiment annotation complexity,
consider the SACs of examples in section 2. The
sentence ?it is messy , uncouth , incomprehensi-
ble , vicious and absurd? has a SAC of 3.3. On the
other hand, the SAC for the sarcastic sentence ?it?s
like an all-star salute to disney?s cheesy commer-
cialism.? is 8.3.
4 Predictive Framework for SAC
The previous section shows how gold labels for
SAC can be obtained using eye-tracking experi-
ments. This section describes our predictive for
SAC that uses four categories of linguistic fea-
tures: lexical, syntactic, semantic and sentiment-
related in order to capture the subprocesses of an-
notation as described in section 2.
4.1 Experiment Setup
The linguistic features described in Table 3.2 are
extracted from the input sentences. Some of these
3
The complete eye-tracking data is available at:http://
www.cfilt.iitb.ac.in/
?
cognitive-nlp/.
38
Feature Description
Lexical
- Word Count
- Degree of polysemy Average number of Wordnet senses per word
- Mean Word Length Average number of characters per word (commonly used in readability studies
as in the case of Pascual et al (2005))
- %ge of nouns and adjs.
- %ge of Out-of-
vocabulary words
Syntactic
- Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996)
- Non-terminal to Ter-
minal ratio
Ratio of the number of non-terminals to the number of terminals in the con-
stituency parse of a sentence
Semantic
- Discourse connectors Number of discourse connectors
- Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence
- Perplexity Trigram perplexity using language models trained on a mixture of sentences
from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus
(mentioned in Sections 3 and 5)
Sentiment-related (Computed using SentiWordNet (Esuli et al, 2006))
- Subjective Word
Count
- Subjective Score Sum of SentiWordNet scores of all words
- Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts
as one sentiment flip
Table 1: Linguistic Features for the Predictive Framework
features are extracted using Stanford Core NLP
4
tools and NLTK (Bird et al, 2009). Words that
do not appear in Academic Word List
5
and Gen-
eral Service List
6
are treated as out-of-vocabulary
words. The training data consists of 1059 tuples,
with 13 features and gold labels from eye-tracking
experiments.
To predict SAC, we use Support Vector Regres-
sion (SVR) (Joachims, 2006). Since we do not
have any information about the nature of the rela-
tionship between the features and SAC, choosing
SVR allows us to try multiple kernels. We carry
out a 5-fold cross validation for both in-domain
and cross-domain settings, to validate that the re-
gressor does not overfit. The model thus learned is
evaluated using: (a) Error metrics namely, Mean
Squared Error estimate, Mean Absolute Error esti-
mate and Mean Percentage Error. (b) the Pearson
correlation coefficient between the gold and pre-
4
http://nlp.stanford.edu/software/
corenlp.shtml
5
www.victoria.ac.nz/lals/resources/academicwordlist/
6
www.jbauman.com/gsl.html
dicted SAC.
4.2 Results
The results are tabulated in Table 2. Our obser-
vation is that a quadratic kernel performs slightly
better than linear. The correlation values are pos-
itive and indicate that even if the predicted scores
are not as accurate as desired, the system is capa-
ble of ranking sentences in the correct order based
on their sentiment complexity. The mean percent-
age error (MPE) of the regressors ranges between
22-38.21%. The cross-domain MPE is higher than
the rest, as expected.
To understand how each of the features per-
forms, we conducted ablation tests by con-
sidering one feature at a time. Based on
the MPE values, the best features are: Mean
word length (MPE=27.54%), Degree of Polysemy
(MPE=36.83%) and %ge of nouns and adjectives
(MPE=38.55%). To our surprise, word count per-
forms the worst (MPE=85.44%). This is unlike
tasks like translation where length has been shown
39
Kernel Linear Quadratic Cross Domain Linear
Domain Mixed Movie Twitter Mixed Movie Twitter Movie Twitter
MSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24
MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19
MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21%
Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46
Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using
Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates
and correlation with the gold labels.
to be one of the best predictors in translation dif-
ficulty (Mishra et al, 2013). We believe that for
sentiment annotation, longer sentences may have
more lexical clues that help detect the sentiment
more easily. Note that some errors may be intro-
duced in feature extraction due to limitations of
the NLP tools.
5 Discussion
Our proposed metric measures complexity of sen-
timent annotation, as perceived by human annota-
tors. It would be worthwhile to study the human-
machine correlation to see if what is difficult for
a machine is also difficult for a human. In other
words, the goal is to show that the confidence
scores of a sentiment classifier are negatively cor-
related with SAC.
We use three sentiment classification tech-
niques: Na??ve Bayes, MaxEnt and SVM with un-
igrams, bigrams and trigrams as features. The
training datasets used are: a) 10000 movie reviews
from Amazon Corpus (McAuley et. al, 2013) and
b) 20000 tweets from the twitter corpus (same as
mentioned in section 3). Using NLTK and Scikit-
learn
7
with default settings, we generate six posi-
tive/negative classifiers, for all possible combina-
tions of the three models and two datasets.
The confidence score of a classifier
8
for given
text t is computed as follows:
P : Probability of predicted class
Confidence(t) =
?
?
?
P if predicted
polarity is correct
1? P otherwise
(2)
7
http://scikit-learn.org/stable/
8
In case of SVM, the probability of predicted class is com-
puted as given in Platt (1999).
Classifier (Corpus) Correlation
Na??ve Bayes (Movie) -0.06 (73.35)
Na??ve Bayes (Twitter) -0.13 (71.18)
MaxEnt (Movie) -0.29 (72.17)
MaxEnt (Twitter) -0.26 (71.68)
SVM (Movie) -0.24 (66.27)
SVM (Twitter) -0.19 (73.15)
Table 3: Correlation between confidence of the
classifiers with SAC; Numbers in parentheses in-
dicate classifier accuracy (%)
Table 3 presents the accuracy of the classifiers
along with the correlations between the confidence
score and observed SAC values. MaxEnt has the
highest negative correlation of -0.29 and -0.26.
For both domains, we observe a weak yet nega-
tive correlation which suggests that the perception
of difficulty by the classifiers are in line with that
of humans, as captured through SAC.
6 Conclusion & Future Work
We presented a metric called Sentiment Annota-
tion Complexity (SAC), a metric in SA research
that has been unexplored until now. First, the pro-
cess of data preparation through eye tracking, la-
beled with the SAC score was elaborated. Using
this data set and a set of linguistic features, we
trained a regression model to predict SAC. Our
predictive framework for SAC resulted in a mean
percentage error of 22.02%, and a moderate corre-
lation of 0.57 between the predicted and observed
SAC values. Finally, we observe a negative corre-
lation between the classifier confidence scores and
a SAC, as expected. As a future work, we would
like to investigate how SAC of a test sentence can
be used to choose a classifier from an ensemble,
and to determine the pre-processing steps (entity-
relationship extraction, for example).
40
References
Balahur, Alexandra and Hermida, Jes?us M and Mon-
toyo, Andr?es. 2011. Detecting implicit expressions
of sentiment in text based on commonsense knowl-
edge. Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis,53-60.
Batali, John and Searle, John R. 1995. The Rediscov-
ery of the Mind. Artif. Intell., Vol. 77, 177-193.
Steven Bird and Ewan Klein and Edward Loper. 2009.
Natural Language Processing with Python O?Reilly
Media.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation.
Dragsted, B. 2010. 2010. Co-ordination of reading
and writing processes in translation. Contribution
to Translation and Cognition. Shreve, G. and An-
gelone, E.(eds.)Cognitive Science Society.
Esuli, Andrea and Sebastiani, Fabrizio. 2006. Sen-
tiwordnet: A publicly available lexical resource for
opinion mining. Proceedings of LREC, vol. 6, 417-
422.
Fellbaum, Christiane 1998. WordNet: An electronic
lexical database. 1998. Cambridge. MA: MIT Press.
Fort, Kar?en and Nazarenko, Adeline and Rosset, So-
phie et al2012. Modeling the complexity of manual
annotation tasks: A grid of analysis Proceedings of
the International Conference on Computational Lin-
guistics.
Ganapathibhotla, G and Liu, Bing. 2008. Identifying
preferred entities in comparative sentences. 22nd In-
ternational Conference on Computational Linguis-
tics (COLING).
Gonz?alez-Ib?a?nez, Roberto and Muresan, Smaranda and
Wacholder, Nina 2011. Identifying Sarcasm in
Twitter: A Closer Look. ACL (Short Papers) 581-
586.
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mart?nez-G?omez, Pascual and Aizawa, Akiko. 2013.
Diagnosing Causes of Reading Difficulty using
Bayesian Networks International Joint Conference
on Natural Language Processing, 13831391.
McAuley, Julian John and Leskovec, Jure 2013 From
amateurs to connoisseurs: modeling the evolution of
user expertise through online reviews. Proceedings
of the 22nd international conference on World Wide
Web.
Mishra, Abhijit and Bhattacharyya, Pushpak and Carl,
Michael. 2013. Automatically Predicting Sentence
Translation Difficulty Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), 346-351.
Narayanan, Ramanathan and Liu, Bing and Choudhary,
Alok 2009. Sentiment Analysis of Conditional Sen-
tences. Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
180-189.
Pang, Bo and Lee, Lillian. 2008. Opinion mining and
sentiment analysis Foundations and trends in infor-
mation retrieval, vol. 2, 1-135.
Pang, Bo and Lee, Lillian. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, 115-124.
Platt, John and others. 1999. Probabilistic outputs for
support vector machines and comparisons to regular-
ized likelihood methods Advances in large margin
classifiers, vol. 10, 61-74.
Ramteke, Ankit and Malu, Akshat and Bhattacharyya,
Pushpak and Nath, J. Saketha 2013. Detect-
ing Turnarounds in Sentiment Analysis: Thwarting
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), 860-865.
Riloff, Ellen and Qadir, Ashequl and Surve, Prafulla
and De Silva, Lalindra and Gilbert, Nathan and
Huang, Ruihong 2013. Sarcasm as Contrast be-
tween a Positive Sentiment and Negative Situation
Conference on Empirical Methods in Natural Lan-
guage Processing, Seattle, USA.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Scott G. , O Donnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783-792
Siegel, Sidney and N. J. Castellan, Jr. 1988. Nonpara-
metric Statistics for the Behavioral Sciences. Second
edition. McGraw-Hill.
41
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138?141,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
OWNS: Cross-lingual Word Sense Disambiguation Using Weighted
Overlap Counts and Wordnet Based Similarity Measures
Lipta Mahapatra Meera Mohan
Dharmsinh Desai University
Nadiad, India
lipta.mahapatra89@gmail.com
mu.mohan@gmail.com
Mitesh M. Khapra Pushpak Bhattacharyya
Indian Institute of Technology Bombay
Powai, Mumbai 400076,India
miteshk@cse.iitb.ac.in
pb@cse.iitb.ac.in
Abstract
We report here our work on English
French Cross-lingual Word Sense Disam-
biguation where the task is to find the
best French translation for a target English
word depending on the context in which it
is used. Our approach relies on identifying
the nearest neighbors of the test sentence
from the training data using a pairwise
similarity measure. The proposed mea-
sure finds the affinity between two sen-
tences by calculating a weighted sum of
the word overlap and the semantic over-
lap between them. The semantic overlap
is calculated using standard Wordnet Sim-
ilarity measures. Once the nearest neigh-
bors have been identified, the best trans-
lation is found by taking a majority vote
over the French translations of the nearest
neighbors.
1 Introduction
Cross Language Word Sense Disambiguation
(CL-WSD) is the problem of finding the correct
target language translation of a word given the
context in which it appears in the source language.
In many cases a full disambiguation may not be
necessary as it is common for different meanings
of a word to have the same translation. This is es-
pecially true in cases where the sense distinction
is very fine and two or more senses of a word are
closely related. For example, the two senses of
the word letter, namely, ?formal document? and
?written/printed message? have the same French
translation ?lettre?. The problem is thus reduced
to distinguishing between the coarser senses of
a word and ignoring the finer sense distinctions
which is known to be a common cause of errors
in conventional WSD. CL-WSD can thus be seen
as a slightly relaxed version of the conventional
WSD problem. However, CL-WSD has its own
set of challenges as described below.
The translations learnt from a parallel corpus
may contain a lot of errors. Such errors are hard
to avoid due to the inherent noise associated with
statistical alignment models. This problem can be
overcome if good bilingual dictionaries are avail-
able between the source and target language. Eu-
roWordNet1 can be used to construct such a bilin-
gual dictionary between English and French but it
is not freely available. Instead, in this work, we
use a noisy statistical dictionary learnt from the
Europarl parallel corpus (Koehn, 2005) which is
freely downloadable.
Another challenge arises in the form of match-
ing the lexical choice of a native speaker. For ex-
ample, the word coach (as in, vehicle) may get
translated differently as autocar, autobus or bus
even when it appears in very similar contexts.
Such decisions depend on the native speaker?s in-
tuition and are very difficult for a machine to repli-
cate due to their inconsistent usage in a parallel
training corpus.
The above challenges are indeed hard to over-
come, especially in an unsupervised setting, as ev-
idenced by the lower accuracies reported by all
systems participating in the SEMEVAL Shared
Task on Cross-lingual Word Sense Disambigua-
tion (Lefever and Hoste, 2010). Our system
ranked second in the English French task (in the
out-of-five evaluation). Even though its average
performance was lower than the baseline by 3%
it performed better than the baseline for 12 out of
the 20 target nouns.
Our approach identifies the top-five translations
of a word by taking a majority vote over the trans-
lations appearing in the nearest neighbors of the
test sentence as found in the training data. We
use a pairwise similarity measure which finds the
affinity between two sentences by calculating a
1http://www.illc.uva.nl/EuroWordNet
138
weighted sum of the word overlap and the seman-
tic overlap between them. The semantic overlap is
calculated using standard Wordnet Similarity mea-
sures.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
WSD. In section 3 we describe our approach. In
Section 4 we present the results followed by con-
clusion in section 5.
2 Related Work
Knowledge based approaches to WSD such as
Lesk?s algorithm (Lesk, 1986), Walker?s algorithm
(Walker and Amsler, 1986), Conceptual Density
(Agirre and Rigau, 1996) and Random Walk Algo-
rithm (Mihalcea, 2005) are fundamentally overlap
based algorithms which suffer from data sparsity.
While these approaches do well in cases where
there is a surface match (i.e., exact word match)
between two occurrences of the target word (say,
training and test sentence) they fail in cases where
their is a semantic match between two occurrences
of the target word even though there is no surface
match between them. The main reason for this
failure is that these approaches do not take into
account semantic generalizations (e.g., train is-
a vehicle).
On the other hand, WSD approaches which use
Wordnet based semantic similarity measures (Pat-
wardhan et al, 2003) account for such seman-
tic generalizations and can be used in conjunc-
tion with overlap based approaches. We there-
fore propose a scoring function which combines
the strength of overlap based approaches ? fre-
quently co-occurring words indeed provide strong
clues ? with semantic generalizations using Word-
net based similarity measures. The disambigua-
tion is then done using k-NN (Ng and Lee, 1996)
where the k nearest neighbors of the test sentence
are identified using this scoring function. Once
the nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors.
3 Our approach
In this section we explain our approach for Cross
Language Word Sense Disambiguation. The main
emphasis is on disambiguation i.e. finding English
sentences from the training data which are closely
related to the test sentence.
3.1 Motivating Examples
To explain our approach we start with two moti-
vating examples. First, consider the following oc-
currences of the word coach:
? S
1
:...carriage of passengers by coach and
bus...
? S
2
:...occasional services by coach and bus
and the transit operations...
? S
3
:...the Gloucester coach saw the game...
In the first two cases, the word coach appears
in the sense of a vehicle and in both the cases the
word bus appears in the context. Hence, the sur-
face similarity (i.e., word-overlap count) of S
1
and
S
2
would be higher than that of S
1
and S
3
and
S
2
and S
3
. This highlights the strength of overlap
based approaches ? frequently co-occurring words
can provide strong clues for identifying similar us-
age patterns of a word.
Next, consider the following two occurrences of
the word coach:
? S
1
:...I boarded the last coach of the train...
? S
2
:...I alighted from the first coach of the
bus...
Here, the surface similarity (i.e., word-overlap
count) of S
1
and S
2
is zero even though in both
the cases the word coach appears in the sense of
vehicle. This problem can be overcome by us-
ing a suitable Wordnet based similarity measure
which can uncover the hidden semantic similarity
between these two sentences by identifying that
{bus, train} and {boarded, alighted} are closely
related words.
3.2 Scoring function
Based on the above motivating examples, we pro-
pose a scoring function for calculating the simi-
larity between two sentences containing the target
word. Let S
1
be the test sentence containing m
words and let S
2
be a training sentence containing
n words. Further, let w
1i
be the i-th word of S
1
and let w
2j
be the j-th word of S
2
. The similarity
between S
1
and S
2
is then given by,
Sim(S
1
, S
2
) = ? ?Overlap(S
1
, S
2
)
+ (1? ?) ? Semantic Sim(S
1
, S
2
)
(1)
where,
139
Overlap(S
1
, S
2
) =
1
m + n
m
?
i=1
n
?
j=1
freq(w
1i
) ? 1
{w
1i
=w
2j
}
and,
Semantic Sim(S
1
, S
2
) =
1
m
m
?
i=1
Best Sim(w
1i
, S
2
)
where,
Best Sim(w
1i
, S
2
) = max
w
2j
?S
2
lch(w
1i
, w
2j
)
We used the lch measure (Leacock and Chodorow,
1998) for calculating semantic similarity of two
words. The semantic similarity between S
1
and
S
2
is then calculated by simply summing over the
maximum semantic similarity of each constituent
word of S
1
over all words of S
2
. Also note that
the overlap count is weighted according to the fre-
quency of the overlapping words. This frequency
is calculated from all the sentences in the train-
ing data containing the target word. The ratio-
nal behind using a frequency-weighted sum is that
more frequently appearing co-occurring words are
better indicators of the sense of the target word
(of course, stop words and function words are not
considered). For example, the word bus appeared
very frequently with coach in the training data
and was a strong indicator of the vehicle sense
of coach. The values of Overlap(S
1
, S
2
) and
Semantic Sim(S
1
, S
2
) are appropriately nor-
malized before summing them in Equation (1). To
prevent the semantic similarity measure from in-
troducing noise by over-generalizing we chose a
very high value of ?. This effectively ensured
that the Semantic Sim(S
1
, S
2
) term in Equation
(1) became active only when the Overlap(S
1
, S
2
)
measure suffered data sparsity. In other words, we
placed a higher bet on Overlap(S
1
, S
2
) than on
Semantic Sim(S
1
, S
2
) as we found the former
to be more reliable.
3.3 Finding translations of the target word
We used GIZA++2 (Och and Ney, 2003), a freely
available implementation of the IBM alignment
models (Brown et al, 1993) to get word level
alignments for the sentences in the English-French
2http://sourceforge.net/projects/giza/
portion of the Europarl corpus. Under this align-
ment, each word in the source sentence is aligned
to zero or more words in the corresponding tar-
get sentence. Once the nearest neighbors for a test
sentence are identified using the similarity score
described earlier, we use the word alignment mod-
els to find the French translation of the target word
in the top-k nearest training sentences. These
translations are then ranked according to the num-
ber of times they appear in these top-k nearest
neighbors. The top-5 most frequent translations
are then returned as the output.
4 Results
We report results on the English-French Cross-
Lingual Word Sense Disambiguation task. The
test data contained 50 instances for 20 polysemous
nouns, namely, coach, education, execution, fig-
ure, job, letter, match, mission, mood, paper, post,
pot, range, rest, ring, scene, side, soil, strain and
test. We first extracted the sentences containing
these words from the English-French portion of
the Europarl corpus. These sentences served as the
training data to be compared with each test sen-
tence for identifying the nearest neighbors. The
appropriate translations for the target word in the
test sentence were then identified using the ap-
proach outlined in section 3.2 and 3.3. For the
best evaluation we submitted two runs: one con-
taining only the top-1 translation and another con-
taining top-2 translations. For the oof evaluation
we submitted one run containing the top-5 trans-
lations. The system was evaluated using Precision
and Recall measures as described in the task pa-
per (Lefever and Hoste, 2010). In the oof evalua-
tion our system gave the second best performance
among all the participants. However, the average
precision was 3% lower than the baseline calcu-
lated by simply identifying the five most frequent
translations of a word according to GIZA++ word
alignments. A detailed analysis showed that in the
oof evaluation we did better than the baseline for
12 out of the 20 nouns and in the best evaluation
we did better than the baseline for 5 out of the 20
nouns. Table 1 summarizes the performance of our
system in the best evaluation and Table 2 gives the
detailed performance of our system in the oof eval-
uation. In both the evaluations our system pro-
vided a translation for every word in the test data
and hence the precision was same as recall in all
cases. We refer to our system as OWNS (Overlap
140
and WordNet Similarity).
System Precision Recall
OWNS 16.05 16.05
Baseline 20.71 20.71
Table 1: Performance of our system in best evalu-
ation
Word OWNS Baseline
(Precision) (Precision)
coach 45.11 39.04
education 82.15 80.4
execution 59.22 39.63
figure 30.56 35.67
job 43.93 40.98
letter 46.01 42.34
match 31.01 15.73
mission 55.33 97.19
mood 35.22 64.81
paper 48.93 40.95
post 36.65 41.76
pot 26.8 65.23
range 16.28 17.02
rest 39.89 38.72
ring 39.74 33.74
scene 33.89 38.7
side 37.85 36.58
soil 67.79 59.9
strain 21.13 30.02
test 64.65 61.31
Average 43.11 45.99
Table 2: Performance of our system in oof evalua-
tion
5 Conclusion
We described our system for English French
Cross-Lingual Word Sense Disambiguation which
calculates the affinity between two sentences by
combining the weighted word overlap counts with
semantic similarity measures. This similarity
score is used to find the nearest neighbors of the
test sentence from the training data. Once the
nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors. Our
system gave the second best performance in the
oof evaluation among all the systems that partic-
ipated in the English French Cross-Lingual Word
Sense Disambiguation task. Even though the av-
erage performance of our system was less than the
baseline by around 3%, it outperformed the base-
line system for 12 out of the 20 nouns.
References
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING).
Peter E Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In In Proceedings of the
MT Summit.
C. Leacock and M. Chodorow, 1998. Combining lo-
cal context and WordNet similarity for word sense
identification, pages 305?332. In C. Fellbaum (Ed.),
MIT Press.
Els Lefever and Veronique Hoste. 2010. Semeval-
2010 task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In In Proceed-
ings of the 5th annual international conference on
Systems documentation.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceed-
ings of the Joint Human Language Technology and
Empirical Methods in Natural Language Processing
Conference (HLT/EMNLP), pages 411?418.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In In pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computation Lin-
guistics (CICLing.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
141
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421?426,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CFILT: Resource Conscious Approaches for All-Words Domain Specific
WSD
Anup Kulkarni Mitesh M. Khapra Saurabh Sohoney Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay,
Powai, Mumbai 400076,
India
{anup,miteshk,saurabhsohoney,pb}@cse.iitb.ac.in
Abstract
We describe two approaches for All-words
Word Sense Disambiguation on a Spe-
cific Domain. The first approach is a
knowledge based approach which extracts
domain-specific largest connected com-
ponents from the Wordnet graph by ex-
ploiting the semantic relations between all
candidate synsets appearing in a domain-
specific untagged corpus. Given a test
word, disambiguation is performed by
considering only those candidate synsets
that belong to the top-k largest connected
components.
The second approach is a weakly super-
vised approach which relies on the ?One
Sense Per Domain? heuristic and uses a
few hand labeled examples for the most
frequently appearing words in the target
domain. Once the most frequent words
have been disambiguated they can pro-
vide strong clues for disambiguating other
words in the sentence using an iterative
disambiguation algorithm. Our weakly
supervised system gave the best perfor-
mance across all systems that participated
in the task even when it used as few as 100
hand labeled examples from the target do-
main.
1 Introduction
Domain specific WSD exhibits high level of ac-
curacy even for the all-words scenario (Khapra et
al., 2010) - provided training and testing are on the
same domain. However, the effort of creating the
training corpus - annotated sense marked corpora
- for every domain of interest has always been a
matter of concern. Therefore, attempts have been
made to develop unsupervised (McCarthy et al,
2007; Koeling et al, 2005) and knowledge based
techniques (Agirre et al, 2009) for WSD which
do not need sense marked corpora. However, such
approaches have not proved effective, since they
typically do not perform better than the Wordnet
first sense baseline accuracy in the all-words sce-
nario.
Motivated by the desire to develop annotation-
lean all-words domain specific techniques for
WSD we propose two resource conscious ap-
proaches. The first approach is a knowledge based
approach which focuses on retaining only domain
specific synsets in the Wordnet using a two step
pruning process. In the first step, the Wordnet
graph is restricted to only those synsets which
contain words appearing in an untagged domain-
specific corpus. In the second step, the graph is
pruned further by retaining only the largest con-
nected components of the pruned graph. Each tar-
get word in a given sentence is then disambiguated
using an iterative disambiguation process by con-
sidering only those candidate synsets which ap-
pear in the top-k largest connected components.
Our knowledge based approach performed better
than current state of the art knowledge based ap-
proach (Agirre et al, 2009). Also, the precision
was better than the Wordnet first sense baseline
even though the F-score was slightly lower than
the baseline.
The second approach is a weakly supervised ap-
proach which uses a few hand labeled examples
for the most frequent words in the target domain
in addition to the publicly available mixed-domain
SemCor (Miller et al, 1993) corpus. The underly-
ing assumption is that words exhibit ?One Sense
Per Domain? phenomenon and hence even as few
as 5 training examples per word would be suffi-
cient to identify the predominant sense of the most
frequent words in the target domain. Further, once
the most frequent words have been disambiguated
using the predominant sense, they can provide
strong clues for disambiguating other words in the
421
sentence. Our weakly supervised system gave the
best performance across all systems that partici-
pated in the task even when it used as few as 100
hand labeled examples from the target domain.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
domain-specific WSD. In section 3 we discuss an
Iterative Word Sense Disambiguation algorithm
which lies at the heart of both our approaches. In
section 4 we describe our knowledge based ap-
proach. In section 5 we describe our weakly su-
pervised approach. In section 6 we present results
and discussions followed by conclusion in section
7.
2 Related Work
There are two important lines of work for do-
main specific WSD. The first focuses on target
word specific WSD where the results are reported
on a handful of target words (41-191 words) on
three lexical sample datasets, viz., DSO corpus
(Ng and Lee, 1996), MEDLINE corpus (Weeber et
al., 2001) and the corpus of Koeling et al (2005).
The second focuses on all-words domain specific
WSD where the results are reported on large anno-
tated corpora from two domains, viz., TOURISM
and HEALTH (Khapra et al, 2010).
In the target word setting, it has been shown that
unsupervised methods (McCarthy et al, 2007) and
knowledge based methods (Agirre et al, 2009)
can do better than wordnet first sense baseline and
in some cases can also outperform supervised ap-
proaches. However, since these systems have been
tested only for certain target words, the question of
their utility in all words WSD it still open .
In the all words setting, Khapra et al (2010)
have shown significant improvements over the
wordnet first sense baseline using a fully super-
vised approach. However, the need for sense anno-
tated corpus in the domain of interest is a matter of
concern and provides motivation for adapting their
approach to annotation scarce scenarios. Here, we
take inspiration from the target-word specific re-
sults reported by Chan and Ng (2007) where by
using just 30% of the target data they obtained the
same performance as that obtained by using the
entire target data.
We take the fully supervised approach of
(Khapra et al, 2010) and convert it to a weakly su-
pervised approach by using only a handful of hand
labeled examples for the most frequent words ap-
pearing in the target domain. For the remaining
words we use the sense distributions learnt from
SemCor (Miller et al, 1993) which is a publicly
available mixed domain corpus. Our approach is
thus based on the ?annotate-little from the target
domain? paradigm and does better than all the sys-
tems that participated in the shared task.
Even our knowledge based approach does better
than current state of the art knowledge based ap-
proaches (Agirre et al, 2009). Here, we use an un-
tagged corpus to prune the Wordnet graph thereby
reducing the number of candidate synsets for each
target word. To the best of our knowledge such an
approach has not been tried earlier.
3 Iterative Word Sense Disambiguation
The Iterative Word Sense Disambiguation (IWSD)
algorithm proposed by Khapra et al (2010) lies at
the heart of both our approaches. They use a scor-
ing function which combines corpus based param-
eters (such as, sense distributions and corpus co-
occurrence) and Wordnet based parameters (such
as, semantic similarity, conceptual distance, etc.)
for ranking the candidates synsets of a word. The
algorithm is iterative in nature and involves the
following steps:
? Tag all monosemous words in the sentence.
? Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
? At each stage rank the candidate senses of a
word using the scoring function of Equation
(1).
S
?
= argmax
i
(?
i
V
i
+
?
j?J
W
ij
? V
i
? V
j
) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?
i
= BelongingnessToDominantConcept(S
i
)
V
i
= P (S
i
|word)
W
ij
= CorpusCooccurrence(S
i
, S
j
)
? 1/WNConceptualDistance(S
i
, S
j
)
? 1/WNSemanticGraphDistance(S
i
, S
j
)
The scoring function as given above cleanly
separates the self-merit of a synset (P (S
i
|word))
422
as learnt from a tagged corpus and its interaction-
merit in the form of corpus co-occurrence, con-
ceptual distance, and wordnet-based semantic dis-
tance with the senses of other words in the sen-
tence. The scoring function can thus be easily
adapted depending upon the amount of informa-
tion available. For example, in the weakly su-
pervised setting, P (S
i
|word) will be available for
some words for which either manually hand la-
beled training data from environment domain is
used or which appear in the SemCor corpus. For
such words, all the parameters in Equation (1) will
be used for scoring the candidate synsets and for
remaining words only the interaction parameters
will be used. Similarly, in the knowledge based
setting, P (S
i
|word) will never be available and
hence only the wordnet based interaction parame-
ters (i.e., WNConceptualDistance(S
i
, S
j
) and
WNSemanticGraphDistance(S
i
, S
j
)) will be
used for scoring the pruned list of candidate
synsets. Please refer to (Khapra et al, 2010) for
the details of how each parameter is calculated.
4 Knowledge-Based WSD using Graph
Pruning
Wordnet can be viewed as a graph where synsets
act as nodes and the semantic relations between
them act as edges. It should be easy to see
that given a domain-specific corpus, synsets from
some portions of this graph would be more likely
to occur than synsets from other portions. For
example, given a corpus from the HEALTH do-
main one might expect synsets belonging to the
sub-trees of ?doctor?, ?medicine?, ?disease? to
appear more frequently than the synsets belonging
to the sub-tree of ?politics?. Such dominance ex-
hibited by different components can be harnessed
for domain-specific WSD and is the motivation for
our work.
The crux of the approach is to identify such do-
main specific components using a two step prun-
ing process as described below:
Step 1: First, we use an untagged corpus from
the environment domain to identify the unique
words appearing in the domain. Note that, by
unique words we mean all content words which
appear at least once in the environment corpus
(these words may or may not appear in a gen-
eral mixed domain corpus). This untagged corpus
containing 15 documents (22K words) was down-
loaded from the websites of WWF1 and ECNC2
and contained articles on Climate Change, De-
forestation, Species Extinction, Marine Life and
Ecology. Once the unique words appearing in
this environment-specific corpus are identified, we
restrict the Wordnet graph to only those synsets
which contain one or more of these unique words
as members. This step thus eliminates all spurious
synsets which are not related to the environment
domain.
Step 2: In the second step, we perform a Breadth-
First-Search on the pruned graph to identify the
connected components of the graph. While
traversing the graph we consider only those edges
which correspond to the hypernymy-hyponymy re-
lation and ignore all other semantic relations as we
observed that such relations add noise to the com-
ponents. The top-5 largest components thus iden-
tified were considered to be environment-specific
components. A subset of synsets appearing in one
such sample component is listed in Table 1.
Each target word in a given sentence is then disam-
biguated using the IWSD algorithm described in
section 3. However, now the argmax of Equation
(1) is computed only over those candidate synsets
which belong to the top-5 largest components and
all other candidate synsets are ignored. The sug-
gested pruning technique is indeed very harsh and
as a result there are many words for which none
of their candidate synsets belong to these top-5
largest components. These are typically domain-
invariant words for which pruning does not make
sense as the synsets of such generic words do
not belong to domain-specific components of the
Wordnet graph. In such cases, we consider all the
candidate synsets of these words while computing
the argmax of Equation (1).
5 Weakly Supervised WSD
Words are known to exhibit ?One Sense Per Do-
main?. For example, in the HEALTH domain the
word cancer will invariably occur in the disease
sense and almost never in the sense of a zodiac
sign. This is especially true for the most frequently
appearing nouns in the domain as these are typi-
cally domain specific nouns. For example, nouns
such as farmer, species, population, conservation,
nature, etc. appear very frequently in the envi-
ronment domain and exhibit a clear predominant
1http://www.wwf.org
2http://www.ecnc.org
423
{ safety} - NOUN - the state of being certain that adverse effects will not be caused by some agent
under defined conditions; ?insure the safety of the children?; ?the reciprocal of safety is risk?
{preservation, saving} - NOUN - the activity of protecting something from loss or danger
{environment} - NOUN - the totality of surrounding conditions; ?he longed for the comfortable
environment of his living room?
{animation, life, living, aliveness} - NOUN - the condition of living or the state of being alive;
?while there?s life there?s hope?; ?life depends on many chemical and physical processes?
{renovation, restoration, refurbishment} - NOUN - the state of being restored to its former good
condition; ?the inn was a renovation of a Colonial house?
{ecology} - NOUN - the environment as it relates to living organisms; ?it changed the ecology of
the island?
{development} - NOUN - a state in which things are improving; the result of developing (as in the
early part of a game of chess); ?after he saw the latest development he changed his mind and be-
came a supporter?; ?in chess your should take care of your development before moving your queen?
{survival, endurance} - NOUN - a state of surviving; remaining alive
. . . . . . . . . . . .
. . . . . . . . . . . .
Table 1: Environment specific component identified after pruning
sense in the domain. As a result as few as 5 hand
labeled examples per noun are sufficient for find-
ing the predominant sense of these nouns. Further,
once these most frequently occurring nouns have
been disambiguated they can help in disambiguat-
ing other words in the sentence by contributing to
the interaction-merit of Equation (1) (note that in
Equation (1), J = Set of disambiguated words).
Based on the above intuition, we slightly mod-
ified the IWSD algorithm and converted it to a
weakly supervised algorithm. The original algo-
rithm as described in section 3 uses monosemous
words as seed input (refer to the first step of the al-
gorithm). Instead, we use the most frequently ap-
pearing nouns as the seed input. These nouns are
disambiguated using their pre-dominant sense as
calculated from the hand labeled examples. Our
weakly supervised IWSD algorithm can thus be
summarized as follows
? If a word w in a test sentence belongs to
the list of most frequently appearing domain-
specific nouns then disambiguate it first us-
ing its self-merit (i.e., P (S
i
|word)) as learnt
from the hand labeled examples.
? Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
? While disambiguating the remaining words
rank the candidate senses of a word using
the self-merit learnt from SemCor and the
interaction-merit based on previously disam-
biguated words.
The most frequent words and the corresponding
examples to be hand labeled are extracted from the
same 15 documents (22K words) as described in
section 4.
6 Results
We report the performance of our systems in the
SEMEVAL task on All-words Word Sense Dis-
ambiguation on a Specific Domain (Agirre et al,
2010). The task involved sense tagging 1398
nouns and verbs from 3 documents extracted from
the environment domain. We submitted one run
for the knowledge based system and 2 runs for the
weakly supervised system. For the weakly super-
vised system, in one run we used 5 training ex-
amples each for the 80 most frequently appear-
ing nouns in the domain and in the second run we
424
used 5 training examples each for the 200 most
frequently appearing nouns. Both our submis-
sions in the weakly supervised setting performed
better than all other systems that participated in
the shared task. Post-submission we even exper-
imented with using 5 training examples each for
as few as 20 most frequent nouns and even in
this case we found that our weakly supervised sys-
tem performed better than all other systems that
participated in the shared task.
The precision of our knowledge based system
was slightly better than the most frequent sense
(MFS) baseline reported by the task organizers
but the recall was slightly lower than the baseline.
Also, our approach does better than the current
state of the art knowledge based approach (Person-
alized Page Rank approach of Agirre et al (2009)).
All results are summarized in Table 2. The fol-
lowing guide specifies the systems reported:
? WS-k: Weakly supervised approach using 5
training examples for the k most frequently
appearing nouns in the environment domain.
? KB: Knowledge based approach using graph
based pruning.
? PPR: Personalized PageRank approach of
Agirre et al (2009).
? MFS: Most Frequent Sense baseline pro-
vided by the task organizers.
? Random: Random baseline provided by the
task organizers.
System Precision Recall Rank in shared task
WS-200 0.570 0.555 1
WS-80 0.554 0.540 2
WS-20 0.548 0.535 3 (Post submission)
KB 0.512 0.495 7
PPR 0.373 0.368 24 (Post submission)
MFS 0.505 0.505 6
Random 0.23 0.23 30
Table 2: The performance of our systems in the
shared task
In Table 3 we provide the results of WS-200 for
each POS category. As expected, the results for
nouns are much better than those for verbs mainly
because nouns are more likely to stick to the ?One
sense per domain? property than verbs.
Category Precision Recall
Verbs 45.37 42.89
Nouns 59.64 59.01
Table 3: The performance of WS-200 on each
POS category
7 Conclusion
We presented two resource conscious approaches
for All-words Word Sense Disambiguation on a
Specific Domain. The first approach is a knowl-
edge based approach which retains only domain
specific synsets from the Wordnet by using a two
step pruning process. This approach does better
than the current state of the art knowledge based
approaches although its performance is slightly
lower than the Most Frequent Sense baseline. The
second approach which is a weakly supervised ap-
proach based on the ?annotate-little from the tar-
get domain? paradigm performed better than all
systems that participated in the task even when it
used as few as 100 hand labeled examples from
the target domain. This approach establishes the
veracity of the ?One sense per domain? phe-
nomenon by showing that even as few as five ex-
amples per word are sufficient for predicting the
predominant sense of a word.
Acknowledgments
We would like to thank Siva Reddy and Abhilash
Inumella (from IIIT Hyderabad, India) for provid-
ing us the results of Personalized PageRank (PPR)
for comparison.
References
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 49?56.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
425
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553?
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 303?308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings of the
American Medical Informatics Association Annual
Symposium (AMIA 2001), pages 746?750.
426
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 662?666,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
janardhan: Semantic Textual Similarity using Universal Networking
Language graph matching
Janardhan Singh
IIT Bombay,
Mumbai, India
janardhan
@cse.iitb.ac.in
Arindam Bhattacharya
IIT Bombay,
Mumbai, India
arindamb
@cse.iitb.ac.in
Pushpak Bhattacharyya
IIT Bombay,
Mumbai, India
pb
@cse.iitb.ac.in
Abstract
Sentences that are syntactically quite different
can often have similar or same meaning. The
SemEval 2012 task of Semantic Textual Sim-
ilarity aims at finding the semantic similarity
between two sentences. The semantic repre-
sentation of Universal Networking Language
(UNL), represents only the inherent meaning
in a sentence without any syntactic details.
Thus, comparing the UNL graphs of two sen-
tences can give an insight into how semanti-
cally similar the two sentences are. This paper
presents the UNL graph matching method for
the Semantic Textual Similarity(STS) task.
1 Introduction
Universal Networking language (UNL) gives the
semantic representation of sentences in a graphi-
cal form. By comparing the similarity of these
graphs, we inherently compare only the semantic
content of the two sentences, rather than compar-
ing the similarities in the syntax. Thus, the UNL
graph matching strategy is a natural choice for the
Semantic Textual Similarity(STS) task of SemEval
2012. UNL graphs are also used in textual en-
tailment and interlingua based machine translation
tasks. We use the UNL enconverter system at:
http://www.cfilt.iitb.ac.in
/UNL enco
to generate the UNL graphs of the sentences. For the
two graphs, generated from the two sentences, we
give a similarity score by matching the two graphs.
In the following sections we describe UNL
matching strategy. section 2 describes the UNL sys-
Figure 1: UNL graph for ?John eats rice?
tem and why this approach is useful, section 3 de-
scribes the matching algorithm, section 4 describes
the challenges faced in this approach, section 5 gives
the results and finally section 6 gives the conclusion
and the future scope.
2 Universal Networking Language
The Universal Networking Language gives a graph-
ical representation of the semantics of a text in the
form of hypergraphs. The representation is at the
semantic level which allows mapping of the simi-
lar meaning sentences having different syntax to the
same representation. To exemplify this point, con-
sider the UNL graphs generated for the following
sentences:
Sentence 1: John ate rice.
Sentence 2: Rice was eaten by John.
The UNL graph generated from the system are
given in figures 1 and 2 respectively.
The UNL graph consists of three components:
662
Figure 2: UNL graph for ?Rice was eaten by John?
? Universal Words
? Relations
? Attributes
2.1 Universal Words
The Universal Words (UWs) form the vocabulary of
the Universal Networking Language. They form the
nodes of the UNL graph. The words are normalized
to their basic lemma, for example, eats becomes eat.
The Universal Word is, usually, followed by a dis-
ambiguating constraint list which is mainly used for
disambiguating the sense of the Universal Word. For
example, John (iof > person), here the word John is
disambiguated as an instance of (iof) a person and
rice is disambiguated to be in the class of (icl) proper
noun. The UNL generation system, uses a Universal
word dictionary created using the wordnet.
2.2 Relations
The UNL manual describes 46 binary semantic re-
lations among the Universal Words as given in UNL
manual. These form the labelled arcs of the UNL
graph. In the example of figures 1 and 2, the rela-
tions agent (agt) and object (obj) are shown. John is
the agent of the action eat and rice is the object of
the action eat. The UNL generation system gener-
ated these relations using complex rules based on the
dependency and constituency parser outputs, Word-
net features and Named Entity recognizer output.
2.3 Attributes
Attributes are attached to the Universal Words to
show the speakers perspective for some subjective
information in the text. For the given example, with
respect to the speaker of the text, the action of eat
happened in the past with respect to the speaker.
This is represented by the attribute @past.
The detailed description of the UNL standard can
be found in the UNL manual available online at
http://www.undl.org/unlsys/unl
/unl2005/.
The two sentences listed above, have the same
semantic content, although their syntax is different.
One sentence is in the active voice, while the other
sentence is in the passive. But if we compare the
UNL graphs of the two sentences, they are almost
identical, with an extra attribute @passive on the
main verb eat in the second graph. The graph match-
ing of the two sentences results in a high score near
to 5. Like voice, most of the syntactic variations are
dropped when we move from syntactic to semantic
representation. Thus, comparing the semantic rep-
resentation of the sentences, is useful, to identify
their semantic similarity. The UNL generation sys-
tem generates the attributes using similar features to
those for relation generation.
3 UNL matching
The UNL system available online at:
http://www.cfilt.iitb.ac.in
/UNL enco
produces graphs for the sentences by listing the
binary relations present in the graph. An example
of such a listing is :
Sentence 3: A man is eating a banana by a tree.
[unl:1]
agt ( eat(icl>eat>do, agt>thing,
obj>thing):4.@present.@progress
.@entry,
man(icl>male>thing,
equ>adult_male):2.@indef )
ins ( eat(icl>eat>do, agt>thing,
obj>thing):4.@present.@progress
.@entry,
tree(icl>woody_plant>thing)
:9.@indef )
obj ( eat(icl>eat>do, agt>thing,
obj>thing):4.@present.@progress
.@entry,
banana(icl>herb>thing,
equ>banana_tree):6.@indef )
[\unl]
663
Sentence 4 : A man is eating a banana.
[unl:1]
agt ( eat(icl>eat>do, agt>thing,
obj>thing):4.@present.@progress
.@entry,
man(icl>male>thing,
equ>adult_male):2.@indef )
obj ( eat(icl>eat>do, agt>thing,
obj>thing):4.@present.@progress
.@entry,
banana(icl>herb>thing,
equ>banana_tree):6.@indef )
[\unl]
We treat the UNL graph of one sentence as goldunl
and the other as testunl. The matching score
between the two is found using the following
formulation (Mohanty, 2008):
score(testunl, goldunl)
= (2?precision?recall)(precision+recall) (1)
precision
=
?
relation?testunl relation score(relation)
(count(relations?testunl)) (2)
recall
=
?
relation?testunl relation score(relation)
(count(relations?goldunl)) (3)
relation score(relation)
= avg(rel match, uw1score, uw2score) (4)
rel match
=
{
1 if relation name matches
0 otherwise
(5)
uwscore
= avg(word score, attribute score) (6)
word score
=
{
1 if universal word matches
0 otherwise
(7)
attribute score
= F1score(testunl attr, goldunl attr) (8)
The matching scheme is based on the idea of the
F1 score. The two UNL graphs are a list of UNL
relations each. Considering, one as the gold UNL
graph and the other as the test UNL graph, we can
find the precision and recall of the total relations that
have matched. For the example given in section 2.4,
the sentence 3 has three relations while sentence 4
has two relations. A correspondence between the
relations agt of the two graphs and also the relation
obj of the two graphs can be established based on
the universal words that they connect. Each such re-
lation match is given a score, explained later, which
is used in the calculation of the precision and recall.
From the precision and recall the F1 score can be
easily calculated which becomes the total matching
score of the two graphs.
The relation score is obtained by averaging the
scores of relation match, and the score of the two
universal word matches. The universal word match
score has a component of the attributes that match
between the corresponding universal words. This
attribute matching is again the F1 score calculation
similar to relation matching. Matching the attributes
of the universal words, contributes to the score of the
matched universal word, which in turn contributes
to the score of the matched relation. Thus, matching
of the semantic relations has more weight than the
matching of the attributes.
The score obtained by this formulation is between
0 and 1. Another score between 0 and 1 is obtained
by flipping the goldunl graph to testunl and testunl
to goldunl. Average of these two scores is then mul-
tiplied by 5 to give the final score.
By this formulation, the score obtained by match-
ing graphs for sentences 3 and 4 is 4.0
4 Challenges in the approach
In the UNL graph matching startegy we faced the
following challenges:
4.1 Sentences with grammatical errors
Many of the sentences, especially, from the MSRpar
dataset, had minor grammatical errors. The UNL
generation requires grammatical correctness. Some
of the examples of such sentences are:
? The no-shows were Sens. John Kerry of Mas-
sachusetts and Bob Graham of Florida.
664
? She countersued for $125 million, saying G+J
broke its contract with her by cutting her out
of key editorial decisions and manipulated the
magazine?s financial figures.
? ?She was crying and scared,? said Isa Yasin, the
owner of the store.
Here, terms like G+J and punctuation errors as in
the third example lead to the generation of improper
UNL graphs. To handle such cases, the UNL gener-
ation needs to get robust.
4.2 Scoping errors
UNL graphs are hypergraphs, in which, a node can
in itself be a UNL graph. Scopes are given iden-
tity numbers like :01,:02 and so on. While matching
two different UNL graphs, this matching of scope
identity numbers cannot be directly achieved. Also,
one graph may have different number of scopes as
compared to the other. Hence, eventhough the UNL
graphs are generated correctly, due to scoping mis-
matches the matching score goes down. To tackle
this problem, the UNL graphs generated are con-
verted into scopeless form before the matching is
performed. Every UNL graph has an entry node,
which is the starting node of the graph. This is de-
noed by an @entry attribute on the node. Every
scope, too, has an entry node. The idea for convert-
ing the UNL graphs into scopeless form is to replace
the scope nodes by the graphs that these nodes repre-
sent, with the connection to the original scope node
going to the entry node of the replacing graph.
4.3 Incomplete or no graph generation
It was observed that for some of the sentences,
the UNL generation system did not produce UNL
graphs or the generation was incomplete. Some of
these sentences are:
? The Metropolitan Transportation Authority
was given two weeks to restore the $1.50 fare
and the old commuter railroad rates, York de-
clared.
? Long lines formed outside gas stations and peo-
ple rushed to get money from cash machines
Sunday as Israelis prepared to weather a strike
that threatened to paralyze the country.
These, are due to some internal system errors of
the UNL generation system. To improve on this, the
UNL generation system itself has to improve.
5 Results
By adopting the methodology described in section 3,
the following results were obtained on the different
datasets.
MSRpar 0.1936
MSRvid 0.5504
SMT-eur 0.3755
On-WN 0.2888
SMT-news 0.3387
As observed, the performance is good for the
MSRvid dataset. This dataset consists of small and
simple sentences which are grammatically correct.
The performance on this dataset should further im-
prove by capturing the synonyms of the Univer-
sal words while matching the UNL relations. The
performance for MSRpar dataset is low. The sen-
tences in this dataset are long and sometimes with
minor grammatical errors resulting in incomplete or
no UNL graphs. As the UNL generation system
becomes more robust, the performance is expected
to improve quickly. The overall result over all the
datasets is given in the following table.
ALL ALLnrm Mean
0.3431 0.6878 0.3481
6 Conclusion and Future Scope
The UNL graph matching approach works well with
grammatically correct sentences. The approach de-
pends on the accuracy of the UNL generation sys-
tem itself. With the increase in the robustness of the
UNL generation system, this approach seems natu-
ral. Since, the approach is unsupervised, it does not
require any training data. The matching algorithm
can be extended to include the synonyms of the Uni-
versal Words while matching relations.
References
Mohanty, R. and Limaye, S. and Prasad, M.K. and Bhat-
tacharyya, P. 2008. Semantic Graph from English
Sentences, Proceedings of ICON-2008: 6th Inter-
national Conference on Natural Language Processing
Macmillan Publishers, India
665
UNL Center of UNDL Foundation 2005 Uni-
versal Networking Language (UNL) Spec-
ifications Version 2005 Online URL:
http://www.undl.org/unlsys/unl
/unl2005/
UNL enconversion system. 2012. Online URL:
http://www.cfilt.iitb.ac.in/UNL enco
666
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 216?220, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CFILT-CORE: Semantic Textual Similarity using Universal Networking
Language
Avishek Dan
IIT Bombay
Mumbai, India
avishekdan@cse.iitb.ac.in
Pushpak Bhattacharyya
IIT Bombay
Mumbai, India
pb@cse.iitb.ac.in
Abstract
This paper describes the system that was sub-
mitted in the *SEM 2013 Semantic Textual
Similarity shared task. The task aims to find
the similarity score between a pair of sen-
tences. We describe a Universal Network-
ing Language (UNL) based semantic extrac-
tion system for measuring the semantic simi-
larity. Our approach combines syntactic and
word level similarity measures along with the
UNL based semantic similarity measures for
finding similarity scores between sentences.
1 Introduction
Semantic Textual Similarity is the task of finding
the degree of semantic equivalence between a pair
of sentences. The core Semantic Textual Similar-
ity shared task of *SEM 2013 (Agirre et al, 2013)
is to generate a score in the range 0-5 for a pair
of sentences depending on their semantic similar-
ity. Textual similarity finds applications in infor-
mation retrieval and it is closely related to textual
entailment. Universal Networking Language (UNL)
(Uchida, 1996) is an ideal mechanism for seman-
tics representation. Our system first converts the
sentences into a UNL graph representation and then
matches the graphs to generate the semantic relat-
edness score. Even though the goal is to judge sen-
tences based on their semantic relatedness, our sys-
tem incorporates some lexical and syntactic similar-
ity measures to make the system robust in the face
of data sparsity.
Section 2 give a brief introduction to UNL. Sec-
tion 3 decribes the English Enconverter developed
Figure 1: UNL Graph for ?The boy chased the dog?
Figure 2: UNL Graph for ?The dog was chased by the
boy?
by us. Section 4 discusses the various similarity
measures used for the task. Section 5 mentions
the corpus used for training and testing. Section 6
describes the method used to train the system and
Section 7 presents the results obtained on the task
datasets.
2 Universal Networking Language
Universal Networking Language (UNL) is an inter-
lingua that represents a sentence in a language inde-
pendent, unambiguous form. The three main build-
ing blocks of UNL are relations, universal words
and attributes. UNL representations have a graphical
structure with concepts being represented as nodes
(universal words) and interactions between concepts
being represented by edges (relations) between the
nodes. Figure 1 shows the UNL graph correspond-
216
ing to the sentence ?The boy chased the dog.? The
conversion from a source language to UNL is called
enconversion. The reverse process of generating a
natural language sentence from UNL is called de-
conversion. The enconversion process is markedly
more difficult than the deconversion process due to
the inherent ambiguity and idiosyncracy of natural
language.
UNL representation captures the semantics inde-
pendent of the structure of the language. Figures
1 and 2 show the UNL representation of two struc-
turally different sentences which convey the same
meaning. The UNL graph structure remains the
same with an additional attribute on the main verb
of figure 2 indicating the voice of the sentence.
2.1 Universal Words
Universal words (UWs) are language independent
concepts that are linked to various language re-
sources. The UWs used by us are linked to
the Princeton WordNet and various other language
WordNet synsets. UWs consist of a head word
which is the word in its lemma form. For example,
in figure 2 the word chased is shown in its lemma
form as chased. The head word is followed by a
constraint list which is used to disambiguate it. For
example, chase icl (includes) pursue indicates that
chase as a type of pursuing is indicated here. Com-
plex concepts are represented by hypernodes, which
are UNL graphs themselves.
2.2 Relations
Relations are two place functions that imdicate the
relationship between UWs. Some of the commonly
used relations are agent (agt), object (obj), instru-
ment (ins), place (plc). For example, in figure 1 the
relation agt between boy and chase indicates that the
boy is the doer of the action.
2.3 Attribute
Attributes are one place functions that convey vari-
ous morphological and pragmatic information. For
example, in figure 1 the attribute past indicates that
the verb is in the past tense.
3 UNL Generation
The conversion from English to UNL involves aug-
menting the sentence with various factors such as
POS tags, NER tags and dependency parse tree
relations and paths. The suitable UW generation
is achieved through a word sense disambiguation
(WSD) system trained on a tourism corpus. The
WSD system maps the words to Wordnet 2.1 synset
ids. The attribute and relation generation is achieved
through a combination of rule-base and classifiers
trained on a small corpus. We use a nearest neighbor
classifier trained on the EOLSS corpus for generat-
ing relations. The attributes are generated by con-
ditional random fields trained on the IGLU corpus.
The attribute generation is a word level phenomena,
hence attributes for complex UWs cannot be gener-
ated by the classifiers. The steps are described in
detail.
3.1 Parts of Speech Tagging
The Stanford POS tagger using the WSJ corpus
trained PCFG model is used to tag the sentences.
Penn Treebank style tags are generated.
3.2 Word Sense Disambiguation
A Supervised Word Sense Disambiguation (WSD)
tool trained in Tourism domain is used. The WSD
system takes a sequence of tagged words and pro-
vides the WordNet synset ids of all nouns, verbs, ad-
jectives and adverbs in the sequence. The accuracy
of the system is depends on the length of the input
sentence.
3.3 Named Entity Recognition
Stanford Named Entity Recognizer is used to tag the
words in the sentence. The tags may be PERSON,
LOCATION or ORGANIZATION.
3.4 Parsing and Clause Marking
Stanford Parser is used to parse the sentences. Rules
based on the constituency parse are used to identify
the clause boundaries. The dependency parse is used
for clause type detection. It is also used in the later
stages of UNL generation.
The clauses are converted into separate sim-
ple sentences for further processing. Independent
clauses can be trivially separated since they have
complete sentential structure of their own. Depen-
dent clauses are converted into complete sentences
using rules based on the type of clause. For exam-
ple, for the sentence, That he is a good sprinter, is
217
known to all, containing a nominal clause, the sim-
ple sentences obtained are he is a good sprinter and
it is known to all. Here the dependent clause is re-
placed by the anaphora it to generate the sentence
corresponding to the main clause.
3.5 UW Generation
WordNet synset ids obtained from the WSD system
and the parts of speech tags are used to generate the
UWs. The head word is the English sentence in its
lemma form. The constraint list is generated from
the WordNet depending on the POS tag.
3.6 Relation Generation
Relations are generated by a combination of rule
base and corpus based techniques. Rules are writ-
ten using parts of speech tags, named entity tags and
parse dependency relations. The corpus based tech-
niques are used when insufficient rules exist for re-
lation generation. We use a corpus of about 28000
sentences consisting of UNL graphs for WordNet
glosses obtained from the UNDL foundation. This
technique tries to find similar examples from the cor-
pus and assigns the observed relation label to the
new part of the sentence.
3.7 Attribute Generation
Attributes are a combination of morphological fea-
tures and pragmatic information. Attribute genera-
tion can be considered to be a sequence labeling task
on the words. A conditional random field trained on
the corpus described in section 5.1 is used for at-
tribute generation.
4 Similarity Measures
We broadly define three categories of similarity
measures based on our classification of perception
of similarity.
4.1 Word based Similarity Measure
Word based similarity measures consider the sen-
tences as sets-of-words. These measures are mo-
tivated by our view that sentences having a lot of
common words will appear quite similar to a human
user. The sentences are tokenized using Stanford
Parser. The Jaccard coefficient (Agirre and Ghosh
and Mooney, 2000) compares the similarity or diver-
sity of two sets. It is the ratio of size of intersection
to the size of union of two sets. We define a new
measure based on the Jaccard similarity coefficient
that captures the relatedness between words. The
tokens in the set are augmented with related words
from Princeton WordNet. (Pedersen and Patward-
han and Michelizzi, 2004) As a preprocessing step,
all the tokens are stemmed using WordNet Stemmer.
For each possible sense of each stem, its synonyms,
antonyms, hypernyms and holonyms are added to
the set as applicable. For example, hypernyms are
added only when the token appears as a noun or verb
in the WordNet. The scoring function used is defined
as
ExtJSim(S1, S2) =
|ExtS1 ? ExtS2|
|S1 ? S2|
The following example illustrates the intuition be-
hind this similarity measure.
? I am cooking chicken in the house.
? I am grilling chicken in the kitchen.
The measure generates a similarity score of 1
since grilling is a kind of cooking (hypernymy) and
kitchen is a part of house (holonymy).
4.2 Syntactic Similarity Measures
Structural similarity as an indicator of textual sim-
ilarity is captured by the syntactic similarity mea-
sures. Parses are obtained for the pair of English
sentences using Stanford Parser. The parser is run on
the English PCFG model. The dependency graphs of
the two sentences are matched to generate the simi-
larity score. A dependency graph consists of a num-
ber of dependency relations of the form dep(word1,
word2) where dep is the type of relation and word1
and word2 are the words between which the rela-
tion holds. A complete match of a dependency re-
lation contributes 1 to the score whereas a match of
only the words in the relation contributes 0.75 to the
score.
SynSim(S1, S2) =
|S1 ? S2|
|S1 ? S2|
+ 0.75?
?
a?S1,b?S2[[a.w1 = b.w1&a.w2 = b.w2]]
|S1 ? S2|
Here S1 and S2 represent the set of dependency
relations.
218
An extended syntactic similarity measure in
which exact word matchings are replaced by a match
within a set formed by extending the word with re-
lated words as described in 4.1 is also used.
4.3 Semantic Similarity Measure
Semantic similarity measures try to capture the sim-
ilarity in the meaning of the sentences. The UNL
graphs generated for the two sentences are compared
using the formula given below. In addition, syn-
onymy is no more used for enriching the word bank
since UWs by design are mapped to synsets, hence
all synonyms are equivalent in a UNL graph.
SemSim(S1, S2) =
|S1 ? S2|
|S1 ? S2|
+
?
a?S1,b?S2
(0.75?
[[a.w1 = b.w1&a.w2 = b.w2]]
|S1 ? S2|
+ 0.75?
[[a.r = b.r&a.Ew1 = b.Ew1&a.Ew2 = b.Ew2]]
|S1 ? S2|
+0.6 ?
[[a.Ew1 = b.Ew1&a.Ew2 = b.Ew2]]
|S1 ? S2|
)
5 Corpus
The system is trained on the Semantic Textual Sim-
ilarity 2012 task data. The training dataset consists
of 750 pairs from the MSR-Paraphrase corpus, 750
sentences from the MSR-Video corpus and 734 pairs
from the SMTeuroparl corpus.
The test set contains headlines mined from sev-
eral news sources mined by European Media Moni-
tor, sense definitions from WordNet and OntoNotes,
sense definitions from WordNet and FrameNet, sen-
tences from DARPA GALE HTER and HyTER,
where one sentence is a MT output and the other is
a reference translation.
Each corpus contains pairs of sentences with an
associated score from 0 to 5. The scores are given
based on whether the sentences are on different top-
ics (0), on the same topic but have different con-
tent (1), not equivalent but sharing some details (2),
roughly equivalent with some inportant information
missing or differing (3), mostly important while dif-
fering in some unimportant details (4) or completely
equivalent (5).
Table 1: Results
Corpus CFILT Best Results
Headlines 0.5336 0.7642
OnWN 0.2381 0.7529
FNWN 0.2261 0.5818
SMT 0.2906 0.3804
Mean 0.3531 0.6181
6 Training
The several scores are combined by training a Lin-
ear Regression model. We use the inbuilt libaries of
Weka to learn the weights. To compute the proba-
bility of a test sentence pair, the following formula
is used.
score(S1, S2) = c+
5?
i=1
?iscorei(S1, S2)
7 Results
The test dataset contained many very long sentences
which could not be parsed by the Stanford parser
used by the UNL system. In addition, the perfor-
mance of the WSD system led to numerous false
negatives. Hence erroneous output were produced in
these cases. In these cases, the word based similar-
ity measures somewhat stabilized the scores. Table
1 summarizes the results.
The UNL system is not robust enough to han-
dle large sentences with long distance relationships
which leads to poor performance on the OnWN and
FNWN datasets.
8 Conclusion and Future Work
The approach discussed in the paper shows promise
for the small sentences. The ongoing development
of UNL is expected to improve the accuracy of the
system. Tuning the scoring parameters on a develop-
ment set instead of arbitrary values may improve re-
sults. A log-linear model instead of the linear com-
bination of scores may capture the relationships be-
tween the scores in a better way.
References
Eneko Agirre and Daniel Cer and Mona Diab and Aitor
Gonzalez-Agirre and Weiwei Guo. *SEM 2013
219
Shared Task: Semantic Textual Similarity, including a
Pilot on Typed-Similarity. *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics. Association for Computational Linguistics.
Hiroshi Uchida. UNL: Universal Networking Lan-
guageAn Electronic Language for Communica-
tion, Understanding, and Collaboration. 1996.
UNU/IAS/UNL Center, Tokyo.
Alexander Strehl and Joydeep Ghosh and Raymond
Mooney Impact of similarity measures on web-page
clustering. 2000. Workshop on Artificial Intelligence
for Web Search (AAAI 2000).
Ted Pedersen and Siddharth Patwardhan and Jason
Michelizzi WordNet:: Similarity: measuring the re-
latedness of concepts. 2004. Demonstration Papers
at HLT-NAACL 2004. Association for Computational
Linguistics.
220
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 495?500, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
 
IITB-Sentiment-Analysts: Participation in Sentiment Analysis 
in Twitter SemEval 2013 Task 
 
Karan Chawla, Ankit Ramteke, Pushpak Bhattacharyya 
Dept. of Computer Science and Engineering, IIT Bombay 
{chawlakaran,ankitr,pb}@cse.iitb.ac.in 
  
Abstract 
We propose a method for using discourse rela-
tions for polarity detection of tweets. We have 
focused on unstructured and noisy text like 
tweets on which linguistic tools like parsers and 
POS-taggers don?t work properly. We have 
showed how conjunctions, connectives, modals 
and conditionals affect the sentiments in tweets. 
We have also handled the commonly used ab-
breviations, slangs and collocations which are 
usually used in short text messages like tweets. 
This work focuses on a Web based application 
which produces results in real time. This ap-
proach is an extension of the previous work 
(Mukherjee et al 2012). 
1. Introduction 
Discourse relation is an important component of 
natural language processing which connects 
phrases and clauses together to establish a cohe-
rent relation. Linguistic constructs like conjunc-
tions, connectives, modals, conditionals and ne-
gation do alter the sentiments of a sentence. For 
example, the movie had quite a few memorable 
moments but I still did not like it. The overall 
polarity of the sentence is negative even though 
it has one positive and one negative clause. This 
is because of the presence of the conjunction but 
which gives more weightage to the clause fol-
lowing the conjunction.  
Traditional works in discourse analysis use a 
discourse parser (Marcu  et al, 2003; Polanyi et 
al., 2004; Wolf et al, 2005; Welner et al, 2006; 
Narayanan et al, 2009; Prasad et al, 2010). 
Many of these works and some other works in 
discourse (Taboada et al, 2008; Zhou et al, 
2011) build on the Rhetorical Structure Theory 
(RTS) proposed by Mann et al (1988) which 
tries to identify the relations between the nucleus 
 
 
and satellite in the sentence. 
 
Most of the work is based on well-structured text 
and the methods applied on that text is not suita-
ble for the discourse analysis on micro-blogs 
because of the following reasons: 
 
1. Micro-blogs like Twitter restricts a post 
(tweet) to be of only 140 characters. Thus, users 
do not use formal language to discuss their 
views. Thus, there are abundant spelling mis-
takes, abbreviations, slangs, collocations, discon-
tinuities and grammatical errors. 
These differences cause NLP tools like POS-
taggers and parsers to fail frequently, as these 
tools are built for well-structured text. Thus, 
most of the methods described in the previous 
works are not well suited for discourse analysis 
on Micro-blogs like text. 
2. The web-based applications require a 
fast response time. Using a heavy linguistic re-
source like parsing increases the processing time 
and slows down the application. 
  
Most of the previous work on discourse analysis 
does not take into consideration the conjunc-
tions, connectives, modals, conditionals etc and 
are based on bag-of-words model with features 
like part-of-speech information, unigrams, bi-
grams etc. along with other domain-specific fea-
tures like emoticons, hashtags etc. Our work 
harness the importance of discourse connectives 
like conjunctions, connectives, modals, condi-
tionals etc and show that along with bag-of-
words model, it gives better sentiment classifica-
tion accuracy. This work is the extension of 
(Mukherjee et al 2012). 
 
The roadmap for the rest of the paper is as fol-
lows: Section 2 studies the effect of discourse 
relations on sentiment analysis and identifies the  
 
495
critical ones. Section 3 talks about the semantic 
operators which influence the discourse rela-
tions. Section 4 discusses the lexicon based clas-
sification approach. Section 5 describes the fea-
ture engineering of the important features. Sec-
tion 6 gives the list of experiments conducted 
and analysis of the results. Conclusion and Fu-
ture Work is presented in Section 7. 
 
2. Discourse Relations Critical for Sen-
timent Analysis 
(Mukherjee et al 2012) showed that that the fol-
lowing discourse relations are critical for SA as 
all relations are not useful for SA. Table 1 pro-
vides examples of various discourse relations. 
 
Violated Expectations and Contrast: In Exam-
ple 2, a simple bag-of-words feature based clas-
sifier will classify it as positive. However, it ac-
tually represents a negative sentiment. Such cas-
es need to be handled separately. In Example 5, 
?memorable" has (+1) score and ?not like" has (-
1) score and overall polarity is 0 or objective 
whereas it should be negative as the final verdict 
following ?but" is the deciding factor. 
 
These kinds of sentences refute the neighboring 
clause. They can be classified as Conj_Prev in 
which the clause preceding the conjunction is 
preferred and Conj_Fol in which the clause fol-
lowing the conjunction is preferred. 
 
Conclusive or Inferential Conjunctions: These 
are the set of conjunctions, Conj_infer, that tend 
to draw a conclusion or inference. Hence, the 
discourse segment following them (subsequently 
in Example 11) should be given more weight. 
 
Conditionals: In Example 3, ?amazing" 
represent a positive sentiment. But the final po-
larity should be objective as we are talking of a 
hypothetical situation. 
 
Other Discourse Relations: Sentences under 
Cause-Effect, Similarity, Temporal Sequence, 
Attribution, Example, Generalization and Elabo-
ration, provide no contrasting, conflicting or hy-
pothetical information. They can be handled by 
taking a simple bag-of-words model.  
3. Semantic Operators Influencing Dis-
course Relations 
There are connectives or semantic operators 
present in the sentences which influence the dis-
course relation within a sentence. For example, 
in the sentence the cannon camera may bad de-
spite good battery life. The connective despite 
increases the weightage of the previous dis-
course element i.e. bad is weighted up but may 
introduces a certain kind of uncertainty which 
cannot be ignored.  
 
1.  (I did not study anything throughout the seme-
ster), so (I failed in the exams). 
2.  (Sourav failed to deliver in the penultimate test) 
despite (great expectations). 
3. If (I had bought the amazing Nokia phone), I 
would not be crying). 
4. (I love Cannon) and (I also love Sony). 
5. (The movie had quite a few memorable moments) 
but (I still did not like it). 
6. (The theater became interesting) after a while. 
7. According (to the reviews), (the movie must be 
bad). 
8. (Salman is a bad guy), for instance (he is always 
late). 
9. In addition (to the bad battery life), (the camera 
is also very costly). 
10. In general, (cameras from cannon (take great 
pictures). 
11. (They were not in favour of that camera) and 
subsequently (decided not to buy it). 
Table 1:  Examples of Discourse Coherent 
Relations 
Similarity, in the sentence He gave his best in 
the movie, but still it was not good enough to win 
an Oscar. The connective but increases the 
weight of the following discourse i.e. good and 
win are weighted up but presence of negation 
operator also cannot be ignored. 
 
496
1. Modals: Events that are happening or are 
bound to happen are called realis events. And 
those events that have possibly occurred or have 
some probability to occur in distant future are 
known as irrealis events. And it is important to 
distinguish between the two as it also alters the 
sentiments in a piece of text. Modals depict ir-
realis events and just cannot be handled by sim-
ple majority valence model. 
 
(Mukherjee et al 2012) divided modals into two 
categories: Strong_Mod and Weak_Mod. 
 
Strong_Mod is the set of modals that express a 
higher degree of uncertainty in any situation. 
Weak_Mod is the set of modals that express 
lesser degree of uncertainty and more emphasis 
on certain events or situations.  
 
Like conditionals, sentences with strong modals 
express higher degree of uncertainty, thus dis-
course elements near strong modals are weighted 
down. Thus, in the previous example the cannon 
camera may bad despite good battery life bad is 
toned down. 
 
Relations Attributes 
Conj_Fol but, however, never-
theless, otherwise, yet, 
still, nonetheless 
Conj_Prev till, until, despite, in 
spite, though, although 
Conj_Inf therefore, furthermore, 
consequently, thus, as 
a result, subsequently, 
eventually, hence 
Conditionals If 
Strong_Mod might, could, can, 
would, may 
Weak_Mod should, ought to, need 
not, shall, will, must 
Neg not, neither, never, no, 
nor 
Table 2: Discourse Relations and Semantic 
Operators Essential for Sentiment Analysis 
 
2. Negation: The negation operator inverts the 
polarity of the sentence following it. Usually, to 
handle negation a window (typically 3-5 words) 
is considered and the polarities of all the words 
are reversed. We have considered the window 
size to be 5 and reverse the polarities of all the 
words within the window, till either a conjunc-
tion comes or window size exceeds. For example 
In the sentence He gave his best in the movie, 
but still it was not good enough to win an Oscar 
polarities of good and win are reversed. 
  
4. Lexicon Based Classification 
We have used Senti-WordNet (Esuli et al 2006), 
Inquirer (Stone et. al 1996) and the Bing Liu 
sentiment lexicon (Hu et al 2004) to find out the 
word polarities. To compensate the bias effects 
introduced by the individual lexicons, we have 
used three different lexicons. The polarities of 
the reviews are given by (Mukherjee et al 2012) 
 
???? (  ??? ? ?????? ? ?(??? ))
??
?=1
?
?=1
 
 
????? ? ???  =  ??? ???   ?? ????? = 0 
                        
                             =  
??? ???  
2
 ?? ????? = 1  
   
Above equation finds the weighted, signed po-
larity of a review. The polarity of each word, 
pol(wij) being +1 or -1, is multiplied with its dis-
course weight fij and all the weighted polarities 
are added. Flipij indicates if the polarity of wij is 
to be negated. 
In case there is any conditional or strong modal 
in the sentence (indicated by ????? = 1 ), then 
the polarity of every word in the sentence is 
toned down, by considering half of its assigned 
polarity (
+1
2
 ,
?1
2
) 
Thus, if good occurs in the user post twice, it 
will contribute a polarity of +1 ? 2 = +2 to the 
overall review polarity, if ????? = 0. In the 
presence of a strong modal or conditional, it will 
contribute a polarity of 
+1
2
? 2 =  +1. 
497
All the stop words, discourse connectives and 
modals are ignored during the classification 
phase, as they have a zero polarity in the lexicon.  
We have handled commonly used slangs, ab-
breviations and collocations by manually tagging 
them as positive, negative or neutral.  
5. Feature Engineering 
The features specific for lexicon based classifi-
cation for the task sentiment Analysis, identified 
in Section 2.4, are handled as follows: 
 
a) The words following the Conj_Fol (Table 2) 
are given more weightage. Hence their frequency 
count is incremented by 1. 
We follow a naive weighting scheme whereby 
we give a (+1) weightage to every word we con-
sider important. In Example 5, ?memorable" gets 
(+1) score, while ?did not like" gets a (-2) score, 
making the overall score (-1) i.e. the example 
suggests a negative sentiment. 
 
b) The weightage of the words occurring before 
the Conj_Prev (Table 2) is increased by 1. In 
Example 2, ?failed" will have polarity (-2) in-
stead of (-1) and ?great expectations" will have 
polarity (+1), making the overall polarity (-1), 
which conforms to the overall sentiment. 
 
c) The weightage of the words in the sentences 
containing conditionals (if) and strong modals 
(might, could, can, would, may) are toned down. 
 
e) The polarity of all words appearing within a 
window of 5 from the occurrence of a negation 
operator (not, neither, nor, no, never) and before 
the occurrence of a violating expectation con-
junction is reversed. 
  
f) Exploiting sentence position information, the 
words appearing in the first k and last k sen-
tences, are given more weightage. The value of k 
is set empirically. 
 
g) The Negation Bias factor is treated as a para-
meter which is learnt from a small set of nega-
tive polarity tagged documents. The frequency 
count of all the negative words (in a rule based 
system) is multiplied with this factor to give 
negative words more weightage than positive 
words. 
6. Experiments and Evaluation 
For the lexicon-based approach, we performed 
two types of experiments- sentiment pertaining 
to a particular instance in a tweet (SemEval-
2013 Task A) and generic sentiment analysis of 
a tweet (SemEval-2013 Task B). We treat both 
the tasks similarly. 
 
6.1 Dataset 
 
We performed experiments on two Datasets: 
 
1) SemEval-2013-task 2 Twitter Dataset A con-
taining 4435 tweets without any external data. 
2) SemEval-2013-task 2 Twitter Dataset B con-
taining 3813 tweets without any external data. 
 
6.2 Results on the Twitter Dataset A and B 
 
The system performs best for the positive class 
tweets as shown in Table 3 and Table 4 and per-
forms badly for the negative class which is due 
to the fact that negative tweets can contain sar-
casm which is a difficult phenomenon to capture. 
Also the results of the neutral category are very 
less which suggests that our system is biased 
towards subjective tweets and we wish to give 
the majority sentiment in the tweets. 
  
Class Precision Recall F-score 
Positive 0.6706 0.5958 0.6310 
Negative 0.4124 0.5328 0.4649 
Neutral 0.0667 0.0063 0.0114 
Table 3: Results on Twitter Dataset A 
 
Class Precision Recall F-score 
Positive 0.4809  0.5941 0.5316 
Negative 0.1753   0.5374 0.2643 
Neutral 0.6071  0.0104 0.0204 
Table 4: Results on Twitter Dataset B 
498
6.3 Discussion 
 
The lexicon based classifier suffers from the 
problem of lexeme space where it is not able 
handle all the word senses. Also, short-noisy text 
like tweets often contain various spelling mis-
takes like great can be grt, g8t etc. or tomorrow 
can be tom, tomm, tommrrw etc. which will not 
be detected and handled properly.  
 
We suggest that a supervised approach compris-
ing of the discourse features along with the bag-
of-words model and the sense based features will 
improve the results. 
 
7. Conclusion and Future Work 
We have showed that discourse connectives, 
conjunctions, negations and conditionals do alter 
the sentiments of a piece of text. Most of the 
work on Micro-blogs like twitter is build on bag-
of-words model and does not incorporate dis-
course relations. We discussed an approach 
where we can incorporate discourse relations 
along-with bag-of-words model for a web-
application where parsers and taggers cannot be 
used as the results are required in real time. 
 
We need to take into consideration word senses 
and a supervised approach to use all the features 
collectively. Also, a spell checker would really 
help in the noisy text like in tweets.  
 
References  
A Agarwal and Pushpak Bhattacharyya. 2005. Senti-
ment Analysis: A New Approach for Effective Use of 
Linguistic Knowledge and Exploiting Similarities in a 
Set of Documents to be classified. International Con-
ference on Natural Language Processing (ICON 05), 
IIT Kanpur, India, December  
 
AR Balamurali, Aditya Joshi and Pushpak Bhattacha-
ryya. 2011. Harnessing WordNet Senses for Super-
vised Sentiment Classification. In Proceedings of 
Empirical Methods in Natural Language Processing 
(EMNLP). 
 
A Esuli and F Sebastiani, 2006. SentiWordNet: A 
Publicly Available Lexical Resource for Opinion 
Mining. In Proceedings from International Confe-
rence on Language Resources and Evaluation 
(LREC), Genoa.  
 
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proc. of ACM SIGKDD. 
 
Aditya Joshi, AR Balamurali, Pushpak Bhattacharyya 
and R Mohanty. 2010. C-Feel-It: A Sentiment Ana-
lyzer for Micro-blogs', Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2011), Ore-
gon, USA.  
 
William C. Mann and Sandra A. Thompson. Rhetori-
cal Structure Theory: Toward a functional theory of 
text organization. Text, 8 (3), 243-281. 1988 
 
R Narayanan, Bing Liu and A Choudhary. 2009. Sen-
timent Analysis of Conditional Sentences. In Pro-
ceedings of Conference on Empirical Methods in 
Natural Language Processing (EMNLP-09).  
 
L Polanyi and A Zaenen. 2004. Contextual Valence 
Shifters. In James G. Shanahan, Yan Qu, Janyce 
Wiebe (eds.), Computing Attitude and Affect in Text: 
Theory and Applications, pp. 1-10.  
 
BP Ramesh, R Prasad and H Yu. 2010. Identifying 
explicit discourse connective in biomedical text. In 
Annual Symposium proceedings, AMIA Symposium, 
Vol. 2010, pp. 657-661.  
 
R Soricut and D Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proc. of HLT-NAACL 
 
PJ Stone, DC Dunphy, MS Smith, DM Ogilvie and 
Associates. 1996. The General Inquirer: A Computer 
Approach to Content Analysis. The MIT Press  
 
Subhabrata Mukherjee and Pushpak Bhattacharyya. 
2012. Sentiment Analysis in Twitter with Lightweight 
Discourse Analysis. In Proceedings of  COLING 
2012 
Subhabrata Mukherjee and Pushpak Bhattacharyya. 
2012. Sentiment Analysis in Twitter with Lightweight 
Discourse Analysis. In Proceedings of the 21st ACM 
Conference on Information and Knowledge Manage-
ment (CIKM), short paper.   
 
Subhabrata Mukherjee, AR Balamurali, Akshat Malu 
and Pushpak Bhattacharyya. 2012. TwiSent: A Ro-
499
bust Multistage System for Analyzing Sentiment on 
Twitter. In Proceedings of the 21st ACM Conference 
on Information and Knowledge Management (CIKM), 
poster paper.  
 
Maite Taboada, Julian Brooke and Kimberly Voll. 
2008. Extracting Sentiment as a Function of Dis-
course Structure and Topicality. Simon Fraser Unive-
risty School of Computing Science Technical Report. 
 
B Wellner, J Pustejovski, A Havasi, A Rumshiskym 
and R Suair. 2006. Classification of discourse cohe-
rence relations: An exploratory study using multiple 
knowledge sources. In Proc. of SIGDIAL  
 
F Wolf and E Gibson. 2005. Representing Discourse 
Coherence: A Corpus-based Study. Computational 
Linguistics, 31(2), pp. 249-287.  
 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei 
and Kam-Fai Wong. 2011. Unsupervised discovery of 
discourse relations for eliminating intra-sentence po-
larity ambiguities. In Proceedings of EMNLP. 
500
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 116?125,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Think Globally, Apply Locally: Using Distributional Characteristics for
Hindi Named Entity Identification
Shalini Gupta Pushpak Bhattacharyya
Department of Computer Science and Engineering
IIT Bombay
Mumbai, India.
{shalini, pb}@cse.iitb.ac.in
Abstract
In this paper, we present a novel ap-
proach for Hindi Named Entity Identifica-
tion (NEI) in a large corpus. The key idea
is to harness the global distributional char-
acteristics of the words in the corpus. We
show that combining the global distribu-
tional characteristics along with the local
context information improves the NEI per-
formance over statistical baseline systems
that employ only local context. The im-
provement is very significant (about 10%)
in scenarios where the test and train cor-
pus belong to different genres. We also
propose a novel measure for NEI based
on term informativeness and show that it
is competitive with the best measure and
better than other well known information
measures.
1 Introduction
NER is the task of identifying and classifying
words in a document into predefined classes like
person, location, organization, etc. It has many ap-
plications in Natural Language Processing (NLP)
NER can be divided into two sub-tasks, Named
Entity Identification (NEI) and Named Entity
Classification (NEC). In this paper, we focus on
the first step, i.e., Named Entity Identification.
NEI is useful in applications where a list of Named
Entities (NEs) is required. Machine Translation
needs identification of named entities, so that they
can be transliterated.
For Indian languages, it is tough to identify
named entities because of the lack of capitaliza-
tion. Many approaches based on MEMM (Saha et
al., 2008b), CRFs (Li and McCallum, 2003) and
hybrid models have been tried for Hindi Named
Entity Recognition. These approaches use only
the local context for tagging the text. Many ap-
plications need entity identification in large cor-
pora. When such a large corpus is to be tagged,
one can use the global distributional characteris-
tics of the words to identify the named entities.
The state-of-the-art methods do not take advantage
of these characteristics. Also, the performance
of these systems degrades when the training and
test corpus are from different domain or different
genre. We present here our approach-Combined
Local and Global Information for Named Entity
Identification (CLGIN) which combines the global
characteristics with the local context for Hindi
Named Entity Identification. The approach com-
prises of two steps: (i) Named Entity Identifica-
tion using Global Information (NGI) which uses
the global distributional characteristics along with
the language cues to identify NEs and (ii) Com-
bining the tagging from step 1 with the MEMM
based statistical system. We consider the MEMM
based statistical system (S-MEMM) as the Base-
line. Results show that the CLGIN approach out-
performs the baseline S-MEMM system by a mar-
gin of about 10% when the training and test corpus
belong to different genre and by a margin of about
2% when both, training and test corpus are similar.
NGI also outperforms the baseline, in the former
case, when training and test corpus are from dif-
ferent genre. Our contributions in this paper are:
? Developing an approach of harnessing the
global characteristics of the corpus for Hindi
Named Entity Identification using informa-
tion measures, distributional similarity, lex-
icon, term co-occurrence and language cues
? Demonstrating that combining the global
characteristics with the local contexts im-
proves the accuracy; and with a very signif-
icant amount when the train and test corpus
are not from same domain or similar genre
? Demonstrating that the system using only the
116
global characteristics is also quite compara-
ble with the existing systems and performs
better than them, when train and test corpus
are unrelated
? Introducing a new scoring function, which
is quite competitive with the best measure
and better than other well known information
measures
Approach Description
S-MEMM
(Baseline)
MEMM based statistical system without
inserting global information
NGI Uses global distributional characteristics
along with language information for NE
Identification
CLGIN Combines the global characteristics de-
rived using NGI with S-MEMM
Table 1: Summary of Approaches
2 Related Work
There is a plethora of work on NER for En-
glish ranging from supervised approaches like
HMMs(Bikel et al, 1999), Maximum Entropy
(Borthwick, 1999) (Borthwick et al, 1998), CRF
(Lafferty et al, 2001) and SVMs to unsupervised
(Alfonseca and Manandhar, 2002), (Volker, 2005)
and semi-supervised approaches (Li and Mccal-
lum, 2005). However, these approaches do not
perform well for Indian languages mainly due to
lack of capitalization and unavailability of good
gazetteer lists. The best F Score reported for Hindi
NER using these approaches on a standard cor-
pus (IJCNLP) is 65.13% ((Saha et al, 2008a)).
Higher accuracies have been reported (81%) (Saha
et al, 2008b), albeit, on a non-standard corpus us-
ing rules and comprehensive gazetteers.
Current state-of-the-art systems (Li and McCal-
lum, 2003) (Saha et al, 2008b) use various lan-
guage independent and language specific features,
like, context word information, POS tags, suffix
and prefix information, gazetteer lists, common
preceding and following words, etc. The perfor-
mance of these systems is significantly hampered
when the test corpus is not similar to the training
corpus. Few studies (Guo et al, 2009), (Poibeau
and Kosseim, 2001) have been performed towards
genre/domain adaptation. But this still remains an
open area. Moreover, no work has been done to-
wards this for Indian languages.
Select words based on Information Measure 
Applying Pruning Heuristics
Corpus
NEIG Tagged DataSet
Applying Augmenting Heuristics
Threshold (Set using Development Set 
Step 1 
Tagging using Global Distribution (NEIG)
Trained Model Statistical System (MEMM) Step 2 MEMM Based Statistical System(S-MEMM)Final Tagged DataSet
Added    as a feature
Features (Context Words, POS Tags, Suffix Info, Gazetteers, Lexicon, etc.)
DataSet to be Tagged
Figure 1: Block diagram of CLGIN Approach
One shortcoming of current approaches is that
they do not leverage on global distributional char-
acteristics of words (e.g., Information Content,
Term Co-occurrence statistics, etc.) when a large
corpus needs NEI. Rennie and Jaakkola (2005)
introduced a new information measure and used
it for NE detection. They used this approach
only on uncapitalized and ungrammatical English
text, like blogs where spellings and POS tags are
not correct. Some semi-supervised approaches
(Collins and Singer, 1999), (Riloff and Jones,
1999), (Pas?ca, 2007) have also used large available
corpora to generate context patterns for named en-
tities or for generating gazetteer lists and entity
expansion using seed entities. Klementiev and
Roth (2006) use cooccurrence of sets of terms
within documents to boost the certainty (in a
cross-lingual setting) that the terms in question
were really transliterations of each other.
In this paper, we contend that using such global
distributional characteristics improves the perfor-
mance of Hindi NEI when applied to a large cor-
pus. Further, we show that the performance of
such systems which use global distribution charac-
teristics is better than current state-of-the-art sys-
tems when the training and test corpus are not sim-
ilar (different domain/genre) thereby being more
suitable for domain adaptation.
3 MEMM based Statistical System
(S-MEMM)
We implemented the Maximum Entropy Markov
Model based system(Saha et al, 2008b) for NE
Identification. We use this system as our Base-
line and compare our approaches NGI and CLGIN
with this baseline. We used various language de-
pendent and independent features. An important
117
Inpu
t?Tex
t:???? ??
 ?????  
??? ?? 
?????? 
?? ????
 ??? ??
 ????| ?
???? ?? ?
??????? 
??? ?? 
?? ?? 
???? ?
???? ??
???? ??|
Engl
ish?T
rans
latio
n:Li
b?de
stro
yed?
all?th
e?fis
hes?
of?Si
saar
Rive
r.?Ba
nkad
smil
ingly
?said
,?tha
t?he
?wou
ld?su
rely?
go?fo
r?fish
ing?t
omo
rrow
.
Tran
slite
ratio
n:Li
b?ne
?Sisa
arN
adik
ima
chliy
onk
a?sa
moo
lnaa
shka
rdiy
a.?Ba
nkad
ne?m
usku
raka
rkah
aki
kalv
ahm
achl
ipak
adne
jaye
gah
i?
???, ???
??, ???, ??
?
Wor
d?
(Tra
nslit
erat
ion,
???????
??????I
nfo?
Valu
e
???
Extr
act??
??No
uns
E
Hih
??????,
 ???, ???
??, 
??, ??
?? 
Lib,?
Sisa
ar,?N
adi,?
Tran
slati
on)
???
(Lib,
?Lib)
2.67
18
?????
(Sisa
ar,?S
isaa
r)
0.99
82
?(
d
)
????? ?????? ?????
Calc
ulat
e?
Extr
act?H
igh?
Info
?Valu
e?
Term
s??
(Ab
Lib,
Sisa
ar,R
iver,
fishe
s,
mac
hliyo
n,?na
ash,
?
Ban
kad,
?kal,
?mac
hli
???
(Nad
i,?Riv
er)
0.28
39
??????
(ma
chliy
on,?f
ishe
s)
0.46
22
???
(naa
sh,?d
estru
ction
)0
.109
7
?
(
kd
kd)
? ???? Apply
Term
Info
rma
tion
?
Con
tent
(Abo
ve?
Thre
shol
d)
Lib,?
Sisa
ar,?R
iver,
?fish
es,?
dest
ruct
ion,?
Bank
ad,?
tom
orro
w,?fi
sh
?????
(Ban
kad,
?Ban
kad)
1.31
75
??
(kal,
?tom
orro
w)
0.22
88
????
(ma
chli,
?fish
)
0.61
48
App
ly?Te
rm?
Excl
udin
g?
Heu
risti
cs?
(Lex
icon
Suff
ix
App
ly?
Aug
men
ting
?
H
iti
(Lex
icon
,?Suf
fix,?
Dist
.?Sim
.)
?????  ?
??
Heu
risti
cs?
(Ter
m?C
o?oc
curr
ence
)
???,????
??,??????
Inpu
t?Tex
t:???? ??
 ????? ?
?? ?? ?
????? ?
? ???? ?
?? ?? ?
???| ????
? ?? ????
???? ??
? ?? ??
 ?? ??
?? ?????
 ?????? 
??|
Outp
ut:
Engl
ish?T
rans
latio
n:?Li
bde
stro
yed?
all?th
e?fis
hes?
of?Si
saar
Rive
r.?Ba
nkad
smil
ingly
?said
,?tha
t?he
?wou
ld?su
rely?
go?fo
r?fish
ing?t
omo
rrow
.
Tran
slite
ratio
n:?Li
bne
?Sisa
arN
adik
ima
chliy
onk
a?sa
moo
lnaa
shka
rdiy
a.?Ba
nkad
ne?m
usku
raka
rkah
aki
kalv
ahm
achl
ipak
adne
jaye
gah
i
Figure 2: An Example explaining the NGI approach
modification was the use of lexicon along with tra-
ditionally used gazetteers. Gazetteers just improve
the recall whereas including the lexicon improves
the precision. The state-of-art Hindi NER sys-
tems do not use lexicon of general words but we
found that using lexicons significantly improves
the performance. Unlike English, NEs in Hindi are
not capitalized and hence it becomes important to
know, if a word is a common word or not.
Features used in S-MEMM were:
? Context Words: Preceding and succeeding two
words of the current word
? Word suffix and prefix: Fixed length (size: 2)
suffix information was used. Besides, suffix
list of common location suffixes was created
? First word and last word information
? Previous NE Tag information
? Digit information
? Gazetteer Lists: Person and Location names,
Frequent words after and before person, orga-
nization and location names, list of common
initials, stopwords, etc.
? POS Tag Information
? Lexicons: If the stemmed word was present in
the lexicon, this feature was true.
4 Our Approach-CLGIN
In this section, we describe our approach, CLGIN
in detail. It combines the global information from
the corpus with the local context. Figure 1 gives
the block diagram of the system while tagging a
corpus and Figure 2 explains the approach using
an example. This approach involves two steps.
Step 1 of CLGIN is NGI which creates a list
of probable NEs (both uni-word and multi-word)
from the given corpus and uses it to tag the whole
corpus. Sections 4.1 and 4.2 explain this step in
detail. Later, in step 2, it combines the tagging
obtained from step 1, as a feature in the MEMM
based statistical system. Output thus obtained
from the MEMM system is the final output of the
CLGIN approach. The creation of list in step 1,
involves the following steps
? A list of all words which appeared as a noun at
least once in the the corpus is extracted.
? List is ordered on the basis of the information
content derived using the whole corpus. Words
above the threshold (set during training using
the development set) are selected as NEs.
? Heuristics are applied for pruning and aug-
menting the list.
? Multi-word NEs derived using term co-
occurrence statistics along with language char-
acteristics are added to the NE list.
The above process generates a list of NEs (uni-
word and multi-word). In the second step, we pro-
vide this tagging to the S-MEMM along with other
set of features described in Section 3
During training, the cutoff threshold is set for
selecting NEs (in bullet 2) above. Also the tagging
obtained from the step 1 is added as a feature to
118
S-MEMM and a model is trained during the train-
ing phase. The following sections describe this ap-
proach in detail.
4.1 Information Measures/Scoring Functions
Various measures have been introduced for de-
termining the information content of the words.
These include, IDF (Inverse Document Fre-
quency) (Jones, 1972) , Residual IDF (Church and
Gale, 1995), xI - measure (Bookstein and Swan-
son, 1974), Gain (Papineni, 2001), etc. We intro-
duced our own information measure, RF (Ratio of
Frequencies).
4.1.1 RF (Ratio of Frequencies)
NEs are highly relevant words in a document
(Clifton et al, 2002) and are expected to have high
information content (Rennie and Jaakkola, 2005).
It has been found that words that appear frequently
in a set of documents and not so frequently in the
rest of the documents are important with respect to
that set of documents where they are frequent.
We expected the NEs to be concentrated in few
documents. We defined a new criteria which mea-
sures the ratio of the total number of times the
word appears in the corpus to the number of doc-
uments containing a word.
RF (w) = cf(w)df(w)
where cf(w) is the total frequency of a word in
the whole corpus and df(w) is the document fre-
quency. This measure is different from the TF-IDF
measure in terms of the term frequency. TF-IDF
considers the frequency of the word in the docu-
ment. RF considers it over the whole corpus.
We use the scoring function (information mea-
sure) to score all the words. During training, we
fix a threshold using the development set. Dur-
ing testing, we pick words above the threshold as
NEs. We then apply heuristics to augment this list
as well as to exclude terms from the generated list.
4.2 Heuristics for Pruning and Augmenting
NE List
Distributional Similarity: The underlying idea
of Distributional Similarity is that a word is char-
acterized by the company it keeps (Firth, 1957).
Two words are said to be distributionally similar
if they appear in similar contexts. From the previ-
ous step (Sect. 4.1), we get a list of words having
high score. Say, top t, words were selected. In
this step, we take t more words and then cluster
together these words. The purpose at this phase is
primarily to remove the false positives and to in-
troduce more words which are expected to be NEs.
For each distinct word, w in the corpus, we cre-
ate a vector of the size of the number of distinct
words in the corpus. Each term in the vector rep-
resents the frequency with which it appears in the
context (context window: size 3) of word, w. It
was observed that the NEs were clustered in some
clusters and general words in other clusters. We
tag a cluster as a NE cluster if most of the words
in the cluster are good words. We define a word
as good if it has high information content. If the
sum of the ranks of 50% of the top ranked word is
low, we tag the cluster as NE and add the words
in that set as NEs. Also, if most of the words in
the cluster have higher rank i.e. lower information
content, we remove it from the NE set.
This heuristic is used for both augmenting the
list as well to exclude terms from the list.
Lexicon: We used this as a list for excluding
terms. Terms present in the lexicon have a high
chance of not being NEs. When used alone, the
lexicon is not very effective (explained in Sec-
tion 5.2). But, when used with other approaches,
it helps in improving the precision of the sys-
tem significantly. State-of-art Hindi NER systems
use lists of gazetteers for Person names, location
names, organization names, etc. (Sangal et al,
2008), but lexicon of general words has not been
used. Unlike English, for Indian languages, it is
important to know, if a word is a general word
or not. Lexicons as opposed to gazetteers are
generic and can be applied to any domain. Un-
like gazetteers, the words would be quite common
and would appear in any text irrespective of the
domain.
Suffixes: NEs in Hindi are open class words and
appear as free morphemes. Unlike nouns, NEs,
usually do not take any suffixes (attached to them).
However, there are few exceptions like, lAl Ekl
k bAhr (laal kile ke baahar, (outside Red Fort))
or when NEs are used as common nouns, df ko
gA\EDyo\ kF j!rta h{ (desh ko gandhiyon ki za-
roorat hai, The country needs Gandhis.) etc. We
remove words appearing with common suffixes
like e\ (ein), ao\ (on), y\g (yenge), etc. from the
NE list.
Term Co-occurrence: We use the term co-
occurrence statistics to detect multi-word NEs. A
word may be a NE in some context but not in an-
other. E.g. mhA(mA (mahatma ?saint?) when ap-
119
pearing with gA\DF (Gandhi ?Gandhi?) is a NE,
but may not be, otherwise. To identify such multi-
words NEs, we use this heuristic. Such words can
be identified using Term Co-occurrence. We use
the given set of documents to find all word pairs.
We then calculate Pointwise Mutual Information
(PMI) (Church and Hanks, 1990) for each of these
word pairs and order the pairs in descending order
of their PMI values. Most of the word pairs belong
to the following categories:
? Adjective Noun combination (Adjectives fol-
lowed by noun): This was the most frequent
combination. E.g. BFnF g\D (bheeni gandh
?sweet smell?)
? Noun Verb combination: Edl DwknA (dil
dhadakna, ?heart beating?)
? Adverb verb combination: EKlEKlAkr
h\snA (khilkhilakar hansna, ?merrily laugh?)
? Cardinal/Ordinal Noun Combination: TowF
dr (thodi der, ?some time?)
? Named Entities
? Hindi Idioms: uSl sFDA (ullu seedha)
? Noun Noun Combination: HyAtaF aEjta (khy-
ati arjit, ?earn fame?)
? Hindi Multiwords: jof Krof (josh kharosh)
We need to extract NEs from these word pairs. The
first four combinations can be easily excluded be-
cause of the presence of a verb, cardinals and ad-
jectives. Sometimes both words in the NEs appear
as nouns. So, we cannot reject the Noun Noun
combination. We handle rest of the cases by look-
ing at the neighbours (context) of the word pairs.
We noticed three important things here:
? Multiwords which are followed (alteast once)
by m\ (mein), s (se), n (ne), k (ke), ko
(ko) (Hindi Case Markers) are usually NEs.
We did not include kF (ki) in the list be-
cause many words in the noun-noun combi-
nation are frequently followed by ki in the
sense of EkyA/ krnA (kiya/karna, ?do/did?)
e.g. HyAtaF aEjta kF (khyati arjit ki, ?earned
fame?), prF"A u?FZ kF (pariksha uttirand
ki, ?cleared the exam?), etc.
? There were word pairs which were followed
by a single word most of the time. E.g I-V
i\EXyA (East India, ?East India?) was followed
by k\pnF (Company, ?Company?) in almost all
the cases. When Company appears alone, it
may not be a NE, but when it appears with East
Corpus No. of Tagged No. of No. of Source Genre
Documents Words NEs
Gyaan 1570 569K 21K Essay, Biography,
Nidhi History and Story
Table 2: Corpus Statistics
India, it appears as a NE. Other examples of
such word pairs were: KA iNn (Khan Ibnu,
?Khan Ibnu?) followed by alFsm (Alisam,
?Alisam?)
? There were word pairs which were followed
by uncommon words were not common words
but were different words each time, it ap-
peared. i.e. Most of the words following the
word pair were not part of lexicon. gvnr
jnrl (governor general, ?Governor Gen-
eral?) followed by [ dlhOsF, bhd  r, solbrF,
m{VkA', lOX ((dalhousie, bahadur, solbari,
metkaf, lord), ?Dalhousie, Bahadur, Solbari,
Metkaf, Lord?)] Such words are multi word
NEs.
4.3 Step 2: Combining NGI with S-MEMM
The tagging obtained as the result of the step 1
(NGI), is given as input to the MEMM based sta-
tistical system (S-MEMM). This feature is intro-
duced as a binary feature OldTag=NE. If a word is
tagged as NE in the previous step, this feature is
turned on, otherwise OldTag=O is turned on.
5 Experiments and Results
We have used Gyaan Nidhi Corpus for eval-
uation which is a collection of various books
in Hindi. It contains about 75000 documents.
The details of the corpus are given in Table
2. Names of persons, locations, organizations,
books, plays, etc. were tagged as NE and other
general words were tagged as O (others). The
tagged documents are publicly made available at
http://www.cfilt.iitb.ac.in/ner.tar.gz.
We use the following metrics for evaluation:
Precision, Recall and F-Score. Precision is the
ratio of the number of words correctly tagged as
NEs to the total number of words tagged as NEs.
Recall is the ratio of the number of words cor-
rectly tagged as NEs to the total number of NEs
present in the data set. F Score is defined as
(F = 2 ? P ?R/(P + R))
120
5.1 Comparison of Information Measures
We compare the performance of the various
term informativeness measures for NEI which are
Residual IDF1, IDF 2, Gain3 and x? measure 4
and the measure defined in Section 4.1.1. Table
3 shows the results averaged after five-fold cross
validation. The graphs in the Figure 3 to Figure
7 show the distribution of words (nouns) over the
range of values of each information measure.
Scoring Function Prec. Recall F Score
Residual IDF 0.476 0.537 0.504
IDF 0.321 0.488 0.387
x-dash Measure 0.125 0.969 0.217
RF (Our Measure) 0.624 0.396 0.484
Gain 0.12 0.887 0.211
Table 3: Comparison of performance of various
information measures
The best results were obtained using Residual
IDF followed by Ratio of Frequencies (RF).
Method Prec Recall F Score
S -MEMM (Baseline) 0.871 0.762 0.812
Res. IDF 0.476 0.537 0.504
Res. IDF + Dist Sim (DS) 0.588 0.522 0.553
Res. IDF + Lexicon (Lex) 0.586 0.569 0.572
Res. IDF + DS + Suffix 0.611 0.524 0.563
Res. IDF + Lex + Suffix 0.752 0.576 0.65
Res. IDF + Lex + Suffix + Term 
Cooccur (NGI) 0.757 0.62 0.68
CLGIN 0.879 0.784 0.829
Table 4: Performance of various Approaches
(Here, train and test are similar)
5.2 NGI and CLGIN Approaches (Training
and Test Set from Similar Genre)
Table 4 compares the results of S-MEMM, NGI
approach and CLGIN. Besides, it also shows the
step wise improvement of NGI approach. The
final F-Score achieved using NGI approach was
68%. The F-Score of the Baseline system im-
plemented using the MaxEnt package1 from the
OpenNLP community was 81.2%.
Using the lexicon alone gives an F-Score of
only 11% (Precision: 5.97 Recall: 59.7 F-Score:
10.8562). But, when used with Residual IDF, the
1Observed IDF - Expected IDF
2IDF = -log df(w)D
3Gain = dwD (
dw
D ? 1? log
dw
D )
4x?(w) = df(w)? cf(w)
1http://maxent.sourceforge.net/index.html
4550
Resid
u
354045 202530
f?Words
101520
%?o
05
Resid
ual?ual
?IDF
Gene
ral?W
ords
Name
d?Ent
ities
IDF?V
alues
Figure 3: Distribution of Residual IDF values over
the nouns in the corpus
performance of the overall system improves sig-
nificantly to about 57%. Note that, the use of lexi-
con resulted in an increase in precision (0.5860)
which was accompanied by improvement in re-
call (0.5693) also. The cutoff thresholds in both
cases (Rows 2 and 4 of Table 4) were different.
Suffix information improved the systems perfor-
mance to 65%. As words were removed, more
words from the initial ordered list (ordered on the
basis of score/information content) were added.
Hence, there was a small improvement in recall,
too. Improvement by distributional similarity was
eclipsed after the pruning by lexicon and suffix in-
formation. But, in the absence of lexicon; distri-
butional similarity and suffix information can be
used as the pruning heuristics. Adding the multi-
word NEs to the list as explained in the section 4.2
using term co-occurrence statistics, improved the
accuracy significantly by 3%. Word pairs were ar-
ranged in the decreasing order of their PMI values
and a list was created. We found that 50% of the
NE word pairs in the whole tagged corpus lied in
the top 1% of this word pairs list and about 70%
of NE word pairs were covered in just top 2% of
the list.
CLGIN which combines the global informa-
tion obtained through NGI with the Baseline S-
MEMM system gives an improvement of about
2%. After including this feature, the F-Score in-
creased to 82.8%.
5.3 Performance Comparison of Baseline,
NGI and CLGIN (Training and Test Data
from different genre)
In the above experiments, documents were ran-
domly placed into different splits. Gyaan Nidhi
is a collection of various books on several top-
121
35
Ga
2530 1520
of? ?Words
105
%?
05
ain
Gene
ral?W
ords
Name
d?Ent
ities
Gain
Figure 4: Distribution of Gain values over the
nouns in the corpus
25
ID
20 1015
of?Words
510
%?
0
DF
Gene
ral?W
ords
N
dEt
iti
Name
d?Ent
ities
IDF
Figure 5: Distribution of IDF values over the
nouns in the corpus
60
Ratio
?Of?Fr
4050 3040
of?Words
1020
%?
010
Ratio
?orequ
encie
s General
?Word
s
Name
d?Ent
ities
of?Fre
quen
cies
Figure 6: Distribution of Ratio of Frequencies(RF)
values over the nouns in the corpus
ics. Random picking resulted into the mixing of
the documents, with each split containing docu-
ments from all books. But, in this experiment,
we divided documents into two groups such that
documents from few books (genre: Story and His-
tory) were placed into one group and rest into an-
other group (Genre: Biography and Essay). Table
5 compares the NGI and CLGIN approaches with
25
x'?Me
20 1015
of?Words
510
%?
0
x'?Meas
ure
Gene
ral?W
ords
Name
d?Ent
ities
Meas
ure
Figure 7: Distribution of xI measure values over
the nouns in the corpus
S-MEMM and shows that the CLGIN results are
significantly better than the Baseline System,
when the training and test sets belong to different
genre. The results were obtained after 2-fold cross
validation.
Method Prec. Recall F Score
S-MEMM 0.842 0.479 0.610
NGI 0.744 0.609 0.67
CLGIN 0.867 0.622 0.723
Table 5: Performance of various Approaches
(Here, train and test are from different genre)
Similar improvements were seen when the sets
were divided into (Story and Biography) and (Es-
say and History) (The proportions of train and test
sets in this division were uneven). The F Score
of NGI system was 0.6576 and S-MEMM was
0.4766. The F Score of the combined system
(CLGIN) was 0.6524.
6 Discussion and Error Analysis
6.1 RF and other information measures
As can be seen from the graphs in Figures 3 to 7,
Residual IDF best separates the NEs from the gen-
eral words. The measure introduced by us, Ratio
of Frequencies is also a good measure, although
not as good as Residual IDF but performs better
than other measures. The words having RF value
greater than 2.5 can be picked up as NEs, giving a
high recall and precision. It is evident that IDF is
better than both, Gain and xI measure, as most of
the general words have low IDF and NEs lie in the
high IDF zone. But, the general words and NEs
are not very clearly separated. As the number of
nouns is about 7-8 times the number of NEs, the
122
words having high IDF cannot be picked up. This
would result in a low precision, as a large num-
ber of non-NEs would get mixed with the general
words. Gain and xI measure do not demarcate the
NEs from the general words clearly. We observed
that they are not good scoring functions for NEs.
Information Gain doesn?t consider the fre-
quency of the terms within the document itself. It
only takes into account the document frequency
for each word. xI measure considers the fre-
quency within document but it is highly biased
towards high frequency words and hence doesn?t
perform well. Hence, common words like smy
(samay, ?time?), Gr (ghar, ?home?), etc. have
higher scores compared to NEs like BArta(bharat,
?India?), klk?A (kalkatta, ?Calcutta?), etc. Our
measure on the other hand, overcomes this draw-
back, by considering the ratio. We could have
combined the measures, instead of using only the
best measure ?Residual IDF?, but the performance
of ?Gain?, ?IDF? and ?x?-measure? was not good.
Also, results of ?RF? and ?Residual IDF? were
quite similar. Hence, we did not see any gain in
combining the measures.
6.2 S-MEMM, NGI and CLGIN
The results in Section 5 show that adding the
global information with the local context helps im-
prove the tagging accuracy especially when the
train and test data are from different genre. Sev-
eral times, the local context is not sufficient to
determine the word as a NE. For example, when
the NEs are not followed by post positions or
case markers, it becomes difficult for S-MEMM to
identify NEs, e.g., V{gor ek apvAd h{\, (tagore ek
apvaad hain,?Tagore is an exception?) or when the
NEs are separated by commas, e.g. s  k  mArF d?,
c  ?FlAl.. (Sukumari Dutt, Chunnilal ... ?Suku-
mari Dutt, Chunnilal ..?). In such cases, because
of the frequency statistics, the NGI approach is
able to detect the words V{gor (Tagore, ?Tagore?),
d? (Dutt, ?Dutt?), etc. as NEs and frequently the
CLGIN approach is able to detect such words as
NEs.
The false positives in NEIG are words which
are not present in the lexicon (uncommon words,
words absent due to spelling variations e.g.
sA\p/sA p (sanp ?snake?)) but have high informa-
tiveness. Using the context words of these words
is a possible way of eliminating these false pos-
itives. Many of the organization names having
common words (m\Xl (mandal, ?board?)) and
person names (like ?kAf (prakash,?light?)) are
present in the lexicon are not tagged by NEIG.
Some errors were introduced because of the re-
moval of morphed words. NEs like g  SbAno\, Vop
(Gulbano, Tope) were excluded.
Many of the errors using CLGIN are because of
the presence of the words in the lexicon. This ef-
fect also gets passed on to the neighbouring words.
But, the precision of CLGIN is significantly high
compared to NGI because CLGIN uses context, as
well.
The statistical system (S-MEMM) provides the
context and the global system(NGI) provides a
strong indication that the word is a NE and the
performance of the combined approach(CLGIN)
improves significantly.
7 Conclusion and Future Work
We presented an novel approach for Hindi NEI
which combines the global distributional charac-
teristics with local context. Results show that the
proposed approach improves performance of NEI
significantly, especially, when the train and test
corpus belong to different genres. We also pro-
posed a new measure for NEI which is based on
term informativeness. The proposed measure per-
forms quite competitively with the best known in-
formation measure in literature.
Future direction of the work will be to study
the distributional characteristics of individual tags
and move towards classification of identified enti-
ties. We also plan to extend the above approach
to other Indian languages and other domains. We
also expect further improvements in accuracy by
replacing the MEMM model by CRF. Currently,
we use a tagged corpus as development set to tune
the cut-off threshold in NGI. To overcome this de-
pendence and to make the approach unsupervised,
a way out can be to find an approximation to the
ratio of the number of nouns which are NEs to the
number of nouns and then use this to decide the
cut-off threshold.
Acknowledgments
We would like to acknowledge the efforts of Mr.
Prabhakar Pandey and Mr. Devendra Kairwan for
tagging the data with NE tags.
123
References
Enrique Alfonseca and Suresh Manandhar. 2002. An
Unsupervised Method For General Named Entity
Recognition and Automated Concept Discovery. In
Proceedings of the 1 st International Conference on
General WordNet.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns
What?s In A Name.
A. Bookstein and D. R. Swanson. 1974. Probabilis-
tic Models for Automatic Indexing. Journal of the
American Society for Information Science, 25:312?
318.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Nyu: Description of
the MENE Named Entity System as used in MUC-
7. In In Proceedings of the Seventh Message Under-
standing Conference (MUC-7.
Andrew Eliot Borthwick. 1999. A Maximum En-
tropy Approach to Named Entity Recognition. Ph.D.
thesis, New York, NY, USA. Adviser-Grishman,
Ralph.
Kenneth Church and William Gale. 1995. Inverse
Document Frequency (IDF): A Measure of Devi-
ations from Poisson. In Third Workshop on Very
Large Corpora, pages 121?130.
Kenneth Ward Church and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography.
Chris Clifton, Robert Cooley, and Jason Rennie. 2002.
Topcat: Data mining for Topic Identification in a
Text Corpus.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 100?110.
J.R. Firth. 1957. A Synopsis of Linguistic Theory
1930-1955. In In Studies in Linguistic Analysis,
pages 1?32.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain Adaptation
with Latent Semantic Association for Named Entity
Recognition. In NAACL ?09, pages 281?289, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Karen Sprck Jones. 1972. A Statistical Interpretation
of Term Specificity and its Application in Retrieval.
Journal of Documentation, 28:11?21.
Alexandre Klementiev and Dan Roth. 2006. Named
Entity Transliteration and Discovery from Multi-
lingual Comparable Corpora. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
82?88, Morristown, NJ, USA. Association for Com-
putational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML ?01: Proceedings of the
Eighteenth International Conference on Machine
Learning, pages 282?289, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Wei Li and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition using
Conditional Random Fields and Feature Induction.
ACM Transactions on Asian Language Information
Processing (TALIP), 2(3):290?294.
Wei Li and Andrew Mccallum. 2005. Semi-supervised
Sequence Modeling with Syntactic Topic Models.
In AAAI-05, The Twentieth National Conference on
Artificial Intelligence.
Marius Pas?ca. 2007. Organizing and Searching the
World Wide Web of facts ? Step Two: Harnessing
the Wisdom of the Crowds. In WWW ?07: Proceed-
ings of the 16th international conference on World
Wide Web, pages 101?110, New York, NY, USA.
ACM.
Kishore Papineni. 2001. Why Inverse Document
Frequency? In NAACL ?01: Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies 2001, pages 1?8, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Thierry Poibeau and Leila Kosseim. 2001. Proper
Name Extraction from Non-Journalistic Texts. In In
Computational Linguistics in the Netherlands, pages
144?157.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using
Term Informativeness for Named Entity Detection.
In SIGIR ?05: Proceedings of the 28th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 353?
360, New York, NY, USA. ACM.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. In AAAI ?99/IAAI ?99: Proceedings
of the sixteenth national conference on Artificial in-
telligence and the eleventh Innovative applications
of artificial intelligence conference innovative ap-
plications of artificial intelligence, pages 474?479,
Menlo Park, CA, USA. American Association for
Artificial Intelligence.
Sujan Kumar Saha, Sanjay Chatterji, Sandipan Danda-
pat, Sudeshna Sarkar, and Pabitra Mitra. 2008a. A
Hybrid Named Entity Recognition System for South
and South East Asian Languages. In Proceedings of
the IJCNLP-08 Workshop on Named Entity Recog-
nition for South and South East Asian Languages,
124
pages 17?24, Hyderabad, India, January. Asian Fed-
eration of Natural Language Processing.
Sujan Kumar Saha, Sudeshna Sarkar, and Pabitra Mi-
tra. 2008b. A Hybrid Feature Set Based Maximum
Entropy Hindi Named Entity Recognition. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing, Kharagpur, India.
Rajeev Sangal, Dipti Sharma, and Anil Singh, editors.
2008. Proceedings of the IJCNLP-08 Workshop on
Named Entity Recognition for South and South East
Asian Languages. Asian Federation of Natural Lan-
guage Processing, Hyderabad, India, January.
Johanna Volker. 2005. Towards Large-Scale, Open-
Domain and Ontology-Based Named Entity Classi-
fication. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP?05, pages 166?172. INCOMA Ltd.
125
Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 26?34,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
A Paradigm-Based Finite State Morphological Analyzer for Marathi 
Mugdha Bapat 
 
Harshada Gune 
 
Pushpak Bhattacharyya 
Department of Computer Science and Engineering, 
Indian Institute of Technology Bombay  
 
 {harshadag,mbapat,pb}@cse.iitb.ac.in
 
 
Abstract 
A morphological analyzer forms the 
foundation for many NLP applications of 
Indian Languages. In this paper, we pro-
pose and evaluate the morphological 
analyzer for Marathi, an inflectional lan-
guage. The morphological analyzer ex-
ploits the efficiency and flexibility of-
fered by finite state machines in model-
ing the morphotactics while using the 
well devised system of paradigms to 
handle the stem alternations intelligently 
by exploiting the regularity in inflection-
al forms. We plug the morphological 
analyzer with statistical pos tagger and 
chunker to see its impact on their per-
formance so as to confirm its usability as 
a foundation for NLP applications. 
1 Motivation and Problem Definition 
A highly inflectional language has the capability 
of generating hundreds of words from a single 
root. Hence, morphological analysis is vital for 
high level applications to understand various 
words in the language. Morphological analyzer 
forms the foundation for applications like infor-
mation retrieval, POS tagging, chunking and 
ultimately the machine translation. Morphologi-
cal analyzers for various languages have been 
studied and developed for years. But, this re-
search is dominated by the morphological ana-
lyzers for agglutinative languages or for the lan-
guages like English that show low degree of in-
flection. Though agglutinative languages show 
high morpheme per word ratio and have com-
plex morphotactic structures, the absence of fu-
sion at morpheme boundaries makes the task of 
segmentation fluent once the model for imple-
mentation of morphotactics is ready. On this 
background, a morphological analyzer for highly 
inflectional language like Marathi which has the 
tendency to overlay the morphemes in a way that 
aggravates the task of segmentation presents an 
interesting case study. 
Eryi?it and Adal? (2004) propose a suf-
fix stripping approach for Turkish. The rule 
based and agglutinative nature of Turkish allows 
the language to be modeled using FSMs and 
does not need a lexicon. The morphological ana-
lyzer does not face the problem of the changes 
taking place at morpheme boundaries which is 
not the case with inflectional languages. Hence, 
although apprehensible this model is not suffi-
cient for handling the morphology of Marathi. 
Many morphological analyzers have 
been developed using the two-level morphologi-
cal model (Koskenniemi, 1983) for 
morphological analysis. (Oflazer, 1993; Kim et 
al., 1994) have been developed using PC-
Kimmo (Antworth, 1991), a morphological 
parser based on the two-level model. Concep-
tually, the model segments the word in its con-
stituent parts, and accounts for phonological and 
orthographical changes within a word. While, 
the model proves to be very useful for develop-
ing the morphological analyzers for agglutina-
tive languages or the languages with very less 
degree of inflection, it fails to explicitly capture 
the regularities within and between paradigms 
present in the inflectional languages. Marathi 
has a well defined paradigm-based system of 
inflection. Hence, we decided to develop our 
own model which works on the similar lines of 
PC-Kimmo (Antworth, 1991) but exploits the 
26
usefulness of paradigm-based inflectional sys-
tem. 
Bharati et al (2004) propose a paradigm 
based algorithm for morphological analysis of 
Hindi, an inflecting language. In Hindi, the in-
flected forms of roots do not allow further at-
tachment of any other suffixes. In contrast, in 
Marathi once the root is transformed into its in-
flected form it is followed by suffixes to show 
its agreement with the other words in the sen-
tence. Some postpositions derive new words 
which themselves may undergo inflection and 
allow attachment of other suffixes. This makes 
the simple paradigm-based model proposed in 
this work unfit for Marathi morphological analy-
sis. 
Dixit et al (2006) developed a morphological 
analyzer with a purpose of using it for spell 
checking. Though their analyzer successfully 
analyzes the words with a single suffix, its scope 
is restricted to the handling of only first level 
suffixes. 
1.1 Our Approach 
In this paper, we present the morphological 
analyzer for Marathi which is official language 
of the state of Maharashtra (India). With 90 
million fluent speakers worldwide, Marathi 
ranks as the 4th most spoken language in India 
and the 15th most in the world. The methodology 
is based on the use of paradigm-based 
inflectional system combined with finite state 
machines (FSMs) for modeling the 
morphotactics. To the best of our knowledge, 
such an approach has never been tried out for 
Marathi. The crux of the system lies in the 
detailed study of morphosyntactic phenomena, 
the morphotactic structure of the language and 
the use of paradigm-based inflectional system. 
The approach can be used for other inflection-
al languages by developing the resources like 
language specific inflection rules and the FSM 
that models the morphotactics for the language. 
1.2 Marathi Morphology 
Marathi is a morphologically rich language. It is 
highly inflectional and also shows derivation to 
a high degree. Like other synthetic languages, 
Marathi morphological analysis faces some 
well-known challenges. Words contain multiple 
morphemes fused together in such a way that, it 
becomes difficult to segment them. A single 
morpheme contains a bunch of grammatical 
attributes associated with it which creates a chal-
lenge for morphological parsing. A single root is 
capable of generating hundreds of words by 
combining with the other morphemes. 
The complexity involved in the formation of a 
polymorphemic word can be better illustrated 
using an example. Consider the word 
 {devaasaarakhyaalaa} (to the one 
like the god). The nominal root ? ? {deva} 
(god) gets inflected to the oblique case, singular 
form ? ? {devaa} which is then followed by 
the adjectival suffix ? ? {saarakhaa} (alike). 
This derives the adjective ? ? {devaa-
saarakhaa} (the one like the god) which then 
starts behaving like a noun. This noun on getting 
inflected to the oblique case, singular form 
 {devasaarakhyaa} is followed by the 
case marker  {laa} (to). This gives the word 
 {devaasaarakhyaalaa} (to the one 
like the god). Equation 1 illustrates this process. 
 
 
 
Equation 1. Formation of  {de-
vaasaarakhyaalaa} (to the one like the god) 
 
This suggests that the process of formation of 
polymorphemic words is recursive in nature with 
inflection taking place at every level of recur-
sion. 
Section 2 discusses the design of the morpho-
logical analyzer which tries to overcome the 
problems discussed above with respect to Mara-
thi language. Sections 3 and 4 discuss the lin-
guistic resources and the processing of words 
belonging to various categories respectively. 
Sections 5 and discuss the classification of suf-
fixes and development of automata based on this 
classification respectively. Section 7 briefs on 
the experimental setup and the results. 
27
2 Morphological Analyzer for Marathi 
The formation of polymorphemic words leads to 
complexities which need to be handled during 
the analysis process. FSMs prove to be elegant 
and computationally efficient tools for modeling 
the suffix ordering in such words. However, the 
recursive process of word formation in Marathi 
involves inflection at the time of attachment of 
every new suffix. The FSMs need to be capable 
of handling them. Koskenniemi (1983) suggests 
the use of separate FSMs to model the ortho-
graphic changes. But, Marathi has a well devised 
system of paradigms to handle them. One of our 
observations led us to a solution that combines 
paradigm-based inflectional system with FSM 
for modeling. The observation was that, during 
the ith recursion only (i-1)th morpheme changes 
its form which can be handled by suitably mod-
ifying the FSM. The formation of the same word 
devaasaarakhyaalaa described above can be 
viewed as illustrated in Equation 2. 
 
 
Equation 2. Simulating the formation of 
 {devaasaarakhyaalaa} (to the 
one like the god) 
Generalizing the word formation process we ar-
rived at the formulation specified by Equation 3.  
 
Equation 3. Formulation of Polymorphemic 
Word Formation 
This requires a morphotactic FSM which is 
aware of the inflected forms of morphemes in 
addition to the actual morphemes to handle the 
above recursive process of word formation. We 
use the paradigm-based system to generate the 
inflected form of the morphemes and feed them 
to the FSM. Figure 1 shows the architecture of 
the morphological analyzer based on this philos-
ophy. 
Inflector inflects all morphemes in the lex-
icon using the inflection rules associated with 
the paradigms to which they belong.  
Given a word, Morphological Recognizer 
recognizes the constituent morphemes in their 
inflected forms using finite state machine that 
models the morphotactics. For example, the out-
put of the Morphological Recognizer for the 
word devaasaarakhyaalaa is devaa + saarakhyaa 
+ laa. Morphological Parser outputs per mor-
pheme analysis of the word using the mor-
phemes recognized by the Morphological Re-
cognizer. 
3 Linguistic Resources 
The linguistic resources required by the morpho-
logical analyzer include a lexicon and inflection 
rules for all paradigms. 
3.1 Lexicon 
An entry in lexicon consists of a tuple <root, 
paradigm, category>. The category specifies the 
grammatical category of the root and the para-
digm helps in retrieving the inflection rules as-
sociated with it. Our lexicon contains in all 
24035 roots belonging to different categories. 
 
Figure 1. Architecture of Marathi Morphological Analyzer 
28
3.2 Inflection Rules 
Inflection rules specify the inflectional suffixes 
to be inserted (or deleted) to (or from) different 
positions in the root to get its inflected form. An 
inflectional rule has the format: <inflectional 
suffixes, morphosyntactic features, label>. The 
element morphosyntactic features specifies the 
set of morphosyntactic features associated with 
the inflectional form obtained by applying the 
given inflection rule. Following is the exhaustive 
list of morphosyntactic features to which differ-
ent morphemes get inflected: 
1) Case: Direct, Oblique 
2) Gender: Masculine, Feminine, Neuter, 
Non-specific 
3) Number: Singular, Plural, Non-specific 
4) Person: 1st, 2nd, 3rd 
5) Tense: Past, Present, Future 
6) Aspect: Perfective, Completive, Fre-
quentative, Habitual, Durative, Incep-
tive, Stative 
7) Mood: Imperative, Probabilitive, Sub-
junctive, Conditional, Deontic, Abiltive, 
Permissive 
The label specifies the morphotactic class to 
which the inflected form (generated by applying 
the inflection rule) belongs. It is used by the 
Morphological Recognizer. 
4 Category Wise Morphological For-
mulation 
The grammatical categories observed in Marathi 
include nouns, pronouns, verbs, adjectives, ad-
verbs, conjunctions, interjections and postposi-
tions. The morphemes belonging to different 
categories undergo different treatment. 
4.1 Noun Morphology 
Marathi nouns inflect for number and case. 
Postpositions get attached to the oblique forms 
of the nouns (known as stems) to show their re-
lationship with other words in the sentence. A 
single stem is used for the attachment of all 
postpositions which makes nominal morphology 
absolute economic in nature. For example vari-
ous forms of the word  {daara} (door) are 
 {daaraasa} (to the door),  {daa-
raane} (by the door),  {daaraashejarii} 
(besides the door). Please note that the same 
stem  {daaraa} is used for the attachment of 
various postpositions. 
Depending upon their ending, gender 
and the inflectional patterns, the nouns in Mara-
thi can be classified into various paradigms. A 
paradigm is a complete set of related inflectional 
forms associated with a given root. All words 
that share the similar inflectional forms fall in 
the same paradigm. Table 1 presents the para-
digm  {daara} (door). 
 Case 
 Direct  Oblique 
Number Singular  {daa-
ra} 
 {daaraa} 
Plural  
{daare} 
 {daa-
raaN} 
Table 1. Paradigm Table for  {daara} 
(door) 
 {kaapaDa} (cloth),  {paana} (leaf), 
 {pustaka} (book),  {kapaaTa} (cup-
board) are the few nouns that fall into this para-
digm. 
Every paradigm has a set of inflection rules 
associated with it one corresponding to every 
inflectional form of the word. A noun has four 
inflectional forms each one corresponding to a 
case-number pair. Hence, every paradigm has 
four inflectional rules associated with it. 
An inflectional rule for Marathi consists of a 
tuple specifying the inflectional suffixes that 
should be inserted and deleted from ultimate and 
penultimate position of the root. Table 2 lists the 
inflectional suffixes that collectively form an 
inflectional rule. 
The procedure to obtain the inflected form 
of the given root R belonging to paradigm P by 
applying the inflectional rule I <UD, UI, PUD, 
PUI> is as follows: 
i. R =R - PUD 
ii. R = R + PUI 
iii. R = R ? UD 
iv. R = R + UI 
Suffix Description 
Ultimate 
Deletion 
Suffix to be deleted from the ul-
timate position of the root 
29
(UD) 
Ultimate 
Insertion 
(UI) 
Suffix  to be inserted at the ulti-
mate position of the root 
Penultimate 
Deletion 
(PUD) 
Suffix  to be deleted from the pe-
nultimate position of the root 
Penultimate 
Insertion 
(PUI) 
Suffix  to be inserted at the ulti-
mate position of the root 
Table 2. Suffixes in an Inflectional Rule 
For a given word, even if a single rule out of the 
four is different from the set of available para-
digms, a new paradigm needs to be created. Ta-
ble 3 shows the paradigm  {bhakta} (devo-
tee). Note that, the only difference between the 
two paradigm tables is in the direct case plural 
form. 
 
 Case 
 Direct  Oblique 
Number Singular  
{bhakta} 
   {bhak-
taa} 
Plural 
{bhakta} 
 {bhak-
taaN} 
Table 3. Paradigm Table for  {bhakta} 
(devotee) 
In this way, our lexicon contains 16448 nouns 
categorized into 76 paradigms. Out of the 76 
paradigms, 30 correspond to feminine gender, 29 
to masculine and 17 to neuter gender. This set of 
paradigms includes three null paradigms, one 
corresponding to each gender. In modern Mara-
thi, the stem of the proper nouns or foreign 
words transliterated in Marathi is same as the 
root. In short, postpositions can be directly at-
tached to these roots without any modification. 
Such nouns belong to the null paradigm. 
4.2 Postposition Morphology 
Postpositions follow the stems of nouns and 
pronouns. Postpositions in Marathi can be 
broadly classified into case markers and shab-
dayogi avyayas. Shabdayogi avyayas show the 
relationship of nouns and pronouns with the oth-
er words in the sentence while deriving the ad-
jectives or adverbs in most of the cases. Depend-
ing upon the category of the word derived by 
them they are classified as adjectival and adver-
bial suffixes respectively. We have 142 postpo-
sitions listed in our lexicon.  
4.3 Classification of Postpositions 
The first step towards defining the morphotactics 
of a language is the classification of various suf-
fixes into classes depending upon the mor-
phemes they can follow and the morphemes that 
can follow them. Given the list of 142 postposi-
tions, we carefully examined each one to under-
stand its morphotactic behavior and came up 
with the classification of Marathi postpositions 
as presented in the Table 4. 
 
Class Ordering Rules Ex-
ample 
Case 
markers 
They can follow any ob-
lique form. No other suf-
fixes can follow them. 
 
{ne} 
(by) 
Adjectival 
Suffixes 
They can follow an obli-
que form of a root. Since 
they derive an adjective, 
they can be followed by 
any other suffixes. 
 
{saa-
rak-
haa} 
(alike 
Possessive 
case 
marker 
It can follow any oblique 
form. It can be followed 
by any other suffixes. 
 
{chaa} 
(the 
one 
belong
ing to 
some-
thing) 
Closing 
suffixes 
They can follow any ob-
lique form. No other suf-
fixes can follow them. 
 
{pek-
shaa} 
(in-
stead 
of) 
 {chaa} 
adjectival 
suffix 
It can follow Peculiar 
NSTs and Nearly closing 
postpositions. It can be 
followed by case mark-
ers. 
 
{chaa} 
(the 
one) 
Peculiar They can follow any ob-  
30
NSTs lique form. They can be 
followed only by Exclu-
sive postpositions and  
{chaa} adjectival suffix. 
{ja-
waLa} 
(near) 
Exclusive 
postposi-
tions 
They can follow peculiar 
NSTs. They close the 
word. 
 {ii} 
(in-
side) 
Nearly 
closing 
postposi-
tions 
They can follow oblique 
forms of nouns and pro-
nouns. They can be fol-
lowed by  {chaa} ad-
jectival suffix. 
 
{pa-
ryan-
ta} 
(uptil) 
Shuddha-
shabdayo-
gi avyayas 
They can follow almost 
any morpheme except 
oblique forms of nouns. 
They can be followed by 
some postpositions. But, 
this behavior is quite ir-
regular and needs more 
investigation. In most of 
the cases, these suffixes 
close the word. Hence, 
we consider them to be 
occurring only at the end 
of the word. 
 
{cha} 
(only) 
Table 4. Classification of Postpositions 
4.4 Verbs 
The verbs inflect for gender, number and person 
of the subject and the direct object in a sentence. 
They also inflect for tense and aspect of the ac-
tion as well as mood of the speaker in an illocu-
tionary act. They may even undergo derivation. 
Further discussion on verbal morphology will be 
based on Aakhyaata theory (inflection) and Kru-
danta theory (derivation) (Damale, 1970). Our 
lexicon contains 1160 verb roots classified into 
22 paradigms. 
Aakhyaata Theory forms the basis of 
verbal inflection in Marathi. Aakhyaata  
refers to tense, aspect, and mood. Aakhyaata is 
realized through an aakhyaata suffix which is a 
closing suffix, attached to the verb root. There 
are 8 types of aakhyaatas named after the pho-
nemic shape of the aakhyaata suffix. Associated 
with every aakhyaata are various aakhyaata-
arthas which indicate the features: tense, aspect 
and mood. An aakhyaata may or may not agree 
with gender. There are around 80 Aakhyaata 
suffixes in Marathi. 
Krudanta Theory forms the basis of 
verbal derivation in Marathi. Krudanta refers to 
the word ending in a krut-pratyaya (a suffix 
which refers to an action). Krut-pratyayas are 
attached at the end of verbs to form non-
infinitive verb forms. These forms usually be-
long to one of the categories: noun, adverb or 
adjective. They contribute to the aspect of the 
verb from which they are derived. We cover on-
ly the krudanta forms which are regular in beha-
vior. 
Irregular Verbs: Some verbs in Marathi have 
different behavior as compared to the other 
verbs (regular verbs). These verbs are present in 
some inflected forms for which no definite stem 
exists.  
4.5 Adjectives 
Marathi adjectives can be classified into two 
categories: ones that do not inflect and others 
that inflect for gender, number and case where 
such an inflection agrees with the gender and 
number of the noun modified by them. The in-
flectional forms of the adjectives are generated 
using similar procedure as that of nouns.  
4.6 Pronouns 
There are nine types of pronouns in Marathi. 
Pronouns possess very irregular behavior result-
ing into a large number of suppletive forms. In 
addition to these forms every pronoun has a spe-
cific oblique form (one each for singular and 
plural) to which shabdayogi avyayas can be at-
tached. 
4.7 Indeclinable Words 
Adverbs, conjunctions and interjections are the 
indeclinable words. Some adverbs can be fol-
lowed by a subset of postpositions. 
5 Morphotactics and Automata 
Along with the postpositions mentioned in the 
Table 4 the complete set of morphemes in Mara-
thi includes the roots and their inflectional 
forms. Every morpheme is labeled according to 
the class it belongs to. These labels are used to 
define the ?Morphotactic FSM? that models Ma-
31
rathi language. Table 5 enlists various labels 
used in the Morphotactic FSM. 
 
Type of Suffix Label 
Nouns, pronouns, nominal or 
adjectival krudantas 
DF 1 
OF 2 
Case markers 3 
Adjectival postpositions DF 4 
OF 5 
Possessive case marker DF 6 
OF 7 
Closing postpositions 8 
Peculiar NSTs 9 
Exclusive postpositions 10 
Nearly closing postpositions 11 
 {chaa} adjectival suffix 12 
Adjective 1 
Aakhyaatas 1 
Adverbial krudantas 1 
Adverbs-1  1 
Adverbs-2 13 
Shuddhashabdayogi avyayas 14 
Table 5. Morphotactic Labels of Morphemes 
DF: Direct form of a root or a suffix 
OF: Oblique form of a root or a suffix 
Adverb-1: The adverbs those cannot be followed 
by any postpositions 
Adverb-2: The adverbs those can be followed by 
some postpositions 
Note that, the label field mentioned in the inflec-
tion rules refers to the corresponding labels of 
the morphemes mentioned in Table 5. 
Figure 2 shows the FSM for morphological 
recognition of Marathi. The input symbols are 
the labels of the morphemes as mentioned in the 
Table 5. The classification of the suffixes as 
specified in Table 5 explains the construction of 
FSM. We use SFST 1  (Stuttgart Finite State 
Transducer) for implementing the FSM. 
6 Experiments 
Morphological analysis caters to the needs of 
variety of application like machine translation, 
information retrieval, spell-checking. Different 
applications are interested in different bit of in-
formation provided by the analyzer like the 
stem, the root, the suffixes or the morphosyntac-
tic features. Hence, the performance evaluation 
of a morphological analyzer has to be observed 
in terms of its impact on the performance of the 
applications that use it. Hence, we carry out the 
evaluation in two parts: In direct evaluation we 
directly measure the accuracy of morphological 
analyzer on the given data. In indirect evalua-
tion, we observe the improvement in the per-
formances of statistical pos tagger and chunker 
                                               
1 http://www.ims.uni-
stuttgart.de/projekte/gramotron/SOFTWARE/SFST.html 
Figure 2. Morphotactic FSM 
32
by using the morphological analyzer to generate 
the morphological features that help in boosting 
their accuracies. We used the corpora in TOUR-
ISM and NEWS domain for all our experiments.  
6.1 Direct Evaluation 
We used Marathi Morphological Analyzer for 
the analysis of 21096 unique words. We manual-
ly measured the accuracy of the morphological 
analyzer by counting the number of correctly 
analyzed words out of the total number of 
words. In the cases where a word has multiple 
analyses, the word was counted as correctly ana-
lyzed only when all of the correct analyses are 
present. Note that, in order to emphasize more 
on the usefulness of our approach towards mor-
phological analysis of Marathi, we added most 
of the roots used in the corpus to the lexicon be-
fore starting the experiments. For a language like 
Marathi, it is required to build a very rich lex-
icon which can be done over a larger period of 
time. 
 Out of the 21096 unique words, 20503 
(97.18%) were found to be correctly analyzed. 
Of the remaining 593 words, 394 words could 
not be recognized by Morphological Recognizer 
and 199 words were assigned the incorrect or 
insufficient analyses. 
By taking a closer look at the 394 words 
which were not recognized (segmented) we 
could come up with the causes of recognition 
failure as listed in Table 6.  
 
Cause Number of 
Words 
Lexicon Coverage 82    (20.81%) 
Absence of Rules 69    (17.51%) 
Acronyms 66    (16.75%) 
Compound words  55    (13.96%) 
Irregular forms needing 
further investigation 
47    (11.92%) 
Transliterated words which 
are uncommon 
25    (6.34%) 
Unidentified words 20    (5.08%) 
Dialect words/ words used 
in spoken language 
20    (5.08%) 
Use of common nouns as 
proper nouns  
5     (1.27%) 
Missing Paradigm 3     (0.76%) 
Fusion (Sandhii) 2     (0.51%) 
Table 6. Causes of Recognition Failure 
6.2 Indirect Evaluation 
CRF based sequence labelers (pos tagger 
and chunker) were trained using morpholog-
ical features and the other elementary features 
like (contextual words and bigram tags). The 
morphological features include ambiguity 
scheme (set of all possible categories of a word) 
and the suffixes for the pos tagger whereas just 
the suffixes in case of chunker.  
    To throw the light of role played by morpho-
logical analyzer in improving the accuracies of 
the sequence labelers, we performed the experi-
ments using two sets of features: The Learning 
Based (LB) labeler was trained using only ele-
mentary features whereas Morphologically Dri-
ven Learning Based (MDLB) labeler used the 
morphological features along with the elementa-
ry features. The results were obtained by per-
forming 4-fold cross validation over the corpora. 
The average accuracy of MDLB Pos tagger 
turned out to be 95.03 as compared to 85% of 
LB. The average accuracy of MDLB chunker 
was found to be 97.87% whereas that of LB was 
found to be 96.91%. . 
7 Conclusion and Future Work 
We presented a high accuracy morphological 
analyzer for Marathi that exploits the regularity 
in the inflectional paradigms while employing 
the Finite State Systems for modeling the lan-
guage in an elegant way. The accuracy figures as 
high as 97.18% in direct evaluation and the per-
formance improvement in shallow parsing speak 
about the performance of the morphological ana-
lyzer. We gave detailed description of the mor-
phological phenomena present in Marathi. The 
classification of postpositions and the develop-
ment of morphotactic FSA is one of the impor-
tant contributions since Marathi has complex 
morphotactics. As a next step the morphological 
analyzer can be further extended to handle the 
derivation morphology and compound words.  
 
References 
Antworth, E. L. 1990.  PC-KIMMO: A Two-
level Processor for Morphological Analysis. 
Occasional Publications in Academic Com-
puting. Summer Institute  of  Linguistics,  
Dallas,  Texas. 
 
33
Bharati, Akshar, Vin eet  Chaitanya, and 
Rajeev Sanghal 1995. Natural Language 
Processing: A Paninian Perspective. Pren-
tice Hall, India. 
 
Damale, M. K. 1970. Shastriya Marathii 
Vyaakarana. Deshmukh 
and Company, Pune, India. 
 
Dixit, Veena, Satish Dethe, and Rushikesh 
K. Joshi. 2006. Design and Implementation 
of a Morphology-based Spellchecker for 
Marathi, an Indian Language. 
In Special issue on Human Language Tech-
nologies as a challenge for Computer 
Science 
and Linguistics. Part I. 15, pages 309?316. 
Archives of Control Sciences. 
 
Eryi?it, G?l?en and Adal? E?ref. 2004. An 
Affix Stripping Morphological Analyzer for 
Turkish. In IASTED International Multi-
Conference on Artificial Intelligence and 
Applications. Innsbruck, Austria, pages 
299?304. 
 
 
Kim, Deok-Bong., Sung-Jin Lee, Key-Sun 
Choi, and Gil-Chang Kim (1994). A two-
level Morphological Analysis of Korean. In 
Conference on Computational Linguistics 
(COLING), pages 535?539. 
 
Koskenniemi, Kimmo 1983.  Two-level 
Morphology: a general  computational 
model for word-form recognition and pro-
duction. University of Helsinki, Helsinki. 
 
Oflazer, Kemal 1993.  Two-level Description 
of Turkish Morphology. In The European 
Chapter of the ACL (EACL). 
 
 
 
 
34
Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 51?55,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Hybrid Stemmer for Gujarati 
Pratikkumar Patel     Kashyap Popat 
Department of Computer Engineering 
Dharmsinh Desai University 
pratikpat88@gmail.com 
kan.pop@gmail.com 
Pushpak Bhattacharyya 
Department of Computer Science and 
Engineering 
Indian Institute of Technology Bombay 
pb@cse.iitb.ac.in 
 
Abstract 
In this paper we present a lightweight 
stemmer for Gujarati using a hybrid ap-
proach. Instead of using a completely 
unsupervised approach, we have har-
nessed linguistic knowledge in the form 
of a hand-crafted Gujarati suffix list in 
order to improve the quality of the stems 
and suffixes learnt during the training 
phase. We used the EMILLE corpus for 
training and evaluating the stemmer?s 
performance. The use of hand-crafted 
suffixes boosted the accuracy of our 
stemmer by about 17% and helped us 
achieve an accuracy of 67.86 %. 
1 Introduction 
Stemming is the process of conflating related 
words to a common stem by chopping off the 
inflectional and derivational endings. Stemming 
plays an important role in Information Retrieval 
(IR) systems by reducing the index size and in-
creasing the recall by retrieving results contain-
ing any of the various possible forms of a word 
present in the query. This is especially true in 
case of a morphologically rich language like 
Gujarati, where a single word may take many 
forms. The aim is to ensure that related words 
map to common stem, irrespective of whether or 
not the stem is a meaningful word in the voca-
bulary of the language.   
Current state of the art approaches to stem-
ming can be classified into three categories, viz., 
rule based, unsupervised and hybrid. Building a 
rule based stemmer for a morphologically rich 
language is an uphill task considering the dif-
ferent inflectional and morphological variations 
possible. Purely unsupervised approaches on the 
other hand fail to take advantage of some lan-
guage phenomenon which can be easily ex-
pressed by simple rules. We thus follow a hybr-
id approach by enhancing an unsupervised sys-
tem with a list of hand-crafted Gujarati suffixes. 
The remainder of this paper is organized as 
follows. We describe related work in section 2. 
Section 3 explains the morphological structure 
of Gujarati. We describe our approach in section 
4. The experiments and results are described in 
section 5. Section 6 concludes the paper hig-
hlighting the future work. 
2 Background and Related Work 
The earliest English stemmer was developed by 
Julie Beth Lovins in 1968. The Porter stemming 
algorithm (Martin Porter, 1980), which was 
published later, is perhaps the most widely used 
algorithm for English stemming. Both of these 
stemmers are rule based and are best suited for 
less inflectional languages like English. 
A lot of work has been done in the field of 
unsupervised learning of morphology. 
Goldsmith (2001, 2006) proposed an unsuper-
vised algorithm for learning the morphology of 
a language based on the minimum description 
length (MDL) framework which focuses on 
representing the data in as compact manner as 
possible. Creutz (2005, 2007) uses probabilistic 
maximum a posteriori (MAP) formulation for 
unsupervised morpheme segmentation. 
Not much work has been reported for stem-
ming for Indian languages compared to English 
and other European languages. The earliest 
work reported by Ramanathan and Rao (2003) 
used a hand crafted suffix list and performed 
longest match stripping for building a Hindi 
stemmer. Majumder et al (2007) developed 
YASS: Yet Another Suffix Stripper which uses 
a clustering based approach based on string dis-
51
tance measures and requires no linguistic know-
ledge. They concluded that stemming improves 
recall of IR systems for Indian languages like 
Bengali. Dasgupta and Ng (2007) worked on 
unsupervised morphological parsing for Benga-
li. Pandey and Siddiqui (2008) proposed an un-
supervised stemming algorithm for Hindi based 
on Goldsmith's (2001) approach. 
Unlike previous approaches for Indian lan-
guages which are either rule based or complete-
ly unsupervised, we propose a hybrid approach 
which harnesses linguistic knowledge in the 
form of a hand-crafted suffix list. 
3 Gujarati Morphology 
Gujarati has three genders (masculine, neuter 
and feminine), two numbers (singular and plur-
al) and three cases (nominative, obli-
que/vocative and locative) for nouns. The gend-
er of a noun is determined either by its meaning 
or by its termination. The nouns get inflected on 
the basis of the word ending, number and case. 
The Gujarati adjectives are of two types ? dec-
linable and indeclinable. The declinable adjec-
tives have the termination -? (????) in neuter ab-
solute. The masculine absolute of these adjec-
tives ends in -o (??) and the feminine absolute in 
-? (??). For example, the adjective ???? (s?r? - 
good) takes the form ???? (s?r?), ???? (s?ro) and 
???? (s?r?) when used for a neuter, masculine 
and feminine object respectively. These adjec-
tives agree with the noun they qualify in gender, 
number and case. The adjectives that do not end 
in -? in neuter absolute singular are classified as 
indeclinable and remain unaltered when affixed 
to a noun. 
The Gujarati verbs are inflected based upon a 
combination of gender, number, person, aspect, 
tense and mood. 
There are several postpositions in Gujarati 
which get bound to the nouns or verbs which 
they postposition. e.g. -n? (??? : genitive marker), 
-m?? (??? : in), -e (?? : ergative marker), etc. These 
postpositions get agglutinated to the nouns or 
verbs and not merely follow them. 
We created a list of hand crafted Gujarati suf-
fixes which contains the postpositions and the 
inflectional suffixes for nouns, adjectives and 
verbs for use in our approach. 
4 Our Approach 
Our approach is based on Goldsmith's (2001) 
take-all-splits method. Goldsmith's method was 
purely unsupervised, but we have used a list of 
hand crafted Gujarati suffixes in our approach 
to learn a better set of stems and suffixes during 
the training phase. In our approach, we make 
use of a list of Gujarati words extracted from 
EMILLE corpus for the purpose of learning the 
probable stems and suffixes for Gujarati during 
the training phase. This set of stems and suffix-
es will be used for stemming any word provided 
to the stemmer. We have described the details 
of our approach below. 
4.1 Training Phase 
During the training phase, we try to obtain the 
optimal split position for each word present in 
the Gujarati word list provided for training. We 
obtain the optimal split for any word by taking 
all possible splits of the word (see Figure 1) and 
choosing the split which maximizes the function 
given in Eqn 1 as the optimal split position. The 
suffix corresponding to the optimal split 
position is verified against the list of 59 Gujarati 
suffixes created by us. If it cannot be generated 
by agglutination of the hand crafted suffixes, 
then the length of the word is chosen as the 
optimal split position. i.e. the entire word is 
treated as a stem with no suffix. 
 
 
 
The function used for finding the optimal 
split position reflects the probability of a partic-
ular split since the probability of any split is 
determined by the frequencies of the stem and 
suffix generated by that split. The frequency of 
shorter stems and suffixes is very high when 
compared to the slightly longer ones. Thus the 
multipliers i (length of stemi) and L-i (length of 
suffixi) have been introduced in the function in 
order to compensate for this disparity. 
 
f(i) = i*log(freq(stemi)) + (L-i)*log(freq(suffixi)) 
 
(Eqn 1) 
i: split position (varies from 1 to L) 
L: Length of the word 
Figure 1. All Possible Word Segmentations 
{stem1+suffix1,stem2+suffix2, ... ,stemL+suffixL} 
????= {? + ???, ?? + ??, ??? + ??,???? + NULL} 
52
Once we obtain the optimal split of any word, 
we update the frequencies of the stem and suffix 
generated by that split. We iterate over the word 
list and re-compute the optimal split position 
until the optimal split positions of all the words 
remain unchanged. The training phase was ob-
served to take three iterations typically. 
4.2 Signatures 
After the training phase, we have a list of stems 
and suffixes along with their frequencies. We 
use this list to create signatures. As shown in 
Figure 2, each signature contains a list of stems 
and a list of suffixes appearing with these stems. 
The signatures which contain very few stems 
or very few suffixes may not be useful in stem-
ming of unknown words, thus we eliminate the 
signatures containing at most one stem or at 
most one suffix. The stems and suffixes in the 
remaining signatures will be used to stem new 
words. An overview of the training algorithm is 
shown in Figure 3. 
 
 
4.3 Stemming of any unknown word 
For stemming of any word given to the stemmer, 
we evaluate the function in Eqn 1 for each poss-
ible split using the frequencies of stems and suf-
fixes obtained from the training process. The 
word is stemmed at the position for which the 
value of the function is maximum.  
5 Experiments and Result 
We performed various experiments to evaluate 
the performance of the stemmer using EMILLE 
Corpus for Gujarati. We extracted around ten 
million words from the corpus. These words 
also contained Gujarati transliterations of Eng-
lish words. We tried to filter out these words by 
using a Gujarati to English transliteration engine 
and an English dictionary. We obtained 
8,525,649 words after this filtering process. 
We have used five-fold cross validation for 
evaluating the performance. We divided the ex-
tracted words into five equal parts of which four 
were used for training and one for testing. In 
order to create gold standard data, we extracted 
thousand words from the corpus randomly and 
tagged the ideal stem for these words manually. 
For each of the five test sets, we measured 
the accuracy of stemming the words which are 
present in the test set as well as gold standard 
data. Accuracy is defined as the percentage of 
words stemmed correctly.  
The experiments were aimed at studying the 
impact of (i) using a hand-crafted suffix list, (ii) 
fixing the minimum permissible stem size and 
(iii) provide unequal weightage to the stem and 
suffix for deciding the optimal split position. 
Various results based on these experiments are 
described in the following subsections. 
5.1 Varying Minimum Stem Size 
We varied the minimum stem size from one to 
six and observed its impact on the system per-
formance. We performed the experiment with 
and without using the hand-crafted suffix list. 
The results of this experiment are shown in Ta-
ble 1 and Figure 4. 
The results of this experiment clearly indicate 
that there is a large improvement in the perfor-
mance of the stemmer with the use of hand-
crafted suffixes and the performance degrades if 
we keep a restriction on the minimum stem size. 
For higher values of minimum stem size, all the 
valid stems which are shorter than the minimum 
stem size do not get generated leading to a de-
cline in accuracy. 
Stems Suffixes 
?? ?(pashu - animal) ?? (n?) 
??? (jang - war) ?? (no) 
 ?? (ne) 
 ??? (n?) 
 ?? (n?) 
Figure 2. Sample Signature 
 
Step 1: Obtain the optimal split position for  each 
word in the word list provided for training 
using Eqn 1 and the list of hand crafted suf-
fixes 
 
Step 2: Repeat Step 1 until the optimal split  posi-
tions of all the words remain unchanged 
 
Step 3: Generate signatures using the stems  and 
suffixes generated from the training phase 
 
Step 4: Discard the signatures which contain either 
only one stem or only one suffix 
Figure 3. Overview of training algorithm 
 
53
Min Stem 
Size 
Accuracy 
With hand-
crafted suffixes 
Without hand-
crafted suffix-
es 
1 67.86 % 50.04 % 
2 67.70 % 49.80 % 
3 66.43 % 49.60 % 
4 59.46 % 46.35 % 
5 51.65 % 41.22 % 
6 43.81 % 36.89 % 
 
Table 1. Effect of use of hand-crafted suffixes and 
fixing min. stem size on stemmer?s performance 
 
 
 
Figure 4. Variation stemmer?s accuracy with the var-
iation in min. stem size 
 
There are several spurious suffixes which get 
generated during the training phase and degrade 
the performance of the stemmer when we don?t 
use the hand-crafted suffix list. e.g. ??? is not a 
valid inflectional Gujarati suffix but it does get 
generated if we don?t use the hand-crafted suf-
fix list due to words such as ?????? (anek - many) 
and ????? (ane - and). A simple validation of the 
suffixes generated during training against the 
hand-crafted suffix list leads to learning of bet-
ter suffixes and in turn better stems during the 
training phase thereby improving the system?s 
performance. 
Thus we decided to make use of the hand-
crafted suffix list during training phase and not 
to put any restriction on the minimum stem size. 
5.2 Providing unequal weightage to stem 
and suffix 
We have provided equal weightage to stem and 
suffix in Eqn 1 which is responsible for deter-
mining the optimal split position of any word. 
We obtained Eqn 2 from Eqn 1 by introducing a 
parameter ??? in order to provide unequal 
weightage to the stem and suffix and observe its 
effect on system performance. We used Eqn 2 
instead of Eqn 1 and varied ? from 0.1 to 0.9 in 
this experiment. The results of this experiment 
are shown in Table 2. 
 
 
 
? Accuracy 
0.1 53.52 % 
0.2 61.71 % 
0.3 65.43 % 
0.4 67.30 % 
0.5 67.86 % 
0.6 67.48 % 
0.7 67.49 % 
0.8 67.72 % 
0.9 66.45 % 
Table 2. Effect of ? on the stemmer?s performance 
 
The accuracy was found to be maximum 
when value of ? was fixed to 0.5 i.e. stem and 
suffix were given equal weightage for determin-
ing the optimal split of any word. 
6 Conclusion and Future Work 
We developed a lightweight stemmer for Guja-
rati using a hybrid approach which has an accu-
racy of 67.86 %. We observed that use of a 
hand-crafted Gujarati suffix list boosts the accu-
racy by about 17 %. We also found that fixing 
the minimum stem size and providing unequal 
weightage to stem and suffix degrades the per-
formance of the system. 
Our stemmer is lightweight and removes only 
the inflectional endings as we have developed it 
for use in IR system. The list of hand-crafted 
suffixes can be extended to include derivational 
suffixes for performing full fledged stemming 
which may be required in applications such as 
displaying words in a user interface. 
We have measured the performance of the 
stemmer in terms of accuracy as of now. We 
plan to evaluate the stemmer in terms of the in-
dex compression achieved and the impact on 
precision and recall of Gujarati IR system. 
 
f(i)  =  ? * i * log(freq(stemi)) + 
     (1-?) * (L-i) * log(freq(suffixi)) 
(Eqn 2) 
54
References 
Creutz, Mathis, and Krista Lagus. 2005. Unsuper-
vised morpheme segmentation and morphology 
induction from text corpora using Morfessor 1.0. 
Technical Report A81, Publications in Computer 
and Information Science, Helsinki University of 
Technology. 
Creutz, Mathis, and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and 
morphology learning. Association for Computing 
Machinery Transactions on Speech and Language 
Processing, 4(1):1-34. 
Dasgupta, Sajib, and Vincent Ng. 2006. Unsuper-
vised Morphological Parsing of Bengali. Lan-
guage Resources and Evaluation, 40(3-4):311-
330. 
Goldsmith, John A. 2001. Unsupervised learning of 
the morphology of a natural language. Computa-
tional Linguistics, 27(2):153-198 
Goldsmith, John A. 2006. An algorithm for the un-
supervised learning of morphology. Natural Lan-
guage Engineering, 12(4):353-371 
Jurafsky, Daniel, and James H. Martin. 2009. Speech 
and Language Processing: An Introduction to 
Natural Language Processing, Speech Recogni-
tion, and Computational Linguistics. 2nd edition. 
Prentice-Hall, Englewood Cliffs, NJ. 
Lovins, Julie B. 1968. Development of a stemming 
algorithm. Mechanical Translation and Computa-
tional Linguistics, 11:22-31 
Majumder, Prasenjit, Mandar Mitra, Swapan K. Pa-
rui, Gobinda Kole, Pabitra Mitra, and Kalyanku-
mar Datta. 2007. YASS: Yet another suffix strip-
per. Association for Computing Machinery Trans-
actions on Information Systems, 25(4):18-38. 
Pandey, Amaresh K., and Tanveer J. Siddiqui. 2008. 
An unsupervised Hindi stemmer with heuristic 
improvements. In Proceedings of the Second 
Workshop on Analytics For Noisy Unstructured 
Text Data, 303:99-105. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130-137. 
Ramanathan, Ananthakrishnan, and Durgesh D. Rao, 
A Lightweight Stemmer for Hindi, Workshop on 
Computational Linguistics for South-Asian Lan-
guages, EACL, 2003. 
Tisdall, William St. Clair. 1892. A simplified gram-
mar of the Gujarati language : together with A 
short reading book and vocabulary. D. B. Tarapo-
revala Sons & Company, Bombay. 
The EMILLE Corpus, 
http://www.lancs.ac.uk/fass/projects/corpus/emille/ 
55
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, page 1,
Beijing, August 2010
Word Sense Disambiguation and IR
Pushpak Bhattacharya
Department of Computer Science & Engineering,
Indian Institute of Technology Bombay,
Powai, Mumbai 400076,
India
pb@cse.iitb.ac.in
1
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 70?78,
Beijing, August 2010
More Languages, More MAP?: A Study of Multiple Assisting Languages
in Multilingual PRF
Vishal Vachhani Manoj K. Chinnakotla Mitesh M. Khapra Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{vishalv,manoj,miteshk,pb}@cse.iitb.ac.in
Abstract
Multilingual Pseudo-Relevance Feedback
(MultiPRF) is a framework to improve
the PRF of a source language by taking
the help of another language called as-
sisting language. In this paper, we ex-
tend the MultiPRF framework to include
multiple assisting languages. We consider
three different configurations to incorpo-
rate multiple assisting languages - a) Par-
allel - all assisting languages combined
simultaneously b) Serial - assisting lan-
guages combined in sequence one after
another and c) Selective - dynamically se-
lecting the best feedback model for each
query. We study their effect on MultiPRF
performance. Results using multiple as-
sisting languages are mixed and it helps in
boosting MultiPRF accuracy only in some
cases. We also observe that MultiPRF be-
comes more robust with increase in num-
ber of assisting languages.
1 Introduction
Pseudo-Relevance Feedback (PRF) (Buckley et
al., 1994; Xu and Croft, 2000; Mitra et al, 1998)
is known to be an effective technique to im-
prove the effectiveness of Information Retrieval
(IR) systems. In PRF, the top ?k? documents
from the ranked list retrieved using the initial key-
word query are assumed to be relevant. Later,
these documents are used to refine the user query
and the final ranked list is obtained using the
above refined query. Although PRF has been
shown to improve retrieval, it suffers from the
following drawbacks: (a) Lexical and Semantic
Non-Inclusion: the type of term associations ob-
tained for query expansion is restricted to only
co-occurrence based relationships in the feedback
documents and (b) Lack of Robustness: due to
the inherent assumption in PRF, i.e., relevance
of top k documents, performance is sensitive to
that of the initial retrieval algorithm and as a re-
sult is not robust. Typically, larger coverage en-
sures higher proportion of relevant documents in
the top k retrieval (Hawking et al, 1999). How-
ever, some resource-constrained languages do not
have adequate information coverage in their own
language. For example, languages like Hungarian
and Finnish have meager online content in their
own languages.
Multilingual Pseudo-Relevance Feedback
(MultiPRF) (Chinnakotla et al, 2010a) is a
novel framework for PRF to overcome the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
Thus, the performance of a resource-constrained
language could be improved by harnessing the
good coverage of another language. MulitiPRF
showed significant improvements on standard
CLEF collections (Braschler and Peters, 2004)
over state-of-art PRF system. On the web, each
language has its own exclusive topical coverage
besides sharing a large number of common topics
with other languages. For example, information
about Saudi Arabia government policies and
regulations is more likely to be found in Arabic
language web and also information about a local
event in Spain is more likely to be covered in
Spanish web than in English. Hence, using
multiple languages in conjunction is more likely
to ensure satisfaction of the user information need
and hence will be more robust.
In this paper, we extend the MultiPRF frame-
work to multiple assisting languages. We study
70
the various possible ways of combining the mod-
els learned from multiple assisting languages. We
propose three different configurations for includ-
ing multiple assisting languages in MultiPRF - a)
Parallel b) Serial and c) Selective. In Parallel com-
bination, all the assisting languages are combined
simultaneously using interpolation. In Serial con-
figuration, the assisting languages are applied in
sequence one after another and finally, in Selec-
tive configuration, the best feedback model is dy-
namically chosen for each query. We experiment
with each of the above configurations and present
both quantitative and qualitative analysis of the re-
sults. Results using multiple assisting languages
are mixed and it helps in boosting MultiPRF ac-
curacy only in some cases. We also observe that
MultiPRF becomes more robust with increase in
number of assisting languages. Besides, we also
study the relation between number of assisting
languages, coverage and the MultiPRF accuracy.
The paper is organized as follows: Section 2,
explains the Language Modeling (LM) based PRF
approach. Section 3, describes the MultiPRF ap-
proach. Section 4 explains the various configu-
rations to extend MultiPRF for multiple assisting
languages. Section 6 presents the results and dis-
cussions. Finally, Section 7 concludes the paper.
2 PRF in the LM Framework
The Language Modeling (LM) Framework allows
PRF to be modeled in a principled manner. In the
LM approach, documents and queries are mod-
eled using multinomial distribution over words
called document language model P (w|D) and
query language model P (w|?Q) respectively. For
a given query, the document language models are
ranked based on their proximity to the query lan-
guage model, measured using KL-Divergence.
KL(?Q||D) =
?
w
P (w|?Q) ? log
P (w|?Q)
P (w|D)
Since the query length is short, it is difficult to es-
timate ?Q accurately using the query alone. In
PRF, the top k documents obtained through the
initial ranking algorithm are assumed to be rele-
vant and used as feedback for improving the es-
timation of ?Q. The feedback documents con-
tain both relevant and noisy terms from which
Symbol Description
?Q Query Language Model
?FL1 Feedback Language Model obtained from PRF in L1
?FL2 Feedback Language Model obtained from PRF in L2
?TransL1 Feedback Model Translated from L2 to L1
t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1
?, ? Interpolation coefficients coefficients used in MultiPRF
Table 1: Glossary of Symbols used in explaining MultiPRF
the feedback language model is inferred based on
a Generative Mixture Model (Zhai and Lafferty,
2001).
Let DF = {d1, d2, . . . , dk} be the top k doc-
uments retrieved using the initial ranking algo-
rithm. Zhai and Lafferty (Zhai and Lafferty, 2001)
model the feedback document setDF as a mixture
of two distributions: (a) the feedback language
model and (b) the collection model P (w|C). The
feedback language model is inferred using the EM
Algorithm (Dempster et al, 1977), which itera-
tively accumulates probability mass on the most
distinguishing terms, i.e. terms which are more
frequent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, ?F is interpolated
with the initial query model ?Q to obtain the final
query model ?Final.
?Final = (1? ?) ??Q + ? ??F
?Final is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
to the above technique as Model Based Feedback
(MBF).
3 Multilingual Pseudo-Relevance
Feedback (MultiPRF)
Chinnakotla et al (Chinnakotla et al, 2010a;
Chinnakotla et al, 2010b) propose the MultiPRF
approach which overcomes the fundamental limi-
tations of PRF with the help of an assisting collec-
tion in a different language. Given a query Q in
the source language L1, it is automatically trans-
lated into the assisting language L2. The docu-
ments in the L2 collection are ranked using the
query likelihood ranking function (John Lafferty
and Chengxiang Zhai, 2003). Using the top k doc-
uments, they estimate the feedback model using
MBF as described in the previous section. Simi-
larly, they also estimate a feedback model using
71
the original query and the top k documents re-
trieved from the initial ranking in L1. Let the re-
sultant feedback models be ?FL2 and ?FL1 respec-tively. The feedback model estimated in the as-
sisting language ?FL2 is translated back into lan-guage L1 using a probabilistic bi-lingual dictio-
nary t(f |e) from L2 ? L1 as follows:
P (f |?TransL1 ) =
?
? e in L2
t(f |e) ? P (e|?FL2) (1)
The probabilistic bi-lingual dictionary t(f |e) is
learned from a parallel sentence-aligned corpora
in L1 ? L2 based on word level alignments. The
probabilistic bi-lingual dictionary acts as a rich
source of morphologically and semantically re-
lated feedback terms. Thus, the translation model
adds related terms in L1 which have their source
as the term from feedback model ?FL2 . The finalMultiPRF model is obtained by interpolating the
above translated feedback model with the original
query model and the feedback model of language
L1 as given below:
?MultiL1 = (1? ? ? ?) ??Q + ? ??FL1 + ? ??TransL1(2)
In order to retain the query focus during back
translation, the feedback model in L2 is interpo-
lated with the translated query before translation
of the L2 feedback model. The parameters ? and
? control the relative importance of the original
query model, feedback model of L1 and the trans-
lated feedback model obtained from L1 and are
tuned based on the choice of L1 and L2.
4 Extending MultiPRF to Multiple
Assisting Languages
In this section, we extend the MultiPRF model
described earlier to multiple assisting languages.
Since each language produces a different feed-
back model, there could be different ways of com-
bining these models as suggested below.
Parallel: One way is to include the new assist-
ing language model using one more interpo-
lation coefficient which gives the effect of us-
ing multiple assisting languages in parallel.
Serial: Alternately, we can have a serial combi-
nation wherein language L2 is first assisted
Initial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L  Index
Final Ranked List Of Documents in L
Feedback ModelInterpolation
Relevance ModelTranslation 
KL-Divergence Ranking Function
Feedback Model  ?L1Feedback Model ?L
Query in L Translated Query to L1
ProbabilisticDictionaryL1? LQuery Model ?Q
Translated Query to LnInitial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L1 Index
Relevance ModelTranslation 
Feedback Model  ?Ln
Initial Retrieval
Top ?k?Results
PRF(Model Based Feedback)
LnIndex
ProbabilisticDictionaryLn? L
Figure 1: Schematic of the Multilingual PRF Approach Us-
ing Parallel Assistance
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
KL-Divergence Ranking Function
L Index
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
L2  Index
Relevance ModelTranslation 
L1 Index
Feedback Model    ?L1
Query in L1
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
Feedback Model    ?L2
Query in L2
Top ?k? ResultsPRF(Model Based Feedback) KL Divergence Ranking
ProbabilisticDictionaryL2 ? L1
Relevance ModelTranslation 
ProbabilisticDictionaryL1? L
Feedback Model    ?LQuery Model ?Q
Figure 2: Schematic of the Multilingual PRF Approach Us-
ing Serial Assistance
by language L3 and then this MultiPRF sys-
tem is used to assist the source language L1.
Selective: Finally, we can have selective assis-
tance wherein we dynamically select which
assisting language to use based on the input
query.
Below we describe each of these systems in detail.
4.1 Parallel Combination
The MultiPRF model as explained in section 3 in-
terpolates the query model of L1 with the MBF
of L1 and the translated feedback model of the
assisting language L2. The most natural exten-
sion to this approach is to translate the query into
multiple languages instead of a single language
and collect the feedback terms from the initial re-
72
Language CLEF Collection Identifier Description
No. of 
Documents
No. of Unique 
Terms CLEF Topics (No. of Topics)
English EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
German DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)
Finnish FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
Table 2: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
trieval of each of these languages. The translated
feedback models resulting from each of these re-
trievals can then be interpolated to get the final
parallel MultiPRF model. Specifically, if L1 is the
source language and L2, L3, . . . Ln are assisting
languages then final parallel MultiPRF model can
be obtained by generalizing Equation 2 as shown
below:
?MultiAssistL1 = (1? ? ?
X
i
?i) ??Q + ? ??F +
X
i
?i ??TransLi
(3)
The schematic representation of parallel combina-
tion is shown in Figure 1.
4.2 Serial Combination
Let L1 be the source language and let L2 and L3
be two assisting languages. A serial combination
can then be achieved by cascading two MultiPRF
systems as described below:
1. Construct a MultiPRF system with L2 as
the source language and L3 as the assist-
ing language. We call this system as L2L3-
MultiPRF system.
2. Next, construct a MultiPRF system with L1
as the source language and L2L3-MultiPRF
as the assisting system.
As compared to a single assistance system where
only L2 is used as the assisting language for
L1, here the performance of language L2 is first
boosted using L3 as the assisting language. This
boosted system is then used for assisting L1. Also
note that unlike parallel assistance here we do
not introduce an extra interpolation co-efficient in
the original MultiPRF model given in Equation 2.
The schematic representation of serial combina-
tion is shown in Figure 2.
4.3 Selective Assistance
We motivate selective assistance by posing the
following question: ?Given a source language
L1 and two assisting languages L2 and L3, is
it possible that L2 is ideal for assisting some
queries whereas L3 is ideal for assisting some
other queries?? For example, suppose L2 has a
rich collection of TOURISM documents whereas
L3 has a rich collection of HEALTH documents.
Now, given a query pertaining to TOURISM do-
main one might expect L2 to serve as a better as-
sisting language whereas given a query pertaining
to the HEALTH domain one might expect L3 to
serve as a better assisting language. This intuition
can be captured by suitably changing the interpo-
lation model as shown below:
?BestL = SelectBestModel(?
F
L ,?
Trans
L1 ,?
Trans
L2 ,?
Trans
L12 )
?MultiL1 = (1? ?) ??Q + ? ??
Best
L (4)
where, SelectBestModel() gives the best
model for a particular query using the algorithm
mentioned below which is based on minimizing
the query drift as described in (?):
1. Obtain the four feedback models, viz.,
?FL ,?TransL1 ,?
Trans
L2 ,?
Trans
L12
2. Build a language model (say, LM ) using
queryQ and top-100 documents of initial re-
trieval in language L.
3. Find the KL-Divergence between LM and
the four models obtained during step 1.
4. Select the model which has minimum KL-
Divergence score from LM . Call this model
?BestL .
5. Get the final model by interpolating the
query model, ?Q, with ?BestL .
73
5 Experimental Setup
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Spanish
and Finnish. The details of the collections and
their corresponding topics used for MultiPRF are
given in Table 2. Note that, in each experiment,
we choose assisting collections such that the top-
ics in the source language are covered in the as-
sisting collection so as to get meaningful feedback
terms. In all the topics, we only use the title field.
We ignore the topics which have no relevant docu-
ments as the true performance on those topics can-
not be evaluated.
We use the Terrier IR platform (Ounis et al,
2005) for indexing the documents. We perform
standard tokenization, stop word removal and
stemming. We use the Porter Stemmer for English
and the stemmers available through the Snowball
package for other languages. Other than these,
we do not perform any language-specific process-
ing on the languages. In case of French, since
some function words like l?, d? etc., occur as pre-
fixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@5 and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary
used in MultiPRF was learnt automatically by run-
ning GIZA++: a word alignment tool (Och and
Ney, 2003) on a parallel sentence aligned corpora.
For all the above language pairs we used the Eu-
roparl Corpus (Philipp, 2005). We use Google
Translate as the query translation system as it has
been shown to perform well for the task (Wu et
al., 2008). We use two-stage Dirichlet smooth-
ing with the optimal parameters tuned based on
the collection (Zhai and Lafferty, 2004). We tune
the parameters of MBF, specifically ? and ?, and
choose the values which give the optimal perfor-
mance on a given collection. We observe that the
optimal parameters ? and ? are uniform across
collections and vary in the range 0.4-0.48. We
Source
Langs
Assist.
Langs
MBF MultiPRF
(L1)
MultiPRF
(L2)
MultiPRF
(L1,L2)
EN
DE-NL
MAP 0.4495 0.4464 0.4471 0.4885(4.8)?
P@5 0.4955 0.4925 0.5045 0.5164(2.4)
P@10 0.4328 0.4343 0.4373 0.4463(2.1)
DE-FI
MAP 0.4495 0.4464 0.4545 0.4713(3.7)?
P@5 0.4955 0.4925 0.5194 0.5224(1.2)
P@10 0.4328 0.4343 0.4373 0.4507(3.1)
NL-ES
MAP 0.4495 0.4471 0.4566 0.4757(4.2)?
P@5 0.4955 0.5045 0.5164 0.5224(0.6)
P@10 0.4328 0.4373 0.4537 0.4448(2.4)
ES-FR
MAP 0.4495 0.4566 0.4563 0.48(5.1)?
P@5 0.4955 0.5164 0.5075 0.5224(1.2)
P@10 0.4328 0.4537 0.4343 0.4388(-3.3)
ES-FI
MAP 0.4495 0.4566 0.4545 0.48(5.1)?
P@5 0.4955 0.5164 0.5194 0.5254(1.7)
P@10 0.4328 0.4537 0.4373 0.4403(-3.0)
FR-FI
MAP 0.4495 0.4563 0.4545 0.4774(4.6)
P@5 0.4955 0.5075 0.5194 0.5284(4.1)?
P@10 0.4328 0.4343 0.4373 0.4373(0.7)
FI
EN-FR
MAP 0.3578 0.3411 0.3553 0.3688(3.8)
P@5 0.3821 0.394 0.397 0.4149(4.5)?
P@10 0.3105 0.3463 0.3433 0.3433(0.1)
NL-DE
MAP 0.3578 0.3722 0.3796 0.3929(3.5)
P@5 0.3821 0.406 0.403 0.4149(3.0)
P@10 0.3105 0.3478 0.3582 0.3597(0.4)
ES-DE
MAP 0.3578 0.369 0.3796 0.4058(6.9)?
P@5 0.3821 0.4119 0.403 0.4239(5.2)
P@10 0.3105 0.3448 0.3582 0.3612(0.8)
FR-DE
MAP 0.3578 0.3553 0.3796 0.3988(5.1)?
P@5 0.3821 0.397 0.403 0.406(0.7)
P@10 0.3105 0.3433 0.3582 0.3507(-2.1)
NL-ES
MAP 0.3578 0.3722 0.369 0.3875(4.1)?
P@5 0.3821 0.406 0.4119 0.4060.0)
P@10 0.3105 0.3478 0.3448 0.3537(1.7)
NL-FR
MAP 0.3578 0.3722 0.3553 0.3875(4.1)?
P@5 0.3821 0.406 0.397 0.409(0.7)
P@10 0.3105 0.3478 0.3433 0.3463(-0.4)
ES-FR
MAP 0.3578 0.369 0.3553 0.3823(3.6)
P@5 0.3821 0.4119 0.397 0.4119(0.0)
P@10 0.3105 0.3448 0.3433 0.3418(-0.9)
FR EN-ES
MAP 0.4356 0.4658 0.4634 0.4803(3.1)
P@5 0.4776 0.4925 0.4925 0.4985(1.2)
P@10 0.4194 0.4358 0.4388 0.4493(3.1)?
Table 3: Comparison of MultiPRF Multiple Assisting Lan-
guages using parallel assistance framework with MultiPRF
with single assisting language. Only language pairs where
positive improvements were obtained are reported here. Re-
sults marked as ? indicate that the improvement was sta-
tistically significant over baseline (Maximum of MultiPRF
with single assisting language) at 90% confidence level (? =
0.01) when tested using a paired two-tailed t-test.
uniformly choose the top ten documents for feed-
back.
6 Results and Discussion
Tables ?? and ?? present the results for Multi-
PRF with two assisting languages using paral-
lel assistance and selective assistance framework.
Out of the total 60 possible combinations, in Ta-
ble ??, we only report the combinations where
we have obtained positive improvements greater
than 3%. We observe most improvements in En-
glish, Finnish and French. We did not observe any
improvements using the serial assistance frame-
work over MultiPRF with single assisting lan-
74
Source
Langs
Assist.
Langs
Parallel Model Selective Model
EN DE-NL
MAP 0.4651 0.4848
P@5 0.5254 0.5224
P@10 0.4493 0.4522
NL-FI
MAP 0.4387 0.4502
P@5 0.5015 0.5164
P@10 0.4284 0.4358
DE
EN-FR
MAP 0.4097 0.4302
P@5 0.594 0.5851
P@10 0.5149 0.5179
FR-ES
MAP 0.4215 0.4333
P@5 0.591 0.591
P@10 0.5239 0.5209
FR-NL
MAP 0.4139 0.4236
P@5 0.5701 0.5701
P@10 0.5075 0.5134
FR-FI
MAP 0.3925 0.4055
P@5 0.5101 0.5642
P@10 0.4851 0.5
NL-FI
MAP 0.3974 0.4192
P@5 0.5731 0.5612
P@10 0.497 0.503
ES EN-FI
MAP 0.4436 0.4501
P@5 0.6179 0.6269
P@10 0.5567 0.5657
DE-FI
MAP 0.4542 0.465
P@5 0.6269 0.6179
P@10 0.5627 0.5582
NL-FI
MAP 0.4531 0.4611
P@5 0.6269 0.6299
P@10 0.5627 0.5627
Table 4: Results showing the positive improvements of Mul-
tiPRF with selective assistance framework over MultiPRF
with parallel assistance framework.
guage. Hence, we do not report their results as
the results were almost equivalent to single as-
sisting language. As shown in Table ??, selec-
tive assistance does give decent improvements in
some language pairs. An interesting point to note
in selective assistance is that it helps languages
like Spanish whose monolingual performance and
document coverage are both high.
6.1 Qualitative Comparison of Feedback
Terms using Multiple Languages
In this section, we qualitatively compare the re-
sults of MultiPRF with two assisting languages
with that of MultiPRF with single assisting lan-
guage, based on the top feedback terms obtained
by each model. Specifically, in Table 5 we com-
pare the terms obtained by MultiPRF using (i)
Only L1 as assisting language, (ii) Only L2 as as-
sisting language and (iii) Both L1 and L2 as as-
sisting languages in a parallel combination. For
example, the first row in the above table shows
the terms obtained by each model for the En-
glish query ?Golden Globes 1994?. Here, L1 is
French and L2 is Spanish. Terms like ?Gold?
and ?Prize? appearing in the translated feedback
model of L1 cause a drift in the topic towards
?Gold Prize? resulting in a lower MAP score
(0.33). Similarly, the terms like ?forrest? and
?spielberg? appearing in the translated feedback
model of L2 cause a drift in topic towards For-
rest Gump and Spielberg Oscars resulting in a
MAP score (0.5). However, when the models
from two languages are combined, terms which
cause a topic drift get ranked lower and as a result
the focus of the query is wrenched back. A sim-
ilar observation was made for the English query
?Damages in Ozone Layer? using French (L1)
and Spanish (L2) as assisting languages. Here,
terms from the translated feedback model of L1
cause a drift in topic towards ?militri bacteria?
whereas the terms from the translated feedback
model of L2 cause a drift in topic towards ?iraq
war?. However, in the combined model these
terms get lower rank there by bringing back the
focus of the query. For the Finnish query ?Lasten
oikeudet? (Children?s Rights), in German (L1),
the topic drift is introduced by terms like ?las,
gram, yhteis?. In case of Dutch (L2), the query
drift is caused by ?mandy, richard, slovakia? (L2)
and in the case of combined model, these terms
get less weightage and the relevant terms like
?laps, oikeuks, vanhemp? which are common in
both models, receive higher weightage causing an
improvement in query performance.
Next, we look at a few negative examples where
the parallel combination actually performs poorer
than the individual models. This happens when
some drift-terms (i.e., terms which can cause
topic drift) get mutually reinforced by both the
models. For example, for the German query
?Konkurs der Baring-Bank? (Bankruptcy of Bar-
ing Bank) the term ?share market? which was ac-
tually ranked lower in the individual models gets
boosted in the combined model resulting in a drift
in topic. Similarly, for the German query ?Ehren-
Oscar fu?r italienische Regisseure? (Honorary Os-
car for Italian directors) the term ?head office?
which was actually ranked lower in the individual
models gets ranked higher in the combined model
due to mutual reinforcement resulting in a topic
drift.
75
TOPIC NO.
QUERIES
(Meaning in 
Eng.)
TRANSLATED ENGLISH 
QUERIES 
(Assisting Lang.)
L1 
M AP
L2
M AP
L1 - L2
M AP
Representative Terms with L1 as
Single Assisting Language (With 
M eaning)
Representative Terms with L2 as
Single Assisting Language (With 
Meaning)
Representative Terms with L1& L2 as 
Assisting Langs. (With Meaning)
English ?03 
TOPIC 165 Globes 1994
Golden Globes 1994 (FR)
Globos de Oro 1994 (ES) 0.33 0.5 1
Gold, prize, oscar, nomin, best award, 
hollywood , actor, director ,actress, world, 
won ,list, winner, televi , foreign ,year, press 
world, nomin, film, award, delici, planet, 
earth, actress, list, drama, director, actor, 
spielberg, music, movie, forrest, hank 
oscar, nomin, best, award, hollywood actor, 
director, cinema, televi , music, actress, 
drama, role, hank, foreign, gold
Finnish '03
TOPIC 152
Lasten oikeudet
(Children?s
Rights)
Rechte des Kindes (DE)
Kinderrechten (NL) 0.2 0.25 0.37
laps (child), oikeuks (rights), oikeud (rights),
kind, oikeus (right), is? (father), oikeut
(justify ), vanhemp (parent), vanhem
(parents), las, gram, yhteis , unicef, sunt,
? iti(mother), yleissopimnks (conventions)
oikeuks (rights), laps (child), oikeud (right),
mandy , richard, slovakia , t?h?nast (to date),
tuomar (judge), tyto , kid, , nuor (young
people), nuort (young ), sano(said) , 
perustam(establishing)
laps (child), oikeuks (rights), oikeud (rights),
oikeus (right), is? (father, parent), vanhemp
(parent), vanhem (parents), oikeut (justify),
las, mandy , nuort (young ), richard, nuor
(young people), slovakia , t?h?nast (to date),
English ?03
TOPIC 148
Damages in 
Ozone Layer
Dommages ? la couche 
d'ozone (FR)
Destrucci?n de la capa de 
ozono (ES)
0.08 0.07 0.2 damag, militri, uv , layer, condition, chemic, bacteria, ban, radiat, ultraviolet
damag, weather, atmospher, earth, problem, 
report, research, harm, iraq , war, scandal, 
illigel, latin, hair
damag, uv , layer,weather , atmospher, earth, 
problem, report, research , utraviolet , chemic
German '03
TOPIC 180
Konkurs der
Baring -Bank
(Bankruptcy of 
Baring Bank )
Bankruptcy of Barings (EN)
Baringsin
Konkurssi (FI) 0.55 0.51 0.33
zentralbank(central bank),bankrott(bank 
cruptcy), investitionsbank, sigapur, london , 
britisch, index, tokio, england, 
werbung(advertising), japan
fall, konkurs, bankrott(Bankruptcy), 
warnsignal(warning), ignoriert, 
zusammenbruch (collepse), london, singapur, 
britisch(british), dollar, tokio, druck(pressur), 
handel(trade) 
aktienmarkt(share market), investitionsbank , 
bankrott, zentralbank (central bank), federal, 
singapur, london, britisch, index, tokio, dollar, 
druck, england, dokument(document)
German '03
TOPIC 198
Ehren-Oscar f?r
italienische
Regisseure
(Honorary Oscar 
for Italian 
directors)
Honorary Oscar for Italian 
Directors (EN)
Kunnia -Oscar italialaisille
elokuvaohjaajille (FI)
0.5 0.35 0.2
Direktor(director), film, regierungschef(prime) 
, best antonionis, antonionins, lieb, 
geschicht(history) , paris, preis, berlin, 
monitor, kamera
Generaldirektion(General director), film, 
ehrenmitglied, regisseur, direktor, verleih , 
itali, oscar, award, antonionins
generaldirektion(head office), 
ehrenmitglied(honorable member), 
regierungschef(prime), regisseur(director 
),oscar, genossenschaftsbank (corporate 
bank)
Table 5: Qualitative Comparison of MultiPRF Results using two assisting languages with single assisting language.
6.2 Effect of Coverage on MultiPRF
Accuracy
A study of the results obtained for MultiPRF using
single assisting language and multiple assisting
languages with different source languages showed
that certain languages are more suited to be ben-
efited by assisting languages. In particular, lan-
guages having smaller collections are more likely
to be benefited if assisted by a language having a
larger collection size. For example, Finnish which
has the smallest collection (55344 documents)
showed maximum improvement when supported
by assisting language(s). Based on this observa-
tion, we plotted a graph of the collection size of a
source language v/s the average improvement ob-
tained by using two assisting languages to see if
their exists a correlation between these two fac-
tors. As shown in Figure 3, there indeed exists a
high correlation between these two entities. At
one extreme, we have a language like Spanish
which has the largest collection (454045 docu-
ments) and is not benefited much by assisting lan-
guages. On the other extreme, we have Finnish
which has the smallest collection size and is ben-
efited most by assisting languages.
454.045 (Spanish)
294.809 (German)
190.604 (Dutch) 169.477 (English)
129.806 (French)
55.344 (Finnish)
0
50
100
150
200
250
300
350
400
450
500
0 1 2 3 4 5 6 7
Coverage(No.of Docs in Thousands)
Avg. Improvement in MAP of MultiPRF using two Assisting Languages (%)
Figure 3: Effect of Coverage on Average MultiPRF MAP
using Two Assisting Languages.
6.3 Effect of Number of Assisting Languages
on MultiPRF Accuracy
Another interesting question which needs to be
addressed is ?Whether it helps to use more than
two assisting languages?? and if so ?Is there an
optimum number of assisting languages beyond
which there will be no improvement??. To an-
swer these questions, we performed experiments
using 1-4 assisting languages with each source
language. As seen in Figure 4, in general as the
number of assisting languages increases the per-
formance saturates (typically after 3 languages).
Thus, for 5 out of the 6 source languages, the per-
formance saturates after 3 languages which is in
line with what we would intuitively expect. How-
ever, in the case of German, on an average, the
76
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of. Assisting Langs.
English
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
French
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
Finnish
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
German
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
Dutch
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of Assisting Langs.
Spanish
Avg. MAP
MBF
Figure 4: Effect of Number of Assisting Languages on Avg. MultiPRF Performance with Multiple Assistance.
0
0. 05
0. 1
0. 15
0. 2
0. 25
0. 3
0. 35
0. 4
English French German Spanish Dutch Finnish
Avg
. G
MA
P
Source Language
MBF
1
2
3
4
Figure 5: Effect of Number of Assisting Languages on Ro-
bustness measured through GMAP.
performance drops as the number of assisting lan-
guages is increased. This drop is counter intuitive
and needs further investigation.
6.4 Effect of Number of Assisting Languages
on Robustness
One of the primary motivations for including mul-
tiple assisting languages in MultiPRF was to in-
crease the robustness of retrieval through better
coverage. We varied the number of assisting lan-
guages for each source and studied the average
GMAP. The results are shown in Figure 5. We
observe that in almost all the source languages,
the GMAP value increases with number of assist-
ing languages and then reaches a saturation after
reaching three languages.
7 Conclusion
In this paper, we extended the MultiPRF frame-
work to multiple assisting languages. We pre-
sented three different configurations for including
multiple assisting languages - a) Parallel b) Serial
and c) Selective. We observe that the results are
mixed with parallel and selective assistance show-
ing improvements in some cases. We also observe
that the robustness of MultiPRF increases with
number of assisting languages. We analyzed the
influence of coverage of MultiPRF accuracy and
observed that it is inversely correlated. Finally,
increasing the number of assisting languages in-
creases the MultiPRF accuracy to some extent and
then it saturates beyond that limit. Many of the
above results (negative results of serial, selective
configurations etc.) require deeper investigation
which we plan to take up in future.
References
Braschler, Martin and Carol Peters. 2004. Cross-
language evaluation forum: Objectives, results,
achievements. Inf. Retr., 7(1-2):7?31.
Buckley, Chris, Gerald Salton, James Allan, and Amit
Singhal. 1994. Automatic query expansion using
smart : Trec 3. In Proceedings of The Third Text
REtrieval Conference (TREC-3, pages 69?80.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010a. Multilingual pseudo-
77
relevance feedback: English lends a helping hand.
In ACM SIGIR 2010, Geneva, Switzerland, July.
ACM.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010b. Multilingual pseudo-
relevance feedback: Performance study of assisting
languages. In ACL 2010, Uppsala, Sweeden, July.
ACL.
Dempster, A., N. Laird, and D. Rubin. 1977. Maxi-
mum Likelihood from Incomplete Data via the EM
Algorithm. Journal of the Royal Statistical Society,
39:1?38.
Hawking, David, Paul Thistlewaite, and Donna Har-
man. 1999. Scaling up the trec collection. Inf. Retr.,
1(1-2):115?137.
John Lafferty and Chengxiang Zhai. 2003. Proba-
bilistic Relevance Models Based on Document and
Query Generation. In Language Modeling for Infor-
mation Retrieval, volume 13, pages 1?10. Kluwer
International Series on IR.
Mitra, Mandar, Amit Singhal, and Chris Buckley.
1998. Improving automatic query expansion. In
SIGIR ?98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 206?214,
New York, NY, USA. ACM.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Ounis, I., G. Amati, Plachouras V., B. He, C. Macdon-
ald, and Johnson. 2005. Terrier Information Re-
trieval Platform. In Proceedings of the 27th Euro-
pean Conference on IR Research (ECIR 2005), vol-
ume 3408 of Lecture Notes in Computer Science,
pages 517?519. Springer.
Philipp, Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Robertson, Stephen. 2006. On gmap: and other trans-
formations. In CIKM ?06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78?83, New York,
NY, USA. ACM.
Voorhees, Ellen. 2006. Overview of the trec 2005
robust retrieval track. In E. M. Voorhees and L.
P. Buckland, editors, The Fourteenth Text REtrieval
Conference, TREC 2005, Gaithersburg, MD. NIST.
Wu, Dan, Daqing He, Heng Ji, and Ralph Grishman.
2008. A study of using an out-of-box commercial
mt system for query translation in clir. In iNEWS
?08: Proceeding of the 2nd ACM workshop on Im-
proving non english web searching, pages 71?76,
New York, NY, USA. ACM.
Xu, Jinxi and W. Bruce Croft. 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Trans. Inf. Syst., 18(1):79?112.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based Feedback in the Language Modeling ap-
proach to Information Retrieval. In CIKM ?01: Pro-
ceedings of the tenth international conference on In-
formation and knowledge management, pages 403?
410, New York, NY, USA. ACM Press.
Zhai, Chengxiang and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179?214.
78
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 132?138,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Robust Sense-Based Sentiment Classification
Balamurali A R1 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
The new trend in sentiment classification is
to use semantic features for representation
of documents. We propose a semantic space
based on WordNet senses for a supervised
document-level sentiment classifier. Not only
does this show a better performance for sen-
timent classification, it also opens opportuni-
ties for building a robust sentiment classifier.
We examine the possibility of using similar-
ity metrics defined on WordNet to address the
problem of not finding a sense in the training
corpus. Using three popular similarity met-
rics, we replace unknown synsets in the test
set with a similar synset from the training set.
An improvement of 6.2% is seen with respect
to baseline using this approach.
1 Introduction
Sentiment classification is a task under Sentiment
Analysis (SA) that deals with automatically tagging
text as positive, negative or neutral from the perspec-
tive of the speaker/writer with respect to a topic.
Thus, a sentiment classifier tags the sentence ?The
movie is entertaining and totally worth your money!?
in a movie review as positive with respect to the
movie. On the other hand, a sentence ?The movie is
so boring that I was dozing away through the second
half.? is labeled as negative. Finally, ?The movie is
directed by Nolan? is labeled as neutral. For the pur-
pose of this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Lexeme-based (bag-of-words) features are com-
monly used for supervised sentiment classifica-
tion (Pang and Lee, 2008). In addition to this, there
also has been work that identifies the roles of dif-
ferent parts-of-speech (POS) like adjectives in sen-
timent classification (Pang et al, 2002; Whitelaw et
al., 2005). Complex features based on parse trees
have been explored for modeling high-accuracy po-
larity classifiers (Matsumoto et al, 2005). Text
parsers have also been found to be helpful in mod-
eling valence shifters as features for classifica-
tion (Kennedy and Inkpen, 2006). In general, the
work in the context of supervised SA has focused on
(but not limited to) different combinations of bag-
of-words-based and syntax-based models.
The focus of this work is to represent a document
as a set of sense-based features. We ask the follow-
ing questions in this context:
1. Are WordNet senses better features as com-
pared to words?
2. Can a sentiment classifier be made robust with
respect to features unseen in the training cor-
pus using similarity metrics defined for con-
cepts in WordNet?
We modify the corpus by Ye et al (2009) for the
purpose of our experiments related to sense-based
sentiment classification. To address the first ques-
tion, we show that the approach that uses senses (ei-
ther manually annotated or obtained through auto-
matic WSD techniques) as features performs better
than the one that uses words as features.
Using senses as features allows us to achieve ro-
bustness for sentiment classification by exploiting
the definition of concepts (sense) and hierarchical
structure of WordNet. Hence to address the second
question, we replace a synset not present in the test
set with a similar synset from the training set us-
ing similarity metrics defined on WordNet. Our re-
sults show that replacement of this nature provides a
boost to the classification performance.
The road map for the rest of the paper is as fol-
lows: Section 2 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
132
in section 3. Details about our experiments are de-
scribed in Section 4. In section 5, we present our
results and discussions. We contextualize our work
with respect to other related works in section 6. Fi-
nally, section 7 concludes the paper and points to
future work.
2 WordNet Senses as Features
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This
paper refers to POS category identifier along with
synset offset as synset identifiers or as senses.
2.1 Motivation
We describe three different scenarios to show the
need of sense-based analysis for SA. Consider the
following sentences as the first scenario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity
of the first sentence from the negative sense of ?fell?
in it. This implies that there is at least one sense of
the word ?fell? that carries sentiment and at least one
that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
2.2 Sense versus Lexeme-based Feature
Representations
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on
two subsets of the corpus, the details of which are
given in Section 4.1. The experiments conducted on
this set determine the ideal case scenario- the skyline
performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. A group of word senses that have been manu-
ally annotated (M)
133
2. A group of word senses that have been anno-
tated by an automatic WSD (I)
3. A group of manually annotated word senses
and words (both separately as features) (Sense
+ Words(M))
4. A group of automatically annotated word
senses and words (both separately as features)
(Sense + Words(I))
Our first set of experiments compares the four fea-
ture representations to find the feature representa-
tion with which sentiment classification gives the
best performance. Sense + Words(M) and Sense
+ Words(I) are used to overcome non-coverage of
WordNet for some noun synsets.
3 Similarity Metrics and Unknown Synsets
3.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in
Algorithm 1. The term concept is used in place
of synset though the two essentially mean the
same in this context. The algorithm aims to find a
concept temp concept for each concept in the test
corpus. The temp concept is the concept closest to
some concept in the training corpus based on the
similarity metrics. The algorithm follows from the
fact that the similarity value for a synset with itself
is maximum.
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
replace synset corpus(C,temp concept,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
The for loop over C finds a concept temp concept
in the training corpus with the maximum
similarity value. The method replace synset corpus
replaces the concept C in the test corpus with
temp concept in the test corpus X.
3.2 Similarity Metrics Used
We evaluate the benefit of three similarity metrics,
namely LIN?s similarity metric, Lesk similarity
metric and Leacock and Chodorow (LCH) similarity
metric for the synset replacement algorithm stated.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
134
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B)
(1)
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
(
len(A,B)
2D
)
(2)
4 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
4.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset(20 positive reviews) of the corpus
showed 91% sense overlap. The manually annotated
corpus consists of 34508 words with 6004 synsets.
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjec-
tive POS based synsets can be detrimental to classi-
fication since adjectives are known to express direct
sentiment (Pang et al, 2002).
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
4.2 Experimental Setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are
not stemmed as per observations by (Leopold and
Kindermann, 2002). To conduct the experiments
based on the synset representation, words in the
corpus are annotated with synset identifiers along
with POS category identifiers. For automatic sense
disambiguation, we used the trained IWSD engine
(trained on tourism domain) from Khapra et al
(2010). These synset identifiers along with POS cat-
egory identifiers are then used as features. For re-
placement using semantic similarity measures, we
used WordNet::Similarity 2.05 package by Pedersen
et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
135
Feature Representation Accuracy PF NF PP NP PR NR
Words 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Sense + Words (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Sense + Words(I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%),
PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
score.
5 Results and Discussions
5.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Words).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
Reported results suggest that it is more difficult to
detect negative sentiment than positive sentiment
(Gindl and Liegl, 2008). However, using sense-
based representation, it is important to note that neg-
ative recall increases by around 8%.
The combined model of words and manually an-
notated senses (Sense + Words (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
5.2 Synset replacement using similarity metrics
Table 3 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 3. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach4. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
4Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
136
Features Representa-
tion
SM A PF NF
Words (Baseline) NA 84.90 85.07 84.76
Sense+Words (M) NA 90.20 89.81 90.43
Sense+Words (I) NA 86.08 86.28 85.92
Sense+Words (M) LCH 90.60 90.20 90.85
Sense+Words (M) LIN 90.70 90.26 90.97
Sense+Words (M) Lesk 91.12 90.70 91.38
Sense+Words (I) LCH 85.66 85.85 85.52
Sense+Words (I) LIN 86.16 86.37 86.00
Sense+Words (I) Lesk 86.25 86.41 86.10
Table 3: Similarity Metric Analysis using different
similarity metrics with synsets and a combinations of
synset and words; SM-Similarity Metric, A-Accuracy,
PF-Positive F-score(%), NF-Negative F-score (%)
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated as
LIN or LCH. A good metric which covers all POS
categories can provide substantial improvement in
the classification accuracy.
6 Related Work
This work deals with studying benefit of a word
sense-based feature space to supervised sentiment
classification. This work assumes the hypothesis
that word sense is associated with the sentiment as
shown by Wiebe and Mihalcea (2006) through hu-
man interannotator agreement.
Akkaya et al (2009) and Martn-Wanton et al
(2010) study rule-based sentiment classification us-
ing word senses where Martn-Wanton et al (2010)
uses a combination of sentiment lexical resources.
Instead of a rule-based implementation, our work
leverages on benefits of a statistical learning-based
methods by using a supervised approach. Rentoumi
et al (2009) suggest an approach to use word senses
to detect sentence level polarity using graph-based
similarity. While Rentoumi et al (2009) targets us-
ing senses to handle metaphors in sentences, we deal
with generating a general-purpose classifier.
Carrillo de Albornoz et al (2010) create an emo-
tional intensity classifier using affective class con-
cepts as features. By using WordNet synsets as fea-
tures, we construct feature vectors that map to a
larger sense-based space.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) deal with
sentiment classification of sentences. On the other
hand, we associate sentiment polarity to a document
on the whole as opposed to Pang and Lee (2004)
which deals with sentiment prediction of subjectiv-
ity content only. Carrillo de Albornoz et al (2010)
suggests expansion using WordNet relations which
we perform in our experiments.
7 Conclusion & Future Work
We present an empirical study to show that sense-
based features work better as compared to word-
based features. We show how the performance im-
pact differs for different automatic and manual tech-
niques. We also show the benefit using WordNet
based similarity metrics for replacing unknown fea-
tures in the test set. Our results support the fact that
not only does sense space improve the performance
of a sentiment classification system but also opens
opportunities for building robust sentiment classi-
fiers that can handle unseen synsets.
Incorporation of syntactical information along
with semantics can be an interesting area of
work. Another line of work is in the context of
cross-lingual sentiment analysis. Current solutions
are based on machine translation which is very
resource-intensive. Using a bi-lingual dictionary
which maps WordNet across languages can prove to
be an alternative.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
137
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
138
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 82?87,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
IITB System for CoNLL 2013 Shared Task: A Hybrid Approach to
Grammatical Error Correction
Anoop Kunchukuttan Ritesh Shah Pushpak Bhattacharyya
Department of Computer Science and Engineering, IIT Bombay
{anoopk,ritesh,pb}@cse.iitb.ac.in
Abstract
We describe our grammar correction sys-
tem for the CoNLL-2013 shared task.
Our system corrects three of the five er-
ror types specified for the shared task -
noun-number, determiner and subject-verb
agreement errors. For noun-number and
determiner correction, we apply a classi-
fication approach using rich lexical and
syntactic features. For subject-verb agree-
ment correction, we propose a new rule-
based system which utilizes dependency
parse information and a set of conditional
rules to ensure agreement of the verb
group with its subject. Our system ob-
tained an F-score of 11.03 on the official
test set using the M2 evaluation method
(the official evaluation method).
1 Introduction
Grammatical Error Correction (GEC) is an inter-
esting and challenging problem and the existing
methods that attempt to solve this problem take
recourse to deep linguistic and statistical analy-
sis. In general, GEC may partly assist in solv-
ing natural language processing (NLP) tasks like
Machine Translation, Natural Language Genera-
tion etc. However, a more evident application of
GEC is in building automated grammar checkers
thereby benefiting non-native speakers of a lan-
guage. The CoNLL-2013 shared task (Ng et al,
2013) looks at improving the current approaches
for GEC and for inviting novel perspectives to-
wards solving the same. The shared task makes
the NUCLE corpus (Dahlmeier et al, 2013) avail-
able in the public domain and participants have
been asked to correct grammatical errors belong-
ing to the following categories: noun-number,
determiner, subject-verb agreement (SVA), verb
form and preposition. The key challenges are han-
dling interaction between different error groups
and handling potential mistakes made by off-the-
shelf NLP components run on erroneous text.
For the shared task, we have addressed the fol-
lowing problems: noun-number, determiner and
subject-verb agreement correction. For noun-
number and determiner correction, we use a clas-
sification based approach to predict corrections
- which is a widely used approach (Knight and
Chander, 1994; Rozovskaya and Roth, 2010). For
subject-verb agreement correction, we propose a
new rule-based approach which applies a set of
conditional rules to correct the verb group to en-
sure its agreement with its subject. Our system
obtained a score of 11.03 on the official test set
using the M2 method. Our SVA correction sys-
tem performs very well with a F-score of 28.45 on
the official test set.
Section 2 outlines our approach to solving the
grammar correction problem. Sections 3, 4 and
5 describe the details of the noun-number, deter-
miner and SVA correction components of our sys-
tem. Section 6 explains our experimental setup.
Section 7 discusses the results of the experiments
and Section 8 concludes the report.
2 Problem Formulation
In this work, we focus on correction of three
error categories related to nouns: noun-number,
determiner and subject-verb agreement. The
number of the noun, the choice of determiner and
verb?s agreement in number with the subject are
clearly inter-related. Therefore, a coordinated
approach is necessary to correct these errors. If
these problems are solved independently of each
other, wrong corrections may be generated. The
following are some examples:
Erroneous sentence
A good workmen does not blame his tools
Good corrections
A good workman does not blame his tools
Good workmen do not blame his tools
82
noun-number
subject-verb agreement determiner
Figure 1: Dependencies between the noun-
number, determiner and subject-verb agreement
errors
Bad corrections
A good workman do not blame his tools
Good workman does not blame his tools
The choice of noun-number is determined by
the discourse and meaning of the text. The choice
of determiner is partly determined by the noun-
number, whereas the verb?s agreement depends
completely on the number of its subject. Fig-
ure 1 shows the proposed dependencies between
the number of a noun, its determiner and num-
ber agreement with the verb for which the noun
is the subject. Assuming these dependencies, we
first correct the noun-number. The corrections to
the determiner and the verb?s agreement with the
subject are done taking into consideration the cor-
rected noun. The noun-number and determiner are
corrected using a classification based approach,
whereas the SVA errors are corrected using a rule-
based system; these are described in the following
sections.
3 Noun Number Correction
The major factors which determine the number
of the noun are: (i) the intended meaning of the
text, (ii) reference to the noun earlier in the dis-
course, and (iii) stylistic considerations. Gram-
matical knowledge is insufficient for determining
the noun-number, which requires a higher level of
natural language processing. For instance, con-
sider the following examples:
(1) I bought all the recommended books. These
books are costly.
(2) Books are the best friends of man.
In Example (1), the choice of plural noun in the
second sentence is determined by a reference to
the entity in the previous sentence. Example (2) is
a general statement about a class of entities, where
the noun is generally a plural. Such phenomena
make noun-number correction a difficult task. As
information at semantic and discourse levels is dif-
ficult to encode, we explored lexical and syntactic
Tokens, POS and chunk tags in
?2 word-window around the noun
Is the noun capitalized ?
Is the noun an acronym ?
Is the noun a named entity?
Is the noun a mass noun, pluralia tantum?
Does the noun group have an article/
demonstrative/quantifier?
What article/demonstrative/quantifier does
the noun phrase have ?
Are there words indicating plurality in
the context of the noun?
The first two words of the sentence
and their POS tags
The number of the verb for which this noun
is the subject
Grammatical Number of majority of nouns
in noun phrase conjunction
Table 1: Feature set for noun-number correction
information to obtain cues about the number of the
noun. The following is a summary of the cues we
have investigated:
Noun properties: Is the noun a mass noun, a plu-
ralia tantum, a named entity or an acronym?
Lexical context: The presence of a plurality indi-
cating word in the context of the noun (e.g. the
ancient scriptures such as the Vedas, Upanishads,
etc.)
Syntactic constraints:
? Nouns linked by a conjunction agree with
each other (e.g. The pens, pencils and books).
? Presence/value of the determiner in the noun
group. However, this is only a secondary cue,
since it is not possible to determine if it is the
determiner or the noun-number that is incor-
rect (e.g. A books).
? Agreement with the verb of which the noun is
the subject. This is also a secondary feature.
Given that we are dealing with erroneous text,
these cues could themselves be wrong. The prob-
lem of noun-number correction is one of mak-
ing a prediction based on multiple cues in the
face of such uncertainty. We model the prob-
lem as a binary classification problem, the task
being to predict if the observed noun-number
of every noun in the text needs correction (la-
bels: requires correction/no correction). Alterna-
83
tively, we could formulate the problem as a sin-
gular/plural number prediction problem, which
would not require annotated learner corpora text.
However, we prefer the former approach since we
can learn corrections from learner corpora text (as
opposed to native speaker text) and use knowledge
of the observed number for prediction. Use of ob-
served values has been shown to be beneficial for
grammar correction (Rozovskaya and Roth, 2010;
Dahlmeier and Ng, 2011).
If the model predicts requires correction, then
the observed number is toggled to obtain the cor-
rected noun-number. In order to bias the system
towards improved precision, we apply the correc-
tion only if classifier?s confidence score for the re-
quires correction prediction exceeds its score for
the no correction prediction by at least a threshold
value. This threshold value is determined empiri-
cally. The feature set designed for the classifier is
shown in Table 1.
4 Determiner Correction
Determiners in English consist of articles, demon-
stratives and quantifiers. The choice of deter-
miners, especially articles, depends on many fac-
tors including lexical, syntactic, semantic and dis-
course phenomena (Han et al, 2006). Therefore,
the correct usage of determiners is difficult to mas-
ter for second language learners, who may (i) in-
sert a determiner where it is not required, (ii) omit
a required determiner, or (iii) use the wrong de-
terminer. We pose the determiner correction prob-
lem as a classification problem, which is a well
explored method (Han et al, 2006; Dahlmeier and
Ng, 2011). Every noun group is a training in-
stance, with the determiner as the class label. Ab-
sence of a determiner is indicated by a special
class label NO DET. However, since the number
of determiners is large, a single multi-class classi-
fier will result in ambiguity. This ambiguity can
be reduced by utilizing of the fact that a partic-
ular observed determiner is replaced by one of a
small subset of all possible determiners (which we
call its confusion set). For instance, the confu-
sion set for a is {a, an, the, NO DET}. It is un-
likely that a is replaced by any other determiner
like this, that, etc. Rozovskaya and Roth (2010)
have used this method for training preposition cor-
rection systems, which we adopt for training a de-
terminer correction system. For each observed de-
terminer, we build a classifier whose prediction is
Description Path
1 Direct subject
verb
nounnsubj
verb
nounnsubjpass
2 Path through Wh-determiner
noun
wh-determiner
ref verbrcmodnsubj
3 Clausal subject
verb
nouncsubj
verb
nouncsubjpass
4 External subject
verb_1
nounnsubj
verb_2xsubjtoaux
5 Path through copula
verb
subj_complementcop
nounnsubj
6 Subject in a different clause
verb_1
verb_3 conjconjunctioncc nounnsubj verb_2conj
7 Multiple subjects
noun_1
noun_2 conjnoun_3conj conjunctioncc
verbnsubj
Table 2: Some rules from the singular-
ize verb group rule-set
limited to the confusion set of the observed deter-
miner. The confusion sets were obtained from the
training corpus. The feature set is almost the same
as the one for noun-number correction. The only
difference is that context window features (token,
POS and chunk tags) are taken around the deter-
miner instead of the noun.
5 Subject-Verb Agreement
The task in subject-verb agreement correction is to
correct the verb group components so that it agrees
with its subject. The correction could be made
either to the verb inflection (He run ? He runs)
or to the auxiliary verbs in the verb group (He
are running ? He is running). We assume that
noun-number and verb form errors (tense, aspect,
modality) do not exist or have already been cor-
rected. We built a rule-based system for perform-
ing SVA correction, whose major components are
(i) a system for detecting the subject of a verb, and
84
(ii) a set of conditional rules to correct the verb
group.
We use a POS tagger, constituency parser and
dependency parser for obtaining linguistic infor-
mation (noun-number, noun/verb groups, depen-
dency paths) required for SVA correction. Our as-
sumption is that these NLP tools are reasonably
robust and do a good analysis when presented with
erroneous text. We have used the Stanford suite of
tools for the shared task and found that it makes
few mistakes on the NUCLE corpus text.
The following is our proposed algorithm for
SVA correction:
1. Identify noun groups in a sentence and the in-
formation associated with each noun group:
(i) number of the head noun of the noun
group, (ii) associated noun groups, if the
noun group is part of a noun phrase conjunc-
tion, and (iii) head and modifier in each noun
group pair related by the if relation.
2. Identify the verb groups in a sentence.
3. For every verb group, identify its subject as
described in Section 5.1.
4. If the verb group does not agree in number
with its subject, correct each verb group by
applying the conditional rules described in
Section 5.2.
5.1 Identifying the subject of the verb
We utilize dependency relations (uncollapsed) ob-
tained from the Stanford dependency parser to
identify the subject of a verb. From analysis of de-
pendency graphs of sentences in the NUCLE cor-
pus, we identified different types of dependency
paths between a verb and its subject, which are
shown in Table 2. Given these possible depen-
dency path types, we identify the subject of a verb
using the following procedure:
? First, check if the subject can be reached us-
ing a direct dependency path (paths (1), (2),
(3) and (4))
? If a direct relation is not found, then look for
a subject via path (5)
? If the subject has not been found in the previ-
ous step, then look for a subject via path (6)
A verb can have multiple subjects, which can be
identified via dependency path (7).
Rule Condition Action
1 ?w ? vg, pos tag(w) = MD Do nothing
2 ?w ? vg, pos tag(w) = TO Do nothing
3 subject(vg) 6= I Replace are by is
4 subject(vg) = I Replace are by am
5 do, does /? vg ? subject(vg) 6= I Replace have by has
6 do, does /? vg ? subject(vg) = I Replace has by have
Table 3: Some rules from the singular-
ize verb group rule-set
w is a word, vg is a verb group, POS tags are from the Penn
tagset
5.2 Correcting the verb group
For correcting the verb group, we have two sets of
conditional rules (singularize verb group and plu-
ralize verb group). The singularize verb group
rule-set is applied if the subject is singular,
whereas the pluralize verb group rule-set is ap-
plied if the subject is plural or if there are multi-
ple subjects (path (7) in Table 2). For verbs which
have subjects related via dependency paths (3) and
(4) no correction is done.
The conditional rules utilize POS tags and lem-
mas in the verb group to check if the verb group
needs to be corrected and appropriate rules are ap-
plied for each condition. Some rules in the sin-
gularize verb group rule-set are shown in Table 3.
The rules for the pluralize verb group rule-set are
analogous.
6 Experimental Setup
Our training data came from the NUCLE corpus
provided for the shared task. The corpus was
split into three parts: training set (55151 sen-
tences), threshold tuning set (1000 sentences) and
development test set (1000 sentences). In addi-
tion, evaluation was done on the official test set
(1381 sentences). Maximum Entropy classifiers
were trained for noun-number and determiner cor-
rection systems. In the training set, the number
of instances with no corrections far exceeds the
number of instances with corrections. Therefore,
a balanced training set was created by including
all the instances with corrections and sampling
? instances with no corrections from the training
set. By trial and error, ? was determined to be
10000 for the noun-number and determiner cor-
rection systems. The confidence score threshold
which maximizes the F-score was calibrated on
the tuning set. We determined threshold = 0
85
Task
Development test set Official test set
P R F-1 P R F-1
Noun Number 31.43 40 35.2 28.47 9.84 14.66
Determiner 35.59 17.5 23.46 21.43 1.3 2.46
SVA 16.67 23.42 19.78 29.57 27.42 28.45
Integrated 29.59 17.24 21.79 28.18 4.99 11.03
Table 4: M2 scores for IIT Bombay correction system: component-wise and integrated
for the noun-number and the determiner correction
systems.
The following tools were used in the devel-
opment of the system for the shared task: (i)
NLTK (MaxEntClassifier, Wordnet lemmatizer),
(ii) Stanford tools - POS Tagger, Parser and NER
and Python interface to the Stanford NER, (iii)
Lingua::EN::Inflect module for noun and verb plu-
ralization, and (iv) Wiktionary list of mass nouns,
pluralia tantum.
7 Results and Discussion
Table 4 shows the results on the test set (de-
velopment and official) for each component of
the correction system and the integrated system.
The evaluation was done using the M2 method
(Dahlmeier and Ng, 2012). This involves comput-
ing F1 measure between a set of proposed system
edits and a set of human-annotated gold-standard
edits. However, evaluation is complicated by the
fact that there may be multiple edits which gen-
erate the same correction. The following example
illustrates this behaviour:
Source: I ate mango
Hypothesis: I ate a mango
The system edit is ? a, whereas the gold stan-
dard edit is mango?a mango. Though both the
edits result in the same corrected sentence, they do
not match. The M2 algorithm resolves this prob-
lem by providing an efficient method to detect the
sequence of phrase-level edits between a source
sentence and a system hypothesis that achieves the
highest overlap with the gold-standard annotation.
It is clear that the low recall of the noun-number
and determiner correction components have re-
sulted in a low overall score for the system. This
underscores the difficulty of the two problems.
The feature sets seem to have been unable to cap-
ture the patterns determining the noun-number and
determiner. Consider a few examples, where the
evidence for correction look strong:
1. products such as RFID tracking system have
become real
2. With the installing of the surveillances for
every corner of Singapore
A cursory inspection of the corpus indicates that
in the absence of a determiner (example (1)), the
noun tends to be plural. This pattern has not been
captured by the correction system. The coverage
of the Wiktionary mass noun and pluralia tantum
dictionaries is low, hence this feature has not had
the desired impact (example(2)).
The SVA correction component has a reason-
ably good precision and recall - performing best
amongst all the correction components. Since
most errors affecting agreement (noun-number,
verb form, etc.) were not corrected, the SVA
agreement component could not correct the agree-
ment errors. If these errors had been corrected, the
accuracy of the standalone SVA correction com-
ponent would have been higher than that indicated
by the official score. To verify this, we manually
analyzed the output from the SVA correction com-
ponent and found that 58% of the missed correc-
tions and 43% of the erroneous corrections would
not have occurred if some of the other related er-
rors had been fixed. If it is assumed that all these
errors are corrected, the effective accuracy of SVA
correction increases substantially as shown in Ta-
ble 5. A few errors in the gold standard for SVA
agreement were also considered for computing the
effective scores. The standalone SVA correction
module therefore has a good accuracy.
A major reason for SVA errors (?18%) is
wrong output from NLP modules like the POS tag-
ger, chunker and parser. The following are a few
examples:
? The verb group is incorrectly identified if
there is an adverb between the main and aux-
iliary verbs.
It [do not only restrict] their freedom in all
86
SVA Score
Development test set Official test set
P R F-1 P R F-1
Official 16.67 23.42 19.78 29.57 27.42 28.45
Effective 51.02 55.55 53.18 65.32 66.94 66.12
Table 5: M2 scores (original and modified) for SVA correction
aspects , but also causes leakage of personal
information .
? Two adjacent verb groups are not distin-
guished as separate chunks by the chunker
when the second verb group is non-finite in-
volving an infinitive.
The police arrested all of them before they
[starts to harm] the poor victim.
? The dependency parser makes errors in iden-
tifying the subject of a verb. The noun prob-
lems is not identified as the subject of is by
the dependency parser.
Although rising of life expectancies is an
challenge to the entire human nation , the
detailed problems each country that will en-
counter is different.
Some phenomena have not been handled by our
rules. Our system does not handle the case where
the subject is a gerund phrase. Consider the exam-
ple,
Collecting coupons from individuals are the first
step.
The verb-number should be singular when a
gerund phrase is the subject. In the absence of
rules to handle this case, coupons is identified as
the subject of are by the dependency parser and
consequently, no correction is done.
Our rules do not handle interrogative sentences
and interrogative pronouns. Hence the following
sentence is not corrected,
People do not know who are tracking them.
Table 6 provides an analysis of the error type
distribution for SVA errors on the official test set.
8 Conclusion
In this paper, we presented a hybrid grammati-
cal correction system which incorporates both ma-
chine learning and rule-based components. We
proposed a new rule-based method for subject-
verb agreement correction. As future work, we
plan to explore richer features for noun-number
and determiner errors.
Error types % distribution
Noun-number errors 58.02 %
Wrong tagging, chunking, parsing 18.52 %
Wrong gold annotations 7.40%
Rules not designed 6.1%
Others 9.88 %
Table 6: Causes for missed SVA corrections and
their distribution in the official test set
References
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies-Volume 1.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS Corpus of Learner English. In To
appear in Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in english article usage by
non-native speakers. Natural Language Engineer-
ing.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In To appear in Proceedings of the Seventeenth Con-
ference on Computational Natural Language Learn-
ing.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
87
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 60?64,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
Tuning a Grammar Correction System for Increased Precision
Anoop Kunchukuttan
?
, Sriram Chaudhury
?
, Pushpak Bhattacharyya
?
?
Department of Computer Science and Engineering, IIT Bombay, India
{anoopk,pb}@cse.iitb.ac.in
?
Crimson Interactive Pvt. Limited, Mumbai, India
Sriram.Chaudhury@crimsoni.com
Abstract
In this paper, we propose two enhance-
ments to a statistical machine translation
based approach to grammar correction for
correcting all error categories. First, we
propose tuning the SMT systems to op-
timize a metric more suited to the gram-
mar correction task (F-? score) rather than
the traditional BLEU metric used for tun-
ing language translation tasks. Since the
F-? score favours higher precision, tun-
ing to this score can potentially improve
precision. While the results do not indi-
cate improvement due to tuning with the
new metric, we believe this could be due
to the small number of grammatical er-
rors in the tuning corpus and further in-
vestigation is required to answer the ques-
tion conclusively. We also explore the
combination of custom-engineered gram-
mar correction techniques, which are tar-
geted to specific error categories, with the
SMT based method. Our simple ensem-
ble methods yield improvements in recall
but decrease the precision. Tuning the
custom-built techniques can help in in-
creasing the overall accuracy also.
1 Introduction
Grammatical Error Correction (GEC) is an inter-
esting and challenging problem and the existing
methods that attempt to solve this problem take
recourse to deep linguistic and statistical analy-
sis. In general, GEC may partly assist in solv-
ing natural language processing (NLP) tasks like
Machine Translation, Natural Language Genera-
tion etc. However, a more evident application of
GEC is in building automated grammar checkers
thereby non-native speakers of a language. The
goal is to have automated tools to help non-native
speakers to generate good content by correcting
grammatical errors made by them.
The CoNLL-2013 Shared Task (Ng et al., 2013)
was focussed towards correcting some of the most
frequent categories of grammatical errors. In con-
trast, the CoNLL-2014 Shared Task (Ng et al.,
2014) set the goal of correcting all grammatical
errors in the text. For correcting specific error
categories, custom methods are generally devel-
oped, which exploit deep knowledge of the prob-
lem to perform the correction (Han et al., 2006;
Kunchukuttan et al., 2013; De Felice and Pulman,
2008). These methods are generally the state-of-
the-art for the concerned error categories, but a lot
of engineering and research effort is required for
correcting each error category. So, the custom de-
velopment approach is infeasible for correcting a
large number of error categories.
Hence, for correction of all the error categories,
generic methods have been investigated - gen-
erally using language models or statistical ma-
chine translation (SMT) systems. The language
model based method (Lee and Seneff, 2006; Kao
et al., 2013) scores sentences based on a lan-
guage model or count ratios of n-grams obtained
from a large native text corpus. But this method
still needs a candidate generation mechanism for
each error category. On the other hand, the SMT
based method (Brockett et al., 2006) formulates
the grammar correction problem as a problem of
translation of incorrect sentences to correct sen-
tences. SMT provides a natural unsupervised
method for identifying candidate corrections in
the form of the translation model, and a method
for scoring them with a variety of measures in-
cluding the language model score. However, the
SMT method requires a lot of parallel non-native
learner corpora. In addition, the machinery in
phrase based SMT is optimized towards solving
the language translation problem. Therefore, the
community has explored approaches to adapt the
60
SMT method for grammar correction (Buys and
van der Merwe, 2013; Yuan and Felice, 2013).
These include use of factored SMT, syntax based
SMT, pruning of the phrase table, disabling or re-
ordering, etc. The generic SMT approach has per-
formed badly as compared to the specific custom
made approaches (Yuan and Felice, 2013).
Our system also builds upon the SMT methods
and tries to address the above mentioned lacunae
in two ways:
? Tuning the SMT model to a metric suitable
for grammar correction (i.e.F-? metric), in-
stead of the BLEU metric.
? Combination of custom-engineered methods
and SMT based methods, by using classifier
based for some error categories.
Section 2 describes our method for tuning the
SMT system to optimize the F-? metric. Sec-
tion 3 explains the combination of classifier based
method with the SMT method. Section 4 lists our
experimental setup. Section 5 analyzes the results
of our experiments.
2 Tuning SMT system for F-? score
We model our grammar correction system as a
phrase based SMT system which translates gram-
matically incorrect sentences to grammatically
correct sentences. The phrase based SMT system
selects the best translation for a source sentence by
searching for a candidate translation which maxi-
mizes the score defined by the maximum entropy
model for phrase based SMT defined below:
P (e,a|f) = exp
?
i
?
i
h
i
(e,a, f)
where,
h
i
: feature function for the i
th
feature. These are
generally features like the phrase/lexical transla-
tion probability, language model score, etc.
?
i
: the weight parameter for the i
th
feature.
The weight parameters (?
i
) define the relative
weights given to each feature. These parame-
ter weights are learnt during a process referred to
as tuning. During tuning, a search over the pa-
rameter space is done to identify the parameter
values which maximize a measure of translation
quality over a held-out dataset (referred to as the
tuning set). One of the most widely used met-
rics for tuning is the BLEU score (Papineni et
al., 2002), tuned using the Minimum Error Rate
Training (MERT) algorithm (Och, 2003). Since
BLEU is a form of weighted precision, along with
a brevity penalty to factor in recall, it is suitable
in the language translation scenario, where fidelity
of the translation is an important in evaluation of
the translation. Tuning to BLEU ensures that the
parameter weights are set such that the fidelity of
translations is high.
However, ensuring fidelity is not the major chal-
lenge in grammar correction since the meaning of
most input sentences is clear and most don?t have
any grammatical errors. The metric to be tuned
must ensure that weights are learnt such that the
features most relevant to correcting the grammar
errors are given due importance and that the tun-
ing focuses on the grammatically incorrect parts
of the sentences. The F-? score, as defined for
the CoNLL shared task, is the most obvious metric
to measure the accuracy of grammar correction on
the tuning set. We choose the F-? metric as a score
to be optimized using MERT for the SMT based
grammar correction model. By choosing an appro-
priate value of ?, it is possible to tune the system
to favour increased recall/precision or a balance of
both.
3 Integrating SMT based and
error-category specific systems
As discussed in Section 1, the generic SMT based
correction based systems are inferior in their cor-
rection capabilities compared to the error-category
specific correction systems which have been cus-
tom engineered for the task. A reasonable solution
to make optimum use of both the approaches is to
develop custom modules for correcting high im-
pact and the most frequent error categories, while
relying on the SMT method for correcting other
error categories. We experiment with two ap-
proaches for integrating the SMT based and error-
category specific systems, and compare both with
the baseline SMT approach:
? Correct all error categories using the SMT
method, followed by correction using the
custom modules.
? Correct only the error categories not han-
dled by the custom modules using the SMT
method, followed by correction using the
custom modules.
61
The error categories for which we built cus-
tom modules are noun number, determiner and
subject-verb agreement (SVA) errors. These er-
rors are amongst the most common errors made
by non-native speakers. The noun number and
determiner errors are corrected using the classifi-
cation model proposed by Rozovskaya and Roth
(2013), where the label space is a cross-product
of the label spaces of the possible noun number
and determiners. We use the feature-set proposed
by Kunchukuttan et al. (2013). SVA correction
is done using a prioritized, conditional rule based
system described by Kunchukuttan et al. (2013).
4 Experimental Setup
We used the NUCLE Corpus v3.1 to build a
phrase based SMT system for grammar correction.
The NUCLE Corpus contains 28 error categories,
whose details are documented in Dahlmeier et al.
(2013). We split the corpus into training, tuning
and test sets are shown in Table 1.
Set Document Count Sentence Count
train 1330 54284
tune 20 854
test 47 2013
Table 1: Details of data split for SMT training
The phrase based system was trained using
the Moses
1
system, with the grow-diag-final-
and heuristic for extracting phrases and the msd-
bidirectional-fe model for lexicalized reordering.
We tuned the trained models using Minimum Er-
ror Rate Training (MERT) with default parame-
ters (100 best list, max 25 iterations). Instead of
BLEU, the tuning metric was the F-0.5 metric. We
trained 5-gram language models on all the sen-
tences from NUCLE corpus using the Kneser-Ney
smoothing algorithm with SRILM
2
.
The classifier for noun number and article cor-
rection is a Maximum Entropy model trained
on the NUCLE v2.2 corpus using the MALLET
toolkit. Details about the resources and tools
used for feature extraction are documented in
Kunchukuttan et al. (2013).
1
http://www.statmt.org/moses/
2
http://goo.gl/4wfLVw
5 Results and Analysis
Table 2 shows the results on the development set
for different experimental configurations gener-
ated by varying the tuning metrics, and the method
of combining the SMT model and custom correc-
tion modules. Table 3 shows the same results on
the official CoNLL 2014 dataset without alterna-
tive answers.
5.1 Effect of tuning with F-0.5 score
We observe that both precision and recall drop
sharply when the SMT model is tuned with the
F-0.5 metric (system S2), as compared to tuning
with the traditional BLEU metric (system S1). We
observe that system S2 proposes very few correc-
tions (82) as compared to system S1 (188), which
contributes to the low recall of system S2. There
are very few errors in the tuning set (202) which
may not be sufficient to reliably tune the system
to the F-0.5 score. It would be worth investigating
the effect of number of errors in the tuning set on
the accuracy of the system.
5.2 Effect of integrating the SMT and custom
modules
Comparing the results of systems S1, S3 and S5, it
is clear that using the SMT method alone gives the
highest F-0.5 score. However, the recall is higher
for systems which use the custom modules for
some error categories. The recall is highest when
custom modules as well as SMT method are used
for the high impact error categories. The above
observation is a consequence of the fact that the
custom modules have higher recall for certain er-
ror categories compared to the SMT method. The
lower precision of custom modules is due to the
large number of false positives. If the custom
modules are optimized for higher precision, then
the overall ensemble can also achieve higher pre-
cision and consequently higher F-0.5 score. Thus,
the integration of SMT method and custom mod-
ules can be beneficial in improving the overall ac-
curacy of the SMT system.
6 Conclusion
We explored two approaches to adapting the SMT
method for the problem of grammatical correc-
tion. Tuning the SMT system to the F-? metric did
not improve performance over the BLEU-based
tuning. However, we plan to further investigate
to understand the reasons for this behaviour. We
62
Id SMT Data Custom Modules Tuning Metric %P %R %F-0.5
S1
All errors
No BLEU 62.23 11.53 33.12
S2 No F-0.5 55.32 5.13 18.71
S3 Yes BLEU 10.99 26.33 12.44
S4 Yes F-0.5 9.80 22.98 11.07
S5 All errors, except Nn,
ArtOrDet, SVA
Yes BLEU 10.15 23.96 11.47
Table 2: Experimental Results for various configurations on the development set
Id SMT Data Custom Modules Tuning Metric %P %R %F-0.5
S1
All errors
No BLEU 38.81 4.15 14.53
S2 No F-0.5 30.77 1.39 5.90
S3 Yes BLEU 29.02 17.98 25.85
S4 Yes F-0.5 28.23 16.72 24.81
S5 All errors, except Nn,
ArtOrDet, SVA
Yes BLEU 28.67 17.29 25.34
Table 3: Experimental Results for various configurations on the CoNLL-2014 test set without alternatives
also plan to explore tuning for recall and other al-
ternative metrics which could be useful in some
scenarios. An ensemble of the SMT method and
custom methods for some high impact error cate-
gories was shown to increase the recall of the sys-
tem, and with proper optimization of the system
can also improve the overall accuracy of the cor-
rection system.
References
Chris Brockett, William B Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
SMT techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics.
Jan Buys and Brink van der Merwe. 2013. A Tree
Transducer Model for Grammatical Error Correc-
tion. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS Corpus of Learner English. In To
appear in Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations.
Rachele De Felice and Stephen G Pulman. 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics-Volume 1.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing.
Ting-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-
Hsi Yen, Joanne Boisson, Jian-cheng Wu, and Ja-
son S. Chang. 2013. CoNLL-2013 Shared Task:
Grammatical Error Correction NTHU System De-
scription. In Proceedings of the Seventeenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-
tacharyya. 2013. IITB System for CoNLL 2013
Shared Task: A Hybrid Approach to Grammati-
cal Error Correction. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning.
J. Lee and S. Seneff. 2006. Automatic grammar cor-
rection for second-language learners. In Proceed-
ings of Interspeech, pages 1978?1981.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
63
the Eighteenth Conference on Computational Natu-
ral Language Learning.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics.
A. Rozovskaya and D. Roth. 2013. Joint Learning
and Inference for Grammatical Error Correction. In
EMNLP.
Zheng Yuan and Mariano Felice. 2013. Constrained
Grammatical Error Correction using Statistical Ma-
chine Translation. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
64
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 113?118,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Dive deeper: Deep Semantics for Sentiment Analysis
Nikhikumar Jadhav
Masters Student
Computer Science & Engineering Dept.
IIT Bombay
nikhilkumar@cse.iitb.ac.in
Pushpak Bhattacharyya
Professor
Computer Science & Engineering Dept.
IIT Bombay
pb@cse.iitb.ac.in
Abstract
This paper illustrates the use of deep se-
mantic processing for sentiment analysis.
Existing methods for sentiment analysis
use supervised approaches which take into
account all the subjective words and or
phrases. Due to this, the fact that not all
of these words and phrases actually con-
tribute to the overall sentiment of the text
is ignored. We propose an unsupervised
rule-based approach using deep semantic
processing to identify only relevant sub-
jective terms. We generate a UNL (Uni-
versal Networking Language) graph for
the input text. Rules are applied on the
graph to extract relevant terms. The sen-
timent expressed in these terms is used to
figure out the overall sentiment of the text.
Results on binary sentiment classification
have shown promising results.
1 Introduction
Many works in sentiment analysis try to make use
of shallow processing techniques. The common
thing in all these works is that they merely try to
identify sentiment-bearing expressions as shown
by Ruppenhofer and Rehbein (2012). No effort
has been made to identify which expression actu-
ally contributes to the overall sentiment of the text.
In Mukherjee and Bhattacharyya (2012) these ex-
pressions are given weight-age according to their
position w.r.t. the discourse elements in the text.
But it still takes into account each expression.
Semantic analysis is essential to understand the
exact meaning conveyed in the text. Some words
tend to mislead the meaning of a given piece of text
as shown in the previous example. WSD (Word
Sense Disambiguation) is a technique which can
been used to get the right sense of the word. Bal-
amurali et al., (2012) have made use of Word-
Net synsets for a supervised sentiment classifica-
tion task. Tamare (2010) and Rentoumi (2009)
have also shown a performance improvement by
using WSD as compared to word-based features
for a supervised sentiment classification task. In
Hasan et al., (2012), semantic concepts have been
used as additional features in addition to word-
based features to show a performance improve-
ment. Syntagmatic or structural properties of
text are used in many NLP applications like ma-
chine translation, speech recognition, named en-
tity recognition, etc. A clustering based approach
which makes use of syntactic features of text has
been shown to improve performance in Kashyap
et al., (2013). Another approach can be found
in Mukherjee and Bhattacharyya (2012) which
makes use of lightweight discourse for sentiment
analysis. In general, approaches using seman-
tic analysis are expensive than syntax-based ap-
proaches due to the shallow processing involved
in the latter. As pointed out earlier, all these works
incorporate all the sentiment-bearing expressions
to evaluate the overall sentiment of the text. The
fact that not all expressions contribute to the over-
all sentiment is completely ignored due to this.
Our approach tries to resolve this issue. To do this,
we create a UNL graph for each piece of text and
include only the relevant expressions to predict the
sentiment. Relevant expressions are those which
satisfy the rules/conditions. After getting these ex-
pressions, we use a simple dictionary lookup along
with attributes of words in a UNL graph to calcu-
late the sentiment.
The rest of the paper is organized as follows.
Section 2 discusses related work. Section 3 ex-
plains our approach in detail. The experimental
setup is explained in Section 4. Results of the ex-
periments are presented in Section 5. Section 6
discusses these results followed by conclusion in
Section 7. Section 8 hints at some future work.
113
2 Related Work
There has been a lot of work on using semantics
in sentiment analysis. Hasan et al., (2012) have
made use of semantic concepts as additional fea-
tures in a word-based supervised sentiment classi-
fier. Each entity is treated as a semantic concept
e.g. iPhone, Apple, Microsoft, MacBook, iPad,
etc.. Using these concepts as features, they try to
measure their correlation with positive and nega-
tive sentiments. In Verma et al., (2009), effort has
been made to construct document feature vectors
that are sentiment-sensitive and use world knowl-
edge. This has been achieved by incorporating
sentiment-bearing words as features in document
vectors. The use of WordNet synsets is found in
Balamurali et al., (2012), Rentoumi (2009) and
Tamara (2010). The one thing common with these
approaches is that they make use of shallow se-
mantics.An argument has been made in Choi and
Carde (2008) for determining the polarity of a
sentiment-bearing expression that words or con-
stituents within the expression can interact with
each other to yield a particular overall polarity.
Structural inference motivated by compositional
semantics has been used in this work. This work
shows use of deep semantic information for the
task of sentiment classification. A novel use of
semantic frames is found in Ruppenhofer and Re-
hbein (2012). As a step towards making use
of deep semantics, they propose SentiFrameNet
which is an extension to FrameNet. A semantic
frame can be thought of as a conceptual struc-
ture describing an event, relation, or object and
the participants in it. It has been shown that po-
tential and relevant sentiment bearing expressions
can be easily pulled out from the sentence using
the SentiFrameNet. All these works try to bridge
the gap between rule-based and machine-learning
based approaches but except the work in Ruppen-
hofer and Rehbein (2012), all the other approaches
consider all the sentiment-bearing expressions in
the text.
3 Use of Deep Semantics
Before devising any solution to a problem, it is ad-
visable to have a concise definition of the prob-
lem. Let us look at the formal definition of the
sentiment analysis problem as given in Liu (2010).
Before we do that, let us consider the following
review for a movie, ?1) I went to watch the new
James Bond flick, Skyfall at IMAX which is the
best theater in Mumbai with my brother a month
ago. 2) I really liked the seating arrangement over
there. 3) The screenplay was superb and kept me
guessing till the end. 4) My brother doesnt like the
hospitality in the theater even now. 5) The movie
is really good and the best bond flick ever.? This is
a snippet of the review for a movie named Skyfall .
There are many entities and opinions expressed in
it. 1) is an objective statement. 2) is subjective but
is intended for the theater and not the movie. 3) is
a positive statement about the screenplay which is
an important aspect of the movie. 4) is a subjective
statement but is made by the authors brother and
also it is about the hospitality in the theater and
not the movie or any of its aspects. 5) reflects a
positive view of the movie for the author. We can
see from this example that not only the opinion but
the opinion holder and the entity about which the
opinion has been expressed are also very impor-
tant for sentiment analysis. Also, as can be seen
from 1),4) and 5) there is also a notion of time as-
sociated with every sentiment expressed. Now, let
us define the sentiment analysis problem formally
as given in Liu (2010).
A direct opinion about the object is a quintuple
< o
j
, f
jk
, oo
ijkl
, h
i
, t
l
>, where o
j
is the the ob-
ject, f
jk
is the feature of the object o
j
, oo
ijkl
is the
orientation or polarity of the opinion on feature
f
jk
of object o
j
, h
i
is the opinion holder and t
i
is
the time when the opinion is expressed by h
i
.
As can be seen from the formal definition of
sentiment analysis and the motivating example,
not all sentiment-bearing expressions contribute to
the overall sentiment of the text. To solve this
problem, we can make use of semantic roles in the
text. Semantic role is the underlying relationship
that the underlying participant has with the main
verb. To identify the semantic roles, we make use
of UNL in our approach.
UNL (Universal Networking Language)
UNL is declarative formal language specifically
designed to represent semantic data extracted from
natural language texts. In UNL, the information
is represented by a semantic network, also called
UNL graph. UNL graph is made up of three dis-
crete semantic entities, Universal Words, Univer-
sal Relations, and Universal Attributes. Universal
Words are nodes in the semantic network, Univer-
sal Relations are arcs linking UWs, and Universal
attributes are properties of UWs. To understand
114
UNL better, let us consider an example. UNL
graph for ?I like that bad boy? is as shown in Fig-
ure 1
Figure 1: UNL graph for ?I like that bad boy?
Here, ?I?, ?like?, ?bad?, and ?boy? are the
UWs. ?agt? (agent), ?obj? (patient), and ?mod?
(modifier) are the Universal Relations. Universal
attributes are the properties associated with UWs
which will be explained as and when necessary
with the rules of our algorithm.
UNL relations
Syntax of a UNL relation is as shown below,
< rel >:< scope >< source >;< target >
Where, < rel > is the name of the rela-
tion, < scope > is the scope of the relation,
< source > is the UW that assigns the relation,
and < target > is the UW that receives the
relation
We have considered the following Universal re-
lations in our approach,
1) agt relation : agt stands for agent. An agent is
a participant in action that provokes a change of
state or location. The agt relation for the sentence
?John killed Mary? is agt( killed , John ). This
means that the action of killing was performed by
John.
2) obj relation : obj stands for patient. A patient is
a participant in action that undergoes a change of
state or location. The obj relation for the sentence
?John killed Mary? is obj( killed , Mary ). This
means that the patient/object of killing is Mary.
3) aoj relation : aoj stands for object of an at-
tribute. In the sentence ?John is happy?, the aoj
relation is aoj( happy , John ).
4) mod relation : mod stands for modifier of an ob-
ject. In the sentence ?a beautiful book?, the mod
relation is mod( book , beautiful ).
5) man relation : man relation stands for manner.
It is used to indicate how the action, experience or
process of an event is carried out. In the sentence
?The scenery is beautifully shot?, the man relation
is man( beautifully , shot ).
6) and relation : and relation is used to state a
conjunction between two entities. In the sen-
tence ?Happy and cheerful?, the and relation is
and(Happy,cheerful).
Architecture
As show in Figure 1, the modifier ?bad? is associ-
ated with the object of the main verb. It shouldn?t
affect the sentiment of the main agent. Therefore,
we can ignore the modifier relation of the main ob-
ject in such cases. After doing that, the sentiment
of this sentence can be inferred to be positive. The
approach followed in the project is to first generate
a UNL graph for the given input sentence. Then a
set of rules is applied and used to infer the sen-
timent of the sentence. The process is shown in
Figure 2. The UNL generator shown in the Figure
2 has been developed at CFILT.
1
Before, the given
piece of text is passed on to the UNL generator,
it goes through a number of pre-processing stages.
Removal of redundant punctuations, special char-
acters, emoticons, etc. are part of this process.
This is extremely important because the UNL gen-
erator is not able to handle special characters at the
moment. We can see that, the performance of the
overall system is limited by this. A more robust
version of the UNL generator will certainly allow
the system to infer the sentiment more accurately.
Figure 2: System Architecture
Rules
There is a separate rule for each relation. For each
UW (Universal word) considered, if it has a @not
attribute then its polarity is reversed. Rules used
by the system are as follows,
1) If a given UW is source of the agt relation, then
its polarity is added to the overall polarity of the
1
http://www.cfilt.iitb.ac.in/
115
text. e.g., ?I like her?. Here, the agt relation will
be agt ( like , I ). The polarity of like being posi-
tive, the overall polarity of the text is positive. e.g,
?I don?t like her?. Here the agt relation will be agt
( like@not , I ). The polarity of like is positive but
it has an attribute @not so its polarity is negative.
The overall polarity of the text is negative in this
case.
2) If a given UW is source or target of the obj rela-
tion and has the attribute @entry then its polarity
is added to the overall polarity of the text. This
rule merely takes into account the main verb of
the sentence into account, and the it?s is polarity
considered. e.g., ?I like her?, here the obj relation
will be obj ( like@entry , her ). The polarity of
like being positive, the overall polarity of the text
is positive
3) If a given UW is the source of the aoj rela-
tion and has the attribute @entry then its polarity
is added to the overall polarity of the text. e.g.,
?Going downtown tonight it will be amazing on
the waterfront with the snow?. Here, the aoj re-
lation is aoj ( amazing@entry , it ). amazing has
a positive polarity and therefore overall polarity is
positive in this case.
4) If a given UW is the target of the mod relation
and the source UW has the attribute @entry or has
the attribute @indef then polarity of the target UW
is added to the overall polarity of the text. e.g., ?I
like that bad boy?. Here, the aoj relation is mod
( boy , bad ). bad has a negative polarity but the
source UW, boy does not have an @entry attribute.
So, in this case negative polarity of bad is not con-
sidered as should be the case. e.g., ?She has a
gorgeous face?. Here, the mod relation is mod (
face@indef , gorgeous ). gorgeous has a positive
polarity and face has an attribute @indef. So, po-
larity of gorgeous should be considered.
5) If a given UW is the target of the man relation
and the source UW has the attribute @entry then
polarity of the target UW is added to the overall
polarity of the text. Or if the target UW has the
attribute @entry then also we can consider polar-
ity of the target UW. e.g., ?He always runs fast?.
Here, the aoj relation is mod ( run@entry , fast ).
fast has a positive polarity and the source UW, run
has the @entry attribute. So, in this case positive
polarity of fast is added to the overall polarity of
the sentence. Polarities of both the source and tar-
get UW of the and relation are considered.
6) In ?Happy and Cheerful?, the and relation is
and(Happy, Cheerful). Happy and Cheerful, both
have a positive polarity, which gives this sentence
an overall positive polarity.
The polarity value of each individual word is
looked up in a dictionary of positive of negative
words used is Liu (2010) After all the rules are
applied, summation of all the calculated polarity
values is done. If this sum is greater than 0 then it
is considered as positive, and negative otherwise.
This system is negative biased due to the fact that
people often tend to express negative sentiment in-
directly or by comparison with something good. A
more detailed discussion on negative texts is pro-
vided in section 6.
4 Experimental Setup
Analysis was performed for monolingual binary
sentiment classification task. The language used
in this case was English. The comparison was
done between 5 systems viz. System using words
as features, WordNet sense based system as given
in Balamurali et al., (2012), Clusters based sys-
tem as described in Kashyap et al., (2013), Dis-
course rules based system as given in Mukherjee
and Bhattacharyya (2012), UNL rule based sys-
tem. Two polarity datasets were used to perform
the experiments.
1. EN-TD: English Tourism corpus as used in
Ye et al., (2009). It consists of 594 positive
and 593 negative reviews.
2. EN-PD: English Product (music albums) re-
view corpus Blitzer et al., (2007). It consists
of 702 positive and 702 negative reviews.
For the WordNet sense, and Clusters based sys-
tems, a manually sense tagged version of the (EN-
PD) has been used. Also, a automatically sense
tagged version of (EN-TD) was used on these sys-
tems. The tagging in the later case was using an
automated WSD engine, trained on a tourism do-
main Khapra et al., (2013). The results reported
for supervised systems are based on 10-fold cross
validation.
5 Results
The results for monolingual binary sentiment
classification task are shown in Table 1. The re-
sults reported are the best results obtained in case
of supervised systems. The cluster based system
116
System EN-TD EN-PD
Bag of Words 85.53 73.24
Synset-based 88.47 71.58
Cluster-based 95.20 79.36
Discourse-based 71.52 64.81
UNL rule-based 86.08 79.55
Table 1: Classification accuracy (in %) for mono-
lingual sentiment analysis
EN-TD EN-PD
System Pos Neg Pos Neg
Discourse rules 94.94 48.06 92.73 36.89
UNL rules 95.72 76.44 90.75 68.35
Table 2: Classification accuracy (in %) for positive
and negative reviews
performs the best in both cases. The UNL rule-
based system performs better only than the bag
of words and discourse rule based system. For
EN-PD ( music album reviews ) dataset, the UNL
based system outperforms every other system .
These results are very promising for a rule-based
system. The difference between accuracy for pos-
itive and negative reviews for the rule-based sys-
tems viz. Discourse rules based and UNL rules
based is shown in Table 2. It can be seen that
the Discourse rules based system performs slightly
better than the UNL based system for positive re-
views. On the other hand, the UNL rules based
system outperforms it in case of negative reviews
by a huge margin.
6 Discussion
The UNL generator used in this case is the bottle-
neck in terms of performance due to it?s speed. It
can take a long time to generate UNL graphs for
large sentences. Also, it makes use of the stan-
dard NLP tools viz. parsing, co-reference resolu-
tion, etc. to assign the proper semantic roles in the
given text. It is well known fact that these tech-
niques work properly only on structured data. The
language used in the reviews present in both the
datasets is unstructured in considerable number of
cases. The UNL generator is still in its infancy
and cannot handle text involving special charac-
ters. Due to these reasons, a proper UNL graph is
not generated in some cases. Also, it is not able to
generator proper UNL graphs for even well struc-
tured sentences. As a result of these things, the
classification accuracy is low. Negative reviews
are difficult to classify due to comparitive state-
ments and presence of positive words. Also there
are some sarcastic sentences which are difficult to
classify. Sarcasm is a very difficult problem to
tackle. Some related works can be found in Car-
valho et al., (2009) and Muresan et al., (2011). In
some cases, the reviewers make use of their native
language and expressions. This is a big problem
for the task of monolingual sentiment classifica-
tion.
7 Conclusion
This paper made use of deep semantics to tackle
the the problem of sentiment analysis. A seman-
tic role labeling method through generation of a
UNL graph was used to do this. The main mo-
tivation behind this research was the fact that not
all sentiment bearing expressions contribute to the
overall sentiment of the text. The approach was
evaluated on two datasets and compared with suc-
cessful previous approaches which don?t make use
of deep semantics. The system underperformed
all the supervised systems but showed promise by
yielding better results than the other rule-based ap-
proach. Also, in some cases the performance was
very close to the other supervised systems. The
system works well on sentences where are inher-
ently complex and difficult for sentiment analysis
as it makes use of semantic role labeling. Any rule
based system can never be exhaustive in terms of
rules. We always need to add new rules to improve
on it. In some case, adding new rules might cause
side-effects. In this case, as the rules are intuitive,
adding of new rules will be easy. Also, analysis
of the results hints at some ways to tackle specific
problems effectively.
8 Future Work
Adding more rules to the system will help to im-
prove the system. Language gets updated almost
daily, we plan to update our dictionary with these
new words and expressions to increase the accu-
racy. Also, we plan to replace the UNL system
with a dependency parsing system and apply rules
similar to the ones described in this work.
117
References
Ruppenhofer, Josef and Rehbein, Ines. 2012. Seman-
tic frames as an anchor representation for sentiment
analysis. Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis
Mukherjee, Subhabrata and Bhattacharyya, Push-
pak. 2012. Sentiment Analysis in Twitter with
Lightweight Discourse Analysis. COLING
Balamurali, AR and Joshi, Aditya and Bhattacharyya,
Pushpak 2011. Harnessing wordnet senses for su-
pervised sentiment classification. Proceedings of
the Conference on Empirical Methods in Natural
Language Processing
Rentoumi, Vassiliki and Giannakopoulos, George and
Karkaletsis, Vangelis and Vouros, George A 2009.
Sentiment analysis of figurative language using a
word sense disambiguation approach. Proceedings
of the International Conference RANLP
Mart?n-Wanton, Tamara and Balahur-Dobrescu,
Alexandra and Montoyo-Guijarro, Andres and
Pons-Porrata, Aurora 2010. Word sense dis-
ambiguation in opinion mining: Pros and cons.
Special issue: Natural Language Processing and its
Applications
Kashyap Popat, Balamurali A.R, Pushpak Bhat-
tacharyya and Gholamreza Haffari 2013. The
Haves and the Have-Nots: Leveraging Unlabelled
Corpora for Sentiment Analysis. The Association
for Computational Linguistics
Saif, Hassan and He, Yulan and Alani, Harith 2012.
Semantic sentiment analysis of twitter. The Seman-
tic Web?ISWC 2012
Verma, Shitanshu and Bhattacharyya, Pushpak 2009.
Incorporating semantic knowledge for sentiment
analysis. Proceedings of ICON
Choi, Yejin and Cardie, Claire 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing
Liu, Bing 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing
Ye, Qiang and Zhang, Ziqiong and Law, Rob 2009.
Sentiment classification of online reviews to travel
destinations by supervised machine learning ap-
proaches. Expert Systems with Applications
Balamurali, AR and Khapra, Mitesh M and Bhat-
tacharyya, Pushpak 2013. Lost in translation: via-
bility of machine translation for cross language sen-
timent analysis. Computational Linguistics and In-
telligent Text Processing
Blitzer, John and Dredze, Mark and Pereira, Fernando
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. ACL
Gonz?alez-Ib?a?nez, Roberto and Muresan, Smaranda and
Wacholder, Nina 2011. Identifying Sarcasm in Twit-
ter: A Closer Look. ACL
Carvalho, Paula and Sarmento, Lu??s and Silva, M?ario
J and de Oliveira, Eug?enio 2009. Clues for detect-
ing irony in user-generated contents: oh...!! it?s so
easy;-). ACM
118
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 142?146,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
A cognitive study of subjectivity extraction in sentiment annotation
Abhijit Mishra
1
Aditya Joshi
1,2,3
Pushpak Bhattacharyya
1
1
IIT Bombay, India
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{abhijitmishra, adityaj, pb}@cse.iitb.ac.in
Abstract
Existing sentiment analysers are weak AI
systems: they try to capture the function-
ality of human sentiment detection faculty,
without worrying about how such faculty
is realized in the hardware of the human.
These analysers are agnostic of the actual
cognitive processes involved. This, how-
ever, does not deliver when applications
demand order of magnitude facelift in ac-
curacy, as well as insight into characteris-
tics of sentiment detection process.
In this paper, we present a cognitive study
of sentiment detection from the perspec-
tive of strong AI. We study the sentiment
detection process of a set of human ?sen-
timent readers?. Using eye-tracking, we
show that on the way to sentiment de-
tection, humans first extract subjectivity.
They focus attention on a subset of sen-
tences before arriving at the overall senti-
ment. This they do either through ?antici-
pation? where sentences are skipped dur-
ing the first pass of reading, or through
?homing? where a subset of the sentences
are read over multiple passes, or through
both. ?Homing? behaviour is also ob-
served at the sub-sentence level in com-
plex sentiment phenomena like sarcasm.
1 Introduction
Over the years, supervised approaches using
polarity-annotated datasets have shown promise
for SA (Pang and Lee, 2008). However, an al-
ternate line of thought has co-existed. Pang and
Lee (2004) showed that for SA, instead of a doc-
ument in its entirety, an extract of the subjec-
tive sentences alone can be used. This process
of generating a subjective extract is referred to
as subjectivity extraction. Mukherjee and Bhat-
tacharyya (2012) show that for sentiment predic-
tion of movie reviews, subjectivity extraction may
be used to discard the sentences describing movie
plots since they do not contribute towards the
speaker?s view of the movie.
While subjectivity extraction helps sentiment
classification, the reason has not been sufficiently
examined from the perspective of strong AI. The
classical definition of strong AI suggests that a
machine must be perform sentiment analysis in
a manner and accuracy similar to human beings.
Our paper takes a step in this direction. We study
the cognitive processes underlying sentiment an-
notation using eye-fixation data of the participants.
Our work is novel in two ways:
? We view documents as a set of sentences
through which sentiment changes. We show
that the nature of these polarity oscillations
leads to changes in the reading behavior.
? To the best of our knowledge, the idea of us-
ing eye-tracking to validate assumptions is
novel in case of sentiment analysis and many
NLP applications.
2 Sentiment oscillations & subjectivity
extraction
We categorize subjective documents as linear and
oscillating. A linear subjective document is the
one where all or most sentences have the same po-
larity. On the other hand, an oscillating subjective
document contains sentences of contrasting polar-
ity (viz. positive and negative). Our discussions
on two forms of subjectivity extraction use the
concepts of linear and oscillating subjective doc-
uments.
Consider a situation where a human reader
needs to annotate two documents with sentiment.
Assume that the first document is linear subjec-
tive - with ten sentences, all of them positive. In
142
case of this document, when he/she reads a cou-
ple of sentences with the same polarity, he/she be-
gins to assume that the next sentence will have the
same sentiment and hence, skips through it. We
refer to this behavior as anticipation. Now, let the
second document be an oscillating subjective doc-
ument with ten sentences, the first three positive,
the next four negative and the last three positive.
In this case, when a human annotator reads this
document and sees the sentiment flip early on, the
annotator begins to carefully read the document.
After completing a first pass of reading, the anno-
tator moves back to read certain crucial sentences.
We refer to this behavior as homing.
The following sections describe our observa-
tions in detail. Based on our experiments, we ob-
serve these two kinds of subjectivity extraction in
our participants: subjectivity extraction as a result
of anticipation and subjectivity extraction as a re-
sult of homing - for linear and oscillating docu-
ments respectively.
3 Experiment Setup
This section describes the framework used for our
eye-tracking experiment. A participant is given
the task of annotating documents with one out of
the following labels: positive, negative and ob-
jective. While she reads the document, her eye-
fixations are recorded.
To log eye-fixation data, we use Tobii T120
remote eye-tracker with Translog(Carl, 2012).
Translog is a freeware for recording eye move-
ments and keystrokes during translation. We con-
figure Translog for reading with the goal of senti-
ment.
3.1 Document description
We choose three movie reviews in English from
IMDB (http://www.imdb.com) and indicate them
as D0, D1 and D2. The lengths of D0, D1 and
D2 are 10, 9 and 13 sentences respectively. Using
the gold-standard rating given by the writer, we
derive the polarity of D0, D1 and D2 as positive,
negative and positive respectively. The three doc-
uments represent three different styles of reviews:
D0 is positive throughout (linear subjective), D1
contains sarcastic statements (linear subjective but
may be perceived as oscillating due to linguistic
difficulty) while D2 consists of many flips in sen-
timent (oscillating subjective).
It may seem that the data set is small and
may not lead to significant findings. However,
we wished to capture the most natural form of
sentiment-oriented reading. A larger data set
would have weakened the experiment because: (i)
Sentiment patterns (linear v/s subjective) begin to
become predictable to a participant if she reads
many documents one after the other. (ii) There
is a possibility that fatigue introduces unexpected
error. To ensure that our observations were signif-
icant despite the limited size of the data set, we
increased the number of our participants to 12.
3.2 Participant description
Our participants are 24-30 year-old graduate stu-
dents with English as the primary language of aca-
demic instruction. We represent them as P0, P1
and so on. The polarity for the documents as re-
ported by the participants are shown in Table 1.
All participants correctly identified the polarity of
document D0. Participant P9 reported that D1 is
confusing. 4 out of 12 participants were unable to
detect correct opinion in D2.
3.3 Experiment Description
We obtain two kinds of annotation from our an-
notators: (a) sentiment (positive, negative and ob-
jective), (b) eye-movement as recorded by an eye-
tracker. They are given a set of instructions before-
hand and can seek clarifications. This experiment
is conducted as follows:
1. A complete document is displayed on the
screen. The font size and line separation are
set to 17pt and 1.5 cm respectively to ensure
clear visibility and minimize recording error.
2. The annotator verbally states the sentiment of
this sentence, before (s)he can proceed to the
next.
3. While the annotator is reading the sentence,
a remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to Translog II software (Carl,
2012) in order to record the data. A snap-
shot of the software is shown in figure 1. The
dots and circles represent position of eyes and
fixations of the annotator respectively. Each
eye-fixation that is recorded consists of: co-
ordinates, timestamp and duration. These
three parameters have been used to generate
sentence progression graphs.
143
Document Orig P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11
D0 +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve
D1 -ve -ve +ve -ve -ve -ve -ve -ve -ve -ve Neu/-ve -ve -ve
D2 +ve +ve +ve -ve +ve +ve Neu +ve Neu Neu +ve +ve +ve
Table 1: Polarity of documents as perceived by the writer (original) and the participants +ve, -ve and
Neu represent positive, negative and neutral polarities respectively.
Figure 1: Gaze-data recording using Translog-II
Figure 2: Sentence progression graph for partici-
pant P7 document D0
4 Observations: Subjectivity extraction
through anticipation
In this section, we describe a case in which partic-
ipants skip sentences. We show that anticipation
of sentiment is linked with subjectivity extraction.
Table 2 shows the number of unique and non-
unique sentences that participants read for each
document. The numbers in the last column in-
dicate average values. The table can be read as:
participant P1 reads 8 unique sentences of docu-
ment D0 (thus skipping two sentences) and includ-
ing repetitions, reads 26 sentences. Participant P0
skips as many as six sentences in case of document
D1.
The number of unique sentences read is lower
than sentence count for four out of twelve partic-
ipants in case of document D0. This skipping is
negligible in case of document D1 and D2. Also,
the average non-unique sentence fixations are 21
in case of D0 and 33.83 for D1 although the total
number of sentences in D0 and D1 is almost the
same. This verifies that participants tend to skip
sentences while reading D0.
Figure 2 shows sentence progression graph for
participant P7. The participant reads a series of
sentences and then skips two sentences. This im-
plies that anticipation behaviour was triggered af-
ter reading sentences of the same polarity. Sim-
ilar traits are observed in other participants who
skipped sentences while reading document D0.
5 Observations: Subjectivity extraction
through homing
This section presents a contrasting case of sub-
jectivity extraction. We refer to a reading pattern
as homing
1
when a participant reads a document
completely and returns to read a selected subset of
sentences. We believe that during sentiment an-
notation, this subset is the subjective extract that
the user has created in her mind. We observe this
phenomenon in reading patterns of documents D1
and D2. The former contains sarcasm because of
which parts of sentences may appear to be of con-
trasting polarity while the latter is an oscillating
subjective document.
1
The word is derived from missile guidance systems. The
definition
2
of homing is ?the process of determining the lo-
cation of something, sometimes the source of a transmission,
and going to it.?
144
Document P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 Avg.
D0
Non-unique 9 26 23 17 18 18 35 16 33 19 15 23 21
Unique 8 8 10 10 10 10 10 8 10 8 10 10
D1
Non-unique 5 23 46 13 15 44 35 26 56 57 40 46 33.83
Unique 3 9 9 9 9 9 8 9 9 9 9 9
D2
Non-unique 36 29 67 21 23 51 64 48 54 59 73 80 50.42
Unique 13 13 13 13 13 13 13 13 13 13 13 13
Table 2: Number of unique and non-unique sentences read by each participant
Figure 3: Sentence progression graph of partici-
pant P2 for document D1 (left) and document D2
(right)
Figure 3 shows sentence progression graphs of
participant P2 for documents D1 and D2. For doc-
ument D1, the participant performs one pass of
reading until sequence number 30. A certain sub-
set of sentences are re-visited in the second pass.
On analyzing sentences in the second pass of read-
ing, we observe a considerable overlap in case of
our participants. We also confirm that all of these
sentences are subjective. This means that the sen-
tences that are read after sequence number 30 form
the subjective extract of document D1.
Similar behaviour is observed in case of docu-
ment D2. The difference in this case is that there
is less overlap of sentences read in the second pass
among participants. This implies , for oscillat-
ing subjective documents, the subjective extract is
user/document-specific.
It may be argued that fixations corresponding
Participant TFD-SE PTFD TFC-SE
(secs) (%)
P5 7.3 8 21
P7 3.1 5 11
P9 51.94 10 26
P11 116.6 16 56
Table 3: Reading statistics for second pass reading
for document D1; TFD: Total fixation duration for
subjective extract; PTFD: Proportion of total fix-
ation duration = (TFD)/(Total duration); TFC-SE:
Total fixation count for subjective extract
to second pass reading are stray fixations and not
subjective extracts. Hence, for the second pass
reading of document D1, we tabulate fixation du-
ration, fixation count and proportion of total dura-
tion in Table 3. The fixation duration and fixation
count are both recorded by the eye-tracker. The
fixation counts are substantial and the participants
spend around 5-15% of the total reading time in
the second pass reading. We also confirm that all
of these sentences are subjective. This means that
these portions indeed correspond to subjective ex-
tracts as a result of homing.
6 A note on linguistic challenges
Our claim is that regression after reading an en-
tire document corresponds to the beginning of
a subjective extract. However, we observe that
some regressions may also happen due to senti-
ment changes at the sub-sentence level. Some of
these are as follows.
1. Sarcasm: Sarcasm involves an implicit flip
in the sentiment. Participant P9 does not cor-
rectly predict sentiment of Document D1. On
analyzing her data, we observe multiple re-
gressions on the sentence ?Add to this mess
some of the cheesiest lines and concepts, and
145
there you have it; I would call it a complete
waste of time, but in some sense it is so bad
it is almost worth seeing.? This sentence has
some positive words but is negative towards
the movie. Hence, the participant reads this
portion back and forth.
2. Thwarted expectations: Thwarted expecta-
tions are expressions with a sentiment rever-
sal within a sentence/snippet. Homing is ob-
served in this case as well. Document D2
has a case of thwarted expectations from sen-
tences 10-12 where there is an unexpected
flip of sentiment. In case of some partici-
pants, we observe regression on these sen-
tences multiple times.
7 Related Work
The work closest to ours is by Scott et al. (2011)
who study the role of emotion words in read-
ing using eye-tracking. They show that the eye-
fixation duration for emotion words is consistently
less than neutral words with the exception of high-
frequency negative words. Eye-tracking
3
technol-
ogy has also been used to study the cognitive as-
pects of language processing tasks like translation
and sense disambiguation. Dragsted (2010) ob-
serve co-ordination between reading and writing
during human translation. Similarly, Joshi et al.
(2011) use eye-tracking to correlate fixation dura-
tion with polysemy of words during word sense
disambiguation.
8 Conclusion & Future work
We studied sentiment annotation in the context of
subjectivity extraction using eye-tracking. Based
on how sentiment changes through a document,
humans may perform subjectivity extraction as a
result of either: (a) anticipation or (b) homing.
These observations are in tandem with the past
work that shows benefit of subjectivity extraction
for automatic sentiment classification.
Our study is beneficial in three perspectives: (i)
Sentiment classifiers may use interaction between
sentiment of sentences. Specifically, this can be
modeled using features like sentiment run length
(i.e. maximal span of sentences bearing same
3
Related Terms:
Eye-fixation: Long stay of visual gaze on a single location
Regression: Revisiting a previously read segment
Sentence Progression Graph: Graph showing reading se-
quence of sentences
sentiment) or sentiment flips (i.e. instances where
consecutive sentences bear opposite polarity),
(ii) Crowd-sourced sentiment annotation can
devise variable pricing models based on our study.
Based on anticipation and homing information
about documents, documents can be grouped into
difficulty categories and priced accordingly.
Acknowledgment
We thank Tobii Corporation for lending us their
eye-tracker for this study, and our annotators from
CFILT, IIT Bombay. Aditya is funded by the TCS
Research Fellowship Program.
References
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts In Proceedings
of the ACL, 271-278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis Foundations and Trends in In-
formation Retrieval, 2008, vol. 2, nos.12 1135.
B Dragsted. 2010. Co-ordination of reading and writ-
ing processes in translation. Contribution to Trans-
lation and Cognition. Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Michael Carl. 2012. Translog-II: A Program for
Recording User Activity Data for Empirical Reading
and Writing Research. In Proceedings of the Eight
International Conference on Language Resources
and Evaluation, European Language Resources As-
sociation.
Scott G. , ODonnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783792.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. WikiSent : Weakly Supervised Senti-
ment Analysis Through Extractive Summarization
With Wikipedia European Conference on Machine
Learning (ECML PKDD 2012), Bristol, U.K.,
146
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 90?96,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The IIT Bombay Hindi?English Translation System at WMT 2014
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra, Anoop Kunchukuttan,
Ritesh Shah, Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
{piyushdd,rajen,abhijitmishra,anoopk,ritesh,pb}@cse.iitb.ac.in
Abstract
In this paper, we describe our English-
Hindi and Hindi-English statistical sys-
tems submitted to the WMT14 shared task.
The core components of our translation
systems are phrase based (Hindi-English)
and factored (English-Hindi) SMT sys-
tems. We show that the use of num-
ber, case and Tree Adjoining Grammar
information as factors helps to improve
English-Hindi translation, primarily by
generating morphological inflections cor-
rectly. We show improvements to the
translation systems using pre-procesing
and post-processing components. To over-
come the structural divergence between
English and Hindi, we preorder the source
side sentence to conform to the target lan-
guage word order. Since parallel cor-
pus is limited, many words are not trans-
lated. We translate out-of-vocabulary
words and transliterate named entities in
a post-processing stage. We also investi-
gate ranking of translations from multiple
systems to select the best translation.
1 Introduction
India is a multilingual country with Hindi be-
ing the most widely spoken language. Hindi and
English act as link languages across the coun-
try and languages of official communication for
the Union Government. Thus, the importance of
English?Hindi translation is obvious. Over the
last decade, several rule based (Sinha, 1995) , in-
terlingua based (Dave et. al., 2001) and statistical
methods (Ramanathan et. al., 2008) have been ex-
plored for English-Hindi translation.
In the WMT 2014 shared task, we undertake
the challenge of improving translation between the
English and Hindi language pair using Statisti-
cal Machine Translation (SMT) techniques. The
WMT 2014 shared task has provided a standard-
ized test set to evaluate multiple approaches and
avails the largest publicly downloadable English-
Hindi parallel corpus. Using these resources,
we have developed a phrase-based and a factored
based system for Hindi-English and English-Hindi
translation respectively, with pre-processing and
post-processing components to handle structural
divergence and morphlogical richness of Hindi.
Section 2 describes the issues in Hindi?English
translation.
The rest of the paper is organized as follows.
Section 3 describes corpus preparation and exper-
imental setup. Section 4 and Section 5 describe
our English-Hindi and Hindi-English translation
systems respectively. Section 6 describes the post-
processing operations on the output from the core
translation system for handling OOV and named
entities, and for reranking outputs from multiple
systems. Section 7 mentions the details regarding
our systems submitted to WMT shared task. Sec-
tion 8 concludes the paper.
2 Problems in Hindi?English
Translation
Languages can be differentiated in terms of
structural divergences and morphological mani-
festations. English is structurally classified as
a Subject-Verb-Object (SVO) language with a
poor morphology whereas Hindi is a morpho-
logically rich, Subject-Object-Verb (SOV) lan-
guage. Largely, these divergences are responsi-
ble for the difficulties in translation using a phrase
based/factored model, which we summarize in this
section.
2.1 English-to-Hindi
The fundamental structural differences described
earlier result in large distance verb and modi-
fier movements across English-Hindi. Local re-
ordering models prove to be inadequate to over-
90
come the problem; hence, we transformed the
source side sentence using pre-ordering rules to
conform to the target word order. Availability of
robust parsers for English makes this approach for
English-Hindi translation effective.
As far as morphology is concerned, Hindi is
more richer in terms of case-markers, inflection-
rich surface forms including verb forms etc. Hindi
exhibits gender agreement and syncretism in in-
flections, which are not observed in English. We
attempt to enrich the source side English corpus
with linguistic factors in order to overcome the
morphological disparity.
2.2 Hindi-to-English
The lack of accurate linguistic parsers makes it dif-
ficult to overcome the structural divergence using
preordering rules. In order to preorder Hindi sen-
tences, we build rules using shallow parsing infor-
mation. The source side reordering helps to reduce
the decoder?s search complexity and learn better
phrase tables. Some of the other challenges in gen-
eration of English output are: (1) generation of ar-
ticles, which Hindi lacks, (2) heavy overloading of
English prepositions, making it difficult to predict
them.
3 Experimental Setup
We process the corpus through appropriate filters
for normalization and then create a train-test split.
3.1 English Corpus Normalization
To begin with, the English data was tokenized us-
ing the Stanford tokenizer (Klein and Manning,
2003) and then true-cased using truecase.perl pro-
vided in MOSES toolkit.
3.2 Hindi Corpus Normalization
For Hindi data, we first normalize the corpus us-
ing NLP Indic Library (Kunchukuttan et. al.,
2014)
1
. Normalization is followed by tokeniza-
tion, wherein we make use of the trivtokenizer.pl
2
provided with WMT14 shared task. In Table 1, we
highlight some of the post normalization statistics
for en-hi parallel corpora.
1
https://bitbucket.org/anoopk/indic_
nlp_library
2
http://ufallab.ms.mff.cuni.cz/~bojar/
hindencorp/
English Hindi
Token 2,898,810 3,092,555
Types 95,551 118,285
Total Characters 18,513,761 17,961,357
Total sentences 289,832 289,832
Sentences (word
count ? 10)
188,993 182,777
Sentences (word
count > 10)
100,839 107,055
Table 1: en-hi corpora statistics, post normalisa-
tion.
3.3 Data Split
Before splitting the data, we first randomize the
parallel corpus. We filter out English sentences
longer than 50 words along with their parallel
Hindi translations. After filtering, we select 5000
sentences which are 10 to 20 words long as the test
data, while remaining 284,832 sentences are used
for training.
4 English-to-Hindi (en-hi) translation
We use the MOSES toolkit (Koehn et. al., 2007a)
for carrying out various experiments. Starting with
Phrase Based Statistical Machine Translation (PB-
SMT)(Koehn et. al., 2003) as baseline system we
go ahead with pre-order PBSMT described in Sec-
tion 4.1. After pre-ordering, we train a Factor
Based SMT(Koehn, 2007b) model, where we add
factors on the pre-ordered source corpus. In Fac-
tor Based SMT we have two variations- (a) using
Supertag as factor described in Section 4.2 and (b)
using number, case as factors described in Section
4.3.
4.1 Pre-ordering source corpus
Research has shown that pre-ordering source lan-
guage to conform to target language word order
significantly improves translation quality (Collins
et. al, 2005). There are many variations of pre-
ordering systems primarily emerging from either
rule based or statistical methods. We use rule
based pre-ordering approach developed by (Pa-
tel et. al., 2013), which uses the Stanford parser
(Klein and Manning, 2003) for parsing English
sentences. This approach is an extension to an ear-
lier approach developed by (Ramanathan et. al.,
2008). The existing source reordering system re-
quires the input text to contain only surface form,
however, we extended it to support surface form
91
along with its factors like POS, lemma etc.. An
example of improvement in translation after pre-
ordering is shown below:
Example: trying to replace bad ideas with good
ideas .
Phr: replace b  r EvcAro\ ko aQC EvcAro\ k
sAT
(replace bure vichaaron ko acche vichaaron ke
saath)
Gloss: replace bad ideas good ideas with
Pre-order PBSMT: aQC EvcAro\ s b  r EvcAro\
ko bdln kF koEff kr rh h{\
(acche vichaaron se bure vichaaron ko badalane
ki koshish kara rahe hain)
Gloss: good ideas with bad ideas to replace trying
4.2 Supertag as Factor
The notion of Supertag was first proposed by
Joshi and Srinivas (1994). Supertags are elemen-
tary trees of Lexicalized Tree Adjoining Grammar
(LTAG) (Joshi and Schabes, 1991). They provide
syntactic as well as dependency information at the
word level by imposing complex constraints in a
local context. These elementary trees are com-
bined in some manner to form a parse tree, due
to which, supertagging is also known as ?An ap-
proach to almost parsing?(Bangalore and Joshi,
1999). A supertag can also be viewed as frag-
ments of parse trees associated with each lexi-
cal item. Figure 1 shows an example of su-
pertagged sentence ?The purchase price includes
taxes?described in (Hassan et. al., 2007). It clearly
shows the sub-categorization information avail-
able in the verb include, which takes subject NP
to its left and an object NP to its right.
Figure 1: LTAG supertag sequence obtained using
MICA Parser.
Use of supertags as factors has already been
studied by Hassan (2007) in context of Arabic-
English SMT. They use supertag language model
along with supertagged English corpus. Ours
is the first study in using supertag as factor
for English-to-Hindi translation on a pre-ordered
source corpus.
We use MICA Parser (Bangalore et. al., 2009)
for obtaining supertags. After supertagging we run
pre-ordering system preserving the supertags in it.
For translation, we create mapping from source-
word|supertag to target-word. An example of im-
provement in translation by using supertag as fac-
tor is shown below:
Example: trying to understand what your child is
saying to you
Phr: aApkA b?A aAps ?A kh rhA h{ yh
(aapkaa bacchaa aapse kya kaha rahaa hai yaha)
Gloss: your child you what saying is this
Supertag Fact: aApkA b?A aAps ?A kh rhA
h{ , us smJn kF koEff krnA
(aapkaa bacchaa aapse kya kaha rahaa hai, use
samajhane kii koshish karnaa)
Gloss: your child to you what saying is , that un-
derstand try
4.3 Number, Case as Factor
In this section, we discuss how to generate correct
noun inflections while translating from English to
Hindi. There has been previous work done in order
to solve the problem of data sparsity due to com-
plex verb morphology for English to Hindi trans-
lation (Gandhe, 2011). Noun inflections in Hindi
are affected by the number and case of the noun
only. Number can be singular or plural, whereas,
case can be direct or oblique. We use the factored
SMT model to incorporate this linguistic informa-
tion during training of the translation models. We
attach root-word, number and case as factors to
English nouns. On the other hand, to Hindi nouns
we attach root-word and suffix as factors. We de-
fine the translation and generation step as follows:
? Translation step (T0): Translates English
root|number|case to Hindi root|suffix
? Generation step (G0): Generates Hindi sur-
face word from Hindi root|suffix
An example of improvement in translation by
using number and case as factors is shown below:
Example: Two sets of statistics
Phr: do k aA kw
(do ke aankade)
Gloss: two of statistics
Num-Case Fact: aA kwo\ k do sV
(aankadon ke do set)
Gloss: statistics of two sets
92
4.3.1 Generating number and case factors
With the help of syntactic and morphological
tools, we extract the number and case of the En-
glish nouns as follows:
? Number factor: We use Stanford POS tag-
ger
3
to identify the English noun entities
(Toutanova, 2003). The POS tagger itself dif-
ferentiates between singular and plural nouns
by using different tags.
? Case factor: It is difficult to find the
direct/oblique case of the nouns as En-
glish nouns do not contain this information.
Hence, to get the case information, we need
to find out features of an English sentence
that correspond to direct/oblique case of the
parallel nouns in Hindi sentence. We use
object of preposition, subject, direct object,
tense as our features. These features are
extracted using semantic relations provided
by Stanford?s typed dependencies (Marneffe,
2008).
4.4 Results
Listed below are different statistical systems
trained using Moses:
? Phrase Based model (Phr)
? Phrase Based model with pre-ordered source
corpus (PhrReord)
? Factor Based Model with factors on pre-
ordered source corpus
? Supertag as factor (PhrReord+STag)
? Number, Case as factor (PhrReord+NC)
We evaluated translation systems with BLEU and
TER as shown in Table 2. Evaluation on the devel-
opment set shows that factor based models achieve
competitive scores as compared to the baseline
system, whereas, evaluation on the WMT14 test
set shows significant improvement in the perfor-
mance of factor based models.
5 Hindi-to-English (hi-en) translation
As English follows SVO word order and Hindi fol-
lows SOV word order, simple distortion penalty in
phrase-based models can not handle the reordering
well. For the shared task, we follow the approach
3
http://nlp.stanford.edu/software/tagger.shtml
Development WMT14
Model BLEU TER BLEU TER
Phr 27.62 0.63 8.0 0.84
PhrReord 28.64 0.62 8.6 0.86
PhrReord+STag 27.05 0.64 9.8 0.83
PhrReord+NC 27.50 0.64 10.1 0.83
Table 2: English-to-Hindi automatic evaluation on
development set and on WMT14 test set.
that pre-orders the source sentence to conform to
target word order.
A substantial volume of work has been done
in the field of source-side reordering for machine
translation. Most of the experiments are based on
applying reordering rules at the nodes of the parse
tree of the source sentence. These reordering rules
can be automatically learnt (Genzel, 2010). But,
many source languages do not have a good robust
parser. Hence, instead we can use shallow pars-
ing techniques to get chunks of words and then
reorder them. Reordering rules can be learned au-
tomatically from chunked data (Zhang, 2007).
Hindi does not have a functional constituency
or dependency parser available, as of now. But,
a shallow parser
4
is available for Hindi. Hence,
we follow a chunk-based pre-ordering approach,
wherein, we develop a set of rules to reorder
the chunks in a source sentence. The follow-
ing are the chunks tags generated by this shallow
parser: Noun chunks (NP), Verb chunks (VGF,
VGNF, VGNN), Adjectival chunks (JJP), Ad-
verb chunks (RBP), Negatives (NEGP), Conjuncts
(CCP), Chunk fragments (FRAGP), and miscella-
neous entities (BLK) (Bharati, 2006).
5.1 Development of rules
After chunking an input sentence, we apply hand-
crafted reordering rules on these chunks. Follow-
ing sections describe these rules. Note that we ap-
ply rules in the same order they are listed below.
5.1.1 Merging of chunks
After chunking, we merge the adjacent chunks, if
they follow same order in target language.
1. Merge {JJP VGF} chunks (Consider this
chunk as a single VGF chunk)
e.g., vEZta h{ (varnit hai), E-Tta h{ (sthit hai)
4
http://ltrc.iiit.ac.in/showfile.php?
filename=downloads/shallow_parser.php
93
2. Merge adjacent verb chunks (Consider this
chunk as a single verb chunk)
e.g., EgrtaA h{ (girataa hai), l  BAtaA h{ (lub-
haataa hai)
3. Merge NP and JJP chunks separated by com-
mas and CCP (Consider this chunk as a single
NP chunk)
e.g., bwA aOr ahm (badaa aur aham)
5.1.2 Preposition chunk reordering
Next we find sequence of contiguous chunks sep-
arated by prepositions (Can end in verb chunks).
We apply following reordering rules on these con-
tiguous chunks:
1. Reorder multi-word preposition locally by re-
versing the order of words in that chunk
e.g., k alAvA (ke alaawaa) ? alAvA k,
k sAmn (ke saamane)? sAmn k
2. Reorder contiguous preposition chunk by re-
versing the order of chunks (Consider this
chunk as a single noun chunk)
e.g., Eh\d Dm m\ taFT kA bwA mh(v (hinduu
dharma me tirtha ka badaa mahatva)? bwA
mh(v kA taFT m\ Eh\d Dm
5.1.3 Verb chunk reordering
We find contiguous verb chunks and apply follow-
ing reordering rules:
1. Reorder chunks locally by reversing the order
of the chunks
e.g., vEZta h{ (varnit hai)? h{ vEZta
2. Verb chunk placement: We place the new
verb chunk after first NP chunk. Same rule
applies for all verb chunks in a sentence, i.e.,
we place each verb chunk after first NP chunk
of the clause to which the verb belongs.
Note that, even though placing verb chunk af-
ter first NP chunk may be wrong reordering.
But we also use distortion window of 6 to 20
while using phrase-based model. Hence, fur-
ther reordering of verb chunks can be some-
what handled by phrase-based model itself.
Thus, using chunker and reordering rules, we
get a source-reordered Hindi sentence.
5.2 Results
We trained two different translation models:
? Phrase-based model without source reorder-
ing (Phr)
? Phrase-based model with chunk-based source
reordering (PhrReord)
Development WMT14
Model BLEU TER BLEU TER
Phr 27.53 0.59 13.5 0.87
PhrReord 25.06 0.62 13.7 0.90
Table 3: Hindi-to-English automatic evaluation on
development set and on WMT14 test set.
Table 3 shows evaluation scores for develop-
ment set and WMT14 test set. Even though we do
not see significant improvement in automatic eval-
uation of PhrReord, but this model contributes in
improving translation quality after ranking, as dis-
cussed in Section 5. In subjective evaluation we
found many translation to be better in PhrReord
model as shown in the following examples:
Example 1: sn 2004 s v kI bAr coVg}-ta
rh h{\ |
(sana 2004 se ve kaii baar chotagrasta rahe hain.)
Phr: since 2004 he is injured sometimes .
PhrReord: he was injured many times since 2004
.
Example 2: aobAmA kA rA?~ pEta pd k c  nAv
?cAr hta  bnAyA aAEDkAErk jAl-Tl
(obama ka rashtrapti pad ke chunaav prachaar
hetu banaayaa aadhikarik jaalsthal)
Phr: of Obama for election campaign
PhrReord: official website of Obama created for
President campaign
6 Post processing
All experimental results reported in this paper are
after post processing the translation output. In post
processing, we remove some Out-of-Vocabulary
(OOV) words as described in subsection 6.1, after
which we transliterate the remaining OOV words.
6.1 Removing OOV
We noticed, there are many words in the training
corpus which were not present in the phrase ta-
ble, but, were present in the lexical tranlsation ta-
ble. So we used the lexical table as a dictionary
to lookup bilingual translations. Table 4 gives the
statistics of number of OOV reduced.
94
Model Before After
Phrased Based 2313 1354
Phrase Based (pre-order) 2256 1334
Supertag as factor 4361 1611
Num-Case as factor 2628 1341
Table 4: Statistics showing number of OOV be-
fore and after post processing the English-to-Hindi
translation output of Development set.
6.2 Transliteration of Untranslated Words
OOV words which were not present in the lexi-
cal translation table were then transliterated using
a naive transliteration system. The transliteration
step was applied on Hindi-to-English translation
outputs only. After transliteration we noticed frac-
tional improvements in BLEU score varying from
0.1 to 0.5.
6.3 Ranking of Ensemble MT Output
We propose a ranking framework to select the best
translation output from an ensemble of multiple
MT systems. In order to exploit the strength of
each system, we augment the translation pipeline
with a ranking module as a post processing step.
For English-to-Hindi ranking we combine the
output of both factor based models, whereas,
for Hindi-to-English ranking we combine phrase
based and phrase based with pre-ordering outputs.
For most of the systems, the output translations
are adequate but not fluent enough. So, based on
their fluency scores, we decided to rank the candi-
date translations. Fluency is well quantified by LM
log probability score and Perplexity. For a given
translation , we compute these scores by querying
the 5-gram language model built using SRILM.
Table 5 shows more than 4% relative improvement
in BLEU score for en-hi as well as hi-en transla-
tion system after applying ranking module.
Model BLEU METEOR TER
Phr(en-hi) 27.62 0.41 0.63
After Ranking (en-hi) 28.82 0.42 0.63
Phr(hi-en) 27.53 0.27 0.59
After Ranking (hi-en) 28.69 0.27 0.59
Table 5: Comparision of ranking score with base-
line
7 Primary Systems in WMT14
For English-to-Hindi, we submitted the ranked
output of factored models trained on pre-ordered
source corpus. For Hindi-to-English, we submit-
ted the ranked output of phrase based and pre-
ordered phrase based models. Table 6 shows eval-
uation scores of these systems on WMT14 test set.
Lang. pair BLEU TER
en-hi 10.4 0.83
hi-en 14.5 0.89
Table 6: WMT14 evaluation for en-hi and hi-en.
8 Conclusion
We conclude that the difficulties in English-Hindi
MT can be tackled by the use of factor based SMT
and various pre-processing and post processing
techniques. Following are our primary contribu-
tions towards English-Hindi machine translation:
? Use of supertag factors for better translation
of structurally complex sentences
? Use of number-case factors for accurately
generating noun inflections in Hindi
? Use of shallow parsing for pre-ordering Hindi
source corpus
We also observed that simple ranking strategy ben-
efits in getting the best translation from an ensem-
ble of translation systems.
References
Avramidis, Eleftherios, and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. ACL.
Banerjee, Satanjeev, and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics.
Srinivas Bangalore, Pierre Boulllier, Alexis Nasr,
Owen Rambow, and Beno?
?
ot Sagot. 2009. MICA:
a probabilistic dependency parser based on tree in-
sertion grammars application note. Proceedings of
95
Human Language Technologies The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Associ-
ation for Computational Linguistics.
A. Bharati, R. Sangal, D. M. Sharma and L. Bai.
2006. AnnCorra: Annotating Corpora Guidelines
for POS and Chunk Annotation for Indian Lan-
guages. Technical Report (TR-LTRC-31), LTRC,
IIIT-Hyderabad.
Dave, Shachi and Parikh, Jignashu and Bhattacharyya,
Pushpak. 2001. Interlingua-based English?Hindi
Machine Translation and Language Divergence
Journal Machine Translation
Gandhe, Ankur, Rashmi Gangadharaiah, Karthik
Visweswariah, and Ananthakrishnan Ramanathan.
2011. Handling verb phrase morphology in highly
inflected Indian languages for Machine Translation.
IJCNLP.
Genzel, Dmitriy. 2010. Automatically learning
source-side reordering rules for large scale machine
translation Proceedings of the 23rd international
conference on computational linguistics. Associa-
tion for Computational Linguistics
Hany Hassan, Khalil Sima?an, and Andy Way 2007.
Supertagged phrase-based statistical machine trans-
lation. Proceedings of the Association for Compu-
tational Linguistics Association for Computational
Linguistics.
Aravind K. Joshi and Yves Schabes 1991. Tree-
adjoining grammars and lexicalized grammars.
Technical Report No. MS-CIS-91-22
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. Pro-
ceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?
?
Zej Bojar, Alexan-
dra Constantin and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. Proceedings of the Second Workshop on Hy-
brid Approaches to Translation. Association for
Computational Linguistics.
Philipp Koehn and Hieu Hoang 2007. Factored Trans-
lation Models Conference on Empirical Methods in
Natural Language Processing.
Anoop Kunchukuttan, Abhijit Mishra, Rajen Chatter-
jee,Ritesh Shah, and Pushpak Bhattacharyya. 2014.
Sata-Anuvadak: Tackling Multiway Translation of
Indian Languages. Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation Conference
De Marneffe, Marie-Catherine, and Christopher
D. Manning. 2008. Stanford typed de-
pendencies manual. URL http://nlp. stanford.
edu/software/dependencies manual. pdf (2008).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. Proceed-
ings of the 40th annual meeting on association for
computational linguistics. Association for Compu-
tational Linguistics.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale and
Sasikumar M. 2013. Reordering rules for English-
Hindi SMT. Proceedings of the Second Workshop
on Hybrid Approaches to Translation. Association
for Computational Linguistics.
Ananthakrishnan Ramanathan, Pushpak Bhat-
tacharyya, Jayprasad Hegde, Ritesh M. Shah,
and M. Sasikumar. 2008. Simple syntactic and
morphological processing can help English-Hindi
statistical machine translation. In International
Joint Conference on NLP.
Sinha, RMK and Sivaraman, K and Agrawal, A and
Jain, R and Srivastava, R and Jain, A. 1995.
ANGLABHARTI: a multilingual machine aided
translation project on translation from English to In-
dian languages IEEE International Conference on
Systems, Man and Cybernetics
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics.
Zhang, Yuqi, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for sta-
tistical machine translation Proceedings of the
NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation. Association
for Computational Linguistics
Collins, Michael, Philipp Koehn, and Ivona Ku
?
cerova
2005 Clause restructuring for statistical machine
translation. Proceedings of the 43rd annual meeting
on association for computational linguistics. Asso-
ciation for Computational Linguistics
96
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 387?393,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LAYERED: Metric for Machine Translation Evaluation
Shubham Gautam
Computer Science & Engineering,
IIT Bombay
shubhamg@cse.iitb.ac.in
Pushpak Bhattacharyya
Computer Science & Engineering,
IIT Bombay
pb@cse.iitb.ac.in
Abstract
This paper describes the LAYERED met-
ric which is used for the shared WMT?14
metrics task. Various metrics exist for MT
evaluation: BLEU (Papineni, 2002), ME-
TEOR (Alon Lavie, 2007), TER (Snover,
2006) etc., but are found inadequate in
quite a few language settings like, for ex-
ample, in case of free word order lan-
guages. In this paper, we propose an MT
evaluation scheme that is based on the
NLP layers: lexical, syntactic and seman-
tic. We contend that higher layer met-
rics are after all needed. Results are pre-
sented on the corpora of ACL-WMT, 2013
and 2014. We end with a metric which is
composed of weighted metrics at individ-
ual layers, which correlates very well with
human judgment.
1 Introduction
Evaluation is an integral component of machine
translation (MT). Human evaluation is difficult
and time consuming so there is a need for a metric
which can give the better evaluation in correlation
to human judgement. There are several existing
metrics such as: BLEU, METEOR etc. but these
only deal with the lexical layer combining bag of
words and n-gram based approach.
We present an analysis of BLEU and the higher
layer metrics on the ACL WMT 2013 corpora
with 3 language pairs: French-English, Spanish-
English and German-English. For syntactic layer,
we considered three metrics: Hamming score,
Kendall?s Tau distance score and the spearman
rank score. Syntactic layer metrics take care of
reordering within the words of the sentences so
these may play an important role when there is
a decision to be made between two MT output
sentences of two different systems when both the
sentences have same number of n-gram matches
wrt the reference sentence but there is a differ-
ence in the ordering of the sentence. We will dis-
cuss these metrics in detail in the following sec-
tions. The next NLP layer in consideration is the
semantic layer which deals with the meaning of
the sentences. For semantic layer, we considered
two metrics: Shallow semantic score and Deep se-
mantic score. On semantic layer, we considered
entailment based measures to get the score.
Ananthkrishnan et al. (2007) mentioned some
issues in automatic evaluation using BLEU. There
are some disadvantages of the existing metrics
also such as: BLEU does not take care of reorder-
ing of the words in the sentence. BLEU-like met-
rics can give same score by permuting word or-
der. These metrics can be unreliable at the level
of individual sentences because there can be small
number of n-grams involved. We would see in this
paper that the correlation of BLEU is lower com-
pared to the semantic layer metrics.
Section 2 presents the study of related work in
MT evaluation. Section 3 presents the importance
of each NLP layer in evaluation of MT output. It
discusses the metrics that each layer contributes to
the achievement of the final result. In section 4,
various experiments are presented with each met-
ric on the top 10 ranking systems of WMT 13
corpora which are ranked on the basis of the hu-
man ranking. Each metric is discussed with the
graphical representation so that it would become
clear to analyze the effect of each metric. In sec-
tion 5, spearman correlation of the metrics is cal-
culated with human judgement and comparisons
are shown. In section 6, we discuss the need of a
metric which should be a combination of the met-
rics presented in the above sections and present a
weighted metric which is the amalgamation of the
metrics at individual layers. Section 7 presents the
results of the proposed metric on WMT 14 data
and compares it with other existing metrics.
387
2 Related Work
Machine translation evaluation has always re-
mained as the most popular measure to judge the
quality of a system output compared to the refer-
ence translation. Papineni (2002) proposed BLEU
as an automatic MT evaluation metric which is
based on the n-gram matching of the reference
and candidate sentences. This is still considered
as the most reliable metric and used widely in
the MT community for the determination of the
translation quality. BLEU averages the precision
for unigram, bigram and up to 4-gram and ap-
plies a length penalty if the generated sentence
is shorter than the best matching (in length) ref-
erence translation. Alternative approaches have
been designed to address problems with BLEU.
Doddington and George (2003) proposed NIST
metric which is derived from the BLEU evalua-
tion criterion but differs in one fundamental as-
pect: instead of n-gram precision, the informa-
tion gain from each n-gram is taken into account.
TER (Snover, 2006) tries to improve the hypothe-
sis/reference matching process based on the edit-
distance and METEOR (Alon Lavie, 2007) con-
sidered linguistic evidence, mostly lexical similar-
ity, for more intelligent matching. Liu and Gildea
(2005), Owczarzak et al. (2007), and Zhang et al.
(2004) use syntactic overlap to calculate the sim-
ilarity between the hypothesis and the reference.
Pad?o and Galley (2009) proposed a metric that
evaluates MT output based on a rich set of textual
entailment features. There are different works that
have been done at various NLP layers. Gim?enez
tl al. (2010) provided various linguistic measures
for MT evaluation at different NLP layers. Ding
Liu and Daniel Gildea (2005) focussed the study
on the syntactic features that can be helpful while
evaluation.
3 Significance of NLP Layers in MT
Evaluation
In this section, we discuss the different NLP layers
and how these are important for evalution of MT
output. We discuss here the significance of three
NLP layers: Lexical, Syntactic and Semantic lay-
ers.
3.1 Lexical Layer
Lexical layer emphasizes on the comparison of the
words in its original form irrespective of any lexi-
cal corpora or any other resource. There are some
metrics in MT evaluation which considers only
these features. Most popular of them is BLEU,
this is based on the n-gram approach and consid-
ers the matching upto 4-grams in the reference and
the candidate translation. BLEU is designed to
approximate human judgement at a corpus level,
and performs badly if used to evaluate the quality
of individual sentences. Another important metric
at this layer is TER (Translation Edit Rate) which
measures the number of edits required to change
a system output into one of the references. For
our experiments, we would consider BLEU as the
baseline metric on lexical layer.
3.2 Syntactic Layer
Syntactic layer takes care of the syntax of the
sentence. It mainly focusses on the reordering
of the words within a sentence. Birch and Os-
borne (2011) has mentioned some metrics on this
layer: Hamming score and Kendall?s Tau Dis-
tance (KTD) score. We additionally calculated the
spearman rank score on this layer. Scores are cal-
culated first by giving ranking of words in the ref-
erence sentence and then putting the rank number
of the word in the candidate sentence. Now, we
have the relative ranking of the words of both the
sentences, so final score is calculated.
3.3 Semantic Layer
Semantic layer goes into the meaning of the sen-
tence, so we need to compare the dependency tree
of the sentences. At this layer, we used entailment
based metrics for the comparison of dependencies.
Pad?o and Galley (2009) illustrated the use of text
entailment based features for MT evaluation. We
introduced two metrics at this layer: first is Shal-
low semantic score, which is based on the depen-
dencies generated by a shallow parser and then
the dependency comparison is carried out. Sec-
ond is Deep semantic score, which goes more deep
into the semantic of the sentence. For shallow se-
mantic score, we used stanford dependency parser
(Marie-Catherine et al., 2006) while for deep se-
mantic score, we used UNL (Universal Network-
ing Language)
1
dependency generator.
Semantic layer may play an important role
when there are different words in two sentences
but they are synonym of each other or are related
to each other in some manner. In this case, lexical
and syntactic layers can?t identify the similarity of
1
http://www.undl.org/unlsys/unl/unl2005/UW.htm
388
the sentences because there exist a need of some
semantic background knowledge which occurs at
the semantic layer. Another important role of se-
mantic layer is that there can be cases when there
is reordering of the phrases in the sentences, e.g.,
active-passive voice sentences. In these cases, de-
pendencies between the words remain intact and
this can be captured through dependency tree gen-
erated by the parser.
4 Experiments
We conducted the experiments on WMT 13 cor-
pora for French-English, Spanish-English and
German-English language pairs. We calculated
the score of each metric for the top 10 ranking
system (wmt, 2013) (as per human judgement) for
each language pair.
Note:
1. In the graphs, metric score is multiplied by 100
so that a better view can be captured.
2. In each graph, the scores of French-English (fr-
en), Spanish-English (es-en) and German-English
(de-en) language pairs are represented by red,
black and blue lines respectively.
4.1 BLEU Score
Figure 1: BLEU Score
We can see from the graph of fig. 1 that for de-
en and es-en language pair, BLEU is not able to
capture the phenomenon appropriately. In fact, it
is worse in de-en pair. Because the graph should
be of decreasing manner i.e., as the rank of the sys-
tem increases (system gets lower rank compared to
the previous one), the score should also decrease.
4.2 Syntactic Layer
Because the BLEU score was not able to capture
the idealistic curve in the last section so we consid-
ered the syntactic layer metrics. This layer is con-
sidered because it takes care of the reordering of
the words within the sentence pair. The idea here
is that if one candidate translation has lower re-
ordering of words w.r.t. reference translation then
it has higher chances of matching to the reference
sentence.
4.2.1 Hamming Score
The hamming distance measures the number of
disagreements between two permutations. First
we calculate the hamming distance and then cal-
culate the fraction of words placed in the same po-
sition in both sentences, finally we calculate the
hamming score by subtracting the fraction from 1.
It is formulated as follows:
d
h
(pi, ?) = 1?
?
n
i=1
x
i
n
, x
i
=
{
0; if pi(i) = ?(i)
1; otherwise
where, n is the length of the permutation.
Hamming scores for all three language pairs
mentioned above are shown in fig. 2. As we can
see from the graph that initially its not good for the
top ranking systems but it follows the ideal curve
for the discrimination of lower ranking systems for
the language pairs fr-en and es-en.
Figure 2: Hamming Score
4.2.2 Kendall?s Tau Distance (KTD)
Kendall?s tau distance is the minimum number of
transpositions of two adjacent symbols necessary
to transform one permutation into another. It rep-
resents the percentage of pairs of elements which
389
share the same order between two permutations. It
is defined as follows:
d
k
(pi, ?) = 1?
?
?
n
i=1
?
n
j=1
z
ij
Z
where, z
ij
=
{
0; if pi(i) < pi(j) and ?(i) < ?(j)
1; otherwise
This can be used for measuring word order dif-
ferences as the relative ordering of words has been
taken into account. KTD scores are shown in fig.
3. It also follows the same phenomenon as the
hamming score for fr-en and es-en pair but for de-
en pair, it gives the worst results.
Figure 3: KTD Score
4.2.3 Spearman Score
Spearman rank correlation coefficient is basically
used for assessing how well the relationship be-
tween two variables can be described using a
monotonic function. Because we are using syntac-
tic layer metrics to keep track of the reordering be-
tween two sentences, so this can be used by rank-
ing the words of the first sentence (ranging from
1 to n, where n is the length of the sentence) and
then checking where the particular word (with in-
dex i) is present in the second sentence in terms of
ranking. Finally, we calculated the spearman score
as follows:
? = 1?
6
?
d
2
i
n(n
2
? 1)
where, d
i
= x
i
? y
i
is the difference between
the ranks of words of two sentences.
Spearman score lies between -1 to +1 so we
convert it to the range of 0 to +1 so that all the
metrics would lie in the same range.
4.3 Semantic Layer
We can see from the last two sections that there
were some loopholes on the metrics of both the
layers as can be seen in the graphical representa-
tions. So, there arises a need to go higher in the
hierarchy. The next one in the queue is semantic
layer which takes care of the meaning of the sen-
tences. At this layer, we considered two metrics.
Both metrics are based on the concept of text en-
tailment. First we should understand, what is it?
Text Entailment
According to wikipedia
2
, ?Textual entailment
(TE) in natural language processing is a direc-
tional relation between text fragments. The rela-
tion holds whenever the truth of one text fragment
follows from another text. In the TE framework,
the entailing and entailed texts are termed text (t)
and hypothesis (h), respectively.?
First, the dependencies for both reference (R)
as well as candidate (C) translation are generated
using the parser that is used (will vary in both
the following metrics). Then, the entailment phe-
nomenon is applied from R to C i.e., dependencies
of C are searched in the dependency graph of R.
Matching number of dependencies are calculated,
then a score is obtained as follows:
Score
R?C
=
No. of matched dependencies of C in R
Total no. of dependencies of C
(1)
Similarly, another score is also obtained by ap-
plying the entailment phenomenon in the reversed
direction i.e. from C to R as follows:
Score
C?R
=
No. of matched dependencies of R in C
Total no. of dependencies of R
(2)
Final score is obtained by taking the average of
the above two scores as follows:
Score
final
=
Score
R?C
+ Score
C?R
2
(3)
Now, we discuss how can we use this concept
in the metrics at semantic layer:
4.3.1 Shallow Semantic Score
This metric uses the stanford dependency parser
(Marie-Catherine et al., 2006) to generate the de-
pendencies. After getting the dependencies for
both reference (R) as well as candidate (C) trans-
lation, entailment phenomenon is applied and the
final score is obtained using eq. (3).
2
http://wikipedia.org/
390
Figure 4: Shallow Semantic Score
We can see from fig. 4 that for French-English
and Spanish-English pairs, the graph is very good
compared to the other metrics at the lower layers.
In fact, there is only one score in es-en pair that
a lower ranking system gets better score than the
higher ranking system.
4.3.2 Deep Semantic Score
This metric uses the UNL dependency graph gen-
erator for taking care of the semantic of the sen-
tence that shallow dependency generator is not
able to capture. Similar to the shallow seman-
tic score, after getting the dependencies from the
UNL, entailment score is calculated in both direc-
tions i.e. R? C and C? R.
Figure 5: Deep Semantic Score
Fig. 5 shows that deep semantic score curve
also follows the same path as shallow semantic
score. In fact, for Spanish-English pair, the path
is ideal i.e., the score is decreasing as the system
rank is increasing.
5 Correlation with Human Judgement
We calculated spearman rank correlation coeffi-
cient for the different scores calculated in the last
section. This score ranges from -1 to +1. Form ta-
Language Pair ?
BLEU
?
Shallow
?
Deep
French-English 0.95 0.96 0.92
Spanish-English 0.89 0.98 1.00
German-English 0.36 0.88 0.89
Table 1: Correlation with BLEU Score, Shallow
Semantic Score and Deep Semantic Score
ble 1, we can see that the correlation score is bet-
ter with semantic layer metrics compared to the
BLEU score (lower layer metrics). In compar-
ison to the WMT 13 results (wmt-result, 2013),
?
Shallow
score for French-English pair is interme-
diate between the highest and lowest correlation
system. ?
Deep
score for Spanish-English is high-
est among all the systems presented at WMT 13.
So, it arises a need to take into account the seman-
tic of the sentence while evaluating the MT output.
6 Hybrid Approach
We reached to a situation where we can?t ig-
nore the score of any layer?s metric because each
metric helps to capture some of the phenomenon
which other may not capture. So, we used a hy-
brid approach where the final score of our pro-
posed metric depends on the layered metrics. As
already said, we performed our experiments on
ACL-WMT 2013 corpora, but it provided only the
rank of the systems. Due to availability of ranking
of the systems, we used SVM-rank to learn the pa-
rameters.
Our final metric looks as follows:
Final-Score = a*BLEU + b*Hamming + c*KTD
+ d*Spearman + e*Shallow-Semantic-Score +
f*Deep-Semantic-Score
where, a,b,c,d,e,f are parameters
6.1 SVM-rank
SVM-rank learns the parameters from the training
data and builds a model which contains the learned
parameters. These parameters (model) can be used
for ranking of a new set of data.
Parameters
We made the training data of the French-English,
Spanish-English and German-English language
391
Metric
Pearson Correlation
fr-en de-en hi-en cs-en ru-en Average
LAYERED .973 .893 .976 .940 .843 .925
BLEU .952 .831 .956 .908 .774 .884
METEOR .975 .926 .457 .980 .792 .826
NIST .955 .810 .783 .983 .785 .863
TER .952 .774 .618 .977 .796 .823
Table 2: Correlation with different metrics in WMT 14 Results
pairs. Then we ran SVM-rank and obtained the
scores for the parameters.
So, our final proposed metric looks like:
Final-Score = 0.26*BLEU + 0.13*Hamming +
0.03*KTD + 0.04*Spearman + 0.28*Shallow-
Semantic-Score + 0.26* Deep-Semantic-Score
7 Performance in WMT 2014
Table 2 shows the performance of our metric on
WMT 2014 data (wmt-result, 2014). It performed
very well in almost all language pairs and it
gave the highest correlation with human in Hindi-
English language pair. On an average, our corre-
lation was 0.925 with human considering all the
language pairs. This way, we stood out on sec-
ond position considering the average score while
the first ranking system obtained the correlation
of 0.942. Its clear from table 2 that the proposed
metric gives the correlation better than the stan-
dard metrics in most of the cases. If we look at
the average score of the metrics in table 2 then we
can see that LAYERED obtains much higher score
than the other metrics.
8 Conclusion
Machine Translation Evaluation is an exciting
field that is attracting the researchers from the past
few years and the work in this field is enormous.
We started with the need of using higher layer
metrics while evaluating the MT output. We un-
derstand that it might be a little time consuming
but its efficient and correlation with human judge-
ment is better with semantic layer metric com-
pared to the lexical layer metric. Because, each
layer captures some linguistic phenomenon so we
can?t completely ignore the metrics at individual
layers. It gives rise to a hybrid approach which
gives the weightage for each metric for the calcu-
lation of final score. We can see from the results
of WMT 2014 that the correlation with LAYERED
metric is better than the standard existing metrics
in most of the language pairs.
References
Alexandra Birch, School of Informatics, University of
Edinburgh Reordering Metrics for Statistical Ma-
chine Translation. Phd Thesis, 2011.
Alexandra Birch and Miles Osborne Reordering Met-
rics for MT. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, se-
ries = HLT 2011.
Alon Lavie and Abhaya Agarwal. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments, Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, StatMT 2007.
Ananthakrishnan R and Pushpak Bhattacharyya and M
Sasikumar and Ritesh M Shah Some Issues in Auto-
matic Evaluation of English-Hindi MT: More Blues
for BLEU. ICON, 2007.
Doddington and George Automatic evaluation of
machine translation quality using N-gram co-
occurrence statistics, NIST. Proceedings of the
2nd International Conference on Human Language
Technology Research HLT 2002.
Ding Liu and Daniel Gildea Syntactic Features for
Evaluation of Machine Translation. Workshop On
Intrinsic And Extrinsic Evaluation Measures For
Machine Translation And/or Summarization, 2005.
Findings of the 2013 Workshop on Statistical Machine
Translation. ACL-WMT 2013.
Gim?enez, Jes?us and M`arquez, Llu??s Linguistic Mea-
sures for Automatic Machine Translation Evalua-
tion. Machine Translation, December, 2010.
Liu D, Gildea D Syntactic features for evaluation of
machine translation. ACL 2005 workshop on intrin-
sic and extrinsic evaluation measures for machine
translation and/or summarization.
Owczarzak K, Genabith J, Way A Evaluating ma-
chine translation with LFG dependencies. Machine
Translation 21(2):95119.
392
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC
2006.
Matthew Snover and Bonnie Dorr and Richard
Schwartz and Linnea Micciulla and John Makhoul.
A Study of Translation Edit Rate with Targeted Hu-
man Annotation, In Proceedings of Association for
Machine Translation in the Americas, 2006.
Papineni, Kishore and Roukos, Salim and Ward, Todd
and Zhu, Wei-Jing. BLEU: A Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL 2002.
Results of the WMT13 Metrics Shared Task. ACL-
WMT 2013.
Results of the WMT14 Metrics Shared Task. ACL-
WMT 2014.
Sebastian Pad?o and Michel Galley and Dan Jurafsky
and Chris Manning Robust Machine Translation
Evaluation with Entailment Features. Proceedings
of ACL-IJCNLP 2009, ACL 2009.
Zhang Y, Vogel S, Waibel A Interpreting Bleu/NIST
scores: how much improvement do we need to have
a better system?. In: Proceedings of the 4th interna-
tional conference on language resources and evalua-
tion. Lisbon, Portugal.
393
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 28?36,
Dublin, Ireland, August 23-29 2014.
A Framework for Learning Morphology using Suffix Association 
Matrix  
Shilpa Desai 
Department of Computer 
Science and Technology 
Goa University, Goa, 
India 
sndesai@gmail.com 
Jyoti Pawar 
Department of Computer 
Science and Technology  
Goa University, Goa, 
India 
jyotidpawar@gmail.com 
Pushpak Bhattacharyya 
Department of Computer 
Science and Engineering 
IIT, Powai, 
Mumbai India 
pb@cse.iitb.ac.in 
|| ?? ?????? ??: || 
  
Abstract 
Unsupervised learning of morphology is used for automatic affix identification, morphological segmentation of 
words and generating paradigms which give a list of all affixes that can be combined with a list of stems. 
Various unsupervised approaches are used to segment words into stem and suffix. Most unsupervised methods 
used to learn morphology assume that suffixes occur frequently in a corpus. We have observed that for 
morphologically rich Indian Languages like Konkani, 31 percent of suffixes are not frequent. In this paper we 
report our framework for Unsupervised Morphology Learner which works for less frequent suffixes. Less 
frequent suffixes can be identified using p-similar technique which has been used for suffix identification, but 
cannot be used for segmentation of short stem words. Using proposed Suffix Association Matrix, our 
Unsupervised Morphology Learner can also do segmentation of short stem words correctly. We tested our 
framework to learn derivational morphology for English and two Indian languages, namely Hindi and Konkani. 
Compared to other similar techniques used for segmentation, there was an improvement in the precision and 
recall. 
1 Introduction 
Learning morphology by a machine is crucial for tasks like stemming, machine translation etc.  Rule 
based affix stripping approach, semi-supervised, unsupervised learning of morphology and finite state 
approach as some of the well known methods used to learn morphology by a machine. Rule based 
affix stripping approaches (Lovins, 1968; Porter, 1980; Paice, 1990; Loftsson, 2008; Maung et. al, 
2008) depend heavily on linguistic input and require a lot of human effort, especially for 
morphologically rich languages. Pure unsupervised approaches learn morphology from a corpus 
(Freitag, 2005; Goldsmith, 2001; Hammarstr?m, 2011). The accuracy of pure unsupervised methods is 
relatively low. Semi-supervised approaches use minimal linguistic input and unsupervised methods to 
automate morphology learning process (Forsberg, 2007; Lind?n, 2008; Chan, 2008; Dreyer, 2011). 
Semi-supervised approaches perform better than pure unsupervised approaches. Finite state 
approaches (Koskenniemi, 1983; Beesley & Kartunnen, 2003) represent morphology using finite state 
machines. Finite state approaches require linguistic input in the form of paradigm identification. 
Unsupervised and semi-supervised methods can provide input to build finite state based morphology 
systems reducing the time taken to build such systems.  
In this paper we report the framework for an Unsupervised Morphology Learner. Most 
unsupervised segmentation techniques (Freitag, 2005; Goldsmith, 2001; Hammarstr?m, 2011) which 
learn morphology from a corpus assume that suffixes are frequent in a corpus. We observed that for 
morphologically rich Indian languages like Hindi and Konkani, the assumption that suffixes are 
frequent does not hold true. These languages are morphologically rich and 31 percent of verb suffixes 
are not frequent in the corpus. Thus, we choose not to make any such assumption about the frequency 
of suffix occurrence in our unsupervised learning of morphology. One promising methodology for 
unsupervised segmentation which does not make any suffix frequency assumptions is p-similar 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings 
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
28
technique for morpheme segmentation first proposed by Gaussier (1999). Researchers have used this 
method for suffix identification and not for segmentation (Gaussier, 1999; Sharma, 2006). We 
extended this less studied technique to segment words by introducing the concept of suffix association 
matrix, thus giving us an unsupervised method which correctly identifies suffixes irrespective of their 
frequency of occurrence in the corpus and also segments short stem words. To the best of our 
knowledge, most reported work which uses p-similar technique for suffix identification (Gaussier, 
1999; Sharma, 2006) enforce a restriction on stem-length that it should be at least five. This restriction 
works well for suffix identification but not for segmentation. For Indian languages like Hindi and 
Konkani, we observed that the restriction leads to an inability to segment many words with short stem-
length.  Especially many verb stems in Indian languages have stem-length less than five. To overcome 
this shortcoming, we have proposed an Unsupervised Morphology Learner (UML) framework.  
We implemented UML framework for derivational morphology and tested our method for English 
language and two Indian languages namely Konkani and Hindi. The rest of the paper is organized as 
follows; section 2 is on related work. Section 3 provides the terminology used in the paper. The 
motivation for this work is presented in section 4. Unsupervised Morphology Learner (UML) 
framework is presented in section 5. Experimental results are discussed in section 6 and finally we 
conclude the paper in section 7. 
2 Related Work 
Unsupervised learning of morphology is done at different levels, namely, affix list identification, 
segmenting word into stem and affix, and generating a list of paradigms i.e. a list of all stems with 
information of the suffixes that each stem combines with (Hammarstr?m, 2011).  In his survey paper, 
Hammarstr?m (2011) summarizes work related to unsupervised morphology. Most recent work in 
morphology learning is semi-supervised. Such methods use a small set of example paradigms as input 
to train the system and classify unseen words into paradigms or learn new paradigms (Lind?n, 2009; 
Dreyer, 2011).  
A popular pure unsupervised morphology technique was first proposed by Goldsmith (2001) which 
does not assume any linguistic input. Goldsmith (2001) introduced a set of heuristics that develops a 
probabilistic morphological grammar, and used Minimum Description Length (MDL) as a tool to 
evaluate it. The technique used for affix and paradigm identification was based on affix occurrence 
frequency. Several different authors have appreciated MDL as the motivation for segmentation. Some 
authors (Gelbukh et. al., 2004; Bacchin, 2005) have used random segmentation and picked the best 
segmentation to minimize size or find splits where constituent morphemes occur in multiple splits. 
Our work is inspired by a less studied p-similar technique proposed by Gaussier (1999). p-similar 
techniques have been used for suffix identification rather than segmentation in most related 
unsupervised morphology learners (Sharma, 2006). Here the restriction on stem-length first proposed 
by Gaussier is upheld.  Sharma?s (2006) work deals with neutral suffix only and does not capture non-
neutral suffixes. These studies are limited to suffix identification and do not generate paradigms.      
3 Terminology Used 
Let L be a language with alphabet set ?.  
W= {w| w ? ?*} be set of valid words in language L. 
Let d: W?W denote a derivation function where d(wx)=wy iff words wx and wy are derivationally 
related to each other in L.  
Let wxsy denote concatenation of strings wx and sy where wx, sy ? ?*. 
Let SN be set of neutral derivational suffixes. 
SN = {s|w2=w1s and w2,w1?W and d(w1)=w2 and s? ?*} 
For example, when s=er, w1=farm and w2=farmer 
Let SB be set of non-neutral derivational suffixes. 
SB = {sx,sy|wsx=wsy and d(wsx)=wsy and w, sx, sy? ?* and w?W } 
For example, when sx=ify, sy=ity and w=quant suffixes ify, ity are non neutral suffixes. 
29
4 Motivation 
Primarily, frequency based suffix identification techniques (Goldsmith, 2001; Hammarstr?m, 2011) 
commonly used in recent times, fail to identify suffixes with low frequency. We explored suffix 
identification techniques which could identify suffixes irrespective of frequency of occurrence in the 
corpus. We chose one such method p-similar technique. However p-similar technique (Gaussier, 1999) 
cannot be used directly for segmentation as it results in a high number of false positives. Hence we 
proposed a suffix association matrix to avoid the false positives. According to p-similar technique, 
given two words x, y ? W, if ? b1 such that x=b1s1 and y=b1s2 where b1, s1, s2 ? ?+, then b1 is a stem  
and s1, s2 are suffixes, provided they satisfy the following conditions: 
a. A suffix is valid only when it occurs with at least two different stems 
b. A stem is valid when it occurs with at least two identified suffixes  
c. Stem length should be five or more 
The third condition on stem length was introduced to improve the precision of the suffix list 
generated. However the aim was to only generate a suffix list and not segment word into stem + suffix. 
We probed the possibility of applying this effective p-similar technique to segment words. We faced 
the following issues when trying to use p-similar technique for segmentation:  
? The technique failed for short-stem length words because of the restriction placed on stem-length. 
Example words with stem like walk, talk are not segmented. 
?  When words like addiction, addictive, aggression and aggressive are part of the input, suffixes 
identified are ?on? and ?ve? in place of ?ion? and ?ive?. This problem is called over-stemming.      
?  When words like cannon, cannot, America, American, agent, agency are part of the input, ?n? and 
?t? are identified as suffix. Although ?n? and ?t? are valid suffix for some words, 
cannon=canno+n and cannot=canno+t are wrong segmentation.  
We realize that the candidate stem-suffix pair bi+si identified using the p-similar technique falls under 
one of the following cases: 
Case 1: bi is a valid stem and si is a valid suffix for stem bi. For example, mistake+NULL,  
mistake+n are valid. Suffixes NULL and n are valid for stem mistake. 
Case 2: bi is an invalid stem and si is a invalid suffix. Example addicti+on and addicti+ve and 
aggressi+on and aggressi+ve are invalid; addict+ion and addict+ive and aggress+ion and 
aggress+ive are valid. 
Case 3: bi is a valid stem and si is a invalid suffix for stem bi. For example year+n is invalid. 
Suffix n is invalid for stem year while suffix NULL and ly are valid for stem year. 
Case 4: bi is an invalid stem for any suffix and si is valid for some other stem. Example canno+n 
and canno+t are invalid pairs; absen-ce and absen-t and valid; mistake+NULL and mistake+n are 
valid. 
To overcome the problems faced in cases 2, 3 and 4 we have proposed the following framework 
 
5 Unsupervised Morphology Learner Framework 
UML can be used to learn derivational morphology or inflectional morphology. When the input given 
is a lexicon, the framework will learn derivational morphology. If a corpus is used as input it will learn 
both derivational and inflectional morphology and not distinguish between the two. We have tested 
our framework with lexicon as input to learn derivational morphology. The framework for the 
proposed UML is shown below in Figure 1. UML has five modules. It uses a lexicon resource or a 
corpus as input. It generates three final resources and two intermediate resources which are enhanced 
into the final resources.  
The resource used as input could be: 
? Lexicon L: It is list of dictionary words found in the language. This resource is generated from a 
WordNet of a language used to learn derivational morphology  or  
? Corpus C: A collection of un-annotated text used to learn both inflectional and derivational 
morphology. 
30
The intermediate resource generated: 
? Candidate Stem-Suffix List: It is the initial list of stems and suffixes identified for an input 
language using the p-similar technique. It consists of two sets namely set of suffix Ssuffix and set of 
stem Sstem. Sample entries in these set for English language are Ssuffix = { er, ic, ly, ness, ment, ?} 
and Sstem= {adorn, attack,?.} 
? Initial Paradigms: This is a list of all stems with information of which suffixes combine with 
which stems in the input lexicon L or Corpus. Sample entry in Initial Paradigms List  is  ic.y= 
academ + allerg + geometr + homeopath + horrif + letharg + majest + prehistor + specif + 
strateg  where ?ic? and ?y? are suffixes which combine with the stems like adadem.  
The final resources generated: 
? Stem-Suffix List: This resource is generated from the Candidate Stem-Suffix List resource by 
pruning invalid suffixes. It is a useful resource as it gives the stems of words from a lexicon which 
could later be used for identifying stems in a corpus for stemming inflectional words.  
? Suffix-Association Matrix: This resource helps us identify for how many instances a suffix s1 has 
occurred with a suffix s2 in the Lexicon/Corpus. It is a crucial resource in eliminating the 
shortcoming of p-similar technique to morphologically segment words with short stem length as 
well as overcome chance association of suffix found.  
? Morphology Paradigms: This resource contains paradigms extracted from the words found in the 
input lexicon/corpus. It is a refined version of Initial Paradigm resource. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Unsupervised Morphology Learner (UML) Framework 
 
UML comprises of five main modules, a brief description and algorithm for each of the module is 
given below: 
 
Module 1 - Suffix Identifier  
Description: Identifies the Candidate suffixes using p-similar technique. It generates a temporary 
resource namely Candidate Stem-Suffix List. For every word in the corpus, it checks if there is another 
word with a common stem, adds common stem to stem list and rest to suffix list, provided that a stem 
occurs with more than one suffix and a suffix occurs with more than one stem. 
Input: Lexicon  /Corpus  
Suffix 
Identifier 
Output: Morphology Paradigms 
Morphology Paradigm Generator 
Stem-Suffix  
Pruner 
Suffix Association Matrix 
Generator 
Candidate Stem-Suffix 
List 
Stem-Suffix List 
Primary Paradigm 
Generator 
Initial Paradigms 
Suffix Association Matrix 
31
Input: Lexicon of the language L (or raw un-annotated corpus for inflectional morphology C) 
Output: Candidate Stem-Suffix List resource 
Algorithm: 
For each input word p ? L,  
find q, r, s ? L, such that ? b1, b2, b3  
where p=b1s1, q=b1s2, r=b2s1, s= b2s3 where b1, b2, b3, s1, s2, s3 ? ?*..  
Add b1 to set of stems Sstem,   
Add s1 to set of suffixes Ssuffix,  
EndFor 
 
Module 2 - Stem-suffix pruner:  
Description: This module applies heuristic H1 stated below. H1 is framed to correct the stem-suffix 
list to fix the problem of over-stemming.  
H1: Given   suffix si for stem bi if ? a ? ?* such that  asi ? Ssuffix and bja=bi and bj? Sstem where Sstem is 
set of stems and Ssuffix is set of suffixes then replace bi by bj and si by asi  
Input: Candidate Stem-Suffix List resource 
Output: Stem-Suffix List resource 
Algorithm:  
For each suffix s1 from suffix list,  
If ? a ? ?* such that as1 ? Ssuffix and b2a=b1 and b1, b2? Sstem then  
replace b1 by b2 and s1 by as1.  
EndIf 
EndFor 
 
Module 3 - Primary Paradigm Generator:  
Description: Using Stem-Suffix List this module generates the Initial Paradigms list.  A paradigm 
is composed of suffixes that go together for a list of stems in the input lexicon/corpus.  
Input: Stem-Suffix List resource 
Output: Initial Paradigms resource  
Algorithm: 
For each input word p ? L, if p=b1s1 where b1? Sstem and s1? Ssuffix.  
Set paradigm-string= s1.  
For every q ? L such that q= b1s2 where b1? Sstem and s2 ? Ssuffix ,  
Set paradigm-string = paradigm-string.s2.  
Add paradigm-string to Sparadigm, set of paradigm.  
EndFor 
EndFor 
For each paradigm-string p1 ? Sparadigm where p1 =?sx1.sx2 ?sxn=b1?  
                                                              and sx1,sx2 , ?, sxn? Ssuffix and b1? Sstem 
Set collapse-paradigm-string = sx1.sx2 ?sxn=b1 
If ? paradigm-string p2? Sparadigm such that p2 =? sx1.sx2 ?sxn =b2? and b2? Sstem 
Set collapse-paradigm-string = collapse-paradigm-string + b2 
Add collapse-paradigm-string to Sinitial-paradigm, set of Initial Paradigms 
EndIf
 
EndFor 
 
Module 4- Suffix Association Matrix Generator:  
Description: From the Initial Paradigms, this module computes the Suffix Association Matrix 
resource. Suffix association matrix is a square matrix where each row and column corresponds to a 
suffix in suffix list. An entry in this matrix gives how many times a particular suffix occurs with 
another suffix in the Initial Paradigms resource.  
Input: Initial Paradigms resource  
Output: Suffix Association Matrix resource 
32
Algorithm: 
Let M be suffix association matrix which is | Ssuffix| * | Ssuffix|. If Ssuffix = {s1, s2, ?..sp} M 
has dimension p X p.
  
Initialize M=0; 
For each paradigm-string p1 ? Sinitial-paradigm where p1 =?sx1.sx2 ?sxn=b1+ b2+ b3+?+ bm? 
For i= 1 to n  
For j= i+1 to n 
  M[sxi ][sxj]= M[sxi ][sxj] + m;    where sxi = sq and sxi = sr and 1<= q, r <=p 
EndFor 
 EndFor 
EndFor  
 
Module 5 - Morphology Paradigm Generator:  
Description: Using Stem-Suffix List and Suffix Association Matrix this module generates 
Morphology Paradigms List resource. It is a pruned version of Initial Paradigms resource which 
uses Suffix Association matrix to remove less likely suffix combination in Initial Paradigms 
Input: Stem-Suffix List resource 
Output: Initial Paradigms resource  
Algorithm: 
For each input word p ? L, if p=b1s1 where b1? Sstem and s1? Ssuffix.  
Set paradigm-string= s1.  
For every q ? L such that q= b1s2 where b1? Sstem and s2 ? Ssuffix ,  
If M[s1][s2] > threshold value 
Set paradigm-string = paradigm-string.s2.  
Add paradigm-string to Sparadigm, set of paradigm.  
EndIf 
EndFor 
EndFor 
For each paradigm-string p1 ? Sparadigm where p1 =?sx1.sx2 ?sxn=b1?  
                                                              and sx1,sx2 , ?, sxn? Ssuffix and b1? Sstem 
Set collapse-paradigm-string = sx1.sx2 ?sxn=b1 
If ? paradigm-string p2? Sparadigm such that p2 =? sx1.sx2 ?sxn =b2?  and b2? Sstem 
Set collapse-paradigm-string =  collapse-paradigm-string + b2 
Add collapse-paradigm-string to Sinitial-paradigm, set of  Initial_Paradigms 
EndIf
 
EndFor 
5.1 Significance of Suffix Association Matrix 
Suffix association matrix is a measure of how many times a particular suffix is associated with 
another suffix in the input resource. It is an important contribution as it provides us an alternate 
way to prune invalid stem-suffix pairs identified, rather than a restriction on the stem-length.  
Suffixes which are associated with each other more frequently are more likely to provide a correct 
paradigm than those where we find only a few chance instances of suffix associations.  
Figure 2 illustrates an instance of suffix association matrix for the English language 
 
 NULL er ing ly 
NULL - 46 225 129 
er 46 - 22 15 
ing 225 22 - 0 
ly 129 15 0 - 
Figure 2: Instance of Suffix Association Matrix  
 
This matrix helps handle valid stem with invalid suffix case. For instance wrong segmentation of 
the word ?bother? as ?both+er?. From the Suffix Association Matrix we check with which 
33
suffixes er is commonly associated. We then make a list of words with stem ?both? and other 
suffix which commonly associate with suffix ?er? like suffix ?ing? We search a corpus for 
existence of such words like ?bothing?. Thus rejecting the segmentation bother=both+er.  This 
matrix also provides a solution to invalid stem with valid suffix. For instance canno+n and 
canno+t are invalid segmentations although the suffix ?n? and ?t? are valid in some other 
context. In such a rare association of a suffix ?n? and ?t? the corresponding entry in the suffix 
association matrix is found to be very low. We ran our algorithm for various values of threshold 
and found five as an optimal value.  Any suffix association below five were pruned as chance 
associations. 
5.2 Significance of heuristic H1 
This heuristic is used to handle the problem of over-stemming that occurs in p-similar technique. For 
example the p-similar technique identifies both ?ion? and ?on? as suffix. While segmenting a word 
like ?addiction? we need to decide if ?addicti+on? or ?addict+ion? is correct. H1 helps us in 
correctly segmenting the word as ?addict+ion?.  
5.3 Limitations of UML 
UML is restricted to identify concatenative morphology paradigms only. Presently it identifies 
suffixes only and does not support irregular morphology wherein the stem undergoes a change before 
suffixation.  
6 Experimental Results 
The implementation of UML is done in Java. After applying our method, the paradigms obtained were 
compared to the paradigms obtained using p-similar method with minimum stem-size five. The 
precision was computed as ratio of number of words correctly segmented to total number of words 
segmented. Recall is computed as ratio of number of words correctly segmented to number of words 
in given input which could be segmented.  The results have been tabulated in Table 1 below. 
 
Method Number of 
Paradigms  
Recall Precision F-Score 
Language : English 
Data Set: English lexicon with 21813 entries was obtained from the English WordNet1 
p-similar with stems size >5  1163 0.85 0.93 0.89 
UML for derivational morphology 413 0.92 0.93 0.92 
Language : Hindi  
Data Set: Hindi lexicon with 23807 entries was extracted from the Hindi WordNet2 
p-similar with stems size >5 1127 0.83 0.87 0.85 
UML for derivational morphology 332 0.87 0.94 0.90 
Language : Konkani  
Data Set: Konkani lexicon with 25838 entries was extracted from the Konkani WordNet3 
p-similar with stems size >5 1088 0.75 0.77 0.75 
UML for derivational morphology 274 0.87 0.87 0.87 
Table 1: Results for English, Hindi and Konkani Language 
                                                 
1
 http://wordnet.princeton.edu/wordnet/download/ 
2
 http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
3
 http://konkaniwordnet.unigoa.ac.in 
34
6.1 Effect of stem length on recall  
We list below in Table 2, a few examples of how recall is reduced as words with short stem length 
are not segmented, when the minimum stem size is five.  
Language  Suffix for which 
word not 
segmented 
Number of 
words not 
segmented 
Few examples of words not segmented 
English er 9 eater, farmer, owner...  
Hindi ??4  
(I;;Hindi suffix) 
35 ???? (arabic; arab; name of a country), 
???? (aalas; lazy; ), ????? (aasani; 
easiness; ) 
Konkani ??  
(I;;Konkani 
suffix) 
43 ????? (anandi; being happy; ),   ????? 
(aaropi; accused; ) 
Table 2: Effect of stem length 
We observe that number of words not segmented in English is relatively very less as compared to the 
Indian languages Hindi and Konkani. Thus the restriction on stem-length works efficiently for English 
as compared to the Indian languages Hindi and Konkani.  
6.2 Effect of stem length on precision 
When we restrict the stem-length to five we observe that some wrong segmentation of words are 
pruned. Listed below in Table 3, are some examples 
Language  Suffix for 
which word not 
segmented 
Number of 
words not 
segmented 
Few examples of words not segmented 
(wrongly) 
English er 32 bother, boxer, cater, sober ? 
Hindi ??  (I;;Hindi 
suffix) 
8 ????? (chandi; silver; ), ???? (choti; 
peak;) 
Konkani ?? (I;;Konkani 
suffix) 
6 ??? (Aaji; grandmother; ),  ???? 
(kaalli; black; ) 
Table 3: Effect of stem-length on precision 
We observe that for English, many word segmentations with stems-length less than five, identified 
by p-similar technique are correctly pruned by applying the restriction. We observe that wrong 
segmentations in case of Indian languages Hindi and Konkani are less when compared to English.  
7 Conclusion  
Unsupervised Morphology Learner framework thus can be effectively used to generate paradigms for 
Indian languages which have low frequency suffixes and words with short stem lengths. Suffix 
Association Matrix and heuristics H1 is advantageous over p-similar technique with stem length 
restriction for languages like Konkani and Hindi which have many short length valid stems. The 
derivational suffixes obtained from UML with Lexicon as input can be used to distinguish from 
inflectional morphology suffixes when the framework is used with a corpus as input.    
                                                 
4
 A word in Indian language is followed by transliteration in Roman Script, translation in English and gloss in brackets 
35
 Reference 
Bacchin, M., Ferro, N., and Melucci, M. (2005). A probabilistic model for stemmer generation. Information 
Processing and Management, 41(1):121?137. 
Beesley K & Karttunen Lauri. 2003. Finite State Morphology. Stanford, CA: CSLI Publications. 
Chan, E. 2008. Structures and Distributions in Morphology Learning. Ph.D thesis, University of Pennsylvania. 
Dreyer, M. 2011. A non-parametric model for the discovery of inflectional paradigms from plain text using 
graphical models over strings. Ph.D thesis, The Johns Hopkins University, Baltimore, Maryland 
Freitag, D. 2005. Morphology induction from term clusters. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning (CoNLL-2005), pages 128?135, Ann Arbor, Michigan. 
Association for Computational Linguistics. 
Gaussier Eric. 1999. Unsupervised learning of derivational morphology from inflectional lexicons. In ACL?99 
Workshop Proceedings: Unsupervised Learning in Natural Language Processing : 24?30 ACL 
Gelbukh, A. F., Alexandrov, M., and Han, S.-Y. (2004). Detecting inflection patterns in natural language by 
minimization of morphological model. In Sanfeliu, A., Trinidad, J. F. M., and Carrasco-Ochoa, J. A., editors, 
Proceedings of Progress in Pattern Recognition, Image Analysis and Applications, 9th Iberoamerican Congress 
on Pattern Recognition, CIARP ?04, volume 3287 of Lecture Notes in Computer Science, pages 432?438. 
Springer-Verlag, Berlin. 
Goldsmith J A. 2001. Unsupervised learning of the morphology of a natural language. Computational 
Linguistics 27(2): 153?198 
Hammarstrom Harald  and Lars Borin. 2011. Unsupervised learning of morphology. Computational Linguistics, 
(2):309?350. 
Koskenniemi, K. 1983. Two-level morphology: a general computational model for word-form recognition and 
production. Helsinki, Department of General Linguistics, University of Helsinki. 
Koskenniemi, K. 1996. Finite-state morphology and information retrieval. Proceedings of the ECAI-96 
Workshop on Extended Finite State Models of Language ECAI, Budapest, Hungary : 42-56 
Lind?n, K. 2008. A probabilistic model for guessing base forms of new words by analogy. In Proceedings of 
CICLing-2008: 9th International Conference on Intelligent Text Processing and Computational Linguistics, 
volume 4919 of Lecture Notes in Computer Science, pages 106?116. Springer. 
Lind?n, K. and Tuovila, J. 2009 Corpus-based Paradigm Selection for Morphological Entries. In Proceedings of 
NODALIDA 2009, Odense, Denmark, May 2009 
Loftsson, H. 2008. Tagging Icelandic text: A linguistic rule-based approach. Nordic Journal of Linguistics 31(1). 
47?72.  
Lovins J. B. 1968. Development of a stemming algorithm. Mechanical Translation and Computer Linguistic., 
vol.11, no.1/2:  22-31. 
Maung, Zin Maung & Yoshiki Mikami. 2008. A rule-based syllable segmentation of myanmar text. In 
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, 51?58. Hyderabad, India: 
Asian Federation of Natural Language Processing. 
Paice, C.D. 1990. Another stemmer. SIGIR Forum, 24: 56-61 
Porter, M. F. 1980. An algorithm for suffix stripping. Program 14 : 130-7. 
Sharma U, (2006). Unsupervised Learning of Morphology of a Highly Inflectional Language, Ph.D. thesis,  
Tezpur University, Assam, India 
36
