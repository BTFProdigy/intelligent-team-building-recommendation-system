Proceedings of the SIGDIAL 2013 Conference, pages 366?368,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Map Task Dialogue System:  
A Test-bed for Modelling Human-Like Dialogue 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se  
  
 
  
 
Abstract 
The demonstrator presents a test-bed for 
collecting data on human?computer dia-
logue: a fully automated dialogue system 
that can perform Map Task with a user. 
In a first step, we have used the test-bed 
to collect human?computer Map Task di-
alogue data, and have trained various da-
ta-driven models on it for detecting feed-
back response locations in the user?s 
speech. One of the trained models has 
been tested in user interactions and was 
perceived better in comparison to a sys-
tem using a random model. The demon-
strator will exhibit three versions of the 
Map Task dialogue system?each using a 
different trained data-driven model of 
Response Location Detection.  
1 Introduction 
A common procedure in modelling human-like 
dialogue systems is to collect data on human?
human dialogue and then train models that pre-
dict the behaviour of the interlocutors. However, 
we think that it might be problematic to use a 
corpus of human?human dialogue as a basis for 
implementing dialogue system components. One 
problem is the interactive nature of the task. If 
the system produces a slightly different behav-
iour than what was found in the original data, 
this would likely result in a different behaviour 
in the interlocutor. Another problem is that hu-
mans are likely to behave differently towards a 
system as compared to another human (even if a 
more human-like behaviour is being modelled). 
Yet another problem is that much dialogue be-
haviour is optional and therefore makes the actu-
al behaviour hard to use as a gold standard. 
 
Figure 1: The Map Task system user interface 
To improve current systems, we need both a 
better understanding of the phenomena of human 
interaction, better computational models and bet-
ter data to build these models. An alternative ap-
proach that has proven to be useful is to train 
models on human?computer dialogue data col-
lected through Wizard-of-Oz studies (Dahlb?ck 
et al, 1993). However, the methodology might 
be hard to use when the issue under investigation 
is time-critical behaviour such as back-channels.  
A third alternative is to use a boot-strapping 
procedure, where more and more advanced (or 
human-like) versions of the system are built iter-
atively. After each iteration, users interact with 
the system and data is collected. This data is then 
used to train/improve data-driven models of in-
teraction in the system. A problem here, howev-
er, is how to build the first iteration of the sys-
tem, since many components, e.g., Automatic 
Speech Recognition (ASR), need some data to be 
useful at all.  
In this demonstration we present a test-bed for 
collecting data on time-critical human?computer 
dialogue phenomena: a fully automated dialogue 
system that can perform the Map Task with a 
366
user (Skantze, 2012). In a first step, following 
the boot-strapping procedure, we collected hu-
man?computer Map Task dialogue data using 
this test-bed and then trained various data-driven 
models on this data for detecting feedback re-
sponse locations in user?s speech. A trained 
model has been implemented and evaluated in 
interaction with users?in the same environment 
used for collecting the data (Meena et al, in 
press). The demonstrator will exhibit three ver-
sions of the Map Task dialogue system?each 
using a different trained data-driven model of 
Response Location Detection (RLD). 
2 The Map Task Dialogue System 
Map Task is a common experimental paradigm 
for studying human?human dialogue. In our set-
up, the user (the information giver) is given the 
task of describing a route on a map to the system 
(the information follower). The choice of Map 
Task is motivated partly because the system may 
allow the user to keep the initiative during the 
whole dialogue, and thus only produce responses 
that are not intended to take the initiative, most 
often some kind of feedback. Thus, the system 
might be described as an attentive listener.  
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the second iteration of the sys-
tem that uses a model trained on the collected 
data. To make the human?computer Map Task 
dialogue feasible without any full speech under-
standing we have implemented a trick: the user is 
presented with a map on a screen (see Figure 1) 
and instructed to move the mouse cursor along 
the route as it is being described. The user is told 
that this is for logging purposes, but the real rea-
son for this is that the system tracks the mouse 
position and thus knows what the user is current-
ly talking about. It is thereby possible to produce 
a coherent system behaviour without any speech 
recognition at all, only basic speech detection. 
This often results in a very realistic interaction1.  
The system uses a simple energy-based speech 
detector to chunk the user?s speech into inter-
pausal units (IPUs), that is, periods of speech that 
contain no sequence of silence longer than 200 
ms. Such a short threshold allows the system to 
give backchannels (seemingly) while the user is 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
speaking or take the turn with barely any gap. 
The end of an IPU is a candidate for the RLD 
model to identify whether it is a Response Loca-
tion (RL). 
 
Prosodic 
analysis
Dialogue 
manager
Map
Window
Speech 
detector
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
 
Figure 2: The basic components of the system. 
Each time the RLD detects a RL, the dialogue 
manager produces a response, depending on the 
current state of the dialogue and the position of 
the mouse cursor. Table 1 shows an example 
Map Task dialogue illustrating the different types 
of responses the system could produce. The dia-
logue manager always starts with an Introduction 
and ends with an Ending, once the mouse cursor 
has reached the destination. Between these, it 
selects from the other responses (Reprise Frag-
ment, Clarification and Backchannel), partly 
randomly, but also depending on the length of 
the last user turn (longer turn triggers Repeat or 
Restart) and the current mouse location (e.g., a 
Guess if mouse hovers over the same area for 
some time). 
Table 1: An example Map Task dialogue between a 
user (U) and system (S) [User utterances with ASR 
error(s) are indicated with a *]  
S Introduction Could you help me to find my way to 
the bus stop? 
U  start from the department store 
S Backchannel yeah 
U  and eh 
S Guess Should I start by going west? 
U  yeah do that 
U  then you will get to a meadow and 
when you get to the meadow 
*you get to a meadow until you get 
to the meadow 
S Repeat Eh, could you repeat that? 
U  you go straight and you see a mead-
ow on your right side 
S Clarification  
Request  
A green field? 
U  ehm yeah a field 
S Backchannel mhm 
U  pass the meadow and turn right so 
you are going north  
*the meadow and turn right to your 
going north 
S Backchannel okay 
U  you get to a restaurant 
367
S Reprise 
Fragment 
A restaurant 
U  then you have to turn right and that 
will take you to 
*you have turned right and at will 
take you turn 
S Restart Eh, I think I lost you at the restau-
rant, could you take that again? 
?   
U  at the junction go south and then 
you will get to the bus stop 
S Ending Okay, thanks a lot. 
3 Data-driven models of RLD 
Using the system described in the previous 
section a corpus of 50 human?computer Map 
Task dialogue was collected and used to train a 
data-driven model of RLD. Since we didn?t have 
a sophisticated model of RLD during the first 
iteration a na?ve model was used. This model 
would wait for a random period between 0 and 
800 ms after an IPU ended. If no new IPUs were 
initiated during this period, a RL was detected. 
Each IPU in the corpus was then manually la-
belled as either Hold (a response would be inap-
propriate) or Respond (a response is expected) 
type. On this data various models were trained 
on online extractable features?covering syntax, 
context and prosody. Table 2 illustrates the per-
formance of the various models. Going a step 
further, model #6 was deployed in the Map Task 
dialogue system (with an ASR component) and 
evaluated in user interactions. The result sug-
gests that the trained model provide for smooth 
turn-transitions in contrast to the Random model 
(Meena et al, in press). 
Table 2: Performance of various models of RLD 
[NB: Na?ve Bayes; SVM: Support Vector Machine; 
Models with * will be exhibited in the demonstration] 
# RLD model % accuracy (on ASR results) 
1* Random 50.79% majority class baseline 
2 Prosody 64.5% (SVM learner) 
3 Context 64.8% (SVM learner) 
4* 
Prosody 
+ Context 
69.1% (SVM learner) 
5 Syntax 81.1% (NB learner) 
6* 
Syntax 
+ Prosody  
+ Context 
82.0 % (NB learner) 
4 Future applications 
The Map Task test-bed presented here has the 
potential for modelling other human-like conver-
sational behaviour in dialogue systems: 
Clarification strategies: by deploying explicit 
(did you mean turn right?) and implicit (a reprise 
such as turn right) or elliptical (?right??) clarifi-
cation forms in the grounding process one could 
investigate the efficiency and effectively of these 
human-like clarification strategies.  
User utterance completion: It has been sug-
gested that completion of user utterances by a 
dialogue system would result in human-like con-
versational interactions. However, completing 
user?s utterance at every opportunity may not be 
the best strategy (DeVault et al, 2009). The pre-
sented system could be used to explore when it is 
appropriate to do so. We have observed in our 
data that the system dialogue acts Guess (cf. Ta-
ble 1) and Reprise often helped the dialogue pro-
ceed further ? by completing user utterances ? 
when the user had difficulty describing a land-
mark on a route. 
Visual cues: the system could be integrated in 
a robotic head, such as Furhat (Al Moubayed et 
al., 2013), and visual cues from the user could be 
used for improving the current model of RLD. 
This could be used further to explore the use of 
extra-linguistic system behaviours, such as head 
nods and facial gestures, as feedback responses. 
Acknowledgement 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237) 
References 
Al Moubayed, S., Skantze, G., & Beskow, J. (2013). 
The Furhat Back-Projected Humanoid Head - Lip 
reading, Gaze and Multiparty Interaction. Interna-
tional Journal of Humanoid Robotics, 10(1). 
Dahlb?ck, N., J?nsson, A., & Ahrenberg, L. (1993). 
Wizard of Oz studies ?  why and how. In Proceed-
ings from the 1993 International Workshop on In-
telligent User Interfaces (pp. 193-200).  
DeVault, D., Sagae, K., & Traum, D. (2009). Can I 
Finish? Learning When to Respond to Incremental 
Interpretation Results in Interactive Dialogue. In 
Proceedings of SIGdial (pp. 11-20). London, UK. 
Meena, R., Skantze, G., & Gustafson, J. (in press). A 
Data-driven Model for Timing Feedback in a Map 
Task Dialogue System. To be published in 14th 
Annual Meeting of the Special Interest Group on 
Discourse and Dialogue - SIGdial. Metz, France. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
368
Proceedings of the SIGDIAL 2013 Conference, pages 375?383,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
 
A Data-driven Model for Timing Feedback 
in a Map Task Dialogue System 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se 
 
  
 
Abstract 
We present a data-driven model for de-
tecting suitable response locations in the 
user?s speech. The model has been 
trained on human?machine dialogue data 
and implemented and tested in a spoken 
dialogue system that can perform the 
Map Task with users. To our knowledge, 
this is the first example of a dialogue sys-
tem that uses automatically extracted 
syntactic, prosodic and contextual fea-
tures for online detection of response lo-
cations. A subjective evaluation of the 
dialogue system suggests that interac-
tions with a system using our trained 
model were perceived significantly better 
than those with a system using a model 
that made decisions at random. 
1 Introduction 
Traditionally, dialogue systems have rested on a 
very simple model for turn-taking, where the sys-
tem uses a fixed silence threshold to detect the 
end of the user?s utterance, after which the sys-
tem responds. However, this model does not cap-
ture human-human dialogue very accurately; 
sometimes a speaker just hesitates and no turn-
change is intended, sometimes the turn changes 
after barely any silence (Sacks et al, 1974). 
Therefore, such models can result in systems that 
interrupt the user or are perceived as unrespon-
sive. Related to the problem of turn-taking is that 
of backchannels (Yngve, 1970).  Backchannel 
feedback ? short acknowledgements such as uh-
huh or mm-hm ? are used by human interlocutors 
to signal continued attention to the speaker, 
without claiming the floor. If a dialogue system 
should be able to manage smooth turn-taking and 
back-channelling, it must be able to first identify 
suitable locations in the user?s speech to do so.  
Duncan (1972) found that human interlocutors 
continuously monitor several cues, such as con-
tent, syntax, intonation, paralanguage, and body 
motion, in parallel to manage turn-taking. Simi-
lar observations have been made in various other 
studies investigating the turn-taking and back-
channelling phenomena in human conversations. 
Ward (1996) has suggested that a low pitch re-
gion is a good cue that backchannel feedback is 
appropriate. On the other hand, Koiso et al 
(1998) have argued that both syntactic and pro-
sodic features make significant contributions in 
identifying turn-taking and back-channelling rel-
evant places. Cathcart et al (2003) have shown 
that syntax in combination with pause duration is 
a strong predictor for backchannel continuers.  
Gravano & Hirschberg (2009) observed that the 
likelihood of occurrence of a backchannel in-
creases with the number of syntactic and prosod-
ic cues conjointly displayed by the speaker. 
However, there is a general lack of studies on 
how such models could be used online in dia-
logue systems and to what extent that would im-
prove the interaction. There are two main prob-
lems in doing so. First, the data used in the stud-
ies mentioned above are from human?human 
dialogue and it is not obvious to what extent the 
models derived from such data transfers to hu-
man?machine dialogue. Second, many of the 
features used were manually extracted. This is 
especially true for the transcription of utterances, 
but several studies also rely on manually anno-
tated prosodic features.  
In this paper, we present a data-driven model 
of what we call Response Location Detection 
(RLD), which is fully online. Thus, it only relies 
375
on automatically extractable features?covering 
syntax, prosody and context. The model has been 
trained on human?machine dialogue data and has 
been implemented in a dialogue system that is in 
turn evaluated with users. The setting is that of a 
Map Task, where the user describes the route and 
the system may respond with for example 
acknowledgements and clarification requests.  
2 Background 
Two influential theories that have examined the 
turn-taking mechanism in human conversations 
are the signal-based mechanism of Duncan 
(1972) and the rule-based mechanism proposed 
by Sacks (1974). According to Duncan, ?the 
turn-taking mechanism is mediated through sig-
nals composed of clear-cut behavioural cues, 
considered to be perceived as discrete?. Duncan 
identified six discrete behavioural cues that a 
speaker may use to signal the intent to yield the 
turn. These behavioural cues are: (i) any devia-
tion from the sustained intermediate pitch level; 
(ii) drawl on the final syllable of a terminal 
clause; (iii) termination of any hand gesticulation 
or the relaxation of tensed hand position?during 
a turn; (iv) a stereotyped expression with trailing 
off effect; (v) a drop in pitch and/or loudness; 
and (vi) completion of a grammatical clause. Ac-
cording to the rule-based mechanism of Sacks 
(1974) turn-taking is regulated by applying rules 
(e.g. ?one party at a time?) at Transition-
Relevance Places (TRPs)?possible completion 
points of basic units of turns, in order to mini-
mize gaps and overlaps. The basic units of turns 
(or turn-constructional units) include sentential, 
clausal, phrasal, and lexical constructions. 
Duncan (1972) also suggested that speakers 
may display behavioural cues either singly or 
together, and when displayed together they may 
occur either simultaneously or in tight sequence. 
In his analysis, he found that the likelihood that a 
listener attempts to take the turn is higher when 
the cues are conjointly displayed across the vari-
ous modalities.  
While these theories have offered a function-
based account of turn-taking, another line of re-
search has delved into corpora-based techniques 
to build models for detecting turn-transition and 
feedback relevant places in speaker utterances.  
Ward (1996) suggested that a 110 millisecond 
(ms) region of low pitch is a fairly good predic-
tor for back-channel feedback in casual conver-
sational interactions. He also argued that more 
obvious factors, such as utterance end, rising in-
tonation, and specific lexical items, account for 
less than they seem to. He contended that proso-
dy alone is sometimes enough to tell you what to 
say and when to say. 
In their analysis of turn-taking and backchan-
nels based on prosodic and syntactic features, in 
Japanese Map Task dialogs, Koiso et al (1998) 
observed that some part-of-speech (POS) fea-
tures are strong syntactic cues for turn-change, 
and some others are strongly associated with no 
turn-change. Using manually extracted prosodic 
features for their analysis, they observed that 
falling and rising F0 patterns are related to 
changes of turn, and flat, flat-fall and rise-fall 
patterns are indications of the speaker continuing 
to speak. Extending their analysis to backchan-
nels, they asserted that syntactic features, such as 
filled pauses, alone might be sufficient to dis-
criminate when back-channelling is inappropri-
ate, whereas presence of backchannels is always 
preceded by certain prosodic patterns. 
Cathcart et al (2003) presented a shallow 
model for predicting the location of backchannel 
continuers in the HCRC Map Task Corpus 
(Anderson et al, 1991). They explored features 
such as POS, word count in the preceding speak-
er turn, and silence pause duration in their mod-
els. A model based on silence pause only insert-
ed a backchannel in every speaker pause longer 
than 900 ms and performed better than a word 
model that predicted a backchannel every sev-
enth word. A tri-gram POS model predicted that 
nouns and pronouns before a pause are the two 
most important cues for predicting backchannel 
continuers. The combination of the tri-gram POS 
model and pause duration model offered a five-
fold improvement over the others. 
Gravano & Hirschberg (2009) investigated 
whether backchannel-inviting cues differ from 
turn-yielding cues. They examined a number of 
acoustic features and lexical cues in the speaker 
utterances preceding smooth turn-changes, back-
channels, and holds. They have identified six 
measureable events that are strong predictors of a 
backchannel at the end of an inter-pausal unit: (i) 
a final rising intonation; (ii) a higher intensity 
level; (iii) a higher pitch level; (iv) a final POS 
bi-gram equal to ?DT NN?, ?JJ NN?, or ?NN 
NN?; (v) lower values of noise-to-harmonic rati-
os; and (vi) a longer IPU duration. They also ob-
served that the likelihood of a backchannel in-
creases in quadratic fashion with the number of 
cues conjointly displayed by the speaker. 
When it comes to using these features for 
making turn-taking decisions in dialogue sys-
376
tems, there is however, very little related work. 
One notable exception is Raux & Eskenazi 
(2008) who presented an algorithm for dynami-
cally setting endpointing silence thresholds based 
on features from discourse, semantics, prosody, 
timing, and speaker characteristics. The model 
was also applied and evaluated in the Let?s Go 
dialogue system for bus timetable information. 
However, that model only predicted the end-
pointing threshold based on the previous interac-
tion up to the last system utterance, it did not 
base the decision on the current user utterance to 
which the system response is to be made. 
In this paper, we train a model for online Re-
sponse Location Detection that makes a decision 
whether to respond at every point where a very 
short silence (200 ms) is detected. The model is 
trained on human?machine dialogue data taken 
from a first set of interactions with a system that 
used a very na?ve policy for Response Location 
Detection. The trained model is then applied to 
the same system, which has allowed us to evalu-
ate the model online in interaction with users.  
3 A Map Task dialogue system 
In a previous study, we presented a fully auto-
mated spoken dialogue system that can perform 
the Map Task with a user (Skantze, 2012). Map 
Task is a common experimental paradigm for 
studying human-human dialogue, where one sub-
ject (the information giver) is given the task of 
describing a route on a map to another subject 
(the information follower). In our case, the user 
acts as the giver and the system as the follower. 
The choice of Map Task is motivated partly be-
cause the system may allow the user to keep the 
initiative during the whole dialogue, and thus 
only produce responses that are not intended to 
take the initiative, most often some kind of feed-
back. Thus, the system might be described as an 
attentive listener.  
Implementing a Map Task dialogue system 
with full speech understanding would indeed be 
a challenging task, given the state-of-the-art in 
automatic recognition of conversational speech. 
In order to make the task feasible, we have im-
plemented a trick: the user is presented with a 
map on a screen (see Figure 1) and instructed to 
move the mouse cursor along the route as it is 
being described. The user is told that this is for 
logging purposes, but the real reason for this is 
that the system tracks the mouse position and 
thus knows what the user is currently talking 
about. It is thereby possible to produce a coher-
ent system behaviour without any speech recog-
nition at all, only basic speech detection. This 
often results in a very realistic interaction, as 
compared to what users are typically used to 
when interacting with dialogue systems?in our 
experiments, several users first thought that there 
was a hidden operator behind it1.  
 
 
Figure 1: The user interface, showing the map. 
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the model presented and eval-
uated here. The system uses a simple energy-
based speech detector to chunk the user?s speech 
into inter-pausal units (IPUs), that is, periods of 
speech that contain no sequence of silence longer 
than 200 ms. Such a short threshold allows the 
system to give backchannels (seemingly) while 
the user is speaking or take the turn with barely 
any gap. Similar to Gravano & Hirschberg 
(2009) and Koiso et al (1998), we define the end 
of an IPU as a candidate for the Response Loca-
tion Detection model to identify as a Response 
Location (RL). We use the term turn to refer to a 
sequence of IPUs which do not have any re-
sponses between them. 
 
 
Figure 2: The basic components of the system. 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
Prosodic 
analysis
Dialogue 
manager
Map
Windo
Speech 
det ctor
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
377
Each time the RLD model detected a RL, the 
dialogue manager produced a Response, depend-
ing on the current state of the dialogue and the 
position of the mouse cursor. Table 1 shows the 
different types of responses the system could 
produce. The dialogue manager always started 
with an Introduction and ended with an Ending, 
once the mouse cursor had reached the destina-
tion. Between these, it selected from the other 
responses, partly randomly, but also depending 
on the length of the last user turn and the current 
mouse location. Longer turns often led to Restart 
or Repetition Requests, thus discouraging longer 
sequences of speech that did not invite the sys-
tem to respond. If the system detected that the 
mouse had been at the same place over a longer 
time, it pushed the task forward by making a 
Guess response. We also wanted to explore other 
kinds of feedback than just backchannels, and 
therefore added short Reprise Fragments and 
Clarification Requests (see for example Skantze 
(2007) for a discussion on these).  
Table 1: Different responses from the system 
Introduction ?Could you help me to find my way to 
the train station?? 
Backchannel ?Yeah?, ?Mhm?, ?Okay?, ?Uhu? 
Reprise  
Fragment  
?A station, yeah? 
Clarification  
Request  
?A station?? 
Restart ?Eh, I think I lost you at the hotel, how 
should I continue from there?? 
Repetition  
Request  
?Sorry, could you take that again?? 
Guess ?Should I continue above the church?? 
Ending ?Okay, thanks a lot.? 
 
A na?ve version of the system was used to col-
lect data. Since we initially did not have any so-
phisticated model of RLD, it was simply set to 
wait for a random period between 0 and 800 ms 
after an IPU ended. If no new IPUs were initiated 
during this period, a RL was detected, resulting 
in random response delays between 200 and 
1000 ms. Ten subjects participated in the data 
collection. Each subject did 5 consecutive tasks 
on 5 different maps, resulting in a total of 50 dia-
logues. 
Each IPU in the corpus was manually annotat-
ed into three categories: Hold (a response would 
be inappropriate), Respond (a response is ex-
pected) and Optional (a response would not be 
inappropriate, but it is perfectly fine not to re-
spond). Two human-annotators labelled the cor-
pus separately. For all the three categories the 
kappa score was 0.68, which is substantial 
agreement (Landis & Koch, 1977). Since only 
2.1% of all the IPUs in the corpus were identified 
for category Optional, we excluded them from 
the corpus and focused on the Respond and Hold 
categories only. The data-set contains 2272 IPUs 
in total; the majority of which belong to the class 
Respond (50.79%), which we take as our majori-
ty class baseline. Since the two annotators agreed 
on 87.20% of the cases, this can be regarded as 
an approximate upper limit for the performance 
expected from a model trained on this data. 
In (Skantze, 2012), we used this collected data 
to build an offline model of RLD that was 
trained on prosodic and contextual features. In 
this paper, we extend this work in three ways. 
First, we bring in Automatic Speech Recognition 
(ASR) for adding syntactic features to the model. 
Second, the model is implemented as a module 
in the dialogue system so that it can extract the 
prosodic features online. Third, we evaluate the 
performance of our RLD model against a base-
line system that makes a random choice, in a dia-
logue system interacting with users.  
In contrast to some related work (e.g. Koiso et 
al., 1998), we do not discriminate between loca-
tions for backchannels and turn-changes. Instead, 
we propose a general model for response loca-
tion detection. The reason for this is that the sys-
tem mostly plays the role of an attentive listener 
that produces utterances that are not intended to 
take the initiative or claim the floor, but only to 
provide different types of feedback (cf. Table 1). 
Thus, suitable response locations will be where 
the user invites the system to give feedback, re-
gardless of whether the feedback is simply an 
acknowledgement that encourages the system to 
continue, or a clarification request. Moreover, it 
is not clear whether the acknowledgements the 
system produces in this domain should really be 
classified as backchannels, since they do not only 
signal continued attention, but also that some 
action has been performed (cf. Clark, 1996). In-
deed, none of the annotators felt the need to mark 
relevant response locations within IPUs.  
4 A data-driven model for response lo-
cation detection 
The human?machine Map Task corpus described 
in the previous section was used for training a 
new model of RLD. We describe below how we 
extracted prosodic, syntactic and contextual fea-
tures from the IPUs. We test the contribution of 
these feature categories?individually as well as 
378
in combination, in classifying a given IPU as 
either Respond or Hold type. For this we explore 
the Na?ve Bayes (NB) and Support Vector Ma-
chine (SVM) algorithms in the WEKA toolkit 
(Hall et al, 2009). All results presented here are 
based on 10-fold cross-validation. 
4.1 Prosodic features 
Pitch and intensity (sampled at 10 ms) for each 
IPU were extracted using ESPS in 
Wavesurfer/Snack (Sj?lander & Beskow, 2000). 
The values were transformed to log scale and z-
normalized for each user. The final 200 ms 
voiced region was then identified for each IPU. 
For this region, the mean pitch, slope of the 
pitch (using linear regression)?in combination 
with the correlation coefficient r for the regres-
sion line, were used as features. In addition to 
these, we also used the duration of the voiced 
region as a feature. The last 500 ms of each IPU 
were used to obtain the mean intensity (also z-
normalised). Table 2 illustrates the power of pro-
sodic features, individually as well as collective-
ly (last row), in classifying an IPU as either Re-
spond or Hold type. Except for mean intensity all 
other features individually provide an improve-
ment over the baseline. The best accuracy, 
64.5%, was obtained by the SVM algorithm us-
ing all the prosodic features. This should be 
compared against the baseline of 50.79%. 
Table 2: Percentage accuracy of prosodic features 
in detecting response locations 
 Algorithm 
Feature(s) NB  SVM  
Mean pitch 60.3 62.7 
Pitch slope 59.0 57.8 
Duration 58.1 55.6 
Mean intensity 50.3 52.2 
Prosody (all combined) 63.3 64.5 
4.2 Syntactic features 
As lexico-syntactic features, we use the word 
form and part-of-speech tag of the last two 
words in an IPU. All the IPUs in the Map Task 
corpus were manually transcribed. To obtain the 
part-of-speech tag we used the LBJ toolkit 
(Rizzolo & Roth, 2010). Column three in Table 3 
illustrates the discriminatory power of syntactic 
features?extracted from the manual transcrip-
tion of the IPUs. Using the last two words and 
their POS tags, the Na?ve Bayes learner achieves 
the best accuracy of 83.6% (cf. row 7). While 
POS tag is a generic feature that would enable 
the model to generalize, using word form as a 
feature has the advantage that some words, such 
as yeah, are strong cues for predicting the Re-
spond class, whereas pause fillers, such as ehm, 
are strong predictors of the Hold class. 
Table 3: Percentage accuracy of syntactic features 
in detecting response locations 
  
Manual  
transcriptions 
ASR  
results 
# Feature(s) NB SVM NB SVM  
1 Last word (Lw) 82.5 83.9 80.8 80.9 
2 
Last word part-of-
speech (Lw-POS)  
79.4 79.5 74.5 74.6 
3 
Second last word 
(2ndLw) 
68.1 67.7 67.1 67.0 
4 
Second last word 
Part-of-speech 
(2ndLw-POS) 
66.9 66.5 65.8 66.1 
5 Lw + 2ndLw 82.3 81.5 80.8 80.6 
6 
Lw-POS 
+ 2ndLw-POS 
80.3 80.5 75.4 74.87 
7 
Lw + 2ndLw 
+ Lw-POS 
+ 2ndLw-POS 
83.6 81.7 79.7 79.7 
8 
Last word diction-
ary (Lw-Dict) 
83.4 83.4 78.0 78.0 
9 
Lw-Dict 
+ 2ndLw-Dict 
81.2 82.6 76.1 77.7 
10 
Lw + 2ndLw 
+ Lw-Conf 
+ 2ndLw-Conf  
82.3 81.5 81.1 80.5 
 
An RLD model for online predictions requires 
that the syntactic features are extracted from the 
output of a speech recogniser. Since speech 
recognition is prone to errors, an RLD model 
trained on manual transcriptions alone would not 
be robust when making predictions in noisy data. 
Therefore we train our RLD model on actual 
speech recognised results. To achieve this, we 
did an 80-20 split of the Map Task corpus into 
training and test sets respectively. The transcrip-
tions of IPUs in the training set were used to 
train the language model of the Nuance 9 ASR 
system. The audio recordings of the IPUs in the 
test set were then recognised by the trained ASR 
system. After performing five iterations of split-
ting, training and testing, we had obtained the 
speech recognised results for all the IPUs in the 
Map Task corpus. The mean word error rate for 
the five iterations was 17.22% (SD = 3.8%).  
Column four in Table 3 illustrates the corre-
sponding performances of the RLD model 
trained on syntactic features extracted from the 
best speech recognized hypotheses for the IPUs. 
With the introduction of a word error rate of 
17.22%, the performances of all the models us-
379
ing only POS tag feature decline. The perfor-
mances are bound to decline further with in-
crease in ASR errors. This is because the POS 
tagger itself uses the left context to make POS 
tag predictions. With the introduction of errors in 
the left context, the tagger?s accuracy is affected, 
which in turn affects the accuracy of the RLD 
models. However, this decline is not significant 
for models that use word form as a feature. This 
suggests that using context independent lexico-
syntactic features would still offer better perfor-
mance for an online model of RLD. We therefore 
also created a word class dictionary, which gen-
eralises the words into domain-specific classes in 
a simple way (much like a class-based n-gram 
model). Row 9 in Table 3 illustrates that using a 
dictionary instead of POS tag (cf. row 6) im-
proves the performance of the online model. We 
have also explored the use of word-level confi-
dence scores (Conf) from the ASR as another 
feature that could be used to reinforce a learning 
algorithm?s confidence in trusting the recognised 
words (cf. row 10 in Table 3).  
The best accuracy, 81.1%, for the online mod-
el of RLD is achieved by the Na?ve Bayes algo-
rithm using the features word form and confi-
dence score, for last two words in an IPU. 
4.3 Contextual features 
We have explored three discourse context fea-
tures: turn and IPU length (in words and se-
conds) and last system dialogue act. Dialogue 
act history information have been shown to be 
vital for predicting a listener response when the 
speaker has just responded to the listener?s clari-
fication request (Koiso et al (1998); Cathcart et 
al. 2003; Gravano & Hirschberg (2009); Skantze, 
2012). To verify if this rule holds in our corpus, 
we extracted turn length and dialogue act labels 
for the IPUs, and trained a J48 decision tree 
learner. The decision tree achieved an accuracy 
of 65.7%. One of the rules learned by the deci-
sion tree is: if the last system dialogue act is 
Clarification or Guess (cf. Table 1), and the turn 
word count is less than equal to 1, then Respond. 
In other words, if the system had previously 
sought a clarification, and the user has responded 
with a yes/no utterance, then a system response 
is expected. A more general rule in the decision 
tree suggests that: if the last system dialogue act 
was a Restart or Repetition Request, and if the 
turn word count is more than 4 then Respond 
otherwise Hold. In other words, the system 
should wait until it gets some amount of infor-
mation from the user.  
Table 4 illustrates the power of these contex-
tual features in discriminating IPUs, using the 
NB and the SVM algorithms. All the features 
individually provide improvement over the base-
line of 50.79%. The best accuracy, 64.8%, is 
achieved by the SVM learner using the features 
last system dialogue act and turn word count. 
Table 4: Percentage accuracy of contextual features 
in detecting response locations 
 
Manual 
transcriptions 
ASR  
results 
Features NB  SVM  NB  SVM  
Last system dialogue act 54.1 54.1 54.1 54.1 
Turn word count 61.8 61.9 61.5 62.9 
Turn length in seconds 58.4 58.8 58.4 58.8 
IPU word count 58.4 58.2 58.1 59.3 
IPU length in seconds 57.3 61.2 57.3 61.2 
Last system dialogue act 
+ Turn word count 
59.9 64.5 60.4 64.8 
 
4.4 Combined model 
Table 5 illustrates the performances of the RLD 
model using various feature category combina-
tions. It could be argued that the discriminatory 
power of prosodic and contextual feature catego-
ries is comparable. A model combining prosodic 
and contextual features offers an improvement 
over their individual performances. Using the 
three feature categories in combination, the Na-
?ve Bayes learner provided the best accuracy: 
84.6% (on transcriptions) and 82.0% (on ASR 
output). These figures are significantly better 
than the majority class baseline of 50.79% and 
approach the expected upper limit of 87.20% on 
the performance.  
Table 5: Percentage accuracy of combined models  
 
Manual  
transcriptions 
ASR  
results 
Feature categories NB SVM NB SVM 
Prosody  63.3 64.5 63.3 64.5 
Context  59.9 64.5 60.4 64.8 
Syntax  82.3 81.5 81.1 80.5 
Prosody + Context 67.7 70.2 67.5 69.1 
Prosody + Context 
+ Syntax 
84.6 77.2 82.0 77.1 
  
Table 6 illustrates that the Na?ve Bayes model 
for Response Location Detection trained on 
combined syntactic, prosodic and contextual fea-
tures, offers better precision (fraction of correct 
decisions in all model decisions) and recall (frac-
tion of all relevant decisions correctly made) in 
comparison to the SVM model. 
380
Table 6: Precision and Recall scores of the NB and 
the SVM learners trained on combined prosodic, con-
textual and syntactic features. 
Prediction class 
Precision (in %) Recall (in %) 
NB  SVM  NB  SVM  
Respond 81.0  73.0 87.0 84.0 
Hold 85.0 81.0 78.0 68.0 
 
5 User evaluation 
In order to evaluate the usefulness of the com-
bined model, we have performed a user evalua-
tion where we test the trained model in the Map 
Task dialogue system that was used to collect the 
corpus (cf. section 3). A version of the dialogue 
system was created that uses a Random model, 
which makes a random choice between Respond 
and Hold. The Random model thus approximates 
our majority class baseline (50.79% for Re-
spond). Another version of the system used the 
Trained model ? our data-driven model ? to 
make the decision. For both models, if the deci-
sion was a Hold, the system waited 1.5 seconds 
and then responded anyway if no more speech 
was detected from the user. 
We hypothesize that since the Random model 
makes random choices, it is likely to produce 
false-positive responses (resulting in overlap in 
interaction) as well as false-negative responses 
(resulting in gap/delayed response) in equal pro-
portion. The Trained model on the other hand 
would produce fewer overlaps and gaps.  
In order to evaluate the models, 8 subjects (2 
female, 6 male) were asked to perform the Map 
Task with the two systems. Each subject per-
formed five dialogues (which included 1 trial and 
2 tests) with each version of the system. This 
resulted in 16 test dialogues each for the two sys-
tems. The trial session was used to allow the us-
ers to familiarize themselves with the dialogue 
system. Also, the audio recording of the users? 
speech from this session was used to normalize 
the user pitch and intensity for the online prosod-
ic extraction. The order in which the systems and 
maps were presented to the subjects was varied 
over the subjects to avoid any ordering effect in 
the analysis.  
The 32 dialogues from the user evaluation 
were, on average, 1.7 min long (SD = 0.5 min). 
The duration of the interactions with the Random 
and the Trained model were not significantly 
different. A total of 557 IPUs were classified by 
the Random model whereas the Trained model 
classified 544 IPUs. While the Trained model 
classified 57.7% of the IPUs as Respond type the 
Random model classified only 48.29% of the 
total IPUs as Respond type, suggesting that the 
Random model was somewhat quieter.  
It turned out that it was very hard for the sub-
jects to perform the Map Task and at the same 
time make a valid subjective comparison be-
tween the two versions of the system, as we had 
initially intended. Therefore, we instead con-
ducted another subjective evaluation to compare 
the two systems. We asked subjects to listen to 
the interactions and press a key whenever a sys-
tem response was either lacking or inappropriate. 
The subjects were asked not to consider how the 
system actually responded, only evaluate the tim-
ing of the response. 
Eight users participated in this subjective 
judgment task. Although five of these were from 
the same set of users who had performed the 
Map Task, none of them got to judge their own 
interactions. The judges listened to the Map Task 
interactions in the same order as the users had 
interacted, including the trial session. Whereas it 
had been hard for the subjects who participated 
in the dialogues to characterize the two versions 
of the system, almost all of the judges could 
clearly tell the two versions apart. They stated 
that the Trained system provided for a smooth 
flow of dialogue. The timing of the IPUs was 
aligned with the timing of the judges? key-
presses in order to measure the numbers of IPUs 
that had been given inappropriate response deci-
sions. The results show that for the Random 
model, 26.75% of the RLD decisions were per-
ceived as inappropriate, whereas only 11.39% of 
the RLD decisions for the Trained model were 
perceived inappropriate. A two-tailed two-
sample t-test for difference in mean of the frac-
tion of inappropriate instances (key-press count 
divided by IPU count) for Random and Trained 
model show a clear significant difference (t = 
4.66, dF = 30, p < 0.001). 
We have not yet analysed whether judges pe-
nalized false-positives or false-negatives to a 
larger extent, this is left to future work. Howev-
er, some judges informed us that they did not 
penalize delayed response (false-negative), as the 
system eventually responded after a delay. In the 
context of a system trying to follow a route de-
scription, such delays could sometimes be ex-
pected and wouldn?t be unnatural. For other 
types of interactions (such as story-telling), such 
delays may on the other hand be perceived as 
unresponsive. Thus, the balance between false-
positives and false-negatives might need to be 
tuned depending on the topic of the conversation.  
381
6 Conclusion  
We have presented a data-driven model for de-
tecting response locations in the user?s speech. 
The model has been trained on human?machine 
dialogue data and has been integrated and tested 
in a spoken dialogue system that can perform the 
Map Task with users. To our knowledge, this is 
the first example of a dialogue system that uses 
automatically extracted syntactic, prosodic and 
contextual features for making online detection 
of response locations. The models presented in 
earlier works have used only prosody (Ward, 
1996), or combinations of syntax and prosody 
(Koiso et al, 1998), syntax and context (Cathcart 
et al, 2003), prosody and context (Skantze, 
2012), or prosody, context and semantics (Raux 
& Eskenazi (2008). Furthermore, we have evalu-
ated the usefulness of our model by performing a 
user evaluation of a dialogue system interacting 
with users. None of the earlier models have been 
tested in user evaluations. 
The significant improvement of the model 
gained by adding lexico-syntactic features such 
as word form and part-of-speech tag corroborates 
with earlier observations about the contribution 
of syntax in predicting response location (Koiso 
et al, 1998; Cathcart et al, 2003; Gravano & 
Hirschberg, 2009). While POS tag alone is a 
strong generic feature for making predictions in 
offline models its contribution to decision mak-
ing in online models is reduced due to speech 
recognition errors. This is because the POS tag-
ger itself uses the left context to make predic-
tions, and is not typically trained to handle noisy 
input. We have shown that using only the word 
form or a dictionary offers a better performance 
despite speech recognition errors. However, this 
of course results in a more domain-dependent 
model. 
Koiso et al, (1998), have shown that prosodic 
features contribute almost as strongly to response 
location prediction as the syntactic features. We 
do not find such results with our model. This 
difference could be partly attributed to inter-
speaker variation in the human?machine Map 
Task corpus used for training the models. All the 
users who participated in the corpus collection 
were non-native speakers of English. Also, our 
algorithm for extracting prosodic features is not 
as powerful as the manual extraction scheme 
used in (Koiso et al, 1998). Although prosodic 
and contextual features do not seem to improve 
the performance very much when syntactic fea-
tures are available, they are clearly useful when 
no ASR is available (70.2% as compared to the 
baseline of 50.79%).  
The subjective evaluation indicates that the in-
teractions with a system using our trained model 
were perceived as smoother (more accurate re-
sponses) as compared to a system using a model 
that makes a random choice between Respond 
and Hold. 
7 Future work 
Coordination problems in turn-transition and re-
sponsiveness have been identified as important 
short-comings of turn-taking models in current 
dialogue systems (Ward et al, 2005). In continu-
ation of the current evaluation exercise, we 
would next evaluate our Trained model?on an 
objective scale, in terms of its responsiveness 
and smoothness in turn-taking and back-
channels. An objective measure is the proportion 
of judge key-presses coinciding with false-
positive and false-negative model decisions. We 
argue that in comparison to the Random model 
our Trained model produces (i) fewer instances 
of false-negatives (gap/delayed response) and 
therefore has a faster response time, and (ii) few-
er instances of false-positives (overlap) and thus 
provides for smooth turn-transitions.  
We have so far explored syntactic, prosodic 
and contextual features for predicting response 
location. An immediate extension to our model 
would be to bring semantic features in the model. 
In Meena et al (2012) we have presented a data-
driven method for semantic interpretation of ver-
bal route descriptions into conceptual route 
graphs?a semantic representation that captures 
the semantics of the way human structure infor-
mation in route descriptions. Another possible 
extension is to situate the interaction in a face-to-
face Map Task between a human and a robot and 
add features from other modalities such as gaze. 
In a future version of the system, we do not 
only want to determine when to give responses 
but also what to respond. In order to do this, the 
system will need to extract the semantic concepts 
of the route directions (as described above) and 
utilize the confidence scores from the spoken 
language understanding component in order to 
select between different forms of clarification 
requests and acknowledgements.  
Acknowledgments 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
382
References 
Anderson, A., Bader, M., Bard, E., Boyle, E., 
Doherty, G., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, 
H., & Weinert, R. (1991). The HCRC Map Task 
corpus. Language and Speech, 34(4), 351-366. 
Cathcart, N., Carletta, J., & Klein, E. (2003). A shal-
low model of backchannel continuers in spoken di-
alogue. In 10th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics. Budapest. 
Clark, H. H. (1996). Using language. Cambridge, 
UK: Cambridge University Press. 
Duncan, S. (1972). Some Signals and Rules for Tak-
ing Speaking Turns in Conversations. Journal of 
Personality and Social Psychology, 23(2), 283-
292. 
Gravano, A., & Hirschberg, J. (2009). Backchannel-
inviting cues in task-oriented dialogue. In Proceed-
ings of Interspeech 2009 (pp. 1019-1022). Bright-
on, U.K. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P., & Witten, I. H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD Ex-
plorations, 11(1). 
Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., & 
Den, Y. (1998). An analysis of turn-taking and 
backchannels based on prosodic and syntactic fea-
tures in Japanese Map Task dialogs. Language and 
Speech, 41, 295-321. 
Landis, J., & Koch, G. (1977). The measurement of 
observer agreement for categorical data. Biomet-
rics, 33(1), 159-174. 
Meena, R., Skantze, G., & Gustafson, J. (2012). A 
Data-driven Approach to Understanding Spoken 
Route Directions in Human-Robot Dialogue. In 
Proceedings of Interspeech. Portland, OR, US. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Rizzolo, N., & Roth, D. (2010). Learning Based Java 
for Rapid Development of NLP Systems. Lan-
guage Resources and Evaluation. 
Sacks, H., Schegloff, E., & Jefferson, G. (1974). A 
simplest systematics for the organization of turn-
taking for conversation. Language, 50, 696-735. 
Sj?lander, K., & Beskow, J. (2000). WaveSurfer - an 
open source speech tool. In Yuan, B., Huang, T., & 
Tang, X. (Eds.), Proceedings of ICSLP 2000, 6th 
Intl Conf on Spoken Language Processing (pp. 
464-467). Beijing. 
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems - Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
Ward, N., Rivera, A., Ward, K., & Novick, D. (2005). 
Root causes of lost time and user stress in a simple 
dialog system. In Proceedings of Interspeech 2005. 
Lisbon, Portugal. 
Ward, N. (1996). Using prosodic clues to decide when 
to produce backchannel utterances. In Proceedings 
of the fourth International Conference on Spoken 
Language Processing (pp. 1728-1731). Philadelph-
ia, USA. 
Yngve, V. H. (1970). On getting a word in edgewise. 
In Papers from the sixth regional meeting of the 
Chicago Linguistic Society (pp. 567-578). Chicago. 
 
383
Proceedings of the SIGDIAL 2014 Conference, pages 2?11,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Crowdsourcing Street-level Geographic Information Using a 
Spoken Dialogue System 
 
 
 Raveesh Meena Johan Boye Gabriel Skantze Joakim Gustafson 
KTH Royal Institute of Technology 
School of Computer Science and Communication  
Stockholm, Sweden 
{raveesh, jboye}@csc.kth.se, {gabriel, jocke}@speech.kth.se 
 
  
 
Abstract 
We present a technique for crowd-
sourcing street-level geographic infor-
mation using spoken natural language. In 
particular, we are interested in obtaining 
first-person-view information about what 
can be seen from different positions in 
the city. This information can then for 
example be used for pedestrian routing 
services. The approach has been tested in 
the lab using a fully implemented spoken 
dialogue system, and has shown promis-
ing results. 
1 Introduction 
Crowdsourcing is increasingly being used in 
speech processing for tasks such as speech data 
acquisition, transcription/labeling, and assess-
ment of speech technology, e.g. spoken dialogue 
systems (Parent & Eskenazi, 2011). However, 
we are not aware of any attempts where a dia-
logue system is the vehicle for crowdsourcing 
rather than the object of study, that is, where a 
spoken dialogue system is used to collect infor-
mation from a large body of users.  A task where 
such crowdsourcing dialogue systems would be 
useful is to populate geographic databases. While 
there are now open databases with geographic 
information, such as OpenStreetMap (Haklay & 
Weber, 2008), these are typically intended for 
map drawing, and therefore lack detailed street-
level information about city landmarks, such as 
colors and height of buildings, ornamentations, 
facade materials, balconies, conspicuous signs, 
etc. Such information could for example be very 
useful for pedestrian navigation (Tom & Denis, 
2003; Ross et al., 2004). With the current grow-
ing usage of smartphones, we might envisage a 
community of users using their phones to con-
tribute information to geographic databases, an-
notating cities to a great level of detail, using 
multi-modal method including speech. The key 
reason for using speech for map annotation is 
convenience; it is easy to talk into a mobile 
phone while walking down the street, so a user 
with a little experience will not be slowed down 
by the activity of interacting with a database. 
This way, useful information could be obtained 
that is really hard to add offline, sitting in front 
of one?s PC using a map interface, things like: 
Can you see X from this point? Is there a big 
sign over the entrance of the restaurant? What 
color is the building on your right? 
Another advantage of using a spoken dialogue 
system is that the users could be asked to freely 
describe objects they consider important in their 
current view. In this way, the system could learn 
new objects not anticipated by the system de-
signers, and their associated properties.   
In this paper we present a proof-of-concept 
study of how a spoken dialogue system could be 
used to enrich geographic databases by 
crowdsourcing. To our knowledge, this is the 
first attempt at using spoken dialogue systems 
for crowdsourcing in this way. In Section 2, we 
elaborate on the need of spoken dialogue systems 
for crowdsourcing geographic information. In 
Section 3 we describe the dialogue system im-
plementation. Section 4 presents our in-lab 
crowdsourcing experiment. We present an analy-
sis of crowd-sourced data in Section 5, and dis-
cuss directions for future work in Section 6. 
2 The pedestrian routing domain 
Routing systems have been around quite some 
time for car navigation, but systems for pedestri-
2
an routing are relatively new and are still in their 
nascent stage (Bartie & Mackaness, 2006; Krug 
et al., 2003; Janarthanam et al., 2012; Boye et al., 
2014). In the case of pedestrian navigation, it is 
preferable for way-finding systems to base their 
instructions on landmarks, by which we under-
stand distinctive objects in the city environment. 
Studies have shown that the inclusion of land-
marks into system-generated instructions for a 
pedestrian raises the user?s confidence in the sys-
tem, compared to only left-right instructions 
(Tom & Denis, 2003; Ross et al., 2004).  
Basing routing instructions on landmarks 
means that the routing system would, for exam-
ple, generate an instruction ?Go towards the red 
brick building? (where, in this case, ?the red 
brick building? is the landmark), rather than 
?Turn slightly left here? or ?Go north 200 me-
ters?. This strategy for providing instructions 
places certain requirements on the geographic 
database: It has to include many landmarks and 
many details about them as well, so that the sys-
tem can generate clear and un-ambiguous in-
structions. However, the information contained 
in current databases is still both sparse and 
coarse-grained in many cases.  
Our starting point is a pedestrian routing sys-
tem we designed and implemented, using the 
landmark-based approach to instruction-giving 
(Boye et al., 2014). The system performs visibil-
ity calculations whenever the pedestrian ap-
proaches a waypoint, in order to compute the set 
of landmarks that are visible for the user from his 
current position. OpenStreetMap (Haklay & We-
ber, 2008) is used as the data source. Figure 1 
shows a typical situation in pedestrian routing 
session. The blue dot indicates the user?s position 
and the blue arrow her direction. Figure 2 shows 
the same situation in a first-person perspective. 
The system can now compute the set of visible 
landmarks, such as buildings and traffic lights, 
along with distances and angles to those land-
marks. The angle to a building is given as an in-
terval in degrees relative to the direction of the 
user (e.g. 90? left to 30? left). This is exemplified 
in Figure 1, where four different buildings are in 
view (with field of view marked with numbers 
1?4). Landmarks that are not buildings are con-
sidered to be a single point, and hence the rela-
tive angle can be given as a single number. 
When comparing the map with the street view 
picture, it becomes obvious that the ?SEB? bank 
office is very hard to see and probably not very 
suitable to use as a landmark in route descrip-
tions. On the other hand, the database does not 
contain the fact that the building has six stories 
and a fa?ade made of yellow bricks, something 
that would be easily recognizable for the pedes-
trian. This is not due to any shortcoming of the 
OpenStreetMap database; it just goes to show 
that the database has been constructed with map 
drawing in mind, rather than pedestrian routing. 
There are also some other notable omissions in 
the database; e.g. the shop on the corner, visible 
right in front of the user, is not present in the da-
tabase. Since OpenStreetMap is crowd-sourced, 
there is no guarantee as to which information 
will be present in the database, and which will 
not. This also highlights the limitation of existing 
approaches to crowd-sourcing geographic infor-
mation: Some useful information is difficult to 
add off-line, using a map interface on a PC. On 
the other hand, it would be a straightforward 
matter given the kind of crowd-sourcing spoken 
dialogue system we present next. 
 
 
 
Figure 1: A pedestrian routing scenario 
  
 
 
Figure 2: The visual scene corresponding to the 
pedestrian routing scenario in Figure 1 
3 A dialogue system for crowd-sourcing 
To verify the potential of the ideas discussed 
above, we implemented a spoken dialogue sys-
tem that can engage in spoken conversation with 
3
users and learn details about landmarks in visual 
scenes (such as Figure 2). To identify the kind of 
details in a visual scene that the system could 
potentially ask the users, we first conducted a 
preliminary informal crowd-sourcing dialogue: 
one person (the receiver), was instructed to seek 
information that could be useful for pedestrian 
navigation from the other person (the giver).  
The receiver only had access to information 
available in the maps from OpenStreetMap, as in 
Figure 1, but without any marking of field of 
views, whereas the giver only had access to the 
corresponding visual scene (as in Figure 2). In-
teraction data from eight such dialogues (from 
four participants, and four different visual 
scenes) suggested that in a city environment, 
buildings are prominent landmarks and much of 
the interaction involves their properties such as 
color, number of stories, color of roof, signs or 
ornamentations on buildings, whether it has 
shops, etc. Seeking further details on mentioned 
signs, shops, and entities (whether mapped or 
unmapped) proved to be a useful strategy to ob-
tain information. We also noted that asking for 
open-ended questions, such as ?Is there anything 
else in this scene that I should be aware of?? 
towards the end has the potential of revealing 
unknown landmarks and details in the map.  
Obtaining specific details about known objects 
from the user corresponds to slot-filling in a dia-
logue system, where the dialogue system seeks a 
value for a certain slot (= attribute). By engaging 
in an open-ended interaction the system could 
also obtain general details to identify new slot-
value pairs. Although slots could be in some cas-
es be multi-valued (e.g., a building could have 
both color red and yellow), we have here made 
the simplifying assumption that they are single 
valued. Since users may not always be able to 
specify values for slots we treat no-value as a 
valid slot-value for all type of slots.  
We also wanted the system to automatically 
learn the most reliable values for the slots, over 
several interactions. As the system interacts with 
new users, it is likely that the system will obtain 
a range of values for certain slots. The variability 
of the answers could appear for various reasons: 
users may have differences in perception about 
slot-values such as colors, some users might 
misunderstand what building is being talked 
about, and errors in speech recognition might 
result in the wrong slot values. Some of these 
values may therefore be in agreement with those 
given by other users, while some may differ 
slightly or be in complete contradiction. Thus the 
system should be able to keep a record of all the 
various slot-values obtained (including the dis-
puted ones), identify slot-values that need to be 
clarified, and engage in a dialogue with users for 
clarification. 
In view of these requirements, we have de-
signed our crowd-sourcing dialogue system to be 
able to (1) take and retain initiative during the 
interactions for slot-filling, (2) behave as a re-
sponsive listener when engaging in open-ended 
dialogue, and (3) ask wh? and yes?no questions 
for seeking and clarifying slot-values, respective-
ly. Thus when performing the slot-filling task, 
the system mainly asks questions, acknowledges, 
or clarifies the concepts learned for the slot-
values. Apart from requesting repetitions, the 
user cannot ask any questions or by other means 
take the initiative. A summary of all the attrib-
utes and corresponding system prompts is pre-
sented in Appendix A. 
The top half of Figure 3 illustrates the key 
components of the dialogue system. The Dia-
logue Manager queries the Scene Manager (SM) 
for slots to be filled or slot-values to be clarified, 
engages in dialogue with users to learn/clarify 
slot-values, and informs the SM about the values 
obtained for these slots. The SM manages a list 
of scenes and the predefined slots ? for each type 
of landmark in visual scenes ? that need to be 
filled, maintains a record of slot-values obtained 
from all the users, and identifies slot-values with 
majority vote as the current reliable slot-value. 
To achieve these objectives, the scene manager 
uses an XML representation of visual scenes. In 
this representation, landmarks (e.g., buildings, 
junctions, etc.) ? automatically acquired through 
the OpenStreetMap database and the visibility 
computations mentioned in Section 2  ? are 
stored as scene-objects (cf. Figure 4). 
 
 
 
Figure 3: Dialogue system architecture 
 
The Dialogue Manager (DM) uses scene-
object attributes, such as type, angle or interval 
of a building, to generate referential expressions, 
such as ?Do you see a building on the far left?? 
4
or ?Do you see a shop on the left?? to draw the 
users? attention to the intended landmark in the 
scene. During the course of interaction, the Sce-
ne Manager (SM) extends scene-objects with a 
set of predefined attributes (= slots) that we iden-
tified in the preliminary study, along with their 
various slot-values (cf. Figure 5). For each slot, 
the SM keeps a record of slot-values obtained 
through wh? questions as well as the ones dis-
puted by the users in yes?no questions (cf. ob-
tained and disputed tags in the XML), and 
uses their tally to identify the slot-value in major-
ity. The system assumes this slot-value (or one of 
them in case of a tie) as its best estimate of a 
slot-value pair, which it could clarify with anoth-
er user using a yes?no query. During the slot-
filling mode the DM switches to open-ended in-
teraction mode to seek general details (using 
prompts such as ?Could you describe it/them??), 
if the user suggests/agrees that there are signs 
on/at a scene-object, or a building has shops or 
restaurants. Once all the slots for all the scene-
objects in a visual scene have been queried, the 
DM once again switches to the open-ended inter-
action mode and queries the users whether there 
are any other relevant signs or landmarks that the 
system may have missed and should be aware of. 
On completion of the open-ended queries the SM 
selects the next visual scene, and the DM engag-
es in a new dialogue.  
 
<scene xmlns="cityCS.scene" name=" view7.jpg" lat="59.34501" 
lon="18.0614" fovl="-60" fovr="60" bearing="320" dist="100"> 
    <scene-object> 
        <id>35274588</id> <type>building</type> 
        <from>-60</from> <end>-39</end> 
    </scene-object> 
    <scene-object> 
        <id>538907080</id> <type>shop</type> 
        <distance>34.82</distance> 
        <angle>-39</angle> <bearing>281</bearing> 
    </scene-object> 
    <scene-object> 
        <id>280604</id> <type>building</type> 
        <from>-38</from> <end>6</end> 
    </scene-object> 
    <scene-object> 
        <id>193906</id> <type>traffic_signals</type> 
        <distance>40.77</distance> 
        <angle>-14</angle> <bearing>306</bearing> 
    </scene-object> 
    ... 
</scene> 
Figure 4: XML representation of visual scenes 
 
For speech recognition and semantic interpre-
tation the system uses a context-free grammar 
with semantic tags (SRGS1), tailored for the do-
main. The output of semantic interpretation is a 
concept. If the concept type matches the type of 
the slot, the dialogue manager informs the scene 
manager about the obtained slot-value. If the 
                                                 
1 http://www.w3.org/TR/speech-grammar/ 
concept type is inappropriate the DM queries the 
user once more (albeit using different utterance 
forms). If still no appropriate concept is learned 
the DM requests the SM for the next slot and 
proceeds with the dialogue. For speech synthesis, 
we use the CereVoice system developed by 
CereProc2. The dialogue system has been imple-
mented using the IrisTK framework (Skantze & 
Al Moubayed, 2012). 
 
<scene-object> 
    <id>35274588</id> <type>building</type> 
    <from>-60</from> <end>-39</end> 
    <slot slotName="VISIBLE">?    </slot> 
    <slot slotName="COLOR"> 
     <obtained> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u01" asrCnf="0.06" qType="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="no-value"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.46" qType ="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="Gray"> 
         <userlist> 
           <usrDtls uid="u03" asrCnf="0.19" qType ="WH"/> 
         </userlist> 
       </value> 
     </obtained> 
     <disputed> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.92" qType ="YN"/> 
         </userlist> 
       </value> 
     </disputed> 
    </slot> 
    <slot slotName="STORIES">?    </slot> 
    <slot slotName="ROOF_COLOR">?    </slot> 
    ? 
</scene-object> 
 
Figure 5: Every slot-value is recorded  
 
In contrast to the slot-filling mode, when en-
gaging in an open-ended interaction, the system 
leaves the initiative to the user and behaves as a 
responsive listener. That is, the system only pro-
duces feedback responses, such as backchannels 
(e.g., okay, mh-hmm, uh-huh), repetition requests 
for longer speaker turns (e.g., could you repeat 
that?), or continuation prompts such as ?any-
thing else?? until the user is finished speaking. 
Unless the system recognized an explicit closing 
statement from the user (e.g., ?I can?t?), the sys-
tem encourages the user to continue the descrip-
tions for 2 to 4 turns (chosen randomly). 
To detect appropriate locations in users? 
speech where the system should give feedback 
response, the system uses a trained data-driven 
model (Meena et al., 2013). When the voice ac-
tivity detector detects a silence of 200 ms in us-
ers? speech, the model uses prosodic, contextual 
and lexico-syntactic features from the preceding 
speech segment to decide whether the system 
                                                 
2 https://www.cereproc.com/ 
5
should produce a feedback response. The lower 
half of Figure 3 shows the additional components 
of the dialogue system used in open-ended inter-
action mode. In this mode, the ASR system uses 
a language model that is trained on interactions 
from a related domain (verbal route descrip-
tions), in parallel to the SRGS grammar.  
4 In-lab crowd-sourcing experiment  
Nine visual scenes (wide-angle pictures in first-
person perspective and taken in Stockholm city, 
cf. Figure 2) were used for the task of 
crowdsourcing. Fifteen human participants (4 
females and 11 males) participated in the 
crowdsourcing exercise. All participants either 
studied or worked at the School of Computer 
Science and Communication, KTH, Stockholm. 
Participants were placed in front of a computer 
display and were told that the system will engage 
them in a spoken conversation to seek or clarify 
details about landmarks and other objects in vis-
ual scenes. They were told that the details would 
be used for pedestrian routing and therefore they 
are free to choose and specify details (in open-
ended questions) that they thought would be use-
ful when giving route instructions to another per-
son. 
Each participant did the nine visual scenes in 
the same order, with a 1 minute pause between 
each of them. The first visual scene was used as 
a trial in order to familiarize participants with the 
interaction scenario. For this reason, the trial in-
teraction was specifically designed to engage the 
participants in both wh? and yes?no type ques-
tions. We also wanted to investigate whether the 
use of wh? or yes?no questions alone has any 
impact on the interactions. For the remaining 
eight interactions, the dialogue system therefore 
alternatively used wh? and yes?no questions 
throughout the whole dialogue. The order of que-
ry types was switched for each successive partic-
ipant. Thus visual scenes presented with wh? 
queries to a participant pi were presented to par-
ticipant pi+1 with only yes?no queries, and vice-
versa. After each interaction participants were 
asked (1) whether the system appeared to be fa-
miliar with the visual scene; and (2) whether the 
interactions went smoothly. 
An example of a conversation with wh?type 
queries is presented in Table 1. The second col-
umn indicates the various types of utterances 
(dialogue acts) used by the system during this 
interaction. Note that during the slot-filling 
mode, the system has the initiative, whereas in 
the open-ended queries (cf. rows 15?22 in Table 
1) the system gives the initiative to the partici-
pant and only produces feedback responses. Ta-
ble 2 presents a conversation with yes?no queries 
following the conversation in Table 1. In a con-
versation with only yes?no  queries, a wh? query 
may be presented if a slot has been observed 
with no-value (cf. rows 9?10 in Table 1) or when 
the participant disputes the system?s current best 
estimate (cf. rows 6?9 in Table 2).  
 
S Introduction Could you help me with some de-
tails about this view? 
P  Yes 
S wh? query Do you see a building on the far 
left? 
P  Yes 
S Backchannel Mh-hmm 
S wh? query What color does the building have? 
P  Cream color [Green color] 
S Reprise Green 
S wh? query How many floors is this building? 
P  I don?t know [I do no] 
 ?  
S wh? query Are there any important signs in 
this scene that I may have missed? 
P  No 
S Backchannel Uh-huh 
S wh? query Is there anything peculiar in this 
scene that I should be familiar with 
P  There is a bike painted on street 
just in front of me 
[ is the right pink university in front 
of may ] 
S Backchannel Anything else? 
P  Yeah its white paint [Yes light pink] 
S Backchannel Mh-hmm 
P  That?s it [that that] 
S Backchannel Ehm 
P  I am done [okay] 
S Ending Okay, thanks 
 
Table 1: An example interaction with wh? que-
ries for the visual scene in Figure 2. S: system, P: 
participant (here, u01 from Figure 5). ASR re-
sults are shown within square brackets. 
 
S Introduction Could you help me with some details 
about this view? 
P  Yes 
S yes?no  
query 
Is the building on the far left visible to 
you? 
P  Yes 
S Backchannel Mh-hmm 
S yes?no  
query 
Does this building have green color? 
P  No 
S Backchannel Uh-huh 
S wh? query But than what color is the building? 
6
P  white [light] 
S Repeat Sorry, what color did you say? 
P  white [light] 
S wh? query How many stories is this building? 
P  I don?t know [I do rental] 
S Backchannel Ehm 
 ?  
S wh? query Are there any important signs in this 
scene that I may have missed? 
P  Ehm no [nine no] 
S Backchannel Ehm 
S yes?no  
query 
I am informed that there are other 
important landmarks or structures in 
this scene that I should be aware of, 
is it true? 
P  No 
S Backchannel uh-huh 
S Ending Okay, thanks 
 
Table 2: An example interaction with yes?no 
queries corresponding to the visual scene in Fig-
ure 2. S: system, P: participant (here u02 from 
Figure 5). ASR results are shown within square 
brackets. 
5 Data analysis 
We analyzed the data (15 8 interactions) col-
lected from the experiment along the following 
tracks: first, we compare the majority value of 
the slots to the ground truth as given by a human 
annotator; second, we explore how the ground 
truth of slot-values could be estimated automati-
cally; third, we also analyzed the instances where 
the participants disputed the system?s current 
estimate of slot-values; and fourth, we examined 
the post-experimental questionnaires.  
5.1 Rate of learning slot-values 
A total of 197 slots were learned in the exper-
iment. We analyzed how many slot-values had 
been correctly retrieved after 1, 2? 15 users. In 
Figure 6, the curve ?Majority? illustrates the 
fraction of slot-values correctly learned with 
each new user, under the assumption that the 
slot-values with majority votes ? from all the 15 
users ? constitute the ground truth. Thus after 
interacting with the first user the system had ob-
tained 67.0% of slot-values correctly (according 
to the majority) and 96.4% of slot-values after 
interacting with the first six users. Another eight 
users, or fourteen in total, were required to learn 
all the slot-values correctly. The progression 
curve thus provides an estimate of how many 
users are required to achieve a specific percent-
age of slot-values correctly if majority is to be 
considered the ground truth. The curve ?Not-in-
Majority? indicates the number of slot with val-
ues that were not in the majority. Thus after in-
teracting with the first user 20.8% of slot-values 
the system had obtained were not in majority and 
could be treated as incorrect. Note that the curves 
Majority and Not-in-Majority do not sum up to 
100%, this is because we consider no-value as a 
valid slot-value, and treat the slot as unfilled. For 
example, 12.2% of the slots remained unfilled 
after interacting with the first user.  
 
 
 
Figure 6: Rate of learning slot-values with two differ-
ent estimates of ground truth 
 
We also investigated how close the majority is 
to the actual truth. A human annotator (one of the 
coauthors) labeled all the obtained slot-values as 
either sensible or insensible, based on the com-
bined knowledge from the corresponding maps, 
the visual scenes, and the set of obtained values. 
Thus a slot could have many sensible values. For 
example, various parts of a building could be 
painted in different colors. The progression 
curves ?Sensible? and ?Insensible? in Figure 6 
illustrate the fraction of total slots for which the 
learned values were actually correct and incor-
rect, respectively. While the curve for sensible 
values follows the same pattern as the progres-
sion curve for majority as the estimate of ground 
truth, the percent of slot-values that were actually 
correct is always lower than the majority as 
ground truth, and it never reached 100%. The 
constant gap between the two curves suggests 
that some slot-values learned by the majority 
were not actually the ground truth. What led the 
majority into giving incorrect slot-values is left 
as a topic for future work. 
As mentioned earlier, much of the slot-filling 
interaction involved buildings and their proper-
ties. Figure 7 illustrates that sensible values for 
most slots, pertaining to whether a building is 
visible, whether it is residential, whether it has 
shops, and the color of roof were obtained by 
interacting with only few participants. In con-
trast, properties such as color of the building and 
7
number of stories required many more partici-
pants. This could be attributed to the fact that 
participants may have differences in perception 
about slot-values. As regards to whether there are 
signs on buildings, we observed that the recall is 
relatively low. This is largely due to lack of 
common ground among participants about what 
could be considered a sign. Our intentions with 
designing this prompt was to retrieve any peculi-
ar detail on the building that is easy to locate: for 
us a sign suggesting a name of restaurant is as 
useful as the knowledge that the building has 
blue sunshade on the windows. Some partici-
pants understood this while other didn?t. 
 
 
 
Figure 7: Learning rate of various slots for land-
mark type building  
5.2 Estimated ground truth of slot-values 
The 15 subjects in the in-lab experiment were all 
asked for the same information. In a real applica-
tion, however, we want the system to only ask 
for slots for which it has insufficient or conflict-
ing information. If the ground truth of a certain 
slot-value pair can be estimated with a certainty 
exceeding some threshold (given the quality re-
quirements of the database, say 0.8), the system 
can consider the matter settled, and need not ask 
about that slot again. We therefore want to esti-
mate the ground truth of slot-values along with a 
certainty measure. To this end, we use the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014), which is based on the 
probabilistic approach for supervised learning 
when we have multiple annotators providing la-
bels (possibly noisy) but no absolute gold stand-
ard, presented in Raykar et al. (2009). 
Using this approach, a question concerning the 
color of a building, say with ID 24, (e.g. ?What 
color is the building??) would be translated into 
several binary predicates COLOR_Red(24), 
COLOR_Brown(24), COLOR_Orange(24), etc. 
The justification for this binary encoding is that 
the different color values are not mutually exclu-
sive: A building might of course have more than 
one color, and in many cases more than one color 
name might be appropriate even though the 
building has only one dominating color (e.g. to 
describe the color either as ?brown? and ?red? 
might be acceptable to most people). Figure 8 
shows the incremental estimates for different 
colors for a certain building (OpenStreetMap ID 
163966736) after 1, 2? 15 subjects had been 
asked. The answer from the first subject was er-
roneously recognized as ?pink?. The next 9 sub-
jects all referred to the building as ?brown?. 
Among the final subjects, 3 subjects referred to 
building as ?red?, and 2 subjects as ?brown?. The 
final truth estimates are 0.98 for ?brown?, 0.002 
for ?red?, and 0.00005 for ?pink?. The diagram 
shows that if the certainty threshold is set to 0.8, 
the value ?brown? would have been established 
already after 4 subjects. 
 
 
 
Figure 8: Probabilities of different estimated ground 
truth values for the color of a certain building 
5.3 Disputed slot-values 
We also examined all system questions of 
yes?no type that received negative answers, i.e. 
instances where the participants disputed the sys-
tem?s current best estimate (based on majority 
vote) of a slot-value. Among the 95 such in-
stances, the system?s current best estimate was 
actually insensible only on 43 occasions. In 30 of 
these instances the participants provided a recti-
fied slot-value that was sensible. For the remain-
ing 13 instances the new slot-values proposed by 
the participant were actually insensible. There 
were 52 instances of false disputations, i.e. the 
system?s current estimate of a slot-value was 
sensible, but the participants disputed it. 6 of the-
se occurrences were due to errors in speech 
recognition, but for the remaining 46 occasions, 
error in grounding the intended landmark (15), 
users? perception of slot-values (3), and ambigui-
ty in what the annotator terms as sensible slot-
values (28), (e.g. whether there are signs on a 
building (as discussed in Section 5.1)) were iden-
8
tified as the main reasons. This suggests that 
slots (i.e. attributes) that are often disputed may 
not be easily understood by users. 
5.4 Post-experimental questionnaire 
As described above, the participants filled in a 
questionnaire after each interaction. They were 
asked to rate the system?s familiarity with the 
visual scene based on the questions asked. A 
Mann?Whitney U test suggests that participants? 
perception of the system?s familiarity with the 
visual scene was significantly higher for interac-
tions with yes?no queries than interactions with 
wh? queries (U=1769.5, p= 0.007). This result 
has implications for the design choice for sys-
tems that provide as well as ask for information 
from users. For example, a pedestrian routing 
system can already be used to offer routing in-
structions as well as crowdsourcing information. 
The system is more likely to give an impression 
of familiarity with the surrounding, to the user, 
by asking yes?no type questions than wh?
questions. This may influence a user?s confi-
dence or trust in using the routing system.  
Since yes?no questions expect a ?yes? or 
?no? in response, we therefore hypothesized that 
interactions with yes?no questions would be per-
ceived smoother in comparison to interactions 
with wh? questions. However, a Mann?Whitney 
U test suggests that the participants perceived no 
significant difference between the two interac-
tion types (U=1529.0, p= 0.248). Feedback 
comments from participants suggest that abrupt 
ending of open-ended interactions by the system 
(due to the simplistic model of detecting whether 
the user has anything more to say) gave users an 
impression that the system is not allowing them 
to speak. 
6 Discussion and future work 
We have presented a proof-of-concept study on 
using a spoken dialogue system for crowd-
sourcing street-level geographic information. To 
our knowledge, this is the first attempt at using 
spoken dialogue systems for crowdsourcing in 
this way. The system is fully automatic, in the 
sense that it (i) starts with minimal details ? ob-
tained from OpenStreetMap ? about a visual sce-
ne, (ii) prompts users with wh? questions to ob-
tain values for a predefined set of attributes; and 
(iii) assumes attribute-values with majority vote 
as its beliefs, and engages in yes?no questions 
with new participants to confirm them. In a data 
collection experiment, we have observed that 
after interacting with only 6 human participants 
the system acquires more than 80% of the slots 
with actually sensible values. 
We have also shown that the majority vote (as 
perceived by the system) could also be incorrect. 
To mitigate this, we have explored the use of the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014) for obtaining the proba-
bilistic estimate of the ground truth of slot-values 
in a real crowd-sourcing system. However, it is 
important not only to consider the ground truth 
probabilities per se, but also on how many con-
tributing users the estimate is based and the qual-
ity of information obtained. We will explore the-
se two issues in future work. 
We have observed that through open-ended 
prompts, the system could potentially collect a 
large amount of details about the visual scenes. 
Since we did not use any automatic interpretation 
of these answers, we transcribed key concepts in 
participants? speech in order to obtain an esti-
mate of this. However, it is not obvious how to 
quantify the number of concepts. For example, 
we have learned that in Figure 2, at the junction 
ahead, there is: a traffic-sign, a speed-limit sign, 
a sign with yellow color, a sign with red color, a 
sign with red boarder, a sign that is round, a sign 
with some text, the text says 50. These are details 
obtained in pieces from various participants. 
Looking at Figure 2 one can see that these pieces 
when put together refer to the speed-limit sign 
mounted on the traffic-signal at the junction. 
How to assimilate these pieces together into a 
unified concept is a task that we have left for fu-
ture work. 
Acknowledgement 
We would like to thank the participants of the in-
lab crowd-sourcing experiment. This work is 
supported by the EIT KIC project 
?CityCrowdSource?, and the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
Reference 
Bartie, P. J., & Mackaness, W. A. (2006). Develop-
ment of a Speech-Based Augmented Reality Sys-
tem to Support Exploration of Cityscape. Transac-
tions in GIS, 10(1), 63-86. 
Boye, J., Fredriksson, M., G?tze, J., Gustafson, J., & 
K?nigsmann, J. (2014). Walk This Way: Spatial 
Grounding for City Exploration. In Mariani, J., 
Rosset, S., Garnier-Rizet, M., & Devillers, L. 
9
(Eds.), Natural Interaction with Robots, Knowbots 
and Smartphones (pp. 59-67). Springer New York. 
Dickens, L., & Lupu, E. (2014). Trust service final 
deliverable report. Technical Report, Imperial Col-
lege, UK. 
Haklay, M., & Weber, P. (2008). OpenStreetMap: 
User-Generated Street Maps. IEEE Pervasive 
Computing, 7(4), 12-18. 
Janarthanam, S., Lemon, O., Liu, X., Bartie, P., 
Mackaness, W., Dalmas, T., & Goetze, J. (2012). 
Integrating Location, Visibility, and Question-
Answering in a Spoken Dialogue System for Pe-
destrian City Exploration. In Proceedings of the 
13th Annual Meeting of the Special Interest Group 
on Discourse and Dialogue (pp. 134-136). Seoul, 
South Korea: Association for Computational Lin-
guistics. 
Krug, K., Mountain, D., & Phan, D. (2003). Webpark: 
Location-based services for mobile users in pro-
tected areas.. GeoInformatics, 26-29. 
Parent, G., & Eskenazi, M. (2011). Speaking to the 
Crowd: Looking at Past Achievements in Using 
Crowdsourcing for Speech and Predicting Future 
Challenges. In INTERSPEECH (pp. 3037-3040). 
ISCA. 
Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Flor-
in, C., Valadez, G. H., Bogoni, L., & Moy, L. 
(2009). Supervised Learning from Multiple Ex-
perts: Whom to Trust when Everyone Lies a Bit. In 
Proceedings of the 26th Annual International Con-
ference on Machine Learning (pp. 889-896). New 
York, NY, USA: ACM. 
Ross, T., May, A., & Thompson, S. (2004). The Use 
of Landmarks in Pedestrian Navigation Instructions 
and the Effects of Context. In Brewster, S., & Dun-
lop, M. (Eds.), Mobile Human-Computer Interac-
tion - MobileHCI 2004 (pp. 300-304). Springer 
Berlin Heidelberg. 
Skantze, G., & Al Moubayed, S. (2012). IrisTK: a 
statechart-based toolkit for multi-party face-to-face 
interaction. In Proceedings of ICMI. Santa Monica, 
CA. 
Tom, A., & Denis, M. (2003). Referring to Landmark 
or Street Information in Route Directions: What 
Difference Does It Make?. In Kuhn, W., Worboys, 
M., & Timpf, S. (Eds.), Spatial Information Theo-
ry. Foundations of Geographic Information Sci-
ence (pp. 362-374). Springer Berlin Heidelberg. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
10
Appendix A 
The table below lists slots (= landmark attributes) and the corresponding wh? and yes?no system questions. For 
attributes marked with * the dialogue manager switches to open-ended interaction mode. 
 
Slot (=attribute ) System wh? questions System yes?no questions 
Visible: whether a particular 
landmark is visible from this 
view. 
? Do you see a building on the far left? 
? Do you see another building in front of 
you? 
? Is there a junction on the right? 
? Do you see a traffic-signal ahead? 
? Is the building on the far right visible to 
you? 
? I think there is another building in front of 
you, do you see it? 
? Can you see the junction on the right? 
? Are you able to see the traffic-signal 
ahead? 
Color of the building 
? What color does the building have? 
? What color is the building? 
? I think this building is red in color, what do 
you think? 
? Does this building have red color? 
Size of the building (in num-
ber of stories) 
? How many floors do you think are 
there in this building 
? How many stories is this building 
? I think there are six floors in this building, 
what do you think? 
? Is this building six storied? 
Color of the building?s roof 
? What color does the roof of this build-
ing have? 
? What color is the roof of this building? 
? I think the roof of this building is orange in 
color, what do you think? 
? Do you think that the roof of this building 
is orange? 
Signs or ornamentation on the 
building 
? Do you see any signs or decorations 
on this building? 
? I think there is a sign or some decoration 
on this building, do you see it? 
? There may be a sign or a name on this 
building, do you see it? 
Shops or restaurants in the 
building 
? Are there any shops or restaurants in 
this building? 
? I am informed that there are some shops or 
restaurants in this building, is it true? 
? I think there are some shops or restaurants 
in this building, what do you think? 
Signs at landmarks 
? Are there any important signs at the 
junction/crossing? 
? I believe there is a sign at this junc-
tion/crossing, do you see it? 
? Do you see the sign at this junc-
tion/crossing? 
*Description of sign  
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
*Signs in the visual scene 
 
? Are there any important signs in this 
scene that I may have missed? 
? Have I missed any relevant signs in 
this scene? 
? There are some important signs in this 
scene that could be useful for my 
knowledge, am I right? 
? I am informed that there are some signs in 
this scene that are relevant for me, is it 
true? 
*Landmarks in the visual sce-
ne 
 
? Are there any other important build-
ings or relevant structures in this scene 
that I should be aware of? 
? Is there anything particular in this 
scene that I should be familiar with? 
? Have I missed any relevant buildings 
or landmarks in this scene? 
? I am informed that there are some im-
portant landmarks or structures in this sce-
ne that I should be aware of, is it true? 
? I have been told that there are some other 
things in this scene that I are relevant for 
me, is it true? 
? I believe I have missed some relevant 
landmarks in this scene, am I right? 
*Description of unknown 
landmarks e.g. shop, restau-
rant, building, etc. 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
 
11
