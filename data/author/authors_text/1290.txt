Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 49?56
Manchester, August 2008
Enhancing Multilingual Latent Semantic Analysis 
with Term Alignment Information 
Brett W. Bader 
Computer Science &  
Informatics Department 
Sandia National Laboratories 
P. O. Box 5800, MS 1318 
Albuquerque, NM 87185-1318, USA 
bwbader@sandia.gov 
Peter A. Chew 
Cognitive Systems Research &  
Applications Department 
Sandia National Laboratories 
P. O. Box 5800, MS 1011 
Albuquerque, NM 87185-1011, USA 
pchew@sandia.gov 
 
Abstract 
Latent Semantic Analysis (LSA) is 
based on the Singular Value Decompo-
sition (SVD) of a term-by-document 
matrix for identifying relationships 
among terms and documents from co-
occurrence patterns. Among the multi-
ple ways of computing the SVD of a 
rectangular matrix X, one approach is to 
compute the eigenvalue decomposition 
(EVD) of a square 2 ? 2 composite ma-
trix consisting of four blocks with X and 
XT in the off-diagonal blocks and zero 
matrices in the diagonal blocks. We 
point out that significant value can be 
added to LSA by filling in some of the 
values in the diagonal blocks (corre-
sponding to explicit term-to-term or 
document-to-document associations) 
and computing a term-by-concept ma-
trix from the EVD.  For the case of mul-
tilingual LSA, we incorporate 
information on cross-language term 
alignments of the same sort used in Sta-
tistical Machine Translation (SMT). 
Since all elements of the proposed 
EVD-based approach can rely entirely 
on lexical statistics, hardly any price is 
paid for the improved empirical results. 
In particular, the approach, like LSA or 
SMT, can still be generalized to virtu-
ally any language(s); computation of the 
EVD takes similar resources to that of 
the SVD since all the blocks are sparse; 
 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
and the results of EVD are just as eco-
nomical as those of SVD. 
1 Introduction 
It is close to two decades now since Deerwester 
et al (1990) first proposed the application of the 
Singular Value Decomposition (SVD) to term-
by-document arrays as a statistics-based way of 
representing how terms and documents fit to-
gether within a semantic space. Since the ap-
proach was supposed to ?get beyond? the terms 
themselves to their underlying semantics, the 
approach became known as Latent Semantic 
Analysis (LSA). 
Soon after this application of SVD was widely 
publicized, it was suggested by Berry et al 
(1994) that, with a parallel corpus, the approach 
could be extended to pairs of languages to allow 
cross-language information retrieval (IR). It has 
since been confirmed that LSA can be applied 
not just to pairs of languages, but also simultane-
ously to groups of languages, again given the 
existence of a multi-parallel corpus (Chew and 
Abdelali 2007). 
In this paper, we return to the basics of LSA 
by examining its relationship with SVD, and, in 
turn, the mathematical relationship of SVD to the 
eigenvalue decomposition (EVD). These details 
are discussed in section 2. It has previously been 
suggested (for example, in Hendrickson 2007) 
that IR results could be improved by filling in 
information beyond that available directly in the 
term-by-document matrix, and replacing SVD 
with the more general EVD. To our knowledge, 
however, these suggestions have not been publi-
cized outside the mathematics community, nor 
have they been empirically tested in IR applica-
tions. With multilingual information retrieval as 
a use case, we consider alternatives in section 3 
for implementation of this idea. One of these re-
49
lies on no extraneous information beyond what is 
already available in the multi-parallel corpus, and 
is based entirely on the statistics of cross-
language term alignments. ?Regular? LSA has 
been shown to work best when a weighting 
scheme such as log-entropy is applied to the 
elements in the term-by-document array (Dumais 
1991), and in section 3 we also consider various 
possibilities for how the term alignments should 
best be weighted. Section 4 recapitulates on a 
framework that allows EVD with term align-
ments to be compared with a number of related 
approaches (including LSA without term align-
ments). This is a recapitulation, because the same 
testing framework has been used previously (for 
other linear-algebra based approaches) by Chew 
and Abdelali (2007) and Chew et al (2007). The 
results of our comparison are presented and dis-
cussed in section 5, and we conclude upon these 
results and suggest further avenues for research 
in section 6. 
2 The relationship of SVD to EVD, and 
its application to information retrieval 
In the standard LSA framework (Deerwester et 
al. 1990) the (sparse) term-by-document matrix 
X is factorized by the singular value decomposi-
tion (SVD),  
 
X = USV T (1) 
 
where U is an orthonormal matrix of left singular 
vectors, S is a diagonal matrix of singular values, 
and V is an orthonormal matrix of right singular 
vectors (Golub and van Loan 1996). 
Typically for LSA, a truncated SVD is com-
puted such that equality in (1) no longer holds 
and that the best rank-R least-squares approxima-
tion to matrix X is formed by keeping the R larg-
est singular values in S and discarding the rest. 
This also means that the first R vectors of U and 
V are retained, where R indicates the number of 
concept dimensions in LSA. Each column vector 
in U maps the terms to a single arbitrary concept, 
such that terms which are semantically related 
(as determined by patterns of co-occurrence) will 
tend to be grouped together with large values in 
columns of U. 
There are many ways to compute the SVD of a 
sparse matrix. One expedient way is to compute 
the eigenvalue decomposition (EVD) of either 
XTX or XXT, depending on the largest di-
mension of X, to obtain U or V, respectively. 
With U or V, one may compute the rest of the 
SVD by a simple matrix-matrix multiplication 
and renormalization. 
Another way to compute the SVD is to com-
pute the eigenvalue decomposition of the 2-by-2 
block matrix 
B = 0 XXT 0



	.
The eigenvalues of B are the singular values of 
X, replicated as both positive and negative, plus 
a number of zeroes if X is not square. The left 
and right singular vectors are contained within 
the eigenvectors of this composite matrix B. As-
sume that X is of size m ? n and that m 
 n, with 
left singular vectors U = Un Umn[ ], where Un
corresponds to the n positive singular values and 
Um-n corresponds to the remaining m-n zero sin-
gular values. Let Q denote the orthogonal matrix 
of eigenvectors corresponding to the nonnegative 
eigenvalues of B, then the matrices of left and 
right singular vectors are stacked on top of each 
other, U on top of V, as follows: 
 Q = 12
Un 2 ?Umn
V 0



	.
Hence, one may compute the truncated SVD of 
X by computing only the eigenvectors corre-
sponding to the largest R eigenvalues and then 
extracting and rescaling the U and V matrices 
from Q. 
 
Figure 1. Eigenvalue decomposition in multilin-
gual information retrieval 
 
In the context of multilingual LSA using a 
parallel corpus, the block matrix B is depicted in 
Figure 1, where the terms are shaded according 
to each language. Each language may have a dif-
ferent number of terms, so the language blocks 
are not expected to be the same size as one an-
other. The eigenvectors and eigenvalues of B are 
also shown. 
We may obtain a pair of U and S matrices for 
each language by extracting the corresponding 
partition of U from the eigenvectors. We desire 
each language-specific U matrix to have columns 
of unit length, which we accomplish by comput-
ing the length of each of its columns and then 
50
rescaling the columns of U by the inverse length 
and multiplying the eigenvalues by these lengths 
for our S matrix. We call this approach ?Tucker1? 
because the result is identical to creating a U and 
S matrix for each language from the general 
Tucker1 model found by three-way analysis of 
the terms-by-documents-by-language array 
(Tucker 1966). 
For applications in information retrieval, we 
usually want to compute a measure of similarity 
between documents. Once we have U and S, we 
can estimate similarities by computing the cosine 
of the angle between the document vectors in the 
smaller ?semantic space? of the R concepts found 
by LSA. New documents in different languages 
can be projected into this common semantic 
space by multiplying their document vectors 
(formed in exactly the same way as the columns 
for X) by the product US-1, to yield a document-
by-concept vector. 
3 From SVD to term-alignment-based 
EVD 
If we compute just the SVD of a term-document 
matrix X, then the technique we use to accom-
plish this (whether computing the EVD of the 
block matrix B or otherwise) is immaterial from 
a computational linguist?s point of view: there is 
no advantage in one technique over another. 
However, the technique of EVD allows one to 
augment the LSA framework with additional in-
formation beyond just the term-document matrix. 
In Figure 1, the two diagonal blocks contain only 
zeroes, but we envision augmenting B with term 
alignment information such that the upper diago-
nal block captures any term-to-term similarities. 
Additional term-term alignment information 
serves to enhance the term-by-concept vectors in 
U by providing explicit, external knowledge so 
that LSA can learn more refined concepts. While 
not explored in this paper, we also envision in-
corporating any document-to-document similari-
ties into the lower diagonal block. 
Let D1 and D2 denote symmetric matrices. We 
augment the block matrix B and redefine it as a 
more general symmetric matrix, 
 B = D1 XXT D2



	.
If both D1 and D2 are equal to the identity matrix, 
then the eigenvalues of B are shifted by one, but 
the eigenvectors are not affected. 
Since our use case here is multilingual infor-
mation retrieval, imagine for the moment that an 
oracle provides dictionary information that 
matches up words in each of our language pairs 
(Arabic-English, Arabic-French, etc.) by mean-
ing. Thus, for example, we might have a pairing 
between English house and French maison. This 
information may be encoded in the diagonal 
block D1 by replacing zeroes in the cells for 
(house, maison) and its symmetric entry with 
some nonzero value indicating the strength of 
association for the two terms. Completing all 
relevant entries in D1 in this fashion serves to 
strengthen the co-occurrence information in the 
parallel corpus that LSA normally finds via the 
SVD.  
In the simplest approach, if the oracle indi-
cates a match between two terms i and j, then a 
one could be inserted in D1 at positions (i,j) and 
(j,i). If D1 were filled with such term alignment 
information, the matrix B would still be sparse. 
Without any document-document information, 
then D2 could be either the identity matrix or the 
zero matrix. Our experience has shown that D2 =
0 works slightly better in practice. Figure 2 
shows a block matrix augmented with term 
alignments in this fashion. 
Figure 2. Augmented block matrix with term 
alignments 
 
The eigenvalue decomposition of B now in-
corporates this extra term information provided 
in D1, and the eigenvectors show stronger corre-
spondence between those terms indicated. How-
ever, with each term aligned with one or more 
other terms, the row and column norms of D1 are 
unequal, which means that some terms may be 
biased to appear more heavily in the eigenvec-
tors. In addition, the magnitude or ?weight? of D1
relative to X needs to be considered, otherwise 
the explicit alignments in D1 and the co-
51
occurrence information in X may be out of bal-
ance with one another. Properly normalizing and 
scaling D1 may mitigate both of these risks. 
There are several possibilities for normalizing 
the matrix D1. Sinkhorn balancing (Sinkhorn 
1964) is a popular technique for creating a dou-
bly stochastic matrix (rows and columns all sum 
to 1) from a square matrix of nonnegative ele-
ments. Sinkhorn balancing is an iterative algo-
rithm in which, at each step, the row and column 
sums are computed and then subsequently used 
to rescale the matrix. For balancing the matrix A, 
each iteration consists of two updates 
A  WRA
A  AWC
where WR is a diagonal matrix containing the 
inverse of row sums of A, and WC is a diagonal 
matrix containing the inverse of column sums of 
A. This algorithm exhibits linear convergence, so 
many iterations may be needed. The algorithm 
may be adapted for normalizing the row and col-
umn vectors according to any norm.  Our experi-
ence has shown that normalizing D1 with respect 
to the Euclidean norm works well in practice. 
In terms of scaling D1 relative to X, we simply 
multiply D1 by a positive scalar value, which we 
denote with the variable . The optimal value of 
 appears to be problem dependent. 
Let us return for the moment to the question of 
how we populate D1 in the first place, and what 
each entry in that block represents. In the simple 
case described above, the existence of a 1 at po-
sition (i,j) indicates that an alignment exists be-
tween terms i and j, and a zero indicates that no 
alignment exists. But in reality, a binary encod-
ing like this may be too simplistic. In this re-
spect, it is instructive to consider how we 
populate D1 in the light of the weighting scheme 
used for X, since the latter is discussed in Du-
mais (1991) and is by now quite well understood.  
In the simplest case, an entry of 1 in X at posi-
tion (i,j) can denote that term i occurs in docu-
ment j, just as in our simple case with D1. A
slightly more refined alternative is to replace 1 
with fi,j, where fi,j denotes the raw frequency of 
term i within document j. But, as Dumais (1991) 
shows, it is significantly better in practice to use 
a ?log-entropy? weighting scheme. This adjusts 
fi,j first by ?dampening? high-frequency terms 
(using the log of the frequency), and secondly by 
giving a lower weight to terms which occur in 
many documents.2 The former adjustment is re-
 
2 One can also raise the global weight in the log-entropy 
scheme to a power (which we denote with the variable ). 
lated to an insight from Zipf?s law, which is that 
the dampened term frequency will be in propor-
tion to the log of the term?s rank in frequency. 
The latter adjustment is based on information 
theory; a term which is scattered across many 
documents (such as ?and? in English) has a high 
entropy, and therefore lower intrinsic informa-
tion content. 
Suppose, therefore, that our ?dictionary? oracle 
could not only indicate the existence of an 
alignment, but also provide some numerical 
value for the strength of association between two 
aligned terms. (In practice, this is probably more 
than one could hope for even from the best pub-
lished bilingual dictionaries.) This information 
could then replace the ones in D1 prior to Sink-
horn balancing and matrix weighting. 
While one cannot expect to obtain this infor-
mation from published dictionaries, there is in 
fact a statistical approach to gathering the neces-
sary information, which we borrow from SMT 
(Brown et al 1994). All that is required is the 
existence of a parallel corpus, which we already 
have in place for multilingual LSA. 
Here, an entry fi,j in D1 is based on the mutual 
information of term I and term J, or I(I;J) (capi-
tals are used to indicate that the terms are treated 
here as random variables). It is an axiom that: 
 
I(I;J) = H(I) + H(J) ? H(I,J) (2) 
 
where H(I) and H(J) are the marginal entropies 
of I and J respectively, and H(I,J) is the joint en-
tropy of I and J. Properties of H(I,J) include the 
following: 
 
H(I,J) 
 H(I) 
 0
H(I,J) 
 H(J) 
 0
H(I,J)  H(I) + H(J) (3) 
 
Considering (2) and (3) together, it should be 
clear that I(I;J) will range between 0 and the 
maximum value for H(I) or H(J). 
For the purposes of populating D1, we com-
pute the entropy of a term i by considering the 
number of documents where i occurs, and the 
number of documents where i does not occur, 
and express these as probabilities. For the joint 
entropy H(I,J), we need to compute four prob-
abilities based on all the possibilities: documents 
where both terms occur, those where I occurs 
without J, those where J occurs without I, and 
 
Selecting   1 can, in practice, yield better results in the 
applications we have tested. 
52
those where neither occur. The result of this is 
that a numerical value is attached to each align-
ment: higher values indicate that terms are 
strongly correlated, and lower values indicate 
that one term predicts little about the other. For 
each pair of words (i,j) which co-occur in any 
text chunk in the parallel corpus, we can say that 
an alignment exists if, among all the possibilities, 
mutual information for i is maximized by select-
ing j, and vice versa. (Since the maximization of 
mutual information is not necessarily reciprocal, 
the effect of this is to be conservative in postulat-
ing alignments.) The weight of this alignment is 
its mutual information (equivalent to the ?global 
weight? of log-entropy) multiplied by the log of 
one plus the number of text chunks in which that 
alignment appears (equivalent to the ?local 
weight? of log-entropy). 
Some examples of English-French pairs at ei-
ther end of this spectrum (where mutual informa-
tion is non-zero) are given in Table 1. 
 
I(I;J) Alignment 
weight 
I J
0.000176 0.000176 hearing ?coutait 
0.000217 0.000217 misery mis?rable 
?
0.270212 2.884297 house maison 
0.321754 3.506663 king roi 
0.415702 6.025456 and et 
0.472925 5.798080 I je 
Table 1. Term alignment and mutual information 
 
We believe that this approach, which weights 
alignments based on mutual information, fits 
very well with the log-entropy scheme used for 
X, since both are solidly based on the same 
foundation of information theory. 
All together, we call this particular process 
LSATA, which stands for LSA with term align-
ments.  
4 Testing framework 
Since the inception of the Cross-Language 
Evaluation Forum (CLEF) in 2000, there has 
been growing interest in cross-language IR, and a 
number of parallel corpora have become avail-
able (for example through the Linguistic Data 
Consortium). Widely used examples include the 
Canadian Hansard parliament proceedings (in 
French and English). Harder to obtain are multi-
parallel corpora ? those where the same text is 
translated into more than two parallel languages. 
One such corpus which has not yet gained 
wide acceptance, perhaps owing to the percep-
tion that it has less relevance to real-world appli-
cations than other parallel corpora, is the Bible. 
Yet the range of languages covered is unarguably 
unmatched elsewhere, and one might contend 
that its relevance is in some ways greater than, 
say, Hansard?s, as its impact on Western culture 
has been broader than that of Canadian govern-
ment debates. Similarly, the Quran, while not 
translated into as many languages as the Bible, 
has had a significant impact on another large 
segment of the world?s population. 
But the relevance or otherwise of the Bible 
and/or Quran, and the extent to which they have 
been accepted by the computational linguistics 
community at large as parallel corpora, are some-
what beside the point for us here. Our interest is 
in developing theory and applications which 
have universal applicability to as many lan-
guages as possible, regardless of the subject mat-
ter or whether the languages are ancient or 
modern. One might compare this approach to 
Chomsky?s quest for Universal Grammar 
(Chomsky 1965), except that the theory in our 
case is based on lexical statistics and linear alge-
bra rather than rule-based generative grammar. 
The Bible and Quran have in fact previously 
been used for experiments similar to ours (e.g., 
Chew et al 2007). By using these texts as paral-
lel corpora, therefore, we facilitate direct com-
parison of our results with previous ones. But 
besides this, the Bible has some especially attrac-
tive properties for our current purposes. First, the 
carefulness of the translations means that we are 
relatively unlikely to encounter situations where 
cross-language term alignments are impossible 
because some text is missing in one of the trans-
lations. Secondly, the relatively small size of the 
parallel text chunks (by and large, each chunk is 
a verse, most of which are about a sentence in 
length) greatly facilitates the process of statistical 
term alignment. (This is based on the combina-
torics: the number of possible term-to-term 
alignments increases approximately quadratically 
with the number of terms per text chunk.) 
Thus, our framework is as follows. In our 
term-by-document matrix X, the documents are 
verses, and the terms are distinct wordforms in 
any of the five languages used in the test data in 
Chew et al (2007): Arabic (AR), English (EN), 
French (FR), Russian (RU) and Spanish (ES). As 
in Chew et al (2007), too, our test data consists 
of the text of the Quran in the same 5 languages. 
In this case, the ?documents? are the 114 parallel 
suras (or chapters) of the Quran. We obtained all 
translations of the Bible and Quran from openly-
53
available websites such as that of Biola Univer-
sity (2005-2006) and http://www.kuran.gen.tr. 
As already mentioned, SVD of a term-by-
document matrix is equivalent to EVD of a block 
matrix in which two of the blocks (the non-
diagonal ones) are X and XT. As described in 
section 3, we fill in some of the values of D1 with 
nonzeroes (from term alignments derived from 
the Bible). In all cases (both SVD and EVD), we 
performed a truncated decomposition in either 
60, 240, or 300 dimensions. 
 
Term alignment  
settings 
SVD/EVD 
dimensions 
Type of  
decomposition 
Include term  
alignments? / 
weighting type Sinkhorn 
balanced? 

Global 
weight
*
Average 
P1 
Average
MP5 
SVD 0.7116 0.5702 
Tucker1 0.7170 0.5770 
PARAFAC2 
N/A 1.8 
0.7420 0.6580 
no N/A 0.7000 0.5691 
4.0 1.8 0.7611 0.6474 
1.0 0.7716 0.5972 yes (binary) yes 
4.0 1.6 0.7979 0.6467 
no N/A 0.6481 0.3804 
1.0 0.7393 0.5972 
12.0 
1.8 
0.8088 0.6972 
1.0 0.7488 0.5789 
60 
LSATA 
yes (log-MI) yes 
12.0 1.6 0.7933 0.6586 
SVD 0.8761 0.6554 240 PARAFAC2 N/A 1.8 0.8975 0.7853 
SVD N/A 1.8 0.8796 0.6575 
yes (binary) 4.0 1.6 0.9421 0.7695 
1.8 0.8982 0.8000 300 LSATA yes (log-MI) yes 12.0 1.6 0.9182 0.8067 
*See footnote 2. 
Table 2. Results with various linear algebraic decomposition methods and weighting schemes 
 
To evaluate the different methods against one 
another, we use similar measures of precision as 
were used with the same dataset by Chew et al 
(2007): precision at 1 document (P1) (the aver-
age proportion of cases where the translation of a 
document ranked highest among all retrieved 
documents of the same language) and multilin-
gual precision at 5 documents (MP5) (the aver-
age proportion of the top 5 ranked documents 
which were translations of the query document 
into any of the 5 languages, among all retrieved 
documents of any language). By definition, MP5 
is always less than or equal to P1; MP5 measures 
success in multilingual clustering, while P1 
measures success in retrieving documents when 
the source and target languages are pre-specified. 
5 Results and Discussion 
Table 2 above presents a summary of our results. 
The main point to note is that the addition of in-
formation on term alignments is clearly benefi-
cial. An approach based on the Tucker1 
decomposition algorithm, without any informa-
tion on term alignments, achieves P1 of 0.7170 
and MP5 of 0.5770. With scaled term alignment 
information, the results improve to 0.7611 and 
0.6474, respectively. Using a chi-squared test, 
we tested the significance of the increase in P1 
and found it to be highly significant (p  1.7 ?
10-7). 
The results also show, however, that one needs 
to be careful about how the word-alignment in-
formation is added. Without some form of bal-
ancing and scaling of D1, there is little 
improvement (and often significant deterioration) 
in the results when alignment information is in-
cluded. 
In addition to comparing a block EVD ap-
proach with term alignments to one without, we 
also compared against another decomposition 
method, PARAFAC2, which has been found to 
be more effective than SVD in cross-language IR 
(Chew et al 2007). Here, the results are more 
equivocal. P1 is slightly higher under the 
LSATA approach (with binary values in D1) than 
54
under PARAFAC2, while the reverse is true for 
MP5. The difference for P1 is significant at p < 
0.05 but not at p < 0.01. In any case, there are 
risks in making a comparison between 
PARAFAC2 and LSATA. For one thing, 
PARAFAC2, as implemented here, includes no 
mechanism for incorporating term-alignment 
information. It is not clear to us yet whether such 
a mechanism could (mathematically or practi-
cally) be incorporated into PARAFAC2. Sec-
ondly, we are not yet confident that we have 
found the optimal weighting scheme for the D1
block under the LSATA model. Our experiments 
with different weighting and normalization 
schemes for the D1 block are still in relatively 
initial stages, though it can also be seen from 
Table 2 that by selecting certain settings under 
LSATA (replacing binary weighting in D1 with 
mutual-information-based weighting, and apply-
ing scaling with beta = 12.0), we were able to 
improve upon PARAFAC2 under both measures. 
Although we have not tested all settings, Table 
2 also shows our best results to date with this 
dataset, which have come from applying EVD to 
the block matrix that includes D1. The precise 
optimal settings for EVD appear to depend on 
whether the objective is to maximize P1 or MP5. 
For P1, our best results (0.9421) were obtained 
with binary weighting, global term  = 1.6, and 
= 4.0. For MP5, the best results (0.8067) were 
obtained with mutual-information based weight-
ing,  = 1.8, and  = 12.0. It appears in all cases 
that D1 needs to be balanced if it contains term 
alignment information. 
The evidence, then, appears to be strongly in 
favor of incorporating information beyond term-
to-document associations within an IR approach 
based on linear algebra. It happens that LSATA 
offers an obvious way to do this, while other 
methods such as PARAFAC2 may or may not. 
Here, we have examined just one form of infor-
mation besides term-to-document statistics: term-
to-term statistics. However, there is no reason to 
suppose that the results might not be improved 
still further by incorporating information on 
document-to-document associations, or for that 
matter associations between terms or documents 
and other linguistic, grammatical, or contextual 
objects. 
6 Conclusion 
In this paper, we have discussed the mathe-
matical relationship between SVD and EVD, and 
specifically the fact that SVD is a special case of 
EVD. For information retrieval, the significance 
of this is that SVD allows for explicit encoding 
of associations between terms and documents, 
but not between terms and terms, or between 
documents and documents. 
By moving from the special case of SVD to 
the general case of EVD, however, we open up 
the possibility that additional information can be 
encoded prior to decomposition. We have exam-
ined a particular use case for SVD: multilingual 
information retrieval. This use case presents an 
interesting example of additional information 
which could be encoded on the term-by-term 
diagonal block: cross-language pairings of 
equivalent terms (such as house/maison). Such 
pairs can be obtained from bilingual dictionaries, 
but we can save ourselves the trouble of obtain-
ing and using these. Multilingual LSA requires 
that a parallel corpus have already been obtained, 
and well-understood statistical term alignment 
procedures can be applied to obtain cross-
language term-to-term associations. Moreover, if 
the corpus is multi-parallel, we can ensure that 
the statistical basis for alignment is the same 
across all language pairs. 
Our results show that by including term-to-
term alignment information, then performing 
EVD, we can improve the results of cross-
language IR quite significantly. 
It should be pointed out that while we have 
successfully used statistics-based information in 
the term-by-term diagonal block, there is no rea-
son to suppose that similar or better results might 
not be achieved by manually filling in nonzeroes 
in either diagonal block. The additional informa-
tion encoded by these nonzeroes could include 
associations known a priori between documents 
(e.g., they were written by the same author) or 
terms (e.g., they occur together in a thesaurus), 
or both. While in these examples the additional 
information required might not be available from 
the training corpus, and its encoding could in-
volve moving away from an entirely statistics-
based model, the additional effort could be justi-
fied depending upon the intended application. 
In future work, we would like to examine in 
particular whether still further statistically-
derivable (or readily available) data could be in-
corporated into the model. For example, one can 
conceive of a block EVD involving ?levels? be-
yond the ?term level? and the ?document level?. 
In a 3?3 block EVD, for example, one might in-
clude n-grams, terms, and documents; this ap-
proach should also be extensible to essentially all 
languages. Might the addition of further informa-
55
tion lead to even higher precision? Avenues for 
research such as this raise their own questions, 
such as the type of weighting scheme which 
would have to be applied in a 3?3 block matrix. 
In summary, however, our results give us 
some confidence that there can be significant 
benefit in making more linguistic and/or statisti-
cal information available to linear algebraic IR 
approaches such as EVD. Cross-language term 
alignments are just one example of the type of 
additional information which could be included; 
we believe that future research will uncover 
many more similar examples. 
Acknowledgement 
Sandia is a multiprogram laboratory operated 
by Sandia Corporation, a Lockheed Martin Com-
pany, for the United States Department of En-
ergy?s National Nuclear Security Administration 
under contract DE-AC04-94AL85000. 
References  
Michael W. Berry, Susan T. Dumais., and G. W. 
O?Brien. 1994. Using Linear Algebra for Intelli-
gent Information Retrieval. SIAM: Review 37, 573-
595. 
Biola University. 2005-2006. The Unbound Bible.
Accessed at http://www.unboundbible.org/ on Jan. 
29, 2008. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1994. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics 
19(2), 263-311. 
Peter A. Chew and Ahmed Abdelali. 2007. Benefits 
of the ?Massively Parallel Rosetta Stone?: Cross-
Language Information Retrieval with over 30 Lan-
guages . Proceedings of the 45th Annual Meeting 
of the Association for Computational Linguistics, 
ACL 2007. Prague, Czech Republic, June 23?30, 
2007. pp. 872-879. 
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Cambridge, MA: MIT Press. 
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. 
Landauer and R. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American 
Society for Information Science 41:6, 391-407. 
Susan Dumais. 1991. Improving the Retrieval of In-
formation from External Sources. Behavior Re-
search Methods, Instruments, and Computers 
23(2):229-236. 
Gene H. Golub and Charles F. van Loan. 1996. Ma-
trix Computations, 3rd edition. The Johns Hopkins 
University Press: London. 
R. A. Harshman. 1972. PARAFAC2: Mathematical 
and Technical Notes. UCLA Working Papers in 
Phonetics 22, 30-47. 
Bruce Hendrickson. 2007. Latent Semantic Analysis 
and Fiedler Retrieval. Linear Algebra and its Ap-
plications 421 (2-3), 345-355. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. Proceedings of the Joint 
Conference on Human Language Technologies and 
the Annual Meeting of the North American Chapter 
of the Association of Computational Linguistics 
(HLT/NAACL), 48-54. 
P. Koehn. 2002. Europarl: a Multilingual Corpus for 
Evaluation of Machine Translation. Unpublished, 
accessed on Jan. 29, 2008 at http:// 
www.iccs.inf.ed.ac.uk/~pkoehn/publications/europ
arl.pdf. 
Philip Resnik, Mari Broman Olsen, and Mona Diab. 
1999. The Bible as a Parallel Corpus: Annotating 
the "Book of 2000 Tongues". Computers and the 
Humanities, 33: 129-153.  
R. Sinkhorn. 1964. A Relation between Arbitrary 
Positive Matrices and Doubly Stochastic Matrices. 
Annals of Mathematical Statistics 35 
(2), 876-879. 
Ledyard R. Tucker. 1966.  Some Mathematical Notes 
on Three-mode Factor Analysis, Psychometrika 31, 
279-311. 
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, 
and C. Lee Giles. 2007. Co-Ranking Authors and 
Documents in a Heterogeneous Network. Seventh 
IEEE InternationalConference on Data Mining,
739-744. 
56
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 129?136
Manchester, August 2008
Latent Morpho-Semantic Analysis: 
Multilingual Information Retrieval with Character N-Grams  
and Mutual Information 
 
Peter A. Chew, Brett W. Bader 
Sandia National Laboratories 
P. O. Box 5800 
Albuquerque, NM 87185, USA 
{pchew,bwbader}@sandia.gov 
Ahmed Abdelali 
New Mexico State University 
P.O. Box 30002, Mail Stop 3CRL 
Las Cruces, NM 88003-8001, USA 
ahmed@crl.nmsu.edu 
 
Abstract 
We describe an entirely statistics-based, 
unsupervised, and language-
independent approach to multilingual 
information retrieval, which we call La-
tent Morpho-Semantic Analysis 
(LMSA). LMSA overcomes some of the 
shortcomings of related previous ap-
proaches such as Latent Semantic 
Analysis (LSA). LMSA has an impor-
tant theoretical advantage over LSA: it 
combines well-known techniques in a 
novel way to break the terms of LSA 
down into units which correspond more 
closely to morphemes. Thus, it has a 
particular appeal for use with morpho-
logically complex languages such as 
Arabic. We show through empirical re-
sults that the theoretical advantages of 
LMSA can translate into significant 
gains in precision in multilingual infor-
mation retrieval tests. These gains are 
not matched either when a standard 
stemmer is used with LSA, or when 
terms are indiscriminately broken down 
into n-grams. 
1 Introduction 
As the linguistic diversity of textual resources 
increases, and need for access to those resources 
grows, there is also greater demand for efficient 
 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
information retrieval (IR) methods which are 
truly language-independent. In the ideal but pos-
sibly unattainable case, an IR algorithm would 
produce equally reliable results for any language 
pair: for example, a query in English would re-
trieve equally good results in Arabic as in 
French. 
A number of developments in recent years 
have brought that goal more within reach. One of 
the factors that severely hampered early attempts 
at machine translation, for example, was the lack 
of available computing power. However, 
Moore?s Law, the driving force of change in 
computing since then, has opened the way for 
recent progress in the field, such as Statistical 
Machine Translation (SMT) (Koehn et al 2003). 
Even more closely related to the topic of the pre-
sent paper, implementations of the Singular 
Value Decomposition (SVD) (which is at the 
heart of LSA), and related algorithms such as 
PARAFAC2 (Harshman 1972), have become 
both more widely available and more powerful. 
SVD, for example, is available in both commer-
cial off-the-shelf packages and at least one open-
source implementation designed to run on a par-
allel cluster (Heroux et  al. 2005). 
Despite these advances, there are (as yet) not 
fully surmounted obstacles to working with cer-
tain language pairs, particularly when the lan-
guages are not closely related. This is 
demonstrated in Chew and Abdelali (2008). At 
least in part, this has to do with the lexical statis-
tics of the languages concerned. For example, 
because Arabic has a much richer morphological 
structure than English and French (meaning is 
varied through the addition of prefixes and suf-
fixes rather than separate terms such as parti-
cles), it has a considerably higher type-to-token 
129
ratio. Exactly this type of language-specific sta-
tistical variation seems to lead to difficulties for 
statistics-based techniques such as LSA, as evi-
denced by lower cross-language information re-
trieval (CLIR) precision for Arabic/English than 
for French/English (Chew and Abdelali 2008). 
In this paper, we present a strategy for over-
coming these difficulties. In section 2, we outline 
the basic problem and the thinking behind our 
approach: that breaking words down into mor-
phemes, or at least morphologically significant 
subconstituents, should enable greater inter-
language comparability. This in turn should in 
theory lead to improved CLIR results. Several 
alternatives for achieving this are considered in 
section 3. One of these, a novel combination of 
mutual-information-based morphological tokeni-
zation (a step beyond simple n-gram tokeniza-
tion) and SVD, is what we call LMSA. Section 4 
discusses the framework for testing our intui-
tions, and the results of these tests are presented 
and discussed in section 5. Finally, we draw 
some conclusions and outline possible directions 
for future research in section 6. 
2 The problem 
In many approaches to IR, the underlying 
method is to represent a corpus as a term-by-
document matrix in which each row corresponds 
to a unique term2, and each column to a docu-
ment in the corpus. The standard LSA frame-
work (Deerwester et al 1990) is no different, 
except that the (sparse) term-by-document matrix 
X is subjected to SVD,  
 
X = USVT (1) 
 
where U is a smaller but dense term-by-concept 
matrix, S is a diagonal matrix of singular values, 
and V is a dense document-by-concept matrix for 
the documents used in training. Effectively, U 
and V respectively map the terms and documents 
to a single set of arbitrary concepts, such that 
semantically related terms or documents (as de-
termined by patterns of co-occurrence) will tend 
to be similar; similarity is usually measured by 
taking the cosine between two (term or docu-
ment) vectors. New documents can also be pro-
jected into the LSA ?semantic space? by 
multiplying their document vectors (formed in 
exactly the same way as the columns for X) by 
 
2 Pragmatically, terms can be defined very straightforwardly 
in the regular expressions language as sequences of charac-
ters delimited by non-word characters. 
the product US-1, to yield document-by-concept 
vectors. LSA is a completely unsupervised ap-
proach to IR in that associations between terms 
simply fall out when SVD is applied to the data. 
With cross-language or multilingual LSA, the 
approach differs little from that just outlined. The 
only required modification is in the training data: 
the term-by-document matrix must be formed 
from a parallel corpus, in which each document 
is the combination of text from the parallel lan-
guages (as described in Berry et al 1994). 
Clearly, this IR model cannot be deployed to any 
languages not in the parallel corpus used for 
training SVD. However, recent work (Chew et 
al. 2007) shows not only that there is no limit (at 
least up to a certain point) to the number of lan-
guages that can be processed in parallel, but that 
precision actually increases for given language 
pairs as more other languages are included. In 
practice, the factors which limit the addition of 
parallel languages are likely to be computational 
power and the availability of parallel aligned 
text. As noted in section 1, the first of these is 
less and less of an issue; and regarding the sec-
ond, parallel corpora (which are the mainstay of 
many current approaches to computational lin-
guistics and IR, particularly in real-world appli-
cations) are becoming increasingly available. 
Substantially all of the Bible, in particular, is al-
ready electronically available in at least 40-50 
languages from diverse language families (Biola 
University 2005-2006). 
Yet, there are clearly variations in how well 
CLIR works. In previous results (Chew et al 
2007, Chew and Abdelali 2008) it is noticeable 
in particular that the results for Arabic and Rus-
sian (the two most morphologically complex 
languages for which they present results) are 
consistently poorer than they are for other lan-
guages. To our knowledge, no solution for this 
has been proposed and validated. Ideally, a solu-
tion would both make sense theoretically (or lin-
guistically) and be statistics-based rather than 
rule-based, consistent with the general frame-
work of LSA and other recent developments in 
the field, such as SMT, and avoiding the need to 
build a separate grammar for every new language 
? an expensive undertaking. 
Translation Types Tokens Ratio 
English (KJV) 12,335 789,744 1.56% 
French (Darby) 20,428 812,947 2.51% 
Spanish (RV 1909) 28,456 704,004 4.04% 
Russian (Syn 1876) 47,226 560,524 8.43% 
Arabic (S. Van Dyke) 55,300 440,435 12.56% 
Table 1. Lexical statistics in a parallel corpus 
130
To begin to assess the problem, one can com-
pare the lexical statistics for the Bible from 
Chew et al (2007), which should be directly 
comparable since they are from a parallel corpus. 
These are arranged in Table 1 in order of type-to-
token ratio. 
This ordering also corresponds to the ordering 
of languages on a scale from ?analytic? to ?syn-
thetic?: meaning is shaped in the former by the 
use of particles and word order, and in the latter 
by inflection and suffixation. Some examples 
illustrating differences between Russian and 
English in this respect are given in Table 2. 
English I read you read they read 
Russian KLMNO KLMNPQR KLMNOM 
Table 2. Analytic versus synthetic languages 
 
The element in Russian, of course, which cor-
responds to ?read? is the stem ?KLMN?, but this is 
embedded within a larger term. Hence, in all 
three examples, Russian takes one term to ex-
press what in English takes two terms. The same 
occurs (although to a lesser extent) in English, in 
which ?read? and ?reads? are treated as distinct 
terms. Without any further context (such as sen-
tences in which these terms are instantiated), the 
similarity in meaning between ?read? and ?reads? 
will be readily apparent to any linguist, simply 
because of the shared orthography and morphol-
ogy. But for an approach like standard LSA in 
which terms are defined simply as distinct enti-
ties delimited by non-word characters, the mor-
phology is considered immaterial ? it is invisible. 
The only way a standard term-based approach 
can detect any similarity between ?read? and 
?reads? is through the associations of terms in 
documents. Clearly, then, such an approach op-
erates under a handicap. 
Two unfortunate consequences will inevitably 
result from this. First, some terms will be treated 
as out-of-vocabulary even when at least some of 
the semantics could perhaps have been derived 
from a part of the term. For example, if the train-
ing corpus contains ?read? and ?reads? but not 
?reading?, valuable information is lost every time 
?reading? is encountered in a new document to 
which LSA might be deployed. Secondly, asso-
ciations that should be made between in-
vocabulary terms will also be missed. Perhaps a 
reason that more attention has not been devoted 
to this is that the problem can largely be disre-
garded in highly analytic languages like English. 
But, as previous results such as Chew and Abde-
lali?s (2008) show, for a language like Arabic, 
the adverse consequences of a morphology-blind 
approach are more severe. The question then is: 
how can information which is clearly available in 
the training corpus be more fully leveraged with-
out sacrificing efficiency? 
3 Possible solutions 
3.1 Replacing terms with n-grams 
At first glance, one might think that stemming 
would be an answer. Stemming has been shown 
to improve IR, in particular for morphologically 
complex languages (recent examples, including 
with Arabic, are Lavie et al 2004 and Abdou et 
al. 2005). We are not aware, however, of any 
previous results that show unequivocally that 
stemming is beneficial specifically in CLIR. 
Chew and Abdelali (2008) examine the use of a 
light stemmer for Arabic (Darwish 2002), and 
while this does result in a small overall increase 
in overall precision, there is paradoxically no 
increase for Arabic. The problem may be that the 
approach for Arabic needs to be matched by a 
similar approach for other languages in the paral-
lel corpus. However, since stemmers are usually 
tailored to particular languages ? and may even 
be unavailable for some languages ? use of exist-
ing stemmers may not always be an option. 
Another more obviously language-
independent approach is to replace terms with 
character n-grams3. This is feasible for more or 
less any language, regardless of script. Moreover, 
implementation of a similar idea is described in 
McNamee and Mayfield (2004) and applied spe-
cifically to CLIR. However, McNamee and May-
field?s CLIR results are solely for European 
languages written in the Roman script. This is 
why they are able to obtain, in their words, ?sur-
prisingly good results? without translation [of 
the query]?, and without using LSA in any form. 
With related languages in the same script, and 
particularly when n-grams are used in place of 
terms, the existence of cognates means that many 
translations can easily be identified, since they 
probably share many of the same n-grams (e.g. 
French ?parisien? versus English ?Parisian?). 
When languages do not all share the same script 
or come from the same language family, how-
ever, the task can be considerably harder. 
Since the approach of n-gram tokenization has 
the advantages of being entirely statistically-
 
3 Hereafter, we use the term ?n-grams? to refer specifically 
to character (not word) n-grams. 
131
based and language-independent, however, we 
examined whether it could be combined with 
LSA to allow CLIR (including cross-script re-
trieval), and whether this would lead to any ad-
vantage over term-based LSA. Our intuition was 
that some (although not all) n-grams would cor-
respond to morphologically significant subcon-
stituents of terms, such as ?read? from ?reading?, 
and therefore associations at the morpheme level 
might be facilitated. The steps for this approach 
are listed in Table 3. 
1 Form a term-by-document array from the paral-
lel corpus as described above 
2 For each term, list all (overlapping) n-grams4
3 Replace terms in the term-by-document array 
with n-grams, to form an n-gram-by-document 
array 
4 Subject the n-gram-by-document array to SVD 
to produce an n-gram-by-concept U matrix, sin-
gular values (the diagonal S matrix), and docu-
ment-by-concept V matrix 
5 Project new documents into the semantic space 
by multiplying their vectors by US-1 
Table 3. Steps for n-gram-based LSA 
 
Under all approaches, we selected the same 
log-entropy term weighting scheme that we used 
for standard LSA. Thus, whether a term t stands 
for a wordform or an n-gram, its weighted fre-
quency W in a particular document k is given by: 
 
W = log2 (F + 1) ? (1 + Ht / log2 (N))X (2) 
 
where F is the raw frequency of t in k, Ht is the 
entropy of the term or n-gram across all docu-
ments, N is the number of documents in the cor-
pus, and X is some arbitrary constant (a power to 
which the global weight is raised).  We have 
found that an X > 1 improves precision by chang-
ing the relative distribution of weighted frequen-
cies. Common terms with high entropy become 
much less influential in the SVD.  
It should be noted that step (2) in Table 3 is 
similar to McNamee and Mayfield?s approach, 
except that we did not include word-spanning n-
grams, owing to computational constraints. We 
also tried two variants of step (2), one in which 
all n-grams were of the same length (as per 
McNamee and Mayfield 2004), and one in which 
n-grams of different lengths were mixed. Under 
the second of these, the number of rows in both 
the term-by-document and U matrices is of 
course considerably larger. For example, Table 4 
 
4 As an example, for ?cat?, the complete list of overlapping 
n-grams would be ?c?, ?a?, ?t?, ?ca?, ?at?, and ?cat?. 
shows that the number of rows in the n-gram-by-
document matrix for English (EN) under the first 
variant (with n = 6) is 19,801, while under the 
second (with n Z 6) it is 58,907. Comparable sta-
tistics are given for Arabic (AR), Spanish (ES), 
French (FR) and Russian (RU). 
n= AR EN ES FR RU 
1 35 27 41 41 47
2 939 516 728 708 827
3 11,127 4,267 5,563 5,067 7,808
4 40,835 13,927 19,686 15,948 30,702
5 53,671 20,369 35,526 25,253 54,647
6 39,822 19,801 42,408 28,274 65,308
Total 146,429 58,907 103,952 75,291 159,339
Table 4. Number of distinct n-grams by language 
and length, up to length 6, based on Bible text 
 
3.2 Replacing terms with morphemes: 
LMSA 
We also attempted a related approach with 
non-overlapping n-grams. This set of 
experiments was guided by the intuition that not 
all n-grams are morphologically significant. 
Before we discuss the details of this approach, 
consider the English example ?comingle?. Here, 
?co? + ?mingle? are likely to be more significant 
to the overall meaning than ?coming? + ?le? ? in 
fact, the presence of the n-gram ?coming? could 
be misleading in this case. One way to model this 
would be to change the weighting scheme. The 
problem with this is that the weighting for one 
token has to be contingent on the weighting for 
another in the same term. Otherwise, in this 
example, the n-gram ?coming? would presumably 
receive a high weighting based on its frequency 
elsewhere in the corpus. 
An alternative is to select the tokenization 
which maximizes mutual information (MI). 
Brown et al (1992) describe one application of 
MI to identify word collocations; Kashioka et al 
(1998) describe another, based on MI of charac-
ter n-grams, for morphological analysis of Japa-
nese. The pointwise MI of a pair s1 and s2 as 
adjacent symbols is 
 
MI = log P(s1 s2) ? log P(s1) ? log P(s2) (3) 
 
If s1 follows s2 less often than expected on the 
basis of their independent frequencies, then MI is 
negative; otherwise, it is positive. 
In our application, we want to consider all 
candidate tokenizations, sum MI for each candi-
date, and rule out all but one candidate. A to-
132
kenization is a candidate if it exhaustively parses 
the entire string and has no overlapping tokens. 
Thus, for ?comingle?, co+mingle, coming+le, 
comingle, c+o+m+i+n+g+l+e, etc., are some of 
the candidates, but comi+ngl and com+mingle 
are not. To obtain MI, we need to compute the 
log probability (logp) of every n-gram in the cor-
pus. If Sk (k = 1, ?, K) denotes the set of all n-
grams of length k, and sn is a particular n-gram of 
length n, then we compute logp for sn as: 
 
logp = log F(sn) ? log [ (F(Sn))  (4) 
 
where F(sn) is the frequency of sn in the corpus, 
and [ (F(Sn)) is the sum of the frequencies of all 
Sn in the corpus.5 In all cases, logp is negative, 
and MI is maximized when the magnitude of the 
sum of logp for all elements in the tokenization 
(also negative) is minimized, i.e. closest to zero. 
Tokenizations consisting of one, two or more 
elements (respective examples are comingle, 
co+mingle, and co+ming+le) will all receive a 
score, although those with fewer elements will 
tend to be favored. 
We considered some minor variants in the set-
tings for this approach in which word-initial and 
word-final n-grams were indexed separately from 
word-medial n-grams. Guided by McNamee and 
Mayfield?s (2004) finding that there is an optimal 
(language-dependent) value of k for Sk, we also 
varied the maximum length of n-grams allowed 
in tokenizations. Under all settings, we followed 
steps 3-5 from Table 3 (including SVD) from 
here on. 
This approach (which we call latent morpho-
semantic analysis), then, is like LSA, except that 
the types and tokens are statistically-derived 
morphemes rather than terms. Whatever LMSA 
variant is used, the underlying approach to mor-
phological tokenization is completely language-
independent. Example output is shown in Table 5 
for wordforms from the Russian lemma 
]^P_`abNMR_c ?to crawl?, where the common 
stem (or at least an approximation thereof) is cor-
rectly identified. 
Wordform Tokenization 
]^P_`abNOdP`e_c ]^P_`abNO dP`e_c 
]^P_`abNOdL`L_c ]^P_`abNO dL`L_c 
]^P_`abNOdL`_c ]^P_`abNO dL`_c 
]^P_`abNOdLf_c ]^P_`abNO dLf_c 
Table 5. Examples of MI-based tokenization 
 
5 Note that (4) is closely related to the ?weighted mutual 
information? measure used in Goldsmith (2001: 172). 
We do not directly test the accuracy of these 
tokenizations. Rather, measures of CLIR preci-
sion (described in section 4) indirectly validate 
our morphological tokenizations. 
4 Testing framework 
To assess our results on a basis comparable with 
previous work, we used the same training and 
test data as used in Chew et al (2007) and Chew 
and Abdelali (2008). The training data consists 
of the text of the Bible in 31,226 parallel chunks, 
corresponding generally to verses, in Arabic, 
English, French, Russian and Spanish. The test 
data is the text of the Quran in the same 5 lan-
guages, in 114 parallel chunks corresponding to 
suras (chapters). 
Questions are sometimes raised as to how rep-
resentative the Bible and/or Quran are of modern 
language. However, there is little question that 
the number and diversity of parallel languages 
covered by the Bible6 is not matched elsewhere 
(Resnik et al 1999), even by more mainstream 
parallel corpora such as Europarl (Koehn 2002)7.
The diversity of languages covered is a particu-
larly important criterion for our purposes, since 
we would like to look at methods which enhance 
retrieval for languages across the analytic-
synthetic spectrum. The Bible also has the ad-
vantage of being readily available in electronic 
form: we downloaded all our data in a tab-
delimited, verse-indexed format from the ?Un-
bound Bible? website mentioned above (Biola 
University, 2005-2006). 
In accordance with previous work, we split the 
test set into each of the 10 possible language-pair 
combinations: AR-EN, AR-FR, EN-FR, and so 
on. For each language pair and test, 228 distinct 
?queries? were submitted ? each query consisting 
of one of the 228 sura ?documents?. To assess the 
aggregate performance of the framework, we 
used average precision at 1 document, hereafter 
?P1? (1 if the translation of the document ranked 
highest, zero otherwise ? thus, a fairly strict 
measure of precision). We also measured preci-
sion on a basis not used by Chew et al (2007) or 
Chew and Abdelali (2008): multilingual preci-
sion at 5 documents (hereafter ?MP5?). For this, 
 
6 At December 31, 2006, complete translations existed in 
429 languages, and partial translations in 2,426 languages 
(Bible Society 2007).  
7 Since the Europarl text is extracted from the proceedings 
of the European Parliament, the languages represented are 
generally closely-related to one another (most being Ger-
manic or Romance). 
133
each of the 570 documents (114 suras, each in 5 
languages) is submitted as a query. The results 
are drawn from the pool of all five languages, so 
MP5 represents the percentage, on average, of 
the top 5 documents which are translations of the 
query. This measure is still stricter than P1 (this 
is a mathematical necessity) because the retrieval 
task is harder. Essentially, MP5 measures how 
well similar documents cluster across languages, 
while P1 measures how reliably document trans-
lations are retrieved when the target language is 
known. 
5 Results and Discussion 
The following tables show the results of our 
tests. First, we present in Table 6 the results us-
ing standard LSA, in which terms are sequences 
of characters delimited by non-word characters. 
Here, in essence, we reperformed an experiment 
in Chew and Abdelali (2008). 
P1 (overall average: 0.8796) 
AR EN ES FR RU 
AR 1.0000 0.7544 0.7193 0.7368 0.7544 
EN 0.7719 1.0000 0.9123 0.9386 0.9474 
ES 0.6316 0.9298 1.0000 0.9298 0.8947 
FR 0.7719 0.9035 0.9298 1.0000 0.9386 
RU 0.7719 0.9298 0.9035 0.9211 1.0000 
MP5: AR 0.4456, EN 0.7211, ES 0.6649,  
FR 0.7614, RU 0.6947; overall average: 0.6575 
Table 6. Results with standard LSA 
 
Our results differ from Chew and Abdelali?s 
(2008) ? our precision is higher ? because we use 
a different value of X in equation (2) above (here, 
1.8 rather than 1). Generally, we selected X so as 
to maximize MP5; discussion of this is beyond 
the scope of this paper, and not strictly relevant 
in any case, since we present like-for-like com-
parisons throughout this section. However, Table 
6 shows clearly that our results replicate those 
previously published, in that precision for Arabic 
(the most ?synthetic? of the five languages) is 
consistently lower than for the other four. 
The next set of results (in Table 7) is for LSA 
with SVD of an array in which the rows corre-
spond to all overlapping, but not word-spanning, 
n-grams of fixed length. The best results here, for 
n=4, are essentially no better on average than 
those obtained with standard LSA. However, 
averaging across languages obscures the fact that 
results for Arabic have significantly improved 
(for example, where Arabic documents are used 
as queries, MP5 is now 0.6205 instead of 
0.4456). Still, the fact that average MP5 is essen-
tially unchanged means that this is at the expense 
of results for other languages. 
n = Average P1 Average MP5 
3 0.8340 0.4951 
4 0.8779 0.6761 
5 0.8232 0.6365 
6 0.6957 0.5197 
7 0.5321 0.3986 
Table 7. Results with LSA / overlapping n-grams 
of fixed length 
 
Now we present results in Table 8 where SVD 
is performed on an array in which the rows cor-
respond to all overlapping, but not word-
spanning, n-grams of any length (varying maxi-
mum length). 
n Z Average P1 Average MP5 
3 0.8235 0.3909 
4 0.9039 0.6256 
5 0.9095 0.6839 
6 0.8863 0.6716 
7 0.8635 0.6470 
Table 8. Results with LSA / overlapping n-grams 
of variable length 
 
Here, the best results (with n<=5) more clearly 
improve upon LSA: the increases in both P1 and 
MP5, though each only about 0.03 in absolute 
terms, are highly significant (p < 0.005). Very 
likely this is related to the fact that when n-grams 
are used in place of words, the out-of-vocabulary 
problem is alleviated. But there is quite a high 
computational cost, which will become apparent 
in Table 10 and the discussion accompanying it. 
A practical advantage of the ?morpheme?-by-
document array of LMSA, on the other hand, is 
that this cost is substantially reduced. This is be-
cause, as already mentioned, the vast majority of 
n-grams are eliminated from consideration. 
However, does taking this step significantly hurt 
performance? The results for LMSA presented in 
Table 9 provide an answer to this. 
For P1, the results are comparable to standard 
LSA when we select settings of n Z 7 (maximum 
permitted morpheme length) or above. But under 
the stricter MP5 measure, LMSA not only sig-
nificantly outperforms standard LSA (p < 0.001, 
at n Z 9); the results are also superior to those 
obtained under any other method we tested. The 
improvement in MP5 is comparable to that for 
P1 ? 0.677 to 0.707 ? when Chew and Abdelali 
(2008) use the Darwish Arabic light stemmer to 
provide input to LSA; our approach, however, 
has the advantage that it is fully unsupervised. 
134
n Z Average P1 Average MP5 
4 0.6947 0.4411 
5 0.8151 0.6102 
6 0.8614 0.6793 
7 0.8709 0.6912 
8 0.8663 0.6856 
9 0.8765 0.6909 
10 0.8772 0.6740 
Table 9. Results with LMSA8
As when n-grams are used without MI, fewer 
types are out-of-vocabulary: for example, with 
certain settings for LMSA, we found that the 
percentage of out-of-vocabulary types dropped 
from 65% under LSA to 29% under LMSA, and 
the effect was even more marked for Arabic 
taken individually (78.5% to 34.4%). This is de-
spite the fact mentioned above that LMSA arrays 
are more economical than LSA arrays: in fact, as 
Table 10 shows, 22% more economical (the size 
of the U matrix output by SVD, used to create 
vectors for new documents, is determined solely 
by the number of rows, or types). Note also that 
both LSA and LMSA are significantly more eco-
nomical than SVD with overlapping n-grams. 
Technique Rows Nonzeros 
LSA 163,745 2,684,938 
LSA with overlapping 
n-grams (where n Z 5) 
527,506 45,878,062 
LMSA 127,722 3,215,078 
Table 10. Comparative matrix sizes 
 
Even the results in Table 9 can still be im-
proved upon. Following McNamee and May-
field?s insight that different length n-grams may 
be optimal for different languages, we attempted 
to improve precision further by varying n inde-
pendently by language. For all languages but 
Arabic, n Z 9 seems to work well (either increas-
ing or decreasing maximum n resulted in a drop 
in precision), but by setting n Z 6 for Arabic, P1 
increased to 0.8874 and MP5 to 0.7368. As com-
parison of Table 11 with Table 6 shows, some of 
the most significant individual increases were for 
Arabic. It should however be noted that the op-
timal value for n may be dataset-dependent. 
Since n is a maximum length (unlike in 
McNamee and Mayfield?s experiments), one 
might expect that increasing n should never re-
 
8 These results are with the stipulation that word-initial and 
word-final n-grams are distinguished from word-medial n-
grams. We also ran experiments in which this distinction 
was not made. Detailed results are not presented here; suf-
fice it to say that when word-medial and other morphemes 
were not distinguished, precision was hurt somewhat (low-
ering it often by several percentage points). 
sult in a drop in precision. We believe the benefit 
to limiting the size of n is connected to Brown et 
al.?s (1992: 470) observation that ?as n increases, 
the accuracy of an n-gram model increases, but 
the reliability of our parameter estimates, drawn 
as they must be from a limited training text, de-
creases?. Effectively, the probabilities used in MI 
are unrepresentatively high for longer n-grams 
(this becomes clear if one considers the extreme 
example of an n-gram the same length as the 
training corpus). 
P1 (overall average: 0.8874) 
AR EN ES FR RU 
AR 1.0000 0.7895 0.7719 0.7281 0.7807 
EN 0.8158 1.0000 0.9298 0.9298 0.9123 
ES 0.7807 0.9474 1.0000 0.9123 0.8684 
FR 0.7632 0.9035 0.9474 1.0000 0.8947 
RU 0.7456 0.9298 0.9298 0.9035 1.0000 
MP5: AR 0.5140, EN 0.8035, ES 0.8228,  
FR 0.8035, RU 0.7404; overall average: 0.7368 
Table 11. Best results with LMSA 
 
If setting a maximum value for n makes sense 
in general, the idea of a lower maximum for 
Arabic in particular also seems reasonable since 
Arabic words, generally written as they are with-
out vowels, contain on average fewer characters 
than the other four languages, and contain roots 
which are usually three or fewer characters long. 
6 Conclusion 
In this paper, we have demonstrated LMSA, a 
linguistically (specifically, morphologically) 
more sophisticated alternative to LSA. By com-
puting mutual information of character n-grams 
of non-fixed length, we are able to obtain an ap-
proximation to a morpheme-by-document matrix 
which can substitute for the commonly-used 
term-by-document matrix. At the same time, be-
cause mutual information is based entirely on 
statistics, rather than grammar rules, all the ad-
vantages of LSA (language-independence, speed 
of implementation and fast run-time processing) 
are retained. In fact, some of these advantages 
may be increased since the number of index 
items is often lower. 
Although from a linguist?s point of view the 
theoretical advantages of LMSA may be intrinsi-
cally satisfying, the benefit is not confined to the 
theoretical realm. Our empirical results show that 
LMSA also brings practical benefits, particularly 
when performing IR with morphologically com-
plex languages like Arabic. Principally, this 
seems to be due to two factors: alleviation of the 
135
out-of-vocabulary problem and improvement in 
the associations made by SVD. 
We believe that the results we have presented 
may point the way towards still more sophisti-
cated types of analysis, particularly for multilin-
gual text. We would like to explore, for example, 
whether it is possible to use tensor decomposi-
tion methods like PARAFAC2 to leverage asso-
ciations between n-grams, words, documents and 
languages to still better advantage. 
Finally, it is worth pointing out that our ap-
proach offers an indirect way to test our statis-
tics-based approach to morphological analysis. 
The better our ?morphemes? correspond to mini-
mal semantic units (as theory dictates they 
should), the more coherently our system should 
work overall. In this case, our final arbiter of the 
system?s overall performance is CLIR precision. 
In short, our initial attempts appear to show 
that statistics-based morphological analysis can 
be integrated into a larger information retrieval 
architecture with some success. 
Acknowledgement 
Sandia is a multiprogram laboratory operated 
by Sandia Corporation, a Lockheed Martin Com-
pany, for the United States Department of En-
ergy?s National Nuclear Security Administration 
under contract DE-AC04-94AL85000. 
References  
S. Abdou, P. Ruck, and J. Savoy. 2005. Evaluation of 
Stemming, Query Expansion and Manual Indexing 
Approaches for the Genomic Task. Proceedings of 
TREC 2005.
M. W. Berry, S. T. Dumais., and G. W. O?Brien. 
1994. Using Linear Algebra for Intelligent Infor-
mation Retrieval. SIAM: Review 37, 573-595. 
Bible Society. 2006. A Statistical Summary of Lan-
guages with the Scriptures. Accessed Jan 5 2007 at 
http://www.biblesociety.org/latestnews/latest341-
slr2005stats.html. 
Biola University. 2005-2006. The Unbound Bible.
Accessed Jan 29 2008 at 
http://www.unboundbible.org/. 
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della 
Pietra, and J. C. Lai. 1992. Class-Based n-gram 
Models of Natural Language. Computational Lin-
guistics 18(4), 467-479. 
P. Chew and A. Abdelali. 2007. Benefits of the ?Mas-
sively Parallel Rosetta Stone?: Cross-Language In-
formation Retrieval with over 30 Languages. 
Proceedings of the Association for Computational 
Linguistics, 872-879. 
P. Chew and A. Abdelali. 2008. The Effects of Lan-
guage Relatedness on Multilingual Information Re-
trieval: A Case Study With Indo-European and 
Semitic Languages. Proceedings of the Workshop 
on Cross-Language Information Access.
K. Darwish. 2002. Building a shallow Arabic morpho-
logical analyzer in one day. Proceedings of the As-
sociation for Computational Linguistics, 47-54. 
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. 
Landauer and R. Harshman. 1990.  Indexing by La-
tent Semantic Analysis. Journal of the American 
Society for Information Science 41:6, 391-407. 
J. Goldsmith. 2001. Unsupervised Learning of the 
Morphology of a Natural Language. Computa-
tional Linguistics 27(2), 153-198. 
R. A. Harshman. 1972. PARAFAC2: Mathematical 
and Technical Notes. UCLA Working Papers in 
Phonetics 22, 30-47. 
M. Heroux, R. Bartlett, V. Howle, R. Hoekstra, J. Hu, 
T. Kolda, R. Lehoucq, K. Long, R. Pawlowski, E. 
Phipps, A. Salinger, H. Thornquist, R. Tuminaro, J. 
Willenbring, A. Williams, and K. Stanley. 2005. 
An Overview of the Trilinos Project. ACM Trans-
actions on Mathematical Software 31:3, 397-423. 
H. Kashioka, Y. Kawata, Y. Kinjo, A. Finch and E. 
W. Black. 1998. Use of Mutual Information Based 
Character Clusters in Dictionary-less Morphologi-
cal Analysis of Japanese. Proceedings of the 17th 
International Conference on Computational Lin-
guistics Vol. 1: 658-662. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. Proceedings of the Joint 
Conference on Human Language Technologies and 
NAACL, 48-54. 
P. Koehn. 2002. Europarl: a Multilingual Corpus for 
Evaluation of Machine Translation. Unpublished. 
Accessed Jan 29 2008 at 
http://www.iccs.inf.ed.ac.uk 
/~pkoehn/publications/europarl.pdf. 
A. Lavie, E. Peterson, K. Probst, S. Wintner, and Y. 
Eytani. 2004. Rapid Prototyping of a Transfer-
Based Hebrew-to-English Machine Translation 
System. Proceedings of the TMI-04.
P. McNamee and J. Mayfield. 2004. Character N-
Gram Tokenization for European Language Text 
Retrieval. Information Retrieval 7, 73-97. 
P. Resnik, M. Broman Olsen, and M. Diab. 1999. The 
Bible as a Parallel Corpus: Annotating the "Book 
of 2000 Tongues". Computers and the Humanities 
33: 129-153.  
136
The Effects of Language Relatedness on Multilingual Information Re-
trieval: A Case Study With Indo-European and Semitic Languages 
Peter A. Chew 
Sandia National Laboratories 
P. O. Box 5800, MS 1012 
Albuquerque, NM 87185-1012, USA 
pchew@sandia.gov 
Ahmed Abdelali 
New Mexico State University 
P.O. Box 30002, Mail Stop 3CRL 
Las Cruces, NM 88003-8001, USA 
ahmed@crl.nmsu.edu 
 
 
Abstract 
We explore the effects of language related-
ness within a multilingual information re-
trieval (IR) framework which can be de-
ployed to virtually any language, focusing 
specifically on Indo-European versus Se-
mitic languages. The Semitic languages 
present unique challenges to IR for a num-
ber of reasons, so we set out to answer the 
question of whether cross-language IR for 
Semitic languages can be boosted by ma-
nipulation of the training data (which, in 
our framework, includes multilingual paral-
lel text, some of which is morphologically 
analyzed). We attempted three measures to 
achieve this: first, the inclusion of geneti-
cally related (i.e., other Semitic) languages 
in the training data; second, the inclusion 
of non-related languages sharing the same 
script, and third, the inclusion of morpho-
logical analysis for Semitic languages. We 
find that language relatedness is a definite 
factor in boosting IR precision; script simi-
larity can probably be ruled out as a factor; 
and morphological analysis can be helpful, 
but ? perhaps paradoxically ? not necessar-
ily to the languages which are subjected to 
morphological analysis. 
1 Introduction 
In this paper, we consider how related languages 
fit into a general framework developed for 
multilingual cross-language information retrieval 
(CLIR). Although this framework can deal with 
virtually any language, there are some special 
considerations which make related languages more 
interesting for exploration. Taking one example, 
Semitic languages are distinguished by their 
complex morphology, a characteristic which 
presents challenges to an information retrieval 
model in which terms (usually, separated by white 
space or punctuation) are implicitly treated as 
individual units of meaning. We consider three 
possible methods for investigating the phenomena. 
In all cases, we keep the overall framework the 
same but simply make changes to the training data. 
One method we consider is to augment the train-
ing data with text from related languages; we com-
pare results obtained from using Semitic languages 
with those obtained when non-Semitic languages 
are used. The other two relate to morphological 
analysis: the second is to replace inflected forms 
(in just one language, Arabic) with just the root in 
the training data; and the third is to remove vowels 
(again in just one language, Hebrew). 
The paper is organized as follows. Section 2 de-
scribes our general framework, which is a standard 
one used for CLIR. At a high level, section 3 out-
lines some of the challenges Semitic languages 
present within the context of our approach. In sec-
tion 4, we compare results from using a number of 
different combinations of training data with the 
same test data. Finally, we conclude on our find-
ings in section 5. 
2 The Framework 
2.1 General description 
The framework that we use for IR is multilingual 
Latent Semantic Analysis (LSA) as described by 
Berry et al (1994:21, and used by Landauer and 
Littman (1990) and Young (1994). A number of 
different approaches to CLIR have been proposed; 
generally, they rely either on the use of a parallel 
corpus for training, or translation of the IR query. 
Either or both of these methods can be based on 
the use of dictionaries, although that is not the ap-
proach that we use. 
In the standard multilingual LSA framework, a 
term-by-document matrix is formed from a parallel 
aligned corpus. Each ?document? consists of the 
concatenation of all the languages, so terms from 
all languages will appear in any given document. 
Thus, if there are K languages, N documents (each 
of which is translated into each of the K lan-
guages), and T distinct linguistic terms across all 
languages, then the term-by-document matrix is of 
dimensions T by N. Each cell in the matrix repre-
sents a weighted frequency of a particular term t 
(in any language) in a particular document n. The 
weighting scheme we use is a standard log-entropy 
scheme in which the weighted frequency xt,n of a 
particular term t in a particular document n is given 
by: 
 W = log2 (F + 1) ? (1 + Ht / log2 (N)) 
 
where F is the raw frequency of t in n, and Ht is a 
measure of the entropy of the term across all 
documents. The last term in the expression above, 
log2 (N), is the maximum entropy that any term 
can have in the corpus, and therefore (1 + Ht / log2 
(N)) is 1 for the most distinctive terms in the cor-
pus, 0 for those which are least distinctive. The 
log-entropy weighting scheme has been shown to 
outperform other schemes such as tf-idf in LSA-
based retrieval (see for example Dumais 1991). 
The sparse term-by-document matrix is sub-
jected to singular value decomposition (SVD), and 
a reduced non-sparse matrix is output. Generally, 
we used the output corresponding to the top 300 
singular values in our experiments. 
To evaluate the similarity of unseen queries or 
documents (those not in the training set) to one 
another, these documents are tokenized, the 
weighted frequencies are calculated in the same 
way as they were for the training set, and the re-
sults are multiplied by the matrices output by the 
SVD to project the unseen queries/documents into 
a ?semantic space?, assigning (in our case) 300-
dimensional vectors to each document. Again, our 
approach to measuring the similarity of one docu-
ment to another is a standard one: we calculate the 
cosine between the respective vectors. 
For CLIR, the main advantages of an approach 
like LSA are that it is by now quite well-
understood; the underlying algorithms remain con-
stant regardless of which languages are being 
compared; and there is wide scope to use different 
sets of training data, providing they exist in paral-
lel corpora. LSA is thus a highly generic approach 
to CLIR: since it relies only on the ability to token-
ize text at the boundaries between words, or more 
generally semantic units, it can be generalized to 
virtually all languages. 
2.2 Training and test data 
For our experiments, the training and test data 
were taken from the Bible and Quran respectively. 
As training data, the Bible lends itself extremely 
well to multilingual LSA. It is highly available in 
multiple languages1 (over 80 parallel translations 
in 50 languages, mostly public-domain, are avail-
able from a single website, 
www.unboundbible.org); and a very fine-grained 
alignment is possible (by verse) (Resnik et al1999, 
Chew and Abdelali 2007). Many purpose-built 
parallel corpora are biased towards particular lan-
guage groups (for example, the European Union 
funds work in CLIR, but it tends to be biased to-
wards European languages ? for example, see Pe-
ters 2001). This is not as true of the Bible, and the 
fact that it covers a wider range of languages is a 
reflection of the reasons it was translated in the 
first place. 
The question which is most commonly raised 
about use of the Bible in this way is whether its 
coverage of vocabulary from other domains is suf-
ficient to allow it to be used as training data for 
most applications. Based on a variety of experi-
ments we have carried out (see for example Chew 
et al forthcoming), we believe this need not al-
ways be a drawback ? it depends largely on the 
intended application. However, it is beyond our 
scope to address this in detail here; it is sufficient 
to note that for the experiments we describe in this 
paper, we were able to achieve perfectly respect-
able CLIR results using the Bible as the training 
data. 
                                                 
1 It has proved hard to come by reliable statistics to al-
low direct comparison, but the Bible is generally be-
lieved to be the world?s most widely translated book. At 
the end of 2006, it is estimated that there were full trans-
lations into 429 languages and partial translations into 
2,426 languages (Bible Society 2007). 
As test data, we used the 114 suras (chapters) of 
the Quran, which has also been translated into a 
wide variety of languages. Clearly, both training 
and  test data have to be available in multiple lan-
guages to allow the effectiveness of CLIR to be 
measured in a meaningful way. For the experi-
ments reported in this paper, we limited the testing 
languages to Arabic, English, French, Russian and 
Spanish (the respective abbreviations AR, EN, FR, 
RU and ES are used hereafter). The test data thus 
amounted to 570 (114 ? 5) documents: a relatively 
small set, but large enough to achieve statistically 
significant results for our purposes, as will be 
shown. In all tests described in this paper, we use 
the same test set: thus, although the test documents 
all come from a single domain, it is reasonable to 
suppose that the comparative results can be gener-
alized to other domains. 
The complete list of languages used for both 
testing and training is given in Table 1. 
 
Language Bible -training- Quran -test- Language Family Sub-Family 
Afrikaans Yes No Indo-European Germanic-West 
Amharic Yes No Afro-Asiatic Semitic-South 
Arabic Yes Yes Afro-Asiatic Semitic-Central 
Aramaic Yes No Afro-Asiatic Semitic-North 
Czech Yes No Indo-European Slavic-West 
Danish Yes No Indo-European Germanic-North 
Dutch Yes No Indo-European Germanic-West 
English Yes Yes Indo-European Germanic-West 
French Yes Yes Indo-European Italic 
Hebrew Yes No Afro-Asiatic Semitic-Central 
Hungarian Yes No Uralic Finno-Ugric 
Japanese Yes No Altaic  
Latin Yes No Indo-European Italic 
Persian Yes No Indo-European Indo-Iranian 
Russian Yes Yes Indo-European Slavic-East 
Spanish Yes Yes Indo-European Italic 
Table 1. Languages used for training and testing 
2.3 Test method 
We tokenized each of the 570 test documents, ap-
plying the weighting scheme described above to 
obtain a vector of weighted frequencies of each 
term in the document, then multiplying that vector 
by U ? S-1, also as described above. The result was 
a set of projected document vectors in the 300-
dimensional LSA space. 
For some of our experiments, we used a light 
stemmer for Arabic (Darwish 2002) to replace in-
flected forms in the training data with citation 
forms. It is commonly accepted that morphology 
improves IR (Abdou et al 2005, Lavie et al 2004, 
Larkey et al 2002, Oard and Gey 2002), and it will 
be seen that our results generally confirm this. 
For Hebrew, we used the Westminster Lenin-
grad Codex in the training data. Since this is avail-
able for download either with vowels or without 
vowels, no morphological pre-processing was re-
quired in this case; we simply substituted one ver-
sion for the other in the training data when neces-
sary. 
Various measurements are used for evaluating 
IR systems performance (Van Rijsbergen 1979). 
However, since the aim of our experiments is to 
assess whether we could identify the correct trans-
lation for a given document among a set of possi-
bilities in another language (i.e., given the lan-
guage of the query and the language of the results), 
we selected ?precision at 1 document? as our pre-
ferred metric. This metric represents the proportion 
of cases, on average, where the translation was re-
trieved first. 
3 Challenges of Semitic languages 
The features which make Semitic languages chal-
lenging for information retrieval are generally 
fairly well understood: it is probably fair to say 
that chief among them is their complex morphol-
ogy (for example, ambiguity resulting from diacri-
tization, root-and-pattern alternations, and the use 
of infix morphemes as described in Habash 2004). 
These challenges can be illustrated by means of a 
statistical comparison of a portion of our training 
data (the Gospel of Matthew) as shown in Table 2. 
 Types Tokens 
Afrikaans 2,112 24,729 
French 2,840 24,438 
English 2,074 23,503 
Dutch 2,613 23,099 
Danish 2,649 21,816 
Spanish 3,075 21,279 
Persian 3,587 21,190 
Hungarian 4,730 18,787 
Czech 4,236 18,000 
Russian 4,196 16,826 
Latin 3,936 16,543 
Hebrew (Modern) 4,337 14,153 
Arabic 4,607 13,930 
Japanese 5,741 13,130 
Amharic 5,161 12,940 
TOTAL 55,894 284,363 
Table 2. Statistics of parallel texts by language 
From Table 2, it should be clear that there is 
generally an inverse relationship between the num-
ber of types and tokens. Modern Indo-European 
(IE) (and particularly Germanic or Italic lan-
guages) are at one end of the spectrum, while the 
Semitic languages (along with Japanese) are at the 
other. The statistics separate ?analytic? languages 
from ?synthetic? ones, and essentially illustrate the 
fact that, thanks to the richness of their morphol-
ogy, the Semitic languages pack more information 
(in the information-theoretic sense) into each term 
than the other languages. Because this results in 
higher average entropy per word (in the informa-
tion theoretic sense), a challenge is presented to 
information retrieval techniques such as LSA 
which rely on tokenization at word boundaries: it is 
harder to isolate each ?unit? of meaning in a syn-
thetic language. The actual effect this has on in-
formation retrieval precision will be shown in the 
next section. 
4 Results with LSA 
The series of experiments described in this section 
have the aims of: 
? clarifying what effect morphological analysis 
of the training data has on CLIR precision; 
? highlighting the effect on CLIR precision of 
adding more languages in training; 
? illustrating what the impact is of adding a par-
tial translation (text in one language which is 
only partially parallel with the texts in the oth-
er languages) 
We choose Arabic as the language of focus in 
our experiment; specifically for these experiments, 
we intended to reveal the effect of adding lan-
guages from the same group (Semitic) compared 
with that of adding languages of different groups. 
First, we present results in Table 3 which con-
firm that morphological analysis of the training 
data improves CLIR performance. 
 ES RU FR EN AR 
without morphological analysis of Arabic 
ES 1.0000 0.5614 0.8333 0.7368 0.2895 
RU 0.4211 1.0000 0.5263 0.7632 0.2632 
FR 0.7807 0.7018 1.0000 0.8158 0.4035 
EN 0.7193 0.8158 0.8596 1.0000 0.4825 
AR 0.5000 0.2807 0.6228 0.5526 1.0000 
Average precision:  
Overall 0.677, within IE 0.783, IE-Semitic 0.488 
with morphological analysis of Arabic 
ES 1.0000 0.6579 0.8772 0.7807 0.4123 
RU 0.4912 1.0000 0.7193 0.8158 0.3947 
FR 0.8421 0.7719 1.0000 0.8421 0.3772 
EN 0.8070 0.8684 0.8947 1.0000 0.3684 
AR 0.3947 0.3509 0.5614 0.4561 1.0000 
Average precision:  
Overall 0.707, within IE 0.836, IE-Semitic 0.480 
Table 3. Effect of morphological analysis2 
An important point to note first is that CLIR 
precision is generally much lower for pairs includ-
ing Arabic than it is elsewhere, lending support to 
our assertion above that Arabic and other Semitic 
languages present special challenges in informa-
tion retrieval. 
It also emerges from Table 3 that when morpho-
logical analysis of Arabic was added, the overall 
average precisions increased from 0.677 to 0.707, a 
highly significant increase (p? 6.7 ? 10-8). (Here 
and below, a chi-squared test is used to measure 
statistical significance.) 
Given that the ability of morphological analysis 
to improve IR precision has been documented, this 
result in itself is not surprising. However, it is in-
teresting that the net benefit of adding morphologi-
cal analysis ? and just to Arabic within the training 
data ? was more or less confined to pairs of non-
Semitic languages. We believe that the explanation 
is that by adding morphology more relations (liai-
                                                 
2 In this and the following tables, the metric used is pre-
cision at 1 document (discussed in section 2.3). 
sons) are defined in LSA between the words from 
different languages. For language pairs including 
Arabic, the average precision actually decreased 
from 0.488 to 0.480 when morphology was added 
(although this decrease is insignificant). 
With the same five training languages as used in 
Table 3, we added Persian. The results are shown 
in Table 4. 
 ES RU FR EN AR 
ES 1.0000 0.6140 0.8246 0.7632 0.3246 
RU 0.5088 1.0000 0.6667 0.7982 0.2281 
FR 0.8772 0.7368 1.0000 0.8158 0.3947 
EN 0.8246 0.8333 0.8947 1.0000 0.4035 
AR 0.4474 0.4386 0.6140 0.5526 1.0000 
Average precision:  
Overall 0.702, within IE 0.822, IE-Semitic 0.489 
Table 4. Effect on CLIR of adding Persian  
First to note is that the addition of Persian (an IE 
language) led to a general increase in precision for 
pairs of IE languages (Spanish, Russian, French 
and English) from 0.783 to 0.822 but no significant 
change for pairs including Arabic (0.488 to 0.489). 
Although Persian and Arabic share the same script, 
these results confirm that genetic relatedness is a 
much more important factor in affecting precision. 
Chew and Abdelali (2007) show that the results 
of multilingual LSA generally improve as the 
number of parallel translations used in training in-
creases. Our next step here, therefore, is to analyze 
whether it makes any difference whether the addi-
tional languages are from the same or different 
language groups. In Table 5 we compare the re-
sults of adding an IE language (Latin), an Altaic 
language (Japanese), and another Semitic language 
(Hebrew) to the training data. In all three cases, no 
morphological analysis of the training data was 
performed. 
Based on these results, cross-language precision 
yielded only very slightly improved results overall 
by adding Latin or Japanese. With Japanese, the 
net improvement (0.677 to 0.680) was not statisti-
cally significant overall, neither was the change 
significant for pairs either including or excluding 
Arabic (0.488 to 0.485 and 0.783 to 0.789 respec-
tively). Note that this is even though Japanese 
shares some statistical (although of course not lin-
guistic) properties with the Semitic languages, as 
shown in Table 2. With Latin, the net overall im-
provement (0.677 to 0.699) was barely significant 
(p ? 0.01) and was insignificant for pairs including 
Arabic (0.488 to 0.496). With Hebrew, however, 
the net improvement was highly significant in all 
cases (0.677 to 0.718, p ? 3.36 ? 10-6 overall, 
0.783 to 0.819, p ? 2.20 ? 10-4 for non-Semitic 
pairs, and 0.488 to 0.538, p ? 1.45 ? 10-3 for pairs 
including Arabic). We believe that these results 
indicate that there is more value overall in ensuring 
that languages are paired with at least one other 
related language in the training data; our least im-
pressive results (with Japanese) were when two 
languages in training (one Semitic and one Altaic 
language) were ?isolated?. 
 ES RU FR EN AR 
Latin included in training data 
ES 1.0000 0.6140 0.8333 0.7456 0.2544 
RU 0.4737 1.0000 0.6316 0.8246 0.3333 
FR 0.8596 0.7368 1.0000 0.8333 0.4474 
EN 0.7719 0.7982 0.8860 1.0000 0.4474 
AR 0.5088 0.3509 0.6140 0.5088 1.0000 
Average precision:  
Overall 0.699, within IE 0.813, IE-Semitic 0.496 
Japanese included in training data 
ES 1.0000 0.5789 0.8333 0.7456 0.2895 
RU 0.4298 1.0000 0.5526 0.7807 0.2719 
FR 0.7719 0.7368 1.0000 0.8070 0.4035 
EN 0.7193 0.807 0.8596 1.0000 0.4123 
AR 0.5088 0.2982 0.614 0.5702 1.0000 
Average precision:  
Overall 0.680, within IE 0.789, IE-Semitic 0.485 
Modern Hebrew (no vowels) in training data 
ES 1.0000 0.6140 0.8596 0.7807 0.3509 
RU 0.4561 1.0000 0.6667 0.7719 0.3684 
FR 0.8509 0.7193 1.0000 0.8684 0.4298 
EN 0.7632 0.8509 0.9035 1.0000 0.4298 
AR 0.5263 0.4474 0.6491 0.6404 1.0000 
Average precision:  
Overall 0.718, within IE 0.819, IE-Semitic 0.538 
Table 5. Effect of language relatedness on CLIR 
The next set of results are for a repetition of the 
previous three experiments, but this time with 
morphological analysis of the Arabic data. These 
results are shown in Table 6. 
As was the case without the additional lan-
guages, the overall effect of adding morphological 
analysis of Arabic is still to increase precision. In 
all three cases, the net improvement for pairs ex-
cluding Arabic is highly significant (0.813 to 0.844 
with Latin, 0.789 to 0.852 with Japanese, and 
0.819 to 0.850 with Hebrew). For pairs including 
Arabic, however, the change is again insignificant. 
This was a consistent but surprising feature of our 
results, that morphological analysis of Arabic in 
fact appears to benefit non-Semitic languages more 
than it benefits Arabic itself, at least with this data-
set. The results might possibly have been different 
if we had included other Semitic languages in the 
test data, although this appears unlikely as we 
found the same phenomenon consistently occur-
ring across a wide variety of tests, and regardless 
of which languages we used in training. 
 ES RU FR EN AR 
Latin included in training data 
ES 1.0000 0.6579 0.8684 0.7456 0.4211 
RU 0.5614 1.0000 0.7456 0.8509 0.4386 
FR 0.8421 0.8158 1.0000 0.8509 0.4211 
EN 0.8421 0.8333 0.8947 1.0000 0.4123 
AR 0.4123 0.3947 0.5351 0.4825 1.0000 
Average precision:  
Overall 0.721, within IE 0.844, IE-Semitic 0.502 
Japanese included in training data 
ES 1.0000 0.7544 0.8684 0.8070 0.4211 
RU 0.4737 1.0000 0.7193 0.8509 0.4123 
FR 0.8246 0.8596 1.0000 0.8772 0.4211 
EN 0.8421 0.8596 0.8947 1.0000 0.4035 
AR 0.3333 0.3509 0.5614 0.4649 1.0000 
Average precision:  
Overall 0.720, within IE 0.852, IE-Semitic 0.485 
Modern Hebrew (no vowels) in training data 
ES 1.0000 0.7018 0.9035 0.7982 0.4561 
RU 0.5614 1.0000 0.7105 0.8070 0.4035 
FR 0.8421 0.8246 1.0000 0.8596 0.4825 
EN 0.8509 0.8509 0.8947 1.0000 0.4123 
AR 0.3947 0.4298 0.5351 0.5175 1.0000 
Average precision:  
Overall 0.729, within IE 0.850, IE-Semitic 0.514 
Table 6. Effect of language relatedness and 
morphology on CLIR 
For further verification, we explored what would 
happen if only the Arabic root were included in 
morphological analysis. As already mentioned, for 
languages that combine affixes with the stem, there 
is a higher token-to-type ratio. Omitting the affix 
from the morphological analysis of these languages 
reveals the importance of considering the affixes 
and their contribution to the semantics of a given 
sentence. Although LSA is not sentence-structure-
aware (as it uses a bag-of-words approach), the 
importance of considering the affixes as part of the 
sentence is very crucial. The results in Table 7 
demonstrate clearly that ignoring or over-looking 
the word affixes has a negative effect on the over-
all performance of the CLIR system. When includ-
ing only the Arabic stem, a performance degrada-
tion is noticeable across all languages, with a lar-
ger impact on IE languages. The results which il-
lustrate can be seen by comparing Table 7 with 
Table 3. 
 ES RU FR EN AR 
morphological analysis of Arabic ?Stem only- 
ES 1.0000 0.5789 0.8070 0.7807 0.3421 
RU 0.4912 1.0000 0.6842 0.8246 0.1842 
FR 0.8421 0.7018 1.0000 0.8333 0.4211 
EN 0.8333 0.8333 0.9211 1.0000 0.4211 
AR 0.4561 0.4386 0.5702 0.4912 1.0000 
Average precision:  
Overall 0.698, within IE 0.821, IE-Semitic 0.481 
Table 7. Effect of Using Stem only 
Next, we turn specifically to a comparison of the 
effect that different Semitic languages have on 
CLIR precision. Here, we compare the results 
when the sixth language used in training is He-
brew, Amharic, or Aramaic. However, since our 
Amharic and Aramaic training data were only par-
tially parallel (we have only the New Testament in 
Amharic, and only portions of the New Testament 
in Aramaic), we first considered the effect that par-
tial translations have on precision. Table 8 shows 
the results we obtained when only the Hebrew Old 
Testament (with vowels) was used as the sixth par-
allel version. No morphological analysis was per-
formed. 
 ES RU FR EN AR 
without morphological analysis of Arabic 
ES 1.0000 0.6842 0.8421 0.8158 0.3947 
RU 0.4211 1.0000 0.6228 0.7982 0.4737 
FR 0.8509 0.7719 1.0000 0.8509 0.4737 
EN 0.7895 0.8333 0.8684 1.0000 0.4649 
AR 0.4561 0.3333 0.6404 0.4561 1.0000 
Average precision:  
Overall 0.714, within IE 0.822, IE-Semitic 0.521 
with morphological analysis of Arabic 
ES 1.0000 0.7105 0.9035 0.8333 0.4737 
RU 0.4649 1.0000 0.7456 0.8333 0.4912 
FR 0.8421 0.8070 1.0000 0.8860 0.4474 
EN 0.8772 0.8421 0.9298 1.0000 0.4298 
AR 0.2719 0.3684 0.5088 0.5000 1.0000 
Average precision:  
Overall 0.727, within IE 0.855, IE-Semitic 0.499 
 Table 8. Effect of partial translation on CLIR 
Although two or more parameters differ from 
those used for Hebrew in Table 5 (a fully-parallel 
text in modern Hebrew without vowels, versus a 
partial text in Ancient Hebrew with vowels), it is 
worth comparing the two sets of results. In particu-
lar, the reductions in average precision from 0.718 
to 0.714 and from 0.729 to 0.727 respectively are 
insignificant. Likewise, the changes for pairs with 
and without Arabic were insignificant. This ap-
pears to show that, at least up to a certain point, 
even only partially parallel corpora can success-
fully be used under our LSA-based approach. We 
now turn to the results we obtained using Aramaic, 
with the intention of comparing these to our previ-
ous results with Hebrew. 
 ES RU FR EN AR 
no morphological analysis of Arabic 
ES 1.0000 0.4035 0.8070 0.7368 0.2632 
RU 0.3509 1.0000 0.5965 0.6579 0.2281 
FR 0.8421 0.6754 1.0000 0.8246 0.2719 
EN 0.7018 0.6754 0.8947 1.0000 0.2719 
AR 0.4825 0.2807 0.4649 0.3947 1.0000 
Average precision:  
Overall 0.633, within IE 0.760, IE-Semitic 0.406 
morphological analysis of Arabic 
ES 1.0000 0.5351 0.8684 0.7719 0.2895 
RU 0.5175 1.0000 0.6930 0.7807 0.3421 
FR 0.8947 0.7807 1.0000 0.8684 0.2807 
EN 0.8070 0.8158 0.9035 1.0000 0.2982 
AR 0.3509 0.2193 0.3772 0.2895 1.0000 
Average precision:  
Overall 0.667, within IE 0.827, IE-Semitic 0.383 
Table 9. Effect of Aramaic on CLIR 
Here, there is a noticeable across-the-board de-
crease in precision from the previous results. We 
believe that this may have more to do with the fact 
that the Aramaic training data we have is fairly 
sparse (2,957 verses of the Bible out of a total of 
31,226, compared with 23,269 out of 31,226 for 
Ancient Hebrew). It is likely that at some point as 
the parallel translation?s coverage drops (some-
where between the coverage of the Hebrew and the 
Aramaic), there is a severe hit to the performance 
of CLIR. Accordingly, we discarded Aramaic for 
further tests. 
Next, we considered the addition of two Semitic 
languages other than Arabic, Modern Hebrew and 
Amharic, to the training data. In this case, we per-
formed morphological analysis of Arabic. 
The results appear to show a significant increase 
in precision for pairs of IE languages and a signifi-
cant decrease for cross-language-group cases 
(those where an IE language is paired with Ara-
bic), compared to when just Modern Hebrew was 
used in the training data (see the relevant part of 
Table 6). It is not clear why this is the case, but in 
this case we believe that it is quite possible that the 
results would have been different if more than one 
Semitic language had been included in the test 
data. 
 ES RU FR EN AR 
ES 1.0000 0.6930 0.8860 0.7719 0.4649 
RU 0.5000 1.0000 0.7456 0.8684 0.5175 
FR 0.8772 0.7982 1.0000 0.8772 0.4649 
EN 0.8684 0.8596 0.9298 1.0000 0.4386 
AR 0.2632 0.2982 0.4386 0.3947 1.0000 
Average precision:  
Overall 0.718, within IE 0.855, IE-Semitic 0.476 
Table 10. CLIR with 7 languages (including 
Modern Hebrew and Amharic) 
We now come to a rare example where we 
achieved a boost in precision specifically for Ara-
bic. In this case, we repeated the last experiment 
but removed the vowels from the Hebrew text. The 
results are shown in Table 11. 
 ES RU FR EN AR 
ES 1.0000 0.7018 0.8772 0.8158 0.5088 
RU 0.5175 1.0000 0.7632 0.8421 0.4825 
FR 0.8596 0.8246 1.0000 0.8860 0.5351 
EN 0.8947 0.8158 0.9298 1.0000 0.5088 
AR 0.2895 0.3772 0.5526 0.5000 1.0000 
Average precision:  
Overall 0.739, within IE 0.858, IE-Semitic 0.528 
Table 11. Effect of removing Hebrew vowels 
Average precision for pairs including Arabic in-
creased from 0.476 to 0.528, an increase which 
was significant (p ? 7.33 ? 10-4), but for other pairs 
the change was insignificant. Since the Arabic text 
in training did not include vowels, we believe that 
the exclusion of vowels from Hebrew placed the 
two languages on a more common footing, allow-
ing LSA, for example, to make associations be-
tween Hebrew and Arabic roots which otherwise 
might not have been made. Although Hebrew and 
Arabic do not always share common stems, it can 
be seen from Table 2 that the type/token statistics 
of Hebrew (without vowels) and Arabic are very 
similar. The inclusion of Hebrew vowels would 
change the statistics for Hebrew considerably, in-
creasing the number of types (since previously in-
distinguishable wordforms would now be listed 
separately). Thus, with the exclusion of Hebrew 
vowels, there should be more instances where Ara-
bic tokens can be paired one-to-one with Hebrew 
tokens. 
Finally, in order to confirm our conclusions and 
to eliminate any doubts about the results obtained 
so far, we experimented with more languages. We 
added Japanese, Afrikaans, Czech, Danish, Dutch, 
Hungarian and Hebrew in addition to our 5 original 
languages. Morphological analysis of the Arabic 
text in training was performed, as in some of the 
previous experiments. The results of these tests are 
shown in Table 12. 
 ES RU FR EN AR 
11 languages (original 5 + Japanese, Afrikaans, 
Czech, Danish, Dutch, and Hungarian) 
ES 1.0000 0.6754 0.9035 0.7719 0.5526 
RU 0.4737 1.0000 0.7632 0.8772 0.5175 
FR 0.8596 0.8070 1.0000 0.8947 0.5088 
EN 0.8421 0.8684 0.9035 1.0000 0.4912 
AR 0.3772 0.2632 0.6316 0.4912 1.0000 
Average precision:  
Overall 0.739, within IE 0.853, IE-Semitic 0.537 
12 languages (as above plus Hebrew) 
ES 1.0000 0.7018 0.8947 0.7719 0.6404 
RU 0.6667 1.0000 0.7105 0.9123 0.6228 
FR 0.8772 0.8333 1.0000 0.8421 0.6404 
EN 0.6667 0.8684 0.9035 1.0000 0.6316 
AR 0.5877 0.4386 0.5965 0.6491 1.0000 
Average precision:  
Overall 0.778, within IE 0.853, IE-Semitic 0.645 
Table 12. Effect of further languages on CLIR 
Generally, these results confirm the finding of 
Chew and Abdelali (2007) about adding more lan-
guages; doing so enhances the ability to identify 
translations across language boundaries. Across the 
board (for Arabic and other languages), the in-
crease in precision gained by adding Afrikaans, 
Czech, Danish, Dutch and Hungarian is highly sig-
nificant (compared to the part of Table 5 which 
deals with Japanese, overall average precision in-
creased from 0.680 to 0.739, with p ? 1.17 ? 10-11; 
for cross-language-group retrieval, from 0.485 to 
0.537, with p ? 9.31 ? 10-4; for pairs within IE, 
from 0.789 to 0.853 with p ? 2.81 ? 10-11). In con-
trast with most previous results, however, with the 
further addition of Hebrew, precision was boosted 
primarily for Arabic (0.537 to 0.645 with p ? 4.39 
? 10-13). From this and previous results, it appears 
that there is no clear pattern to when the addition 
of a Semitic language in training was beneficial to 
the Semitic language in testing. 
5 Conclusion and future work 
Based on our results, it appears that although 
clear genetic relationships exist between certain 
languages in our training data, it was less possible 
than we had anticipated to leverage this to our ad-
vantage. We had expected, for example, that by 
including multiple Semitic languages in the train-
ing data within an LSA framework, we would have 
been able to improve cross-language information 
retrieval results specifically for Arabic. Perhaps 
surprisingly, the greatest benefit of including addi-
tional Semitic languages in the training data is 
most consistently to non-Semitic languages. A 
clear observation is that any additional languages 
in training are generally beneficial, and the benefit 
of additional languages can be considerably greater 
than the benefits of linguistic pre-processing (such 
as morphological analysis). Secondly, it is not nec-
essarily the case that cross-language retrieval with 
Arabic is helped most by including other Semitic 
languages, despite the genetic relationship. Finally, 
as we expected, we were able to rule out script 
similarity (e.g. between Persian and Arabic) as a 
factor which might improve precision. Our results 
appear to demonstrate clearly that language relat-
edness is much more important in the training data 
than use of the same script. 
Finally, to improve cross-language retrieval with 
Arabic ? the most difficult case in the languages 
we tested ? we attempted to ?prime? the training 
data by including Arabic morphological analysis. 
This did lead to a statistically significant improve-
ment overall in CLIR, but ? perhaps paradoxically 
? the improvement specifically for cross-language 
retrieval with Arabic was negligible in most cases. 
The only two measures which were successful in 
boosting precision for Arabic significantly were (1) 
the inclusion of Modern Hebrew in the training 
data; and (2) the elimination of vowels in the An-
cient Hebrew training data ? both measures which 
would have placed the training data for the two 
Semitic languages (Arabic and Hebrew) on a more 
common statistical footing. These results appear to 
confirm our hypothesis that there is value, within 
the current framework, of ?pairing? genetically re-
lated languages in the training data. In short, lan-
guage relatedness does matter in cross-language 
information retrieval. 
6 Acknowledgement 
Sandia is a multiprogram laboratory operated by 
Sandia Corporation, a Lockheed Martin Company, 
for the United States Department of Energy?s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000. 
7 References 
Abdou, S., Ruck, P., and Savoy, J. 2005. Evaluation of 
Stemming, Query Expansion and Manual Indexing 
Approaches for the Genomic Task. In Proceedings of 
TREC 2005. 
Berry, M. W., Dumais, S. T., and O?Brien, G. W. 1994. 
Using Linear Algebra for Intelligent Information Re-
trieval. SIAM: Review, 37, 573-595. 
Biola University. 2005-2006. The Unbound Bible. Ac-
cessed at http://www.unboundbible.com/ on February 
27, 2007. 
Chew, P. A., and Abdelali, A. 2007. Benefits of the 
?Massively Parallel Rosetta Stone?: Cross-Language 
Information Retrieval with over 30 Languages, Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2007. Pra-
gue, Czech Republic, June 23?30, 2007. pp. 872-879. 
Chew, P. A., Kegelmeyer, W. P., Bader, B. W. and Ab-
delali, A. Forthcoming. The Knowledge of Good and 
Evil: Multilingual Ideology Classification with 
PARAFAC2 and Maching Learning. 
Chew, P. A., Verzi, S. J., Bauer, T. L., and McClain, J. 
T. 2006. Evaluation of the Bible as a Resource for 
Cross-Language Information Retrieval. Proceedings 
of the Workshop on Multilingual Language Re-
sources and Interoperability, 68?74. 
Darwish, K. 2002. Building a shallow Arabic morpho-
logical analyzer in one day. In Proceedings of the 
Association for Computational Linguistics (ACL-02), 
40th Anniversary Meeting. pp. 47-54. 
Dumais, S. T. 1991. Improving the Retrieval of Infor-
mation from External Sources. Behavior Research 
Methods, Instruments, and Computers 23 (2), 229-
236. 
Dumais, S. T., Furnas, G. W., Landauer, T. K., Deer-
wester, S. and Harshman, R. 1998. Using Latent Se-
mantic Analysis to Improve Access to Textual In-
formation. In CHI?88: Proceedings of the SIGCHI 
Conference on Human Factors in Computing Sys-
tems, 281-285. ACM Press. 
Frakes, W. B. and Baeza-Yates, R. 1992. Information 
Retrieval: Data Structures and Algorithms. Prentice-
Hall: New Jersey. 
Habash, N. 2004. Large Scale Lexeme Based Arabic 
Morphological Generation. In Proc. of Traitement 
Automatique du Langage Naturel. 
Larkey, L., Ballesteros, L. and Connell, M. 2002. Im-
proving Stemming for Arabic Information Retrieval: 
Light Stemming and Co-Occurrence Analysis. SIGIR 
2002, Finland, pp. 275-282. 
Larkey, L. and Connell, M. 2002. Arabic Information 
Retrieval at Umass in TREC-10. In Voorhees, E.M. 
and Harman, D.K. (eds.): The Tenth Text Retrieval 
Conference, TREC 2001 NIST Special Publication 
500-250, pp. 562-570. 
Lavie, A., Peterson, E., Probst, K., Wintner, S., and Ey-
tani, Y. 2004. Rapid Prototyping of a Transfer-Based 
Hebrew-to-English Machine Translation System. In 
Proceedings of the TMI-04. 
Mathieu, B., Besan?on, R. and Fluhr, C. 2004. Multilin-
gual Document Clusters Discovery. Recherche 
d?Information Assist?e par Ordinateur (RIAO) Pro-
ceedings, 1-10. 
Oard, D. and Gey, F. 2002. The TREC 2002 Ara-
bic/English CLIR Track, NIST TREC 2002 Proceed-
ings, pp. 16-26. 
Peters, C. (ed.). 2001. Cross-Language Information 
Retrieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000. Berlin: 
Springer-Verlag. 
Resnik, P., Olsen, M. B., and Diab, M. 1999. The Bible 
as a Parallel Corpus: Annotating the "Book of 2000 
Tongues". Computers and the Humanities, 33, 129-
153. 
Van Rijsbergen, C. 1979. Information Retrieval (2nd 
edition). Butterworth: London. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 872?879,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Benefits of the ?Massively Parallel Rosetta Stone?:  
Cross-Language Information Retrieval with over 30 Languages 
Peter A. Chew 
Sandia National Laboratories 
P. O. Box 5800, MS 1012 
Albuquerque, NM 87185-1012, USA 
pchew@sandia.gov
Ahmed Abdelali 
New Mexico State University 
P.O. Box 30002, Mail Stop 3CRL 
Las Cruces, NM 88003-8001, USA 
ahmed@crl.nmsu.edu
Abstract 
In this paper, we describe our experiences 
in extending a standard cross-language in-
formation retrieval (CLIR) approach 
which uses parallel aligned corpora and 
Latent Semantic Indexing. Most, if not 
all, previous work which follows this ap-
proach has focused on bilingual retrieval; 
two examples involve the use of French-
English or English-Greek parallel cor-
pora. Our extension to the approach is 
?massively parallel? in two senses, one 
linguistic and the other computational. 
First, we make use of a parallel aligned 
corpus consisting of almost 50 parallel 
translations in over 30 distinct languages, 
each in over 30,000 documents. Given the 
size of this dataset, a ?massively parallel? 
approach was also necessitated in the 
more usual computational sense. Our re-
sults indicate that, far from adding more 
noise, more linguistic parallelism is better 
when it comes to cross-language retrieval 
precision, in addition to the self-evident 
benefit that CLIR can be performed on 
more languages. 
1 Introduction 
Approaches to cross-language information retrieval 
(CLIR) fall generally into one of two types, or 
some combination thereof: the ?query translation? 
approach or the ?parallel corpus? approach. The 
first of these, which is perhaps more common, in-
volves translation of the query into the target lan-
guage, for example using machine translation or 
on-line dictionaries.  The second makes use of par-
allel aligned corpora as training sets. One approach 
which uses parallel corpora does this in conjunc-
tion with Latent Semantic Indexing (LSI) (Lan-
dauer and Littman 1990, Young 1994). According 
to Berry et al (1994:21), the use of LSI with paral-
lel corpora can be just as effective as the query 
translation approach, and avoids some of the draw-
backs of the latter, discussed in Nie et al (1999). 
Generally, research in CLIR has not attempted 
to use very many languages at a time (see for ex-
ample Nie and Jin 2002). With query translation 
(although that is not the approach that Nie and Jin 
take), this is perhaps understandable, as for each 
new language, a new translation algorithm must be 
included. The effort involved in extending query 
translation to multiple languages, therefore, is 
likely to be in proportion to the number of lan-
guages. 
With parallel corpora, the reason that research 
has been limited to only a few languages at a time 
? and usually just two at a time, as in the LSI work 
cited above ? is more likely to be rooted in the 
widespread perception that good parallel corpora 
are difficult to obtain (see for example Asker 
2004). However, recent work (Resnik et al 1999, 
Chew et al 2006) has challenged this idea. 
One advantage of a ?massively parallel? multi-
lingual corpus is perhaps self-evident: within the 
LSI framework, the more languages are mapped 
into the single conceptual space, the fewer restric-
tions there are on which languages documents can 
be selected from for cross-language retrieval. 
However, several questions were raised for us as 
872
we contemplated the use of a massively parallel 
corpus. Would the addition of languages not used 
in testing create ?noise? for a given language pair, 
reducing the precision of CLIR? Could partially 
parallel corpora be used? Our work appears to 
show both that more languages are generally bene-
ficial, and even incomplete parallel corpora can be 
used. In the remainder of this paper, we provide 
evidence for this claim. The paper is organized as 
follows: section 2 describes the work we undertook 
to build the parallel corpus and its characteristics. 
In section 3, we outline the mechanics behind the 
'Rosetta-Stone' type method we use for cross-
language comparison. In section 4, we present and 
discuss the results of the various tests we per-
formed. Finally, we conclude on our findings in 
section 5. 
2 The massively parallel corpus  
Following Chew et al (2006), our parallel corpus 
was built up from translations of the Bible which 
are freely available on the World Wide Web. Al-
though reliable comparable statistics are hard to 
find, it appears to be generally agreed that the Bi-
ble is the world?s most widely translated book, 
with complete translations in 426 languages and 
partial translations in 2,403 as of December 31, 
2005 (Bible Society, 2006). Great care is taken 
over the translations, and they are alignable by 
chapter and verse. According to Resnik et al 
(1999), the Bible?s coverage of modern vocabulary 
may be as high as 85%. The vast majority of the 
translations we used came from the ?Unbound Bi-
ble? website (Biola University, 2005-2006); from 
this website, the text of a large number of different 
translations of the Bible can ? most importantly for 
our purposes ? be downloaded in a tab-delimited 
format convenient for loading into a database and 
then indexing by chapter and verse in order to en-
sure ?parallelism? in the corpus. The number of 
translations available at the website is apparently 
being added to, based on our observations access-
ing the website on a number of different occasions. 
The languages we have included in our multilin-
gual parallel corpus include those both ancient and 
modern, and are as follows: 
Language No. of 
translations 
Used in 
tests 
Afrikaans 1 12+ 
Albanian 1 27+ 
Arabic 1 All 
Chinese (Simplified) 1 44+ 
Chinese (Traditional) 1 44+ 
Croatian 1 27+ 
Czech 2 12+ 
Danish 1 12+ 
Dutch 1 12+ 
English 7 All 
Finnish 3 27+ 
French 2 All 
German 4 8,27+ 
Greek (New Testament) 2 46+ 
Hebrew (Old Testament) 1 46+ 
Hebrew (Modern) 1 6,12+ 
Hungarian 1 6+ 
Italian 2 8,27+ 
Japanese* 1 9+ 
Korean 1 27+ 
Latin 1 8,9,28+ 
Maori 1 7,8,9,27+ 
Norwegian 1 27+ 
Polish* 1 27+ 
Portuguese 1 27+ 
Russian 1 All 
Spanish 2 All 
Swedish 1 27+ 
Tagalog 1 27+ 
Thai 1 27+ 
Vietnamese 1 27,44+ 
Table 1. Languages1
The languages above represent many of the ma-
jor language groups: Austronesian (Maori and 
Tagalog); Altaic (Japanese and Korean); Sino-
Tibetan (Chinese); Semitic (Arabic and Hebrew);  
Finno-Ugric (Finnish and Hungarian); Austro-
Asiatic (Vietnamese); Tai-Kadai (Thai); and Indo-
European (the remaining languages). The two New 
Testament Greek versions are the Byzan-
tine/Majority Text (2000), and the parsed version 
of the same text, in which we treated distinct mor-
phological elements (such as roots or inflectional 
endings) as distinct terms. Overall, the list includes 
 
1 Translations in languages marked with an asterisk above 
were obtained from websites other than the ?Unbound Bible? 
website. ?Used in tests? indicates in which tests in Table 2 
below the language was used as training data, and hence the 
order of addition of languages to the training data. 
873
47 versions in 31 distinct languages (assuming 
without further discussion here that each entry in 
the list represents a distinct language). 
We aligned the translations by verse, and, since 
there are some differences in versification between 
translations (for example, the Hebrew Old Testa-
ment includes the headings for the Psalms as sepa-
rate verses, unlike most translations), we spent 
some time cleaning the data to ensure the align-
ment was as good as possible, given available re-
sources and our knowledge of the languages. (Even 
after this process, the alignment was not perfect, 
and differences in how well the various transla-
tions were aligned may account for some of the 
variability in the outcome of our experiments, de-
pending on which translations were used.) The end 
result was that our parallel corpus consisted of 
31,226 ?mini-documents? ? the total number of text 
chunks2 after the cleaning process, aligned across 
all 47 versions. The two New Testament Greek 
versions, and the one Old Testament Hebrew ver-
sion, were exceptions because these are only par-
tially complete; the former have text in only 7,953 
of the verses, and the latter has text in 23,266 of 
the verses. For some versions, a few of the verse 
translations are incomplete where a particular verse 
has been skipped in translation; this also explains 
the fact that the number of Hebrew and Greek text 
chunks together do not add up to 31,226. However, 
the number of such verses is negligible in compari-
son to the total. 
3 Framework 
The framework we used was the standard LSI 
framework described in Berry et al (1994). Each 
aligned mini-document from the parallel corpus 
consists of the combination of text from all the 31 
languages. A document-by-term matrix is formed 
in which each cell represents a weighted frequency 
of a particular term t in a particular document k.
We used a standard log-entropy weighting scheme, 
where the weighted frequency W is given by: 
 
W = log2 (F) ? (1 + Ht / log2 (N)) 
 
where F is the raw frequency of t in k, Ht is the 
standard ?p log p? measure of the entropy of the 
term across all documents, and N is the number of 
 
2 The text chunks generally had the same boundaries as the 
verses in the original text. 
documents in the corpus. The last term in the ex-
pression above, log2 (N), is the maximum entropy 
that any term can have in the corpus, and therefore 
(1 + Ht / log2 (N)) is 1 for the most distinctive 
terms in the corpus, 0 for those which are least dis-
tinctive. 
The sparse document-by-term matrix is sub-
jected to singular value decomposition (SVD), and 
a reduced non-sparse matrix is output. Generally, 
we used the output corresponding to the top 300 
singular values in our experiments. When we had a 
smaller number of languages in the mix, it was 
possible to use SVDPACK (Berry et al 1996), 
which is an open-source non-parallel algorithm for 
computing the SVD, but for larger problems (in-
volving more than a couple of dozen parallel ver-
sions), use of a parallel algorithm (in a library 
called Trilinos) was necessitated. (This was run on 
a Linux cluster consisting of 4,096 dual CPU com-
pute nodes, running on Dell PowerEdge 1850 1U 
Servers with 6GB of RAM.) 
In order to test the precision versus recall of our 
framework, we used translations of the 114 suras 
of the Qu?ran into five languages, Arabic, English, 
French, Russian and Spanish. The number of 
documents used for testing is fairly small, but large 
enough to give comparative results for our pur-
poses which are still highly statistically significant. 
The test set was split into each of the 10 possible 
language-pair combinations: Arabic-English, Ara-
bic-French, English-French, and so on. 
For each language pair and test, 228 distinct 
?queries? were submitted ? each query consisting 
of one of the 228 sura ?documents?. If the highest-
ranking document in the other language of the pair 
was in fact the query?s translation, then the result 
was deemed ?correct?. To assess the aggregate per-
formance of the framework, we used two meas-
ures: average precision at 0 (the maximum 
precision at any level of recall), and average preci-
sion at 1 document (1 if the ?correct? document 
ranked highest, zero otherwise). The second meas-
ure is a stricter one, but we generally found that 
there is a high rate of correlation between the two 
measures anyway. 
4 Results and Discussion 
The following tables show the results of our tests. 
First, we present in Table 2 the overall summary, 
874
with averages across all language pairs used in 
testing. 
 
Average precision No. of 
parallel 
versions
At 0 at 1 doc. 
2 0.706064 0.571491 
3 0.747620 0.649269 
4 0.617615 0.531873 
5 0.744951 0.656140 
6 0.811666 0.732602 
7 0.827246 0.753070 
8 0.824501 0.750000 
9 0.823430 0.746053 
12 0.827761 0.752632 
27 0.825577 0.751316 
28 0.823137 0.747807 
44 0.839346 0.765789 
46 0.839319 0.766667 
47 0.842936 0.774561 
Table 2. Summary results for all language pairs 
 
From the above, the following should be clear: 
as more parallel translations are added to the index, 
the average precision rises considerably at first, 
and then begins to level off after about the seventh 
parallel translation. The results will of course vary 
according to which combination of translations is 
selected for the index. The number of such combi-
nations is generally very large: for example, with 
47 translations available, there are 47! / (40! 7!), or 
62,891,499, possible ways of selecting 7 transla-
tions. Thus, for any particular number of parallel 
versions, we had to use some judgement in which 
parallel versions to select, since there was no way 
to achieve anything like exhaustive coverage of the 
possibilities. 
Further, with more than 7 parallel translations, 
there is certainly no justification for saying that 
adding more translations or languages increases the 
?noise? for languages in the test set, since beyond 7 
the average precision remains fairly level. If any-
thing, in fact, the precision still appears to rise 
slightly. For example, the average precision at 1 
document rises by more than 0.75 percentage 
points between 46 and 47 versions. Given that in 
each of these experiments, we are measuring preci-
sion 228 times per language pair, and therefore 
2,280 times in total, this small rise in precision is 
significant (p ? 0.034). Interestingly, the 47th ver-
sion to be added was parsed New Testament 
Greek. It appears, therefore, that the parsing helped 
in particular; we also have evidence from other 
experiments (not presented here) that overall preci-
sion is generally improved for all languages when 
Arabic wordforms are replaced by their respective 
citation forms (the bare root, or stem) ? also a form 
of morphological parsing. Ancient Greek, like 
Arabic, is morphologically highly complex, so it 
would be understandable that parsing (or stem-
ming) would help when parsing of either language 
is used in training. 
One other point needs to be made here: the three 
versions added after the 44th version were the three 
incomplete versions (the two Greek versions cover 
just the New Testament, while Ancient Hebrew 
covers just the Old Testament). The above-
mentioned increase in precision which resulted 
from the addition of these three versions is clear 
evidence that even in the case where a parallel cor-
pus is defective for some language(s), including 
those languages can still result in the twofold bene-
fit that (1) those languages are now available for 
analysis, and (2) precision is maintained or in-
creased for the remaining languages. 
Finally, precision at 1 document, the stricter of 
the two measures, is by definition less than or 
equal to precision at 0. This taken into account, it 
is also interesting that the gap between the two 
measures seems to narrow as more parallel transla-
tions and parsing are added, as Figure 1 shows. 
For certain applications where it is important 
that the translation is ranked first, not just highly, 
among all retrieved documents, there is thus a par-
ticular benefit in using a ?massively parallel? 
aligned corpus. 
0%
2%
4%
6%
8%
10%
12%
14%
16%
2 3 4 5 6 7 8 9 12 27 28 44 46 47
Number of parallel translations
Di
ffe
re
nt
ial
in
pe
rc
en
tag
ep
oin
ts
Figure 1. Differential between precision at 0 and 
precision at 1 document, by number of languages 
875
Now we move on to look at more detailed re-
sults by language pair. Figure 2 below breaks 
down the results for precision at 1 document by 
language pair. In all tests, the two languages in 
each pair were (naturally) always included in the 
languages used for training. There is more volatil-
ity in the results by language pair than there is in 
the overall results, shown again at the right of the 
graph, which should come as no surprise since the 
averages are based on samples a tenth of the size. 
Generally, however, the pattern is the same for 
particular language pairs as it is overall; the more 
parallel versions are used in training, the better the 
average precision. 
There are some more detailed observations 
which should also be made from Figure 2. First, 
the average precision clearly varies quite widely 
between language pairs. The language pairs with 
the best average precision are those in which two 
of English, French and Spanish are present. Of the  
five languages used for testing, these three cluster 
together genetically, since all three are Western 
(Germanic or Romance) Indo-European languages. 
Moreover, these are the three languages of the five 
which are written in the Roman alphabet. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Spanish-
Arabic
Arabic-
French
Russian-
Arabic
English-
Arabic
Spanish-
Russian
Russian-
French
English-
Russian
English-
Spanish
Spanish-
French
English-
French
OVERALL
Language Pair
Pr
ec
isi
on
at
1d
oc
um
en
t
2 3 4 5 6 7 8 9 12 27 28 44 46 47Number of languages  
Figure 2. Chart of precision at 1 doc. by language pair and number of parallel training versions 
 
However, we believe the explanation for the 
poorer results for language pairs involving either 
Arabic, Russian, or both, can be pinned down to 
something more specific. We have already par-
tially alluded to the obvious difference between 
Arabic and Russian on the one hand, and English, 
French and Spanish on the other: that Arabic and 
Russian are highly morphologically rich, while 
English, French and Spanish are generally analytic 
languages. This has a clear effect on the statistics 
for the languages in question, as can be seen in 
Table 3, which is based on selected translations of 
the Bible for each of the languages in question. 
 
Translation Types Tokens 
English (King James) 12,335 789,744 
Spanish (Reina Valera 1909) 28,456 704,004 
Russian (Synodal 1876) 47,226 560,524 
Arabic (Smith Van Dyke) 55,300 440,435 
French (Darby) 20,428 812,947 
Table 3. Statistics for Bible translations in 5 lan-
guages used in test data 
876
Assuming that the respective translations are 
faithful (and we have no reason to believe other-
wise), and based on the statistics in Table 3, it 
should be the case that Arabic contains the most 
?information? per term (in the information theoretic 
sense), followed by Russian, Spanish, English and 
French.3 Again, this corresponds to our intuition 
that much information is contained in Arabic pat-
terns and Russian inflectional morphemes, which 
in English, French and Spanish would be contained 
in separate terms (for example, prepositions). 
Without additional pre-processing, however, 
LSI cannot deal adequately with root-pattern or 
inflectional morphology. Moreover, it is clearly a 
weakness of LSI, or at least the standard log-
entropy weighting scheme as applied within this 
framework, that it makes no adjustment for differ-
ences in information content per word between 
languages. Even though we can assume near-
equivalency of information content between the 
different translations above, according to the stan-
dard log-entropy weighting scheme there are large 
differences between the total entropy of particular 
parallel documents; in general, languages such as 
English are overweighted while those such as Ara-
bic are underweighted. 
Now that this issue is in perspective, we should 
draw attention to another detail in Figure 2. Note 
that the language pairs which benefited most from 
the addition of Ancient Greek and Hebrew into the 
training data were those which included Russian, 
and Russian-Arabic saw the greatest increase in 
precision. Recall also that the 47th version to be 
added was the parsed Greek, so that essentially 
each Greek morpheme is represented by a distinct 
term. From Figure 2, it seems clear that the inclu-
sion of parsed Greek in particular boosted the pre-
cision for Russian (this is most visible at the right-
hand side of the set of columns for Russian-Arabic 
and English-Russian). There are, after all, notable 
similarities between modern Russian and Ancient 
Greek morphology (for example, the nominal case 
system). Essentially, the parsed Greek acts as a 
?clue? to LSI in associating inflected forms in Rus-
 
3 To clarify the meaning of ?term? here: for all languages ex-
cept Chinese, text is tokenized in our framework into terms 
using regular expressions; each non-word character (such as 
punctuation or white space) is assumed to mark the boundary 
of a word. For Chinese, we made the simplifying assumption 
that each character represented a separate term. 
sian with preposition/non-inflected combinations 
in other languages. These results seem to be further 
confirmation of the notion that parsing just one of 
the languages in the mix helps overall; the greatest 
boost is for those languages with morphology re-
lated to that of the parsed language, but there is at 
least a maintenance, and perhaps a small boost, in 
the precision for unrelated languages too. 
Finally, we turn to look at some effects of the 
particular languages selected for training. Included 
in the results above, there were three separate tests 
run in which there were 6 training versions. In all 
three, Arabic, English, French, Russian and Span-
ish were included. The only factor we varied in the 
three tests was the sixth version. In the three tests, 
we used Modern Hebrew (a Semitic language, 
along with Arabic), Hungarian (a Uralic language, 
not closely related to any of the other five lan-
guages), and a second English version respectively. 
The results of these tests are shown in Figure 3, 
with figures for the test in which only 5 versions 
were included for comparative purposes. 
From these results, it is apparent first of all that 
it was generally beneficial to add a sixth version, 
regardless of whether the version added was Eng-
lish, Hebrew or Hungarian. This is consistent with 
the results reported elsewhere in this paper. Sec-
ond, it is also apparent that the greatest benefit 
overall was had by using an additional English ver-
sion, rather than using Hebrew or Hungarian. 
Moreover, perhaps surprisingly, the use of Hebrew 
in training ? even though Hebrew is related to 
Arabic ? was of less benefit to Arabic than either 
Hungarian or an additional English version. It ap-
pears that the use of multiple versions in the same 
language is beneficial because it enables LSI to 
make use of the many different instantiations in the 
expression of a concept in a single language, and 
that this effect can be greater than the effect which 
obtains from using heterogeneous languages, even 
if there is a genetic relationship to existing lan-
guages. 
877
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Russian-
Arabic
Spanish-
Arabic
Arabic-
French
English-
Arabic
Spanish-
Russian
Russian-
French
English-
Russian
English-
Spanish
Spanish-
French
English-
French
OVERALL
Language pair
Pr
ec
isi
on
at
1d
oc
.
Arabic, English, French, Russian, Spanish Arabic, English, French, Hebrew, Russian, Spanish
Arabic, English, French, Hungarian, Russian, Spanish Arabic, English x 2, French, Russian, Spanish
 
Figure 3. Precision at 1 document for 6 training versions, with results of using different mixes of lan-
guages for training 
 
Figure 3 may also shed some additional light on 
one other detail from Figure 2: a perceptible jump 
in precision between 28 and 44 training versions 
for Arabic-English and Arabic-French. It should be 
mentioned that among the 16 additional versions 
were five English versions (American Standard 
Version, Basic English Bible, Darby, Webster?s 
Bible, and Young?s Literal Translation), and one 
French version (Louis Segond 1910). It seems that 
Figure 2 and Figure 3 both point to the same thing: 
that the use of parallel versions or translations in a 
single language can be particularly beneficial to 
overall precision within the LSI framework ? even 
to a greater extent than the use of parallel transla-
tions in different languages. 
5 Conclusion 
In this paper, we have shown how ?massive paral-
lelism? in an aligned corpus can be used to im-
prove the results of cross-language information 
retrieval. Apart from the obvious advantage (the 
ability to automate the processing of a greater vari-
ety of linguistic data within a single framework), 
we have shown that including more parallel trans-
lations in training improves the precision of CLIR 
across the board. This is true whether the addi-
tional translations are in the language of another 
translation already within the training set, whether 
they are in a related language, or whether they are 
in an unrelated language; although this is not to say 
that these choices do not lead to (generally minor) 
variations in the results. The improvement in preci-
sion also appears to hold whether the additional 
translations are complete or incomplete, and it ap-
pears that morphological pre-processing helps, not 
just for the languages pre-processed, but again 
across the board. 
Our work also offers further evidence that the 
supply of useful pre-existing parallel corpora is not 
perhaps as scarce as it is sometimes claimed to be. 
Compilation of the 47-version parallel corpus we 
used was not very time-consuming, especially if 
the time taken to clean the data is not taken into 
878
account, and all the textual material we used is 
publicly available on the World Wide Web. 
While the experiments we performed were on 
non-standard test collections (primarily because 
the Qu?ran was easy to obtain in multiple lan-
guages), it seems that there is no reason to believe 
our general observation ? that more parallelism in 
the training data is beneficial for cross-language 
retrieval ? would not hold for text from other do-
mains. Whether the genre of text used as training 
data affects the absolute rate of retrieval precision 
for text of a different genre (e.g. news articles, 
shopping websites) is a separate question, and one 
we intend to address more fully in future work. 
In summary, it appears that we are able to 
achieve the results we do partly because of the in-
herent properties of LSI. In essence, when the data 
from more and more parallel translations are sub-
jected to SVD, the LSI ?concepts? become more 
and more reinforced. The resulting trend for preci-
sion to increase, despite ?blips? for individual lan-
guages, can be seen for all languages. To put it in 
more prosaic terms, the more different ways the 
same things are said in, the more understandable 
they become ? including in cross-language infor-
mation retrieval. 
Acknowledgement 
Sandia is a multiprogram laboratory operated by 
Sandia Corporation, a Lockheed Martin Company, 
for the United States Department of Energy?s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000. 
References  
Lars Asker. 2004. Building Resources: Experiences 
from Amharic Cross Language Information Re-
trieval. Paper presented at Cross-Language Informa-
tion Retrieval and Evaluation: Workshop of the 
Cross-Language Evaluation Forum, CLEF 2004.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval. New York: ACM 
Press. 
Michael Berry, Theresa Do, Gavin O?Brien, Vijay 
Krishna, and Sowmimi Varadhan. 1996. 
SVDPACKC (Version 1.0) User?s Guide. Knoxville, 
TN: University of Tennessee. 
Bible Society. 2006. A Statistical Summary of Lan-
guages with the Scriptures. Accessed at 
http://www.biblesociety.org/latestnews/latest341-
slr2005stats.html on Jan. 5, 2007. 
Biola University. 2005-2006. The Unbound Bible. Ac-
cessed at http://www.unboundbible.com/ on Jan. 5, 
2007. 
Peter Chew, Stephen Verzi, Travis Bauer and Jonathan 
McClain. 2006. Evaluation of the Bible as a Re-
source for Cross-Language Information Retrieval. In 
Proceedings of the Workshop on Multilingual Lan-
guage Resources and Interoperability, 68-74. Syd-
ney: Association for Computational Linguistics. 
Susan Dumais. 1991. Improving the Retrieval of Infor-
mation from External Sources. Behavior Research 
Methods, Instruments, and Computers 23(2):229-
236. 
Julio Gonzalo. 2001. Language Resources in Cross-
Language Text Retrieval: a CLEF Perspective. In 
Carol Peters (ed.). Cross-Language Information Re-
trieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000: 36-47. 
Berlin: Springer-Verlag.  
Dragos Munteanu and Daniel Marcu. 2006. Improving 
Machine Translation Performance by Exploiting 
Non-Parallel Corpora. Computational Linguistics 
31(4):477-504. 
Jian-Yun Nie and Fuman Jin. 2002. A Multilingual Ap-
proach to Multilingual Information Retrieval. Pro-
ceedings of the Cross-Language Evaluation Forum,
101-110. Berlin: Springer-Verlag. 
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Rich-
ard Durand. 1999. Cross-Language Retrieval based 
on Parallel Texts and Automatic Mining of Parallel 
Texts from the Web. Proceedings of the 22nd Annual 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, 74-81, 
August 15-19, 1999, Berkeley, CA.  
Carol Peters (ed.). 2001. Cross-Language Information 
Retrieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000. Berlin: 
Springer-Verlag. 
Recherche appliqu?e en linguistique informatique 
(RALI). 2006. Corpus align? bilingue anglais-
fran?ais. Accessed at http://rali.iro.umontreal.ca/ on 
February 22, 2006. 
Philip Resnik, Mari Broman Olsen, and Mona Diab. 
1999. The Bible as a Parallel Corpus: Annotating the 
"Book of 2000 Tongues". Computers and the Hu-
manities, 33: 129-153.  
879
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 68?74,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluation of the Bible as a Resource
for Cross-Language Information Retrieval
Peter A. Chew Steve J. Verzi Travis L. Bauer Jonathan T. McClain
Sandia National Laboratories
P. O. Box 5800
Albuquerque, NM 87185, USA
{pchew,sjverzi,tlbauer,jtmccl}@sandia.gov
Abstract
An area of recent interest in cross-
language information retrieval (CLIR)
is the question of which parallel corpora
might be best suited to tasks in CLIR, or
even to what extent parallel corpora can
be obtained or are necessary. One pro-
posal, which in our opinion has been
somewhat overlooked, is that the Bible
holds a unique value as a multilingual
corpus, being (among other things)
widely available in a broad range of
languages and having a high coverage
of modern-day vocabulary. In this pa-
per, we test empirically whether this
claim is justified through a series of
validation tests on various information
retrieval tasks. Our results appear to in-
dicate that our methodology may sig-
nificantly outperform others recently
proposed.
1 Introduction
This paper describes an empirical evaluation of
the Bible as a resource for cross-language infor-
mation retrieval (CLIR). The paper is organized
as follows: section 2 describes the background to
this project and explains our need for CLIR. Sec-
tion 3 sets out the various alternatives available
(as far as multilingual corpora are concerned) for
the type of textual CLIR which we want to per-
form, and details in qualitative terms why the
Bible would appear to be a good candidate. In
section 4, we outline the mechanics behind the
'Rosetta-Stone' type method we use for cross-
language comparison. The manner in which both
this method, and the reliability of using the Bible
as the basis for cross-language comparison, are
validated is outlined in section 5, together with
the results of our tests. Finally, we conclude on
and discuss these results in section 6.
2 Background
This paper describes a project which is part of a
larger, ongoing, undertaking, the goal of which is
to harvest a representative sample of material
from the internet and determine, on a very broad
scale, the answers to such questions as:
? what ideas in the global public discourse
enjoy most currency;
? how the popularity of ideas changes over
time.
Ideas are, of course, expressed in words; or, to
put it another way, a document's vocabulary is
likely to reveal something about the author's ide-
ology (Lakoff, 2002). In view of this, and since
ultimately we are interested in clustering the
documents harvested from the internet by their
ideology (and we understand 'ideology' in the
broadest possible sense), we approach the prob-
lem as a textual information retrieval (IR) task.
There is another level of complexity to the
problem, however. The language of the internet
is not, of course, confined to English; on the con-
trary, the representation of other languages is
probably increasing (Hill and Hughes, 1998;
Nunberg, 2000). Thus, for our results to be repre-
sentative, we require a way to compare docu-
ments in one language to those in potentially any
other language. Essentially, we would like to
answer the question of how ideologically aligned
two documents are, regardless of their respective
languages. In cross-language IR, this must be
approached by the use of a parallel multilingual
corpus, or at least some kind of appropriate train-
ing material available in multiple languages.
3 Parallel multilingual corpora: avail-
able alternatives
One collection of multilingual corpora gathered
with a specific view towards CLIR has been de-
68
veloped by the Cross-Language Evaluation Fo-
rum (CLEF); see, for example, Gonzalo (2001).
This collection, and its most recent revision (at
the CLEF website, www.clef-campaign.org), are
based on news documents or governmental
communications. Use of such corpora is wide-
spread in much recent CLIR work; one such ex-
ample is Nie, Simard, Isabelle and Durand
(1999), which uses the Hansard corpus, parallel
French-English texts of eight years of the Cana-
dian parliamentary proceedings, to train a CLIR
model.
It should be noted that the stated objective of
CLEF is to 'develop and maintain an infrastruc-
ture for the testing and evaluation of information
retrieval systems operating on European lan-
guages' (Peters 2001:1). Indeed, there is good
reason for this: CLEF is an activity under the
auspices of the European Commission. Likewise,
the Canadian Hansard corpus covers only Eng-
lish and French, the most widespread languages
of Canada. It is to be expected that governmental
institutions would have most interest in promot-
ing resources and research in the languages fal-
ling most within their respective domains.
But in many ways, not least for the computa-
tional linguistics community, nor for anyone in-
terested in understanding trends in global
opinion, this represents an inherent limitation.
Since many of the languages of interest for our
project are not European ? Arabic is a good ex-
ample ? resources such as the CLEF collection
will be insufficient by themselves. The output of
global news organizations is a more promising
avenue, because many such organizations make
an effort to provide translations in a wide variety
of languages. For example, the BBC news web-
site (http://news.bbc.co.uk/) provides translations
in 34 languages, as follows:
Albanian, Arabic, Azeri, Bengali, Bur-
mese, Chinese, Czech, English, French,
Hausa, Hindi, Indonesian, Kinyarwanda,
Kirundi, Kyrgyz, Macedonian, Nepali,
Pashto, Persian, Portuguese, Romanian,
Russian, Serbian, Sinhala, Slovene, So-
mali, Spanish, Swahili, Tamil, Turkish,
Ukrainian, Urdu, Uzbek, Vietnamese
However, there is usually no assurance that a
news article in one language will be translated
into any, let alne all, of the other languages.
In view of this, even more promising still as a
parallel corpus for our purposes is the Bible. Res-
nik, Olsen and Diab (1999) elaborate on some of
the reasons for this: it is the world's most trans-
lated book, with translations in over 2,100 lan-
guages (often, multiple translations per language)
and easy availability, often in electronic form
and in the public domain; it covers a variety of
literary styles including narrative, poetry, and
correspondence; great care is taken over the
translations; it has a standard structure which
allows parallel alignment on a verse-by-verse
basis; and, perhaps surprisingly, its vocabulary
appears to have a high rate of coverage (as much
as 85%) of modern-day language. Resnik, Olsen
and Diab note that the Bible is small compared to
many corpora currently used in computational
linguistics research, but still falls within the
range of acceptability based on the fact that other
corpora of similar size are used; and as previ-
ously noted, the breadth of languages covered is
simply not available elsewhere. This in itself
makes the Bible attractive to us as a resource for
our CLIR task. It is an open question whether,
because of the Bible's content, relatively small
size, or some other attribute, it can successfully
be used for the type of CLIR we envisage. The
rest of this paper describes our attempt to estab-
lish a definitive answer to this question.
4 Methods for Cross-Language Com-
parison
All of the work described in this section was im-
plemented using the Sandia Text Analysis Exten-
sible Library (STANLEY). STANLEY allows
for information retrieval based on a standard vec-
tor model (Baeza-Yates and Ribeiro-Neto, 1999:
27-30) with term weighting based on log en-
tropy. Previous work (Bauer et al2005) has
shown that the precision-recall curve for
STANLEY is better than many other published
algorithms; Dumais (1991) finds specifically that
the precision-recall curve for information re-
trieval based on log-entropy weighting compares
favorably to that for other weighting schemes.
Two distinct methods for cross-language com-
parison are described in this section, and these
are as follows.
The first method (Method 1) involves creating
a separate textual model for each 'minimal unit'
of each translation of the Bible. A 'minimal unit'
could be as small as a verse (e.g. Genesis 1:1),
but it could be a group of verses (e.g. Genesis
1:1-10); the key is that alignment is possible be-
cause of the chapter-and-verse structure of the
Bible, and that whatever grouping is used should
be the same in each translation. Thus, for each
69
language ?we end up with a set of models (m1,?,
m2,?, ? mn,?). If the Bible is used as the parallel
corpus and the 'minimal unit' is the verse, then n
= 31,102 (the number of verses in the Bible).
Let us suppose now that we wish to compare
document di with document dj, and that we hap-
pen to know that di is in English and dj is in Rus-
sian. In order to assess to what extent di and dj
are 'about' the same thing, we treat the text of
each document as a query against all of the mod-
els in its respective language. So, di is evaluated
against m1,English, m2,English, ?, mn,English to give
simi,1, simi,2, ?, simi,n, where simx,y (a value be-
tween 0 and 1) represents the similarity of docu-
ment dx in language?to model mn in language?,
based on the cosine of the angle between the vec-
tor for dx and the vector for mn. Similar evalua-
tions are performed for dj against the set of
models in Russian. Now, each set of n results for
a particular document can itself be thought of an
n-dimensional vector. Thus, di is associated with
(simi,1, simi,2, ?, simi,n) and dj with (simj,1, simj,2,
?, sim j,n). To quantify the similarity between di
and dj, we now compute the cosine between
these two vectors to yield a single measure, also
a value between 0 and 1. In effect, we have used
the multilingual corpus ? the Bible, in this case ?
in 'Rosetta-Stone' fashion to bridge the language
gap between di and dj. Method 1 is summarized
graphically in Figure 1, for two hypothetical
documents.
The second method of comparison (Method 2)
is quite similar. This time, however, instead of
building one set of textual models for each trans-
lation in language?(m1,?, m2,?, ? mn,?), we build
a single set of textual models for all translations,
with each language represented at least once (m1,
m2, ? mn). Thus, m1 might represent a model
based on the concatenation of Genesis 1:1 in
English, Russian, Arabic, and so on. In a fashion
similar to that of Method 1, each incoming
document di is evaluated as a query against m1,
m2, ?, mn, to give an n-dimensional vector
where each cell is a value between 0 and 1.
Method 2 is summarized graphically in Figure 2,
for just English and Russian.
There are at least two features of Method 2
which make it attractive, from a linguist's point
of view, for CLIR. The first is that it allows for
the possibility that a single input document may
be multilingual. In Figure 2, document dj is rep-
resented by an symbol with a mainly light-
colored background, but with a small dark-
colored section. This is intended to represent a
document with mainly English content, but some
small subsection in Russian. Under Method 1, in
which dj is compared to an English-language
model, the Russian content would have been ef-
fectively ignored, but under Method 2 this is no
longer the case. Accordingly, the hypothetical
similarity measure for the first 'minimal unit' has
changed very slightly, as has the overall measure
of similarity between document di and dj.
The second linguistic attraction of Method 2 is
that it is not necessary to know a priori the lan-
guage of di or dj, providing that the language is
one of those for which we have textual data in
the model set. Since, as already stated, the Bible
covers over 2,100 languages, this should not be a
significant theoretical impediment.
The theoretical advantages of Method 1 have
principally to do with the ease of technical im-
plementation. New model sets for additional lan-
guages can be easily added as they become
available, whereas under Method 2 the entire
model set must be rebuilt (statistics recomputed,
etc.) each time a new language is added.
5 Validation of the Bible as a resource
for CLIR
In previous sections, we have rehearsed some of
the qualitative arguments for our choice of the
70
Bible as the basis for CLIR. In this section, we
consider how this choice may be validated em-
pirically. We would like to know how reliable
the cross-language comparison methods outlined
in the previous section are at identifying docu-
ments in different languages but which happen to
be similar in content. This reliability will be in
part a function of the particular text analysis
model we employ, but it will also be a function
of our choice of parallel text used to train the
model. The Bible has some undeniable qualita-
tive advantages for our purposes, but are the
CLIR results based on it satisfactory in practice?
Three tests are described in this section; the aim
of these is to provide an answer to this question.
5.1 Preliminary analysis
In order to obtain a preliminary idea of whether
this method was likely to work, we populated the
entire matrix of similarity measures, verse by
verse, for each language pair. There are 31,102
verses in the Bible (allowing for some variation
in versification between different translations,
which we carefully controlled for by adopting a
common versification schema). Thus, this step
involved building a 31,102 by 31,102 matrix for
each language pair, in which the cell in row m
and column n contains a number between 0 and 1
representing the similarity of verse m in one lan-
guage to verse n in the other language. If use of
the Bible for CLIR is a sound approach, we
would expect to see the highest similarity meas-
ures in what we will call the matrix's diagonal
values ? the values occurring down the diagonal
of the matrix from top-left to bottom-right ?
meaning that verse n in one language is most
similar to verse n in the other, for all n.
Here, we would simply like to note an inciden-
tal finding. We found that for certain language
pairs, the diagonal values were significantly
higher than for other language pairs, as shown in
Table 1.
Language pair Mean similarity,
verse by verse
English-Russian 0.3728
English-Spanish 0.5421
English-French 0.5508
Spanish-French 0.5691
Table 1. Mean similarities by language pair
One hypothesis we have is that the lower overall
similarity for English-Russian is at least partly
due to the fact that Russian is a much more
highly inflected language then any of English,
French, or Spanish. That many verses containing
non-dictionary forms are the ones that score the
highest for similarity, and many of those that do
not score lowest, appears to confirm this. How-
ever, there appear to be other factors at play as
well, since many of the highest-scoring verses
contain proper names or other infrequently oc-
curring lexical items (examples are Esther 9:9:
'and Parmashta, and Arisai, and Aridai, and Vai-
zatha', and Exodus 37:19: 'three cups made like
almond-blossoms in one branch, a bud and a
flower, and three cups made like almond-
blossoms in the other branch, a bud and a flower:
so for the six branches going out of the lamp-
stand'). A third possibility, consistent with the
first, is that Table 1 actually reflects more gen-
eral measures of similarity between languages,
the Western European languages (for example)
all being more closely related to Latin than their
Slavic counterparts. At any rate, if our hypothesis
about inflection being an important factor is cor-
rect, then this would seem to underline the im-
portance of stemming for highly-inflected
languages.
5.2 Simple validation
In this test, the CLIR algorithm is trained on the
entire Bible, and validation is performed against
available extra-Biblical multilingual corpora
such as the FQS (2006) and RALI (2006) cor-
pora. This test, together with the tests already
described, should provide a reliable measure of
how well our CLIR model will work when ap-
plied to our target domain (documents collected
from the internet).
For this test, five abstracts in the FQS (2006)
were selected. These abstracts are in both Span-
ish and English, and the five are listed in Table 2
below.
Eng. 1 Perspectives
Eng. 2 Public and Private Narratives
Eng. 3 Qualitative Research
Eng. 4 How Much Culture is Psychology
Able to Deal With
Eng. 5 Conference Report
Sp. 1 Perspectivas
Sp. 2 Narrativas p?blicas y privadas
Sp. 3 Cu?nta cultura es capaz de abordar la
Psicolog?a
Sp. 4 Investigaci?n cualitativa
Sp. 5 Nota sobre la conferencia
Table 2. Documents selected for analysis
71
The results based on these five abstracts, where
comparison was performed between Spanish and
English and vice-versa, are as shown in Table 3.
The results shown in Table 3 are the actual (raw)
similarity values provided by our CLIR frame-
work using the FQS corpus.
Eng. 1 Eng. 2 Eng. 3 Eng. 4 Eng. 5
Sp. 1 0.6067 0.0430 0.0447 0.0821 0.1661
Sp. 2 0.0487 0.3969 0.0377 0.0346 0.0223
Sp. 3 0.1018 0.0956 0.0796 0.1887 0.1053
Sp. 4 0.0303 0.0502 0.0450 0.1013 0.0493
Sp. 5 0.0354 0.1314 0.0387 0.0425 0.1682
Table 3. Raw similarity values of Spanish and
English documents from FQS corpus
In this table, 'Eng. 1', 'Sp. 1', etc., refer to the
documents as listed in Table 2.
In four out of five cases, the CLIR engine cor-
rectly predicted which English document was
related to which Spanish document, and in four
out of five cases it also correctly predicted which
Spanish document was related to which English
document. We can relate these results to tradi-
tional IR measures such as precision-recall and
mean average precision by using a query that
returns the top-most similar document. Thus, our
?right? answer set as well as our CLIR answers
will consist of a single document. For the FQS
corpus, this represents a mean average precision
(MAP) of 0.8 at a recall point of 1 (the first
document recalled). The incorrect cases were
Eng. 4, where Sp. 3 was predicted, and Sp. 3,
where Eng. 4 was predicted. (By way of possible
explanation, both these two documents included
the keywords 'qualitative research' with the ab-
stract.) Furthermore, in most of the cases where
the prediction was correct, there is a clear margin
between the score for the correct choice and the
scores for the incorrect choices. This leads us to
believe that our general approach to CLIR is at
very least promising.
5.3 Validation on a larger test set
To address the question of whether the CLIR
approach performs as well on larger test sets,
where the possibility of an incorrect prediction is
greater simply because there are more documents
to select from, we trained the CLIR engine on the
Bible and validated it against the 114 suras of the
Quran, performing a four-by-four-way test using
the original Arabic (AR) text plus English (EN),
Russian (RU) and Spanish (ES) translations. The
MAP at a recall point of 1 is shown for each lan-
guage pair in Table 4.
Language of predicted document
AR EN RU ES
AR 1.0000 0.2193 0.2281 0.2105
EN 0.2632 1.0000 0.3333 0.5263
RU 0.2719 0.3860 1.0000 0.4386
Language
of input
ES 0.2105 0.4912 0.4035 1.0000
Table 4. Results based on Quran test
This table shows, for example, that for 52.63%
(or 60) of the 114 English documents used as
input, the correct Spanish document was re-
trieved first. As with the results in the previous
section, we can relate these results to MAP at a
recall of 1. If we were to consider more than just
the top-most similar document in our CLIR out-
put, we would expect the chance of seeing the
correct document to increase. However, since in
this experiment the number of relevant docu-
ments can never exceed 1, the precision will be
diluted as more documents are retrieved (except
at the point when the one correct document is
retrieved). The values shown in the table are, of
course, greater by a couple of orders of magni-
tude than that expected of random retrieval, of
0.0088 (1/114). Our methodology appears sig-
nificantly to outperform that proposed by
McNamee and Mayfield (2004), who report an
MAP of 0.3539, and a precision of 0.4520 at a
recall level of 10, for English-to-Spanish CLIR
based on 5-gram tokenization. (We have not yet
been able to compare our results to McNamee
and Mayfield's using the same corpora that they
use, but we intend to do this later. We do not ex-
pect our results to differ significantly from those
we report above.) Perhaps not surprisingly, our
results appear to be better for more closely-
related languages, with pairs including Arabic
being consistently those with the lowest average
predictive precision across all suras.
6 Discussion
In this paper, we have presented a non-language-
specific framework for cross-language informa-
tion retrieval which appears promising at least
for our purposes, and potentially for many others.
It has the advantages of being easily extensible,
and, with the results we have presented, it is em-
pirically benchmarked. It is extensible in two
dimensions; first, by language (substantially any
72
human language which might be represented on
the internet can be covered, and the cost of add-
ing resources for each additional language is
relatively small), secondly, by extending the
training set with additional corpora, for available
language pairs. Doubtless, also, the methodology
could be further tuned for better performance.
It is perhaps surprising that the Bible has not
been more widely used as a multilingual corpus
by the computational linguistics and information
retrieval community. In fact, it usually appears to
be assumed by researchers that parallel texts,
particularly those which have been as carefully
translated as the Bible and are easy to align, are
scarce and hard to come by (for two examples,
see McNamee and Mayfield 2004 and Munteanu
and Marcu 2006). The reason for the Bible being
ignored may be the often unspoken assumption
that the domain of the Bible is too limited (being
a religious document) or that its content is too
archaic. Yet, the truth is that much of the Bible's
content has to do with enduring human concerns
(life, death, war, love, etc.), and if the language is
archaic, that may have more a matter of transla-
tion style than of content.
There are a number of future research direc-
tions in computational linguistics we would like
to pursue, besides those which may be of interest
in other disciplines. The first is to use this
framework to evaluate the relative faithfulness of
different translations. For example, we would
expect to see similar statistical relationships
within the model for a translation of the Bible as
are seen in its original languages (Hebrew and
Greek). Statistical comparisons could thus be
used as the basis for evaluating a translation's
faithfulness to the original. Such an analysis
could be of theological, as well as linguistic, in-
terest.
Secondly, we would like to examine whether
the model's performance can be improved by
introducing more sophisticated morphological
analysis, so that the units of analysis are mor-
phemes instead of words, or possibly morphemes
as well as words.
Third, we intend to investigate further which
of the two methods outlined in section 4 per-
forms better in cross-language comparison, par-
ticularly when the language of the source
document is unknown. In particular, we are in-
terested in the extent to which homographic cog-
nates across languages (e.g. French coin 'corner'
versus English coin), may affect the performance
of the CLIR engine.
Acknowledgement
Sandia is a multiprogram laboratory operated
by Sandia Corporation, a Lockheed Martin Com-
pany, for the United States Department of En-
ergy?s National Nuclear Security Administration
under contract DE-AC04-94AL85000.
References
Lars Asker. 2004. Building Resources: Experiences
from Amharic Cross Language Information Re-
trieval. Paper presented at Cross-Language Infor-
mation Retrieval and Evaluation: Workshop of the
Cross-Language Evaluation Forum, CLEF 2004.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. New York:
ACM Press.
Travis Bauer, Steve Verzi, and Justin Basilico. 2005.
Automated Context Modeling through Text Analy-
sis. Paper presented at Cognitive Systems: Human
Cognitive Models in System Design.
Susan Dumais. 1991. Improving the Retrieval of In-
formation from External Sources. Behavior Re-
search Methods, Instruments, and Computers
23(2):229-236.
Forum: Qualitative Social research (FQS). 2006. Pub-
lished Conference Reports. (Conference reports
available on-line in multiple languages.) Accessed
at http://www.qualitative-
research.net/fqs/conferences/conferences-pub-
e.htm on February 22, 2006.
Julio Gonzalo. 2001. Language Resources in Cross-
Language Text Retrieval: a CLEF Perspective. In
Carol Peters (ed.). Cross-Language Information
Retrieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000: 36-47.
Berlin: Springer-Verlag.
George Lakoff. 2002. Moral politics : how liberals
and conservatives think. Chicago : University of
Chicago Press.
Paul McNamee and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval 7: 73-97.
Dragos Munteanu and Daniel Marcu. 2006. Improv-
ing Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational
Linguistics 31(4):477-504.
Geoffrey Nunberg. 2000. Will the Internet Always
Speak English? The American Prospect 11(10).
Carol Peters (ed.). 2001. Cross-Language Information
Retrieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000. Berlin:
Springer-Verlag.
73
Recherche appliqu?e en linguistique informatique
(RALI). 2006. Corpus align? bilingue anglais-
fran?ais. Accessed at http://rali.iro.umontreal.ca/
on February 22, 2006.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a Parallel Corpus: Annotating
the "Book of 2000 Tongues". Computers and the
Humanities, 33: 129-153.
74
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54?62,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using DEDICOM for  
Completely Unsupervised Part-of-Speech Tagging 
Peter A. Chew, Brett W. Bader 
Sandia National Laboratories 
P. O. Box 5800, MS 1012 
Albuquerque, NM 87185-1012, USA 
{pchew,bwbader}@sandia.gov 
Alla Rozovskaya 
Department of Computer Science 
University of Illinois 
Urbana, IL 61801, USA 
rozovska@illinois.edu 
 
Abstract 
A standard and widespread approach to 
part-of-speech tagging is based on Hidden 
Markov Models (HMMs). An alternative 
approach, pioneered by Sch?tze (1993), 
induces parts of speech from scratch using 
singular value decomposition (SVD). We 
introduce DEDICOM as an alternative to 
SVD for part-of-speech induction. 
DEDICOM retains the advantages of 
SVD in that it is completely unsupervised: 
no prior knowledge is required to induce 
either the tagset or the associations of 
types with tags. However, unlike SVD, it 
is also fully compatible with the HMM 
framework, in that it can be used to esti-
mate emission- and transition-probability 
matrices which can then be used as the 
input for an HMM. We apply the 
DEDICOM method to the CONLL corpus 
(CONLL 2000) and compare the output of 
DEDICOM to the part-of-speech tags 
given in the corpus, and find that the cor-
relation (almost 0.5) is quite high. Using 
DEDICOM, we also estimate part-of-
speech ambiguity for each type, and find 
that these estimates correlate highly with 
part-of-speech ambiguity as measured in 
the original corpus (around 0.88). Finally, 
we show how the output of DEDICOM 
can be evaluated and compared against 
the more familiar output of supervised 
HMM-based tagging. 
1 Introduction 
Traditionally, part-of-speech tagging has been ap-
proached either in a rule-based fashion, or stochas-
tically. Harris (1962) was among the first to 
develop algorithms of the former type. The rule-
based approach relies on two elements: a dictio-
nary to assign possible parts of speech to each 
word, and a list of hand-written rules ? which must 
be painstakingly developed for each new language 
or domain ? to disambiguate tokens in context. 
Stochastic taggers, on the other hand, avoid the 
need for hand-written rules by tabulating probabili-
ties of types and part-of-speech tags (which must 
be gathered from a tagged training corpus), and 
applying a special case of Bayesian inference 
(usually, Hidden Markov Models [HMMs]) to dis-
ambiguate tokens in context. The latter approach 
was pioneered by Stolz et al (1965) and Bahl and 
Mercer (1976), and became widely known through 
the work of e.g. Church (1988) and DeRose 
(1988). 
A third and more recent approach, known as 
?distributional tagging? and exemplified by 
Sch?tze (1993, 1995) and Biemann (2006), aims to 
eliminate the need for both hand-written rules and 
a tagged training corpus, since the latter may not 
be available for every language or domain. Distri-
butional tagging is fully-unsupervised, unlike the 
two traditional approaches described above. 
Sch?tze suggests analyzing the distributional pat-
terns of words by forming a term adjacency matrix, 
then subjecting that matrix to Singular Value De-
composition (SVD) to reveal latent dimensions. He 
shows that in the reduced-dimensional space im-
plied by SVD, tokens do indeed cluster intuitively 
by part-of-speech; and that if context is taken into 
account, something akin to part-of-speech tagging 
54
can be achieved. Whereas the performance of sto-
chastic taggers is generally sub-optimal when the 
domain of the training data differs from that of the 
test data, distributional tagging sidesteps this prob-
lem, since each corpus can be considered in its 
own right. Sch?tze (1995) notes two general draw-
backs of distributional tagging methods: the per-
formance is relatively modest compared to that of 
supervised methods; and languages with rich mor-
phology may pose a challenge.1
In this paper, we present an alternative unsuper-
vised approach to distributional tagging. Instead of 
SVD, we use a dimensionality reduction technique 
known as DEDICOM, which has various advan-
tages over the SVD-based approach. Principal 
among these is that, even though no pre-tagged 
corpus is required, DEDICOM can easily be used 
as input to a HMM-based approach (and the two 
share linear-algebraic similarities, as we will make 
clear in section 4). Although our empirical results, 
like those of Sch?tze (1995), are perhaps still rela-
tively modest, the fact that a clearer connection 
exists between DEDICOM and HMMs than be-
tween SVD and HMMs gives us good reason to 
believe that with further refinements, DEDICOM 
may be able to give us ?the best of both worlds? in 
many respects: the benefits of avoiding the need 
for a pre-tagged corpus, with empirical results ap-
proaching those of HMM-based tagging. 
In the following sections, we introduce 
DEDICOM, describe its applicability to the part-
of-speech tagging problem, and outline its connec-
tions to the standard HMM-based approach to tag-
ging. We evaluate the use of DEDICOM on the 
CONLL 2000 shared task data, discuss the results 
and suggest avenues for improvement. 
2 DEDICOM 
DEDICOM, which stands for ?DEcomposition into 
DIrectional COMponents?, is a linear-algebraic 
decomposition method attributable to Harshman 
(1978) which has been used to analyze matrices of 
 
1 We note the latter is also true for languages in which word 
order is relatively free ? usually the same languages as those 
with rich morphology. While English word order is signifi-
cantly constrained by part-of-speech categorizations, this is 
not as true of, say, Russian. Thus, an adjacency matrix formed 
from a Russian corpus is likely to be less informative about 
part-of-speech classifications as one formed from an English 
corpus. Quite possibly, this is as much of a limitation for 
DEDICOM as it is for SVD. 
asymmetrical directional relationships between 
objects or persons. Early on, the technique was 
applied by Harshman et al (1982) to the analysis 
of two types of marketing data: ?free associations? 
? how often one phrase (describing hair shampoo) 
evokes another in the minds of survey respondents, 
and ?car switching data? ? how often people switch 
from one to another of 16 car types. Both datasets 
are asymmetric and directional: in the first dataset, 
for example, the phrase ?body? (referring to sham-
poo) evoked the phrase ?fullness? twice as often in 
the minds of respondents as ?fullness? evoked 
?body?. Likewise, the data from Harshman et al 
(1982) show that in the given period, 3,820 people 
switched from ?midsize import? cars to ?midsize 
domestic? cars, but only 2,140 switches were made 
in the reverse direction. Another characteristic of 
these ?asymmetric directional? datasets is that they 
can be represented in square matrices. For exam-
ple, the raw car switching data can be represented 
in a 16 ? 16 matrix, since there are 16 car types. 
The objective of DEDICOM, which can be 
compared to that of SVD, is to factorize the raw 
data matrices into a lower-dimensional space iden-
tifying underlying, idealized directional patterns in 
the data. For example, while there are 16 car types 
in the raw car switching data, Harshman shows 
that under a 4-dimensional DEDICOM analysis, 
these can be ?boiled down? to the basic types ?plain 
large-midsize?, ?specialty?, ?fancy large?, and 
?small? ? and that patterns of switching among 
these more basic types can then be identified. 
If X represents the original n ? n matrix of 
asymmetric relationships, and a general entry xij in 
X represents the strength of the directed relation-
ship of object i to object j, then the single-domain 
DEDICOM model2 can be written as follows: 
 
X = ARAT + E (1) 
 
where A denotes an n ? q matrix of weights of the 
n observed objects in q dimensions (where q < n), 
and R is a dense q ? q asymmetric matrix express-
ing the directional relationships between the q di-
mensions or basic types. AT is simply the transpose 
 
2 There is a dual-domain DEDICOM model, which is also 
described in Harshman (1978). The dual-domain DEDICOM 
model is not relevant to our discussion, and thus it will not be 
mentioned further. References in this paper to ?DEDICOM? 
are to be understood as references in shorthand to ?single-
domain DEDICOM?. 
55
of A, and E is a matrix of error terms. Our objec-
tive is to minimize E, so we can also write: 
 
X  ARAT (2) 
 
As noted by Harshman (1978: 209), the fact that 
A appears on both the left and right of R means 
that the data is described ?in terms of asymmetric 
relations among a single set of things? ? in other 
words, when objects are on the receiving end of the 
directional relationships, they are still of the same 
type as those on the initiating end. 
One difference between DEDICOM and SVD is 
that there is no unique solution: either A or R can 
be scaled or rotated without changing the goodness 
of fit, so long as the inverse operation is applied to 
the other. For example, if we let ? = AD, where D 
is any diagonal scaling matrix (or, more generally, 
any nonsingular matrix), then we can write 
 
X  ARAT = ?D-1RD-1?T (3) 
since ?T = (AD) T = DAT
(In our application, we constrain A and R to be 
nonnegative as noted below.) 
To our knowledge, there have been no applica-
tions of DEDICOM to date in computational lin-
guistics. This is in contrast to SVD, which has 
been extensively used for text analysis (for appli-
cations other than unsupervised part-of-speech 
tagging, see Baeza-Yates and Ribeiro-Neto 1999). 
3 Applicability of DEDICOM to part-of-
speech tagging 
Sch?tze?s (1993) key insight is that ? at least in 
English ? adjacencies between types are a good 
guide to their grammatical functions. That insight 
can be leveraged by applying either SVD or 
DEDICOM to a type-by-type adjacency matrix. 
With DEDICOM, however, we add the constraint 
(already stated) that the types are a ?single set of 
things?: whether a type ?precedes? or ?follows? ? 
i.e., whether it is in a row or a column of the ma-
trix ? does not affect its grammatical function. This 
constraint is as it should be, and, to our knowledge, 
sets DEDICOM apart from all previous unsuper-
vised approaches including those of Sch?tze (1993, 
1995) and Biemann (2006). 
Given any corpus containing n types and k to-
kens, we can let X be an n ? n token-adjacency 
matrix. Let each entry xij in X denote the number 
of times in the corpus that type i immediately pre-
cedes type j. X is thus a matrix of bigram frequen-
cies. It follows that the sum of the elements of X 
equals k ? 1 (because the first token in the corpus 
is preceded by nothing, and the last token is fol-
lowed by nothing). Any given row sum of X (the 
type frequency corresponding to the particular 
row) will equal the corresponding column sum, 
except if the type happens to occur in the first or 
last position in the corpus. X will be asymmetric, 
since the frequency of bigram ij is clearly not the 
same as that of bigram ji for all i and j.
It can be seen, therefore, that our X represents 
asymmetric directional data, very similar to the 
data analyzed in Harshman (1978) and Harshman 
et al (1982). If we fit the DEDICOM model to our 
X matrix, then we obtain an A matrix which 
represents types by latent classes, and an R matrix 
which represents directional relationships between 
latent classes. We can think of the latent classes as 
induced parts of speech. 
With SVD, we believe that the orthogonality of 
the reduced-dimensional features militates against 
any attempt to correlate these features with parts of 
speech. From a linguistic point of view, there is no 
reason to believe that parts of speech are orthogon-
al to one another in any sense. For example, nouns 
and adjectives (traditionally classified together as 
?nominals?) seem to share more in common with 
one another than nouns and verbs. With 
DEDICOM, this is not an issue, because the col-
umns of A are not required to be mutually ortho-
gonal to one another, unlike the left and right 
singular vectors from SVD. 
Thus, the A matrix from DEDICOM shows how 
strongly associated each type is with the different 
induced parts of speech; we would expect types 
which are ambiguous (such as ?claims?, which can 
be either a noun or a verb) to have high loadings 
on more than one column in A. Again, if the 
classes correlate with parts of speech, the R matrix 
will show the latent patterns of adjacency between 
different parts of speech. 
4 Connections between DEDICOM and 
HMM-based tagging 
For any HMM, two components are necessary: a 
set of emission probabilities and a set of transition 
probabilities. Applying this framework to part-of-
56
speech tagging, the tags are conceived of as the 
hidden layer of the HMM and the tokens (each of 
which is associated with a type) as the visible 
layer. The emission probabilities are then the prob-
abilities of types given the tags, and the transition 
probabilities are the probabilities of the tags given 
the preceding tags. If these probabilities are 
known, then there are algorithms (such as the Vi-
terbi algorithm) to determine the most likely se-
quence of tags given the visible sequence of types. 
In the case of supervised learning, we obtain the 
emission and transition probabilities by observing 
actual frequencies in a tagged corpus. Suppose our 
corpus, as previously discussed, consists of n types 
and k tokens. Since we are dealing with supervised 
learning, the number of the tags in the tagset is also 
known: we denote this number q. Now, the ob-
served frequencies can be represented, respective-
ly, as n ? q and q ? q matrices: we denote these A* 
and R*. Each entry aij in A* denotes the number of 
times type i is associated with tag j, and each entry 
rij in R* denotes the number of times tag j imme-
diately follows tag i. Moreover, we know some 
other properties of A* and R*: 
 
 the respective sums of the elements of A* and 
R* are equal to k ? 1; 
 each row sum of A* (
=
q
x
ixa
1
) corresponds to 
the frequency in the corpus of type i;
 each column sum of A*, as well as the corres-
ponding row and column sums of R*, are the 
frequencies of the given tags in the corpus (for 
all j, 
===
==
q
x
jx
q
x
xj
q
x
xj rra
111
). 
 
If A* and R* contain frequencies, however, we 
must perform a matrix operation to obtain transi-
tion and emission probabilities for use in an 
HMM-based tagger. In effect, A* must be made 
column-stochastic, and R* must be made row-
stochastic. Since the column sums of A* equal the 
respective row sums of R*, this can be achieved by 
post-multiplying both A* and R* by DA, where DA
is a diagonal scaling matrix containing the inverses 
of the column sums of A (or equivalently, the row 
sums of R). Then the matrix of emission probabili-
ties is given by A*DA, and the matrix of transition 
probabilities by R*DA.
We can now make the connection to DEDICOM 
explicit. Let A = A*DA and R = R*, then we can 
rewrite (2) as follows: 
 
X  ARAT = (A*DA) R* (A*DA)T (4) 
X  A*DA R*DA A*T (5) 
 
In other words, for any corpus we may compute 
a probabilistic representation of the type adjacency 
matrix X (which will contain expected frequencies 
comparable to the actual frequencies) by multiply-
ing the emission probability matrix A*DA, the 
transition probability matrix R*DA, and the type-
by-tag frequency matrix A*. (Presumably, the 
closer the approximation, the better the tagging in 
the training set actually factorizes the true direc-
tional relationships.) 
Conversely, for fully unsupervised tagging, we 
can fit the DEDICOM model to the type adjacency 
matrix X. The resulting A matrix contains esti-
mates of what the tags should be (if a tagged train-
ing corpus is unavailable), as well as the emission 
probability of each type given each tag, and the 
resulting R matrix is the corresponding transition 
probability matrix given those tags. In this case, a 
column-stochastic A can be used directly as the 
emission probability matrix, and we simply make 
R* row-stochastic to obtain the matrix of transition 
probabilities. The only difference then between the 
output of the fully-unsupervised DEDICOM/HMM 
tagger and that of a supervised HMM tagger is that 
in the first case, the ?tags? are numeric indices 
representing the corresponding column of A, and 
in the second case, they are the members of the 
tagset used in the training data. 
The fact that emission and transition probabili-
ties (or at least something very like them) are a 
natural by-product of DEDICOM sets DEDICOM 
apart from Sch?tze?s SVD-based approach, and is 
for us a significant reason which recommends the 
use of DEDICOM. 
5 Evaluation 
For all evaluation described here, we used the 
CONLL 2000 shared task data (CONLL 2000). 
This English-language newswire corpus consists of 
19,440 types and 259,104 tokens (including punc-
tuation marks as separate types/tokens). Each to-
ken is associated with a part-of-speech tag and a 
chunk tag, although we did not use the chunk tags 
57
in the work described here. The tags are from a 44-
item tagset. The CONLL 2000 tags against which 
we measure our own results are in fact assigned by 
the Brill tagger (Brill 1992), and while these may 
not correlate perfectly with those that would have 
been assigned by a human linguist, we believe that 
the correlation is likely to be good enough to allow 
for an informative evaluation of our method. 
Before discussing the evaluation of unsuper-
vised DEDICOM, let us briefly reconsider the si-
milarities of DEDICOM to the supervised HMM 
model in the light of actual data in the CONLL 
corpus. We stated in (5) that X  A*DAR*DAA*T.
For the CONLL 2000 tagged data, A* is a 19,440 
? 44 matrix and R* is a 44 ? 44 matrix. Using 
A*DA and R*DA as emission- and transition-
probability matrices within a standard HMM 
(where the entire CONLL 2000 corpus is treated as 
both training and test data), we obtained a tagging 
accuracy of 95.6%. By multiplying 
A*DAR*DAA*T, we expect to obtain a matrix ap-
proximating X, the table of bigram frequencies. 
This is indeed what we found: it will be apparent 
from Table 1 that the top 10 expected bigram fre-
quencies based on this matrix multiplication are 
generally quite close to actual frequencies. Moreo-
ver, the sum of the elements in A*DAR*DAA*T is 
equal to the sum of the elements in X, and if we let 
E be the matrix of error terms (X - 
A*DAR*DAA*T), then we find that ||E|| (the Frobe-
nius norm of E) is 38.764% of ||X|| - in other 
words, A*DAR*DAA*T accounts for just over 60% 
of the data in X. 
 
Type 1 Type 2 Actual 
frequency 
Expected 
frequency 
of the 1,421.000 1,202.606 
in the 1,213.000 875.822 
for the 553.000 457.067 
to the 445.000 415.524 
on the 439.000 271.528 
the company 383.000 105.794 
a share 371.000 32.447 
that the 315.000 258.679 
and the 302.000 296.737 
to be 285.000 499.315 
Table 1. Actual versus expected frequencies for 10 most 
common bigrams in CONLL 2000 corpus 
 
Having confirmed that there exists an A 
(=A*DA) and R (=R*) which both satisfies the 
DEDICOM model and can be used directly within 
a HMM-based tagger to achieve satisfactory re-
sults, we now consider whether A and R can be 
estimated if no tagged training set is available. 
We start, therefore, from X, the square 19,440 ?
19,440 (sparse) matrix of raw bigram frequencies 
from the CONLL 2000 data. Using Matlab and the 
Tensor Toolbox (Bader and Kolda 2006, 2007), we 
computed the best rank-44 non-negative 
DEDICOM3 decomposition of this matrix using 
the 2-way version of the ASALSAN algorithm 
presented in Bader et al (2007), which is based on 
iteratively improving random initial guesses for A 
and R. As with SVD, the rank of the decomposi-
tion can be selected by the user; we chose 44 since 
that was known to be the number of items in the 
CONLL 2000 tagset, but a lower number could be 
selected for a coarser-grained part-of-speech anal-
ysis. Ultimately, perhaps the best way to determine 
the optimal rank would be to evaluate different 
options within a larger end-to-end system, for ex-
ample an information retrieval system; this, how-
ever, was beyond our scope in this study. 
As already mentioned, there are indeterminacies 
of rotation and scale in DEDICOM. As Harshman 
et al (1982: 211) point out, ?when the columns of 
A are standardized? the R matrix can then be in-
terpreted as expressing relationships among the 
dimensions in the same units as the original data. 
That is, the R matrix can be interpreted as a ma-
trix of the same kind as the original data matrix X, 
but describing the relations among the latent as-
pects of the phrases, rather than the phrases them-
selves?. Thus, if DEDICOM is constrained so that 
A is column-stochastic (which is required in any 
case of the matrix of emission probabilities), then 
the sum of the elements in R should approximate 
the sum of the elements in X. R is therefore com-
parable to R* (with some provisos which shall be 
enumerated below), and to obtain the row-
stochastic transition-probability matrix, we simply 
multiply R by a diagonal matrix DR whose ele-
ments are the inverses of R?s row sums. 
 
3 Non-negative DEDICOM imposes the constraint not present 
in Harshman (1978, 1982) that all entries in A and R must be 
non-negative. This constraint is appropriate in the present 
case, since the entries in A* and R* (and of course the proba-
bilities in A*D and R*D) are by definition non-negative. 
58
Table 2. Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 dataset 
With A as an emission-probability matrix and 
RDR as a transition-probability matrix, we now 
have all that is needed for an HMM-based tagger 
to estimate the most likely sequence of ?tags? given 
the corpus. However, since the ?tags? here are nu-
merical indices, as mentioned, to evaluate the out-
put we must look at the correlation between these 
?tags? and the gold-standard tags given in the 
CONLL 2000 data. One way this can be done is by 
presenting a 44 ? 44 confusion matrix (of gold-
standard tags against induced tags), and then mea-
suring the correlation coefficient (Pearson?s R) 
between that matrix and the ?idealized? confusion 
matrix in which each induced tag corresponds to 
one and only one ?gold standard? tag. Using A and 
RDR as the input to a HMM-based tagger, we 
tagged the CONLL 2000 dataset with induced tags 
and obtained the confusion matrix shown in Table 
2 (owing to space constraints, only the first 20 col-
umns are shown). The correlation between this 
matrix and the equivalent diagonalized ?ideal? ma-
trix is in fact 0.4942, which is significantly higher 
than could have occurred by chance. 
It should be noted that a lack of correlation be-
tween the induced tags and the gold standard tags 
can be attributed to at least two independent fac-
tors. The first, of course, is any inability of the 
DEDICOM model to fit the particular problem and 
data. Clearly, this is undesirable. The other factor 
to be borne in mind, which works to DEDICOM?s 
favor, is that the DEDICOM model could yield an 
A and R which factorize the data more optimally 
than the A*D and R* implied by the gold-standard 
tags. There are three methods we can use to try and 
tease apart these competing explanations of the 
results, two quantitative and the other subjective. 
Quantitatively, we can compare the respective er-
ror matrices E. We have already mentioned that 
38764.0||X||
||ADRDAX|| T*A*A*  (6) 
Similarly, using the A and R from DEDICOM we 
can compute 
24078.0||X||
||ARAX|| T  (7) 
59
The fact that the error is lower in the second case 
implies that DEDICOM allows us to find a part-of-
speech ?factorization? of the data which fits better 
even than the gold standard, although again there 
are some caveats to this; we will return to these in 
the discussion. 
Another way to evaluate the output of 
DEDICOM is by comparing the number of part-of-
speech tags for a type in the gold standard to the 
number of classes in the A matrix with which the 
type is strongly associated. We test this by measur-
ing the Pearson correlation between the two va-
riables. First, we compute the average number of 
part-of-speech tags per type using the gold stan-
dard. We refer to this value as ambiguity coeffi-
cient; for the CONLL dataset, this is 1.05. Because 
A is dense, if we count all non-zero columns for a 
type in the A matrix as possible classes, we obtain 
a much higher ambiguity coefficient. We therefore 
set a threshold and consider only those columns 
whose values exceed a certain threshold. The thre-
shold is selected so that the ambiguity coefficient 
of the A matrix is the same as that of the gold stan-
dard. For a given type, every column with a value 
exceeding the threshold is counted as a possible 
class for that type. We then compute the Pearson 
correlation coefficient between the number of 
classes for a type in the A matrix and the number 
of part-of speech tags for that type in the CONLL 
dataset as provided by the Brill tagger. We ob-
tained a correlation coefficient of 0.88, which 
shows that there is indeed a high correlation be-
tween the induced tags and the gold standard tags 
obtained with DEDICOM. 
Finally, we can evaluate the output subjectively 
by looking at the content of the A matrix. For each 
?tag? (column) in A, the ?types? (rows) can be 
listed in decreasing order of their weighting in A. 
This gives us an idea of which types are most cha-
racteristic of which tags, and whether the grouping 
into tags makes any intuitive sense. These results 
(for selected tags only, owing to limitations of 
space) are given in Table 3. 
Many groupings in Table 3 do make sense: for 
example, the fourth tag is clearly associated with 
verbs, while the two types with significant weight-
ings for tag 2 are both determiners. By referring 
back to Table 2, we can see that many tokens in the 
CONLL 2000 dataset tagged as verbs are indeed 
tagged by the DEDICOM tagger as ?tag 4?, while 
many determiners are tagged as ?tag 3?. To under-
stand where a lack of correlation may arise, how-
ever, it is informative to look at apparent 
anomalies in the A matrix. For example, it can be 
seen from Table 3 that ?new?, an adjective, is 
grouped in the third tag with ?a? and ?the? (and 
ranking above ?an?). Although not in agreement 
with the CONLL 2000 ?gold standard? tagging, the 
idea that determiners are a type of adjective is in 
fact in accordance with traditional English gram-
mar. Here, the grouping of ?new?, ?a? and ?the? can 
be explained by the distributional similarities (all 
precede nouns). It should also be emphasized that 
the A matrix is essentially a ?soft clustering? of 
types (meaning that types can belong to more than 
one cluster). Thus, for example, ?u.s.? (the abbrevi-
ation for United States) appears under both tag 2 
(which appears to have high loadings for nouns) 
and tag 8 (with high loadings for adjectives). 
We have alluded above in passing to possible 
methods for improving the results of the 
DEDICOM analysis. One would be to pre-process 
the data differently. Here, a variety of options are 
available which maintain a generally unsupervised 
approach (one example is to avoid treating punctu-
ation as tokens). However, variations in pre-
processing are beyond the scope of this paper. 
Tag Top 10 types (by weight) with weightings 
1 million share said . year billion inc. corp. years quarter 
0.0246 0.0146 0.0129 0.0098 0.0088 0.0069 0.0064 0.0061 0.0058 0.0054 
2 company u.s. new first market share year stock . government 
0.0264 0.0136 0.0113 0.0095 0.0086 0.0086 0.0079 0.0077 0.0065 0.006 
3 the a new an other its any addition their 1988 
0.2889 0.1194 0.0121 0.0094 0.0092 0.0085 0.0067 0.0062 0.0062 0.0057 
?
8 the its his about those their all u.s. . this 
0.0935 0.0462 0.0208 0.0160 0.0096 0.0095 0.0088 0.0077 0.0074 0.0071 
?
Table 3. Type weightings in A matrix, by tag 
60
Another method would be to constrain 
DEDICOM so that the output more closely models 
the characteristics of A* and R*, the emission- and 
transition-probability matrices obtained from a 
tagged training set. In particular, there is one im-
portant constraint on R* which is not replicated in 
R: the constraint mentioned above that for all j,

==
=
q
x
jx
q
x
xj rr
11
. We note that this constraint can be 
satisfied by Sinkhorn balancing (Sinkhorn 1964)4,
although it remains to be seen how the constraint 
on R can best be incorporated into the DEDICOM 
architecture. Assuming that A is column-
stochastic, another desirable constraint is that the 
rows of A(DR)-1 should sum to the same as the 
rows of X (the respective type frequencies). With 
the implementation of these (and any other) con-
straints, one would expect the fit of DEDICOM to 
the data to worsen (cf. (6) and (7) above), but in-
curring this cost could be worthwhile if the payoff 
were somehow linguistically interesting (for ex-
ample, if it turned out we could achieve a much 
higher correlation to gold-standard tagging). 
6 Conclusion 
In this paper, we have introduced DEDICOM, an 
analytical technique which to our knowledge has 
not previously been used in computational linguis-
tics, and applied it to the problem of completely 
unsupervised part-of-speech tagging. Theoretical-
ly, the model has features which recommend it 
over other previous approaches to unsupervised 
tagging, specifically SVD. Principal among the 
advantages is the compatibility of DEDICOM with 
the standard HMM-based approach to part-of-
speech tagging, but another significant advantage 
is the fact that types are treated as ?a single set of 
objects? regardless of whether they occupy the first 
or second position in a bigram. 
By applying DEDICOM to a tagged dataset, we 
have shown that there is a significant correlation 
between the tags induced by unsupervised, 
DEDICOM-based tagging, and the pre-existing 
gold-standard tags. This points both to an inherent 
validity in the gold-standard tags (as a reasonable 
 
4 It is also worth noting that Sinkhorn was motivated by the 
same problem which concerns us, that of estimating a transi-
tion-probability matrix for a Markov model. 
factorization of the data) and to the fact that 
DEDICOM appears promising as a method of in-
ducing tags in cases where no gold standard is 
available. 
We have also shown that the factors of 
DEDICOM are interesting in their own right: our 
tests show that the A matrix (similar to an emis-
sion-probability matrix) models type part-of-
speech ambiguity well. Using insights from 
DEDICOM, we have also shown how linear alge-
braic techniques may be used to estimate the fit of 
a given part-of-speech factorization (whether in-
duced or manually created) to a given dataset, by 
comparing actual versus expected bigram frequen-
cies. 
In summary, it appears that DEDICOM is a 
promising way forward for bridging the gap be-
tween unsupervised and supervised approaches to 
part-of-speech tagging, and we are optimistic that 
with further refinements to DEDICOM (such as 
the addition of appropriate constraints), more in-
sight will be gained on how DEDICOM may most 
profitably be used to improve part-of-speech tag-
ging when few pre-existing resources (such as 
tagged corpora) are available. 
Acknowledgements 
We are grateful to Danny Dunlavy for contributing 
his thoughts to this work. 
Sandia is a multiprogram laboratory operated by 
Sandia Corporation, a Lockheed Martin Company, 
for the United States Department of Energy?s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000. 
61
References  
Brett W. Bader, Richard A. Harshman, and Tamara G. 
Kolda. 2007. Temporal analysis of semantic graphs 
using ASALSAN. In Proceedings of the 7th IEEE In-
ternational Conference on Data Mining, 33-42. 
Brett W. Bader and Tamara G. Kolda. 2006.  Efficient 
MATLAB Computations with Sparse and Factored 
Tensors.  Technical Report SAND2006-7592, Sandia 
National Laboratories, Albuquerque, NM and Liver-
more, CA. 
Brett W. Bader and Tamara G. Kolda. 2007.  The 
MATLAB Tensor Toolbox, version 2.2.  
http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.  
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval. New York: ACM 
Press. 
L. R. Bahl and R. L. Mercer. 1976. Part of speech as-
signment by a statistical decision algorithm. In Pro-
ceedings of the IEEE International Symposium on 
Information Theory, 88-89. 
C. Biemann. 2006. Unsupervised part-of-speech tagging 
employing efficient graph clustering. In Proceedings 
of the COLING/ACL 2006 Student Research Work-
shop, 7-12. 
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the Third Conference on Ap-
plied Natural Language Processing, 152-155. 
K. W. Church. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. In ANLP 
1988, 136-143. 
CONLL 2000. Shared task data. Retrieved Dec. 1, 2008 
from http://www.cnts.ua.ac.be/conll2000/chunking/. 
S. J. DeRose. 1988. Grammatical category disambigua-
tion by statistical optimization. Computational Lin-
guistics 14, 31-39. 
Harris, Z. S. 1962. String Analysis of Sentence Struc-
ture. Mouton: The Hague. 
Richard Harshman. 1978. Models for Analysis of 
Asymmetrical Relationships Among N Objects or 
Stimuli. Paper presented at the First Joint Meeting of 
the Psychometric Society and The Society for Ma-
thematical Psychology. Hamilton, Canada. 
Richard Harshman, Paul Green, Yoram Wind, and Mar-
garet Lundy. 1982. A Model for the Analysis of 
Asymmetric Data in Marketing Research. Marketing 
Science 1(2), 205-242. 
Hinrich Sch?tze. 1993. Part-of-Speech Induction from 
Scratch. In Proceedings of the 31st Annual Meeting of 
the Association for Computational Linguistics, 251-
258. 
Hinrich Sch?tze. 1995. Distributional Part-of-Speech 
Tagging. In Proceedings of the 7th Conference of the 
European Chapter of the Association for Computa-
tional Linguistics, 141-148. 
Richard Sinkhorn. 1964. A Relationship Between Arbi-
trary Positive Matrices and Doubly Stochastic Ma-
trices. The Annals of Mathematical Statistics 35(2), 
876-879. 
W. S. Stolz, P. H. Tannenbaum, and F. V. Carstensen. 
1965. A stochastic approach to the grammatical cod-
ing of English. Communications of the ACM 8(6), 
399-405. 
62
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465?473,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
TermWeighting Schemes for Latent Dirichlet Allocation
Andrew T. Wilson
Sandia National Laboratories
PO Box 5800, MS 1323
Albuquerque, NM 87185-1323, USA
atwilso@sandia.gov
Peter A. Chew
Moss Adams LLP
6100 Uptown Blvd. NE, Suite 400
Albuquerque, NM 87110-4489, USA
Peter.Chew@MossAdams.com
Abstract
Many implementations of Latent Dirichlet Al-
location (LDA), including those described in
Blei et al (2003), rely at some point on the
removal of stopwords, words which are as-
sumed to contribute little to the meaning of
the text. This step is considered necessary be-
cause otherwise high-frequency words tend to
end up scattered across many of the latent top-
ics without much rhyme or reason. We show,
however, that the ?problem? of high-frequency
words can be dealt with more elegantly, and
in a way that to our knowledge has not been
considered in LDA, through the use of appro-
priate weighting schemes comparable to those
sometimes used in Latent Semantic Indexing
(LSI). Our proposed weighting methods not
only make theoretical sense, but can also be
shown to improve precision significantly on a
non-trivial cross-language retrieval task.
1 Introduction
Latent Dirichlet Allocation (LDA) (Blei et al, 2003),
like its more established competitors Latent Seman-
tic Indexing (LSI) (Deerwester et al, 1990) and
Probabilistic Latent Semantic Indexing (PLSI) (Hof-
mann, 1999), is a model which is applicable to the
analysis of text corpora. It is claimed to differ from
LSI in that LDA is a generative Bayesianmodel (Blei
et al, 2003), although this may depend upon the
manner in which one approaches LSI (see for exam-
ple Chew et al (2010)). In LDA as applied to text
analysis, each document in the corpus is modeled as
a mixture over an underlying set of topics, and each
topic is modeled as a probability distribution over the
terms in the vocabulary.
As the newest among the above-mentioned tech-
niques, LDA is still in a relatively early stage of de-
velopment. It is also sufficiently different from LSI,
probably themost popular andwell-known compres-
sion technique for information retrieval (IR), that
many practitioners of LSI may perceive a ?barrier to
entry? to LDA. This in turn perhaps explains why no-
tions such as term weighting, which have been com-
monplace in LSI for some time (Dumais, 1991), have
not yet found a place in LDA. In fact, it is often as-
sumed that weighting is unnecessary in LDA. For
example, Blei et al (2003) contrast the use of tf-
idf weighting in both non-reduced space (Salton and
McGill, 1983) and LSI on the one hand with PLSI
and LDA on the other, where no mention is made of
weighting. Ramage et al (2008) propose a simple
term-frequency weighting scheme for tagged docu-
ments within the framework of LDA, although term
weighting is not their focus and their scheme is in-
tended to incorporate document tags into the same
model that represents the documents themselves.
In this paper, we produce evidence that term
weighting should be given consideration within
LDA. First and foremost, this is shown empiri-
cally through a non-trivial multilingual retrieval task
which has previously been used as the basis for
tests of variants of LSI. We also show that term
weighting allows one to avoid maintenance of stop-
lists, which can be awkward especially for multilin-
gual data. With appropriate term weighting, high-
frequency words (which might otherwise be elimi-
nated as stopwords) are assigned naturally to topics
465
by LDA, rather than dominating and being scattered
across many topics as happens with the standard uni-
form weighting. Our approach belies the usually
unstated, but widespread, assumption in papers on
LDA that the removal of stopwords is a necessary
pre-processing step (see e.g. Blei et al (2003); Grif-
fiths and Steyvers (2004)).
It might seem that to demonstrate this it would be
necessary to perform a test that directly compares the
results when stoplists are used to those when weight-
ing are used. However, we believe that stopwords
are highly ad-hoc to begin with. Assuming a vocab-
ulary of n words and a stoplist of x items, there are
(at least in theory)
(n
x
)
possible stoplists. To be sure
that no stoplist improves on a particular termweight-
ing scheme we would have to test every one of these.
In addition, our tests are with a multilingual dataset,
which raises the issue that a domain-appropriate sto-
plist for a particular corpus and language may not be
available. This is even more true if we pre-process
the dataset morphologically (for example, with stem-
ming). Therefore, rather than attempting a direct
comparison of this type, we take the position that it
is possible to sidestep the need for stoplists and to do
so in a non-ad-hoc way.
The paper is organized as follows. Section 2 de-
scribes the general framework of LDA, which has
only very recently been applied to cross-language
IR. In Section 3, we look at alternatives to the
?standard? uniform weighting scheme (i.e., lack of
weighting scheme) commonly used in LDA. Sec-
tion 4 discusses the framework we use for empiri-
cal testing of our hypothesis that a weighting scheme
would be beneficial. We present the results of this
comparison in Section 5 along with an impressionis-
tic comparison of the output of the different alterna-
tives. We conclude in Section 6.
2 Latent Dirichlet Allocation
Our IR framework is multilingual Latent Dirich-
let Allocation (LDA), first proposed by Blei et al
(2003) as a general Bayesian framework with initial
application to topicmodeling. It is only very recently
that variants of LDA have been applied to cross-
language IR: examples are Cimiano et al (2009) and
Ni et al (2009).
As an approach to topic modeling, LDA relies on
the idea that the tokens in a document are drawn in-
dependently from a set of topics where each topic is
a distribution over types (words) in the vocabulary.
The mixing coefficients for topics within each docu-
ment and weights for types in each topic can be spec-
ified a priori or learned from a training corpus. Blei
et al initially proposed a variational model (2003)
for learning topics from data. Griffiths and Steyvers
(2004) later developed a Markov chain Monte Carlo
approach based on collapsed Gibbs sampling.
In this model, the mixing weights for topics within
each document and the multinomial coefficients for
terms within each topic are hidden (latent) and must
be learned from a training corpus. Blei et al (2003)
proposed LDA as a general Bayesian framework and
gave a variational model for learning topics from
data. Griffiths and Steyvers (2004) subsequently de-
veloped a stochastic learning algorithm based on col-
lapsed Gibbs sampling. In this paper we will focus
on the Gibbs sampling approach.
2.1 Generative Document Model
The LDA algorithm models the D documents in a
corpus as mixtures of K topics where each topic is
in turn a distribution over W terms. Given ?, the
matrix of mixing weights for topics within each doc-
ument, and?, the matrix of multinomial coefficients
for each topic, we can use this formulation to de-
scribe a generative model for documents (Alg. 1).
Restating the LDA model in linear-algebraic
terms, we can say that the product of ? (theK ?W
column-stochastic topic-by-type matrix) and ? (the
D ? K column-stochastic topic-by-document ma-
trix) is the originalD?W term-by-documentmatrix.
In this sense, LDA computes a matrix factorization
of the term-by-document matrix in the sameway that
LSI or non-negative matrix factorization (NMF) do.
In fact, LDA is a special case of NMF, but unlike in
NMF, there is a unique factorization in LDA. We see
this as a feature recommending LDA above NMF.
Our objective is to reverse the generative model to
learn the contents of ? and ? given a training corpus
D, a number of topics K, and symmetric Dirichlet
prior distributions over both ? and ? with hyperpa-
rameters ? and ?, respectively.
466
for k = 1 toK do
Draw ?k ? Dirichlet(?)
end for
for d = 1 to D do
Draw ? ? Dirichlet(?)
Draw N ? Poisson(?)
for i = 1 to N do
Draw z ? Multinomial(?)
Draw w ? Multinomial(?(z))
end for
end for
Algorithm 1: Generative algorithm for LDA. This will
generate D documents with N tokens each. Each token
is drawn from one of K topics. The distributions over
topics and terms have Dirichlet hyperparameters ? and
? respectively. The Poisson distribution over the token
count may be replaced with any other convenient distri-
bution.
2.2 Learning Topics via Collapsed Gibbs
Sampling
Rather than learn ? and ? directly, we use collapsed
Gibbs sampling (Geman et al (1993), Chatterji and
Pachter (2004)) to learn the latent assignment of to-
kens to topics z given the observed tokens x.
The algorithm operates by repeatedly sampling
each zij from a distribution conditioned on the val-
ues of all other elements of z. This requires main-
taining counts of tokens assigned to topics globally
and within each document. We use the following no-
tation for these sums:
Nijk: Number of tokens of type wi in document dj
assigned to topic k
N?stijk : The sum Nijk with the contribution of token
xst excluded
We indicate summation over all values of an index
with (?).
Given the current state of z the conditional proba-
bility of zij is:
p(zij = k|z?ij , x, d, ?, ?) =
p(xij |?k) p(k|dj) ?
N?iji(?)k + ?
N?ij(?)(?)k + W?
N?ij(?)jk + ?
N(?)j(?) + T?
(1)
As Griffiths and Steyvers (2004) point out, this is
an intuitive result. The first term, p(xij |?k), indi-
cates the importance of term xij in topic k. The sec-
ond term, p(k|dj), indicates the importance of topic
k in document j. The sum of the terms is normalized
implicitly to 1 when we draw each new zij .
We sample a new value for zij for every token xij
during each iteration of Gibbs sampling. We run the
sampler for a burn-in period of a few hundred itera-
tions to allow it to reach its converged state and then
estimate ? and ? from z as follows:
?jk =
N(?)jk + ?
N(?)j(?) + T?
(2)
?ki =
Ni(?)k + ?
N(?)(?)k + W?
(3)
2.3 Classifying New Documents
In LSI, new documents not in the original training
set can be ?projected? into the semantic space of the
training set. The equivalent process in LDA is one
of classification: given a corpus D? of one or more
new documents we use the existing topics ? to com-
pute a maximum a posteriori estimate of the mixing
coefficients ??. This follows the same Monte Carlo
process of repeatedly resampling a set of token-to-
topic assignments z? for the tokens x? in the new doc-
uments. These new tokens are used to compute the
first term p(k|dj) in Eq. 1. We re-use the topic as-
signments z from the training corpus to compute the
second term p(xij |?k). Tokens with new types that
were not present in the vocabulary of the training
corpus do not participate in classification.
The resulting distribution ?? essentially encodes
how likely each new document is to relate to each of
the K topics. We can use this matrix to compute
pairwise similarities between any two documents
from either corpus (training or newly-classified).
Whereas in LSI it may make sense to compute sim-
ilarity between documents using the cosine met-
ric (since the ?dimensions? defining the space are
orthogonal), we compute similarities in LDA us-
ing either the symmetrized Kullback-Leibler (KL)
or Jensen-Shannon (JS) divergences (Kullback and
Leibler (1951), Lin (2002)) since these are methods
of measuring the similarity between probability dis-
tributions.
467
3 Term Weighting Schemes and LDA
The standard approach presented above assumes, ef-
fectively, that each token is equally important in cal-
culating the conditional probabilities. From both an
information-theoretic and a linguistic point of view,
however, it is clear that this is not the case. In En-
glish, a term such as ?the? which occurs with high
frequency in many documents does not contribute as
much to the meaning of each document as a lower-
frequency term such as ?corpus?. It is an axiom of
information theory that an event a?s information con-
tent (in bits) is equal to log2 1p(a) = ? log2 p(a).
Treating tokens as events, we can say that the in-
formation content of a particular token of type t is
? log2 p(t). Furthermore, as is well-known, we can
estimate p(t) from observed frequencies in a corpus:
it is simply the number of tokens of type t in the cor-
pus, divided by the total number of tokens in the cor-
pus. For high-probability terms such as ?the?, there-
fore, ? log2 p(t) is low. Our basic hypothesis is that
recalculating p(zij |z, x, ?, ?) to take the information
content of each token into account will improve the
results of LDA. Specifically, we have incorporated
a weighting term into Eq. 1 by replacing the counts
denoted N with weights denotedM .
p(zij = k|z?ij , x, d, ?, ?) ?
M?iji(?)k + ?
M?ij(?)(?)k + W?
M?ij(?)jk + ?
M(?)j(?) + T?
(4)
Here Mijk is the total weight of tokens of type i
in document j assigned to topic k instead of the total
number of tokens. All of the machinery for Gibbs
sampling and the estimation of ? and ? from z re-
mains unchanged.
We appeal to an urn model to explain the intuition
behind this approach. In the original LDA formula-
tion, each topic ? can be modeled as an urn contain-
ing a large number of balls of uniform size. Each
ball assumes one ofW different colors (one color for
each term in the vocabulary). The frequency of oc-
currence of each color in the urn is proportional to the
corresponding term?s weight in topic ?. We incor-
porate a term weighting scheme by making the size
of each ball proportional to the weight of its corre-
sponding term. This makes the probability of draw-
ing the ball for a termw proportional to both the term
weightm(w) and its multinomial weight ?w:
p(w|?, ?,m) = ?
w m(w)
?
w?W m(w)
(5)
We can now expand Eq. 4 to obtain a new sampling
equation for use with the Gibbs sampler.
p(zij = k|z?ij , x,d,m, ?, ?) =
m(xi)N?iji(?)k + ?
?
w m(w)N
?ij
w(?)k + W?
?
w m(w)N
?ij
wjk + ?
?
w m(w)Nwj(?) + T?
(6)
If all weightsm(w) = 1 this reduces immediately
to the standard LDA formulation in Eq. 1.
The information measure we describe above is
constant for a particular term across the entire cor-
pus, but it is possible to conceive of other, more so-
phisticated weighting schemes as well, for example
those where term weights vary by document. Point-
wise mutual information (PMI) is one such weight-
ing scheme which has a solid basis in information
theory and has been shown to work well in the con-
text of LSI (Chew et al, 2010). According to PMI,
the weight of a given term w in a given document
d is the pointwise mutual information of the term
and document, or? log2
p(w|d)
p(w) . Extending the LDA
model to accommodate PMI is straightforward. We
replace m(xi) and m(w) in Eq. 4 with m(xi, d) as
follows.
m(xi, d) = ? log2
p(xi|d)
p(xi)
= ? log2
#[tokens of type xi in d]
#[tokens of type xi]
(7)
It is possible for PMI of a term within a document
to be negative. When this happens, we clamp the
weight of the offending term to zero in that docu-
ment. In practice, we observe this only with com-
mon words (e.g. ?and?, ?in?, ?of?, ?that?, ?the? and
?to? in English) that are assigned very lowweight ev-
erywhere else in the corpus. This clamping does not
noticeably affect the results.
In the next sections, we describe tests which have
enabled us to evaluate empirically which of these
formulations works best in practice.
468
4 Testing Framework
In this paper, we chose to test our hypotheses with
the same cross-language retrieval task used in a num-
ber of previous studies of LSI (e.g. Chew and Abde-
lali (2007)). Briefly, the task is to train an IR model
on one particular multilingual corpus, then deploy
it on a separate multilingual corpus, using a docu-
ment in one language to retrieve related documents
in other languages. This task is difficult because of
the size of the datasets involved. Its usefulness be-
comes apparent when we consider the following two
use cases: a humanwishing (1) to use a search engine
to retrieve relevant documents in many languages re-
gardless of the language in which the query is posed;
or (2) to produce a clustering or visualization of doc-
uments according to their topics even when the doc-
uments are in different languages.
The training corpus consists of the text of the Bible
in 31,226 parallel chunks, corresponding generally
to verses, in Arabic, English, French, Russian and
Spanish. These data were obtained from the Un-
bound Bible project (Biola University (2006)). The
test data, obtained from http://www.kuran.gen.
tr/, is the text of the Quran in the same 5 languages,
in 114 parallel chunks corresponding to suras (chap-
ters). The task, in short, is to use the training data
to inform whatever linguistic, semantic, or statistical
model is being tested, and then to infer characteris-
tics of the test data in such a way that the test docu-
ments can automatically be matched with their trans-
lations in other languages. Though the documents
come from a specific domain (scriptural texts), what
is of interest is comparative results using different
weighting schemes, holding the datasets and other
settings constant. The training and test datasets are
large enough to allow statistically significant obser-
vations to be made, and if a significant difference is
observed between experiments using two settings, it
is to be expected that similar basic differences would
be observed with any other set of training and test
data. In any case, it should be noted that the Bible
and Quran were written centuries apart, and in differ-
ent original languages; we believe this contributes
to a clean separation of training and test data, and
makes for a non-trivial retrieval task.
In our framework, a term-by-document matrix is
formed from the Bible as a parallel verse-aligned
corpus. We employed two different approaches
to tokenization, one (word-based tokenization) in
which text was tokenized at every non-word char-
acter, and the other (unsupervised morpheme-based
tokenization) in which after word-based tokeniza-
tion, a further pre-processing step (based on Gold-
smith (2001)) was performed to add extra breaks at
everymorpheme. It is shown elsewhere (Chew et al,
2010) that this step leads to improved performance
with LSI. In each verse, all languages are concate-
nated together, allowing terms (either morphemes or
words) from all languages to be represented in every
verse. Cross-language homographs such as ?mien?
in English and French are treated as distinct terms
in our framework. Thus, if there are L languages,
D documents (each of which is translated into each
of the L languages), andW distinct linguistic terms
across all languages, then the term-by-document ma-
trix is of dimensionsW byD (notW byD?L); with
the Bible as a training corpus, the actual numbers in
our case are 160,345? 31,226. As described in Sec.
2.2, we use this matrix as the input to a collapsed
Gibbs sampling algorithm to learn the latent assign-
ment of tokens in all five languages to language-
independent topics, as well as the latent assignment
of language-independent topics to the multilingual
(parallel) documents. In general, we specified, arbi-
trarily but consistently across all tests, that the num-
ber of topics to be learned should be 200. Other pa-
rameters for the Gibbs sampler held constant were
the number of iterations for burn-in (200) and the
number of iterations for sampling (1).
To evaluate our different approaches to weighting,
we use classification as described in Sec. 2.3 to ob-
tain, for each document from the Quran test corpus,
a probability distribution across the topics learned
from the Bible. While in training we have D multi-
lingual documents, in testing we haveD? ?L docu-
ments, each in a specific language, for which a distri-
bution is computed. For theQuran data, this amounts
to 114 ? 5 = 570 documents. This is because our
goal is to match documents with their translations
in other languages using just the probability distri-
butions. For each source-language/target-language
pair L1 and L2, we obtain the similarity of each of
the 114 documents in L1 to each of the 114 doc-
uments in L2. We found that similarity here is
best computed using the Jensen-Shannon divergence
469
Tokenization
Weighting Scheme Word Morpheme
Unweighted 0.505 0.544
log p(w|L) 0.616 0.641
PMI 0.612 0.686
Table 1: Summary of comparison results. This table
shows the average precision at one document (P1) for
each of the tokenization and weighting schemes we eval-
uated. Detailed results are presented in Table 2.
(Lin, 2002) and so this measure was used in all
tests. Ultimately, the measure of how well a partic-
ular method performs is average precision at 1 doc-
ument (P1). Among the various measurements for
evaluating the performance of IR systems (Salton
and McGill (1983), van Rijsbergen (1979)), this is
a fairly standard measure. For a particular source-
target pair, this is the percentage (out of 114 cases)
where a document in L1 is most similar to its mate
in L2. With 5 languages, there are 25 source-target
pairs, and we can also calculate average P1 across
all language pairs. Here, we average across 114 ?
25 (or 2,850) cases. This is why even small differ-
ences in P1 can be statistically significant.
5 Results
First, we present a summary of our results in Table 1
which clearly demonstrates that it is better in LDA to
use some kind of weighting scheme rather than the
uniform weights in the standard LDA formulation
from Eq. 1. This is true whether tokenization is by
word or by morpheme. All increases from the base-
line precision at 1 document (0.505 and 0.544 re-
spectively), whether under log or PMIweighting, are
highly significant (p < 10?11). Furthermore, all in-
creases in precision when moving from word-based
to morphology-based tokenization are also highly
significant (p < 5 ? 10?5 without weighting, p <
5?10?3 with log-weighting, and p< 2?10?15 with
PMI weighting). The best result overall, where P1 is
0.686, is obtained with morphological tokenization
and PMI weighting (parallel to the results in (Chew
et al, 2010) with LSI), and again the difference be-
tween this result and its nearest competitor of 0.641
is highly significant (p < 3 ? 10?6). We return to
comment below on lack of an increase in P1 when
moving from log-weighting to PMI-weighting under
word-based tokenization.
These results can also be broken out by language
pair, as shown in Table 2. Here, it is apparent that
Arabic, and to a lesser extent Russian, are harder lan-
guages in the IR problem at hand. Our intuition is
that this is connected with the fact that these two lan-
guages have a more complex morphological struc-
ture: words are formed by a process of agglutination.
A consequence of this is that single Arabic and Rus-
sian tokens can less frequently be mapped to single
tokens in other languages, which appears to ?con-
fuse? LDA (and also, as we have found, LSI). The
complex morphology of Russian and Arabic is also
reflected in the type-token ratios for each language:
in our English Bible, there are 12,335 types (unique
words) and 789,744 tokens, a type-token ratio of
0.0156. The ratios for French, Spanish, Russian and
Arabic are 0.0251, 0.0404, 0.0843 and 0.1256 re-
spectively. Though the differences may not be ex-
plicable in purely statistical terms (there may be lin-
guistic factors at play which cannot be reduced to
statistics), it seems plausible that choosing a subop-
timal term-weighting scheme could exacerbate any
intrinsic problems of statistical imbalance. Consid-
ering this, it is interesting to note that the greatest
gains, when moving from unweighted LDA to ei-
ther form of weighted LDA, are often to be found
where Russian and/or Arabic are involved. This, to
us, shows the value of using a multilingual dataset
as a testbed for our different formulations of LDA:
it allows problems which may not be apparent when
working with a monolingual dataset to come more
easily to light.
We have mentioned that the best results are with
PMI and morphological tokenization, and also that
there is an increase in precision for many language of
the pairs when morphological (as opposed to word-
based) tokenization is employed. To us, the results
leave little doubt that both weighting and morpho-
logical tokenization are independently beneficial. It
appears, though, that morphology and weighting are
also complementary and synergistic strategies for
improving the results of LDA: for example, a subop-
timal approach in tokenization may at best place an
upper bound on the overall precision achievable, and
perhaps at worst undo the benefits of a good weight-
ing scheme. This may explain the one apparently
anomalous result, which is the lack of an increase in
470
Original Words Morphological Tokenization
EN ES RU AR FR EN ES RU AR FR
LDA
EN 1.000 0.500 0.447 0.132 0.816 1.000 0.500 0.658 0.211 0.640 EN
ES 0.649 1.000 0.307 0.175 0.781 0.605 1.000 0.482 0.175 0.737 ES
RU 0.430 0.316 1.000 0.149 0.430 0.553 0.421 1.000 0.272 0.553 RU
AR 0.070 0.149 0.114 1.000 0.096 0.123 0.105 0.228 1.000 0.114 AR
FR 0.781 0.693 0.421 0.175 1.000 0.693 0.640 0.667 0.211 1.000 FR
Log-WLDA
EN 1.000 0.518 0.518 0.228 0.658 1.000 0.675 0.561 0.219 0.754 EN
ES 0.558 1.000 0.605 0.254 0.763 0.711 1.000 0.570 0.289 0.860 ES
RU 0.605 0.615 1.000 0.298 0.702 0.684 0.667 1.000 0.289 0.728 RU
AR 0.404 0.430 0.526 1.000 0.439 0.430 0.439 0.535 1.000 0.404 AR
FR 0.667 0.667 0.658 0.281 1.000 0.711 0.667 0.561 0.289 1.000 FR
PMI-WLDA
EN 1.000 0.579 0.658 0.272 0.702 1.000 0.719 0.658 0.342 0.851 EN
ES 0.596 1.000 0.623 0.246 0.693 0.816 1.000 0.675 0.272 0.798 ES
RU 0.649 0.579 1.000 0.307 0.693 0.702 0.693 1.000 0.360 0.772 RU
AR 0.351 0.368 0.421 1.000 0.351 0.456 0.474 0.509 1.000 0.377 AR
FR 0.693 0.667 0.605 0.254 1.000 0.825 0.772 0.719 0.333 1.000 FR
Table 2: Full results for precision at one document for all combinations of LDA, Log-WLDA, PMI-WLDA, word
tokenization and morphological tokenization.
precision moving from log-WLDA to PMI-WLDA
under word-based tokenization: if word-based tok-
enization is suboptimal, PMI weighting cannot com-
pensate for that. Effectively, for best results, the
right strategies have to be pursued with respect both
to morphology and to weighting.
Finally, we can illustrate the differences between
weighted and unweighted LDA in another way. As
discussed earlier, each topic in LDA is a probabil-
ity distribution over terms. For each topic, we can
list the most probable terms in decreasing order of
probability; this gives a sense of what each topic
is ?about? and whether the groupings of terms ap-
pear reasonable. Since we use 200 topics, an ex-
haustive listing is impractical here, but in Table 3
we present some representative examples from un-
weighted LDA and PMI-WLDA that we judged to
be of interest. It appears to us that the groupings are
not perfect under either LDA or PMI-WLDA; under
both methods, we find examples of rather heteroge-
neous topics, whereas we would like each topic to be
semantically focused. Still, a comparison of the out-
put with LDA and PMI-WLDA sheds some light on
why PMI-WLDA makes it less necessary to remove
stopwords. Note that all words listed for the top two
topics under LDA would commonly be considered
stopwords. This might also be true of the words in
topic 1 for PMI-WLDA, but in the latter case, the
topic is actually one of themost semantically focused
in that the top words have a clear semantic connec-
tion to one another. This cannot be said of topics 1
and 2 in LDA. For one thing, many of the same terms
that appear in topic 1 reappear in topic 2, making the
two topics hard to distinguish from one another. Sec-
ondly, the terms have only a loose semantic connec-
tion to one another: ?the?, ?and?, and ?of? are all high-
frequency and likely to co-occur, but they are differ-
ent parts of speech and have very different functions
in English. One might say that topics 1 and 2 in LDA
are a rag-bag of high-frequency words, and it is un-
surprising that these topics do little to help charac-
terize documents in our cross-language IR task. The
same cannot be said of any of the top 5 topics in PMI-
WLDA.We believe this illustrates well, and at a fun-
damental level, why weighted forms of LDA work
better in practice than unweighted LDA.
6 Conclusion
We have conducted a series of experiments to evalu-
ate the effect of different weighting schemes on La-
tent Dirichlet Allocation. Our results demonstrate,
perhaps contrary to the conventional wisdom that
weighting is unnecessary in LDA, that weighting
schemes (and other pre-processing strategies) simi-
471
Weighting Scheme
LDA (no weighting) PMI-WLDA
Topic 1 2 3 4 5 1 2 3 4 5
Terms
the the vanit? as c?rcel under city coeur sat col?re
et de vanidad comme prison sous ville heart assis ira
and et vanity como ????? ??? ciudad coraz?n vent wrath
los of ???? ??? prison ??? ?????? ?????? wind anger
? and ????? un ??????? debajo ????? ?????? viento furor
y y aflicci?n a prisonniers ombre twelve ???? sentado ????
les de poursuite one ??????? bases douze ??? ????? fureur
? ? ?????? ??? bound basas doce ???? ????? ???
de la pr?dicateur une prisi?n sombra ???? ???? sitting ?????
of la ???? ???? prisoners dessous ?????? ??????? ??? contre
Table 3: Top 10 terms within top 5 topics for each of LDA and PMI-WLDA. Terms that appear twice within the same
topic (e.g. ?la? in LDA topic 2) are words from different languages with the same spelling (here Spanish and French).
lar to those commonly employed in other approaches
to IR (such as LSI) can significantly improve the
performance of a system. Our approach also runs
counter to the standard position in LDA that it is
necessary or desirable to remove stopwords as a pre-
processing step, and we have presented an alterna-
tive approach of applying an appropriate weighting
scheme within LDA. This approach is preferable be-
cause it is considerably less ad-hoc than the construc-
tion of stoplists. We have shown mathematically
how alternative weighting schemes can be incorpo-
rated into the Gibbs sampling model. We have also
demonstrated that, far from being arbitrary, the in-
troduction of weighting into the LDA model has a
solid and rational basis in information and probabil-
ity theory, just as the basic LDA model itself has.
In future work, we would like to explore further
enhancements to weighting in LDA. There are many
variants which can be considered: one example is
the incorporation of word order and context through
an n-gram model based on conditional probabilities.
We also aim to evaluate LDA against LSIwith a view
to establishingwhether one can be said to outperform
the other consistently in terms of precision, with ap-
propriate settings held constant. Finally, we would
like to determine whether other techniques which
have been shown to benefit LSI can also be usefully
brought to bear in LDA, just as we have shown here
in the case of term weighting.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research 3, pages 993?1022.
Sourav Chatterji and Lior Pachter. 2004. Multiple Or-
ganism Gene Finding by Collapsed Gibbs Sampling.
In RECOMB ?04: Proceedings of the eighth annual in-
ternational conference on Research in computational
molecular biology, pages 187?193, New York, NY,
USA. ACM.
Peter A. Chew and Ahmed Abdelali. 2007. Bene-
fits of the ?Massively Parallel Rosetta Stone?: Cross-
Language Information Retrieval with Over 30 Lan-
guages. In Association for Computational Linguistics,
editor, Proceedings of the 45th meeting of the Associ-
ation of Computational Linguistics, pages 872?879.
Peter A. Chew, Brett W. Bader, Stephen Helmreich,
Ahmed Abdelali, and Stephen J. Verzi. 2010.
An Information-Theoretic, Vector-Space-Model Ap-
proach to Cross-Language Information Retrieval.
Journal of Natural Language Engineering. Forthcom-
ing.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit Versus
Latent Concept Models for Cross-Language Informa-
tion Retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artificial Intelligence,
pages 1513?1518.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Susan T. Dumais. 1991. Improving the Retrieval of In-
formation from External Sources. Behavior Research
Methods, Instruments and Computers, 23(2):229?236.
472
Stuart Geman, Donald Geman, K. Abend, T. J. Harley,
and L. N. Kanal. 1993. Stochastic Relaxation, Gibbs
Distributions and the Bayesian Restoration of Images*.
Journal of Applied Statistics, 20(5):25?62.
J. Goldsmith. 2001. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics, 27(2):153?198.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the Na-
tional Academy of Sciences USA, volume 101, pages
5228?5235.
Thomas Hofmann. 1999. Probablistic Latent Semantic
Indexing. In Proceedings of the 22nd Annual Interna-
tional SIGIR Conference, pages 53?57.
Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. Annals of Mathematical
Statistics, 22:49?86.
J. Lin. 2002. DivergenceMeasures based on the Shannon
Entropy. IEEE Transactions on Information Theory,
37(1):145?151, August.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining Multilingual Topics from Wikipedia. In
18th International World Wide Web Conference, pages
1155?1155, April.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2008. Clustering the
Tagged Web. In Second ACM International Confer-
ence on Web Search and Data Mining (WSDM 2009),
November.
G. Salton and M. McGill, editors. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
Biola University. 2006. The Unbound Bible.
http://www.unboundbible.com.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann.
473
