Proceedings of the Workshop on BioNLP, pages 144?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Two Biomedical Text Genres for Disease Recognition 
 
 
Aur?lie N?v?ol, Won Kim, W. John Wilbur, Zhiyong Lu* 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.gov 
 
  
 
 
Abstract 
In the framework of contextual information 
retrieval in the biomedical domain, this paper 
reports on the automatic detection of disease 
concepts in two genres of biomedical text: 
sentences from the literature and PubMed user 
queries. A statistical model and a Natural 
Language Processing algorithm for disease 
recognition were applied on both corpora. 
While both methods show good performance 
(F=77% vs. F=76%) on the sentence corpus, 
results on the query corpus indicate that the 
statistical model is more robust (F=74% vs. 
F=70%).  
1 Introduction 
Contextual Information Retrieval (IR) is making 
use of additional information or assumptions about 
the users? needs beyond the obvious intent of the 
query. IR systems need to go beyond the task of 
providing generally relevant information by assist-
ing users in finding information that is relevant to 
them and their specific needs at the time of the 
search. A practical example of a Google contextual 
IR feature is when the search engine returns a map 
showing restaurant locations to a user entering a 
query such as ?Paris restaurants.? 
The contextual aspects of a user?s search were 
defined for example by Saracevic (1997) who dis-
cussed integrating the cognitive, affective, and sit-
uational levels of human computer interaction in 
IR systems. Other research efforts studied users? 
search behavior based on their level of domain 
knowledge (Zhang et al, 2005) or aimed at  mod-
eling users? interests and search habits (Rose and 
Levinson, 2004; Teevan et al, 2005).  
Information about the search context may be 
sought explicitly from the user through profiling or 
relevance feedback (Shen et al, 2005). Recent 
work also exploited query log analysis and basic 
computer environment information (Wen et al 
2004), which involve no explicit interaction with 
the user. In adaptive information retrieval, context 
information is inferred based on query analysis and 
collection characteristics (Bai and Nie 2008).  
In the biomedical domain, a need for contextual 
information retrieval was identified in particular 
for clinical queries submitted to PubMed (Pratt and 
Wasserman, 2000). Building on the idea that a spe-
cific type of document is required for searches with 
a ?clinical? context, the PubMed Clinical Queries 
portal was developed (Haynes and Wilczynski, 
2004). A perhaps more prominent contextual fea-
ture of PubMed is the ?citation sensor?, which 
identifies queries classified by Rose and Levinson 
as reflecting a ?Navigational? or ?Obtain resource? 
goal. For example, the citation sensor will identify 
and retrieve a specific citation if the user enters the 
article title as the query. The analysis of Entrez 
logs shows that MEDLINE is the most popular 
database among the 30 or so databases maintained 
by the National Center for Biotechnology Informa-
tion (NCBI) as it receives most of Entrez traffic. 
This suggests that there is a need to complement 
the information retrieved from MEDLINE by giv-
ing contextual access to other NCBI resources re-
144
levant to users? queries, such as Entrez Gene, Clin-
ical Q&A or BookShelf. In addition, the NLM es-
timated that about 1/3 of PubMed users are not 
biomedical professionals. In this light, providing 
an access point to consumer information such as 
the Genetics Home Reference might also be useful. 
To achieve this, the sensor project was recently 
launched with the goal of recognizing a variety of 
biomedical concepts (e.g. gene, protein and drug 
names) in PubMed queries. These high-level con-
cepts will help characterize users? search context in 
order to provide them with information related to 
their need beyond PubMed. For instance, if a user 
query contains the drug name ?Lipitor?, it will be 
recognized by the drug sensor and additional in-
formation on this drug from Clinical Q&A will be 
shown in the side bar in addition to default 
PubMed results. Since disease names are common 
in PubMed queries, the goal of this work is to in-
vestigate and benchmark computational techniques 
for automatic disease name recognition as an aid to 
implementing PubMed search contexts. 
2 Related Work 
Despite a significant body of literature in biomedi-
cal named entity recognition, most work has been 
focused on gene, protein, drug and chemical names 
through challenges such as BioCreAtIvE1 or the 
TREC Genomics/Chemical tracks (Park and Kim, 
2006). Other work addressed the identification of 
?medical problems? in clinical text (Aronson et al 
2007; Meystre and Haug, 2005). This task was the 
topic of a Medical NLP challenge2, which released 
a corpus of anonymized radiography reports anno-
tated with ICD9 codes. Although there is some 
interest in the biomedical community in the identi-
fication of disease names and more specifically the 
identification of relationships between diseases and 
genes or proteins (Rindflesh and Fizman, 2003), 
there are very few resources available to train or 
evaluate automatic disease recognition systems. To 
the best of our knowledge, the only publicly avail-
able corpus for disease identification in the litera-
ture was developed by Jimeno et al (2008). The 
authors annotated 551 MEDLINE sentences with 
UMLS concepts and used this dataset to bench-
mark three different automatic methods for disease 
name recognition. A MEDLINE corpus annotated 
                                                        
1 http://biocreative.sourceforge.net/ 
2 http://www.computationalmedicine.org/challenge/index.php 
with ?malignancy? mentions and part-of-speech 
tags is also available (Jin et al 2006). This corpus 
is targeted to a very restricted type of diseases. The 
annotations are also domain specific, so that ?can-
cer of the lung? is not considered a malignancy 
mention but a mention of malignancy and a men-
tion of malignancy location. 
As in previous studies, we aim to investigate the 
complexity of automatic disease recognition using 
state-of-the-art computational techniques. This 
work is novel in at least three aspects: first, in ad-
dition to using the MEDLINE sentence corpus 
(Jimeno et al2008), we developed a new corpus 
comprising disease annotations on 500 randomly 
selected PubMed queries. This allowed us to inves-
tigate the influence of local context3 through the 
comparison of system performance between two 
different genres of biomedical text. Second, by 
using a knowledge based tool previously ben-
chmarked on the same MEDLINE corpus (Jimeno 
et al 2008), we show that significant performance 
differences can be observed when parameters are 
adjusted. Finally, a state-of-the-art statistical ap-
proach was adapted for disease name recognition 
and evaluated on both corpora.  
3 Two Biomedical Corpora with disease 
annotations 
The first issue in the development of such a corpus 
is to define the very concept of disease. Among the 
numerous terminological resources available, such 
as Medical Subject Headings (MeSH?, 4,354 dis-
ease concepts) or the International Classification of 
Diseases (ICD9, ~18,000 disease concepts), the 
UMLS Metathesaurus? is the most comprehensive: 
the 2008AB release includes 252,284 concepts in 
the disorder Semantic Group defined by McCray 
et al (2001). The UMLS Metathesaurus is part of 
the Semantic Network, which also includes a set of 
broad subject categories, or Semantic Types, that 
provide a consistent categorization of all concepts 
represented in the Metathesaurus. The Semantic 
Groups aim at providing an even broader categori-
zation for UMLS concepts. For example, the dis-
order Semantic Group comprises 12 Semantic 
Types including Disease or Syndrome, Cell or Mo-
lecular Dysfunction and Congenital Abnormalities.  
                                                        
3 Here, by context, we mean the information surrounding a 
disease mention available in the corpora. This is different from 
the ?search context? previously discussed.   
145
Furthermore, like the gene mention (Morgan et 
al. 2008) and gene normalization (Smith et al 
2008) tasks in BioCreative II, the task of disease 
name recognition can also be performed at two 
different levels: 
 
1. disease mention: the detection of a snippet 
of text that refers to a disease concept (e.g. 
?alzheimer? in the sample query shown in 
Table 2)  
2. disease concept: the recognition of a con-
trolled vocabulary disease concept (e.g. 
?C0002395-alzheimer?s disease? in our Ta-
ble 2 example) in text.  
 
In this work, we evaluate and report system per-
formance at the concept level. 
3.1 Biomedical literature corpus 
Sentence Kniest dysplasia is a moderately 
severe chondrodysplasia pheno-
type that results from mutations 
in the gene for type ii collagen 
col2a1.  
Annotations C0265279-Kniest dysplasia 
C0343284-Chondrodysplasia, 
unspecified 
Table 1: Excerpt of literature corpus (PMID: 7874117) 
 
The corpus made available by Jimeno et al con-
sists of 551 MEDLINE sentences annotated with 
UMLS concepts or concept clusters: concepts that 
were found to be linked to the same term. For ex-
ample, the concepts ?Pancreatic carcinoma? 
(C0235974) and ?Malignant neoplasm of pan-
creas? (C0346647) share the same synonym ?Pan-
creas Cancer?, thus they were clustered. The 
sentences were selected from a set of articles cu-
rated for Online Mendelian Inheritance in Man 
(OMIM) and contain an average of 27(+/- 11) to-
kens, where tokens are defined as sequences of 
characters separated by white space. A set of 
UMLS concepts (or clusters) is associated with 
each sentence in the corpus. However, no boun-
dary information linking a phrase in a sentence to 
an annotation was available. Table 1 shows a sam-
ple sentence and its annotations. 
 
 
 
3.2 Biomedical query corpus 
A total of 500 PubMed queries were randomly se-
lected and divided into two batches of 300 and 200 
queries, respectively. Queries were on average 
3.45(+/- 2.64) tokens long in the 300 query batch 
and 3.58(+/- 4.63) for the 200 query batch, which 
is consistent with the average length of PubMed 
queries (3 tokens) reported by Herskovic et al 
(2007).  
The queries in the first set were annotated using 
Knowtator (Ogren, 2006) by three annotators with 
different backgrounds (one biologist, one informa-
tion scientist, one computational linguist). Two 
annotators annotated the queries using UMLS con-
cepts from the disorder group, while the other an-
notator simply annotated diseases without 
reference to UMLS concepts. Table 2 shows a 
sample query and its annotations. A consensus set 
was obtained after a meeting between the annota-
tors where diverging annotations were discussed 
and annotators agreed on a final, unique, version of 
all annotations.  The consensus set contains 89 dis-
ease concepts (76 unique). 
 
Query alzheimer csf amyloid 
Annotations  Ann. 1: ?alzheimer?; 0-8;  
Ann. 2, 3: ?alzheimer?; 0-8; 
C0002395-alzheimer?s disease 
Table 2: Excerpt of annotated 300-query corpus. Boun-
dary information is given as the character interval of the 
annotated string in the query (here, 0-8). 
 
The queries in the second set were annotated 
with UMLS concepts from the disorder group by 
one of the annotators who also worked on the pre-
vious set. In this set, 53 disease concepts were an-
notated (51 unique). 
4 Automatic disease recognition 
With the perspective of a contextual IR applica-
tion where the disease concepts found in queries 
will be used to refer users to disease-specific in-
formation in databases other than MEDLINE, we 
are concerned with high precision performance. 
For this reason, we decided to experiment with 
methods that showed the highest precision when 
compared to others. In addition, given the size of 
the corpora available and the type of the annota-
146
tions, machine learning methods such as CRFs or 
SVM did not seem applicable.  
Table 3 shows a description of the training and 
test sets for each corpus. 
 
 Table 3: Description of the training and test sets 
4.1 Natural Language Processing 
Disease recognition was performed using the Natu-
ral Language Processing algorithm implemented in 
MetaMap (Aronson, 2001)4. The tool was re-
stricted to retrieve concepts from the disorder 
group, using the UMLS 2008AB release and 
?longest match? feature. 
In practice, MetaMap parses the input text into 
noun phrases, generates variants of these phrases 
using knowledge sources such as the SPECIALIST 
lexicon, and maps the phrases to UMLS concepts.  
4.2 Priority Model 
The priority model was first introduced in (Tanabe 
and Wilbur, 2006) and is adapted here to detect 
disease mentions in free text. Because our evalua-
tion is performed at the concept level, the mentions 
extracted by the model are then mapped to UMLS 
using MetaMap.  
The priority model approach is based on two sets 
of phrases: one names of diseases, D, and one 
names of non-diseases, N. One trains the model to 
assign two numbers, p and q, to each token t that 
appears in a phrase in either D or N. Roughly, p is 
the probability that a phrase from D or N that has 
the token t in it is actually from D and q is the rela-
tive weight that should be assigned to t for this 
purpose and represents a quality estimate. Given a 
phrase 
                                                        
4 Additional information is also available at 
http://metamap.nlm.nih.gov/ 
 
1 2 kph t t t?
    (1) 
and for each it  the corresponding numbers ip  and 
iq  we estimate the probability that ph D  by 
 
1 22 11 1k kkj i i jij j iprob p q q p q  
(2) 
 
The training procedure for the model actually 
chooses the values of all the p and q quantities to 
optimize the 
prob
 values over all of D and N.  
For this work we have extended the approach to 
include a quantity  
21 1 22 11 1k kkj i i jij j iqual q p q q p q prob
(3) 
 
which represents a weighted average of all the 
quality numbers iq . We apply this formula to ob-
tain 
qual
as long as 
0.5.prob
 If 
0.5prob
we 
replace all numbers 
ip  by 1 ip  in (2) and (3) to 
obtain 
qual
.  
For this application we obtained the sets D and 
N from the SEMCAT data (Tanabe, Thom et al 
2006) supplemented with the latest UMLS data. 
We removed any term from D and N that contained 
less than five characters in order to decrease the 
occurrence of ambiguous terms.  Also the 1,000 
most frequent terms from D were examined ma-
nually and the ambiguous ones were removed. The 
end result is a set of 332,984 phrases in D and 
4,253,758 phrases in N. We trained the priority 
model on D and N and applied the resulting train-
ing to compute for each phrase in D and N a vector 
of values 
,prob qual
. In this way D and N are 
converted to 
DV  and NV . We then constructed a 
Mahalanobis classifier (Duda, Hart and Stork, 
2001) for two dimensional vectors as the differ-
ence in the Mahalanobis distance of any such vec-
tor to Gaussian approximations to 
DV  and NV .  We 
refer to the number produced by this classifier as 
the Mahalanobis score.  By randomly dividing both 
D and N into three equal size pieces and training 
on two from each and testing on the third, in a 
three-fold cross validation we found the Mahala-
nobis classifier to perform at 98.4% average preci-
sion and 93.9% precision-recall breakeven point. 
In a final step we applied a simple regression me-
thod to estimate the probability that a given Maha-
Data Lit. Corpus Query Corpus 
Training 276 sentences 
(487 disease con-
cepts, 185 unique) 
300 queries (89 
disease concepts, 
76 unique) 
Testing 275 sentences 
(437 disease con-
cepts, 185 unique) 
200 queries (53 
disease concepts, 
51 unique) 
All 551 sentences 
(924 disease con-
cepts, 280 unique) 
500 queries (142 
disease concepts, 
120 unique) 
147
lanobis score was produced by a phrase belonging 
to D and not N. Given a phrase phr we will denote 
this final probability produced as PMA(phr).  
The second important ingredient of our statistic-
al process is how we produce phrases from a piece 
of text. Given a string of text TX we apply tokeni-
zation to TX to produce an ordered set of tokens 
1 2, , , nt t t?
. Among the tokens produced will be 
punctuation marks and stop words and we denote 
the set of all such tokens by Z . We call a token 
segment 
, ,j kt t?
 maximal if it contains no ele-
ment of Z  and if either 1j  or 
1jt Z
 and 
likewise if k n  or 
1kt Z
. Given text TX we 
will denote the set of all maximal token segments 
produced in this way by 
max ( ).S TX
 Now given a 
maximal token segment mts=
, ,j kt t?
 we define 
two different methods of finding phrases in mts. 
The first assumes we are given an arbitrary set of 
phrases PH.  We recursively define a set of phrases 
,I mts PH
 beginning with this set empty and 
with the parameter 
u j
.  Each iteration consists 
of asking for the largest v k  for which 
, ,u vt t PH?
. If there is such a v  we add 
, ,u vt t?
 to 
,I mts PH
 and set 
1u v
. 
Otherwise we set 
1u u
. We repeat this process 
as long as u k .  The second approach assumes 
we are given an arbitrary set of two token phrases 
P2.  Again we recursively define a set of phrases 
, 2J mts P
 beginning with this set empty and 
with the parameter 
u j
. Each iteration consists 
of asking for the largest v k  for which given any 
,  i u i v
,  
1, 2i it t P
. If there is such a v  
we add 
, ,u vt t?
 to 
, 2J mts P
 and set 
1u v
. Otherwise we set 
1u u
. We repeat 
this process as long as u k .   
In order to apply our phrase extraction proce-
dures we need good sets of phrases. In addition to 
D and N already defined above, we use another set 
of phrases defined as follows. Let R denote the set 
of all token strings with two or more tokens which 
do not contain tokens from Z and for which there 
are at least three MEDLINE records (title and ab-
stract text only) in which the token string is re-
peated at least twice. 
We then define
R R D N
. We make 
use of R  in addition to D and N. For the set 2P  
we take the set of all two token phrases in 
MEDLINE documents for which the two tokens 
co-occur as this phrase much more than expected, 
i.e., with a 
2 10,000
(based on the two-by-two 
contingency table).  
 
 
#Initialization: Given a text TX, set 
maxS S TX
 and .X  
#Processing: While(S ){ 
  I. select mts S  
  II. If( ,I mts D ) ,K I mts D 
       else if( ,I mts R ) ,K I mts R  
        else if( ,I mts N ) K  
        else 
if( , 2J mts P ) , 2K J mts P 
        else K  
  III. X X K  
  IV. S S mts  
     } 
#Return: All pairs , ,  phr PMA phr phr X 
 
Figure 1: Phrase finding algorithm 
 
With these preliminaries, our phrase finding al-
gorithm in pseudo-code is shown in Figure 1. 
The output of this algorithm may then be filtered 
by setting a threshold on the PMA values to accept. 
5 Results  
5.1 Assessing the difficulty of the task 
To assess the difficulty of disease recognition, we 
computed the inter-annotator agreement (IAA) on 
the 300-query corpus. Agreement was computed at 
the disease mention level for all three annotators 
and at the disease concept level for the two annota-
tors who produced UMLS annotations.  
Inter-annotator agreement measures for NLP 
applications have been recently discussed by 
Artstein and Poesio (2008) who advocate for the 
use of chance corrected measures. However, in our 
case, agreement was partly computed on a very 
large set of categories (UMLS concepts) so we 
decided to use Knowtator?s built-in feature, which 
computes IAA as the percentage of agreement and 
148
allows partial string matches. For example, in the 
query ?dog model transient ischemic attacks?, an-
notator 1 selected ?ischemic attacks? as a disorder 
while annotator 2 and 3 selected ?transient ischem-
ic attacks? as UMLS concept C0007787: Attacks, 
Transient Ischemic. In this case, at the subclass 
level (?disorder?) we have a match for this annota-
tion. But at the exact span or exact category level, 
there is no match. Table 4 shows details of IAA at 
the disease mention level when partial matches are 
taken into account. For exact span matches, the 
IAA is lower, at 64.87% on average. 
 
Disorder IAA Ann. 1 Ann. 2 Ann. 3 
Ann. 1 100% 71.77% 75.86% 
Ann. 2  100% 71.68% 
Ann. 3   100% 
Table 4: Agreement on disease mention annotations 
(partial match allowed) ? average is 73.10% 
 
At the concept level, the agreement (when par-
tial matches were allowed) varied significantly 
depending on the semantic types. It ranged be-
tween 33% for Findings and 83% for Mental or 
Behavioral Dysfunction. However, agreement on 
the most frequent category, Disease or Syndrome, 
was 72%, which is close to the annotators? overall 
agreement at the mention level. One major cause 
of disagreement was ambiguity caused by concepts 
that were clustered by Jimeno et al For example, 
in query ?osteoporosis and ?fracture pattern?, an-
notator 2 marked ?osteoporosis? with both 
?C0029456-osteoporosis?(a Disease or Syndrome 
concept) and ?C1962963-osteoporosis adverse 
event?(a Finding concept) while annotator 3 only 
used ?C0029456-osteoporosis?.    
5.2 Results on Literature corpus 
As shown in Table 3, the corpus was randomly 
split into a training set (276 sentences) and a test 
set (275 sentences). The training set was used to 
determine the optimal probability threshold for the 
Priority Model and parameter selection for Meta-
Map, respectively. 
 
Priority Model parameter adjustments: the first 
result observed from applying the Priority Model 
was that D yielded about 90% of the output of the 
algorithm. Also results coming from R  and 2P  
were not well mapped to UMLS concepts by Me-
taMap. As a result, in this work we ignored disease 
candidates retrieved based on R  and 2P . The best 
F-measure was obtained for a threshold of 0.3, 
which was consequently used on the test set.  
Since the Priority Model algorithm does not per-
form any mapping to a controlled vocabulary 
source, the mapping was performed by applying 
MetaMap to the snippets of text returned with a 
probability value above the threshold. 
 
Threshold P R F 
0 64 73 67 
.1 67 73 70 
.2 67 73 70 
.3 68 73 71 
.4 68 73 70 
.5 68 72 69 
.6 68 72 69 
.7 68 72 69 
.8 68 68 68 
.9 65 60 62 
Table 5: Precision (P), Recall (R) and F-measure of the 
Priority Model on the training set for different values of 
the probability threshold. 
 
The results presented in Table 5 were obtained 
before any MetaMap adjustments were made.  
 
MetaMap parameter adjustments: an error anal-
ysis was performed to adjust MetaMap settings. 
Errors fell into the following categories:  
 A more specific disease should have been 
recognized (e.g. ?deficiency? vs. ?C2 defi-
ciency?) 
 The definition of a cluster was lacking 
(e.g. ?G6PD deficiency? comprised 
C0237987- Glucose-6-phosphate dehydro-
genase deficiency anemia and C0017758- 
Glucosphosphate Dehydrogenase Defi-
ciency but not C0017920- Deficiency of 
glucose-6-phosphatase)  
 MetaMap mapping was erroneous (e.g. 
?hereditary breast? was mapped to 
C0729233-Dissecting aneurysm of the 
thoracic aorta instead of C0346153-
Hereditary Breast Cancer)  
 
The results of inter-annotator agreement and fur-
ther study of MetaMap mappings indicated that 
concepts with the semantic type Findings seemed 
149
to be frequently retrieved erroneously. For this rea-
son, we also experimented not taking Findings into 
account as an additional adjustment for MetaMap. 
Table 6 shows the results of applying the MetaMap 
adjustments yielded from the error analysis on the 
training corpus. 
 
Threshold Findings P R F 
.3 Yes 80 78 79 
.3 No 85 78 81 
Table 6: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments?      
 
MetaMap disorder detection was also performed 
directly on the training corpus. An error analysis 
similar to what was presented above was carried 
out to determine the best parameters. Table 7 be-
low shows the results obtained when all concepts 
from the 12 Semantic Types (STs) in the disorder 
group are taken into account with no adjustments 
(?raw?). Then, results including the adjustments 
from the error analysis are shown when all 12 STs 
are taken into account, when Findings are excluded 
(11STs) and when only the most frequent 6STs in 
the training set are taken into account.    
 
Processing P R F 
Raw (12 STs) 50 77 61 
Adjusted (12 STs) 52 75 61 
Adjusted (11 STs) 57 73 64 
Adjusted (6 STs) 77 72 74 
Table 7: Performance of MetaMap on the training set      
 
Finally, Table 8 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 80 74 77 
MetaMap 75 78 76 
Table 8: Precision (P), Recall (R) and F-measure of the 
Priority Model and MetaMap on the test set     
5.3 Results on Query Corpus 
The 300-query corpus was used as a training set 
and the 200-query corpus was used as a test set. 
For consistency with work on the literature corpus, 
we assessed the disease recognition on a gold stan-
dard set including ?clusters? of UMLS concepts 
were appropriate. As previously with the Literature 
corpus, we used the training set to determine the 
best settings for each method. The performance of 
the Priority Model at different values of the proba-
bility threshold, based on the use of D and N as the 
sets of sample phrases is similar to that obtained 
with the literature corpus; 0.3 stands out as one of 
the three values for which the best F-measure is 
obtained (tied with .5 and .8).  
Because of the brevity of queries vs. sentences, 
the MetaMap error analysis was very succinct and 
resulted in:  
 Removal of C0011860-Diabetes mellitus 
type 2  as mapping for ?diabetes? 
 Removal of all occurrences of C0600688-
Toxicity and C0424653-Weight symptom 
(finding)  
 Adjustment on the number of STs taken in-
to account 
 
The difference in performance obtained on the 
training set for the different MetaMap adjustments 
considered is shown in Table 9 when MetaMap 
was applied to Priority Model output and in Table 
10 when it was applied directly on the queries.    
 
Threshold Findings P R F 
.3 Yes 60 72 65 
.3 No 73 70 71 
Table 9: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments? 
 
Processing P R F 
Raw (12 STs) 41 82 55 
Adjusted (12 STs) 44 82 57 
Adjusted (11 STs) 58 81 68 
Adjusted (6 STs) 64 75 69 
Table 10: performance of MetaMap on the training set 
 
Finally, Table 11 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 76 72 74 
MetaMap 66 74 70 
Table 11: Precision (P), Recall (R) and F-measure of 
the Priority Model and MetaMap on the test set 
150
6 Discussion 
Comparing the Two Methods. The performance 
of both methods on the query corpus is comparable 
to inter-annotator agreement (F=70-74 vs. IAA=72 
on Disease and Syndromes). On both corpora, the 
Priority Model achieves higher precision and F-
measure, while MetaMap achieves better recall.  
Comparing the results obtained with MetaMap 
with those reported by Jimeno et al, precision is 
lower, but recall is much higher. This is likely to 
be due to the different MetaMap settings, and the 
use of different UMLS versions - Jimeno et al did 
not provide any of this information, but based on 
the publication date of their paper, it is likely that 
they used one of the 2006 UMLS releases. Meystre 
and Haug (2006) also found that significant per-
formance differences could be obtained with Me-
taMap by adjusting the content of the knowledge 
sources used.   
On both text genres, 0.3 was found to be the op-
timal probability threshold for the Priority Model. 
Based on the performance at different values of the 
threshold, it seems that the model is quite efficient 
at ruling out highly unlikely diseases. However, for 
values above .3 the performance does not vary 
greatly.  
 
Comparing Text Genres. For both methods, 
disease recognition seems more efficient on sen-
tences. This is to be expected: sentences provide 
more context (e.g. more tokens surrounding the 
disease mention are available) and allow for more 
efficient disambiguation, for example on acro-
nyms. Although acronyms are frequent both in 
queries and sentences, more undefined acronyms 
are found in queries. However, the difference in 
performance between the two methods seems 
higher on the query corpus. This indicates that the 
Priority Model could be more robust to sparse con-
text.  
It should be noted that there were diseases in all 
sentences in the literature corpus vs. about 1/3 to 
1/2 of the queries. In addition, the query corpus 
included many author names, which could create 
confusion with disease names (in particular for the 
Priority Model). This difficulty was not found in 
the sentence corpus. However, sentences some-
times contain negated mention of diseases, which 
never occurred in the query corpus where little to 
no syntax is used.  
We also noticed that while Findings seemed to 
be generally problematic concepts in both corpora, 
other concepts such as Injury and Poisoning were 
much more prevalent in the query corpus. For this 
reason, for the general task of disease recognition, 
a drastic restriction to as little as 6 STs is probably 
not advisable.  
 
Limitations of the study. One limitation of our 
study is the relatively small number of disease 
concepts in the query corpus. Although the query 
and sentence corpus contain about 500 que-
ries/sentences each, there are significantly less dis-
ease concepts found in queries compared to 
sentences. As a result, there is also less repetition 
in the disease concept found. This is partly due to 
the brevity of queries compared to sentences but 
mainly to the fact that while all the sentences in the 
literature corpus had at least one disease concept, 
this was not the case for the query corpus. We are 
currently addressing this issue with the ongoing 
development of a large scale query corpus anno-
tated for diseases and other relevant biomedical 
entities.  
7 Conclusions 
We found that of the two steps of disease recogni-
tion, disease mention gets the higher inter-
annotator agreement (vs. concept mapping). We 
have applied a statistical and an NLP method for 
the automatic recognition of disease concepts in 
two genres of biomedical text. While both methods 
show good performance (F=77% vs. F=76%) on 
the sentence corpus, results indicate that the statis-
tical model is more robust on the query corpus 
where very little disease context information is 
available (F=74% vs. F=70%). As a result, the 
priority model will be used for disease detection in 
PubMed queries in order to characterize users? 
search contexts for contextual IR. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The authors would like to thank S. 
Shooshan and T. Tao for their contribution to the 
annotation of the query corpus; colleagues in the 
NCBI engineering branch for their valuable feed-
back at every step of the project.   
151
References  
Alan R. Aronson, Olivier Bodenreider, Dina Demner-
Fushman, Kin Wah Fung, Vivan E. Lee, James G. 
Mork et al 2007. From Indexing the Biomedical Li-
terature to Coding Clinical Text: Experience with 
MTI and Machine Learning Approaches. ACL 
Workshop BioNLP.  
Alan Aronson. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proceedings of AMIA Symp:17-21. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics 34(4): 555-596 
Jing Bai, and Jian-Yun Nie. 2008. Adapting information 
retrieval to query contexts. Information Processing & 
Management. 44(6):1902-22 
Robert O. Duda, Peter. E. Hart and David G. Stork. 
2001. Pattern Classification. New York: John Wiley 
& Sons, Inc. 
R. Brian Haynes and Nancy L. Wilczynski. 2004. Op-
timal search strategies for retrieving scientifically 
strong studies of diagnosis from Medline: analytical 
survey. BMJ. 328(7447):1040. 
Jorge R. Herskovic, Len Y. Tanaka, William Hersh and 
Elmer V. Bernstam. 2007. A day in the life of 
PubMed: analysis of a typical day's query log. Jour-
nal of the American Medical Informatics Association. 
14(2):212-20. 
Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee, 
Sylvain Gaudan, Rafael Berlanga and Dietrich 
Rebholz-Schuhmann. 2008. Assessment of disease 
named entity recognition on a corpus of annotated 
sentences. BMC Bioinformatics. 11;9 Suppl 3:S3. 
Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A 
Mandel, Steven Carroll, Mark Y Liberman et al 
2006. Automated recognition of malignancy men-
tions in biomedical literature. BMC Bioinformatics.  
7:492. 
Alexa T. McCray, Anita Burgun and Olivier Bodenreid-
er. 2001. Aggregating UMLS semantic types for 
reducing conceptual complexity. Proceedings of 
Medinfo 10(Pt 1):216-20. 
St?phane Meystre and Peter J. Haug. 2006. Natural lan-
guage processing to extract medical problems from 
electronic clinical documents: performance evalua-
tion. J Biomed Inform. 39(6):589-99. 
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, 
Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al 
2008. Overview of BioCreative II gene normaliza-
tion. Genome Biol. 9 Suppl 2:S3. 
Phillip V. Ogren. 2006. Knowtator: A plug-in for creat-
ing training and evaluation data sets for Biomedical 
Natural Language systems. 9th Intl. Prot?g? Confe-
rence  
Jong C. Park and Jung-Jae Kim. 2006. Named Entity 
Recognition. In S. Ananiadou and J. McNaught 
(Eds), Text Mining for Biology and Biomedicine (pp. 
121-42). Boston|London:Artech House Inc.  
Wanda Pratt and Henry Wasserman. 2000. QueryCat: 
automatic categorization of MEDLINE queries. Pro-
ceedings of AMIA Symp:655-9.  
Tom C. Rindflesh and Marcelo Fiszman. 2003. The 
interaction of domain knowledge and linguistic struc-
ture in natural language processing: interpreting 
hypernymic propositions in biomedical text. J Bio-
med Inform. 36(6):462-77 
Daniel E. Rose and Danny Levinson. 2004. Understand-
ing user goals in web search. In Proceedings of the 
13th international Conference on World Wide 
Web:13-9  
Tefko Saracevic. 1997. The Stratified Model of Infor-
mation Retrieval Interaction: Extension and Applica-
tion. Proceedings of the 60th meeting of the. 
American Society for Information Science:313-27 
Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005 
Context-sensitive information retrieval using impli-
cit feedback, In Proceedings of the 28th annual in-
ternational conference ACM SIGIR conference on 
Research and development in information retrieval: 
43-50.  
Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-
Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al 2008. 
Overview of BioCreative II gene mention recogni-
tion. Genome Biol. 9 Suppl 2:S2. 
Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten, 
Donald C. Comeau and W. John Wilbur. 2006. 
SemCat: semantically categorized entities for ge-
nomics. Proceedings of AMIA Symp: 754-8. 
Laurraine K. Tanabe and W. John Wilbur. 2006. A 
Priority Model for Named Entities. Proceedings of 
HLT-NAACL BioNLP Workshop:33-40 
Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005. 
Personalizing search via automated analysis of in-
terests and activities. In Proceeding of ACM-
SIGIR?05:449?56. 
Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilis-
tic model for contextual retrieval. Proceedings of 
ACM-SIGIR?04:57?63 
Xiangmin Zhang, Hermina G.B. Anghelescu and Xiao-
jun Yuan. 2005. Domain knowledge, search beha-
vior, and search effectiveness of engineering and 
science students: An exploratory study, Information 
Research 10(2): 217. 
152
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 155?163,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Text Mining Techniques for Leveraging Positively Labeled Data 
Lana Yeganova*, Donald C. Comeau, Won Kim, W. John Wilbur 
National Center for Biotechnology Information, NLM, NIH, Bethesda, MD 20894 USA 
{yeganova, comeau, wonkim, wilbur}@mail.nih.gov 
* Corresponding author. Tel.:+1 301 402 0776 
Abstract 
Suppose we have a large collection of 
documents most of which are unlabeled. Suppose 
further that we have a small subset of these 
documents which represent a particular class of 
documents we are interested in, i.e. these are 
labeled as positive examples. We may have reason 
to believe that there are more of these positive 
class documents in our large unlabeled collection. 
What data mining techniques could help us find 
these unlabeled positive examples? Here we 
examine machine learning strategies designed to 
solve this problem. We find that a proper choice of 
machine learning method as well as training 
strategies can give substantial improvement in 
retrieving, from the large collection, data enriched 
with positive examples. We illustrate the principles 
with a real example consisting of multiword 
UMLS phrases among a much larger collection of 
phrases from Medline. 
 
1 Introduction 
Given a large collection of documents, a few of 
which are labeled as interesting, our task is to 
identify unlabeled documents that are also 
interesting. Since the labeled data represents the 
data we are interested in, we will refer to it as the 
positive class and to the remainder of the data as 
the negative class. We use the term negative class, 
however, documents in the negative class are not 
necessarily negative, they are simply unlabeled and 
the negative class may contain documents relevant 
to the topic of interest. Our goal is to retrieve these 
unknown relevant documents. 
A na?ve approach to this problem would simply 
take the positive examples as the positive class and 
the rest of the collection as the negative class and 
apply machine learning to learn the difference and 
rank the negative class based on the resulting 
scores. It is reasonable to expect that the top of this 
ranking would be enriched for the positive class. 
But an appropriate choice of methods can improve 
over the na?ve approach.  
One issue of importance would be choosing the 
most appropriate machine learning method. Our 
problem can be viewed from two different 
perspectives: the problem of learning from 
imbalanced data as well as the problem of 
recommender systems. In terms of learning from 
imbalanced data, our positive class is significantly 
smaller than the negative, which is the remainder 
of the collection. Therefore we are learning from 
imbalanced data. Our problem is also a 
recommender problem in that based on a few 
examples found of interest to a customer we seek 
similar positive examples amongst a large 
collection of unknown status. Our bias is to use 
some form of wide margin classifier for our 
problem as such classifiers have given good 
performance for both the imbalanced data problem 
and the recommender problem (Zhang and Iyengar 
2002; Abkani, Kwek et al 2004; Lewis, Yang et 
al. 2004).  
Imbalanced data sets arise very frequently in 
text classification problems. The issue with 
imbalanced learning is that the large prevalence of 
negative documents dominates the decision 
process and harms classification performance. 
Several approaches have been proposed to deal 
with the problem including sampling methods and 
cost-sensitive learning methods and are described 
in (Chawla, Bowyer et al 2002; Maloof 2003; 
Weiss, McCarthy et al 2007). These studies have 
shown that there is no clear advantage of one 
approach versus another. Elkan (2001) points out 
that cost-sensitive methods and sampling methods 
are related in the sense that altering the class 
distribution of training data is equivalent to 
altering misclassification cost. Based on these 
studies we examine cost-sensitive learning in 
which the cost on the positive set is increased, as a 
useful approach to consider when using an SVM.  
In order to show how cost-sensitive learning for 
an SVM is formulated, we write the standard 
equations for an SVM following (Zhang 2004). 
155
Given training data ? ?? ?,i ix y  where iy  is 1 or ?1 
depending on whether the data point 
ix  is 
classified as positive or negative, an SVM seeks 
that vector 
iw  which minimizes  
? ? 2( )     (1)2i ii h y x w w??? ? ??
 
 
where the loss function is defined by  
? ? 1 ,  1           (2) 0,         z>1.
z zh z ? ??? ??
 
The cost-sensitive version modifies (1) to become  
 
 
and now we can choose r?  and r?  to magnify the 
losses appropriately. Generally we take r?  to be 1, 
and r?  to be some factor larger than 1. We refer to 
this formulation as CS-SVM. Generally, the same 
algorithms used to minimize (1) can be used to 
minimize (3). 
Recommender systems use historical data on 
user preferences, purchases and other available 
data to predict items of interest to a user. Zhang 
and Iyengar (2002) propose a wide margin 
classifier with a quadratic loss function as very 
effective for this purpose (see appendix). It is used 
in (1) and requires no adjustment in cost between 
positive and negative examples. It is proposed as a 
better method than varying costs because it does 
not require searching for the optimal cost 
relationship between positive and negative 
examples. We will use for our wide margin 
classifier the modified Huber loss function (Zhang 
2004).  The modified Huber loss function is 
quadratic where this is important and has the form  
? ? ? ?2
4 ,     1
                    (4)1 ,  -1 1
0,     z>1.
z z
h z z z
? ? ? ??
?
? ? ? ??
?
?
 
We also use it in (1). We refer to this approach as 
the Huber method (Zhang 2004) as opposed to 
SVM. We compare it with SVM and CS-SVM. We 
used our own implementations for SVM, CS-SVM, 
and Huber that use gradient descent to optimize the 
objective function. 
The methods we develop are related to semi-
supervised learning approaches (Blum and 
Mitchell 1998; Nigam, McCallum et al 1999) and 
active learning (Roy and McCallum 2001; Tong 
and Koller 2001). Our method differs from active 
learning in that active learning seeks those 
unlabeled examples for which labels prove most 
informative in improving the classifier. Typically 
these examples are the most uncertain. Some semi-
supervised learning approaches start with labeled 
examples and iteratively seek unlabeled examples 
closest to already labeled data and impute the 
known label to the nearby unlabeled examples. Our 
goal is simply to retrieve plausible members for the 
positive class with as high a precision as possible. 
Our method has value even in cases where human 
review of retrieved examples is necessary. The 
imbalanced nature of the data and the presence of 
positives in the negative class make this a 
challenging problem. 
In Section 2 we discuss additional strategies 
proposed in this work, describe the data used and 
design of experiments, and provide the evaluation 
measure used. In Section 3 we present our results, 
in Sections 4 and 5 we discuss our approach and 
draw conclusions.  
 
2 Methods 
2.1 Cross Training 
Let D  represent our set of documents, and C?  
those documents that are known positives in D . 
Generally C?  would be a small fraction of D  and 
for the purposes of learning we assume that 
\C D C? ?? . 
 We are interested in the case when some of the 
negatively labeled documents actually belong to 
the positive class. We will apply machine learning 
to learn the difference between the documents in 
the class C?  and documents in the class C?  and 
use the weights obtained by training to score the 
documents in the negative class C? . The highest 
scoring documents in set C?  are candidate 
mislabeled documents. However, there may be a 
problem with this approach, because the classifier 
is based on partially mislabeled data. Candidate 
? ? ? ? 2( ) ( )  (3)2i i i ii C i Cr h y x w r h y x w w
?? ?
? ?
? ?? ?
? ? ? ? ? ? ? ?? ?
156
mislabeled documents are part of the C?  class. In 
the process of training, the algorithm purposely 
learns to score them low. This effect can be 
magnified by any overtraining that takes place. It 
will also be promoted by a large number of 
features, which makes it more likely that any 
positive point in the negative class is in some 
aspect different from any member of C? . 
Another way to set up the learning is by 
excluding documents from directly participating in 
the training used to score them. We first divide the 
negative set into disjoint pieces 
1 2C Z Z? ? ?  
Then train documents in C?  versus documents in 
1Z  to rank documents in 2Z  and train documents 
in C?  versus documents in 2Z  to rank documents 
in
1Z . We refer to this method as cross training 
(CT). We will apply this approach and show that it 
confers benefit in ranking the false negatives in 
C? .  
2.2 Data Sources and Preparation 
The databases we studied are MeSH25, Reuters, 
20NewsGroups, and MedPhrase. 
MeSH25.   We selected 25 MeSH? terms with 
occurrences covering a wide frequency range: from 
1,000 to 100,000 articles. A detailed explanation of 
MeSH can be found at 
http://www.nlm.nih.gov/mesh/. 
For a given MeSH term m, we treat the records 
assigned that MeSH term m as positive. The 
remaining MEDLINE? records do not have m 
assigned as a MeSH and are treated as negative. 
Any given MeSH term generally appears in a small 
minority of the approximately 20 million MEDLINE 
documents making the data highly imbalanced for 
all MeSH terms.  
Reuters. The data set consists of 21,578 Reuters 
newswire articles in 135 overlapping topic 
categories. We experimented on the 23 most 
populated classes. 
For each of these 23 classes, the articles in the 
class of interest are positive, and the rest of 21,578 
articles are negative. The most populous positive 
class contains 3,987 records, and the least 
populous class contains 112 records.  
 20NewsGroups. The dataset is a collection of 
messages from twenty different newsgroups with 
about one thousand messages in each newsgroup. 
We used each newsgroup as the positive class and 
pooled the remaining nineteen newsgroups as the 
negative class. 
Text in the MeSH25 and Reuters databases has 
been preprocessed as follows: all alphabetic 
characters were lowercased, non-alphanumeric 
characters replaced by blanks, and no stemming 
was done. Features in the MeSH25 dataset are all 
single nonstop terms and all pairs of adjacent 
nonstop terms that are not separated by 
punctuation. Features in the Reuters database are 
single nonstop terms only. Features in the 
20Newsgroups are extracted using the Rainbow 
toolbox (McCallum 1996).  
MedPhrase. We process MEDLINE to extract all 
multiword UMLS? 
(http://www.nlm.nih.gov/research/umls/) phrases 
that are present in MEDLINE. From the resulting 
set of strings, we drop the strings that contain 
punctuation marks or stop words. The remaining 
strings are normalized (lowercased, redundant 
white space is removed) and duplicates are 
removed. We denote the resulting set of 315,679 
phrases by 
phrasesU .  
For each phrase in ,phrasesU  we randomly 
sample, as available, up to 5 MEDLINE sentences 
containing it. We denote the resulting set of 
728,197 MEDLINE sentences by 
phrasesS . From
phrasesS  we extract all contiguous multiword 
expressions that are not present in 
phrasesU . We 
call them n-grams, where n>1. N-grams containing 
punctuation marks and stop words are removed 
and remaining n-grams are normalized and 
duplicates are dropped. The result is 8,765,444 n-
grams that we refer to as .ngramM  We believe that 
ngramM contains many high quality biological 
phrases. We use 
phrasesU  , a known set of high 
quality biomedical phrases, as the positive class, 
and 
ngramM  
as the negative class. 
In order to apply machine learning we need to 
define features for each n-gram. Given an n-gram 
grm that is composed of n  words,
1 2 ngrm w w w? , we extract a set of 11 numbers 
157
? ?111i if ?  associated with the n-gram grm. These are 
as follows: 
f1: number of occurrences of grm throughout 
Medline; 
f2: -(number of occurrences of w2?wn not 
following w1 in documents that contain grm)/ f1; 
f3: -(number of occurrences of w1?wn-1 not 
preceding wn in documents that contain grm)/ f1; 
f4: number of occurrences of (n+1)-grams of the 
form xw1?wn throughout Medline; 
f5: number of occurrences of (n+1)-grams  of 
the form w1?wn x throughout Medline; 
f6: ? ? ? ?? ?
? ?? ? ? ?
1 2 1 2
1 2 1 2
| 1 |log 1 | |
p w w p w w
p w w p w w
? ?? ?
? ?? ?? ?? ?
 
f7: mutual information between w1 and w2; 
f8: ? ? ? ?? ?
? ?? ? ? ?
1 1
1 1
| 1 |log 1 | |
n n n n
n n n n
p w w p w w
p w w p w w
? ?
? ?
? ?? ?
? ?? ?? ?
 
f9: mutual information between wn-1 and wn; 
f10: -(number of different multiword expressions 
beginning with w1 in Medline); 
f11: -(number of different multiword expressions 
ending with wn in Medline).   
We discretize the numeric values of the ? ?111i if ?  
into categorical values.  
In addition to these features, for every n-gram 
grm, we include the part of speech tags predicted 
by the MedPost tagger (Smith, Rindflesch et al 
2004).  To obtain the tags for a given n-gram grm 
we randomly select a sentence from 
phrasesS  
containing grm, tag the sentence, and consider the 
tags 
0 1 2 1 1n n nt t t t t t? ?  where 0t is the tag of the 
word preceding word 
1w in n-gram grm, 1t  is the 
tag of word 
1w  in n-gram grm, and so on. We 
construct the features  
  
 
These features emphasize the left and right ends of 
the n-gram and include parts-of-speech in the 
middle without marking their position. The 
resulting features are included with ? ?111i if ?  to 
represent the n-gram. 
2.3 Experimental Design  
A standard way to measure the success of a 
classifier is to evaluate its performance on a 
collection of documents that have been previously 
classified as positive or negative. This is usually 
accomplished by randomly dividing up the data 
into training and test portions which are separate. 
The classifier is then trained on the training 
portion, and is tested on test portion. This can be 
done in a cross-validation scheme or by randomly 
re-sampling train and test portions repeatedly.   
We are interested in studying the case where 
only some of the positive documents are labeled. 
We simulate that situation by taking a portion of 
the positive data and including it in the negative 
training set. We refer to that subset of positive 
documents as tracer data (Tr). The tracer data is 
then effectively mislabeled as negative. By 
introducing such an artificial supplement to the 
negative training set we are not only certain that 
the negative set contains mislabeled positive 
examples, but we know exactly which ones they 
are. Our goal is to automatically identify these 
mislabeled documents in the negative set and 
knowing their true labels will allow us to measure 
how successful we are. Our measurements will be 
carried out on the negative class and for this 
purpose it is convenient to write the negative class 
as composed of true negatives and tracer data 
(false negatives) 
'C C Tr? ?? ? . 
 
When we have trained a classifier, we evaluate 
performance by ranking 'C?  and measuring how 
well tracer data is moved to the top ranks. The 
challenge is that Tr appears in the negative class 
and will interact with the training in some way.  
2.4 Evaluation 
We evaluate performance using Mean Average 
Precision (MAP) (Baeza-Yates and Ribeiro-Neto 
1999). The mean average precision is the mean 
value of the average precisions computed for all 
topics in each of the datasets in our study. Average 
precision is the average of the precisions at each 
rank that contains a true positive document. 
 
? ? ? ?? ?? ?? ?
? ? ? ?? ?? ?? ?
0 1 1 2 1
0 1 1
if 2 :  ,1 , , 2 ,3 ,4 , ,...,  
otherwise: ,1 , , 2 ,3 ,4 .
n n
n n
n t t t t t t
t t t t
? ?
?
? ???
??
158
Table 1: MAP scores trained with three levels of tracer data introduced to the negative training set. 
 
No Cross Training No Tracer Data Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM Huber SVM 
celiac disease 0.694 0.677 0.466 0.484 0.472 0.373 
lactose intolerance 0.632 0.635 0.263 0.234 0.266 0.223 
myasthenia gravis 0.779 0.752 0.632 0.602 0.562 0.502 
carotid stenosis 0.466 0.419 0.270 0.245 0.262 0.186 
diabetes mellitus 0.181 0.181 0.160 0.129 0.155 0.102 
rats, wistar 0.241 0.201 0.217 0.168 0.217 0.081 
myocardial infarction 0.617 0.575 0.580 0.537 0.567 0.487 
blood platelets 0.509 0.498 0.453 0.427 0.425 0.342 
serotonin 0.514 0.523 0.462 0.432 0.441 0.332 
state medicine 0.158 0.164 0.146 0.134 0.150 0.092 
urinary bladder 0.366 0.379 0.312 0.285 0.285 0.219 
drosophila melanogaster 0.553 0.503 0.383 0.377 0.375 0.288 
tryptophan 0.487 0.480 0.410 0.376 0.402 0.328 
laparotomy 0.186 0.173 0.138 0.101 0.136 0.066 
crowns 0.520 0.497 0.380 0.365 0.376 0.305 
streptococcus mutans 0.795 0.738 0.306 0.362 0.218 0.306 
infectious mononucleosis 0.622 0.614 0.489 0.476 0.487 0.376 
blood banks 0.283 0.266 0.170 0.153 0.168 0.115 
humeral fractures 0.526 0.495 0.315 0.307 0.289 0.193 
tuberculosis, lymph node 0.385 0.397 0.270 0.239 0.214 0.159 
mentors 0.416 0.420 0.268 0.215 0.257 0.137 
tooth discoloration 0.499 0.499 0.248 0.215 0.199 0.151 
pentazocine 0.710 0.716 0.351 0.264 0.380 0.272 
hepatitis e 0.858 0.862 0.288 0.393 0.194 0.271 
genes, p16 0.278 0.313 0.041 0.067 0.072 0.058 
Avg 0.491 0.479 0.321 0.303 0.303 0.238 
 
3 Results 
3.1 MeSH25, Reuters, and 20NewsGroups 
We begin by presenting results for the MeSH25 
dataset. Table 1 shows the comparison between 
Huber and SVM methods. It also compares the 
performance of the classifiers with different levels 
of tracer data in the negative set. We set aside 50% 
of C?  to be used as tracer data and used the 
remaining 50% of C?  as the positive set for 
training. We describe three experiments where we 
have different levels of tracer data in the negative 
set at training time. These sets are ,C?  20 ,C Tr? ?  
and 
50  C Tr? ? representing no tracer data, 20% of 
C?  as tracer data and 50% of C?  as tracer data, 
respectively. The test set 
20C Tr? ?  is the same for 
all of these experiments. Results indicate that on 
average Huber outperforms SVM on these highly 
imbalanced datasets. We also observe that 
performance of both methods deteriorates with 
increasing levels of tracer data.   
Table 2 shows the performance of Huber and 
SVM methods on negative training sets with tracer 
data 
20C Tr? ?  and 50  C Tr? ? as in Table 1, but 
with cross training. As mentioned in the Methods 
section, we first divide each negative training set 
into two disjoint pieces
1Z  and 2Z . We then train 
documents in the positive training set versus 
documents in 
1Z  to score documents in 2Z  and 
train documents in the positive training set versus  
documents in 
2Z  to score documents in 1Z . We 
then merge 
1Z  and 2Z  as scored sets and report 
measurements on the combined ranked set of 
documents. Comparing with Table 1, we see a 
significant improvement in the MAP when using 
cross training. 
 
159
 
Table 2: MAP scores for Huber and SVM trained with two levels of tracer data introduced to the 
negative training set using cross training technique. 
 
2-fold Cross Training Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM 
celiac disease 0.550 0.552 0.534 0.521 
lactose intolerance 0.415 0.426 0.382 0.393 
myasthenia gravis 0.652 0.643 0.623 0.631 
carotid stenosis 0.262 0.269 0.241 0.241 
diabetes mellitus 0.148 0.147 0.144 0.122 
rats, wistar 0.212 0.186 0.209 0.175 
myocardial infarction 0.565 0.556 0.553 0.544 
blood platelets 0.432 0.435 0.408 0.426 
serotonin 0.435 0.447 0.417 0.437 
state medicine 0.135 0.136 0.133 0.132 
urinary bladder 0.295 0.305 0.278 0.280 
drosophila melanogaster 0.426 0.411 0.383 0.404 
tryptophan 0.405 0.399 0.390 0.391 
laparotomy 0.141 0.128 0.136 0.126 
crowns 0.375 0.376 0.355 0.353 
streptococcus mutans 0.477 0.517 0.448 0.445 
infectious mononucleosis 0.519 0.514 0.496 0.491 
blood banks 0.174 0.169 0.168 0.157 
humeral fractures 0.335 0.335 0.278 0.293 
tuberculosis, lymph node 0.270 0.259 0.262 0.244 
mentors 0.284 0.278 0.275 0.265 
tooth discoloration 0.207 0.225 0.209 0.194 
pentazocine 0.474 0.515 0.495 0.475 
hepatitis e 0.474 0.499 0.482 0.478 
genes, p16 0.102 0.101 0.083 0.093 
Avg 0.350 0.353 0.335 0.332 
 
 
We performed similar experiments with the 
Reuters and 20NewsGroups datasets, where 20%  
and 50% of the good set is used as tracer data. We 
report MAP scores for these datasets in Tables 3 
and 4. 
 
3.2 Identifying high quality biomedical 
phrases in the MEDLINE Database 
We illustrate our findings with a real example 
of detecting high quality biomedical phrases 
among ,ngramM a large collection of multiword 
expressions from Medline. We believe that 
ngramM  
contains many high quality biomedical phrases. 
These examples are the counterpart of the 
mislabeled positive examples (tracer data) in the 
previous tests. 
  
Table 3: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for Reuters dataset. 
 
Reuters 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.478 0.451 0.429 0.403 
2-Fold CT 0.662 0.654 0.565 0.555 
 
Table 4: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for 20NewsGroups dataset. 
 
20News 
Groups 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.492 0.436 0.405 0.350 
2-Fold CT 0.588 0.595 0.502 0.512 
160
To identify these examples, we learn the 
difference between the phrases in 
phrasesU  
 and 
.ngramM  Based on the training we rank the n-grams 
in .ngramM  
We expect the n-grams that cannot be 
separated from UMLS phrases are high quality 
biomedical phrases. In our experiments, we 
perform 3-fold cross validation for training and 
testing. This insures we obtain any possible benefit 
from cross training. The results shown in figure 1 
are MAP values for these 3 folds.  
 
Figure 1. Huber, CS-SVM, and na?ve Bayes 
classifiers applied to the MedPhrase dataset. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We trained na?ve Bayes, Huber, and CS-SVM 
with a range of different cost factors. The results 
are presented in Figure 1. We observe that the 
Huber classifier performs better than na?ve Bayes. 
CS-SVM with the cost factor of 1 (standard SVM) 
is quite ineffective. As we increase the cost factor, 
the performance of CS-SVM improves until it is 
comparable to Huber. We believe that the quality 
of ranking is better when the separation of 
phrasesU  
from 
ngramM  is better.  
Because we have no tracer data we have no 
direct way to evaluate the ranking of .ngramM  
However, we selected a random set of 100 n-grams 
from ,ngramM  which score as high as top-scoring 
10% of phrases in 
phrasesU . Two reviewers 
manually reviewed that list and identified that 99 
of these 100 n-grams were high quality biomedical 
phrases. Examples are: aminoshikimate pathway, 
berberis aristata, dna hybridization, subcellular 
distribution, acetylacetoin synthase, etc. One false-
positive example in that list was congestive heart.  
 
 
4 Discussion 
We observed that the Huber classifier performs 
better than SVM on imbalanced data with no cross 
training (see appendix). The improvement of 
Huber over SVM becomes more marked as the 
percentage of tracer data in the negative training 
set is increased. However, the results also show 
that cross training, using either SVM or Huber 
(which are essentially equivalent), is better than 
using Huber without cross training. This is 
demonstrated in our experiments using the tracer 
data. The results are consistent over the range of 
different data sets. We expect cross training to 
have benefit in actual applications.  
Where does cost-sensitive learning fit into this 
picture? We tested cost-sensitive learning on all of 
our corpora using the tracer data. We observed 
small and inconsistent improvements (data not 
shown). The optimal cost factor varied markedly 
between cases in the same corpus. We could not 
conclude this was a useful approach and instead 
saw better results simply using Huber. This 
conclusion is consistent with (Zhang and Iyengar 
2002) which recommend using a quadratic loss 
function. It is also consistent with results reported 
in (Lewis, Yang et al 2004) where CS-SVM is 
compared with SVM on multiple imbalanced text 
classification problems and no benefit is seen using 
CS-SVM. Others have reported a benefit with CS-
SVM (Abkani, Kwek et al 2004; Eitrich and Lang 
2005). However, their datasets involve relatively 
few features and we believe this is an important 
aspect where cost-sensitive learning has proven 
effective. We hypothesize that this is the case 
because with few features the positive data is more 
likely to be duplicated in the negative set. In our 
case, the MedPhrase dataset involves relatively 
few features (410) and indeed we see a dramatic 
improvement of CS-SVM over SVM. 
One approach to dealing with imbalanced data 
is the artificial generation of positive examples as 
seen with the SMOTE algorithm (Chawla, Bowyer 
et al 2002). We did not try this method and do not 
know if this approach would be beneficial for 
1 11 21 31 41
Cost  Factor r
+
0.20
0.22
0.24
0.26
0.28
A
v
e
r
a
g
e
 
P
r
e
c
i
s
i
o
n
Comparison of different Machine Learning Methods
Huber
CS-SVM
Bayes
161
textual data or data with many features. This is an 
area for possible future research. 
Effective methods for leveraging positively 
labeled data have several potential applications:  
? Given a set of documents discussing a 
particular gene, one may be interested in 
finding other documents that talk about the 
same gene but use an alternate form of the 
gene name.  
? Given a set of documents that are indexed with 
a particular MeSH term, one may want to find 
new documents that are candidates for being 
indexed with the same MeSH term. 
? Given a set of papers that describe a particular 
disease, one may be interested in other 
diseases that exhibit a similar set of symptoms. 
? One may identify incorrectly tagged web 
pages.  
These methods can address both removing 
incorrect labels and adding correct ones. 
 
5 Conclusions 
Given a large set of documents and a small set 
of positively labeled examples, we study how best 
to use this information in finding additional 
positive examples. We examine the SVM and 
Huber classifiers and conclude that the Huber 
classifier provides an advantage over the SVM 
classifier on such imbalanced data. We introduce a 
technique which we term cross training. When this 
technique is applied we find that the SVM and 
Huber classifiers are essentially equivalent and 
superior to applying either method without cross 
training.  We confirm this on three different 
corpora. We also analyze an example where cost-
sensitive learning is effective. We hypothesize that 
with datasets having few features, cost-sensitive 
learning can be beneficial and comparable to using 
the Huber classifier.  
 
Appendix: Why Huber Loss Function works 
better for problems with Unbalanced Class 
Distributions. 
The drawback of the standard SVM for the 
problem with an unbalanced class distribution 
results from the shape of ( )h z  in (2). Consider the 
initial condition at 0w ?  and also imagine that there is 
a lot more C?  training data than C?  training data.  In 
this case, by choosing 1? ? ? , we can achieve the 
minimum value of the loss function in (1) for the initial 
condition 0w ? . Under these conditions, all C?  points 
yield 1z ?  and ( ) 0h z ? and all C?  points yield 
1z ? ?  and ( ) 2h z ? . The change of the loss function 
( )h z?  in (2) with a change w?  is given by 
 
 
 
In order to reduce the loss at a C?  
data point ( , )i ix y , 
we must choose w?  such that 0.ix w?? ?   But we 
assume that there are significantly more C?  class 
data points than C?  and many such points x? are 
mislabeled and close to
ix  such that 0.x w? ?? ?  
Then ( )h z  is likely be increased by ( 0)x w? ?? ?  
for these mislabeled points. Clearly, if there are 
significantly more C?  class data than those of  C?
class and the C? set  contains a lot of mislabeled 
points, it may be difficult to find w?  that can 
result in a net effect of decreasing the right hand 
side of (2). The above analysis shows why the 
standard support vector machine formulation in (2) 
is vulnerable to an unbalanced and noisy training 
data set. The problem is clearly caused by the fact 
that the SVM loss function ( )h z  in (2) has a 
constant slope for 1z ? . In order to alleviate this 
problem, Zhang and Iyengar (2002) proposed the 
loss function 2 ( )h z  which is a smooth non-
increasing function with slope 0 at 1z ? . This 
allows the loss to decrease while the positive 
points move a small distance away from the bulk 
of the negative points and take mislabeled points 
with them. The same argument applies to the 
Huber loss function defined in (4). 
 
Acknowledgments 
This research was supported by the Intramural Research 
Program of the NIH, National Library of Medicine. 
  
? ? ( )      (5).w i idh zh z z w y x wdz? ? ? ?? ? ? ??
162
References  
 
Abkani, R., S. Kwek, et al (2004). Applying Support 
Vector Machines to Imballanced Datasets. ECML. 
  
Baeza-Yates, R. and B. Ribeiro-Neto (1999). Modern 
Information Retrieval. New York, ACM Press. 
  
Blum, A. and T. Mitchell (1998). "Combining Labeled 
and Unlabeled Data with Co-Training." COLT: 
Proceedings of the Workshop on Computational 
Learning Theory: 92-100. 
  
Chawla, N. V., K. W. Bowyer, et al (2002). "SMOTE: 
Synthetic Minority Over-sampling Technique." Journal 
of Artificial Intelligence Research 16: 321-357. 
  
Eitrich, T. and B. Lang (2005). "Efficient optimization 
of support vector machine learning parameters for 
unbalanced datasets." Journal of Computational and 
Applied Mathematics 196(2): 425-436. 
  
Elkan, C. (2001). The Foundations of Cost Sensitive 
Learning. Proceedings of the Seventeenth International 
Joint Conference on Artificial Intelligence. 
  
Lewis, D. D., Y. Yang, et al (2004). "RCV1: A New 
Benchmark Collection for Text Categorization 
Research." Journal of Machine Learning Research 5: 
361-397. 
  
Maloof, M. A. (2003). Learning when data sets are 
imbalanced and when costs are unequal and unknown. 
ICML 2003, Workshop on Imballanced Data Sets. 
  
McCallum, A. K. (1996). "Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. http://www.cs.cmu.edu/~mccallum/bow/." 
  
Nigam, K., A. K. McCallum, et al (1999). "Text 
Classification from Labeled and Unlabeled Documents 
using EM." Machine Learning: 1-34. 
  
Roy, N. and A. McCallum (2001). Toward Optimal 
Active Learning through Sampling Estimation of Error 
Reduction. Eighteenth International Conference on 
Machine Learning. 
  
Smith, L., T. Rindflesch, et al (2004). "MedPost: A part 
of speech tagger for biomedical text." Bioinformatics 
20: 2320-2321. 
  
Tong, S. and D. Koller (2001). "Support vector machine 
active learning with applications to text classification." 
Journal of Machine Learning Research 2: 45-66. 
  
Weiss, G., K. McCarthy, et al (2007). Cost-Sensitive 
Learning vs. Sampling: Which is Best for Handling 
Unbalanced Classes with Unequal Error Costs? 
Proceedings of the 2007 International Conference on 
Data Mining. 
  
Zhang, T. (2004). Solving large scale linear prediction 
problems using stochastic gradient descent algorithms. 
Twenty-first International Conference on Machine 
learning, Omnipress. 
  
Zhang, T. and V. S. Iyengar (2002). "Recommender 
Systems Using Linear Classifiers." Journal of Machine 
Learning Research 2: 313-334. 
  
 
 
163
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 185?192,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Classifying Gene Sentences in Biomedical Literature by 
Combining High-Precision Gene Identifiers 
 
 
Sun Kim, Won Kim, Don Comeau, and W. John Wilbur 
National Center for Biotechnology Information 
National Library of Medicine, National Institutes of Health 
Bethesda, MD 20894, USA 
{sun.kim,won.kim,donald.comeau,john.wilbur}@nih.gov 
 
 
 
 
 
 
Abstract 
Gene name identification is a fundamental 
step to solve more complicated text mining 
problems such as gene normalization and pro-
tein-protein interactions. However, state-of-
the-art name identification methods are not 
yet sufficient for use in a fully automated sys-
tem. In this regard, a relaxed task, 
gene/protein sentence identification, may 
serve more effectively for manually searching 
and browsing biomedical literature. In this pa-
per, we set up a new task, gene/protein sen-
tence classification and propose an ensemble 
approach for addressing this problem. Well-
known named entity tools use similar gold-
standard sets for training and testing, which 
results in relatively poor performance for un-
known sets. We here explore how to combine 
diverse high-precision gene identifiers for 
more robust performance. The experimental 
results show that the proposed approach out-
performs BANNER as a stand-alone classifier 
for newly annotated sets as well as previous 
gold-standard sets. 
1 Introduction 
With the rapidly increasing biomedical literature, 
text mining has become popular for finding bio-
medical information in text. Among others, named 
entity recognition (NER) for bio-entities such as 
genes and proteins is a fundamental task because 
extracting biological relationships begins with enti-
ty identification. However, NER in biomedical 
literature is challenging due to the irregularities 
and ambiguities in bio-entities nomenclature (Yang 
et al, 2008). In particular, compound entity names 
make this problem difficult because it also requires 
deciding word boundaries. 
Recent bio-text competitions such as JNLPBA 
(Kim et al, 2004) and BioCreative (Lu et al, 2011; 
Smith et al, 2008) have evaluated NER systems 
for gene mentions. Even though progress has been 
made in several areas, gene identification methods 
are not yet sufficient for real-world use without 
human interaction (Arighi et al, 2011). Thus, at 
the present, a realistic suggestion is to use these 
algorithms as an aid to human curation and infor-
mation retrieval (Altman et al, 2008). 
In this paper, we define a new task, gene/protein 
sentence classification. A gene or protein sentence 
means a sentence including at least one specific 
gene or protein name. This new task has ad-
vantages over gene mention identification. First, 
gene name boundaries are not important at the sen-
tence level and human judges will agree more in 
their judgments. Second, highlighting gene sen-
tences may be more useful in manual search and 
browsing environments since this can be done 
more accurately and with less distraction from in-
correct annotations. 
To classify gene/protein sentences, we here pro-
pose an ensemble approach to combine different 
NER identifiers. Previous NER approaches are 
mostly developed on a small number of gold-
185
standard sets including GENIA (Kim et al, 2003) 
and BioCreative (Smith et al, 2008) corpora. The-
se sets help to find regular name patterns in a lim-
ited set of articles, but also limit the NER 
performance for real-world use. In the proposed 
approach, we use a Semantic Model and a Priority 
Model along with BANNER (Leaman and 
Gonzalez, 2008). The Semantic and Priority Mod-
els are used to provide more robust performance on 
gene/protein sentence classification because they 
utilize larger resources such as SemCat and Pub-
Med?R  to detect gene names. 
For experiments, we created three new gold-
standard sets to include cases appearing in the most 
recent publications. The experimental results show 
that our approach outperforms machine learning 
classifiers using unigrams and substring features as 
well as stand-alone BANNER classification on five 
gold-standard datasets. 
The paper is organized as follows. In Section 2, 
the ensemble approach for gene/protein sentence 
classification is described. Section 3 explains the 
gold-standard sets used for our experiments. Sec-
tion 4 presents and discusses the experimental re-
sults. Conclusions are drawn in Section 5. 
2 Methods 
 Figure 1. Method Overview. 
 
Figure 1 shows the overall framework for our pro-
posed approach. We basically assume that a main 
NER module works as a strong predictor, i.e., the 
majority of outputs obtained from this module are 
correct. We here use BANNER (Leaman and 
Gonzalez, 2008) as the main NER method because 
it adopts features and methods which are generally 
known to be effective for gene name recognition. 
While BANNER shows good performance on 
well-known gold-standard sets, it suffers from rela-
tively poor performance on unknown examples. To 
overcome this problem, we combine BANNER 
with two other predictors, a Sematic Model and a 
Priority Model. First, the Semantic Model and the 
Priority Model do not use previous gold-standard 
sets for training. Second, these two models learn 
name patterns in different ways, i.e., semantic rela-
tionships for the Semantic Model and positional 
and lexical information for the Priority Model. 
This combination of a strong predictor and two 
weaker but more general predictors can respond 
better to unknown name patterns. 
As described above, the proposed method main-
ly relies on outputs from different NER methods, 
whereas word features can still provide useful evi-
dence for discriminating gene and non-gene sen-
tences. Hence, we alternatively utilize word 
features such as unigrams and substrings along 
with NER features. For NER features only, the 
output is the sum of binary decisions from three 
NER modules. For word and NER features, the 
Huber classifier (Kim and Wilbur, 2011) is trained 
to combine the features. The parameter set in the 
Huber classifier is optimized to show the best clas-
sification performance on test sets. The following 
subsections describe each feature type used for 
gene sentence classification. 
2.1 Word Features 
Unigrams are a set of words obtained from to-
kenizing sentences on white space. All letters in 
unigrams are converted to lower case.  
Substrings are all contiguous substrings of a sen-
tence, sized three to six characters. This substring 
feature may help reduce the difference between 
distributions on training and test sets (Huang et al, 
2008). Substrings encode the roots and morpholo-
gy of words without identifying syllables or stems. 
They also capture neighboring patterns between 
words. 
2.2 BANNER 
BANNER is a freely available tool for identifying 
gene mentions. Due to its open-source policy and 
Java implementation, it has become a popular tool. 
BANNER uses conditional random fields (CRF) 
as a discriminative method and utilizes a set of fea-
ture types that are known to be good for identify-
ing gene names. The feature sets used are 
186
orthographic, morphological and shallow syntax 
features (Leaman and Gonzalez, 2008): 
 
(1) The part of speech (POS) of a token in a sen-
tence. 
(2) The lemma of a word. 
(3) 2, 3 and 4-character prefixes and suffixes. 
(4) 2 and 3 character n-grams including start-of-
token and end-of-token indicators. 
(5) Word patterns by converting upper-case letters, 
lower-case letters and digits to their corresponding 
representative characters (A, a, 0). 
(6) Numeric normalization by converting digits to 
?0?s. 
(7) Roman numerals. 
(8) Names of Greek letters. 
 
Even though BANNER covers most popular 
feature types, it does not apply semantic features or 
other post-processing procedures such as abbrevia-
tion processing. However, these features may not 
have much impact for reducing performance since 
our goal is to classify gene sentences, not gene 
mentions. 
2.3 Semantic Model  
The distributional approach to semantics (Harris, 
1954) has become more useful as computational 
power has increased, and we have found this ap-
proach helpful in the attempt to categorize entities 
found in text. We use a vector space approach to 
modeling semantics (Turney and Pantel, 2010) and 
compute our vectors as described in (Pantel and 
Lin, 2002) except we ignore the actual mutual in-
formation and just include a component of 1 if the 
dependency relation occurs at all for a word, else 
the component is set to 0. We constructed our vec-
tor space from all single tokens (a token must have 
an alphabetic character) throughout the titles and 
abstracts of the records in the whole of the Pub-
Med database based on a snapshot of the database 
taken in January 2012. We included only tokens 
that occurred in the data sufficient to accumulate 
10 or more dependency relations. There were just 
over 750 thousand token types that satisfied this 
condition and are represented in the space. We de-
note this space by h. We then took all the single 
tokens and all head words from multi-token strings 
in the categories ?chemical?, ?disease?, and 
?gene/protein? from an updated version of the  
SemCat database (Tanabe et al, 2006) and placed 
all the other SemCat categories similarly processed 
into a category we called ?other?. We consider on-
ly the tokens in these categories that also occur in 
our semantic vector space h and refer to these sets 
as Chemicalh , Diseaseh , inGene/Proteh , Otherh . Table 1 shows 
the size of overlaps between sets. 
 
 Chemicalh Diseaseh  inGene/Proteh  Otherh
Chemicalh 54478 209 4605 5495 
Diseaseh  8801 1139 169 
inGene/Proteh   76440 9466 
Otherh    127337 
Table 1. Pairwise overlap between sets representing the 
different categories. 
 
Class 'Chemicalh 'Diseaseh  ' inGene/Proteh  'OtherhStrings 49800 7589 70832 113815 
Ave. Prec. 0.8680 0.7060 0.9140 0.9120 
Table 2. Row two contains the number of unique strings 
in the four different semantic classes studied. The last 
row shows the mean average precisions from a 10-fold 
cross validation to learn how to distinguish each class 
from the union of the other three. 
 
In order to remove noise or ambiguity in the 
training set, we removed the tokens that appeared 
in more than one semantic class as follows. 
 ? ?
? ?
? ?
? ?inGene/ProteDiseaseChemicalOther'Other
DiseaseChemicalinGene/Prote
'
inGene/Prote
inGene/ProteChemicalDisease
'
Disease
inGene/ProteDiseaseChemical
'
Chemical
hhhhh
hhhh
hhhh
hhhh
????
???
???
???
       
   (1)
  
We then applied Support Vector Machine learn-
ing to the four resulting disjoint semantic classes in 
a one-against-all strategy to learn how to classify 
into the different classes. We used 31064.1 ??C  
based upon the size of the training set. As a test of 
this process we applied this same learning with 10-
fold cross validation on the training data and the 
results are given in the last row of Table 2. 
This Semantic Model is an efficient and general 
way to identify words indicating gene names. Un-
like other NER approaches, this model decides a 
target class solely based on a single word. Howev-
er, evaluating all tokens from sentences may in-
crease incorrect predictions. A dependency parser 
analyzes a sentence as a set of head- and depend-
187
ent-word combinations. Since gene names likely 
appear in describing a relationship with other enti-
ties, a name indicating a gene mention will be 
mostly placed in a dependent position. Thus, we 
first apply the C&C CCG parser (Curran et al, 
2007), and evaluate words in dependent positions 
only. 
 
2.4 Priority Model 
The Semantic Model detects four different catego-
ries for a single word. However, the Priority Model 
captures gene name patterns by analyzing the order 
of words and the character strings making up 
words. Since gene names are noun phrases in gen-
eral, we parse sentences and identify noun phrases 
first. These phrases are then evaluated using the 
Priority Model. 
The Priority Model is a statistical language 
model for named entity recognition (Tanabe and 
Wilbur, 2006). For named entities, a word to the 
right is more likely to be the word determining the 
nature of the entity than a word to the left in gen-
eral.  
Let T1 be the set of training data for class C1 and 
T2 for class C2. Let ? ? At ???  denote the set of all to-
kens used in names contained in 21 TT ? . For each 
token t?, A?? , it is assumed that there are associ-
ated two probabilities p? and q?, where p? is the 
probability that the appearance of the token t?  in a 
name indicates that name belongs to class C1 and 
q? is the probability that t? is a more reliable indi-
cator of the class of a name than any token to its 
left. Let )()2()1( ktttn ??? ??  be composed of the 
tokens on the right in the given order. Then the 
probability of n belonging to class C1 can be com-
puted as follows. 
 
? ? ? ? ? ?? ??
? ???
????
k
i
k
ij
jii
k
j
j qpqqpnCp
2 1
)()()(
2
)()1(1 11| ?????  (2) 
 
A limited memory BFGS method (Nash and 
Nocedal, 1991) and a variable order Markov model 
(Tanabe and Wilbur, 2006) are used to obtain p?   
and q?. An updated version of SemCat (Tanabe and 
Wilbur, 2006) was used to learn gene names. 
2.5 Semantic and Priority Models for High-
Precision Scores 
The Semantic and Priority Models learn gene 
names and other necessary information from the 
SemCat database, where names are semantically 
categorized based on UMLS?R  (Unified Medical 
Language System) Semantic Network. Even 
though the Semantic and Priority Models show 
good performance on names in SemCat, they can-
not avoid noise obtained from incorrect pre-
processing, e.g., parsing errors. The use of a gen-
eral category for training may also limit perfor-
mance. To obtain high-precision scores for our 
ensemble approach, it is important to reduce the 
number of false positives from predictions. Hence, 
we apply the Semantic and Priority Models on 
training sets, and mark false positive cases. These 
false positives are automatically removed from 
predictions on test sets. These false positive cases 
tend to be terms for entities too general to warrant 
annotation. 
Table 3 shows the classification performance 
with and without false positive corrections on 
training data. For both Semantic and Priority Mod-
els, precision rates are increased by removing false 
positives. Even though recall drops drastically, this 
does not cause a big problem in our setup since 
these models try to detect gene names which are 
not identified by BANNER. 
 
 SEM SEMFP PM PMFP
Accuracy 0.7907 0.7773 0.7805 0.8390 
Precision 0.7755 0.8510 0.7405 1.0000 
Recall 0.8323 0.6852 0.8799 0.6856 
F1 0.8029 0.7592 0.8042 0.8135 
Table 3. Performance changes on training set for the 
Semantic Model (SEM) and the Priority Model (PM). 
FP indicates that learned false positives were removed 
from predictions. 
3 Datasets 
For experiments, we rigorously tested the proposed 
method on gene mention gold-standard sets and 
newly annotated sets. GENETAG (Smith et al, 
2008) is the dataset released for BioCreative I and 
BioCreative II workshops. Since it is well-known 
for a gene mention gold-standard set, we used 
GENETAG as training data. 
For test data, two previous gold-standard sets 
were selected and new test sets were also built for 
gene sentence classification. YAPEX (Franzen et 
al., 2002) and JNLPBA (Kim et al, 2004) are con-
sidered of moderate difficulty because they are 
188
both related to GENIA corpus, a well-known gold-
standard set. However, Disease, Cell Line and 
Reptiles are considered as more difficult tasks be-
cause they represent new areas and contain recent-
ly published articles. The annotation guideline for 
new test sets basically followed those used in 
GENETAG (Tanabe et al, 2005), however do-
mains, complexes, subunits and promoters were 
not included in new sets. 
 
(1) ?Disease? Set: This set of 60 PubMed docu-
ments was obtained from two sources. Fifty of the 
documents were obtained from the 793 PubMed 
documents used to construct the AZDC (Leaman et 
al., 2009). They are the fifty most recent among 
these records. In addition to these fifty documents, 
ten documents were selected from PubMed on the 
topic of maize to add variety to the set and because 
one of the curators who worked with the set had 
experience studying the maize genome.  These ten 
were chosen as recent documents as of early March 
2012 and which contained the text word maize and 
discussed genetics.  The whole set of 60 docu-
ments were annotated by WJW to produce a gold 
standard. 
 
(2) ?CellLine? Set: This set comprised the most 
recent 50 documents satisfying the query ?cell 
line[MeSH]? in PubMed on March 15, 2012. This 
query was used to obtain documents which discuss 
cell lines, but most of these documents also discuss 
genes and for this reason the set was expected to be 
challenging. The set was annotated by WJW and 
DC and after independently annotating the set they 
reconciled differences to produce a final gold 
standard. 
 
(3) ?Reptiles? Set: This set comprised the most 
recent 50 documents satisfying the query ?reptiles 
AND genes [text]? in PubMed on March 15, 2012. 
This set was chosen because it would have little 
about human or model organisms and for this rea-
son it was expected to be challenging.  The set was 
annotated by WJW and DC and after independent-
ly annotating the set they reconciled differences to 
produce a final gold standard. 
 
For both ?CellLine? and ?Reptiles? Sets, the 
most recent data was chosen in an effort to make 
the task more challenging. Presumably such docu-
ments will contain more recently created names 
and phrases that do not appear in the older training 
data. This will then pose a more difficult test for 
NER systems. 
Table 4 shows all datasets used for training and 
testing. The new sets, ?Disease?, ?CellLine? and 
?Reptiles? are also freely available at 
http://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/
IRET/bionlp.zip 
 
 Positives Negatives Total 
GENETAG 10245 9755 20000 
YAPEX 1298 378 1676 
JNLPBA 17761 4641 22402 
Disease 345 251 596 
CellLine 211 217 428 
Reptiles 179 328 507 
Table 4. Datasets. ?GENETAG? was used for training 
data and others were used for test data. ?YAPEX? and 
?JNLPBA? were selected from previous gold-standard 
corpora. ?Disease?, ?Cell Line? and ?Reptiles? are new-
ly created from recent publications and considered as 
difficult sets. 
4 Results and Discussion  
In this paper, our goal is to achieve higher-
prediction performance on a wide range of gene 
sentences by combining multiple gene mention 
identifiers. The basic assumption here is that there 
is a strong predictor that performs well for previ-
ously known gold-standard datasets. For this 
strong predictor, we selected BANNER since it 
includes basic features that are known to give good 
performance. 
 
 Accuracy Precision Recall F1 
GENETAG 0.9794 0.9817 0.9779 0.9799 
YAPEX 0.9051 0.9304 0.9483 0.9392 
JNLPBA 0.8693 0.9349 0.8976 0.9159 
Disease 0.8591 0.9223 0.8261 0.8716 
Cell Line 0.8925 0.9146 0.8626 0.8878 
Reptiles 0.8994 0.8478 0.8715 0.8595 
Table 5. Performance of BANNER on training and test 
datasets. 
 
Table 5 presents the gene sentence classification 
performance of BANNER on training and test sets. 
We emphasize that performance here means that if 
BANNER annotates a gene/protein name in a sen-
tence, that sentence is classified as positive, other-
wise it is classified as negative. BANNER used 
GENETAG as training data, hence it shows excel-
lent classification performance on the same set. 
189
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.9414 0.9491 0.9685 0.9704 0.9624 0.9678 
JNLPBA 0.9512 0.9504 0.9584 0.9651 0.9625 0.9619 
Disease 0.8255 0.8852 0.9238 0.9501 0.9573 0.9610 
CellLine 0.8174 0.9004 0.9281 0.9539 0.9429 0.9496 
Reptiles 0.6684 0.7360 0.8696 0.9049 0.9001 0.8937 
Table 6. Average precision results on test sets for different feature combinations. 
 
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.8735 0.8819 0.9321 0.9196 0.9298 0.9336 
JNLPBA 0.8902 0.8938 0.9111 0.9197 0.9262 0.9264 
Disease 0.7449 0.7884 0.8479 0.8894 0.8957 0.9043 
CellLine 0.7346 0.8057 0.8698 0.9017 0.9052 0.8957 
Reptiles 0.6257 0.6816 0.8499 0.8199 0.8547 0.8547 
Table 7. Breakeven results on test sets for different feature combinations. 
 
 
? Just one fiber gene was revealed in this strain. 
 
? This transcription factor family is characterized by 
a DNA-binding alpha-subunit harboring the Runt 
domain and a secondary subunit, beta, which binds 
to the Runt domain and enhances its interaction 
with DNA.  
  
Figure 2. False positive examples including misleading 
words. 
 
YAPEX and JNLPBA are gold-standard sets that 
partially overlap the GENIA corpus. Since 
BANNER utilizes features from previous research 
on GENETAG, YAPEX and JNLPBA, we expect 
good performance on these data sets. For that rea-
son, we created the three additional gold-standard 
sets to use in this study, and we believe the per-
formance on these sets is more representative of 
what could be expected when our method is ap-
plied to cases recently appearing in the literature. 
Table 6 show average precision results for the 
different methods and all the test sets. GENETAG 
is left out because BANNER is trained on 
GENETAG. We observe improved performance of 
the ensemble methods over unigrams, substrings 
and BANNER. The improvement is small on 
YAPEX and JNLPBA, but larger for Disease, 
CellLine and Reptiles. We see that unigrams and 
substrings tend to add little to the plain ensemble. 
The MAP (Mean Average Precision) values in 
Table 6 are in contrast to the breakeven results in 
Table 7, where we see that unigrams and sub-
strings included with the ensemble generally give 
improved results.  Some of the unigrams and sub-
strings are specific enough to detect gene/protein 
names with high accuracy, and improve precision 
in top ranks in a way that cannot be duplicated by 
the annotations coming from Semantic or Priority 
Models or BANNER. In addition, substrings may 
capture more information than unigrams because 
of their greater generality. 
Some of our errors are due to false positive NER 
identifications. By this we mean a token was clas-
sified as a gene/protein by BANNER or the Se-
mantic or Priority Models. This often happens 
when the name indeed represents a gene/protein 
class, which is too general to be marked positive 
(Figure 2). A general way in which this problem 
could be approached is to process a large amount 
of literature discussing genes or proteins and look 
for names that are marked as positives by one of 
the NER identifiers, and which appear frequently 
in plural form as well as in the singular. Such 
names are likely general class names, and have a 
high probability to be false positives. 
Another type of error will arise when unseen to-
kens are encountered. If such tokens have string 
similarity to gene/protein names already encoun-
tered in the SemCat data, they may be recognized 
by the Priority Model. But there will be completely 
new strings. Then one must rely on context and 
this may not be adequate. We think there is little 
that can be done to solve this short of better lan-
guage understanding by computers. 
There is a benefit in considering whole sentenc-
es as opposed to named entities. By considering 
whole sentences, name boundaries become a non-
issue. For this reason, one can expect training data 
to be more accurate, i.e., human judges will tend to 
agree more in their judgments. This may allow for 
improved training and testing performance of ma-
190
chine learning methods. We believe it beneficial 
that human users are directed to sentences that con-
tain the entities they seek without necessity of 
viewing the less accurate entity specific tagging 
which they may then have to correct. 
5 Conclusions 
We defined a new task for classifying gene/protein 
sentences as an aid to human curation and infor-
mation retrieval. An ensemble approach was used 
to combine three different NER identifiers for im-
proved gene/protein sentence recognition. Our ex-
periments show that one can indeed find improved 
performance over a single NER identifier for this 
task. An additional advantage is that performance 
at this task is significantly more accurate than 
gene/protein NER. We believe this improved accu-
racy may benefit human users of this technology. 
We also make available to the research community 
three gold-standard gene mention sets, and two of 
these are taken from the most recent literature ap-
pearing in PubMed. 
Acknowledgments 
This work was supported by the Intramural Re-
search Program of the National Institutes of 
Health, National Library of Medicine. 
References  
R. B. Altman, C. M. Bergman, J. Blake, C. Blaschke, A. 
Cohen, F. Gannon, L. Grivell, U. Hahn, W. Hersh, L. 
Hirschman, L. J. Jensen, M. Krallinger, B. Mons, S. 
I. O'donoghue, M. C. Peitsch, D. Rebholz-
Schuhmann, H. Shatkay, and A. Valencia. 2008. Text 
mining for biology - the way forward: opinions from 
leading scientists. Genome Biol, 9 Suppl 2:S7. 
C. N. Arighi, Z. Lu, M. Krallinger, K. B. Cohen, W. J. 
Wilbur, A. Valencia, L. Hirschman, and C. H. Wu. 
2011. Overview of the BioCreative III workshop. 
BMC Bioinformatics, 12 Suppl 8:S1. 
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically 
motivated large-scale NLP with C&C and boxer. In 
Proceedings of the 45th Annual Meeting of the ACL 
on Interactive Poster and Demonstration Sessions, 
pages 33-36. 
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, 
and J. Coster. 2002. Protein names and how to find 
them. Int J Med Inform, 67:49-61. 
Z. S. Harris. 1954. Distributional structure. Word, 
10:146-162. 
M. Huang, S. Ding, H. Wang, and X. Zhu. 2008. 
Mining physical protein-protein interactions from the 
literature. Genome Biol, 9 Suppl 2:S12. 
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. 
GENIA corpus - semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1:i180-
i182. 
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. 
Collier. 2004. Introduction to the bio-entity 
recognition task at JNLPBA. In Proceedings of the 
International Joint Workshop on Natural Language 
Processing in Biomedicine and its Applications, 
pages 70-75. 
S. Kim and W. J. Wilbur. 2011. Classifying protein-
protein interaction articles using word and syntactic 
features. BMC Bioinformatics, 12 Suppl 8:S9. 
R. Leaman and G. Gonzalez. 2008. BANNER: an 
executable survey of advances in biomedical named 
entity recognition. In Proceedings of the Pacific 
Symposium on Biocomputing, pages 652-663. 
R. Leaman, C. Miller, and G. Gonzalez. 2009. Enabling 
recognition of diseases in biomedical text with 
machine learning: corpus and benchmark. In 2009 
Symposium on Languages in Biology and Medicine. 
Z. Lu, H. Y. Kao, C. H. Wei, M. Huang, J. Liu, C. J. 
Kuo, C. N. Hsu, R. T. Tsai, H. J. Dai, N. Okazaki, H. 
C. Cho, M. Gerner, I. Solt, S. Agarwal, F. Liu, D. 
Vishnyakova, P. Ruch, M. Romacker, F. Rinaldi, S. 
Bhattacharya, P. Srinivasan, H. Liu, M. Torii, S. 
Matos, D. Campos, K. Verspoor, K. M. Livingston, 
and W. J. Wilbur. 2011. The gene normalization task 
in BioCreative III. BMC Bioinformatics, 12 Suppl 
8:S2. 
S. G. Nash and J. Nocedal. 1991. A numerical study of 
the limited memory BFGS method and the truncated-
Newton method for large scale optimization. SIAM 
Journal on Optimization, 1:358-372. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In Proceedings of the Eighth ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pages 613-619. 
L. Smith, L. K. Tanabe, R. J. Ando, C. J. Kuo, I. F. 
Chung, C. N. Hsu, Y. S. Lin, R. Klinger, C. M. 
Friedrich, K. Ganchev, M. Torii, H. Liu, B. Haddow, 
C. A. Struble, R. J. Povinelli, A. Vlachos, W. A. 
Baumgartner, Jr., L. Hunter, B. Carpenter, R. T. Tsai, 
H. J. Dai, F. Liu, Y. Chen, C. Sun, S. Katrenko, P. 
Adriaans, C. Blaschke, R. Torres, M. Neves, P. 
Nakov, A. Divoli, M. Mana-Lopez, J. Mata, and W. 
J. Wilbur. 2008. Overview of BioCreative II gene 
mention recognition. Genome Biol, 9 Suppl 2:S2. 
L. Tanabe, L. H. Thom, W. Matten, D. C. Comeau, and 
W. J. Wilbur. 2006. SemCat: semantically 
categorized entities for genomics. In AMIA Annu 
Symp Proc, pages 754-758. 
191
L. Tanabe and W. J. Wilbur. 2006. A priority model for 
named entities. In Proceedings of the Workshop on 
Linking Natural Language Processing and Biology: 
Towards Deeper Biological Literature Analysis, 
pages 33-40. 
L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J. 
Wilbur. 2005. GENETAG: a tagged corpus for 
gene/protein named entity recognition. BMC 
Bioinformatics, 6 Suppl 1:S3. 
P. D. Turney and P. Pantel. 2010. From frequency to 
meaning: vector space models of semantics. Journal 
of Artificial Intelligence Research, 37:141-188. 
Z. Yang, H. Lin, and Y. Li. 2008. Exploiting the 
contextual cues for bio-entity name recognition in 
biomedical literature. J Biomed Inform, 41:580-587. 
 
 
192
