Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1218?1227, Dublin, Ireland, August 23-29 2014.
Integrating Language and Vision
to Generate Natural Language Descriptions of Videos in the Wild
Jesse Thomason
?
University of Texas at Austin
jesse@cs.utexas.edu
Subhashini Venugopalan
?
University of Texas at Austin
vsub@cs.utexas.edu
Sergio Guadarrama
University of California Berkeley
sguada@eecs.berkeley.edu
Kate Saenko
University of Massachusetts Lowell
saenko@cs.uml.edu
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
This paper integrates techniques in natural language processing and computer vision to improve
recognition and description of entities and activities in real-world videos. We propose a strategy
for generating textual descriptions of videos by using a factor graph to combine visual detections
with language statistics. We use state-of-the-art visual recognition systems to obtain confidences
on entities, activities, and scenes present in the video. Our factor graph model combines these
detection confidences with probabilistic knowledge mined from text corpora to estimate the most
likely subject, verb, object, and place. Results on YouTube videos show that our approach im-
proves both the joint detection of these latent, diverse sentence components and the detection of
some individual components when compared to using the vision system alone, as well as over
a previous n-gram language-modeling approach. The joint detection allows us to automatically
generate more accurate, richer sentential descriptions of videos with a wide array of possible
content.
1 Introduction
Integrating language and vision is a topic that is attracting increasing attention in computational lin-
guistics (Berg and Hockenmaier, 2013). Although there is a fair bit of research on generating natural-
language descriptions of images (Feng and Lapata, 2013; Yang et al., 2011; Li et al., 2011; Ordonez et
al., 2011), there is significantly less work on describing videos (Barbu et al., 2012; Guadarrama et al.,
2013; Das et al., 2013; Rohrbach et al., 2013; Senina et al., 2014). In particular, much of the research
on videos utilizes artificially constructed videos with prescribed sets of objects and actions (Barbu et al.,
2012; Yu and Siskind, 2013). Generating natural-language descriptions of videos in the wild, such as
those posted on YouTube, is a very challenging task.
In this paper, we focus on selecting content for generating sentences to describe videos. Due to the
large numbers of video actions and objects and scarcity of training data, we introduce a graphical model
for integrating statistical linguistic knowledge mined from large text corpora with noisy computer vi-
sion detections. This integration allows us to infer which vision detections to trust given prior linguistic
knowledge. Using a large, realistic collection of YouTube videos, we demonstrate that this model effec-
tively exploits linguistic knowledge to improve visual interpretation, producing more accurate descrip-
tions compared to relying solely on visual information. For example, consider the frames of the video
in Figure 1. Instead of generating the inaccurate description ?A person is playing on the keyboard in the
kitchen? using purely visual information, our system generates the more correct ?A person is playing the
piano in the house? by using statistics mined from parsed corpora to improve the interpretation of the
uncertain visual detections, such as the presence of both a computer keyboard and a piano in the video.
2 Background and Related Work
Several recent projects have integrated linguistic and visual information to aid description of images and
videos. The most related work on image description is Baby Talk (Kulkarni et al., 2011), which uses
?
Indicates equal contribution
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1218
Figure 1: Frames which depict a person playing a piano in front of a keyboard from one of the videos
in our dataset. Purely visual information is more confident in the computer keyboard?s presence than the
piano?s, while our model can correctly determine that the person is more likely to be playing the piano
than the computer keyboard.
a Conditional Random Field (CRF) to integrate visual detections with statistical linguistic knowledge
mined from parsed image descriptions and Google queries, and the work of Yang et al. (2011) which
uses corpus statistics to aid the description of objects and scenes. We go beyond the scope of these
previous works by also selecting verbs through the integration of activity recognition from video and
statistics from parsed corpora.
With regard to video description, the work of Barbu et al. (2012) uses a small, hand-coded grammar to
describe a sparse set of prescribed activities. In contrast, we utilize corpus statistics to aid the description
of a wide range of naturally-occurring videos. The most similar work is (Krishnamoorthy et al., 2013;
Guadarrama et al., 2013) which uses an n-gram language model to help determine the best subject-verb-
object for describing a video. Krishnamoorthy et al. (2013) used a limited set of videos containing
a small set of 20 entities, and the work of Guadarrama et al. (2013) showed an advantage of using
linguistic knowledge only for the case of ?zero shot activity recognition,? in which the appropriate verb
for describing the activity was never seen during training. Compared to this prior work, we explore a
much larger set of entities and activities (see Section 3.2) and add scene recognition (see Section 3.3) to
further enrich the descriptions. Our experiments demonstrate that our graphical model produces a more
accurate subject-verb-object-place description than these simpler n-gram language modeling approaches.
Our Contributions:
? We present a new method, a Factor Graph Model (FGM), to perform content selection by integrating
visual and linguistic information to select the best subject-verb-object-place description of a video.
? Our model includes scene (location) information which has not been addressed by previous video
description works (Barbu et al., 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013).
? We demonstrate the scalability of our model by evaluating it on a large dataset of naturally occurring
videos (1297 training, 670 testing), recognizing sentential subjects out of 45 candidate entities,
objects out of 218 candidate objects, verbs out of 218 candidate activities, and places out of 12
candidate scenes.
3 Approach
Our overall approach uses a probabilistic graphical model to integrate the visual detection of entities,
activities, and scenes with language statistics to determine the best subject, verb, object, and place to
describe a given video. A descriptive English sentence is generated from the selected sentential compo-
nents.
3.1 Video Dataset
We use the video dataset collected by Chen and Dolan (2011). The dataset contains 1,967 short YouTube
video clips paired with multiple human-generated natural-language descriptions. The video clips are 10
to 25 seconds in duration and typically consist of a single activity. Portions of this dataset have been
used in previous work on video description (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013;
Guadarrama et al., 2013). We use 1,297 randomly selected videos for training and evaluate predictions
on the remaining 670 test videos.
1219
3.2 Visual Recognition of Subject, Verb, and Object
We utilize the visual recognition techniques employed by Guadarrama et al. (2013) to process the videos
and produce probabilistic detections of grammatical subjects, verbs, and objects. In our data-set there are
45 candidate entities for the grammatical subject (such as animal, baby, cat, chef, and person) and 241
for the grammatical object (such as flute, motorbike, shrimp, person, and tv). There are 218 candidate
activities for the grammatical verb, including climb, cut, play, ride, and walk.
Entity Related Features From each video two frames per second are extracted and passed to pre-
trained visual object classifiers and detectors. As in Guadarrama et al. (2013), we compute represen-
tations based on detected objects using ObjectBank (Li et al., 2010) and the 20 PASCAL (Everingham
et al., 2010) object classes for each frame. We use the PASCAL scores and ObjectBank scores with
max pooling over the set of frames as the entity descriptors for the video clip. Additionally, to be able
to recognize more objects, we use the LLC-10k proposed by Deng et al. (2012) which was trained on
ImageNet 2011 object dataset with 10k categories. LLC-10K uses a bank of linear SVM classifiers over
pooled local vector-quantized features learned from the 7K bottom level synsets of the 10K ImageNet
database. We aggregate the 10K classifier scores obtained for each frame by doing max pooling across
frames.
Activity Related Features We use the activity recognizers described in Guadarrama et al. (2013) to
produce probabilistic verb detections. They extract Dense Trajectories developed by Wang et al. (2011)
and compute HoG (Histogram of Gradients), HoF (Histograms of Optical Flow) and MBH (Motion
Boundary Histogram) features over space time volumes around the trajectories. We used the default
parameters proposed in Wang et al. (2011) (N = 32, n
?
= 2, n
r
= 3) and adopted a standard bag-
of-features representation. We construct a codebook for each descriptor (Trajectory, HoG, HoF, MBH)
separately. For each descriptor we randomly sampled 100K points and clustered them using K-means
into a codebook of 4000 words. Descriptors are assigned to their closest vocabulary word using Eu-
clidean distance. Each video is then represented as a histogram over these clusters.
Multi-channel SVM To allow object and activity features inform one another, we combine all the
features extracted using a multi-channel approach inspired by Zhang et al. (2007) to build three non-linear
SVM (Chang and Lin, 2011) classifiers for the subject, verb, and object, as described in Guadarrama et
al. (2013). Note that we do not employ the hierarchical semantic model of Guadarrama et al. (2013) to
augment our object or activity recognition. In addition, each SVM learns a Platt scaling (Platt, 1999) to
predict the label and a visual confidence value, C(t) ? [0, 1], for each entity or activity t. The output
of the SVMs constitute the visual confidences on subject, verb, and object in all the models described
henceforth.
3.3 Visual Scene Recognition
In addition to the techniques employed by Guadarrama et al. (2013) used to obtain probabilistic de-
tections of grammatical subjects, verbs, and objects, we developed a novel scene detector based on
state-of-the-art computer vision methods.
We examined the description of all the 1,967 videos in the YouTube dataset and extracted scene words
from the dependency parses as described in Section 3.4. With the help of WordNet
1
we grouped the list
of scene words and their synonyms into distinct scene classes. Based on the frequency of mentions and
the coverage of scenes in the dataset, we shortlisted a set of 12 final scenes (mountain, pool, beach, road,
kitchen, field, snow, forest, house, stage, track, and sky).
For the detection itself, we follow Xiao et al. (2010) and select several state-of-the-art features that are
potentially useful for scene recognition. We extract GIST, HOG2x2, SSIM (self-similarity) and Dense
SIFT descriptors. We also extract LBP (Local Binary Patterns), Sparse SIFT Histograms, Line features,
Color Histograms, Texton Histograms, Tiny Images, Geometric Probability Map and Geometric specific
histograms. The code for extracting the features and computing kernels for the features is taken from
1
http://wordnet.princeton.edu
1220
the original papers as described in Xiao et al. (2010). Using the features and kernels, we train one-vs-all
SVMs (Chang and Lin, 2011) to classify images into scene categories. As in Xiao et al. (2010), this gave
us 51 different SVM classifiers with different feature and kernel choices. We use the images from the
UIUC 15 scene dataset (Lazebnik et al., 2006) and the SUN 397 scene dataset (Xiao et al., 2010) for
training the scene classifiers for all scenes except kitchen. The training images for kitchen were obtained
by selecting 100 frames from about 15 training videos, since the classifier trained on images from the
existing scene datasets performed extremely poorly on the videos. We use all the classifiers to detect
scenes for each frame. We then average the scene detection scores over all the classifiers across all the
frames of the video. This gives us visual confidence values, C(t), over all scene categories t for the
video.
3.4 Language Statistics
A key aspect of our approach is the use of language statistics mined from English text corpora to
bias visual interpretation. Like Krishnamoorthy et al. (2013), we use dependency-parsed text from
four large ?out of domain? corpora: English Gigaword, British National Corpus (BNC), ukWac and
WaCkypedia EN. We also use a small, specialized ?in domain? corpus: dependency parsed sentences
from the human-generated, English descriptions for the YouTube training videos mentioned in Sec-
tion 3.1. We extract SVOP (subject, verb, object, place) tuples from the dependency parses. The subject-
verb relationships are identified using nsubj dependencies, the verb-object relationships using dobj and
prep dependencies. Object-place relationships are identified using the prep dependency, checking that
the noun modified by the preposition is one of our recognizable places (or synonyms of the recognizable
scenes as indicated by WordNet). We then extract co-occuring SV, VO, and OP bigram statistics from
the resulting SVOP tuples to inform our factor-graph model, which uses both the out-of-domain (p
o
) and
in-domain (p
i
) bigram probabilities.
3.5 Content Selection Using Factor Graphs
In order to combine visual and linguistic evidence, we use the probabilistic factor-graph model shown
in Figure 2. This model integrates the uncertain visual detections described in Sections 3.2 and 3.3 with
the language statistics described in Section 3.4 to predict the best words for describing the subject (S),
verb (V), object (O), and place (P) for each test video. After instantiating the potential functions for
this model, we perform a maximum a posteriori (MAP) estimation (via the max-product algorithm) to
determine the most probable joint set of values for these latent variables.
Figure 2: The factor graph model used for content selection (right), and sample frames from a video to
be described (left). Visual confidence values are observed (gray potentials) and inform sentence com-
ponents. Language potentials (dashed) connect latent words between sentence components. Samples of
the vision confidence values used as observations for the verb and object are shown for the example test
video.
1221
Observation Potentials. The observations in our model take the form of confidence scores from the
visual detectors described in Sections 3.2 and 3.3. That is, the potential for each sentence component
k ? {S, V,O, P}, ?
k
(t) = C
k
(t) is the detection confidence that the classifier for component k (C
k
)
gives to the word t.
Language Potentials. Language statistics were gathered as described in Section 3.4 and used to deter-
mine the language potentials as follows:
?
k,l
(t, s) := p(l = s|k = t) := ?p
o
(l = s|k = t) + (1? ?)p
i
(l = s|k = t)
Where k and l are two contiguous components in the SVOP sequence and t and s are words that are
possible values for these two components, respectively. We would expect
?
V,O
(ride,motorbike) := p(O=motorbike|V=ride)
to be relatively high, since motorbike is a likely object of the verb ride. The potential between two
sequential components k and l in the SVOP sequence is computed by linearly interpolating the bigram
probability observed in the out-of-domain corpus of general text (p
o
) and the in-domain corpus of video
descriptions (p
i
). The interpolation parameter ? adjusts the importance of these two corpora in deter-
mining the bigram probability. We optimized performance by fixing ? = 0.25 when cross-validating on
the training data. This weighting effectively allows general text corpora to be used to smooth the prob-
ability estimates for video descriptions. We note that meaningful information would likely be captured
by non-contiguous language potentials such as ?
V,P
, but that the resulting factor graphs would contain
cycles, preventing us from performing exact inference tractably.
3.6 Sentence Generation
Finally, we use the SVOP tuple chosen by our model to generate an English sentence using the following
template: ?Determiner (A,The) - Subject - Verb (Present, Present Continuous) - Preposition (optional)
- Determiner (A,The) - Object (optional) - Preposition - Determiner (A,The) - Place (optional)? The
most probable prepositions are identified using preposition-object and preposition-place bigram statistics
mined from the dependency parsed corpora described in Section 3.4. Given an SVOP tuple, our objective
is to generate a rich sentence using the subject, verb, object, and place information. However, it is not
prudent to add the object and place to the description of all videos since some verbs may be intransitive
and the place information may be redundant. In order to achieve the best set of components to include,
we use the above template to first generate a set of candidate sentences based on the SVO triple, SVP
triple and the SVOP quadruple. Then, each sentence type (SVO, SVP, and SVOP) is ranked using the
BerkeleyLM language model (Pauls and Klein, 2011) trained on the GoogleNgram corpus. Finally, we
output the sentence with the highest average 5-gram probability in order to normalize for sentence length.
4 Experimental Results
We compared using the vision system alone to our model, which augments that system with linguistic
knowledge. Specifically, we consider the Highest Vision Confidence (HVC) model, which takes for
each sentence component the word with the highest confidence from the state-of-the-art vision detectors
described in Sections 3.2 and 3.3. We compare the results of this model on the 670 test videos to those
of our Factor Graph Model (FGM), as discussed in Section 3.5.
4.1 N-gram Baseline
Additionally, we compare both models against the existing, baseline n-gram model of Krishnamoorthy
et al. (2013) by extending their best n-gram model to support places. To be specific, we build a quadra-
gram model, similar to the trigram model of Krishnamoorthy et al. (2013). We first extract SVOP tuples
from the dependency parses as described in Section 3.4. We then train a backoff language model with
Kneyser-Ney smoothing (Chen and Goodman, 1996) for estimating the likelihood of the SVOP quadru-
ple. On quadruples that are not seen during training, this quadragram language model backs off to SVO
1222
Most S% V% O% [P]% SVO% SVO[P]%
n-gram 76.57 11.04 11.19 18.30 2.39 1.86
HVC 76.57
+
22.24 11.94 17.24
+
4.33
+
2.92
FGM 76.42
+
21.34 12.39 19.89
+
5.67
+
3.71
Any
n-gram 86.87 19.25 21.94 21.75 5.67 2.65
HVC 86.57
+
38.66 22.09 21.22
+
10.15
+
4.24
FGM 86.27
+
37.16
+
24.63 24.67
+
10.45
+
6.10
Table 1: Average binary accuracy of predicting the most common word (top) and of predicting any given
word (bottom). Bold entries are statistically significantly (p < 0.05) greater than the HVC model, while
+
entries are significantly greater than the n-gram model. No model scored significantly higher than
FGM on any metric. [P] indicates that the score ranges only over the subset of videos for which any
annotator provided a place.
triple and subject-verb, verb-object, object-place bigrams to estimate the probability of the quadruple.
As in the case of the factor graph model, we consider the effect of learning from a domain specific text
corpus. We build quadragram language models for both out-of-domain and in-domain text-corpora de-
scribed in Section 3.4. The probability of a quadragram in the language model is computed by linearly
interpolating the probabilities from the in-domain and out-of-domain corpus. We experiment with dif-
ferent number of top subjects, objects, verbs, and places to estimate the most likely SVOP quadruple
from the quadragram language model. We report the results for the best performing n-gram model that
considers the top 5 subjects, 5 objects, 10 verbs, and 3 places based on the vision confidences and an out-
of-domain corpus weight of 1. This model also incorporates verb expansion as described in the original
work (Krishnamoorthy et al., 2013).
4.2 Content Evaluation
Table 1 shows the accuracy of the models when their prediction for each sentence component is consid-
ered correct only if it is the word most commonly used by human annotators to describe the video, as well
as the accuracy of the models when the prediction is considered correct if used by any of the annotators to
describe the video. We evaluate the accuracy of each component (S,V,O,P) individually, and for complete
SVO and SVOP tuples, where all components must be correct in order for a complete tuple to be judged
correct. Because only about half (56.3%) of test videos were described with a place by some annotator,
accuracies involving places (?[P]?) are averaged only over the subset of videos for which any annotator
provided a place. Significance was determined using a paired t-test which compared the distributions of
the binary correctness of each model?s prediction on each video for the specified component(s).
We also use the WUP metric from Wordnet::Similarity
2
to measure the quality of the predicted words
to account for semantically similar words. For example, where the binary metric would mark ?slice? as
an incorrect substitute for ?cut?, the WUP metric will provide ?partial credit? for such predictions. The
results using WUP similarity metrics for the most common word and any valid word (maximum WUP
similarity is chosen from among valid words) are presented in Table 2. Since WUP provides scores are in
the range [0,1], we view the scores as ?percent relevance,? and we obtain tuple scores for each sentence
by taking the product of the component WUP scores.
5 Discussion
It is clear from the results in Table 1 that both the HVC and the FGM outperform the n-gram language
model approach used in the most-similar previous work (Krishnamoorthy et al., 2013; Guadarrama et
al., 2013). Note that while Krishnamoorthy et al. (2013) showed an improvement with an n-gram model
considering only the top few vision detections, the FGM considers vision confidences over the entire set
2
http://wn-similarity.sourceforge.net/
1223
Most S% V% O% [P]% SVO% SVO[P]%
n-gram 89.00 41.56 44.01 57.62 17.53 10.83
HVC 89.09
+?
48.85 43.99 56.00
+
20.82
+
12.95
FGM 89.01
+
47.05
+
45.29
+
59.64
+
21.54
+
14.50
Any
n-gram 96.60 55.08 65.52 61.98 35.70 22.84
HVC 96.54
+?
65.61 65.32 60.67
+
42.53
+
27.75
FGM 96.32
+
63.49
+
67.52
+
64.68
+
42.43
+
29.34
Table 2: Average WUP score of the predicted word against the most common word (top) and the max-
imum score against any given word (bottom). Bold entries are statistically significantly (p < 0.05)
greater than the HVC model;
+
entries are significantly greater than the n-gram model;
?
entries are
significantly greater than the FGM. [P] indicates that the score ranges only over the subset of videos for
which any annotator provided a place.
of grammatical objects. Additionally, our models are evaluated on a much more diverse set of videos
while Krishnamoorthy et al. (2013) evaluate the n-gram model on 185 videos (a small subset of the
1,967 videos containing the 20 grammatical objects that their system recognized).
The performance differences between the vision system (HVC) and our integrated model (FGM) are
modest but significant in important places. Specifically, the FGM makes improvements to SVO (Table 1,
top) and SVOP (Table 2, top) tuple accuracies. FGM also significantly improves both the O and [P] (Ta-
ble 1, bottom, and Table 2) component accuracies, suggesting that it can help clean up some noise from
the vision systems even at the component level by considering related bigram probabilities. FGM causes
no significant losses under the binary metric, but performs worse than the HVC model on predicting a
verb component semantically similar to the correct verb under the WUP metric (Table 2). This loss on
the verb component is worth the gains in tuple accuracy, since tuple prediction is the more difficult and
most central part of the content selection task. Additionally, experiments by the authors of Guadarrama
et al. (2013) on Amazon Mechanical Turk have shown that humans tend to heavily penalize tuples and
descriptions even if they have most of the components correct.
Table 3 shows frames from some test videos and the sentence components chosen by the models to
describe them. In the top four videos we see the FGM improving raw vision results. For example, it
determines that a person is more likely slicing an onion than an egg. Some specific confidence values
for the HVC can be seen for this video in Figure 2. In the bottom two videos of Table 3 we see the HVC
performing better without linguistic information. For example, the FGM intuits that a person is more
likely to be driving a car than lifting it, and steers the prediction away from the correct verb. This may
be part of a larger phenomenon in which YouTube videos often depict unusual actions, and consequently
general language knowledge can sometimes hurt performance by selecting more common activities.
6 Future Work
Compared to the human gold standard descriptions, there appears to be room for improvement in de-
tecting activities, objects, and scenes with high precision. Visual recognition of entities and activities in
diverse real-world videos is extremely challenging, partially due to lack of training data. As a result our
current model is faced with large amounts of noise in the vision potentials, especially for objects. Going
forward, we believe that improving visual recognition will allow the language statistics to be even more
useful. We are currently exploring deep image feature representations (Donahue et al., 2013) to improve
object and verb recognition, as well as model transfer from large labeled object ontologies (Deng et al.,
2009).
From the generation perspective, there is scope to move beyond the template based sentence gener-
ation. This becomes particularly relevant if we detect multiple grammatical objects such as adjectives
or adverbs. We need to decide whether additional grammatical objects would enrich the sentence de-
1224
FGM improves over HVC
?A person is slicing the onion in the kitchen?
Gold: person, slice, onion, (none)
HVC: person, slice, egg, kitchen
FGM: person, slice, onion, kitchen
?A person is running a race on the road?
Gold: person, run, race, (none)
HVC: person, ride, race, ground
FGM: person, run, race, road
?A person is playing the guitar on the stage?
Gold: person, play, guitar, tree
HVC: person, play, water, kitchen
FGM: person, play, guitar, stage
?A person is playing a guitar in the house?
Gold: person, play, guitar, (none)
HVC: person, pour, chili, kitchen
FGM: person, play, guitar, house
HVC better alone
?A person is lifting a car on the road?
Gold: person, lift, car, ground
HVC: person, lift, car, road
FGM: person, drive, car, road
?A person is pouring the egg in the kitchen?
Gold: person, pour, mushroom, kitchen
HVC: person, pour, egg, kitchen
FGM: person, play, egg, kitchen
Table 3: Example videos and: (Gold) the most common SVOP provided by annotators; (HVC) the
highest vision confidence selections; (FGM) the selections from our factor graph model. The top section
shows videos where the FGM improved over HVC; the bottom shows videos where the HVC did better
alone. For each video, the sentence generated from the components chosen from the more successful
system is shown.
scription and identify when to add them appropriately. With increasing applications for such systems in
automatic video surveillance and video retrieval, generating richer and more diverse sentences for longer
videos is an area for future research. In comparison to previous approaches (Krishnamoorthy et al., 2013;
Yang et al., 2011) the factor graph model can be easily extended to support this. Additional nodes can be
attached suitably to the graph to enable the prediction of adjectives and adverbs to enrich the base SVOP
tuple.
7 Conclusions
This work introduces a new framework to generate simple descriptions of short videos by integrating
visual detection confidences with language statistics obtained from large textual corpora. Experimental
results show that our approach achieves modest improvements over a pure vision system and signifi-
cantly improves over previous methods in predicting the complete subject-verb-object and subject-verb-
object-place tuples. Our work has a broad coverage of objects and verbs and extends previous works by
predicting place information.
1225
There are instances where our model fails to predict the correct verb when compared to the HVC
model. This could partially be because the SVM classifiers that detect activity already leverage entity
information during training, and adding external language does not appear to improve verb prediction
significantly. Further detracting from performance, our model occasionally propagates, rather than cor-
recting, errors from the HVC. For example, when the HVC predicts the correct verb and incorrect object,
such as in ?person ride car? when the video truly depicts a person riding a motorbike, our model selects
the more likely verb pairing ?person drive car?, extending the error from the object to the verb as well.
Despite these drawbacks, our approach predicts complete subject-verb-object-place tuples more
closely related to the most commonly used human descriptions than vision alone (Table 2), and in general
improves both object and place recognition accuracies (Tables 1, 2).
Acknowledgements
This work was funded by NSF grant IIS1016312, DARPA Minds Eye grant W911NF-10-9-0059, and
NSF ONR ATL grant N00014-11-1-0105. Some of our experiments were run on the Mastodon Cluster
(NSF grant EIA-0303609).
References
Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux,
Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark
Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, and Zhiqi Zhang. 2012. Video in sentences out.
In Association for Uncertainty in Artificial Intelligence (UAI).
Tamara Berg and Julia Hockenmaier. 2013. Workshop on vision and language. In Proceedings of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).
NAACL.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions
on Intelligent Systems and Technology (TIST), 2(3):27.
David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pages 190?200. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling.
In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL), pages 310?318.
Association for Computational Linguistics.
Pradipto Das, Chenliang Xu, Richard F. Doell, and Jason J. Corso. 2013. A thousand frames in just a few
words: Lingual description of videos through latent topics and sparse object stitching. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
Jia Deng, Kai Li, Minh Do, Hao Su, and Li Fei-Fei. 2009. Construction and analysis of a large scale image
ontology. Vision Sciences Society.
Jia Deng, Jonathan Krause, Alex Berg, and Li Fei-Fei. 2012. Hedging Your Bets: Optimizing Accuracy-
Specificity Trade-offs in Large Scale Visual Recognition. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2013.
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2010. The
pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV), 88(2):303?338,
June.
Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 35(4):797?812.
1226
Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,
Trevor Darrell, and Kate Saenko. 2013. Youtube2text: Recognizing and describing arbitrary activities using
semantic hierarchies and zero-shot recognition. In IEEE International Conference on Computer Vision (ICCV),
December.
Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J. Mooney, Kate Saenko, and Sergio Guadarrama. 2013.
Generating natural-language video descriptions using text-mined knowledge. In Proceedings of the AAAI Con-
ference on Artificial Intelligence (AAAI), pages 541?547.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Alexander Berg, Yejin Choi, and Tamara Berg. 2011.
Baby talk: Understanding and generating image descriptions. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching
for recognizing natural scene categories. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 2, pages 2169?2178. IEEE.
Li-Jia Li, Hao Su, Eric Xing, and Li Fei-Fei. 2010. Object bank: A high-level image representation for scene
classication and semantic feature sparsification. In Advances in Neural Information Processing Systems (NIPS).
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple
image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational
Natural Language Learning (CoNLL), pages 220?228, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Improving video activity recognition using object recognition
and text mining. In Proceedings of the European Conference on Artificial Intelligence (ECAI), pages 600?605.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages
1143?1151.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258?267.
Association for Computational Linguistics.
John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. In Advances In Large Margin Classifiers, pages 61?74. MIT Press.
Marcus Rohrbach, Qiu Wei, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. 2013. Translating video
content to natural language descriptions. In IEEE International Conference on Computer Vision (ICCV).
Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred
Pinkal, and Bernt Schiele. 2014. Coherent multi-sentence video description with variable level of detail. arXiv
preprint arXiv:1403.6173.
Heng Wang, Alexander Klaser, Cordelia Schmid, and Cheng-Lin Liu. 2011. Action recognition by dense trajec-
tories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3169?3176. IEEE.
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-
scale scene recognition from abbey to zoo. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3485?3492.
Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation
of natural images. In Conference on Emperical Methods in Natural Language Processing (EMNLP), pages
444?454.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In
Proceedings of the Association for Computational Linguistics (ACL), pages 53?63.
Jianguo Zhang, Marcin Marsza?ek, Svetlana Lazebnik, and Cordelia Schmid. 2007. Local features and kernels
for classification of texture and object categories: A comprehensive study. International Journal of Computer
Vision (IJCV), 73(2):213?238.
1227
Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 10?19,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Generating Natural-Language Video Descriptions
Using Text-Mined Knowledge
Niveda Krishnamoorthy ?
UT Austin
niveda@cs.utexas.edu
Girish Malkarnenkar ?
UT Austin
girish@cs.utexas.edu
Raymond Mooney
UT Austin
mooney@cs.utexas.edu
Kate Saenko
UMass Lowell
saenko@cs.uml.edu
Sergio Guadarrama
UC Berkeley
sguada@eecs.berkeley.edu
Abstract
We present a holistic data-driven technique
that generates natural-language descriptions
for videos. We combine the output of state-of-
the-art object and activity detectors with ?real-
world? knowledge to select the most proba-
ble subject-verb-object triplet for describing a
video. We show that this knowledge, automat-
ically mined from web-scale text corpora, en-
hances the triplet selection algorithm by pro-
viding it contextual information and leads to
a four-fold increase in activity identification.
Unlike previous methods, our approach can
annotate arbitrary videos without requiring the
expensive collection and annotation of a sim-
ilar training video corpus. We evaluate our
technique against a baseline that does not use
text-mined knowledge and show that humans
prefer our descriptions 61% of the time.
1 Introduction
Combining natural-language processing (NLP) with
computer vision to to generate English descriptions
of visual data is an important area of active research
(Farhadi et al, 2010; Motwani and Mooney, 2012;
Yang et al, 2011). We present a novel approach to
generating a simple sentence for describing a short
video that:
1. Identifies the most likely subject, verb and
object (SVO) using a combination of visual
object and activity detectors and text-mined
knowledge to judge the likelihood of SVO
triplets. From a natural-language generation
?Indicates equal contribution
(NLG) perspective, this is the content planning
stage.
2. Given the selected SVO triplet, it uses a simple
template-based approach to generate candidate
sentences which are then ranked using a statis-
tical language model trained on web-scale data
to obtain the best overall description. This is
the surface realization stage.
Figure 1 shows sample system output. Our ap-
proach can be viewed as a holistic data-driven three-
step process where we first detect objects and ac-
tivities using state-of-the-art visual recognition al-
gorithms. Next, we combine these often noisy de-
tections with an estimate of real-world likelihood,
which we obtain by mining SVO triplets from large-
scale web corpora. Finally, these triplets are used to
generate candidate sentences which are then ranked
for plausibility and grammaticality. The resulting
natural-language descriptions can be usefully em-
ployed in applications such as semantic video search
and summarization, and providing video interpreta-
tions for the visually impaired.
Using vision models alone to predict the best sub-
ject and object for a given activity is problematic,
especially while dealing with challenging real-world
YouTube videos as shown in Figures 4 and 5, as
it requires a large annotated video corpus of simi-
lar SVO triplets (Packer et al, 2012). We are in-
terested in annotating arbitrary short videos using
off-the-shelf visual detectors, without the engineer-
ing effort required to build domain-specific activity
models. Our main contribution is incorporating the
pragmatics of various entities? likelihood of being
10
Figure 1: Content planning and surface realization
the subject/object of a given activity, learned from
web-scale text corpora. For example, animate ob-
jects like people and dogs are more likely to be sub-
jects compared to inanimate objects like balls or TV
monitors. Likewise, certain objects are more likely
to function as subjects/objects of certain activities,
e.g., ?riding a horse? vs. ?riding a house.?
Selecting the best verb may also require recog-
nizing activities for which no explicit training data
has been provided. For example, consider a video
with a man walking his dog. The object detectors
might identify the man and dog; however the action
detectors may only have the more general activity,
?move,? in their training data. In such cases, real-
world pragmatics is very helpful in suggesting that
?walk? is best used to describe a man ?moving? with
his dog. We refer to this process as verb expansion.
After describing the details of our approach, we
present experiments evaluating it on a real-world
corpus of YouTube videos. Using a variety of meth-
ods for judging the output of the system, we demon-
strate that it frequently generates useful descriptions
of videos and outperforms a purely vision-based ap-
proach that does not utilize text-mined knowledge.
2 Background and Related Work
Although there has been a lot of interesting work
done in natural language generation (Bangalore and
Rambow, 2000; Langkilde and Knight, 1998), we
use a simple template for generating our sentences
as we found it to work well for our task.
Most prior work on natural-language descrip-
tion of visual data has focused on static images
(Felzenszwalb et al, 2008; Kulkarni et al, 2011;
Kuznetsova et al, 2012; Laptev et al, 2008; Li et al,
2011; Yao et al, 2010). The small amount of exist-
ing work on videos (Ding et al, 2012; Khan and Go-
toh, 2012; Kojima et al, 2002; Lee et al, 2008; Yao
and Fei-Fei, 2010) uses hand-crafted templates or
rule-based systems, works in constrained domains,
and does not exploit text mining. Barbu et al (2012)
produce sentential descriptions for short video clips
by using an interesting dynamic programming ap-
proach combined with Hidden Markov Models for
obtaining verb labels for each video. However, they
do not use any text mining to improve the quality of
their visual detections.
Our work differs in that we make extensive use of
text-mined knowledge to select the best SVO triple
and generate coherent sentences. We also evaluate
our approach on a generic, large and diverse set of
challenging YouTube videos that cover a wide range
of activities. Motwani and Mooney (2012) explore
how object detection and text mining can aid activity
recognition in videos; however, they do not deter-
mine a complete SVO triple for describing a video
nor generate a full sentential description.
With respect to static image description, Li et al
(2011) generate sentences given visual detections of
objects, visual attributes and spatial relationships;
however, they do not consider actions. Farhadi et al
(2010) propose a system that maps images and the
corresponding textual descriptions to a ?meaning?
space which consists of an object, action and scene
triplet. However, they assume a single object per
image and do not use text-mining to determine the
likelihood of objects matching different verbs. Yang
et al (2011) is the most similar to our approach in
that it uses text-mined knowledge to generate sen-
tential descriptions of static images after performing
object and scene detection. However, they do not
perform activity recognition nor use text-mining to
select the best verb.
3 Approach
Our overall approach is illustrated in Figure 2 and
consists of visual object and activity recognition fol-
lowed by content-planning to generate the best SVO
triple and surface realization to generate the final
sentence.
11
Figure 2: Summary of our approach
3.1 Dataset
We used the English portion of the YouTube data
collected by Chen et al (2010), consisting of short
videos each with multiple natural-language descrip-
tions. This data was previously used by Motwani
and Mooney (2012), and like them, we ensured that
the test data only contained videos in which we can
potentially detect objects. We used the object de-
tector by Felzenszwalb et al (2008) as it achieves
the state-of-the-art performance on the PASCAL Vi-
sual Object Classes (VOC) Challenge. As such, we
selected test videos whose subjects and objects be-
long to the 20 VOC object classes - aeroplane, car,
horse, sheep, bicycle, cat, sofa, bird, chair, motor-
bike, train, boat, cow, person, tv monitor, bottle,
dining table, bus, dog, potted plant. During this
filtering, we also allow synonyms of these object
names by including all words with a Lesk similar-
ity (as implemented by Pedersen et al (2004)) of at
least 0.5.1 Using this approach, we chose 235 po-
tential test videos; the remaining 1,735 videos were
reserved for training.
All the published activity recognition methods
that work on datasets such as KTH (Schuldt et al,
2004), Drinking and Smoking (Laptev and Perez,
2007) and UCF50 (Reddy and Shah, 2012) have
a very limited recognition vocabulary of activity
classes. Since we did not have explicit activity la-
1Empirically, this method worked better than using WordNet
synsets.
Figure 3: Activity clusters discovered by HAC
bels for our YouTube videos, we followed Motwani
and Mooney (2012)?s approach to automatically dis-
cover activity clusters. We first parsed the train-
ing descriptions using Stanford?s dependency parser
(De Marneffe et al, 2006) to obtain the set of verbs
describing each video. We then clustered these verbs
using Hierarchical Agglomerative Clustering (HAC)
using the res metric from WordNet::Similarity by
Pedersen et al (2004) to measure the distance be-
tween verbs. By manually cutting the resulting hi-
erarchy at a desired level (ensuring that each clus-
ter has at least 9 videos), we discovered the 58 ac-
tivity clusters shown in Figure 3. We then filtered
the training and test sets to ensure that all verbs be-
longed to these 58 activity clusters. The final data
contains 185 test and 1,596 training videos.
3.2 Object Detection
We used Felzenszwalb et al (2008)?s
discriminatively-trained deformable parts mod-
els to detect the most likely objects in each video.
Since these object detectors were designed for
static images, each video was split into frames at
one-second intervals. For each frame, we ran the
object detectors and selected the maximum score
assigned to each object in any of the frames. We
converted the detection scores, f(x), to estimated
probabilities p(x) using a sigmoid p(x) = 1
1+e?f(x)
.
3.3 Activity Recognition
In order to get an initial probability distribution for
activities detected in the videos, we used the motion
descriptors developed by Laptev et al (2008). Their
approach extracts spatio-temporal interest points
(STIPs) from which it computes HoG (Histograms
12
Corpora Size of text
British National Corpus (BNC) 1.5GB
WaCkypedia EN 2.6GB
ukWaC 5.5GB
Gigaword 26GB
GoogleNgrams 1012 words
Table 1: Corpora used to Mine SVO Triplets
of Oriented Gradients) and HoF (Histograms of Op-
tical Flow) features over a 3-dimensional space-time
volume. These descriptors are then randomly sam-
pled and clustered to obtain a ?bag of visual words,?
and each video is then represented as a histogram
over these clusters. We experimented with different
classifiers such as LIBSVM (Chang and Lin, 2011)
to train a final activity detector using these features.
Since we achieved the best classification accuracy
(still only 8.65%) using an SVM with the intersec-
tion kernel, we used this approach to obtain a prob-
ability distribution over the 58 activity clusters for
each test video. We later experimented with Dense
Trajectories (Wang et al, 2011) for activity recogni-
tion but there was only a minor improvement.
3.4 Text Mining
We improve these initial probability distributions
over objects and activities by incorporating the like-
lihood of different activities occuring with particular
subjects and objects using two different approaches.
In the first approach, using the Stanford dependency
parser, we parsed 4 different text corpora covering a
wide variety of text: English Gigaword, British Na-
tional Corpus (BNC), ukWac and WaCkypedia EN.
In order to obtain useful estimates, it is essential to
collect text that approximates all of the written lan-
guage in scale and distribution. The sizes of these
corpora (after preprocessing) are shown in Table 1.
Using the dependency parses for these corpora,
we mined SVO triplets. Specifically, we looked for
subject-verb relationships using nsubj dependencies
and verb-object relationships using dobj and prep
dependencies. The prep dependency ensures that
we account for intransitive verbs with prepositional
objects. Synonyms of subjects and objects and con-
jugations of verbs were reduced to their base forms
(20 object classes, 58 activity clusters) while form-
ing triplets. If a subject, verb or object not belonging
to these base forms is encountered, it is ignored dur-
ing triplet construction.
These triplets are then used to train a backoff lan-
guage model with Kneser-Ney smoothing (Chen and
Goodman, 1999) for estimating the likelihood of an
SVO triple. In this model, if we have not seen train-
ing data for a particular SVO trigram, we ?back-off?
to the Subject-Verb and Verb-Object bigrams to co-
herently estimate its probability. This results in a
sophisticated statistical model for estimating triplet
probabilities using the syntactic context in which
the words have previously occurred. This allows us
to effectively determine the real-world plausibility
of any SVO using knowledge automatically mined
from raw text. We call this the ?SVO Language
Model? approach (SVO LM).
In a second approach to estimating SVO prob-
abilities, we used BerkeleyLM (Pauls and Klein,
2011) to train an n-gram language model on the
GoogleNgram corpus (Lin et al, 2012). This sim-
ple model does not consider synonyms, verb con-
jugations, or SVO dependencies but only looks at
word sequences. Given an SVO triplet as an in-
put sequence, it estimates its probability based on
n-grams. We refer to this as the ?Language Model?
approach (LM).
3.5 Verb Expansion
As mentioned earlier, the top activity detections are
expanded with their most similar verbs in order to
generate a larger set of potential words for describ-
ing the action. We used the WUP metric from Word-
Net::Similarity to expand each activity cluster to in-
clude all verbs with a similarity of at least 0.5. For
example, we expand the verb ?move? with go 1.0,
walk 0.8, pass 0.8, follow 0.8, fly 0.8, fall 0.8, come
0.8, ride 0.8, run 0.67, chase 0.67, approach 0.67,
where the number is the WUP similarity.
3.6 Content Planning
To combine the vision detection and NLP scores and
determine the best overall SVO, we use simple lin-
ear interpolation as shown in Equation 1. When
computing the overall vision score, we make a con-
ditional independence assumption and multiply the
probabilities of the subject, activity and object. To
account for expanded verbs, we additionally mul-
tiply by the WUP similarity between the original
13
(Vorig) and expanded (Vsim) verbs. The NLP score
is obtained from either the ?SVO Language Model?
or the ?Language Model? approach, as previously
described.
score = w1 ? vis score + w2 ? nlp score (1)
(2)vis score = P (S|vid) ? P (Vorig|vid)
? Sim(Vsim, Vorig) ? P (O|vid)
After determining the top n=5 object detections
and top k=10 verb detections for each video, we
generate all possible SVO triplets from these nouns
and verbs, including all potential verb expansions.
Each resulting SVO is then scored using Equation 1,
and the best is selected. We compare this approach
to a ?pure vision? baseline where the subject is the
highest scored object detection (which empirically
is more likely to be the subject than the object), the
object is the second highest scored object detection,
and the verb is the activity cluster with the highest
detection probability.
3.7 Surface Realization
Finally, the subject, verb and object from the top-
scoring SVO are used to produce a set of candi-
date sentences, which are then ranked using a lan-
guage model. The text corpora in Table 1 are mined
again to get the top three prepositions for every verb-
object pair. We use a template-based approach in
which each sentence is of the form:
?Determiner (A,The) - Subject - Verb (Present,
Present Continuous) - Preposition (optional) - De-
terminer (A,The) - Object.?
Using this template, a set of candidate sentences are
generated and ranked using the BerkeleyLM lan-
guage model trained on the GoogleNgram corpus.
The top sentence is then used to describe the video.
This surface realization technique is used for both
the vision baseline triplet and our proposed triplet.
In addition to the one presented here, we tried al-
ternative ?pure vision? baselines, but they are not
included since they performed worse. We tried a
non-parametric approach similar to Ordonez et al
(2011), which computes global similarity of the
query to a large captioned dataset and returns the
nearest neighbor?s description. To compute the sim-
ilarity we used an RBF-Chi2 kernel over bag-of-
words STIP features. However, as noted by Ordonez
et al (2011), who used 1 million Flickr images, our
dataset is likely not large enough to produce good
matches. In an attempt to combine information from
both object and activity recognition, we also tried
combining object detections from 20 PASCAL ob-
ject detectors (Felzenszwalb et al, 2008) and from
Object Bank (Li et al, 2010) using a multi-channel
approach as proposed in (Zhang et al, 2007), with a
RBF-Chi2 kernel for the STIP features and a RBF-
Correlation Distance kernel for object detections.
4 Experimental Results
4.1 Content Planning
We first evaluated the ability of the system to iden-
tify the best SVO content. From the ? 50 human
descriptions available for each video, we identified
the SVO for each description and then determined
the ground-truth SVO for each of the 185 test videos
using majority vote. These verbs were then mapped
back to their 58 activity clusters. For the results pre-
sented in Tables 2 and 3, we assigned the vision
score a weight of 0 (w1 = 0) and the NLP score
a weight of 1 (w2 = 1) since these weights gave us
the best performance for thresholds of 5 and 10 for
the objects and activity detections respectively. Note
that while the vision score is given a weight of zero,
the vision detections still play a vital role in the de-
termination of the final triplet since our model only
considers the objects and activities with the highest
vision detection scores.
To evaluate the accuracy of SVO identification,
we used two metrics. The first is a binary metric that
requires exactly matching the gold-standard subject,
verb and object. We also evaluate the overall triplet
accuracy. Note that the verb accuracy in the vision
baseline is not word-based and is measured on the
58 activity classes. Its results are shown in Table 2,
where VE and NVE stand for ?verb expansion?
and ?no verb expansion? respectively. However,
the binary evaluation can be unduly harsh. If we
incorrectly choose ?bicycle? instead of a ?motor-
bike? as the object, it should be considered better
than choosing ?dog.? Similarly, predicting ?chop?
instead of ?slice? is better than choosing ?go?.
14
Method Subject% Verb% Object% All%
Vision Baseline 71.35 8.65 29.19 1.62
LM(VE) 71.35 8.11 10.81 0.00
SVO LM(NVE) 85.95 16.22 24.32 11.35
SVO LM(VE) 85.95 36.76 33.51 23.78
Table 2: SVO Triplet accuracy: Binary metric
Method Subject% Verb% Object% All%
Vision Baseline 87.76 40.20 61.18 63.05
LM(VE) 85.77 53.32 61.54 66.88
SVO LM(NVE) 94.90 63.54 69.39 75.94
SVO LM(VE) 94.90 66.36 72.74 78.00
Table 3: SVO Triplet accuracy: WUP metric
In order to account for such similarities, we also
measure the WUP similarity between the predicted
and correct items. For the examples above, the rel-
evant scores are: wup(motorbike,bicycle)=0.7826,
wup(motorbike,dog)=0.1, wup(slice,chop)=0.8,
wup(slice,go)=0.2857. The results for the WUP
metric are shown in Table 3.
4.2 Surface Realization
Figures 4 and 5 show examples of good and bad sen-
tences generated by our method compared to the vi-
sion baseline.
4.2.1 Automatic Metrics
To automatically compare the sentences gener-
ated for the test videos to ground-truth human de-
scriptions, we employed the BLEU and METEOR
metrics used to evaluate machine-translation output.
METEOR was designed to fix some of the prob-
lems with the more popular BLEU metric. They
both measure the number of matching n-grams (for
various values of n) between the automatic and hu-
man generated sentences. METEOR takes stem-
ming and synonymy into consideration. We used
the SVO Language Model (with verb expansion) ap-
proach since it gave us the best results for triplets.
The results are given in Table 4.
4.2.2 Human Evaluation using Mechanical
Turk
Given the limitations of metrics like BLEU and
METEOR, we also asked human judges to evalu-
ate the quality of the sentences generated by our ap-
Figure 4: Examples where we outperform the baseline
Figure 5: Examples where we underperform the baseline
proach compared to those generated by the baseline
system. For each of the 185 test videos, we asked 9
unique workers (with >95% HIT approval rate and
who had worked on more than 1000 HITs) on Ama-
zon Mechanical Turk to pick which sentence better
described the video. We also gave them a ?none
of the above two sentences? option in case neither
of the sentences were relevant to the video. Qual-
ity was controlled by also including in each HIT a
gold-standard example generated from the human
descriptions, and discarding judgements of workers
who incorrectly answered this gold-standard item.
Overall, when they expressed a preference, hu-
mans picked our descriptions to that of the baseline
Method BLEU score METEOR score
Vision Baseline 0.37?0.05 0.25?0.08
SVO LM(VE) 0.45?0.05 0.36?0.27
Table 4: Automatic evaluation of sentence quality
15
61.04% of the time. Out of the 84 videos where the
majority of judges had a clear preference, they chose
our descriptions 65.48% of the time.
5 Discussion
Overall, the results consistently show the advantage
of utilizing text-mined knowledge to improve the se-
lection of an SVO that best describes a video. Below
we discuss various specific aspects of the results.
Vision Baseline: For the vision baseline, the sub-
ject accuracy is quite high compared to the object
and activity accuracies. This is likely because the
person detector has higher recall and confidence
than the other object detectors. Since most test
videos have a person as the subject, this works in
favor of the vision baseline, as typically the top ob-
ject detection is ?person?. Activity (verb) accuracy
is quite low (8.65% binary accuracy). This is be-
cause there are 58 activity clusters, some with very
little training data. Object accuracy is not as high
as subject accuracy because the true object, while
usually present in the top object detections, is not
always the second-highest object detection. By al-
lowing ?partial credit?, the WUP metric increases
the verb and object accuracies to 40.2% and 61.18%,
respectively.
Language Model(VE): The Language Model ap-
proach performs even worse than the vision baseline
especially for object identification. This is because
we consider the language model score directly for
the SVO triplet without any verb conjugations and
presence of determiners between the verb and ob-
ject. For example, while the GoogleNgram corpus
is likely to contain many instances of a sentence like
?A person is walking with a dog?, it will probably
not contain many instances of ?person walk dog?,
resulting in lower scores.
SVO Language Model(NVE): The SVO Lan-
guage Model (without verb expansion) improves
verb accuracy from 8.65% to 16.22%. For the WUP
metric, we see an improvement in accuracy in all
cases. This indicates that we are getting semanti-
cally closer to the right object compared to the ob-
ject predicted by the vision baseline.
SVO Language Model(VE): When used with
verb expansion, the SVO Language Model approach
results in a dramatic improvement in verb accu-
racy, causing it to jump to 36.76%. The WUP
score increase for verbs between SVO Language
Model(VE) and SVO Language Model(NVE) is mi-
nor, probably because even without verb expansion,
semantically similar verbs are selected but not the
one used in most human descriptions. So, the jump
in verb accuracy for the binary metric is much more
than the one for WUP.
Importance of verb expansion: Verb expansion
clearly improves activity accuracy. This idea could
be extended to a scenario where the test set contains
many activities for which we do not have any ex-
plicit training data. As such, we cannot train activ-
ity classifiers for these ?missing? classes. However,
we can train a ?coarse? activity classifier using the
training data that is available, get the top predictions
from this coarse classifier and then refine them by
using verb expansion. Thus, we can even detect and
describe activities that were unseen at training time
by using text-mined knowledge to determine the de-
scription of an activity that best fits the detected ob-
jects.
Effect of different training corpora: As men-
tioned earlier, we used a variety of textual cor-
pora. Since they cover newswire articles, web pages,
Wikipedia pages and neutral content, we compared
their individual effect on the accuracy of triplet se-
lection. The results of this ablation study are shown
in Tables 5 and 6 for the binary and WUP met-
ric respectively. We also show results for training
the SVO model on the descriptions of the training
videos. The WaCkypedia EN corpus gives us the
best overall results, probably because it covers a
wide variety of topics, unlike Gigaword which is re-
stricted to the news domain. Also, using our SVO
Language Model approach on the triplets from the
descriptions of the training videos is not sufficient.
This is because of the relatively small size and nar-
row domain of the training descriptions in compari-
son to the other textual corpora.
Effect of changing the weight of the NLP score
We experimented with different weights for the Vi-
sion and NLP scores (in Equation 1). These results
can be seen in Figure 6 for the binary-metric evalu-
ation. The WUP-metric evaluation graph is qualita-
tively similar. A general trend seems to be that the
subject and activity accuracies increase with increas-
ing weights of the NLP score. There is a significant
16
Method Subject% Verb% Object% All%
Vision Baseline 71.35 8.65 29.19 1.62
Train Desc. 85.95 16.22 16.22 8.65
Gigaword 85.95 32.43 20.00 14.05
BNC 85.95 17.30 29.73 14.59
ukWaC 85.95 34.05 32.97 22.16
WaCkypedia EN 85.95 35.14 40.00 28.11
All 85.95 36.76 33.51 23.78
Table 5: Effect of training corpus on SVO binary accu-
racy
Method Subject% Verb% Object% All%
Vision Baseline 87.76 40.20 61.18 63.05
Train Desc. 94.95 45.12 61.43 67.17
Gigaword 94.90 63.99 65.71 74.87
BNC 94.88 51.48 73.93 73.43
ukWaC 94.86 60.59 72.83 76.09
WaCkypedia EN 94.90 62.52 76.48 77.97
All 94.90 66.36 72.74 78.00
Table 6: Effect of training corpus on SVO WUP accuracy
improvement in verb accuracy as the NLP weight is
increased towards 1. However, for objects we notice
a slight increase in accuracy until the weight for the
NLP component is 0.9 after which there is a slight
dip. We hypothesize that this dip is caused by the
loss of vision-based information about the objects
which provide some guidance for the NLP system.
BLEU and METEOR results: From the results
in Table 4, it is clear that the sentences generated
by our approach outperform those generated by the
vision baseline, using both the BLEU and METEOR
evaluation metrics.
MTurk results: The Mechanical Turk results
show that human judges generally prefer our sys-
tem?s sentences to those of the vision baseline. As
previously seen, our method improves verbs far
more than it improves subjects or objects. We hy-
pothesize that the reason we do not achieve a simi-
larly large jump in performance in the MTurk evalu-
ation is because people seem to be more influenced
by the object than the verb when both options are
partially irrelevant. For example, in a video of a per-
son riding his bike onto the top of a car, our pro-
posed sentence was ?A person is a riding a motor-
bike? while the vision sentence was ?A person plays
Figure 6: Effect of increasing NLP weights (Binary met-
ric)
a car?, and most workers selected the vision sen-
tence.
Drawback of Using YouTube Videos: YouTube
videos often depict unusual and ?interesting? events,
and these might not agree with the statistics on typ-
ical SVOs mined from text corpora. For instance,
the last video in Figure 5 shows a person dragging a
cat on the floor. Since sentences describing people
moving or dragging cats around are not common in
text corpora, our system actually down-weights the
correct interpretation.
6 Conclusion
This paper has introduced a holistic data-driven
approach for generating natural-language descrip-
tions of short videos by identifying the best subject-
verb-object triplet for describing realistic YouTube
videos. By exploiting knowledge mined from large
corpora to determine the likelihood of various SVO
combinations, we improve the ability to select the
best triplet for describing a video and generate de-
scriptive sentences that are prefered by both au-
tomatic and human evaluation. From our experi-
ments, we see that linguistic knowledge significantly
improves activity detection, especially when train-
ing and test distributions are very different, one of
the advantages of our approach. Generating more
complex sentences with adjectives, adverbs, and
multiple objects and multi-sentential descriptions of
longer videos with multiple activities are areas for
future research.
17
7 Acknowledgements
This work was funded by NSF grant IIS1016312
and DARPA Minds Eye grant W911NF-10-2-0059.
Some of our experiments were run on the Mastodon
Cluster (NSF Grant EIA-0303609).
References
Bangalore, S. and Rambow, O. (2000), Exploiting
a probabilistic hierarchical model for generation,
in ?Proceedings of the 18th conference on Com-
putational linguistics-Volume 1?, Association for
Computational Linguistics, pp. 42?48.
Barbu, A., Bridge, A., Burchill, Z., Coroian, D.,
Dickinson, S., Fidler, S., Michaux, A., Mussman,
S., Narayanaswamy, S., Salvi, D. et al (2012),
Video in sentences out, in ?Proceedings of the
28th Conference on Uncertainty in Artificial In-
telligence (UAI)?, pp. 102?12.
Chang, C. and Lin, C. (2011), ?LIBSVM: a li-
brary for support vector machines?, ACM Trans-
actions on Intelligent Systems and Technology
(TIST) 2(3), 27.
Chen, D., Dolan, W., Raghavan, S., Huynh, T.,
Mooney, R., Blythe, J., Hobbs, J., Domingos, P.,
Kate, R., Garrette, D. et al (2010), ?Collecting
highly parallel data for paraphrase evaluation?,
Journal of Artificial Intelligence Research (JAIR)
37, 397?435.
Chen, S. and Goodman, J. (1999), ?An empirical
study of smoothing techniques for language mod-
eling?, Computer Speech & Language 13(4), 359?
393.
De Marneffe, M., MacCartney, B. and Manning, C.
(2006), Generating typed dependency parses from
phrase structure parses, in ?Proceedings of the In-
ternational Conference on Language Resources
and Evaluation (LREC)?, Vol. 6, pp. 449?454.
Ding, D., Metze, F., Rawat, S., Schulam, P., Burger,
S., Younessian, E., Bao, L., Christel, M. and
Hauptmann, A. (2012), Beyond audio and video
retrieval: towards multimedia summarization, in
?Proceedings of the 2nd ACM International Con-
ference on Multimedia Retrieval?.
Farhadi, A., Hejrati, M., Sadeghi, M., Young, P.,
Rashtchian, C., Hockenmaier, J. and Forsyth, D.
(2010), ?Every picture tells a story: Generating
sentences from images?, Computer Vision?
European Conference on Computer Vision
(ECCV) pp. 15?29.
Felzenszwalb, P., McAllester, D. and Ramanan,
D. (2008), A discriminatively trained, multi-
scale, deformable part model, in ?IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR)?, pp. 1?8.
Khan, M. and Gotoh, Y. (2012), ?Describing video
contents in natural language?, European Chapter
of the Association for Computational Linguistics
(EACL) .
Kojima, A., Tamura, T. and Fukunaga, K. (2002),
?Natural language description of human activities
from video images based on concept hierarchy of
actions?, International Journal of Computer Vi-
sion (IJCV) 50(2), 171?184.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi,
Y., Berg, A. and Berg, T. (2011), Baby talk:
Understanding and generating simple image de-
scriptions, in ?IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR)?, pp. 1601?
1608.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg,
T. L. and Choi, Y. (2012), Collective genera-
tion of natural image descriptions, in ?Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1?, Association for Computational Lin-
guistics, pp. 359?368.
Langkilde, I. and Knight, K. (1998), Generation that
exploits corpus-based statistical knowledge, in
?Proceedings of the 17th international conference
on Computational linguistics-Volume 1?, Associ-
ation for Computational Linguistics, pp. 704?710.
Laptev, I., Marszalek, M., Schmid, C. and Rozen-
feld, B. (2008), Learning realistic human actions
from movies, in ?IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)?, pp. 1?8.
Laptev, I. and Perez, P. (2007), Retrieving actions in
movies, in ?Proceedings of the 11th IEEE Interna-
tional Conference on Computer Vision (ICCV)?,
pp. 1?8.
Lee, M., Hakeem, A., Haering, N. and Zhu, S.
18
(2008), Save: A framework for semantic anno-
tation of visual events, in ?IEEE Computer Vision
and Pattern Recognition Workshops (CVPR-W)?,
pp. 1?8.
Li, L., Su, H., Xing, E. and Fei-Fei, L. (2010), ?Ob-
ject bank: A high-level image representation for
scene classification and semantic feature sparsifi-
cation?, Advances in Neural Information Process-
ing Systems (NIPS) 24.
Li, S., Kulkarni, G., Berg, T., Berg, A. and Choi,
Y. (2011), Composing simple image descriptions
using web-scale n-grams, in ?Proceedings of the
Fifteenth Conference on Computational Natural
Language Learning (CoNLL)?, Association for
Computational Linguistics (ACL), pp. 220?228.
Lin, Y., Michel, J., Aiden, E., Orwant, J., Brockman,
W. and Petrov, S. (2012), Syntactic annotations
for the google books ngram corpus, in ?Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL)?.
Motwani, T. and Mooney, R. (2012), Improving
video activity recognition using object recogni-
tion and text mining, in ?European Conference on
Artificial Intelligence (ECAI)?.
Ordonez, V., Kulkarni, G. and Berg, T. (2011),
Im2text: Describing images using 1 million
captioned photographs, in ?Proceedings of Ad-
vances in Neural Information Processing Systems
(NIPS)?.
Packer, B., Saenko, K. and Koller, D. (2012), A
combined pose, object, and feature model for ac-
tion understanding, in ?IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR)?,
pp. 1378?1385.
Pauls, A. and Klein, D. (2011), Faster and smaller
n-gram language models, in ?Proceedings of the
49th annual meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies?, Vol. 1, pp. 258?267.
Pedersen, T., Patwardhan, S. and Michelizzi, J.
(2004), Wordnet:: Similarity: measuring the re-
latedness of concepts, in ?Demonstration Papers
at Human Language Technologies-NAACL?, As-
sociation for Computational Linguistics, pp. 38?
41.
Reddy, K. and Shah, M. (2012), ?Recognizing 50
human action categories of web videos?, Machine
Vision and Applications pp. 1?11.
Schuldt, C., Laptev, I. and Caputo, B. (2004), Rec-
ognizing human actions: A local SVM approach,
in ?Proceedings of the 17th International Con-
ference on Pattern Recognition (ICPR)?, Vol. 3,
pp. 32?36.
Wang, H., Klaser, A., Schmid, C. and Liu, C.-L.
(2011), Action recognition by dense trajectories,
in ?IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)?, pp. 3169?3176.
Yang, Y., Teo, C. L., Daume?, III, H. and Aloimonos,
Y. (2011), Corpus-guided sentence generation of
natural images, in ?Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP)?, Association for Computa-
tional Linguistics, pp. 444?454.
Yao, B. and Fei-Fei, L. (2010), Modeling mutual
context of object and human pose in human-
object interaction activities, in ?IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR)?.
Yao, B., Yang, X., Lin, L., Lee, M. and Zhu, S.
(2010), ?I2t: Image parsing to text description?,
Proceedings of the IEEE 98(8), 1485?1508.
Zhang, J., Marsza?ek, M., Lazebnik, S. and Schmid,
C. (2007), ?Local features and kernels for classi-
fication of texture and object categories: A com-
prehensive study?, International Journal of Com-
puter Vision (IJCV) 73(2), 213?238.
19
