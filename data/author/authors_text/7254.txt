Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152?159,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Topic-Focused Multi-document Summarization
Using an Approximate Oracle Score
John M. Conroy, Judith D. Schlesinger
IDA Center for Computing Sciences
Bowie, Maryland, USA
conroy@super.org, judith@super.org
Dianne P. O?Leary
University of Maryland
College Park, Maryland, USA
oleary@cs.umd.edu
Abstract
We consider the problem of producing a
multi-document summary given a collec-
tion of documents. Since most success-
ful methods of multi-document summa-
rization are still largely extractive, in this
paper, we explore just how well an ex-
tractive method can perform. We intro-
duce an ?oracle? score, based on the prob-
ability distribution of unigrams in human
summaries. We then demonstrate that with
the oracle score, we can generate extracts
which score, on average, better than the
human summaries, when evaluated with
ROUGE. In addition, we introduce an ap-
proximation to the oracle score which pro-
duces a system with the best known per-
formance for the 2005 Document Under-
standing Conference (DUC) evaluation.
1 Introduction
We consider the problem of producing a multi-
document summary given a collection of doc-
uments. Most automatic methods of multi-
document summarization are largely extractive.
This mimics the behavior of humans for sin-
gle document summarization; (Kupiec, Pendersen,
and Chen 1995) reported that 79% of the sentences
in a human-generated abstract were a ?direct
match? to a sentence in a document. In contrast,
for multi-document summarization, (Copeck and
Szpakowicz 2004) report that no more than 55% of
the vocabulary contained in human-generated ab-
stracts can be found in the given documents. Fur-
thermore, multiple human summaries on the same
collection of documents often have little agree-
ment. For example, (Hovy and Lin 2002) report
that unigram overlap is around 40%. (Teufel and
van Halteren 2004) used a ?factoid? agreement
analysis of human summaries for a single doc-
ument and concluded that a resulting consensus
summary is stable only if 30?40 summaries are
collected.
In light of the strong evidence that nearly half
of the terms in human-generated multi-document
abstracts are not from the original documents, and
that agreement of vocabulary among human ab-
stracts is only about 40%, we pose two coupled
questions about the quality of summaries that can
be attained by document extraction:
1. Given the sets of unigrams used by four hu-
man summarizers, can we produce an extract
summary that is statistically indistinguish-
able from the human abstracts when mea-
sured by current automatic evaluation meth-
ods such as ROUGE?
2. If such unigram information can produce
good summaries, can we replace this infor-
mation by a statistical model and still produce
good summaries?
We will show that the answer to the first question
is, indeed, yes and, in fact, the unigram set infor-
mation gives rise to extract summaries that usually
score better than the 4 human abstractors! Sec-
ondly, we give a method to statistically approxi-
mate the set of unigrams and find it produces ex-
tracts of the DUC 05 data which outperform all
known evaluated machine entries. We conclude
with experiments on the extent that redundancy
removal improves extracts, as well as a method
of moving beyond simple extracting by employ-
ing shallow parsing techniques to shorten the sen-
tences prior to selection.
152
2 The Data
The 2005 Document Understanding Conference
(DUC 2005) data used in our experiments is par-
titioned into 50 topic sets, each containing 25?50
documents. A topic for each set was intended
to mimic a real-world complex questioning-
answering task for which the answer could not
be given in a short ?nugget.? For each topic,
four human summarizers were asked to provide
a 250-word summary of the topic. Topics were
labeled as either ?general? or ?specific?. We
present an example of one of each category.
Set d408c
Granularity: Specific
Title: Human Toll of Tropical Storms
Narrative: What has been the human toll in death or injury
of tropical storms in recent years? Where and when have
each of the storms caused human casualties? What are the
approximate total number of casualties attributed to each of
the storms?
Set d436j
Granularity: General
Title: Reasons for Train Wrecks
Narrative: What causes train wrecks and what can be done
to prevent them? Train wrecks are those events that result
in actual damage to the trains themselves not just accidents
where people are killed or injured.
For each topic, the goal is to produce a 250-
word summary. The basic unit we extract from
a document is a sentence.
To prepare the data for processing, we
segment each document into sentences using
a POS (part-of-speech) tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm). The
newswire documents in the DUC 05 data have
markers indicating the regions of the document,
including titles, bylines, and text portions. All of
the extracted sentences in this study are taken from
the text portions of the documents only.
We define a ?term? to be any ?non-stop word.?
Our stop list contains the 400 most frequently oc-
curring English words.
3 The Oracle Score
Recently, a crisp analysis of the frequency of
content words used by humans relative to the
high frequency content words that occur in the
relevant documents has yielded a simple and
powerful summarization method called SumBa-
sic (Nenkova and Vanderwende, 2005). SumBa-
sic produced extract summaries which performed
nearly as well as the best machine systems for
generic 100 word summaries, as evaluated in DUC
2003 and 2004, as well as the Multi-lingual Sum-
marization Evaluation (MSE 2005).
Instead of using term frequencies of the corpus
to infer highly likely terms in human summaries,
we propose to directly model the set of terms (vo-
cabulary) that is likely to occur in a sample of hu-
man summaries. We seek to estimate the proba-
bility that a term will be used by a human sum-
marizer to first get an estimate of the best possible
extract and later to produce a statistical model for
an extractive summary system. While the primary
focus of this work is ?task oriented? summaries,
we will also address a comparison with SumBa-
sic and other systems on generic multi-document
summaries for the DUC 2004 dataset in Section 8.
Our extractive summarization system is given a
topic, ? , specified by a text description. It then
evaluates each sentence in each document in the
set to determine its appropriateness to be included
in the summary for the topic ?.
We seek a statistic which can score an individ-
ual sentence to determine if it should be included
as a candidate. We desire that this statistic take
into account the great variability that occurs in
the space of human summaries on a given topic
?. One possibility is to simply judge a sentence
based upon the expected fraction of the ?human
summary?-terms that it contains. We posit an or-
acle, which answers the question ?Does human
summary i contain the term t??
By invoking this oracle over the set of terms
and a sample of human summaries, we can
readily compute the expected fraction of human
summary-terms the sentence contains. To model
the variation in human summaries, we use the or-
acle to build a probabilistic model of the space
of human abstracts. Our ?oracle score? will then
compute the expected number of summary terms a
sentence contains, where the expectation is taken
from the space of all human summaries on the
topic ?.
We model human variation in summary gener-
ation with a unigram bag-of-words model on the
terms. In particular, consider P (t|?) to be the
probability that a human will select term t in a
summary given a topic ?. The oracle score for a
sentence x, ?(x), can then be defined in terms of
153
P :
?(x) =
1
|x|
?
t?T
x(t)P (t|?)
where |x| is the number of distinct terms sentence
x contains, T is the universal set of all terms used
in the topic ? and x(t) = 1 if the sentence x con-
tains the term t and 0 otherwise. (We affectionally
refer to this score as the ?Average Jo? score, as it is
derived the average uni-gram distribution of terms
in human summaries.)
While we will consider several approximations
to P (t|?) (and, correspondingly, ?), we first ex-
plore the maximum-likelihood estimate of P (t|?)
given by a sample of human summaries. Suppose
we are given h sample summaries generated in-
dependently. Let cit(?) = 1 if the i-th summary
contains the term t and 0 otherwise. Then the
maximum-likelihood estimate of P (t?) is given
by
P? (t|?) =
1
h
h?
i=1
cit(?).
We define ?? by replacing P with P? in the defi-
nition of ?. Thus, ?? is the maximum-likelihood
estimate for ?, given a set of h human summaries.
Given the score ??, we can compute an extract
summary of a desired length by choosing the top
scoring sentences from the collection of docu-
ments until the desired length (250 words) is ob-
tained. We limit our selection to sentences which
have 8 or more distinct terms to avoid selecting in-
complete sentences which may have been tagged
by the sentence splitter.
Before turning to how well our idealized score,
??, performs on extract summaries, we first define
the scoring mechanism used to evaluate these sum-
maries.
4 ROUGE
The state-of-the-art automatic summarization
evaluation method is ROUGE (Recall Oriented
Understudy for Gisting Evaluation, (Hovy and Lin
2002)), an n-gram based comparison that was mo-
tivated by the machine translation evaluation met-
ric, Bleu (Papineni et. al. 2001). This system uses
a variety of n-gram matching approaches, some of
which allow gaps within the matches as well as
more sophistcated analyses. Surprisingly, simple
unigram and bigram matching works extremely
well. For example, at DUC 05, ROUGE-2 (bi-
gram match) had a Spearman correlation of 0.95
and a Pearson correlation of 0.97 when compared
with human evaluation of the summaries for re-
sponsiveness (Dang 2005). ROUGE-n for match-
ing n?grams of a summary X against h model
human summaries is given by:
Rn(X) =
?h
j=1
?
i?Nn min(Xn(i),Mn(i, j))
?h
j=1
?
i?Nn Mn(i, j),
where Xn(i) is the count of the number of
times the n-gram i occurred in the summary and
Mn(i, j) is the number of times the n-gram i
occurred in the j-th model (human) summary.
(Note that for brevity of notation, we assume that
lemmatization (stemming) is done apriori on the
terms.)
When computing ROUGE scores, a jackknife
procedure is done to make comparison of machine
systems and humans more amenable. In particu-
lar, if there are k human summaries available for
a topic, then the ROUGE score is computed for a
human summary by comparing it to the remaining
k ? 1 summaries, while the ROUGE score for a
machine summary is computed against all k sub-
sets of size k ? 1 of the human summaries and
taking the average of these k scores.
5 The Oracle or Average Jo Summary
We now present results on the performance of
the oracle method as compared with human sum-
maries. We give the ROUGE-2 (R2) scores as
well as the 95% confidence error bars. In Fig-
ure 1, the human summarizers are represented by
the letters A?H, and systems 15, 17, 8, and 4
are the top performing machine summaries from
DUC 05. The letter ?O? represents the ROUGE-2
scores for extract summaries produced by the ora-
cle score, ??. Perhaps surprisingly, the oracle pro-
duced extracts which performed better than the hu-
man summaries! Since each human only summa-
rized 10 document clusters, the human error bars
are larger. However, even with the large error bars,
we observe that the mean ROUGE-2 scores for the
oracle extracts exceeds the 95% confidence error
bars for several humans.
While the oracle was, of course, given the un-
igram term probabilities, its performance is no-
table on two counts. First, the evaluation met-
ric scored on 2-grams, while the oracle was only
given unigram information. In a sense, optimizing
for ROUGE-1 is a ?sufficient statistic? scoring at
154
the human level for ROUGE-2. Second, the hu-
mans wrote abstracts while the oracle simply did
extracting. Consequently, the documents contain
sufficient text to produce human-quality extract
summaries as measured by ROUGE. The human
performance ROUGE scores indicate that this ap-
proach is capable of producing automatic extrac-
tive summaries that produce vocabulary compara-
ble to that chosen by humans. Human evaluation
(which we have not yet performed) is required to
determine to what extent this high ROUGE-2 per-
formance is indicative of high quality summaries
for human use.
The encouraging results of the oracle score nat-
urally lead to approximations, which, perhaps,
will give rise to strong machine system perfor-
mance. Our goal is to approximate P (t|?), the
probability that a term will be used in a human
abstract. In the next section, we present two ap-
proaches which will be used in tandem to make
this approximation.
Figure 1: The Oracle (Average Jo score) Score ??
6 Approximating P (t|?)
We seek to approximate P (t|?) in an analo-
gous fashion to the maximum-likelihood estimate
P? (t|?). To this end, we devise methods to isolate
a subset of terms which would likely be included
in the human summary. These terms are gleaned
from two sources, the topic description and the
collection of documents which were judged rele-
vant to the topic. The former will give rise to query
terms and the latter to signature terms.
6.1 Query Term Identification
A set of query terms is automatically ex-
tracted from the given topic description. We
identified individual words and phrases from
both the <topic> (Title) tagged paragraph as
well as whichever of the <narr> (Narrative)
Set d408c: approximate, casualties,
death, human, injury, number, recent,
storms, toll, total, tropical, years
Set d436j: accidents, actual, causes,
damage, events, injured, killed, prevent,
result, train, train wrecks, trains, wrecks
Table 1: Query Terms for ?Tropical Storms? and
?Train Wrecks? Topics
tagged paragraphs occurred in the topic descrip-
tion. We made no use of the <granularity>
paragraph marking. We tagged the topic de-
scription using the POS-tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm), and
any words that were tagged with any NN (noun),
VB (verb), JJ (adjective), or RB (adverb) tag were
included in a list of words to use as query terms.
Table 1 shows a list of query terms for our two
illustrative topics.
The number of query terms extracted in this way
ranged from a low of 3 terms for document set
d360f to 20 terms for document set d324e.
6.2 Signature Terms
The second collection of terms we use to estimate
P (t|?) are signature terms. Signature terms are
the terms that are more likely to occur in the doc-
ument set than in the background corpus. They
are generally indicative of the content contained
in the collection of documents. To identify these
terms, we use the log-likelihood statistic suggested
by Dunning (Dunning 1993) and first used in sum-
marization by Lin and Hovy (Hovy and Lin 2000).
The statistic is equivalent to a mutual information
statistic and is based on a 2-by-2 contingency ta-
ble of counts for each term. Table 2 shows a list of
signature terms for our two illustrative topics.
6.3 An estimate of P (t|?)
To estimate P (t|?), we view both the query terms
and the signature terms as ?samples? from ideal-
ized human summaries. They represent the terms
that we would most likely see in a human sum-
mary. As such, we expect that these sample terms
may approximate the underlying set of human
summary terms. Given a collection of query terms
and signature terms, we can readily estimate our
target objective, P (t|?) by the following:
Pqs(t|?) =
1
2
qt(?) +
1
2
st(?)
155
Set d408c: ahmed, allison, andrew,
bahamas, bangladesh, bn, caribbean,
carolina, caused, cent, coast, coastal,
croix, cyclone, damage, destroyed, dev-
astated, disaster, dollars, drowned, flood,
flooded, flooding, floods, florida, gulf,
ham, hit, homeless, homes, hugo, hurri-
cane, insurance, insurers, island, islands,
lloyd, losses, louisiana, manila, miles,
nicaragua, north, port, pounds, rain,
rains, rebuild, rebuilding, relief, rem-
nants, residents, roared, salt, st, storm,
storms, supplies, tourists, trees, tropi-
cal, typhoon, virgin, volunteers, weather,
west, winds, yesterday.
Set d436j: accident, accidents, am-
munition, beach, bernardino, board,
boulevard, brake, brakes, braking, cab,
car, cargo, cars, caused, collided, col-
lision, conductor, coroner, crash, crew,
crossing, curve, derail, derailed, driver,
emergency, engineer, engineers, equip-
ment, fe, fire, freight, grade, hit, holland,
injured, injuries, investigators, killed,
line, locomotives, maintenance, mechan-
ical, miles, morning, nearby, ntsb, oc-
curred, officials, pacific, passenger, pas-
sengers, path, rail, railroad, railroads,
railway, routes, runaway, safety, san,
santa, shells, sheriff, signals, southern,
speed, station, train, trains, transporta-
tion, truck, weight, wreck
Table 2: Signature Terms for ?Tropical Storms?
and ?Train Wrecks? Topics
Figure 2: Scatter Plot of ?? versus ?qs
where st(?)=1 if t is a signature term for topic ?
and 0 otherwise and qt(?) = 1 if t is a query term
for topic ? and 0 otherwise.
More sophisticated weightings of the query and
signature have been considered; however, for this
paper we limit our attention to the above ele-
mentary scheme. (Note, in particular, a psuedo-
relevance feedback method was employed by
(Conroy et. al. 2005), which gives improved per-
formance.)
Similarly, we estimate the oracle score of a sen-
tence?s expected number of human abstract terms
as
?qs(x) =
1
|x|
?
t?T
x(t)Pqs(t|?)
where |x| is the number of distinct terms that sen-
tence x contains, T is the universal set of all terms
and x(t) = 1 if the sentence x contains the term t
and 0 otherwise.
For both the oracle score and the approximation,
we form the summary by taking the top scoring
sentences among those sentences with at least 8
distinct terms, until the desired length (250 words
for the DUC05 data) is achieved or exceeded. (The
threshold of 8 was based upon previous analysis
of the sentence splitter, which indicated that sen-
tences shorter than 8 terms tended not be be well
formed sentences or had minimal, if any, content.)
If the length is too long, the last sentence chosen
is truncated to reach the target length.
Figure 2 gives a scatter plot of the oracle score
? and its approximation ?qs for all sentences with
at least 8 unique terms. The overall Pearson corre-
lation coefficient is approximately 0.70. The cor-
relation varies substantially over the topics. Fig-
ure 3 gives a histogram of the Pearson correlation
coefficients for the 50 topic sets.
156
Figure 3: Histogram of Document Set Pearson Co-
efficients of ?? versus ?qs
7 Enhancements
In the this section we explore two approaches to
improve the quality of the summary, linguistic pre-
processing (sentence trimming) and a redundancy
removal method.
7.1 Linguistic Preprocessing
We developed patterns using ?shallow parsing?
techniques, keying off of lexical cues in the sen-
tences after processing them with the POS-tagger.
We initially used some full sentence eliminations
along with the phrase eliminations itemized be-
low; analysis of DUC 03 results, however, demon-
strated that the full sentence eliminations were not
useful.
The following phrase eliminations were made,
when appropriate:
? gerund clauses;
? restricted relative-clause appositives;
? intra-sentential attribution;
? lead adverbs.
See (Dunlavy et. al) for the specific rules used
for these eliminations. Comparison of two runs
in DUC 04 convinced us of the benefit of applying
these phrase eliminations on the full documents,
prior to summarization, rather than on the selected
sentences after scoring and sentence selection had
been performed. See (Conroy et. al. 2004) for
details on this comparison.
After the trimmed text has been generated, we
then compute the signature terms of the document
sets and recompute the approximate oracle scores.
Note that since the sentences have usually had
some extraneous information removed, we expect
some improvement in the quality of the signature
terms and the resulting scores. Indeed, the median
ROUGE-2 score increases from 0.078 to 0.080.
7.2 Redundancy Removal
The greedy sentence selection process we de-
scribed in Section 6 gives no penalty for sentences
which are redundant to information already con-
tained in the partially formed summary. A method
for reducing redundancy can be employed. One
popular method for reducing redundancy is max-
imum marginal relevance (MMR) (2). Based on
previous studies, we have found that a pivoted
QR, a method from numerical linear algebra, has
some advantages over MMR and performs some-
what better.
Pivoted QR works on a term-sentence matrix
formed from a set of candidate sentences for in-
clusion in the summary. We start with enough
sentences so the total number of terms is approx-
imately twice the desired summary length. Let B
be the term-sentence matrix with Bij = 1 if sen-
tence j contains term i.
The columns of B are then normalized so their
2-norm (Euclidean norm) is the corresponding ap-
proximate oracle score, i.e. ?qs(bj), where bj is
the j-th column ofB.We call this normalized term
sentence matrix A.
Given a normalized term-sentence matrix A,
QR factorization attempts to select columns of A
in the order of their importance in spanning the
subspace spanned by all of the columns. The stan-
dard implementation of pivoted QR decomposi-
tion is a ?Gram-Schmidt? process. The first r sen-
tences (columns) selected by the pivoted QR are
used to form the summary. The number r is cho-
sen so that the summary length is close to the tar-
get length. A more complete description can be
found in (Conroy and O?Leary 2001).
Note, that the selection process of using the piv-
oted QR on the weighted term sentence matrix
will first choose the sentence with the highest ?pq
score as was the case with the greedy selection
process. Its subsequent choices are affected by
previous choices as the weights of the columns are
decreased for any sentence which can be approxi-
mated by a linear combination of the current set of
selected sentences. This is more general than sim-
ply demanding that the sentence have small over-
lap with the set of previous chosen sentences as
157
Figure 4: ROUGE-2 Performance of Oracle Score
Approximations ?? vs. Humans and Peers
would be done using MMR.
8 Results
Figure 4 gives the ROUGE-2 scores with error
bars for the approximations of the oracle score as
well as the ROUGE-2 scores for the human sum-
marizers and the top performing systems at DUC
2005. In the graph, qs is the approximate oracle,
qs(p) is the approximation using linguistic prepro-
cessing, and qs(pr) is the approximation with both
linguistic preprocessing and redundancy removal.
Note that while there is some improvement using
the linguistic preprocessing, the improvement us-
ing our redundancy removal technique is quite mi-
nor. Regardless, our system using signature terms
and query terms as estimates for the oracle score
performs comparably to the top scoring system at
DUC 05.
Table 3 gives the ROUGE-2 scores for the re-
cent DUC 06 evaluation which was essentially
the same task as for DUC 2005. The manner in
which the linguistic preprocessing is performed
has changed from DUC 2005, although the types
of removals have remained the same. In addition,
pseudo-relevance feedback was employed for re-
dundancy removal as mentioned earlier. See (Con-
roy et. al. 2005) for details.
While the main focus of this study is task-
oriented multidocument summarization, it is in-
structive to see how well such an approach would
perform for a generic summarization task as with
the 2004 DUC Task 2 dataset. Note, the ? score
for generic summaries uses only the signature
term portion of the score, as no topic descrip-
tion is given. We present ROUGE-1 (rather than
Submission Mean 95% CI Lower 95% CI Upper
O (?) 0.13710 0.13124 0.14299
C 0.13260 0.11596 0.15197
D 0.12380 0.10751 0.14003
B 0.11788 0.10501 0.13351
G 0.11324 0.10195 0.12366
F 0.10893 0.09310 0.12780
H 0.10777 0.09833 0.11746
J 0.10717 0.09293 0.12460
I 0.10634 0.09632 0.11628
E 0.10365 0.08935 0.11926
A 0.10361 0.09260 0.11617
24 0.09558 0.09144 0.09977
?(pr)qs 0.09160 0.08729 0.09570
15 0.09097 0.08671 0.09478
12 0.08987 0.08583 0.09385
8 0.08954 0.08540 0.09338
23 0.08792 0.08371 0.09204
?(p)qs 0.08738 0.08335 0.09145
?qs 0.08713 0.08317 0.09110
28 0.08700 0.08332 0.09096
Table 3: Average ROUGE 2 Scores for DUC06:
Humans A-I
ROUGE-2) scores with stop words removed for
comparison with the published results given in
(Nenkova and Vanderwende, 2005).
Table 4 gives these scores for the top perform-
ing systems at DUC04 as well as SumBasic and
?(pr)qs , the approximate oracle based on signature
terms alone with linguistic preprocess trimming
and pivot QR for redundancy removal. As dis-
played, ?(pr)qs scored second highest and within the
95% confidence intervals of the top system, peer
65, as well as SumBasic, and peer 34.
Submission Mean 95% CI Lower 95% CI Upper
F 0.36787 0.34442 0.39467
B 0.36126 0.33387 0.38754
O (?) 0.35810 0.34263 0.37330
H 0.33871 0.31540 0.36423
A 0.33289 0.30591 0.35759
D 0.33212 0.30805 0.35628
E 0.33277 0.30959 0.35687
C 0.30237 0.27863 0.32496
G 0.30909 0.28847 0.32987
?(pr)qs 0.308 0.294 0.322
peer 65 0.308 0.293 0.323
SumBasic 0.302 0.285 0.319
peer 34 0.290 0.273 0.307
peer 124 0.286 0.268 0.303
peer 102 0.285 0.267 0.302
Table 4: Average ROUGE 1 Scores with stop
words removed for DUC04, Task 2
158
9 Conclusions
We introduced an oracle score based upon the
simple model of the probability that a human
will choose to include a term in a summary.
The oracle score demonstrated that for task-based
summarization, extract summaries score as well
as human-generated abstracts using ROUGE. We
then demonstrated that an approximation of the or-
acle score based upon query terms and signature
terms gives rise to an automatic method of summa-
rization, which outperforms the systems entered
in DUC05. The approximation also performed
very well in DUC 06. Further enhancements based
upon linguistic trimming and redundancy removal
via a pivoted QR algorithm give significantly bet-
ter results.
References
Jamie Carbonnell and Jade Goldstein ?The of MMR,
diversity-based reranking for reordering documents
and producing summaries.? In Proc. ACM SIGIR,
pages 335?336.
JohnM. Conroy and Dianne P. O?Leary. ?Text Summa-
rization via HiddenMarkovModels and Pivoted QR
Matrix Decomposition?. Technical report, Univer-
sity of Maryland, College Park, Maryland, March,
2001.
John M. Conroy and Judith D. Schlesinger and
Jade Goldstein and Dianne P. O?Leary, Left-
Brain Right-Brain Multi-Document Summariza-
tion, Document Understanding Conference 2004
http://duc.nist.gov/ 2004
John M. Conroy and Judith D. Schlesinger and Jade
Goldstein, CLASSY Tasked Based Summarization:
Back to Basics, Document Understanding Confer-
ence 2005 http://duc.nist.gov/ 2005
John M. Conroy and Judith D. Schlesinger Dianne
P. O?Leary, and Jade Goldstein, Back to Basciss:
CLASSY 2006, Document Understanding Confer-
ence 2006 http://duc.nist.gov/ 2006
Terry Copeck and Stan Szpakowicz 2004 Vocabulary
Agreement Among Model Summaries and Source
Documents In ACL Text Summarization Workshop,
ACL 2004.
Hoa Trang Dang Overview of DUC 2005 Document
Understanding Conference 2005 http://duc.nist.gov
Daniel M. Dunlavy and John M. Conroy and Judith
D. Schlesinger and Sarah A. Goodman and Mary
Ellen Okurowski and Dianne P. O?Leary and Hans
van Halteren, ?Performance of a Three-Stage Sys-
tem for Multi-Document Summarization?, DUC 03
Conference Proceedings, http://duc.nist.gov/, 2003
Ted Dunning, ?Accurate Methods for Statistics of Sur-
prise and Coincidence?, Computational Linguistics,
19:61-74, 1993.
Julian Kupiec,, Jan Pedersen, and Francine Chen. ?A
Trainable Document Summarizer?. Proceedings of
the 18th Annual International SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68?73, 1995.
Chin-Yew Lin and Eduard Hovy. The automated ac-
quisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495?501, Morristown, NJ,
USA, 2000. Association for Computational Linguis-
tics.
Chin-Yew Lin and Eduard Hovy. Manual and Auto-
matic Evaluation of Summaries In Document Un-
derstanding Conference 2002 http:/duc.nist.gov
Multi-Lingual Summarization Evaluation
http://www.isi.edu/ cyl/MTSE2005/MLSummEval.html
NLProcessor http://www.infogistics.com/posdemo.htm
Ani Nenkova and Lucy Vanderwende. 2005. The
Impact of Frequency on Summarization,MSR-TR-
2005-101. Microsoft Research Technical Report.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-Jing Zhu, Bleu: a method for automatic
evaluation of machine translation, Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center (2001)
Simone Teufel and Hans van Halteren. 2004. 4:
Evaluating Information Content by Factoid Analy-
sis: Human Annotation and Stability, EMNLP-04,
Barcelona
159
Squibs
Nouveau-ROUGE: A Novelty Metric for
Update Summarization
John M. Conroy?
IDA/Center for Computing Sciences
Judith D. Schlesinger?
IDA/Center for Computing Sciences
Dianne P. O?Leary??
University of Maryland
An update summary should provide a fluent summarization of new information on a time-
evolving topic, assuming that the reader has already reviewed older documents or summaries.
In 2007 and 2008, an annual summarization evaluation included an update summarization
task. Several participating systems produced update summaries indistinguishable from human-
generated summaries when measured using ROUGE. However, no machine system performed
near human-level performance in manual evaluations such as pyramid and overall responsive-
ness scoring.
We present a metric called Nouveau-ROUGE that improves correlation with manual
evaluation metrics and can be used to predict both the pyramid score and overall responsiveness
for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual
evaluations when comparing existing systems and when developing new ones.
1. Introduction
Update summaries focus on what is new relative to a previous body of information.
They pose new challenges both to algorithm developers and to evaluation of sum-
maries. In 2007, DUC (Document Understanding Conference) introduced an update
summarization task, repeated in 2008 for TAC (Text Analysis Conference).1 This task
consisted of producing a multi-document summary for a set of articles on a single topic,
followed by one (2008) or two (2007) multi-document summaries for sets of articles on
? Institute for Defense Analyses, Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715
USA. E-mail: {judith,conroy}@super.org.
?? Computer Science Department, Institute for Advanced Computer Studies, University of Maryland,
College Park, MD 20742 USA. E-mail: oleary@cs.umd.edu.
1 DUC (http://duc.nist.gov), the summarization evaluation event, was replaced in 2008 by TAC
(http://www.nist.gov/tac). Both were sponsored by NIST, the U.S. National Institute of Standards
and Technology.
Submission received: 14 January 2010; revised submission received: 15 May 2010; accepted for publication:
27 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
the same topic published at later dates. The goal was to generate a good first summary,
along with update(s) that contained new content and minimized redundancy.
The modifier manual is used to identify evaluations, and the corresponding scores,
produced by humans. The modifier automatic is used to identify evaluations, and
the corresponding scores, produced by machines. Similarly, human-generated and
machine-generated will be used to distinguish between summaries created by humans
and those generated by machine systems, respectively.2
Because we are working with update summarization, there is a minimum of two
summaries for a set of documents. The first summary for the document set is called the
original (Task A) summary and a later summary is called an update (Task B) summary.
Several machine summarizing systems produced update summaries that were
statistically indistinguishable from human-generated summaries, as measured by the
ROUGE metrics, the standard metrics for automatic evaluation of summaries. However,
none of these machine systems performed near human levels in overall responsiveness
or pyramid evaluation, the currently used manual evaluation metrics.
We define the metric gap (or gap) as the distance between a prediction of a manual
score, based on automatic scores, and the observed manual score.
The purpose of our work is to investigate and mitigate this metric gap by in-
troducing an automatic evaluation that is a better predictor of manual evaluation.
Reducing the metric gap is important for two reasons. First, the gap severely limits the
usefulness of automatic evaluation and forces the use of much more expensive manual
evaluation for comparing existing systems. More importantly, however, this gap is a
severe handicap to research on new update summarization methods because it makes
it difficult to evaluate new ideas and compare them with existing methods.
In Section 2, we analyze the results of the TAC 2008 summarization task, demon-
strating the large gap between ROUGE automatic metrics and manual evaluation of
update summaries. In Section 3, we modify ROUGE to produce scores that correlate
significantly better with manual evaluation. We evaluate our new metric on TAC 2008
data in Section 4, demonstrating its superiority as a predictor of manual evaluations.
2. State-of-the-Art Evaluation of Update Summaries
TAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, A
and B. Subset B documents were more recent. Original summaries were generated for
the A subsets and update summaries were then produced for the B subsets.
In TAC 2008, ROUGE was used for automatic evaluation. ROUGE (Lin and Hovy
2000) compares any summary to any other (typically human-generated) summary using
a recall-oriented approach. ROUGE-1 and -2 are based on unigrams and bigrams,
respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between
bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic
elements, computed via parsing or automatic entity recognition. ROUGE-2, ROUGE-
SU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to com-
pare machine-generated summaries to human-generated summaries, and to compare
human-generated summaries to each other using a jackknife approach. In addition to
the three official metrics, we include ROUGE-1 in our study as it is often competitive
with the official metrics.
2 NIST uses ?model? for human-generated summaries and ?peer? for machine-generated summaries.
2
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Three manual evaluation metrics were used in TAC 2008: pyramid, overall respon-
siveness, and linguistic quality (not considered in our work). The pyramid method
(Nenkova and Passonneau 2004) is a content-based metric for which human annotators
mark content units in the human-generated summaries. The content units are collected
across a set of human-generated summaries for a topic, and a weight is computed
based on how many human-generated summaries include this content unit. TAC 2008
also used a manual overall responsiveness score. After evaluating data from 2005?
2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which eval-
uates summary usefulness including linguistic quality, is a reliable and stable manual
evaluation.
We analyzed the three official TAC 2008 automatic evaluation scores to see how
well they predict the manual evaluation metrics of overall responsiveness and pyramid
score. Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the three
official ROUGE measures for the TAC 2008 update task. Each solid data point represents
the average score for a human summarizer over 24 document sets, and a dashed line
marks the minimum; each open data point represents the average score for a machine
system. We also show robust linear least squares fits to the data as well as the Pearson
correlation coefficients between ROUGE-BE, -2, and -SU4 and the manual-evaluation
scores. Surprisingly, these correlations are higher for the update task than for the
original summarization task (data not shown); nevertheless, the gap between the lines?
predictions and the scores for the human-generated summaries is larger.
Figure 1
TAC 2008: The update task (Task B) responsiveness and pyramid scores vs. ROUGE scores;
human-generated summaries (solid points) and machine-generated summaries (open points).
3
Computational Linguistics Volume 37, Number 1
We report correlation coefficients only for the machine-generated summaries. Scores
for the human-generated summaries are distributed differently, and correlation for the
set of human-generated summaries is often not significant due to the small number of
human summarizers.
The correlation coefficients in Figure 1 show that the automatic metrics do well
in predicting responsiveness and pyramid scoring for machine-generated summaries. In
contrast, the scores for human-generated summaries far exceed the predictions, with
a large gap between predicted and actual scores. As may be expected, ROUGE is more
highly correlated with the pyramid evaluation, which is a pure content evaluation score,
whereas the responsiveness score also reflects linguistic quality.
3. Improving Automatic Evaluation?Nouveau-ROUGE
We more formally define the metric gap to be the absolute value of the difference between
a manual evaluation score and our prediction of it based only on automatic evaluation
scores. A number of TAC 2008 machine systems performed within statistical confidence
of human performance in the automatic evaluation metrics, but no system performed
near human performance in the manual evaluations. This has also been observed in
previous summarization evaluations (Conroy and Dang 2008). Progress has been made
in closing this metric gap but it persists, especially for update summaries.
A good update summary must contain essential information but focus on new
information. When a machine-generated update summary is good, it is similar to the
human-generated update summaries. This is assessed quite well by a ROUGE score. But the
machine-generated update summary should also be different from the human-generated
original summaries, and we need an automatic metric to assess this difference, or lack of
redundancy. We suggest using a ROUGE score to measure similarity, and thus redun-
dancy, between a given original summary and an update summary: A high ROUGE
score indicates high redundancy.
To illustrate this, we used the CLASSY algorithm (Conroy, Schlesinger, and O?Leary
2006; Schlesinger, O?Leary, and Conroy 2008) to produce original summaries and update
summaries for the TAC 2008 data. We also produced update summaries using a variant,
projected-CLASSY, that reduces overlap by using a linear algebra projection of the
term-sentence matrix (Conroy, Schlesinger, and O?Leary 2007) of candidate sentences
against the matrix for the original (Task A) summary in order to favor new infor-
mation. Table 1 gives average ROUGE-2 scores and 95% confidence intervals, com-
puted via bootstrapping (Efron and Tibshirani 1993), over the 48 document sets.
Two scores are given: R(BB)2 compares each CLASSY update (Task B) summary to the
human-generated summaries, and R(AB)2 compares each to the original (Task A) model
summaries. Whereas the two variants score comparably using R(BB)2 , there is a significant
difference in the R(AB)2 metric, as desired.
Table 1
TAC 2008: Average ROUGE-2 scores and 95% confidence intervals for update summaries
produced by two variants of CLASSY.
Variation R(BB)2 R
(AB)
2
projected-CLASSY 0.087 (0.080, 0.094) 0.075 (0.070, 0.079)
CLASSY 0.089 (0.082, 0.096) 0.083 (0.078, 0.088)
4
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Table 2
TAC 2008: Nouveau-ROUGE ?-parameters.
Predicting Responsiveness Predicting Pyramid Scores
?i,0 ?i,1 ?i,2 ?i,0 ?i,1 ?i,2
R1 ?0.0271 ?7.3550 13.4227 ?0.2143 ?1.9011 3.1118
R2 0.9126 ?5.4536 21.1556 ?0.0143 ?1.3499 4.3778
RSU4 1.1381 ?2.6931 35.8555 0.0346 ?1.1680 7.2589
RBE 1.0602 ?5.0811 24.8365 0.0145 ?1.3156 5.0446
Given this evidence, we propose predicting manual scores for update summaries by
using two ROUGE scores, R(AB)i and R
(BB)
i (i = 1, 2, SU4, ...), in a three-parameter model
called Nouveau-ROUGE:
Ni = ?i,0 + ?i,1R
(AB)
i + ?i,2R
(BB)
i
We determine the ? parameters (Table 2) using robust linear regression on the TAC 2008
evaluation data so that the Nouveau-ROUGE score Ni best predicts the manual scores
of responsiveness and pyramid performance.
Nouveau-ROUGE could be used by researchers to predict how a new system would
compare with the TAC 2008 systems in overall responsiveness and pyramid scoring, a
comparison that up to now has been impossible.
4. Evaluating Nouveau-ROUGE
We evaluate Nouveau-ROUGE using cross validation studies to demonstrate that if
manual scores are available for a subset of summaries (in this case, those from 29
machine systems, half of those that participated in TAC 2008), then Nouveau-ROUGE
can predict manual scores for the remaining (held-back) summaries.
4.1 Improved Correlation with Manual Evaluation
Correlation scores between automatic and manual scores have traditionally been used
as a measure of the effectiveness of automatic evaluation as a surrogate for manual
evaluation. Pearson correlation coefficients, shown in Table 3, were computed for the
scores for the held-back subset of summaries. Correlation is indeed higher for the
Nouveau-ROUGE scores than for any of the ROUGE scores.
Table 3
Correlation scores for TAC 2008 human evaluations.
Average Responsiveness Score Average Pyramid Score
Metric i = 1 i = 2 i = SU4 i = BE i = 1 i = 2 i = SU4 i = BE
R(AB)i 0.676 0.576 0.619 0.490 0.698 0.592 0.634 0.483
R(BB)i 0.870 0.921 0.902 0.933 0.910 0.952 0.933 0.964
Ni 0.888 0.929 0.912 0.935 0.946 0.961 0.951 0.969
5
Computational Linguistics Volume 37, Number 1
Figure 2
TAC 2008: ROUGE and Nouveau-ROUGE responsiveness and pyramid predictions for
subtask B.
6
Conroy, Schlesinger, and O?Leary Nouveau-ROUGE
Table 4
TAC 2008: Median Pearson correlation coefficients for automatic vs. manual evaluations.
Average Responsiveness Score Jackknife Pyramid Score
Metric R(AB)i R
(BB)
i Ni p-value R
(AB)
i R
(BB)
i Ni p-value
R1 0.378 0.804 0.920 5.4e-284 0.406 0.837 0.943 3.5e-307
R2 0.149 0.889 0.925 1.6e-104 0.177 0.909 0.941 1.8e-99
RSU4 0.267 0.846 0.913 1.3e-176 0.291 0.875 0.933 8.7e-214
RBE 0.222 0.913 0.919 6.2e-09 0.243 0.924 0.933 1.7e-17
Figure 2 shows that ROUGE-BE and ROUGE-1 predictions of both responsiveness
and pyramid scores are inferior to the Nouveau-ROUGE-BE predictions. Plots for N2
and NSU4 are omitted due to space restrictions, but performance improvement relative
to ROUGE is greater than that for NBE and less than that for N1.
4.2 Validation Using Bootstrapping Experiments
To show that our results are not due to a lucky partitioning of the data, we used
bootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us to
compute our statistical confidence in the results. This model assumes that observed data
(scores for the 58 systems) characterize all data. Given this model, the proper sampling
method is to choose subsets with replacement. We chose 58 systems (with replacement)
and used half to determine the Nouveau-ROUGE parameters and half to test the model.
We repeated this process 1,000 times. Table 4 gives the correlation coefficients (for the
tested-half of the data) for all four ROUGE metrics with each of the manual evalua-
tions when comparing the machine-generated summaries with the human-generated
summaries. Data in the columns labeled ?p-value? result from a Mann-Whitney U-
test for equal medians of R(BB)i and Ni of the distributions of correlations returned by
the bootstrapping procedure. Because all p-values are small, we can conclude that the
differences between the R(BB)i and Ni correlation scores are statistically significant for all
variants of ROUGE.
4.3 Narrowing the Gap for Update Summaries
Table 5 gives the median gap on the TAC 2008 data for predicting responsiveness and
pyramid scores. Recall that the gap is the absolute value of the difference between the
Table 5
Narrowing the TAC 2008 metric gap.
Responsiveness Metric Gap Pyramid Metric Gap
Metric R Gap N Gap p-value R Gap N Gap p-value
R1 2.025 1.277 7.8e-03 0.285 0.187 7.8e-03
R2 1.655 1.518 7.8e-03 0.241 0.197 7.8e-03
RSU4 1.887 1.344 7.8e-03 0.273 0.206 7.8e-03
RBE 1.591 1.547 7.8e-03 0.229 0.206 7.8e-03
7
Computational Linguistics Volume 37, Number 1
manual score and the prediction of it. The median gap is always smaller for Nouveau-
ROUGE than for ROUGE; in fact, the gap is smaller on every trial.
We used the Wilcox sign test to test the significance of this observation. The null
hypothesis is that the differences in the gaps between ROUGE and Nouveau-ROUGE
has median 0. The p-values from the Wilcox test indicate that the null hypothesis is true
with probability 1128 ? 7.812 ? 10?3, so it can be safely rejected.
5. Conclusion
Our new metric, Nouveau-ROUGE, includes a measure of novelty for an update sum-
mary. We demonstrated that it has higher correlation to manual evaluation of overall
responsiveness and to pyramid scores than does ROUGE. The most obvious deficiency
in ROUGE is the lack of a linguistic quality measurement, which we take to encompass all
language-related issues: lexical, syntactic, and semantic. Therefore, we conjecture that
most remaining prediction error in Nouveau-ROUGE is a result of omitting linguistic
quality and caution that better prediction would be achieved only for systems of
comparable linguistic quality.
We believe that responsiveness is an imperfect surrogate for task-based summary
evaluation such as that done in SUMMAC (Mani et al 1999). We would welcome
a return to task-based evaluation, as well as research increasing the reliability and
consistency of manual evaluation metrics. Investigation could also quantify the impact
of low responsiveness and pyramid scores on the ability to perform a specific task.
References
Conroy, John M. and Hoa Trang Dang. 2008.
Mind the gap: Dangers of divorcing
evaluations of summary content from
linguistic quality. In Proceedings of the 22nd
International Conference on Computational
Linguistics (Coling 2008), pages 145?152,
Manchester.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. O?Leary. 2006. Topic-focused
multi-document summarization using an
approximate oracle score. In Proceedings of
the COLING/ACL 2006 Main Conference
Poster Sessions, pages 152?159, Sydney.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. O?Leary. 2007. CLASSY 2007 at
DUC 2007. In Proceedings of the Seventh
Document Understanding Conference (DUC),
Rochester, NY. Available at http://duc.
nist.gov/pubs.html#2007.
Dang, Hoa Trang. 2007. Overview of DUC
2007. In Proceedings of the Seventh Document
Understanding Conference (DUC), Rochester,
NY. Available at http://duc.nist.gov/
pubs.html#2005.
Efron, B. and R. J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman
& Hall, New York.
Hovy, Eduard, Chin-Yew Lin, and Liang
Zhou. 2005. Evaluating DUC 2005 using
basic elements. In Proceedings of the Fifth
Document Understanding Conference (DUC),
Vancouver. Available at www-nlpir/
nist.gov/projects/duc/pubs.html.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings
of the 18th Conference on Computational
Linguistics, pages 495?501, Morristown, NJ.
Mani, Inderjeet, Therese Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 1999. The TIPSTER
SUMMAC text summarization evaluation.
In Proceedings of EACL?99: Ninth Conference
of the European Chapter of the Association for
Computational Linguistics, pages 77?85,
Bergen.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145?152,
Boston, MA.
Schlesinger, Judith D., Dianne P. O?Leary,
and John M. Conroy. 2008. Arabic/
English multi-document summarization
with CLASSY?The past and the future.
In Alexander F. Gelbukh, editor, CICLing,
volume 4919 of Lecture Notes in Computer
Science. Springer, Haifa, pages 568?581.
8

Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 29?38,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
ACL 2013 MultiLing Pilot Overview
Jeff Kubina
U.S. Department of Defense
9800 Savage Rd., Ft. Meade, MD 20755
jmkubin@tycho.ncsc.mil
John M. Conroy, Judith D. Schlesinger
IDA/Center for Computing Sciences
17100 Science Dr., Bowie, MD
conroy@super.org, drj1945@gmail.com
Abstract
The 2013 Association for Computational
Linguistics MultiLing Pilot posed a task
to measure the performance of multi-
lingual, single-document, summarization
systems using a dataset derived from many
Wikipedias. The objective of the pilot
was to assess automatic summarization of
multilingual text documents outside the
news domain and the potential of using
Wikipedia articles for such research. This
report describes the pilot task, the dataset,
the methods used to evaluate the submitted
summaries, and the overall performance of
each participant?s system.
1 Introduction
Document summarization is an active subject of
research and development. The ACM Digital Li-
brary has about 806 reports on the subject pub-
lished since 1993, with over half of them appear-
ing in the last five years. While the impetus for
much of this research is the annual Text Anal-
ysis Conference (TAC) workshop on document
summarization, there is a growing demand in the
consumer market for news summarization appli-
cations being met by tablet and smart-phone ap-
plications such as Clipped1, Summoner2, TLDR3,
and Yahoo News. Yahoo and Google even ac-
quired two companies developing such applica-
tions, Summly (Stelter, 2013) and Wavii (Tsotsis,
2013) respectively, earlier this year. While sum-
marization technology for news sources is com-
ing to fruition, the performance of such technol-
ogy on non-English documents outside the news
domain has not been throughly assessed and may
need further research. Since the datasets used by
1
http://goo.gl/dFKD9
2
http://goo.gl/0QFaZ
3
http://goo.gl/qEgCs
the TAC summarization workshops have predom-
inately been English news articles, with some ex-
ceptions (Giannakopoulos et al, 2011), the objec-
tive of the 2013 ACL MultiLing Pilot was to assess
the performance of automatic multilingual single-
document summarization systems on non-English
text outside the news domain and to determine the
potential of using Wikipedia articles for such re-
search.
This report starts with a description of the task
and dataset, the methods used to evaluate the sub-
mitted summaries, the performance of each partic-
ipating system, and concludes with an assessment
of the pilot and potential future work.
2 Task and Dataset Description
The objective of each participant system of the pi-
lot was simple: compute a summary for each doc-
ument in at least two of the datasets languages.
No restrictions were placed on the languages that
could be chosen nor was any target summary size
specified.
The dataset was derived from a corpus cre-
ated in 2010 to measure the performance of the
CLASSY (Conroy et al, 2009) summarization al-
gorithm on non-English documents outside the
news domain. At the time such a corpus did not
exist so one was created from the Wikipedias.
To date there are Wikipedias in 285 languages
comprising over 75 million pages. Some of the
Wikipedias maintain a list of Feature Articles,
which are articles reviewed and voted upon by ed-
itors as the best that fulfill Wikipedia?s require-
ments in accuracy, neutrality, completeness, and
style. One such requirement is that the article have
a lead section that should
. . . be able to stand alone as a concise
overview. It should . . . summarize the
most important points . . . [and] material
in the lead should roughly reflect its im-
29
portance to the topic . . . 4
So the lead section of a featured article is
an excellent summary of it, hence, the fea-
tured articles were used to create the corpus.
In 2010 there were 41 Wikipedias with more
than nine featured articles. The Perl module
Text::Corpus::Summaries::Wikipedia
5 was de-
veloped to automatically create the corpus from
the featured articles of those Wikipedias. The cor-
pus is publicly available (Kubina, 2010) and the
Perl module can be used to create an updated cor-
pus.
The dataset for the pilot was created from a
subset of the 2010 corpus. This was done to en-
sure that each language had 30 articles and that
the size of each article?s body text was sufficiently
large. First, for each article the summary and body
were compressed to approximate their informa-
tion content size. For example, given a Chinese
and English article with the same character length
the Chinese article will usually contain more in-
formation than an English article and their com-
pressed sizes will approximation their true infor-
mation content. Next, if the compressed body
size of an article was less than five times its com-
pressed summary size, then the article was dis-
carded. The factor of five was simply chosen to
ensure the body of each article was sufficiently
large relative to the summary size. For each lan-
guage the median of the ratio of compressed body
size to compressed summary size was computed
and only the 30 articles closest to the median were
included in the dataset. This filtering reduced the
corpus from 12, 819 articles in 41 languages to the
dataset containing 1, 200 articles in 40 languages.
For each language in the dataset Table 1 contains
the mean size of the articles, their bodies, and their
summaries, in characters.
3 Evaluation Methods and Results
Four teams submitted the results of six summa-
rization systems. The teams are denoted by AIS,
LAN, MD, and MUS; the MD team submitted
three systems. Throughout this report the systems
are denoted by AIS, LAN, MD1, MD2, MD3, and
MUS. Table 2 contains the list of languages sub-
mitted for each system and the mean size, in char-
acters, of the summaries submitted.
4
http://en.wikipedia.org/wiki/Wikipedia:LEAD
5
http://goo.gl/ySgOS
For the evaluation a baseline summary was ex-
tracted from the each article in the dataset that is
the prefix substring of the article?s body text with
the same length as the text in the lead section of
the article. For the remainder of this report the
lead section of an article is called the human sum-
mary. An oracle summary was also computed for
each article by heuristically extracting sentences
from its body text to maximize its ROUGE-2 score
against the human summary until its size exceeded
the human summary, upon which it was truncated.
Submitted summaries were automatically eval-
uated against the human summary of each arti-
cle using ROUGE-1, ROUGE-2 (Lin, 2004) and
MeMoG (Giannakopoulos et al, 2008). For
ROUGE, the languages Chinese, Japanese, Ko-
rean, and Thai were tokenized into individual
characters. For MeMoG the character n-gram size
used for each language is listed in Table 3, which
is the n-gram size that maximized the standard de-
viation divided by the mean of the n-gram fre-
quency distribution of the language in the dataset.
So the selected n-gram size maximizes the vari-
ability of the distribution values relative to their
mean. A shorter n-gram size would inflate the
MeMoG scores because of their inherent frequent
co-occurrence and conversely a longer size would
penalize MeMoG scores due to their infrequent
co-occurrence.
Each scoring method was performed twice, first
by truncating, if necessary, each system summary
to the size of the human summary, which is called
HSS-scoring. The second set of scores were com-
puted by truncating all the summaries of an ar-
ticle, including the human summary, to the size
of the shortest summary amongst the system and
human summaries for the article, which is called
SSS-scoring. For HSS-scoring the system sum-
maries shorter that the human summary are penal-
ized since ROUGE is recall oriented. Alternately,
SSS-scoring gives preference to shorter system
summaries that have their best content (extracted
sentences) first.
The performance for HSS-scoring of the sys-
tems on the seven languages that at least two teams
submitted summaries for are given in Figures 1, 2,
and 3. Table 4 gives an overview of how often sig-
nificant differences in each of the three automatic
metrics was observed. In particular, the last row
gives the fraction of times that an non-parametric
analysis of variance (ANOVA) indicated that the
30
Table 1: Dataset Languages and Sizes
ISO LANGUAGE ARTICLE BODY SUMMARY
af Afrikaans 24752 (10214) 23448 (10230) 1303 (196)
ar Arabic 27845 (9490) 26354 (9530) 1491 (220)
bg Bulgarian 23965 (9248) 22981 (9250) 984 (134)
ca Catalan 30611 (15248) 29322 (15274) 1289 (140)
cs Czech 26300 (10453) 24777 (10414) 1522 (190)
de German 32023 (12522) 31160 (12530) 862 (53)
el Greek 26072 (11113) 24937 (11096) 1134 (224)
en English 26572 (9010) 24860 (9013) 1712 (114)
eo Esperanto 22295 (10031) 21304 (10022) 990 (106)
es Spanish 40467 (19563) 38726 (19533) 1740 (113)
eu Basque 17886 (9845) 17231 (9821) 655 (91)
fa Persian 15132 (7630) 14099 (7217) 1032 (517)
fi Finnish 27379 (11783) 26353 (11805) 1025 (105)
fr French 41578 (21952) 40186 (21959) 1392 (73)
he Hebrew 18492 (8283) 17697 (8283) 794 (82)
hr Croatian 21132 (11094) 20276 (11113) 855 (96)
hu Hungarian 26256 (12161) 25175 (12139) 1081 (90)
id Indonesian 18550 (9131) 17649 (9124) 901 (148)
it Italian 39189 (19235) 38042 (19220) 1146 (80)
ja Japanese 14352 (11890) 14131 (11895) 221 (38)
ka Georgian 15282 (9570) 14558 (9551) 723 (124)
ko Korean 17140 (7899) 16416 (7889) 724 (175)
ml Malayalam 27329 (10645) 26158 (10639) 1170 (331)
ms Malay 19346 (16577) 18436 (16348) 909 (411)
nl Dutch 29575 (16346) 28580 (16363) 994 (89)
nn Norwegian-Nynorsk 16107 (8056) 15384 (7917) 722 (297)
no Norwegian-Bokmal 30225 (17652) 29218 (17594) 1006 (125)
pl Polish 23028 (12853) 22067 (12861) 960 (66)
pt Portuguese 30967 (17998) 29310 (18004) 1657 (110)
ro Romanian 21921 (12812) 20782 (12773) 1139 (108)
ru Russian 34069 (13792) 33134 (13771) 934 (70)
sh Serbo-Croatian 21776 (21469) 21060 (21341) 716 (308)
sk Slovak 21694 (10067) 20983 (10071) 711 (169)
sl Slovenian 17900 (7222) 17077 (7194) 823 (135)
sr Serbian 30239 (9812) 28927 (9764) 1312 (176)
sv Swedish 23476 (10169) 22314 (10156) 1162 (99)
th Thai 27041 (8312) 25425 (8291) 1616 (226)
tr Turkish 32956 (16423) 31346 (16338) 1610 (257)
vi Vietnamese 35376 (16099) 33857 (16050) 1518 (161)
zh Chinese 10110 (4341) 9608 (4357) 501 (42)
Table 1: The table lists the languages in the dataset with the first column containing the ISO code for
each the language, the second column the name of the language, and the remaining columns containing
the mean size, in characters, and standard deviation, in parentheses, of the entire article, their bodies, and
their summaries. For example, for English the mean size of the human summaries is 1,712 characters.
31
Table 2: Mean Summary Size For Submitted Languages of Systems
ISO LANGUAGE AIS LAN MD1 MD2 MD3 MUS SUM
af Afrikaans 966 953 967 1303
ar Arabic 1461 876 858 874 2232 1491
bg Bulgarian 1302 969 946 967 984
ca Catalan 911 921 925 1289
cs Czech 1061 1020 1062 1522
de German 1492 1072 1037 1087 862
el Greek 1367 989 979 991 1134
en English 1262 1551 944 957 958 1197 1712
eo Esperanto 947 933 956 990
es Spanish 922 916 927 1740
eu Basque 1154 1151 1167 655
fa Persian 793 792 800 1032
fi Finnish 1328 1284 1323 1025
fr French 936 930 952 1392
he Hebrew 871 867 876 1098 794
hr Croatian 979 954 976 855
hu Hungarian 1092 1064 1089 1081
id Indonesian 1091 1085 1091 901
it Italian 981 952 975 1146
ja Japanese 546 564 563 221
ka Georgian 1180 1195 1218 723
ko Korean 663 638 656 724
ml Malayalam 670 648 676 1170
ms Malay 1089 1089 1098 909
nl Dutch 994 974 1000 994
nn Norwegian-Nynorsk 928 908 929 722
no Norwegian-Bokmal 967 937 977 1006
pl Polish 1086 1056 1083 960
pt Portuguese 942 936 939 1657
ro Romanian 1311 938 940 948 1139
ru Russian 1095 1046 1078 934
sh Serbo-Croatian 969 955 983 716
sk Slovak 1026 997 1031 711
sl Slovenian 967 949 981 823
sr Serbian 990 954 979 1312
sv Swedish 997 990 1006 1162
th Thai 553 566 563 1616
tr Turkish 1166 1132 1152 1610
vi Vietnamese 696 684 691 1518
zh Chinese 523 559 552 501
Table 2: The mean summary size, in characters, for each language submitted by each system including
the mean of the human summaries in the last column named SUM.
32
Table 3: N-gram Size Per Language for MeMoG
ISO LANGUAGE SIZE ISO LANGUAGE SIZE
af Afrikaans 5 ka Georgian 3
ar Arabic 3 ko Korean 1
bg Bulgarian 4 ml Malayalam 3
ca Catalan 4 ms Malay 4
cs Czech 4 nl Dutch 4
de German 4 nn Norwegian-Nynorsk 4
el Greek 4 no Norwegian-Bokmal 4
en English 5 pl Polish 4
eo Esperanto 4 pt Portuguese 4
es Spanish 4 ro Romanian 4
eu Basque 4 ru Russian 4
fa Persian 4 sh Serbo-Croatian 3
fi Finnish 4 sk Slovak 4
fr French 4 sl Slovenian 4
he Hebrew 3 sr Serbian 4
hr Croatian 4 sv Swedish 5
hu Hungarian 4 th Thai 3
id Indonesian 5 tr Turkish 5
it Italian 5 vi Vietnamese 5
ja Japanese 1 zh Chinese 1
Table 3: The table lists the n-gram size used for each language when evaluating summaries using
MeMoG, which is the n-gram size that maximized the standard deviation divided by the mean of the
n-gram frequency distribution of the language in the dataset.
33
Figure 1: ROUGE-1 scores for HSS.
Figure 2: ROUGE-2 scores for HSS.
34
Figure 3: MeMoG scores for HSS.
Table 4: Fraction of time a system beat the base-
line for HSS.
System ROUGE-1 ROUGE-2 MeMoG
AIC 2/5 0/5 0/5
LAN 0/2 0/2 0/2
MD1 15/40 4/40 2/39
MD2 16/40 4/40 0/39
MD3 15/40 4/40 0/39
MUS 2/3 1/3 0/3
ANOVA 28/40 13/40 5/39
Table 4: The table gives the fraction of languages
each system significantly outperform the base-
line. The last line gives the number of times an
ANOVA rejected the null hypothesis, indicating
significance.
medians of the system scores were not the same,
using a rejection threshold of 0.05. Also, the frac-
tion of time that each system significantly outper-
formed the lead baseline is also recorded. A paired
Wilcoxon test was invoked whenever the ANOVA
indicated a significant difference was present, with
a threshold of 0.05.
Lastly, each systems performance for SSS-
scoring is provided in Figures 4, 5, and 6. Sur-
prisingly, the results change little. Lastly Table 5
contains the number of times that each system beat
the baseline summary with a 95% confidence mea-
sured as a result of the non-parametric ANOVA
and the Wilcoxon paired sign rank test. The results
show that the number of significant differences go
down for ROUGE scores and up for MeMoG.
4 Summary
Overall, the authors believe the pilot was success-
ful in that it exposed researchers to the poten-
tial for using Wikipedia articles for summarization
research and demonstrated that generating sum-
maries for the genre of Wikipedia articles is a more
challenging task than newswire documents. No-
tably, no system outperformed the baseline for En-
glish! In hindsight this is not too surprising since
news articles have a prose style6 significantly dif-
ferent from Wikipedia articles7. Wikipedia arti-
cles are written as expositions having a topical
flow that can vary significantly between sections
but news articles are written in a style8 that ad-
dresses the most important information first?the
6
http://en.wikipedia.org/wiki/News_style
7
http://en.wikipedia.org/wiki/MOS:
8
http://en.wikipedia.org/wiki/Inverted_pyramid
35
Figure 4: ROUGE-1 scores for SSS.
Figure 5: ROUGE-2 scores for SSS.
36
Figure 6: MeMoG scores for SSS.
Table 5: Fraction of the time a system beat the lead
baseline for SSS.
System ROUGE-1 ROUGE-2 MeMoG
AIC 0/5 0/5 0/5
LAN 0/2 0/2 0/2
MD1 8/40 2/40 6/39
MD2 10/40 2/40 2/39
MD3 7/40 2/40 4/39
MUS 0/3 0/3 0/3
ANOVA 11/40 5/40 7/39
Table 5: The table gives the fraction of lan-
guages that each system significantly outperform
the baseline on. The last line contains the number
of times an ANOVA rejected the null hypothesis,
indicating significance.
who, what, when, where and why?with the sub-
sequent text providing more details. Hence news
articles have a more even topical flow. The authors
hope these results stimulate research and devel-
opment of summarization algorithms outside the
news domain.
As for the metrics, ROUGE-1 observed the
most significant differences among the systems
and MeMoG observed the least as measured by
a non-parametric ANOVA. However, a human
evaluation of the summaries generated would be
needed to detemine which of the automatic metrics
is best at predicting significant differences among
systems for such data.
References
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?leary. 2009. Classy 2009: summarization and
metrics. In Proceedings of the text analysis confer-
ence (TAC).
George Giannakopoulos, Vangelis Karkaletsis, George
Vouros, and Panagiotis Stamatopoulos. 2008.
Summarization system evaluation revisited: N-
gram graphs. ACM Trans. Speech Lang. Process.,
5(3):5:1?5:39, October.
George Giannakopoulos, Mahmoud El-Haj, Beno?t
Favre, Marina Litvak, Josef Steinberger, and Va-
37
sudeva Varma. 2011. Tac 2011 multiling pilot
overview.
Jeff Kubina. 2010. Wikipedia featured article cor-
pus. http://goo.gl/AmMGN. [Online; accessed
30-May-2013].
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81.
Brian Stelter. 2013. He has millions and a new job at
yahoo. soon, he?ll be 18. New York Times.
Alexia Tsotsis. 2013. Google buys wavii for north of
$30 million. TechCrunch.
38
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 55?63,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multilingual Summarization: Dimensionality Reduction and a Step
Towards Optimal Term Coverage
John M. Conroy
IDA / Center for Computing Sciences
conroy@super.org
Sashka T. Davis
IDA / Center for Computing Sciences
stdavi3@super.org
Jeff Kubina
Department of Defense
jmkubin@tycho.ncsc.mil
Yi-Kai Liu
National Institute of Standards and Technology
yi-kai.liu@nist.gov
Dianne P. O?Leary
University of Maryland and NIST
oleary@cs.umd.edu
Judith D. Schlesinger
IDA/Center for Computing Sciences
drj1945@gmail.com
Abstract
In this paper we present three term weight-
ing approaches for multi-lingual docu-
ment summarization and give results on
the DUC 2002 data as well as on the
2013 Multilingual Wikipedia feature arti-
cles data set. We introduce a new interval-
bounded nonnegative matrix factorization.
We use this new method, latent semantic
analysis (LSA), and latent Dirichlet alo-
cation (LDA) to give three term-weighting
methods for multi-document multi-lingual
summarization. Results on DUC and TAC
data, as well as on the MultiLing 2013
data, demonstrate that these methods are
very promising, since they achieve ora-
cle coverage scores in the range of hu-
mans for 6 of the 10 test languages. Fi-
nally, we present three term weighting ap-
proaches for the MultiLing13 single docu-
ment summarization task on the Wikipedia
featured articles. Our submissions signifi-
cantly outperformed the baseline in 19 out
of 41 languages.
1 Our Approach to Single and
Multi-Document Summarization
The past 20 years of research have yielded a
bounty of successful methods for single docu-
ment summarization (SDS) and multi-document
summarization (MDS). Techniques from statistics,
machine learning, numerical optimization, graph
theory, and combinatorics are generally language-
independent and have been applied both to single
and multi-document extractive summarization of
multi-lingual data.
In this paper we extend the work of our re-
search group, most recently discussed in Davis et
al. (2012) for multi-document summarization, and
apply it to both single and multi-document multi-
lingual document summarization. Our extractive
multi-document summarization performs the fol-
lowing steps:
1. Sentence boundary detection;
2. Tokenization and term identification;
3. Term-sentence matrix generation;
4. Term weight determination;
5. Sentence selection;
6. Sentence ordering.
Sentence boundary detection and tokenization are
language dependent, while steps (3)-(6) are lan-
guage independent. We briefly discuss each of
these steps.
We use a rule based sentence splitter FASST-
E (very Fast, very Accurate Sentence Splitter for
Text ? English) (Conroy et al, 2009) and its multi-
lingual extensions (Conroy et al, 2011) for deter-
mining the boundary of individual sentences.
Proper tokenization improves the quality of
the summary and may include stemming and
also morphological analysis to disambiguate com-
pound words in languages such as Arabic. To-
kenization may also include stop word removal.
The result of this step is that each sentence is rep-
resented as a sequence of terms, where a term can
be a single word, a sequence of words, or char-
acter n-grams. The specifics of tokenization are
discussed in Section 2.
Matrix generation (the vector space model) was
pioneered by Salton (1991). Later Dumais (1994)
introduced dimensionality reduction in document
retrieval systems, and this approach has also been
55
used by many researchers for document summa-
rization. (In addition to our own work, see, for
example Steinberger and Jezek (2009).) We con-
struct a single term-sentence matrix A = (ai,j),
where i = 1, . . . ,m ranges over all terms, and
j = 1, . . . , n ranges over all sentences, for ei-
ther a single document, when we perform SDS, or
for a collection of documents for MDS. The row
labels of the term-sentence matrix are the terms
T = (t1, . . . , tm) determined after tokenization.
The column labels are the sentences S1, . . . , Sn of
the document(s). The entries of the matrix A are
defined by
ai,j = `i,jgi,
Here, `i,j is the local weight, which is 1 when term
i appears in sentence j and 0 otherwise.
The global weight gi should be proportional to
the contribution of the term in describing the major
themes of the document. While the global weight
could be used as a term weight in a sentence se-
lection scheme, it may be beneficial to perform di-
mensionality reduction on the matrix A and com-
pute term weights based on the lower dimensional
matrix. In this work we seek to find strong term
weights for both single- and and multi-document
summarization. These cases are handled sepa-
rately, as we found that multi-document summa-
rization benefits a lot from dimensionality reduc-
tion while single document summarization does
not.
Our previous multi-document summarization
algorithm, OCCAMS (Davis et al, 2012), used
the linear algebraic technique of Latent Seman-
tic Analysis (LSA) to determine term weights and
used techniques from combinatorial optimization
for sentence selection. In our CLASSY algorithms
(e.g., (Conroy et al, 2011)), we used both a lan-
guage model and machine learning as two alterna-
tive approaches to assign term weights. CLASSY
then used linear algebraic techniques or an inte-
ger linear program for sentence selection. Sec-
tion 3 describes the term weights we use when
we summarize single documents. In Section 4
we present three different dimensionality reduc-
tion techniques for the term-sentence matrix A.
Once term weight learning has assigned weights
for each term of the document(s) and dimension-
ality reduction has been applied (if desired), the
next step, sentence selection, chooses a set of sen-
tences of maximal length L for the extract sum-
mary. These sentences should cover the major
themes of the document(s), minimize redundancy,
and satisfy the bound on the length of the sum-
mary. We discuss our OCCAMS V sentence se-
lection algorithm in Section 5.
Sentence ordering is performed using an ap-
proximate traveling salesperson algorithm (Con-
roy et al, 2009).
Three term weighting variants were used to gen-
erate summaries for each of the 10 languages in
the MultiLing 2013 multi-document summariza-
tion task. The target summary length was set to be
250 words for all languages except Chinese, where
700 characters were generated.
We now present the details of our improvements
to our algorithms and results of our experiments.
2 From Text to Term-Sentence Matrix
After sentence boundaries are determined, we
used one of three simple tokenization methods
and then one of two term-creation methods, as
summarized in Table 1. Languages were divided
into three categories: English, non-English lan-
guages with space delimited words, and ideo-
graphic languages (Chinese for MDS and Chi-
nese, Japanese, and Thai for the SDS pilot task).
For non-ideographic languages, tokens are formed
based on a regular expression. For English, to-
kens are defined as contiguous sequences of upper
or lower case letters and numbers. For other non-
ideographic languages, tokens were defined sim-
ilarly, and the regular expression describes what
characters are used to break the tokens. These
characters include white space and most punctu-
ation except the apostrophe. For English, Porter
stemming was used for both SDS and MDS, with a
stop list of approximately 600 words for SDS. For
English and other word-based languages, lower-
cased bi-tokens were used in MDS and lower-
cased tokens for SDS. For all languages, and both
SDS and MDS, Dunning?s mutual information
statistic (Dunning, 1993) is used to select terms,
using the other documents as background. The p-
value (rejection threshold), initially set at 5.0e-4,
is repeatedly doubled until the number of terms
is at least twice the length of the target summary
(250 for MDS, 150 words or 500 characters for
SDS). Note that these terms are high confidence
signature terms (Lin and Hovy, 2000) i.e., the p-
value is small. We describe our terms as high mu-
tual information (HMI), since Dunning?s statistic
is equivalent to mutual information as defined by
56
Language Tokens Terms for MDS Terms for SDS
English [?A-Za-z0-9] HMI bi-tokens HMI non-stop-word
tokens
Non-English [\s.?,";:?![](){}<>&*=+@#$] HMI bi-tokens HMI tokens
Ideographic 4-byte grams HMI tokens HMI tokens
Table 1: Term and token definition as a function of language and task.
Cover and Thomas (1991).
3 Determining Term Weights for Single
Document Summarization
For SDS we consider three term weighting meth-
ods.
The first is global entropy as proposed by Du-
mais et al for information retrieval (Dumais,
1994) (Rehder et al, 1997) and by Steinberger and
Jezek for document summarization (Steinberger
and Jezek, 2009). Global entropy weighting is
given by
w(GE)i = 1?
?
j pi,j ? log pi,j
log n
,
where n is the number of sentences, pi,j = tij/fi,
tij is the number of times term i appears in sen-
tence j, and fi is the total number of times term i
appears in all sentences.1
The second term weighting is simply the loga-
rithm of frequency of the term in all the sentences:
w(LF)i = 1 + log(fi).
Log frequency is motivated by the fact that the
sum of the term scores for a given sentence is
(up to an affine transformation) the log probabil-
ity of generating that sentence by sampling terms
independently at random, where the probability
of each term is estimated by maximum likelihood
from the observed frequencies fi.
The third method is a personalized variant of
TextRank, which was first proposed by Mihal-
cea (2005) and motivated by PageRank (Page et
al., 1999). The personalized version smooths the
Markov chain used in TextRank (PageRank) with
term (page) preferences. Previously, a sentence
based version of personalization has been used
for summarization; see, for example, Zhao et al
(2009). Our current work may be the first use of
1We make the usual convenient definition that pi log pi =
0 when pi = 0.
a term based personalized TextRank (TermRank),
which we call PTR. The personalization vector we
choose is simply the normalized frequency, and
the Markov chain is defined by the transition ma-
trix
M =
1
2
LLTD +
1
2
peT
where
pi = fi/
?
i
(fi),
L is the incidence term-sentence matrix. The el-
ements of L are previously defined local weights,
`ij . The vector e is all 1?s and D is a diagonal
matrix chosen to make the column sums equal to
one. The estimated weight vector used by OC-
CAMS V, w(PTR), is computed using 5 iterations
of the power method to approximate the station-
ary vector of this matrix. Note, there is no need to
form the matrix M since the applications of M to
a vector may be achieved by vector operations and
matrix-vector multiplies by L and LT .
We test the performance of these three term
weighting methods on two data sets: DUC 2002
English single-document data and the Wikipedia
Pilot at MultiLing 2013.
3.1 Results for DUC 2002 Data
The DUC 2002 English single-document data con-
tains 567 newswire documents for which there are
one or two human-generated summaries.
In addition to computing ROUGE-2 scores, we
also compute an oracle coverage score (Conroy et
al., 2006). At TAC 2011 (Conroy et al, 2011)
(Rankel et al, 2012) bigram coverage was shown
to be a useful feature for predicting the perfor-
mance of automated summarization systems rela-
tive to a human summarizer. Oracle unigram cov-
erage score is defined by
C1(X) =
?
i?T
f1(i),
where T is the set of terms and f1(i) is the frac-
tion of humans who included the ith term in the
57
Term Weight ROUGE-2 C1 Group
PTR 0.194 19.1 1
LF 0.192 18.7 2
GE 0.190 18.6 2
Table 2: ROUGE-2 and Coverage bi-grams Scores
summary. More generally, we define Cn in similar
way for n-gram oracle coverage scores. Coverage
scores differ from ROUGE scores since the score
is not affected by the number of times that a given
human or machine-generated summary uses the
term, but only whether or not the term is included
in the machine summary and the estimated frac-
tion of humans that would use this term. We note
that this score can be modified to compute scores
for human summarizers using the analogous jack-
knife procedure employed by ROUGE.
Table 2 gives a summary of the results. We
ran a Wilcoxon test to check for statistical dis-
tinguishability in the performance of the different
term-weighting methods. Methods were placed in
the same group if they produced results in cover-
age (C1) that were indistinguishable. More pre-
cisely, we used the null hypothesis that the differ-
ence between the vector of scores for two methods
has median 0. If the p-value of two consecutive
entries in the table was less than 0.05, the group
label was increased and is shown in the last col-
umn.
Log frequency (LF) and global entropy (GE)
are correlated. For the DUC 2002 data they per-
form comparably. Personalized term rank (PTR)
weighting is statistically stronger than the other
two approaches, as measured by the oracle term
coverage score. For these data the definition of
term for the purposes of the computation of the
oracle coverage score is non-stop word stemmed
(unigram) tokens.
3.2 Results for the Wikipedia Pilot at
MultiLing 2013
This task involves single-document summariza-
tion for 1200 Wikipedia feature articles: 30 doc-
uments in each of 40 languages. For each doc-
ument, the organizers generated a baseline lead
summary consisting of the first portion of the fea-
ture article following the ?hidden summary.? Sum-
mary lengths were approximately 150 words for
all non-ideograph languages and 500 characters
for the ideograph languages. Sentences were or-
dered in the order selected by OCCAMS V. Thus,
sentences covering the largest number of relevant
terms, as measured by the term-weighting scheme,
will appear first.
Results of this pilot study will be presented in
detail in the overview workshop paper, but we note
here that, as measured by ROUGE-1, in 19 of the
40 languages, at least one of our three submit-
ted methods significantly outperformed the lead-
summary baseline.
4 Dimensionality Reduction
The goal of dimensionality reduction is to iden-
tify the major factors of the term-sentence ma-
trix A and to throw away those factors which are
?irrelevant? for summarization. Here we survey
three algorithms: the well-known LSA, the more
recent latent Dirichlet alocation (LDA), and the
new interval-bounded nonnegative matrix factor-
ization.
4.1 Latent Semantic Analysis
Davis et al (2012) successfully used an approx-
imation to A, computed using the singular value
decomposition (SVD) A = USV T . They used
the first 200 columns U200 of the singular vector
matrix U and the corresponding part of the singu-
lar value matrix S. They eliminated negative en-
tries in U200 by taking absolute values. The term
weights were computed as the L1 norm (sum of
the entries) in the rows of W = |U200|S200.
Our method is similar, except that we use 250
columns and form them in a slightly different way.
Observe that in the SVD, if ui is a column of U
and vTi is a row of V , then they can be replaced
by ?ui and ?vTi . This is true since if D is any
diagonal matrix with entries +1 and ?1, then
A = USV T = (UD)S(DV T ).
Therefore, we propose choosingD so that the sum
of the positive entries in each column of U is max-
imized. Then we form U? by setting each negative
entry of UD to zero and form W = U?250S250.
4.2 Latent Dirichlet Allocation
We use the term-sentence matrix to train a simple
generative topic model based on LDA (Blei et al,
2003). This model is described by the following
parameters: the number of terms m; the number
of topics k; a vector w representing a probability
distribution over topics; and an m? k matrix A in
58
which each column represents a probability distri-
bution over terms.
In this model, sentences are generated inde-
pendently. We use the ?pure-topic? LDA model
and assume, for simplicity, that the length of the
sentence is fixed a priori. First, a topic i ?
{1, . . . , k} is chosen from the probability distribu-
tion w. Then, terms are generated by sampling in-
dependently from the distribution specified by the
ith column of the matrix A.
We train this model using a recently-developed
spectral algorithm based on third-order tensor de-
compositions (Anandkumar et al, 2012a; Anand-
kumar et al, 2012b). This algorithm is guaranteed
to recover the parameters of the LDA model, pro-
vided that the columns of the matrixA are linearly
independent. For our experiments, we used a Mat-
lab implementation from Hsu (2012).
4.3 Interval Bounded Nonnegative Matrix
Factorization (IBNMF)
We also use a new method for dimensionality
reduction, a nonnegative matrix factorization al-
gorithm that handles uncertainty in a new way
(O?Leary and et al, In preparation).
Since the term-sentence matrix A is not known
with certainty, let?s suppose that we are given up-
per and lower bound matrices U and L so that
L ? A ? U . We compute a sparse nonnega-
tive low-rank approximation toA of the formXY ,
where X is nonnegative (i.e., X ? 0) and has
r columns and Y is nonnegative and has r rows.
This gives us an approximate nonnegative factor-
ization of A of rank at most r.
We choose to measure closeness of two ma-
trices using the Frobenius norm-squared, where
?Z?2F denotes the sum of the squares of the entries
of Z. Since A is sparse, we also want X and Y to
be sparse. We use the common trick of forcing this
by minimizing the sum of the entries of the matri-
ces, denoted by sum(X) + sum(Y ). This leads
us to determine X and Y by choosing a weighting
constant ? and solving
min
X,Y,Z
? ?XY ? Z?2F + sum(X) + sum(Y )
subject to the constraints
L ? Z ? U,
X ? 0,
Y ? 0.
We simplify this problem by noting that for any
W = XY , the entries of the optimal Z are
zij =
?
?
?
`ij , wij ? `ij ,
wij , `ij ? wij ? uij ,
uij , uij ? wij .
We solve our minimization problem by an alter-
nating algorithm, iterating by fixing X and deter-
mining the optimal Y and then fixing Y and de-
termining the optimal X . Either non-negativity is
imposed during the solution to the subproblems,
making each step more expensive, or negative en-
tries of the updated matrices are set to zero, ruin-
ing theoretical convergence properties but yielding
a more practical algorithm. Each iteration reduces
the distance to the term matrix, but setting nega-
tive values to zero increases it again.
For our summarization system we chose r = 50
and ? = 1000. We scaled the rows of the matrix
using global entropy weights and used L = 0.9A
and U = 1.1A.
4.4 Term Weighting and Dimension Choice
for Multi-Document Summarization
A natural term weighting can be obtained by com-
puting the row sums of the dimension-reduced
approximation to the term-sentence matrix. For
LSA, the resulting term weights are the sum of
the entries in the rows of W = U?250S250. For
the LDA method the initial matrix is the matrix of
counts. The model has three components similar
to that of the SVD in LSA, and the term weights
are computed analogously. For IBNMF, the term
weights are the sum of the entries in the rows of
the optimal XY .
Each of the three dimensionality reduction
methods require us to specify the dimension of the
?topic space.? We explored this question using the
DUC 2005-2007 and the TAC 2011 data. Tables 3,
4, 5, and 6 give the average ROUGE-2, ROUGE-
4, and bi-gram coverage scores, with confidence
intervals, for the dimension that gave the best cov-
erage. The optimal ranks were 250 for LSA, 5 for
LDA, and 50 for IBNMF. We emphasize the these
results are very strong despite the fact that no use
of the topic descriptions or the guided summary
aspects for the TAC 2010 and 2011 are used. Thus,
we treat these data as if the task were to generate
a generic summary, as is the case in the MultiLing
2013 task. 2
2We note that some of the coverage (C2), and ROUGE-
59
System R2 R4 C2
A 0.117 ( 0.106,0.129) 0.016 ( 0.011, 0.021) 26.333 (23.849,28.962)
C 0.118 ( 0.105,0.131) 0.016 ( 0.012, 0.022) 25.882 (23.086,28.710)
E 0.105 ( 0.092,0.120) 0.016 ( 0.010, 0.022) 23.625 (18.938,28.573)
F 0.100 ( 0.089,0.111) 0.014 ( 0.010, 0.019) 23.500 (19.319,27.806)
B 0.100 ( 0.086,0.115) 0.013 ( 0.008, 0.019) 23.118 (20.129,26.285)
D 0.100 ( 0.089,0.113) 0.012 ( 0.007, 0.017) 22.957 (20.387,25.742)
I 0.099 ( 0.085,0.116) 0.010 ( 0.007, 0.014) 21.806 (17.722,26.250)
H 0.088 ( 0.077,0.101) 0.011 ( 0.007, 0.016) 20.972 (17.389,24.750)
J 0.100 ( 0.090,0.111) 0.010 ( 0.007, 0.013) 20.472 (17.167,24.389)
G 0.097 ( 0.085,0.108) 0.012 ( 0.008, 0.017) 20.111 (16.694,24.000)
LSA250 0.085 ( 0.076,0.093) 0.008 ( 0.006, 0.009) 17.950 (17.072,18.838)
IBNMF50 0.079 ( 0.068,0.089) 0.007 ( 0.005, 0.009) 17.730 (16.843,18.614)
LDA5 0.077 ( 0.074,0.080) 0.008 ( 0.007, 0.009) 17.165 (16.320,18.024)
Table 3: DUC 2005
System R2 R4 C2
C 0.133 ( 0.116,0.152) 0.025 ( 0.018, 0.033) 30.517 (26.750,34.908)
D 0.124 ( 0.108,0.140) 0.017 ( 0.011, 0.023) 27.283 (23.567,31.050)
B 0.118 ( 0.105,0.134) 0.015 ( 0.012, 0.020) 25.933 (23.333,29.033)
G 0.113 ( 0.102,0.124) 0.016 ( 0.011, 0.022) 25.717 (23.342,28.017)
H 0.108 ( 0.098,0.117) 0.013 ( 0.010, 0.016) 24.767 (22.433,27.067)
F 0.109 ( 0.093,0.128) 0.016 ( 0.010, 0.023) 24.183 (20.650,28.292)
I 0.106 ( 0.096,0.116) 0.012 ( 0.008, 0.015) 24.133 (22.133,26.283)
J 0.107 ( 0.093,0.125) 0.015 ( 0.010, 0.022) 23.933 (20.908,27.233)
A 0.104 ( 0.093,0.116) 0.015 ( 0.010, 0.022) 23.283 (20.483,26.283)
E 0.104 ( 0.089,0.119) 0.014 ( 0.010, 0.020) 22.950 (19.833,26.450)
LDA5 0.103 ( 0.099,0.107) 0.012 ( 0.011, 0.013) 22.620 (21.772,23.450)
IBNMF50 0.095 ( 0.091,0.099) 0.010 ( 0.009, 0.011) 22.400 (21.615,23.177)
LSA250 0.099 ( 0.096,0.103) 0.012 ( 0.011, 0.013) 22.335 (21.497,23.200)
Table 4: DUC 2006
System R2 R4 C2
D 0.175 ( 0.157,0.196) 0.038 ( 0.029, 0.050) 39.481 (34.907,44.546)
C 0.151 ( 0.134,0.169) 0.035 ( 0.024, 0.049) 34.148 (29.870,38.926)
E 0.139 ( 0.125,0.154) 0.025 ( 0.020, 0.030) 30.907 (27.426,34.574)
J 0.139 ( 0.120,0.160) 0.028 ( 0.019, 0.038) 30.759 (25.593,36.389)
B 0.140 ( 0.116,0.163) 0.027 ( 0.019, 0.036) 30.537 (25.815,35.537)
I 0.136 ( 0.113,0.159) 0.022 ( 0.014, 0.030) 30.537 (25.806,35.241)
G 0.134 ( 0.118,0.150) 0.027 ( 0.018, 0.035) 30.259 (26.509,33.926)
F 0.134 ( 0.120,0.149) 0.024 ( 0.017, 0.033) 29.944 (26.481,33.870)
A 0.133 ( 0.117,0.149) 0.024 ( 0.016, 0.033) 29.315 (25.685,33.093)
H 0.130 ( 0.117,0.143) 0.020 ( 0.015, 0.027) 28.815 (25.537,32.185)
IBNMF50 0.140 ( 0.122,0.158) 0.023 ( 0.017, 0.031) 28.350 (27.092,29.567)
LSA250 0.125 ( 0.120,0.130) 0.022 ( 0.020, 0.024) 28.144 (26.893,29.344)
LDA5 0.124 ( 0.118,0.129) 0.021 ( 0.019, 0.023) 27.722 (26.556,28.893)
Table 5: DUC 2007
60
System R2 R4 C2
IBNMF50 0.132 ( 0.124,0.140) 0.033 ( 0.029, 0.038) 12.585 (11.806,13.402)
D 0.128 ( 0.110,0.146) 0.024 ( 0.017, 0.032) 12.212 (10.394,14.045)
LSA250 0.128 ( 0.120,0.136) 0.030 ( 0.025, 0.034) 12.210 (11.441,12.975)
A 0.119 ( 0.099,0.138) 0.024 ( 0.016, 0.033) 11.591 ( 9.758,13.455)
LDA5 0.120 ( 0.112,0.128) 0.028 ( 0.024, 0.033) 11.409 (10.678,12.159)
E 0.118 ( 0.099,0.138) 0.025 ( 0.016, 0.035) 11.288 ( 9.409,13.258)
H 0.115 ( 0.097,0.132) 0.020 ( 0.014, 0.027) 11.212 ( 9.439,12.955)
B 0.111 ( 0.099,0.125) 0.018 ( 0.013, 0.023) 10.591 ( 9.379,11.864)
F 0.109 ( 0.090,0.128) 0.017 ( 0.010, 0.025) 10.530 ( 8.515,12.500)
C 0.110 ( 0.095,0.126) 0.015 ( 0.010, 0.021) 10.379 ( 8.939,11.924)
G 0.110 ( 0.092,0.127) 0.016 ( 0.010, 0.023) 10.258 ( 8.682,11.894)
Table 6: TAC 2011
5 Sentence Selection
Our sentence selection algorithm, OCCAMS V,
is an extension of the one used in (Davis et al,
2012), which uses the (1 ? e?1/2)-approximation
scheme for the Budgeted Maximal Coverage
(BMC) problem and the Dynamic Programming
based FPTAS for the knapsack problem.
Algorithm OCCAMS V (T, D,W, c, L)
1. K1 = Greedy BMC(T,D,W, c, L)
2. K2 = Smax ? Greedy BMC(T ?,D?,W, c?, L?)),
where Smax = argmax{Si?D}
{?
tj?Si w(tj)
}
and T ?,D?,W, c?, L? represent quantities updated
by deleting sentence Smax from the collection.
3. K3 = KS(Greedy BMC(T,D,W, c, 5L), L);
4. K4 = KS(K ?4, L), where
K ?4 = Smax ? Greedy BMC(T
?,D?,W, C?, 5L?));
5. K = argmaxk=1,2,3,4
{?
T (Ki)w(ti)
}
where T (Ki) is the set of terms covered by Ki.
This algorithm selects minimally overlapping
sentences, thus reducing redundancy, while maxi-
mizing term coverage. The algorithm guarantees
a (1? e?1/2) approximation ratio for BMC.
We use the m terms T = {t1, . . . , tm} and
their corresponding weightsW = {w1, . . . , wm}.
We also use the n sentences D = {S1, . . . , Sn},
where each Si is the set of terms in the ith sen-
tence, so that Si ? T . We define c to be a vec-
tor whose components are the lengths of each sen-
tence. Our algorithm, OCCAMS V, determines
four candidate sets of summary sentences and then
2 scores reported in (Davis et al, 2012), where a rank 200
approximation and a large background corpus were used,
are higher than the ones reported here, where a small self-
background and a rank 250 approximation is used.
chooses the one with maximal coverage weight.
The first three candidate sets were used in the OC-
CAMS algorithm (Davis et al, 2012). The set
K1 is determined using the Greedy BMC heuris-
tic of Khuller et al (1999) to maximize the sum
of weights corresponding to terms in the sum-
mary sentences. The set K2 is determined the
same way, but the sentence that has the best sum
of weights is forced to be included. The third
candidate K3 is determined by applying a fully
polynomial-time approximation scheme (FPTAS)
dynamic programming algorithm, denoted by KS,
to the knapsack problem using sentences chosen
by the Greedy BMC heuristic, asking for a length
of 5L. The fourth candidate K4 is similar, but the
sentence with the best sum of weights is forced to
be included in the input to KS.
OCCAMS V guarantees an approximation ratio
of (1? e?1/2) for the result because the quality of
the solution chosen is no worse than the approxi-
mation ratio achieved by the OCCAMS algorithm.
6 Coverage Results for MultiLing 2013
We defined a term oracle coverage score in Section
3.1, an automatic summarization evaluation score
that computes the expected number of n-grams
that a summary will have in common with a hu-
man summary selected at random, assuming that
humans select terms independently. As reported
in (Davis et al, 2012), the 2-gram oracle cover-
age correlates as well with human evaluations of
English summaries as ROUGE-2 does for English
newswire summaries.3 It is natural then to ask to
what extent oracle coverage scores can predict a
summary?s quality for other languages.
3Here a term is defined as a stemmed 2-gram token.
61
For each of the 10 MultiLing 2013 languages
we can tokenize and generate bigrams (or charac-
ter n-grams for Chinese) for the human-generated
summaries and the machine-generated summaries.
Table 7 gives the average oracle term (bi-gram)
coverage score (C2) for the lowest-scoring human
and for each of the dimensionality reduction algo-
rithms described in Section 4.
In all but four of the languages (Romanian,
Hindi, Spanish, and Chinese), at least one of our
methods scored higher than the lowest scoring hu-
man. As with the DUC and TAC testing, the LDA
method of term-weighting was the weakest of the
three. In fact, in eight of the languages one or both
of OCCAMS V(LSA) and OCCAMS V(IBNMF)
(indicated in boldface in the table) scored signif-
icantly higher than OCCAMS V(LDA) (p-value
< 0.05 using a paired Wilcoxon test).
The human coverage scores for three of the lan-
guages (Romanian, Hindi, and Chinese) are sur-
prisingly high. Examining these data more closely
indicates that a large number of the summaries are
nearly identical. As an example, in one of the Ro-
manian document sets, there were 266 bi-grams
in the union of the three summaries, and the sum-
mary length was 250. Document sets similar to
this are the major cause of the anomalously high
scores for humans in these languages.
Language Human LSA IBNMF LDA
english 37 38 37 34
arabic 22 29 28 23
czech 22 34 35 33
french 28 38 38 34
greek 19 25 25 24
hebrew 16 19 22 19
hindi 64 20 20 18
spanish 47 40 44 36
romanian 118 31 28 29
chinese 68 23 24 18
Table 7: MultiLing 2013 Coverage Results
Human evaluation of the multi-lingual multi-
document summaries is currently under way.
These evaluations will be extremely informative
and will help measure to what extent ROUGE,
coverage, and character n-gram based methods
such as MeMoG (Giannakopoulos et al, 2010),
are effective in predicting performance.
7 Conclusions and Future Work
In this paper we presented three term weight-
ing approaches for single document multi-lingual
summarization. These approaches were tested on
the DUC 2002 data and on a submission to the
MultiLing 2013 single document pilot task for all
40 languages. Automatic evaluation of these sum-
maries with ROUGE-1 indicates that the strongest
of the approaches significantly outperformed the
lead baseline. The Wikipedia feature articles pose
a challenge due to their variable summary size and
genre. Further analysis of the results as well as hu-
man evaluation of the submitted summaries would
deepen our understanding.
A new nonnegative matrix factorization
method, interval bounded nonnegative matrix
factorization (IBNMF), was used. This method
allows specifying interval bounds, which give
an intuitive way to express uncertainty in the
term-sentence matrix.
For MDS we presented a variation of a LSA
term-weighting for OCCAMS V as well as novel
use of both of the IBNMF and an LDA model.
Based on automatic evaluation using cover-
age, it appears that the LSA method and the
IBNMF term-weighting give rise to competitive
summaries with term coverage scores approach-
ing that of humans for 6 of the 10 languages. The
automatic evaluation of these summaries, which
should soon be finished, will be illuminating.
Note: Contributions to this article by NIST, an agency of the
US government, are not subject to US copyright. Any men-
tion of commercial products is for information only, and does
not imply recommendation or endorsement by NIST.
References
Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham
Kakade, and Yi-Kai Liu. 2012a. A spectral al-
gorithm for latent dirichlet alocation. In Peter L.
Bartlett, Fernando C. N. Pereira, Christopher J. C.
Burges, Le?on Bottou, and Kilian Q. Weinberger, ed-
itors, NIPS, pages 926?934.
Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M.
Kakade, and Matus Telgarsky. 2012b. Tensor de-
compositions for learning latent variable models.
CoRR, abs/1210.7559.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
62
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of the ACL?06/COLING?06 Confer-
ences, pages 152?159, Sydney, Australia, July.
John M Conroy, Judith D Schlesinger, and Dianne P
O?leary. 2009. Classy 2009: summarization and
metrics. Proceedings of the text analysis conference
(TAC).
John M Conroy, Judith D Schlesinger, Jeff Kubina, Pe-
ter A Rankel, and Dianne P O?Leary. 2011. Classy
2011 at tac: Guided and multi-lingual summaries
and evaluation metrics. Proceedings of the Text
Analysis Conference.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of information theory. Wiley-Interscience,
New York, NY, USA.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combina-
torial covering algorithm for multi-document sum-
marization. In Jilles Vreeken, Charles Ling, Mo-
hammed Javeed Zaki, Arno Siebes, Jeffrey Xu Yu,
Bart Goethals, Geoffrey I. Webb, and Xindong Wu,
editors, ICDM Workshops, pages 454?463. IEEE
Computer Society.
Susan T. Dumais. 1994. Latent semantic indexing
(lsi): Trec-3 report. In TREC, pages 105?115.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
George Giannakopoulos, George A. Vouros, and Van-
gelis Karkaletsis. 2010. Mudos-ng: Multi-
document summaries using n-gram graphs (tech re-
port). CoRR, abs/1012.2042.
Daniel Hsu. 2012. Estimating a simple topic model.
http://cseweb.ucsd.edu/?djhsu/
code/learn_topics.m.
Samir Khuller, Anna Moss, and Joseph Naor. 1999.
The Budgeted Maximum Coverage Problem. Inf.
Process. Lett., 70(1):39?45.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th conference on Com-
putational linguistics, pages 495?501, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Rada Mihalcea. 2005. Language independent extrac-
tive summarization. In Proceedings of ACL 2005,
Ann Arbor, MI, USA.
Dianne P. O?Leary and et al In preparation. An inter-
val bounded nonnegative matrix factorization. Tech-
nical report.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. Previous
number = SIDL-WP-1999-0120.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better metrics to automatically
predict the quality of a text summary. Algorithms,
5(4):398?420.
Bob Rehder, Michael L. Littman, Susan T. Dumais, and
Thomas K. Landauer. 1997. Automatic 3-language
cross-language information retrieval with latent se-
mantic indexing. In TREC, pages 233?239.
Gerard Salton. 1991. The smart information retrieval
system after 30 years - panel. In Abraham Book-
stein, Yves Chiaramella, Gerard Salton, and Vijay V.
Raghavan, editors, SIGIR, pages 356?358. ACM.
Josef Steinberger and Karel Jezek. 2009. Update
summarization based on novel topic distribution. In
ACM Symposium on Document Engineering, pages
205?213.
Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using
query expansion in graph-based approach for query-
focused multi-document summarization. Informa-
tion Processing & Management, 45(1):35 ? 41.
63
