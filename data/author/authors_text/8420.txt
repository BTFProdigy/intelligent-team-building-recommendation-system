Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 21?30,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Regular Expression Learning for Information Extraction
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar Vaithyanathan
IBM Almaden Research Center
San Jose, CA 95120
{yunyaoli, rajase, rsriram}@us.ibm.com, shiv@almaden.ibm.com
H. V. Jagadish?
Department of EECS
University of Michigan
Ann Arbor, MI 48109
jag@umich.edu
Abstract
Regular expressions have served as the dom-
inant workhorse of practical information ex-
traction for several years. However, there has
been little work on reducing the manual ef-
fort involved in building high-quality, com-
plex regular expressions for information ex-
traction tasks. In this paper, we propose Re-
LIE, a novel transformation-based algorithm
for learning such complex regular expressions.
We evaluate the performance of our algorithm
on multiple datasets and compare it against the
CRF algorithm. We show that ReLIE, in ad-
dition to being an order of magnitude faster,
outperforms CRF under conditions of limited
training data and cross-domain data. Finally,
we show how the accuracy of CRF can be im-
proved by using features extracted by ReLIE.
1 Introduction
A large class of entity extraction tasks can be ac-
complished by the use of carefully constructed reg-
ular expressions (regexes). Examples of entities
amenable to such extractions include email ad-
dresses and software names (web collections), credit
card numbers and social security numbers (email
compliance), and gene and protein names (bioinfor-
matics), etc. These entities share the characteristic
that their key representative patterns (features) are
expressible in standard constructs of regular expres-
sions. At first glance, it may seem that constructing
?Supported in part by NSF 0438909 and NIH 1-U54-
DA021519.
a regex to extract such entities is fairly straightfor-
ward. In reality, robust extraction requires the use
of rather complex expressions, as illustrated by the
following example.
Example 1 (Phone number extraction). An obvious
pattern for identifying phone numbers is ?blocks of
digits separated by hyphens? represented as R1 =
(\d+\-)+\d+.1 While R1 matches valid phone numbers
like 800-865-1125 and 725-1234, it suffers from both
?precision? and ?recall? problems. Not only does R1
produce incorrect matches (e.g., social security numbers
like 123-45-6789), it also fails to identify valid phone
numbers such as 800.865.1125, and (800)865-CARE. An
improved regex that addresses these problems is R2 =
(\d{3}[-.\ ()]){1,2}[\dA-Z]{4}.
While multiple machine learning approaches have
been proposed for information extraction in recent
years (McCallum et al, 2000; Cohen and McCal-
lum, 2003; Klein et al, 2003; Krishnan and Man-
ning, 2006), manually created regexes remain a
widely adopted practical solution for information
extraction (Appelt and Onyshkevych, 1998; Fukuda
et al, 1998; Cunningham, 1999; Tanabe and Wilbur,
2002; Li et al, 2006; DeRose et al, 2007; Zhu et al,
2007). Yet, with a few notable exceptions, which we
discuss later in Section 1.1, there has been very little
work in reducing this human effort through the use
of automatic learning techniques. In this paper, we
propose a novel formulation of the problem of learn-
1Throughout this paper, we use the syntax of the standard
Java regex engine (Java, 2008).
21
ing regexes for information extraction tasks. We
demonstrate that high quality regex extractors can be
learned with significantly reduced manual effort. To
motivate our approach, we first discuss prior work
in the area of learning regexes and describe some of
the limitations of these techniques.
1.1 Learning Regular Expressions
The problem of inducing regular languages from
positive and negative examples has been studied in
the past, even outside the context of information
extraction (Alquezar and Sanfeliu, 1994; Dupont,
1996; Firoiu et al, 1998; Garofalakis et al, 2000;
Denis, 2001; Denis et al, 2004; Fernau, 2005;
Galassi and Giordana, 2005; Bex et al, 2006).
Much of this work assumes that the target regex
is small and compact thereby allowing the learn-
ing algorithm to exploit this information. Consider,
for example, the learning of patterns motivated by
DNA sequencing applications (Galassi and Gior-
dana, 2005). Here the input sequence is viewed
as multiple atomic events separated by gaps. Since
each atomic event is easily described by a small and
compact regex, the problem reduces to one of learn-
ing simple regexes. Similarly, in XML DTD infer-
ence (Garofalakis et al, 2000; Bex et al, 2006), it
is possible to exploit the fact that the XML docu-
ments of interest are often described using simple
DTDs. E.g., in an online books store, each book
has a title, one or more authors and price. This in-
formation can be described in a DTD as ?book? ?
?title??author? + ?price?. However, as shown in Ex-
ample 1, regexes for information extraction rely on
more complex constructs.
In the context of information extraction, prior
work has concentrated primarily on learning regexes
over relatively small alphabet sizes. A common
theme in (Soderland, 1999; Ciravegna, 2001; Wu
and Pottenger, 2005; Feldman et al, 2006) is the
problem of learning regexes over tagged tokens
produced by other text-processing steps such as
POS tagging, morphological analysis, and gazetteer
matching. Thus, the alphabet is defined by the space
of possible tags output by these analysis steps. A
similar approach has been proposed in (Brill, 2000)
for POS disambiguation. In contrast, our paper ad-
dresses extraction tasks that require ?fine-grained?
control to accurately capture the structural features
of the entity of interest. Consequently, the domain
of interest consists of all characters thereby dramat-
ically increasing the size of the alphabet. To enable
this scale-up, the techniques presented in this paper
exploit advanced syntactic constructs (such as char-
acter classes and quantifiers) supported by modern
regex languages.
Finally, we note that almost all of the above de-
scribed work define the learning problem over a
restricted class of regexes. Typically, the restric-
tions involve either disallowing or limiting the use of
Kleene disclosure and disjunction operations. How-
ever, our work imposes no such restrictions.
1.2 Contributions
In a key departure from prior formulations, the
learning algorithm presented in this work takes as
input not just labeled examples but also an initial
regular expression. The use of an initial regex has
two major advantages. First, this expression pro-
vides a natural mechanism for a domain expert to
provide domain knowledge about the structure of the
entity being extracted. Second, as we show in Sec-
tion 2, the space of output regular expressions un-
der consideration can be meaningfully restricted by
appropriately defining their relationship to the input
expression. Such a principled approach to restrict
the search space permits the learning algorithm to
consider complex regexes in a tractable manner. In
contrast, prior work defined a tractable search space
by placing restrictions on the target class of regular
expressions. Our specific contributions are:
? A novel regex learning problem consisting of learn-
ing an ?improved? regex given an initial regex and
labeled examples
? Formulation of this learning task as an optimization
problem over a search space of regexes
? ReLIE, a regex learning algorithm that employs
transformations to navigate the search space
? Extensive experimental results over multiple
datasets to show the effectiveness of ReLIE and
a comparison study with the Conditional Random
Field (CRF) algorithm
? Finally, experiments that demonstrate the benefits
of using ReLIE as a feature extractor for CRF and
possibly other machine learning algorithms.
22
2 The Regex Learning Problem
Consider the task of identifying instances of some
entity E . Let R0 denote the input regex provided by
the user and let M(R0 ,D) denote the set of matches
obtained by evaluating R0 over a document col-
lection D. Let Mp(R0 ,D) = {x ? M(R0 ,D) :
x instance of E} and Mn(R0 ,D) = {x ? M(R0 ,D) :
x not an instance of E} denote the set of positive and
negative matches for R0 . Note that a match is pos-
itive if it corresponds to an instance of the entity of
interest and is negative otherwise. The goal of our
learning task is to produce a regex that is ?better?
than R0 at identifying instances of E .
Given a candidate regex R, we need a mechanism
to judge whether R is indeed a better extractor for
E than R0 . To make this judgment even for just the
original document collection D, we must be able to
label each instance matched byR (i.e., each element
of M(R,D)) as positive or negative. Clearly, this
can be accomplished if the set of matches produced
byR are contained within the set of available labeled
examples, i.e., if M(R,D) ? M(R0 ,D). Based on
this observation, we make the following assumption:
Assumption 1. Given an input regex R0 over some al-
phabet ?, any other regexR over ? is a candidate for our
learning algorithm only if L(R) ? L(R0 ). (L(R) denotes
the language accepted by R).
Even with this assumption, we are left with a po-
tentially infinite set of candidate regexes from which
our learning algorithm must choose one. To explore
this set in a principled fashion, we need a mecha-
nism to move from one element in this space to an-
other, i.e., from one candidate regex to another. In
addition, we need an objective function to judge the
extraction quality of each candidate regex. We ad-
dress these two issues below.
Regex Transformations To systematically ex-
plore the search space, we introduce the concept of
regex transformations.
Definition 1 (Regex Transformation). LetR? denote
the set of all regular expressions over some alphabet ?. A
regex transformation is a function T : R? ? 2R? such
that ?R? ? T (R), L(R?) ? L(R).
For example, by replacing different occurrences
of the quantifier + in R1 from Example 1 with
specific ranges (such as {1,2} or {3}), we obtain
expressions such as R3 = (\d+\-){1,2}\d+ and
R4 = (\d{3}\-)+\d+. The operation of replacing
quantifiers with restricted ranges is an example of a
particular class of transformations that we describe
further in Section 3. For the present, it is sufficient
to view a transformation as a function applied to a
regexR that produces, as output, a set of regexes that
accept sublanguages of L(R). We now define the
search space of our learning algorithm as follows:
Definition 2 (Search Space). Given an input regex R0
and a set of transformations T , the search space of our
learning algorithm is T (R0 ), the set of all regexes ob-
tained by (repeatedly) applying the transformations in T
to R0 .
For instance, if the operation of restricting quanti-
fiers that we described above is part of the transfor-
mation set, then R3 and R4 are in the search space
of our algorithm, given R1 as input.
Objective Function We now define an objective
function, based on the well known F-measure, to
compare the extraction quality of different candidate
regexes in our search space. Using Mp(R,D) (resp.
Mn(R,D)) to denote the set of positive (resp. nega-
tive) matches of a regex R, we define
precision(R,D) =
Mp(R,D)
Mp(R,D) + Mn(R,D)
recall(R,D) =
Mp(R,D)
Mp(R0,D)
F(R,D) =
2 ? precision(R,D) ? recall(R,D)
precision(R,D) + recall(R,D)
The regex learning task addressed in this paper
can now be formally stated as the following opti-
mization problem:
Definition 3 (Regex Learning Problem). Given
an input regex R0 , a document collection D, labeled
sets of positive and negative examples Mp(R0 ,D) and
Mn(R0 ,D), and a set of transformations T , compute the
output regex Rf = argmaxR?T (R0 ) F(R,D).
3 Instantiating Regex Transformations
In this section, we describe how transformations
can be implemented by exploiting the syntactic con-
structs of modern regex engines. To help with our
description, we introduce the following task:
Example 2 (Software name extraction). Consider the
task of identifying names of software products in text.
A simple pattern for this task is: ?one or more capital-
ized words followed by a version number?, represented
as R5 = ([A-Z]\w*\s*)+[Vv]?(\d+\.?)+.
23
When applied to a collection of University web
pages, we discovered that R5 identified correct in-
stances such as Netscape 2.0, Windows 2000 and
Installation Designer v1.1. However, R5 also ex-
tracted incorrect instances such as course numbers
(e.g. ENGLISH 317), room numbers (e.g. Room
330), and section headings (e.g. Chapter 2.2). To
eliminate spurious matches such as ENGLISH 317,
let us enforce the condition that ?each word is a
single upper-case letter followed by one or more
lower-case letters?. To accomplish this, we focus
on the sub-expression of R5 that identifies capital-
ized words, R51 = ([A-Z]\w*\s*)+, and replace it
with R51a = ([A-Z][a-z]*\s*)+. The regex result-
ing from R5 by replacing R51 with R51a will avoid
matches such as ENGLISH 317.
An alternate way to improve R5 is by explicitly
disallowing matches against strings like ENGLISH,
Room and Chapter. To accomplish this, we can
exploit the negative lookahead operator supported
in modern regex engines. Lookaheads are special
constructs that allow a sequence of characters
to be checked for matches against a regex with-
out the characters themselves being part of the
match. As an example, (?!Ra)Rb (??!? being
the negative lookahead operator) returns matches
of regex Rb but only if they do not match Ra.
Thus, by replacing R51 in our original regex with
R51b =(?! ENGLISH|Room|Chapter)[A-Z]\w*\s*,
we produce an improved regex for software names.
The above examples illustrate the general prin-
ciple of our transformation technique. In essence,
we isolate a sub-expression of a given regex R and
modify it such that the resulting regex accepts a sub-
language of R. We consider two kinds of modifica-
tions ? drop-disjunct and include-intersect. In drop-
disjunct, we operate on a sub-expression that corre-
sponds to a disjunct and drop one or more operands
of that disjunct. In include-intersect, we restrict the
chosen sub-expression by intersecting it with some
other regex. Formally,
Definition 4 (Drop-disjunct Transformation). Let
R ? R? be a regex of the form R = Ra?(X)Rb,
where ?(X) denotes the disjunction R1|R2| . . . |Rn of
any non-empty set of regexes X = {R1, R2, . . . , Rn}.
The drop-disjunct transformation DD(R,X, Y ) for some
Y ? X, Y 6= ? results in the new regex Ra?(Y )Rb.
Definition 5 (Include-Intersect Transformation). Let
.\W \s \w[a-zA-Z] \d|[0-9] _[a-z] [A-Z]
Figure 1: Sample Character Classes in Regex
R ? R? be a regex of the form R = RaXRb for some
X ? R?, X 6= ?. The include-intersect transformation
II(R,X, Y ) for some Y ? R?, Y 6= ? results in the new
regex Ra(X ? Y )Rb.
We state the following proposition (proof omit-
ted in the interest of space) that guarantees that both
drop-disjunct and include-intersect restrict the lan-
guage of the resulting regex, and therefore are valid
transformations according to Definition 1.
Proposition 1. Given regexes R,X1, Y1, X2 and Y2
from R? such that DD(R,X1, Y1) and II(R,X2, Y2)
are applicable, L(DD(R,X1, Y1)) ? L(R) and
L(II(R,X2, Y2)) ? L(R).
We now proceed to describe how we use differ-
ent syntactic constructs to apply drop-disjunct and
include-intersect transformations.
Character Class Restrictions Character
classes are short-hand notations for denoting
the disjunction of a set of characters (\d is
equivalent to (0|1...|9); \w is equivalent to
(a|. . .|z|A|. . .|Z|0|1. . .|9| ); etc.).2 Figure 1
illustrates a character class hierarchy in which
each node is a stricter class than its parent (e.g.,
\d is stricter than \w). A replacement of any of
these character classes by one of its descendants
is an instance of the drop-disjunct transformation.
Notice that in Example 2, when replacing R51 with
R51a , we were in effect applying a character class
restriction.
Quantifier Restrictions Quantifiers are used to
define the range of valid counts of a repetitive se-
quence. For instance, a{m,n} looks for a sequence
of a?s of length at least m and at most n. Since
quantifiers are also disjuncts (e.g., a{1,3} is equiv-
alent to a|aa|aaa), the replacement of an expres-
sion R{m,n} with an expression R{m1, n1} (m ?
m1 ? n1 ? n) is an instance of the drop-disjunct
transformation. For example, given a subexpres-
sion of the form a{1,3}, we can replace it with
2Note that there are two distinct character classes \W and \w
24
one of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.
Note that, before applying this transformation, wild-
card expressions such as a+ and a* are replaced by
a{0,maxCount} and a{1,maxCount} respectively,
where maxCount is a user configured maximum
length for the entity being extracted.
Negative Dictionaries Observe that the include-
intersect transformation (Definition 5) is applicable
for every possible sub-expression of a given regex
R. Note that a valid sub-expression in R is any
portion of R where a capturing group can be intro-
duced.3 Consider a regex R = RaXRb with a sub-
expression X; the application of include-intersect
requires another regex Y to yieldRa(X?Y )Rb. We
would like to construct Y such thatRa(X ?Y )Rb is
?better? than R for the task at hand. Therefore, we
construct Y as ?Y ? where Y ? is a regex constructed
from negative matches ofR. Specifically, we look at
each negative match of R and identify the substring
of the match that corresponds to X . We then apply
a greedy heuristic (see below) to these substrings to
yield a negative dictionary Y ?. Finally, the trans-
formed regexRa(X??Y ?)Rb is implemented using
the negative lookahead expression Ra(?! Y?)XRb.
Greedy Heuristic for Negative Dictionaries Im-
plementation of the above procedure requires cer-
tain judicious choices in the construction of the neg-
ative dictionary to ensure tractability of this trans-
formation. Let S(X) denote the distinct strings
that correspond to the sub-expression X in the neg-
ative matches of R.4 Since any subset of S(X)
is a candidate negative dictionary, we are left with
an exponential number of possible transformations.
In our implementation, we used a greedy heuris-
tic to pick a single negative dictionary consisting
of all those elements of S(X) that individually
improve the F-measure. For instance, in Exam-
ple 2, if the independent substitution of R51 with
(?!ENGLISH)[A-Z]\w*\s*, (?!Room)[A-Z]
\w*\s*, and (?!Chapter)[A-Z]\w*\s* each im-
proves the F-measure, we produce a nega-
tive dictionary consisting of ENGLISH, Room, and
Chapter. This is precisely how the disjunct
ENGLISH|Room|Chapter is constructed in R51b .
3For instance, the sub-expressions of ab{1,2}c are a,
ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c.
4S(X) can be obtained automatically by identifying the sub-
string corresponding to the group X in each entry in Mn(R,D)
Procedure ReLIE(Mtr ,Mval,R0 ,T )
//Mtr : set of labeled matches used as training data
//Mval: set of labeled matches used as validation data
// R0 : user-provided regular expression
// T : set of transformations
begin
1. Rnew = R0
2. do {
3. for each transformation ti ? T
4. Candidatei=ApplyTransformations(Rnew, ti)
5. let Candidates =
?
i Candidatei
6. let R? = argmaxR?Candidates F(R,Mtr)
7. if (F(R?,Mtr) <= F(Rnew,Mtr)) return Rnew
8. if (F(R?,Mval) < F(Rnew,Mval)) return Rnew
9. Rnew = R?
10. } while(true)
end
Figure 2: ReLIE Search Algorithm
4 ReLIE Search Algorithm
Figure 2 describes the ReLIE algorithm for the
Regex Learning Problem (Definition 3) based on the
transformations described in Section 3. ReLIE is a
greedy hill climbing search procedure that chooses,
at every iteration, the regex with the highest F-
measure. An iteration in ReLIE consists of:
? Applying every transformation on the current regex
Rnew to obtain a set of candidate regexes
? From the candidates, choosing the regex R? whose
F-measure over the training dataset is maximum
To avoid overfitting, ReLIE terminates when either
of the following conditions is true: (i) there is no
improvement in F-measure over the training set;
(ii) there is a drop in F-measure when applying R?
on the validation set.
The following proposition provides an upper
bound for the running time of the ReLIE algorithm.
Proposition 2. Given any valid set of inputs Mtr,
Mval, R0 , and T , the ReLIE algorithm terminates in at
most |Mn(R0 ,Mtr )| iterations. The running time of the
algorithm TTotal(R0 ,Mtr ,Mval) ? |Mn(R0 ,Mtr )| ?
t0 , where t0 is the time taken for the first iteration of the
algorithm.
Proof. With reference to Figure 2, in each iteration, the
F-measure of the ?best? regex R? is strictly better than
Rnew. Since L(R?) ? L(Rnew), R? eliminates at least
one additional negative match compared toRnew. Hence,
the maximum number of iterations is |Mn(R0 ,Mtr )|.
For a regular expression R, let ncc(R) and nq(R) de-
note, respectively, the number of character classes and
quantifiers in R. The maximum number of possible sub-
expressions in R is |R|2, where |R| is the length of R.
Let MaxQ(R) denote the maximum number of ways in
25
which a single quantifier appearing in R can be restricted
to a smaller range. Let Fcc denote the maximum fanout5
of the character class hierarchy. Let TReEval(D) denote
the average time taken to evaluate a regex over datasetD.
Let Ri denote the regex at the beginning of iteration
i. The number of candidate regexes obtained by applying
the three transformations is
NumRE(Ri,Mtr) ? ncc(Ri)?Fcc+nq(Ri)?MaxQ(Ri)+|Ri|
2
The time taken to enumerate the character class and
quantifier restriction transformations is proportional to
the resulting number of candidate regexes. The time
taken for the negative dictionaries transformation is given
by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is
given by (for some constant c)
TEnum(Ri,Mtr) ? c ? (ncc(Ri) ? Fcc + nq(Ri) ?MaxQ(Ri)
+ |Ri|
2 ?Mn(Ri,Mtr) ? TReEval(Mtr))
Choosing the best transformation involves evaluating
each candidate regex over the training and validation cor-
pus and the time taken for this step is
TPickBest(Ri,Mtr,Mval) = NumRE(Ri,Mtr)
?(TReEval(Mtr) + TReEval(Mval))
The total time taken for an iteration can be written as
TI(Ri,Mtr,Mval) =TEnum(Ri,Mtr)
+ TPickBest(Ri,Mtr,Mval)
It can be shown that the time taken in each iteration
decreases monotonically (details omitted in the interest of
space). Therefore, the total running time of the algorithm
is given by
TTotal(R0 ,Mtr ,Mval) =
?
TI(Ri,Mtr,Mval)
? |Mn(R0 ,Mtr )| ? t0 .
where t0 = TI(R0 ,Mtr ,Mval) is the running time
of the first iteration of the algorithm.
5 Experiments
In this section, we present an empirical study of
the ReLIE algorithm using four extraction tasks over
three real-life data sets. The goal of this study is to
evaluate the effectiveness of ReLIE in learning com-
plex regexes and to investigate how it compares with
standard machine learning algorithms.
5.1 Experimental Setup
Data Set The datasets used in our experiments are:
? EWeb: A collection of 50,000 web pages crawled
from a corporate intranet.
5Fanout is the number of ways in which a character class
may be restricted as defined by the hierarchy (e.g. Figure 1).
? AWeb: A set of 50,000 web pages obtained from
the publicly available University of Michigan Web
page collection (Li et al, 2006), including a sub-
collection of 10,000 pages (AWeb-S).
? Email: A collection of 10,000 emails obtained
from the publicly available Enron email collec-
tion (Minkov et al, 2005).
Extraction Tasks SoftwareNameTask, CourseNum-
berTask and PhoneNumberTask were evaluated on
EWeb, AWeb and Email, respectively. Since web
pages have large number of URLs, to keep the la-
beling task manageable, URLTask was evaluated on
AWeb-S.
Gold Standard For each task, the gold standard
was created by manually labeling all matches for the
initial regex. Note that only exact matches with the
gold standard are considered correct in our evalua-
tions. 6
Comparison Study To evaluate ReLIE for entity
extraction vis-a-vis existing algorithms, we used the
popular conditional random field (CRF). Specifi-
cally, we used the MinorThird (Cohen, 2004) imple-
mentation of CRF to train models for all four extrac-
tion tasks. For training the CRF we provided it with
the set of positive and negative matches from the ini-
tial regex with a context of 200 characters on either
side of each match7. Since it is unlikely that useful
features are located far away from the entity, we be-
lieve that 200 characters on either side is sufficient
context. The CRF used the base features described
in (Cohen et al, 2005). To ensure fair compari-
son with ReLIE, we also included the matches corre-
sponding to the input regex as a feature to the CRF.
In practice, more complex features (e.g., dictionar-
ies, simple regexes) derived by domain experts are
often provided to CRFs. However, such features can
also be used to refine the initial regex given to ReLIE.
Hence, with a view to investigating the ?raw? learn-
ing capability of the two approaches, we chose to
run all our experiments without any additional man-
ually derived features. In fact, the patterns learned
by ReLIE through transformations are often similar
6The labeled data will be made publicly available at
http://www.eecs.umich.edu/db/regexLearning/.
7Ideally, we would have preferred to let MinorThird extract
appropriate features from complete documents in the training-
set but could not get it to load our large datasets.
26
(a) SoftwareNameTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(b) CourseNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(c) URLTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(d) PhoneNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
Figure 3: Extraction Qualitya
aFor SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program
failed repeatedly during the training phase.
to the features that domain experts may provide to
CRF. We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to
evaluate the effectiveness of ReLIE and CRF. We di-
vided each dataset into 10 equal parts and used X%
of the dataset for training (X=10, 40 and 80), 10%
for validation, and remaining (90-X)% for testing.
All results are reported on the test set.
5.2 Results
Four extraction tasks were chosen to reflect the enti-
ties commonly present in the three datasets.
? SoftwareNameTask: Extracting software names such
as Lotus Notes 8.0, Open Office Suite 2007.
? CourseNumberTask: Extracting university course
numbers such as EECS 584, Pharm 101.
? PhoneNumberTask: Extracting phone numbers such
as 1-800-COMCAST, (425)123 5678.
? URLTask: Extracting URLs such as
http:\\www.abc.com and lsa.umich.edu/ foo/.8
This section summarizes the results of our empir-
ical evaluation comparing ReLIE and CRF.
8URLTask may appear to be simplistic. However, extracting
URLs without the leading protocol definitions (e.g. http) can
be challenging.
Raw Extraction Quality The cross-validated re-
sults across all four tasks are presented in Figure 3.
? With 10% training data, ReLIE outperforms CRF
on three out of four tasks with a difference in F-
measure ranging from 0.1 to 0.2.
? As training data increases, both algorithms perform
better with the gap between the two reducing for
all the four tasks. For CourseNumberTask and URL-
Task, CRF does slightly better than ReLIE for larger
training dataset. For the other two tasks, ReLIE re-
tains its advantage over CRF.9
The above results indicate that ReLIE performs
comparably with CRF with a slight edge in condi-
tions of limited training data. Indeed, the capability
to learn high-quality extractors using a small train-
ing set is important because labeled data is often ex-
pensive to obtain. For precisely this same reason, we
would ideally like to learn the extractors once and
then apply them to other datasets as needed. Since
these other datasets may be from a different domain,
we next performed a cross-domain test (i.e., training
9For SoftwareNameTask, with 80% training data we could
not obtain results for CRF as the program failed repeatedly dur-
ing the training phase.
27
and testing on different domains).
Task(Training, Testing)
Data for Training 10% 40% 80%
ReLIE CRF ReLIE CRF ReLIE CRF
SoftwareNameTask(EWeb,AWeb) 0.920 0.297 0.977 0.503 0.971 N/A
URLTask(AWeb-S,Email) 0.690 0.209 0.784 0.380 0.801 0.507
PhoneNumberTask(Email,AWeb) 0.357 0.130 0.475 0.125 0.513 0.120
Table 1: Cross Domain Test (F-measure).
Technique
SoftwareNameTask CourseNumberTask URLTask PhoneNumberTask
training testing training testing training testing training testing
ReLIE 511.7 20.6 69.3 18.4 73.8 7.7 39.4 1.1
CRF 7597.0 2315.8 482.5 75.4 438.7 53.8 434.8 57.7
t(ReLIE)
t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019
Table 2: Average Training/Testing Time (sec)(with 40% data for training)
Task(Extra Feature)
Data for Training 10% 40% 80%
CRF C+RL CRF C+RL CRF C+RL
CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845
PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964
Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced with
features learned by ReLIE).
Cross-domain Evaluation Table 1
summarizes the results of training
the algorithms on one data set and
testing on another. The scenarios
chosen are: (i) SoftwareNameTask
trained on EWeb and tested on
AWeb, (ii) URLTask trained on AWeb
and tested on Email, and (iii) Pho-
neNumberTask trained on Email
and tested on AWeb.10 We can
see that ReLIE significantly out-
performs CRF for all three tasks,
even when provided with a large
training dataset. Compared to test-
ing on the same dataset, there is a
reduction in F-measure (less than
0.1 in many cases) when the regex
learned by ReLIE is applied to a dif-
ferent dataset, while the drop for
CRF is much more significant (over 0.5 in many
cases).11
Training Time Another issue of practical consid-
eration is the efficiency of the learning algorithm.
Table 2 reports the average training and testing time
for both algorithms on the four tasks. On average Re-
LIE is an order of magnitude faster than CRF in both
building the model and applying the learnt model.
Robustness to Variations in Input Regexes The
transformations done by ReLIE are based on the
structure of the input regex. Therefore given differ-
ent input regexes, the final regexes learned by ReLIE
will be different. To evaluate the impact of the struc-
ture of the input regex on the quality of the regex
learned by ReLIE, we started with different regexes12
for the same task. We found that ReLIE is robust
to variations in input regexes. For instance, on Soft-
wareNameTask, the standard deviation in F-measure
10We do not report results for CourseNumberTask as course
numbers are specific to academic webpages and do not appear
in the other two domains
11Similar cross-domain performance deterioration for a ma-
chine learning approach has been observed by (Guo et al,
2006).
12Recall that the search space of ReLIE is limited by L(R0)
(Assumption 1). Thus to ensure meaningful comparison, for
the same task any two given input regexes R0 and R?0 are cho-
sen in such a way that although their structures are different,
Mp(R0,D) = Mp(R?0,D) and Mn(R0,D) = Mn(R
?
0,D).
of the final regexes generated from six different in-
put regexes was less than 0.05. Further details of this
experiment are omitted in the interest of space.
5.3 Discussion
The results of our comparison study (Figure 3) in-
dicates that for raw extraction quality ReLIE has a
slight edge over CRF for small training data. How-
ever, in cross-domain performance (Table 1) ReLIE
is significantly better than CRF (by 0.41 on aver-
age) . To understand this discrepancy, we examined
the final regex learned by ReLIE and compared that
with the features learned by CRF. Examples of ini-
tial regexes with corresponding final regexes learnt
by ReLIE with 10% training data are listed in Ta-
ble 4. Recall, from Section 3, that ReLIE transfor-
mations include character class restrictions, quanti-
fier restrictions and addition of negative dictionar-
ies. For instance, in the SoftwareNameTask, the final
regex listed was obtained by restricting [a-zA-Z]
to [a-z], \w to [a-zA-Z], and adding the nega-
tive dictionary (Copyright|Fall| ? ? ? |Issue). Sim-
ilarly, for the PhoneNumberTask, the final regex
involved two negative dictionaries (expressed as
(?![,]) and (?![,:])) 13 and quantifier restric-
tions (e.g. the first [A-Z\d]{2,4} was transformed
13To obtain these negative dictionaries, ReLIE not only
needs to correctly identify the dictionary entries from negative
matches but also has to place the corresponding negative looka-
head expression at the appropriate place in the regex.
28
SoftwareNameTask
R0 \b([A-Z][a-zA-Z]{1,10}\s){1,5}\s*(\w{0,2}\d[\.]?){1,4}\b
Rfinal
\b((?!(Copyright|Page|Physics|Question| ? ? ? |Article|Issue))[A-Z][a-z]{1,10}
\s){1,5}\s*([a-zA-Z]{0,2}\d[\.]?){1,4}\b
PhoneNumberTask R0 \b(1\W+)?\W?\d{3,3}\W*\s*\W?[A-Z\d]{2,4}\s*\W?[A-Z\d]{2,4}\bRfinal \b(1\W+)?\W?\d{3,3}((?![,])\W*)\s*\W?[A-Z\d]{3,3}\s*((?![,:])\W?)[A-Z\d]{3,4}\b
CourseNumberTask R0 \b([A-Z][a-zA-Z]+)\s+\d{3,3}\bRfinal \b(((?!(At|Between| ? ? ?Contact|Some|Suite|Volume))[A-Z][a-zA-Z]+))\s+\d{3,3}\b
URLTask R0 \b(\w+://)?(\w+\.){0,2}\w+\.\w+(/[
?\s]+){0,20}\b
Rfinal
\b((?!(Response 20010702 1607.csv| ? ? ?))((\w+://)?(\w+\.){0,2}\w+\.(?!(ppt
| ? ? ?doc))[a-zA-Z]{2,3}))(/[?\s]+){0,20}\b
Table 4: Sample Regular Expressions Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0
modified by ReLIE and the corresponding parts in Rfinal are highlighted.)
into [A-Z\d]{3,3}).
After examining the features learnt by CRF, it was
clear that while CRF could learn features such as the
negative dictionary it is unable to learn character-
level features. This should not be surprising since
our CRF was trained with primarily tokens as fea-
tures (cf. Section 5.1). While this limitation was less
of a factor in experiments involving data from the
same domain (some effects were seen with smaller
training data), it does explain the significant differ-
ence between the two algorithms in cross-domain
tasks where the vocabulary can be significantly dif-
ferent. Indeed, in practical usage of CRF, the main
challenge is to come up with additional complex fea-
tures (often in the form of dictionary and regex pat-
terns) that need to be given to the CRF (Minkov et
al., 2005). Such complex features are largely hand-
crafted and thus expensive to obtain. Since the Re-
LIE transformations are operations over characters,
a natural question to ask is: ?Can the regex learned
by ReLIE be used to provide features to CRF?? We
answer this question below.
5.4 ReLIE as Feature Extractor for CRF
To understand the effect of incorporating ReLIE-
identified features into CRF, we chose the two tasks
(CourseNumberTask and PhoneNumberTask) with the
least F-measure in our experiments to determine raw
extraction quality. We examined the final regex pro-
duced by ReLIE and manually extracted portions
to serve as features. For example, the negative
dictionary learned by ReLIE for the CourseNumber-
Task (At|Between| ? ? ? |Volume) was incorporated as
a feature into CRF. To help isolate the effects, for
each task, we only incorporated features correspond-
ing to a single transformation: negative dictionar-
ies for CourseNumberTask and quantifier restrictions
for PhoneNumberTask. The results of these experi-
ments are shown in Table 3. The first point worthy of
note is that performance has improved in all but one
case. Second, despite the F-measure on CourseNum-
berTask being lower than PhoneNumberTask (presum-
ably more potential for improvement), the improve-
ments on PhoneNumberTask are significantly higher.
This observation is consistent with our conjecture
in Section 5.1 that CRF learns token-level features;
therefore incorporating negative dictionaries as extra
feature provides only limited improvement. Admit-
tedly more experiments are needed to understand the
full impact of incorporating ReLIE-identified fea-
tures into CRF. However, we do believe that this is
an exciting direction of future research.
6 Summary and Future Work
We proposed a novel formulation of the problem of
learning complex character-level regexes for entity
extraction tasks. We introduced the concept of regex
transformations and described how these could be
realized using the syntactic constructs of modern
regex languages. We presented ReLIE, a powerful
regex learning algorithm that exploits these ideas.
Our experiments demonstrate that ReLIE is very ef-
fective for certain classes of entity extraction, partic-
ularly under conditions of cross-domain and limited
training data. Our preliminary results also indicate
the possibility of using ReLIE as a powerful feature
extractor for CRF and other machine learning algo-
rithms. Further investigation of this aspect of ReLIE
presents an interesting avenue of future work.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful and constructive comments and suggestions. We
are also grateful for comments from David Gondek
and Sebastian Blohm.
29
References
R. Alquezar and A. Sanfeliu. 1994. Incremental gram-
matical inference from positive and negative data using
unbiased finite state automata. In SSPR.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
TEXT PROGRAM.
Geert Jan Bex et al 2006. Inference of concise DTDs
from XML data. In VLDB.
Eric Brill. 2000. Pattern-based disambiguation for natu-
ral language processing. In SIGDAT.
William W. Cohen and Andrew McCallum. 2003. Infor-
mation Extraction from the World Wide Web. in KDD
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
William W. Cohen et al 2005. Learning to Understand
Web Site Update Requests. In IJCAI.
Fabio Ciravegna. 2001. Adaptive information extraction
from text by rule induction and generalization. In IJ-
CAI.
H. Cunningham. 1999. JAPE ? a java annotation patterns
engine.
Francois Denis et al 2004. Learning regular languages
using RFSAs. Theor. Comput. Sci., 313(2):267?294.
Francois Denis. 2001. Learning regular languages
from simple positive examples. Machine Learning,
44(1/2):37?66.
Pedro DeRose et al 2007. DBLife: A Community In-
formation Management Platform for the Database Re-
search Community In CIDR
Pierre Dupont. 1996. Incremental regular inference. In
ICGI.
Ronen Feldman et al. 2006. Self-supervised Relation
Extraction from the Web. In ISMIS.
Henning Fernau. 2005. Algorithms for learning regular
expressions. In ALT.
Laura Firoiu et al 1998. Learning regular languages
from positive evidence. In CogSci.
K. Fukuda et al 1998. Toward information extraction:
identifying protein names from biological papers. Pac
Symp Biocomput., 1998:707?718
Ugo Galassi and Attilio Giordana. 2005. Learning regu-
lar expressions from noisy sequences. In SARA.
Minos Garofalakis et al 2000. XTRACT: a system for
extracting document type descriptors from XML doc-
uments. In SIGMOD.
Hong Lei Guo et al 2006. Empirical Study on the
Performance Stability of Named Entity Recognition
Model across Domains In EMNLP.
Java Regular Expressions. 2008. http://java.sun.com
/javase/6/docs/api/java/util/regex/package-
summary.html.
Dan Klein et al 2003. Named Entity Recognition with
Character-Level Models. In HLT-NAACL.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In ACL.
Yunyao Li et al 2006. Getting work done on the web:
Supporting transactional queries. In SIGIR.
Andrew McCallum et al 2000. Maximum Entropy
Markov Models for Information Extraction and Seg-
mentation. In ICML.
Einat Minkov et al 2005. Extracting personal names
from emails: Applying named entity recognition to in-
formal text. In HLT/EMNLP.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34:233?272.
Lorraine Tanabe and W. John Wilbur 2002. Tagging
gene and protein names in biomedical text. Bioinfor-
matics, 18:1124?1132.
Tianhao Wu and William M. Pottenger. 2005. A semi-
supervised active learning algorithm for information
extraction from textual data. JASIST, 56(3):258?271.
Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shiv-
akumar Vaithyanathan 2007. Navigating the intranet
with high precision. In WWW.
30
The Sentimental Factor: Improving Review Classification via
Human-Provided Information
Philip Beineke?and Trevor Hastie
Dept. of Statistics
Stanford University
Stanford, CA 94305
Shivakumar Vaithyanathan
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120-6099
Abstract
Sentiment classification is the task of labeling a re-
view document according to the polarity of its pre-
vailing opinion (favorable or unfavorable). In ap-
proaching this problem, a model builder often has
three sources of information available: a small col-
lection of labeled documents, a large collection of
unlabeled documents, and human understanding of
language. Ideally, a learning method will utilize all
three sources. To accomplish this goal, we general-
ize an existing procedure that uses the latter two.
We extend this procedure by re-interpreting it
as a Naive Bayes model for document sentiment.
Viewed as such, it can also be seen to extract a
pair of derived features that are linearly combined
to predict sentiment. This perspective allows us to
improve upon previous methods, primarily through
two strategies: incorporating additional derived fea-
tures into the model and, where possible, using la-
beled data to estimate their relative influence.
1 Introduction
Text documents are available in ever-increasing
numbers, making automated techniques for infor-
mation extraction increasingly useful. Traditionally,
most research effort has been directed towards ?ob-
jective? information, such as classification accord-
ing to topic; however, interest is growing in produc-
ing information about the opinions that a document
contains; for instance, Morinaga et al (2002). In
March, 2004, the American Association for Artifi-
cial Intelligence held a symposium in this area, en-
titled ?Exploring Affect and Attitude in Text.?
One task in opinion extraction is to label a re-
view document d according to its prevailing senti-
ment s ? {?1, 1} (unfavorable or favorable). Sev-
eral previous papers have addressed this problem
by building models that rely exclusively upon la-
beled documents, e.g. Pang et al (2002), Dave
et al (2003). By learning models from labeled
data, one can apply familiar, powerful techniques
directly; however, in practice it may be difficult to
obtain enough labeled reviews to learn model pa-
rameters accurately.
A contrasting approach (Turney, 2002) relies only
upon documents whose labels are unknown. This
makes it possible to use a large underlying corpus ?
in this case, the entire Internet as seen through the
AltaVista search engine. As a result, estimates for
model parameters are subject to a relatively small
amount of random variation. The corresponding
drawback to such an approach is that its predictions
are not validated on actual documents.
In machine learning, it has often been effec-
tive to use labeled and unlabeled examples in tan-
dem, e.g. Nigam et al (2000). Turney?s model
introduces the further consideration of incorporat-
ing human-provided knowledge about language. In
this paper we build models that utilize all three
sources: labeled documents, unlabeled documents,
and human-provided information.
The basic concept behind Turney?s model is quite
simple. The ?sentiment orientation? (Hatzivas-
siloglou and McKeown, 1997) of a pair of words
is taken to be known. These words serve as ?an-
chors? for positive and negative sentiment. Words
that co-occur more frequently with one anchor than
the other are themselves taken to be predictive of
sentiment. As a result, information about a pair of
words is generalized to many words, and then to
documents.
In the following section, we relate this model
with Naive Bayes classification, showing that Tur-
ney?s classifier is a ?pseudo-supervised? approach:
it effectively generates a new corpus of labeled doc-
uments, upon which it fits a Naive Bayes classifier.
This insight allows the procedure to be represented
as a probability model that is linear on the logistic
scale, which in turn suggests generalizations that are
developed in subsequent sections.
2 A Logistic Model for Sentiment
2.1 Turney?s Sentiment Classifier
In Turney?s model, the ?sentiment orientation? ? of
word w is estimated as follows.
??(w) = log
N(w,excellent)/Nexcellent
N(w,poor)/Npoor
(1)
Here, Na is the total number of sites on the Internet
that contain an occurrence of a ? a feature that can
be a word type or a phrase. N(w,a) is the number of
sites in which features w and a appear ?near? each
other, i.e. in the same passage of text, within a span
of ten words. Both numbers are obtained from the
hit count that results from a query of the AltaVista
search engine. The rationale for this estimate is that
words that express similar sentiment often co-occur,
while words that express conflicting sentiment co-
occur more rarely. Thus, a word that co-occurs more
frequently with excellent than poor is estimated to
have a positive sentiment orientation.
To extrapolate from words to documents, the esti-
mated sentiment s? ? {?1, 1} of a review document
d is the sign of the average sentiment orientation of
its constituent features.1 To represent this estimate
formally, we introduce the following notation: W
is a ?dictionary? of features: (w1, . . . , wp). Each
feature?s respective sentiment orientation is repre-
sented as an entry in the vector ?? of length p:
??j = ??(wj) (2)
Given a collection of n review documents, the i-th
each di is also represented as a vector of length p,
with dij equal to the number of times that feature wj
occurs in di. The length of a document is its total
number of features, |di| =
?p
j=1 dij .
Turney?s classifier for the i-th document?s senti-
ment si can now be written:
s?i = sign
(?p
j=1 ??jdij
|di|
)
(3)
Using a carefully chosen collection of features,
this classifier produces correct results on 65.8% of
a collection of 120 movie reviews, where 60 are
labeled positive and 60 negative. Although this is
not a particularly encouraging result, movie reviews
tend to be a difficult domain. Accuracy on senti-
ment classification in other domains exceeds 80%
(Turney, 2002).
1Note that not all words or phrases need to be considered as
features. In Turney (2002), features are selected according to
part-of-speech labels.
2.2 Naive Bayes Classification
Bayes? Theorem provides a convenient framework
for predicting a binary response s ? {?1, 1} from a
feature vector x:
Pr(s = 1|x) = Pr(x|s = 1)pi1?
k?{?1,1} Pr(x|s = k)pik
(4)
For a labeled sample of data (xi, si), i = 1, ..., n,
a class?s marginal probability pik can be estimated
trivially as the proportion of training samples be-
longing to the class. Thus the critical aspect of clas-
sification by Bayes? Theorem is to estimate the con-
ditional distribution of x given s. Naive Bayes sim-
plifies this problem by making a ?naive? assump-
tion: within a class, the different feature values are
taken to be independent of one another.
Pr(x|s) =
?
j
Pr(xj|s) (5)
As a result, the estimation problem is reduced to
univariate distributions.
? Naive Bayes for a Multinomial Distribution
We consider a ?bag of words? model for a docu-
ment that belongs to class k, where features are as-
sumed to result from a sequence of |di| independent
multinomial draws with outcome probability vector
qk = (qk1, . . . , qkp).
Given a collection of documents with labels,
(di, si), i = 1, . . . , n, a natural estimate for qkj is
the fraction of all features in documents of class k
that equal wj:
q?kj =
?
i:si=k dij?
i:si=k |di|
(6)
In the two-class case, the logit transformation
provides a revealing representation of the class pos-
terior probabilities of the Naive Bayes model.
l?ogit(s|d) , log P?r(s = 1|d)
P?r(s = ?1|d)
(7)
= log p?i1p?i?1
+
p?
j=1
dj log
q?1j
q??1j
(8)
= ??0 +
p?
j=1
dj??j (9)
where ??0 = log
p?i1
p?i?1
(10)
??j = log
q?1j
q??1j
(11)
Observe that the estimate for the logit in Equation
9 has a simple structure: it is a linear function of
d. Models that take this form are commonplace in
classification.
2.3 Turney?s Classifier as Naive Bayes
Although Naive Bayes classification requires a la-
beled corpus of documents, we show in this sec-
tion that Turney?s approach corresponds to a Naive
Bayes model. The necessary documents and their
corresponding labels are built from the spans of text
that surround the anchor words excellent and poor.
More formally, a labeled corpus may be produced
by the following procedure:
1. For a particular anchor ak, locate all of the sites
on the Internet where it occurs.
2. From all of the pages within a site, gather the
features that occur within ten words of an oc-
currence of ak, with any particular feature in-
cluded at most once. This list comprises a new
?document,? representing that site.2
3. Label this document +1 if ak = excellent, -1
if ak = poor.
When a Naive Bayes model is fit to the corpus
described above, it results in a vector ?? of length
p, consisting of coefficient estimates for all fea-
tures. In Propositions 1 and 2 below, we show that
Turney?s estimates of sentiment orientation ?? are
closely related to ??, and that both estimates produce
identical classifiers.
Proposition 1
?? = C1?? (12)
where C1 =
Nexc./
?
i:si=1 |di|
Npoor/
?
i:si=?1 |di|
(13)
Proof: Because a feature is restricted to at most one
occurrence in a document,
?
i:si=k
dij = N(w,ak) (14)
Then from Equations 6 and 11:
??j = log
q?1j
q??1j
(15)
= log
N(w,exc.)/
?
i:si=1 |di|
N(w,poor)/
?
i:si=?1 |di|
(16)
= C1??j (17)
2
2If both anchors occur on a site, then there will actually be
two documents, one for each sentiment
Proposition 2 Turney?s classifier is identical to a
Naive Bayes classifier fit on this corpus, with pi1 =
pi?1 = 0.5.
Proof: A Naive Bayes classifier typically assigns an
observation to its most probable class. This is equiv-
alent to classifying according to the sign of the es-
timated logit. So for any document, we must show
that both the logit estimate and the average senti-
ment orientation are identical in sign.
When pi1 = 0.5, ?0 = 0. Thus the estimated logit
is
l?ogit(s|d) =
p?
j=1
??jdj (18)
= C1
p?
j=1
??jdj (19)
This is a positive multiple of Turney?s classifier
(Equation 3), so they clearly match in sign. 2
3 A More Versatile Model
3.1 Desired Extensions
By understanding Turney?s model within a Naive
Bayes framework, we are able to interpret its out-
put as a probability model for document classes. In
the presence of labeled examples, this insight also
makes it possible to estimate the intercept term ?0.
Further, we are able to view this model as a mem-
ber of a broad class: linear estimates for the logit.
This understanding facilitates further extensions, in
particular, utilizing the following:
1. Labeled documents
2. More anchor words
The reason for using labeled documents is
straightforward; labels offer validation for any cho-
sen model. Using additional anchors is desirable
in part because it is inexpensive to produce lists of
words that are believed to reflect positive sentiment,
perhaps by reference to a thesaurus. In addition, a
single anchor may be at once too general and too
specific.
An anchor may be too general in the sense that
many common words have multiple meanings, and
not all of them reflect a chosen sentiment orien-
tation. For example, poor can refer to an objec-
tive economic state that does not necessarily express
negative sentiment. As a result, a word such as
income appears 4.18 times as frequently with poor
as excellent, even though it does not convey nega-
tive sentiment. Similarly, excellent has a technical
meaning in antiquity trading, which causes it to ap-
pear 3.34 times as frequently with furniture.
An anchor may also be too specific, in the sense
that there are a variety of different ways to express
sentiment, and a single anchor may not capture them
all. So a word like pretentious carries a strong
negative sentiment but co-occurs only slightly more
frequently (1.23 times) with excellent than poor.
Likewise, fascination generally reflects a positive
sentiment, yet it appears slightly more frequently
(1.06 times) with poor than excellent.
3.2 Other Sources of Unlabeled Data
The use of additional anchors has a drawback in
terms of being resource-intensive. A feature set may
contain many words and phrases, and each of them
requires a separate AltaVista query for every chosen
anchor word. In the case of 30,000 features and ten
queries per minute, downloads for a single anchor
word require over two days of data collection.
An alternative approach is to access a large
collection of documents directly. Then all co-
occurrences can be counted in a single pass.
Although this approach dramatically reduces the
amount of data available, it does offer several ad-
vantages.
? Increased Query Options Search engine
queries of the form phrase NEAR anchor
may not produce all of the desired co-
occurrence counts. For instance, one may wish
to run queries that use stemmed words, hy-
phenated words, or punctuation marks. One
may also wish to modify the definition of
NEAR, or to count individual co-occurrences,
rather than counting sites that contain at least
one co-occurrence.
? Topic Matching Across the Internet as a
whole, features may not exhibit the same cor-
relation structure as they do within a specific
domain. By restricting attention to documents
within a domain, one may hope to avoid co-
occurrences that are primarily relevant to other
subjects.
? Reproducibility On a fixed corpus, counts of
word occurrences produce consistent results.
Due to the dynamic nature of the Internet,
numbers may fluctuate.
3.3 Co-Occurrences and Derived Features
The Naive Bayes coefficient estimate ??j may itself
be interpreted as an intercept term plus a linear com-
bination of features of the form log N(wj ,ak).
Num. of Labeled Occurrences Correlation
1 - 5 0.022
6 - 10 0.082
11 - 25 0.113
26 - 50 0.183
51 - 75 0.283
76 - 100 0.316
Figure 1: Correlation between Supervised and Un-
supervised Coefficient Estimates
??j = log
N(j,exc.)/
?
i:si=1 |di|
N(j,pr.)/
?
i:si=?1 |di|
(20)
= log C1 + log N(j,exc.) ? log N(j,pr.)
(21)
We generalize this estimate as follows: for a col-
lection of K different anchor words, we consider a
general linear combination of logged co-occurrence
counts.
??j =
K?
k=1
?k log N(wj ,ak) (22)
In the special case of a Naive Bayes model, ?k =
1 when the k-th anchor word ak conveys positive
sentiment, ?1 when it conveys negative sentiment.
Replacing the logit estimate in Equation 9 with
an estimate of this form, the model becomes:
l?ogit(s|d) = ??0 +
p?
j=1
dj??j (23)
= ??0 +
p?
j=1
K?
k=1
dj?k log N(wj ,ak)
(24)
= ?0 +
K?
k=1
?k
p?
j=1
dj log N(wj ,ak)
(25)
(26)
This model has only K + 1 parameters:
?0, ?1, . . . , ?K . These can be learned straightfor-
wardly from labeled documents by a method such
as logistic regression.
Observe that a document receives a score for each
anchor word
?p
j=1 dj log N(wj ,ak). Effectively, the
predictor variables in this model are no longer
counts of the original features dj . Rather, they are
?2.0 ?1.5 ?1.0 ?0.5 0.0 0.5 1.0 1.5
?
3
?
2
?
1
0
1
2
3
4
Traditional Naive Bayes Coefs.
Tu
rn
ey
 N
ai
ve
 B
ay
es
 C
oe
fs
.
Unsupervised vs. Supervised Coefficients
Figure 2: Unsupervised versus Supervised Coeffi-
cient Estimates
inner products between the entire feature vector d
and the logged co-occurence vector N(w,ak). In this
respect, the vector of logged co-occurrences is used
to produce derived feature.
4 Data Analysis
4.1 Accuracy of Unsupervised Coefficients
By means of a Perl script that uses the Lynx
browser, Version 2.8.3rel.1, we download AltaVista
hit counts for queries of the form ?target NEAR
anchor.? The initial list of targets consists of
44,321 word types extracted from the Pang cor-
pus of 1400 labeled movie reviews. After pre-
processing, this number is reduced to 28,629.3
In Figure 1, we compare estimates produced by
two Naive Bayes procedures. For each feature wj ,
we estimate ?j by using Turney?s procedure, and
by fitting a traditional Naive Bayes model to the
labeled documents. The traditional estimates are
smoothed by assuming a Beta prior distribution that
is equivalent to having four previous observations of
wj in documents of each class.
q?1j
q??1j
= C2
4 +?i:si=1 dij
4 +?i:si=?1 dij
(27)
where C2 =
4p +?i:si=1 |di|
4p +
?
i:si=?1 |di|
(28)
Here, dij is used to indicate feature presence:
dij =
{
1 if wj appears in di
0 otherwise (29)
3We eliminate extremely rare words by requiring each target
to co-occur at least once with each anchor. In addition, certain
types, such as words containing hyphens, apostrophes, or other
punctuation marks, do not appear to produce valid counts, so
they are discarded.
Positive Negative
best awful
brilliant bad
excellent pathetic
spectacular poor
wonderful worst
Figure 3: Selected Anchor Words
We choose this fitting procedure among several can-
didates because it performs well in classifying test
documents.
In Figure 1, each entry in the right-hand col-
umn is the observed correlation between these two
estimates over a subset of features. For features
that occur in five documents or fewer, the corre-
lation is very weak (0.022). This is not surpris-
ing, as it is difficult to estimate a coefficient from
such a small number of labeled examples. Corre-
lations are stronger for more common features, but
never strong. As a baseline for comparison, Naive
Bayes coefficients can be estimated using a subset
of their labeled occurrences. With two independent
sets of 51-75 occurrences, Naive Bayes coefficient
estimates had a correlation of 0.475.
Figure 2 is a scatterplot of the same coefficient
estimates for word types that appear in 51 to 100
documents. The great majority of features do not
have large coefficients, but even for the ones that
do, there is not a tight correlation.
4.2 Additional Anchors
We wish to learn how our model performance de-
pends on the choice and number of anchor words.
Selecting from WordNet synonym lists (Fellbaum,
1998), we choose five positive anchor words and
five negative (Figure 3). This produces a total of
25 different possible pairs for use in producing co-
efficient estimates.
Figure 4 shows the classification performance
of unsupervised procedures using the 1400 labeled
Pang documents as test data. Coefficients ??j are es-
timated as described in Equation 22. Several differ-
ent experimental conditions are applied. The meth-
ods labeled ?Count? use the original un-normalized
coefficients, while those labeled ?Norm.? have been
normalized so that the number of co-occurrences
with each anchor have identical variance. Results
are shown when rare words (with three or fewer oc-
currences in the labeled corpus) are included and
omitted. The methods ?pair? and ?10? describe
whether all ten anchor coefficients are used at once,
or just the ones that correspond to a single pair of
Method Feat. Misclass. St.Dev
Count Pair >3 39.6% 2.9%
Norm. Pair >3 38.4% 3.0%
Count Pair all 37.4% 3.1%
Norm. Pair all 37.3% 3.0%
Count 10 > 3 36.4% ?
Norm. 10 > 3 35.4% ?
Count 10 all 34.6% ?
Norm. 10 all 34.1% ?
Figure 4: Classification Error Rates for Different
Unsupervised Approaches
anchor words. For anchor pairs, the mean error
across all 25 pairs is reported, along with its stan-
dard deviation.
Patterns are consistent across the different condi-
tions. A relatively large improvement comes from
using all ten anchor words. Smaller benefits arise
from including rare words and from normalizing
model coefficients.
Models that use the original pair of anchor words,
excellent and poor, perform slightly better than the
average pair. Whereas mean performance ranges
from 37.3% to 39.6%, misclassification rates for
this pair of anchors ranges from 37.4% to 38.1%.
4.3 A Smaller Unlabeled Corpus
As described in Section 3.2, there are several rea-
sons to explore the use of a smaller unlabeled cor-
pus, rather than the entire Internet. In our experi-
ments, we use additional movie reviews as our doc-
uments. For this domain, Pang makes available
27,886 reviews.4
Because this corpus offers dramatically fewer in-
stances of anchor words, we modify our estimation
procedure. Rather than discarding words that rarely
co-occur with anchors, we use the same feature set
as before and regularize estimates by the same pro-
cedure used in the Naive Bayes procedure described
earlier.
Using all features, and ten anchor words with nor-
malized scores, test error is 35.0%. This suggests
that comparable results can be attained while re-
ferring to a considerably smaller unlabeled corpus.
Rather than requiring several days of downloads,
the count of nearby co-occurrences was completed
in under ten minutes.
Because this procedure enables fast access to
counts, we explore the possibility of dramatically
enlarging our collection of anchor words. We col-
4This corpus is freely available on the following website:
http://www.cs.cornell.edu/people/pabo/movie-review-data/.
100 200 300 400 500 600
0.
30
0.
32
0.
34
0.
36
0.
38
0.
40
Num. of Labeled Documents
Cl
as
sif
. E
rro
r
Misclassification versus Sample Size
Figure 5: Misclassification with Labeled Docu-
ments. The solid curve represents a latent fac-
tor model with estimated coefficients. The dashed
curve uses a Naive Bayes classifier. The two hor-
izontal lines represent unsupervised estimates; the
upper one is for the original unsupervised classifier,
and the lower is for the most successful unsuper-
vised method.
lect data for the complete set of WordNet syn-
onyms for the words good, best, bad, boring, and
dreadful. This yields a total of 83 anchor words,
35 positive and 48 negative. When all of these an-
chors are used in conjunction, test error increases to
38.3%. One possible difficulty in using this auto-
mated procedure is that some synonyms for a word
do not carry the same sentiment orientation. For in-
stance, intense is listed as a synonym for bad, even
though its presence in a movie review is a strongly
positive indication.5
4.4 Methods with Supervision
As demonstrated in Section 3.3, each anchor word
ak is associated with a coefficient ?k. In unsu-
pervised models, these coefficients are assumed to
be known. However, when labeled documents are
available, it may be advantageous to estimate them.
Figure 5 compares the performance of a model
with estimated coefficient vector ?, as opposed to
unsupervised models and a traditional supervised
approach. When a moderate number of labeled doc-
uments are available, it offers a noticeable improve-
ment.
The supervised method used for reference in this
case is the Naive Bayes model that is described in
section 4.1. Naive Bayes classification is of partic-
ular interest here because it converges faster to its
asymptotic optimum than do discriminative meth-
ods (Ng, A. Y. and Jordan, M., 2002). Further, with
5In the labeled Pang corpus, intense appears in 38 positive
rev ews and only 6 negative ones.
a larger number of labeled documents, its perfor-
mance on this corpus is comparable to that of Sup-
port Vector Machines and Maximum Entropy mod-
els (Pang et al, 2002).
The coefficient vector ? is estimated by regular-
ized logistic regression. This method has been used
in other text classification problems, as in Zhang
and Yang (2003). In our case, the regularization6
is introduced in order to enforce the beliefs that:
?1 ? ?2, if a1, a2 synonyms (30)
?1 ? ??2, if a1, a2 antonyms (31)
For further information on regularized model fitting,
see for instance, Hastie et al (2001).
5 Conclusion
In business settings, there is growing interest in
learning product reputations from the Internet. For
such problems, it is often difficult or expensive to
obtain labeled data. As a result, a change in mod-
eling strategies is needed, towards approaches that
require less supervision. In this paper we pro-
vide a framework for allowing human-provided in-
formation to be combined with unlabeled docu-
ments and labeled documents. We have found that
this framework enables improvements over existing
techniques, both in terms of the speed of model es-
timation and in classification accuracy. As a result,
we believe that this is a promising new approach to
problems of practical importance.
References
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews.
C. Fellbaum. 1998. Wordnet an electronic lexical
database.
T. Hastie, R. Tibshirani, and J. Friedman. 2001.
The Elements of Statistical Learning: Data Min-
ing, Inference, and Prediction. Springer-Verlag.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Philip R. Cohen and Wolfgang
Wahlster, editors, Proceedings of the Thirty-Fifth
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 174?181, Somerset,
New Jersey. Association for Computational Lin-
guistics.
6By cross-validation, we choose the regularization term ? =
1.5/sqrt(n), where n is the number of labeled documents.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web.
Ng, A. Y. and Jordan, M. 2002. On discriminative
vs. generative classifiers: A comparison of logis-
tic regression and naive bayes. Advances in Neu-
ral Information Processing Systems, 14.
Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text clas-
sification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103?134.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? senti-
ment classification using machine learning
techniques. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised
classification of reviews. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 417?
424, Philadelphia, Pennsylvania. Association for
Computational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proc. 17th National Con-
ference on Artificial Intelligence (AAAI-2000),
Austin, Texas.
Jian Zhang and Yiming Yang. 2003. ?robustness of
regularized linear classification methods in text
categorization?. In Proceedings of the 26th An-
nual International ACM SIGIR Conference (SI-
GIR 2003).
Thumbs up? Sentiment Classification using Machine Learning
Techniques
Bo Pang and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
{pabo,llee}@cs.cornell.edu
Shivakumar Vaithyanathan
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120 USA
shiv@almaden.ibm.com
Abstract
We consider the problem of classifying doc-
uments not by topic, but by overall senti-
ment, e.g., determining whether a review
is positive or negative. Using movie re-
views as data, we find that standard ma-
chine learning techniques definitively out-
perform human-produced baselines. How-
ever, the three machine learning methods
we employed (Naive Bayes, maximum en-
tropy classification, and support vector ma-
chines) do not perform as well on sentiment
classification as on traditional topic-based
categorization. We conclude by examining
factors that make the sentiment classifica-
tion problem more challenging.
1 Introduction
Today, very large amounts of information are avail-
able in on-line documents. As part of the effort to
better organize this information for users, researchers
have been actively investigating the problem of au-
tomatic text categorization.
The bulk of such work has focused on topical cat-
egorization, attempting to sort documents accord-
ing to their subject matter (e.g., sports vs. poli-
tics). However, recent years have seen rapid growth
in on-line discussion groups and review sites (e.g.,
the New York Times? Books web page) where a cru-
cial characteristic of the posted articles is their senti-
ment, or overall opinion towards the subject matter
? for example, whether a product review is pos-
itive or negative. Labeling these articles with their
sentiment would provide succinct summaries to read-
ers; indeed, these labels are part of the appeal and
value-add of such sites as www.rottentomatoes.com,
which both labels movie reviews that do not con-
tain explicit rating indicators and normalizes the
different rating schemes that individual reviewers
use. Sentiment classification would also be helpful in
business intelligence applications (e.g. MindfulEye?s
Lexant system1) and recommender systems (e.g.,
Terveen et al (1997), Tatemura (2000)), where user
input and feedback could be quickly summarized; in-
deed, in general, free-form survey responses given in
natural language format could be processed using
sentiment categorization. Moreover, there are also
potential applications to message filtering; for exam-
ple, one might be able to use sentiment information
to recognize and discard ?flames?(Spertus, 1997).
In this paper, we examine the effectiveness of ap-
plying machine learning techniques to the sentiment
classification problem. A challenging aspect of this
problem that seems to distinguish it from traditional
topic-based classification is that while topics are of-
ten identifiable by keywords alone, sentiment can be
expressed in a more subtle manner. For example, the
sentence ?How could anyone sit through this movie??
contains no single word that is obviously negative.
(See Section 7 for more examples). Thus, sentiment
seems to require more understanding than the usual
topic-based classification. So, apart from presenting
our results obtained via machine learning techniques,
we also analyze the problem to gain a better under-
standing of how difficult it is.
2 Previous Work
This section briefly surveys previous work on non-
topic-based text categorization.
One area of research concentrates on classifying
documents according to their source or source style,
with statistically-detected stylistic variation (Biber,
1988) serving as an important cue. Examples in-
clude author, publisher (e.g., the New York Times vs.
The Daily News), native-language background, and
?brow? (e.g., high-brow vs. ?popular?, or low-brow)
(Mosteller and Wallace, 1984; Argamon-Engelson et
1http://www.mindfuleye.com/about/lexant.htm
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.
                         Proceedings of the Conference on Empirical Methods in Natural
al., 1998; Tomokiyo and Jones, 2001; Kessler et al,
1997).
Another, more related area of research is that of
determining the genre of texts; subjective genres,
such as ?editorial?, are often one of the possible
categories (Karlgren and Cutting, 1994; Kessler et
al., 1997; Finn et al, 2002). Other work explicitly
attempts to find features indicating that subjective
language is being used (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). But, while techniques for
genre categorization and subjectivity detection can
help us recognize documents that express an opin-
ion, they do not address our specific classification
task of determining what that opinion actually is.
Most previous research on sentiment-based classi-
fication has been at least partially knowledge-based.
Some of this work focuses on classifying the semantic
orientation of individual words or phrases, using lin-
guistic heuristics or a pre-selected set of seed words
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2002). Past work on sentiment-based cat-
egorization of entire documents has often involved
either the use of models inspired by cognitive lin-
guistics (Hearst, 1992; Sack, 1994) or the manual or
semi-manual construction of discriminant-word lex-
icons (Huettner and Subasic, 2000; Das and Chen,
2001; Tong, 2001). Interestingly, our baseline exper-
iments, described in Section 4, show that humans
may not always have the best intuition for choosing
discriminating words.
Turney?s (2002) work on classification of reviews
is perhaps the closest to ours.2 He applied a spe-
cific unsupervised learning technique based on the
mutual information between document phrases and
the words ?excellent? and ?poor?, where the mu-
tual information is computed using statistics gath-
ered by a search engine. In contrast, we utilize sev-
eral completely prior-knowledge-free supervised ma-
chine learning methods, with the goal of understand-
ing the inherent difficulty of the task.
3 The Movie-Review Domain
For our experiments, we chose to work with movie
reviews. This domain is experimentally convenient
because there are large on-line collections of such re-
views, and because reviewers often summarize their
overall sentiment with a machine-extractable rat-
ing indicator, such as a number of stars; hence, we
did not need to hand-label the data for supervised
learning or evaluation purposes. We also note that
Turney (2002) found movie reviews to be the most
2Indeed, although our choice of title was completely
independent of his, our selections were eerily similar.
difficult of several domains for sentiment classifica-
tion, reporting an accuracy of 65.83% on a 120-
document set (random-choice performance: 50%).
But we stress that the machine learning methods and
features we use are not specific to movie reviews, and
should be easily applicable to other domains as long
as sufficient training data exists.
Our data source was the Internet Movie Database
(IMDb) archive of the rec.arts.movies.reviews
newsgroup.3 We selected only reviews where the au-
thor rating was expressed either with stars or some
numerical value (other conventions varied too widely
to allow for automatic processing). Ratings were
automatically extracted and converted into one of
three categories: positive, negative, or neutral. For
the work described in this paper, we concentrated
only on discriminating between positive and nega-
tive sentiment. To avoid domination of the corpus
by a small number of prolific reviewers, we imposed
a limit of fewer than 20 reviews per author per sen-
timent category, yielding a corpus of 752 negative
and 1301 positive reviews, with a total of 144 re-
viewers represented. This dataset will be available
on-line at http://www.cs.cornell.edu/people/pabo/-
movie-review-data/ (the URL contains hyphens only
around the word ?review?).
4 A Closer Look At the Problem
Intuitions seem to differ as to the difficulty of the sen-
timent detection problem. An expert on using ma-
chine learning for text categorization predicted rela-
tively low performance for automatic methods. On
the other hand, it seems that distinguishing positive
from negative reviews is relatively easy for humans,
especially in comparison to the standard text catego-
rization problem, where topics can be closely related.
One might also suspect that there are certain words
people tend to use to express strong sentiments, so
that it might suffice to simply produce a list of such
words by introspection and rely on them alone to
classify the texts.
To test this latter hypothesis, we asked two gradu-
ate students in computer science to (independently)
choose good indicator words for positive and nega-
tive sentiments in movie reviews. Their selections,
shown in Figure 1, seem intuitively plausible. We
then converted their responses into simple decision
procedures that essentially count the number of the
proposed positive and negative words in a given doc-
ument. We applied these procedures to uniformly-
distributed data, so that the random-choice baseline
result would be 50%. As shown in Figure 1, the
3http://reviews.imdb.com/Reviews/
Proposed word lists Accuracy Ties
Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic 58% 75%
negative: suck, terrible, awful, unwatchable, hideous
Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, 64% 39%
awesome, thrilling, badass, excellent, moving, exciting
negative: bad, cliched, sucks, boring, stupid, slow
Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.
Proposed word lists Accuracy Ties
Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful 69% 16%
negative: bad, worst, stupid, waste, boring, ?, !
Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).
accuracy ? percentage of documents classified cor-
rectly ? for the human-based classifiers were 58%
and 64%, respectively.4 Note that the tie rates ?
percentage of documents where the two sentiments
were rated equally likely ? are quite high5 (we chose
a tie breaking policy that maximized the accuracy of
the baselines).
While the tie rates suggest that the brevity of
the human-produced lists is a factor in the relatively
poor performance results, it is not the case that size
alone necessarily limits accuracy. Based on a very
preliminary examination of frequency counts in the
entire corpus (including test data) plus introspection,
we created a list of seven positive and seven negative
words (including punctuation), shown in Figure 2.
As that figure indicates, using these words raised the
accuracy to 69%. Also, although this third list is of
comparable length to the other two, it has a much
lower tie rate of 16%. We further observe that some
of the items in this third list, such as ??? or ?still?,
would probably not have been proposed as possible
candidates merely through introspection, although
upon reflection one sees their merit (the question
mark tends to occur in sentences like ?What was the
director thinking??; ?still? appears in sentences like
?Still, though, it was worth seeing?).
We conclude from these preliminary experiments
that it is worthwhile to explore corpus-based tech-
niques, rather than relying on prior intuitions, to se-
lect good indicator features and to perform sentiment
classification in general. These experiments also pro-
vide us with baselines for experimental comparison;
in particular, the third baseline of 69% might actu-
ally be considered somewhat difficult to beat, since
it was achieved by examination of the test data (al-
though our examination was rather cursory; we do
4Later experiments using these words as features for
machine learning methods did not yield better results.
5This is largely due to 0-0 ties.
not claim that our list was the optimal set of four-
teen words).
5 Machine Learning Methods
Our aim in this work was to examine whether it suf-
fices to treat sentiment classification simply as a spe-
cial case of topic-based categorization (with the two
?topics? being positive sentiment and negative sen-
timent), or whether special sentiment-categorization
methods need to be developed. We experimented
with three standard algorithms: Naive Bayes clas-
sification, maximum entropy classification, and sup-
port vector machines. The philosophies behind these
three algorithms are quite different, but each has
been shown to be effective in previous text catego-
rization studies.
To implement these machine learning algorithms
on our document data, we used the following stan-
dard bag-of-features framework. Let {f1, . . . , fm} be
a predefined set of m features that can appear in
a document; examples include the word ?still? or
the bigram ?really stinks?. Let ni(d) be the num-
ber of times fi occurs in document d. Then, each
document d is represented by the document vector
~d := (n1(d), n2(d), . . . , nm(d)).
5.1 Naive Bayes
One approach to text classification is to assign to a
given document d the class c? = argmaxc P (c | d).
We derive the Naive Bayes (NB) classifier by first
observing that by Bayes? rule,
P (c | d) = P (c)P (d | c)P (d) ,
where P (d) plays no role in selecting c?. To estimate
the term P (d | c), Naive Bayes decomposes it by as-
suming the fi?s are conditionally independent given
d?s class:
PNB(c | d) :=
P (c)
(
?m
i=1 P (fi | c)ni(d)
)
P (d) .
Our training method consists of relative-frequency
estimation of P (c) and P (fi | c), using add-one
smoothing.
Despite its simplicity and the fact that its con-
ditional independence assumption clearly does not
hold in real-world situations, Naive Bayes-based text
categorization still tends to perform surprisingly well
(Lewis, 1998); indeed, Domingos and Pazzani (1997)
show that Naive Bayes is optimal for certain problem
classes with highly dependent features. On the other
hand, more sophisticated algorithms might (and of-
ten do) yield better results; we examine two such
algorithms next.
5.2 Maximum Entropy
Maximum entropy classification (MaxEnt, or ME,
for short) is an alternative technique which has
proven effective in a number of natural lan-
guage processing applications (Berger et al, 1996).
Nigam et al (1999) show that it sometimes, but not
always, outperforms Naive Bayes at standard text
classification. Its estimate of P (c | d) takes the fol-
lowing exponential form:
PME(c | d) :=
1
Z(d) exp
(
?
i
?i,cFi,c(d, c)
)
,
where Z(d) is a normalization function. Fi,c is a fea-
ture/class function for feature fi and class c, defined
as follows:6
Fi,c(d, c?) :=
{ 1, ni(d) > 0 and c? = c
0 otherwise .
For instance, a particular feature/class function
might fire if and only if the bigram ?still hate? ap-
pears and the document?s sentiment is hypothesized
to be negative.7 Importantly, unlike Naive Bayes,
MaxEnt makes no assumptions about the relation-
ships between features, and so might potentially per-
form better when conditional independence assump-
tions are not met.
The ?i,c?s are feature-weight parameters; inspec-
tion of the definition of PME shows that a large ?i,c
means that fi is considered a strong indicator for
6We use a restricted definition of feature/class func-
tions so that MaxEnt relies on the same sort of feature
information as Naive Bayes.
7The dependence on class is necessary for parameter
induction. See Nigam et al (1999) for additional moti-
vation.
class c. The parameter values are set so as to max-
imize the entropy of the induced distribution (hence
the classifier?s name) subject to the constraint that
the expected values of the feature/class functions
with respect to the model are equal to their expected
values with respect to the training data: the under-
lying philosophy is that we should choose the model
making the fewest assumptions about the data while
still remaining consistent with it, which makes intu-
itive sense. We use ten iterations of the improved
iterative scaling algorithm (Della Pietra et al, 1997)
for parameter training (this was a sufficient num-
ber of iterations for convergence of training-data ac-
curacy), together with a Gaussian prior to prevent
overfitting (Chen and Rosenfeld, 2000).
5.3 Support Vector Machines
Support vector machines (SVMs) have been shown to
be highly effective at traditional text categorization,
generally outperforming Naive Bayes (Joachims,
1998). They are large-margin, rather than proba-
bilistic, classifiers, in contrast to Naive Bayes and
MaxEnt. In the two-category case, the basic idea be-
hind the training procedure is to find a hyperplane,
represented by vector ~w, that not only separates
the document vectors in one class from those in the
other, but for which the separation, or margin, is as
large as possible. This search corresponds to a con-
strained optimization problem; letting cj ? {1,?1}
(corresponding to positive and negative) be the cor-
rect class of document dj , the solution can be written
as
~w :=
?
j
?jcj ~dj , ?j ? 0,
where the ?j ?s are obtained by solving a dual opti-
mization problem. Those ~dj such that ?j is greater
than zero are called support vectors, since they are
the only document vectors contributing to ~w. Clas-
sification of test instances consists simply of deter-
mining which side of ~w?s hyperplane they fall on.
We used Joachim?s (1999) SVM light package8 for
training and testing, with all parameters set to their
default values, after first length-normalizing the doc-
ument vectors, as is standard (neglecting to normal-
ize generally hurt performance slightly).
6 Evaluation
6.1 Experimental Set-up
We used documents from the movie-review corpus
described in Section 3. To create a data set with uni-
form class distribution (studying the effect of skewed
8http://svmlight.joachims.org
Features # of frequency or NB ME SVM
features presence?
(1) unigrams 16165 freq. 78.7 N/A 72.8
(2) unigrams ? pres. 81.0 80.4 82.9
(3) unigrams+bigrams 32330 pres. 80.6 80.8 82.7
(4) bigrams 16165 pres. 77.3 77.4 77.1
(5) unigrams+POS 16695 pres. 81.5 80.4 81.9
(6) adjectives 2633 pres. 77.0 77.7 75.1
(7) top 2633 unigrams 2633 pres. 80.3 81.0 81.4
(8) unigrams+position 22430 pres. 81.0 80.1 81.6
Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given
setting (row). Recall that our baseline results ranged from 50% to 69%.
class distributions was out of the scope of this study),
we randomly selected 700 positive-sentiment and 700
negative-sentiment documents. We then divided this
data into three equal-sized folds, maintaining bal-
anced class distributions in each fold. (We did not
use a larger number of folds due to the slowness of
the MaxEnt training procedure.) All results reported
below, as well as the baseline results from Section 4,
are the average three-fold cross-validation results on
this data (of course, the baseline algorithms had no
parameters to tune).
To prepare the documents, we automatically re-
moved the rating indicators and extracted the tex-
tual information from the original HTML docu-
ment format, treating punctuation as separate lex-
ical items. No stemming or stoplists were used.
One unconventional step we took was to attempt
to model the potentially important contextual effect
of negation: clearly ?good? and ?not very good? in-
dicate opposite sentiment orientations. Adapting a
technique of Das and Chen (2001), we added the tag
NOT to every word between a negation word (?not?,
?isn?t?, ?didn?t?, etc.) and the first punctuation
mark following the negation word. (Preliminary ex-
periments indicate that removing the negation tag
had a negligible, but on average slightly harmful, ef-
fect on performance.)
For this study, we focused on features based on
unigrams (with negation tagging) and bigrams. Be-
cause training MaxEnt is expensive in the number of
features, we limited consideration to (1) the 16165
unigrams appearing at least four times in our 1400-
document corpus (lower count cutoffs did not yield
significantly different results), and (2) the 16165 bi-
grams occurring most often in the same data (the
selected bigrams all occurred at least seven times).
Note that we did not add negation tags to the bi-
grams, since we consider bigrams (and n-grams in
general) to be an orthogonal way to incorporate con-
text.
6.2 Results
Initial unigram results The classification accu-
racies resulting from using only unigrams as fea-
tures are shown in line (1) of Figure 3. As a whole,
the machine learning algorithms clearly surpass the
random-choice baseline of 50%. They also hand-
ily beat our two human-selected-unigram baselines
of 58% and 64%, and, furthermore, perform well in
comparison to the 69% baseline achieved via limited
access to the test-data statistics, although the im-
provement in the case of SVMs is not so large.
On the other hand, in topic-based classification,
all three classifiers have been reported to use bag-
of-unigram features to achieve accuracies of 90%
and above for particular categories (Joachims, 1998;
Nigam et al, 1999)9 ? and such results are for set-
tings with more than two classes. This provides
suggestive evidence that sentiment categorization is
more difficult than topic classification, which cor-
responds to the intuitions of the text categoriza-
tion expert mentioned above.10 Nonetheless, we still
wanted to investigate ways to improve our senti-
ment categorization results; these experiments are
reported below.
Feature frequency vs. presence Recall that we
represent each document d by a feature-count vector
(n1(d), . . . , nm(d)). However, the definition of the
9Joachims (1998) used stemming and stoplists; in
some of their experiments, Nigam et al (1999), like us,
did not.
10We could not perform the natural experiment of at-
tempting topic-based categorization on our data because
the only obvious topics would be the film being reviewed;
unfortunately, in our data, the maximum number of re-
views per movie is 27, too small for meaningful results.
MaxEnt feature/class functions Fi,c only reflects the
presence or absence of a feature, rather than directly
incorporating feature frequency. In order to investi-
gate whether reliance on frequency information could
account for the higher accuracies of Naive Bayes and
SVMs, we binarized the document vectors, setting
ni(d) to 1 if and only feature fi appears in d, and
reran Naive Bayes and SVM light on these new vec-
tors.11
As can be seen from line (2) of Figure 3,
better performance (much better performance for
SVMs) is achieved by accounting only for fea-
ture presence, not feature frequency. Interestingly,
this is in direct opposition to the observations of
McCallum and Nigam (1998) with respect to Naive
Bayes topic classification. We speculate that this in-
dicates a difference between sentiment and topic cat-
egorization ? perhaps due to topic being conveyed
mostly by particular content words that tend to be
repeated ? but this remains to be verified. In any
event, as a result of this finding, we did not incor-
porate frequency information into Naive Bayes and
SVMs in any of the following experiments.
Bigrams In addition to looking specifically for
negation words in the context of a word, we also
studied the use of bigrams to capture more context
in general. Note that bigrams and unigrams are
surely not conditionally independent, meaning that
the feature set they comprise violates Naive Bayes?
conditional-independence assumptions; on the other
hand, recall that this does not imply that Naive
Bayes will necessarily do poorly (Domingos and Paz-
zani, 1997).
Line (3) of the results table shows that bigram
information does not improve performance beyond
that of unigram presence, although adding in the bi-
grams does not seriously impact the results, even for
Naive Bayes. This would not rule out the possibility
that bigram presence is as equally useful a feature
as unigram presence; in fact, Pedersen (2001) found
that bigrams alone can be effective features for word
sense disambiguation. However, comparing line (4)
to line (2) shows that relying just on bigrams causes
accuracy to decline by as much as 5.8 percentage
points. Hence, if context is in fact important, as our
intuitions suggest, bigrams are not effective at cap-
turing it in our setting.
11Alternatively, we could have tried integrating fre-
quency information into MaxEnt. However, feature/class
functions are traditionally defined as binary (Berger et
al., 1996); hence, explicitly incorporating frequencies
would require different functions for each count (or count
bin), making training impractical. But cf. (Nigam et al,
1999).
Parts of speech We also experimented with ap-
pending POS tags to every word via Oliver Mason?s
Qtag program.12 This serves as a crude form of word
sense disambiguation (Wilks and Stevenson, 1998):
for example, it would distinguish the different usages
of ?love? in ?I love this movie? (indicating sentiment
orientation) versus ?This is a love story? (neutral
with respect to sentiment). However, the effect of
this information seems to be a wash: as depicted in
line (5) of Figure 3, the accuracy improves slightly
for Naive Bayes but declines for SVMs, and the per-
formance of MaxEnt is unchanged.
Since adjectives have been a focus of previous work
in sentiment detection (Hatzivassiloglou and Wiebe,
2000; Turney, 2002)13, we looked at the performance
of using adjectives alone. Intuitively, we might ex-
pect that adjectives carry a great deal of informa-
tion regarding a document?s sentiment; indeed, the
human-produced lists from Section 4 contain almost
no other parts of speech. Yet, the results, shown in
line (6) of Figure 3, are relatively poor: the 2633
adjectives provide less useful information than uni-
gram presence. Indeed, line (7) shows that simply
using the 2633 most frequent unigrams is a better
choice, yielding performance comparable to that of
using (the presence of) all 16165 (line (2)). This may
imply that applying explicit feature-selection algo-
rithms on unigrams could improve performance.
Position An additional intuition we had was that
the position of a word in the text might make a dif-
ference: movie reviews, in particular, might begin
with an overall sentiment statement, proceed with
a plot discussion, and conclude by summarizing the
author?s views. As a rough approximation to deter-
mining this kind of structure, we tagged each word
according to whether it appeared in the first quar-
ter, last quarter, or middle half of the document14.
The results (line (8)) didn?t differ greatly from using
unigrams alone, but more refined notions of position
might be more successful.
7 Discussion
The results produced via machine learning tech-
niques are quite good in comparison to the human-
generated baselines discussed in Section 4. In terms
of relative performance, Naive Bayes tends to do the
worst and SVMs tend to do the best, although the
12http://www.english.bham.ac.uk/staff/oliver/soft-
ware/tagger/index.htm
13Turney?s (2002) unsupervised algorithm uses bi-
grams containing an adjective or an adverb.
14We tried a few other settings, e.g., first third vs. last
third vs middle third, and found them to be less effective.
differences aren?t very large.
On the other hand, we were not able to achieve ac-
curacies on the sentiment classification problem com-
parable to those reported for standard topic-based
categorization, despite the several different types of
features we tried. Unigram presence information
turned out to be the most effective; in fact, none of
the alternative features we employed provided consis-
tently better performance once unigram presence was
incorporated. Interestingly, though, the superiority
of presence information in comparison to frequency
information in our setting contradicts previous obser-
vations made in topic-classification work (McCallum
and Nigam, 1998).
What accounts for these two differences ? dif-
ficulty and types of information proving useful ?
between topic and sentiment classification, and how
might we improve the latter? To answer these ques-
tions, we examined the data further. (All examples
below are drawn from the full 2053-document cor-
pus.)
As it turns out, a common phenomenon in the doc-
uments was a kind of ?thwarted expectations? narra-
tive, where the author sets up a deliberate contrast
to earlier discussion: for example, ?This film should
be brilliant. It sounds like a great plot, the actors are
first grade, and the supporting cast is good as well, and
Stallone is attempting to deliver a good performance.
However, it can?t hold up? or ?I hate the Spice Girls.
...[3 things the author hates about them]... Why I saw
this movie is a really, really, really long story, but I
did, and one would think I?d despise every minute of
it. But... Okay, I?m really ashamed of it, but I enjoyed
it. I mean, I admit it?s a really awful movie ...the ninth
floor of hell...The plot is such a mess that it?s terrible.
But I loved it.? 15
In these examples, a human would easily detect
the true sentiment of the review, but bag-of-features
classifiers would presumably find these instances dif-
ficult, since there are many words indicative of the
opposite sentiment to that of the entire review. Fun-
damentally, it seems that some form of discourse
analysis is necessary (using more sophisticated tech-
15This phenomenon is related to another common
theme, that of ?a good actor trapped in a bad movie?:
?AN AMERICAN WEREWOLF IN PARIS is a failed at-
tempt... Julie Delpy is far too good for this movie. She im-
bues Serafine with spirit, spunk, and humanity. This isn?t
necessarily a good thing, since it prevents us from relax-
ing and enjoying AN AMERICAN WEREWOLF IN PARIS
as a completely mindless, campy entertainment experience.
Delpy?s injection of class into an otherwise classless produc-
tion raises the specter of what this film could have been
with a better script and a better cast ... She was radiant,
charismatic, and effective ....?
niques than our positional feature mentioned above),
or at least some way of determining the focus of each
sentence, so that one can decide when the author is
talking about the film itself. (Turney (2002) makes
a similar point, noting that for reviews, ?the whole
is not necessarily the sum of the parts?.) Further-
more, it seems likely that this thwarted-expectations
rhetorical device will appear in many types of texts
(e.g., editorials) devoted to expressing an overall
opinion about some topic. Hence, we believe that an
important next step is the identification of features
indicating whether sentences are on-topic (which is
a kind of co-reference problem); we look forward to
addressing this challenge in future work.
Acknowledgments
We thank Joshua Goodman, Thorsten Joachims, Jon
Kleinberg, Vikas Krishna, John Lafferty, Jussi Myl-
lymaki, Phoebe Sengers, Richard Tong, Peter Tur-
ney, and the anonymous reviewers for many valuable
comments and helpful suggestions, and Hubie Chen
and Tony Faradjian for participating in our baseline
experiments. Portions of this work were done while
the first author was visiting IBM Almaden. This pa-
per is based upon work supported in part by the Na-
tional Science Foundation under ITR/IM grant IIS-
0081334. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Shlomo Argamon-Engelson, Moshe Koppel, and
Galit Avneri. 1998. Style-based text categoriza-
tion: What newspaper am I reading? In Proc. of
the AAAI Workshop on Text Categorization, pages
1?4.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for ME models. IEEE
Trans. Speech and Audio Processing, 8(1):37?50.
Sanjiv Das and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Proc. of the 8th Asia Pacific
Finance Association Annual Conference (APFA
2001).
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380?393.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple Bayesian classifier un-
der zero-one loss. Machine Learning, 29(2-3):103?
130.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer
for information filtering. In Proc. of the Eu-
ropean Colloquium on Information Retrieval Re-
search, pages 353?362, Glasgow.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th ACL/8th EACL, pages
174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proc. of COLING.
Marti Hearst. 1992. Direction-based text interpre-
tation as an information access refinement. In
Paul Jacobs, editor, Text-Based Intelligent Sys-
tems. Lawrence Erlbaum Associates.
Alison Huettner and Pero Subasic. 2000. Fuzzy
typing for document management. In ACL
2000 Companion Volume: Tutorial Abstracts and
Demonstration Notes, pages 26?27.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Proc. of the European Confer-
ence on Machine Learning (ECML), pages 137?
142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf and
Alexander Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 44?56.
MIT Press.
Jussi Karlgren and Douglass Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proc. of COLING.
Brett Kessler, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proc. of the 35th ACL/8th EACL, pages 32?38.
David D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information retrieval.
In Proc. of the European Conference on Machine
Learning (ECML), pages 4?15. Invited talk.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proc. of the AAAI-98 Workshop on
Learning for Text Categorization, pages 41?48.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Kamal Nigam, John Lafferty, and Andrew McCal-
lum. 1999. Using maximum entropy for text clas-
sification. In Proc. of the IJCAI-99 Workshop on
Machine Learning for Information Filtering, pages
61?67.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proc. of the
Second NAACL, pages 79?86.
Warren Sack. 1994. On the computation of point of
view. In Proc. of the Twelfth AAAI, page 1488.
Student abstract.
Ellen Spertus. 1997. Smokey: Automatic recog-
nition of hostile messages. In Proc. of Innova-
tive Applications of Artificial Intelligence (IAAI),
pages 1058?1065.
Junichi Tatemura. 2000. Virtual reviewers for col-
laborative exploration of movie reviews. In Proc.
of the 5th International Conference on Intelligent
User Interfaces, pages 272?275.
Loren Terveen, Will Hill, Brian Amento, David Mc-
Donald, and Josh Creter. 1997. PHOAKS: A sys-
tem for sharing recommendations. Communica-
tions of the ACM, 40(3):59?62.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? Naive Bayes
detection of non-native utterance text. In Proc. of
the Second NAACL, pages 239?246.
Richard M. Tong. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. Workshop note, SIGIR 2001 Workshop on
Operational Text Classification.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
EGB-1094, National Research Council Canada.
Peter Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL.
Janyce M. Wiebe, Theresa Wilson, and Matthew
Bell. 2001. Identifying collocations for recognizing
opinions. In Proc. of the ACL/EACL Workshop
on Collocation.
Yorick Wilks and Mark Stevenson. 1998. The gram-
mar of sense: Using part-of-speech tags as a first
step in semantic disambiguation. Journal of Nat-
ural Language Engineering, 4(2):135?144.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002?1012,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Domain Adaptation of Rule-Based Annotators
for Named-Entity Recognition Tasks
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
650 Harry Road, San Jose, CA 95120, USA
{chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com
Abstract
Named-entity recognition (NER) is an impor-
tant task required in a wide variety of ap-
plications. While rule-based systems are ap-
pealing due to their well-known ?explainabil-
ity,? most, if not all, state-of-the-art results
for NER tasks are based on machine learning
techniques. Motivated by these results, we ex-
plore the following natural question in this pa-
per: Are rule-based systems still a viable ap-
proach to named-entity recognition? Specif-
ically, we have designed and implemented
a high-level language NERL on top of Sys-
temT, a general-purpose algebraic informa-
tion extraction system. NERL is tuned to the
needs of NER tasks and simplifies the pro-
cess of building, understanding, and customiz-
ing complex rule-based named-entity annota-
tors. We show that these customized annota-
tors match or outperform the best published
results achieved with machine learning tech-
niques. These results confirm that we can
reap the benefits of rule-based extractors? ex-
plainability without sacrificing accuracy. We
conclude by discussing lessons learned while
building and customizing complex rule-based
annotators and outlining several research di-
rections towards facilitating rule development.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
While NER over formal text such as news articles
and webpages is a well-studied problem (Bikel et
al., 1999; McCallum and Li, 2003; Etzioni et al,
2005), there has been recent work on NER over in-
formal text such as emails and blogs (Huang et al,
2001; Poibeau and Kosseim, 2001; Jansche and Ab-
ney, 2002; Minkov et al, 2005; Gruhl et al, 2009).
The techniques proposed in the literature fall under
three categories: rule-based (Krupka and Hausman,
2001; Sekine and Nobata, 2004), machine learning-
based (O. Bender and Ney, 2003; Florian et al,
2003; McCallum and Li, 2003; Finkel and Manning,
2009; Singh et al, 2010) and hybrid solutions (Sri-
hari et al, 2001; Jansche and Abney, 2002).
1.1 Motivation
Although there are well-established rule-based sys-
tems to perform NER tasks, most, if not all, state-of-
the-art results for NER tasks are based on machine
learning techniques. However, the rule-based ap-
proach is still extremely appealing due to the associ-
ated transparency of the internal system state, which
leads to better explainability of errors (Siniakov,
2010). Ideally, one would like to benefit from the
transparency and explainability of rule-based tech-
niques, while achieving state-of-the-art accuracy.
A particularly challenging aspect of rule-based
NER in practice is domain customization ? cus-
tomizing existing annotators to produce accurate re-
sults in new domains. In machine learning-based
systems, adapting to a new domain has tradition-
ally involved acquiring additional labeled data and
learning a new model from scratch. However, recent
work has proposed more sophisticated approaches
that learn a domain-independent base model, which
can later be adapted to specific domains (Florian et
1002
BASEBALL - MAJOR LEAGUE STANDINGS AFTER TUESDAY 'S GAMES NEW YORK 1996-08-28?
?AMERICAN LEAGUE EASTERN DIVISION W L PCT GB NEW YORK 74 57 .565 -BALTIMORE 70 61 .534 4 BOSTON 68 65 .511 7 
?TEXAS AT KANSAS CITYBOSTON AT CALIFORNIANEW YORK AT SEATTLE
?
BASEBALL - ORIOLES WIN , YANKEESLOSE . BALTIMORE 1996-08-27
?In Seattle , Jay Buhner 's eighth-inning single snapped a tie as the Seattle Mariners edged the New York Yankees 2-1 in the opener of a three-game series .New York starter Jimmy Key left the game in the first inning after Seattle shortstop Alex Rodriguez lined a shot off his left elbow .
?
Document d1 Document d2
Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself.Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location
Organization Location
Figure 1: Example Customization Requirement
al., 2004; Blitzer et al, 2006; Jiang and Zhai, 2006;
Arnold et al, 2008; Wu et al, 2009). Implement-
ing a similar approach for rule-based NER typically
requires a significant amount of manual effort to (a)
identify the explicit semantic changes required for
the new domain (e.g., differences in entity type def-
inition), (b) identify the portions of the (complex)
core annotator that should be modified for each dif-
ference and (c) implement the required customiza-
tion rules without compromising the extraction qual-
ity of the core annotator. Domain customization of
rule-based NER has not received much attention in
the recent literature with a few exceptions (Petasis et
al., 2001; Maynard et al, 2003; Zhu et al, 2005).
1.2 Problem Statement
In this paper, we explore the following natural ques-
tion: Are rule-based systems still a viable approach
to named-entity recognition? Specifically, (a) Is it
possible to build, maintain and customize rule-based
NER annotators that match the state-of-the-art re-
sults obtained using machine-learning techniques?
and (b) Can this be achieved with a reasonable
amount of manual effort?
1.3 Contributions
In this paper, we address the challenges mentioned
above by (i) defining a taxonomy of the different
types of customizations that a rule developer may
perform when adapting to a new domain (Sec. 2), (ii)
identifying a set of high-level operations required
for building and customizing NER annotators, and
(iii) exposing these operations in a domain-specific
NER rule language, NERL, developed on top of Sys-
// Core rules identify Organization and Location candidates
// Begin customization// Identify articles covering sports event from article title CR1 <SportsArticle>  Evaluate Regular Expressions <R1>// Identify locations in sports articlesCR2 Retain <Location> As <LocationMaybeOrg> If ContainedWithin <SportsArticle>// City/County/State references (e.g., New York) may refer to the sports team in that cityCR3 Retain <LocationMaybeOrg> If Matches Dictionaries<?cities.dict?,?counties.dict?,?states.dict?>
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2>on Left Context 2 Tokens
// City references to sports teams are added to Organization and removed from LocationCR5 Augment <Organization> With <LocationMaybeOrg>// End customization
// Continuation of core rules// Remove Locations that overlap with OrganizationsDiscard <Location> If Overlaps Concepts <Organization>Figure 2: Example Customization Rules in NERL
temT (Chiticariu et al, 2010), a general-purpose
algebraic information extraction system (Sec. 3).
NERL is specifically geared towards building and
customizing complex NER annotators and makes it
easy to understand a complex annotator that may
comprise hundreds of rules. It simplifies the iden-
tification of what portions need to be modified for
a given customization requirement. It also makes
individual customizations easier to implement, as il-
lustrated by the following example.
Suppose we have to customize a domain-
independent rule-based NER annotator for the
CoNLL corpus (Tjong et al, 2003). Consider the
two sports-related news articles in Fig. 1 from the
corpus, where city names such as ?New York? or
?Seattle? can refer to either a Location or an Orga-
nization (the sports team based in that city). In the
domain-independent annotator, city names were al-
ways identified as Location, as this subtle require-
ment was not considered during rule development.
A customization to address this issue is shown in
Fig. 1, which can be implemented in NERL with five
rules (Fig. 2). This customization (explained in de-
tail in Sec. 3) improved the F?=1 score for Organi-
zation and Location by approximately 9% and 3%,
respectively (Sec. 4).
We used NERL to customize a domain-
independent rule-based NER annotator for three
different domains ? CoNLL03 (Tjong et al, 2003),
Enron (Minkov et al, 2005) and ACE05 (NIST,
2005). Our experimental results (Sec. 4.3) demon-
strate that the customized annotators have extraction
quality better than the best-known results for
1003
Affects Single Affects Multiple
Entity Type Entity Types
Identify New Instances CS , CDD , CDSD BR
Modify Existing instances CEB , CDD CATA, CG
Table 1: Categorizing NER Customizations
individual domains, which were achieved with
machine learning techniques. The fact that we are
able to achieve such results across multiple domains
answers our earlier question and confirms that
we can reap the benefits of rule-based extractors?
explainability without sacrificing accuracy.
However, we found that even using NERL, the
amount of manual effort and expertise required in
rule-based NER may still be significant. In Sec. 5,
we report on the lessons learned and outline several
interesting research directions towards simplifying
rule development and facilitating the adoption of the
rule-based approach towards NER.
2 Domain Customization for NER
We consider NER tasks following the broad defini-
tion put forth by (Nadeau and Sekine, 2007), for-
mally defined as follows:
Definition 1 Named entity recognition is the task of
identifying and classifying mentions of entities with
one or more rigid designators, as defined by (Kripke,
1982).
For instance, the identification of proper nouns
representing persons, organizations, locations, prod-
uct names, proteins, drugs and chemicals are consid-
ered as NER tasks.
Based on our experience of customizing NER an-
notators for multiple domains, we categorize the
customizations involved into two main categories as
listed below. This categorization motivates the de-
sign of NERL (Sec. 3).
Data-driven (CDD): The most common NER cus-
tomization is data-driven, where the customizations
mostly involve the addition of new patterns and
dictionary entries, driven by observations from the
training data in the new domain. An example is
the addition of a new rule to identify locations from
the beginning of news articles (e.g., ?BALTIMORE
1995-08-27? and ?MURCIA , Spain 1996-09-10?).
Application-driven: What is considered a valid
named entity and its corresponding type can vary
across application domains. The most common di-
mensions on which the definition of a named entity
can vary are:
Entity Boundary (CEB): Different application do-
mains may have different definitions of where the
same entity starts or ends. For example, a Person
may (CoNLL03) or may not (Enron) include gener-
ational markers (e.g. ?Jr.? in ?Bush Jr.? or ?IV? in
?Henry IV?).
Ambiguous Type Assignment (CATA): The exact
type of a given named entity can be ambiguous.
Different applications may assign different types
for the same named entity. For instance, all in-
stances of ?White House? may be considered as Lo-
cation (CoNLL03), or be assigned as Facility or Or-
ganization based on their context (ACE05). In fact,
even within the same application domain, entities
typically considered as of the same type may be as-
signed differently. For example, given ?New York
beat Seattle? and ?Ethiopia beat Uganda?, both
?New York? and ?Ethiopia? are teams referred by their
locations. However, (Tjong et al, 2003) considers
the former, which corresponds to a city, as an Orga-
nization, and the latter, which corresponds to a coun-
try, as a Location.
Domain-Specific Definition (CDSD): Whether a
given term is even considered a named entity may
depend on the specific domain. As an example, con-
sider the text ?Commercialization Meeting - SBeck,
BHall, BSuperty, TBusby, SGandhi-Gupta?. Infor-
mal names such as ?SBeck? and ?BHall? may be con-
sidered as valid person names (Enron).
Scope(CS): Each type of named entity usually con-
tains several subtypes. For the same named en-
tity task, different applications may choose to in-
clude different sets of subtypes. For instance,
roads and buildings are considered part of Location
in CoNLL03, while they are not included in ACE05.
Granularity(CG): Name entity types are hierarchi-
cal. Different applications may define NER tasks
at different granularities. For instance, in ACE05,
Organization and Location entity types were split
into four entity types (Organization, Location, Geo-
Political Entity and Facility).
The different customizations are summarized as
shown in Tab. 1, based on the following criteria: (i)
whether the customization identifies new instances
or modifies existing instances; and (ii) whether the
1004
customization affects single or multiple entities. For
instance, CS identifies new instances for a single en-
tity type, as it adds instances of a new subtype for an
existing entity type. Note that BR in the table de-
notes the rules used to build the core annotator.
3 Named Entity Rule Language
3.1 Grammar vs. Algebraic NER
Traditionally, rule-based NER systems were based
on the popular CPSL cascading grammar specifi-
cation (Appelt and Onyshkevych, 1998). CPSL is
designed so that rules that adhere to the standard
can be executed efficiently with finite state transduc-
ers. Accordingly, the standard defines a rigid left-to-
right execution model where a region of text can be
matched by at most one rule according to a fixed rule
priority, and where overlapping annotations are dis-
allowed in the output of each grammar phase.
While it simplifies the design of CPSL engines,
the rigidity of the rule matching semantics makes
it difficult to express operations frequently used in
rule-based information extraction. These limitations
have been recognized in the literature, and several
extensions have been proposed to allow more flex-
ible matching semantics, and to allow overlapping
annotations (Cunningham et al, 2000; Boguraev,
2003; Drozdzynski et al, 2004). However, even
with these extensions, common operations such as
filtering annotations (e.g. CR4 in Fig. 2), are dif-
ficult to express in grammars and often require an
escape to custom procedural code.
Recently, several declarative algebraic languages
have been proposed for rule-based IE systems, no-
tably AQL (Chiticariu et al, 2010) and Xlog (Shen
et al, 2007). These languages are not constrained
by the requirement that all rules map onto finite state
transducers, and therefore can express a significantly
richer semantics than grammar-based languages. In
particular, the AQL rule language as implemented in
SystemT (Chiticariu et al, 2010) can express many
common operations used in rule-based information
extraction without requiring custom code. In addi-
tion, the separation of extraction semantics from ex-
ecution enables SystemT?s rule optimizer and effi-
cient runtime engine. Indeed, as shown in (Chiti-
cariu et al, 2010), SystemT can deliver an order of
magitude higher annotation throughput compared to
a state-of-the-art CPSL-based IE system.
Since AQL is a general purpose information ex-
traction rule language, similar to CPSL and JAPE,
it exposes an expressive set of capabilities that go
beyond what is required for NER tasks. These ad-
ditional capabilities can make AQL rules more ver-
bose than is necessary for implementing rules in the
NER domain. For example, Fig. 3 shows how the
same customization rule CR4 from Fig. 2 can be
implemented in JAPE or in AQL. Notice how im-
plementing even a single customization may lead to
defining complex rules (e.g. JAPE-R1, AQL-R1)
and sometimes even using custom code (e.g. JAPE-
R2). As illustrated by this example, the rules in AQL
and JAPE tend to be complex since some operations
? e.g., filtering the outputs of one rule based on the
outputs of another rule ? that are common in NER
rule sets require multiple rules in AQL or multiple
grammar phases in JAPE.
To make NER rules easier to develop and to
understand, we designed and implemented Named
Entity Rule Language (NERL) on top of SystemT.
NERL is a declarative rule language designed specif-
ically for named entity recognition. The design of
NERL draws on our experience with building and
customizing multiple complex NER annotators. In
particular, we have identified the operations required
in practice for such tasks, and expose these opera-
tions as built-in constructs in NERL. In doing so, we
ensure that frequently performed operations can be
expressed succinctly, so as not to complicate the rule
set unnecessarily. As a result, NERL rules for named
entity recognition tasks are significantly more com-
pact and easy to understand than the equivalent AQL
rules. At the same time, NERL rules can easily be
compiled to AQL, allowing our NER rule develop-
ment framework to take advantage of the capabilities
of the SystemT rule optimizer and efficient runtime
execution engine.
3.2 NERL
For the rest of this section, we focus on describ-
ing the types of rules supported in NERL. In Sec. 4,
we shall demonstrate empirically that NERL can be
successfully employed in building and customizing
complex NER annotators.
A NERL rule has the following form:
IntConcept ? RuleBody(IntConcept1, IntConcept2, . . .)
1005
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2> on Left Context 2 Tokens
Rule in NERL
JAPE Phase 1Rule : AmbiguousLocationContext
({Token}[2]):context({AmbiguousLoc}): annot
  :annot.AmbiguousLoc = {lc = context.string}
JAPE Phase 2Rule : RetainValidLocation
({AmbiguousLoc.lc =~ R2}):ambiguousloc -->{  // rule to discard ambiguous locationsAnnotationSet loc = bindings.get(?ambiguousloc");
outputAS.removeAll(loc); }
Rule : RetainValidLocation({Token}[2]):context({AmbiguousLoc}):loc    -->{   // Action part in Java to test R2 on left context // and delete annotationAnnotationSet loc = bindings.get(?loc");AnnotationSet context = bindings.get(?context");int begOffset = context.firstNode().getOffset().intValue(); int endOffset = context.lastNode().getOffset().intValue(); String mydocContent = doc.getContent().toString(); String contextString =
mydocContent.substring(begOffset, endOffset);if (Pattern.matches(?R2?, contextString)) {
outputAS.removeAll(loc); }}
create view LocationMaybeOrgInvalid as
select LMO.value as valuefrom LocationMaybeOrg LMO 
where MatchesRegex(/R2/,LeftContextTok(LMO.value,2));
create view LocationMaybeOrgValid as(select LMO.value as value from LocationMaybeOrg LMO)
minus(select LMOI.value as value from LocationMaybeOrgInvalid LMOI);
Two Alternative Rule sets in JAPE Equivalent Rule set in AQL
JAPE-R1 JAPE-R2 AQL-R1
Figure 3: Single Customization Rule expressed in NERL, JAPE and AQL
Intuitively, a NERL rule creates an intermediate con-
cept or named entity (IntConcept for short) by ap-
plying a NERL rule on the input text and zero or
more previously defined intermediate concepts.
NERL Rule Types The types of rules supported in
NERL are summarized in Tab. 2. In what follows,
we illustrate these types by means of examples.
Feature definition (FD): FD rules identify basic
features from text (e.g., FirstName, LastName and
CapsWord features for identifying person names).
Candidate definition (CD): CD rules identify com-
plete occurrences of the target entity. For instance,
the Sequence rule ?LastName followed by ?,? fol-
lowed by FirstName? identifies person annotations
as a sequence of three tokens, where the first and
third tokens occur in dictionaries containing last and
first names.
Candidate Refinement (CR): CR rules are used to
refine candidates generated for different annotation
types. E.g., the Filter rule CR3 in Fig. 2 retains Loca-
tionMaybeOrg annotations that appear in one of sev-
eral dictionaries.
Consolidation (CO): CO rules are used to resolve
overlapping candidates generated by multiple CD
rules. For instance, consider the text ?Please see
the following request from Dr. Kenneth Lim of the
BAAQMD.?. A CD rule may identify ?Dr. Kenneth
Lim? as a person, while another CD rule may identify
?Kenneth Lim? as a candidate person. A consolidation
rule is then used to merge these two annotations to
produce a single annotation for ?Dr. Kenneth Lim?.
NERL Examples Within these categories, three
types of rules deserve special attention, as they cor-
respond to frequently used operations and are specif-
ically designed to ensure compactness of the rule-
set. In contrast, as discussed earlier (Fig. 3), each of
these operations require several rules and possibly
custom code in existing rule-based IE systems.
DynamicDict: The DynamicDict rule is used to create
customized gazetteers on the fly. The following ex-
ample shows the need for such a rule: While ?Clin-
ton? does not always refer to a person?s last name
(Clinton is the name of several cities in USA), in
documents containing a full person name with ?Clin-
ton? as a last name (e.g., ?Hillary Clinton?) it is rea-
sonable to annotate all references to the (possibly)
ambiguous word ?Clinton? as a person. This goal
can be accomplished using the rule <Create Dynamic
Dictionary using Person with length 1 to 2 tokens>,
which creates a gazetteer on a per-document basis.
Filter: The Filter rule is used to discard/retain cer-
tain intermediate annotations based on predicates on
the annotation text and its local context. Example
filtering predicates include
? Discard C If Matches Regular Expression R
? Retain C If Contains Dictionary D on Local Context LC
? Discard C If Overlaps Concepts C1, C2, . . .
ModifySpan: The ModifySpan rule is used to expand
or trim the span of a candidate annotation. For
instance, an Entity Boundary customization to in-
clude generational markers as part of a Person anno-
tation can be implemented using a ModifySpan rule
<Expand Person Using Dictionary ?generation.dict? on
RightContext 2 Tokens>.
Using NERL Tab. 2 shows how different types of
rules are used during rule building and customiza-
tions. Since BR and CS involve identifying one
1006
Rule Category Syntax BR CDD CG
CS CDSD CEB CATA
Dictionary FD Evaluate Dictionaries < D1, D2, . . . > with flags? X X
Regex FD Evaluate Regular Expressions < R1, R2, . . . > with flags? X X
PoS FD Evaluate Part of Speech < P1, P2, . . . > with language < L >? X X
DynamicDict FD Create Dynamic Dictionary using IntConcept with flags? X X
Sequence CD IntConceptorString multiplicity?
(followed by IntConceptorString multiplicity?)+ X X
Filter CR Discard/Retain IntConcept(As IntConcept)?
If SatisfiesPredicate on LocalContext X X X
ModifySpan CR Trim/Expand IntConcept Using Dictionary < D >
on LocalContext X X
Augment CO Augment IntConcept With IntConcept X X
Consolidate CO Consolidate IntConcept using ConsolidationPolicy X X
Table 2: Description of rules supported in NERL
or more entity (sub)types from scratch, all types
of rules are used. CDD and CDSD identify addi-
tional instances for an existing type and therefore
mainly rely on FD and CD rules. On the other hand,
the customizations that modify existing instances
(CEB ,CATA,CG) require CR and CO rules.
Revisiting the example in Fig. 2, CR rules were
used to implement a fairly sophisticated customiza-
tion in a compact fashion, as follows. Rule CR1
first identifies sports articles using a regular expres-
sion based on the article title. Rule CR2 marks
Locations within these articles as LocationMaybeOrg
and Rule CR3 only retains those occurrences that
match a city, county or state name (e.g., ?Seattle?).
Rule CR4 identifies occurrences that have a contex-
tual clue confirming that the mention was to a lo-
cation (e.g., ?In? or ?At?). These occurrences are al-
ready classified correctly as Location and do not need
to be changed. Finally, CR5 adds the remaining am-
biguous mentions to Organization, which would be
deleted from Location by a subsequent core rule.
4 Development and Customization of NER
extractors with NERL
Using NERL, we have developed CoreNER, a
domain-independent generic library for multiple
NER extraction tasks commonly encountered in
practice, including Person, Organization, Location,
EmailAddress, PhoneNumber, URL, and DateTime, but
we shall focus the discussion on the first three tasks
(see Tab. 3 for entity definitions), since they are the
most challenging. In this section, we first overview
the process of developing CoreNER (Sec. 4.1). We
then describe how we have customized CoreNER
for three different domains (Sec. 4.2), and present
a quality comparison with best published results ob-
tained with state-of-the-art machine learning tech-
niques (Sec. 4.3). The tasks we consider are not re-
stricted to documents in a particular language, but
due to limited availability of non-English corpora
and extractors for comparison, our evaluation uses
English-language text. In Sec. 5 we shall elaborate
on the difficulties encountered while building and
customizing CoreNER using NERL and the lessons
we learned in the process.
4.1 Developing CoreNER
We have built our domain independent CoreNER li-
brary using a variety of formal and informal text
(e.g. web pages, emails, blogs, etc.), and informa-
tion from public data sources such as the US Census
Bureau (Census, 2007) and Wikipedia.
The development process proceeded as follows.
We first collected dictionaries for each entity
type from different resources, followed by man-
ual cleanup when needed to categorize entries col-
lected into ?strong? and ?weak? dictionaries. For
instance, we used US Census data to create several
name dictionaries, placing ambiguous entries such
as ?White? and ?Price? in a dictionary of ambigu-
ous last names, while unambiguous entries such as
?Johnson? and ?Williams? went to the dictionary for
strict last names. Second, we developed FD and
CD rules to identify candidate entities based on the
way named entities generally occur in text. E.g.,
<Salutation CapsWord CapsWord> and <FirstName
1007
Type Subtypes
PER individual
LOC
Address, Boundary, Land-Region-Natural, Region-General,
Region-International, Airport, Buildings-Grounds, Path, Plant,
Subarea-Facility, Continent, Country-or-District, Nation,
Population-Center, State-or-Province
ORG Commercial, Educational, Government, Media, Medical-ScienceNon-Governmental
Table 3: NER Task Types and Subtypes
LastName> for Person, and <CapsWord{1,3} OrgSuf-
fix> and <CapsWord{1,2} Industry> for Organization.
We then added CR and CO rules to account for
contextual clues and overlapping annotations (e.g.,
Delete Person annotations appearing within an Orga-
nization annotation).
The final CoreNER library consists of 104 FD (in-
volving 68 dictionaries, 33 regexes and 3 dynamic
dictionaries), 74 CD, 123 CR and 102 CO rules.
4.2 Customizing CoreNER
In this section we describe the process of customiz-
ing our domain-independent CoreNER library for
several different datasets. We start by discussing our
choice of datasets to use for customization.
Datasets For a rigorous evaluation of CoreNER?s
customizability, we require multiple datasets satis-
fying the following criteria: First, the datasets must
cover diverse sources and styles of text. Second,
the set of the most challenging NER tasks Person,
Organization and Location (see Tab. 3) considered
in CoreNER should be applicable to them. Finally,
they should be publicly available and preferably
have associated published results, against which we
can compare our experimental results. Towards this
end, we chose the following public datasets.
? CoNLL03 (Tjong et al, 2003): a collection of
Reuters news stories. Consists of formal text.
? Enron (Minkov et al, 2005): a collection of
emails with meeting information from the Enron
dataset. Contains predominantly informal text.
? ACE05 (NIST, 2005)1 a collection of broadcast
news, broadcast conversations and newswire re-
ports. Consists of both formal and informal text.
Customization Process The goal of customization
1The evaluation test set is not publicly available. Thus, fol-
lowing the example of (Florian et al, 2006), the publicly avail-
able set is split into a 80%/20% data split, with the last 20% of
the data in chronological order selected as test data.
is to refine the original CoreNER (hence referred
to as CoreNERorig) in order to improve its extrac-
tion quality on the training set (in terms of F?=1)
for each dataset individually. In addition, a devel-
opment set is available for CoNLL03 (referred to as
CoNLL03dev), therefore we seek to improve F?=1 on
CoNLL03dev as well.
The customization process for each dataset pro-
ceeded as follows. First, we studied the entity defini-
tions and identified their differences when compared
with the definitions used for CoreNERorig (Tab. 3).
We then added rules to account for the differences.
For example, the definition of Organization in the
CoNLL03 dataset contained a sports organization
subtype, which was not considered when develop-
ing CoreNER. Therefore, we have used public data
sources (e.g., Wikipedia) to collect and curate dic-
tionaries of major sports associations and sport clubs
from around the world. The new dictionaries, along
with regular expressions identifying sports teams in
sports articles were used for defining FD and CD
rules such as CR1 (Fig. 2). Finally, CR and CO rules
were added to filter invalid candidates and augment
the Organization type with the new sports subtype
(similar in spirit to rules CR4 and CR5 in Fig. 2).
In addition to the train and development sets, the
customization process for CoNLL03 also involved
unlabeled data from the corpus as follows. 1) Since
data-driven rules (CDD) are often created based on a
few instances from the training data, testing them on
the unlabeled data helped fine tune the rules for pre-
cision. 2) CoNLL03 is largely dominated by sports
news, but only a subset of all sports were represented
in the train dataset. Using the unlabeled data, we
were able to add CDD rules for five additional types
of sports, resulting in 0.31% improvement in F?=1
score on CoNLL03dev. 3) Unlabeled data was also
useful in identifying domain-specific gazetteers by
using simple extraction rules followed by a man-
ual cleanup phase. For instance, for CoNLL03 we
collected five gazetteers of organization and person
names from the unlabeled data, resulting in 0.45%
improvement in recall for CoNLL03dev.
The quality of the customization on the train col-
lections is shown in Tab. 5. The total number of
rules added during customization for each of the
three domains is listed in Tab. 4. Notice how rules
of all four types are used both in the development
1008
FD CD CR CO
CoreNERorig 104 74 123 102
CoreNERconll 179 56 284 71
CoreNERenron 13 10 9 1
CoreNERace 83 35 117 26
Table 4: Rules added during customization
Precision Recall F?=1
CoreNERconll 97.64 95.60 96.61
CoreNERenron 91.15 92.58 91.86
CoreNERace 92.32 91.22 91.77
Table 5: Quality of customization on train datasets (%)
of the domain independent NER annotator, and dur-
ing customizations for different domains. A total of
8 person weeks were spent on customizations, and
we believe this effort is quite reasonable by rule-
based extraction standards. For example, (Maynard
et al, 2003) reports that customizing the ANNIE
domain independent NER annotator developed us-
ing the JAPE grammar-based rule language for the
ACE05 dataset required 6 weeks (and subsequent
tuning over the next 6 months), resulting in im-
proving the quality to 82% for this dataset. As we
shall discuss shortly, with similar manual effort, we
were able to achieve results outperforming state-of-
art published results on three different datasets, in-
cluding ACE05. However, one may rightfully ar-
gue that the process is still too lengthy impeding the
widespread deployment of rule-based NER extrac-
tion. We elaborate on the effort involved and the
lessons learned in the process in Sec. 5.
4.3 Evaluation of Customization
We now present an experimental evaluation of the
customizability of CoreNER. The main goals are
to investigate: (i) the feasibility of CoreNER cus-
tomization for different application domains; (ii)
the effectiveness of such customization compared to
state-of-the-art results; (iii) the impact of different
types of customization (Tab. 1); and (iv) how often
different categories of NERL rules (Tab. 2) are used
during customization.
We measured the effectiveness of customization
using the improvement in extraction quality of the
customized CoreNER over CoreNERorig. As shown
in Tab. 6, customization significantly improved
Precision Recall F?=1
CoNLL03dev
CoreNERorig 83.81 61.77 71.12
CoreNERconll 96.49 93.76 95.11
Improvement 12.68 31.99 13.99
CoNLL03test
CoreNERorig 77.21 54.87 64.15
CoreNERconll 93.89 89.75 91.77
Improvement 15.68 34.88 27.62
Enron
CoreNERorig 85.06 69.55 76.53
CoreNERenron 88.41 82.39 85.29
Improvement 3.35 12.84 8.76
ACE2005
CoreNERorig 57.23 57.41 57.32
CoreNERace 90.11 87.82 88.95
Improvement 32.88 30.41 31.63
Table 6: Overall Improvement due to Customization (%)
Precision Recall F?=1
LOC CoreNERconll 97.17 95.37 96.26
CoNLL03dev
Florian 96.59 95.65 96.12
ORG CoreNERconll 93.70 88.67 91.11Florian 90.85 89.63 90.24
PER CoreNERconll 97.79 95.87 96.82Florian 96.08 97.12 96.60
LOC CoreNERconll 93.11 91.61 92.35
CoNLL03test
Florian 90.59 91.73 91.15
ORG CoreNERconll 92.25 85.31 88.65Florian 85.93 83.44 84.67
PER CoreNERconll 96.32 92.39 94.32Florian 92.49 95.24 93.85
Enron PER CoreNERenron 87.27 81.82 84.46Minkov 81.1 74.9 77.9
Table 7: Comparison with state-of-the-art results(%)
F?=1 score for CoreNERorig across all datasets. 2
We note that the extraction quality of
CoreNERorig was low on CoNLL03 and ACE05
mainly due to differences in entity type definitions.
In particular, sports organizations, which occurred
frequently in the CoNLL03 collection, were not
considered during the development of CoreNERorig,
while in ACE05, ORG and LOC entity types were
split into four entity types (Organization, Location,
Geo-Political Entity and Facility). Customizations
such as CS and CG address the above changes
in named-entity type definition and substantially
improve the extraction quality of CoreNERorig.
Next, we compare the extraction quality of the
2CoNLL03dev and CoNLL03test correspond to the develop-
ment and test sets for CoNLL03 respectively.
1009
customized CoreNER for CoNLL03 and Enron3 with
the corresponding best published results by (Florian
et al, 2003) and (Minkov et al, 2005). Tab. 7 shows
that our customized CoreNER outperforms the cor-
responding state-of-the-art numbers for all the NER
tasks on both CoNLL03 and Enron. 4 These results
demonstrate that high-quality annotators can be built
by customizing CoreNERorig using NERL, with the
final extraction quality matching that of state-of-the-
art machine learning-based extractors.
It is worthwhile noting that the best pub-
lished results for CoNLL03 (Florian et al, 2003)
were obtained by using four different classifiers
(Robust Risk Minimization, Maximum Entropy,
Transformation-based learning, and Hidden Markov
Model) and trying six different classifier combi-
nation methods. Compared to the best published
result obtained by combining the four classifiers,
the individual classifiers performed between 2.5-
7.6% worse for Location, 5.6-15.2% for Organiza-
tion and 3.9-14.0% for Person5. Taking this into
account, the extraction quality advantage of cus-
tomized CoreNER is significant when compared
with the individual state-of-the-art classifiers.
Impact of Customizations by Type. While cus-
tomizing CoreNER for the three datasets, all types
of changes described in Sec. 2 were performed. We
measured the impact of each type of customization
by comparing the extraction quality of CoreNERorig
with CoreNERorig enhanced with all the customiza-
tions of that type. From the results for CoNLL03
(Tab. 8), we make the following observations.
? Customizations that identify additional subtypes
of entities (CS) or modify existing instances for
multiple types (CATA) have significant impact.
This effect can be especially high when the miss-
ing subtype appears very often in the new do-
main (E.g., over 50% of the organizations in
CoNLL03 are sports teams).
? Data-driven customizations (CDD) rely on the
aggregated impact of many rules. While individ-
ual rules may have considerable impact on their
3We cannot meaningfully compare our results against previ-
ously published results for ACE05, which is originally used for
mention detection while CoreNER considers only NER tasks.
4For Enron the comparison is reported only for Person, as
labeled data is available only for that type.
5Extended version obtained via private communication.
# rules added Precision Recall F?=1
CEB 3
CoNLL03dev
LOC ?0.21 ?0.22 ?0.22
ORG ?1.35 ?0.38 ?0.59
PER - - -
CoNLL03test
LOC ?0.30 ?0.36 ?0.33
ORG ?0.54 ?0.12 ?0.20
PER - - -
CATA 5
CoNLL03dev
LOC ?7.18 ?0.87 ?3.19
ORG ?1.37 ?10.67 ?9.04
PER ?0.04 - ?0.01
CoNLL03test
LOC ?7.73 ?1.20 ?3.77
ORG ?1.37 ?11.62 ?14.18
PER - - -
CDSD 2
CoNLL03dev
LOC ?0.85 - ?0.45
ORG ?1.00 ?0.07 ?0.01
PER - - -
CoNLL03test
LOC ?0.04 ?0.12 ?0.12
ORG ?0.64 - ?0.04
PER - - -
CS 149
CoNLL03dev
LOC ?1.63 ?0.21 ?0.85
ORG ?11.44 ?40.79 ?39.73
PER ?0.13 - ?0.05
CoNLL03test
LOC ?3.71 ?0.18 ?2.05
ORG ?9.2 ?36.24 ?37.96
PER ?0.58 - ?0.2
CDD 431
CoNLL03dev
LOC ?0.94 ?10.18 ?3.99
ORG ?9.63 ?11.93 ?14.71
PER ?6.12 ?28.5 ?18.84
CoNLL03test
LOC ?1.66 ?6.72 ?1.64
ORG ?8.84 ?12.40 ?15.90
PER ?9.15 ?31.48 ?22.21
Table 8: Impact by customization type on CoNLL03(%)
own (e.g., identifying all names that appear as
part of a player list increases the recall of PER by
over 6% on both CoNLL03dev and CoNLL03test),
the overall impact relies on the accumulative ef-
fect of many small improvements.
? Certain customizations (CEB and CDSD) pro-
vide smaller quality improvements, both per rule
and in aggregate.
5 Lessons Learned
Our experimental evaluation shows that rule-based
annotators can achieve quality comparable to that of
state-of-the-art machine learning techniques. In this
section we discuss three important lessons learned
regarding the human effort involved in developing
such rule-based extractors.
Usefulness of NERL We found NERL very helpful
in that it provided a higher-level abstraction catered
specifically towards NER tasks, thus hiding the com-
plexity inherent in a general-purpose IE rule lan-
guage. In doing so, NERL restricts the large space
of operations possible within a general-purpose lan-
guage to the small number of predefined ?templates?
1010
listed in Tab. 2. (We have shown empirically that our
choice of NERL rules is sufficient to achieve high
accuracy for NER tasks.) Therefore, NERL simpli-
fies development and maintenance of complex NER
extractors, since one does not need to worry about
multiple AQL statements or JAPE grammar phases
for implementing a single conceptual operation such
as filtering (see Fig. 3).
Is NERL Sufficient? Even using NERL, building
and customizing NER rules remains a labor inten-
sive process. Consider the example of designing the
filter rule CR4 from Fig. 3. First, one must exam-
ine multiple false positive Location entities to even
decide that a filter rule is appropriate. Second, one
must understand how those false positives were pro-
duced, and decide accordingly on the particular con-
cept to be used as filter (LocationMaybeOrg in this
case). Finally, one needs to decide how to build the
filter. Tab. 9 lists all the attributes that need to be
specified for a Filter rule, along with examples of
the search space for each rule attribute.
Rule Attributes Examples of Search Space
Location Intermediate Concept to filter
Predicate Type Matches Regex, Contains Dictionary, . . .
Predicate Parameter Regular Expressions, Dictionary Entries, . . .
Context Type Entity text, Left or Right context
Context Parameter k tokens, l characters
Table 9: Search space explored while adding a Filter rule
This search space problem is not unique to filter
rules. In fact, most rules in Tab. 2 have two or more
rule attributes. Therefore, designing an individual
NERL rule remains a time-consuming ?trial and er-
ror? process, in which multiple ?promising? combi-
nations are implemented and evaluated individually
before deciding on a satisfactory final rule.
Tooling for NERL The fact that NERL is a high-
level language exposing a restricted set of operators
can be exploited to reduce the human effort involved
in building NER annotators by enabling the follow-
ing tools:
Annotation Provenance Tools tracking prove-
nance (Cheney et al, 2009) for NERL rules can
help in explaining exactly which sequence of rules
is responsible for producing a given false positive,
thereby enabling one to quickly identify ?misbe-
haved? rules. For instance, one can quickly narrow
down the choices for the location where the filter
rule CR4 (Fig. 2) should be applied based on the
provenance of the false positives. Similarly, tools
for explaining false positives in the spirit of (Huang
et al, 2008), are also conceivable.
Automatic Parameter Learning The most time-
consuming part in building a rule often is to decide
the value of its parameters, especially for FD and
CR rules. For instance, while defining a CR rule,
one has to choose values for the Predicate parame-
ter and the Context parameter (see Tab. 9). Some
parameter values can be learned ? for example, dic-
tionaries (Riloff, 1993) and regular expressions (Li
et al, 2008).
Automatic Rule Refinement Tools automatically
suggesting entire customization rules to a complex
NERL program in the spirit of (Liu et al, 2010) can
further reduce human effort in building NER anno-
tators. With the help of such tools, one only needs
to consider good candidate NERL rules suggested
by the system without having to go through the
conventional manual ?trial and error? process.
6 Conclusion
In this paper, we described NERL, a high-level rule
language for building and customizing NER annota-
tors. We demonstrated that a complex NER annota-
tor built using NERL can be effectively customized
for different domains, achieving extraction quality
superior to the state-of-the-art numbers. However,
our experience also indicates that the process of de-
signing the rules themselves is still manual and time-
consuming. Finally, we discuss how NERL opens
up several interesting research directions towards the
development of sophisticated tooling for automating
some of the rule development tasks.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
A. Arnold, R. Nallapati, and W. W. Cohen. 2008.
Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. In
Machine Learning, volume 34, pages 211?231.
J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP.
1011
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
Census. 2007. U.S. Census Bureau.
http://www.census.gov.
J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance
in databases: Why, how, and where. Foundations and
Trends in Databases, 1(4):379?474.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
W. Drozdzynski, H. Krieger, J. Piskorski, U. Scha?fer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures ? foundations and applica-
tions. Ku?nstliche Intelligenz, 1:17?23.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005.
Unsupervised named-entity extraction from the web:
an experimental study. Artif. Intell., 165(1):91?134.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In EMNLP.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In NAACL-HLT.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in mention
detection. In ACL.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and
A. Sheth. 2009. Context and domain knowledge en-
hanced entity spotting in informal text. In ISWC.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In ACL.
Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F.
Naughton. 2008. On the Provenance of Non-Answers
to Queries over Extracted Data. PVLDB, 1(1):736?
747.
M. Jansche and S. Abney. 2002. Information extraction
from voicemail transcripts. In EMNLP.
J. Jiang and C. Zhai. 2006. Exploiting domain structure
for named entity recognition. In NAACL-HLT.
Saul Kripke. 1982. Naming and Necessity. Harvard Uni-
versity Press.
G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: De-
scription of the NetOwlTM extractor system as used
for MUC-7. In MUC-7.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3.
D. Maynard, K. Bontcheva, and H. Cunningham. 2003.
Towards a semantic extraction of named entities. In
RANLP.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In CoNLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from emails: Applying named
entity recognition to informal text. In HLT/EMNLP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30(1):3?26.
NIST. 2005. The ace evaluation plan.
F. J. Och O. Bender and H. Ney. 2003. Maximum en-
tropy models for named entity recognition. In CoNLL.
G. Petasis, F. Vichot, F. Wolinski, G. Paliouras,
V. Karkaletsis, and C. Spyropoulos. 2001. Using
machine learning to maintain rule-based named-entity
recognition and classification systems. In ACL.
T. Poibeau and L. Kosseim. 2001. Proper name ex-
traction from non-journalistic texts. In Computational
Linguistics in the Netherlands, pages 144?157.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In KDD.
S. Sekine and C. Nobata. 2004. Definition, dictionaries
and tagger for extended named entity hierarchy. In
Conference on Language Resources and Evaluation.
W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan.
2007. Declarative information extraction using data-
log with embedded extraction predicates. In VLDB.
S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimally-
supervised extraction of entities from text advertise-
ments. In NAACL-HLT.
P. Siniakov. 2010. GROPUS - an adaptive rule-based al-
gorithm for information extraction. Ph.D. thesis, Freie
Universitat Berlin.
R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach
for named entity and sub-type tagging. In ANLP.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain
adaptive bootstrapping for named entity recognition.
In EMNLP.
J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive
named entity recognition for web browsing. In WM.
1012
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 128?137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SystemT: An Algebraic Approach to Declarative Information Extraction
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li
Sriram Raghavan Frederick R. Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
San Jose, CA, USA
{chiti,sekar,yunyaoli,rsriram,frreiss,vaithyan}@us.ibm.com
Abstract
As information extraction (IE) becomes
more central to enterprise applications,
rule-based IE engines have become in-
creasingly important. In this paper, we
describe SystemT, a rule-based IE sys-
tem whose basic design removes the ex-
pressivity and performance limitations of
current systems based on cascading gram-
mars. SystemT uses a declarative rule
language, AQL, and an optimizer that
generates high-performance algebraic ex-
ecution plans for AQL rules. We com-
pare SystemT?s approach against cascad-
ing grammars, both theoretically and with
a thorough experimental evaluation. Our
results show that SystemT can deliver re-
sult quality comparable to the state-of-the-
art and an order of magnitude higher an-
notation throughput.
1 Introduction
In recent years, enterprises have seen the emer-
gence of important text analytics applications like
compliance and data redaction. This increase,
combined with the inclusion of text into traditional
applications like Business Intelligence, has dra-
matically increased the use of information extrac-
tion (IE) within the enterprise. While the tradi-
tional requirement of extraction quality remains
critical, enterprise applications also demand ef-
ficiency, transparency, customizability and main-
tainability. In recent years, these systemic require-
ments have led to renewed interest in rule-based
IE systems (Doan et al, 2008; SAP, 2010; IBM,
2010; SAS, 2010).
Until recently, rule-based IE systems (Cunning-
ham et al, 2000; Boguraev, 2003; Drozdzynski
et al, 2004) were predominantly based on the
cascading grammar formalism exemplified by the
Common Pattern Specification Language (CPSL)
specification (Appelt and Onyshkevych, 1998). In
CPSL, the input text is viewed as a sequence of an-
notations, and extraction rules are written as pat-
tern/action rules over the lexical features of these
annotations. In a single phase of the grammar, a
set of rules are evaluated in a left-to-right fash-
ion over the input annotations. Multiple grammar
phases are cascaded together, with the evaluation
proceeding in a bottom-up fashion.
As demonstrated by prior work (Grishman and
Sundheim, 1996), grammar-based IE systems can
be effective in many scenarios. However, these
systems suffer from two severe drawbacks. First,
the expressivity of CPSL falls short when used
for complex IE tasks over increasingly pervasive
informal text (emails, blogs, discussion forums
etc.). To address this limitation, grammar-based
IE systems resort to significant amounts of user-
defined code in the rules, combined with pre-
and post-processing stages beyond the scope of
CPSL (Cunningham et al, 2010). Second, the
rigid evaluation order imposed in these systems
has significant performance implications.
Three decades ago, the database community
faced similar expressivity and efficiency chal-
lenges in accessing structured information. The
community addressed these problems by introduc-
ing a relational algebra formalism and an associ-
ated declarative query language SQL. The ground-
breaking work on System R (Chamberlin et al,
1981) demonstrated how the expressivity of SQL
can be efficiently realized in practice by means of
a query optimizer that translates an SQL query into
an optimized query execution plan.
Borrowing ideas from the database community,
we have developed SystemT, a declarative IE sys-
tem based on an algebraic framework, to address
both expressivity and performance issues. In Sys-
temT, extraction rules are expressed in a declar-
ative language called AQL. At compilation time,
128
({First} {Last} ) :full :full.Person
({Caps}  {Last} ) :full :full.Person
({Last} {Token.orth = comma} {Caps | First}) : reverse
:reverse.Person
({First}) : fn  :fn.Person
({Last}) : ln  :ln.Person
({Lookup.majorType = FirstGaz})  : fn  :fn.First
({Lookup.majorType = LastGaz}) : ln  :ln.Last
({Token.orth = upperInitial} | 
{Token.orth = mixedCaps } ) :cw  :cw.Caps
Rule Patterns
50
20
10
10
10
50
50
10
Priority
P2R1
P2R2
P2R3
P2R4
P2R5
P1R1
P1R2
P1R3
RuleId
Input
First
Last
Caps
Token
Output
Person
Input
Lookup
Token
Output
First
Last
Caps
TypesPhase
P2
P1
P2R3        ({Last} {Token.orth = comma} {Caps | First}) : reverse   :reverse.Person
Last followed by Token whose orth attribute has value 
comma followed by Caps or First
Rule part Action part
Create Person
annotation
Bind match 
to variables
Syntax:
Gazetteers containing first names and last names
Figure 1: Cascading grammar for identifying Person names
SystemT translates AQL statements into an al-
gebraic expression called an operator graph that
implements the semantics of the statements. The
SystemT optimizer then picks a fast execution
plan from many logically equivalent plans. Sys-
temT is currently deployed in a multitude of real-
world applications and commercial products1.
We formally demonstrate the superiority of
AQL and SystemT in terms of both expressivity
and efficiency (Section 4). Specifically, we show
that 1) the expressivity of AQL is a strict superset
of CPSL grammars not using external functions
and 2) the search space explored by the SystemT
optimizer includes operator graphs correspond-
ing to efficient finite state transducer implemen-
tations. Finally, we present an extensive experi-
mental evaluation that validates that high-quality
annotators can be developed with SystemT, and
that their runtime performance is an order of mag-
nitude better when compared to annotators devel-
oped with a state-of-the-art grammar-based IE sys-
tem (Section 5).
2 Grammar-based Systems and CPSL
A cascading grammar consists of a sequence of
phases, each of which consists of one or more
rules. Each phase applies its rules from left to
right over an input sequence of annotations and
generates an output sequence of annotations that
the next phase consumes. Most cascading gram-
mar systems today adhere to the CPSL standard.
Fig. 1 shows a sample CPSL grammar that iden-
tifies person names from text in two phases. The
first phase, P1, operates over the results of the tok-
1A trial version is available at
http://www.alphaworks.ibm.com/tech/systemt
Rule skipped 
due to priority 
semantics
CPSL
Phase P1
Last(P1R2) Last(P1R2)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
CPSL
Phase P2
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4)
Person(P2R4)
Person (P2R5)
Person(P2R4)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
JAPE
Phase P1
(Brill) Caps(P1R3) Last(P1R2) Last(P1R2)
Caps(P1R3) Caps(P1R3)
Caps(P1R3)
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4, P2R5)
JAPE
Phase P2
(Appelt)
Person(P2R1)
Person (P2R2)
Some discarded 
matches omitted
for clarity
? Tomorrow, we will meet Mark Scott, Howard Smith and ?Document d1
Rule fired
Legend
3 persons
identified
2 persons
identified
(a)
(b)
Figure 2: Sample output of CPSL and JAPE
enizer and gazetteer (input types Token and Lookup,
respectively) to identify words that may be part of
a person name. The second phase, P2, identifies
complete names using the results of phase P1.
Applying the above grammar to document d1
(Fig. 2), one would expect that to match ?Mark
Scott? and ?Howard Smith? as Person. However,
as shown in Fig. 2(a), the grammar actually finds
three Person annotations, instead of two. CPSL has
several limitations that lead to such discrepancies:
L1. Lossy sequencing. In a CPSL grammar,
each phase operates on a sequence of annotations
from left to right. If the input annotations to a
phase may overlap with each other, the CPSL en-
gine must drop some of them to create a non-
overlapping sequence. For instance, in phase P1
(Fig. 2(a)), ?Scott? has both a Lookup and a To-
ken annotation. The system has made an arbitrary
choice to retain the Lookup annotation and discard
the Token annotation. Consequently, no Caps anno-
tations are output by phase P1.
L2. Rigid matching priority. CPSL specifies
that, for each input annotation, only one rule can
actually match. When multiple rules match at the
same start position, the following tie-breaker con-
ditions are applied (in order): (a) the rule match-
ing the most annotations in the input stream; (b)
the rule with highest priority; and (c) the rule de-
clared earlier in the grammar. This rigid match-
ing priority can lead to mistakes. For instance,
as illustrated in Fig. 2(a), phase P1 only identi-
fies ?Scott? as a First. Matching priority causes
the grammar to skip the corresponding match for
?Scott? as a Last. Consequently, phase P2 fails to
identify ?Mark Scott? as one single Person.
L3. Limited expressivity in rule patterns. It is
not possible to express rules that compare annota-
tions overlapping with each other. E.g., ?Identify
129
[A-Z]{\w|-}+
DocumentInput Tuple
?
we will meet Mark 
Scott, ?
Output Tuple 2 Span 2Document
Span 1Output Tuple 1 Document
Regex
Caps
Figure 3: Regular Expression Extraction Operator
words that are both capitalized and present in the
FirstGaz gazetteer? or ?Identify Person annotations
that occur within an EmailAddress?.
Extensions to CPSL
In order to address the above limitations, several
extensions to CPSL have been proposed in JAPE,
AFst and XTDL (Cunningham et al, 2000; Bogu-
raev, 2003; Drozdzynski et al, 2004). The exten-
sions are summarized as below, where each solu-
tion Si corresponds to limitation Li.
? S1. Grammar rules are allowed to operate on
graphs of input annotations in JAPE and AFst.
? S2. JAPE introduces more matching regimes
besides the CPSL?s matching priority and thus
allows more flexibility when multiple rules
match at the same starting position.
? S3. The rule part of a pattern has been ex-
panded to allow more expressivity in JAPE,
AFst and XTDL.
Fig. 2(b) illustrates how the above extensions
help in identifying the correct matches ?Mark Scott?
and ?Howard Smith? in JAPE. Phase P1 uses a match-
ing regime (denoted by Brill) that allows multiple
rules to match at the same starting position, and
phase P2 uses CPSL?s matching priority, Appelt.
3 SystemT
SystemT is a declarative IE system based on an
algebraic framework. In SystemT, developers
write rules in a language called AQL. The system
then generates a graph of operators that imple-
ment the semantics of the AQL rules. This decou-
pling allows for greater rule expressivity, because
the rule language is not constrained by the need to
compile to a finite state transducer. Likewise, the
decoupled approach leads to greater flexibility in
choosing an efficient execution strategy, because
many possible operator graphs may exist for the
same AQL annotator.
In the rest of the section, we describe the parts
of SystemT, starting with the algebraic formalism
behind SystemT?s operators.
3.1 Algebraic Foundation of SystemT
SystemT executes IE rules using graphs of op-
erators. The formal definition of these operators
takes the form of an algebra that is similar to the
relational algebra, but with extensions for text pro-
cessing.
The algebra operates over a simple relational
data model with three data types: span, tuple, and
relation. In this data model, a span is a region of
text within a document identified by its ?begin?
and ?end? positions; a tuple is a fixed-size list of
spans. A relation is a multiset of tuples, where ev-
ery tuple in the relation must be of the same size.
Each operator in our algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples.
Fig. 3 illustrates the regular expression ex-
traction operator in the algebra, which per-
forms character-level regular expression match-
ing. Overall, the algebra contains 12 different op-
erators, a full description of which can be found
in (Reiss et al, 2008). The following four oper-
ators are necessary to understand the examples in
this paper:
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, creating a tuple
for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples. It
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets
of tuples and a predicate to apply to pairs of
tuples from the input sets. It outputs all pairs
of input tuples that satisfy the predicate.
? The consolidate operator (?) takes as input a
set of tuples and the index of a particular col-
umn in those tuples. It removes selected over-
lapping spans from the indicated column, ac-
cording to the specified policy.
3.2 AQL
Extraction rules in SystemT are written in AQL,
a declarative relational language similar in syn-
tax to the database language SQL. We chose SQL
as a basis for our language due to its expres-
sivity and its familiarity. The expressivity of
SQL, which consists of first-order logic predicates
130
Figure 4: Person annotator as AQL query
over sets of tuples, is well-documented and well-
understood (Codd, 1990). As SQL is the pri-
mary interface to most relational database sys-
tems, the language?s syntax and semantics are
common knowledge among enterprise application
programmers. Similar to SQL terminology, we
call a collection of AQL rules an AQL query.
Fig. 4 shows portions of an AQL query. As
can be seen, the basic building block of AQL is
a view: A logical description of a set of tuples in
terms of either the document text (denoted by a
special view called Document) or the contents of
other views. Every SystemT annotator consists
of at least one view. The output view statement in-
dicates that the tuples in a view are part of the final
results of the annotator.
Fig. 4 also illustrates three of the basic con-
structs that can be used to define a view.
? The extract statement specifies basic
character-level extraction primitives to be
applied directly to a tuple.
? The select statement is similar to the SQL
select statement but it contains an additional
consolidate on clause, along with an exten-
sive collection of text-specific predicates.
? The union all statement merges the outputs
of one or more select or extract statements.
To keep rules compact, AQL also provides a
shorthand sequence pattern notation similar to the
syntax of CPSL. For example, the CapsLast
view in Figure 4 could have been written as:
create view CapsLast as
extract pattern <C.name> <L.name>
from Caps C, Last L;
Internally, SystemT translates each of these ex-
tract pattern statements into one or more select
and extract statements.
AQL SystemT
Optimizer
SystemT
Runtime
Compiled
Operator
Graph
Figure 5: The compilation process in SystemT
Figure 6: Execution strategies for the CapsLast rule
in Fig. 4
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using Language-
Ware (IBM, 2010). Rule developers can utilize
the multilingual support via AQL without hav-
ing to configure or manage any additional re-
sources. In addition, AQL allows user-defined
functions to be used in a restricted context in or-
der to support operations such as validation (e.g.
for extracted credit card numbers), or normaliza-
tion (e.g., compute abbreviations of multi-token
organization candidates that are useful in gener-
ating additional candidates). More details on AQL
can be found in the AQL manual (SystemT, 2010).
3.3 Optimizer and Operator Graph
Grammar-based IE engines place rigid restrictions
on the order in which rules can be executed. Due
to the semantics of the CPSL standard, systems
that implement the standard must use a finite state
transducer that evaluates each level of the cascade
with one or more left to right passes over the entire
token stream.
In contrast, SystemT places no explicit con-
straints on the order of rule evaluation, nor does
it require that intermediate results of an annota-
tor collapse to a fixed-size sequence. As shown in
Fig. 5, the SystemT engine does not execute AQL
directly; instead, the SystemT optimizer compiles
AQL into a graph of operators. By tying a collec-
tion of operators together by their inputs and out-
puts, the system can implement a wide variety of
different execution strategies. Different execution
strategies are associated with different evaluation
costs. The optimizer chooses the execution strat-
egy with the lowest estimated evaluation cost.
131
Fig. 6 presents three possible execution strate-
gies for the CapsLast rule in Fig. 4. If the opti-
mizer estimates that the evaluation cost of Last is
much lower than that of Caps, then it can deter-
mine that Plan C has the lowest evaluation cost
among the three, because Plan C only evaluates
Caps in the ?left? neighborhood for each instance
of Last. More details of our algorithms for enumer-
ating plans can be found in (Reiss et al, 2008).
The optimizer in SystemT chooses the best ex-
ecution plan from a large number of different al-
gebra graphs available to it. Many of these graphs
implement strategies that a transducer could not
express: such as evaluating rules from right to left,
sharing work across different rules, or selectively
skipping rule evaluations. Within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could. We refer
the reader to (Reiss et al, 2008) for a detailed de-
scription of the types of plan the optimizer consid-
ers, as well as an experimental analysis of the per-
formance benefits of different parts of this search
space.
Several parallel efforts have been made recently
to improve the efficiency of IE tasks by optimiz-
ing low-level feature extraction (Ramakrishnan et
al., 2006; Ramakrishnan et al, 2008; Chandel et
al., 2006) or by reordering operations at a macro-
scopic level (Ipeirotis et al, 2006; Shen et al,
2007; Jain et al, 2009). However, to the best of
our knowledge, SystemT is the only IE system
in which the optimizer generates a full end-to-end
plan, beginning with low-level extraction primi-
tives and ending with the final output tuples.
3.4 Deployment Scenarios
SystemT is designed to be usable in various de-
ployment scenarios. It can be used as a stand-
alone system with its own development and run-
time environment. Furthermore, SystemT ex-
poses a generic Java API that enables the integra-
tion of its runtime environment with other applica-
tions. For example, a specific instantiation of this
API allows SystemT annotators to be seamlessly
embedded in applications using the UIMA analyt-
ics framework (UIMA, 2010).
4 Grammar vs. Algebra
Having described both the traditional cascading
grammar approach and the declarative approach
Figure 7: Supporting Complex Rule Interactions
used in SystemT, we now compare the two in
terms of expressivity and performance.
4.1 Expressivity
In Section 2, we described three expressivity lim-
itations of CPSL grammars: Lossy sequencing,
rigid matching priority, and limited expressivity in
rule patterns. As we noted, cascading grammar
systems extend the CPSL specification in various
ways to provide workarounds for these limitations.
In SystemT, the basic design of the AQL lan-
guage eliminates these three problems without the
need for any special workaround. The key design
difference is that AQL views operate over sets of
tuples, not sequences of tokens. The input or out-
put tuples of a view can contain spans that overlap
in arbitrary ways, so the lossy sequencing prob-
lem never occurs. The annotator will retain these
overlapping spans across any number of views un-
til a view definition explicitly removes the over-
lap. Likewise, the tuples that a given view pro-
duces are in no way constrained by the outputs of
other, unrelated views, so the rigid matching prior-
ity problem never occurs. Finally, the select state-
ment in AQL allows arbitrary predicates over the
cross-product of its input tuple sets, eliminating
the limited expressivity in rule patterns problem.
Beyond eliminating the major limitations of
CPSL grammars, AQL provides a number of other
information extraction operations that even ex-
tended CPSL cannot express without custom code.
Complex rule interactions. Consider an exam-
ple document from the Enron corpus (Minkov et
al., 2005), shown in Fig. 7, which contains a list
of person names. Because the first person in the
list (?Skilling?) is referred to by only a last name,
rule P2R3 in Fig. 1 incorrectly identifies ?Skilling,
Cindy? as a person. Consequently, the output of
phase P2 of the cascading grammar contains sev-
eral mistakes as shown in the figure. This problem
132
went to the Switchfoot concert at the Roxy. It was pretty fun,? The lead singer/guitarist 
was really good, and even though there was another guitarist  (an Asian guy), he ended up 
playing most of the guitar parts, which was really impressive. The biggest surprise though is 
that I actually liked the opening bands. ?I especially liked the first band
Consecutive review snippets are within 25 tokens
At least 4 occurrences of MusicReviewSnippet or GenericReviewSnippet
At least 3 of them should be MusicReviewSnippets
Review ends with one of these.
Start with 
ConcertMention
Complete review is
within 200 tokens
ConcertMention
MusicReviewSnippet
GenericReviewSnippet
Example Rule
Informal Band Review
Figure 8: Extracting informal band reviews from web logs
occurs because CPSL only evaluates rules over
the input sequence in a strict left-to-right fashion.
On the other hand, the AQL query Q1 shown in
the figure applies the following condition: ?Al-
ways discard matches to Rule P2R3 if they overlap
with matches to rules P2R1 or P2R2? (even if the
match to Rule P2R3 starts earlier). Applying this
rule ensures that the person names in the list are
identified correctly. Obtaining the same effect in
grammar-based systems would require the use of
custom code (as recommended by (Cunningham
et al, 2010)).
Counting and Aggregation. Complex extraction
tasks sometimes require operations such as count-
ing and aggregation that go beyond the expressiv-
ity of regular languages, and thus can be expressed
in CPSL only using external functions. One such
task is that of identifying informal concert reviews
embedded within blog entries. Fig. 8 describes, by
example, how these reviews consist of reference
to a live concert followed by several review snip-
pets, some specific to musical performances and
others that are more general review expressions.
An example rule to identify informal reviews is
also shown in the figure. Notice how implement-
ing this rule requires counting the number of Mu-
sicReviewSnippet and GenericReviewSnippet annotations
within a region of text and aggregating this occur-
rence count across the two review types. While
this rule can be written in AQL, it can only be ap-
proximated in CPSL grammars.
Character-Level Regular Expression CPSL
cannot specify character-level regular expressions
that span multiple tokens. In contrast, the extract
regex statement in AQL fully supports these ex-
pressions.
We have described above several cases where
AQL can express concepts that can only be ex-
pressed through external functions in a cascad-
ing grammar. These examples naturally raise the
question of whether similar cases exist where a
cascading grammar can express patterns that can-
not be expressed in AQL.
It turns out that we can make a strong statement
that such examples do not exist. In the absence
of an escape to arbitrary procedural code, AQL is
strictly more expressive than a CPSL grammar. To
state this relationship formally, we first introduce
the following definitions.
We refer to a grammar conforming to the CPSL
specification as a CPSL grammar. When a CPSL
grammar contains no external functions, we refer
to it as a Code-free CPSL grammar. Finally, we
refer to a grammar that conforms to one of the
CPSL, JAPE, AFst and XTDL specifications as an
expanded CPSL grammar.
Ambiguous Grammar Specification An ex-
panded CPSL grammar may be under-specified in
some cases. For example, a single rule contain-
ing the disjunction operator (|) may match a given
region of text in multiple ways. Consider the eval-
uation of Rule P2R3 over the text fragment ?Scott,
Howard? from document d1 (Fig. 1). If ?Howard?
is identified both as Caps and First, then there are
two evaluations for Rule P2R3 over this text frag-
ment. Since the system has to arbitrarily choose
one evaluation, the results of the grammar can be
non-deterministic (as pointed out in (Cunning-
ham et al, 2010)). We refer to a grammar G as
an ambiguous grammar specification for a docu-
ment collection D if the system makes an arbitrary
choice while evaluating G over D.
Definition 1 (UnambigEquiv) A query Q is Un-
ambigEquiv to a cascading grammar G if and only
if for every document collection D, where G is not
an ambiguous grammar specification for D, the
results of the grammar invocation and the query
evaluation are identical.
We now formally compare the expressivity of
AQL and expanded CPSL grammars. The detailed
proof is omitted due to space limitations.
Theorem 1 The class of extraction tasks express-
ible as AQL queries is a strict superset of that ex-
pressible through expanded code-free CPSL gram-
mars. Specifically,
(a) Every expanded code-free CPSL grammar can
be expressed as an UnambigEquiv AQL query.
(b) AQL supports information extraction opera-
tions that cannot be expressed in expanded code-
free CPSL grammars.
133
Proof Outline: (a) A single CPSL grammar can
be expressed in AQL as follows. First, each rule
r in the grammar is translated into a set of AQL
statements. If r does not contain the disjunct (|)
operator, then it is translated into a single AQL
select statement. Otherwise, a set of AQL state-
ments are generated, one for each disjunct opera-
tor in rule r, and the results merged using union
all statements. Then, a union all statement is used
to combine the results of individual rules in the
grammar phase. Finally, the AQL statements for
multiple phases are combined in the same order as
the cascading grammar specification.
The main extensions to CPSL supported by ex-
panded CPSL grammars (listed in Sec. 2) are han-
dled as follows. AQL queries operate on graphs
on annotations just like expanded CPSL gram-
mars. In addition, AQL supports different match-
ing regimes through consolidation operators, span
predicates through selection predicates and co-
references through join operators.
(b) Example operations supported in AQL that
cannot be expressed in expanded code-free CPSL
grammars include (i) character-level regular ex-
pressions spanning multiple tokens, (ii) count-
ing the number of annotations occurring within a
given bounded window and (iii) deleting annota-
tions if they overlap with other annotations start-
ing later in the document. 2
4.2 Performance
For the annotators we test in our experiments
(See Section 5), the SystemT optimizer is able to
choose algebraic plans that are faster than a com-
parable transducer-based implementation. The
question arises as to whether there are other an-
notators for which the traditional transducer ap-
proach is superior. That is, for a given annota-
tor, might there exist a finite state transducer that
is combinatorially faster than any possible algebra
graph? It turns out that this scenario is not possi-
ble, as the theorem below shows.
Definition 2 (Token-Based FST) A token-based
finite state transducer (FST) is a nondeterministic
finite state machine in which state transitions are
triggered by predicates on tokens. A token-based
FST is acyclic if its state graph does not contain
any cycles and has exactly one ?accept? state.
Definition 3 (Thompson?s Algorithm)
Thompson?s algorithm is a common strategy
for evaluating a token-based FST (based on
(Thompson, 1968)). This algorithm processes the
input tokens from left to right, keeping track of the
set of states that are currently active.
Theorem 2 For any acyclic token-based finite
state transducer T , there exists an UnambigEquiv
operator graph G, such that evaluating G has the
same computational complexity as evaluating T
with Thompson?s algorithm starting from each to-
ken position in the input document.
Proof Outline: The proof constructs G by struc-
tural induction over the transducer T . The base
case converts transitions out of the start state into
Extract operators. The inductive case adds a Se-
lect operator to G for each of the remaining state
transitions, with each selection predicate being the
same as the predicate that drives the corresponding
state transition. For each state transition predicate
that T would evaluate when processing a given
document, G performs a constant amount of work
on a single tuple. 2
5 Experimental Evaluation
In this section we present an extensive comparison
study between SystemT and implementations of
expanded CPSL grammar in terms of quality, run-
time performance and resource requirements.
TasksWe chose two tasks for our evaluation:
? NER : named-entity recognition for Person,
Organization, Location, Address, PhoneNumber,
EmailAddress, URL and DateTime.
? BandReview : identify informal reviews in
blogs (Fig. 8).
We chose NER primarily because named-entity
recognition is a well-studied problem and standard
datasets are available for evaluation. For this task
we use GATE and ANNIE for comparison3. We
chose BandReview to conduct performance evalu-
ation for a more complex extraction task.
Datasets. For quality evaluation, we use:
? EnronMeetings (Minkov et al, 2005): collec-
tion of emails with meeting information from
the Enron corpus4 with Person labeled data;
? ACE (NIST, 2005): collection of newswire re-
ports and broadcast news/conversations with
Person, Organization, Location labeled data5.
3To the best of our knowledge, ANNIE (Cunningham et
al., 2002) is the only publicly available NER library imple-
mented in a grammar-based system (JAPE in GATE).
4http://www.cs.cmu.edu/ enron/
5Only entities of type NAM have been considered.
134
Table 1: Datasets for performance evaluation.
Dataset Description of the Content Number of Document size
documents range average
Enronx Emails randomly sampled from the Enron corpus of average size xKB (0.5 < x < 100)2 1000 xKB +/? 10% xKB
WebCrawl Small to medium size web pages representing company news, with HTML tags removed 1931 68b - 388.6KB 8.8KB
FinanceM Medium size financial regulatory filings 100 240KB - 0.9MB 401KB
FinanceL Large size financial regulatory filings 30 1MB - 3.4MB 1.54MB
Table 2: Quality of Person on test datasets.
Precision (%) Recall (%) F1 measure (%)
(Exact/Partial) (Exact/Partial) (Exact/Partial)
EnronMeetings
ANNIE 57.05/76.84 48.59/65.46 52.48/70.69
T-NE 88.41/92.99 82.39/86.65 85.29/89.71
Minkov 81.1/NA 74.9/NA 77.9/NA
ACE
ANNIE 39.41/78.15 30.39/60.27 34.32/68.06
T-NE 93.90/95.82 90.90/92.76 92.38/94.27
Table 1 lists the datasets used for performance
evaluation. The size of FinanceLis purposely
small because GATE takes a significant amount of
time processing large documents (see Sec. 5.2).
Set Up. The experiments were run on a server
with two 2.4 GHz 4-core Intel Xeon CPUs and
64GB of memory. We use GATE 5.1 (build 3431)
and two configurations for ANNIE: 1) the default
configuration, and 2) an optimized configuration
where the Ontotext Japec Transducer6 replaces the
default NE transducer for optimized performance.
We refer to these configurations as ANNIE and
ANNIE-Optimized, respectively.
5.1 Quality Evaluation
The goal of our quality evaluation is two-fold:
to validate that annotators can be built in Sys-
temT with quality comparable to those built in
a grammar-based system; and to ensure a fair
performance comparison between SystemT and
GATE by verifying that the annotators used in the
study are comparable.
Table 2 shows results of our comparison study
for Person annotators. We report the classical
(exact) precision, recall, and F1 measures that
credit only exact matches, and corresponding par-
tial measures that credit partial matches in a fash-
ion similar to (NIST, 2005). As can be seen, T-
NE produced results of significantly higher quality
than ANNIE on both datasets, for the same Person
extraction task. In fact, on EnronMeetings, the F1
measure of T-NE is 7.4% higher than the best pub-
lished result (Minkov et al, 2005). Similar results
6http://www.ontotext.com/gate/japec.html
a) Throughput on Enron
0
100
200
300
400
500
600
700
0 20 40 60 80 100
Average document size (KB)
Th
ro
u
gh
pu
t (
K
B
/s
ec
)
ANNIE
ANNIE-Optimized
T-NE
x
b) Memory Utilization on Enron
0
200
400
600
0 20 40 60 80 100
Average document size (KB)
A
v
g 
H
ea
p 
si
ze
 
(M
B
) ANNIE
ANNIE-Optimized
T-NE
Error bars show
25th and 75th
percentile 
x
Figure 9: Throughput (a) and memory consump-
tion (b) comparisons on Enronx datasets.
can be observed for Organization and Location on
ACE (exact numbers omitted in interest of space).
Clearly, considering the large gap between
ANNIE?s F1 and partial F1 measures on both
datasets, ANNIE?s quality can be improved via
dataset-specific tuning as demonstrated in (May-
nard et al, 2003). However, dataset-specific tun-
ing for ANNIE is beyond the scope of this paper.
Based on the experimental results above and our
previous formal comparison in Sec. 4, we believe
it is reasonable to conclude that annotators can be
built in SystemT of quality at least comparable to
those built in a grammar-based system.
5.2 Performance Evaluation
We now focus our attention on the throughput and
memory behavior of SystemT, and draw a com-
parison with GATE. For this purpose, we have con-
figured both ANNIE and T-NE to identify only the
same eight types of entities listed for NER task.
Throughput. Fig. 9(a) plots the throughput of
the two systems on multiple Enronx datasets with
average document sizes of between 0.5KB and
100KB. For this experiment, both systems ran
with a maximum Java heap size of 1GB.
135
Table 3: Throughput and mean heap size.
ANNIE ANNIE-Optimized T-NE
Dataset ThroughputMemoryThroughput Memory ThroughputMemory
(KB/s) (MB) (KB/s) (MB) (KB/s) (MB)
WebCrawl 23.9 212.6 42.8 201.8 498.9 77.2
FinanceM 18.82 715.1 26.3 601.8 703.5 143.7
FinanceL 19.2 2586.2 21.1 2683.5 954.5 189.6
As shown in Fig. 9(a), even though the through-
put of ANNIE-Optimized (using the optimized trans-
ducer) increases two-fold compared to ANNIE un-
der default configuration, T-NE is between 8 and
24 times faster compared to ANNIE-Optimized. For
both systems, throughput varied with document
size. For T-NE, the relatively low throughput on
very small document sizes (less than 1KB) is due
to fixed overhead in setting up operators to pro-
cess a document. As document size increases, the
overhead becomes less noticeable.
We have observed similar trends on the rest
of the test collections. Table 3 shows that T-
NE is at least an order of magnitude faster than
ANNIE-Optimized across all datasets. In partic-
ular, on FinanceL T-NE?s throughput remains
high, whereas the performance of both ANNIE and
ANNIE-Optimized degraded significantly.
To ascertain whether the difference in perfor-
mance in the two systems is due to low-level com-
ponents such as dictionary evaluation, we per-
formed detailed profiling of the systems. The pro-
filing revealed that 8.2%, 16.2% and respectively
14.2% of the execution time was spent on aver-
age on low-level components in the case of ANNIE,
ANNIE-Optimized and T-NE, respectively, thus lead-
ing us to conclude that the observed differences
are due to SystemT?s efficient use of resources at
a macroscopic level.
Memory utilization. In theory, grammar based
systems can stream tuples through each stage
for minimal memory consumption, whereas Sys-
temT operator graphs may need to materialize in-
termediate results for the full document at certain
points to evaluate the constraints in the original
AQL. The goal of this study is to evaluate whether
this potential problem does occur in practice.
In this experiment we ran both systems with a
maximum heap size of 2GB, and used the Java
garbage collector?s built-in telemetry to measure
the total quantity of live objects in the heap over
time while annotating the different test corpora.
Fig. 9(b) plots the minimum, maximum, and mean
heap sizes with the Enronx datasets. On small doc-
uments of size up to 15KB, memory consumption
is dominated by the fixed size of the data struc-
tures used (e.g., dictionaries, FST/operator graph),
and is comparable for both systems. As docu-
ments get larger, memory consumption increases
for both systems. However, the increase is much
smaller for T-NE compared to that for both AN-
NIE and ANNIE-Optimized. A similar trend can be
observed on the other datasets as shown in Ta-
ble 3. In particular, for FinanceL, both ANNIE and
ANNIE-Optimized required 8GB of Java heap size to
achieve reasonable throughput7 , in contrast to T-
NE which utilized at most 300MB out of the 2GB
of maximum Java heap size allocation.
SystemT requires much less memory than
GATE in general due to its runtime, which monitors
data dependencies between operators and clears
out low-level results when they are no longer
needed. Although a streaming CPSL implemen-
tation is theoretically possible, in practice mecha-
nisms that allow an escape to custom code make it
difficult to decide when an intermediate result will
no longer be used, hence GATE keeps most inter-
mediate data in memory until it is done analyzing
the current document.
The BandReview Task. We conclude by briefly dis-
cussing our experience with the BandReview task
from Fig. 8. We built two versions of this anno-
tator, one in AQL, and the other using expanded
CPSL grammar. The grammar implementation
processed a 4.5GB collection of 1.05 million blogs
in 5.6 hours and output 280 reviews. In contrast,
the SystemT version (85 AQL statements) ex-
tracted 323 reviews in only 10 minutes!
6 Conclusion
In this paper, we described SystemT, a declar-
ative IE system based on an algebraic frame-
work. We presented both formal and empirical
arguments for the benefits of our approach to IE.
Our extensive experimental results show that high-
quality annotators can be built using SystemT,
with an order of magnitude throughput improve-
ment compared to state-of-the-art grammar-based
systems. Going forward, SystemT opens up sev-
eral new areas of research, including implement-
ing better optimization strategies and augmenting
the algebra with additional operators to support
advanced features such as coreference resolution.
7GATE ran out of memory when using less than 5GB of
Java heap size, and thrashed when run with 5GB to 7GB
136
References
Douglas E. Appelt and Boyan Onyshkevych. 1998.
The common pattern specification language. In TIP-
STER workshop.
Branimir Boguraev. 2003. Annotation-based finite
state processing in a large-scale nlp arhitecture. In
RANLP, pages 61?80.
D. D. Chamberlin, A. M. Gilbert, and Robert A. Yost.
1981. A history of System R and SQL/data system.
In vldb.
Amit Chandel, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE.
E. F. Codd. 1990. The relational model for database
management: version 2. Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Sec-
ond Edition). Research Memorandum CS?00?10,
Department of Computer Science, University of
Sheffield, November.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics, pages 168 ? 175.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Marin Dimitrov, Mike
Dowman, Niraj Aswani, Ian Roberts, Yaoyong
Li, and Adam Funk. 2010. Developing language
processing components with gate version 5 (a user
guide).
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, and
Shivakumar Vaithyanathan. 2008. Special issue on
managing information extraction. SIGMOD Record,
37(4).
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Scha?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
Ku?nstliche Intelligenz, 1:17?23.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In
COLING, pages 466?471.
IBM. 2010. IBM LanguageWare.
P. G. Ipeirotis, E. Agichtein, P. Jain, and L. Gravano.
2006. To search or to crawl?: towards a query opti-
mizer for text-centric tasks. In SIGMOD.
Alpa Jain, Panagiotis G. Ipeirotis, AnHai Doan, and
Luis Gravano. 2009. Join optimization of informa-
tion extraction output: Quality matters! In ICDE.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In Recent Advances in Natural Lan-
guage Processing.
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from emails:
Applying named entity recognition to informal text.
In HLT/EMNLP.
NIST. 2005. The ACE evaluation plan.
Ganesh Ramakrishnan, Sreeram Balakrishnan, and
Sachindra Joshi. 2006. Entity annotation based on
inverse index operations. In EMNLP.
Ganesh Ramakrishnan, Sachindra Joshi, Sanjeet Khai-
tan, and Sreeram Balakrishnan. 2008. Optimization
issues in inverted index-based entity annotation. In
InfoScale.
Frederick Reiss, Sriram Raghavan, Rajasekar Kr-
ishnamurthy, Huaiyu Zhu, and Shivakumar
Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE, pages
933?942.
SAP. 2010. Inxight ThingFinder.
SAS. 2010. Text Mining with SAS Text Miner.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative informa-
tion extraction using datalog with embedded extrac-
tion predicates. In vldb.
SystemT. 2010. AQL Manual.
http://www.alphaworks.ibm.com/tech/systemt.
Ken Thompson. 1968. Regular expression search al-
gorithm. pages 419?422.
UIMA. 2010. Unstructured Information Management
Architecture.
http://uima.apache.org.
137
