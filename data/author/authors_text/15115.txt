Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223?229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Query Snowball: A Co-occurrence-based Approach to Multi-document
Summarization for Question Answering
Hajime Morita1 2 and Tetsuya Sakai1 and Manabu Okumura3
1Microsoft Research Asia, Beijing, China
2Tokyo Institute of Technology, Tokyo, Japan
3Precision and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo, Japan
morita@lr.pi.titech.ac.jp, tetsuyasakai@acm.org,
oku@pi.titech.ac.jp
Abstract
We propose a new method for query-oriented
extractive multi-document summarization. To
enrich the information need representation of
a given query, we build a co-occurrence graph
to obtain words that augment the original
query terms. We then formulate the sum-
marization problem as a Maximum Coverage
Problem with Knapsack Constraints based on
word pairs rather than single words. Our
experiments with the NTCIR ACLIA ques-
tion answering test collections show that our
method achieves a pyramid F3-score of up to
0.313, a 36% improvement over a baseline us-
ing Maximal Marginal Relevance.
1 Introduction
Automatic text summarization aims at reducing the
amount of text the user has to read while preserv-
ing important contents, and has many applications
in this age of digital information overload (Mani,
2001). In particular, query-oriented multi-document
summarization is useful for helping the user satisfy
his information need efficiently by gathering impor-
tant pieces of information from multiple documents.
In this study, we focus on extractive summariza-
tion (Liu and Liu, 2009), in particular, on sentence
selection from a given set of source documents that
contain relevant sentences. One well-known chal-
lenge in selecting sentences relevant to the informa-
tion need is the vocabulary mismatch between the
query (i.e. information need representation) and the
candidate sentences. Hence, to enrich the informa-
tion need representation, we build a co-occurrence
graph to obtain words that augment the original
query terms. We call this method Query Snowball.
Another challenge in sentence selection for
query-oriented multi-document summarization is
how to avoid redundancy so that diverse pieces of
information (i.e. nuggets (Voorhees, 2003)) can be
covered. For penalizing redundancy across sen-
tences, using single words as the basic unit may not
always be appropriate, because different nuggets for
a given information need often have many words
in common. Figure 1 shows an example of this
word overlap problem from the NTCIR-8 ACLIA2
Japanese question answering test collection. Here,
two gold-standard nuggets for the question ?Sen to
Chihiro no Kamikakushi (Spirited Away) is a full-
length animated movie from Japan. The user wants
to know how it was received overseas.? (in English
translation) is shown. Each nugget represents a par-
ticular award that the movie received, and the two
Japanese nugget strings have as many as three words
in common: ??? (review/critic)?, ???? (ani-
mation)? and ?? (award).? Thus, if we use single
words as the basis for penalising redundancy in sen-
tence selection, it would be difficult to cover both of
these nuggets in the summary because of the word
overlaps.
We therefore use word pairs as the basic unit for
computing sentence scores, and then formulate the
summarization problem as a Maximum Cover Prob-
lem with Knapsack Constraints (MCKP) (Filatova
and Hatzivassiloglou, 2004; Takamura and Oku-
mura, 2009a). This problem is an optimization prob-
lem that maximizes the total score of words covered
by a summary under a summary length limit.
223
? Question
Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length
animated movie from Japan. The user wants to know how it
was received overseas.
? Nugget example 1
?????????????
National Board of Review of Motion Pictures Best Animated
Feature
? Nugget example 2
?????????????????
Los Angeles Film Critics Association Award for Best Ani-
mated Film
Figure 1: Question and gold-standard nuggets example in
NTCIR-8 ACLIA2 dataset
We evaluate our proposed method using Japanese
complex question answering test collections from
NTCIR ACLIA?Advanced Cross-lingual Informa-
tion Access task (Mitamura et al, 2008; Mitamura
et al, 2010). However, our method can easily be
extended for handling other languages.
2 Related Work
Much work has been done for generic multi-
document summarization (Takamura and Okumura,
2009a; Takamura and Okumura, 2009b; Celiky-
ilmaz and Hakkani-Tur, 2010; Lin et al, 2010a;
Lin and Bilmes, 2010). Carbonell and Goldstein
(1998) proposed the Maximal Marginal Relevance
(MMR) criteria for non-redundant sentence selec-
tion, which consist of document similarity and re-
dundancy penalty. McDonald (2007) presented
an approximate dynamic programming approach to
maximize the MMR criteria. Yih et al (2007)
formulated the document summarization problem
as an MCKP, and proposed a supervised method.
Whereas, our method is unsupervised. Filatova
and Hatzivassiloglou (2004) also formulated sum-
marization as an MCKP, and they used two types
of concepts in documents: single words and events
(named entity pairs with a verb or a noun). While
their work was for generic summarization, our
method is designed specifically for query-oriented
summarization.
MMR-based methods are also popular for query-
oriented summarization (Jagarlamudi et al, 2005;
Li et al, 2008; Hasegawa et al, 2010; Lin et al,
2010b). Moreover, graph-based methods for sum-
marization and sentence retrieval are popular (Otter-
bacher et al, 2005; Varadarajan and Hristidis, 2006;
Bosma, 2009). Unlike existing graph-based meth-
ods, our method explicitly computes indirect rela-
tionships between the query and words in the docu-
ments to enrich the information need representation.
To this end, our method utilizes within-sentence co-
occurrences of words.
The approach taken by Jagarlamudi et al (2005)
is similar to our proposed method in that it uses word
co-occurrence and dependencies within sentences in
order to measure relevance of words to the query.
However, while their approach measures the generic
relevance of each word based on Hyperspace Ana-
logue to Language (Lund and Burgess, 1996) using
an external corpus, our method measures the rele-
vance of each word within the document contexts,
and the query relevance scores are propagated recur-
sively.
3 Proposed Method
Section 3.1 introduces the Query Snowball (QSB)
method which computes the query relevance score
for each word. Then, Section 3.2 describes how
we formulate the summarization problem based on
word pairs.
3.1 Query Snowball method (QSB)
The basic idea behind QSB is to close the gap
between the query (i.e. information need rep-
resentation) and relevant sentences by enriching
the information need representation based on co-
occurrences. To this end, QSB computes a query
relevance score for each word in the source docu-
ments as described below.
Figure 2 shows the concept of QSB. Here, Q is
the set of query terms (each represented by q), R1
is the set of words (r1) that co-occur with a query
term in the same sentence, andR2 is the set of words
(r2) that co-occur with a word from R1, excluding
those that are already in R1. The imaginary root
node at the center represents the information need,
and we assume that the need is propagated through
this graph, where edges represent within-sentence
co-occurrences. Thus, to compute sentence scores,
we use not only the query terms but also the words
in R1 and R2.
Our first clue for computing a word score is
the query-independent importance of the word.
224
 q 
q 
q 
r1 
r1 
r1 
r1 r1 
r1 
r1 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
R1 
R2 
Q 
root 
r2 
r2 r2 
r2 
r2 
Figure 2: Co-occurrence Graph (Query Snowball)
We represent this base word score by sb(w) =
log(N/ctf (w)) or sb(w) = log(N/n(w)), where
ctf (w) is the total number of occurrences of w
within the corpus and n(w) is the document fre-
quency of w, and N is the total number of docu-
ments in the corpus. We will refer to these two ver-
sions as itf and idf, respectively. Our second clue
is the weight propagated from the center of the co-
occurence graph shown in Figure 1. Below, we de-
scribe how to compute the word scores for words in
R1 and then those for words in R2.
As Figure 2 suggests, the query relevance score
for r1 ? R1 is computed based not only on its base
word score but also on the relationship between r1
and q ? Q. To be more specific, let freq(w,w?)
denote the within-sentence co-occurrence frequency
for words w and w?, and let distance(w,w?) denote
the minimum dependency distance between w and
w?: A dependency distance is the path length be-
tween nodes w and w? within a dependency parse
tree; the minimum dependency distance is the short-
est path length among all dependency parse trees of
source-document sentences in which w and w? co-
occur. Then, the query relevance score for r1 can be
computed as:
sr(r1) =
?
q?Q
sb(r1)
( sb(q)
sumQ
)( freq(q, r1)
distance(q, r1) + 1.0
)
(1)
where sumQ =
?
q?Q sb(q). It can be observed that
the query relevance score sr(r1) reflects the base
word scores of both q and r1, as well as the co-
occurrence frequency freq(q, r1). Moreover, sr(r1)
depends on distance(q, r1), the minimum depen-
dency distance between q and r1, which reflects
the strength of relationship between q and r1. This
quantity is used in one of its denominators in Eq.1
as small values of distance(q, r1) imply a strong re-
lationship between q and r1. The 1.0 in the denom-
inator avoids division by zero.
Similarly, the query relevance score for r2 ? R2
is computed based on the base word score of r2 and
the relationship between r2 and r1 ? R1:
sr(r2) =
?
r1?R1
sb(r2)
( sr(r1)
sumR1
)( freq(r1, r2)
distance(r1, r2) + 1.0
)
(2)
where sumR1 =
?
r1?R1sr(r1).
3.2 Score Maximization Using Word Pairs
Having determined the query relevance score, the
next step is to define the summary score. To this end,
we use word pairs rather than individual words as the
basic unit. This is because word pairs are more in-
formative for discriminating across different pieces
of information than single common words. (Re-
call the example mentioned in Section 1) Thus, the
word pair score is simply defined as: sp(w1, w2) =
sr(w1)sr(w2) and the summary score is computed
as:
fQSBP (S) =
?
{w1,w2|w1 6=w2 and w1,w2?u and u?S}
sp(w1, w2) (3)
where u is a textual unit, which in our case is a
sentence. Our problem then is to select S to maxi-
mize fQSBP (S). The above function based on word
pairs is still submodular, and therefore we can apply
a greedy approximate algorithm with performance
guarantee as proposed in previous work (Khuller
et al, 1999; Takamura and Okumura, 2009a). Let
l(u) denote the length of u. Given a set of source
documents D and a length limit L for a sum-
mary,
Require: D,L
1: W = D,S = ?
2: while W 6= ? do
3: u = argmaxu?W
f(S?{u})?f(S)
l(u)
4: if l(u) +
?
uS?S l(uS) ? L then
5: S = S ? {u}
6: end if
7: W = W/{u}
8: end while
9: umax = argmaxu?D f(u)
10: if f(umax) > f(S) then
11: return umax
12: else return S
13: end if
where f(?) is some score function such as fQSBP .
We call our proposed method QSBP: Query Snow-
ball with Word Pairs.
225
4 Experiments
4.1 Experimental Environment
ACLIA1 ACLIA2
Development Test Test
#of questions 101 100 80*
#of avg. nuggets 5.8 12.8 11.2*
Question types DEFINITION, BIOGRAPHY,RELATIONSHIP, EVENT +WHY
Articles years 1998-2001 2002-2005
Documents Mainichi Newspaper
*After removing the factoid questions.
Table 1: ACLIA dataset statistics
We evaluate our method using Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain complex ques-
tions and their answer nuggets with weights. Ta-
ble 1 shows some statistics of the data. We use the
ACLIA1 development data for tuning a parameter
for our baseline as shown in Section 4.2 (whereas
our proposed method is parameter-free), and the
ACLIA1 and ACLIA2 test data for evaluating dif-
ferent methods The results for the ACLIA1 test data
are omitted due to lack of space. As our aim is
to answer complex questions by means of multi-
document summarization, we removed factoid ques-
tions from the ACLIA2 test data.
Although the ACLIA test collections were origi-
nally designed for Japanese QA evaluation, we treat
them as query-oriented summarization test collec-
tions. We use all the candidate documents from
which nuggets were extracted as input to the multi-
document summarizers. That is, in our problem set-
ting, the relevant documents are already given, al-
though the given document sets also occasionally
contain documents that were eventually never used
for nugget extraction (Mitamura et al, 2008; Mita-
mura et al, 2010).
We preprocessed the Japanese documents basi-
cally by automatically detecting sentence bound-
aries based on Japanese punctuation marks, but we
also used regular-expression-based heuristics to de-
tect glossary of terms in articles. As the descrip-
tions of these glossaries are usually very useful for
answering BIOGRAPHY and DEFINITION ques-
tions, we treated each term description (generally
multiple sentences) as a single sentence.
We used Mecab (Kudo et al, 2004) for morpho-
logical analysis, and calculated base word scores
sb(w) using Mainichi articles from 1991 to 2005.
We also used Mecab to convert each word to its base
form and to filter using POS tags to extract content
words. As for dependency parsing for distance com-
putation, we used Cabocha (Kudo and Matsumoto,
2000). We did not use a stop word list or any other
external knowledge.
Following the NTCIR-9 one click access task
setting1, we aimed at generating summaries of
Japanese 500 characters or less. To evaluate the
summaries, we followed the practices at the TAC
summarization tasks (Dang, 2008) and NTCIR
ACLIA tasks, and computed pyramid-based preci-
sion with an allowance parameter of C, recall, F?
(where ? is 1 or 3) scores. The value of C was
determined based on the average nugget length for
each question type of the ACLIA2 collection (Mita-
mura et al, 2010). Precision and recall are computed
based on the nuggets that the summary covered as
well as their weights. The first author of this paper
manually evaluated whether each nugget matches a
summary. The evaluation metrics are formally de-
fined as follows:
precision = min
(C ? (] of matched nuggets)
summary length , 1
)
,
recall = sum of weights over matched nuggetssum of weights over all nuggets ,
F? = (1 + ?
2) ? precision ? recall
?2 ? recision + recall .
4.2 Baseline
MMR is a popular approach in query-oriented sum-
marization. For example, at the TAC 2008 opin-
ion summarization track, a top performer in terms
of pyramid F score used an MMR-based method.
Our own implementation of an MMR-based base-
line uses an existing algorithm to maximize the fol-
lowing summary set score function (Lin and Bilmes,
2010):
fMMR(S) = ?
(
?
u?S
Sim(u, vD) +
?
u?S
Sim(u, vQ)
)
?(1 ? ?)
?
{(ui,uj)|i 6=j and ui,uj?S}
Sim(ui, uj) (4)
where vD is the vector representing the source docu-
ments, vQ is the vector representing the query terms,
Sim is the cosine similarity, and ? is a parameter.
1http://research.microsoft.com/en-us/people/tesakai/1click.aspx
226
Thus, the first term of this function reflects how the
sentences reflect the entire documents; the second
term reflects the relevance of the sentences to the
query; and finally the function penalizes redundant
sentences. We set ? to 0.8 and the scaling factor
used in the algorithm to 0.3 based on a preliminary
experiment with a part of the ACLIA1 development
data. We also tried incorporating sentence position
information (Radev, 2001) to our MMR baseline but
this actually hurt performance in our preliminary ex-
periments.
4.3 Variants of the Proposed Method
To clarify the contributions of each components, the
minimum dependency distance, QSB and the word
pair, we also evaluated the following simplified ver-
sions of QSBP. (We use the itf version by default,
and will refer to the idf version as QSBP(idf). ) To
examine the contribution of using minimum depen-
dency distance, We remove distance(w,w?) from
Eq.1 and Eq.2. We call the method QSBP(nodist).
To examine the contribution of using word pairs for
score maximization (see Section 3.2) on the perfor-
mance of QSBP, we replaced Eq.3 with:
fQSB(S) =
?
{w|w?ui and ui?S}
sr(w) . (5)
To examine the contribution of the QSB relevance
scoring (see Section 3.1) on the performance of
QSBP, we replaced Eq.3 with:
fWP (S) =
?
{w1,w2|w1 6=w2 and w1,w2?ui and ui?S}
sb(w1)sb(w2) . (6)
We will refer to this as WP. Note that this relies only
on base word scores and is query-independent.
4.4 Results
Tables 2 and 3 summarize our results. We used
the two-tailed sign test for testing statistical signif-
icance. Significant improvements over the MMR
baseline are marked with a ? (?=0.05) or a ?
(?=0.01); those over QSBP(nodist) are marked with
a ] (?=0.05) or a ]] (?=0.01); and those over QSB
are marked with a ? (?=0.05) or a ?? (?=0.01); and
those over WP are marked with a ? (?=0.05) or a
?
? (?=0.01). From Table 2, it can be observed that
both QSBP and QSBP(idf) significantly outperforms
QSBP(nodist), QSB, WP and the baseline in terms
of all evaluation metrics. Thus, the minimum depen-
dency distance, Query Snowball and the use of word
pairs all contribute significantly to the performance
of QSBP. Note that we are using the ACLIA data as
summarization test collections and that the official
QA results of ACLIA should not be compared with
ours.
QSBP and QSBP(idf) achieve 0.312 and 0.313 in
F3 score, and the differences between the two are
not statistically significant. Table 3 shows the F3
scores for each question type. It can be observed
that QSBP is the top performer for BIO, DEF and
REL questions on average, while QSBP(idf) is the
top performer for EVENT and WHY questions on
average. It is possible that different word scoring
methods work well for different question types.
Method Precision Recall F1 score F3 score
Baseline 0.076?? 0.370?? 0.116?? 0.231??
QSBP 0.107????? ]] 0.482????? ]] 0.161????? ]] 0.312????? ]]
QSBP(idf) 0.106????? ]] 0.485????? ]] 0.161????? ]] 0.313????? ]]
QSBP(nodist) 0.083??? 0.396?? 0.125?? 0.248??
QSB 0.086??? 0.400?? 0.129??? 0.253???
WP 0.053 0.222 0.080 0.152
Table 2: ACLIA2 test data results
Type BIO DEF REL EVENT WHY
Baseline 0.207? 0.251?? 0.270 0.212 0.213
QSBP 0.315?? 0.329??? 0.401? 0.258??? ]] 0.275?]
QSBP(idf) 0.304??] 0.328??? 0.397? 0.268??? 0.280??
QSBP(nodist) 0.255 0.281?? 0.329 0.196 0.212??
QSB 0.245?? 0.273?? 0.324 0.217 0.215
WP 0.109 0.037 0.235 0.141 0.161
Table 3: F3-scores for each question type (ACLIA2 test)
5 Conclusions and Future work
We proposed the Query Snowball (QSB) method for
query-oriented multi-document summarization. To
enrich the information need representation of a given
query, QSB obtains words that augment the original
query terms from a co-occurrence graph. We then
formulated the summarization problem as an MCKP
based on word pairs rather than single words. Our
method, QSBP, achieves a pyramid F3-score of up
to 0.313 with the ACLIA2 Japanese test collection,
a 36% improvement over a baseline using Maximal
Marginal Relevance.
Moreover, as the principles of QSBP are basically
language independent, we will investigate the effec-
tiveness of QSBP in other languages. Also, we plan
to extend our approach to abstractive summariza-
tion.
227
References
Wauter Bosma. 2009. Contextual salience in query-
based summarization. In Proceedings of the Interna-
tional Conference RANLP-2009, pages 39?44. Asso-
ciation for Computational Linguistics.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?98, pages 335?336. Association for Comput-
ing Machinery.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 815?824. Association for Computational
Linguistics.
Hoa Trang Dang. 2008. Overview of the tac 2008 opin-
ion question answering and summarization tasks. In
Proceedings of Text Analysis Conference.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04. Association for Computational Linguis-
tics.
Takaaki Hasegawa, Hitoshi Nishikawa, Kenji Imamura,
Genichiro Kikui, and Manabu Okumura. 2010. A
Web Page Summarization for Mobile Phones. Trans-
actions of the Japanese Society for Artificial Intelli-
gence, 25:133?143.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva
Varma. 2005. A relevance-based language modeling
approach to duc 2005. In Proceedings of Document
Understanding Conferences (along with HLT-EMNLP
2005).
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Informa-
tion Processing Letters, 70(1):39?45.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the 2000 Joint SIGDAT
conference on Empirical methods in natural language
processing and very large corpora: held in conjunc-
tion with the 38th Annual Meeting of the Association
for Computational Linguistics, volume 13, pages 18?
25. Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Emprical Methods in Natural Language Pro-
cessing (EMNLP 2004), volume 2004, pages 230?237.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008.
PolyU at TAC 2008. In Proceedings of Text Analysis
Conference.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 912?920. Association for Computa-
tional Linguistics.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2010a. Graph-
based submodular selection for extractive summariza-
tion. In Automatic Speech Recognition & Understand-
ing, 2009. ASRU 2009. IEEEWorkshop on, pages 381?
386. IEEE.
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr. 2010b.
Putting the user in the loop: interactive maximal
marginal relevance for query-focused summarization.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 305?308. Association for Computational Lin-
guistics.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ?09, pages
261?264. Association for Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Inderjeet Mani. 2001. Automatic summarization. John
Benjamins Publishing Co.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR research,
ECIR?07, pages 557?564. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki
Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song,
Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and
Noriko Kando. 2008. Overview of the NTCIR-7
ACLIA tasks: Advanced cross-lingual information ac-
cess. In Proceedings of the 7th NTCIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko
Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin,
Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee.
2010. Overview of the NTCIR-8 ACLIA tasks: Ad-
vanced cross-lingual information access. In Proceed-
ings of the 8th NTCIR Workshop.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the conference on
Human Language Technology and Empirical Methods
228
in Natural Language Processing, HLT ?05, pages 915?
922. Association for Computational Linguistics.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In First
Document Understanding Conference.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of the 12th
Conference of the European Chapter of the ACL
(EACL 2009), pages 781?789. Association for Com-
putational Linguistics.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceeding of the 18th ACM conference
on Information and knowledge management, CIKM
?09, pages 1589?1592. Association for Computing
Machinery.
Ramakrishna Varadarajan and Vagelis Hristidis. 2006.
A system for query-specific document summarization.
In Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
CIKM ?06, pages 622?631. ACM.
Ellen M. Voorhees. 2003. Overview of the TREC
2003 Question Answering Track. In Proceedings of
the Twelfth Text REtrieval Conference (TREC 2003),
pages 54?68.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and
Hisami Suzuki. 2007. Multi-document summariza-
tion by maximizing informative content-words. In
Proceedings of the 20th international joint conference
on Artifical intelligence, pages 1776?1782. Morgan
Kaufmann Publishers Inc.
229
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Subtree Extractive Summarization via Submodular Maximization
Hajime Morita
Tokyo Institute of Technology, Japan
morita@lr.pi.titech.ac.jp
Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
Ryohei Sasano
Tokyo Institute of Technology, Japan
sasano@pi.titech.ac.jp
Manabu Okumura
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 12(1 ? e?1). Ourexperiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
1 Introduction
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al, 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al, 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al, 2009; Kulik et al, 2009; Gupta
et al, 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
1023
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1? e?1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
2 Related Work
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2V ? R maps a subset S ? V to a real value.
If for any S, T ? V , f(S ? T ) + f(S ? T ) ?
f(S)+f(T ), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S ?{u})? f(S) ? f(T ?{u})? f(T ), where
T ? S ? V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1? e?1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
3 Budgeted Submodular Maximization
with Cost Function
3.1 Problem Definition
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxS?V {f(S) : c (S) ? L} , where S is a sum-
mary represented as a set of subtrees, c(?) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(?) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsetsB of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
1024
max
S?V
{
f(S) :
?
B?B
c (B ? S) ? L
}
. (1)
For example, if we add a subtree t containing
words {wa,wb,wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
3.2 Greedy Algorithm
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al (1999) and Krause
et al (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi?1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s?} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive ?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
, for any i(1 ? i ? |G|),
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function .
1: G0 ? ?
2: U ? V
3: i? 1
4: while U 6= ? do
5: si ? argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?{s})?c(Gi?1))r
6: if c({si} ?Gi?1) ? L then
7: Gi ? Gi?1 ? {si}
8: i? i + 1
9: end if
10: U ? U\{si}
11: end while
12: s?? argmaxs?V,c(s)?L f({s})
13: return Gf = argmaxS?{{s?},Gi} f(S)
Theorem 1 For a normalized monotone submod-
ular function f(?), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
f(Gf ) ?
(1
2(1? e
?1)
)
f(S?), (2)
where S? is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
3.3 Relation with Discrete Optimization
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornue?jols,
1984; Calinescu et al, 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1? e?1) was already proved by Krause andGuestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The
optimal solution is S? = {e1, e2}. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
?u ? S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
.
1025
4 Joint Model of Extraction and
Compression
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
4.1 Objective Function
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
1026
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
reward(S) = c(S)? |S|, (3)
where c(S) is the cost of summary S, and |S| is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, countS(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, qsb(w) be the query relevance score
of word w, and ? be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
f(S) =
?
w?words(S)
?
?
?
countS(w)?1?
i=0
qsb(w)di
?
?
?+ ? reward(S).
(4)
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
3The score is still submodular for the purpose of sentence
extraction.
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
4.2 Local Search for Maximal Density
Subtree
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
g(t) =
?
w?t
{gainS(w) + freqt(w)c(w)?},
gainS(w) = qsb(w)dcountS(w),
where freqt(w) is the number of ws in subtree
t, and gainS(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)n , . . . ,mdsLn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 ? mdsa and t2 ? mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
1027
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) ? g(n) + g(mdska) +
g(mdsc?k?c(n)b ), where k and c are variables. Thescore reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t ? mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o ? O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t andm
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t ?m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: MDSs.
Function: MDSs
Require: root node n
1: subt[c(n),words(n)?O] = tree(n)
2: newt = ?
3: for i ? child node of n do
4: for t ?MDSs(i) do
5: for m ? subt do
6: index = [c(t ?m), words(t ?m) ? O]
7: newtindex = argmaxj?{newtindex,t?m} g(j)8: end for
9: end for
10: subt = newt
11: end for
12: return subt
Figure 1: Maximal density subtree extraction. The
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
5 Experimental Settings
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al, 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used ?ACLIA1
test data? to tune the parameters, and evaluated our
method on ?ACLIA2 test? data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
1028
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
Table 1: Results on ACLIA2 test data.
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an ?obligatory case? or an
?adjacent case?, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University?s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and F? (where ? is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
Table 2: Effect of sentence compression.
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain ??i?V wi,j ,
which is a small fraction ? of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
F(S) = L(S) +
3?
k=1
?kRQ,k(S),
L(S) = ?
i?V
min
??
?
?
j?S
wi,j , ?
?
k?V
wi,k
??
? ,
RQ,k =
?
c?Ck
???? ?
j?S?c
( ?N
?
i?V
wi,j + (1? ?)rj,Q),
where ?, ? and ?k are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N ,
0.15N , 0.05N ) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords ???? (tell),?
??? (know),? ?? (what)? and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
6 Results
Table 1 summarizes our results. ?Subtree ex-
traction (SbE)? is our method, and ?Sentence ex-
traction (NC)? is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
1029
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
7 Conclusions and Future Work
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 ? e?1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
Appendix
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S? is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i? is the
last step before the algorithm discards a subtree
s ? S? or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s? from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi?1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(?) : 2V ? R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
?t1, t2,?te, te ? {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:?
ui?S={u1,...,u|S|} cui(Si?1) = c(S).
Lemma 1 ?X,Y ? V, f(X) ? f(Y ) +?
u?X\Y ?u(Y ), where ?u(S) = f(S ? {u}) ?
f(S).
The inequality can be derived from the definition
of submodularity. 2
Lemma 2 For i = 1, . . . , i?+1, when 0 ? r ? 1,
f(S?)?f(Gi?1)?L
r |S?|1?r
csi (Gi?1)
(f(Gi?1?{si})?f(Gi?1)),
where cu(S)=c(S?{u})?c(S).
Proof. From line 5 of Algorithm 1, we have
?u ? S?\Gi?1,
?u(Gi?1)
cu(Gi?1)r
? ?si(Gi?1)csi(Gi?1)r
.
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
1030
?T ? B, ?b ? B, b = union(T ), because we
have an equivalent tree b ? B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ? Bi. Without loss of generality,
we can replace the difference set S?\Gi?1 with
a set T ?i?1 = {b0, . . . , b|T ?i?1|} that does not con-tain any two elements extracted from the same
valid set. Thus when 0 ? r ? 1 and 0 ?
i ? i? + 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =
?T ?i?1 (Gi?1)
cT ?i?1 (Gi?1)
r , and
?bj ? T ?i?1,
?bj (Gi?1)
cbj (Gi?1)r
? ?si (Gi?1)csi (Gi?1)r . Thus,
?T ?i?1 (Gi?1) =
?
u?T ?i?1
?u(Gi?1)
? ?si (Gi?1)csi (Gi?1)r
?
u?T ?i?1
cu(Gi?1)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|
(?
u?T ?i?1
cu(Gi?1)
|T ?i?1|
)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|1?r
(?
u?T ?i?1
cu(?)
)r
? ?si (Gi?1)csi (Gi?1)r |S
?|1?rLr,
where the second inequality is from Ho?lder?s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi?1) = c({u} ?Gi?1)? c(Gi?1) ? cu(?)
and the fact that |S?| ? |S?\Gi?1| ? |T ?i?1|, and?
u?T ?i?1 cu(?) = c(T
?
i?1) ? L .
As a result, we have
?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)
? ?si(Gi?1)csi(Gi?1)r
|S?|1?rLr.
Let X = S? and Y = Gi?1. Applying Lemma
1 yields
f(S?) ? f(Gi?1) + ?u?S?\Gi?1(Gi?1).
? f(Gi?1) +
?si(Gi?1)
csi(Gi?1)
|S?|1?rLr.
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(?), for i = 1, . . . , i? + 1 and 0 ? r ? 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
f(Gi) ?
(
1?
i?
k=1
(
1? csk(Gk?1)
r
Lr|S?|1?r
))
f(S?).
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
481?490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,
and Jan Vondra?k. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740?1766.
Michele Conforti and Ge?rard Cornue?jols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 ? 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE?10, pages 246?257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366?380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
1031
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ?09, pages 545?554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ?09,
pages 323?332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 510?520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565?
587, November.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR?07, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 223?229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75?88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM?2009), pages 1147?1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
1032
