Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820?828,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Integrating Multi-level Linguistic Knowledge with a Unified Framework for
Mandarin Speech Recognition
Xinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu?
Speech and Hearing Research Center,
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing, 100871, China
{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cn
Abstract
To improve the Mandarin large vocabulary
continuous speech recognition (LVCSR), a
unified framework based approach is intro-
duced to exploit multi-level linguistic knowl-
edge. In this framework, each knowledge
source is represented by a Weighted Finite
State Transducer (WFST), and then they are
combined to obtain a so-called analyzer for in-
tegrating multi-level knowledge sources. Due
to the uniform transducer representation, any
knowledge source can be easily integrated into
the analyzer, as long as it can be encoded
into WFSTs. Moreover, as the knowledge in
each level is modeled independently and the
combination is processed in the model level,
the information inherently in each knowledge
source has a chance to be thoroughly ex-
ploited. By simulations, the effectiveness
of the analyzer is investigated, and then a
LVCSR system embedding the presented ana-
lyzer is evaluated. Experimental results reveal
that this unified framework is an effective ap-
proach which significantly improves the per-
formance of speech recognition with a 9.9%
relative reduction of character error rate on
the HUB-4 test set, a widely used Mandarin
speech recognition task.
1 Introduction
Language modeling is essential for large vocabu-
lary continuous speech recognition (LVCSR), which
aims to determine the prior probability of a supposed
word string W , p(W ). Although the word-based n-
gram language model remains the mainstream for
?Corresponding author: Xihong Wu
most speech recognition systems, the utilization of
linguistic knowledge is too limited in this model.
Consequently, many researchers have focused on
introducing more linguistic knowledge in language
modeling, such as lexical knowledge , syntax and
semantics of language (Wang and Vergyri, 2006;
Wang et al, 2004; Charniak, 2001; Roark, 2001;
Chelba, 2000; Heeman, 1998; Chelba et al, 1997).
Recently, structured language models have been
introduced to make use of syntactic hierarchi-
cal characteristics (Roark, 2001; Charniak, 2001;
Chelba, 2000). Nevertheless, the computational
complexity of decoding will be heavily increased, as
they are parser-based models. In contrast, the class-
based language model groups the words that have
similar functions of syntax or semantics into mean-
ingful classes. As a result, it handles the questions of
data sparsity and generalization of unseen event. In
practice, the part-of-speech (POS) information, cap-
turing the syntactic role of words, has been widely
used in clustering words (Wang and Vergyri, 2006;
Maltese et al, 2001; Samuelsson and Reichl, 1999).
In Heeman?s POS language model (Heeman, 1998),
the joint probability of word sequence and associ-
ated POS sequence was estimated directly, which
has been demonstrated to be superior to the condi-
tional probability previously used in the class-based
models (Johnson, 2001). Moreover, a SuperARV
language model was presented (Wang and Harper,
2002), in which lexical features and syntactic con-
straints were tightly integrated into a linguistic struc-
ture of SuperARV serving as a class in the model.
Thus, these knowledge was integrated in the rep-
resentation level, and then the joint probabilities
820
of words and corresponding SuperARVs were esti-
mated. However, in the class-based language mod-
els, words are taken as the model units, while other
units smaller or larger than words are unfeasible for
modeling simultaneously, such as the Chinese char-
acters for Chinese names.
Usually, speech recognition systems can only rec-
ognize the words within a predefined dictionary.
With the increase of unknown words, i.e., out-of-
vocabulary (OOV) words, the performance will de-
grade dramatically. This is because not only those
unknown words cannot be recognized correctly, but
the words surrounding them will be affected. Thus,
many efforts have been made to deal with the is-
sue of OOV words (Martins et al, 2006; Galescu,
2003; Bazzi and Glass, 2001), and various model
units smaller than words have been examined to rec-
ognize OOVs from speech, such as phonemes (Bazzi
and Glass, 2000a), variable-length phoneme se-
quence (Bazzi and Glass, 2001), syllable (Bazzi and
Glass, 2000b) and sub-word (Galescu, 2003). Since
the proper name is a typical category of OOV words
and usually takes a very large proportion among all
kinds of OOV words, it has been specially addressed
in (Hu et al, 2006; Tanigaki et al, 2000).
All those attempts mentioned above succeed in
utilizing linguistic knowledge in language modeling
in some degree respectively. In this study, a uni-
fied framework based approach, which aims to ex-
ploit information from multi-level linguistic knowl-
edge, is presented. Here, the Weighted Finite State
Transducer (WFST) turns to be an ideal choice for
our purpose. WFSTs were formerly introduced to
simplify the integration of models in speech recog-
nition, including acoustic models, phonetic mod-
els and word n-gram (Mohri, 1997; Mohri et al,
2002). In recent years, the WFST has been suc-
cessfully applied in several state-of-the-art speech
recognition systems, such as systems developed by
the AMI project (Hain et al, 2006), IBM (Saon et
al., 2003) and AT&T (Mohri et al, 1996), and in
various fields of natural language processing, such
as smoothed n-gram model, partial parsing (Abney,
1996), named entities recognition (Friburger and
Maurel, 2004), semantic interpretation (Raymond et
al., 2006) and machine translation (Tsukada and Na-
gata, 2004). In (Takaaki Hori and Minami, 2003),
the WFST has been further used for language model
adaptation, where language models of different vo-
cabularies that represented different styles were in-
tegrated through the framework of speech transla-
tion. In WFST-based systems, all of the models are
represented uniformly by WFSTs, and the general
composition algorithm (Mohri et al, 2000) com-
bines these representations flexibly and efficiently.
Thereby, rather than integrating the models step by
step in decoding stage, a complete search network is
constructed in advance. The combined WFST will
be more efficient by optimizing with determiniza-
tion, minimization and pushing algorithms of WF-
STs (Mohri, 1997). Besides, the researches on opti-
mizing the search space and improving WFST-based
speech recognition has been carried out, especially
on how to perform on-the-fly WFSTs composition
more efficiently (Hori et al, 2007; Diamantino Ca-
seiro, 2002).
In this study, we extend the linguistic knowledge
used in speech recognition. As WFSTs provide a
common and natural representation for lexical con-
straints, n-gram language model, Hidden Markov
Model models and context-dependency, multi-level
knowledge sources can be encoded into WFSTs un-
der the uniform transducer representation. Then this
group of WFSTs is flexibly combined together to
obtain an analyzer representing knowledge of per-
son and location names as well as POS information.
Afterwards, the presented analyzer is incorporated
into LVCSR to evaluate the linguistic correctness of
recognition candidates by an n-best rescoring.
Unlike other methods, this approach holds two
distinct features. Firstly, as all multi-level knowl-
edge sources are modeled independently, the model
units such as character, words, phrase, etc., can be
chosen freely. Meanwhile, the integration of these
information sources is conducted in the model level
rather than the representation level. This setup will
help to model each knowledge source sufficiently
and may promote the accuracy of speech recogni-
tion. Secondly, under this unified framework, it is
easy to combine additional knowledge source into
the framework with the only requirement that the
new knowledge source can be represented by WF-
STs. Moreover, since all knowledge sources are fi-
nally represented by a single WFST, additional ef-
forts are not required for decoding the new knowl-
edge source.
821
The remainder of this paper is structured as fol-
lows. In section 2, we introduce our analyzer in de-
tail, and incorporate it into a Mandarin speech recog-
nition system. In section 3, the simulations are per-
formed to evaluate the analyzer and test its effective-
ness when being applied to LVCSR. The conclusion
appears in section 4.
2 Incorporation of Multi-level linguistic
knowledge in LVCSR
In this section, we start by giving a brief descrip-
tion on WFSTs. Then some special characteristics
of Chinese are investigated, and the model units are
fixed. Afterwards, each knowledge source is rep-
resented with WFSTs, and then they are combined
into a final WFST, so-called analyzer. At last, this
analyzer is incorporated into Mandarin LVCSR.
2.1 Weighted Finite State Transducers
The Weighted Finite State Transducer (WFST) is the
generalization of the finite state automata, in which,
besides of an input label, an output label and a
weight are also placed on each transition. With these
labels, a WFST is capable of realizing a weighted re-
lation between strings. In our system, log probabili-
ties are adopted as transition weights and the relation
between two strings is associated with a weight indi-
cating the probability of the mapping between them.
Given a group of WFSTs, each of which models a
stage of a mapping cascade, the composition opera-
tion provides an efficient approach to combine them
into a single one (Mohri et al, 2002; Mohri et al,
1996). In particular, for two WFSTs R and S, the
composition T = RoS represents the composition
of relations realized by R and S. The combination
is performed strictly on R?s output and S?s input. It
means for each path in T, mapping string r to string
s, there must exist a path mapping r to some string
t in R and a path mapping t to s in S. Decoding on
the combined WFST enables to find the joint opti-
mal results for multi-level weighted relations.
2.2 Model Unit Selection
This study primarily takes the person and location
names as well as the POS information into account.
To deal with Chinese OOV words, different from
the western language in which the phoneme, sylla-
ble or sub-word are used as the model units (Bazzi
and Glass, 2000a; Bazzi and Glass, 2000b; Galescu,
2003), Chinese characters are taken as the basic
units. In general, a person name of Han nation-
ality consists of a surname and a given name usu-
ally with one or two characters. Surnames com-
monly come from a fixed set that has been histori-
cally used. According to a recent investigation on
surnames involving 296 million people, 4100 sur-
names are found, and 129 most used surnames ac-
count for 87% (conducted by the Institute of Genet-
ics and Developmental Biology, Chinese Academy
of Sciences). In contrast, the characters used in
given names can be selected freely, and in many situ-
ations, some commonly used words may also appear
in names, such as ???? (victory) and ???? (the
Changjiang River). Therefore, both Chinese charac-
ters and words are considered as model units in this
study, and a word re-segmentation process on recog-
nition hypotheses is necessary, where an n-gram lan-
guage model based on word classes is adopted.
2.3 Representation and Integration of
Multi-level Knowledge
In this work, we ignore the word boundaries of n-
best hypotheses and perform a word re-segmentation
for names recognition. Given an input Chinese
character, it is encoded by a finite state acceptor
FSAinput. For example, the input ???????
(while synthesizing molecule) is represented as in
Figure 1(a). Then a dictionary is represented by a
50 321
0
?:??:?
?:??
?:?
(a)
(b)
4
?:?
?:? ?:? ?:?
?:??
?:?
?:?
?:?
?:? ?:?
?:??
?:??
?? ? ? ?
1 3
6
5
4
2
10
9
8
7
Figure 1: (a) is an example of the FSA representing a
given input; (b) is the FST representing a toy dictionary.
822
transducer with empty weights, denoted as FSTdict.
Figure 1(b) illustrates a toy dictionary listed in Ta-
ble 1, in which a successful path encodes a mapping
from a Chinese character sequence to some word
in the dictionary. In practice, all Chinese charac-
Chinese Words English Words
?? synthesize
?? element
?? molecule
?? the period of the day from11 p.m.to l a.m.
? together
? present
Table 1: The Toy dictionary
ters should appear in the dictionary for further in-
corporating models of names. Then the combination
of FSAinput and FSTdict, FSTseg = FSAinput ?
FSTdict, will result in a WFST embracing all the
possible candidate segmentations. Afterwards an n-
gram language model based on word classes is used
to weight the candidate segmentations. As in Fig-
ure 2, a toy bigram with three words is depicted by
WFSTn?gram, and the word classes are defined in
Table 2. Here, both in the training and test stages,
0
w1/un(w1)
w2/un(w2)
w3/un(w3)
4
w3/un(w3)
?/back(w1)
w1/un(w1)
?/back(w3)
w2/un(w2)
?/back(w2)
w1/bi(w2,w1)
w2/bi(w3,w2)
w2/bi(w1,w2)
w3/bi(w2,w3)
w1/bi(w3,w1)
w3/bi(w1,w3)
2
3
1
Figure 2: The WFST representing a toy bigram language
model, in which un(w1) denotes the unigram of w1;
bi(w1, w2) and back(w1) respectively denotes the bi-
gram of w2 and the backoff weight given the word history
w1.
the strings of numbers or letters in sentences are ex-
Classes Description
wi Each word wi listed in the dictionary
CNAME Person names of Han nationality
TNAME Translated person names
LOC Location names
NUM Number expressions
LETTER Letter strings
NON Other non Chinese character strings
BEGIN Beginning of sentence
END End of sentence
Table 2: The Definition of word classes
tracted according to the rules, and then substituted
with the class tags, ?NUM? and ?LETTER? respec-
tively. At the same time, the words, such as ????
and ?A??, are replaced with ?NUM?? and ?LET-
TER?? in the dictionary. In addition, name classes,
including ?CNAME?, ?TNAME? and ?LOC?, will
be set according to names recognition.
Hidden Markov Models (HMMs) are adopted
both for names recognition and POS tagging. Here,
each HMM is represented with two WFSTs. Tak-
ing the POS tagging as an example, the toy POS
WFSTs with 3 different tags are illustrated in Fig-
ure 3. The emission probability of a word by a POS,
(P (word/pos)), is represented as in Figure 3(a),
and the bigram transition probabilities between POS
tags are represented as in Figure 3(b), similar to the
word n-gram. In terms of names recognition, the
HMM states correspond to 30 role tags of names,
some for model units of Chinese characters, such as
surname, the first or second character of a given per-
son name with two characters, the first or last charac-
ter of a location name and so on, but others for model
units of words, such as the word before or after a
name, the words in a name and so on. When rec-
ognizing the person names, since there is a big dif-
ference between the translated names and the names
of Han nationality, two types of person names are
modeled separately, and substituted with two differ-
ent class tags in the segmentation language model,
as ?TNAME? and ?CNAME?. Some rules, which
can be encoded into WFSTs, are responsible for the
transformation from a role sequence to correspond-
ing name class (for example, a role sequence might
consist of the surname, the first character of the
823
0pos1/un(pos1)
pos2/un(pos2)
pos3/un(pos3)
pos1/bi(pos2,pos1)
pos3/bi(pos2,pos3)
pos2/bi(pos1,pos2)
pos3/bi(pos1,pos3)
pos1/bi(pos3,pos1)pos2/bi(pos3,pos2)
(a)
(b)
word: pos/p(word/pos)
3
2
1
0
Figure 3: The toy POS WFSTs. (a) is the WFST rep-
resenting the relationship between the word and the pos;
(b) is the WFSA representing the bigram transition prob-
abilities between POS tags
given name, and the second character of the given
name, which will be transformed to ?CNAME? in
FSTseg). Hence, taking names recognition into ac-
count, a WFST, including all possible segmentations
as well as recognized candidates of names, can be
obtained as below, denoted as WFSTwords:
FSAinput ? FSTdict ?WFSTne ?WFSAn?gram
(1)
POS information is integrated as follows.
(? ?WFSTwords) ?WFSTPOS (2)
Consequently, the desired analyzer, a combined
WFST that represents multi-level linguistic knowl-
edge sources, has been obtained.
2.4 Incorporation in LVCSR
The presented analyzer models linguistic knowledge
at different levels, which will be useful to find an
optimal words sequence among a large number of
speech recognition hypotheses. Thus in this re-
search, the analyzer is incorporated after the first
pass recognition, and the n-best hypotheses are
reranked according to the total path scores adjusted
with the analyzer scores as follows.
W? = argmax
W
?
??
log (PAM (O|W ))
+? ? log (PLM (W ))
+? ? log (PAnalyzer (W ))
?
??
(3)
where PAM (O|W ) and PLM (W ) are the acoustic
and language scores produced in first pass decoding,
and PAnalyzer (W ) reflects the linguistic correctness
of one hypothesis scored by the analyzer. Through
the reranking paradigm, a new best sentence hypoth-
esis is obtained.
3 Simulation
Under the unified framework, multi-level linguistic
knowledge is represented by the analyzer as men-
tioned above. To guarantee the effectiveness of
the introduced framework in integrating knowledge
sources, the analyzer is evaluated in this section.
Then the experiments using an LVCSR system in
which the analyzer is embedded are performed.
3.1 Analyzer Evaluation
Considering the function of the analyzer, cascaded
subtasks of word segmentation, names recognition
and POS tagging can be processed jointly, while
they are traditionally handled in a pipeline manner.
Hence, a comparison between the analyzer and the
pipeline system can be used to evaluate the effec-
tiveness of the introduced framework for knowledge
integration. As illustrated in Figure 4, two systems
based on the presented analyzer and the pipeline
manner are constructed respectively.
The evaluation data came from the People?s Daily
of China in 1998 from January to June (annotated by
the Institute of Computational Linguistics of Peking
University1), among which the January to May data
was taken as the training set, and the June data was
taken as the test set (consisted of 21,143 sentences
and about 1.2 million words). The first two thou-
sand sentences from the June data were extracted
as the development set, used to fix the composition
weight ? in equation 2. A dictionary including about
113,000 words was extracted from the training data,
1http://icl.pku.edu.cn/icl res/
824
input d ict ne n gramF SA F ST W F ST W F ST q q q
Decode
The best segmentation
posWFST
CCompose ompose
Decode Decode
Pipeline System Presented Analyzer
output output
Figure 4: The pipeline system vs The analyzer
in which a person or location name was accounted
as a word in vocabulary, only when the number of
its appearances was no less than three.
In Figure 5, the analyzer is compared with the
pipeline system, where the analyzer outperforms the
pipeline manner on all the subtasks in terms of F1-
score metric. Furthermore to detect the differences,
the statistical significance test using approximate
randomization approach (Yeh, 2000) is done on the
word segmentation results. Since there are more
than 21,000 sentences in the test set, which is not
appropriate for approximate randomization test, ten
sets (500 sentences for each) are randomly selected
from the test corpus. For each set, we run 1048576
shuffles twice and calculate the significance level p-
value according to the shuffled results. It has been
shown that all p-value are less than 0.001 on the ten
sets. Accordingly the improvement is statistically
significant. Actually, this significant improvement
is reasonable, since the joint processing avoids error
propagation and provides the opportunity of shar-
ing information between different level knowledge
sources. The superiority of this analyzer also shows
that the integration of multi-level linguistic knowl-
edge under the unified framework is effective, which
may lead to improved LVCSR.
95.9
91.1
89.9
96.8
91.8
88.5
90.9
88
92
96 Pipeline Analyzer
Integrated Analyzer
83.3
80
84
Word Segmentation POS Tagging Person Name Recognition Location Name Recognition
Figure 5: The Performance comparison between the
pipeline system and the analyzer. The system perfor-
mances are measured with the F1-score in the tasks
of word segmentation, POS tagging, the person names
recognition and the location names recognition.
3.2 Experimental Setup for Mandarin Speech
Recognition
In the baseline speech recognition system, the
acoustic models consisted of context-dependent
Initial-Final models, in which the left-to-right model
topology was used to represent each unit. Accord-
ing to the phonetic structures, the number of states
in each model was set to 2 or 3 for initials, and 4
or 5 for tonal finals. Each state was trained to have
32 Gaussian mixtures. The used 39-dimension fea-
ture vector comprised 12 MFCC coefficients, en-
ergy, and their first-order and second-order deltas.
Since in this work we focused on modeling knowl-
edge of language in Mandarin LVCSR, only clean
male acoustic models were trained with a speech
database that contained about 360 hours speech of
over 750 male speakers. This training data was
picked up from three continuous Mandarin speech
corpora: the 863-I, 863-II and Intel corpora. The
brief information about these three speech corpora
was listed in Table 3. As in this work, the eval-
uation data was the 1997 HUB-4 Mandarin broad-
cast news evaluation data (HUB-4 test set), to bet-
ter fit this task, the acoustic models were adapted
by the approach of maximum a posterior (MAP)
adaptation. The adaption data was drawn from the
HUB4 training set, excluding the HUB-4 develop-
825
Corpus Speakers Amount of Speech
(hours)
863-I (male) 83 56.67
863-II(male) 120 78.08
Intel (male) 556 227.30
total 759 362.05
Table 3: The information of the speech training data
ing set, where only the cleaned male speech data
(data under condition f0 defined as (Doddington,
1996)) was used. The partition for the clean data
was done with the acoustic segmentation software
CMUseg 0.52 (Siegler et al, 1997), and finally 8.6
hours adaptation data was obtained.
The language model was a word-based trigram
built on 60,000 words entries and trained with a cor-
pus about 1.5 billion characters. The training set
consisted of broadcast news data from the Xinhua
News Agency released by LDC (Xinhua part of Chi-
nese Gigaword), seven years data of People?s Daily
of China from 1995 to 2002 released by People?s
Daily Online3, and some other data from news web-
sites, such as yahoo, sina and so on.
In addition, the analyzer incorporated in speech
recognition was trained with a larger corpus from
People?s Daily of China, including the data in 1998
from January to June and the data in 2000 from
January to November (annotated by the Institute
of Computational Linguistics of Peking University).
The December data in 2000 was taken as the devel-
opment set used to fix the composition weight ? in
equation 2.
3.3 Experimental Results
In our experiments, the clean male speech data from
the Hub-4 test set was used, and 238 sentences were
finally extracted for testing. The weight of the ana-
lyzer was empirically derived from the development
set, including 649 clean male sentences from the de-
vSet of HUB-4 Evaluation. The recognition results
are shown in Table 4. The baseline system has a
character error rate (CER) of 14.85%. When the an-
alyzer is incorporated, a 9.9% relative reduction is
2Acoustic segmentation software downloaded from
http://www.nist.gov/speech/tools/CMUseg 05targz.htm.
3http://www.people.com.cn
System Err. Sub. Del. Ins.
Baseline 14.85 13.02 0.76 1.07
Analyzer 13.38 11.78 1.00 0.60incorporation
Table 4: The Speech recognition results
achieved. Furthermore, we ran the statistical signif-
icance test to detect the performance improvement,
in which the approximate randomization approach
(Yeh, 2000) was modified to output the significance
level, p-value, for the CER metric. The p-levels pro-
duced through two rounds of 1048576 shuffles are
0.0058 and 0.0057 respectively, both less than 0.01.
Thus the performance improvement imposed by the
utilization of the analyzer is statistically significant.
4 Conclusion
Addressing the challenges of Mandarin large vocab-
ulary continuous speech recognition task, within the
unified framework of WFSTs, this study presents
an analyzer integrating multi-level linguistic knowl-
edge. Unlike other methods, model units, such as
characters and words, can be chosen freely in this
approach since multi-level knowledge sources are
modeled independently. As a consequence, the fi-
nal analyzer can be derived from the combination
of better optimized models based on proper model
units. Along with two level knowledge sources, i.e.,
the person and location names as well as the part-of-
speech information, the analyzer is built and evalu-
ated by a comparative simulation. Further evaluation
is also conducted on an LVCSR system in which the
analyzer is embedded. Experimental results consis-
tently reveal that the approach is effective, and suc-
cessfully improves the performance of speech recog-
nition by a 9.9% relative reduction of character error
rate on the HUB-4 test set. Also, the unified frame-
work based approach provides a property of integrat-
ing additional linguistic knowledge flexibly, such as
organization name and syntactic structure. Further-
more, the presented approach has a benefit of ef-
ficiency that additional efforts are not required for
decoding as new knowledge comes, since all knowl-
edge sources are finally encoded into a single WFST.
826
Acknowledgments
The work was supported in part by the National
Natural Science Foundation of China (60435010;
60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the Na-
tional Key Basic Research Program of China
(2004CB318005), and the New-Century Training
Program Foundation for the Talents by the Ministry
of Education of China.
References
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4):337?344.
Issam Bazzi and James R. Glass. 2000a. Modeling out-
of-vocabulary words for robust speech recognition. In
Proc. of 6th International Conference on Spoken Lan-
guage Processing, pages 401?404, Beijing, China, Oc-
tober.
Issam Bazzi and James Glass. 2000b. Heterogeneous
lexical units for automatic speech recognition: prelim-
inary investigations. In Proc. of ICASSP, pages 1257?
1260, Istanbul, Turkey, June.
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Proc. of EUROSPEECH, pages 61?64, Aal-
borg, Denmark, September.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of ACL, pages 116?123,
Toulouse, France, July.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, and Dekai Wu. 1997. Structure and performance
of a dependency language model. In Proc. of EU-
ROSPEECH, pages 2775?2778, Rhodes, Greece.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns Hop-
kins University.
Isabel Trancoso Diamantino Caseiro. 2002. Using dy-
namic WFST composition for recognizing broadcast
news. In Proc. of ICSLP, pages 1301?1304, Denver,
Colorado, USA, September.
George Doddington. 1996. The 1996 hub-
4 annotation specification for evaluation of
speech recognition on broadcast news. In
ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.
N. Friburger and D. Maurel. 2004. Finite-state trans-
ducer cascades to extract named entities in texts. The-
oretical Computer Science, 313(1):93?104.
Lucian Galescu. 2003. Recognition of out-of-vocabulary
words with sub-lexical language models. In Proc.
of EUROSPEECH, pages 249?252, Geneva, Switzer-
land, September.
Thomas Hain, Lukas Burget, John Dines, Giulia Garau,
Martin Karafiat, Mike Lincoln, Jithendra Vepa, and
Vincent Wan. 2006. The AMI meeting transcription
system: Progress and performance. In Proc. of Rich
Transcription 2006 Spring Meeting Recognition Eval-
uation.
Peter A. Heeman. 1998. Pos tagging versus classes in
language modeling. In Proc. of the 6th Workshop on
very large corpora, pages 179?187, Montreal, Canada.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring in
extremely large vocabulary continuous speech recog-
nition. IEEE Transactions on audio, speech, and lan-
guage processing, 15(4):1352?1365.
Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and
Yoshinori Sagisaka. 2006. Language modeling of
chinese personal names based on character units for
continuous chinese speech recognition. In Proc. of
INTERSPEECH, pages 249?252, Pittsburgh, USA,
September.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL, pages
322 ? 329, Toulouse, France.
G. Maltese, P. Bravetti, H. Cr?py, B. J. Grainger, M. Her-
zog, and F. Palou. 2001. Combining word- and
class-based language models: A comparative study in
several languages using automatic and manual word-
clustering techniques. In Proc. of EUROSPEECH,
pages 21?24, Aalborg, Denmark, September.
Ciro Martins, Antonio Texeira, and Joao Neto. 2006.
Dynamic vocabulary adaptation for a daily and real-
time broadcast news transcription system. In Proc. of
Spoken Language Technology Workshop, pages 146?
149, December.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17?32.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehrya Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
827
Christan Raymond, Fre de ric Be chet, Renato D. Mori,
and Ge raldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288?304.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Christer Samuelsson and Wolfgang Reichl. 1999. A
class-based language model for large-vocabulary
speechrecognition extracted from part-of-speech
statistics. In Proc. of ICASSP, pages 537?540,
Phoenix, Arizona, USA, March.
George Saon, Geoffrey Zweig, Brain KingsBury, Lidia
Mangu, and Upendra Canudhari. 2003. An architec-
ture for rapid decoding of large vocabulary conversa-
tional speech. In Proc. of Eurospeech, pages 1977?
1980, Geneva, Switzerland, September.
Matthew A. Siegler, Uday Jain, Bhiksha Raj, and
Richard M. Stern. 1997. Automatic segmentation,
classification and clustering of broadcast news audio.
In Proc. of DARPA Speech Recognition Workshop,
pages 97?99, Chantilly, Virginia, February.
Daniel Willett Takaaki Hori and Yasuhiro Minami.
2003. Language model adaptation using WFST-based
speaking-style translation. In Proc. of ICASSP, pages
I.228?I.231, Hong Kong, April.
Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori
Sagisaka. 2000. A hierarchical language model incor-
porating class-dependent word models for oov words
recognition. In Proc. of 6th International Conference
on Spoken Language Processing, pages 123?126, Bei-
jing, China, October.
Hajime Tsukada and Masaaki Nagata. 2004. Efficient
decoding for statistical machine translation with a fully
expanded WFST model. In Proc. of EMNLP, pages
427?433, Barcelona, Spain, July.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. of
EMNLP, pages 238?247, Philadelphia, USA, July.
Wen Wang and Dimitra Vergyri. 2006. The use of word
n-grams and parts of speech for hierarchical cluster
language modeling. In Proc. of ICASSP, pages 1057?
1060, Toulouse, France, May.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pages 261?264, Montreal, Canada, May.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proc. of
COLING, pages 947?953, Saarbr?cken, August.
828
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 269?272,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Text Segmentation with LDA-Based Fisher Kernel
Qi Sun, Runxin Li, Dingsheng Luo and Xihong Wu
Speech and Hearing Research Center, and
Key Laboratory of Machine Perception (Ministry of Education)
Peking University
100871, Beijing, China
{sunq,lirx,dsluo,wxh}@cis.pku.edu.cn
Abstract
In this paper we propose a domain-
independent text segmentation method,
which consists of three components. Latent
Dirichlet alocation (LDA) is employed to
compute words semantic distribution, and we
measure semantic similarity by the Fisher
kernel. Finally global best segmentation is
achieved by dynamic programming. Experi-
ments on Chinese data sets with the technique
show it can be effective. Introducing latent
semantic information, our algorithm is robust
on irregular-sized segments.
1 Introduction
The aim of text segmentation is to partition a doc-
ument into a set of segments, each of which is co-
herent about a specific topic. This task is inspired
by problems in information retrieval, summariza-
tion, and language modeling, in which the ability
to provide access to smaller, coherent segments in
a document is desired.
A lot of research has been done on text seg-
mentation. Some of them utilize linguistic criteria
(Beeferman et al, 1999; Mochizuki et al, 1998),
while others use statistical similarity measures to
uncover lexical cohesion. Lexical cohesion meth-
ods believe a coherent topic segment contains parts
with similar vocabularies. For example, the Text-
Tiling algorithm, introduced by (Hearst, 1994), as-
sumes that the local minima of the word similarity
curve are the points of low lexical cohesion and thus
the natural boundary candidates. (Reynar, 1998)
has proposed a method called dotplotting depending
on the distribution of word repetitions to find tight
regions of topic similarity graphically. One of the
problems with those works is that they treat terms
uncorrelated, assigning them orthogonal directions
in the feature space. But in reality words are corre-
lated, and sometimes even synonymous, so that texts
with very few common terms can potentially be on
closely related topics. So (Choi et al, 2001; Brants
et al, 2002) utilize semantic similarity to identify
cohesion. Unsupervised models of texts that capture
semantic information would be useful, particularly
if they could be achieved with a ?semantic kernel?
(Cristianini et al, 2001) , which computes the simi-
larity between texts by also considering relations be-
tween different terms. A Fisher kernel is a function
that measures the similarity between two data items
not in isolation, but rather in the context provided
by a probability distribution. In this paper, we use
the Fisher kernel to describe semantic information
similarity. In addition, (Fragkou et al, 2004; Ji and
Zha, 2004) has treated this task as an optimization
problem with global cost function and used dynamic
programming for segments selection.
The remainder of the paper is organized as fol-
lows. In section 2, after a brief overview of our
method, some key aspects of the algorithm are de-
scribed. In section 3, some experiments are pre-
sented. Finally conclusion and future research di-
rections are drawn in section 4.
2 Methodology
This paper considers the sentence to be the smallest
unit, and a block b is the segment candidate which
consists of one or more sentences. We employ LDA
269
model (Blei et al, 2003) in order to find out latent
semantic topics in blocks, and LDA-based Fisher
kernel is used to measure the similarity of adjacent
blocks. Each block is then given a final score based
on its length and semantic similarity with its previ-
ous block. Finally the segmentation points are de-
cided by dynamic programming.
2.1 LDA Model
We adopt LDA framework, which regards the cor-
pus as mixture of latent topics and uses document as
the unit of topic mixtures. In our method, the blocks
defined in previous paragraph are regarded as ?doc-
uments? in LDA model.
The LDA model defines two corpus-level parame-
ters ? and ?. In its generative process, the marginal
distribution of a document p(d|?, ?) is given by the
following formula:
?
p(?|?)(
N?
n=1
?
k
p(zk|?d)p(wn|zk, ?))d?
where d is a word sequence (w1, w2, ...wN ) of
length N . ? parameterizes a Dirichlet distribution
and derives the document-related random variable
?d, then we choose a topic zk, k ? {1...K} from the
multinomial distribution of ?d. Word probabilities
are parameterized by a k?V matrix ? with V being
the size of vocabulary and ?vk = P (w = v|zk). We
use variational EM (Blei et al, 2003) to estimate the
parameters.
2.2 LDA-Based Fisher Kernel
In general, a kernel function k(x, y) is a way of mea-
suring the resemblance between two data items x
and y. The Fisher kernel?s key idea is to derive a ker-
nel function from a generative probability model. In
this paper we follow (Hofmann, 2000) to consider
the average log-probability of a block, utilizing the
LDA model. The likelihood of b is given by:
l(b) =
N?
i=1
P? (wi|b) log
K?
k=1
?wik?(k)b
where the empirical distribution of words in the
block P? (wi|b) can be obtained from the number of
word-block co-occurrence n(b, wi), normalized by
the length of the block.
The Fisher kernel is defined as
K(b1, b2) = 5T? l(b1)I?1 5? l(b2)
which engenders a measure of similarity between
any two blocks b1 and b2. The derivation of the
kernel is quite straightforward and following (Hof-
mann, 2000) we finally have the result:
K(b1, b2) = K1(b1, b2) +K2(b1, b2), with
K1(b1, b2) =
?
k
?(k)b1 ?
(k)
b2 /?(k)corpus
K2(b1, b2) =
?
i P? (wi|b1)P? (wi|b2)
?
k
P (zk|b1,wi)P (zk|b2,wi)
P (wi|zk)
where K1(b1, b2) is a measure of how much b1 and
b2 share the same latent topic, taking synonymy
into account. And K2(b1, b2) is the traditional inner
product of common term frequencies, but weighted
by the degree to which these terms belong to the
same latent topic, taking polysemy into account.
2.3 Cost Function and Dynamic Programming
The local minima of LDA-based Fisher kernel sim-
ilarities indicate low semantic cohesion and seg-
mentation candidates, which is not enough to get
reasonably-sized segments. The lengths of segmen-
tation candidates have to be considered, thus we
build a cost function including two parts of infor-
mation. Segmentation points can be given in terms
of a vector ~t = (t0, ..., tm, ..., tM ), where tm is the
sentence label with m indicating the mth block. We
define a cost function as follows:
J(~t;?) =
M?
m=1
?F (ltm+1,tm+1)
+ K(btm?1+1,tm , btm+1,tm+1)
where F (ltm+1,tm+1) is equal to
(ltm+1,tm+1??)2
2?2 and
ltm+1,tm+1 is equal to tm+1?tm indicating the num-
ber of sentences in block m. The LDA-based ker-
nel function measures similarity of block m? 1 and
block m, where block m?1 spans sentence tm?1+1
to tm and block m spans sentence tm + 1 to tm+1
The cost function is the sum of the costs of as-
sumed unknown M segments, each of which is
made up of the length probability of block m and the
similarity score of block m with its previous block
m ? 1. The optimal segmentation ~t gives a global
minimum of J(~t;?).
270
3 Experiments
3.1 Preparation
In our experiments, we evaluate the performance of
our algorithms on Chinese corpus. With news docu-
ments from Chinese websites, collected from 10 dif-
ferent categories, we design an artificial test corpus
in the similar way of (Choi, 2000), in which we
take each n-sentence document as a coherent topic
segment, randomly choose ten such segments and
concatenate them as a sample. Three data sets, Set
3-5, Set 13-15 and Set 5-20, are prepared in our ex-
periments, each of which contains 100 samples. The
data sets? names are represented by a range number
n of sentences in a segment.
Due to generality, we take three indices to eval-
uate our algorithm: precision, recall and error rate
metric (Beeferman et al, 1999) . And all exper-
imental results are averaged scores generated from
the individual results of different samples. In order
to determine appropriate parameters, some hold-out
data are used.
We compare the performance of our methods with
the algorithm in (Fragkou et al, 2004) on our test
set. In particular, the similarity representation is a
main difference between those two methods. While
we pay attention to latent topic information behind
words of adjacent blocks, (Fragkou et al, 2004) cal-
culates word density as the similarity score function.
3.2 Results
In order to demonstrate the improvement of LDA-
based Fisher kernel technique in text similarity eval-
uation, we omit the length probability part in the cost
function and compare the LDA-based Fisher kernel
and the word-frequency cosine similarity by the er-
ror rate Pk of segmenting texts. Figure 1 shows
the error rates for different sets of data. On av-
erage, the error rates are reduced by as much as
about 30% over word-frequency cosine similarity
with our methods, which shows Fisher kernel sim-
ilarity measure,with latent topic information added
by LDA, outperforms traditional word similarity
measure. The performance comparisons drawn from
Set 3-5 and Set 13-15 indicates that our similarity al-
gorithm can uncover more descriptive statistics than
traditional one especially for segments with less sen-
tences due to its prediction on latent topics.
set 3-5 set  13-15 set 5-20
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
P
k
 LDA-based Fisher kernel
 Word-Frequency Cosine Similarity
Figure 1: Error Rate Pk on different data sets with differ-
ent similarity metrics.
In the cost function, there are three parameters ?
, ? and ?. We determine appropriate ? and ? with
hold-out data. For the value of ?, we take it between
0 and 1 because the length part is less important than
the similarity part according to our preliminary ex-
periments. We design the experiment to study ??s
impact on segmentation by varying it over a certain
range. Experimental results in Figure 2 show that
the reduce of error rate achieved by our algorithm
is in a range from 14.71% to 53.93%. Set 13-15
achieves best segmentation performance, which in-
dicates the importance of text structure: it is easier
to segment the topic with regular length and more
sentences. The performance on Set 5-20 obtains the
best improvement with our methods, which illus-
trates that LDA-based Fisher kernel can express text
similarity more exactly than word density similarity
on irregular-sized segments.
Table 1: Evaluation against different algorithms on Set
5-20.
Algo. Pk Recall Precision
TextTiling 0.226 66.00% 60.72 %
P. Fragkou Algo. 0.344 69.00% 37.92 %
Our Algo. 0.205 59.00% 62.27 %
While most experiments of other authors were
taken on short regular-sized segments which was
firstly presented by (Choi, 2000), we use compar-
atively long range of segments, Set 5-20, to evaluate
different algorithms. Table 1 shows that, in terms of
271
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
lambda
Pk
Set 3?5Set 13?15Set 5?20Set 3?5Set 13?15Set 5?20
Figure 2: Error Rate Pk when the ? changes. There are
two groups of lines, the solid lines representing algorithm
of (Fragkou et al, 2004) while the dash ones indicate
performance of our algorithm, and each line in a group
shows error rates in different data sets.
Pk, our algorithm employing dynamic programming
as P. Fragkou Algo. achieves the best performance
among those three. As for long irregular-sized text
segmentation, although local even-sized blocks sim-
ilarity provides more exact information than the sim-
ilarity between global irregular-sized texts, with the
consideration of latent topic information, the latter
will perform better in the task of text segmentation.
Though the performance of the proposed method is
not superior to TextTiling method, it avoids thresh-
olds selection, which makes it robust in applications.
4 Conclusions and Future Work
We present a new method for topic-based text seg-
mentation that yields better results than previously
methods. The method introduces a LDA-based
Fisher kernel to exploit text semantic similarities and
employs dynamic programming to obtain global op-
timization. Our algorithm is robust and insensitive
to the variation of segment length. In the future,
we plan to investigate more other similarity mea-
sures based on semantic information and to deal
with more complicated segmentation tasks. Also,
we want to exam the factor importance of similar-
ity and length in this text segmentation task.
Acknowledgments
The authors would like to thank Jiazhong Nie for his help
and constructive suggestions. The work was supported
in part by the National Natural Science Foundation of
China (60435010; 60535030; 60605016), the National
High Technology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the National
Key Basic Research Program of China (2004CB318005),
and the New-Century Training Program Foundation for
the Talents by the Ministry of Education of China.
References
Doug Beeferman, Adam Berger and John D. Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177?210.
David M. Blei and Andrew Y. Ng and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of machine
Learning Research 3: 993?1022.
Thorsten Brants, Francine Chen and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. CIKM
?02211?218
Freddy Choi, Peter Wiemer-Hastings and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. Proceedings of 6th EMNLP, 109?117.
Freddy Y. Y. Choi. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. Proceedings of
NAACL-00.
Nello Cristianini, John Shawe-Taylor and Huma Lodhi.
2001. Latent Semantic Kernels. Proceedings of
ICML-01, 18th International Conference on Machine
Learning 66?73.
Pavlina Fragkou, Petridis Vassilios and Kehagias Athana-
sios. 2004. A Dynamic Programming Algorithm for
Linear Text Segmentation. J. Intell. Inf. Syst., 23(2):
179?197.
Marti Hearst. 1994. Multi-Paragraph Segmentation of
Expository Text. Proceedings of the 32nd. Annual
Meeting of the ACL, 9?16.
Thomas Hofmann. 2000. Learning the Similarity of
Documents: An Information-Geometric Approach to
Document Retrieval and Categorization. Advances in
Neural Information Processing Systems 12: 914?920.
Xiang Ji and Hongyuan Zha. 2003. Domain-
Independent Text Segmentation Using Anisotropic
Diffusion and Dynamic Programming. Proceedings
of the 26th annual international ACM SIGIR Confer-
ence on Research and Development in Informaion Re-
trieval, 322?329.
Hajime Mochizuki, Takeo Honda and Manabu Okumura.
1998. Text Segmentation with Multiple Surface Lin-
guistic Cues. Proceedings of the COLING-ACL?98,
881-885.
Jeffrey C. Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. PhD thesis. University of
Pennsylvania.
272
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1886?1896, Dublin, Ireland, August 23-29 2014.
Learning the Taxonomy of Function Words for Parsing
Dongchen Li, Xiantao Zhang, Dingsheng Luo and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
{lidc,zhangxt,dsluo,wxh}@cis.pku.edu.cn
Abstract
Completely data-driven grammar training is prone to over-fitting. Human-defined word class
knowledge is useful to address this issue. However, the manual word class taxonomy may be
unreliable and irrational for statistical natural language processing, aside from its insufficient
linguistic phenomena coverage and domain adaptivity. In this paper, a formalized representation
of function word subcategorization is developed for parsing in an automatic manner. The function
word classification representing intrinsic features of syntactic usages is used to supervise the
grammar induction, and the structure of the taxonomy is learned simultaneously. The grammar
learning process is no longer a unilaterally supervised training by hierarchical knowledge, but
an interactive process between the knowledge structure learning and the grammar training. The
established taxonomy implies the stochastic significance of the diversified syntactic features.
The experiments on both Penn Chinese Treebank and Tsinghua Treebank show that the proposed
method improves parsing performance by 1.6% and 7.6% respectively over the baseline.
1 Introduction
Probabilistic context-free grammar (PCFG) is widely used in the fields of speech recognition, machine
translation, information retrieval, etc. It takes the empirical rules and probabilities from a Treebank.
However, due to the context-free assumption, PCFG does not always perform well (Klein and Man-
ning, 2003). For instance, it assumes adverbs, including temporal adverbs, degree adverbs and negation
adverbs, to share the same distribution, whereas the distinction would provide useful indication for dis-
ambiguating the syntactic structure of the context.
It arose that the manual word classification in linguistic research was used to enrich PCFG and im-
prove the performance. However, from the point of view of statistical natural language processing, there
are some drawbacks for manual classification. Firstly, Linguistic phenomena covered by the manual
refinement may be limited by the linguistic observations of human. Secondly, the evidence of manual
refinement is often based on a particular corpus or specific sources of knowledge acquisition. As a result,
its adaptivity to different domains or genres may be insufficient. As for function words, due to the ambi-
guity and complexity in syntactic grammar, it is more difficult to develop formalized representation than
for content words. There are diversified standards for grammar refinement. Consequently, the word clas-
sification or category refinement can be conducted in distinct manners, while each of them is reasonable
in some sense. A delicate hierarchical classification inevitably involves in multiple dividing standards.
However, the word sets under distinct dividing standards may be overlapping. The problems come up
that how to choose the set of the multiple standards to cooperate to build the taxonomy, and how to de-
cide the priority of each standard. Regarding that the manual method is hard to overcome critical issues,
manual taxonomy for function words may not be reliable for statistical natural language processing.
This article attempts to address these issues in a data-driven manner. we first manually construct a
cursory and flat classification of function words. A hierarchical split-merge approach is employed to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1886
introduce our classification, and the PCFG training procedure is supervised to alleviate the over-fitting
issue. The priorities of the subcategorization standards are determined by the measurement of effec-
tiveness for parsing in a greedy manner in the hierarchical classification. And the hierarchical structure
of the classification is learned by data-driven approach in the course of grammar induction, so as to fit
the practical usages in the Treebank. Accordingly, the grammar learning process is no longer a unilat-
erally supervised training by hierarchical knowledge, but an interactive process between the knowledge
representation induction and the grammar training. That is, the grammar induction is supervised by the
knowledge and the structure of the taxonomy is learned simultaneously. These two processes are iterated
for several rounds and the hierarchical structure of the function word taxonomy is constructed. In each
round, the induced grammar could benefit from the optimized taxonomy during the learning process. The
category split in the early rounds take more priorities than in the late ones. Thus, the learned taxonomy
implies the stochastic significance of the series of the syntactic features.
Experiments on Penn Chinese Treebank Fifth Edition (CTB5.0) (Xue et al., 2002) and Tsinghua Chi-
nese Treebank (TCT) (Zhou, 2004) are performed. The results show that the induced grammars with
refined conjunction categories gain parsing performance improvement by 1.6% on CTB and by 7.6% on
TCT. During the training process, a taxonomy of function words is learned, which reflects their practical
usages in the corpus.
The rest of this paper is organized as follows. We first review related work on category refinement
for parsing. Then we describe our manually defined categories of function words in Section 3. The
hierarchical state-split approach for introducing the the categories are presented in Section 4, and our
taxonomy learning method is described in Section 5. In Section 7, experimental comparison is conducted
among various methods on granularity choosing. And conclusions of this research are drawn in last
section.
2 Related Work
A variety of techniques have been proposed to enrich PCFG in either manual (Klein and Manning, 2003;
Zhang and Clark, 2011) or automatic (Petrov, 2009; Cohen et al., 2012) manner.
2.1 Automatic Refinement of Function Words for Parsing
One way of grammar refinement is data-driven state-split methods (Matsuzaki et al., 2005; Prescher,
2005). The part-of-speech and syntactic tags in the grammar are automatically split to encode the kinds
of linguistic distinctions exhibited in the Treebank. The hierarchical state-split approach (Petrov et al.,
2006) started from a bare-bones Treebank derived grammar, and iteratively refined it in a split-merge-
smooth cycle with the EM-based parameter re-estimation. It achieved state of the art accuracies for many
languages including English, Chinese and German.
One tag is usually heterogeneous, in the sense that its word set can be of multiple different types.
Nevertheless, the automatic process tries to split the tags through a greedy data-driven manner, where
multiple distinctive information is used simultaneously when dividing tags. Thus the refined tags are
not intuitively interpretable. Meanwhile, considering that the EM algorithm usually gets stuck at a sub-
optimal configuration, this data-driven method suffers from the risk of over-fitting. As shown in their
experiments, there is little to be gained from splitting the closed part-of-speech classes (e.g. DT, CC, IN)
or the nonterminal ADJP.
To alleviate the risk of over-fitting, we employ the human-defined knowledge to constrain the splitting
process in this research. Based on the state-split model, our approach aims to reach a compromise
between manual and automatic refinement approaches.
2.2 Manual Refinement of Function Words for Parsing
The other way to refine the annotation for training a parser is incorporating knowledge base. Semantic
knowledge of content words has been proved to be effective in alleviate the data sparsity. Some re-
searches utilized semantic knowledge in WordNet (Miller, 1995; Fellbaum, 1999) for English parsing
(Fujita et al., 2010; Agirre et al., 2008), and Xiong et al. (2005; Lin et al. (2009) improved Chinese pars-
1887
ing by incorporating semantic knowledge in HowNet (Dong and Dong, 2003; Dong and Dong, 2006).
While WordNet and Hownet contain word classification for content words, Li et al. (2014b; Li et al.
(2014a) have focused on exploiting manual classification for conjunction in parsing.
Klein and Manning (2003) examined the annotation in Penn English Treebank, manually split the ma-
jority of the part-of-speech (POS) tags. For the function words, they split the tag ?IN? into subordinating
conjunctions, complementizers and prepositions, and appended
?
BE to all forms of ?be? and
?
HAVE to
all forms of ?have?. Conjunction tags are also marked to indicate whether they were ?But?, ?but? or
?&?. The experimental results showed that the split tags of function words surprisingly make much con-
tribution to the overall improved parsing accuracy. Levy and Manning (2003) transferred this work to
Penn Chinese Treebank. They found that, in some cases, certain adverbs such as ?however (,)? and
?especially (c??)? preferred IP modification and could help disambiguate IP coordination from VP
coordination. To capture this point, they marked those adverbs possessing an IP grandparent. However,
these manual refinement methods seems to split the tags in a rough way, which might account for a mod-
est accuracy achieved. Some existing work used heuristic rules to simply split the tags of function words
(Klein and Manning, 2003; Levy and Manning, 2003). They demonstrated that many function words
stood out to be helpful in predicting the syntactic structure and syntactic label.
3 Manual Tabular Subcategories of Function Words
When subcategorizing function words, in this section, we manually list various grammatical distinctions
that are commonly made in traditional and generative grammar in a fairly flat taxonomy. The grammar
training procedure learns by using our manual taxonomy as a starting point, and constructs a reasonable
and subtle hierarchical strucutre based on the distribution of function words usages in the corpus.
Based on some existing knowledge base (Xia, 2000; Xue et al., 2000; Zhu et al., 1995; Wang and
Yu, 2003) and previous research work (Li et al., 2014b), we investigate and summarize the usage of
function words, and come up with a hierarchical subcategories. The taxonomy of the function words is
represented in a tree structure, where each subcategory of a function word corresponds to a node in the
taxonomy, the nonterminals are subcategories and the terminals are words.
For the convenience and consistence, our manual classification just gives a rough and broad taxonomy.
It is labor-intensive and error-prone of classifying the function words manually to produce a consistent
output. Fine-grained hierarchical structure is not obligatory, but would be harmful if inappropriately clas-
sified, as it may mislead the learning process. To avoid this kind of risk, the elaboration is saved, rather
than introducing unnecessary bias. The learning process would perform the hierarchical classification
according to the distribution in the corpus.
For instance, the distinction within conjunctions is intricate. Conjunctions are the words that are
called ?connective words? in traditional Chinese grammar books. In Penn Chinese Treebank, they are
tagged as coordinating conjunctions (CC), subordinating conjunctions (CS), or adverbs (AD) according
to their syntactic distribution. CC conjoins two equivalent constituents (noun phrases, clauses, etc.),
each of which has approximately the same function as the whole construction. CS precedes a subordi-
nating clause, whereas conjunctive adverbs often appear in the main clause and pair with a subordinating
conjunction (e.g., if (XJ)/CS ... then (?)/AD). However, in Chinese, it is often hard to tell the sub-
ordinating clause from the main clause in the compound statement. As a result, in the prospective of
linguistic computing, the confusion is that, CS and conjunctive adverbs both precedes the subordinating
clauses or main clauses, while CC connects two phrases or precedes the main clause. In our scheme,
we simply conflates the CC, CS and conjunctive adverbs together. This result in a general ?conjunction?
category, within which we just enumerate all the possible uses of the conjunctions. As a result, the struc-
ture of our human-defined taxonomy is fairly flat, as briefly shown in Figure 1 and Figure 2. Our scheme
releases our hands from the confusing situations, by leaving them to our data-driven method described in
the following section. Figure 1 and Figure 2 abbreviate the manual classification and their corresponding
examples.
Many prepositions in Chinese are evolved from verbs, thus the linguistic characteristics of preposi-
tions are somewhat similar to verbs. Therefore, this paper divides the preposition word set according to
1888
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
Progression: not only?=
Transition: although?,
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Assumption: ifXJ
Universalization: whatever??
Unnecessary Condition: sinceQ,
Insufficient Condition: although=?
Sufficient Condition: as long as??
Necessary Condition: only if?k
Equality: unless??
... ...
Figure 1: Abbreviated Hierarchical subcategories of subordinating conjunctions with examples.
Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
Preference: would rather?X
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
Therefore: so??
Then, As a result: as soon?
So that: so that?B
Progression
?
?
?
?
?
?
?
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Adjunct Adverb
?
?
?
Frequency Adverbs: for many times?g
Degree Adverbs: very4?
...
...
Figure 2: Abbreviated Hierarchical subcategories of adverbs with examples.
the types of their associated arguments: ?benefactive?, such as ??(for)? and ??(to)?, marks the ben-
eficiary of an action; ?locative?, such as ?3(in)?, marks adverbials that indicate the place of the event;
?direction?, such as ? ?(towards)? and ? d(from)?, marks adverbials that answer the questions ?from
where?? and ?to where??; ?temporal?, such as ?3(on)?, marks temporal or aspectual adverbials that
answer the question ?when??, and so on.
4 Refining Grammar with Hierarchical Category Refinement
In this section, we choose the appropriate granularity in a data-driven manner based on the split-merge
learning method in Section 2.1. Our approach first initializes the categories with the most general sub-
categories in the taxonomy and then splits the categories through the hypernym-hyponym relation in the
1889
taxonomy. Data-driven method is used to merge the overly refined subcategories.
The top category in the taxonomy is used as the starting annotations of POS tags. As we cannot
predict which layer should be the most adequate one, we try to avoid applying any priori restriction on
the refinement granularity, and start with the most general tags.
With the hierarchical knowledge, it turns out to be a critical issue that which granularity should be
used to refine the tags for parsing. We intend to take neither too coarse subcategories nor too fine ones in
the hierarchical knowledge for parsing. Instead, it would be our advantage to split the tags with the very
granularity where needed, rather than splitting them all to one specific granularity in the taxonomy.
For example, ?Conjunctive Adverbs? are divided into three subcategories in our taxonomy as shown
in Figure 2. The evidence for the refinement may occur in very rare case, and certainly some of the
context of the different subcategories are quite the same. Splitting symbols with the same context is
not only unnecessary, but potentially harmful, since it unreasonably fragments observations of other
symbols.behavior.
In this paper, the hierarchical subcategory knowledge is used to refine grammars by supervising the
automatic hierarchical state-split approach. In the split stage in each cycle, the function word subcategory
is split along the hierarchy of the knowledge, instead of being randomly split and classified automatically.
In this way, we try to alleviate the over-fitting of the greedy data-driven approach, and a new set of
knowledge-related tags are generated. In the following step, we retreat some of the split subcategories to
their more general layer according to its likelihood loss of merging them. In this way, we try to avoid the
excessive refinement in our hierarchical knowledge without sufficient data support.
There are two issues that we have to consider in this process: a) how to deal with the polysemous
words, and b) how to deal with the multi-branch situation other than binary branch in the taxonomy.
Regarding to the polysemous words, they occur mostly in two situation for function words. Some are the
polysemous words which can be taken as conjunctions or auxiliary words, while the others can be taken
as preposition or adverbs. Fortunately there is no ambiguity for a word given its POS tag, so we could
neglect this situation in the split process when training. We demonstrated the solution for the multiple
branches in the Section 5.
5 Learning the Taxonomy of Function Words
There are multiple subcategorization criterions for building function word taxonomy, and it is diffi-
culty for human to rank the ordering in the classification process. This section represents the method
of learning the taxonomy of the function words in data-driven manner. Based on the manual tabular
classification, the similar word classes are conflated to express the data distribution.
The multiple branches in the taxonomy are intractable for the original split-merge method, because it
splits every category into two and merges half of them for efficiency. If we follow this scheme in our
training process, it would be difficult to deal with the multi-branch situation in the taxonomy, because
how to choose the first two to split among the multiple branches is another challenge. It is an equally
difficult problem for us to binarize the taxonomy by hand comparing to directly choosing the granularity.
It would be our advantage to binarize the taxonomy by a data-driven method. For automatic binariza-
tion, a straightforward approach is to measure the utility of traversing all the plausible ways of cutting all
the branches into two sets individually and use the best one. Then we can deal with the divided two sets
in the same manner recursively. However, not only is this impractical, requiring an entire training phase
for each possible binarization scheme which is exponentially expensive, but it assumes the contributions
of multiple binarizations in different branches are independent. In fact, extra sub-symbols may need to
be added to several nonterminals before they can cooperate to pass information along the parse tree.
Therefore, we go in the opposite direction, and propose an extended version of split-merge learning
to handle the multiple branches in the taxonomy. That is, we split each state into all the subcategories in
the lower layer in the taxonomy even if it has multiple branches, train, and then measure for every two
sibling subcategories in the same layer the loss in likelihood incurred when merging them. If this loss is
small, the new division of these two subcategories does not carry enough useful information and can be
merged back. Contrary to the gain in likelihood for splitting, the loss in likelihood for merging can be
1890
efficiently approximated (Petrov et al., 2006).
More specifically, we assume transitivity in merging multiple subcategories in one layer. Figure 3
gives an illustration. After the split stage, the category A has been split into subcategories A-1, A-2, ...
to A-7. Then we compute the loss in likelihood of the training data by merging back each pair of two
subcategories through A-1 to A-7. If the loss is lower than a certain threshold
1
set for each round of
merge, this pair of newly split subcategories will be merged. We only show the sibling ones for brevity in
this example. Assume the losses of merging these pairs (A-1, A-2), (A-2, A-3), (A-3, A-4) and (A-4, A-
5) are below the threshold ?. Thus, A-1, A-2, A-3, A-4 and A-5 are merged to X-1 due to the transitivity
of the connected points, where X-1 is the automatically generated subcategory which contains the five
conflated subcategories as its descendants. At the meantime, A-6 and A-7 still remain. This scheme is an
approximation because it merges subcategories that should be merged with the same subcategory. But it
will leave the split of this instances to the next round when more evidence on interaction with other more
refined subcategories is given.
(a) Refined subcategories before the merge stage (b) Refined subcategories after the merge stage
Figure 3: Illustration of merging the subcategories for multiple branches in the taxonomy. Where ? is
a certain threshold below which this pair of subcategories will be merged, and X is the automatically
generated subcategory which contains the conflated subcategories as its descendants.
After merging in each round, the hierarchical knowledge is reshaped to fit the practical usage in the
Treebank. The split-merge cycles allow us to progressively increase the complexity of the hierarchical
knowledge, and the more useful distinctions are represented as the higher level in the taxonomy, which
gives priority to the most useful distinctions in return by supervising the grammar induction. Figure 4
demonstrates the transformation of the hierarchical structure from the tabular classification. Along this
road, the training scheme is not a unilateral training, but an interactive process between the knowledge
representation learning and the grammar training. Our learning process exerts a mutual effect to both the
induced grammar and the optimized structure of the hierarchical knowledge. In this way, the set of di-
viding standards are chosen iteratively according to their syntactic features. The more effective divisions
are conducted in the early stages. In the following stages, the divisions which interact with previous
divisions to give the most effective disambiguating information are adopted. The final taxonomy are
built based on manual classification in data-driven approach, and the hierarchical structure are optimized
and rational in the perspective of actual data distribution. Figure 4 illustrates a concrete instance of the
procedure of learning the taxonomy. On one hand, this procedure provides a more rational hierarchical
subcategorization structure according to data distribution. On the other hand, the order of the division
criterions represents the priorities the grammar induction takes for each criterion. The structure in the
higher levels of the taxonomy are determined by the dominant syntactic characteristics. And the division
in the later iterations are on the basis of minor distinctive characteristics.
6 Experiments and Results
6.1 Data Set
We present experimental results on both CTB5.0 (All traces and functional tags were stripped.) and TCT.
We ran experiments on CTB5.0 using the standard data allocation: files from CHTB 001.fid to
CHTB 270.fid, and files from CHTB 400.fid to CHTB 1151.fid were used as training set. The develop-
ment set includes files from CHTB 301.fid to CHTB 325.fid, and the test set includes files CHTB 271.fid
1
In practice, instead of setting a predefined threshold for merging, we merge a specific number of the newly split subcate-
gories.
1891
AA-7A-6A-5A-4A-3A-2A-1
(a) First round of category split
A
A-7A-6X-1
X-1
A-5A-4A-3A-2A-1
(b) First round of category merge
A
A-7A-6X-1
A-5A-4A-3A-2A-1
(c) Second round of category split
A
A-7A-6X-1
X-3X-2
X-2
A-2A-1
X-3
A-5A-4A-3
(d) Second round of category merge
A
A-7A-6X-1
X-3
A-5A-4A-3
X-2
A-2A-1
(e) Third round of category split
A
A-7A-6X-1
X-3
X-4A-3
X-2
A-2A-1
X-4
A-5A-4
(f) Third round of category merge
Figure 4: Iteration of grammar induction and taxonomy structure learning
to CHTB 300.fid. Experiments on TCT use the data set as in CIPS-SIGHAN-ParsEval-2012 (Zhou,
2012). We have parsed on the segmented text in the Treebank, namely, no use of gold POS-tags, use
of gold segmentations, and full-length sentences. This is the same as for other 5 parsers in Table 1 for
comparison. All the experiments were carried out after six cycles of split-merge.
1892
6.2 Final Results
The final results are shown in Table 1. Our final parsing performance is higher than both the manual
annotation method (Levy and Manning, 2003) and the data-driven method (Petrov, 2009).
Parser Precision Recall F
1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Lin(2009) 86.00 83.10 84.50
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
This paper 86.55 83.41 84.95
Table 1: Our final parsing performance compared with the best previous work on CTB5.0.
On test set TCT, the method achieves the best precision, recall and F-measure in the CIPS-SIGHAN-
ParsEval-2012 competition, and table 2 compares our results with the system of Beijing Information
Science and Technology University (BISTU) which got the second place in the competition.
Parser Precision Recall F
1
BISTU 70.10 68.08 69.08
This paper 76.81 76.66 76.74
Table 2: Our final parsing performance compared with the best previous works on TCT.
Given the manual labor required for generating the taxonomy (and in languages where there is a
taxonomy, determining whether it is suitable), this first study focuses on a language where there is quite
a bit of under- and over-specification in the Treebanks? tag sets. So this work is only implemented on
Chinese. We regard it as future work to transfer this approach to other languages.
6.3 Analysis
The outline of constructing the taxonomy of function words are as follows. Firstly, the function words are
manually subcategorized in a rough and cursory way. When dealing with subcategories hard to resolve
their relation of subordination, we simply treat them as siblings in the tree in a rather flat stricture, and
leave the elaboration of exquisite clustering to the algorithms. The data-driven approach in Section 4 au-
tomatically choose the appropriate granularity of refinement for our grammar. Moreover, the split-merge
learning for multiple branches in the hierarchical subcategories in Section 5 exploits the relationship be-
tween the sibling nodes in the same layer, making use of the Treebank data to adjust and optimize the
hierarchy.
During the split-merge process, the hierarchical subcategories are learned to fit the data, which is
a transformation of our manually defined hierarchy. The transformed hierarchy is just the route map
of subcategories employed in our model. As abbreviated in Figure 5 and Figure 6, many distinctions
between word sets of the subcategories have been exploited by our approach, and the learned taxonomy
is interpretable. For instance, It shows that the learned structure of the taxonomy is reasonable.
6.4 Comparison with Previous Work
Although the taxonomy of function words are learned in the grammar training process, the grammar is
trained on the Treebank in supervised manner. Thus, this work is not directly relevant with unsupervised
grammar induction literature (Headden III et al., 2009; Berant et al., 2007; Mare?cek and
?
Zabokrtsk`y,
2014).
1893
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
X
{
Progression: not only?=
Transition: although?,
X
{
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Universalization: ??
Equality: ??
X
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Assumption: XJ
Sufficient Condition: ??
Necessary Condition: ?k
X
{
Unnecessary Condition: Q,
Insufficient Condition: =?
...
Figure 5: Abbreviated automatically learned hierarchical subcategories of subordinating conjunctions
with examples. Where ?X? represents the automatically generated subcategory.
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
?
?
Preference: would rather?X
X
{
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
?
?
X
{
Therefore: so??
So that: so that?B
Then, As a result: as soon?
Progression
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Figure 6: Abbreviated automatically learned hierarchical subcategories of adverbs with examples.
Lin et al. (2009) and Li et al. (2014b) presented ideas of using either hierarchical semantic knowledge
from HowNet for content words or grammar knowledge for subordinating conjunctions. They introduced
hierarchical subcategory knowledge in a different stage. They split the original Treebank categories
in split-merge process according to the data, and then find a method to map the subcategories to the
node in the taxonomy, and constrain their further splitting. Comparing to their work, our approach is
more delicate, which is splitting the categories according to the knowledge, and learning the knowledge
structure according to data during the training course. Lin et al. (2009) incorporated semantic knowledge
of content words into the data-driven method. It would be promising if this work stacks with the content
word knowledge. However, the work with content word knowledge have to handle the polysemous words
in the semantic taxonomy, so they split the categories according to the data, and then find a way to map
the subcategories to the node in the taxonomy, and constrain their further splitting. It is our goal to make
these two methods compatible with each other.
Incorporating word formation knowledge achieved higher parsing accuracy according to Zhang and
1894
Clark (2011). However, they ran their experiment on gold POS-tags and a different data set split, which
is different form the setup of work in Table 1 including this work. They also presented their result on
automatically assigned POS-tags and the same data set split as in the work in Table 1 to facilitate the
performance comparison. It gave F
1
score of 81.45% for sentences with less than 40 words and 78.3%
for all sentences, significantly lower than Petrov and Klein (2007).
Zhang et al. (2013) exhaustively exploited character-level syntactic structures for words, and achieved
84.43% on F
1
measure. They placed more emphasis on the word-formation of content words, which
our model highlights the value of the function words. The complementary intuitions make it possible to
integrate these approaches together in the future work.
7 Conclusion
This paper presents an approach for inducing finer syntactic categories while learning the taxonomy for
function words. It used linguistic insight to guide the state-split process, and the hierarchical structure
representing syntactic features of function word usages was established during the grammar training
process. Empirical evidence has been provided that automatically subcategorizing function words con-
tributes to high parsing performance. The induced grammar supervised by the taxonomy outperformed
pervious approaches, which benefited from both the knowledge and the data-driven method. The pro-
posed approach for learning the structure of the taxonomy could be generalized to construct semantic
knowledge base.
Acknowledgments
This work was supported in part by the National Basic Research Program of China (973 Program) under
grant 2013CB329304, the Research Special Fund for Public Welfare Industry of Health under grant
201202001, the Key National Social Science Foundation of China under grant 12&ZD119, the National
Natural Science Foundation of China under grant 91120001.
References
Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and pp attachment performance
with sense information. Proceedings of ACL-08: HLT, pages 317?325.
Jonathan Berant, Yaron Gross, Matan Mussel, Ben Sandbank, Eytan Ruppin, and Shimon Edelman. 2007. Boost-
ing unsupervised grammar induction by splitting complex sentences on function words. In Proceedings of the
Boston University Conference on Language Development.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-
variable pcfgs. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 223?231. Association for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hybrid language and knowledge resource. In Proceedings
of the international conference on natural language processing and knowledge engineering, pages 820?824.
IEEE.
Zhengdong Dong and Qiang Dong. 2006. HowNet and the computation of meaning. World Scientific Publishing
Co. Pte. Ltd.
Christiane Fellbaum. 1999. WordNet. Wiley Online Library.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2010. Exploiting semantic information for hpsg
parse selection. Research on language and computation, 8(1):1?22.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109.
Association for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st annual
meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computational
Linguistics.
1895
Roger Levy and Christopher D Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 439?
446. Association for Computational Linguistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014a. Improved parsing with taxonomy of conjunctions. In 2014
IEEE China Summit & International Conference on Signal and Information Processing. IEEE.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014b. Learning grammar with explicit annotations for subordi-
nating conjunctions in chinese. In Proceedings of the 52th annual meeting of the Association for Computational
Linguistics Student Research Workshop. Association for Computational Linguistics.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu, and Huisheng Chi. 2009. Refining grammars for parsing
with hierarchical semantic knowledge. In Proceedings of the 2009 conference on empirical methods in natural
language processing: Volume 3-Volume 3, pages 1298?1307. Association for Computational Linguistics.
David Mare?cek and Zden?ek
?
Zabokrtsk`y. 2014. Dealing with function words in unsupervised dependency parsing.
In Computational Linguistics and Intelligent Text Processing, pages 250?261. Springer.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Pro-
ceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 75?82. Association
for Computational Linguistics.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human language technologies
2007: the conference of the North American chapter of the Association for Computational Linguistics, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st international conference on computational linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural language processing. Ph.D. thesis, University of California.
Detlef Prescher. 2005. Inducing head-driven pcfgs with latent heads: Refining a tree-bank grammar for parsing.
In Machine Learning: ECML 2005, pages 292?304. Springer.
Hui Wang and Shiwen Yu. 2003. The semantic knowledge-base of contemporary chinese and its applications
in wsd. In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages
112?118. Association for Computational Linguistics.
Fei Xia. 2000. The part-of-speech tagging guidelines for the penn chinese treebank (3.0). Technical report.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the penn chinese treebank
with semantic knowledge. In Natural language processing?IJCNLP 2005, pages 70?81. Springer.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch. 2000. The bracketing guidelines for the penn chinese
treebank (3.0). Technical report.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In
Proceedings of the 19th international conference on computational linguistics-Volume 1, pages 1?8. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational linguistics, 37(1):105?151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. 51st
annual meeting of the Association for Computational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese treebank. Journal of Chinese information processing, 18(4):1?
8.
Qiang Zhou. 2012. Evaluation report of the third chinese parsing evaluation: Cips-sighan-parseval-2012. In
Proceedings of the second CIPS-SIGHAN joint conference on Chinese language processing, pages 159?167.
Xuefeng Zhu, Shiwen Yu, and Hui Wang. 1995. The development of contemporary chinese grammatical knowl-
edge base and its applications. International journal of asian language processing, 5(1,2):39?41.
1896
